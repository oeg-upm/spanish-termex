{
    "id": "I-63",
    "original_text": "Combinatorial Resource Scheduling for Multiagent MDPs Dmitri A. Dolgov, Michael R. James, and Michael E. Samples AI and Robotics Group Technical Research, Toyota Technical Center USA {ddolgov, michael.r.james, michael.samples}@gmail.com ABSTRACT Optimal resource scheduling in multiagent systems is a computationally challenging task, particularly when the values of resources are not additive. We consider the combinatorial problem of scheduling the usage of multiple resources among agents that operate in stochastic environments, modeled as Markov decision processes (MDPs). In recent years, efficient resource-allocation algorithms have been developed for agents with resource values induced by MDPs. However, this prior work has focused on static resource-allocation problems where resources are distributed once and then utilized in infinite-horizon MDPs. We extend those existing models to the problem of combinatorial resource scheduling, where agents persist only for finite periods between their (predefined) arrival and departure times, requiring resources only for those time periods. We provide a computationally efficient procedure for computing globally optimal resource assignments to agents over time. We illustrate and empirically analyze the method in the context of a stochastic jobscheduling domain. Categories and Subject Descriptors I.2.8 [Artificial Intelligence]: Problem Solving, Control Methods, and Search; I.2.11 [Artificial Intelligence]: Distributed Artificial Intelligence-Multiagent systems General Terms Algorithms, Performance, Design 1. INTRODUCTION The tasks of optimal resource allocation and scheduling are ubiquitous in multiagent systems, but solving such optimization problems can be computationally difficult, due to a number of factors. In particular, when the value of a set of resources to an agent is not additive (as is often the case with resources that are substitutes or complements), the utility function might have to be defined on an exponentially large space of resource bundles, which very quickly becomes computationally intractable. Further, even when each agent has a utility function that is nonzero only on a small subset of the possible resource bundles, obtaining optimal allocation is still computationally prohibitive, as the problem becomes NP-complete [14]. Such computational issues have recently spawned several threads of work in using compact models of agents preferences. One idea is to use any structure present in utility functions to represent them compactly, via, for example, logical formulas [15, 10, 4, 3]. An alternative is to directly model the mechanisms that define the agents utility functions and perform resource allocation directly with these models [9]. A way of accomplishing this is to model the processes by which an agent might utilize the resources and define the utility function as the payoff of these processes. In particular, if an agent uses resources to act in a stochastic environment, its utility function can be naturally modeled with a Markov decision process, whose action set is parameterized by the available resources. This representation can then be used to construct very efficient resource-allocation algorithms that lead to an exponential speedup over a straightforward optimization problem with flat representations of combinatorial preferences [6, 7, 8]. However, this existing work on resource allocation with preferences induced by resource-parameterized MDPs makes an assumption that the resources are only allocated once and are then utilized by the agents independently within their infinite-horizon MDPs. This assumption that no reallocation of resources is possible can be limiting in domains where agents arrive and depart dynamically. In this paper, we extend the work on resource allocation under MDP-induced preferences to discrete-time scheduling problems, where agents are present in the system for finite time intervals and can only use resources within these intervals. In particular, agents arrive and depart at arbitrary (predefined) times and within these intervals use resources to execute tasks in finite-horizon MDPs. We address the problem of globally optimal resource scheduling, where the objective is to find an allocation of resources to the agents across time that maximizes the sum of the expected rewards that they obtain. In this context, our main contribution is a mixed-integerprogramming formulation of the scheduling problem that chooses globally optimal resource assignments, starting times, and execution horizons for all agents (within their arrival1220 978-81-904262-7-5 (RPS) c 2007 IFAAMAS departure intervals). We analyze and empirically compare two flavors of the scheduling problem: one, where agents have static resource assignments within their finite-horizon MDPs, and another, where resources can be dynamically reallocated between agents at every time step. In the rest of the paper, we first lay down the necessary groundwork in Section 2 and then introduce our model and formal problem statement in Section 3. In Section 4.2, we describe our main result, the optimization program for globally optimal resource scheduling. Following the discussion of our experimental results on a job-scheduling problem in Section 5, we conclude in Section 6 with a discussion of possible extensions and generalizations of our method. 2. BACKGROUND Similarly to the model used in previous work on resourceallocation with MDP-induced preferences [6, 7], we define the value of a set of resources to an agent as the value of the best MDP policy that is realizable, given those resources. However, since the focus of our work is on scheduling problems, and a large part of the optimization problem is to decide how resources are allocated in time among agents with finite arrival and departure times, we model the agents planning problems as finite-horizon MDPs, in contrast to previous work that used infinite-horizon discounted MDPs. In the rest of this section, we first introduce some necessary background on finite-horizon MDPs and present a linear-programming formulation that serves as the basis for our solution algorithm developed in Section 4. We also outline the standard methods for combinatorial resource scheduling with flat resource values, which serve as a comparison benchmark for the new model developed here. 2.1 Markov Decision Processes A stationary, finite-domain, discrete-time MDP (see, for example, [13] for a thorough and detailed development) can be described as S, A, p, r , where: S is a finite set of system states; A is a finite set of actions that are available to the agent; p is a stationary stochastic transition function, where p(σ|s, a) is the probability of transitioning to state σ upon executing action a in state s; r is a stationary reward function, where r(s, a) specifies the reward obtained upon executing action a in state s. Given such an MDP, a decision problem under a finite horizon T is to choose an optimal action at every time step to maximize the expected value of the total reward accrued during the agents (finite) lifetime. The agents optimal policy is then a function of current state s and the time until the horizon. An optimal policy for such a problem is to act greedily with respect to the optimal value function, defined recursively by the following system of finite-time Bellman equations [2]: v(s, t) = max a r(s, a) + X σ p(σ|s, a)v(σ, t + 1), ∀s ∈ S, t ∈ [1, T − 1]; v(s, T) = 0, ∀s ∈ S; where v(s, t) is the optimal value of being in state s at time t ∈ [1, T]. This optimal value function can be easily computed using dynamic programming, leading to the following optimal policy π, where π(s, a, t) is the probability of executing action a in state s at time t: π(s, a, t) = ( 1, a = argmaxa r(s, a) + P σ p(σ|s, a)v(σ, t + 1), 0, otherwise. The above is the most common way of computing the optimal value function (and therefore an optimal policy) for a finite-horizon MDP. However, we can also formulate the problem as the following linear program (similarly to the dual LP for infinite-horizon discounted MDPs [13, 6, 7]): max X s X a r(s, a) X t x(s, a, t) subject to: X a x(σ, a, t + 1) = X s,a p(σ|s, a)x(s, a, t) ∀σ, t ∈ [1, T − 1]; X a x(s, a, 1) = α(s), ∀s ∈ S; (1) where α(s) is the initial distribution over the state space, and x is the (non-stationary) occupation measure (x(s, a, t) ∈ [0, 1] is the total expected number of times action a is executed in state s at time t). An optimal (non-stationary) policy is obtained from the occupation measure as follows: π(s, a, t) = x(s, a, t)/ X a x(s, a, t) ∀s ∈ S, t ∈ [1, T]. (2) Note that the standard unconstrained finite-horizon MDP, as described above, always has a uniformly-optimal solution (optimal for any initial distribution α(s)). Therefore, an optimal policy can be obtained by using an arbitrary constant α(s) > 0 (in particular, α(s) = 1 will result in x(s, a, t) = π(s, a, t)). However, for MDPs with resource constraints (as defined below in Section 3), uniformly-optimal policies do not in general exist. In such cases, α becomes a part of the problem input, and a resulting policy is only optimal for that particular α. This result is well known for infinite-horizon MDPs with various types of constraints [1, 6], and it also holds for our finite-horizon model, which can be easily established via a line of reasoning completely analogous to the arguments in [6]. 2.2 Combinatorial Resource Scheduling A straightforward approach to resource scheduling for a set of agents M, whose values for the resources are induced by stochastic planning problems (in our case, finite-horizon MDPs) would be to have each agent enumerate all possible resource assignments over time and, for each one, compute its value by solving the corresponding MDP. Then, each agent would provide valuations for each possible resource bundle over time to a centralized coordinator, who would compute the optimal resource assignments across time based on these valuations. When resources can be allocated at different times to different agents, each agent must submit valuations for every combination of possible time horizons. Let each agent m ∈ M execute its MDP within the arrival-departure time interval τ ∈ [τa m, τd m]. Hence, agent m will execute an MDP with time horizon no greater than Tm = τd m−τa m+1. Let bτ be the global time horizon for the problem, before which all of the agents MDPs must finish. We assume τd m < bτ, ∀m ∈ M. The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1221 For the scheduling problem where agents have static resource requirements within their finite-horizon MDPs, the agents provide a valuation for each resource bundle for each possible time horizon (from [1, Tm]) that they may use. Let Ω be the set of resources to be allocated among the agents. An agent will get at most one resource bundle for one of the time horizons. Let the variable ψ ∈ Ψm enumerate all possible pairs of resource bundles and time horizons for agent m, so there are 2|Ω| × Tm values for ψ (the space of bundles is exponential in the number of resource types |Ω|). The agent m must provide a value vψ m for each ψ, and the coordinator will allocate at most one ψ (resource, time horizon) pair to each agent. This allocation is expressed as an indicator variable zψ m ∈ {0, 1} that shows whether ψ is assigned to agent m. For time τ and resource ω, the function nm(ψ, τ, ω) ∈ {0, 1} indicates whether the bundle in ψ uses resource ω at time τ (we make the assumption that agents have binary resource requirements). This allocation problem is NP-complete, even when considering only a single time step, and its difficulty increases significantly with multiple time steps because of the increasing number of values of ψ. The problem of finding an optimal allocation that satisfies the global constraint that the amount of each resource ω allocated to all agents does not exceed the available amount bϕ(ω) can be expressed as the following integer program: max X m∈M X ψ∈Ψm zψ mvψ m subject to: X ψ∈Ψm zψ m ≤ 1, ∀m ∈ M; X m∈M X ψ∈Ψm zψ mnm(ψ, τ, ω) ≤ bϕ(ω), ∀τ ∈ [1, bτ], ∀ω ∈ Ω; (3) The first constraint in equation 3 says that no agent can receive more than one bundle, and the second constraint ensures that the total assignment of resource ω does not, at any time, exceed the resource bound. For the scheduling problem where the agents are able to dynamically reallocate resources, each agent must specify a value for every combination of bundles and time steps within its time horizon. Let the variable ψ ∈ Ψm in this case enumerate all possible resource bundles for which at most one bundle may be assigned to agent m at each time step. Therefore, in this case there are P t∈[1,Tm](2|Ω| )t ∼ 2|Ω|Tm possibilities of resource bundles assigned to different time slots, for the Tm different time horizons. The same set of equations (3) can be used to solve this dynamic scheduling problem, but the integer program is different because of the difference in how ψ is defined. In this case, the number of ψ values is exponential in each agents planning horizon Tm, resulting in a much larger program. This straightforward approach to solving both of these scheduling problems requires an enumeration and solution of either 2|Ω| Tm (static allocation) or P t∈[1,Tm] 2|Ω|t (dynamic reallocation) MDPs for each agent, which very quickly becomes intractable with the growth of the number of resources |Ω| or the time horizon Tm. 3. MODEL AND PROBLEM STATEMENT We now formally introduce our model of the resourcescheduling problem. The problem input consists of the following components: • M, Ω, bϕ, τa m, τd m, bτ are as defined above in Section 2.2. • {Θm} = {S, A, pm, rm, αm} are the MDPs of all agents m ∈ M. Without loss of generality, we assume that state and action spaces of all agents are the same, but each has its own transition function pm, reward function rm, and initial conditions αm. • ϕm : A×Ω → {0, 1} is the mapping of actions to resources for agent m. ϕm(a, ω) indicates whether action a of agent m needs resource ω. An agent m that receives a set of resources that does not include resource ω cannot execute in its MDP policy any action a for which ϕm(a, ω) = 0. We assume all resource requirements are binary; as discussed below in Section 6, this assumption is not limiting. Given the above input, the optimization problem we consider is to find the globally optimal-maximizing the sum of expected rewards-mapping of resources to agents for all time steps: Δ : τ × M × Ω → {0, 1}. A solution is feasible if the corresponding assignment of resources to the agents does not violate the global resource constraint: X m Δm(τ, ω) ≤ bϕ(ω), ∀ω ∈ Ω, τ ∈ [1, bτ]. (4) We consider two flavors of the resource-scheduling problem. The first formulation restricts resource assignments to the space where the allocation of resources to each agent is static during the agents lifetime. The second formulation allows reassignment of resources between agents at every time step within their lifetimes. Figure 1 depicts a resource-scheduling problem with three agents M = {m1, m2, m3}, three resources Ω = {ω1, ω2, ω3}, and a global problem horizon of bτ = 11. The agents arrival and departure times are shown as gray boxes and are {1, 6}, {3, 7}, and {2, 11}, respectively. A solution to this problem is shown via horizontal bars within each agents box, where the bars correspond to the allocation of the three resource types. Figure 1a shows a solution to a static scheduling problem. According to the shown solution, agent m1 begins the execution of its MDP at time τ = 1 and has a lock on all three resources until it finishes execution at time τ = 3. Note that agent m1 relinquishes its hold on the resources before its announced departure time of τd m1 = 6, ostensibly because other agents can utilize the resources more effectively. Thus, at time τ = 4, resources ω1 and ω3 are allocated to agent m2, who then uses them to execute its MDP (using only actions supported by resources ω1 and ω3) until time τ = 7. Agent m3 holds resource ω3 during the interval τ ∈ [4, 10]. Figure 1b shows a possible solution to the dynamic version of the same problem. There, resources can be reallocated between agents at every time step. For example, agent m1 gives up its use of resource ω2 at time τ = 2, although it continues the execution of its MDP until time τ = 6. Notice that an agent is not allowed to stop and restart its MDP, so agent m1 is only able to continue executing in the interval τ ∈ [3, 4] if it has actions that do not require any resources (ϕm(a, ω) = 0). Clearly, the model and problem statement described above make a number of assumptions about the problem and the desired solution properties. We discuss some of those assumptions and their implications in Section 6. 1222 The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) (a) (b) Figure 1: Illustration of a solution to a resource-scheduling problem with three agents and three resources: a) static resource assignments (resource assignments are constant within agents lifetimes; b) dynamic assignment (resource assignments are allowed to change at every time step). 4. RESOURCE SCHEDULING Our resource-scheduling algorithm proceeds in two stages. First, we perform a preprocessing step that augments the agent MDPs; this process is described in Section 4.1. Second, using these augmented MDPs we construct a global optimization problem, which is described in Section 4.2. 4.1 Augmenting Agents MDPs In the model described in the previous section, we assume that if an agent does not possess the necessary resources to perform actions in its MDP, its execution is halted and the agent leaves the system. In other words, the MDPs cannot be paused and resumed. For example, in the problem shown in Figure 1a, agent m1 releases all resources after time τ = 3, at which point the execution of its MDP is halted. Similarly, agents m2 and m3 only execute their MDPs in the intervals τ ∈ [4, 6] and τ ∈ [4, 10], respectively. Therefore, an important part of the global decision-making problem is to decide the window of time during which each of the agents is active (i.e., executing its MDP). To accomplish this, we augment each agents MDP with two new states (start and finish states sb , sf , respectively) and a new start/stop action a∗ , as illustrated in Figure 2. The idea is that an agent stays in the start state sb until it is ready to execute its MDP, at which point it performs the start/stop action a∗ and transitions into the state space of the original MDP with the transition probability that corresponds to the original initial distribution α(s). For example, in Figure 1a, for agent m2 this would happen at time τ = 4. Once the agent gets to the end of its activity window (time τ = 6 for agent m2 in Figure 1a), it performs the start/stop action, which takes it into the sink finish state sf at time τ = 7. More precisely, given an MDP S, A, pm, rm, αm , we define an augmented MDP S , A , pm, rm, αm as follows: S = S ∪ sb ∪ sf ; A = A ∪ a∗ ; p (s|sb , a∗ ) = α(s), ∀s ∈ S; p (sb |sb , a) = 1.0, ∀a ∈ A; p (sf |s, a∗ ) = 1.0, ∀s ∈ S; p (σ|s, a) = p(σ|s, a), ∀s, σ ∈ S, a ∈ A; r (sb , a) = r (sf , a) = 0, ∀a ∈ A ; r (s, a) = r(s, a), ∀s ∈ S, a ∈ A; α (sb ) = 1; α (s) = 0, ∀s ∈ S; where all non-specified transition probabilities are assumed to be zero. Further, in order to account for the new starting state, we begin the MDP one time-step earlier, setting τa m ← τa m − 1. This will not affect the resource allocation due to the resource constraints only being enforced for the original MDP states, as will be discussed in the next section. For example, the augmented MDPs shown in Figure 2b (which starts in state sb at time τ = 2) would be constructed from an MDP with original arrival time τ = 3. Figure 2b also shows a sample trajectory through the state space: the agent starts in state sb , transitions into the state space S of the original MDP, and finally exists into the sink state sf . Note that if we wanted to model a problem where agents could pause their MDPs at arbitrary time steps (which might be useful for domains where dynamic reallocation is possible), we could easily accomplish this by including an extra action that transitions from each state to itself with zero reward. 4.2 MILP for Resource Scheduling Given a set of augmented MDPs, as defined above, the goal of this section is to formulate a global optimization program that solves the resource-scheduling problem. In this section and below, all MDPs are assumed to be the augmented MDPs as defined in Section 4.1. Our approach is similar to the idea used in [6]: we begin with the linear-program formulation of agents MDPs (1) and augment it with constraints that ensure that the corresponding resource allocation across agents and time is valid. The resulting optimization problem then simultaneously solves the agents MDPs and resource-scheduling problems. In the rest of this section, we incrementally develop a mixed integer program (MILP) that achieves this. In the absence of resource constraints, the agents finitehorizon MDPs are completely independent, and the globally optimal solution can be trivially obtained via the following LP, which is simply an aggregation of single-agent finitehorizon LPs: max X m X s X a rm(s, a) X t xm(s, a, t) subject to: X a xm(σ, a, t + 1) = X s,a pm(σ|s, a)xm(s, a, t), ∀m ∈ M, σ ∈ S, t ∈ [1, Tm − 1]; X a xm(s, a, 1) = αm(s), ∀m ∈ M, s ∈ S; (12) where xm(s, a, t) is the occupation measure of agent m, and The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1223 (a) (b) Figure 2: Illustration of augmenting an MDP to allow for variable starting and stopping times: a) (left) the original two-state MDP with a single action; (right) the augmented MDP with new states sb and sf and the new action a∗ (note that the origianl transitions are not changed in the augmentation process); b) the augmented MDP displayed as a trajectory through time (grey lines indicate all transitions, while black lines indicate a given trajectory. Objective Function (sum of expected rewards over all agents) max X m X s X a rm(s, a) X t xm(s, a, t) (5) Meaning Implication Linear Constraints Tie x to θ. Agent is only active when occupation measure is nonzero in original MDP states. θm(τ) = 0 =⇒ xm(s, a, τ −τa m+1) = 0 ∀s /∈ {sb , sf }, a ∈ A X s/∈{sb,sf } X a xm(s, a, t) ≤ θm(τa m + t − 1) ∀m ∈ M, ∀t ∈ [1, Tm] (6) Agent can only be active in τ ∈ (τa m, τd m) θm(τ) = 0 ∀m ∈ M, τ /∈ (τa m, τd m) (7) Cannot use resources when not active θm(τ) = 0 =⇒ Δm(τ, ω) = 0 ∀τ ∈ [0, bτ], ω ∈ Ω Δm(τ, ω) ≤ θm(τ) ∀m ∈ M, τ ∈ [0, bτ], ω ∈ Ω (8) Tie x to Δ (nonzero x forces corresponding Δ to be nonzero.) Δm(τ, ω) = 0, ϕm(a, ω) = 1 =⇒ xm(s, a, τ − τa m + 1) = 0 ∀s /∈ {sb , sf } 1/|A| X a ϕm(a, ω) X s/∈{sb,sf } xm(s, a, t) ≤ Δm(t + τa m − 1, ω) ∀m ∈ M, ω ∈ Ω, t ∈ [1, Tm] (9) Resource bounds X m Δm(τ, ω) ≤ bϕ(ω) ∀ω ∈ Ω, τ ∈ [0, bτ] (10) Agent cannot change resources while active. Only enabled for scheduling with static assignments. θm(τ) = 1 and θm(τ + 1) = 1 =⇒ Δm(τ, ω) = Δm(τ + 1, ω) Δm(τ, ω) − Z(1 − θm(τ + 1)) ≤ Δm(τ + 1, ω) + Z(1 − θm(τ)) Δm(τ, ω) + Z(1 − θm(τ + 1)) ≥ Δm(τ + 1, ω) − Z(1 − θm(τ)) ∀m ∈ M, ω ∈ Ω, τ ∈ [0, bτ] (11) Table 1: MILP for globally optimal resource scheduling. Tm = τd m − τa m + 1 is the time horizon for the agents MDP. Using this LP as a basis, we augment it with constraints that ensure that the resource usage implied by the agents occupation measures {xm} does not violate the global resource requirements bϕ at any time step τ ∈ [0, bτ]. To formulate these resource constraints, we use the following binary variables: • Δm(τ, ω) = {0, 1}, ∀m ∈ M, τ ∈ [0, bτ], ω ∈ Ω, which serve as indicator variables that define whether agent m possesses resource ω at time τ. These are analogous to the static indicator variables used in the one-shot static resource-allocation problem in [6]. • θm = {0, 1}, ∀m ∈ M, τ ∈ [0, bτ] are indicator variables that specify whether agent m is active (i.e., executing its MDP) at time τ. The meaning of resource-usage variables Δ is illustrated in Figure 1: Δm(τ, ω) = 1 only if resource ω is allocated to agent m at time τ. The meaning of the activity indicators θ is illustrated in Figure 2b: when agent m is in either the start state sb or the finish state sf , the corresponding θm = 0, but once the agent becomes active and enters one of the other states, we set θm = 1 . This meaning of θ can be enforced with a linear constraint that synchronizes the values of the agents occupation measures xm and the activity 1224 The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) indicators θ, as shown in (6) in Table 1. Another constraint we have to add-because the activity indicators θ are defined on the global timeline τ-is to enforce the fact that the agent is inactive outside of its arrivaldeparture window. This is accomplished by constraint (7) in Table 1. Furthermore, agents should not be using resources while they are inactive. This constraint can also be enforced via a linear inequality on θ and Δ, as shown in (8). Constraint (6) sets the value of θ to match the policy defined by the occupation measure xm. In a similar fashion, we have to make sure that the resource-usage variables Δ are also synchronized with the occupation measure xm. This is done via constraint (9) in Table 1, which is nearly identical to the analogous constraint from [6]. After implementing the above constraint, which enforces the meaning of Δ, we add a constraint that ensures that the agents resource usage never exceeds the amounts of available resources. This condition is also trivially expressed as a linear inequality (10) in Table 1. Finally, for the problem formulation where resource assignments are static during a lifetime of an agent, we add a constraint that ensures that the resource-usage variables Δ do not change their value while the agent is active (θ = 1). This is accomplished via the linear constraint (11), where Z ≥ 2 is a constant that is used to turn off the constraints when θm(τ) = 0 or θm(τ + 1) = 0. This constraint is not used for the dynamic problem formulation, where resources can be reallocated between agents at every time step. To summarize, Table 1 together with the conservationof-flow constraints from (12) defines the MILP that simultaneously computes an optimal resource assignment for all agents across time as well as optimal finite-horizon MDP policies that are valid under that resource assignment. As a rough measure of the complexity of this MILP, let us consider the number of optimization variables and constraints. Let TM = P Tm = P m(τa m − τd m + 1) be the sum of the lengths of the arrival-departure windows across all agents. Then, the number of optimization variables is: TM + bτ|M||Ω| + bτ|M|, TM of which are continuous (xm), and bτ|M||Ω| + bτ|M| are binary (Δ and θ). However, notice that all but TM|M| of the θ are set to zero by constraint (7), which also immediately forces all but TM|M||Ω| of the Δ to be zero via the constraints (8). The number of constraints (not including the degenerate constraints in (7)) in the MILP is: TM + TM|Ω| + bτ|Ω| + bτ|M||Ω|. Despite the fact that the complexity of the MILP is, in the worst case, exponential1 in the number of binary variables, the complexity of this MILP is significantly (exponentially) lower than that of the MILP with flat utility functions, described in Section 2.2. This result echos the efficiency gains reported in [6] for single-shot resource-allocation problems, but is much more pronounced, because of the explosion of the flat utility representation due to the temporal aspect of the problem (recall the prohibitive complexity of the combinatorial optimization in Section 2.2). We empirically analyze the performance of this method in Section 5. 1 Strictly speaking, solving MILPs to optimality is NPcomplete in the number of integer variables. 5. EXPERIMENTAL RESULTS Although the complexity of solving MILPs is in the worst case exponential in the number of integer variables, there are many efficient methods for solving MILPs that allow our algorithm to scale well for parameters common to resource allocation and scheduling problems. In particular, this section introduces a problem domain-the repairshop problem-used to empirically evaluate our algorithms scalability in terms of the number of agents |M|, the number of shared resources |Ω|, and the varied lengths of global time bτ during which agents may enter and exit the system. The repairshop problem is a simple parameterized MDP adopting the metaphor of a vehicular repair shop. Agents in the repair shop are mechanics with a number of independent tasks that yield reward only when completed. In our MDP model of this system, actions taken to advance through the state space are only allowed if the agent holds certain resources that are publicly available to the shop. These resources are in finite supply, and optimal policies for the shop will determine when each agent may hold the limited resources to take actions and earn individual rewards. Each task to be completed is associated with a single action, although the agent is required to repeat the action numerous times before completing the task and earning a reward. This model was parameterized in terms of the number of agents in the system, the number of different types of resources that could be linked to necessary actions, a global time during which agents are allowed to arrive and depart, and a maximum length for the number of time steps an agent may remain in the system. All datapoints in our experiments were obtained with 20 evaluations using CPLEX to solve the MILPs on a Pentium4 computer with 2Gb of RAM. Trials were conducted on both the static and the dynamic version of the resourcescheduling problem, as defined earlier. Figure 3 shows the runtime and policy value for independent modifications to the parameter set. The top row shows how the solution time for the MILP scales as we increase the number of agents |M|, the global time horizon bτ, and the number of resources |Ω|. Increasing the number of agents leads to exponential complexity scaling, which is to be expected for an NP-complete problem. However, increasing the global time limit bτ or the total number of resource types |Ω|-while holding the number of agents constantdoes not lead to decreased performance. This occurs because the problems get easier as they become under-constrained, which is also a common phenomenon for NP-complete problems. We also observe that the solution to the dynamic version of the problem can often be computed much faster than the static version. The bottom row of Figure 3 shows the joint policy value of the policies that correspond to the computed optimal resource-allocation schedules. We can observe that the dynamic version yields higher reward (as expected, since the reward for the dynamic version is always no less than the reward of the static version). We should point out that these graphs should not be viewed as a measure of performance of two different algorithms (both algorithms produce optimal solutions but to different problems), but rather as observations about how the quality of optimal solutions change as more flexibility is allowed in the reallocation of resources. Figure 4 shows runtime and policy value for trials in which common input variables are scaled together. This allows The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1225 2 4 6 8 10 10 −3 10 −2 10 −1 10 0 10 1 10 2 10 3 10 4 Number of Agents |M| CPUTime,sec |Ω| = 5, τ = 50 static dynamic 50 100 150 200 10 −2 10 −1 10 0 10 1 10 2 10 3 Global Time Boundary τ CPUTime,sec |M| = 5, |Ω| = 5 static dynamic 10 20 30 40 50 10 −2 10 −1 10 0 10 1 10 2 Number of Resources |Ω| CPUTime,sec |M| = 5, τ = 50 static dynamic 2 4 6 8 10 200 400 600 800 1000 1200 1400 1600 Number of Agents |M| Value |Ω| = 5, τ = 50 static dynamic 50 100 150 200 400 500 600 700 800 900 1000 1100 1200 1300 1400 Global Time Boundary τ Value |M| = 5, |Ω| = 5 static dynamic 10 20 30 40 50 500 600 700 800 900 1000 1100 1200 1300 1400 Number of Resources |Ω| Value |M| = 5, τ = 50 static dynamic Figure 3: Evaluation of our MILP for variable numbers of agents (column 1), lengths of global-time window (column 2), and numbers of resource types (column 3). Top row shows CPU time, and bottom row shows the joint reward of agents MDP policies. Error bars show the 1st and 3rd quartiles (25% and 75%). 2 4 6 8 10 10 −3 10 −2 10 −1 10 0 10 1 10 2 10 3 Number of Agents |M| CPUTime,sec τ = 10|M| static dynamic 2 4 6 8 10 10 −3 10 −2 10 −1 10 0 10 1 10 2 10 3 10 4 Number of Agents |M| CPUTime,sec |Ω| = 2|M| static dynamic 2 4 6 8 10 10 −3 10 −2 10 −1 10 0 10 1 10 2 10 3 10 4 Number of Agents |M| CPUTime,sec |Ω| = 5|M| static dynamic 2 4 6 8 10 200 400 600 800 1000 1200 1400 1600 1800 2000 2200 Number of Agents |M| Value τ = 10|M| static dynamic 2 4 6 8 10 200 400 600 800 1000 1200 1400 1600 1800 2000 Number of Agents |M| Value |Ω| = 2|M| static dynamic 2 4 6 8 10 0 500 1000 1500 2000 2500 Number of Agents |M| Value |Ω| = 5|M| static dynamic Figure 4: Evaluation of our MILP using correlated input variables. The left column tracks the performance and CPU time as the number of agents and global-time window increase together (bτ = 10|M|). The middle and the right column track the performance and CPU time as the number of resources and the number of agents increase together as |Ω| = 2|M| and |Ω| = 5|M|, respectively. Error bars show the 1st and 3rd quartiles (25% and 75%). 1226 The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) us to explore domains where the total number of agents scales proportionally to the total number of resource types or the global time horizon, while keeping constant the average agent density (per unit of global time) or the average number of resources per agent (which commonly occurs in real-life applications). Overall, we believe that these experimental results indicate that our MILP formulation can be used to effectively solve resource-scheduling problems of nontrivial size. 6. DISCUSSION AND CONCLUSIONS Throughout the paper, we have made a number of assumptions in our model and solution algorithm; we discuss their implications below. • Continual execution. We assume that once an agent stops executing its MDP (transitions into state sf ), it exits the system and cannot return. It is easy to relax this assumption for domains where agents MDPs can be paused and restarted. All that is required is to include an additional pause action which transitions from a given state back to itself, and has zero reward. • Indifference to start time. We used a reward model where agents rewards depend only on the time horizon of their MDPs and not the global start time. This is a consequence of our MDP-augmentation procedure from Section 4.1. It is easy to extend the model so that the agents incur an explicit penalty for idling by assigning a non-zero negative reward to the start state sb . • Binary resource requirements. For simplicity, we have assumed that resource costs are binary: ϕm(a, ω) = {0, 1}, but our results generalize in a straightforward manner to non-binary resource mappings, analogously to the procedure used in [5]. • Cooperative agents. The optimization procedure discussed in this paper was developed in the context of cooperative agents, but it can also be used to design a mechanism for scheduling resources among self-interested agents. This optimization procedure can be embedded in a VickreyClarke-Groves auction, completely analogously to the way it was done in [7]. In fact, all the results of [7] about the properties of the auction and information privacy directly carry over to the scheduling domain discussed in this paper, requiring only slight modifications to deal with finitehorizon MDPs. • Known, deterministic arrival and departure times. Finally, we have assumed that agents arrival and departure times (τa m and τd m) are deterministic and known a priori. This assumption is fundamental to our solution method. While there are many domains where this assumption is valid, in many cases agents arrive and depart dynamically and their arrival and departure times can only be predicted probabilistically, leading to online resource-allocation problems. In particular, in the case of self-interested agents, this becomes an interesting version of an online-mechanism-design problem [11, 12]. In summary, we have presented an MILP formulation for the combinatorial resource-scheduling problem where agents values for possible resource assignments are defined by finitehorizon MDPs. This result extends previous work ([6, 7]) on static one-shot resource allocation under MDP-induced preferences to resource-scheduling problems with a temporal aspect. As such, this work takes a step in the direction of designing an online mechanism for agents with combinatorial resource preferences induced by stochastic planning problems. Relaxing the assumption about deterministic arrival and departure times of the agents is a focus of our future work. We would like to thank the anonymous reviewers for their insightful comments and suggestions. 7. REFERENCES [1] E. Altman and A. Shwartz. Adaptive control of constrained Markov chains: Criteria and policies. Annals of Operations Research, special issue on Markov Decision Processes, 28:101-134, 1991. [2] R. Bellman. Dynamic Programming. Princeton University Press, 1957. [3] C. Boutilier. Solving concisely expressed combinatorial auction problems. In Proc. of AAAI-02, pages 359-366, 2002. [4] C. Boutilier and H. H. Hoos. Bidding languages for combinatorial auctions. In Proc. of IJCAI-01, pages 1211-1217, 2001. [5] D. Dolgov. Integrated Resource Allocation and Planning in Stochastic Multiagent Environments. PhD thesis, Computer Science Department, University of Michigan, February 2006. [6] D. A. Dolgov and E. H. Durfee. Optimal resource allocation and policy formulation in loosely-coupled Markov decision processes. In Proc. of ICAPS-04, pages 315-324, June 2004. [7] D. A. Dolgov and E. H. Durfee. Computationally efficient combinatorial auctions for resource allocation in weakly-coupled MDPs. In Proc. of AAMAS-05, New York, NY, USA, 2005. ACM Press. [8] D. A. Dolgov and E. H. Durfee. Resource allocation among agents with preferences induced by factored MDPs. In Proc. of AAMAS-06, 2006. [9] K. Larson and T. Sandholm. Mechanism design and deliberative agents. In Proc. of AAMAS-05, pages 650-656, New York, NY, USA, 2005. ACM Press. [10] N. Nisan. Bidding and allocation in combinatorial auctions. In Electronic Commerce, 2000. [11] D. C. Parkes and S. Singh. An MDP-based approach to Online Mechanism Design. In Proc. of the Seventeenths Annual Conference on Neural Information Processing Systems (NIPS-03), 2003. [12] D. C. Parkes, S. Singh, and D. Yanovsky. Approximately efficient online mechanism design. In Proc. of the Eighteenths Annual Conference on Neural Information Processing Systems (NIPS-04), 2004. [13] M. L. Puterman. Markov Decision Processes. John Wiley & Sons, New York, 1994. [14] M. H. Rothkopf, A. Pekec, and R. M. Harstad. Computationally manageable combinational auctions. Management Science, 44(8):1131-1147, 1998. [15] T. Sandholm. An algorithm for optimal winner determination in combinatorial auctions. In Proc. of IJCAI-99, pages 542-547, San Francisco, CA, USA, 1999. Morgan Kaufmann Publishers Inc. The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1227",
    "original_translation": "La programación de recursos combinatoria para MDPs multiagentes. Dmitri A. Dolgov, Michael R. James y Michael E. Samples. Grupo de Investigación en IA y Robótica, Centro Técnico de Toyota en EE. UU. {ddolgov, michael.r.james, michael.samples}@gmail.com. RESUMEN La programación óptima de recursos en sistemas multiagentes es una tarea computacionalmente desafiante, especialmente cuando los valores de los recursos no son aditivos. Consideramos el problema combinatorio de programar el uso de múltiples recursos entre agentes que operan en entornos estocásticos, modelados como procesos de decisión de Markov (MDPs). En los últimos años, se han desarrollado algoritmos eficientes de asignación de recursos para agentes con valores de recursos inducidos por MDPs. Sin embargo, este trabajo previo se ha centrado en problemas de asignación de recursos estáticos donde los recursos se distribuyen una vez y luego se utilizan en MDPs de horizonte infinito. Extendemos esos modelos existentes al problema de la programación de recursos combinatoria, donde los agentes persisten solo por períodos finitos entre sus tiempos de llegada y salida (predefinidos), requiriendo recursos solo durante esos períodos de tiempo. Proporcionamos un procedimiento computacionalmente eficiente para calcular asignaciones de recursos óptimas a nivel global a agentes a lo largo del tiempo. Ilustramos y analizamos empíricamente el método en el contexto de un dominio de programación de trabajos estocásticos. Categorías y Descriptores de Asignaturas I.2.8 [Inteligencia Artificial]: Resolución de Problemas, Métodos de Control y Búsqueda; I.2.11 [Inteligencia Artificial]: Inteligencia Artificial Distribuida-Sistemas Multiagente Términos Generales Algoritmos, Rendimiento, Diseño 1. INTRODUCCIÓN Las tareas de asignación óptima de recursos y programación son ubicuas en sistemas multiagentes, pero resolver tales problemas de optimización puede ser computacionalmente difícil, debido a una serie de factores. En particular, cuando el valor de un conjunto de recursos para un agente no es aditivo (como suele ser el caso con recursos que son sustitutos o complementos), la función de utilidad podría tener que definirse en un espacio exponencialmente grande de combinaciones de recursos, lo que rápidamente se vuelve computacionalmente intratable. Además, incluso cuando cada agente tiene una función de utilidad que es distinta de cero solo en un pequeño subconjunto de los posibles conjuntos de recursos, obtener una asignación óptima sigue siendo computacionalmente prohibitivo, ya que el problema se vuelve NP-completo [14]. Tales problemas computacionales han dado lugar recientemente a varias líneas de trabajo en el uso de modelos compactos de las preferencias de los agentes. Una idea es utilizar cualquier estructura presente en las funciones de utilidad para representarlas de forma compacta, por ejemplo, a través de fórmulas lógicas [15, 10, 4, 3]. Una alternativa es modelar directamente los mecanismos que definen las funciones de utilidad de los agentes y realizar la asignación de recursos directamente con estos modelos [9]. Una forma de lograr esto es modelar los procesos mediante los cuales un agente podría utilizar los recursos y definir la función de utilidad como la ganancia de estos procesos. En particular, si un agente utiliza recursos para actuar en un entorno estocástico, su función de utilidad puede ser modelada de forma natural con un proceso de decisión de Markov, cuyo conjunto de acciones está parametrizado por los recursos disponibles. Esta representación puede ser utilizada para construir algoritmos de asignación de recursos muy eficientes que resultan en una aceleración exponencial sobre un problema de optimización directa con representaciones planas de preferencias combinatorias [6, 7, 8]. Sin embargo, este trabajo existente sobre asignación de recursos con preferencias inducidas por MDPs parametrizados por recursos hace una suposición de que los recursos se asignan solo una vez y luego son utilizados por los agentes de forma independiente dentro de sus MDPs de horizonte infinito. Esta suposición de que no es posible la reasignación de recursos puede ser limitante en dominios donde los agentes llegan y se van dinámicamente. En este artículo, ampliamos el trabajo sobre asignación de recursos bajo preferencias inducidas por MDP a problemas de programación en tiempo discreto, donde los agentes están presentes en el sistema durante intervalos de tiempo finitos y solo pueden utilizar recursos dentro de estos intervalos. En particular, los agentes llegan y parten en momentos arbitrarios (predefinidos) y dentro de estos intervalos utilizan recursos para ejecutar tareas en MDPs de horizonte finito. Abordamos el problema de la programación de recursos óptima a nivel global, donde el objetivo es encontrar una asignación de recursos a los agentes a lo largo del tiempo que maximice la suma de las recompensas esperadas que obtienen. En este contexto, nuestra principal contribución es una formulación de programación entera mixta del problema de programación que elige asignaciones de recursos globalmente óptimas, tiempos de inicio y horizontes de ejecución para todos los agentes (dentro de sus intervalos de llegada y salida). Analizamos y comparamos empíricamente dos variantes del problema de programación: uno, donde los agentes tienen asignaciones de recursos estáticas dentro de sus MDPs de horizonte finito, y otro, donde los recursos pueden ser realocados dinámicamente entre agentes en cada paso de tiempo. En el resto del documento, primero establecemos las bases necesarias en la Sección 2 y luego presentamos nuestro modelo y declaración formal del problema en la Sección 3. En la Sección 4.2, describimos nuestro resultado principal, el programa de optimización para la programación de recursos globalmente óptima. Tras la discusión de nuestros resultados experimentales sobre un problema de programación de trabajos en la Sección 5, concluimos en la Sección 6 con una discusión de posibles extensiones y generalizaciones de nuestro método. De manera similar al modelo utilizado en trabajos anteriores sobre asignación de recursos con preferencias inducidas por MDP [6, 7], definimos el valor de un conjunto de recursos para un agente como el valor de la mejor política MDP que es realizable, dadas esos recursos. Sin embargo, dado que el enfoque de nuestro trabajo se centra en problemas de programación, y una gran parte del problema de optimización consiste en decidir cómo se asignan los recursos en el tiempo entre agentes con tiempos finitos de llegada y salida, modelamos los problemas de planificación de los agentes como MDPs de horizonte finito, en contraste con trabajos anteriores que utilizaron MDPs descontados de horizonte infinito. En el resto de esta sección, primero presentamos algo de antecedentes necesarios sobre MDPs de horizonte finito y presentamos una formulación de programación lineal que sirve como base para nuestro algoritmo de solución desarrollado en la Sección 4. También describimos los métodos estándar para la programación de recursos combinatorios con valores de recursos planos, que sirven como punto de referencia de comparación para el nuevo modelo desarrollado aquí. 2.1 Procesos de Decisión de Markov Un MDP (Proceso de Decisión de Markov) estacionario, de dominio finito y discreto en el tiempo (ver, por ejemplo, [13] para un desarrollo detallado y exhaustivo) se puede describir como S, A, p, r, donde: S es un conjunto finito de estados del sistema; A es un conjunto finito de acciones disponibles para el agente; p es una función de transición estocástica estacionaria, donde p(σ|s, a) es la probabilidad de transicionar al estado σ al ejecutar la acción a en el estado s; r es una función de recompensa estacionaria, donde r(s, a) especifica la recompensa obtenida al ejecutar la acción a en el estado s. Dado un MDP de este tipo, un problema de decisión bajo un horizonte finito T consiste en elegir una acción óptima en cada paso de tiempo para maximizar el valor esperado de la recompensa total acumulada durante la vida (finita) de los agentes. La política óptima del agente es entonces una función del estado actual s y el tiempo hasta el horizonte. Una política óptima para tal problema es actuar de forma codiciosa con respecto a la función de valor óptimo, definida de forma recursiva por el siguiente sistema de ecuaciones de Bellman de tiempo finito [2]: v(s, t) = max a r(s, a) + X σ p(σ|s, a)v(σ, t + 1), ∀s ∈ S, t ∈ [1, T − 1]; v(s, T) = 0, ∀s ∈ S; donde v(s, t) es el valor óptimo de estar en el estado s en el tiempo t ∈ [1, T]. Esta función de valor óptimo se puede calcular fácilmente utilizando programación dinámica, lo que conduce a la siguiente política óptima π, donde π(s, a, t) es la probabilidad de ejecutar la acción a en el estado s en el tiempo t: π(s, a, t) = (1, a = argmaxa r(s, a) + P σ p(σ|s, a)v(σ, t + 1), 0, en caso contrario. La anterior es la forma más común de calcular la función de valor óptimo (y por lo tanto una política óptima) para un MDP de horizonte finito. Sin embargo, también podemos formular el problema como el siguiente programa lineal (similarmente al LP dual para MDPs descontados de horizonte infinito [13, 6, 7]): max X s X a r(s, a) X t x(s, a, t) sujeto a: X a x(σ, a, t + 1) = X s,a p(σ|s, a)x(s, a, t) ∀σ, t ∈ [1, T − 1]; X a x(s, a, 1) = α(s), ∀s ∈ S; (1) donde α(s) es la distribución inicial sobre el espacio de estados, y x es la medida de ocupación (no estacionaria) (x(s, a, t) ∈ [0, 1] es el número total esperado de veces que se ejecuta la acción a en el estado s en el tiempo t). Una política óptima (no estacionaria) se obtiene a partir de la medida de ocupación de la siguiente manera: π(s, a, t) = x(s, a, t)/ X a x(s, a, t) ∀s ∈ S, t ∈ [1, T]. (2) Tenga en cuenta que el MDP estándar de horizonte finito sin restricciones, como se describe anteriormente, siempre tiene una solución uniformemente óptima (óptima para cualquier distribución inicial α(s)). Por lo tanto, una política óptima se puede obtener utilizando una constante arbitraria α(s) > 0 (en particular, α(s) = 1 resultará en x(s, a, t) = π(s, a, t)). Sin embargo, para los MDPs con restricciones de recursos (como se definen a continuación en la Sección 3), las políticas óptimas uniformemente no existen en general. En tales casos, α se convierte en parte de la entrada del problema, y una política resultante solo es óptima para ese α en particular. Este resultado es bien conocido para MDPs de horizonte infinito con varios tipos de restricciones [1, 6], y también se cumple para nuestro modelo de horizonte finito, el cual puede establecerse fácilmente a través de una línea de razonamiento completamente análoga a los argumentos en [6]. 2.2 Programación de Recursos Combinatoria Un enfoque directo para la programación de recursos para un conjunto de agentes M, cuyos valores para los recursos son inducidos por problemas de planificación estocástica (en nuestro caso, MDPs de horizonte finito) sería que cada agente enumerara todas las asignaciones de recursos posibles a lo largo del tiempo y, para cada una, calcular su valor resolviendo el MDP correspondiente. Entonces, cada agente proporcionaría valoraciones para cada posible conjunto de recursos a lo largo del tiempo a un coordinador centralizado, quien calcularía las asignaciones óptimas de recursos a lo largo del tiempo basándose en estas valoraciones. Cuando los recursos pueden asignarse en diferentes momentos a diferentes agentes, cada agente debe presentar valoraciones para cada combinación de posibles horizontes temporales. Que cada agente m ∈ M ejecute su MDP dentro del intervalo de tiempo de llegada-salida τ ∈ [τa m, τd m]. Por lo tanto, el agente m ejecutará un MDP con un horizonte temporal no mayor que Tm = τd m−τa m+1. Que bτ sea el horizonte temporal global para el problema, antes del cual todos los MDP de los agentes deben finalizar. Suponemos que τd m < bτ, ∀m ∈ M. El Sexto Congreso Internacional. En el problema de programación donde los agentes tienen requisitos de recursos estáticos dentro de sus MDPs de horizonte finito, los agentes proporcionan una valoración para cada conjunto de recursos para cada horizonte de tiempo posible (desde [1, Tm]) que puedan utilizar. Sea Ω el conjunto de recursos a ser asignados entre los agentes. Un agente recibirá como máximo un paquete de recursos para uno de los horizontes temporales. Que la variable ψ ∈ Ψm enumere todos los posibles pares de conjuntos de recursos y horizontes temporales para el agente m, por lo que hay 2|Ω| × Tm valores para ψ (el espacio de conjuntos es exponencial en el número de tipos de recursos |Ω|). El agente m debe proporcionar un valor vψ m para cada ψ, y el coordinador asignará como máximo un par (recurso, horizonte temporal) ψ a cada agente. Esta asignación se expresa como una variable indicadora zψ m ∈ {0, 1} que muestra si ψ está asignado al agente m. Para el tiempo τ y el recurso ω, la función nm(ψ, τ, ω) ∈ {0, 1} indica si el paquete en ψ utiliza el recurso ω en el tiempo τ (hacemos la suposición de que los agentes tienen requisitos de recursos binarios). Este problema de asignación es NP-completo, incluso al considerar solo un paso de tiempo, y su dificultad aumenta significativamente con múltiples pasos de tiempo debido al creciente número de valores de ψ. El problema de encontrar una asignación óptima que satisfaga la restricción global de que la cantidad de cada recurso ω asignada a todos los agentes no exceda la cantidad disponible bϕ(ω) se puede expresar como el siguiente programa entero: max X m∈M X ψ∈Ψm zψ mvψ m sujeto a: X ψ∈Ψm zψ m ≤ 1, ∀m ∈ M; X m∈M X ψ∈Ψm zψ mnm(ψ, τ, ω) ≤ bϕ(ω), ∀τ ∈ [1, bτ], ∀ω ∈ Ω; (3) La primera restricción en la ecuación 3 dice que ningún agente puede recibir más de un paquete, y la segunda restricción asegura que la asignación total del recurso ω no exceda, en ningún momento, el límite del recurso. Para el problema de programación en el que los agentes pueden reasignar dinámicamente recursos, cada agente debe especificar un valor para cada combinación de paquetes y pasos de tiempo dentro de su horizonte temporal. Que la variable ψ ∈ Ψm en este caso enumere todos los posibles conjuntos de recursos para los cuales a lo sumo un conjunto puede ser asignado al agente m en cada paso de tiempo. Por lo tanto, en este caso hay P t∈[1,Tm](2|Ω| )t ∼ 2|Ω|Tm posibilidades de combinaciones de recursos asignadas a diferentes intervalos de tiempo, para los Tm horizontes de tiempo diferentes. El mismo conjunto de ecuaciones (3) se puede utilizar para resolver este problema de programación dinámica, pero el programa entero es diferente debido a la diferencia en cómo se define ψ. En este caso, el número de valores de ψ es exponencial en el horizonte de planificación Tm de cada agente, lo que resulta en un programa mucho más grande. Este enfoque directo para resolver ambos problemas de programación requiere una enumeración y solución de 2|Ω| Tm (asignación estática) o P t∈[1,Tm] 2|Ω|t (reubicación dinámica) MDPs para cada agente, lo cual rápidamente se vuelve intratable con el crecimiento del número de recursos |Ω| o el horizonte de tiempo Tm. MODELO Y DECLARACIÓN DEL PROBLEMA Ahora presentamos formalmente nuestro modelo del problema de programación de recursos. La entrada del problema consiste en los siguientes componentes: • M, Ω, bϕ, τa m, τd m, bτ están definidos arriba en la Sección 2.2. • {Θm} = {S, A, pm, rm, αm} son los MDPs de todos los agentes m ∈ M. Sin pérdida de generalidad, asumimos que los espacios de estados y acciones de todos los agentes son iguales, pero cada uno tiene su propia función de transición pm, función de recompensa rm, y condiciones iniciales αm. • ϕm : A×Ω → {0, 1} es la asignación de acciones a recursos para el agente m. ϕm(a, ω) indica si la acción a del agente m necesita el recurso ω. Un agente m que recibe un conjunto de recursos que no incluye el recurso ω no puede ejecutar en su política MDP ninguna acción a para la cual ϕm(a, ω) = 0. Suponemos que todos los requisitos de recursos son binarios; como se discute más adelante en la Sección 6, esta suposición no es limitante. Dado el input anterior, el problema de optimización que consideramos es encontrar la asignación globalmente óptima que maximice la suma de recompensas esperadas de recursos a agentes para todos los pasos de tiempo: Δ : τ × M × Ω → {0, 1}. Una solución es factible si la asignación correspondiente de recursos a los agentes no viola la restricción global de recursos: X m Δm(τ, ω) ≤ bϕ(ω), ∀ω ∈ Ω, τ ∈ [1, bτ]. Consideramos dos variantes del problema de programación de recursos. La primera formulación restringe las asignaciones de recursos al espacio donde la asignación de recursos a cada agente es estática durante la vida de los agentes. La segunda formulación permite la reasignación de recursos entre agentes en cada paso de tiempo durante sus vidas. La Figura 1 representa un problema de programación de recursos con tres agentes M = {m1, m2, m3}, tres recursos Ω = {ω1, ω2, ω3}, y un horizonte de problema global de bτ = 11. Los tiempos de llegada y salida de los agentes se muestran como cajas grises y son {1, 6}, {3, 7} y {2, 11}, respectivamente. Una solución a este problema se muestra a través de barras horizontales dentro de cada casilla de agentes, donde las barras corresponden a la asignación de los tres tipos de recursos. La Figura 1a muestra una solución a un problema de programación estática. Según la solución mostrada, el agente m1 comienza la ejecución de su MDP en el tiempo τ = 1 y tiene un bloqueo en los tres recursos hasta que finaliza la ejecución en el tiempo τ = 3. Se debe tener en cuenta que el agente m1 renuncia a su control sobre los recursos antes de su hora de salida anunciada de τd m1 = 6, aparentemente porque otros agentes pueden utilizar los recursos de manera más efectiva. Por lo tanto, en el tiempo τ = 4, los recursos ω1 y ω3 se asignan al agente m2, quien luego los utiliza para ejecutar su MDP (utilizando solo acciones admitidas por los recursos ω1 y ω3) hasta el tiempo τ = 7. El agente m3 posee el recurso ω3 durante el intervalo τ ∈ [4, 10]. La Figura 1b muestra una posible solución a la versión dinámica del mismo problema. Allí, los recursos pueden ser reasignados entre agentes en cada paso de tiempo. Por ejemplo, el agente m1 renuncia a su uso del recurso ω2 en el tiempo τ = 2, aunque continúa la ejecución de su MDP hasta el tiempo τ = 6. Ten en cuenta que a un agente no se le permite detener y reiniciar su MDP, por lo que el agente m1 solo puede continuar ejecutando en el intervalo τ ∈ [3, 4] si tiene acciones que no requieren recursos (ϕm(a, ω) = 0). Claramente, el modelo y la declaración del problema descritos anteriormente hacen una serie de suposiciones sobre el problema y las propiedades de la solución deseada. Discutimos algunas de esas suposiciones y sus implicaciones en la Sección 6. 1222 La Sexta Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) (a) (b) Figura 1: Ilustración de una solución a un problema de programación de recursos con tres agentes y tres recursos: a) asignaciones de recursos estáticas (las asignaciones de recursos son constantes durante la vida de los agentes); b) asignación dinámica (las asignaciones de recursos pueden cambiar en cada paso de tiempo). 4. PROGRAMACIÓN DE RECURSOS Nuestro algoritmo de programación de recursos se lleva a cabo en dos etapas. Primero, realizamos un paso de preprocesamiento que aumenta los MDP de los agentes; este proceso se describe en la Sección 4.1. Segundo, utilizando estos MDPs aumentados construimos un problema de optimización global, el cual se describe en la Sección 4.2. 4.1 Aumentando los MDPs de los Agentes En el modelo descrito en la sección anterior, asumimos que si un agente no posee los recursos necesarios para realizar acciones en su MDP, su ejecución se detiene y el agente abandona el sistema. En otras palabras, los MDPs no pueden ser pausados y reanudados. Por ejemplo, en el problema mostrado en la Figura 1a, el agente m1 libera todos los recursos después del tiempo τ = 3, momento en el cual la ejecución de su MDP se detiene. De manera similar, los agentes m2 y m3 solo ejecutan sus MDPs en los intervalos τ ∈ [4, 6] y τ ∈ [4, 10], respectivamente. Por lo tanto, una parte importante del problema de toma de decisiones global es decidir la ventana de tiempo durante la cual cada uno de los agentes está activo (es decir, ejecutando su MDP). Para lograr esto, ampliamos el MDP de cada agente con dos nuevos estados (estados de inicio y final sb, sf, respectivamente) y una nueva acción de inicio/parada a∗, como se ilustra en la Figura 2. La idea es que un agente permanezca en el estado inicial sb hasta que esté listo para ejecutar su MDP, momento en el que realiza la acción de inicio/parada a∗ y transita al espacio de estados del MDP original con la probabilidad de transición que corresponde a la distribución inicial original α(s). Por ejemplo, en la Figura 1a, para el agente m2 esto ocurriría en el tiempo τ = 4. Una vez que el agente llega al final de su ventana de actividad (tiempo τ = 6 para el agente m2 en la Figura 1a), realiza la acción de inicio/parada, lo que lo lleva al estado final de finalización del sumidero sf en el tiempo τ = 7. Más precisamente, dado un MDP S, A, pm, rm, αm, definimos un MDP aumentado S, A, pm, rm, αm de la siguiente manera: S = S ∪ sb ∪ sf; A = A ∪ a∗; p(s|sb, a∗) = α(s), ∀s ∈ S; p(sb|sb, a) = 1.0, ∀a ∈ A; p(sf|s, a∗) = 1.0, ∀s ∈ S; p(σ|s, a) = p(σ|s, a), ∀s, σ ∈ S, a ∈ A; r(sb, a) = r(sf, a) = 0, ∀a ∈ A; r(s, a) = r(s, a), ∀s ∈ S, a ∈ A; α(sb) = 1; α(s) = 0, ∀s ∈ S; donde se asume que todas las probabilidades de transición no especificadas son cero. Además, para tener en cuenta el nuevo estado inicial, comenzamos el MDP un paso de tiempo antes, estableciendo τa m ← τa m − 1. Esto no afectará la asignación de recursos debido a que las restricciones de recursos solo se aplican a los estados originales del MDP, como se discutirá en la siguiente sección. Por ejemplo, los MDPs aumentados mostrados en la Figura 2b (que comienza en el estado sb en el tiempo τ = 2) se construirían a partir de un MDP con tiempo de llegada original τ = 3. La Figura 2b también muestra una trayectoria de muestra a través del espacio de estados: el agente comienza en el estado sb, transita al espacio de estados S del MDP original y finalmente sale al estado de sumidero sf. Cabe destacar que si quisiéramos modelar un problema en el que los agentes pudieran pausar sus MDP en pasos de tiempo arbitrarios (lo cual podría ser útil para dominios donde la reasignación dinámica es posible), podríamos lograrlo fácilmente incluyendo una acción adicional que transiciona de cada estado a sí mismo con una recompensa de cero. 4.2 MILP para la programación de recursos Dado un conjunto de MDPs aumentados, como se define arriba, el objetivo de esta sección es formular un programa de optimización global que resuelva el problema de programación de recursos. En esta sección y a continuación, se asume que todos los MDPs son los MDPs aumentados tal como se definen en la Sección 4.1. Nuestro enfoque es similar a la idea utilizada en [6]: comenzamos con la formulación del programa lineal de los MDP de los agentes (1) y lo complementamos con restricciones que aseguran que la asignación de recursos correspondiente entre agentes y tiempo sea válida. El problema de optimización resultante resuelve simultáneamente los MDP de los agentes y los problemas de programación de recursos. En el resto de esta sección, desarrollamos de forma incremental un programa de enteros mixtos (MILP) que logra esto. En ausencia de restricciones de recursos, los MDPs de horizonte finito de los agentes son completamente independientes, y la solución óptima global puede obtenerse trivialmente a través del siguiente LP, que es simplemente una agregación de LPs de horizonte finito de un solo agente: max X m X s X a rm(s, a) X t xm(s, a, t) sujeto a: X a xm(σ, a, t + 1) = X s,a pm(σ|s, a)xm(s, a, t), ∀m ∈ M, σ ∈ S, t ∈ [1, Tm − 1]; X a xm(s, a, 1) = αm(s), ∀m ∈ M, s ∈ S; (12) donde xm(s, a, t) es la medida de ocupación del agente m, y The Sixth Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 1223 (a) (b) Figura 2: Ilustración de la ampliación de un MDP para permitir tiempos de inicio y finalización variables: a) (izquierda) el MDP original de dos estados con una sola acción; (derecha) el MDP ampliado con nuevos estados sb y sf y la nueva acción a∗ (nota que las transiciones originales no se modifican en el proceso de ampliación); b) el MDP ampl augmentado mostrado como una trayectoria a través del tiempo (las líneas grises indican todas las transiciones, mientras que las líneas negras indican una trayectoria dada). Función objetivo (suma de recompensas esperadas de todos los agentes) máx X m X s X a rm(s, a) X t xm(s, a, t) (5) Implicación de significado Restricciones lineales Atar x a θ. El agente solo está activo cuando la medida de ocupación es distinta de cero en los estados originales del MDP. θm(τ) = 0 =⇒ xm(s, a, τ −τa m+1) = 0 ∀s /∈ {sb , sf }, a ∈ A X s/∈{sb,sf } X a xm(s, a, t) ≤ θm(τa m + t − 1) ∀m ∈ M, ∀t ∈ [1, Tm] (6) El agente solo puede estar activo en τ ∈ (τa m, τd m) θm(τ) = 0 ∀m ∈ M, τ /∈ (τa m, τd m) (7) No se pueden utilizar recursos cuando no está activo θm(τ) = 0 =⇒ Δm(τ, ω) = 0 ∀τ ∈ [0, bτ], ω ∈ Ω Δm(τ, ω) ≤ θm(τ) ∀m ∈ M, τ ∈ [0, bτ], ω ∈ Ω (8) Vincular x a Δ (x distinto de cero obliga a que el correspondiente Δ sea distinto de cero). Δm(τ, ω) = 0, ϕm(a, ω) = 1 =⇒ xm(s, a, τ − τa m + 1) = 0 ∀s /∈ {sb , sf } 1/|A| X a ϕm(a, ω) X s/∈{sb,sf } xm(s, a, t) ≤ Δm(t + τa m − 1, ω) ∀m ∈ M, ω ∈ Ω, t ∈ [1, Tm] (9) Límites de recursos X m Δm(τ, ω) ≤ bϕ(ω) ∀ω ∈ Ω, τ ∈ [0, bτ] (10) El agente no puede cambiar los recursos mientras está activo. Solo habilitado para programación con asignaciones estáticas. θm(τ) = 1 y θm(τ + 1) = 1 =⇒ Δm(τ, ω) = Δm(τ + 1, ω) Δm(τ, ω) − Z(1 − θm(τ + 1)) ≤ Δm(τ + 1, ω) + Z(1 − θm(τ)) Δm(τ, ω) + Z(1 − θm(τ + 1)) ≥ Δm(τ + 1, ω) − Z(1 − θm(τ)) ∀m ∈ M, ω ∈ Ω, τ ∈ [0, bτ] (11) Tabla 1: MILP para programación óptima global de recursos. Tm = τd m − τa m + 1 es el horizonte temporal para los agentes MDP. Usando este LP como base, lo ampliamos con restricciones que aseguran que el uso de recursos implicado por las medidas de ocupación de los agentes {xm} no viola los requisitos globales de recursos bϕ en ningún paso de tiempo τ ∈ [0, bτ]. Para formular estas restricciones de recursos, utilizamos las siguientes variables binarias: • Δm(τ, ω) = {0, 1}, ∀m ∈ M, τ ∈ [0, bτ], ω ∈ Ω, que sirven como variables indicadoras que definen si el agente m posee el recurso ω en el tiempo τ. Estos son análogos a las variables indicadoras estáticas utilizadas en el problema de asignación de recursos estáticos de una sola vez en [6]. • θm = {0, 1}, ∀m ∈ M, τ ∈ [0, bτ] son variables indicadoras que especifican si el agente m está activo (es decir, ejecutando su MDP) en el tiempo τ. El significado de las variables de uso de recursos Δ se ilustra en la Figura 1: Δm(τ, ω) = 1 solo si el recurso ω se asigna al agente m en el tiempo τ. El significado de los indicadores de actividad θ se ilustra en la Figura 2b: cuando el agente m se encuentra en el estado inicial sb o en el estado final sf, el θm correspondiente es igual a 0, pero una vez que el agente se vuelve activo y entra en uno de los otros estados, establecemos θm = 1. Este significado de θ puede ser impuesto con una restricción lineal que sincroniza los valores de las medidas de ocupación de los agentes xm y la actividad 1224 The Sixth Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) indicadores θ, como se muestra en (6) en la Tabla 1. Otra restricción que debemos agregar, debido a que los indicadores de actividad θ están definidos en la línea de tiempo global τ, es hacer cumplir el hecho de que el agente está inactivo fuera de su ventana de llegada y salida. Esto se logra mediante la restricción (7) en la Tabla 1. Además, los agentes no deben estar utilizando recursos mientras están inactivos. Esta restricción también puede ser impuesta a través de una desigualdad lineal en θ y Δ, como se muestra en (8). La restricción (6) establece el valor de θ para que coincida con la política definida por la medida de ocupación xm. De manera similar, debemos asegurarnos de que las variables de uso de recursos Δ estén también sincronizadas con la medida de ocupación xm. Esto se realiza a través de la restricción (9) en la Tabla 1, que es casi idéntica a la restricción análoga de [6]. Después de implementar la restricción anterior, que refuerza el significado de Δ, agregamos una restricción que garantiza que el uso de recursos de los agentes nunca exceda los montos de recursos disponibles. Esta condición también se expresa trivialmente como una desigualdad lineal (10) en la Tabla 1. Finalmente, para la formulación del problema en la que las asignaciones de recursos son estáticas durante la vida de un agente, agregamos una restricción que garantiza que las variables de uso de recursos Δ no cambien su valor mientras el agente está activo (θ = 1). Esto se logra a través de la restricción lineal (11), donde Z ≥ 2 es una constante que se utiliza para desactivar las restricciones cuando θm(τ) = 0 o θm(τ + 1) = 0. Esta restricción no se utiliza para la formulación del problema dinámico, donde los recursos pueden ser reasignados entre agentes en cada paso de tiempo. En resumen, la Tabla 1 junto con las restricciones de conservación de flujo de (12) define el MILP que calcula simultáneamente una asignación óptima de recursos para todos los agentes a lo largo del tiempo, así como políticas óptimas de MDP de horizonte finito que son válidas bajo esa asignación de recursos. Como una medida aproximada de la complejidad de este MILP, consideremos el número de variables de optimización y restricciones. Sea TM = P Tm = P m(τa m − τd m + 1) la suma de las longitudes de las ventanas de llegada-salida a través de todos los agentes. Entonces, el número de variables de optimización es: TM + bτ|M||Ω| + bτ|M|, de las cuales TM son continuas (xm), y bτ|M||Ω| + bτ|M| son binarias (Δ y θ). Sin embargo, hay que tener en cuenta que todos menos TM|M| de los θ se establecen en cero por la restricción (7), lo que también obliga inmediatamente a que todos menos TM|M||Ω| de los Δ sean cero a través de las restricciones (8). El número de restricciones (sin incluir las restricciones degeneradas en (7)) en el MILP es: TM + TM|Ω| + bτ|Ω| + bτ|M||Ω|. A pesar de que la complejidad del MILP es, en el peor de los casos, exponencial en el número de variables binarias, la complejidad de este MILP es significativamente (exponencialmente) menor que la del MILP con funciones de utilidad planas, descrito en la Sección 2.2. Este resultado refleja las ganancias de eficiencia reportadas en [6] para problemas de asignación de recursos de un solo disparo, pero es mucho más pronunciado, debido a la explosión de la representación de utilidad plana debido al aspecto temporal del problema (recuerde la complejidad prohibitiva de la optimización combinatoria en la Sección 2.2). Analizamos empíricamente el rendimiento de este método en la Sección 5.1. Estrictamente hablando, resolver MILPs hasta la optimalidad es NP-completo en el número de variables enteras. RESULTADOS EXPERIMENTALES Aunque la complejidad de resolver MILPs es en el peor de los casos exponencial en el número de variables enteras, existen muchos métodos eficientes para resolver MILPs que permiten que nuestro algoritmo se adapte bien a parámetros comunes en problemas de asignación de recursos y programación. En particular, esta sección introduce un dominio de problema: el problema del taller de reparación, que se utiliza para evaluar empíricamente la escalabilidad de nuestros algoritmos en términos del número de agentes |M|, el número de recursos compartidos |Ω| y las diferentes longitudes de tiempo global bτ durante las cuales los agentes pueden entrar y salir del sistema. El problema del taller de reparaciones es un MDP parametrizado simple que adopta la metáfora de un taller de reparaciones de vehículos. Los agentes en el taller de reparación son mecánicos con una serie de tareas independientes que solo generan recompensa cuando se completan. En nuestro modelo MDP de este sistema, las acciones tomadas para avanzar a través del espacio de estados solo están permitidas si el agente posee ciertos recursos que están públicamente disponibles para la tienda. Estos recursos son de suministro finito, y las políticas óptimas para la tienda determinarán cuándo cada agente puede retener los recursos limitados para tomar acciones y obtener recompensas individuales. Cada tarea a completar está asociada con una sola acción, aunque se requiere que el agente repita la acción numerosas veces antes de completar la tarea y obtener una recompensa. Este modelo fue parametrizado en términos del número de agentes en el sistema, el número de diferentes tipos de recursos que podrían estar vinculados a acciones necesarias, un tiempo global durante el cual se permite a los agentes llegar y partir, y una longitud máxima para el número de pasos de tiempo que un agente puede permanecer en el sistema. Todos los puntos de datos en nuestros experimentos se obtuvieron con 20 evaluaciones utilizando CPLEX para resolver los MILPs en una computadora Pentium4 con 2Gb de RAM. Se realizaron pruebas tanto en la versión estática como en la dinámica del problema de programación de recursos, tal como se definió anteriormente. La Figura 3 muestra el tiempo de ejecución y el valor de la política para modificaciones independientes en el conjunto de parámetros. La fila superior muestra cómo el tiempo de solución para el MILP escala a medida que aumentamos el número de agentes |M|, el horizonte de tiempo global bτ y el número de recursos |Ω|. Aumentar el número de agentes conduce a una escalabilidad exponencial en complejidad, lo cual es de esperarse para un problema NP-completo. Sin embargo, aumentar el límite de tiempo global bτ o el número total de tipos de recursos |Ω|, manteniendo constante el número de agentes, no conduce a una disminución en el rendimiento. Esto ocurre porque los problemas se vuelven más fáciles a medida que se vuelven sub-determinados, lo cual también es un fenómeno común en problemas NP-completos. También observamos que la solución a la versión dinámica del problema a menudo se puede calcular mucho más rápido que la versión estática. La fila inferior de la Figura 3 muestra el valor de la política conjunta de las políticas que corresponden a los horarios óptimos de asignación de recursos calculados. Podemos observar que la versión dinámica produce una recompensa más alta (como era de esperar, ya que la recompensa de la versión dinámica siempre es igual o mayor que la recompensa de la versión estática). Deberíamos señalar que estos gráficos no deben ser vistos como una medida del rendimiento de dos algoritmos diferentes (ambos algoritmos producen soluciones óptimas pero para problemas distintos), sino más bien como observaciones sobre cómo cambia la calidad de las soluciones óptimas a medida que se permite más flexibilidad en la reasignación de recursos. La Figura 4 muestra el tiempo de ejecución y el valor de la política para las pruebas en las que las variables de entrada comunes se escalan juntas. Esto permite la Sexta Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 1225 2 4 6 8 10 10 −3 10 −2 10 −1 10 0 10 1 10 2 10 3 10 4 Número de Agentes |M| Tiempo de CPU, seg |Ω| = 5, τ = 50 estático dinámico 50 100 150 200 10 −2 10 −1 10 0 10 1 10 2 10 3 Límite de Tiempo Global τ Tiempo de CPU, seg |M| = 5, |Ω| = 5 estático dinámico 10 20 30 40 50 10 −2 10 −1 10 0 10 1 10 2 Número de Recursos |Ω| Tiempo de CPU, seg |M| = 5, τ = 50 estático dinámico 2 4 6 8 10 200 400 600 800 1000 1200 1400 1600 Número de Agentes |M| Valor |Ω| = 5, τ = 50 estático dinámico 50 100 150 200 400 500 600 700 800 900 1000 1100 1200 1300 1400 Límite de Tiempo Global τ Valor |M| = 5, |Ω| = 5 estático dinámico 10 20 30 40 50 500 600 700 800 900 1000 1100 1200 1300 1400 Número de Recursos |Ω| Valor |M| = 5, τ = 50 estático dinámico Figura 3: Evaluación de nuestro MILP para números variables de agentes (columna 1), longitudes de ventana de tiempo global (columna 2) y números de tipos de recursos (columna 3). La fila superior muestra el tiempo de CPU, y la fila inferior muestra la recompensa conjunta de las políticas MDP de los agentes. Las barras de error muestran los cuartiles 1ro y 3ro (25% y 75%). 2 4 6 8 10 10 −3 10 −2 10 −1 10 0 10 1 10 2 10 3 Número de Agentes |M| Tiempo de CPU, seg τ = 10|M| estático dinámico 2 4 6 8 10 10 −3 10 −2 10 −1 10 0 10 1 10 2 10 3 10 4 Número de Agentes |M| Tiempo de CPU, seg |Ω| = 2|M| estático dinámico 2 4 6 8 10 10 −3 10 −2 10 −1 10 0 10 1 10 2 10 3 10 4 Número de Agentes |M| Tiempo de CPU, seg |Ω| = 5|M| estático dinámico 2 4 6 8 10 200 400 600 800 1000 1200 1400 1600 1800 2000 2200 Número de Agentes |M| Valor τ = 10|M| estático dinámico 2 4 6 8 10 200 400 600 800 1000 1200 1400 1600 1800 2000 Número de Agentes |M| Valor |Ω| = 2|M| estático dinámico 2 4 6 8 10 0 500 1000 1500 2000 2500 Número de Agentes |M| Valor |Ω| = 5|M| estático dinámico Figura 4: Evaluación de nuestro MILP utilizando variables de entrada correlacionadas. La columna izquierda sigue el rendimiento y el tiempo de CPU a medida que aumenta el número de agentes y la ventana de tiempo global juntos (bτ = 10|M|). La columna del medio y la columna de la derecha siguen el rendimiento y el tiempo de CPU a medida que aumenta el número de recursos y el número de agentes juntos, con |Ω| = 2|M| y |Ω| = 5|M|, respectivamente. Las barras de error muestran el primer y tercer cuartil (25% y 75%). 1226 The Sixth Intl. La Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) nos permite explorar dominios donde el número total de agentes escala proporcionalmente al número total de tipos de recursos o al horizonte temporal global, manteniendo constante la densidad promedio de agentes (por unidad de tiempo global) o el número promedio de recursos por agente (lo cual comúnmente ocurre en aplicaciones de la vida real). En general, creemos que estos resultados experimentales indican que nuestra formulación MILP puede ser utilizada para resolver de manera efectiva problemas de programación de recursos de tamaño no trivial. DISCUSIÓN Y CONCLUSIONES A lo largo del documento, hemos realizado una serie de suposiciones en nuestro modelo y algoritmo de solución; discutimos sus implicaciones a continuación. • Ejecución continua. Suponemos que una vez que un agente deja de ejecutar su MDP (transiciona al estado sf), sale del sistema y no puede regresar. Es fácil relajar esta suposición para dominios donde los agentes de MDP pueden ser pausados y reiniciados. Todo lo que se requiere es incluir una acción de pausa adicional que haga la transición desde un estado dado de regreso a sí mismo, y que tenga una recompensa de cero. • Indiferencia al tiempo de inicio. Utilizamos un modelo de recompensa donde las recompensas de los agentes dependen solo del horizonte temporal de sus MDP y no del tiempo de inicio global. Esta es una consecuencia de nuestro procedimiento de aumento de MDP de la Sección 4.1. Es fácil extender el modelo para que los agentes incurran en una penalización explícita por inactividad al asignar una recompensa negativa no nula al estado inicial sb. • Requisitos de recursos binarios. Para simplificar, hemos asumido que los costos de recursos son binarios: ϕm(a, ω) = {0, 1}, pero nuestros resultados se generalizan de manera directa a asignaciones de recursos no binarias, de manera análoga al procedimiento utilizado en [5]. • Agentes cooperativos. El procedimiento de optimización discutido en este artículo fue desarrollado en el contexto de agentes cooperativos, pero también puede ser utilizado para diseñar un mecanismo para programar recursos entre agentes egoístas. Este procedimiento de optimización puede ser incorporado en una subasta de Vickrey-Clarke-Groves, de manera completamente análoga a como se hizo en [7]. De hecho, todos los resultados de [7] sobre las propiedades de la subasta y la privacidad de la información se aplican directamente al dominio de programación discutido en este documento, requiriendo solo ligeras modificaciones para tratar con MDPs de horizonte finito. • Tiempos de llegada y salida conocidos y deterministas. Finalmente, hemos asumido que los tiempos de llegada y salida de los agentes (τa m y τd m) son deterministas y conocidos de antemano. Esta suposición es fundamental para nuestro método de solución. Si bien hay muchos dominios donde esta suposición es válida, en muchos casos los agentes llegan y se van de forma dinámica y sus tiempos de llegada y salida solo pueden predecirse de manera probabilística, lo que lleva a problemas de asignación de recursos en línea. En particular, en el caso de agentes con interés propio, esto se convierte en una versión interesante de un problema de diseño de mecanismos en línea [11, 12]. En resumen, hemos presentado una formulación MILP para el problema combinatorio de programación de recursos donde los valores de los agentes para posibles asignaciones de recursos están definidos por MDPs de horizonte finito. Este resultado amplía el trabajo previo ([6, 7]) sobre asignación estática de recursos de una sola vez bajo preferencias inducidas por MDP a problemas de programación de recursos con un aspecto temporal. Por lo tanto, este trabajo da un paso en la dirección de diseñar un mecanismo en línea para agentes con preferencias de recursos combinatorios inducidas por problemas de planificación estocástica. Relajar la suposición sobre los tiempos deterministas de llegada y salida de los agentes es un enfoque de nuestro trabajo futuro. Nos gustaría agradecer a los revisores anónimos por sus comentarios y sugerencias perspicaces. 7. REFERENCIAS [1] E. Altman y A. Shwartz. Control adaptativo de cadenas de Markov restringidas: Criterios y políticas. Anales de Investigación de Operaciones, número especial sobre Procesos de Decisión de Markov, 28:101-134, 1991. [2] R. Bellman. Programación dinámica. Princeton University Press, 1957. [3] C. Boutilier. \n\nPrensa de la Universidad de Princeton, 1957. [3] C. Boutilier. Resolviendo problemas de subasta combinatoria expresados de manera concisa. En Proc. de AAAI-02, páginas 359-366, 2002. [4] C. Boutilier y H. H. Hoos. Subastando idiomas para subastas combinatorias. En Proc. de IJCAI-01, páginas 1211-1217, 2001. [5] D. Dolgov. Asignación integrada de recursos y planificación en entornos estocásticos de múltiples agentes. Tesis doctoral, Departamento de Ciencias de la Computación, Universidad de Michigan, febrero de 2006. [6] D. A. Dolgov y E. H. Durfee. Asignación óptima de recursos y formulación de políticas en procesos de decisión de Markov débilmente acoplados. En Proc. de ICAPS-04, páginas 315-324, junio de 2004. [7] D. A. Dolgov y E. H. Durfee. Subastas combinatorias computacionalmente eficientes para asignación de recursos en MDPs débilmente acoplados. En Proc. de AAMAS-05, Nueva York, NY, EE. UU., 2005. ACM Press. [8] D. A. Dolgov y E. H. Durfee. Asignación de recursos entre agentes con preferencias inducidas por MDP factorizados. En Proc. de AAMAS-06, 2006. [9] K. Larson y T. Sandholm. Diseño de mecanismos y agentes deliberativos. En Proc. de AAMAS-05, páginas 650-656, Nueva York, NY, EE. UU., 2005. ACM Press. [10] N. Nisan. \n\nACM Press. [10] N. Nisan. Subasta y asignación en subastas combinatorias. En Comercio Electrónico, 2000. [11] D. C. Parkes y S. Singh. Un enfoque basado en MDP para el Diseño de Mecanismos en Línea. En Proc. de la Decimoséptima Conferencia Anual sobre Sistemas de Procesamiento de Información Neural (NIPS-03), 2003. [12] D. C. Parkes, S. Singh y D. Yanovsky. Diseño de mecanismos en línea aproximadamente eficiente. En Actas de la Decimoctava Conferencia Anual sobre Sistemas de Procesamiento de Información Neural (NIPS-04), 2004. [13] M. L. Puterman. Procesos de Decisión de Markov. John Wiley & Sons, Nueva York, 1994. [14] M. H. Rothkopf, A. Pekec y R. M. Harstad. Subastas combinatorias manejables computacionalmente. Ciencia de la Gestión, 44(8):1131-1147, 1998. [15] T. Sandholm. Un algoritmo para la determinación óptima del ganador en subastas combinatorias. En Proc. de IJCAI-99, páginas 542-547, San Francisco, CA, EE. UU., 1999. Morgan Kaufmann Publishers Inc. - Editores Morgan Kaufmann Inc. El Sexto Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 1227",
    "original_sentences": [
        "Combinatorial Resource Scheduling for Multiagent MDPs Dmitri A. Dolgov, Michael R. James, and Michael E. Samples AI and Robotics Group Technical Research, Toyota Technical Center USA {ddolgov, michael.r.james, michael.samples}@gmail.com ABSTRACT Optimal resource scheduling in multiagent systems is a computationally challenging task, particularly when the values of resources are not additive.",
        "We consider the combinatorial problem of scheduling the usage of multiple resources among agents that operate in stochastic environments, modeled as Markov decision processes (MDPs).",
        "In recent years, efficient resource-allocation algorithms have been developed for agents with resource values induced by MDPs.",
        "However, this prior work has focused on static resource-allocation problems where resources are distributed once and then utilized in infinite-horizon MDPs.",
        "We extend those existing models to the problem of combinatorial resource scheduling, where agents persist only for finite periods between their (predefined) arrival and departure times, requiring resources only for those time periods.",
        "We provide a computationally efficient procedure for computing globally optimal resource assignments to agents over time.",
        "We illustrate and empirically analyze the method in the context of a stochastic jobscheduling domain.",
        "Categories and Subject Descriptors I.2.8 [Artificial Intelligence]: Problem Solving, Control Methods, and Search; I.2.11 [Artificial Intelligence]: Distributed Artificial Intelligence-Multiagent systems General Terms Algorithms, Performance, Design 1.",
        "INTRODUCTION The tasks of optimal resource allocation and scheduling are ubiquitous in multiagent systems, but solving such optimization problems can be computationally difficult, due to a number of factors.",
        "In particular, when the value of a set of resources to an agent is not additive (as is often the case with resources that are substitutes or complements), the utility function might have to be defined on an exponentially large space of resource bundles, which very quickly becomes computationally intractable.",
        "Further, even when each agent has a utility function that is nonzero only on a small subset of the possible resource bundles, obtaining optimal allocation is still computationally prohibitive, as the problem becomes NP-complete [14].",
        "Such computational issues have recently spawned several threads of work in using compact models of agents preferences.",
        "One idea is to use any structure present in utility functions to represent them compactly, via, for example, logical formulas [15, 10, 4, 3].",
        "An alternative is to directly model the mechanisms that define the agents utility functions and perform resource allocation directly with these models [9].",
        "A way of accomplishing this is to model the processes by which an agent might utilize the resources and define the utility function as the payoff of these processes.",
        "In particular, if an agent uses resources to act in a stochastic environment, its utility function can be naturally modeled with a Markov decision process, whose action set is parameterized by the available resources.",
        "This representation can then be used to construct very efficient resource-allocation algorithms that lead to an exponential speedup over a straightforward optimization problem with flat representations of combinatorial preferences [6, 7, 8].",
        "However, this existing work on resource allocation with preferences induced by resource-parameterized MDPs makes an assumption that the resources are only allocated once and are then utilized by the agents independently within their infinite-horizon MDPs.",
        "This assumption that no reallocation of resources is possible can be limiting in domains where agents arrive and depart dynamically.",
        "In this paper, we extend the work on resource allocation under MDP-induced preferences to discrete-time scheduling problems, where agents are present in the system for finite time intervals and can only use resources within these intervals.",
        "In particular, agents arrive and depart at arbitrary (predefined) times and within these intervals use resources to execute tasks in finite-horizon MDPs.",
        "We address the problem of globally optimal resource scheduling, where the objective is to find an allocation of resources to the agents across time that maximizes the sum of the expected rewards that they obtain.",
        "In this context, our main contribution is a mixed-integerprogramming formulation of the scheduling problem that chooses globally optimal resource assignments, starting times, and execution horizons for all agents (within their arrival1220 978-81-904262-7-5 (RPS) c 2007 IFAAMAS departure intervals).",
        "We analyze and empirically compare two flavors of the scheduling problem: one, where agents have static resource assignments within their finite-horizon MDPs, and another, where resources can be dynamically reallocated between agents at every time step.",
        "In the rest of the paper, we first lay down the necessary groundwork in Section 2 and then introduce our model and formal problem statement in Section 3.",
        "In Section 4.2, we describe our main result, the optimization program for globally optimal resource scheduling.",
        "Following the discussion of our experimental results on a job-scheduling problem in Section 5, we conclude in Section 6 with a discussion of possible extensions and generalizations of our method. 2.",
        "BACKGROUND Similarly to the model used in previous work on resourceallocation with MDP-induced preferences [6, 7], we define the value of a set of resources to an agent as the value of the best MDP policy that is realizable, given those resources.",
        "However, since the focus of our work is on scheduling problems, and a large part of the optimization problem is to decide how resources are allocated in time among agents with finite arrival and departure times, we model the agents planning problems as finite-horizon MDPs, in contrast to previous work that used infinite-horizon discounted MDPs.",
        "In the rest of this section, we first introduce some necessary background on finite-horizon MDPs and present a linear-programming formulation that serves as the basis for our solution algorithm developed in Section 4.",
        "We also outline the standard methods for combinatorial resource scheduling with flat resource values, which serve as a comparison benchmark for the new model developed here. 2.1 Markov Decision Processes A stationary, finite-domain, discrete-time MDP (see, for example, [13] for a thorough and detailed development) can be described as S, A, p, r , where: S is a finite set of system states; A is a finite set of actions that are available to the agent; p is a stationary stochastic transition function, where p(σ|s, a) is the probability of transitioning to state σ upon executing action a in state s; r is a stationary reward function, where r(s, a) specifies the reward obtained upon executing action a in state s. Given such an MDP, a decision problem under a finite horizon T is to choose an optimal action at every time step to maximize the expected value of the total reward accrued during the agents (finite) lifetime.",
        "The agents optimal policy is then a function of current state s and the time until the horizon.",
        "An optimal policy for such a problem is to act greedily with respect to the optimal value function, defined recursively by the following system of finite-time Bellman equations [2]: v(s, t) = max a r(s, a) + X σ p(σ|s, a)v(σ, t + 1), ∀s ∈ S, t ∈ [1, T − 1]; v(s, T) = 0, ∀s ∈ S; where v(s, t) is the optimal value of being in state s at time t ∈ [1, T].",
        "This optimal value function can be easily computed using dynamic programming, leading to the following optimal policy π, where π(s, a, t) is the probability of executing action a in state s at time t: π(s, a, t) = ( 1, a = argmaxa r(s, a) + P σ p(σ|s, a)v(σ, t + 1), 0, otherwise.",
        "The above is the most common way of computing the optimal value function (and therefore an optimal policy) for a finite-horizon MDP.",
        "However, we can also formulate the problem as the following linear program (similarly to the dual LP for infinite-horizon discounted MDPs [13, 6, 7]): max X s X a r(s, a) X t x(s, a, t) subject to: X a x(σ, a, t + 1) = X s,a p(σ|s, a)x(s, a, t) ∀σ, t ∈ [1, T − 1]; X a x(s, a, 1) = α(s), ∀s ∈ S; (1) where α(s) is the initial distribution over the state space, and x is the (non-stationary) occupation measure (x(s, a, t) ∈ [0, 1] is the total expected number of times action a is executed in state s at time t).",
        "An optimal (non-stationary) policy is obtained from the occupation measure as follows: π(s, a, t) = x(s, a, t)/ X a x(s, a, t) ∀s ∈ S, t ∈ [1, T]. (2) Note that the standard unconstrained finite-horizon MDP, as described above, always has a uniformly-optimal solution (optimal for any initial distribution α(s)).",
        "Therefore, an optimal policy can be obtained by using an arbitrary constant α(s) > 0 (in particular, α(s) = 1 will result in x(s, a, t) = π(s, a, t)).",
        "However, for MDPs with resource constraints (as defined below in Section 3), uniformly-optimal policies do not in general exist.",
        "In such cases, α becomes a part of the problem input, and a resulting policy is only optimal for that particular α.",
        "This result is well known for infinite-horizon MDPs with various types of constraints [1, 6], and it also holds for our finite-horizon model, which can be easily established via a line of reasoning completely analogous to the arguments in [6]. 2.2 Combinatorial Resource Scheduling A straightforward approach to resource scheduling for a set of agents M, whose values for the resources are induced by stochastic planning problems (in our case, finite-horizon MDPs) would be to have each agent enumerate all possible resource assignments over time and, for each one, compute its value by solving the corresponding MDP.",
        "Then, each agent would provide valuations for each possible resource bundle over time to a centralized coordinator, who would compute the optimal resource assignments across time based on these valuations.",
        "When resources can be allocated at different times to different agents, each agent must submit valuations for every combination of possible time horizons.",
        "Let each agent m ∈ M execute its MDP within the arrival-departure time interval τ ∈ [τa m, τd m].",
        "Hence, agent m will execute an MDP with time horizon no greater than Tm = τd m−τa m+1.",
        "Let bτ be the global time horizon for the problem, before which all of the agents MDPs must finish.",
        "We assume τd m < bτ, ∀m ∈ M. The Sixth Intl.",
        "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1221 For the scheduling problem where agents have static resource requirements within their finite-horizon MDPs, the agents provide a valuation for each resource bundle for each possible time horizon (from [1, Tm]) that they may use.",
        "Let Ω be the set of resources to be allocated among the agents.",
        "An agent will get at most one resource bundle for one of the time horizons.",
        "Let the variable ψ ∈ Ψm enumerate all possible pairs of resource bundles and time horizons for agent m, so there are 2|Ω| × Tm values for ψ (the space of bundles is exponential in the number of resource types |Ω|).",
        "The agent m must provide a value vψ m for each ψ, and the coordinator will allocate at most one ψ (resource, time horizon) pair to each agent.",
        "This allocation is expressed as an indicator variable zψ m ∈ {0, 1} that shows whether ψ is assigned to agent m. For time τ and resource ω, the function nm(ψ, τ, ω) ∈ {0, 1} indicates whether the bundle in ψ uses resource ω at time τ (we make the assumption that agents have binary resource requirements).",
        "This allocation problem is NP-complete, even when considering only a single time step, and its difficulty increases significantly with multiple time steps because of the increasing number of values of ψ.",
        "The problem of finding an optimal allocation that satisfies the global constraint that the amount of each resource ω allocated to all agents does not exceed the available amount bϕ(ω) can be expressed as the following integer program: max X m∈M X ψ∈Ψm zψ mvψ m subject to: X ψ∈Ψm zψ m ≤ 1, ∀m ∈ M; X m∈M X ψ∈Ψm zψ mnm(ψ, τ, ω) ≤ bϕ(ω), ∀τ ∈ [1, bτ], ∀ω ∈ Ω; (3) The first constraint in equation 3 says that no agent can receive more than one bundle, and the second constraint ensures that the total assignment of resource ω does not, at any time, exceed the resource bound.",
        "For the scheduling problem where the agents are able to dynamically reallocate resources, each agent must specify a value for every combination of bundles and time steps within its time horizon.",
        "Let the variable ψ ∈ Ψm in this case enumerate all possible resource bundles for which at most one bundle may be assigned to agent m at each time step.",
        "Therefore, in this case there are P t∈[1,Tm](2|Ω| )t ∼ 2|Ω|Tm possibilities of resource bundles assigned to different time slots, for the Tm different time horizons.",
        "The same set of equations (3) can be used to solve this dynamic scheduling problem, but the integer program is different because of the difference in how ψ is defined.",
        "In this case, the number of ψ values is exponential in each agents planning horizon Tm, resulting in a much larger program.",
        "This straightforward approach to solving both of these scheduling problems requires an enumeration and solution of either 2|Ω| Tm (static allocation) or P t∈[1,Tm] 2|Ω|t (dynamic reallocation) MDPs for each agent, which very quickly becomes intractable with the growth of the number of resources |Ω| or the time horizon Tm. 3.",
        "MODEL AND PROBLEM STATEMENT We now formally introduce our model of the resourcescheduling problem.",
        "The problem input consists of the following components: • M, Ω, bϕ, τa m, τd m, bτ are as defined above in Section 2.2. • {Θm} = {S, A, pm, rm, αm} are the MDPs of all agents m ∈ M. Without loss of generality, we assume that state and action spaces of all agents are the same, but each has its own transition function pm, reward function rm, and initial conditions αm. • ϕm : A×Ω → {0, 1} is the mapping of actions to resources for agent m. ϕm(a, ω) indicates whether action a of agent m needs resource ω.",
        "An agent m that receives a set of resources that does not include resource ω cannot execute in its MDP policy any action a for which ϕm(a, ω) = 0.",
        "We assume all resource requirements are binary; as discussed below in Section 6, this assumption is not limiting.",
        "Given the above input, the optimization problem we consider is to find the globally optimal-maximizing the sum of expected rewards-mapping of resources to agents for all time steps: Δ : τ × M × Ω → {0, 1}.",
        "A solution is feasible if the corresponding assignment of resources to the agents does not violate the global resource constraint: X m Δm(τ, ω) ≤ bϕ(ω), ∀ω ∈ Ω, τ ∈ [1, bτ]. (4) We consider two flavors of the resource-scheduling problem.",
        "The first formulation restricts resource assignments to the space where the allocation of resources to each agent is static during the agents lifetime.",
        "The second formulation allows reassignment of resources between agents at every time step within their lifetimes.",
        "Figure 1 depicts a resource-scheduling problem with three agents M = {m1, m2, m3}, three resources Ω = {ω1, ω2, ω3}, and a global problem horizon of bτ = 11.",
        "The agents arrival and departure times are shown as gray boxes and are {1, 6}, {3, 7}, and {2, 11}, respectively.",
        "A solution to this problem is shown via horizontal bars within each agents box, where the bars correspond to the allocation of the three resource types.",
        "Figure 1a shows a solution to a static scheduling problem.",
        "According to the shown solution, agent m1 begins the execution of its MDP at time τ = 1 and has a lock on all three resources until it finishes execution at time τ = 3.",
        "Note that agent m1 relinquishes its hold on the resources before its announced departure time of τd m1 = 6, ostensibly because other agents can utilize the resources more effectively.",
        "Thus, at time τ = 4, resources ω1 and ω3 are allocated to agent m2, who then uses them to execute its MDP (using only actions supported by resources ω1 and ω3) until time τ = 7.",
        "Agent m3 holds resource ω3 during the interval τ ∈ [4, 10].",
        "Figure 1b shows a possible solution to the dynamic version of the same problem.",
        "There, resources can be reallocated between agents at every time step.",
        "For example, agent m1 gives up its use of resource ω2 at time τ = 2, although it continues the execution of its MDP until time τ = 6.",
        "Notice that an agent is not allowed to stop and restart its MDP, so agent m1 is only able to continue executing in the interval τ ∈ [3, 4] if it has actions that do not require any resources (ϕm(a, ω) = 0).",
        "Clearly, the model and problem statement described above make a number of assumptions about the problem and the desired solution properties.",
        "We discuss some of those assumptions and their implications in Section 6. 1222 The Sixth Intl.",
        "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) (a) (b) Figure 1: Illustration of a solution to a resource-scheduling problem with three agents and three resources: a) static resource assignments (resource assignments are constant within agents lifetimes; b) dynamic assignment (resource assignments are allowed to change at every time step). 4.",
        "RESOURCE SCHEDULING Our resource-scheduling algorithm proceeds in two stages.",
        "First, we perform a preprocessing step that augments the agent MDPs; this process is described in Section 4.1.",
        "Second, using these augmented MDPs we construct a global optimization problem, which is described in Section 4.2. 4.1 Augmenting Agents MDPs In the model described in the previous section, we assume that if an agent does not possess the necessary resources to perform actions in its MDP, its execution is halted and the agent leaves the system.",
        "In other words, the MDPs cannot be paused and resumed.",
        "For example, in the problem shown in Figure 1a, agent m1 releases all resources after time τ = 3, at which point the execution of its MDP is halted.",
        "Similarly, agents m2 and m3 only execute their MDPs in the intervals τ ∈ [4, 6] and τ ∈ [4, 10], respectively.",
        "Therefore, an important part of the global decision-making problem is to decide the window of time during which each of the agents is active (i.e., executing its MDP).",
        "To accomplish this, we augment each agents MDP with two new states (start and finish states sb , sf , respectively) and a new start/stop action a∗ , as illustrated in Figure 2.",
        "The idea is that an agent stays in the start state sb until it is ready to execute its MDP, at which point it performs the start/stop action a∗ and transitions into the state space of the original MDP with the transition probability that corresponds to the original initial distribution α(s).",
        "For example, in Figure 1a, for agent m2 this would happen at time τ = 4.",
        "Once the agent gets to the end of its activity window (time τ = 6 for agent m2 in Figure 1a), it performs the start/stop action, which takes it into the sink finish state sf at time τ = 7.",
        "More precisely, given an MDP S, A, pm, rm, αm , we define an augmented MDP S , A , pm, rm, αm as follows: S = S ∪ sb ∪ sf ; A = A ∪ a∗ ; p (s|sb , a∗ ) = α(s), ∀s ∈ S; p (sb |sb , a) = 1.0, ∀a ∈ A; p (sf |s, a∗ ) = 1.0, ∀s ∈ S; p (σ|s, a) = p(σ|s, a), ∀s, σ ∈ S, a ∈ A; r (sb , a) = r (sf , a) = 0, ∀a ∈ A ; r (s, a) = r(s, a), ∀s ∈ S, a ∈ A; α (sb ) = 1; α (s) = 0, ∀s ∈ S; where all non-specified transition probabilities are assumed to be zero.",
        "Further, in order to account for the new starting state, we begin the MDP one time-step earlier, setting τa m ← τa m − 1.",
        "This will not affect the resource allocation due to the resource constraints only being enforced for the original MDP states, as will be discussed in the next section.",
        "For example, the augmented MDPs shown in Figure 2b (which starts in state sb at time τ = 2) would be constructed from an MDP with original arrival time τ = 3.",
        "Figure 2b also shows a sample trajectory through the state space: the agent starts in state sb , transitions into the state space S of the original MDP, and finally exists into the sink state sf .",
        "Note that if we wanted to model a problem where agents could pause their MDPs at arbitrary time steps (which might be useful for domains where dynamic reallocation is possible), we could easily accomplish this by including an extra action that transitions from each state to itself with zero reward. 4.2 MILP for Resource Scheduling Given a set of augmented MDPs, as defined above, the goal of this section is to formulate a global optimization program that solves the resource-scheduling problem.",
        "In this section and below, all MDPs are assumed to be the augmented MDPs as defined in Section 4.1.",
        "Our approach is similar to the idea used in [6]: we begin with the linear-program formulation of agents MDPs (1) and augment it with constraints that ensure that the corresponding resource allocation across agents and time is valid.",
        "The resulting optimization problem then simultaneously solves the agents MDPs and resource-scheduling problems.",
        "In the rest of this section, we incrementally develop a mixed integer program (MILP) that achieves this.",
        "In the absence of resource constraints, the agents finitehorizon MDPs are completely independent, and the globally optimal solution can be trivially obtained via the following LP, which is simply an aggregation of single-agent finitehorizon LPs: max X m X s X a rm(s, a) X t xm(s, a, t) subject to: X a xm(σ, a, t + 1) = X s,a pm(σ|s, a)xm(s, a, t), ∀m ∈ M, σ ∈ S, t ∈ [1, Tm − 1]; X a xm(s, a, 1) = αm(s), ∀m ∈ M, s ∈ S; (12) where xm(s, a, t) is the occupation measure of agent m, and The Sixth Intl.",
        "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1223 (a) (b) Figure 2: Illustration of augmenting an MDP to allow for variable starting and stopping times: a) (left) the original two-state MDP with a single action; (right) the augmented MDP with new states sb and sf and the new action a∗ (note that the origianl transitions are not changed in the augmentation process); b) the augmented MDP displayed as a trajectory through time (grey lines indicate all transitions, while black lines indicate a given trajectory.",
        "Objective Function (sum of expected rewards over all agents) max X m X s X a rm(s, a) X t xm(s, a, t) (5) Meaning Implication Linear Constraints Tie x to θ.",
        "Agent is only active when occupation measure is nonzero in original MDP states. θm(τ) = 0 =⇒ xm(s, a, τ −τa m+1) = 0 ∀s /∈ {sb , sf }, a ∈ A X s/∈{sb,sf } X a xm(s, a, t) ≤ θm(τa m + t − 1) ∀m ∈ M, ∀t ∈ [1, Tm] (6) Agent can only be active in τ ∈ (τa m, τd m) θm(τ) = 0 ∀m ∈ M, τ /∈ (τa m, τd m) (7) Cannot use resources when not active θm(τ) = 0 =⇒ Δm(τ, ω) = 0 ∀τ ∈ [0, bτ], ω ∈ Ω Δm(τ, ω) ≤ θm(τ) ∀m ∈ M, τ ∈ [0, bτ], ω ∈ Ω (8) Tie x to Δ (nonzero x forces corresponding Δ to be nonzero.)",
        "Δm(τ, ω) = 0, ϕm(a, ω) = 1 =⇒ xm(s, a, τ − τa m + 1) = 0 ∀s /∈ {sb , sf } 1/|A| X a ϕm(a, ω) X s/∈{sb,sf } xm(s, a, t) ≤ Δm(t + τa m − 1, ω) ∀m ∈ M, ω ∈ Ω, t ∈ [1, Tm] (9) Resource bounds X m Δm(τ, ω) ≤ bϕ(ω) ∀ω ∈ Ω, τ ∈ [0, bτ] (10) Agent cannot change resources while active.",
        "Only enabled for scheduling with static assignments. θm(τ) = 1 and θm(τ + 1) = 1 =⇒ Δm(τ, ω) = Δm(τ + 1, ω) Δm(τ, ω) − Z(1 − θm(τ + 1)) ≤ Δm(τ + 1, ω) + Z(1 − θm(τ)) Δm(τ, ω) + Z(1 − θm(τ + 1)) ≥ Δm(τ + 1, ω) − Z(1 − θm(τ)) ∀m ∈ M, ω ∈ Ω, τ ∈ [0, bτ] (11) Table 1: MILP for globally optimal resource scheduling.",
        "Tm = τd m − τa m + 1 is the time horizon for the agents MDP.",
        "Using this LP as a basis, we augment it with constraints that ensure that the resource usage implied by the agents occupation measures {xm} does not violate the global resource requirements bϕ at any time step τ ∈ [0, bτ].",
        "To formulate these resource constraints, we use the following binary variables: • Δm(τ, ω) = {0, 1}, ∀m ∈ M, τ ∈ [0, bτ], ω ∈ Ω, which serve as indicator variables that define whether agent m possesses resource ω at time τ.",
        "These are analogous to the static indicator variables used in the one-shot static resource-allocation problem in [6]. • θm = {0, 1}, ∀m ∈ M, τ ∈ [0, bτ] are indicator variables that specify whether agent m is active (i.e., executing its MDP) at time τ.",
        "The meaning of resource-usage variables Δ is illustrated in Figure 1: Δm(τ, ω) = 1 only if resource ω is allocated to agent m at time τ.",
        "The meaning of the activity indicators θ is illustrated in Figure 2b: when agent m is in either the start state sb or the finish state sf , the corresponding θm = 0, but once the agent becomes active and enters one of the other states, we set θm = 1 .",
        "This meaning of θ can be enforced with a linear constraint that synchronizes the values of the agents occupation measures xm and the activity 1224 The Sixth Intl.",
        "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) indicators θ, as shown in (6) in Table 1.",
        "Another constraint we have to add-because the activity indicators θ are defined on the global timeline τ-is to enforce the fact that the agent is inactive outside of its arrivaldeparture window.",
        "This is accomplished by constraint (7) in Table 1.",
        "Furthermore, agents should not be using resources while they are inactive.",
        "This constraint can also be enforced via a linear inequality on θ and Δ, as shown in (8).",
        "Constraint (6) sets the value of θ to match the policy defined by the occupation measure xm.",
        "In a similar fashion, we have to make sure that the resource-usage variables Δ are also synchronized with the occupation measure xm.",
        "This is done via constraint (9) in Table 1, which is nearly identical to the analogous constraint from [6].",
        "After implementing the above constraint, which enforces the meaning of Δ, we add a constraint that ensures that the agents resource usage never exceeds the amounts of available resources.",
        "This condition is also trivially expressed as a linear inequality (10) in Table 1.",
        "Finally, for the problem formulation where resource assignments are static during a lifetime of an agent, we add a constraint that ensures that the resource-usage variables Δ do not change their value while the agent is active (θ = 1).",
        "This is accomplished via the linear constraint (11), where Z ≥ 2 is a constant that is used to turn off the constraints when θm(τ) = 0 or θm(τ + 1) = 0.",
        "This constraint is not used for the dynamic problem formulation, where resources can be reallocated between agents at every time step.",
        "To summarize, Table 1 together with the conservationof-flow constraints from (12) defines the MILP that simultaneously computes an optimal resource assignment for all agents across time as well as optimal finite-horizon MDP policies that are valid under that resource assignment.",
        "As a rough measure of the complexity of this MILP, let us consider the number of optimization variables and constraints.",
        "Let TM = P Tm = P m(τa m − τd m + 1) be the sum of the lengths of the arrival-departure windows across all agents.",
        "Then, the number of optimization variables is: TM + bτ|M||Ω| + bτ|M|, TM of which are continuous (xm), and bτ|M||Ω| + bτ|M| are binary (Δ and θ).",
        "However, notice that all but TM|M| of the θ are set to zero by constraint (7), which also immediately forces all but TM|M||Ω| of the Δ to be zero via the constraints (8).",
        "The number of constraints (not including the degenerate constraints in (7)) in the MILP is: TM + TM|Ω| + bτ|Ω| + bτ|M||Ω|.",
        "Despite the fact that the complexity of the MILP is, in the worst case, exponential1 in the number of binary variables, the complexity of this MILP is significantly (exponentially) lower than that of the MILP with flat utility functions, described in Section 2.2.",
        "This result echos the efficiency gains reported in [6] for single-shot resource-allocation problems, but is much more pronounced, because of the explosion of the flat utility representation due to the temporal aspect of the problem (recall the prohibitive complexity of the combinatorial optimization in Section 2.2).",
        "We empirically analyze the performance of this method in Section 5. 1 Strictly speaking, solving MILPs to optimality is NPcomplete in the number of integer variables. 5.",
        "EXPERIMENTAL RESULTS Although the complexity of solving MILPs is in the worst case exponential in the number of integer variables, there are many efficient methods for solving MILPs that allow our algorithm to scale well for parameters common to resource allocation and scheduling problems.",
        "In particular, this section introduces a problem domain-the repairshop problem-used to empirically evaluate our algorithms scalability in terms of the number of agents |M|, the number of shared resources |Ω|, and the varied lengths of global time bτ during which agents may enter and exit the system.",
        "The repairshop problem is a simple parameterized MDP adopting the metaphor of a vehicular repair shop.",
        "Agents in the repair shop are mechanics with a number of independent tasks that yield reward only when completed.",
        "In our MDP model of this system, actions taken to advance through the state space are only allowed if the agent holds certain resources that are publicly available to the shop.",
        "These resources are in finite supply, and optimal policies for the shop will determine when each agent may hold the limited resources to take actions and earn individual rewards.",
        "Each task to be completed is associated with a single action, although the agent is required to repeat the action numerous times before completing the task and earning a reward.",
        "This model was parameterized in terms of the number of agents in the system, the number of different types of resources that could be linked to necessary actions, a global time during which agents are allowed to arrive and depart, and a maximum length for the number of time steps an agent may remain in the system.",
        "All datapoints in our experiments were obtained with 20 evaluations using CPLEX to solve the MILPs on a Pentium4 computer with 2Gb of RAM.",
        "Trials were conducted on both the static and the dynamic version of the resourcescheduling problem, as defined earlier.",
        "Figure 3 shows the runtime and policy value for independent modifications to the parameter set.",
        "The top row shows how the solution time for the MILP scales as we increase the number of agents |M|, the global time horizon bτ, and the number of resources |Ω|.",
        "Increasing the number of agents leads to exponential complexity scaling, which is to be expected for an NP-complete problem.",
        "However, increasing the global time limit bτ or the total number of resource types |Ω|-while holding the number of agents constantdoes not lead to decreased performance.",
        "This occurs because the problems get easier as they become under-constrained, which is also a common phenomenon for NP-complete problems.",
        "We also observe that the solution to the dynamic version of the problem can often be computed much faster than the static version.",
        "The bottom row of Figure 3 shows the joint policy value of the policies that correspond to the computed optimal resource-allocation schedules.",
        "We can observe that the dynamic version yields higher reward (as expected, since the reward for the dynamic version is always no less than the reward of the static version).",
        "We should point out that these graphs should not be viewed as a measure of performance of two different algorithms (both algorithms produce optimal solutions but to different problems), but rather as observations about how the quality of optimal solutions change as more flexibility is allowed in the reallocation of resources.",
        "Figure 4 shows runtime and policy value for trials in which common input variables are scaled together.",
        "This allows The Sixth Intl.",
        "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1225 2 4 6 8 10 10 −3 10 −2 10 −1 10 0 10 1 10 2 10 3 10 4 Number of Agents |M| CPUTime,sec |Ω| = 5, τ = 50 static dynamic 50 100 150 200 10 −2 10 −1 10 0 10 1 10 2 10 3 Global Time Boundary τ CPUTime,sec |M| = 5, |Ω| = 5 static dynamic 10 20 30 40 50 10 −2 10 −1 10 0 10 1 10 2 Number of Resources |Ω| CPUTime,sec |M| = 5, τ = 50 static dynamic 2 4 6 8 10 200 400 600 800 1000 1200 1400 1600 Number of Agents |M| Value |Ω| = 5, τ = 50 static dynamic 50 100 150 200 400 500 600 700 800 900 1000 1100 1200 1300 1400 Global Time Boundary τ Value |M| = 5, |Ω| = 5 static dynamic 10 20 30 40 50 500 600 700 800 900 1000 1100 1200 1300 1400 Number of Resources |Ω| Value |M| = 5, τ = 50 static dynamic Figure 3: Evaluation of our MILP for variable numbers of agents (column 1), lengths of global-time window (column 2), and numbers of resource types (column 3).",
        "Top row shows CPU time, and bottom row shows the joint reward of agents MDP policies.",
        "Error bars show the 1st and 3rd quartiles (25% and 75%). 2 4 6 8 10 10 −3 10 −2 10 −1 10 0 10 1 10 2 10 3 Number of Agents |M| CPUTime,sec τ = 10|M| static dynamic 2 4 6 8 10 10 −3 10 −2 10 −1 10 0 10 1 10 2 10 3 10 4 Number of Agents |M| CPUTime,sec |Ω| = 2|M| static dynamic 2 4 6 8 10 10 −3 10 −2 10 −1 10 0 10 1 10 2 10 3 10 4 Number of Agents |M| CPUTime,sec |Ω| = 5|M| static dynamic 2 4 6 8 10 200 400 600 800 1000 1200 1400 1600 1800 2000 2200 Number of Agents |M| Value τ = 10|M| static dynamic 2 4 6 8 10 200 400 600 800 1000 1200 1400 1600 1800 2000 Number of Agents |M| Value |Ω| = 2|M| static dynamic 2 4 6 8 10 0 500 1000 1500 2000 2500 Number of Agents |M| Value |Ω| = 5|M| static dynamic Figure 4: Evaluation of our MILP using correlated input variables.",
        "The left column tracks the performance and CPU time as the number of agents and global-time window increase together (bτ = 10|M|).",
        "The middle and the right column track the performance and CPU time as the number of resources and the number of agents increase together as |Ω| = 2|M| and |Ω| = 5|M|, respectively.",
        "Error bars show the 1st and 3rd quartiles (25% and 75%). 1226 The Sixth Intl.",
        "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) us to explore domains where the total number of agents scales proportionally to the total number of resource types or the global time horizon, while keeping constant the average agent density (per unit of global time) or the average number of resources per agent (which commonly occurs in real-life applications).",
        "Overall, we believe that these experimental results indicate that our MILP formulation can be used to effectively solve resource-scheduling problems of nontrivial size. 6.",
        "DISCUSSION AND CONCLUSIONS Throughout the paper, we have made a number of assumptions in our model and solution algorithm; we discuss their implications below. • Continual execution.",
        "We assume that once an agent stops executing its MDP (transitions into state sf ), it exits the system and cannot return.",
        "It is easy to relax this assumption for domains where agents MDPs can be paused and restarted.",
        "All that is required is to include an additional pause action which transitions from a given state back to itself, and has zero reward. • Indifference to start time.",
        "We used a reward model where agents rewards depend only on the time horizon of their MDPs and not the global start time.",
        "This is a consequence of our MDP-augmentation procedure from Section 4.1.",
        "It is easy to extend the model so that the agents incur an explicit penalty for idling by assigning a non-zero negative reward to the start state sb . • Binary resource requirements.",
        "For simplicity, we have assumed that resource costs are binary: ϕm(a, ω) = {0, 1}, but our results generalize in a straightforward manner to non-binary resource mappings, analogously to the procedure used in [5]. • Cooperative agents.",
        "The optimization procedure discussed in this paper was developed in the context of cooperative agents, but it can also be used to design a mechanism for scheduling resources among self-interested agents.",
        "This optimization procedure can be embedded in a VickreyClarke-Groves auction, completely analogously to the way it was done in [7].",
        "In fact, all the results of [7] about the properties of the auction and information privacy directly carry over to the scheduling domain discussed in this paper, requiring only slight modifications to deal with finitehorizon MDPs. • Known, deterministic arrival and departure times.",
        "Finally, we have assumed that agents arrival and departure times (τa m and τd m) are deterministic and known a priori.",
        "This assumption is fundamental to our solution method.",
        "While there are many domains where this assumption is valid, in many cases agents arrive and depart dynamically and their arrival and departure times can only be predicted probabilistically, leading to online resource-allocation problems.",
        "In particular, in the case of self-interested agents, this becomes an interesting version of an online-mechanism-design problem [11, 12].",
        "In summary, we have presented an MILP formulation for the combinatorial resource-scheduling problem where agents values for possible resource assignments are defined by finitehorizon MDPs.",
        "This result extends previous work ([6, 7]) on static one-shot resource allocation under MDP-induced preferences to resource-scheduling problems with a temporal aspect.",
        "As such, this work takes a step in the direction of designing an online mechanism for agents with combinatorial resource preferences induced by stochastic planning problems.",
        "Relaxing the assumption about deterministic arrival and departure times of the agents is a focus of our future work.",
        "We would like to thank the anonymous reviewers for their insightful comments and suggestions. 7.",
        "REFERENCES [1] E. Altman and A. Shwartz.",
        "Adaptive control of constrained Markov chains: Criteria and policies.",
        "Annals of Operations Research, special issue on Markov Decision Processes, 28:101-134, 1991. [2] R. Bellman.",
        "Dynamic Programming.",
        "Princeton University Press, 1957. [3] C. Boutilier.",
        "Solving concisely expressed combinatorial auction problems.",
        "In Proc. of AAAI-02, pages 359-366, 2002. [4] C. Boutilier and H. H. Hoos.",
        "Bidding languages for combinatorial auctions.",
        "In Proc. of IJCAI-01, pages 1211-1217, 2001. [5] D. Dolgov.",
        "Integrated Resource Allocation and Planning in Stochastic Multiagent Environments.",
        "PhD thesis, Computer Science Department, University of Michigan, February 2006. [6] D. A. Dolgov and E. H. Durfee.",
        "Optimal resource allocation and policy formulation in loosely-coupled Markov decision processes.",
        "In Proc. of ICAPS-04, pages 315-324, June 2004. [7] D. A. Dolgov and E. H. Durfee.",
        "Computationally efficient combinatorial auctions for resource allocation in weakly-coupled MDPs.",
        "In Proc. of AAMAS-05, New York, NY, USA, 2005.",
        "ACM Press. [8] D. A. Dolgov and E. H. Durfee.",
        "Resource allocation among agents with preferences induced by factored MDPs.",
        "In Proc. of AAMAS-06, 2006. [9] K. Larson and T. Sandholm.",
        "Mechanism design and deliberative agents.",
        "In Proc. of AAMAS-05, pages 650-656, New York, NY, USA, 2005.",
        "ACM Press. [10] N. Nisan.",
        "Bidding and allocation in combinatorial auctions.",
        "In Electronic Commerce, 2000. [11] D. C. Parkes and S. Singh.",
        "An MDP-based approach to Online Mechanism Design.",
        "In Proc. of the Seventeenths Annual Conference on Neural Information Processing Systems (NIPS-03), 2003. [12] D. C. Parkes, S. Singh, and D. Yanovsky.",
        "Approximately efficient online mechanism design.",
        "In Proc. of the Eighteenths Annual Conference on Neural Information Processing Systems (NIPS-04), 2004. [13] M. L. Puterman.",
        "Markov Decision Processes.",
        "John Wiley & Sons, New York, 1994. [14] M. H. Rothkopf, A. Pekec, and R. M. Harstad.",
        "Computationally manageable combinational auctions.",
        "Management Science, 44(8):1131-1147, 1998. [15] T. Sandholm.",
        "An algorithm for optimal winner determination in combinatorial auctions.",
        "In Proc. of IJCAI-99, pages 542-547, San Francisco, CA, USA, 1999.",
        "Morgan Kaufmann Publishers Inc.",
        "The Sixth Intl.",
        "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1227"
    ],
    "translated_text_sentences": [
        "La programación de recursos combinatoria para MDPs multiagentes. Dmitri A. Dolgov, Michael R. James y Michael E. Samples. Grupo de Investigación en IA y Robótica, Centro Técnico de Toyota en EE. UU. {ddolgov, michael.r.james, michael.samples}@gmail.com. RESUMEN La programación óptima de recursos en sistemas multiagentes es una tarea computacionalmente desafiante, especialmente cuando los valores de los recursos no son aditivos.",
        "Consideramos el problema combinatorio de programar el uso de múltiples recursos entre agentes que operan en entornos estocásticos, modelados como procesos de decisión de Markov (MDPs).",
        "En los últimos años, se han desarrollado algoritmos eficientes de asignación de recursos para agentes con valores de recursos inducidos por MDPs.",
        "Sin embargo, este trabajo previo se ha centrado en problemas de asignación de recursos estáticos donde los recursos se distribuyen una vez y luego se utilizan en MDPs de horizonte infinito.",
        "Extendemos esos modelos existentes al problema de la programación de recursos combinatoria, donde los agentes persisten solo por períodos finitos entre sus tiempos de llegada y salida (predefinidos), requiriendo recursos solo durante esos períodos de tiempo.",
        "Proporcionamos un procedimiento computacionalmente eficiente para calcular asignaciones de recursos óptimas a nivel global a agentes a lo largo del tiempo.",
        "Ilustramos y analizamos empíricamente el método en el contexto de un dominio de programación de trabajos estocásticos.",
        "Categorías y Descriptores de Asignaturas I.2.8 [Inteligencia Artificial]: Resolución de Problemas, Métodos de Control y Búsqueda; I.2.11 [Inteligencia Artificial]: Inteligencia Artificial Distribuida-Sistemas Multiagente Términos Generales Algoritmos, Rendimiento, Diseño 1.",
        "INTRODUCCIÓN Las tareas de asignación óptima de recursos y programación son ubicuas en sistemas multiagentes, pero resolver tales problemas de optimización puede ser computacionalmente difícil, debido a una serie de factores.",
        "En particular, cuando el valor de un conjunto de recursos para un agente no es aditivo (como suele ser el caso con recursos que son sustitutos o complementos), la función de utilidad podría tener que definirse en un espacio exponencialmente grande de combinaciones de recursos, lo que rápidamente se vuelve computacionalmente intratable.",
        "Además, incluso cuando cada agente tiene una función de utilidad que es distinta de cero solo en un pequeño subconjunto de los posibles conjuntos de recursos, obtener una asignación óptima sigue siendo computacionalmente prohibitivo, ya que el problema se vuelve NP-completo [14].",
        "Tales problemas computacionales han dado lugar recientemente a varias líneas de trabajo en el uso de modelos compactos de las preferencias de los agentes.",
        "Una idea es utilizar cualquier estructura presente en las funciones de utilidad para representarlas de forma compacta, por ejemplo, a través de fórmulas lógicas [15, 10, 4, 3].",
        "Una alternativa es modelar directamente los mecanismos que definen las funciones de utilidad de los agentes y realizar la asignación de recursos directamente con estos modelos [9].",
        "Una forma de lograr esto es modelar los procesos mediante los cuales un agente podría utilizar los recursos y definir la función de utilidad como la ganancia de estos procesos.",
        "En particular, si un agente utiliza recursos para actuar en un entorno estocástico, su función de utilidad puede ser modelada de forma natural con un proceso de decisión de Markov, cuyo conjunto de acciones está parametrizado por los recursos disponibles.",
        "Esta representación puede ser utilizada para construir algoritmos de asignación de recursos muy eficientes que resultan en una aceleración exponencial sobre un problema de optimización directa con representaciones planas de preferencias combinatorias [6, 7, 8].",
        "Sin embargo, este trabajo existente sobre asignación de recursos con preferencias inducidas por MDPs parametrizados por recursos hace una suposición de que los recursos se asignan solo una vez y luego son utilizados por los agentes de forma independiente dentro de sus MDPs de horizonte infinito.",
        "Esta suposición de que no es posible la reasignación de recursos puede ser limitante en dominios donde los agentes llegan y se van dinámicamente.",
        "En este artículo, ampliamos el trabajo sobre asignación de recursos bajo preferencias inducidas por MDP a problemas de programación en tiempo discreto, donde los agentes están presentes en el sistema durante intervalos de tiempo finitos y solo pueden utilizar recursos dentro de estos intervalos.",
        "En particular, los agentes llegan y parten en momentos arbitrarios (predefinidos) y dentro de estos intervalos utilizan recursos para ejecutar tareas en MDPs de horizonte finito.",
        "Abordamos el problema de la programación de recursos óptima a nivel global, donde el objetivo es encontrar una asignación de recursos a los agentes a lo largo del tiempo que maximice la suma de las recompensas esperadas que obtienen.",
        "En este contexto, nuestra principal contribución es una formulación de programación entera mixta del problema de programación que elige asignaciones de recursos globalmente óptimas, tiempos de inicio y horizontes de ejecución para todos los agentes (dentro de sus intervalos de llegada y salida).",
        "Analizamos y comparamos empíricamente dos variantes del problema de programación: uno, donde los agentes tienen asignaciones de recursos estáticas dentro de sus MDPs de horizonte finito, y otro, donde los recursos pueden ser realocados dinámicamente entre agentes en cada paso de tiempo.",
        "En el resto del documento, primero establecemos las bases necesarias en la Sección 2 y luego presentamos nuestro modelo y declaración formal del problema en la Sección 3.",
        "En la Sección 4.2, describimos nuestro resultado principal, el programa de optimización para la programación de recursos globalmente óptima.",
        "Tras la discusión de nuestros resultados experimentales sobre un problema de programación de trabajos en la Sección 5, concluimos en la Sección 6 con una discusión de posibles extensiones y generalizaciones de nuestro método.",
        "De manera similar al modelo utilizado en trabajos anteriores sobre asignación de recursos con preferencias inducidas por MDP [6, 7], definimos el valor de un conjunto de recursos para un agente como el valor de la mejor política MDP que es realizable, dadas esos recursos.",
        "Sin embargo, dado que el enfoque de nuestro trabajo se centra en problemas de programación, y una gran parte del problema de optimización consiste en decidir cómo se asignan los recursos en el tiempo entre agentes con tiempos finitos de llegada y salida, modelamos los problemas de planificación de los agentes como MDPs de horizonte finito, en contraste con trabajos anteriores que utilizaron MDPs descontados de horizonte infinito.",
        "En el resto de esta sección, primero presentamos algo de antecedentes necesarios sobre MDPs de horizonte finito y presentamos una formulación de programación lineal que sirve como base para nuestro algoritmo de solución desarrollado en la Sección 4.",
        "También describimos los métodos estándar para la programación de recursos combinatorios con valores de recursos planos, que sirven como punto de referencia de comparación para el nuevo modelo desarrollado aquí. 2.1 Procesos de Decisión de Markov Un MDP (Proceso de Decisión de Markov) estacionario, de dominio finito y discreto en el tiempo (ver, por ejemplo, [13] para un desarrollo detallado y exhaustivo) se puede describir como S, A, p, r, donde: S es un conjunto finito de estados del sistema; A es un conjunto finito de acciones disponibles para el agente; p es una función de transición estocástica estacionaria, donde p(σ|s, a) es la probabilidad de transicionar al estado σ al ejecutar la acción a en el estado s; r es una función de recompensa estacionaria, donde r(s, a) especifica la recompensa obtenida al ejecutar la acción a en el estado s. Dado un MDP de este tipo, un problema de decisión bajo un horizonte finito T consiste en elegir una acción óptima en cada paso de tiempo para maximizar el valor esperado de la recompensa total acumulada durante la vida (finita) de los agentes.",
        "La política óptima del agente es entonces una función del estado actual s y el tiempo hasta el horizonte.",
        "Una política óptima para tal problema es actuar de forma codiciosa con respecto a la función de valor óptimo, definida de forma recursiva por el siguiente sistema de ecuaciones de Bellman de tiempo finito [2]: v(s, t) = max a r(s, a) + X σ p(σ|s, a)v(σ, t + 1), ∀s ∈ S, t ∈ [1, T − 1]; v(s, T) = 0, ∀s ∈ S; donde v(s, t) es el valor óptimo de estar en el estado s en el tiempo t ∈ [1, T].",
        "Esta función de valor óptimo se puede calcular fácilmente utilizando programación dinámica, lo que conduce a la siguiente política óptima π, donde π(s, a, t) es la probabilidad de ejecutar la acción a en el estado s en el tiempo t: π(s, a, t) = (1, a = argmaxa r(s, a) + P σ p(σ|s, a)v(σ, t + 1), 0, en caso contrario.",
        "La anterior es la forma más común de calcular la función de valor óptimo (y por lo tanto una política óptima) para un MDP de horizonte finito.",
        "Sin embargo, también podemos formular el problema como el siguiente programa lineal (similarmente al LP dual para MDPs descontados de horizonte infinito [13, 6, 7]): max X s X a r(s, a) X t x(s, a, t) sujeto a: X a x(σ, a, t + 1) = X s,a p(σ|s, a)x(s, a, t) ∀σ, t ∈ [1, T − 1]; X a x(s, a, 1) = α(s), ∀s ∈ S; (1) donde α(s) es la distribución inicial sobre el espacio de estados, y x es la medida de ocupación (no estacionaria) (x(s, a, t) ∈ [0, 1] es el número total esperado de veces que se ejecuta la acción a en el estado s en el tiempo t).",
        "Una política óptima (no estacionaria) se obtiene a partir de la medida de ocupación de la siguiente manera: π(s, a, t) = x(s, a, t)/ X a x(s, a, t) ∀s ∈ S, t ∈ [1, T]. (2) Tenga en cuenta que el MDP estándar de horizonte finito sin restricciones, como se describe anteriormente, siempre tiene una solución uniformemente óptima (óptima para cualquier distribución inicial α(s)).",
        "Por lo tanto, una política óptima se puede obtener utilizando una constante arbitraria α(s) > 0 (en particular, α(s) = 1 resultará en x(s, a, t) = π(s, a, t)).",
        "Sin embargo, para los MDPs con restricciones de recursos (como se definen a continuación en la Sección 3), las políticas óptimas uniformemente no existen en general.",
        "En tales casos, α se convierte en parte de la entrada del problema, y una política resultante solo es óptima para ese α en particular.",
        "Este resultado es bien conocido para MDPs de horizonte infinito con varios tipos de restricciones [1, 6], y también se cumple para nuestro modelo de horizonte finito, el cual puede establecerse fácilmente a través de una línea de razonamiento completamente análoga a los argumentos en [6]. 2.2 Programación de Recursos Combinatoria Un enfoque directo para la programación de recursos para un conjunto de agentes M, cuyos valores para los recursos son inducidos por problemas de planificación estocástica (en nuestro caso, MDPs de horizonte finito) sería que cada agente enumerara todas las asignaciones de recursos posibles a lo largo del tiempo y, para cada una, calcular su valor resolviendo el MDP correspondiente.",
        "Entonces, cada agente proporcionaría valoraciones para cada posible conjunto de recursos a lo largo del tiempo a un coordinador centralizado, quien calcularía las asignaciones óptimas de recursos a lo largo del tiempo basándose en estas valoraciones.",
        "Cuando los recursos pueden asignarse en diferentes momentos a diferentes agentes, cada agente debe presentar valoraciones para cada combinación de posibles horizontes temporales.",
        "Que cada agente m ∈ M ejecute su MDP dentro del intervalo de tiempo de llegada-salida τ ∈ [τa m, τd m].",
        "Por lo tanto, el agente m ejecutará un MDP con un horizonte temporal no mayor que Tm = τd m−τa m+1.",
        "Que bτ sea el horizonte temporal global para el problema, antes del cual todos los MDP de los agentes deben finalizar.",
        "Suponemos que τd m < bτ, ∀m ∈ M. El Sexto Congreso Internacional.",
        "En el problema de programación donde los agentes tienen requisitos de recursos estáticos dentro de sus MDPs de horizonte finito, los agentes proporcionan una valoración para cada conjunto de recursos para cada horizonte de tiempo posible (desde [1, Tm]) que puedan utilizar.",
        "Sea Ω el conjunto de recursos a ser asignados entre los agentes.",
        "Un agente recibirá como máximo un paquete de recursos para uno de los horizontes temporales.",
        "Que la variable ψ ∈ Ψm enumere todos los posibles pares de conjuntos de recursos y horizontes temporales para el agente m, por lo que hay 2|Ω| × Tm valores para ψ (el espacio de conjuntos es exponencial en el número de tipos de recursos |Ω|).",
        "El agente m debe proporcionar un valor vψ m para cada ψ, y el coordinador asignará como máximo un par (recurso, horizonte temporal) ψ a cada agente.",
        "Esta asignación se expresa como una variable indicadora zψ m ∈ {0, 1} que muestra si ψ está asignado al agente m. Para el tiempo τ y el recurso ω, la función nm(ψ, τ, ω) ∈ {0, 1} indica si el paquete en ψ utiliza el recurso ω en el tiempo τ (hacemos la suposición de que los agentes tienen requisitos de recursos binarios).",
        "Este problema de asignación es NP-completo, incluso al considerar solo un paso de tiempo, y su dificultad aumenta significativamente con múltiples pasos de tiempo debido al creciente número de valores de ψ.",
        "El problema de encontrar una asignación óptima que satisfaga la restricción global de que la cantidad de cada recurso ω asignada a todos los agentes no exceda la cantidad disponible bϕ(ω) se puede expresar como el siguiente programa entero: max X m∈M X ψ∈Ψm zψ mvψ m sujeto a: X ψ∈Ψm zψ m ≤ 1, ∀m ∈ M; X m∈M X ψ∈Ψm zψ mnm(ψ, τ, ω) ≤ bϕ(ω), ∀τ ∈ [1, bτ], ∀ω ∈ Ω; (3) La primera restricción en la ecuación 3 dice que ningún agente puede recibir más de un paquete, y la segunda restricción asegura que la asignación total del recurso ω no exceda, en ningún momento, el límite del recurso.",
        "Para el problema de programación en el que los agentes pueden reasignar dinámicamente recursos, cada agente debe especificar un valor para cada combinación de paquetes y pasos de tiempo dentro de su horizonte temporal.",
        "Que la variable ψ ∈ Ψm en este caso enumere todos los posibles conjuntos de recursos para los cuales a lo sumo un conjunto puede ser asignado al agente m en cada paso de tiempo.",
        "Por lo tanto, en este caso hay P t∈[1,Tm](2|Ω| )t ∼ 2|Ω|Tm posibilidades de combinaciones de recursos asignadas a diferentes intervalos de tiempo, para los Tm horizontes de tiempo diferentes.",
        "El mismo conjunto de ecuaciones (3) se puede utilizar para resolver este problema de programación dinámica, pero el programa entero es diferente debido a la diferencia en cómo se define ψ.",
        "En este caso, el número de valores de ψ es exponencial en el horizonte de planificación Tm de cada agente, lo que resulta en un programa mucho más grande.",
        "Este enfoque directo para resolver ambos problemas de programación requiere una enumeración y solución de 2|Ω| Tm (asignación estática) o P t∈[1,Tm] 2|Ω|t (reubicación dinámica) MDPs para cada agente, lo cual rápidamente se vuelve intratable con el crecimiento del número de recursos |Ω| o el horizonte de tiempo Tm.",
        "MODELO Y DECLARACIÓN DEL PROBLEMA Ahora presentamos formalmente nuestro modelo del problema de programación de recursos.",
        "La entrada del problema consiste en los siguientes componentes: • M, Ω, bϕ, τa m, τd m, bτ están definidos arriba en la Sección 2.2. • {Θm} = {S, A, pm, rm, αm} son los MDPs de todos los agentes m ∈ M. Sin pérdida de generalidad, asumimos que los espacios de estados y acciones de todos los agentes son iguales, pero cada uno tiene su propia función de transición pm, función de recompensa rm, y condiciones iniciales αm. • ϕm : A×Ω → {0, 1} es la asignación de acciones a recursos para el agente m. ϕm(a, ω) indica si la acción a del agente m necesita el recurso ω.",
        "Un agente m que recibe un conjunto de recursos que no incluye el recurso ω no puede ejecutar en su política MDP ninguna acción a para la cual ϕm(a, ω) = 0.",
        "Suponemos que todos los requisitos de recursos son binarios; como se discute más adelante en la Sección 6, esta suposición no es limitante.",
        "Dado el input anterior, el problema de optimización que consideramos es encontrar la asignación globalmente óptima que maximice la suma de recompensas esperadas de recursos a agentes para todos los pasos de tiempo: Δ : τ × M × Ω → {0, 1}.",
        "Una solución es factible si la asignación correspondiente de recursos a los agentes no viola la restricción global de recursos: X m Δm(τ, ω) ≤ bϕ(ω), ∀ω ∈ Ω, τ ∈ [1, bτ]. Consideramos dos variantes del problema de programación de recursos.",
        "La primera formulación restringe las asignaciones de recursos al espacio donde la asignación de recursos a cada agente es estática durante la vida de los agentes.",
        "La segunda formulación permite la reasignación de recursos entre agentes en cada paso de tiempo durante sus vidas.",
        "La Figura 1 representa un problema de programación de recursos con tres agentes M = {m1, m2, m3}, tres recursos Ω = {ω1, ω2, ω3}, y un horizonte de problema global de bτ = 11.",
        "Los tiempos de llegada y salida de los agentes se muestran como cajas grises y son {1, 6}, {3, 7} y {2, 11}, respectivamente.",
        "Una solución a este problema se muestra a través de barras horizontales dentro de cada casilla de agentes, donde las barras corresponden a la asignación de los tres tipos de recursos.",
        "La Figura 1a muestra una solución a un problema de programación estática.",
        "Según la solución mostrada, el agente m1 comienza la ejecución de su MDP en el tiempo τ = 1 y tiene un bloqueo en los tres recursos hasta que finaliza la ejecución en el tiempo τ = 3.",
        "Se debe tener en cuenta que el agente m1 renuncia a su control sobre los recursos antes de su hora de salida anunciada de τd m1 = 6, aparentemente porque otros agentes pueden utilizar los recursos de manera más efectiva.",
        "Por lo tanto, en el tiempo τ = 4, los recursos ω1 y ω3 se asignan al agente m2, quien luego los utiliza para ejecutar su MDP (utilizando solo acciones admitidas por los recursos ω1 y ω3) hasta el tiempo τ = 7.",
        "El agente m3 posee el recurso ω3 durante el intervalo τ ∈ [4, 10].",
        "La Figura 1b muestra una posible solución a la versión dinámica del mismo problema.",
        "Allí, los recursos pueden ser reasignados entre agentes en cada paso de tiempo.",
        "Por ejemplo, el agente m1 renuncia a su uso del recurso ω2 en el tiempo τ = 2, aunque continúa la ejecución de su MDP hasta el tiempo τ = 6.",
        "Ten en cuenta que a un agente no se le permite detener y reiniciar su MDP, por lo que el agente m1 solo puede continuar ejecutando en el intervalo τ ∈ [3, 4] si tiene acciones que no requieren recursos (ϕm(a, ω) = 0).",
        "Claramente, el modelo y la declaración del problema descritos anteriormente hacen una serie de suposiciones sobre el problema y las propiedades de la solución deseada.",
        "Discutimos algunas de esas suposiciones y sus implicaciones en la Sección 6. 1222 La Sexta Internacional.",
        "Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) (a) (b) Figura 1: Ilustración de una solución a un problema de programación de recursos con tres agentes y tres recursos: a) asignaciones de recursos estáticas (las asignaciones de recursos son constantes durante la vida de los agentes); b) asignación dinámica (las asignaciones de recursos pueden cambiar en cada paso de tiempo). 4.",
        "PROGRAMACIÓN DE RECURSOS Nuestro algoritmo de programación de recursos se lleva a cabo en dos etapas.",
        "Primero, realizamos un paso de preprocesamiento que aumenta los MDP de los agentes; este proceso se describe en la Sección 4.1.",
        "Segundo, utilizando estos MDPs aumentados construimos un problema de optimización global, el cual se describe en la Sección 4.2. 4.1 Aumentando los MDPs de los Agentes En el modelo descrito en la sección anterior, asumimos que si un agente no posee los recursos necesarios para realizar acciones en su MDP, su ejecución se detiene y el agente abandona el sistema.",
        "En otras palabras, los MDPs no pueden ser pausados y reanudados.",
        "Por ejemplo, en el problema mostrado en la Figura 1a, el agente m1 libera todos los recursos después del tiempo τ = 3, momento en el cual la ejecución de su MDP se detiene.",
        "De manera similar, los agentes m2 y m3 solo ejecutan sus MDPs en los intervalos τ ∈ [4, 6] y τ ∈ [4, 10], respectivamente.",
        "Por lo tanto, una parte importante del problema de toma de decisiones global es decidir la ventana de tiempo durante la cual cada uno de los agentes está activo (es decir, ejecutando su MDP).",
        "Para lograr esto, ampliamos el MDP de cada agente con dos nuevos estados (estados de inicio y final sb, sf, respectivamente) y una nueva acción de inicio/parada a∗, como se ilustra en la Figura 2.",
        "La idea es que un agente permanezca en el estado inicial sb hasta que esté listo para ejecutar su MDP, momento en el que realiza la acción de inicio/parada a∗ y transita al espacio de estados del MDP original con la probabilidad de transición que corresponde a la distribución inicial original α(s).",
        "Por ejemplo, en la Figura 1a, para el agente m2 esto ocurriría en el tiempo τ = 4.",
        "Una vez que el agente llega al final de su ventana de actividad (tiempo τ = 6 para el agente m2 en la Figura 1a), realiza la acción de inicio/parada, lo que lo lleva al estado final de finalización del sumidero sf en el tiempo τ = 7.",
        "Más precisamente, dado un MDP S, A, pm, rm, αm, definimos un MDP aumentado S, A, pm, rm, αm de la siguiente manera: S = S ∪ sb ∪ sf; A = A ∪ a∗; p(s|sb, a∗) = α(s), ∀s ∈ S; p(sb|sb, a) = 1.0, ∀a ∈ A; p(sf|s, a∗) = 1.0, ∀s ∈ S; p(σ|s, a) = p(σ|s, a), ∀s, σ ∈ S, a ∈ A; r(sb, a) = r(sf, a) = 0, ∀a ∈ A; r(s, a) = r(s, a), ∀s ∈ S, a ∈ A; α(sb) = 1; α(s) = 0, ∀s ∈ S; donde se asume que todas las probabilidades de transición no especificadas son cero.",
        "Además, para tener en cuenta el nuevo estado inicial, comenzamos el MDP un paso de tiempo antes, estableciendo τa m ← τa m − 1.",
        "Esto no afectará la asignación de recursos debido a que las restricciones de recursos solo se aplican a los estados originales del MDP, como se discutirá en la siguiente sección.",
        "Por ejemplo, los MDPs aumentados mostrados en la Figura 2b (que comienza en el estado sb en el tiempo τ = 2) se construirían a partir de un MDP con tiempo de llegada original τ = 3.",
        "La Figura 2b también muestra una trayectoria de muestra a través del espacio de estados: el agente comienza en el estado sb, transita al espacio de estados S del MDP original y finalmente sale al estado de sumidero sf.",
        "Cabe destacar que si quisiéramos modelar un problema en el que los agentes pudieran pausar sus MDP en pasos de tiempo arbitrarios (lo cual podría ser útil para dominios donde la reasignación dinámica es posible), podríamos lograrlo fácilmente incluyendo una acción adicional que transiciona de cada estado a sí mismo con una recompensa de cero. 4.2 MILP para la programación de recursos Dado un conjunto de MDPs aumentados, como se define arriba, el objetivo de esta sección es formular un programa de optimización global que resuelva el problema de programación de recursos.",
        "En esta sección y a continuación, se asume que todos los MDPs son los MDPs aumentados tal como se definen en la Sección 4.1.",
        "Nuestro enfoque es similar a la idea utilizada en [6]: comenzamos con la formulación del programa lineal de los MDP de los agentes (1) y lo complementamos con restricciones que aseguran que la asignación de recursos correspondiente entre agentes y tiempo sea válida.",
        "El problema de optimización resultante resuelve simultáneamente los MDP de los agentes y los problemas de programación de recursos.",
        "En el resto de esta sección, desarrollamos de forma incremental un programa de enteros mixtos (MILP) que logra esto.",
        "En ausencia de restricciones de recursos, los MDPs de horizonte finito de los agentes son completamente independientes, y la solución óptima global puede obtenerse trivialmente a través del siguiente LP, que es simplemente una agregación de LPs de horizonte finito de un solo agente: max X m X s X a rm(s, a) X t xm(s, a, t) sujeto a: X a xm(σ, a, t + 1) = X s,a pm(σ|s, a)xm(s, a, t), ∀m ∈ M, σ ∈ S, t ∈ [1, Tm − 1]; X a xm(s, a, 1) = αm(s), ∀m ∈ M, s ∈ S; (12) donde xm(s, a, t) es la medida de ocupación del agente m, y The Sixth Intl.",
        "Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 1223 (a) (b) Figura 2: Ilustración de la ampliación de un MDP para permitir tiempos de inicio y finalización variables: a) (izquierda) el MDP original de dos estados con una sola acción; (derecha) el MDP ampliado con nuevos estados sb y sf y la nueva acción a∗ (nota que las transiciones originales no se modifican en el proceso de ampliación); b) el MDP ampl augmentado mostrado como una trayectoria a través del tiempo (las líneas grises indican todas las transiciones, mientras que las líneas negras indican una trayectoria dada).",
        "Función objetivo (suma de recompensas esperadas de todos los agentes) máx X m X s X a rm(s, a) X t xm(s, a, t) (5) Implicación de significado Restricciones lineales Atar x a θ.",
        "El agente solo está activo cuando la medida de ocupación es distinta de cero en los estados originales del MDP. θm(τ) = 0 =⇒ xm(s, a, τ −τa m+1) = 0 ∀s /∈ {sb , sf }, a ∈ A X s/∈{sb,sf } X a xm(s, a, t) ≤ θm(τa m + t − 1) ∀m ∈ M, ∀t ∈ [1, Tm] (6) El agente solo puede estar activo en τ ∈ (τa m, τd m) θm(τ) = 0 ∀m ∈ M, τ /∈ (τa m, τd m) (7) No se pueden utilizar recursos cuando no está activo θm(τ) = 0 =⇒ Δm(τ, ω) = 0 ∀τ ∈ [0, bτ], ω ∈ Ω Δm(τ, ω) ≤ θm(τ) ∀m ∈ M, τ ∈ [0, bτ], ω ∈ Ω (8) Vincular x a Δ (x distinto de cero obliga a que el correspondiente Δ sea distinto de cero).",
        "Δm(τ, ω) = 0, ϕm(a, ω) = 1 =⇒ xm(s, a, τ − τa m + 1) = 0 ∀s /∈ {sb , sf } 1/|A| X a ϕm(a, ω) X s/∈{sb,sf } xm(s, a, t) ≤ Δm(t + τa m − 1, ω) ∀m ∈ M, ω ∈ Ω, t ∈ [1, Tm] (9) Límites de recursos X m Δm(τ, ω) ≤ bϕ(ω) ∀ω ∈ Ω, τ ∈ [0, bτ] (10) El agente no puede cambiar los recursos mientras está activo.",
        "Solo habilitado para programación con asignaciones estáticas. θm(τ) = 1 y θm(τ + 1) = 1 =⇒ Δm(τ, ω) = Δm(τ + 1, ω) Δm(τ, ω) − Z(1 − θm(τ + 1)) ≤ Δm(τ + 1, ω) + Z(1 − θm(τ)) Δm(τ, ω) + Z(1 − θm(τ + 1)) ≥ Δm(τ + 1, ω) − Z(1 − θm(τ)) ∀m ∈ M, ω ∈ Ω, τ ∈ [0, bτ] (11) Tabla 1: MILP para programación óptima global de recursos.",
        "Tm = τd m − τa m + 1 es el horizonte temporal para los agentes MDP.",
        "Usando este LP como base, lo ampliamos con restricciones que aseguran que el uso de recursos implicado por las medidas de ocupación de los agentes {xm} no viola los requisitos globales de recursos bϕ en ningún paso de tiempo τ ∈ [0, bτ].",
        "Para formular estas restricciones de recursos, utilizamos las siguientes variables binarias: • Δm(τ, ω) = {0, 1}, ∀m ∈ M, τ ∈ [0, bτ], ω ∈ Ω, que sirven como variables indicadoras que definen si el agente m posee el recurso ω en el tiempo τ.",
        "Estos son análogos a las variables indicadoras estáticas utilizadas en el problema de asignación de recursos estáticos de una sola vez en [6]. • θm = {0, 1}, ∀m ∈ M, τ ∈ [0, bτ] son variables indicadoras que especifican si el agente m está activo (es decir, ejecutando su MDP) en el tiempo τ.",
        "El significado de las variables de uso de recursos Δ se ilustra en la Figura 1: Δm(τ, ω) = 1 solo si el recurso ω se asigna al agente m en el tiempo τ.",
        "El significado de los indicadores de actividad θ se ilustra en la Figura 2b: cuando el agente m se encuentra en el estado inicial sb o en el estado final sf, el θm correspondiente es igual a 0, pero una vez que el agente se vuelve activo y entra en uno de los otros estados, establecemos θm = 1.",
        "Este significado de θ puede ser impuesto con una restricción lineal que sincroniza los valores de las medidas de ocupación de los agentes xm y la actividad 1224 The Sixth Intl.",
        "Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) indicadores θ, como se muestra en (6) en la Tabla 1.",
        "Otra restricción que debemos agregar, debido a que los indicadores de actividad θ están definidos en la línea de tiempo global τ, es hacer cumplir el hecho de que el agente está inactivo fuera de su ventana de llegada y salida.",
        "Esto se logra mediante la restricción (7) en la Tabla 1.",
        "Además, los agentes no deben estar utilizando recursos mientras están inactivos.",
        "Esta restricción también puede ser impuesta a través de una desigualdad lineal en θ y Δ, como se muestra en (8).",
        "La restricción (6) establece el valor de θ para que coincida con la política definida por la medida de ocupación xm.",
        "De manera similar, debemos asegurarnos de que las variables de uso de recursos Δ estén también sincronizadas con la medida de ocupación xm.",
        "Esto se realiza a través de la restricción (9) en la Tabla 1, que es casi idéntica a la restricción análoga de [6].",
        "Después de implementar la restricción anterior, que refuerza el significado de Δ, agregamos una restricción que garantiza que el uso de recursos de los agentes nunca exceda los montos de recursos disponibles.",
        "Esta condición también se expresa trivialmente como una desigualdad lineal (10) en la Tabla 1.",
        "Finalmente, para la formulación del problema en la que las asignaciones de recursos son estáticas durante la vida de un agente, agregamos una restricción que garantiza que las variables de uso de recursos Δ no cambien su valor mientras el agente está activo (θ = 1).",
        "Esto se logra a través de la restricción lineal (11), donde Z ≥ 2 es una constante que se utiliza para desactivar las restricciones cuando θm(τ) = 0 o θm(τ + 1) = 0.",
        "Esta restricción no se utiliza para la formulación del problema dinámico, donde los recursos pueden ser reasignados entre agentes en cada paso de tiempo.",
        "En resumen, la Tabla 1 junto con las restricciones de conservación de flujo de (12) define el MILP que calcula simultáneamente una asignación óptima de recursos para todos los agentes a lo largo del tiempo, así como políticas óptimas de MDP de horizonte finito que son válidas bajo esa asignación de recursos.",
        "Como una medida aproximada de la complejidad de este MILP, consideremos el número de variables de optimización y restricciones.",
        "Sea TM = P Tm = P m(τa m − τd m + 1) la suma de las longitudes de las ventanas de llegada-salida a través de todos los agentes.",
        "Entonces, el número de variables de optimización es: TM + bτ|M||Ω| + bτ|M|, de las cuales TM son continuas (xm), y bτ|M||Ω| + bτ|M| son binarias (Δ y θ).",
        "Sin embargo, hay que tener en cuenta que todos menos TM|M| de los θ se establecen en cero por la restricción (7), lo que también obliga inmediatamente a que todos menos TM|M||Ω| de los Δ sean cero a través de las restricciones (8).",
        "El número de restricciones (sin incluir las restricciones degeneradas en (7)) en el MILP es: TM + TM|Ω| + bτ|Ω| + bτ|M||Ω|.",
        "A pesar de que la complejidad del MILP es, en el peor de los casos, exponencial en el número de variables binarias, la complejidad de este MILP es significativamente (exponencialmente) menor que la del MILP con funciones de utilidad planas, descrito en la Sección 2.2.",
        "Este resultado refleja las ganancias de eficiencia reportadas en [6] para problemas de asignación de recursos de un solo disparo, pero es mucho más pronunciado, debido a la explosión de la representación de utilidad plana debido al aspecto temporal del problema (recuerde la complejidad prohibitiva de la optimización combinatoria en la Sección 2.2).",
        "Analizamos empíricamente el rendimiento de este método en la Sección 5.1. Estrictamente hablando, resolver MILPs hasta la optimalidad es NP-completo en el número de variables enteras.",
        "RESULTADOS EXPERIMENTALES Aunque la complejidad de resolver MILPs es en el peor de los casos exponencial en el número de variables enteras, existen muchos métodos eficientes para resolver MILPs que permiten que nuestro algoritmo se adapte bien a parámetros comunes en problemas de asignación de recursos y programación.",
        "En particular, esta sección introduce un dominio de problema: el problema del taller de reparación, que se utiliza para evaluar empíricamente la escalabilidad de nuestros algoritmos en términos del número de agentes |M|, el número de recursos compartidos |Ω| y las diferentes longitudes de tiempo global bτ durante las cuales los agentes pueden entrar y salir del sistema.",
        "El problema del taller de reparaciones es un MDP parametrizado simple que adopta la metáfora de un taller de reparaciones de vehículos.",
        "Los agentes en el taller de reparación son mecánicos con una serie de tareas independientes que solo generan recompensa cuando se completan.",
        "En nuestro modelo MDP de este sistema, las acciones tomadas para avanzar a través del espacio de estados solo están permitidas si el agente posee ciertos recursos que están públicamente disponibles para la tienda.",
        "Estos recursos son de suministro finito, y las políticas óptimas para la tienda determinarán cuándo cada agente puede retener los recursos limitados para tomar acciones y obtener recompensas individuales.",
        "Cada tarea a completar está asociada con una sola acción, aunque se requiere que el agente repita la acción numerosas veces antes de completar la tarea y obtener una recompensa.",
        "Este modelo fue parametrizado en términos del número de agentes en el sistema, el número de diferentes tipos de recursos que podrían estar vinculados a acciones necesarias, un tiempo global durante el cual se permite a los agentes llegar y partir, y una longitud máxima para el número de pasos de tiempo que un agente puede permanecer en el sistema.",
        "Todos los puntos de datos en nuestros experimentos se obtuvieron con 20 evaluaciones utilizando CPLEX para resolver los MILPs en una computadora Pentium4 con 2Gb de RAM.",
        "Se realizaron pruebas tanto en la versión estática como en la dinámica del problema de programación de recursos, tal como se definió anteriormente.",
        "La Figura 3 muestra el tiempo de ejecución y el valor de la política para modificaciones independientes en el conjunto de parámetros.",
        "La fila superior muestra cómo el tiempo de solución para el MILP escala a medida que aumentamos el número de agentes |M|, el horizonte de tiempo global bτ y el número de recursos |Ω|.",
        "Aumentar el número de agentes conduce a una escalabilidad exponencial en complejidad, lo cual es de esperarse para un problema NP-completo.",
        "Sin embargo, aumentar el límite de tiempo global bτ o el número total de tipos de recursos |Ω|, manteniendo constante el número de agentes, no conduce a una disminución en el rendimiento.",
        "Esto ocurre porque los problemas se vuelven más fáciles a medida que se vuelven sub-determinados, lo cual también es un fenómeno común en problemas NP-completos.",
        "También observamos que la solución a la versión dinámica del problema a menudo se puede calcular mucho más rápido que la versión estática.",
        "La fila inferior de la Figura 3 muestra el valor de la política conjunta de las políticas que corresponden a los horarios óptimos de asignación de recursos calculados.",
        "Podemos observar que la versión dinámica produce una recompensa más alta (como era de esperar, ya que la recompensa de la versión dinámica siempre es igual o mayor que la recompensa de la versión estática).",
        "Deberíamos señalar que estos gráficos no deben ser vistos como una medida del rendimiento de dos algoritmos diferentes (ambos algoritmos producen soluciones óptimas pero para problemas distintos), sino más bien como observaciones sobre cómo cambia la calidad de las soluciones óptimas a medida que se permite más flexibilidad en la reasignación de recursos.",
        "La Figura 4 muestra el tiempo de ejecución y el valor de la política para las pruebas en las que las variables de entrada comunes se escalan juntas.",
        "Esto permite la Sexta Internacional.",
        "Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 1225 2 4 6 8 10 10 −3 10 −2 10 −1 10 0 10 1 10 2 10 3 10 4 Número de Agentes |M| Tiempo de CPU, seg |Ω| = 5, τ = 50 estático dinámico 50 100 150 200 10 −2 10 −1 10 0 10 1 10 2 10 3 Límite de Tiempo Global τ Tiempo de CPU, seg |M| = 5, |Ω| = 5 estático dinámico 10 20 30 40 50 10 −2 10 −1 10 0 10 1 10 2 Número de Recursos |Ω| Tiempo de CPU, seg |M| = 5, τ = 50 estático dinámico 2 4 6 8 10 200 400 600 800 1000 1200 1400 1600 Número de Agentes |M| Valor |Ω| = 5, τ = 50 estático dinámico 50 100 150 200 400 500 600 700 800 900 1000 1100 1200 1300 1400 Límite de Tiempo Global τ Valor |M| = 5, |Ω| = 5 estático dinámico 10 20 30 40 50 500 600 700 800 900 1000 1100 1200 1300 1400 Número de Recursos |Ω| Valor |M| = 5, τ = 50 estático dinámico Figura 3: Evaluación de nuestro MILP para números variables de agentes (columna 1), longitudes de ventana de tiempo global (columna 2) y números de tipos de recursos (columna 3).",
        "La fila superior muestra el tiempo de CPU, y la fila inferior muestra la recompensa conjunta de las políticas MDP de los agentes.",
        "Las barras de error muestran los cuartiles 1ro y 3ro (25% y 75%). 2 4 6 8 10 10 −3 10 −2 10 −1 10 0 10 1 10 2 10 3 Número de Agentes |M| Tiempo de CPU, seg τ = 10|M| estático dinámico 2 4 6 8 10 10 −3 10 −2 10 −1 10 0 10 1 10 2 10 3 10 4 Número de Agentes |M| Tiempo de CPU, seg |Ω| = 2|M| estático dinámico 2 4 6 8 10 10 −3 10 −2 10 −1 10 0 10 1 10 2 10 3 10 4 Número de Agentes |M| Tiempo de CPU, seg |Ω| = 5|M| estático dinámico 2 4 6 8 10 200 400 600 800 1000 1200 1400 1600 1800 2000 2200 Número de Agentes |M| Valor τ = 10|M| estático dinámico 2 4 6 8 10 200 400 600 800 1000 1200 1400 1600 1800 2000 Número de Agentes |M| Valor |Ω| = 2|M| estático dinámico 2 4 6 8 10 0 500 1000 1500 2000 2500 Número de Agentes |M| Valor |Ω| = 5|M| estático dinámico Figura 4: Evaluación de nuestro MILP utilizando variables de entrada correlacionadas.",
        "La columna izquierda sigue el rendimiento y el tiempo de CPU a medida que aumenta el número de agentes y la ventana de tiempo global juntos (bτ = 10|M|).",
        "La columna del medio y la columna de la derecha siguen el rendimiento y el tiempo de CPU a medida que aumenta el número de recursos y el número de agentes juntos, con |Ω| = 2|M| y |Ω| = 5|M|, respectivamente.",
        "Las barras de error muestran el primer y tercer cuartil (25% y 75%). 1226 The Sixth Intl.",
        "La Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) nos permite explorar dominios donde el número total de agentes escala proporcionalmente al número total de tipos de recursos o al horizonte temporal global, manteniendo constante la densidad promedio de agentes (por unidad de tiempo global) o el número promedio de recursos por agente (lo cual comúnmente ocurre en aplicaciones de la vida real).",
        "En general, creemos que estos resultados experimentales indican que nuestra formulación MILP puede ser utilizada para resolver de manera efectiva problemas de programación de recursos de tamaño no trivial.",
        "DISCUSIÓN Y CONCLUSIONES A lo largo del documento, hemos realizado una serie de suposiciones en nuestro modelo y algoritmo de solución; discutimos sus implicaciones a continuación. • Ejecución continua.",
        "Suponemos que una vez que un agente deja de ejecutar su MDP (transiciona al estado sf), sale del sistema y no puede regresar.",
        "Es fácil relajar esta suposición para dominios donde los agentes de MDP pueden ser pausados y reiniciados.",
        "Todo lo que se requiere es incluir una acción de pausa adicional que haga la transición desde un estado dado de regreso a sí mismo, y que tenga una recompensa de cero. • Indiferencia al tiempo de inicio.",
        "Utilizamos un modelo de recompensa donde las recompensas de los agentes dependen solo del horizonte temporal de sus MDP y no del tiempo de inicio global.",
        "Esta es una consecuencia de nuestro procedimiento de aumento de MDP de la Sección 4.1.",
        "Es fácil extender el modelo para que los agentes incurran en una penalización explícita por inactividad al asignar una recompensa negativa no nula al estado inicial sb. • Requisitos de recursos binarios.",
        "Para simplificar, hemos asumido que los costos de recursos son binarios: ϕm(a, ω) = {0, 1}, pero nuestros resultados se generalizan de manera directa a asignaciones de recursos no binarias, de manera análoga al procedimiento utilizado en [5]. • Agentes cooperativos.",
        "El procedimiento de optimización discutido en este artículo fue desarrollado en el contexto de agentes cooperativos, pero también puede ser utilizado para diseñar un mecanismo para programar recursos entre agentes egoístas.",
        "Este procedimiento de optimización puede ser incorporado en una subasta de Vickrey-Clarke-Groves, de manera completamente análoga a como se hizo en [7].",
        "De hecho, todos los resultados de [7] sobre las propiedades de la subasta y la privacidad de la información se aplican directamente al dominio de programación discutido en este documento, requiriendo solo ligeras modificaciones para tratar con MDPs de horizonte finito. • Tiempos de llegada y salida conocidos y deterministas.",
        "Finalmente, hemos asumido que los tiempos de llegada y salida de los agentes (τa m y τd m) son deterministas y conocidos de antemano.",
        "Esta suposición es fundamental para nuestro método de solución.",
        "Si bien hay muchos dominios donde esta suposición es válida, en muchos casos los agentes llegan y se van de forma dinámica y sus tiempos de llegada y salida solo pueden predecirse de manera probabilística, lo que lleva a problemas de asignación de recursos en línea.",
        "En particular, en el caso de agentes con interés propio, esto se convierte en una versión interesante de un problema de diseño de mecanismos en línea [11, 12].",
        "En resumen, hemos presentado una formulación MILP para el problema combinatorio de programación de recursos donde los valores de los agentes para posibles asignaciones de recursos están definidos por MDPs de horizonte finito.",
        "Este resultado amplía el trabajo previo ([6, 7]) sobre asignación estática de recursos de una sola vez bajo preferencias inducidas por MDP a problemas de programación de recursos con un aspecto temporal.",
        "Por lo tanto, este trabajo da un paso en la dirección de diseñar un mecanismo en línea para agentes con preferencias de recursos combinatorios inducidas por problemas de planificación estocástica.",
        "Relajar la suposición sobre los tiempos deterministas de llegada y salida de los agentes es un enfoque de nuestro trabajo futuro.",
        "Nos gustaría agradecer a los revisores anónimos por sus comentarios y sugerencias perspicaces. 7.",
        "REFERENCIAS [1] E. Altman y A. Shwartz.",
        "Control adaptativo de cadenas de Markov restringidas: Criterios y políticas.",
        "Anales de Investigación de Operaciones, número especial sobre Procesos de Decisión de Markov, 28:101-134, 1991. [2] R. Bellman.",
        "Programación dinámica.",
        "Princeton University Press, 1957. [3] C. Boutilier. \n\nPrensa de la Universidad de Princeton, 1957. [3] C. Boutilier.",
        "Resolviendo problemas de subasta combinatoria expresados de manera concisa.",
        "En Proc. de AAAI-02, páginas 359-366, 2002. [4] C. Boutilier y H. H. Hoos.",
        "Subastando idiomas para subastas combinatorias.",
        "En Proc. de IJCAI-01, páginas 1211-1217, 2001. [5] D. Dolgov.",
        "Asignación integrada de recursos y planificación en entornos estocásticos de múltiples agentes.",
        "Tesis doctoral, Departamento de Ciencias de la Computación, Universidad de Michigan, febrero de 2006. [6] D. A. Dolgov y E. H. Durfee.",
        "Asignación óptima de recursos y formulación de políticas en procesos de decisión de Markov débilmente acoplados.",
        "En Proc. de ICAPS-04, páginas 315-324, junio de 2004. [7] D. A. Dolgov y E. H. Durfee.",
        "Subastas combinatorias computacionalmente eficientes para asignación de recursos en MDPs débilmente acoplados.",
        "En Proc. de AAMAS-05, Nueva York, NY, EE. UU., 2005.",
        "ACM Press. [8] D. A. Dolgov y E. H. Durfee.",
        "Asignación de recursos entre agentes con preferencias inducidas por MDP factorizados.",
        "En Proc. de AAMAS-06, 2006. [9] K. Larson y T. Sandholm.",
        "Diseño de mecanismos y agentes deliberativos.",
        "En Proc. de AAMAS-05, páginas 650-656, Nueva York, NY, EE. UU., 2005.",
        "ACM Press. [10] N. Nisan. \n\nACM Press. [10] N. Nisan.",
        "Subasta y asignación en subastas combinatorias.",
        "En Comercio Electrónico, 2000. [11] D. C. Parkes y S. Singh.",
        "Un enfoque basado en MDP para el Diseño de Mecanismos en Línea.",
        "En Proc. de la Decimoséptima Conferencia Anual sobre Sistemas de Procesamiento de Información Neural (NIPS-03), 2003. [12] D. C. Parkes, S. Singh y D. Yanovsky.",
        "Diseño de mecanismos en línea aproximadamente eficiente.",
        "En Actas de la Decimoctava Conferencia Anual sobre Sistemas de Procesamiento de Información Neural (NIPS-04), 2004. [13] M. L. Puterman.",
        "Procesos de Decisión de Markov.",
        "John Wiley & Sons, Nueva York, 1994. [14] M. H. Rothkopf, A. Pekec y R. M. Harstad.",
        "Subastas combinatorias manejables computacionalmente.",
        "Ciencia de la Gestión, 44(8):1131-1147, 1998. [15] T. Sandholm.",
        "Un algoritmo para la determinación óptima del ganador en subastas combinatorias.",
        "En Proc. de IJCAI-99, páginas 542-547, San Francisco, CA, EE. UU., 1999.",
        "Morgan Kaufmann Publishers Inc. - Editores Morgan Kaufmann Inc.",
        "El Sexto Internacional.",
        "Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 1227"
    ],
    "error_count": 7,
    "keys": {
        "combinatorial resource scheduling": {
            "translated_key": "",
            "is_in_text": true,
            "original_annotated_sentences": [
                "<br>combinatorial resource scheduling</br> for Multiagent MDPs Dmitri A. Dolgov, Michael R. James, and Michael E. Samples AI and Robotics Group Technical Research, Toyota Technical Center USA {ddolgov, michael.r.james, michael.samples}@gmail.com ABSTRACT Optimal resource scheduling in multiagent systems is a computationally challenging task, particularly when the values of resources are not additive.",
                "We consider the combinatorial problem of scheduling the usage of multiple resources among agents that operate in stochastic environments, modeled as Markov decision processes (MDPs).",
                "In recent years, efficient resource-allocation algorithms have been developed for agents with resource values induced by MDPs.",
                "However, this prior work has focused on static resource-allocation problems where resources are distributed once and then utilized in infinite-horizon MDPs.",
                "We extend those existing models to the problem of <br>combinatorial resource scheduling</br>, where agents persist only for finite periods between their (predefined) arrival and departure times, requiring resources only for those time periods.",
                "We provide a computationally efficient procedure for computing globally optimal resource assignments to agents over time.",
                "We illustrate and empirically analyze the method in the context of a stochastic jobscheduling domain.",
                "Categories and Subject Descriptors I.2.8 [Artificial Intelligence]: Problem Solving, Control Methods, and Search; I.2.11 [Artificial Intelligence]: Distributed Artificial Intelligence-Multiagent systems General Terms Algorithms, Performance, Design 1.",
                "INTRODUCTION The tasks of optimal resource allocation and scheduling are ubiquitous in multiagent systems, but solving such optimization problems can be computationally difficult, due to a number of factors.",
                "In particular, when the value of a set of resources to an agent is not additive (as is often the case with resources that are substitutes or complements), the utility function might have to be defined on an exponentially large space of resource bundles, which very quickly becomes computationally intractable.",
                "Further, even when each agent has a utility function that is nonzero only on a small subset of the possible resource bundles, obtaining optimal allocation is still computationally prohibitive, as the problem becomes NP-complete [14].",
                "Such computational issues have recently spawned several threads of work in using compact models of agents preferences.",
                "One idea is to use any structure present in utility functions to represent them compactly, via, for example, logical formulas [15, 10, 4, 3].",
                "An alternative is to directly model the mechanisms that define the agents utility functions and perform resource allocation directly with these models [9].",
                "A way of accomplishing this is to model the processes by which an agent might utilize the resources and define the utility function as the payoff of these processes.",
                "In particular, if an agent uses resources to act in a stochastic environment, its utility function can be naturally modeled with a Markov decision process, whose action set is parameterized by the available resources.",
                "This representation can then be used to construct very efficient resource-allocation algorithms that lead to an exponential speedup over a straightforward optimization problem with flat representations of combinatorial preferences [6, 7, 8].",
                "However, this existing work on resource allocation with preferences induced by resource-parameterized MDPs makes an assumption that the resources are only allocated once and are then utilized by the agents independently within their infinite-horizon MDPs.",
                "This assumption that no reallocation of resources is possible can be limiting in domains where agents arrive and depart dynamically.",
                "In this paper, we extend the work on resource allocation under MDP-induced preferences to discrete-time scheduling problems, where agents are present in the system for finite time intervals and can only use resources within these intervals.",
                "In particular, agents arrive and depart at arbitrary (predefined) times and within these intervals use resources to execute tasks in finite-horizon MDPs.",
                "We address the problem of globally optimal resource scheduling, where the objective is to find an allocation of resources to the agents across time that maximizes the sum of the expected rewards that they obtain.",
                "In this context, our main contribution is a mixed-integerprogramming formulation of the scheduling problem that chooses globally optimal resource assignments, starting times, and execution horizons for all agents (within their arrival1220 978-81-904262-7-5 (RPS) c 2007 IFAAMAS departure intervals).",
                "We analyze and empirically compare two flavors of the scheduling problem: one, where agents have static resource assignments within their finite-horizon MDPs, and another, where resources can be dynamically reallocated between agents at every time step.",
                "In the rest of the paper, we first lay down the necessary groundwork in Section 2 and then introduce our model and formal problem statement in Section 3.",
                "In Section 4.2, we describe our main result, the optimization program for globally optimal resource scheduling.",
                "Following the discussion of our experimental results on a job-scheduling problem in Section 5, we conclude in Section 6 with a discussion of possible extensions and generalizations of our method. 2.",
                "BACKGROUND Similarly to the model used in previous work on resourceallocation with MDP-induced preferences [6, 7], we define the value of a set of resources to an agent as the value of the best MDP policy that is realizable, given those resources.",
                "However, since the focus of our work is on scheduling problems, and a large part of the optimization problem is to decide how resources are allocated in time among agents with finite arrival and departure times, we model the agents planning problems as finite-horizon MDPs, in contrast to previous work that used infinite-horizon discounted MDPs.",
                "In the rest of this section, we first introduce some necessary background on finite-horizon MDPs and present a linear-programming formulation that serves as the basis for our solution algorithm developed in Section 4.",
                "We also outline the standard methods for <br>combinatorial resource scheduling</br> with flat resource values, which serve as a comparison benchmark for the new model developed here. 2.1 Markov Decision Processes A stationary, finite-domain, discrete-time MDP (see, for example, [13] for a thorough and detailed development) can be described as S, A, p, r , where: S is a finite set of system states; A is a finite set of actions that are available to the agent; p is a stationary stochastic transition function, where p(σ|s, a) is the probability of transitioning to state σ upon executing action a in state s; r is a stationary reward function, where r(s, a) specifies the reward obtained upon executing action a in state s. Given such an MDP, a decision problem under a finite horizon T is to choose an optimal action at every time step to maximize the expected value of the total reward accrued during the agents (finite) lifetime.",
                "The agents optimal policy is then a function of current state s and the time until the horizon.",
                "An optimal policy for such a problem is to act greedily with respect to the optimal value function, defined recursively by the following system of finite-time Bellman equations [2]: v(s, t) = max a r(s, a) + X σ p(σ|s, a)v(σ, t + 1), ∀s ∈ S, t ∈ [1, T − 1]; v(s, T) = 0, ∀s ∈ S; where v(s, t) is the optimal value of being in state s at time t ∈ [1, T].",
                "This optimal value function can be easily computed using dynamic programming, leading to the following optimal policy π, where π(s, a, t) is the probability of executing action a in state s at time t: π(s, a, t) = ( 1, a = argmaxa r(s, a) + P σ p(σ|s, a)v(σ, t + 1), 0, otherwise.",
                "The above is the most common way of computing the optimal value function (and therefore an optimal policy) for a finite-horizon MDP.",
                "However, we can also formulate the problem as the following linear program (similarly to the dual LP for infinite-horizon discounted MDPs [13, 6, 7]): max X s X a r(s, a) X t x(s, a, t) subject to: X a x(σ, a, t + 1) = X s,a p(σ|s, a)x(s, a, t) ∀σ, t ∈ [1, T − 1]; X a x(s, a, 1) = α(s), ∀s ∈ S; (1) where α(s) is the initial distribution over the state space, and x is the (non-stationary) occupation measure (x(s, a, t) ∈ [0, 1] is the total expected number of times action a is executed in state s at time t).",
                "An optimal (non-stationary) policy is obtained from the occupation measure as follows: π(s, a, t) = x(s, a, t)/ X a x(s, a, t) ∀s ∈ S, t ∈ [1, T]. (2) Note that the standard unconstrained finite-horizon MDP, as described above, always has a uniformly-optimal solution (optimal for any initial distribution α(s)).",
                "Therefore, an optimal policy can be obtained by using an arbitrary constant α(s) > 0 (in particular, α(s) = 1 will result in x(s, a, t) = π(s, a, t)).",
                "However, for MDPs with resource constraints (as defined below in Section 3), uniformly-optimal policies do not in general exist.",
                "In such cases, α becomes a part of the problem input, and a resulting policy is only optimal for that particular α.",
                "This result is well known for infinite-horizon MDPs with various types of constraints [1, 6], and it also holds for our finite-horizon model, which can be easily established via a line of reasoning completely analogous to the arguments in [6]. 2.2 <br>combinatorial resource scheduling</br> A straightforward approach to resource scheduling for a set of agents M, whose values for the resources are induced by stochastic planning problems (in our case, finite-horizon MDPs) would be to have each agent enumerate all possible resource assignments over time and, for each one, compute its value by solving the corresponding MDP.",
                "Then, each agent would provide valuations for each possible resource bundle over time to a centralized coordinator, who would compute the optimal resource assignments across time based on these valuations.",
                "When resources can be allocated at different times to different agents, each agent must submit valuations for every combination of possible time horizons.",
                "Let each agent m ∈ M execute its MDP within the arrival-departure time interval τ ∈ [τa m, τd m].",
                "Hence, agent m will execute an MDP with time horizon no greater than Tm = τd m−τa m+1.",
                "Let bτ be the global time horizon for the problem, before which all of the agents MDPs must finish.",
                "We assume τd m < bτ, ∀m ∈ M. The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1221 For the scheduling problem where agents have static resource requirements within their finite-horizon MDPs, the agents provide a valuation for each resource bundle for each possible time horizon (from [1, Tm]) that they may use.",
                "Let Ω be the set of resources to be allocated among the agents.",
                "An agent will get at most one resource bundle for one of the time horizons.",
                "Let the variable ψ ∈ Ψm enumerate all possible pairs of resource bundles and time horizons for agent m, so there are 2|Ω| × Tm values for ψ (the space of bundles is exponential in the number of resource types |Ω|).",
                "The agent m must provide a value vψ m for each ψ, and the coordinator will allocate at most one ψ (resource, time horizon) pair to each agent.",
                "This allocation is expressed as an indicator variable zψ m ∈ {0, 1} that shows whether ψ is assigned to agent m. For time τ and resource ω, the function nm(ψ, τ, ω) ∈ {0, 1} indicates whether the bundle in ψ uses resource ω at time τ (we make the assumption that agents have binary resource requirements).",
                "This allocation problem is NP-complete, even when considering only a single time step, and its difficulty increases significantly with multiple time steps because of the increasing number of values of ψ.",
                "The problem of finding an optimal allocation that satisfies the global constraint that the amount of each resource ω allocated to all agents does not exceed the available amount bϕ(ω) can be expressed as the following integer program: max X m∈M X ψ∈Ψm zψ mvψ m subject to: X ψ∈Ψm zψ m ≤ 1, ∀m ∈ M; X m∈M X ψ∈Ψm zψ mnm(ψ, τ, ω) ≤ bϕ(ω), ∀τ ∈ [1, bτ], ∀ω ∈ Ω; (3) The first constraint in equation 3 says that no agent can receive more than one bundle, and the second constraint ensures that the total assignment of resource ω does not, at any time, exceed the resource bound.",
                "For the scheduling problem where the agents are able to dynamically reallocate resources, each agent must specify a value for every combination of bundles and time steps within its time horizon.",
                "Let the variable ψ ∈ Ψm in this case enumerate all possible resource bundles for which at most one bundle may be assigned to agent m at each time step.",
                "Therefore, in this case there are P t∈[1,Tm](2|Ω| )t ∼ 2|Ω|Tm possibilities of resource bundles assigned to different time slots, for the Tm different time horizons.",
                "The same set of equations (3) can be used to solve this dynamic scheduling problem, but the integer program is different because of the difference in how ψ is defined.",
                "In this case, the number of ψ values is exponential in each agents planning horizon Tm, resulting in a much larger program.",
                "This straightforward approach to solving both of these scheduling problems requires an enumeration and solution of either 2|Ω| Tm (static allocation) or P t∈[1,Tm] 2|Ω|t (dynamic reallocation) MDPs for each agent, which very quickly becomes intractable with the growth of the number of resources |Ω| or the time horizon Tm. 3.",
                "MODEL AND PROBLEM STATEMENT We now formally introduce our model of the resourcescheduling problem.",
                "The problem input consists of the following components: • M, Ω, bϕ, τa m, τd m, bτ are as defined above in Section 2.2. • {Θm} = {S, A, pm, rm, αm} are the MDPs of all agents m ∈ M. Without loss of generality, we assume that state and action spaces of all agents are the same, but each has its own transition function pm, reward function rm, and initial conditions αm. • ϕm : A×Ω → {0, 1} is the mapping of actions to resources for agent m. ϕm(a, ω) indicates whether action a of agent m needs resource ω.",
                "An agent m that receives a set of resources that does not include resource ω cannot execute in its MDP policy any action a for which ϕm(a, ω) = 0.",
                "We assume all resource requirements are binary; as discussed below in Section 6, this assumption is not limiting.",
                "Given the above input, the optimization problem we consider is to find the globally optimal-maximizing the sum of expected rewards-mapping of resources to agents for all time steps: Δ : τ × M × Ω → {0, 1}.",
                "A solution is feasible if the corresponding assignment of resources to the agents does not violate the global resource constraint: X m Δm(τ, ω) ≤ bϕ(ω), ∀ω ∈ Ω, τ ∈ [1, bτ]. (4) We consider two flavors of the resource-scheduling problem.",
                "The first formulation restricts resource assignments to the space where the allocation of resources to each agent is static during the agents lifetime.",
                "The second formulation allows reassignment of resources between agents at every time step within their lifetimes.",
                "Figure 1 depicts a resource-scheduling problem with three agents M = {m1, m2, m3}, three resources Ω = {ω1, ω2, ω3}, and a global problem horizon of bτ = 11.",
                "The agents arrival and departure times are shown as gray boxes and are {1, 6}, {3, 7}, and {2, 11}, respectively.",
                "A solution to this problem is shown via horizontal bars within each agents box, where the bars correspond to the allocation of the three resource types.",
                "Figure 1a shows a solution to a static scheduling problem.",
                "According to the shown solution, agent m1 begins the execution of its MDP at time τ = 1 and has a lock on all three resources until it finishes execution at time τ = 3.",
                "Note that agent m1 relinquishes its hold on the resources before its announced departure time of τd m1 = 6, ostensibly because other agents can utilize the resources more effectively.",
                "Thus, at time τ = 4, resources ω1 and ω3 are allocated to agent m2, who then uses them to execute its MDP (using only actions supported by resources ω1 and ω3) until time τ = 7.",
                "Agent m3 holds resource ω3 during the interval τ ∈ [4, 10].",
                "Figure 1b shows a possible solution to the dynamic version of the same problem.",
                "There, resources can be reallocated between agents at every time step.",
                "For example, agent m1 gives up its use of resource ω2 at time τ = 2, although it continues the execution of its MDP until time τ = 6.",
                "Notice that an agent is not allowed to stop and restart its MDP, so agent m1 is only able to continue executing in the interval τ ∈ [3, 4] if it has actions that do not require any resources (ϕm(a, ω) = 0).",
                "Clearly, the model and problem statement described above make a number of assumptions about the problem and the desired solution properties.",
                "We discuss some of those assumptions and their implications in Section 6. 1222 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) (a) (b) Figure 1: Illustration of a solution to a resource-scheduling problem with three agents and three resources: a) static resource assignments (resource assignments are constant within agents lifetimes; b) dynamic assignment (resource assignments are allowed to change at every time step). 4.",
                "RESOURCE SCHEDULING Our resource-scheduling algorithm proceeds in two stages.",
                "First, we perform a preprocessing step that augments the agent MDPs; this process is described in Section 4.1.",
                "Second, using these augmented MDPs we construct a global optimization problem, which is described in Section 4.2. 4.1 Augmenting Agents MDPs In the model described in the previous section, we assume that if an agent does not possess the necessary resources to perform actions in its MDP, its execution is halted and the agent leaves the system.",
                "In other words, the MDPs cannot be paused and resumed.",
                "For example, in the problem shown in Figure 1a, agent m1 releases all resources after time τ = 3, at which point the execution of its MDP is halted.",
                "Similarly, agents m2 and m3 only execute their MDPs in the intervals τ ∈ [4, 6] and τ ∈ [4, 10], respectively.",
                "Therefore, an important part of the global decision-making problem is to decide the window of time during which each of the agents is active (i.e., executing its MDP).",
                "To accomplish this, we augment each agents MDP with two new states (start and finish states sb , sf , respectively) and a new start/stop action a∗ , as illustrated in Figure 2.",
                "The idea is that an agent stays in the start state sb until it is ready to execute its MDP, at which point it performs the start/stop action a∗ and transitions into the state space of the original MDP with the transition probability that corresponds to the original initial distribution α(s).",
                "For example, in Figure 1a, for agent m2 this would happen at time τ = 4.",
                "Once the agent gets to the end of its activity window (time τ = 6 for agent m2 in Figure 1a), it performs the start/stop action, which takes it into the sink finish state sf at time τ = 7.",
                "More precisely, given an MDP S, A, pm, rm, αm , we define an augmented MDP S , A , pm, rm, αm as follows: S = S ∪ sb ∪ sf ; A = A ∪ a∗ ; p (s|sb , a∗ ) = α(s), ∀s ∈ S; p (sb |sb , a) = 1.0, ∀a ∈ A; p (sf |s, a∗ ) = 1.0, ∀s ∈ S; p (σ|s, a) = p(σ|s, a), ∀s, σ ∈ S, a ∈ A; r (sb , a) = r (sf , a) = 0, ∀a ∈ A ; r (s, a) = r(s, a), ∀s ∈ S, a ∈ A; α (sb ) = 1; α (s) = 0, ∀s ∈ S; where all non-specified transition probabilities are assumed to be zero.",
                "Further, in order to account for the new starting state, we begin the MDP one time-step earlier, setting τa m ← τa m − 1.",
                "This will not affect the resource allocation due to the resource constraints only being enforced for the original MDP states, as will be discussed in the next section.",
                "For example, the augmented MDPs shown in Figure 2b (which starts in state sb at time τ = 2) would be constructed from an MDP with original arrival time τ = 3.",
                "Figure 2b also shows a sample trajectory through the state space: the agent starts in state sb , transitions into the state space S of the original MDP, and finally exists into the sink state sf .",
                "Note that if we wanted to model a problem where agents could pause their MDPs at arbitrary time steps (which might be useful for domains where dynamic reallocation is possible), we could easily accomplish this by including an extra action that transitions from each state to itself with zero reward. 4.2 MILP for Resource Scheduling Given a set of augmented MDPs, as defined above, the goal of this section is to formulate a global optimization program that solves the resource-scheduling problem.",
                "In this section and below, all MDPs are assumed to be the augmented MDPs as defined in Section 4.1.",
                "Our approach is similar to the idea used in [6]: we begin with the linear-program formulation of agents MDPs (1) and augment it with constraints that ensure that the corresponding resource allocation across agents and time is valid.",
                "The resulting optimization problem then simultaneously solves the agents MDPs and resource-scheduling problems.",
                "In the rest of this section, we incrementally develop a mixed integer program (MILP) that achieves this.",
                "In the absence of resource constraints, the agents finitehorizon MDPs are completely independent, and the globally optimal solution can be trivially obtained via the following LP, which is simply an aggregation of single-agent finitehorizon LPs: max X m X s X a rm(s, a) X t xm(s, a, t) subject to: X a xm(σ, a, t + 1) = X s,a pm(σ|s, a)xm(s, a, t), ∀m ∈ M, σ ∈ S, t ∈ [1, Tm − 1]; X a xm(s, a, 1) = αm(s), ∀m ∈ M, s ∈ S; (12) where xm(s, a, t) is the occupation measure of agent m, and The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1223 (a) (b) Figure 2: Illustration of augmenting an MDP to allow for variable starting and stopping times: a) (left) the original two-state MDP with a single action; (right) the augmented MDP with new states sb and sf and the new action a∗ (note that the origianl transitions are not changed in the augmentation process); b) the augmented MDP displayed as a trajectory through time (grey lines indicate all transitions, while black lines indicate a given trajectory.",
                "Objective Function (sum of expected rewards over all agents) max X m X s X a rm(s, a) X t xm(s, a, t) (5) Meaning Implication Linear Constraints Tie x to θ.",
                "Agent is only active when occupation measure is nonzero in original MDP states. θm(τ) = 0 =⇒ xm(s, a, τ −τa m+1) = 0 ∀s /∈ {sb , sf }, a ∈ A X s/∈{sb,sf } X a xm(s, a, t) ≤ θm(τa m + t − 1) ∀m ∈ M, ∀t ∈ [1, Tm] (6) Agent can only be active in τ ∈ (τa m, τd m) θm(τ) = 0 ∀m ∈ M, τ /∈ (τa m, τd m) (7) Cannot use resources when not active θm(τ) = 0 =⇒ Δm(τ, ω) = 0 ∀τ ∈ [0, bτ], ω ∈ Ω Δm(τ, ω) ≤ θm(τ) ∀m ∈ M, τ ∈ [0, bτ], ω ∈ Ω (8) Tie x to Δ (nonzero x forces corresponding Δ to be nonzero.)",
                "Δm(τ, ω) = 0, ϕm(a, ω) = 1 =⇒ xm(s, a, τ − τa m + 1) = 0 ∀s /∈ {sb , sf } 1/|A| X a ϕm(a, ω) X s/∈{sb,sf } xm(s, a, t) ≤ Δm(t + τa m − 1, ω) ∀m ∈ M, ω ∈ Ω, t ∈ [1, Tm] (9) Resource bounds X m Δm(τ, ω) ≤ bϕ(ω) ∀ω ∈ Ω, τ ∈ [0, bτ] (10) Agent cannot change resources while active.",
                "Only enabled for scheduling with static assignments. θm(τ) = 1 and θm(τ + 1) = 1 =⇒ Δm(τ, ω) = Δm(τ + 1, ω) Δm(τ, ω) − Z(1 − θm(τ + 1)) ≤ Δm(τ + 1, ω) + Z(1 − θm(τ)) Δm(τ, ω) + Z(1 − θm(τ + 1)) ≥ Δm(τ + 1, ω) − Z(1 − θm(τ)) ∀m ∈ M, ω ∈ Ω, τ ∈ [0, bτ] (11) Table 1: MILP for globally optimal resource scheduling.",
                "Tm = τd m − τa m + 1 is the time horizon for the agents MDP.",
                "Using this LP as a basis, we augment it with constraints that ensure that the resource usage implied by the agents occupation measures {xm} does not violate the global resource requirements bϕ at any time step τ ∈ [0, bτ].",
                "To formulate these resource constraints, we use the following binary variables: • Δm(τ, ω) = {0, 1}, ∀m ∈ M, τ ∈ [0, bτ], ω ∈ Ω, which serve as indicator variables that define whether agent m possesses resource ω at time τ.",
                "These are analogous to the static indicator variables used in the one-shot static resource-allocation problem in [6]. • θm = {0, 1}, ∀m ∈ M, τ ∈ [0, bτ] are indicator variables that specify whether agent m is active (i.e., executing its MDP) at time τ.",
                "The meaning of resource-usage variables Δ is illustrated in Figure 1: Δm(τ, ω) = 1 only if resource ω is allocated to agent m at time τ.",
                "The meaning of the activity indicators θ is illustrated in Figure 2b: when agent m is in either the start state sb or the finish state sf , the corresponding θm = 0, but once the agent becomes active and enters one of the other states, we set θm = 1 .",
                "This meaning of θ can be enforced with a linear constraint that synchronizes the values of the agents occupation measures xm and the activity 1224 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) indicators θ, as shown in (6) in Table 1.",
                "Another constraint we have to add-because the activity indicators θ are defined on the global timeline τ-is to enforce the fact that the agent is inactive outside of its arrivaldeparture window.",
                "This is accomplished by constraint (7) in Table 1.",
                "Furthermore, agents should not be using resources while they are inactive.",
                "This constraint can also be enforced via a linear inequality on θ and Δ, as shown in (8).",
                "Constraint (6) sets the value of θ to match the policy defined by the occupation measure xm.",
                "In a similar fashion, we have to make sure that the resource-usage variables Δ are also synchronized with the occupation measure xm.",
                "This is done via constraint (9) in Table 1, which is nearly identical to the analogous constraint from [6].",
                "After implementing the above constraint, which enforces the meaning of Δ, we add a constraint that ensures that the agents resource usage never exceeds the amounts of available resources.",
                "This condition is also trivially expressed as a linear inequality (10) in Table 1.",
                "Finally, for the problem formulation where resource assignments are static during a lifetime of an agent, we add a constraint that ensures that the resource-usage variables Δ do not change their value while the agent is active (θ = 1).",
                "This is accomplished via the linear constraint (11), where Z ≥ 2 is a constant that is used to turn off the constraints when θm(τ) = 0 or θm(τ + 1) = 0.",
                "This constraint is not used for the dynamic problem formulation, where resources can be reallocated between agents at every time step.",
                "To summarize, Table 1 together with the conservationof-flow constraints from (12) defines the MILP that simultaneously computes an optimal resource assignment for all agents across time as well as optimal finite-horizon MDP policies that are valid under that resource assignment.",
                "As a rough measure of the complexity of this MILP, let us consider the number of optimization variables and constraints.",
                "Let TM = P Tm = P m(τa m − τd m + 1) be the sum of the lengths of the arrival-departure windows across all agents.",
                "Then, the number of optimization variables is: TM + bτ|M||Ω| + bτ|M|, TM of which are continuous (xm), and bτ|M||Ω| + bτ|M| are binary (Δ and θ).",
                "However, notice that all but TM|M| of the θ are set to zero by constraint (7), which also immediately forces all but TM|M||Ω| of the Δ to be zero via the constraints (8).",
                "The number of constraints (not including the degenerate constraints in (7)) in the MILP is: TM + TM|Ω| + bτ|Ω| + bτ|M||Ω|.",
                "Despite the fact that the complexity of the MILP is, in the worst case, exponential1 in the number of binary variables, the complexity of this MILP is significantly (exponentially) lower than that of the MILP with flat utility functions, described in Section 2.2.",
                "This result echos the efficiency gains reported in [6] for single-shot resource-allocation problems, but is much more pronounced, because of the explosion of the flat utility representation due to the temporal aspect of the problem (recall the prohibitive complexity of the combinatorial optimization in Section 2.2).",
                "We empirically analyze the performance of this method in Section 5. 1 Strictly speaking, solving MILPs to optimality is NPcomplete in the number of integer variables. 5.",
                "EXPERIMENTAL RESULTS Although the complexity of solving MILPs is in the worst case exponential in the number of integer variables, there are many efficient methods for solving MILPs that allow our algorithm to scale well for parameters common to resource allocation and scheduling problems.",
                "In particular, this section introduces a problem domain-the repairshop problem-used to empirically evaluate our algorithms scalability in terms of the number of agents |M|, the number of shared resources |Ω|, and the varied lengths of global time bτ during which agents may enter and exit the system.",
                "The repairshop problem is a simple parameterized MDP adopting the metaphor of a vehicular repair shop.",
                "Agents in the repair shop are mechanics with a number of independent tasks that yield reward only when completed.",
                "In our MDP model of this system, actions taken to advance through the state space are only allowed if the agent holds certain resources that are publicly available to the shop.",
                "These resources are in finite supply, and optimal policies for the shop will determine when each agent may hold the limited resources to take actions and earn individual rewards.",
                "Each task to be completed is associated with a single action, although the agent is required to repeat the action numerous times before completing the task and earning a reward.",
                "This model was parameterized in terms of the number of agents in the system, the number of different types of resources that could be linked to necessary actions, a global time during which agents are allowed to arrive and depart, and a maximum length for the number of time steps an agent may remain in the system.",
                "All datapoints in our experiments were obtained with 20 evaluations using CPLEX to solve the MILPs on a Pentium4 computer with 2Gb of RAM.",
                "Trials were conducted on both the static and the dynamic version of the resourcescheduling problem, as defined earlier.",
                "Figure 3 shows the runtime and policy value for independent modifications to the parameter set.",
                "The top row shows how the solution time for the MILP scales as we increase the number of agents |M|, the global time horizon bτ, and the number of resources |Ω|.",
                "Increasing the number of agents leads to exponential complexity scaling, which is to be expected for an NP-complete problem.",
                "However, increasing the global time limit bτ or the total number of resource types |Ω|-while holding the number of agents constantdoes not lead to decreased performance.",
                "This occurs because the problems get easier as they become under-constrained, which is also a common phenomenon for NP-complete problems.",
                "We also observe that the solution to the dynamic version of the problem can often be computed much faster than the static version.",
                "The bottom row of Figure 3 shows the joint policy value of the policies that correspond to the computed optimal resource-allocation schedules.",
                "We can observe that the dynamic version yields higher reward (as expected, since the reward for the dynamic version is always no less than the reward of the static version).",
                "We should point out that these graphs should not be viewed as a measure of performance of two different algorithms (both algorithms produce optimal solutions but to different problems), but rather as observations about how the quality of optimal solutions change as more flexibility is allowed in the reallocation of resources.",
                "Figure 4 shows runtime and policy value for trials in which common input variables are scaled together.",
                "This allows The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1225 2 4 6 8 10 10 −3 10 −2 10 −1 10 0 10 1 10 2 10 3 10 4 Number of Agents |M| CPUTime,sec |Ω| = 5, τ = 50 static dynamic 50 100 150 200 10 −2 10 −1 10 0 10 1 10 2 10 3 Global Time Boundary τ CPUTime,sec |M| = 5, |Ω| = 5 static dynamic 10 20 30 40 50 10 −2 10 −1 10 0 10 1 10 2 Number of Resources |Ω| CPUTime,sec |M| = 5, τ = 50 static dynamic 2 4 6 8 10 200 400 600 800 1000 1200 1400 1600 Number of Agents |M| Value |Ω| = 5, τ = 50 static dynamic 50 100 150 200 400 500 600 700 800 900 1000 1100 1200 1300 1400 Global Time Boundary τ Value |M| = 5, |Ω| = 5 static dynamic 10 20 30 40 50 500 600 700 800 900 1000 1100 1200 1300 1400 Number of Resources |Ω| Value |M| = 5, τ = 50 static dynamic Figure 3: Evaluation of our MILP for variable numbers of agents (column 1), lengths of global-time window (column 2), and numbers of resource types (column 3).",
                "Top row shows CPU time, and bottom row shows the joint reward of agents MDP policies.",
                "Error bars show the 1st and 3rd quartiles (25% and 75%). 2 4 6 8 10 10 −3 10 −2 10 −1 10 0 10 1 10 2 10 3 Number of Agents |M| CPUTime,sec τ = 10|M| static dynamic 2 4 6 8 10 10 −3 10 −2 10 −1 10 0 10 1 10 2 10 3 10 4 Number of Agents |M| CPUTime,sec |Ω| = 2|M| static dynamic 2 4 6 8 10 10 −3 10 −2 10 −1 10 0 10 1 10 2 10 3 10 4 Number of Agents |M| CPUTime,sec |Ω| = 5|M| static dynamic 2 4 6 8 10 200 400 600 800 1000 1200 1400 1600 1800 2000 2200 Number of Agents |M| Value τ = 10|M| static dynamic 2 4 6 8 10 200 400 600 800 1000 1200 1400 1600 1800 2000 Number of Agents |M| Value |Ω| = 2|M| static dynamic 2 4 6 8 10 0 500 1000 1500 2000 2500 Number of Agents |M| Value |Ω| = 5|M| static dynamic Figure 4: Evaluation of our MILP using correlated input variables.",
                "The left column tracks the performance and CPU time as the number of agents and global-time window increase together (bτ = 10|M|).",
                "The middle and the right column track the performance and CPU time as the number of resources and the number of agents increase together as |Ω| = 2|M| and |Ω| = 5|M|, respectively.",
                "Error bars show the 1st and 3rd quartiles (25% and 75%). 1226 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) us to explore domains where the total number of agents scales proportionally to the total number of resource types or the global time horizon, while keeping constant the average agent density (per unit of global time) or the average number of resources per agent (which commonly occurs in real-life applications).",
                "Overall, we believe that these experimental results indicate that our MILP formulation can be used to effectively solve resource-scheduling problems of nontrivial size. 6.",
                "DISCUSSION AND CONCLUSIONS Throughout the paper, we have made a number of assumptions in our model and solution algorithm; we discuss their implications below. • Continual execution.",
                "We assume that once an agent stops executing its MDP (transitions into state sf ), it exits the system and cannot return.",
                "It is easy to relax this assumption for domains where agents MDPs can be paused and restarted.",
                "All that is required is to include an additional pause action which transitions from a given state back to itself, and has zero reward. • Indifference to start time.",
                "We used a reward model where agents rewards depend only on the time horizon of their MDPs and not the global start time.",
                "This is a consequence of our MDP-augmentation procedure from Section 4.1.",
                "It is easy to extend the model so that the agents incur an explicit penalty for idling by assigning a non-zero negative reward to the start state sb . • Binary resource requirements.",
                "For simplicity, we have assumed that resource costs are binary: ϕm(a, ω) = {0, 1}, but our results generalize in a straightforward manner to non-binary resource mappings, analogously to the procedure used in [5]. • Cooperative agents.",
                "The optimization procedure discussed in this paper was developed in the context of cooperative agents, but it can also be used to design a mechanism for scheduling resources among self-interested agents.",
                "This optimization procedure can be embedded in a VickreyClarke-Groves auction, completely analogously to the way it was done in [7].",
                "In fact, all the results of [7] about the properties of the auction and information privacy directly carry over to the scheduling domain discussed in this paper, requiring only slight modifications to deal with finitehorizon MDPs. • Known, deterministic arrival and departure times.",
                "Finally, we have assumed that agents arrival and departure times (τa m and τd m) are deterministic and known a priori.",
                "This assumption is fundamental to our solution method.",
                "While there are many domains where this assumption is valid, in many cases agents arrive and depart dynamically and their arrival and departure times can only be predicted probabilistically, leading to online resource-allocation problems.",
                "In particular, in the case of self-interested agents, this becomes an interesting version of an online-mechanism-design problem [11, 12].",
                "In summary, we have presented an MILP formulation for the combinatorial resource-scheduling problem where agents values for possible resource assignments are defined by finitehorizon MDPs.",
                "This result extends previous work ([6, 7]) on static one-shot resource allocation under MDP-induced preferences to resource-scheduling problems with a temporal aspect.",
                "As such, this work takes a step in the direction of designing an online mechanism for agents with combinatorial resource preferences induced by stochastic planning problems.",
                "Relaxing the assumption about deterministic arrival and departure times of the agents is a focus of our future work.",
                "We would like to thank the anonymous reviewers for their insightful comments and suggestions. 7.",
                "REFERENCES [1] E. Altman and A. Shwartz.",
                "Adaptive control of constrained Markov chains: Criteria and policies.",
                "Annals of Operations Research, special issue on Markov Decision Processes, 28:101-134, 1991. [2] R. Bellman.",
                "Dynamic Programming.",
                "Princeton University Press, 1957. [3] C. Boutilier.",
                "Solving concisely expressed combinatorial auction problems.",
                "In Proc. of AAAI-02, pages 359-366, 2002. [4] C. Boutilier and H. H. Hoos.",
                "Bidding languages for combinatorial auctions.",
                "In Proc. of IJCAI-01, pages 1211-1217, 2001. [5] D. Dolgov.",
                "Integrated Resource Allocation and Planning in Stochastic Multiagent Environments.",
                "PhD thesis, Computer Science Department, University of Michigan, February 2006. [6] D. A. Dolgov and E. H. Durfee.",
                "Optimal resource allocation and policy formulation in loosely-coupled Markov decision processes.",
                "In Proc. of ICAPS-04, pages 315-324, June 2004. [7] D. A. Dolgov and E. H. Durfee.",
                "Computationally efficient combinatorial auctions for resource allocation in weakly-coupled MDPs.",
                "In Proc. of AAMAS-05, New York, NY, USA, 2005.",
                "ACM Press. [8] D. A. Dolgov and E. H. Durfee.",
                "Resource allocation among agents with preferences induced by factored MDPs.",
                "In Proc. of AAMAS-06, 2006. [9] K. Larson and T. Sandholm.",
                "Mechanism design and deliberative agents.",
                "In Proc. of AAMAS-05, pages 650-656, New York, NY, USA, 2005.",
                "ACM Press. [10] N. Nisan.",
                "Bidding and allocation in combinatorial auctions.",
                "In Electronic Commerce, 2000. [11] D. C. Parkes and S. Singh.",
                "An MDP-based approach to Online Mechanism Design.",
                "In Proc. of the Seventeenths Annual Conference on Neural Information Processing Systems (NIPS-03), 2003. [12] D. C. Parkes, S. Singh, and D. Yanovsky.",
                "Approximately efficient online mechanism design.",
                "In Proc. of the Eighteenths Annual Conference on Neural Information Processing Systems (NIPS-04), 2004. [13] M. L. Puterman.",
                "Markov Decision Processes.",
                "John Wiley & Sons, New York, 1994. [14] M. H. Rothkopf, A. Pekec, and R. M. Harstad.",
                "Computationally manageable combinational auctions.",
                "Management Science, 44(8):1131-1147, 1998. [15] T. Sandholm.",
                "An algorithm for optimal winner determination in combinatorial auctions.",
                "In Proc. of IJCAI-99, pages 542-547, San Francisco, CA, USA, 1999.",
                "Morgan Kaufmann Publishers Inc.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1227"
            ],
            "original_annotated_samples": [
                "<br>combinatorial resource scheduling</br> for Multiagent MDPs Dmitri A. Dolgov, Michael R. James, and Michael E. Samples AI and Robotics Group Technical Research, Toyota Technical Center USA {ddolgov, michael.r.james, michael.samples}@gmail.com ABSTRACT Optimal resource scheduling in multiagent systems is a computationally challenging task, particularly when the values of resources are not additive.",
                "We extend those existing models to the problem of <br>combinatorial resource scheduling</br>, where agents persist only for finite periods between their (predefined) arrival and departure times, requiring resources only for those time periods.",
                "We also outline the standard methods for <br>combinatorial resource scheduling</br> with flat resource values, which serve as a comparison benchmark for the new model developed here. 2.1 Markov Decision Processes A stationary, finite-domain, discrete-time MDP (see, for example, [13] for a thorough and detailed development) can be described as S, A, p, r , where: S is a finite set of system states; A is a finite set of actions that are available to the agent; p is a stationary stochastic transition function, where p(σ|s, a) is the probability of transitioning to state σ upon executing action a in state s; r is a stationary reward function, where r(s, a) specifies the reward obtained upon executing action a in state s. Given such an MDP, a decision problem under a finite horizon T is to choose an optimal action at every time step to maximize the expected value of the total reward accrued during the agents (finite) lifetime.",
                "This result is well known for infinite-horizon MDPs with various types of constraints [1, 6], and it also holds for our finite-horizon model, which can be easily established via a line of reasoning completely analogous to the arguments in [6]. 2.2 <br>combinatorial resource scheduling</br> A straightforward approach to resource scheduling for a set of agents M, whose values for the resources are induced by stochastic planning problems (in our case, finite-horizon MDPs) would be to have each agent enumerate all possible resource assignments over time and, for each one, compute its value by solving the corresponding MDP."
            ],
            "translated_annotated_samples": [
                "La <br>programación de recursos combinatoria</br> para MDPs multiagentes. Dmitri A. Dolgov, Michael R. James y Michael E. Samples. Grupo de Investigación en IA y Robótica, Centro Técnico de Toyota en EE. UU. {ddolgov, michael.r.james, michael.samples}@gmail.com. RESUMEN La programación óptima de recursos en sistemas multiagentes es una tarea computacionalmente desafiante, especialmente cuando los valores de los recursos no son aditivos.",
                "Extendemos esos modelos existentes al problema de la <br>programación de recursos combinatoria</br>, donde los agentes persisten solo por períodos finitos entre sus tiempos de llegada y salida (predefinidos), requiriendo recursos solo durante esos períodos de tiempo.",
                "También describimos los métodos estándar para la <br>programación de recursos combinatorios</br> con valores de recursos planos, que sirven como punto de referencia de comparación para el nuevo modelo desarrollado aquí. 2.1 Procesos de Decisión de Markov Un MDP (Proceso de Decisión de Markov) estacionario, de dominio finito y discreto en el tiempo (ver, por ejemplo, [13] para un desarrollo detallado y exhaustivo) se puede describir como S, A, p, r, donde: S es un conjunto finito de estados del sistema; A es un conjunto finito de acciones disponibles para el agente; p es una función de transición estocástica estacionaria, donde p(σ|s, a) es la probabilidad de transicionar al estado σ al ejecutar la acción a en el estado s; r es una función de recompensa estacionaria, donde r(s, a) especifica la recompensa obtenida al ejecutar la acción a en el estado s. Dado un MDP de este tipo, un problema de decisión bajo un horizonte finito T consiste en elegir una acción óptima en cada paso de tiempo para maximizar el valor esperado de la recompensa total acumulada durante la vida (finita) de los agentes.",
                "Este resultado es bien conocido para MDPs de horizonte infinito con varios tipos de restricciones [1, 6], y también se cumple para nuestro modelo de horizonte finito, el cual puede establecerse fácilmente a través de una línea de razonamiento completamente análoga a los argumentos en [6]. 2.2 Programación de Recursos Combinatoria Un enfoque directo para la programación de recursos para un conjunto de agentes M, cuyos valores para los recursos son inducidos por problemas de planificación estocástica (en nuestro caso, MDPs de horizonte finito) sería que cada agente enumerara todas las asignaciones de recursos posibles a lo largo del tiempo y, para cada una, calcular su valor resolviendo el MDP correspondiente."
            ],
            "translated_text": "La <br>programación de recursos combinatoria</br> para MDPs multiagentes. Dmitri A. Dolgov, Michael R. James y Michael E. Samples. Grupo de Investigación en IA y Robótica, Centro Técnico de Toyota en EE. UU. {ddolgov, michael.r.james, michael.samples}@gmail.com. RESUMEN La programación óptima de recursos en sistemas multiagentes es una tarea computacionalmente desafiante, especialmente cuando los valores de los recursos no son aditivos. Consideramos el problema combinatorio de programar el uso de múltiples recursos entre agentes que operan en entornos estocásticos, modelados como procesos de decisión de Markov (MDPs). En los últimos años, se han desarrollado algoritmos eficientes de asignación de recursos para agentes con valores de recursos inducidos por MDPs. Sin embargo, este trabajo previo se ha centrado en problemas de asignación de recursos estáticos donde los recursos se distribuyen una vez y luego se utilizan en MDPs de horizonte infinito. Extendemos esos modelos existentes al problema de la <br>programación de recursos combinatoria</br>, donde los agentes persisten solo por períodos finitos entre sus tiempos de llegada y salida (predefinidos), requiriendo recursos solo durante esos períodos de tiempo. Proporcionamos un procedimiento computacionalmente eficiente para calcular asignaciones de recursos óptimas a nivel global a agentes a lo largo del tiempo. Ilustramos y analizamos empíricamente el método en el contexto de un dominio de programación de trabajos estocásticos. Categorías y Descriptores de Asignaturas I.2.8 [Inteligencia Artificial]: Resolución de Problemas, Métodos de Control y Búsqueda; I.2.11 [Inteligencia Artificial]: Inteligencia Artificial Distribuida-Sistemas Multiagente Términos Generales Algoritmos, Rendimiento, Diseño 1. INTRODUCCIÓN Las tareas de asignación óptima de recursos y programación son ubicuas en sistemas multiagentes, pero resolver tales problemas de optimización puede ser computacionalmente difícil, debido a una serie de factores. En particular, cuando el valor de un conjunto de recursos para un agente no es aditivo (como suele ser el caso con recursos que son sustitutos o complementos), la función de utilidad podría tener que definirse en un espacio exponencialmente grande de combinaciones de recursos, lo que rápidamente se vuelve computacionalmente intratable. Además, incluso cuando cada agente tiene una función de utilidad que es distinta de cero solo en un pequeño subconjunto de los posibles conjuntos de recursos, obtener una asignación óptima sigue siendo computacionalmente prohibitivo, ya que el problema se vuelve NP-completo [14]. Tales problemas computacionales han dado lugar recientemente a varias líneas de trabajo en el uso de modelos compactos de las preferencias de los agentes. Una idea es utilizar cualquier estructura presente en las funciones de utilidad para representarlas de forma compacta, por ejemplo, a través de fórmulas lógicas [15, 10, 4, 3]. Una alternativa es modelar directamente los mecanismos que definen las funciones de utilidad de los agentes y realizar la asignación de recursos directamente con estos modelos [9]. Una forma de lograr esto es modelar los procesos mediante los cuales un agente podría utilizar los recursos y definir la función de utilidad como la ganancia de estos procesos. En particular, si un agente utiliza recursos para actuar en un entorno estocástico, su función de utilidad puede ser modelada de forma natural con un proceso de decisión de Markov, cuyo conjunto de acciones está parametrizado por los recursos disponibles. Esta representación puede ser utilizada para construir algoritmos de asignación de recursos muy eficientes que resultan en una aceleración exponencial sobre un problema de optimización directa con representaciones planas de preferencias combinatorias [6, 7, 8]. Sin embargo, este trabajo existente sobre asignación de recursos con preferencias inducidas por MDPs parametrizados por recursos hace una suposición de que los recursos se asignan solo una vez y luego son utilizados por los agentes de forma independiente dentro de sus MDPs de horizonte infinito. Esta suposición de que no es posible la reasignación de recursos puede ser limitante en dominios donde los agentes llegan y se van dinámicamente. En este artículo, ampliamos el trabajo sobre asignación de recursos bajo preferencias inducidas por MDP a problemas de programación en tiempo discreto, donde los agentes están presentes en el sistema durante intervalos de tiempo finitos y solo pueden utilizar recursos dentro de estos intervalos. En particular, los agentes llegan y parten en momentos arbitrarios (predefinidos) y dentro de estos intervalos utilizan recursos para ejecutar tareas en MDPs de horizonte finito. Abordamos el problema de la programación de recursos óptima a nivel global, donde el objetivo es encontrar una asignación de recursos a los agentes a lo largo del tiempo que maximice la suma de las recompensas esperadas que obtienen. En este contexto, nuestra principal contribución es una formulación de programación entera mixta del problema de programación que elige asignaciones de recursos globalmente óptimas, tiempos de inicio y horizontes de ejecución para todos los agentes (dentro de sus intervalos de llegada y salida). Analizamos y comparamos empíricamente dos variantes del problema de programación: uno, donde los agentes tienen asignaciones de recursos estáticas dentro de sus MDPs de horizonte finito, y otro, donde los recursos pueden ser realocados dinámicamente entre agentes en cada paso de tiempo. En el resto del documento, primero establecemos las bases necesarias en la Sección 2 y luego presentamos nuestro modelo y declaración formal del problema en la Sección 3. En la Sección 4.2, describimos nuestro resultado principal, el programa de optimización para la programación de recursos globalmente óptima. Tras la discusión de nuestros resultados experimentales sobre un problema de programación de trabajos en la Sección 5, concluimos en la Sección 6 con una discusión de posibles extensiones y generalizaciones de nuestro método. De manera similar al modelo utilizado en trabajos anteriores sobre asignación de recursos con preferencias inducidas por MDP [6, 7], definimos el valor de un conjunto de recursos para un agente como el valor de la mejor política MDP que es realizable, dadas esos recursos. Sin embargo, dado que el enfoque de nuestro trabajo se centra en problemas de programación, y una gran parte del problema de optimización consiste en decidir cómo se asignan los recursos en el tiempo entre agentes con tiempos finitos de llegada y salida, modelamos los problemas de planificación de los agentes como MDPs de horizonte finito, en contraste con trabajos anteriores que utilizaron MDPs descontados de horizonte infinito. En el resto de esta sección, primero presentamos algo de antecedentes necesarios sobre MDPs de horizonte finito y presentamos una formulación de programación lineal que sirve como base para nuestro algoritmo de solución desarrollado en la Sección 4. También describimos los métodos estándar para la <br>programación de recursos combinatorios</br> con valores de recursos planos, que sirven como punto de referencia de comparación para el nuevo modelo desarrollado aquí. 2.1 Procesos de Decisión de Markov Un MDP (Proceso de Decisión de Markov) estacionario, de dominio finito y discreto en el tiempo (ver, por ejemplo, [13] para un desarrollo detallado y exhaustivo) se puede describir como S, A, p, r, donde: S es un conjunto finito de estados del sistema; A es un conjunto finito de acciones disponibles para el agente; p es una función de transición estocástica estacionaria, donde p(σ|s, a) es la probabilidad de transicionar al estado σ al ejecutar la acción a en el estado s; r es una función de recompensa estacionaria, donde r(s, a) especifica la recompensa obtenida al ejecutar la acción a en el estado s. Dado un MDP de este tipo, un problema de decisión bajo un horizonte finito T consiste en elegir una acción óptima en cada paso de tiempo para maximizar el valor esperado de la recompensa total acumulada durante la vida (finita) de los agentes. La política óptima del agente es entonces una función del estado actual s y el tiempo hasta el horizonte. Una política óptima para tal problema es actuar de forma codiciosa con respecto a la función de valor óptimo, definida de forma recursiva por el siguiente sistema de ecuaciones de Bellman de tiempo finito [2]: v(s, t) = max a r(s, a) + X σ p(σ|s, a)v(σ, t + 1), ∀s ∈ S, t ∈ [1, T − 1]; v(s, T) = 0, ∀s ∈ S; donde v(s, t) es el valor óptimo de estar en el estado s en el tiempo t ∈ [1, T]. Esta función de valor óptimo se puede calcular fácilmente utilizando programación dinámica, lo que conduce a la siguiente política óptima π, donde π(s, a, t) es la probabilidad de ejecutar la acción a en el estado s en el tiempo t: π(s, a, t) = (1, a = argmaxa r(s, a) + P σ p(σ|s, a)v(σ, t + 1), 0, en caso contrario. La anterior es la forma más común de calcular la función de valor óptimo (y por lo tanto una política óptima) para un MDP de horizonte finito. Sin embargo, también podemos formular el problema como el siguiente programa lineal (similarmente al LP dual para MDPs descontados de horizonte infinito [13, 6, 7]): max X s X a r(s, a) X t x(s, a, t) sujeto a: X a x(σ, a, t + 1) = X s,a p(σ|s, a)x(s, a, t) ∀σ, t ∈ [1, T − 1]; X a x(s, a, 1) = α(s), ∀s ∈ S; (1) donde α(s) es la distribución inicial sobre el espacio de estados, y x es la medida de ocupación (no estacionaria) (x(s, a, t) ∈ [0, 1] es el número total esperado de veces que se ejecuta la acción a en el estado s en el tiempo t). Una política óptima (no estacionaria) se obtiene a partir de la medida de ocupación de la siguiente manera: π(s, a, t) = x(s, a, t)/ X a x(s, a, t) ∀s ∈ S, t ∈ [1, T]. (2) Tenga en cuenta que el MDP estándar de horizonte finito sin restricciones, como se describe anteriormente, siempre tiene una solución uniformemente óptima (óptima para cualquier distribución inicial α(s)). Por lo tanto, una política óptima se puede obtener utilizando una constante arbitraria α(s) > 0 (en particular, α(s) = 1 resultará en x(s, a, t) = π(s, a, t)). Sin embargo, para los MDPs con restricciones de recursos (como se definen a continuación en la Sección 3), las políticas óptimas uniformemente no existen en general. En tales casos, α se convierte en parte de la entrada del problema, y una política resultante solo es óptima para ese α en particular. Este resultado es bien conocido para MDPs de horizonte infinito con varios tipos de restricciones [1, 6], y también se cumple para nuestro modelo de horizonte finito, el cual puede establecerse fácilmente a través de una línea de razonamiento completamente análoga a los argumentos en [6]. 2.2 Programación de Recursos Combinatoria Un enfoque directo para la programación de recursos para un conjunto de agentes M, cuyos valores para los recursos son inducidos por problemas de planificación estocástica (en nuestro caso, MDPs de horizonte finito) sería que cada agente enumerara todas las asignaciones de recursos posibles a lo largo del tiempo y, para cada una, calcular su valor resolviendo el MDP correspondiente. Entonces, cada agente proporcionaría valoraciones para cada posible conjunto de recursos a lo largo del tiempo a un coordinador centralizado, quien calcularía las asignaciones óptimas de recursos a lo largo del tiempo basándose en estas valoraciones. Cuando los recursos pueden asignarse en diferentes momentos a diferentes agentes, cada agente debe presentar valoraciones para cada combinación de posibles horizontes temporales. Que cada agente m ∈ M ejecute su MDP dentro del intervalo de tiempo de llegada-salida τ ∈ [τa m, τd m]. Por lo tanto, el agente m ejecutará un MDP con un horizonte temporal no mayor que Tm = τd m−τa m+1. Que bτ sea el horizonte temporal global para el problema, antes del cual todos los MDP de los agentes deben finalizar. Suponemos que τd m < bτ, ∀m ∈ M. El Sexto Congreso Internacional. En el problema de programación donde los agentes tienen requisitos de recursos estáticos dentro de sus MDPs de horizonte finito, los agentes proporcionan una valoración para cada conjunto de recursos para cada horizonte de tiempo posible (desde [1, Tm]) que puedan utilizar. Sea Ω el conjunto de recursos a ser asignados entre los agentes. Un agente recibirá como máximo un paquete de recursos para uno de los horizontes temporales. Que la variable ψ ∈ Ψm enumere todos los posibles pares de conjuntos de recursos y horizontes temporales para el agente m, por lo que hay 2|Ω| × Tm valores para ψ (el espacio de conjuntos es exponencial en el número de tipos de recursos |Ω|). El agente m debe proporcionar un valor vψ m para cada ψ, y el coordinador asignará como máximo un par (recurso, horizonte temporal) ψ a cada agente. Esta asignación se expresa como una variable indicadora zψ m ∈ {0, 1} que muestra si ψ está asignado al agente m. Para el tiempo τ y el recurso ω, la función nm(ψ, τ, ω) ∈ {0, 1} indica si el paquete en ψ utiliza el recurso ω en el tiempo τ (hacemos la suposición de que los agentes tienen requisitos de recursos binarios). Este problema de asignación es NP-completo, incluso al considerar solo un paso de tiempo, y su dificultad aumenta significativamente con múltiples pasos de tiempo debido al creciente número de valores de ψ. El problema de encontrar una asignación óptima que satisfaga la restricción global de que la cantidad de cada recurso ω asignada a todos los agentes no exceda la cantidad disponible bϕ(ω) se puede expresar como el siguiente programa entero: max X m∈M X ψ∈Ψm zψ mvψ m sujeto a: X ψ∈Ψm zψ m ≤ 1, ∀m ∈ M; X m∈M X ψ∈Ψm zψ mnm(ψ, τ, ω) ≤ bϕ(ω), ∀τ ∈ [1, bτ], ∀ω ∈ Ω; (3) La primera restricción en la ecuación 3 dice que ningún agente puede recibir más de un paquete, y la segunda restricción asegura que la asignación total del recurso ω no exceda, en ningún momento, el límite del recurso. Para el problema de programación en el que los agentes pueden reasignar dinámicamente recursos, cada agente debe especificar un valor para cada combinación de paquetes y pasos de tiempo dentro de su horizonte temporal. Que la variable ψ ∈ Ψm en este caso enumere todos los posibles conjuntos de recursos para los cuales a lo sumo un conjunto puede ser asignado al agente m en cada paso de tiempo. Por lo tanto, en este caso hay P t∈[1,Tm](2|Ω| )t ∼ 2|Ω|Tm posibilidades de combinaciones de recursos asignadas a diferentes intervalos de tiempo, para los Tm horizontes de tiempo diferentes. El mismo conjunto de ecuaciones (3) se puede utilizar para resolver este problema de programación dinámica, pero el programa entero es diferente debido a la diferencia en cómo se define ψ. En este caso, el número de valores de ψ es exponencial en el horizonte de planificación Tm de cada agente, lo que resulta en un programa mucho más grande. Este enfoque directo para resolver ambos problemas de programación requiere una enumeración y solución de 2|Ω| Tm (asignación estática) o P t∈[1,Tm] 2|Ω|t (reubicación dinámica) MDPs para cada agente, lo cual rápidamente se vuelve intratable con el crecimiento del número de recursos |Ω| o el horizonte de tiempo Tm. MODELO Y DECLARACIÓN DEL PROBLEMA Ahora presentamos formalmente nuestro modelo del problema de programación de recursos. La entrada del problema consiste en los siguientes componentes: • M, Ω, bϕ, τa m, τd m, bτ están definidos arriba en la Sección 2.2. • {Θm} = {S, A, pm, rm, αm} son los MDPs de todos los agentes m ∈ M. Sin pérdida de generalidad, asumimos que los espacios de estados y acciones de todos los agentes son iguales, pero cada uno tiene su propia función de transición pm, función de recompensa rm, y condiciones iniciales αm. • ϕm : A×Ω → {0, 1} es la asignación de acciones a recursos para el agente m. ϕm(a, ω) indica si la acción a del agente m necesita el recurso ω. Un agente m que recibe un conjunto de recursos que no incluye el recurso ω no puede ejecutar en su política MDP ninguna acción a para la cual ϕm(a, ω) = 0. Suponemos que todos los requisitos de recursos son binarios; como se discute más adelante en la Sección 6, esta suposición no es limitante. Dado el input anterior, el problema de optimización que consideramos es encontrar la asignación globalmente óptima que maximice la suma de recompensas esperadas de recursos a agentes para todos los pasos de tiempo: Δ : τ × M × Ω → {0, 1}. Una solución es factible si la asignación correspondiente de recursos a los agentes no viola la restricción global de recursos: X m Δm(τ, ω) ≤ bϕ(ω), ∀ω ∈ Ω, τ ∈ [1, bτ]. Consideramos dos variantes del problema de programación de recursos. La primera formulación restringe las asignaciones de recursos al espacio donde la asignación de recursos a cada agente es estática durante la vida de los agentes. La segunda formulación permite la reasignación de recursos entre agentes en cada paso de tiempo durante sus vidas. La Figura 1 representa un problema de programación de recursos con tres agentes M = {m1, m2, m3}, tres recursos Ω = {ω1, ω2, ω3}, y un horizonte de problema global de bτ = 11. Los tiempos de llegada y salida de los agentes se muestran como cajas grises y son {1, 6}, {3, 7} y {2, 11}, respectivamente. Una solución a este problema se muestra a través de barras horizontales dentro de cada casilla de agentes, donde las barras corresponden a la asignación de los tres tipos de recursos. La Figura 1a muestra una solución a un problema de programación estática. Según la solución mostrada, el agente m1 comienza la ejecución de su MDP en el tiempo τ = 1 y tiene un bloqueo en los tres recursos hasta que finaliza la ejecución en el tiempo τ = 3. Se debe tener en cuenta que el agente m1 renuncia a su control sobre los recursos antes de su hora de salida anunciada de τd m1 = 6, aparentemente porque otros agentes pueden utilizar los recursos de manera más efectiva. Por lo tanto, en el tiempo τ = 4, los recursos ω1 y ω3 se asignan al agente m2, quien luego los utiliza para ejecutar su MDP (utilizando solo acciones admitidas por los recursos ω1 y ω3) hasta el tiempo τ = 7. El agente m3 posee el recurso ω3 durante el intervalo τ ∈ [4, 10]. La Figura 1b muestra una posible solución a la versión dinámica del mismo problema. Allí, los recursos pueden ser reasignados entre agentes en cada paso de tiempo. Por ejemplo, el agente m1 renuncia a su uso del recurso ω2 en el tiempo τ = 2, aunque continúa la ejecución de su MDP hasta el tiempo τ = 6. Ten en cuenta que a un agente no se le permite detener y reiniciar su MDP, por lo que el agente m1 solo puede continuar ejecutando en el intervalo τ ∈ [3, 4] si tiene acciones que no requieren recursos (ϕm(a, ω) = 0). Claramente, el modelo y la declaración del problema descritos anteriormente hacen una serie de suposiciones sobre el problema y las propiedades de la solución deseada. Discutimos algunas de esas suposiciones y sus implicaciones en la Sección 6. 1222 La Sexta Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) (a) (b) Figura 1: Ilustración de una solución a un problema de programación de recursos con tres agentes y tres recursos: a) asignaciones de recursos estáticas (las asignaciones de recursos son constantes durante la vida de los agentes); b) asignación dinámica (las asignaciones de recursos pueden cambiar en cada paso de tiempo). 4. PROGRAMACIÓN DE RECURSOS Nuestro algoritmo de programación de recursos se lleva a cabo en dos etapas. Primero, realizamos un paso de preprocesamiento que aumenta los MDP de los agentes; este proceso se describe en la Sección 4.1. Segundo, utilizando estos MDPs aumentados construimos un problema de optimización global, el cual se describe en la Sección 4.2. 4.1 Aumentando los MDPs de los Agentes En el modelo descrito en la sección anterior, asumimos que si un agente no posee los recursos necesarios para realizar acciones en su MDP, su ejecución se detiene y el agente abandona el sistema. En otras palabras, los MDPs no pueden ser pausados y reanudados. Por ejemplo, en el problema mostrado en la Figura 1a, el agente m1 libera todos los recursos después del tiempo τ = 3, momento en el cual la ejecución de su MDP se detiene. De manera similar, los agentes m2 y m3 solo ejecutan sus MDPs en los intervalos τ ∈ [4, 6] y τ ∈ [4, 10], respectivamente. Por lo tanto, una parte importante del problema de toma de decisiones global es decidir la ventana de tiempo durante la cual cada uno de los agentes está activo (es decir, ejecutando su MDP). Para lograr esto, ampliamos el MDP de cada agente con dos nuevos estados (estados de inicio y final sb, sf, respectivamente) y una nueva acción de inicio/parada a∗, como se ilustra en la Figura 2. La idea es que un agente permanezca en el estado inicial sb hasta que esté listo para ejecutar su MDP, momento en el que realiza la acción de inicio/parada a∗ y transita al espacio de estados del MDP original con la probabilidad de transición que corresponde a la distribución inicial original α(s). Por ejemplo, en la Figura 1a, para el agente m2 esto ocurriría en el tiempo τ = 4. Una vez que el agente llega al final de su ventana de actividad (tiempo τ = 6 para el agente m2 en la Figura 1a), realiza la acción de inicio/parada, lo que lo lleva al estado final de finalización del sumidero sf en el tiempo τ = 7. Más precisamente, dado un MDP S, A, pm, rm, αm, definimos un MDP aumentado S, A, pm, rm, αm de la siguiente manera: S = S ∪ sb ∪ sf; A = A ∪ a∗; p(s|sb, a∗) = α(s), ∀s ∈ S; p(sb|sb, a) = 1.0, ∀a ∈ A; p(sf|s, a∗) = 1.0, ∀s ∈ S; p(σ|s, a) = p(σ|s, a), ∀s, σ ∈ S, a ∈ A; r(sb, a) = r(sf, a) = 0, ∀a ∈ A; r(s, a) = r(s, a), ∀s ∈ S, a ∈ A; α(sb) = 1; α(s) = 0, ∀s ∈ S; donde se asume que todas las probabilidades de transición no especificadas son cero. Además, para tener en cuenta el nuevo estado inicial, comenzamos el MDP un paso de tiempo antes, estableciendo τa m ← τa m − 1. Esto no afectará la asignación de recursos debido a que las restricciones de recursos solo se aplican a los estados originales del MDP, como se discutirá en la siguiente sección. Por ejemplo, los MDPs aumentados mostrados en la Figura 2b (que comienza en el estado sb en el tiempo τ = 2) se construirían a partir de un MDP con tiempo de llegada original τ = 3. La Figura 2b también muestra una trayectoria de muestra a través del espacio de estados: el agente comienza en el estado sb, transita al espacio de estados S del MDP original y finalmente sale al estado de sumidero sf. Cabe destacar que si quisiéramos modelar un problema en el que los agentes pudieran pausar sus MDP en pasos de tiempo arbitrarios (lo cual podría ser útil para dominios donde la reasignación dinámica es posible), podríamos lograrlo fácilmente incluyendo una acción adicional que transiciona de cada estado a sí mismo con una recompensa de cero. 4.2 MILP para la programación de recursos Dado un conjunto de MDPs aumentados, como se define arriba, el objetivo de esta sección es formular un programa de optimización global que resuelva el problema de programación de recursos. En esta sección y a continuación, se asume que todos los MDPs son los MDPs aumentados tal como se definen en la Sección 4.1. Nuestro enfoque es similar a la idea utilizada en [6]: comenzamos con la formulación del programa lineal de los MDP de los agentes (1) y lo complementamos con restricciones que aseguran que la asignación de recursos correspondiente entre agentes y tiempo sea válida. El problema de optimización resultante resuelve simultáneamente los MDP de los agentes y los problemas de programación de recursos. En el resto de esta sección, desarrollamos de forma incremental un programa de enteros mixtos (MILP) que logra esto. En ausencia de restricciones de recursos, los MDPs de horizonte finito de los agentes son completamente independientes, y la solución óptima global puede obtenerse trivialmente a través del siguiente LP, que es simplemente una agregación de LPs de horizonte finito de un solo agente: max X m X s X a rm(s, a) X t xm(s, a, t) sujeto a: X a xm(σ, a, t + 1) = X s,a pm(σ|s, a)xm(s, a, t), ∀m ∈ M, σ ∈ S, t ∈ [1, Tm − 1]; X a xm(s, a, 1) = αm(s), ∀m ∈ M, s ∈ S; (12) donde xm(s, a, t) es la medida de ocupación del agente m, y The Sixth Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 1223 (a) (b) Figura 2: Ilustración de la ampliación de un MDP para permitir tiempos de inicio y finalización variables: a) (izquierda) el MDP original de dos estados con una sola acción; (derecha) el MDP ampliado con nuevos estados sb y sf y la nueva acción a∗ (nota que las transiciones originales no se modifican en el proceso de ampliación); b) el MDP ampl augmentado mostrado como una trayectoria a través del tiempo (las líneas grises indican todas las transiciones, mientras que las líneas negras indican una trayectoria dada). Función objetivo (suma de recompensas esperadas de todos los agentes) máx X m X s X a rm(s, a) X t xm(s, a, t) (5) Implicación de significado Restricciones lineales Atar x a θ. El agente solo está activo cuando la medida de ocupación es distinta de cero en los estados originales del MDP. θm(τ) = 0 =⇒ xm(s, a, τ −τa m+1) = 0 ∀s /∈ {sb , sf }, a ∈ A X s/∈{sb,sf } X a xm(s, a, t) ≤ θm(τa m + t − 1) ∀m ∈ M, ∀t ∈ [1, Tm] (6) El agente solo puede estar activo en τ ∈ (τa m, τd m) θm(τ) = 0 ∀m ∈ M, τ /∈ (τa m, τd m) (7) No se pueden utilizar recursos cuando no está activo θm(τ) = 0 =⇒ Δm(τ, ω) = 0 ∀τ ∈ [0, bτ], ω ∈ Ω Δm(τ, ω) ≤ θm(τ) ∀m ∈ M, τ ∈ [0, bτ], ω ∈ Ω (8) Vincular x a Δ (x distinto de cero obliga a que el correspondiente Δ sea distinto de cero). Δm(τ, ω) = 0, ϕm(a, ω) = 1 =⇒ xm(s, a, τ − τa m + 1) = 0 ∀s /∈ {sb , sf } 1/|A| X a ϕm(a, ω) X s/∈{sb,sf } xm(s, a, t) ≤ Δm(t + τa m − 1, ω) ∀m ∈ M, ω ∈ Ω, t ∈ [1, Tm] (9) Límites de recursos X m Δm(τ, ω) ≤ bϕ(ω) ∀ω ∈ Ω, τ ∈ [0, bτ] (10) El agente no puede cambiar los recursos mientras está activo. Solo habilitado para programación con asignaciones estáticas. θm(τ) = 1 y θm(τ + 1) = 1 =⇒ Δm(τ, ω) = Δm(τ + 1, ω) Δm(τ, ω) − Z(1 − θm(τ + 1)) ≤ Δm(τ + 1, ω) + Z(1 − θm(τ)) Δm(τ, ω) + Z(1 − θm(τ + 1)) ≥ Δm(τ + 1, ω) − Z(1 − θm(τ)) ∀m ∈ M, ω ∈ Ω, τ ∈ [0, bτ] (11) Tabla 1: MILP para programación óptima global de recursos. Tm = τd m − τa m + 1 es el horizonte temporal para los agentes MDP. Usando este LP como base, lo ampliamos con restricciones que aseguran que el uso de recursos implicado por las medidas de ocupación de los agentes {xm} no viola los requisitos globales de recursos bϕ en ningún paso de tiempo τ ∈ [0, bτ]. Para formular estas restricciones de recursos, utilizamos las siguientes variables binarias: • Δm(τ, ω) = {0, 1}, ∀m ∈ M, τ ∈ [0, bτ], ω ∈ Ω, que sirven como variables indicadoras que definen si el agente m posee el recurso ω en el tiempo τ. Estos son análogos a las variables indicadoras estáticas utilizadas en el problema de asignación de recursos estáticos de una sola vez en [6]. • θm = {0, 1}, ∀m ∈ M, τ ∈ [0, bτ] son variables indicadoras que especifican si el agente m está activo (es decir, ejecutando su MDP) en el tiempo τ. El significado de las variables de uso de recursos Δ se ilustra en la Figura 1: Δm(τ, ω) = 1 solo si el recurso ω se asigna al agente m en el tiempo τ. El significado de los indicadores de actividad θ se ilustra en la Figura 2b: cuando el agente m se encuentra en el estado inicial sb o en el estado final sf, el θm correspondiente es igual a 0, pero una vez que el agente se vuelve activo y entra en uno de los otros estados, establecemos θm = 1. Este significado de θ puede ser impuesto con una restricción lineal que sincroniza los valores de las medidas de ocupación de los agentes xm y la actividad 1224 The Sixth Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) indicadores θ, como se muestra en (6) en la Tabla 1. Otra restricción que debemos agregar, debido a que los indicadores de actividad θ están definidos en la línea de tiempo global τ, es hacer cumplir el hecho de que el agente está inactivo fuera de su ventana de llegada y salida. Esto se logra mediante la restricción (7) en la Tabla 1. Además, los agentes no deben estar utilizando recursos mientras están inactivos. Esta restricción también puede ser impuesta a través de una desigualdad lineal en θ y Δ, como se muestra en (8). La restricción (6) establece el valor de θ para que coincida con la política definida por la medida de ocupación xm. De manera similar, debemos asegurarnos de que las variables de uso de recursos Δ estén también sincronizadas con la medida de ocupación xm. Esto se realiza a través de la restricción (9) en la Tabla 1, que es casi idéntica a la restricción análoga de [6]. Después de implementar la restricción anterior, que refuerza el significado de Δ, agregamos una restricción que garantiza que el uso de recursos de los agentes nunca exceda los montos de recursos disponibles. Esta condición también se expresa trivialmente como una desigualdad lineal (10) en la Tabla 1. Finalmente, para la formulación del problema en la que las asignaciones de recursos son estáticas durante la vida de un agente, agregamos una restricción que garantiza que las variables de uso de recursos Δ no cambien su valor mientras el agente está activo (θ = 1). Esto se logra a través de la restricción lineal (11), donde Z ≥ 2 es una constante que se utiliza para desactivar las restricciones cuando θm(τ) = 0 o θm(τ + 1) = 0. Esta restricción no se utiliza para la formulación del problema dinámico, donde los recursos pueden ser reasignados entre agentes en cada paso de tiempo. En resumen, la Tabla 1 junto con las restricciones de conservación de flujo de (12) define el MILP que calcula simultáneamente una asignación óptima de recursos para todos los agentes a lo largo del tiempo, así como políticas óptimas de MDP de horizonte finito que son válidas bajo esa asignación de recursos. Como una medida aproximada de la complejidad de este MILP, consideremos el número de variables de optimización y restricciones. Sea TM = P Tm = P m(τa m − τd m + 1) la suma de las longitudes de las ventanas de llegada-salida a través de todos los agentes. Entonces, el número de variables de optimización es: TM + bτ|M||Ω| + bτ|M|, de las cuales TM son continuas (xm), y bτ|M||Ω| + bτ|M| son binarias (Δ y θ). Sin embargo, hay que tener en cuenta que todos menos TM|M| de los θ se establecen en cero por la restricción (7), lo que también obliga inmediatamente a que todos menos TM|M||Ω| de los Δ sean cero a través de las restricciones (8). El número de restricciones (sin incluir las restricciones degeneradas en (7)) en el MILP es: TM + TM|Ω| + bτ|Ω| + bτ|M||Ω|. A pesar de que la complejidad del MILP es, en el peor de los casos, exponencial en el número de variables binarias, la complejidad de este MILP es significativamente (exponencialmente) menor que la del MILP con funciones de utilidad planas, descrito en la Sección 2.2. Este resultado refleja las ganancias de eficiencia reportadas en [6] para problemas de asignación de recursos de un solo disparo, pero es mucho más pronunciado, debido a la explosión de la representación de utilidad plana debido al aspecto temporal del problema (recuerde la complejidad prohibitiva de la optimización combinatoria en la Sección 2.2). Analizamos empíricamente el rendimiento de este método en la Sección 5.1. Estrictamente hablando, resolver MILPs hasta la optimalidad es NP-completo en el número de variables enteras. RESULTADOS EXPERIMENTALES Aunque la complejidad de resolver MILPs es en el peor de los casos exponencial en el número de variables enteras, existen muchos métodos eficientes para resolver MILPs que permiten que nuestro algoritmo se adapte bien a parámetros comunes en problemas de asignación de recursos y programación. En particular, esta sección introduce un dominio de problema: el problema del taller de reparación, que se utiliza para evaluar empíricamente la escalabilidad de nuestros algoritmos en términos del número de agentes |M|, el número de recursos compartidos |Ω| y las diferentes longitudes de tiempo global bτ durante las cuales los agentes pueden entrar y salir del sistema. El problema del taller de reparaciones es un MDP parametrizado simple que adopta la metáfora de un taller de reparaciones de vehículos. Los agentes en el taller de reparación son mecánicos con una serie de tareas independientes que solo generan recompensa cuando se completan. En nuestro modelo MDP de este sistema, las acciones tomadas para avanzar a través del espacio de estados solo están permitidas si el agente posee ciertos recursos que están públicamente disponibles para la tienda. Estos recursos son de suministro finito, y las políticas óptimas para la tienda determinarán cuándo cada agente puede retener los recursos limitados para tomar acciones y obtener recompensas individuales. Cada tarea a completar está asociada con una sola acción, aunque se requiere que el agente repita la acción numerosas veces antes de completar la tarea y obtener una recompensa. Este modelo fue parametrizado en términos del número de agentes en el sistema, el número de diferentes tipos de recursos que podrían estar vinculados a acciones necesarias, un tiempo global durante el cual se permite a los agentes llegar y partir, y una longitud máxima para el número de pasos de tiempo que un agente puede permanecer en el sistema. Todos los puntos de datos en nuestros experimentos se obtuvieron con 20 evaluaciones utilizando CPLEX para resolver los MILPs en una computadora Pentium4 con 2Gb de RAM. Se realizaron pruebas tanto en la versión estática como en la dinámica del problema de programación de recursos, tal como se definió anteriormente. La Figura 3 muestra el tiempo de ejecución y el valor de la política para modificaciones independientes en el conjunto de parámetros. La fila superior muestra cómo el tiempo de solución para el MILP escala a medida que aumentamos el número de agentes |M|, el horizonte de tiempo global bτ y el número de recursos |Ω|. Aumentar el número de agentes conduce a una escalabilidad exponencial en complejidad, lo cual es de esperarse para un problema NP-completo. Sin embargo, aumentar el límite de tiempo global bτ o el número total de tipos de recursos |Ω|, manteniendo constante el número de agentes, no conduce a una disminución en el rendimiento. Esto ocurre porque los problemas se vuelven más fáciles a medida que se vuelven sub-determinados, lo cual también es un fenómeno común en problemas NP-completos. También observamos que la solución a la versión dinámica del problema a menudo se puede calcular mucho más rápido que la versión estática. La fila inferior de la Figura 3 muestra el valor de la política conjunta de las políticas que corresponden a los horarios óptimos de asignación de recursos calculados. Podemos observar que la versión dinámica produce una recompensa más alta (como era de esperar, ya que la recompensa de la versión dinámica siempre es igual o mayor que la recompensa de la versión estática). Deberíamos señalar que estos gráficos no deben ser vistos como una medida del rendimiento de dos algoritmos diferentes (ambos algoritmos producen soluciones óptimas pero para problemas distintos), sino más bien como observaciones sobre cómo cambia la calidad de las soluciones óptimas a medida que se permite más flexibilidad en la reasignación de recursos. La Figura 4 muestra el tiempo de ejecución y el valor de la política para las pruebas en las que las variables de entrada comunes se escalan juntas. Esto permite la Sexta Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 1225 2 4 6 8 10 10 −3 10 −2 10 −1 10 0 10 1 10 2 10 3 10 4 Número de Agentes |M| Tiempo de CPU, seg |Ω| = 5, τ = 50 estático dinámico 50 100 150 200 10 −2 10 −1 10 0 10 1 10 2 10 3 Límite de Tiempo Global τ Tiempo de CPU, seg |M| = 5, |Ω| = 5 estático dinámico 10 20 30 40 50 10 −2 10 −1 10 0 10 1 10 2 Número de Recursos |Ω| Tiempo de CPU, seg |M| = 5, τ = 50 estático dinámico 2 4 6 8 10 200 400 600 800 1000 1200 1400 1600 Número de Agentes |M| Valor |Ω| = 5, τ = 50 estático dinámico 50 100 150 200 400 500 600 700 800 900 1000 1100 1200 1300 1400 Límite de Tiempo Global τ Valor |M| = 5, |Ω| = 5 estático dinámico 10 20 30 40 50 500 600 700 800 900 1000 1100 1200 1300 1400 Número de Recursos |Ω| Valor |M| = 5, τ = 50 estático dinámico Figura 3: Evaluación de nuestro MILP para números variables de agentes (columna 1), longitudes de ventana de tiempo global (columna 2) y números de tipos de recursos (columna 3). La fila superior muestra el tiempo de CPU, y la fila inferior muestra la recompensa conjunta de las políticas MDP de los agentes. Las barras de error muestran los cuartiles 1ro y 3ro (25% y 75%). 2 4 6 8 10 10 −3 10 −2 10 −1 10 0 10 1 10 2 10 3 Número de Agentes |M| Tiempo de CPU, seg τ = 10|M| estático dinámico 2 4 6 8 10 10 −3 10 −2 10 −1 10 0 10 1 10 2 10 3 10 4 Número de Agentes |M| Tiempo de CPU, seg |Ω| = 2|M| estático dinámico 2 4 6 8 10 10 −3 10 −2 10 −1 10 0 10 1 10 2 10 3 10 4 Número de Agentes |M| Tiempo de CPU, seg |Ω| = 5|M| estático dinámico 2 4 6 8 10 200 400 600 800 1000 1200 1400 1600 1800 2000 2200 Número de Agentes |M| Valor τ = 10|M| estático dinámico 2 4 6 8 10 200 400 600 800 1000 1200 1400 1600 1800 2000 Número de Agentes |M| Valor |Ω| = 2|M| estático dinámico 2 4 6 8 10 0 500 1000 1500 2000 2500 Número de Agentes |M| Valor |Ω| = 5|M| estático dinámico Figura 4: Evaluación de nuestro MILP utilizando variables de entrada correlacionadas. La columna izquierda sigue el rendimiento y el tiempo de CPU a medida que aumenta el número de agentes y la ventana de tiempo global juntos (bτ = 10|M|). La columna del medio y la columna de la derecha siguen el rendimiento y el tiempo de CPU a medida que aumenta el número de recursos y el número de agentes juntos, con |Ω| = 2|M| y |Ω| = 5|M|, respectivamente. Las barras de error muestran el primer y tercer cuartil (25% y 75%). 1226 The Sixth Intl. La Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) nos permite explorar dominios donde el número total de agentes escala proporcionalmente al número total de tipos de recursos o al horizonte temporal global, manteniendo constante la densidad promedio de agentes (por unidad de tiempo global) o el número promedio de recursos por agente (lo cual comúnmente ocurre en aplicaciones de la vida real). En general, creemos que estos resultados experimentales indican que nuestra formulación MILP puede ser utilizada para resolver de manera efectiva problemas de programación de recursos de tamaño no trivial. DISCUSIÓN Y CONCLUSIONES A lo largo del documento, hemos realizado una serie de suposiciones en nuestro modelo y algoritmo de solución; discutimos sus implicaciones a continuación. • Ejecución continua. Suponemos que una vez que un agente deja de ejecutar su MDP (transiciona al estado sf), sale del sistema y no puede regresar. Es fácil relajar esta suposición para dominios donde los agentes de MDP pueden ser pausados y reiniciados. Todo lo que se requiere es incluir una acción de pausa adicional que haga la transición desde un estado dado de regreso a sí mismo, y que tenga una recompensa de cero. • Indiferencia al tiempo de inicio. Utilizamos un modelo de recompensa donde las recompensas de los agentes dependen solo del horizonte temporal de sus MDP y no del tiempo de inicio global. Esta es una consecuencia de nuestro procedimiento de aumento de MDP de la Sección 4.1. Es fácil extender el modelo para que los agentes incurran en una penalización explícita por inactividad al asignar una recompensa negativa no nula al estado inicial sb. • Requisitos de recursos binarios. Para simplificar, hemos asumido que los costos de recursos son binarios: ϕm(a, ω) = {0, 1}, pero nuestros resultados se generalizan de manera directa a asignaciones de recursos no binarias, de manera análoga al procedimiento utilizado en [5]. • Agentes cooperativos. El procedimiento de optimización discutido en este artículo fue desarrollado en el contexto de agentes cooperativos, pero también puede ser utilizado para diseñar un mecanismo para programar recursos entre agentes egoístas. Este procedimiento de optimización puede ser incorporado en una subasta de Vickrey-Clarke-Groves, de manera completamente análoga a como se hizo en [7]. De hecho, todos los resultados de [7] sobre las propiedades de la subasta y la privacidad de la información se aplican directamente al dominio de programación discutido en este documento, requiriendo solo ligeras modificaciones para tratar con MDPs de horizonte finito. • Tiempos de llegada y salida conocidos y deterministas. Finalmente, hemos asumido que los tiempos de llegada y salida de los agentes (τa m y τd m) son deterministas y conocidos de antemano. Esta suposición es fundamental para nuestro método de solución. Si bien hay muchos dominios donde esta suposición es válida, en muchos casos los agentes llegan y se van de forma dinámica y sus tiempos de llegada y salida solo pueden predecirse de manera probabilística, lo que lleva a problemas de asignación de recursos en línea. En particular, en el caso de agentes con interés propio, esto se convierte en una versión interesante de un problema de diseño de mecanismos en línea [11, 12]. En resumen, hemos presentado una formulación MILP para el problema combinatorio de programación de recursos donde los valores de los agentes para posibles asignaciones de recursos están definidos por MDPs de horizonte finito. Este resultado amplía el trabajo previo ([6, 7]) sobre asignación estática de recursos de una sola vez bajo preferencias inducidas por MDP a problemas de programación de recursos con un aspecto temporal. Por lo tanto, este trabajo da un paso en la dirección de diseñar un mecanismo en línea para agentes con preferencias de recursos combinatorios inducidas por problemas de planificación estocástica. Relajar la suposición sobre los tiempos deterministas de llegada y salida de los agentes es un enfoque de nuestro trabajo futuro. Nos gustaría agradecer a los revisores anónimos por sus comentarios y sugerencias perspicaces. 7. REFERENCIAS [1] E. Altman y A. Shwartz. Control adaptativo de cadenas de Markov restringidas: Criterios y políticas. Anales de Investigación de Operaciones, número especial sobre Procesos de Decisión de Markov, 28:101-134, 1991. [2] R. Bellman. Programación dinámica. Princeton University Press, 1957. [3] C. Boutilier. \n\nPrensa de la Universidad de Princeton, 1957. [3] C. Boutilier. Resolviendo problemas de subasta combinatoria expresados de manera concisa. En Proc. de AAAI-02, páginas 359-366, 2002. [4] C. Boutilier y H. H. Hoos. Subastando idiomas para subastas combinatorias. En Proc. de IJCAI-01, páginas 1211-1217, 2001. [5] D. Dolgov. Asignación integrada de recursos y planificación en entornos estocásticos de múltiples agentes. Tesis doctoral, Departamento de Ciencias de la Computación, Universidad de Michigan, febrero de 2006. [6] D. A. Dolgov y E. H. Durfee. Asignación óptima de recursos y formulación de políticas en procesos de decisión de Markov débilmente acoplados. En Proc. de ICAPS-04, páginas 315-324, junio de 2004. [7] D. A. Dolgov y E. H. Durfee. Subastas combinatorias computacionalmente eficientes para asignación de recursos en MDPs débilmente acoplados. En Proc. de AAMAS-05, Nueva York, NY, EE. UU., 2005. ACM Press. [8] D. A. Dolgov y E. H. Durfee. Asignación de recursos entre agentes con preferencias inducidas por MDP factorizados. En Proc. de AAMAS-06, 2006. [9] K. Larson y T. Sandholm. Diseño de mecanismos y agentes deliberativos. En Proc. de AAMAS-05, páginas 650-656, Nueva York, NY, EE. UU., 2005. ACM Press. [10] N. Nisan. \n\nACM Press. [10] N. Nisan. Subasta y asignación en subastas combinatorias. En Comercio Electrónico, 2000. [11] D. C. Parkes y S. Singh. Un enfoque basado en MDP para el Diseño de Mecanismos en Línea. En Proc. de la Decimoséptima Conferencia Anual sobre Sistemas de Procesamiento de Información Neural (NIPS-03), 2003. [12] D. C. Parkes, S. Singh y D. Yanovsky. Diseño de mecanismos en línea aproximadamente eficiente. En Actas de la Decimoctava Conferencia Anual sobre Sistemas de Procesamiento de Información Neural (NIPS-04), 2004. [13] M. L. Puterman. Procesos de Decisión de Markov. John Wiley & Sons, Nueva York, 1994. [14] M. H. Rothkopf, A. Pekec y R. M. Harstad. Subastas combinatorias manejables computacionalmente. Ciencia de la Gestión, 44(8):1131-1147, 1998. [15] T. Sandholm. Un algoritmo para la determinación óptima del ganador en subastas combinatorias. En Proc. de IJCAI-99, páginas 542-547, San Francisco, CA, EE. UU., 1999. Morgan Kaufmann Publishers Inc. - Editores Morgan Kaufmann Inc. El Sexto Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 1227 ",
            "candidates": [],
            "error": [
                [
                    "programación de recursos combinatoria",
                    "programación de recursos combinatoria",
                    "programación de recursos combinatorios"
                ]
            ]
        },
        "optimal resource scheduling": {
            "translated_key": "",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Combinatorial Resource Scheduling for Multiagent MDPs Dmitri A. Dolgov, Michael R. James, and Michael E. Samples AI and Robotics Group Technical Research, Toyota Technical Center USA {ddolgov, michael.r.james, michael.samples}@gmail.com ABSTRACT <br>optimal resource scheduling</br> in multiagent systems is a computationally challenging task, particularly when the values of resources are not additive.",
                "We consider the combinatorial problem of scheduling the usage of multiple resources among agents that operate in stochastic environments, modeled as Markov decision processes (MDPs).",
                "In recent years, efficient resource-allocation algorithms have been developed for agents with resource values induced by MDPs.",
                "However, this prior work has focused on static resource-allocation problems where resources are distributed once and then utilized in infinite-horizon MDPs.",
                "We extend those existing models to the problem of combinatorial resource scheduling, where agents persist only for finite periods between their (predefined) arrival and departure times, requiring resources only for those time periods.",
                "We provide a computationally efficient procedure for computing globally optimal resource assignments to agents over time.",
                "We illustrate and empirically analyze the method in the context of a stochastic jobscheduling domain.",
                "Categories and Subject Descriptors I.2.8 [Artificial Intelligence]: Problem Solving, Control Methods, and Search; I.2.11 [Artificial Intelligence]: Distributed Artificial Intelligence-Multiagent systems General Terms Algorithms, Performance, Design 1.",
                "INTRODUCTION The tasks of optimal resource allocation and scheduling are ubiquitous in multiagent systems, but solving such optimization problems can be computationally difficult, due to a number of factors.",
                "In particular, when the value of a set of resources to an agent is not additive (as is often the case with resources that are substitutes or complements), the utility function might have to be defined on an exponentially large space of resource bundles, which very quickly becomes computationally intractable.",
                "Further, even when each agent has a utility function that is nonzero only on a small subset of the possible resource bundles, obtaining optimal allocation is still computationally prohibitive, as the problem becomes NP-complete [14].",
                "Such computational issues have recently spawned several threads of work in using compact models of agents preferences.",
                "One idea is to use any structure present in utility functions to represent them compactly, via, for example, logical formulas [15, 10, 4, 3].",
                "An alternative is to directly model the mechanisms that define the agents utility functions and perform resource allocation directly with these models [9].",
                "A way of accomplishing this is to model the processes by which an agent might utilize the resources and define the utility function as the payoff of these processes.",
                "In particular, if an agent uses resources to act in a stochastic environment, its utility function can be naturally modeled with a Markov decision process, whose action set is parameterized by the available resources.",
                "This representation can then be used to construct very efficient resource-allocation algorithms that lead to an exponential speedup over a straightforward optimization problem with flat representations of combinatorial preferences [6, 7, 8].",
                "However, this existing work on resource allocation with preferences induced by resource-parameterized MDPs makes an assumption that the resources are only allocated once and are then utilized by the agents independently within their infinite-horizon MDPs.",
                "This assumption that no reallocation of resources is possible can be limiting in domains where agents arrive and depart dynamically.",
                "In this paper, we extend the work on resource allocation under MDP-induced preferences to discrete-time scheduling problems, where agents are present in the system for finite time intervals and can only use resources within these intervals.",
                "In particular, agents arrive and depart at arbitrary (predefined) times and within these intervals use resources to execute tasks in finite-horizon MDPs.",
                "We address the problem of globally <br>optimal resource scheduling</br>, where the objective is to find an allocation of resources to the agents across time that maximizes the sum of the expected rewards that they obtain.",
                "In this context, our main contribution is a mixed-integerprogramming formulation of the scheduling problem that chooses globally optimal resource assignments, starting times, and execution horizons for all agents (within their arrival1220 978-81-904262-7-5 (RPS) c 2007 IFAAMAS departure intervals).",
                "We analyze and empirically compare two flavors of the scheduling problem: one, where agents have static resource assignments within their finite-horizon MDPs, and another, where resources can be dynamically reallocated between agents at every time step.",
                "In the rest of the paper, we first lay down the necessary groundwork in Section 2 and then introduce our model and formal problem statement in Section 3.",
                "In Section 4.2, we describe our main result, the optimization program for globally <br>optimal resource scheduling</br>.",
                "Following the discussion of our experimental results on a job-scheduling problem in Section 5, we conclude in Section 6 with a discussion of possible extensions and generalizations of our method. 2.",
                "BACKGROUND Similarly to the model used in previous work on resourceallocation with MDP-induced preferences [6, 7], we define the value of a set of resources to an agent as the value of the best MDP policy that is realizable, given those resources.",
                "However, since the focus of our work is on scheduling problems, and a large part of the optimization problem is to decide how resources are allocated in time among agents with finite arrival and departure times, we model the agents planning problems as finite-horizon MDPs, in contrast to previous work that used infinite-horizon discounted MDPs.",
                "In the rest of this section, we first introduce some necessary background on finite-horizon MDPs and present a linear-programming formulation that serves as the basis for our solution algorithm developed in Section 4.",
                "We also outline the standard methods for combinatorial resource scheduling with flat resource values, which serve as a comparison benchmark for the new model developed here. 2.1 Markov Decision Processes A stationary, finite-domain, discrete-time MDP (see, for example, [13] for a thorough and detailed development) can be described as S, A, p, r , where: S is a finite set of system states; A is a finite set of actions that are available to the agent; p is a stationary stochastic transition function, where p(σ|s, a) is the probability of transitioning to state σ upon executing action a in state s; r is a stationary reward function, where r(s, a) specifies the reward obtained upon executing action a in state s. Given such an MDP, a decision problem under a finite horizon T is to choose an optimal action at every time step to maximize the expected value of the total reward accrued during the agents (finite) lifetime.",
                "The agents optimal policy is then a function of current state s and the time until the horizon.",
                "An optimal policy for such a problem is to act greedily with respect to the optimal value function, defined recursively by the following system of finite-time Bellman equations [2]: v(s, t) = max a r(s, a) + X σ p(σ|s, a)v(σ, t + 1), ∀s ∈ S, t ∈ [1, T − 1]; v(s, T) = 0, ∀s ∈ S; where v(s, t) is the optimal value of being in state s at time t ∈ [1, T].",
                "This optimal value function can be easily computed using dynamic programming, leading to the following optimal policy π, where π(s, a, t) is the probability of executing action a in state s at time t: π(s, a, t) = ( 1, a = argmaxa r(s, a) + P σ p(σ|s, a)v(σ, t + 1), 0, otherwise.",
                "The above is the most common way of computing the optimal value function (and therefore an optimal policy) for a finite-horizon MDP.",
                "However, we can also formulate the problem as the following linear program (similarly to the dual LP for infinite-horizon discounted MDPs [13, 6, 7]): max X s X a r(s, a) X t x(s, a, t) subject to: X a x(σ, a, t + 1) = X s,a p(σ|s, a)x(s, a, t) ∀σ, t ∈ [1, T − 1]; X a x(s, a, 1) = α(s), ∀s ∈ S; (1) where α(s) is the initial distribution over the state space, and x is the (non-stationary) occupation measure (x(s, a, t) ∈ [0, 1] is the total expected number of times action a is executed in state s at time t).",
                "An optimal (non-stationary) policy is obtained from the occupation measure as follows: π(s, a, t) = x(s, a, t)/ X a x(s, a, t) ∀s ∈ S, t ∈ [1, T]. (2) Note that the standard unconstrained finite-horizon MDP, as described above, always has a uniformly-optimal solution (optimal for any initial distribution α(s)).",
                "Therefore, an optimal policy can be obtained by using an arbitrary constant α(s) > 0 (in particular, α(s) = 1 will result in x(s, a, t) = π(s, a, t)).",
                "However, for MDPs with resource constraints (as defined below in Section 3), uniformly-optimal policies do not in general exist.",
                "In such cases, α becomes a part of the problem input, and a resulting policy is only optimal for that particular α.",
                "This result is well known for infinite-horizon MDPs with various types of constraints [1, 6], and it also holds for our finite-horizon model, which can be easily established via a line of reasoning completely analogous to the arguments in [6]. 2.2 Combinatorial Resource Scheduling A straightforward approach to resource scheduling for a set of agents M, whose values for the resources are induced by stochastic planning problems (in our case, finite-horizon MDPs) would be to have each agent enumerate all possible resource assignments over time and, for each one, compute its value by solving the corresponding MDP.",
                "Then, each agent would provide valuations for each possible resource bundle over time to a centralized coordinator, who would compute the optimal resource assignments across time based on these valuations.",
                "When resources can be allocated at different times to different agents, each agent must submit valuations for every combination of possible time horizons.",
                "Let each agent m ∈ M execute its MDP within the arrival-departure time interval τ ∈ [τa m, τd m].",
                "Hence, agent m will execute an MDP with time horizon no greater than Tm = τd m−τa m+1.",
                "Let bτ be the global time horizon for the problem, before which all of the agents MDPs must finish.",
                "We assume τd m < bτ, ∀m ∈ M. The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1221 For the scheduling problem where agents have static resource requirements within their finite-horizon MDPs, the agents provide a valuation for each resource bundle for each possible time horizon (from [1, Tm]) that they may use.",
                "Let Ω be the set of resources to be allocated among the agents.",
                "An agent will get at most one resource bundle for one of the time horizons.",
                "Let the variable ψ ∈ Ψm enumerate all possible pairs of resource bundles and time horizons for agent m, so there are 2|Ω| × Tm values for ψ (the space of bundles is exponential in the number of resource types |Ω|).",
                "The agent m must provide a value vψ m for each ψ, and the coordinator will allocate at most one ψ (resource, time horizon) pair to each agent.",
                "This allocation is expressed as an indicator variable zψ m ∈ {0, 1} that shows whether ψ is assigned to agent m. For time τ and resource ω, the function nm(ψ, τ, ω) ∈ {0, 1} indicates whether the bundle in ψ uses resource ω at time τ (we make the assumption that agents have binary resource requirements).",
                "This allocation problem is NP-complete, even when considering only a single time step, and its difficulty increases significantly with multiple time steps because of the increasing number of values of ψ.",
                "The problem of finding an optimal allocation that satisfies the global constraint that the amount of each resource ω allocated to all agents does not exceed the available amount bϕ(ω) can be expressed as the following integer program: max X m∈M X ψ∈Ψm zψ mvψ m subject to: X ψ∈Ψm zψ m ≤ 1, ∀m ∈ M; X m∈M X ψ∈Ψm zψ mnm(ψ, τ, ω) ≤ bϕ(ω), ∀τ ∈ [1, bτ], ∀ω ∈ Ω; (3) The first constraint in equation 3 says that no agent can receive more than one bundle, and the second constraint ensures that the total assignment of resource ω does not, at any time, exceed the resource bound.",
                "For the scheduling problem where the agents are able to dynamically reallocate resources, each agent must specify a value for every combination of bundles and time steps within its time horizon.",
                "Let the variable ψ ∈ Ψm in this case enumerate all possible resource bundles for which at most one bundle may be assigned to agent m at each time step.",
                "Therefore, in this case there are P t∈[1,Tm](2|Ω| )t ∼ 2|Ω|Tm possibilities of resource bundles assigned to different time slots, for the Tm different time horizons.",
                "The same set of equations (3) can be used to solve this dynamic scheduling problem, but the integer program is different because of the difference in how ψ is defined.",
                "In this case, the number of ψ values is exponential in each agents planning horizon Tm, resulting in a much larger program.",
                "This straightforward approach to solving both of these scheduling problems requires an enumeration and solution of either 2|Ω| Tm (static allocation) or P t∈[1,Tm] 2|Ω|t (dynamic reallocation) MDPs for each agent, which very quickly becomes intractable with the growth of the number of resources |Ω| or the time horizon Tm. 3.",
                "MODEL AND PROBLEM STATEMENT We now formally introduce our model of the resourcescheduling problem.",
                "The problem input consists of the following components: • M, Ω, bϕ, τa m, τd m, bτ are as defined above in Section 2.2. • {Θm} = {S, A, pm, rm, αm} are the MDPs of all agents m ∈ M. Without loss of generality, we assume that state and action spaces of all agents are the same, but each has its own transition function pm, reward function rm, and initial conditions αm. • ϕm : A×Ω → {0, 1} is the mapping of actions to resources for agent m. ϕm(a, ω) indicates whether action a of agent m needs resource ω.",
                "An agent m that receives a set of resources that does not include resource ω cannot execute in its MDP policy any action a for which ϕm(a, ω) = 0.",
                "We assume all resource requirements are binary; as discussed below in Section 6, this assumption is not limiting.",
                "Given the above input, the optimization problem we consider is to find the globally optimal-maximizing the sum of expected rewards-mapping of resources to agents for all time steps: Δ : τ × M × Ω → {0, 1}.",
                "A solution is feasible if the corresponding assignment of resources to the agents does not violate the global resource constraint: X m Δm(τ, ω) ≤ bϕ(ω), ∀ω ∈ Ω, τ ∈ [1, bτ]. (4) We consider two flavors of the resource-scheduling problem.",
                "The first formulation restricts resource assignments to the space where the allocation of resources to each agent is static during the agents lifetime.",
                "The second formulation allows reassignment of resources between agents at every time step within their lifetimes.",
                "Figure 1 depicts a resource-scheduling problem with three agents M = {m1, m2, m3}, three resources Ω = {ω1, ω2, ω3}, and a global problem horizon of bτ = 11.",
                "The agents arrival and departure times are shown as gray boxes and are {1, 6}, {3, 7}, and {2, 11}, respectively.",
                "A solution to this problem is shown via horizontal bars within each agents box, where the bars correspond to the allocation of the three resource types.",
                "Figure 1a shows a solution to a static scheduling problem.",
                "According to the shown solution, agent m1 begins the execution of its MDP at time τ = 1 and has a lock on all three resources until it finishes execution at time τ = 3.",
                "Note that agent m1 relinquishes its hold on the resources before its announced departure time of τd m1 = 6, ostensibly because other agents can utilize the resources more effectively.",
                "Thus, at time τ = 4, resources ω1 and ω3 are allocated to agent m2, who then uses them to execute its MDP (using only actions supported by resources ω1 and ω3) until time τ = 7.",
                "Agent m3 holds resource ω3 during the interval τ ∈ [4, 10].",
                "Figure 1b shows a possible solution to the dynamic version of the same problem.",
                "There, resources can be reallocated between agents at every time step.",
                "For example, agent m1 gives up its use of resource ω2 at time τ = 2, although it continues the execution of its MDP until time τ = 6.",
                "Notice that an agent is not allowed to stop and restart its MDP, so agent m1 is only able to continue executing in the interval τ ∈ [3, 4] if it has actions that do not require any resources (ϕm(a, ω) = 0).",
                "Clearly, the model and problem statement described above make a number of assumptions about the problem and the desired solution properties.",
                "We discuss some of those assumptions and their implications in Section 6. 1222 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) (a) (b) Figure 1: Illustration of a solution to a resource-scheduling problem with three agents and three resources: a) static resource assignments (resource assignments are constant within agents lifetimes; b) dynamic assignment (resource assignments are allowed to change at every time step). 4.",
                "RESOURCE SCHEDULING Our resource-scheduling algorithm proceeds in two stages.",
                "First, we perform a preprocessing step that augments the agent MDPs; this process is described in Section 4.1.",
                "Second, using these augmented MDPs we construct a global optimization problem, which is described in Section 4.2. 4.1 Augmenting Agents MDPs In the model described in the previous section, we assume that if an agent does not possess the necessary resources to perform actions in its MDP, its execution is halted and the agent leaves the system.",
                "In other words, the MDPs cannot be paused and resumed.",
                "For example, in the problem shown in Figure 1a, agent m1 releases all resources after time τ = 3, at which point the execution of its MDP is halted.",
                "Similarly, agents m2 and m3 only execute their MDPs in the intervals τ ∈ [4, 6] and τ ∈ [4, 10], respectively.",
                "Therefore, an important part of the global decision-making problem is to decide the window of time during which each of the agents is active (i.e., executing its MDP).",
                "To accomplish this, we augment each agents MDP with two new states (start and finish states sb , sf , respectively) and a new start/stop action a∗ , as illustrated in Figure 2.",
                "The idea is that an agent stays in the start state sb until it is ready to execute its MDP, at which point it performs the start/stop action a∗ and transitions into the state space of the original MDP with the transition probability that corresponds to the original initial distribution α(s).",
                "For example, in Figure 1a, for agent m2 this would happen at time τ = 4.",
                "Once the agent gets to the end of its activity window (time τ = 6 for agent m2 in Figure 1a), it performs the start/stop action, which takes it into the sink finish state sf at time τ = 7.",
                "More precisely, given an MDP S, A, pm, rm, αm , we define an augmented MDP S , A , pm, rm, αm as follows: S = S ∪ sb ∪ sf ; A = A ∪ a∗ ; p (s|sb , a∗ ) = α(s), ∀s ∈ S; p (sb |sb , a) = 1.0, ∀a ∈ A; p (sf |s, a∗ ) = 1.0, ∀s ∈ S; p (σ|s, a) = p(σ|s, a), ∀s, σ ∈ S, a ∈ A; r (sb , a) = r (sf , a) = 0, ∀a ∈ A ; r (s, a) = r(s, a), ∀s ∈ S, a ∈ A; α (sb ) = 1; α (s) = 0, ∀s ∈ S; where all non-specified transition probabilities are assumed to be zero.",
                "Further, in order to account for the new starting state, we begin the MDP one time-step earlier, setting τa m ← τa m − 1.",
                "This will not affect the resource allocation due to the resource constraints only being enforced for the original MDP states, as will be discussed in the next section.",
                "For example, the augmented MDPs shown in Figure 2b (which starts in state sb at time τ = 2) would be constructed from an MDP with original arrival time τ = 3.",
                "Figure 2b also shows a sample trajectory through the state space: the agent starts in state sb , transitions into the state space S of the original MDP, and finally exists into the sink state sf .",
                "Note that if we wanted to model a problem where agents could pause their MDPs at arbitrary time steps (which might be useful for domains where dynamic reallocation is possible), we could easily accomplish this by including an extra action that transitions from each state to itself with zero reward. 4.2 MILP for Resource Scheduling Given a set of augmented MDPs, as defined above, the goal of this section is to formulate a global optimization program that solves the resource-scheduling problem.",
                "In this section and below, all MDPs are assumed to be the augmented MDPs as defined in Section 4.1.",
                "Our approach is similar to the idea used in [6]: we begin with the linear-program formulation of agents MDPs (1) and augment it with constraints that ensure that the corresponding resource allocation across agents and time is valid.",
                "The resulting optimization problem then simultaneously solves the agents MDPs and resource-scheduling problems.",
                "In the rest of this section, we incrementally develop a mixed integer program (MILP) that achieves this.",
                "In the absence of resource constraints, the agents finitehorizon MDPs are completely independent, and the globally optimal solution can be trivially obtained via the following LP, which is simply an aggregation of single-agent finitehorizon LPs: max X m X s X a rm(s, a) X t xm(s, a, t) subject to: X a xm(σ, a, t + 1) = X s,a pm(σ|s, a)xm(s, a, t), ∀m ∈ M, σ ∈ S, t ∈ [1, Tm − 1]; X a xm(s, a, 1) = αm(s), ∀m ∈ M, s ∈ S; (12) where xm(s, a, t) is the occupation measure of agent m, and The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1223 (a) (b) Figure 2: Illustration of augmenting an MDP to allow for variable starting and stopping times: a) (left) the original two-state MDP with a single action; (right) the augmented MDP with new states sb and sf and the new action a∗ (note that the origianl transitions are not changed in the augmentation process); b) the augmented MDP displayed as a trajectory through time (grey lines indicate all transitions, while black lines indicate a given trajectory.",
                "Objective Function (sum of expected rewards over all agents) max X m X s X a rm(s, a) X t xm(s, a, t) (5) Meaning Implication Linear Constraints Tie x to θ.",
                "Agent is only active when occupation measure is nonzero in original MDP states. θm(τ) = 0 =⇒ xm(s, a, τ −τa m+1) = 0 ∀s /∈ {sb , sf }, a ∈ A X s/∈{sb,sf } X a xm(s, a, t) ≤ θm(τa m + t − 1) ∀m ∈ M, ∀t ∈ [1, Tm] (6) Agent can only be active in τ ∈ (τa m, τd m) θm(τ) = 0 ∀m ∈ M, τ /∈ (τa m, τd m) (7) Cannot use resources when not active θm(τ) = 0 =⇒ Δm(τ, ω) = 0 ∀τ ∈ [0, bτ], ω ∈ Ω Δm(τ, ω) ≤ θm(τ) ∀m ∈ M, τ ∈ [0, bτ], ω ∈ Ω (8) Tie x to Δ (nonzero x forces corresponding Δ to be nonzero.)",
                "Δm(τ, ω) = 0, ϕm(a, ω) = 1 =⇒ xm(s, a, τ − τa m + 1) = 0 ∀s /∈ {sb , sf } 1/|A| X a ϕm(a, ω) X s/∈{sb,sf } xm(s, a, t) ≤ Δm(t + τa m − 1, ω) ∀m ∈ M, ω ∈ Ω, t ∈ [1, Tm] (9) Resource bounds X m Δm(τ, ω) ≤ bϕ(ω) ∀ω ∈ Ω, τ ∈ [0, bτ] (10) Agent cannot change resources while active.",
                "Only enabled for scheduling with static assignments. θm(τ) = 1 and θm(τ + 1) = 1 =⇒ Δm(τ, ω) = Δm(τ + 1, ω) Δm(τ, ω) − Z(1 − θm(τ + 1)) ≤ Δm(τ + 1, ω) + Z(1 − θm(τ)) Δm(τ, ω) + Z(1 − θm(τ + 1)) ≥ Δm(τ + 1, ω) − Z(1 − θm(τ)) ∀m ∈ M, ω ∈ Ω, τ ∈ [0, bτ] (11) Table 1: MILP for globally <br>optimal resource scheduling</br>.",
                "Tm = τd m − τa m + 1 is the time horizon for the agents MDP.",
                "Using this LP as a basis, we augment it with constraints that ensure that the resource usage implied by the agents occupation measures {xm} does not violate the global resource requirements bϕ at any time step τ ∈ [0, bτ].",
                "To formulate these resource constraints, we use the following binary variables: • Δm(τ, ω) = {0, 1}, ∀m ∈ M, τ ∈ [0, bτ], ω ∈ Ω, which serve as indicator variables that define whether agent m possesses resource ω at time τ.",
                "These are analogous to the static indicator variables used in the one-shot static resource-allocation problem in [6]. • θm = {0, 1}, ∀m ∈ M, τ ∈ [0, bτ] are indicator variables that specify whether agent m is active (i.e., executing its MDP) at time τ.",
                "The meaning of resource-usage variables Δ is illustrated in Figure 1: Δm(τ, ω) = 1 only if resource ω is allocated to agent m at time τ.",
                "The meaning of the activity indicators θ is illustrated in Figure 2b: when agent m is in either the start state sb or the finish state sf , the corresponding θm = 0, but once the agent becomes active and enters one of the other states, we set θm = 1 .",
                "This meaning of θ can be enforced with a linear constraint that synchronizes the values of the agents occupation measures xm and the activity 1224 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) indicators θ, as shown in (6) in Table 1.",
                "Another constraint we have to add-because the activity indicators θ are defined on the global timeline τ-is to enforce the fact that the agent is inactive outside of its arrivaldeparture window.",
                "This is accomplished by constraint (7) in Table 1.",
                "Furthermore, agents should not be using resources while they are inactive.",
                "This constraint can also be enforced via a linear inequality on θ and Δ, as shown in (8).",
                "Constraint (6) sets the value of θ to match the policy defined by the occupation measure xm.",
                "In a similar fashion, we have to make sure that the resource-usage variables Δ are also synchronized with the occupation measure xm.",
                "This is done via constraint (9) in Table 1, which is nearly identical to the analogous constraint from [6].",
                "After implementing the above constraint, which enforces the meaning of Δ, we add a constraint that ensures that the agents resource usage never exceeds the amounts of available resources.",
                "This condition is also trivially expressed as a linear inequality (10) in Table 1.",
                "Finally, for the problem formulation where resource assignments are static during a lifetime of an agent, we add a constraint that ensures that the resource-usage variables Δ do not change their value while the agent is active (θ = 1).",
                "This is accomplished via the linear constraint (11), where Z ≥ 2 is a constant that is used to turn off the constraints when θm(τ) = 0 or θm(τ + 1) = 0.",
                "This constraint is not used for the dynamic problem formulation, where resources can be reallocated between agents at every time step.",
                "To summarize, Table 1 together with the conservationof-flow constraints from (12) defines the MILP that simultaneously computes an optimal resource assignment for all agents across time as well as optimal finite-horizon MDP policies that are valid under that resource assignment.",
                "As a rough measure of the complexity of this MILP, let us consider the number of optimization variables and constraints.",
                "Let TM = P Tm = P m(τa m − τd m + 1) be the sum of the lengths of the arrival-departure windows across all agents.",
                "Then, the number of optimization variables is: TM + bτ|M||Ω| + bτ|M|, TM of which are continuous (xm), and bτ|M||Ω| + bτ|M| are binary (Δ and θ).",
                "However, notice that all but TM|M| of the θ are set to zero by constraint (7), which also immediately forces all but TM|M||Ω| of the Δ to be zero via the constraints (8).",
                "The number of constraints (not including the degenerate constraints in (7)) in the MILP is: TM + TM|Ω| + bτ|Ω| + bτ|M||Ω|.",
                "Despite the fact that the complexity of the MILP is, in the worst case, exponential1 in the number of binary variables, the complexity of this MILP is significantly (exponentially) lower than that of the MILP with flat utility functions, described in Section 2.2.",
                "This result echos the efficiency gains reported in [6] for single-shot resource-allocation problems, but is much more pronounced, because of the explosion of the flat utility representation due to the temporal aspect of the problem (recall the prohibitive complexity of the combinatorial optimization in Section 2.2).",
                "We empirically analyze the performance of this method in Section 5. 1 Strictly speaking, solving MILPs to optimality is NPcomplete in the number of integer variables. 5.",
                "EXPERIMENTAL RESULTS Although the complexity of solving MILPs is in the worst case exponential in the number of integer variables, there are many efficient methods for solving MILPs that allow our algorithm to scale well for parameters common to resource allocation and scheduling problems.",
                "In particular, this section introduces a problem domain-the repairshop problem-used to empirically evaluate our algorithms scalability in terms of the number of agents |M|, the number of shared resources |Ω|, and the varied lengths of global time bτ during which agents may enter and exit the system.",
                "The repairshop problem is a simple parameterized MDP adopting the metaphor of a vehicular repair shop.",
                "Agents in the repair shop are mechanics with a number of independent tasks that yield reward only when completed.",
                "In our MDP model of this system, actions taken to advance through the state space are only allowed if the agent holds certain resources that are publicly available to the shop.",
                "These resources are in finite supply, and optimal policies for the shop will determine when each agent may hold the limited resources to take actions and earn individual rewards.",
                "Each task to be completed is associated with a single action, although the agent is required to repeat the action numerous times before completing the task and earning a reward.",
                "This model was parameterized in terms of the number of agents in the system, the number of different types of resources that could be linked to necessary actions, a global time during which agents are allowed to arrive and depart, and a maximum length for the number of time steps an agent may remain in the system.",
                "All datapoints in our experiments were obtained with 20 evaluations using CPLEX to solve the MILPs on a Pentium4 computer with 2Gb of RAM.",
                "Trials were conducted on both the static and the dynamic version of the resourcescheduling problem, as defined earlier.",
                "Figure 3 shows the runtime and policy value for independent modifications to the parameter set.",
                "The top row shows how the solution time for the MILP scales as we increase the number of agents |M|, the global time horizon bτ, and the number of resources |Ω|.",
                "Increasing the number of agents leads to exponential complexity scaling, which is to be expected for an NP-complete problem.",
                "However, increasing the global time limit bτ or the total number of resource types |Ω|-while holding the number of agents constantdoes not lead to decreased performance.",
                "This occurs because the problems get easier as they become under-constrained, which is also a common phenomenon for NP-complete problems.",
                "We also observe that the solution to the dynamic version of the problem can often be computed much faster than the static version.",
                "The bottom row of Figure 3 shows the joint policy value of the policies that correspond to the computed optimal resource-allocation schedules.",
                "We can observe that the dynamic version yields higher reward (as expected, since the reward for the dynamic version is always no less than the reward of the static version).",
                "We should point out that these graphs should not be viewed as a measure of performance of two different algorithms (both algorithms produce optimal solutions but to different problems), but rather as observations about how the quality of optimal solutions change as more flexibility is allowed in the reallocation of resources.",
                "Figure 4 shows runtime and policy value for trials in which common input variables are scaled together.",
                "This allows The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1225 2 4 6 8 10 10 −3 10 −2 10 −1 10 0 10 1 10 2 10 3 10 4 Number of Agents |M| CPUTime,sec |Ω| = 5, τ = 50 static dynamic 50 100 150 200 10 −2 10 −1 10 0 10 1 10 2 10 3 Global Time Boundary τ CPUTime,sec |M| = 5, |Ω| = 5 static dynamic 10 20 30 40 50 10 −2 10 −1 10 0 10 1 10 2 Number of Resources |Ω| CPUTime,sec |M| = 5, τ = 50 static dynamic 2 4 6 8 10 200 400 600 800 1000 1200 1400 1600 Number of Agents |M| Value |Ω| = 5, τ = 50 static dynamic 50 100 150 200 400 500 600 700 800 900 1000 1100 1200 1300 1400 Global Time Boundary τ Value |M| = 5, |Ω| = 5 static dynamic 10 20 30 40 50 500 600 700 800 900 1000 1100 1200 1300 1400 Number of Resources |Ω| Value |M| = 5, τ = 50 static dynamic Figure 3: Evaluation of our MILP for variable numbers of agents (column 1), lengths of global-time window (column 2), and numbers of resource types (column 3).",
                "Top row shows CPU time, and bottom row shows the joint reward of agents MDP policies.",
                "Error bars show the 1st and 3rd quartiles (25% and 75%). 2 4 6 8 10 10 −3 10 −2 10 −1 10 0 10 1 10 2 10 3 Number of Agents |M| CPUTime,sec τ = 10|M| static dynamic 2 4 6 8 10 10 −3 10 −2 10 −1 10 0 10 1 10 2 10 3 10 4 Number of Agents |M| CPUTime,sec |Ω| = 2|M| static dynamic 2 4 6 8 10 10 −3 10 −2 10 −1 10 0 10 1 10 2 10 3 10 4 Number of Agents |M| CPUTime,sec |Ω| = 5|M| static dynamic 2 4 6 8 10 200 400 600 800 1000 1200 1400 1600 1800 2000 2200 Number of Agents |M| Value τ = 10|M| static dynamic 2 4 6 8 10 200 400 600 800 1000 1200 1400 1600 1800 2000 Number of Agents |M| Value |Ω| = 2|M| static dynamic 2 4 6 8 10 0 500 1000 1500 2000 2500 Number of Agents |M| Value |Ω| = 5|M| static dynamic Figure 4: Evaluation of our MILP using correlated input variables.",
                "The left column tracks the performance and CPU time as the number of agents and global-time window increase together (bτ = 10|M|).",
                "The middle and the right column track the performance and CPU time as the number of resources and the number of agents increase together as |Ω| = 2|M| and |Ω| = 5|M|, respectively.",
                "Error bars show the 1st and 3rd quartiles (25% and 75%). 1226 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) us to explore domains where the total number of agents scales proportionally to the total number of resource types or the global time horizon, while keeping constant the average agent density (per unit of global time) or the average number of resources per agent (which commonly occurs in real-life applications).",
                "Overall, we believe that these experimental results indicate that our MILP formulation can be used to effectively solve resource-scheduling problems of nontrivial size. 6.",
                "DISCUSSION AND CONCLUSIONS Throughout the paper, we have made a number of assumptions in our model and solution algorithm; we discuss their implications below. • Continual execution.",
                "We assume that once an agent stops executing its MDP (transitions into state sf ), it exits the system and cannot return.",
                "It is easy to relax this assumption for domains where agents MDPs can be paused and restarted.",
                "All that is required is to include an additional pause action which transitions from a given state back to itself, and has zero reward. • Indifference to start time.",
                "We used a reward model where agents rewards depend only on the time horizon of their MDPs and not the global start time.",
                "This is a consequence of our MDP-augmentation procedure from Section 4.1.",
                "It is easy to extend the model so that the agents incur an explicit penalty for idling by assigning a non-zero negative reward to the start state sb . • Binary resource requirements.",
                "For simplicity, we have assumed that resource costs are binary: ϕm(a, ω) = {0, 1}, but our results generalize in a straightforward manner to non-binary resource mappings, analogously to the procedure used in [5]. • Cooperative agents.",
                "The optimization procedure discussed in this paper was developed in the context of cooperative agents, but it can also be used to design a mechanism for scheduling resources among self-interested agents.",
                "This optimization procedure can be embedded in a VickreyClarke-Groves auction, completely analogously to the way it was done in [7].",
                "In fact, all the results of [7] about the properties of the auction and information privacy directly carry over to the scheduling domain discussed in this paper, requiring only slight modifications to deal with finitehorizon MDPs. • Known, deterministic arrival and departure times.",
                "Finally, we have assumed that agents arrival and departure times (τa m and τd m) are deterministic and known a priori.",
                "This assumption is fundamental to our solution method.",
                "While there are many domains where this assumption is valid, in many cases agents arrive and depart dynamically and their arrival and departure times can only be predicted probabilistically, leading to online resource-allocation problems.",
                "In particular, in the case of self-interested agents, this becomes an interesting version of an online-mechanism-design problem [11, 12].",
                "In summary, we have presented an MILP formulation for the combinatorial resource-scheduling problem where agents values for possible resource assignments are defined by finitehorizon MDPs.",
                "This result extends previous work ([6, 7]) on static one-shot resource allocation under MDP-induced preferences to resource-scheduling problems with a temporal aspect.",
                "As such, this work takes a step in the direction of designing an online mechanism for agents with combinatorial resource preferences induced by stochastic planning problems.",
                "Relaxing the assumption about deterministic arrival and departure times of the agents is a focus of our future work.",
                "We would like to thank the anonymous reviewers for their insightful comments and suggestions. 7.",
                "REFERENCES [1] E. Altman and A. Shwartz.",
                "Adaptive control of constrained Markov chains: Criteria and policies.",
                "Annals of Operations Research, special issue on Markov Decision Processes, 28:101-134, 1991. [2] R. Bellman.",
                "Dynamic Programming.",
                "Princeton University Press, 1957. [3] C. Boutilier.",
                "Solving concisely expressed combinatorial auction problems.",
                "In Proc. of AAAI-02, pages 359-366, 2002. [4] C. Boutilier and H. H. Hoos.",
                "Bidding languages for combinatorial auctions.",
                "In Proc. of IJCAI-01, pages 1211-1217, 2001. [5] D. Dolgov.",
                "Integrated Resource Allocation and Planning in Stochastic Multiagent Environments.",
                "PhD thesis, Computer Science Department, University of Michigan, February 2006. [6] D. A. Dolgov and E. H. Durfee.",
                "Optimal resource allocation and policy formulation in loosely-coupled Markov decision processes.",
                "In Proc. of ICAPS-04, pages 315-324, June 2004. [7] D. A. Dolgov and E. H. Durfee.",
                "Computationally efficient combinatorial auctions for resource allocation in weakly-coupled MDPs.",
                "In Proc. of AAMAS-05, New York, NY, USA, 2005.",
                "ACM Press. [8] D. A. Dolgov and E. H. Durfee.",
                "Resource allocation among agents with preferences induced by factored MDPs.",
                "In Proc. of AAMAS-06, 2006. [9] K. Larson and T. Sandholm.",
                "Mechanism design and deliberative agents.",
                "In Proc. of AAMAS-05, pages 650-656, New York, NY, USA, 2005.",
                "ACM Press. [10] N. Nisan.",
                "Bidding and allocation in combinatorial auctions.",
                "In Electronic Commerce, 2000. [11] D. C. Parkes and S. Singh.",
                "An MDP-based approach to Online Mechanism Design.",
                "In Proc. of the Seventeenths Annual Conference on Neural Information Processing Systems (NIPS-03), 2003. [12] D. C. Parkes, S. Singh, and D. Yanovsky.",
                "Approximately efficient online mechanism design.",
                "In Proc. of the Eighteenths Annual Conference on Neural Information Processing Systems (NIPS-04), 2004. [13] M. L. Puterman.",
                "Markov Decision Processes.",
                "John Wiley & Sons, New York, 1994. [14] M. H. Rothkopf, A. Pekec, and R. M. Harstad.",
                "Computationally manageable combinational auctions.",
                "Management Science, 44(8):1131-1147, 1998. [15] T. Sandholm.",
                "An algorithm for optimal winner determination in combinatorial auctions.",
                "In Proc. of IJCAI-99, pages 542-547, San Francisco, CA, USA, 1999.",
                "Morgan Kaufmann Publishers Inc.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1227"
            ],
            "original_annotated_samples": [
                "Combinatorial Resource Scheduling for Multiagent MDPs Dmitri A. Dolgov, Michael R. James, and Michael E. Samples AI and Robotics Group Technical Research, Toyota Technical Center USA {ddolgov, michael.r.james, michael.samples}@gmail.com ABSTRACT <br>optimal resource scheduling</br> in multiagent systems is a computationally challenging task, particularly when the values of resources are not additive.",
                "We address the problem of globally <br>optimal resource scheduling</br>, where the objective is to find an allocation of resources to the agents across time that maximizes the sum of the expected rewards that they obtain.",
                "In Section 4.2, we describe our main result, the optimization program for globally <br>optimal resource scheduling</br>.",
                "Only enabled for scheduling with static assignments. θm(τ) = 1 and θm(τ + 1) = 1 =⇒ Δm(τ, ω) = Δm(τ + 1, ω) Δm(τ, ω) − Z(1 − θm(τ + 1)) ≤ Δm(τ + 1, ω) + Z(1 − θm(τ)) Δm(τ, ω) + Z(1 − θm(τ + 1)) ≥ Δm(τ + 1, ω) − Z(1 − θm(τ)) ∀m ∈ M, ω ∈ Ω, τ ∈ [0, bτ] (11) Table 1: MILP for globally <br>optimal resource scheduling</br>."
            ],
            "translated_annotated_samples": [
                "La programación de recursos combinatoria para MDPs multiagentes. Dmitri A. Dolgov, Michael R. James y Michael E. Samples. Grupo de Investigación en IA y Robótica, Centro Técnico de Toyota en EE. UU. {ddolgov, michael.r.james, michael.samples}@gmail.com. RESUMEN La <br>programación óptima de recursos</br> en sistemas multiagentes es una tarea computacionalmente desafiante, especialmente cuando los valores de los recursos no son aditivos.",
                "Abordamos el problema de la <br>programación de recursos óptima</br> a nivel global, donde el objetivo es encontrar una asignación de recursos a los agentes a lo largo del tiempo que maximice la suma de las recompensas esperadas que obtienen.",
                "En la Sección 4.2, describimos nuestro resultado principal, el <br>programa de optimización para la programación de recursos globalmente óptima</br>.",
                "Solo habilitado para programación con asignaciones estáticas. θm(τ) = 1 y θm(τ + 1) = 1 =⇒ Δm(τ, ω) = Δm(τ + 1, ω) Δm(τ, ω) − Z(1 − θm(τ + 1)) ≤ Δm(τ + 1, ω) + Z(1 − θm(τ)) Δm(τ, ω) + Z(1 − θm(τ + 1)) ≥ Δm(τ + 1, ω) − Z(1 − θm(τ)) ∀m ∈ M, ω ∈ Ω, τ ∈ [0, bτ] (11) Tabla 1: MILP para <br>programación óptima global de recursos</br>."
            ],
            "translated_text": "La programación de recursos combinatoria para MDPs multiagentes. Dmitri A. Dolgov, Michael R. James y Michael E. Samples. Grupo de Investigación en IA y Robótica, Centro Técnico de Toyota en EE. UU. {ddolgov, michael.r.james, michael.samples}@gmail.com. RESUMEN La <br>programación óptima de recursos</br> en sistemas multiagentes es una tarea computacionalmente desafiante, especialmente cuando los valores de los recursos no son aditivos. Consideramos el problema combinatorio de programar el uso de múltiples recursos entre agentes que operan en entornos estocásticos, modelados como procesos de decisión de Markov (MDPs). En los últimos años, se han desarrollado algoritmos eficientes de asignación de recursos para agentes con valores de recursos inducidos por MDPs. Sin embargo, este trabajo previo se ha centrado en problemas de asignación de recursos estáticos donde los recursos se distribuyen una vez y luego se utilizan en MDPs de horizonte infinito. Extendemos esos modelos existentes al problema de la programación de recursos combinatoria, donde los agentes persisten solo por períodos finitos entre sus tiempos de llegada y salida (predefinidos), requiriendo recursos solo durante esos períodos de tiempo. Proporcionamos un procedimiento computacionalmente eficiente para calcular asignaciones de recursos óptimas a nivel global a agentes a lo largo del tiempo. Ilustramos y analizamos empíricamente el método en el contexto de un dominio de programación de trabajos estocásticos. Categorías y Descriptores de Asignaturas I.2.8 [Inteligencia Artificial]: Resolución de Problemas, Métodos de Control y Búsqueda; I.2.11 [Inteligencia Artificial]: Inteligencia Artificial Distribuida-Sistemas Multiagente Términos Generales Algoritmos, Rendimiento, Diseño 1. INTRODUCCIÓN Las tareas de asignación óptima de recursos y programación son ubicuas en sistemas multiagentes, pero resolver tales problemas de optimización puede ser computacionalmente difícil, debido a una serie de factores. En particular, cuando el valor de un conjunto de recursos para un agente no es aditivo (como suele ser el caso con recursos que son sustitutos o complementos), la función de utilidad podría tener que definirse en un espacio exponencialmente grande de combinaciones de recursos, lo que rápidamente se vuelve computacionalmente intratable. Además, incluso cuando cada agente tiene una función de utilidad que es distinta de cero solo en un pequeño subconjunto de los posibles conjuntos de recursos, obtener una asignación óptima sigue siendo computacionalmente prohibitivo, ya que el problema se vuelve NP-completo [14]. Tales problemas computacionales han dado lugar recientemente a varias líneas de trabajo en el uso de modelos compactos de las preferencias de los agentes. Una idea es utilizar cualquier estructura presente en las funciones de utilidad para representarlas de forma compacta, por ejemplo, a través de fórmulas lógicas [15, 10, 4, 3]. Una alternativa es modelar directamente los mecanismos que definen las funciones de utilidad de los agentes y realizar la asignación de recursos directamente con estos modelos [9]. Una forma de lograr esto es modelar los procesos mediante los cuales un agente podría utilizar los recursos y definir la función de utilidad como la ganancia de estos procesos. En particular, si un agente utiliza recursos para actuar en un entorno estocástico, su función de utilidad puede ser modelada de forma natural con un proceso de decisión de Markov, cuyo conjunto de acciones está parametrizado por los recursos disponibles. Esta representación puede ser utilizada para construir algoritmos de asignación de recursos muy eficientes que resultan en una aceleración exponencial sobre un problema de optimización directa con representaciones planas de preferencias combinatorias [6, 7, 8]. Sin embargo, este trabajo existente sobre asignación de recursos con preferencias inducidas por MDPs parametrizados por recursos hace una suposición de que los recursos se asignan solo una vez y luego son utilizados por los agentes de forma independiente dentro de sus MDPs de horizonte infinito. Esta suposición de que no es posible la reasignación de recursos puede ser limitante en dominios donde los agentes llegan y se van dinámicamente. En este artículo, ampliamos el trabajo sobre asignación de recursos bajo preferencias inducidas por MDP a problemas de programación en tiempo discreto, donde los agentes están presentes en el sistema durante intervalos de tiempo finitos y solo pueden utilizar recursos dentro de estos intervalos. En particular, los agentes llegan y parten en momentos arbitrarios (predefinidos) y dentro de estos intervalos utilizan recursos para ejecutar tareas en MDPs de horizonte finito. Abordamos el problema de la <br>programación de recursos óptima</br> a nivel global, donde el objetivo es encontrar una asignación de recursos a los agentes a lo largo del tiempo que maximice la suma de las recompensas esperadas que obtienen. En este contexto, nuestra principal contribución es una formulación de programación entera mixta del problema de programación que elige asignaciones de recursos globalmente óptimas, tiempos de inicio y horizontes de ejecución para todos los agentes (dentro de sus intervalos de llegada y salida). Analizamos y comparamos empíricamente dos variantes del problema de programación: uno, donde los agentes tienen asignaciones de recursos estáticas dentro de sus MDPs de horizonte finito, y otro, donde los recursos pueden ser realocados dinámicamente entre agentes en cada paso de tiempo. En el resto del documento, primero establecemos las bases necesarias en la Sección 2 y luego presentamos nuestro modelo y declaración formal del problema en la Sección 3. En la Sección 4.2, describimos nuestro resultado principal, el <br>programa de optimización para la programación de recursos globalmente óptima</br>. Tras la discusión de nuestros resultados experimentales sobre un problema de programación de trabajos en la Sección 5, concluimos en la Sección 6 con una discusión de posibles extensiones y generalizaciones de nuestro método. De manera similar al modelo utilizado en trabajos anteriores sobre asignación de recursos con preferencias inducidas por MDP [6, 7], definimos el valor de un conjunto de recursos para un agente como el valor de la mejor política MDP que es realizable, dadas esos recursos. Sin embargo, dado que el enfoque de nuestro trabajo se centra en problemas de programación, y una gran parte del problema de optimización consiste en decidir cómo se asignan los recursos en el tiempo entre agentes con tiempos finitos de llegada y salida, modelamos los problemas de planificación de los agentes como MDPs de horizonte finito, en contraste con trabajos anteriores que utilizaron MDPs descontados de horizonte infinito. En el resto de esta sección, primero presentamos algo de antecedentes necesarios sobre MDPs de horizonte finito y presentamos una formulación de programación lineal que sirve como base para nuestro algoritmo de solución desarrollado en la Sección 4. También describimos los métodos estándar para la programación de recursos combinatorios con valores de recursos planos, que sirven como punto de referencia de comparación para el nuevo modelo desarrollado aquí. 2.1 Procesos de Decisión de Markov Un MDP (Proceso de Decisión de Markov) estacionario, de dominio finito y discreto en el tiempo (ver, por ejemplo, [13] para un desarrollo detallado y exhaustivo) se puede describir como S, A, p, r, donde: S es un conjunto finito de estados del sistema; A es un conjunto finito de acciones disponibles para el agente; p es una función de transición estocástica estacionaria, donde p(σ|s, a) es la probabilidad de transicionar al estado σ al ejecutar la acción a en el estado s; r es una función de recompensa estacionaria, donde r(s, a) especifica la recompensa obtenida al ejecutar la acción a en el estado s. Dado un MDP de este tipo, un problema de decisión bajo un horizonte finito T consiste en elegir una acción óptima en cada paso de tiempo para maximizar el valor esperado de la recompensa total acumulada durante la vida (finita) de los agentes. La política óptima del agente es entonces una función del estado actual s y el tiempo hasta el horizonte. Una política óptima para tal problema es actuar de forma codiciosa con respecto a la función de valor óptimo, definida de forma recursiva por el siguiente sistema de ecuaciones de Bellman de tiempo finito [2]: v(s, t) = max a r(s, a) + X σ p(σ|s, a)v(σ, t + 1), ∀s ∈ S, t ∈ [1, T − 1]; v(s, T) = 0, ∀s ∈ S; donde v(s, t) es el valor óptimo de estar en el estado s en el tiempo t ∈ [1, T]. Esta función de valor óptimo se puede calcular fácilmente utilizando programación dinámica, lo que conduce a la siguiente política óptima π, donde π(s, a, t) es la probabilidad de ejecutar la acción a en el estado s en el tiempo t: π(s, a, t) = (1, a = argmaxa r(s, a) + P σ p(σ|s, a)v(σ, t + 1), 0, en caso contrario. La anterior es la forma más común de calcular la función de valor óptimo (y por lo tanto una política óptima) para un MDP de horizonte finito. Sin embargo, también podemos formular el problema como el siguiente programa lineal (similarmente al LP dual para MDPs descontados de horizonte infinito [13, 6, 7]): max X s X a r(s, a) X t x(s, a, t) sujeto a: X a x(σ, a, t + 1) = X s,a p(σ|s, a)x(s, a, t) ∀σ, t ∈ [1, T − 1]; X a x(s, a, 1) = α(s), ∀s ∈ S; (1) donde α(s) es la distribución inicial sobre el espacio de estados, y x es la medida de ocupación (no estacionaria) (x(s, a, t) ∈ [0, 1] es el número total esperado de veces que se ejecuta la acción a en el estado s en el tiempo t). Una política óptima (no estacionaria) se obtiene a partir de la medida de ocupación de la siguiente manera: π(s, a, t) = x(s, a, t)/ X a x(s, a, t) ∀s ∈ S, t ∈ [1, T]. (2) Tenga en cuenta que el MDP estándar de horizonte finito sin restricciones, como se describe anteriormente, siempre tiene una solución uniformemente óptima (óptima para cualquier distribución inicial α(s)). Por lo tanto, una política óptima se puede obtener utilizando una constante arbitraria α(s) > 0 (en particular, α(s) = 1 resultará en x(s, a, t) = π(s, a, t)). Sin embargo, para los MDPs con restricciones de recursos (como se definen a continuación en la Sección 3), las políticas óptimas uniformemente no existen en general. En tales casos, α se convierte en parte de la entrada del problema, y una política resultante solo es óptima para ese α en particular. Este resultado es bien conocido para MDPs de horizonte infinito con varios tipos de restricciones [1, 6], y también se cumple para nuestro modelo de horizonte finito, el cual puede establecerse fácilmente a través de una línea de razonamiento completamente análoga a los argumentos en [6]. 2.2 Programación de Recursos Combinatoria Un enfoque directo para la programación de recursos para un conjunto de agentes M, cuyos valores para los recursos son inducidos por problemas de planificación estocástica (en nuestro caso, MDPs de horizonte finito) sería que cada agente enumerara todas las asignaciones de recursos posibles a lo largo del tiempo y, para cada una, calcular su valor resolviendo el MDP correspondiente. Entonces, cada agente proporcionaría valoraciones para cada posible conjunto de recursos a lo largo del tiempo a un coordinador centralizado, quien calcularía las asignaciones óptimas de recursos a lo largo del tiempo basándose en estas valoraciones. Cuando los recursos pueden asignarse en diferentes momentos a diferentes agentes, cada agente debe presentar valoraciones para cada combinación de posibles horizontes temporales. Que cada agente m ∈ M ejecute su MDP dentro del intervalo de tiempo de llegada-salida τ ∈ [τa m, τd m]. Por lo tanto, el agente m ejecutará un MDP con un horizonte temporal no mayor que Tm = τd m−τa m+1. Que bτ sea el horizonte temporal global para el problema, antes del cual todos los MDP de los agentes deben finalizar. Suponemos que τd m < bτ, ∀m ∈ M. El Sexto Congreso Internacional. En el problema de programación donde los agentes tienen requisitos de recursos estáticos dentro de sus MDPs de horizonte finito, los agentes proporcionan una valoración para cada conjunto de recursos para cada horizonte de tiempo posible (desde [1, Tm]) que puedan utilizar. Sea Ω el conjunto de recursos a ser asignados entre los agentes. Un agente recibirá como máximo un paquete de recursos para uno de los horizontes temporales. Que la variable ψ ∈ Ψm enumere todos los posibles pares de conjuntos de recursos y horizontes temporales para el agente m, por lo que hay 2|Ω| × Tm valores para ψ (el espacio de conjuntos es exponencial en el número de tipos de recursos |Ω|). El agente m debe proporcionar un valor vψ m para cada ψ, y el coordinador asignará como máximo un par (recurso, horizonte temporal) ψ a cada agente. Esta asignación se expresa como una variable indicadora zψ m ∈ {0, 1} que muestra si ψ está asignado al agente m. Para el tiempo τ y el recurso ω, la función nm(ψ, τ, ω) ∈ {0, 1} indica si el paquete en ψ utiliza el recurso ω en el tiempo τ (hacemos la suposición de que los agentes tienen requisitos de recursos binarios). Este problema de asignación es NP-completo, incluso al considerar solo un paso de tiempo, y su dificultad aumenta significativamente con múltiples pasos de tiempo debido al creciente número de valores de ψ. El problema de encontrar una asignación óptima que satisfaga la restricción global de que la cantidad de cada recurso ω asignada a todos los agentes no exceda la cantidad disponible bϕ(ω) se puede expresar como el siguiente programa entero: max X m∈M X ψ∈Ψm zψ mvψ m sujeto a: X ψ∈Ψm zψ m ≤ 1, ∀m ∈ M; X m∈M X ψ∈Ψm zψ mnm(ψ, τ, ω) ≤ bϕ(ω), ∀τ ∈ [1, bτ], ∀ω ∈ Ω; (3) La primera restricción en la ecuación 3 dice que ningún agente puede recibir más de un paquete, y la segunda restricción asegura que la asignación total del recurso ω no exceda, en ningún momento, el límite del recurso. Para el problema de programación en el que los agentes pueden reasignar dinámicamente recursos, cada agente debe especificar un valor para cada combinación de paquetes y pasos de tiempo dentro de su horizonte temporal. Que la variable ψ ∈ Ψm en este caso enumere todos los posibles conjuntos de recursos para los cuales a lo sumo un conjunto puede ser asignado al agente m en cada paso de tiempo. Por lo tanto, en este caso hay P t∈[1,Tm](2|Ω| )t ∼ 2|Ω|Tm posibilidades de combinaciones de recursos asignadas a diferentes intervalos de tiempo, para los Tm horizontes de tiempo diferentes. El mismo conjunto de ecuaciones (3) se puede utilizar para resolver este problema de programación dinámica, pero el programa entero es diferente debido a la diferencia en cómo se define ψ. En este caso, el número de valores de ψ es exponencial en el horizonte de planificación Tm de cada agente, lo que resulta en un programa mucho más grande. Este enfoque directo para resolver ambos problemas de programación requiere una enumeración y solución de 2|Ω| Tm (asignación estática) o P t∈[1,Tm] 2|Ω|t (reubicación dinámica) MDPs para cada agente, lo cual rápidamente se vuelve intratable con el crecimiento del número de recursos |Ω| o el horizonte de tiempo Tm. MODELO Y DECLARACIÓN DEL PROBLEMA Ahora presentamos formalmente nuestro modelo del problema de programación de recursos. La entrada del problema consiste en los siguientes componentes: • M, Ω, bϕ, τa m, τd m, bτ están definidos arriba en la Sección 2.2. • {Θm} = {S, A, pm, rm, αm} son los MDPs de todos los agentes m ∈ M. Sin pérdida de generalidad, asumimos que los espacios de estados y acciones de todos los agentes son iguales, pero cada uno tiene su propia función de transición pm, función de recompensa rm, y condiciones iniciales αm. • ϕm : A×Ω → {0, 1} es la asignación de acciones a recursos para el agente m. ϕm(a, ω) indica si la acción a del agente m necesita el recurso ω. Un agente m que recibe un conjunto de recursos que no incluye el recurso ω no puede ejecutar en su política MDP ninguna acción a para la cual ϕm(a, ω) = 0. Suponemos que todos los requisitos de recursos son binarios; como se discute más adelante en la Sección 6, esta suposición no es limitante. Dado el input anterior, el problema de optimización que consideramos es encontrar la asignación globalmente óptima que maximice la suma de recompensas esperadas de recursos a agentes para todos los pasos de tiempo: Δ : τ × M × Ω → {0, 1}. Una solución es factible si la asignación correspondiente de recursos a los agentes no viola la restricción global de recursos: X m Δm(τ, ω) ≤ bϕ(ω), ∀ω ∈ Ω, τ ∈ [1, bτ]. Consideramos dos variantes del problema de programación de recursos. La primera formulación restringe las asignaciones de recursos al espacio donde la asignación de recursos a cada agente es estática durante la vida de los agentes. La segunda formulación permite la reasignación de recursos entre agentes en cada paso de tiempo durante sus vidas. La Figura 1 representa un problema de programación de recursos con tres agentes M = {m1, m2, m3}, tres recursos Ω = {ω1, ω2, ω3}, y un horizonte de problema global de bτ = 11. Los tiempos de llegada y salida de los agentes se muestran como cajas grises y son {1, 6}, {3, 7} y {2, 11}, respectivamente. Una solución a este problema se muestra a través de barras horizontales dentro de cada casilla de agentes, donde las barras corresponden a la asignación de los tres tipos de recursos. La Figura 1a muestra una solución a un problema de programación estática. Según la solución mostrada, el agente m1 comienza la ejecución de su MDP en el tiempo τ = 1 y tiene un bloqueo en los tres recursos hasta que finaliza la ejecución en el tiempo τ = 3. Se debe tener en cuenta que el agente m1 renuncia a su control sobre los recursos antes de su hora de salida anunciada de τd m1 = 6, aparentemente porque otros agentes pueden utilizar los recursos de manera más efectiva. Por lo tanto, en el tiempo τ = 4, los recursos ω1 y ω3 se asignan al agente m2, quien luego los utiliza para ejecutar su MDP (utilizando solo acciones admitidas por los recursos ω1 y ω3) hasta el tiempo τ = 7. El agente m3 posee el recurso ω3 durante el intervalo τ ∈ [4, 10]. La Figura 1b muestra una posible solución a la versión dinámica del mismo problema. Allí, los recursos pueden ser reasignados entre agentes en cada paso de tiempo. Por ejemplo, el agente m1 renuncia a su uso del recurso ω2 en el tiempo τ = 2, aunque continúa la ejecución de su MDP hasta el tiempo τ = 6. Ten en cuenta que a un agente no se le permite detener y reiniciar su MDP, por lo que el agente m1 solo puede continuar ejecutando en el intervalo τ ∈ [3, 4] si tiene acciones que no requieren recursos (ϕm(a, ω) = 0). Claramente, el modelo y la declaración del problema descritos anteriormente hacen una serie de suposiciones sobre el problema y las propiedades de la solución deseada. Discutimos algunas de esas suposiciones y sus implicaciones en la Sección 6. 1222 La Sexta Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) (a) (b) Figura 1: Ilustración de una solución a un problema de programación de recursos con tres agentes y tres recursos: a) asignaciones de recursos estáticas (las asignaciones de recursos son constantes durante la vida de los agentes); b) asignación dinámica (las asignaciones de recursos pueden cambiar en cada paso de tiempo). 4. PROGRAMACIÓN DE RECURSOS Nuestro algoritmo de programación de recursos se lleva a cabo en dos etapas. Primero, realizamos un paso de preprocesamiento que aumenta los MDP de los agentes; este proceso se describe en la Sección 4.1. Segundo, utilizando estos MDPs aumentados construimos un problema de optimización global, el cual se describe en la Sección 4.2. 4.1 Aumentando los MDPs de los Agentes En el modelo descrito en la sección anterior, asumimos que si un agente no posee los recursos necesarios para realizar acciones en su MDP, su ejecución se detiene y el agente abandona el sistema. En otras palabras, los MDPs no pueden ser pausados y reanudados. Por ejemplo, en el problema mostrado en la Figura 1a, el agente m1 libera todos los recursos después del tiempo τ = 3, momento en el cual la ejecución de su MDP se detiene. De manera similar, los agentes m2 y m3 solo ejecutan sus MDPs en los intervalos τ ∈ [4, 6] y τ ∈ [4, 10], respectivamente. Por lo tanto, una parte importante del problema de toma de decisiones global es decidir la ventana de tiempo durante la cual cada uno de los agentes está activo (es decir, ejecutando su MDP). Para lograr esto, ampliamos el MDP de cada agente con dos nuevos estados (estados de inicio y final sb, sf, respectivamente) y una nueva acción de inicio/parada a∗, como se ilustra en la Figura 2. La idea es que un agente permanezca en el estado inicial sb hasta que esté listo para ejecutar su MDP, momento en el que realiza la acción de inicio/parada a∗ y transita al espacio de estados del MDP original con la probabilidad de transición que corresponde a la distribución inicial original α(s). Por ejemplo, en la Figura 1a, para el agente m2 esto ocurriría en el tiempo τ = 4. Una vez que el agente llega al final de su ventana de actividad (tiempo τ = 6 para el agente m2 en la Figura 1a), realiza la acción de inicio/parada, lo que lo lleva al estado final de finalización del sumidero sf en el tiempo τ = 7. Más precisamente, dado un MDP S, A, pm, rm, αm, definimos un MDP aumentado S, A, pm, rm, αm de la siguiente manera: S = S ∪ sb ∪ sf; A = A ∪ a∗; p(s|sb, a∗) = α(s), ∀s ∈ S; p(sb|sb, a) = 1.0, ∀a ∈ A; p(sf|s, a∗) = 1.0, ∀s ∈ S; p(σ|s, a) = p(σ|s, a), ∀s, σ ∈ S, a ∈ A; r(sb, a) = r(sf, a) = 0, ∀a ∈ A; r(s, a) = r(s, a), ∀s ∈ S, a ∈ A; α(sb) = 1; α(s) = 0, ∀s ∈ S; donde se asume que todas las probabilidades de transición no especificadas son cero. Además, para tener en cuenta el nuevo estado inicial, comenzamos el MDP un paso de tiempo antes, estableciendo τa m ← τa m − 1. Esto no afectará la asignación de recursos debido a que las restricciones de recursos solo se aplican a los estados originales del MDP, como se discutirá en la siguiente sección. Por ejemplo, los MDPs aumentados mostrados en la Figura 2b (que comienza en el estado sb en el tiempo τ = 2) se construirían a partir de un MDP con tiempo de llegada original τ = 3. La Figura 2b también muestra una trayectoria de muestra a través del espacio de estados: el agente comienza en el estado sb, transita al espacio de estados S del MDP original y finalmente sale al estado de sumidero sf. Cabe destacar que si quisiéramos modelar un problema en el que los agentes pudieran pausar sus MDP en pasos de tiempo arbitrarios (lo cual podría ser útil para dominios donde la reasignación dinámica es posible), podríamos lograrlo fácilmente incluyendo una acción adicional que transiciona de cada estado a sí mismo con una recompensa de cero. 4.2 MILP para la programación de recursos Dado un conjunto de MDPs aumentados, como se define arriba, el objetivo de esta sección es formular un programa de optimización global que resuelva el problema de programación de recursos. En esta sección y a continuación, se asume que todos los MDPs son los MDPs aumentados tal como se definen en la Sección 4.1. Nuestro enfoque es similar a la idea utilizada en [6]: comenzamos con la formulación del programa lineal de los MDP de los agentes (1) y lo complementamos con restricciones que aseguran que la asignación de recursos correspondiente entre agentes y tiempo sea válida. El problema de optimización resultante resuelve simultáneamente los MDP de los agentes y los problemas de programación de recursos. En el resto de esta sección, desarrollamos de forma incremental un programa de enteros mixtos (MILP) que logra esto. En ausencia de restricciones de recursos, los MDPs de horizonte finito de los agentes son completamente independientes, y la solución óptima global puede obtenerse trivialmente a través del siguiente LP, que es simplemente una agregación de LPs de horizonte finito de un solo agente: max X m X s X a rm(s, a) X t xm(s, a, t) sujeto a: X a xm(σ, a, t + 1) = X s,a pm(σ|s, a)xm(s, a, t), ∀m ∈ M, σ ∈ S, t ∈ [1, Tm − 1]; X a xm(s, a, 1) = αm(s), ∀m ∈ M, s ∈ S; (12) donde xm(s, a, t) es la medida de ocupación del agente m, y The Sixth Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 1223 (a) (b) Figura 2: Ilustración de la ampliación de un MDP para permitir tiempos de inicio y finalización variables: a) (izquierda) el MDP original de dos estados con una sola acción; (derecha) el MDP ampliado con nuevos estados sb y sf y la nueva acción a∗ (nota que las transiciones originales no se modifican en el proceso de ampliación); b) el MDP ampl augmentado mostrado como una trayectoria a través del tiempo (las líneas grises indican todas las transiciones, mientras que las líneas negras indican una trayectoria dada). Función objetivo (suma de recompensas esperadas de todos los agentes) máx X m X s X a rm(s, a) X t xm(s, a, t) (5) Implicación de significado Restricciones lineales Atar x a θ. El agente solo está activo cuando la medida de ocupación es distinta de cero en los estados originales del MDP. θm(τ) = 0 =⇒ xm(s, a, τ −τa m+1) = 0 ∀s /∈ {sb , sf }, a ∈ A X s/∈{sb,sf } X a xm(s, a, t) ≤ θm(τa m + t − 1) ∀m ∈ M, ∀t ∈ [1, Tm] (6) El agente solo puede estar activo en τ ∈ (τa m, τd m) θm(τ) = 0 ∀m ∈ M, τ /∈ (τa m, τd m) (7) No se pueden utilizar recursos cuando no está activo θm(τ) = 0 =⇒ Δm(τ, ω) = 0 ∀τ ∈ [0, bτ], ω ∈ Ω Δm(τ, ω) ≤ θm(τ) ∀m ∈ M, τ ∈ [0, bτ], ω ∈ Ω (8) Vincular x a Δ (x distinto de cero obliga a que el correspondiente Δ sea distinto de cero). Δm(τ, ω) = 0, ϕm(a, ω) = 1 =⇒ xm(s, a, τ − τa m + 1) = 0 ∀s /∈ {sb , sf } 1/|A| X a ϕm(a, ω) X s/∈{sb,sf } xm(s, a, t) ≤ Δm(t + τa m − 1, ω) ∀m ∈ M, ω ∈ Ω, t ∈ [1, Tm] (9) Límites de recursos X m Δm(τ, ω) ≤ bϕ(ω) ∀ω ∈ Ω, τ ∈ [0, bτ] (10) El agente no puede cambiar los recursos mientras está activo. Solo habilitado para programación con asignaciones estáticas. θm(τ) = 1 y θm(τ + 1) = 1 =⇒ Δm(τ, ω) = Δm(τ + 1, ω) Δm(τ, ω) − Z(1 − θm(τ + 1)) ≤ Δm(τ + 1, ω) + Z(1 − θm(τ)) Δm(τ, ω) + Z(1 − θm(τ + 1)) ≥ Δm(τ + 1, ω) − Z(1 − θm(τ)) ∀m ∈ M, ω ∈ Ω, τ ∈ [0, bτ] (11) Tabla 1: MILP para <br>programación óptima global de recursos</br>. Tm = τd m − τa m + 1 es el horizonte temporal para los agentes MDP. Usando este LP como base, lo ampliamos con restricciones que aseguran que el uso de recursos implicado por las medidas de ocupación de los agentes {xm} no viola los requisitos globales de recursos bϕ en ningún paso de tiempo τ ∈ [0, bτ]. Para formular estas restricciones de recursos, utilizamos las siguientes variables binarias: • Δm(τ, ω) = {0, 1}, ∀m ∈ M, τ ∈ [0, bτ], ω ∈ Ω, que sirven como variables indicadoras que definen si el agente m posee el recurso ω en el tiempo τ. Estos son análogos a las variables indicadoras estáticas utilizadas en el problema de asignación de recursos estáticos de una sola vez en [6]. • θm = {0, 1}, ∀m ∈ M, τ ∈ [0, bτ] son variables indicadoras que especifican si el agente m está activo (es decir, ejecutando su MDP) en el tiempo τ. El significado de las variables de uso de recursos Δ se ilustra en la Figura 1: Δm(τ, ω) = 1 solo si el recurso ω se asigna al agente m en el tiempo τ. El significado de los indicadores de actividad θ se ilustra en la Figura 2b: cuando el agente m se encuentra en el estado inicial sb o en el estado final sf, el θm correspondiente es igual a 0, pero una vez que el agente se vuelve activo y entra en uno de los otros estados, establecemos θm = 1. Este significado de θ puede ser impuesto con una restricción lineal que sincroniza los valores de las medidas de ocupación de los agentes xm y la actividad 1224 The Sixth Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) indicadores θ, como se muestra en (6) en la Tabla 1. Otra restricción que debemos agregar, debido a que los indicadores de actividad θ están definidos en la línea de tiempo global τ, es hacer cumplir el hecho de que el agente está inactivo fuera de su ventana de llegada y salida. Esto se logra mediante la restricción (7) en la Tabla 1. Además, los agentes no deben estar utilizando recursos mientras están inactivos. Esta restricción también puede ser impuesta a través de una desigualdad lineal en θ y Δ, como se muestra en (8). La restricción (6) establece el valor de θ para que coincida con la política definida por la medida de ocupación xm. De manera similar, debemos asegurarnos de que las variables de uso de recursos Δ estén también sincronizadas con la medida de ocupación xm. Esto se realiza a través de la restricción (9) en la Tabla 1, que es casi idéntica a la restricción análoga de [6]. Después de implementar la restricción anterior, que refuerza el significado de Δ, agregamos una restricción que garantiza que el uso de recursos de los agentes nunca exceda los montos de recursos disponibles. Esta condición también se expresa trivialmente como una desigualdad lineal (10) en la Tabla 1. Finalmente, para la formulación del problema en la que las asignaciones de recursos son estáticas durante la vida de un agente, agregamos una restricción que garantiza que las variables de uso de recursos Δ no cambien su valor mientras el agente está activo (θ = 1). Esto se logra a través de la restricción lineal (11), donde Z ≥ 2 es una constante que se utiliza para desactivar las restricciones cuando θm(τ) = 0 o θm(τ + 1) = 0. Esta restricción no se utiliza para la formulación del problema dinámico, donde los recursos pueden ser reasignados entre agentes en cada paso de tiempo. En resumen, la Tabla 1 junto con las restricciones de conservación de flujo de (12) define el MILP que calcula simultáneamente una asignación óptima de recursos para todos los agentes a lo largo del tiempo, así como políticas óptimas de MDP de horizonte finito que son válidas bajo esa asignación de recursos. Como una medida aproximada de la complejidad de este MILP, consideremos el número de variables de optimización y restricciones. Sea TM = P Tm = P m(τa m − τd m + 1) la suma de las longitudes de las ventanas de llegada-salida a través de todos los agentes. Entonces, el número de variables de optimización es: TM + bτ|M||Ω| + bτ|M|, de las cuales TM son continuas (xm), y bτ|M||Ω| + bτ|M| son binarias (Δ y θ). Sin embargo, hay que tener en cuenta que todos menos TM|M| de los θ se establecen en cero por la restricción (7), lo que también obliga inmediatamente a que todos menos TM|M||Ω| de los Δ sean cero a través de las restricciones (8). El número de restricciones (sin incluir las restricciones degeneradas en (7)) en el MILP es: TM + TM|Ω| + bτ|Ω| + bτ|M||Ω|. A pesar de que la complejidad del MILP es, en el peor de los casos, exponencial en el número de variables binarias, la complejidad de este MILP es significativamente (exponencialmente) menor que la del MILP con funciones de utilidad planas, descrito en la Sección 2.2. Este resultado refleja las ganancias de eficiencia reportadas en [6] para problemas de asignación de recursos de un solo disparo, pero es mucho más pronunciado, debido a la explosión de la representación de utilidad plana debido al aspecto temporal del problema (recuerde la complejidad prohibitiva de la optimización combinatoria en la Sección 2.2). Analizamos empíricamente el rendimiento de este método en la Sección 5.1. Estrictamente hablando, resolver MILPs hasta la optimalidad es NP-completo en el número de variables enteras. RESULTADOS EXPERIMENTALES Aunque la complejidad de resolver MILPs es en el peor de los casos exponencial en el número de variables enteras, existen muchos métodos eficientes para resolver MILPs que permiten que nuestro algoritmo se adapte bien a parámetros comunes en problemas de asignación de recursos y programación. En particular, esta sección introduce un dominio de problema: el problema del taller de reparación, que se utiliza para evaluar empíricamente la escalabilidad de nuestros algoritmos en términos del número de agentes |M|, el número de recursos compartidos |Ω| y las diferentes longitudes de tiempo global bτ durante las cuales los agentes pueden entrar y salir del sistema. El problema del taller de reparaciones es un MDP parametrizado simple que adopta la metáfora de un taller de reparaciones de vehículos. Los agentes en el taller de reparación son mecánicos con una serie de tareas independientes que solo generan recompensa cuando se completan. En nuestro modelo MDP de este sistema, las acciones tomadas para avanzar a través del espacio de estados solo están permitidas si el agente posee ciertos recursos que están públicamente disponibles para la tienda. Estos recursos son de suministro finito, y las políticas óptimas para la tienda determinarán cuándo cada agente puede retener los recursos limitados para tomar acciones y obtener recompensas individuales. Cada tarea a completar está asociada con una sola acción, aunque se requiere que el agente repita la acción numerosas veces antes de completar la tarea y obtener una recompensa. Este modelo fue parametrizado en términos del número de agentes en el sistema, el número de diferentes tipos de recursos que podrían estar vinculados a acciones necesarias, un tiempo global durante el cual se permite a los agentes llegar y partir, y una longitud máxima para el número de pasos de tiempo que un agente puede permanecer en el sistema. Todos los puntos de datos en nuestros experimentos se obtuvieron con 20 evaluaciones utilizando CPLEX para resolver los MILPs en una computadora Pentium4 con 2Gb de RAM. Se realizaron pruebas tanto en la versión estática como en la dinámica del problema de programación de recursos, tal como se definió anteriormente. La Figura 3 muestra el tiempo de ejecución y el valor de la política para modificaciones independientes en el conjunto de parámetros. La fila superior muestra cómo el tiempo de solución para el MILP escala a medida que aumentamos el número de agentes |M|, el horizonte de tiempo global bτ y el número de recursos |Ω|. Aumentar el número de agentes conduce a una escalabilidad exponencial en complejidad, lo cual es de esperarse para un problema NP-completo. Sin embargo, aumentar el límite de tiempo global bτ o el número total de tipos de recursos |Ω|, manteniendo constante el número de agentes, no conduce a una disminución en el rendimiento. Esto ocurre porque los problemas se vuelven más fáciles a medida que se vuelven sub-determinados, lo cual también es un fenómeno común en problemas NP-completos. También observamos que la solución a la versión dinámica del problema a menudo se puede calcular mucho más rápido que la versión estática. La fila inferior de la Figura 3 muestra el valor de la política conjunta de las políticas que corresponden a los horarios óptimos de asignación de recursos calculados. Podemos observar que la versión dinámica produce una recompensa más alta (como era de esperar, ya que la recompensa de la versión dinámica siempre es igual o mayor que la recompensa de la versión estática). Deberíamos señalar que estos gráficos no deben ser vistos como una medida del rendimiento de dos algoritmos diferentes (ambos algoritmos producen soluciones óptimas pero para problemas distintos), sino más bien como observaciones sobre cómo cambia la calidad de las soluciones óptimas a medida que se permite más flexibilidad en la reasignación de recursos. La Figura 4 muestra el tiempo de ejecución y el valor de la política para las pruebas en las que las variables de entrada comunes se escalan juntas. Esto permite la Sexta Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 1225 2 4 6 8 10 10 −3 10 −2 10 −1 10 0 10 1 10 2 10 3 10 4 Número de Agentes |M| Tiempo de CPU, seg |Ω| = 5, τ = 50 estático dinámico 50 100 150 200 10 −2 10 −1 10 0 10 1 10 2 10 3 Límite de Tiempo Global τ Tiempo de CPU, seg |M| = 5, |Ω| = 5 estático dinámico 10 20 30 40 50 10 −2 10 −1 10 0 10 1 10 2 Número de Recursos |Ω| Tiempo de CPU, seg |M| = 5, τ = 50 estático dinámico 2 4 6 8 10 200 400 600 800 1000 1200 1400 1600 Número de Agentes |M| Valor |Ω| = 5, τ = 50 estático dinámico 50 100 150 200 400 500 600 700 800 900 1000 1100 1200 1300 1400 Límite de Tiempo Global τ Valor |M| = 5, |Ω| = 5 estático dinámico 10 20 30 40 50 500 600 700 800 900 1000 1100 1200 1300 1400 Número de Recursos |Ω| Valor |M| = 5, τ = 50 estático dinámico Figura 3: Evaluación de nuestro MILP para números variables de agentes (columna 1), longitudes de ventana de tiempo global (columna 2) y números de tipos de recursos (columna 3). La fila superior muestra el tiempo de CPU, y la fila inferior muestra la recompensa conjunta de las políticas MDP de los agentes. Las barras de error muestran los cuartiles 1ro y 3ro (25% y 75%). 2 4 6 8 10 10 −3 10 −2 10 −1 10 0 10 1 10 2 10 3 Número de Agentes |M| Tiempo de CPU, seg τ = 10|M| estático dinámico 2 4 6 8 10 10 −3 10 −2 10 −1 10 0 10 1 10 2 10 3 10 4 Número de Agentes |M| Tiempo de CPU, seg |Ω| = 2|M| estático dinámico 2 4 6 8 10 10 −3 10 −2 10 −1 10 0 10 1 10 2 10 3 10 4 Número de Agentes |M| Tiempo de CPU, seg |Ω| = 5|M| estático dinámico 2 4 6 8 10 200 400 600 800 1000 1200 1400 1600 1800 2000 2200 Número de Agentes |M| Valor τ = 10|M| estático dinámico 2 4 6 8 10 200 400 600 800 1000 1200 1400 1600 1800 2000 Número de Agentes |M| Valor |Ω| = 2|M| estático dinámico 2 4 6 8 10 0 500 1000 1500 2000 2500 Número de Agentes |M| Valor |Ω| = 5|M| estático dinámico Figura 4: Evaluación de nuestro MILP utilizando variables de entrada correlacionadas. La columna izquierda sigue el rendimiento y el tiempo de CPU a medida que aumenta el número de agentes y la ventana de tiempo global juntos (bτ = 10|M|). La columna del medio y la columna de la derecha siguen el rendimiento y el tiempo de CPU a medida que aumenta el número de recursos y el número de agentes juntos, con |Ω| = 2|M| y |Ω| = 5|M|, respectivamente. Las barras de error muestran el primer y tercer cuartil (25% y 75%). 1226 The Sixth Intl. La Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) nos permite explorar dominios donde el número total de agentes escala proporcionalmente al número total de tipos de recursos o al horizonte temporal global, manteniendo constante la densidad promedio de agentes (por unidad de tiempo global) o el número promedio de recursos por agente (lo cual comúnmente ocurre en aplicaciones de la vida real). En general, creemos que estos resultados experimentales indican que nuestra formulación MILP puede ser utilizada para resolver de manera efectiva problemas de programación de recursos de tamaño no trivial. DISCUSIÓN Y CONCLUSIONES A lo largo del documento, hemos realizado una serie de suposiciones en nuestro modelo y algoritmo de solución; discutimos sus implicaciones a continuación. • Ejecución continua. Suponemos que una vez que un agente deja de ejecutar su MDP (transiciona al estado sf), sale del sistema y no puede regresar. Es fácil relajar esta suposición para dominios donde los agentes de MDP pueden ser pausados y reiniciados. Todo lo que se requiere es incluir una acción de pausa adicional que haga la transición desde un estado dado de regreso a sí mismo, y que tenga una recompensa de cero. • Indiferencia al tiempo de inicio. Utilizamos un modelo de recompensa donde las recompensas de los agentes dependen solo del horizonte temporal de sus MDP y no del tiempo de inicio global. Esta es una consecuencia de nuestro procedimiento de aumento de MDP de la Sección 4.1. Es fácil extender el modelo para que los agentes incurran en una penalización explícita por inactividad al asignar una recompensa negativa no nula al estado inicial sb. • Requisitos de recursos binarios. Para simplificar, hemos asumido que los costos de recursos son binarios: ϕm(a, ω) = {0, 1}, pero nuestros resultados se generalizan de manera directa a asignaciones de recursos no binarias, de manera análoga al procedimiento utilizado en [5]. • Agentes cooperativos. El procedimiento de optimización discutido en este artículo fue desarrollado en el contexto de agentes cooperativos, pero también puede ser utilizado para diseñar un mecanismo para programar recursos entre agentes egoístas. Este procedimiento de optimización puede ser incorporado en una subasta de Vickrey-Clarke-Groves, de manera completamente análoga a como se hizo en [7]. De hecho, todos los resultados de [7] sobre las propiedades de la subasta y la privacidad de la información se aplican directamente al dominio de programación discutido en este documento, requiriendo solo ligeras modificaciones para tratar con MDPs de horizonte finito. • Tiempos de llegada y salida conocidos y deterministas. Finalmente, hemos asumido que los tiempos de llegada y salida de los agentes (τa m y τd m) son deterministas y conocidos de antemano. Esta suposición es fundamental para nuestro método de solución. Si bien hay muchos dominios donde esta suposición es válida, en muchos casos los agentes llegan y se van de forma dinámica y sus tiempos de llegada y salida solo pueden predecirse de manera probabilística, lo que lleva a problemas de asignación de recursos en línea. En particular, en el caso de agentes con interés propio, esto se convierte en una versión interesante de un problema de diseño de mecanismos en línea [11, 12]. En resumen, hemos presentado una formulación MILP para el problema combinatorio de programación de recursos donde los valores de los agentes para posibles asignaciones de recursos están definidos por MDPs de horizonte finito. Este resultado amplía el trabajo previo ([6, 7]) sobre asignación estática de recursos de una sola vez bajo preferencias inducidas por MDP a problemas de programación de recursos con un aspecto temporal. Por lo tanto, este trabajo da un paso en la dirección de diseñar un mecanismo en línea para agentes con preferencias de recursos combinatorios inducidas por problemas de planificación estocástica. Relajar la suposición sobre los tiempos deterministas de llegada y salida de los agentes es un enfoque de nuestro trabajo futuro. Nos gustaría agradecer a los revisores anónimos por sus comentarios y sugerencias perspicaces. 7. REFERENCIAS [1] E. Altman y A. Shwartz. Control adaptativo de cadenas de Markov restringidas: Criterios y políticas. Anales de Investigación de Operaciones, número especial sobre Procesos de Decisión de Markov, 28:101-134, 1991. [2] R. Bellman. Programación dinámica. Princeton University Press, 1957. [3] C. Boutilier. \n\nPrensa de la Universidad de Princeton, 1957. [3] C. Boutilier. Resolviendo problemas de subasta combinatoria expresados de manera concisa. En Proc. de AAAI-02, páginas 359-366, 2002. [4] C. Boutilier y H. H. Hoos. Subastando idiomas para subastas combinatorias. En Proc. de IJCAI-01, páginas 1211-1217, 2001. [5] D. Dolgov. Asignación integrada de recursos y planificación en entornos estocásticos de múltiples agentes. Tesis doctoral, Departamento de Ciencias de la Computación, Universidad de Michigan, febrero de 2006. [6] D. A. Dolgov y E. H. Durfee. Asignación óptima de recursos y formulación de políticas en procesos de decisión de Markov débilmente acoplados. En Proc. de ICAPS-04, páginas 315-324, junio de 2004. [7] D. A. Dolgov y E. H. Durfee. Subastas combinatorias computacionalmente eficientes para asignación de recursos en MDPs débilmente acoplados. En Proc. de AAMAS-05, Nueva York, NY, EE. UU., 2005. ACM Press. [8] D. A. Dolgov y E. H. Durfee. Asignación de recursos entre agentes con preferencias inducidas por MDP factorizados. En Proc. de AAMAS-06, 2006. [9] K. Larson y T. Sandholm. Diseño de mecanismos y agentes deliberativos. En Proc. de AAMAS-05, páginas 650-656, Nueva York, NY, EE. UU., 2005. ACM Press. [10] N. Nisan. \n\nACM Press. [10] N. Nisan. Subasta y asignación en subastas combinatorias. En Comercio Electrónico, 2000. [11] D. C. Parkes y S. Singh. Un enfoque basado en MDP para el Diseño de Mecanismos en Línea. En Proc. de la Decimoséptima Conferencia Anual sobre Sistemas de Procesamiento de Información Neural (NIPS-03), 2003. [12] D. C. Parkes, S. Singh y D. Yanovsky. Diseño de mecanismos en línea aproximadamente eficiente. En Actas de la Decimoctava Conferencia Anual sobre Sistemas de Procesamiento de Información Neural (NIPS-04), 2004. [13] M. L. Puterman. Procesos de Decisión de Markov. John Wiley & Sons, Nueva York, 1994. [14] M. H. Rothkopf, A. Pekec y R. M. Harstad. Subastas combinatorias manejables computacionalmente. Ciencia de la Gestión, 44(8):1131-1147, 1998. [15] T. Sandholm. Un algoritmo para la determinación óptima del ganador en subastas combinatorias. En Proc. de IJCAI-99, páginas 542-547, San Francisco, CA, EE. UU., 1999. Morgan Kaufmann Publishers Inc. - Editores Morgan Kaufmann Inc. El Sexto Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 1227 ",
            "candidates": [],
            "error": [
                [
                    "programación óptima de recursos",
                    "programación de recursos óptima",
                    "programa de optimización para la programación de recursos globalmente óptima",
                    "programación óptima global de recursos"
                ]
            ]
        },
        "multiagent system": {
            "translated_key": "sistemas multiagentes",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Combinatorial Resource Scheduling for Multiagent MDPs Dmitri A. Dolgov, Michael R. James, and Michael E. Samples AI and Robotics Group Technical Research, Toyota Technical Center USA {ddolgov, michael.r.james, michael.samples}@gmail.com ABSTRACT Optimal resource scheduling in <br>multiagent system</br>s is a computationally challenging task, particularly when the values of resources are not additive.",
                "We consider the combinatorial problem of scheduling the usage of multiple resources among agents that operate in stochastic environments, modeled as Markov decision processes (MDPs).",
                "In recent years, efficient resource-allocation algorithms have been developed for agents with resource values induced by MDPs.",
                "However, this prior work has focused on static resource-allocation problems where resources are distributed once and then utilized in infinite-horizon MDPs.",
                "We extend those existing models to the problem of combinatorial resource scheduling, where agents persist only for finite periods between their (predefined) arrival and departure times, requiring resources only for those time periods.",
                "We provide a computationally efficient procedure for computing globally optimal resource assignments to agents over time.",
                "We illustrate and empirically analyze the method in the context of a stochastic jobscheduling domain.",
                "Categories and Subject Descriptors I.2.8 [Artificial Intelligence]: Problem Solving, Control Methods, and Search; I.2.11 [Artificial Intelligence]: Distributed Artificial Intelligence-Multiagent systems General Terms Algorithms, Performance, Design 1.",
                "INTRODUCTION The tasks of optimal resource allocation and scheduling are ubiquitous in <br>multiagent system</br>s, but solving such optimization problems can be computationally difficult, due to a number of factors.",
                "In particular, when the value of a set of resources to an agent is not additive (as is often the case with resources that are substitutes or complements), the utility function might have to be defined on an exponentially large space of resource bundles, which very quickly becomes computationally intractable.",
                "Further, even when each agent has a utility function that is nonzero only on a small subset of the possible resource bundles, obtaining optimal allocation is still computationally prohibitive, as the problem becomes NP-complete [14].",
                "Such computational issues have recently spawned several threads of work in using compact models of agents preferences.",
                "One idea is to use any structure present in utility functions to represent them compactly, via, for example, logical formulas [15, 10, 4, 3].",
                "An alternative is to directly model the mechanisms that define the agents utility functions and perform resource allocation directly with these models [9].",
                "A way of accomplishing this is to model the processes by which an agent might utilize the resources and define the utility function as the payoff of these processes.",
                "In particular, if an agent uses resources to act in a stochastic environment, its utility function can be naturally modeled with a Markov decision process, whose action set is parameterized by the available resources.",
                "This representation can then be used to construct very efficient resource-allocation algorithms that lead to an exponential speedup over a straightforward optimization problem with flat representations of combinatorial preferences [6, 7, 8].",
                "However, this existing work on resource allocation with preferences induced by resource-parameterized MDPs makes an assumption that the resources are only allocated once and are then utilized by the agents independently within their infinite-horizon MDPs.",
                "This assumption that no reallocation of resources is possible can be limiting in domains where agents arrive and depart dynamically.",
                "In this paper, we extend the work on resource allocation under MDP-induced preferences to discrete-time scheduling problems, where agents are present in the system for finite time intervals and can only use resources within these intervals.",
                "In particular, agents arrive and depart at arbitrary (predefined) times and within these intervals use resources to execute tasks in finite-horizon MDPs.",
                "We address the problem of globally optimal resource scheduling, where the objective is to find an allocation of resources to the agents across time that maximizes the sum of the expected rewards that they obtain.",
                "In this context, our main contribution is a mixed-integerprogramming formulation of the scheduling problem that chooses globally optimal resource assignments, starting times, and execution horizons for all agents (within their arrival1220 978-81-904262-7-5 (RPS) c 2007 IFAAMAS departure intervals).",
                "We analyze and empirically compare two flavors of the scheduling problem: one, where agents have static resource assignments within their finite-horizon MDPs, and another, where resources can be dynamically reallocated between agents at every time step.",
                "In the rest of the paper, we first lay down the necessary groundwork in Section 2 and then introduce our model and formal problem statement in Section 3.",
                "In Section 4.2, we describe our main result, the optimization program for globally optimal resource scheduling.",
                "Following the discussion of our experimental results on a job-scheduling problem in Section 5, we conclude in Section 6 with a discussion of possible extensions and generalizations of our method. 2.",
                "BACKGROUND Similarly to the model used in previous work on resourceallocation with MDP-induced preferences [6, 7], we define the value of a set of resources to an agent as the value of the best MDP policy that is realizable, given those resources.",
                "However, since the focus of our work is on scheduling problems, and a large part of the optimization problem is to decide how resources are allocated in time among agents with finite arrival and departure times, we model the agents planning problems as finite-horizon MDPs, in contrast to previous work that used infinite-horizon discounted MDPs.",
                "In the rest of this section, we first introduce some necessary background on finite-horizon MDPs and present a linear-programming formulation that serves as the basis for our solution algorithm developed in Section 4.",
                "We also outline the standard methods for combinatorial resource scheduling with flat resource values, which serve as a comparison benchmark for the new model developed here. 2.1 Markov Decision Processes A stationary, finite-domain, discrete-time MDP (see, for example, [13] for a thorough and detailed development) can be described as S, A, p, r , where: S is a finite set of system states; A is a finite set of actions that are available to the agent; p is a stationary stochastic transition function, where p(σ|s, a) is the probability of transitioning to state σ upon executing action a in state s; r is a stationary reward function, where r(s, a) specifies the reward obtained upon executing action a in state s. Given such an MDP, a decision problem under a finite horizon T is to choose an optimal action at every time step to maximize the expected value of the total reward accrued during the agents (finite) lifetime.",
                "The agents optimal policy is then a function of current state s and the time until the horizon.",
                "An optimal policy for such a problem is to act greedily with respect to the optimal value function, defined recursively by the following system of finite-time Bellman equations [2]: v(s, t) = max a r(s, a) + X σ p(σ|s, a)v(σ, t + 1), ∀s ∈ S, t ∈ [1, T − 1]; v(s, T) = 0, ∀s ∈ S; where v(s, t) is the optimal value of being in state s at time t ∈ [1, T].",
                "This optimal value function can be easily computed using dynamic programming, leading to the following optimal policy π, where π(s, a, t) is the probability of executing action a in state s at time t: π(s, a, t) = ( 1, a = argmaxa r(s, a) + P σ p(σ|s, a)v(σ, t + 1), 0, otherwise.",
                "The above is the most common way of computing the optimal value function (and therefore an optimal policy) for a finite-horizon MDP.",
                "However, we can also formulate the problem as the following linear program (similarly to the dual LP for infinite-horizon discounted MDPs [13, 6, 7]): max X s X a r(s, a) X t x(s, a, t) subject to: X a x(σ, a, t + 1) = X s,a p(σ|s, a)x(s, a, t) ∀σ, t ∈ [1, T − 1]; X a x(s, a, 1) = α(s), ∀s ∈ S; (1) where α(s) is the initial distribution over the state space, and x is the (non-stationary) occupation measure (x(s, a, t) ∈ [0, 1] is the total expected number of times action a is executed in state s at time t).",
                "An optimal (non-stationary) policy is obtained from the occupation measure as follows: π(s, a, t) = x(s, a, t)/ X a x(s, a, t) ∀s ∈ S, t ∈ [1, T]. (2) Note that the standard unconstrained finite-horizon MDP, as described above, always has a uniformly-optimal solution (optimal for any initial distribution α(s)).",
                "Therefore, an optimal policy can be obtained by using an arbitrary constant α(s) > 0 (in particular, α(s) = 1 will result in x(s, a, t) = π(s, a, t)).",
                "However, for MDPs with resource constraints (as defined below in Section 3), uniformly-optimal policies do not in general exist.",
                "In such cases, α becomes a part of the problem input, and a resulting policy is only optimal for that particular α.",
                "This result is well known for infinite-horizon MDPs with various types of constraints [1, 6], and it also holds for our finite-horizon model, which can be easily established via a line of reasoning completely analogous to the arguments in [6]. 2.2 Combinatorial Resource Scheduling A straightforward approach to resource scheduling for a set of agents M, whose values for the resources are induced by stochastic planning problems (in our case, finite-horizon MDPs) would be to have each agent enumerate all possible resource assignments over time and, for each one, compute its value by solving the corresponding MDP.",
                "Then, each agent would provide valuations for each possible resource bundle over time to a centralized coordinator, who would compute the optimal resource assignments across time based on these valuations.",
                "When resources can be allocated at different times to different agents, each agent must submit valuations for every combination of possible time horizons.",
                "Let each agent m ∈ M execute its MDP within the arrival-departure time interval τ ∈ [τa m, τd m].",
                "Hence, agent m will execute an MDP with time horizon no greater than Tm = τd m−τa m+1.",
                "Let bτ be the global time horizon for the problem, before which all of the agents MDPs must finish.",
                "We assume τd m < bτ, ∀m ∈ M. The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1221 For the scheduling problem where agents have static resource requirements within their finite-horizon MDPs, the agents provide a valuation for each resource bundle for each possible time horizon (from [1, Tm]) that they may use.",
                "Let Ω be the set of resources to be allocated among the agents.",
                "An agent will get at most one resource bundle for one of the time horizons.",
                "Let the variable ψ ∈ Ψm enumerate all possible pairs of resource bundles and time horizons for agent m, so there are 2|Ω| × Tm values for ψ (the space of bundles is exponential in the number of resource types |Ω|).",
                "The agent m must provide a value vψ m for each ψ, and the coordinator will allocate at most one ψ (resource, time horizon) pair to each agent.",
                "This allocation is expressed as an indicator variable zψ m ∈ {0, 1} that shows whether ψ is assigned to agent m. For time τ and resource ω, the function nm(ψ, τ, ω) ∈ {0, 1} indicates whether the bundle in ψ uses resource ω at time τ (we make the assumption that agents have binary resource requirements).",
                "This allocation problem is NP-complete, even when considering only a single time step, and its difficulty increases significantly with multiple time steps because of the increasing number of values of ψ.",
                "The problem of finding an optimal allocation that satisfies the global constraint that the amount of each resource ω allocated to all agents does not exceed the available amount bϕ(ω) can be expressed as the following integer program: max X m∈M X ψ∈Ψm zψ mvψ m subject to: X ψ∈Ψm zψ m ≤ 1, ∀m ∈ M; X m∈M X ψ∈Ψm zψ mnm(ψ, τ, ω) ≤ bϕ(ω), ∀τ ∈ [1, bτ], ∀ω ∈ Ω; (3) The first constraint in equation 3 says that no agent can receive more than one bundle, and the second constraint ensures that the total assignment of resource ω does not, at any time, exceed the resource bound.",
                "For the scheduling problem where the agents are able to dynamically reallocate resources, each agent must specify a value for every combination of bundles and time steps within its time horizon.",
                "Let the variable ψ ∈ Ψm in this case enumerate all possible resource bundles for which at most one bundle may be assigned to agent m at each time step.",
                "Therefore, in this case there are P t∈[1,Tm](2|Ω| )t ∼ 2|Ω|Tm possibilities of resource bundles assigned to different time slots, for the Tm different time horizons.",
                "The same set of equations (3) can be used to solve this dynamic scheduling problem, but the integer program is different because of the difference in how ψ is defined.",
                "In this case, the number of ψ values is exponential in each agents planning horizon Tm, resulting in a much larger program.",
                "This straightforward approach to solving both of these scheduling problems requires an enumeration and solution of either 2|Ω| Tm (static allocation) or P t∈[1,Tm] 2|Ω|t (dynamic reallocation) MDPs for each agent, which very quickly becomes intractable with the growth of the number of resources |Ω| or the time horizon Tm. 3.",
                "MODEL AND PROBLEM STATEMENT We now formally introduce our model of the resourcescheduling problem.",
                "The problem input consists of the following components: • M, Ω, bϕ, τa m, τd m, bτ are as defined above in Section 2.2. • {Θm} = {S, A, pm, rm, αm} are the MDPs of all agents m ∈ M. Without loss of generality, we assume that state and action spaces of all agents are the same, but each has its own transition function pm, reward function rm, and initial conditions αm. • ϕm : A×Ω → {0, 1} is the mapping of actions to resources for agent m. ϕm(a, ω) indicates whether action a of agent m needs resource ω.",
                "An agent m that receives a set of resources that does not include resource ω cannot execute in its MDP policy any action a for which ϕm(a, ω) = 0.",
                "We assume all resource requirements are binary; as discussed below in Section 6, this assumption is not limiting.",
                "Given the above input, the optimization problem we consider is to find the globally optimal-maximizing the sum of expected rewards-mapping of resources to agents for all time steps: Δ : τ × M × Ω → {0, 1}.",
                "A solution is feasible if the corresponding assignment of resources to the agents does not violate the global resource constraint: X m Δm(τ, ω) ≤ bϕ(ω), ∀ω ∈ Ω, τ ∈ [1, bτ]. (4) We consider two flavors of the resource-scheduling problem.",
                "The first formulation restricts resource assignments to the space where the allocation of resources to each agent is static during the agents lifetime.",
                "The second formulation allows reassignment of resources between agents at every time step within their lifetimes.",
                "Figure 1 depicts a resource-scheduling problem with three agents M = {m1, m2, m3}, three resources Ω = {ω1, ω2, ω3}, and a global problem horizon of bτ = 11.",
                "The agents arrival and departure times are shown as gray boxes and are {1, 6}, {3, 7}, and {2, 11}, respectively.",
                "A solution to this problem is shown via horizontal bars within each agents box, where the bars correspond to the allocation of the three resource types.",
                "Figure 1a shows a solution to a static scheduling problem.",
                "According to the shown solution, agent m1 begins the execution of its MDP at time τ = 1 and has a lock on all three resources until it finishes execution at time τ = 3.",
                "Note that agent m1 relinquishes its hold on the resources before its announced departure time of τd m1 = 6, ostensibly because other agents can utilize the resources more effectively.",
                "Thus, at time τ = 4, resources ω1 and ω3 are allocated to agent m2, who then uses them to execute its MDP (using only actions supported by resources ω1 and ω3) until time τ = 7.",
                "Agent m3 holds resource ω3 during the interval τ ∈ [4, 10].",
                "Figure 1b shows a possible solution to the dynamic version of the same problem.",
                "There, resources can be reallocated between agents at every time step.",
                "For example, agent m1 gives up its use of resource ω2 at time τ = 2, although it continues the execution of its MDP until time τ = 6.",
                "Notice that an agent is not allowed to stop and restart its MDP, so agent m1 is only able to continue executing in the interval τ ∈ [3, 4] if it has actions that do not require any resources (ϕm(a, ω) = 0).",
                "Clearly, the model and problem statement described above make a number of assumptions about the problem and the desired solution properties.",
                "We discuss some of those assumptions and their implications in Section 6. 1222 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) (a) (b) Figure 1: Illustration of a solution to a resource-scheduling problem with three agents and three resources: a) static resource assignments (resource assignments are constant within agents lifetimes; b) dynamic assignment (resource assignments are allowed to change at every time step). 4.",
                "RESOURCE SCHEDULING Our resource-scheduling algorithm proceeds in two stages.",
                "First, we perform a preprocessing step that augments the agent MDPs; this process is described in Section 4.1.",
                "Second, using these augmented MDPs we construct a global optimization problem, which is described in Section 4.2. 4.1 Augmenting Agents MDPs In the model described in the previous section, we assume that if an agent does not possess the necessary resources to perform actions in its MDP, its execution is halted and the agent leaves the system.",
                "In other words, the MDPs cannot be paused and resumed.",
                "For example, in the problem shown in Figure 1a, agent m1 releases all resources after time τ = 3, at which point the execution of its MDP is halted.",
                "Similarly, agents m2 and m3 only execute their MDPs in the intervals τ ∈ [4, 6] and τ ∈ [4, 10], respectively.",
                "Therefore, an important part of the global decision-making problem is to decide the window of time during which each of the agents is active (i.e., executing its MDP).",
                "To accomplish this, we augment each agents MDP with two new states (start and finish states sb , sf , respectively) and a new start/stop action a∗ , as illustrated in Figure 2.",
                "The idea is that an agent stays in the start state sb until it is ready to execute its MDP, at which point it performs the start/stop action a∗ and transitions into the state space of the original MDP with the transition probability that corresponds to the original initial distribution α(s).",
                "For example, in Figure 1a, for agent m2 this would happen at time τ = 4.",
                "Once the agent gets to the end of its activity window (time τ = 6 for agent m2 in Figure 1a), it performs the start/stop action, which takes it into the sink finish state sf at time τ = 7.",
                "More precisely, given an MDP S, A, pm, rm, αm , we define an augmented MDP S , A , pm, rm, αm as follows: S = S ∪ sb ∪ sf ; A = A ∪ a∗ ; p (s|sb , a∗ ) = α(s), ∀s ∈ S; p (sb |sb , a) = 1.0, ∀a ∈ A; p (sf |s, a∗ ) = 1.0, ∀s ∈ S; p (σ|s, a) = p(σ|s, a), ∀s, σ ∈ S, a ∈ A; r (sb , a) = r (sf , a) = 0, ∀a ∈ A ; r (s, a) = r(s, a), ∀s ∈ S, a ∈ A; α (sb ) = 1; α (s) = 0, ∀s ∈ S; where all non-specified transition probabilities are assumed to be zero.",
                "Further, in order to account for the new starting state, we begin the MDP one time-step earlier, setting τa m ← τa m − 1.",
                "This will not affect the resource allocation due to the resource constraints only being enforced for the original MDP states, as will be discussed in the next section.",
                "For example, the augmented MDPs shown in Figure 2b (which starts in state sb at time τ = 2) would be constructed from an MDP with original arrival time τ = 3.",
                "Figure 2b also shows a sample trajectory through the state space: the agent starts in state sb , transitions into the state space S of the original MDP, and finally exists into the sink state sf .",
                "Note that if we wanted to model a problem where agents could pause their MDPs at arbitrary time steps (which might be useful for domains where dynamic reallocation is possible), we could easily accomplish this by including an extra action that transitions from each state to itself with zero reward. 4.2 MILP for Resource Scheduling Given a set of augmented MDPs, as defined above, the goal of this section is to formulate a global optimization program that solves the resource-scheduling problem.",
                "In this section and below, all MDPs are assumed to be the augmented MDPs as defined in Section 4.1.",
                "Our approach is similar to the idea used in [6]: we begin with the linear-program formulation of agents MDPs (1) and augment it with constraints that ensure that the corresponding resource allocation across agents and time is valid.",
                "The resulting optimization problem then simultaneously solves the agents MDPs and resource-scheduling problems.",
                "In the rest of this section, we incrementally develop a mixed integer program (MILP) that achieves this.",
                "In the absence of resource constraints, the agents finitehorizon MDPs are completely independent, and the globally optimal solution can be trivially obtained via the following LP, which is simply an aggregation of single-agent finitehorizon LPs: max X m X s X a rm(s, a) X t xm(s, a, t) subject to: X a xm(σ, a, t + 1) = X s,a pm(σ|s, a)xm(s, a, t), ∀m ∈ M, σ ∈ S, t ∈ [1, Tm − 1]; X a xm(s, a, 1) = αm(s), ∀m ∈ M, s ∈ S; (12) where xm(s, a, t) is the occupation measure of agent m, and The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1223 (a) (b) Figure 2: Illustration of augmenting an MDP to allow for variable starting and stopping times: a) (left) the original two-state MDP with a single action; (right) the augmented MDP with new states sb and sf and the new action a∗ (note that the origianl transitions are not changed in the augmentation process); b) the augmented MDP displayed as a trajectory through time (grey lines indicate all transitions, while black lines indicate a given trajectory.",
                "Objective Function (sum of expected rewards over all agents) max X m X s X a rm(s, a) X t xm(s, a, t) (5) Meaning Implication Linear Constraints Tie x to θ.",
                "Agent is only active when occupation measure is nonzero in original MDP states. θm(τ) = 0 =⇒ xm(s, a, τ −τa m+1) = 0 ∀s /∈ {sb , sf }, a ∈ A X s/∈{sb,sf } X a xm(s, a, t) ≤ θm(τa m + t − 1) ∀m ∈ M, ∀t ∈ [1, Tm] (6) Agent can only be active in τ ∈ (τa m, τd m) θm(τ) = 0 ∀m ∈ M, τ /∈ (τa m, τd m) (7) Cannot use resources when not active θm(τ) = 0 =⇒ Δm(τ, ω) = 0 ∀τ ∈ [0, bτ], ω ∈ Ω Δm(τ, ω) ≤ θm(τ) ∀m ∈ M, τ ∈ [0, bτ], ω ∈ Ω (8) Tie x to Δ (nonzero x forces corresponding Δ to be nonzero.)",
                "Δm(τ, ω) = 0, ϕm(a, ω) = 1 =⇒ xm(s, a, τ − τa m + 1) = 0 ∀s /∈ {sb , sf } 1/|A| X a ϕm(a, ω) X s/∈{sb,sf } xm(s, a, t) ≤ Δm(t + τa m − 1, ω) ∀m ∈ M, ω ∈ Ω, t ∈ [1, Tm] (9) Resource bounds X m Δm(τ, ω) ≤ bϕ(ω) ∀ω ∈ Ω, τ ∈ [0, bτ] (10) Agent cannot change resources while active.",
                "Only enabled for scheduling with static assignments. θm(τ) = 1 and θm(τ + 1) = 1 =⇒ Δm(τ, ω) = Δm(τ + 1, ω) Δm(τ, ω) − Z(1 − θm(τ + 1)) ≤ Δm(τ + 1, ω) + Z(1 − θm(τ)) Δm(τ, ω) + Z(1 − θm(τ + 1)) ≥ Δm(τ + 1, ω) − Z(1 − θm(τ)) ∀m ∈ M, ω ∈ Ω, τ ∈ [0, bτ] (11) Table 1: MILP for globally optimal resource scheduling.",
                "Tm = τd m − τa m + 1 is the time horizon for the agents MDP.",
                "Using this LP as a basis, we augment it with constraints that ensure that the resource usage implied by the agents occupation measures {xm} does not violate the global resource requirements bϕ at any time step τ ∈ [0, bτ].",
                "To formulate these resource constraints, we use the following binary variables: • Δm(τ, ω) = {0, 1}, ∀m ∈ M, τ ∈ [0, bτ], ω ∈ Ω, which serve as indicator variables that define whether agent m possesses resource ω at time τ.",
                "These are analogous to the static indicator variables used in the one-shot static resource-allocation problem in [6]. • θm = {0, 1}, ∀m ∈ M, τ ∈ [0, bτ] are indicator variables that specify whether agent m is active (i.e., executing its MDP) at time τ.",
                "The meaning of resource-usage variables Δ is illustrated in Figure 1: Δm(τ, ω) = 1 only if resource ω is allocated to agent m at time τ.",
                "The meaning of the activity indicators θ is illustrated in Figure 2b: when agent m is in either the start state sb or the finish state sf , the corresponding θm = 0, but once the agent becomes active and enters one of the other states, we set θm = 1 .",
                "This meaning of θ can be enforced with a linear constraint that synchronizes the values of the agents occupation measures xm and the activity 1224 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) indicators θ, as shown in (6) in Table 1.",
                "Another constraint we have to add-because the activity indicators θ are defined on the global timeline τ-is to enforce the fact that the agent is inactive outside of its arrivaldeparture window.",
                "This is accomplished by constraint (7) in Table 1.",
                "Furthermore, agents should not be using resources while they are inactive.",
                "This constraint can also be enforced via a linear inequality on θ and Δ, as shown in (8).",
                "Constraint (6) sets the value of θ to match the policy defined by the occupation measure xm.",
                "In a similar fashion, we have to make sure that the resource-usage variables Δ are also synchronized with the occupation measure xm.",
                "This is done via constraint (9) in Table 1, which is nearly identical to the analogous constraint from [6].",
                "After implementing the above constraint, which enforces the meaning of Δ, we add a constraint that ensures that the agents resource usage never exceeds the amounts of available resources.",
                "This condition is also trivially expressed as a linear inequality (10) in Table 1.",
                "Finally, for the problem formulation where resource assignments are static during a lifetime of an agent, we add a constraint that ensures that the resource-usage variables Δ do not change their value while the agent is active (θ = 1).",
                "This is accomplished via the linear constraint (11), where Z ≥ 2 is a constant that is used to turn off the constraints when θm(τ) = 0 or θm(τ + 1) = 0.",
                "This constraint is not used for the dynamic problem formulation, where resources can be reallocated between agents at every time step.",
                "To summarize, Table 1 together with the conservationof-flow constraints from (12) defines the MILP that simultaneously computes an optimal resource assignment for all agents across time as well as optimal finite-horizon MDP policies that are valid under that resource assignment.",
                "As a rough measure of the complexity of this MILP, let us consider the number of optimization variables and constraints.",
                "Let TM = P Tm = P m(τa m − τd m + 1) be the sum of the lengths of the arrival-departure windows across all agents.",
                "Then, the number of optimization variables is: TM + bτ|M||Ω| + bτ|M|, TM of which are continuous (xm), and bτ|M||Ω| + bτ|M| are binary (Δ and θ).",
                "However, notice that all but TM|M| of the θ are set to zero by constraint (7), which also immediately forces all but TM|M||Ω| of the Δ to be zero via the constraints (8).",
                "The number of constraints (not including the degenerate constraints in (7)) in the MILP is: TM + TM|Ω| + bτ|Ω| + bτ|M||Ω|.",
                "Despite the fact that the complexity of the MILP is, in the worst case, exponential1 in the number of binary variables, the complexity of this MILP is significantly (exponentially) lower than that of the MILP with flat utility functions, described in Section 2.2.",
                "This result echos the efficiency gains reported in [6] for single-shot resource-allocation problems, but is much more pronounced, because of the explosion of the flat utility representation due to the temporal aspect of the problem (recall the prohibitive complexity of the combinatorial optimization in Section 2.2).",
                "We empirically analyze the performance of this method in Section 5. 1 Strictly speaking, solving MILPs to optimality is NPcomplete in the number of integer variables. 5.",
                "EXPERIMENTAL RESULTS Although the complexity of solving MILPs is in the worst case exponential in the number of integer variables, there are many efficient methods for solving MILPs that allow our algorithm to scale well for parameters common to resource allocation and scheduling problems.",
                "In particular, this section introduces a problem domain-the repairshop problem-used to empirically evaluate our algorithms scalability in terms of the number of agents |M|, the number of shared resources |Ω|, and the varied lengths of global time bτ during which agents may enter and exit the system.",
                "The repairshop problem is a simple parameterized MDP adopting the metaphor of a vehicular repair shop.",
                "Agents in the repair shop are mechanics with a number of independent tasks that yield reward only when completed.",
                "In our MDP model of this system, actions taken to advance through the state space are only allowed if the agent holds certain resources that are publicly available to the shop.",
                "These resources are in finite supply, and optimal policies for the shop will determine when each agent may hold the limited resources to take actions and earn individual rewards.",
                "Each task to be completed is associated with a single action, although the agent is required to repeat the action numerous times before completing the task and earning a reward.",
                "This model was parameterized in terms of the number of agents in the system, the number of different types of resources that could be linked to necessary actions, a global time during which agents are allowed to arrive and depart, and a maximum length for the number of time steps an agent may remain in the system.",
                "All datapoints in our experiments were obtained with 20 evaluations using CPLEX to solve the MILPs on a Pentium4 computer with 2Gb of RAM.",
                "Trials were conducted on both the static and the dynamic version of the resourcescheduling problem, as defined earlier.",
                "Figure 3 shows the runtime and policy value for independent modifications to the parameter set.",
                "The top row shows how the solution time for the MILP scales as we increase the number of agents |M|, the global time horizon bτ, and the number of resources |Ω|.",
                "Increasing the number of agents leads to exponential complexity scaling, which is to be expected for an NP-complete problem.",
                "However, increasing the global time limit bτ or the total number of resource types |Ω|-while holding the number of agents constantdoes not lead to decreased performance.",
                "This occurs because the problems get easier as they become under-constrained, which is also a common phenomenon for NP-complete problems.",
                "We also observe that the solution to the dynamic version of the problem can often be computed much faster than the static version.",
                "The bottom row of Figure 3 shows the joint policy value of the policies that correspond to the computed optimal resource-allocation schedules.",
                "We can observe that the dynamic version yields higher reward (as expected, since the reward for the dynamic version is always no less than the reward of the static version).",
                "We should point out that these graphs should not be viewed as a measure of performance of two different algorithms (both algorithms produce optimal solutions but to different problems), but rather as observations about how the quality of optimal solutions change as more flexibility is allowed in the reallocation of resources.",
                "Figure 4 shows runtime and policy value for trials in which common input variables are scaled together.",
                "This allows The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1225 2 4 6 8 10 10 −3 10 −2 10 −1 10 0 10 1 10 2 10 3 10 4 Number of Agents |M| CPUTime,sec |Ω| = 5, τ = 50 static dynamic 50 100 150 200 10 −2 10 −1 10 0 10 1 10 2 10 3 Global Time Boundary τ CPUTime,sec |M| = 5, |Ω| = 5 static dynamic 10 20 30 40 50 10 −2 10 −1 10 0 10 1 10 2 Number of Resources |Ω| CPUTime,sec |M| = 5, τ = 50 static dynamic 2 4 6 8 10 200 400 600 800 1000 1200 1400 1600 Number of Agents |M| Value |Ω| = 5, τ = 50 static dynamic 50 100 150 200 400 500 600 700 800 900 1000 1100 1200 1300 1400 Global Time Boundary τ Value |M| = 5, |Ω| = 5 static dynamic 10 20 30 40 50 500 600 700 800 900 1000 1100 1200 1300 1400 Number of Resources |Ω| Value |M| = 5, τ = 50 static dynamic Figure 3: Evaluation of our MILP for variable numbers of agents (column 1), lengths of global-time window (column 2), and numbers of resource types (column 3).",
                "Top row shows CPU time, and bottom row shows the joint reward of agents MDP policies.",
                "Error bars show the 1st and 3rd quartiles (25% and 75%). 2 4 6 8 10 10 −3 10 −2 10 −1 10 0 10 1 10 2 10 3 Number of Agents |M| CPUTime,sec τ = 10|M| static dynamic 2 4 6 8 10 10 −3 10 −2 10 −1 10 0 10 1 10 2 10 3 10 4 Number of Agents |M| CPUTime,sec |Ω| = 2|M| static dynamic 2 4 6 8 10 10 −3 10 −2 10 −1 10 0 10 1 10 2 10 3 10 4 Number of Agents |M| CPUTime,sec |Ω| = 5|M| static dynamic 2 4 6 8 10 200 400 600 800 1000 1200 1400 1600 1800 2000 2200 Number of Agents |M| Value τ = 10|M| static dynamic 2 4 6 8 10 200 400 600 800 1000 1200 1400 1600 1800 2000 Number of Agents |M| Value |Ω| = 2|M| static dynamic 2 4 6 8 10 0 500 1000 1500 2000 2500 Number of Agents |M| Value |Ω| = 5|M| static dynamic Figure 4: Evaluation of our MILP using correlated input variables.",
                "The left column tracks the performance and CPU time as the number of agents and global-time window increase together (bτ = 10|M|).",
                "The middle and the right column track the performance and CPU time as the number of resources and the number of agents increase together as |Ω| = 2|M| and |Ω| = 5|M|, respectively.",
                "Error bars show the 1st and 3rd quartiles (25% and 75%). 1226 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) us to explore domains where the total number of agents scales proportionally to the total number of resource types or the global time horizon, while keeping constant the average agent density (per unit of global time) or the average number of resources per agent (which commonly occurs in real-life applications).",
                "Overall, we believe that these experimental results indicate that our MILP formulation can be used to effectively solve resource-scheduling problems of nontrivial size. 6.",
                "DISCUSSION AND CONCLUSIONS Throughout the paper, we have made a number of assumptions in our model and solution algorithm; we discuss their implications below. • Continual execution.",
                "We assume that once an agent stops executing its MDP (transitions into state sf ), it exits the system and cannot return.",
                "It is easy to relax this assumption for domains where agents MDPs can be paused and restarted.",
                "All that is required is to include an additional pause action which transitions from a given state back to itself, and has zero reward. • Indifference to start time.",
                "We used a reward model where agents rewards depend only on the time horizon of their MDPs and not the global start time.",
                "This is a consequence of our MDP-augmentation procedure from Section 4.1.",
                "It is easy to extend the model so that the agents incur an explicit penalty for idling by assigning a non-zero negative reward to the start state sb . • Binary resource requirements.",
                "For simplicity, we have assumed that resource costs are binary: ϕm(a, ω) = {0, 1}, but our results generalize in a straightforward manner to non-binary resource mappings, analogously to the procedure used in [5]. • Cooperative agents.",
                "The optimization procedure discussed in this paper was developed in the context of cooperative agents, but it can also be used to design a mechanism for scheduling resources among self-interested agents.",
                "This optimization procedure can be embedded in a VickreyClarke-Groves auction, completely analogously to the way it was done in [7].",
                "In fact, all the results of [7] about the properties of the auction and information privacy directly carry over to the scheduling domain discussed in this paper, requiring only slight modifications to deal with finitehorizon MDPs. • Known, deterministic arrival and departure times.",
                "Finally, we have assumed that agents arrival and departure times (τa m and τd m) are deterministic and known a priori.",
                "This assumption is fundamental to our solution method.",
                "While there are many domains where this assumption is valid, in many cases agents arrive and depart dynamically and their arrival and departure times can only be predicted probabilistically, leading to online resource-allocation problems.",
                "In particular, in the case of self-interested agents, this becomes an interesting version of an online-mechanism-design problem [11, 12].",
                "In summary, we have presented an MILP formulation for the combinatorial resource-scheduling problem where agents values for possible resource assignments are defined by finitehorizon MDPs.",
                "This result extends previous work ([6, 7]) on static one-shot resource allocation under MDP-induced preferences to resource-scheduling problems with a temporal aspect.",
                "As such, this work takes a step in the direction of designing an online mechanism for agents with combinatorial resource preferences induced by stochastic planning problems.",
                "Relaxing the assumption about deterministic arrival and departure times of the agents is a focus of our future work.",
                "We would like to thank the anonymous reviewers for their insightful comments and suggestions. 7.",
                "REFERENCES [1] E. Altman and A. Shwartz.",
                "Adaptive control of constrained Markov chains: Criteria and policies.",
                "Annals of Operations Research, special issue on Markov Decision Processes, 28:101-134, 1991. [2] R. Bellman.",
                "Dynamic Programming.",
                "Princeton University Press, 1957. [3] C. Boutilier.",
                "Solving concisely expressed combinatorial auction problems.",
                "In Proc. of AAAI-02, pages 359-366, 2002. [4] C. Boutilier and H. H. Hoos.",
                "Bidding languages for combinatorial auctions.",
                "In Proc. of IJCAI-01, pages 1211-1217, 2001. [5] D. Dolgov.",
                "Integrated Resource Allocation and Planning in Stochastic Multiagent Environments.",
                "PhD thesis, Computer Science Department, University of Michigan, February 2006. [6] D. A. Dolgov and E. H. Durfee.",
                "Optimal resource allocation and policy formulation in loosely-coupled Markov decision processes.",
                "In Proc. of ICAPS-04, pages 315-324, June 2004. [7] D. A. Dolgov and E. H. Durfee.",
                "Computationally efficient combinatorial auctions for resource allocation in weakly-coupled MDPs.",
                "In Proc. of AAMAS-05, New York, NY, USA, 2005.",
                "ACM Press. [8] D. A. Dolgov and E. H. Durfee.",
                "Resource allocation among agents with preferences induced by factored MDPs.",
                "In Proc. of AAMAS-06, 2006. [9] K. Larson and T. Sandholm.",
                "Mechanism design and deliberative agents.",
                "In Proc. of AAMAS-05, pages 650-656, New York, NY, USA, 2005.",
                "ACM Press. [10] N. Nisan.",
                "Bidding and allocation in combinatorial auctions.",
                "In Electronic Commerce, 2000. [11] D. C. Parkes and S. Singh.",
                "An MDP-based approach to Online Mechanism Design.",
                "In Proc. of the Seventeenths Annual Conference on Neural Information Processing Systems (NIPS-03), 2003. [12] D. C. Parkes, S. Singh, and D. Yanovsky.",
                "Approximately efficient online mechanism design.",
                "In Proc. of the Eighteenths Annual Conference on Neural Information Processing Systems (NIPS-04), 2004. [13] M. L. Puterman.",
                "Markov Decision Processes.",
                "John Wiley & Sons, New York, 1994. [14] M. H. Rothkopf, A. Pekec, and R. M. Harstad.",
                "Computationally manageable combinational auctions.",
                "Management Science, 44(8):1131-1147, 1998. [15] T. Sandholm.",
                "An algorithm for optimal winner determination in combinatorial auctions.",
                "In Proc. of IJCAI-99, pages 542-547, San Francisco, CA, USA, 1999.",
                "Morgan Kaufmann Publishers Inc.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1227"
            ],
            "original_annotated_samples": [
                "Combinatorial Resource Scheduling for Multiagent MDPs Dmitri A. Dolgov, Michael R. James, and Michael E. Samples AI and Robotics Group Technical Research, Toyota Technical Center USA {ddolgov, michael.r.james, michael.samples}@gmail.com ABSTRACT Optimal resource scheduling in <br>multiagent system</br>s is a computationally challenging task, particularly when the values of resources are not additive.",
                "INTRODUCTION The tasks of optimal resource allocation and scheduling are ubiquitous in <br>multiagent system</br>s, but solving such optimization problems can be computationally difficult, due to a number of factors."
            ],
            "translated_annotated_samples": [
                "La programación de recursos combinatoria para MDPs multiagentes. Dmitri A. Dolgov, Michael R. James y Michael E. Samples. Grupo de Investigación en IA y Robótica, Centro Técnico de Toyota en EE. UU. {ddolgov, michael.r.james, michael.samples}@gmail.com. RESUMEN La programación óptima de recursos en <br>sistemas multiagentes</br> es una tarea computacionalmente desafiante, especialmente cuando los valores de los recursos no son aditivos.",
                "INTRODUCCIÓN Las tareas de asignación óptima de recursos y programación son ubicuas en <br>sistemas multiagentes</br>, pero resolver tales problemas de optimización puede ser computacionalmente difícil, debido a una serie de factores."
            ],
            "translated_text": "La programación de recursos combinatoria para MDPs multiagentes. Dmitri A. Dolgov, Michael R. James y Michael E. Samples. Grupo de Investigación en IA y Robótica, Centro Técnico de Toyota en EE. UU. {ddolgov, michael.r.james, michael.samples}@gmail.com. RESUMEN La programación óptima de recursos en <br>sistemas multiagentes</br> es una tarea computacionalmente desafiante, especialmente cuando los valores de los recursos no son aditivos. Consideramos el problema combinatorio de programar el uso de múltiples recursos entre agentes que operan en entornos estocásticos, modelados como procesos de decisión de Markov (MDPs). En los últimos años, se han desarrollado algoritmos eficientes de asignación de recursos para agentes con valores de recursos inducidos por MDPs. Sin embargo, este trabajo previo se ha centrado en problemas de asignación de recursos estáticos donde los recursos se distribuyen una vez y luego se utilizan en MDPs de horizonte infinito. Extendemos esos modelos existentes al problema de la programación de recursos combinatoria, donde los agentes persisten solo por períodos finitos entre sus tiempos de llegada y salida (predefinidos), requiriendo recursos solo durante esos períodos de tiempo. Proporcionamos un procedimiento computacionalmente eficiente para calcular asignaciones de recursos óptimas a nivel global a agentes a lo largo del tiempo. Ilustramos y analizamos empíricamente el método en el contexto de un dominio de programación de trabajos estocásticos. Categorías y Descriptores de Asignaturas I.2.8 [Inteligencia Artificial]: Resolución de Problemas, Métodos de Control y Búsqueda; I.2.11 [Inteligencia Artificial]: Inteligencia Artificial Distribuida-Sistemas Multiagente Términos Generales Algoritmos, Rendimiento, Diseño 1. INTRODUCCIÓN Las tareas de asignación óptima de recursos y programación son ubicuas en <br>sistemas multiagentes</br>, pero resolver tales problemas de optimización puede ser computacionalmente difícil, debido a una serie de factores. En particular, cuando el valor de un conjunto de recursos para un agente no es aditivo (como suele ser el caso con recursos que son sustitutos o complementos), la función de utilidad podría tener que definirse en un espacio exponencialmente grande de combinaciones de recursos, lo que rápidamente se vuelve computacionalmente intratable. Además, incluso cuando cada agente tiene una función de utilidad que es distinta de cero solo en un pequeño subconjunto de los posibles conjuntos de recursos, obtener una asignación óptima sigue siendo computacionalmente prohibitivo, ya que el problema se vuelve NP-completo [14]. Tales problemas computacionales han dado lugar recientemente a varias líneas de trabajo en el uso de modelos compactos de las preferencias de los agentes. Una idea es utilizar cualquier estructura presente en las funciones de utilidad para representarlas de forma compacta, por ejemplo, a través de fórmulas lógicas [15, 10, 4, 3]. Una alternativa es modelar directamente los mecanismos que definen las funciones de utilidad de los agentes y realizar la asignación de recursos directamente con estos modelos [9]. Una forma de lograr esto es modelar los procesos mediante los cuales un agente podría utilizar los recursos y definir la función de utilidad como la ganancia de estos procesos. En particular, si un agente utiliza recursos para actuar en un entorno estocástico, su función de utilidad puede ser modelada de forma natural con un proceso de decisión de Markov, cuyo conjunto de acciones está parametrizado por los recursos disponibles. Esta representación puede ser utilizada para construir algoritmos de asignación de recursos muy eficientes que resultan en una aceleración exponencial sobre un problema de optimización directa con representaciones planas de preferencias combinatorias [6, 7, 8]. Sin embargo, este trabajo existente sobre asignación de recursos con preferencias inducidas por MDPs parametrizados por recursos hace una suposición de que los recursos se asignan solo una vez y luego son utilizados por los agentes de forma independiente dentro de sus MDPs de horizonte infinito. Esta suposición de que no es posible la reasignación de recursos puede ser limitante en dominios donde los agentes llegan y se van dinámicamente. En este artículo, ampliamos el trabajo sobre asignación de recursos bajo preferencias inducidas por MDP a problemas de programación en tiempo discreto, donde los agentes están presentes en el sistema durante intervalos de tiempo finitos y solo pueden utilizar recursos dentro de estos intervalos. En particular, los agentes llegan y parten en momentos arbitrarios (predefinidos) y dentro de estos intervalos utilizan recursos para ejecutar tareas en MDPs de horizonte finito. Abordamos el problema de la programación de recursos óptima a nivel global, donde el objetivo es encontrar una asignación de recursos a los agentes a lo largo del tiempo que maximice la suma de las recompensas esperadas que obtienen. En este contexto, nuestra principal contribución es una formulación de programación entera mixta del problema de programación que elige asignaciones de recursos globalmente óptimas, tiempos de inicio y horizontes de ejecución para todos los agentes (dentro de sus intervalos de llegada y salida). Analizamos y comparamos empíricamente dos variantes del problema de programación: uno, donde los agentes tienen asignaciones de recursos estáticas dentro de sus MDPs de horizonte finito, y otro, donde los recursos pueden ser realocados dinámicamente entre agentes en cada paso de tiempo. En el resto del documento, primero establecemos las bases necesarias en la Sección 2 y luego presentamos nuestro modelo y declaración formal del problema en la Sección 3. En la Sección 4.2, describimos nuestro resultado principal, el programa de optimización para la programación de recursos globalmente óptima. Tras la discusión de nuestros resultados experimentales sobre un problema de programación de trabajos en la Sección 5, concluimos en la Sección 6 con una discusión de posibles extensiones y generalizaciones de nuestro método. De manera similar al modelo utilizado en trabajos anteriores sobre asignación de recursos con preferencias inducidas por MDP [6, 7], definimos el valor de un conjunto de recursos para un agente como el valor de la mejor política MDP que es realizable, dadas esos recursos. Sin embargo, dado que el enfoque de nuestro trabajo se centra en problemas de programación, y una gran parte del problema de optimización consiste en decidir cómo se asignan los recursos en el tiempo entre agentes con tiempos finitos de llegada y salida, modelamos los problemas de planificación de los agentes como MDPs de horizonte finito, en contraste con trabajos anteriores que utilizaron MDPs descontados de horizonte infinito. En el resto de esta sección, primero presentamos algo de antecedentes necesarios sobre MDPs de horizonte finito y presentamos una formulación de programación lineal que sirve como base para nuestro algoritmo de solución desarrollado en la Sección 4. También describimos los métodos estándar para la programación de recursos combinatorios con valores de recursos planos, que sirven como punto de referencia de comparación para el nuevo modelo desarrollado aquí. 2.1 Procesos de Decisión de Markov Un MDP (Proceso de Decisión de Markov) estacionario, de dominio finito y discreto en el tiempo (ver, por ejemplo, [13] para un desarrollo detallado y exhaustivo) se puede describir como S, A, p, r, donde: S es un conjunto finito de estados del sistema; A es un conjunto finito de acciones disponibles para el agente; p es una función de transición estocástica estacionaria, donde p(σ|s, a) es la probabilidad de transicionar al estado σ al ejecutar la acción a en el estado s; r es una función de recompensa estacionaria, donde r(s, a) especifica la recompensa obtenida al ejecutar la acción a en el estado s. Dado un MDP de este tipo, un problema de decisión bajo un horizonte finito T consiste en elegir una acción óptima en cada paso de tiempo para maximizar el valor esperado de la recompensa total acumulada durante la vida (finita) de los agentes. La política óptima del agente es entonces una función del estado actual s y el tiempo hasta el horizonte. Una política óptima para tal problema es actuar de forma codiciosa con respecto a la función de valor óptimo, definida de forma recursiva por el siguiente sistema de ecuaciones de Bellman de tiempo finito [2]: v(s, t) = max a r(s, a) + X σ p(σ|s, a)v(σ, t + 1), ∀s ∈ S, t ∈ [1, T − 1]; v(s, T) = 0, ∀s ∈ S; donde v(s, t) es el valor óptimo de estar en el estado s en el tiempo t ∈ [1, T]. Esta función de valor óptimo se puede calcular fácilmente utilizando programación dinámica, lo que conduce a la siguiente política óptima π, donde π(s, a, t) es la probabilidad de ejecutar la acción a en el estado s en el tiempo t: π(s, a, t) = (1, a = argmaxa r(s, a) + P σ p(σ|s, a)v(σ, t + 1), 0, en caso contrario. La anterior es la forma más común de calcular la función de valor óptimo (y por lo tanto una política óptima) para un MDP de horizonte finito. Sin embargo, también podemos formular el problema como el siguiente programa lineal (similarmente al LP dual para MDPs descontados de horizonte infinito [13, 6, 7]): max X s X a r(s, a) X t x(s, a, t) sujeto a: X a x(σ, a, t + 1) = X s,a p(σ|s, a)x(s, a, t) ∀σ, t ∈ [1, T − 1]; X a x(s, a, 1) = α(s), ∀s ∈ S; (1) donde α(s) es la distribución inicial sobre el espacio de estados, y x es la medida de ocupación (no estacionaria) (x(s, a, t) ∈ [0, 1] es el número total esperado de veces que se ejecuta la acción a en el estado s en el tiempo t). Una política óptima (no estacionaria) se obtiene a partir de la medida de ocupación de la siguiente manera: π(s, a, t) = x(s, a, t)/ X a x(s, a, t) ∀s ∈ S, t ∈ [1, T]. (2) Tenga en cuenta que el MDP estándar de horizonte finito sin restricciones, como se describe anteriormente, siempre tiene una solución uniformemente óptima (óptima para cualquier distribución inicial α(s)). Por lo tanto, una política óptima se puede obtener utilizando una constante arbitraria α(s) > 0 (en particular, α(s) = 1 resultará en x(s, a, t) = π(s, a, t)). Sin embargo, para los MDPs con restricciones de recursos (como se definen a continuación en la Sección 3), las políticas óptimas uniformemente no existen en general. En tales casos, α se convierte en parte de la entrada del problema, y una política resultante solo es óptima para ese α en particular. Este resultado es bien conocido para MDPs de horizonte infinito con varios tipos de restricciones [1, 6], y también se cumple para nuestro modelo de horizonte finito, el cual puede establecerse fácilmente a través de una línea de razonamiento completamente análoga a los argumentos en [6]. 2.2 Programación de Recursos Combinatoria Un enfoque directo para la programación de recursos para un conjunto de agentes M, cuyos valores para los recursos son inducidos por problemas de planificación estocástica (en nuestro caso, MDPs de horizonte finito) sería que cada agente enumerara todas las asignaciones de recursos posibles a lo largo del tiempo y, para cada una, calcular su valor resolviendo el MDP correspondiente. Entonces, cada agente proporcionaría valoraciones para cada posible conjunto de recursos a lo largo del tiempo a un coordinador centralizado, quien calcularía las asignaciones óptimas de recursos a lo largo del tiempo basándose en estas valoraciones. Cuando los recursos pueden asignarse en diferentes momentos a diferentes agentes, cada agente debe presentar valoraciones para cada combinación de posibles horizontes temporales. Que cada agente m ∈ M ejecute su MDP dentro del intervalo de tiempo de llegada-salida τ ∈ [τa m, τd m]. Por lo tanto, el agente m ejecutará un MDP con un horizonte temporal no mayor que Tm = τd m−τa m+1. Que bτ sea el horizonte temporal global para el problema, antes del cual todos los MDP de los agentes deben finalizar. Suponemos que τd m < bτ, ∀m ∈ M. El Sexto Congreso Internacional. En el problema de programación donde los agentes tienen requisitos de recursos estáticos dentro de sus MDPs de horizonte finito, los agentes proporcionan una valoración para cada conjunto de recursos para cada horizonte de tiempo posible (desde [1, Tm]) que puedan utilizar. Sea Ω el conjunto de recursos a ser asignados entre los agentes. Un agente recibirá como máximo un paquete de recursos para uno de los horizontes temporales. Que la variable ψ ∈ Ψm enumere todos los posibles pares de conjuntos de recursos y horizontes temporales para el agente m, por lo que hay 2|Ω| × Tm valores para ψ (el espacio de conjuntos es exponencial en el número de tipos de recursos |Ω|). El agente m debe proporcionar un valor vψ m para cada ψ, y el coordinador asignará como máximo un par (recurso, horizonte temporal) ψ a cada agente. Esta asignación se expresa como una variable indicadora zψ m ∈ {0, 1} que muestra si ψ está asignado al agente m. Para el tiempo τ y el recurso ω, la función nm(ψ, τ, ω) ∈ {0, 1} indica si el paquete en ψ utiliza el recurso ω en el tiempo τ (hacemos la suposición de que los agentes tienen requisitos de recursos binarios). Este problema de asignación es NP-completo, incluso al considerar solo un paso de tiempo, y su dificultad aumenta significativamente con múltiples pasos de tiempo debido al creciente número de valores de ψ. El problema de encontrar una asignación óptima que satisfaga la restricción global de que la cantidad de cada recurso ω asignada a todos los agentes no exceda la cantidad disponible bϕ(ω) se puede expresar como el siguiente programa entero: max X m∈M X ψ∈Ψm zψ mvψ m sujeto a: X ψ∈Ψm zψ m ≤ 1, ∀m ∈ M; X m∈M X ψ∈Ψm zψ mnm(ψ, τ, ω) ≤ bϕ(ω), ∀τ ∈ [1, bτ], ∀ω ∈ Ω; (3) La primera restricción en la ecuación 3 dice que ningún agente puede recibir más de un paquete, y la segunda restricción asegura que la asignación total del recurso ω no exceda, en ningún momento, el límite del recurso. Para el problema de programación en el que los agentes pueden reasignar dinámicamente recursos, cada agente debe especificar un valor para cada combinación de paquetes y pasos de tiempo dentro de su horizonte temporal. Que la variable ψ ∈ Ψm en este caso enumere todos los posibles conjuntos de recursos para los cuales a lo sumo un conjunto puede ser asignado al agente m en cada paso de tiempo. Por lo tanto, en este caso hay P t∈[1,Tm](2|Ω| )t ∼ 2|Ω|Tm posibilidades de combinaciones de recursos asignadas a diferentes intervalos de tiempo, para los Tm horizontes de tiempo diferentes. El mismo conjunto de ecuaciones (3) se puede utilizar para resolver este problema de programación dinámica, pero el programa entero es diferente debido a la diferencia en cómo se define ψ. En este caso, el número de valores de ψ es exponencial en el horizonte de planificación Tm de cada agente, lo que resulta en un programa mucho más grande. Este enfoque directo para resolver ambos problemas de programación requiere una enumeración y solución de 2|Ω| Tm (asignación estática) o P t∈[1,Tm] 2|Ω|t (reubicación dinámica) MDPs para cada agente, lo cual rápidamente se vuelve intratable con el crecimiento del número de recursos |Ω| o el horizonte de tiempo Tm. MODELO Y DECLARACIÓN DEL PROBLEMA Ahora presentamos formalmente nuestro modelo del problema de programación de recursos. La entrada del problema consiste en los siguientes componentes: • M, Ω, bϕ, τa m, τd m, bτ están definidos arriba en la Sección 2.2. • {Θm} = {S, A, pm, rm, αm} son los MDPs de todos los agentes m ∈ M. Sin pérdida de generalidad, asumimos que los espacios de estados y acciones de todos los agentes son iguales, pero cada uno tiene su propia función de transición pm, función de recompensa rm, y condiciones iniciales αm. • ϕm : A×Ω → {0, 1} es la asignación de acciones a recursos para el agente m. ϕm(a, ω) indica si la acción a del agente m necesita el recurso ω. Un agente m que recibe un conjunto de recursos que no incluye el recurso ω no puede ejecutar en su política MDP ninguna acción a para la cual ϕm(a, ω) = 0. Suponemos que todos los requisitos de recursos son binarios; como se discute más adelante en la Sección 6, esta suposición no es limitante. Dado el input anterior, el problema de optimización que consideramos es encontrar la asignación globalmente óptima que maximice la suma de recompensas esperadas de recursos a agentes para todos los pasos de tiempo: Δ : τ × M × Ω → {0, 1}. Una solución es factible si la asignación correspondiente de recursos a los agentes no viola la restricción global de recursos: X m Δm(τ, ω) ≤ bϕ(ω), ∀ω ∈ Ω, τ ∈ [1, bτ]. Consideramos dos variantes del problema de programación de recursos. La primera formulación restringe las asignaciones de recursos al espacio donde la asignación de recursos a cada agente es estática durante la vida de los agentes. La segunda formulación permite la reasignación de recursos entre agentes en cada paso de tiempo durante sus vidas. La Figura 1 representa un problema de programación de recursos con tres agentes M = {m1, m2, m3}, tres recursos Ω = {ω1, ω2, ω3}, y un horizonte de problema global de bτ = 11. Los tiempos de llegada y salida de los agentes se muestran como cajas grises y son {1, 6}, {3, 7} y {2, 11}, respectivamente. Una solución a este problema se muestra a través de barras horizontales dentro de cada casilla de agentes, donde las barras corresponden a la asignación de los tres tipos de recursos. La Figura 1a muestra una solución a un problema de programación estática. Según la solución mostrada, el agente m1 comienza la ejecución de su MDP en el tiempo τ = 1 y tiene un bloqueo en los tres recursos hasta que finaliza la ejecución en el tiempo τ = 3. Se debe tener en cuenta que el agente m1 renuncia a su control sobre los recursos antes de su hora de salida anunciada de τd m1 = 6, aparentemente porque otros agentes pueden utilizar los recursos de manera más efectiva. Por lo tanto, en el tiempo τ = 4, los recursos ω1 y ω3 se asignan al agente m2, quien luego los utiliza para ejecutar su MDP (utilizando solo acciones admitidas por los recursos ω1 y ω3) hasta el tiempo τ = 7. El agente m3 posee el recurso ω3 durante el intervalo τ ∈ [4, 10]. La Figura 1b muestra una posible solución a la versión dinámica del mismo problema. Allí, los recursos pueden ser reasignados entre agentes en cada paso de tiempo. Por ejemplo, el agente m1 renuncia a su uso del recurso ω2 en el tiempo τ = 2, aunque continúa la ejecución de su MDP hasta el tiempo τ = 6. Ten en cuenta que a un agente no se le permite detener y reiniciar su MDP, por lo que el agente m1 solo puede continuar ejecutando en el intervalo τ ∈ [3, 4] si tiene acciones que no requieren recursos (ϕm(a, ω) = 0). Claramente, el modelo y la declaración del problema descritos anteriormente hacen una serie de suposiciones sobre el problema y las propiedades de la solución deseada. Discutimos algunas de esas suposiciones y sus implicaciones en la Sección 6. 1222 La Sexta Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) (a) (b) Figura 1: Ilustración de una solución a un problema de programación de recursos con tres agentes y tres recursos: a) asignaciones de recursos estáticas (las asignaciones de recursos son constantes durante la vida de los agentes); b) asignación dinámica (las asignaciones de recursos pueden cambiar en cada paso de tiempo). 4. PROGRAMACIÓN DE RECURSOS Nuestro algoritmo de programación de recursos se lleva a cabo en dos etapas. Primero, realizamos un paso de preprocesamiento que aumenta los MDP de los agentes; este proceso se describe en la Sección 4.1. Segundo, utilizando estos MDPs aumentados construimos un problema de optimización global, el cual se describe en la Sección 4.2. 4.1 Aumentando los MDPs de los Agentes En el modelo descrito en la sección anterior, asumimos que si un agente no posee los recursos necesarios para realizar acciones en su MDP, su ejecución se detiene y el agente abandona el sistema. En otras palabras, los MDPs no pueden ser pausados y reanudados. Por ejemplo, en el problema mostrado en la Figura 1a, el agente m1 libera todos los recursos después del tiempo τ = 3, momento en el cual la ejecución de su MDP se detiene. De manera similar, los agentes m2 y m3 solo ejecutan sus MDPs en los intervalos τ ∈ [4, 6] y τ ∈ [4, 10], respectivamente. Por lo tanto, una parte importante del problema de toma de decisiones global es decidir la ventana de tiempo durante la cual cada uno de los agentes está activo (es decir, ejecutando su MDP). Para lograr esto, ampliamos el MDP de cada agente con dos nuevos estados (estados de inicio y final sb, sf, respectivamente) y una nueva acción de inicio/parada a∗, como se ilustra en la Figura 2. La idea es que un agente permanezca en el estado inicial sb hasta que esté listo para ejecutar su MDP, momento en el que realiza la acción de inicio/parada a∗ y transita al espacio de estados del MDP original con la probabilidad de transición que corresponde a la distribución inicial original α(s). Por ejemplo, en la Figura 1a, para el agente m2 esto ocurriría en el tiempo τ = 4. Una vez que el agente llega al final de su ventana de actividad (tiempo τ = 6 para el agente m2 en la Figura 1a), realiza la acción de inicio/parada, lo que lo lleva al estado final de finalización del sumidero sf en el tiempo τ = 7. Más precisamente, dado un MDP S, A, pm, rm, αm, definimos un MDP aumentado S, A, pm, rm, αm de la siguiente manera: S = S ∪ sb ∪ sf; A = A ∪ a∗; p(s|sb, a∗) = α(s), ∀s ∈ S; p(sb|sb, a) = 1.0, ∀a ∈ A; p(sf|s, a∗) = 1.0, ∀s ∈ S; p(σ|s, a) = p(σ|s, a), ∀s, σ ∈ S, a ∈ A; r(sb, a) = r(sf, a) = 0, ∀a ∈ A; r(s, a) = r(s, a), ∀s ∈ S, a ∈ A; α(sb) = 1; α(s) = 0, ∀s ∈ S; donde se asume que todas las probabilidades de transición no especificadas son cero. Además, para tener en cuenta el nuevo estado inicial, comenzamos el MDP un paso de tiempo antes, estableciendo τa m ← τa m − 1. Esto no afectará la asignación de recursos debido a que las restricciones de recursos solo se aplican a los estados originales del MDP, como se discutirá en la siguiente sección. Por ejemplo, los MDPs aumentados mostrados en la Figura 2b (que comienza en el estado sb en el tiempo τ = 2) se construirían a partir de un MDP con tiempo de llegada original τ = 3. La Figura 2b también muestra una trayectoria de muestra a través del espacio de estados: el agente comienza en el estado sb, transita al espacio de estados S del MDP original y finalmente sale al estado de sumidero sf. Cabe destacar que si quisiéramos modelar un problema en el que los agentes pudieran pausar sus MDP en pasos de tiempo arbitrarios (lo cual podría ser útil para dominios donde la reasignación dinámica es posible), podríamos lograrlo fácilmente incluyendo una acción adicional que transiciona de cada estado a sí mismo con una recompensa de cero. 4.2 MILP para la programación de recursos Dado un conjunto de MDPs aumentados, como se define arriba, el objetivo de esta sección es formular un programa de optimización global que resuelva el problema de programación de recursos. En esta sección y a continuación, se asume que todos los MDPs son los MDPs aumentados tal como se definen en la Sección 4.1. Nuestro enfoque es similar a la idea utilizada en [6]: comenzamos con la formulación del programa lineal de los MDP de los agentes (1) y lo complementamos con restricciones que aseguran que la asignación de recursos correspondiente entre agentes y tiempo sea válida. El problema de optimización resultante resuelve simultáneamente los MDP de los agentes y los problemas de programación de recursos. En el resto de esta sección, desarrollamos de forma incremental un programa de enteros mixtos (MILP) que logra esto. En ausencia de restricciones de recursos, los MDPs de horizonte finito de los agentes son completamente independientes, y la solución óptima global puede obtenerse trivialmente a través del siguiente LP, que es simplemente una agregación de LPs de horizonte finito de un solo agente: max X m X s X a rm(s, a) X t xm(s, a, t) sujeto a: X a xm(σ, a, t + 1) = X s,a pm(σ|s, a)xm(s, a, t), ∀m ∈ M, σ ∈ S, t ∈ [1, Tm − 1]; X a xm(s, a, 1) = αm(s), ∀m ∈ M, s ∈ S; (12) donde xm(s, a, t) es la medida de ocupación del agente m, y The Sixth Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 1223 (a) (b) Figura 2: Ilustración de la ampliación de un MDP para permitir tiempos de inicio y finalización variables: a) (izquierda) el MDP original de dos estados con una sola acción; (derecha) el MDP ampliado con nuevos estados sb y sf y la nueva acción a∗ (nota que las transiciones originales no se modifican en el proceso de ampliación); b) el MDP ampl augmentado mostrado como una trayectoria a través del tiempo (las líneas grises indican todas las transiciones, mientras que las líneas negras indican una trayectoria dada). Función objetivo (suma de recompensas esperadas de todos los agentes) máx X m X s X a rm(s, a) X t xm(s, a, t) (5) Implicación de significado Restricciones lineales Atar x a θ. El agente solo está activo cuando la medida de ocupación es distinta de cero en los estados originales del MDP. θm(τ) = 0 =⇒ xm(s, a, τ −τa m+1) = 0 ∀s /∈ {sb , sf }, a ∈ A X s/∈{sb,sf } X a xm(s, a, t) ≤ θm(τa m + t − 1) ∀m ∈ M, ∀t ∈ [1, Tm] (6) El agente solo puede estar activo en τ ∈ (τa m, τd m) θm(τ) = 0 ∀m ∈ M, τ /∈ (τa m, τd m) (7) No se pueden utilizar recursos cuando no está activo θm(τ) = 0 =⇒ Δm(τ, ω) = 0 ∀τ ∈ [0, bτ], ω ∈ Ω Δm(τ, ω) ≤ θm(τ) ∀m ∈ M, τ ∈ [0, bτ], ω ∈ Ω (8) Vincular x a Δ (x distinto de cero obliga a que el correspondiente Δ sea distinto de cero). Δm(τ, ω) = 0, ϕm(a, ω) = 1 =⇒ xm(s, a, τ − τa m + 1) = 0 ∀s /∈ {sb , sf } 1/|A| X a ϕm(a, ω) X s/∈{sb,sf } xm(s, a, t) ≤ Δm(t + τa m − 1, ω) ∀m ∈ M, ω ∈ Ω, t ∈ [1, Tm] (9) Límites de recursos X m Δm(τ, ω) ≤ bϕ(ω) ∀ω ∈ Ω, τ ∈ [0, bτ] (10) El agente no puede cambiar los recursos mientras está activo. Solo habilitado para programación con asignaciones estáticas. θm(τ) = 1 y θm(τ + 1) = 1 =⇒ Δm(τ, ω) = Δm(τ + 1, ω) Δm(τ, ω) − Z(1 − θm(τ + 1)) ≤ Δm(τ + 1, ω) + Z(1 − θm(τ)) Δm(τ, ω) + Z(1 − θm(τ + 1)) ≥ Δm(τ + 1, ω) − Z(1 − θm(τ)) ∀m ∈ M, ω ∈ Ω, τ ∈ [0, bτ] (11) Tabla 1: MILP para programación óptima global de recursos. Tm = τd m − τa m + 1 es el horizonte temporal para los agentes MDP. Usando este LP como base, lo ampliamos con restricciones que aseguran que el uso de recursos implicado por las medidas de ocupación de los agentes {xm} no viola los requisitos globales de recursos bϕ en ningún paso de tiempo τ ∈ [0, bτ]. Para formular estas restricciones de recursos, utilizamos las siguientes variables binarias: • Δm(τ, ω) = {0, 1}, ∀m ∈ M, τ ∈ [0, bτ], ω ∈ Ω, que sirven como variables indicadoras que definen si el agente m posee el recurso ω en el tiempo τ. Estos son análogos a las variables indicadoras estáticas utilizadas en el problema de asignación de recursos estáticos de una sola vez en [6]. • θm = {0, 1}, ∀m ∈ M, τ ∈ [0, bτ] son variables indicadoras que especifican si el agente m está activo (es decir, ejecutando su MDP) en el tiempo τ. El significado de las variables de uso de recursos Δ se ilustra en la Figura 1: Δm(τ, ω) = 1 solo si el recurso ω se asigna al agente m en el tiempo τ. El significado de los indicadores de actividad θ se ilustra en la Figura 2b: cuando el agente m se encuentra en el estado inicial sb o en el estado final sf, el θm correspondiente es igual a 0, pero una vez que el agente se vuelve activo y entra en uno de los otros estados, establecemos θm = 1. Este significado de θ puede ser impuesto con una restricción lineal que sincroniza los valores de las medidas de ocupación de los agentes xm y la actividad 1224 The Sixth Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) indicadores θ, como se muestra en (6) en la Tabla 1. Otra restricción que debemos agregar, debido a que los indicadores de actividad θ están definidos en la línea de tiempo global τ, es hacer cumplir el hecho de que el agente está inactivo fuera de su ventana de llegada y salida. Esto se logra mediante la restricción (7) en la Tabla 1. Además, los agentes no deben estar utilizando recursos mientras están inactivos. Esta restricción también puede ser impuesta a través de una desigualdad lineal en θ y Δ, como se muestra en (8). La restricción (6) establece el valor de θ para que coincida con la política definida por la medida de ocupación xm. De manera similar, debemos asegurarnos de que las variables de uso de recursos Δ estén también sincronizadas con la medida de ocupación xm. Esto se realiza a través de la restricción (9) en la Tabla 1, que es casi idéntica a la restricción análoga de [6]. Después de implementar la restricción anterior, que refuerza el significado de Δ, agregamos una restricción que garantiza que el uso de recursos de los agentes nunca exceda los montos de recursos disponibles. Esta condición también se expresa trivialmente como una desigualdad lineal (10) en la Tabla 1. Finalmente, para la formulación del problema en la que las asignaciones de recursos son estáticas durante la vida de un agente, agregamos una restricción que garantiza que las variables de uso de recursos Δ no cambien su valor mientras el agente está activo (θ = 1). Esto se logra a través de la restricción lineal (11), donde Z ≥ 2 es una constante que se utiliza para desactivar las restricciones cuando θm(τ) = 0 o θm(τ + 1) = 0. Esta restricción no se utiliza para la formulación del problema dinámico, donde los recursos pueden ser reasignados entre agentes en cada paso de tiempo. En resumen, la Tabla 1 junto con las restricciones de conservación de flujo de (12) define el MILP que calcula simultáneamente una asignación óptima de recursos para todos los agentes a lo largo del tiempo, así como políticas óptimas de MDP de horizonte finito que son válidas bajo esa asignación de recursos. Como una medida aproximada de la complejidad de este MILP, consideremos el número de variables de optimización y restricciones. Sea TM = P Tm = P m(τa m − τd m + 1) la suma de las longitudes de las ventanas de llegada-salida a través de todos los agentes. Entonces, el número de variables de optimización es: TM + bτ|M||Ω| + bτ|M|, de las cuales TM son continuas (xm), y bτ|M||Ω| + bτ|M| son binarias (Δ y θ). Sin embargo, hay que tener en cuenta que todos menos TM|M| de los θ se establecen en cero por la restricción (7), lo que también obliga inmediatamente a que todos menos TM|M||Ω| de los Δ sean cero a través de las restricciones (8). El número de restricciones (sin incluir las restricciones degeneradas en (7)) en el MILP es: TM + TM|Ω| + bτ|Ω| + bτ|M||Ω|. A pesar de que la complejidad del MILP es, en el peor de los casos, exponencial en el número de variables binarias, la complejidad de este MILP es significativamente (exponencialmente) menor que la del MILP con funciones de utilidad planas, descrito en la Sección 2.2. Este resultado refleja las ganancias de eficiencia reportadas en [6] para problemas de asignación de recursos de un solo disparo, pero es mucho más pronunciado, debido a la explosión de la representación de utilidad plana debido al aspecto temporal del problema (recuerde la complejidad prohibitiva de la optimización combinatoria en la Sección 2.2). Analizamos empíricamente el rendimiento de este método en la Sección 5.1. Estrictamente hablando, resolver MILPs hasta la optimalidad es NP-completo en el número de variables enteras. RESULTADOS EXPERIMENTALES Aunque la complejidad de resolver MILPs es en el peor de los casos exponencial en el número de variables enteras, existen muchos métodos eficientes para resolver MILPs que permiten que nuestro algoritmo se adapte bien a parámetros comunes en problemas de asignación de recursos y programación. En particular, esta sección introduce un dominio de problema: el problema del taller de reparación, que se utiliza para evaluar empíricamente la escalabilidad de nuestros algoritmos en términos del número de agentes |M|, el número de recursos compartidos |Ω| y las diferentes longitudes de tiempo global bτ durante las cuales los agentes pueden entrar y salir del sistema. El problema del taller de reparaciones es un MDP parametrizado simple que adopta la metáfora de un taller de reparaciones de vehículos. Los agentes en el taller de reparación son mecánicos con una serie de tareas independientes que solo generan recompensa cuando se completan. En nuestro modelo MDP de este sistema, las acciones tomadas para avanzar a través del espacio de estados solo están permitidas si el agente posee ciertos recursos que están públicamente disponibles para la tienda. Estos recursos son de suministro finito, y las políticas óptimas para la tienda determinarán cuándo cada agente puede retener los recursos limitados para tomar acciones y obtener recompensas individuales. Cada tarea a completar está asociada con una sola acción, aunque se requiere que el agente repita la acción numerosas veces antes de completar la tarea y obtener una recompensa. Este modelo fue parametrizado en términos del número de agentes en el sistema, el número de diferentes tipos de recursos que podrían estar vinculados a acciones necesarias, un tiempo global durante el cual se permite a los agentes llegar y partir, y una longitud máxima para el número de pasos de tiempo que un agente puede permanecer en el sistema. Todos los puntos de datos en nuestros experimentos se obtuvieron con 20 evaluaciones utilizando CPLEX para resolver los MILPs en una computadora Pentium4 con 2Gb de RAM. Se realizaron pruebas tanto en la versión estática como en la dinámica del problema de programación de recursos, tal como se definió anteriormente. La Figura 3 muestra el tiempo de ejecución y el valor de la política para modificaciones independientes en el conjunto de parámetros. La fila superior muestra cómo el tiempo de solución para el MILP escala a medida que aumentamos el número de agentes |M|, el horizonte de tiempo global bτ y el número de recursos |Ω|. Aumentar el número de agentes conduce a una escalabilidad exponencial en complejidad, lo cual es de esperarse para un problema NP-completo. Sin embargo, aumentar el límite de tiempo global bτ o el número total de tipos de recursos |Ω|, manteniendo constante el número de agentes, no conduce a una disminución en el rendimiento. Esto ocurre porque los problemas se vuelven más fáciles a medida que se vuelven sub-determinados, lo cual también es un fenómeno común en problemas NP-completos. También observamos que la solución a la versión dinámica del problema a menudo se puede calcular mucho más rápido que la versión estática. La fila inferior de la Figura 3 muestra el valor de la política conjunta de las políticas que corresponden a los horarios óptimos de asignación de recursos calculados. Podemos observar que la versión dinámica produce una recompensa más alta (como era de esperar, ya que la recompensa de la versión dinámica siempre es igual o mayor que la recompensa de la versión estática). Deberíamos señalar que estos gráficos no deben ser vistos como una medida del rendimiento de dos algoritmos diferentes (ambos algoritmos producen soluciones óptimas pero para problemas distintos), sino más bien como observaciones sobre cómo cambia la calidad de las soluciones óptimas a medida que se permite más flexibilidad en la reasignación de recursos. La Figura 4 muestra el tiempo de ejecución y el valor de la política para las pruebas en las que las variables de entrada comunes se escalan juntas. Esto permite la Sexta Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 1225 2 4 6 8 10 10 −3 10 −2 10 −1 10 0 10 1 10 2 10 3 10 4 Número de Agentes |M| Tiempo de CPU, seg |Ω| = 5, τ = 50 estático dinámico 50 100 150 200 10 −2 10 −1 10 0 10 1 10 2 10 3 Límite de Tiempo Global τ Tiempo de CPU, seg |M| = 5, |Ω| = 5 estático dinámico 10 20 30 40 50 10 −2 10 −1 10 0 10 1 10 2 Número de Recursos |Ω| Tiempo de CPU, seg |M| = 5, τ = 50 estático dinámico 2 4 6 8 10 200 400 600 800 1000 1200 1400 1600 Número de Agentes |M| Valor |Ω| = 5, τ = 50 estático dinámico 50 100 150 200 400 500 600 700 800 900 1000 1100 1200 1300 1400 Límite de Tiempo Global τ Valor |M| = 5, |Ω| = 5 estático dinámico 10 20 30 40 50 500 600 700 800 900 1000 1100 1200 1300 1400 Número de Recursos |Ω| Valor |M| = 5, τ = 50 estático dinámico Figura 3: Evaluación de nuestro MILP para números variables de agentes (columna 1), longitudes de ventana de tiempo global (columna 2) y números de tipos de recursos (columna 3). La fila superior muestra el tiempo de CPU, y la fila inferior muestra la recompensa conjunta de las políticas MDP de los agentes. Las barras de error muestran los cuartiles 1ro y 3ro (25% y 75%). 2 4 6 8 10 10 −3 10 −2 10 −1 10 0 10 1 10 2 10 3 Número de Agentes |M| Tiempo de CPU, seg τ = 10|M| estático dinámico 2 4 6 8 10 10 −3 10 −2 10 −1 10 0 10 1 10 2 10 3 10 4 Número de Agentes |M| Tiempo de CPU, seg |Ω| = 2|M| estático dinámico 2 4 6 8 10 10 −3 10 −2 10 −1 10 0 10 1 10 2 10 3 10 4 Número de Agentes |M| Tiempo de CPU, seg |Ω| = 5|M| estático dinámico 2 4 6 8 10 200 400 600 800 1000 1200 1400 1600 1800 2000 2200 Número de Agentes |M| Valor τ = 10|M| estático dinámico 2 4 6 8 10 200 400 600 800 1000 1200 1400 1600 1800 2000 Número de Agentes |M| Valor |Ω| = 2|M| estático dinámico 2 4 6 8 10 0 500 1000 1500 2000 2500 Número de Agentes |M| Valor |Ω| = 5|M| estático dinámico Figura 4: Evaluación de nuestro MILP utilizando variables de entrada correlacionadas. La columna izquierda sigue el rendimiento y el tiempo de CPU a medida que aumenta el número de agentes y la ventana de tiempo global juntos (bτ = 10|M|). La columna del medio y la columna de la derecha siguen el rendimiento y el tiempo de CPU a medida que aumenta el número de recursos y el número de agentes juntos, con |Ω| = 2|M| y |Ω| = 5|M|, respectivamente. Las barras de error muestran el primer y tercer cuartil (25% y 75%). 1226 The Sixth Intl. La Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) nos permite explorar dominios donde el número total de agentes escala proporcionalmente al número total de tipos de recursos o al horizonte temporal global, manteniendo constante la densidad promedio de agentes (por unidad de tiempo global) o el número promedio de recursos por agente (lo cual comúnmente ocurre en aplicaciones de la vida real). En general, creemos que estos resultados experimentales indican que nuestra formulación MILP puede ser utilizada para resolver de manera efectiva problemas de programación de recursos de tamaño no trivial. DISCUSIÓN Y CONCLUSIONES A lo largo del documento, hemos realizado una serie de suposiciones en nuestro modelo y algoritmo de solución; discutimos sus implicaciones a continuación. • Ejecución continua. Suponemos que una vez que un agente deja de ejecutar su MDP (transiciona al estado sf), sale del sistema y no puede regresar. Es fácil relajar esta suposición para dominios donde los agentes de MDP pueden ser pausados y reiniciados. Todo lo que se requiere es incluir una acción de pausa adicional que haga la transición desde un estado dado de regreso a sí mismo, y que tenga una recompensa de cero. • Indiferencia al tiempo de inicio. Utilizamos un modelo de recompensa donde las recompensas de los agentes dependen solo del horizonte temporal de sus MDP y no del tiempo de inicio global. Esta es una consecuencia de nuestro procedimiento de aumento de MDP de la Sección 4.1. Es fácil extender el modelo para que los agentes incurran en una penalización explícita por inactividad al asignar una recompensa negativa no nula al estado inicial sb. • Requisitos de recursos binarios. Para simplificar, hemos asumido que los costos de recursos son binarios: ϕm(a, ω) = {0, 1}, pero nuestros resultados se generalizan de manera directa a asignaciones de recursos no binarias, de manera análoga al procedimiento utilizado en [5]. • Agentes cooperativos. El procedimiento de optimización discutido en este artículo fue desarrollado en el contexto de agentes cooperativos, pero también puede ser utilizado para diseñar un mecanismo para programar recursos entre agentes egoístas. Este procedimiento de optimización puede ser incorporado en una subasta de Vickrey-Clarke-Groves, de manera completamente análoga a como se hizo en [7]. De hecho, todos los resultados de [7] sobre las propiedades de la subasta y la privacidad de la información se aplican directamente al dominio de programación discutido en este documento, requiriendo solo ligeras modificaciones para tratar con MDPs de horizonte finito. • Tiempos de llegada y salida conocidos y deterministas. Finalmente, hemos asumido que los tiempos de llegada y salida de los agentes (τa m y τd m) son deterministas y conocidos de antemano. Esta suposición es fundamental para nuestro método de solución. Si bien hay muchos dominios donde esta suposición es válida, en muchos casos los agentes llegan y se van de forma dinámica y sus tiempos de llegada y salida solo pueden predecirse de manera probabilística, lo que lleva a problemas de asignación de recursos en línea. En particular, en el caso de agentes con interés propio, esto se convierte en una versión interesante de un problema de diseño de mecanismos en línea [11, 12]. En resumen, hemos presentado una formulación MILP para el problema combinatorio de programación de recursos donde los valores de los agentes para posibles asignaciones de recursos están definidos por MDPs de horizonte finito. Este resultado amplía el trabajo previo ([6, 7]) sobre asignación estática de recursos de una sola vez bajo preferencias inducidas por MDP a problemas de programación de recursos con un aspecto temporal. Por lo tanto, este trabajo da un paso en la dirección de diseñar un mecanismo en línea para agentes con preferencias de recursos combinatorios inducidas por problemas de planificación estocástica. Relajar la suposición sobre los tiempos deterministas de llegada y salida de los agentes es un enfoque de nuestro trabajo futuro. Nos gustaría agradecer a los revisores anónimos por sus comentarios y sugerencias perspicaces. 7. REFERENCIAS [1] E. Altman y A. Shwartz. Control adaptativo de cadenas de Markov restringidas: Criterios y políticas. Anales de Investigación de Operaciones, número especial sobre Procesos de Decisión de Markov, 28:101-134, 1991. [2] R. Bellman. Programación dinámica. Princeton University Press, 1957. [3] C. Boutilier. \n\nPrensa de la Universidad de Princeton, 1957. [3] C. Boutilier. Resolviendo problemas de subasta combinatoria expresados de manera concisa. En Proc. de AAAI-02, páginas 359-366, 2002. [4] C. Boutilier y H. H. Hoos. Subastando idiomas para subastas combinatorias. En Proc. de IJCAI-01, páginas 1211-1217, 2001. [5] D. Dolgov. Asignación integrada de recursos y planificación en entornos estocásticos de múltiples agentes. Tesis doctoral, Departamento de Ciencias de la Computación, Universidad de Michigan, febrero de 2006. [6] D. A. Dolgov y E. H. Durfee. Asignación óptima de recursos y formulación de políticas en procesos de decisión de Markov débilmente acoplados. En Proc. de ICAPS-04, páginas 315-324, junio de 2004. [7] D. A. Dolgov y E. H. Durfee. Subastas combinatorias computacionalmente eficientes para asignación de recursos en MDPs débilmente acoplados. En Proc. de AAMAS-05, Nueva York, NY, EE. UU., 2005. ACM Press. [8] D. A. Dolgov y E. H. Durfee. Asignación de recursos entre agentes con preferencias inducidas por MDP factorizados. En Proc. de AAMAS-06, 2006. [9] K. Larson y T. Sandholm. Diseño de mecanismos y agentes deliberativos. En Proc. de AAMAS-05, páginas 650-656, Nueva York, NY, EE. UU., 2005. ACM Press. [10] N. Nisan. \n\nACM Press. [10] N. Nisan. Subasta y asignación en subastas combinatorias. En Comercio Electrónico, 2000. [11] D. C. Parkes y S. Singh. Un enfoque basado en MDP para el Diseño de Mecanismos en Línea. En Proc. de la Decimoséptima Conferencia Anual sobre Sistemas de Procesamiento de Información Neural (NIPS-03), 2003. [12] D. C. Parkes, S. Singh y D. Yanovsky. Diseño de mecanismos en línea aproximadamente eficiente. En Actas de la Decimoctava Conferencia Anual sobre Sistemas de Procesamiento de Información Neural (NIPS-04), 2004. [13] M. L. Puterman. Procesos de Decisión de Markov. John Wiley & Sons, Nueva York, 1994. [14] M. H. Rothkopf, A. Pekec y R. M. Harstad. Subastas combinatorias manejables computacionalmente. Ciencia de la Gestión, 44(8):1131-1147, 1998. [15] T. Sandholm. Un algoritmo para la determinación óptima del ganador en subastas combinatorias. En Proc. de IJCAI-99, páginas 542-547, San Francisco, CA, EE. UU., 1999. Morgan Kaufmann Publishers Inc. - Editores Morgan Kaufmann Inc. El Sexto Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 1227 ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "resource": {
            "translated_key": "",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Combinatorial <br>resource</br> Scheduling for Multiagent MDPs Dmitri A. Dolgov, Michael R. James, and Michael E. Samples AI and Robotics Group Technical Research, Toyota Technical Center USA {ddolgov, michael.r.james, michael.samples}@gmail.com ABSTRACT Optimal <br>resource</br> scheduling in multiagent systems is a computationally challenging task, particularly when the values of resources are not additive.",
                "We consider the combinatorial problem of scheduling the usage of multiple resources among agents that operate in stochastic environments, modeled as Markov decision processes (MDPs).",
                "In recent years, efficient <br>resource</br>-allocation algorithms have been developed for agents with <br>resource</br> values induced by MDPs.",
                "However, this prior work has focused on static <br>resource</br>-allocation problems where resources are distributed once and then utilized in infinite-horizon MDPs.",
                "We extend those existing models to the problem of combinatorial <br>resource</br> scheduling, where agents persist only for finite periods between their (predefined) arrival and departure times, requiring resources only for those time periods.",
                "We provide a computationally efficient procedure for computing globally optimal <br>resource</br> assignments to agents over time.",
                "We illustrate and empirically analyze the method in the context of a stochastic jobscheduling domain.",
                "Categories and Subject Descriptors I.2.8 [Artificial Intelligence]: Problem Solving, Control Methods, and Search; I.2.11 [Artificial Intelligence]: Distributed Artificial Intelligence-Multiagent systems General Terms Algorithms, Performance, Design 1.",
                "INTRODUCTION The tasks of optimal <br>resource</br> allocation and scheduling are ubiquitous in multiagent systems, but solving such optimization problems can be computationally difficult, due to a number of factors.",
                "In particular, when the value of a set of resources to an agent is not additive (as is often the case with resources that are substitutes or complements), the utility function might have to be defined on an exponentially large space of <br>resource</br> bundles, which very quickly becomes computationally intractable.",
                "Further, even when each agent has a utility function that is nonzero only on a small subset of the possible <br>resource</br> bundles, obtaining optimal allocation is still computationally prohibitive, as the problem becomes NP-complete [14].",
                "Such computational issues have recently spawned several threads of work in using compact models of agents preferences.",
                "One idea is to use any structure present in utility functions to represent them compactly, via, for example, logical formulas [15, 10, 4, 3].",
                "An alternative is to directly model the mechanisms that define the agents utility functions and perform <br>resource</br> allocation directly with these models [9].",
                "A way of accomplishing this is to model the processes by which an agent might utilize the resources and define the utility function as the payoff of these processes.",
                "In particular, if an agent uses resources to act in a stochastic environment, its utility function can be naturally modeled with a Markov decision process, whose action set is parameterized by the available resources.",
                "This representation can then be used to construct very efficient <br>resource</br>-allocation algorithms that lead to an exponential speedup over a straightforward optimization problem with flat representations of combinatorial preferences [6, 7, 8].",
                "However, this existing work on <br>resource</br> allocation with preferences induced by <br>resource</br>-parameterized MDPs makes an assumption that the resources are only allocated once and are then utilized by the agents independently within their infinite-horizon MDPs.",
                "This assumption that no reallocation of resources is possible can be limiting in domains where agents arrive and depart dynamically.",
                "In this paper, we extend the work on <br>resource</br> allocation under MDP-induced preferences to discrete-time scheduling problems, where agents are present in the system for finite time intervals and can only use resources within these intervals.",
                "In particular, agents arrive and depart at arbitrary (predefined) times and within these intervals use resources to execute tasks in finite-horizon MDPs.",
                "We address the problem of globally optimal <br>resource</br> scheduling, where the objective is to find an allocation of resources to the agents across time that maximizes the sum of the expected rewards that they obtain.",
                "In this context, our main contribution is a mixed-integerprogramming formulation of the scheduling problem that chooses globally optimal <br>resource</br> assignments, starting times, and execution horizons for all agents (within their arrival1220 978-81-904262-7-5 (RPS) c 2007 IFAAMAS departure intervals).",
                "We analyze and empirically compare two flavors of the scheduling problem: one, where agents have static <br>resource</br> assignments within their finite-horizon MDPs, and another, where resources can be dynamically reallocated between agents at every time step.",
                "In the rest of the paper, we first lay down the necessary groundwork in Section 2 and then introduce our model and formal problem statement in Section 3.",
                "In Section 4.2, we describe our main result, the optimization program for globally optimal <br>resource</br> scheduling.",
                "Following the discussion of our experimental results on a job-scheduling problem in Section 5, we conclude in Section 6 with a discussion of possible extensions and generalizations of our method. 2.",
                "BACKGROUND Similarly to the model used in previous work on resourceallocation with MDP-induced preferences [6, 7], we define the value of a set of resources to an agent as the value of the best MDP policy that is realizable, given those resources.",
                "However, since the focus of our work is on scheduling problems, and a large part of the optimization problem is to decide how resources are allocated in time among agents with finite arrival and departure times, we model the agents planning problems as finite-horizon MDPs, in contrast to previous work that used infinite-horizon discounted MDPs.",
                "In the rest of this section, we first introduce some necessary background on finite-horizon MDPs and present a linear-programming formulation that serves as the basis for our solution algorithm developed in Section 4.",
                "We also outline the standard methods for combinatorial <br>resource</br> scheduling with flat <br>resource</br> values, which serve as a comparison benchmark for the new model developed here. 2.1 Markov Decision Processes A stationary, finite-domain, discrete-time MDP (see, for example, [13] for a thorough and detailed development) can be described as S, A, p, r , where: S is a finite set of system states; A is a finite set of actions that are available to the agent; p is a stationary stochastic transition function, where p(σ|s, a) is the probability of transitioning to state σ upon executing action a in state s; r is a stationary reward function, where r(s, a) specifies the reward obtained upon executing action a in state s. Given such an MDP, a decision problem under a finite horizon T is to choose an optimal action at every time step to maximize the expected value of the total reward accrued during the agents (finite) lifetime.",
                "The agents optimal policy is then a function of current state s and the time until the horizon.",
                "An optimal policy for such a problem is to act greedily with respect to the optimal value function, defined recursively by the following system of finite-time Bellman equations [2]: v(s, t) = max a r(s, a) + X σ p(σ|s, a)v(σ, t + 1), ∀s ∈ S, t ∈ [1, T − 1]; v(s, T) = 0, ∀s ∈ S; where v(s, t) is the optimal value of being in state s at time t ∈ [1, T].",
                "This optimal value function can be easily computed using dynamic programming, leading to the following optimal policy π, where π(s, a, t) is the probability of executing action a in state s at time t: π(s, a, t) = ( 1, a = argmaxa r(s, a) + P σ p(σ|s, a)v(σ, t + 1), 0, otherwise.",
                "The above is the most common way of computing the optimal value function (and therefore an optimal policy) for a finite-horizon MDP.",
                "However, we can also formulate the problem as the following linear program (similarly to the dual LP for infinite-horizon discounted MDPs [13, 6, 7]): max X s X a r(s, a) X t x(s, a, t) subject to: X a x(σ, a, t + 1) = X s,a p(σ|s, a)x(s, a, t) ∀σ, t ∈ [1, T − 1]; X a x(s, a, 1) = α(s), ∀s ∈ S; (1) where α(s) is the initial distribution over the state space, and x is the (non-stationary) occupation measure (x(s, a, t) ∈ [0, 1] is the total expected number of times action a is executed in state s at time t).",
                "An optimal (non-stationary) policy is obtained from the occupation measure as follows: π(s, a, t) = x(s, a, t)/ X a x(s, a, t) ∀s ∈ S, t ∈ [1, T]. (2) Note that the standard unconstrained finite-horizon MDP, as described above, always has a uniformly-optimal solution (optimal for any initial distribution α(s)).",
                "Therefore, an optimal policy can be obtained by using an arbitrary constant α(s) > 0 (in particular, α(s) = 1 will result in x(s, a, t) = π(s, a, t)).",
                "However, for MDPs with <br>resource</br> constraints (as defined below in Section 3), uniformly-optimal policies do not in general exist.",
                "In such cases, α becomes a part of the problem input, and a resulting policy is only optimal for that particular α.",
                "This result is well known for infinite-horizon MDPs with various types of constraints [1, 6], and it also holds for our finite-horizon model, which can be easily established via a line of reasoning completely analogous to the arguments in [6]. 2.2 Combinatorial <br>resource</br> Scheduling A straightforward approach to <br>resource</br> scheduling for a set of agents M, whose values for the resources are induced by stochastic planning problems (in our case, finite-horizon MDPs) would be to have each agent enumerate all possible resource assignments over time and, for each one, compute its value by solving the corresponding MDP.",
                "Then, each agent would provide valuations for each possible <br>resource</br> bundle over time to a centralized coordinator, who would compute the optimal <br>resource</br> assignments across time based on these valuations.",
                "When resources can be allocated at different times to different agents, each agent must submit valuations for every combination of possible time horizons.",
                "Let each agent m ∈ M execute its MDP within the arrival-departure time interval τ ∈ [τa m, τd m].",
                "Hence, agent m will execute an MDP with time horizon no greater than Tm = τd m−τa m+1.",
                "Let bτ be the global time horizon for the problem, before which all of the agents MDPs must finish.",
                "We assume τd m < bτ, ∀m ∈ M. The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1221 For the scheduling problem where agents have static <br>resource</br> requirements within their finite-horizon MDPs, the agents provide a valuation for each <br>resource</br> bundle for each possible time horizon (from [1, Tm]) that they may use.",
                "Let Ω be the set of resources to be allocated among the agents.",
                "An agent will get at most one <br>resource</br> bundle for one of the time horizons.",
                "Let the variable ψ ∈ Ψm enumerate all possible pairs of <br>resource</br> bundles and time horizons for agent m, so there are 2|Ω| × Tm values for ψ (the space of bundles is exponential in the number of <br>resource</br> types |Ω|).",
                "The agent m must provide a value vψ m for each ψ, and the coordinator will allocate at most one ψ (<br>resource</br>, time horizon) pair to each agent.",
                "This allocation is expressed as an indicator variable zψ m ∈ {0, 1} that shows whether ψ is assigned to agent m. For time τ and <br>resource</br> ω, the function nm(ψ, τ, ω) ∈ {0, 1} indicates whether the bundle in ψ uses <br>resource</br> ω at time τ (we make the assumption that agents have binary resource requirements).",
                "This allocation problem is NP-complete, even when considering only a single time step, and its difficulty increases significantly with multiple time steps because of the increasing number of values of ψ.",
                "The problem of finding an optimal allocation that satisfies the global constraint that the amount of each <br>resource</br> ω allocated to all agents does not exceed the available amount bϕ(ω) can be expressed as the following integer program: max X m∈M X ψ∈Ψm zψ mvψ m subject to: X ψ∈Ψm zψ m ≤ 1, ∀m ∈ M; X m∈M X ψ∈Ψm zψ mnm(ψ, τ, ω) ≤ bϕ(ω), ∀τ ∈ [1, bτ], ∀ω ∈ Ω; (3) The first constraint in equation 3 says that no agent can receive more than one bundle, and the second constraint ensures that the total assignment of <br>resource</br> ω does not, at any time, exceed the resource bound.",
                "For the scheduling problem where the agents are able to dynamically reallocate resources, each agent must specify a value for every combination of bundles and time steps within its time horizon.",
                "Let the variable ψ ∈ Ψm in this case enumerate all possible <br>resource</br> bundles for which at most one bundle may be assigned to agent m at each time step.",
                "Therefore, in this case there are P t∈[1,Tm](2|Ω| )t ∼ 2|Ω|Tm possibilities of <br>resource</br> bundles assigned to different time slots, for the Tm different time horizons.",
                "The same set of equations (3) can be used to solve this dynamic scheduling problem, but the integer program is different because of the difference in how ψ is defined.",
                "In this case, the number of ψ values is exponential in each agents planning horizon Tm, resulting in a much larger program.",
                "This straightforward approach to solving both of these scheduling problems requires an enumeration and solution of either 2|Ω| Tm (static allocation) or P t∈[1,Tm] 2|Ω|t (dynamic reallocation) MDPs for each agent, which very quickly becomes intractable with the growth of the number of resources |Ω| or the time horizon Tm. 3.",
                "MODEL AND PROBLEM STATEMENT We now formally introduce our model of the resourcescheduling problem.",
                "The problem input consists of the following components: • M, Ω, bϕ, τa m, τd m, bτ are as defined above in Section 2.2. • {Θm} = {S, A, pm, rm, αm} are the MDPs of all agents m ∈ M. Without loss of generality, we assume that state and action spaces of all agents are the same, but each has its own transition function pm, reward function rm, and initial conditions αm. • ϕm : A×Ω → {0, 1} is the mapping of actions to resources for agent m. ϕm(a, ω) indicates whether action a of agent m needs <br>resource</br> ω.",
                "An agent m that receives a set of resources that does not include <br>resource</br> ω cannot execute in its MDP policy any action a for which ϕm(a, ω) = 0.",
                "We assume all <br>resource</br> requirements are binary; as discussed below in Section 6, this assumption is not limiting.",
                "Given the above input, the optimization problem we consider is to find the globally optimal-maximizing the sum of expected rewards-mapping of resources to agents for all time steps: Δ : τ × M × Ω → {0, 1}.",
                "A solution is feasible if the corresponding assignment of resources to the agents does not violate the global <br>resource</br> constraint: X m Δm(τ, ω) ≤ bϕ(ω), ∀ω ∈ Ω, τ ∈ [1, bτ]. (4) We consider two flavors of the <br>resource</br>-scheduling problem.",
                "The first formulation restricts <br>resource</br> assignments to the space where the allocation of resources to each agent is static during the agents lifetime.",
                "The second formulation allows reassignment of resources between agents at every time step within their lifetimes.",
                "Figure 1 depicts a <br>resource</br>-scheduling problem with three agents M = {m1, m2, m3}, three resources Ω = {ω1, ω2, ω3}, and a global problem horizon of bτ = 11.",
                "The agents arrival and departure times are shown as gray boxes and are {1, 6}, {3, 7}, and {2, 11}, respectively.",
                "A solution to this problem is shown via horizontal bars within each agents box, where the bars correspond to the allocation of the three <br>resource</br> types.",
                "Figure 1a shows a solution to a static scheduling problem.",
                "According to the shown solution, agent m1 begins the execution of its MDP at time τ = 1 and has a lock on all three resources until it finishes execution at time τ = 3.",
                "Note that agent m1 relinquishes its hold on the resources before its announced departure time of τd m1 = 6, ostensibly because other agents can utilize the resources more effectively.",
                "Thus, at time τ = 4, resources ω1 and ω3 are allocated to agent m2, who then uses them to execute its MDP (using only actions supported by resources ω1 and ω3) until time τ = 7.",
                "Agent m3 holds <br>resource</br> ω3 during the interval τ ∈ [4, 10].",
                "Figure 1b shows a possible solution to the dynamic version of the same problem.",
                "There, resources can be reallocated between agents at every time step.",
                "For example, agent m1 gives up its use of <br>resource</br> ω2 at time τ = 2, although it continues the execution of its MDP until time τ = 6.",
                "Notice that an agent is not allowed to stop and restart its MDP, so agent m1 is only able to continue executing in the interval τ ∈ [3, 4] if it has actions that do not require any resources (ϕm(a, ω) = 0).",
                "Clearly, the model and problem statement described above make a number of assumptions about the problem and the desired solution properties.",
                "We discuss some of those assumptions and their implications in Section 6. 1222 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) (a) (b) Figure 1: Illustration of a solution to a <br>resource</br>-scheduling problem with three agents and three resources: a) static <br>resource</br> assignments (resource assignments are constant within agents lifetimes; b) dynamic assignment (resource assignments are allowed to change at every time step). 4.",
                "<br>resource</br> SCHEDULING Our <br>resource</br>-scheduling algorithm proceeds in two stages.",
                "First, we perform a preprocessing step that augments the agent MDPs; this process is described in Section 4.1.",
                "Second, using these augmented MDPs we construct a global optimization problem, which is described in Section 4.2. 4.1 Augmenting Agents MDPs In the model described in the previous section, we assume that if an agent does not possess the necessary resources to perform actions in its MDP, its execution is halted and the agent leaves the system.",
                "In other words, the MDPs cannot be paused and resumed.",
                "For example, in the problem shown in Figure 1a, agent m1 releases all resources after time τ = 3, at which point the execution of its MDP is halted.",
                "Similarly, agents m2 and m3 only execute their MDPs in the intervals τ ∈ [4, 6] and τ ∈ [4, 10], respectively.",
                "Therefore, an important part of the global decision-making problem is to decide the window of time during which each of the agents is active (i.e., executing its MDP).",
                "To accomplish this, we augment each agents MDP with two new states (start and finish states sb , sf , respectively) and a new start/stop action a∗ , as illustrated in Figure 2.",
                "The idea is that an agent stays in the start state sb until it is ready to execute its MDP, at which point it performs the start/stop action a∗ and transitions into the state space of the original MDP with the transition probability that corresponds to the original initial distribution α(s).",
                "For example, in Figure 1a, for agent m2 this would happen at time τ = 4.",
                "Once the agent gets to the end of its activity window (time τ = 6 for agent m2 in Figure 1a), it performs the start/stop action, which takes it into the sink finish state sf at time τ = 7.",
                "More precisely, given an MDP S, A, pm, rm, αm , we define an augmented MDP S , A , pm, rm, αm as follows: S = S ∪ sb ∪ sf ; A = A ∪ a∗ ; p (s|sb , a∗ ) = α(s), ∀s ∈ S; p (sb |sb , a) = 1.0, ∀a ∈ A; p (sf |s, a∗ ) = 1.0, ∀s ∈ S; p (σ|s, a) = p(σ|s, a), ∀s, σ ∈ S, a ∈ A; r (sb , a) = r (sf , a) = 0, ∀a ∈ A ; r (s, a) = r(s, a), ∀s ∈ S, a ∈ A; α (sb ) = 1; α (s) = 0, ∀s ∈ S; where all non-specified transition probabilities are assumed to be zero.",
                "Further, in order to account for the new starting state, we begin the MDP one time-step earlier, setting τa m ← τa m − 1.",
                "This will not affect the <br>resource</br> allocation due to the <br>resource</br> constraints only being enforced for the original MDP states, as will be discussed in the next section.",
                "For example, the augmented MDPs shown in Figure 2b (which starts in state sb at time τ = 2) would be constructed from an MDP with original arrival time τ = 3.",
                "Figure 2b also shows a sample trajectory through the state space: the agent starts in state sb , transitions into the state space S of the original MDP, and finally exists into the sink state sf .",
                "Note that if we wanted to model a problem where agents could pause their MDPs at arbitrary time steps (which might be useful for domains where dynamic reallocation is possible), we could easily accomplish this by including an extra action that transitions from each state to itself with zero reward. 4.2 MILP for <br>resource</br> Scheduling Given a set of augmented MDPs, as defined above, the goal of this section is to formulate a global optimization program that solves the <br>resource</br>-scheduling problem.",
                "In this section and below, all MDPs are assumed to be the augmented MDPs as defined in Section 4.1.",
                "Our approach is similar to the idea used in [6]: we begin with the linear-program formulation of agents MDPs (1) and augment it with constraints that ensure that the corresponding <br>resource</br> allocation across agents and time is valid.",
                "The resulting optimization problem then simultaneously solves the agents MDPs and <br>resource</br>-scheduling problems.",
                "In the rest of this section, we incrementally develop a mixed integer program (MILP) that achieves this.",
                "In the absence of <br>resource</br> constraints, the agents finitehorizon MDPs are completely independent, and the globally optimal solution can be trivially obtained via the following LP, which is simply an aggregation of single-agent finitehorizon LPs: max X m X s X a rm(s, a) X t xm(s, a, t) subject to: X a xm(σ, a, t + 1) = X s,a pm(σ|s, a)xm(s, a, t), ∀m ∈ M, σ ∈ S, t ∈ [1, Tm − 1]; X a xm(s, a, 1) = αm(s), ∀m ∈ M, s ∈ S; (12) where xm(s, a, t) is the occupation measure of agent m, and The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1223 (a) (b) Figure 2: Illustration of augmenting an MDP to allow for variable starting and stopping times: a) (left) the original two-state MDP with a single action; (right) the augmented MDP with new states sb and sf and the new action a∗ (note that the origianl transitions are not changed in the augmentation process); b) the augmented MDP displayed as a trajectory through time (grey lines indicate all transitions, while black lines indicate a given trajectory.",
                "Objective Function (sum of expected rewards over all agents) max X m X s X a rm(s, a) X t xm(s, a, t) (5) Meaning Implication Linear Constraints Tie x to θ.",
                "Agent is only active when occupation measure is nonzero in original MDP states. θm(τ) = 0 =⇒ xm(s, a, τ −τa m+1) = 0 ∀s /∈ {sb , sf }, a ∈ A X s/∈{sb,sf } X a xm(s, a, t) ≤ θm(τa m + t − 1) ∀m ∈ M, ∀t ∈ [1, Tm] (6) Agent can only be active in τ ∈ (τa m, τd m) θm(τ) = 0 ∀m ∈ M, τ /∈ (τa m, τd m) (7) Cannot use resources when not active θm(τ) = 0 =⇒ Δm(τ, ω) = 0 ∀τ ∈ [0, bτ], ω ∈ Ω Δm(τ, ω) ≤ θm(τ) ∀m ∈ M, τ ∈ [0, bτ], ω ∈ Ω (8) Tie x to Δ (nonzero x forces corresponding Δ to be nonzero.)",
                "Δm(τ, ω) = 0, ϕm(a, ω) = 1 =⇒ xm(s, a, τ − τa m + 1) = 0 ∀s /∈ {sb , sf } 1/|A| X a ϕm(a, ω) X s/∈{sb,sf } xm(s, a, t) ≤ Δm(t + τa m − 1, ω) ∀m ∈ M, ω ∈ Ω, t ∈ [1, Tm] (9) <br>resource</br> bounds X m Δm(τ, ω) ≤ bϕ(ω) ∀ω ∈ Ω, τ ∈ [0, bτ] (10) Agent cannot change resources while active.",
                "Only enabled for scheduling with static assignments. θm(τ) = 1 and θm(τ + 1) = 1 =⇒ Δm(τ, ω) = Δm(τ + 1, ω) Δm(τ, ω) − Z(1 − θm(τ + 1)) ≤ Δm(τ + 1, ω) + Z(1 − θm(τ)) Δm(τ, ω) + Z(1 − θm(τ + 1)) ≥ Δm(τ + 1, ω) − Z(1 − θm(τ)) ∀m ∈ M, ω ∈ Ω, τ ∈ [0, bτ] (11) Table 1: MILP for globally optimal <br>resource</br> scheduling.",
                "Tm = τd m − τa m + 1 is the time horizon for the agents MDP.",
                "Using this LP as a basis, we augment it with constraints that ensure that the <br>resource</br> usage implied by the agents occupation measures {xm} does not violate the global <br>resource</br> requirements bϕ at any time step τ ∈ [0, bτ].",
                "To formulate these <br>resource</br> constraints, we use the following binary variables: • Δm(τ, ω) = {0, 1}, ∀m ∈ M, τ ∈ [0, bτ], ω ∈ Ω, which serve as indicator variables that define whether agent m possesses <br>resource</br> ω at time τ.",
                "These are analogous to the static indicator variables used in the one-shot static <br>resource</br>-allocation problem in [6]. • θm = {0, 1}, ∀m ∈ M, τ ∈ [0, bτ] are indicator variables that specify whether agent m is active (i.e., executing its MDP) at time τ.",
                "The meaning of <br>resource</br>-usage variables Δ is illustrated in Figure 1: Δm(τ, ω) = 1 only if <br>resource</br> ω is allocated to agent m at time τ.",
                "The meaning of the activity indicators θ is illustrated in Figure 2b: when agent m is in either the start state sb or the finish state sf , the corresponding θm = 0, but once the agent becomes active and enters one of the other states, we set θm = 1 .",
                "This meaning of θ can be enforced with a linear constraint that synchronizes the values of the agents occupation measures xm and the activity 1224 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) indicators θ, as shown in (6) in Table 1.",
                "Another constraint we have to add-because the activity indicators θ are defined on the global timeline τ-is to enforce the fact that the agent is inactive outside of its arrivaldeparture window.",
                "This is accomplished by constraint (7) in Table 1.",
                "Furthermore, agents should not be using resources while they are inactive.",
                "This constraint can also be enforced via a linear inequality on θ and Δ, as shown in (8).",
                "Constraint (6) sets the value of θ to match the policy defined by the occupation measure xm.",
                "In a similar fashion, we have to make sure that the <br>resource</br>-usage variables Δ are also synchronized with the occupation measure xm.",
                "This is done via constraint (9) in Table 1, which is nearly identical to the analogous constraint from [6].",
                "After implementing the above constraint, which enforces the meaning of Δ, we add a constraint that ensures that the agents <br>resource</br> usage never exceeds the amounts of available resources.",
                "This condition is also trivially expressed as a linear inequality (10) in Table 1.",
                "Finally, for the problem formulation where <br>resource</br> assignments are static during a lifetime of an agent, we add a constraint that ensures that the <br>resource</br>-usage variables Δ do not change their value while the agent is active (θ = 1).",
                "This is accomplished via the linear constraint (11), where Z ≥ 2 is a constant that is used to turn off the constraints when θm(τ) = 0 or θm(τ + 1) = 0.",
                "This constraint is not used for the dynamic problem formulation, where resources can be reallocated between agents at every time step.",
                "To summarize, Table 1 together with the conservationof-flow constraints from (12) defines the MILP that simultaneously computes an optimal <br>resource</br> assignment for all agents across time as well as optimal finite-horizon MDP policies that are valid under that <br>resource</br> assignment.",
                "As a rough measure of the complexity of this MILP, let us consider the number of optimization variables and constraints.",
                "Let TM = P Tm = P m(τa m − τd m + 1) be the sum of the lengths of the arrival-departure windows across all agents.",
                "Then, the number of optimization variables is: TM + bτ|M||Ω| + bτ|M|, TM of which are continuous (xm), and bτ|M||Ω| + bτ|M| are binary (Δ and θ).",
                "However, notice that all but TM|M| of the θ are set to zero by constraint (7), which also immediately forces all but TM|M||Ω| of the Δ to be zero via the constraints (8).",
                "The number of constraints (not including the degenerate constraints in (7)) in the MILP is: TM + TM|Ω| + bτ|Ω| + bτ|M||Ω|.",
                "Despite the fact that the complexity of the MILP is, in the worst case, exponential1 in the number of binary variables, the complexity of this MILP is significantly (exponentially) lower than that of the MILP with flat utility functions, described in Section 2.2.",
                "This result echos the efficiency gains reported in [6] for single-shot <br>resource</br>-allocation problems, but is much more pronounced, because of the explosion of the flat utility representation due to the temporal aspect of the problem (recall the prohibitive complexity of the combinatorial optimization in Section 2.2).",
                "We empirically analyze the performance of this method in Section 5. 1 Strictly speaking, solving MILPs to optimality is NPcomplete in the number of integer variables. 5.",
                "EXPERIMENTAL RESULTS Although the complexity of solving MILPs is in the worst case exponential in the number of integer variables, there are many efficient methods for solving MILPs that allow our algorithm to scale well for parameters common to <br>resource</br> allocation and scheduling problems.",
                "In particular, this section introduces a problem domain-the repairshop problem-used to empirically evaluate our algorithms scalability in terms of the number of agents |M|, the number of shared resources |Ω|, and the varied lengths of global time bτ during which agents may enter and exit the system.",
                "The repairshop problem is a simple parameterized MDP adopting the metaphor of a vehicular repair shop.",
                "Agents in the repair shop are mechanics with a number of independent tasks that yield reward only when completed.",
                "In our MDP model of this system, actions taken to advance through the state space are only allowed if the agent holds certain resources that are publicly available to the shop.",
                "These resources are in finite supply, and optimal policies for the shop will determine when each agent may hold the limited resources to take actions and earn individual rewards.",
                "Each task to be completed is associated with a single action, although the agent is required to repeat the action numerous times before completing the task and earning a reward.",
                "This model was parameterized in terms of the number of agents in the system, the number of different types of resources that could be linked to necessary actions, a global time during which agents are allowed to arrive and depart, and a maximum length for the number of time steps an agent may remain in the system.",
                "All datapoints in our experiments were obtained with 20 evaluations using CPLEX to solve the MILPs on a Pentium4 computer with 2Gb of RAM.",
                "Trials were conducted on both the static and the dynamic version of the resourcescheduling problem, as defined earlier.",
                "Figure 3 shows the runtime and policy value for independent modifications to the parameter set.",
                "The top row shows how the solution time for the MILP scales as we increase the number of agents |M|, the global time horizon bτ, and the number of resources |Ω|.",
                "Increasing the number of agents leads to exponential complexity scaling, which is to be expected for an NP-complete problem.",
                "However, increasing the global time limit bτ or the total number of <br>resource</br> types |Ω|-while holding the number of agents constantdoes not lead to decreased performance.",
                "This occurs because the problems get easier as they become under-constrained, which is also a common phenomenon for NP-complete problems.",
                "We also observe that the solution to the dynamic version of the problem can often be computed much faster than the static version.",
                "The bottom row of Figure 3 shows the joint policy value of the policies that correspond to the computed optimal <br>resource</br>-allocation schedules.",
                "We can observe that the dynamic version yields higher reward (as expected, since the reward for the dynamic version is always no less than the reward of the static version).",
                "We should point out that these graphs should not be viewed as a measure of performance of two different algorithms (both algorithms produce optimal solutions but to different problems), but rather as observations about how the quality of optimal solutions change as more flexibility is allowed in the reallocation of resources.",
                "Figure 4 shows runtime and policy value for trials in which common input variables are scaled together.",
                "This allows The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1225 2 4 6 8 10 10 −3 10 −2 10 −1 10 0 10 1 10 2 10 3 10 4 Number of Agents |M| CPUTime,sec |Ω| = 5, τ = 50 static dynamic 50 100 150 200 10 −2 10 −1 10 0 10 1 10 2 10 3 Global Time Boundary τ CPUTime,sec |M| = 5, |Ω| = 5 static dynamic 10 20 30 40 50 10 −2 10 −1 10 0 10 1 10 2 Number of Resources |Ω| CPUTime,sec |M| = 5, τ = 50 static dynamic 2 4 6 8 10 200 400 600 800 1000 1200 1400 1600 Number of Agents |M| Value |Ω| = 5, τ = 50 static dynamic 50 100 150 200 400 500 600 700 800 900 1000 1100 1200 1300 1400 Global Time Boundary τ Value |M| = 5, |Ω| = 5 static dynamic 10 20 30 40 50 500 600 700 800 900 1000 1100 1200 1300 1400 Number of Resources |Ω| Value |M| = 5, τ = 50 static dynamic Figure 3: Evaluation of our MILP for variable numbers of agents (column 1), lengths of global-time window (column 2), and numbers of <br>resource</br> types (column 3).",
                "Top row shows CPU time, and bottom row shows the joint reward of agents MDP policies.",
                "Error bars show the 1st and 3rd quartiles (25% and 75%). 2 4 6 8 10 10 −3 10 −2 10 −1 10 0 10 1 10 2 10 3 Number of Agents |M| CPUTime,sec τ = 10|M| static dynamic 2 4 6 8 10 10 −3 10 −2 10 −1 10 0 10 1 10 2 10 3 10 4 Number of Agents |M| CPUTime,sec |Ω| = 2|M| static dynamic 2 4 6 8 10 10 −3 10 −2 10 −1 10 0 10 1 10 2 10 3 10 4 Number of Agents |M| CPUTime,sec |Ω| = 5|M| static dynamic 2 4 6 8 10 200 400 600 800 1000 1200 1400 1600 1800 2000 2200 Number of Agents |M| Value τ = 10|M| static dynamic 2 4 6 8 10 200 400 600 800 1000 1200 1400 1600 1800 2000 Number of Agents |M| Value |Ω| = 2|M| static dynamic 2 4 6 8 10 0 500 1000 1500 2000 2500 Number of Agents |M| Value |Ω| = 5|M| static dynamic Figure 4: Evaluation of our MILP using correlated input variables.",
                "The left column tracks the performance and CPU time as the number of agents and global-time window increase together (bτ = 10|M|).",
                "The middle and the right column track the performance and CPU time as the number of resources and the number of agents increase together as |Ω| = 2|M| and |Ω| = 5|M|, respectively.",
                "Error bars show the 1st and 3rd quartiles (25% and 75%). 1226 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) us to explore domains where the total number of agents scales proportionally to the total number of <br>resource</br> types or the global time horizon, while keeping constant the average agent density (per unit of global time) or the average number of resources per agent (which commonly occurs in real-life applications).",
                "Overall, we believe that these experimental results indicate that our MILP formulation can be used to effectively solve <br>resource</br>-scheduling problems of nontrivial size. 6.",
                "DISCUSSION AND CONCLUSIONS Throughout the paper, we have made a number of assumptions in our model and solution algorithm; we discuss their implications below. • Continual execution.",
                "We assume that once an agent stops executing its MDP (transitions into state sf ), it exits the system and cannot return.",
                "It is easy to relax this assumption for domains where agents MDPs can be paused and restarted.",
                "All that is required is to include an additional pause action which transitions from a given state back to itself, and has zero reward. • Indifference to start time.",
                "We used a reward model where agents rewards depend only on the time horizon of their MDPs and not the global start time.",
                "This is a consequence of our MDP-augmentation procedure from Section 4.1.",
                "It is easy to extend the model so that the agents incur an explicit penalty for idling by assigning a non-zero negative reward to the start state sb . • Binary <br>resource</br> requirements.",
                "For simplicity, we have assumed that <br>resource</br> costs are binary: ϕm(a, ω) = {0, 1}, but our results generalize in a straightforward manner to non-binary <br>resource</br> mappings, analogously to the procedure used in [5]. • Cooperative agents.",
                "The optimization procedure discussed in this paper was developed in the context of cooperative agents, but it can also be used to design a mechanism for scheduling resources among self-interested agents.",
                "This optimization procedure can be embedded in a VickreyClarke-Groves auction, completely analogously to the way it was done in [7].",
                "In fact, all the results of [7] about the properties of the auction and information privacy directly carry over to the scheduling domain discussed in this paper, requiring only slight modifications to deal with finitehorizon MDPs. • Known, deterministic arrival and departure times.",
                "Finally, we have assumed that agents arrival and departure times (τa m and τd m) are deterministic and known a priori.",
                "This assumption is fundamental to our solution method.",
                "While there are many domains where this assumption is valid, in many cases agents arrive and depart dynamically and their arrival and departure times can only be predicted probabilistically, leading to online <br>resource</br>-allocation problems.",
                "In particular, in the case of self-interested agents, this becomes an interesting version of an online-mechanism-design problem [11, 12].",
                "In summary, we have presented an MILP formulation for the combinatorial <br>resource</br>-scheduling problem where agents values for possible <br>resource</br> assignments are defined by finitehorizon MDPs.",
                "This result extends previous work ([6, 7]) on static one-shot <br>resource</br> allocation under MDP-induced preferences to <br>resource</br>-scheduling problems with a temporal aspect.",
                "As such, this work takes a step in the direction of designing an online mechanism for agents with combinatorial <br>resource</br> preferences induced by stochastic planning problems.",
                "Relaxing the assumption about deterministic arrival and departure times of the agents is a focus of our future work.",
                "We would like to thank the anonymous reviewers for their insightful comments and suggestions. 7.",
                "REFERENCES [1] E. Altman and A. Shwartz.",
                "Adaptive control of constrained Markov chains: Criteria and policies.",
                "Annals of Operations Research, special issue on Markov Decision Processes, 28:101-134, 1991. [2] R. Bellman.",
                "Dynamic Programming.",
                "Princeton University Press, 1957. [3] C. Boutilier.",
                "Solving concisely expressed combinatorial auction problems.",
                "In Proc. of AAAI-02, pages 359-366, 2002. [4] C. Boutilier and H. H. Hoos.",
                "Bidding languages for combinatorial auctions.",
                "In Proc. of IJCAI-01, pages 1211-1217, 2001. [5] D. Dolgov.",
                "Integrated <br>resource</br> Allocation and Planning in Stochastic Multiagent Environments.",
                "PhD thesis, Computer Science Department, University of Michigan, February 2006. [6] D. A. Dolgov and E. H. Durfee.",
                "Optimal <br>resource</br> allocation and policy formulation in loosely-coupled Markov decision processes.",
                "In Proc. of ICAPS-04, pages 315-324, June 2004. [7] D. A. Dolgov and E. H. Durfee.",
                "Computationally efficient combinatorial auctions for <br>resource</br> allocation in weakly-coupled MDPs.",
                "In Proc. of AAMAS-05, New York, NY, USA, 2005.",
                "ACM Press. [8] D. A. Dolgov and E. H. Durfee.",
                "<br>resource</br> allocation among agents with preferences induced by factored MDPs.",
                "In Proc. of AAMAS-06, 2006. [9] K. Larson and T. Sandholm.",
                "Mechanism design and deliberative agents.",
                "In Proc. of AAMAS-05, pages 650-656, New York, NY, USA, 2005.",
                "ACM Press. [10] N. Nisan.",
                "Bidding and allocation in combinatorial auctions.",
                "In Electronic Commerce, 2000. [11] D. C. Parkes and S. Singh.",
                "An MDP-based approach to Online Mechanism Design.",
                "In Proc. of the Seventeenths Annual Conference on Neural Information Processing Systems (NIPS-03), 2003. [12] D. C. Parkes, S. Singh, and D. Yanovsky.",
                "Approximately efficient online mechanism design.",
                "In Proc. of the Eighteenths Annual Conference on Neural Information Processing Systems (NIPS-04), 2004. [13] M. L. Puterman.",
                "Markov Decision Processes.",
                "John Wiley & Sons, New York, 1994. [14] M. H. Rothkopf, A. Pekec, and R. M. Harstad.",
                "Computationally manageable combinational auctions.",
                "Management Science, 44(8):1131-1147, 1998. [15] T. Sandholm.",
                "An algorithm for optimal winner determination in combinatorial auctions.",
                "In Proc. of IJCAI-99, pages 542-547, San Francisco, CA, USA, 1999.",
                "Morgan Kaufmann Publishers Inc.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1227"
            ],
            "original_annotated_samples": [
                "Combinatorial <br>resource</br> Scheduling for Multiagent MDPs Dmitri A. Dolgov, Michael R. James, and Michael E. Samples AI and Robotics Group Technical Research, Toyota Technical Center USA {ddolgov, michael.r.james, michael.samples}@gmail.com ABSTRACT Optimal <br>resource</br> scheduling in multiagent systems is a computationally challenging task, particularly when the values of resources are not additive.",
                "In recent years, efficient <br>resource</br>-allocation algorithms have been developed for agents with <br>resource</br> values induced by MDPs.",
                "However, this prior work has focused on static <br>resource</br>-allocation problems where resources are distributed once and then utilized in infinite-horizon MDPs.",
                "We extend those existing models to the problem of combinatorial <br>resource</br> scheduling, where agents persist only for finite periods between their (predefined) arrival and departure times, requiring resources only for those time periods.",
                "We provide a computationally efficient procedure for computing globally optimal <br>resource</br> assignments to agents over time."
            ],
            "translated_annotated_samples": [
                "La programación de <br>recursos</br> combinatoria para MDPs multiagentes. Dmitri A. Dolgov, Michael R. James y Michael E. Samples. Grupo de Investigación en IA y Robótica, Centro Técnico de Toyota en EE. UU. {ddolgov, michael.r.james, michael.samples}@gmail.com. RESUMEN La programación óptima de <br>recursos</br> en sistemas multiagentes es una tarea computacionalmente desafiante, especialmente cuando los valores de los recursos no son aditivos.",
                "En los últimos años, se han desarrollado algoritmos eficientes de asignación de recursos para agentes con <br>valores de recursos</br> inducidos por MDPs.",
                "Sin embargo, este trabajo previo se ha centrado en problemas de asignación de <br>recurso</br>s estáticos donde los <br>recurso</br>s se distribuyen una vez y luego se utilizan en MDPs de horizonte infinito.",
                "Extendemos esos modelos existentes al problema de la <br>programación de recursos</br> combinatoria, donde los agentes persisten solo por períodos finitos entre sus tiempos de llegada y salida (predefinidos), requiriendo recursos solo durante esos períodos de tiempo.",
                "Proporcionamos un procedimiento computacionalmente eficiente para calcular <br>asignaciones de recursos</br> óptimas a nivel global a agentes a lo largo del tiempo."
            ],
            "translated_text": "La programación de <br>recursos</br> combinatoria para MDPs multiagentes. Dmitri A. Dolgov, Michael R. James y Michael E. Samples. Grupo de Investigación en IA y Robótica, Centro Técnico de Toyota en EE. UU. {ddolgov, michael.r.james, michael.samples}@gmail.com. RESUMEN La programación óptima de <br>recursos</br> en sistemas multiagentes es una tarea computacionalmente desafiante, especialmente cuando los valores de los recursos no son aditivos. Consideramos el problema combinatorio de programar el uso de múltiples recursos entre agentes que operan en entornos estocásticos, modelados como procesos de decisión de Markov (MDPs). En los últimos años, se han desarrollado algoritmos eficientes de asignación de recursos para agentes con <br>valores de recursos</br> inducidos por MDPs. Sin embargo, este trabajo previo se ha centrado en problemas de asignación de <br>recurso</br>s estáticos donde los <br>recurso</br>s se distribuyen una vez y luego se utilizan en MDPs de horizonte infinito. Extendemos esos modelos existentes al problema de la <br>programación de recursos</br> combinatoria, donde los agentes persisten solo por períodos finitos entre sus tiempos de llegada y salida (predefinidos), requiriendo recursos solo durante esos períodos de tiempo. Proporcionamos un procedimiento computacionalmente eficiente para calcular <br>asignaciones de recursos</br> óptimas a nivel global a agentes a lo largo del tiempo. ",
            "candidates": [],
            "error": [
                [
                    "recursos",
                    "recursos",
                    "valores de recursos",
                    "recurso",
                    "recurso",
                    "programación de recursos",
                    "asignaciones de recursos"
                ]
            ]
        },
        "markov decision process": {
            "translated_key": "proceso de decisión de Markov",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Combinatorial Resource Scheduling for Multiagent MDPs Dmitri A. Dolgov, Michael R. James, and Michael E. Samples AI and Robotics Group Technical Research, Toyota Technical Center USA {ddolgov, michael.r.james, michael.samples}@gmail.com ABSTRACT Optimal resource scheduling in multiagent systems is a computationally challenging task, particularly when the values of resources are not additive.",
                "We consider the combinatorial problem of scheduling the usage of multiple resources among agents that operate in stochastic environments, modeled as Markov decision processes (MDPs).",
                "In recent years, efficient resource-allocation algorithms have been developed for agents with resource values induced by MDPs.",
                "However, this prior work has focused on static resource-allocation problems where resources are distributed once and then utilized in infinite-horizon MDPs.",
                "We extend those existing models to the problem of combinatorial resource scheduling, where agents persist only for finite periods between their (predefined) arrival and departure times, requiring resources only for those time periods.",
                "We provide a computationally efficient procedure for computing globally optimal resource assignments to agents over time.",
                "We illustrate and empirically analyze the method in the context of a stochastic jobscheduling domain.",
                "Categories and Subject Descriptors I.2.8 [Artificial Intelligence]: Problem Solving, Control Methods, and Search; I.2.11 [Artificial Intelligence]: Distributed Artificial Intelligence-Multiagent systems General Terms Algorithms, Performance, Design 1.",
                "INTRODUCTION The tasks of optimal resource allocation and scheduling are ubiquitous in multiagent systems, but solving such optimization problems can be computationally difficult, due to a number of factors.",
                "In particular, when the value of a set of resources to an agent is not additive (as is often the case with resources that are substitutes or complements), the utility function might have to be defined on an exponentially large space of resource bundles, which very quickly becomes computationally intractable.",
                "Further, even when each agent has a utility function that is nonzero only on a small subset of the possible resource bundles, obtaining optimal allocation is still computationally prohibitive, as the problem becomes NP-complete [14].",
                "Such computational issues have recently spawned several threads of work in using compact models of agents preferences.",
                "One idea is to use any structure present in utility functions to represent them compactly, via, for example, logical formulas [15, 10, 4, 3].",
                "An alternative is to directly model the mechanisms that define the agents utility functions and perform resource allocation directly with these models [9].",
                "A way of accomplishing this is to model the processes by which an agent might utilize the resources and define the utility function as the payoff of these processes.",
                "In particular, if an agent uses resources to act in a stochastic environment, its utility function can be naturally modeled with a <br>markov decision process</br>, whose action set is parameterized by the available resources.",
                "This representation can then be used to construct very efficient resource-allocation algorithms that lead to an exponential speedup over a straightforward optimization problem with flat representations of combinatorial preferences [6, 7, 8].",
                "However, this existing work on resource allocation with preferences induced by resource-parameterized MDPs makes an assumption that the resources are only allocated once and are then utilized by the agents independently within their infinite-horizon MDPs.",
                "This assumption that no reallocation of resources is possible can be limiting in domains where agents arrive and depart dynamically.",
                "In this paper, we extend the work on resource allocation under MDP-induced preferences to discrete-time scheduling problems, where agents are present in the system for finite time intervals and can only use resources within these intervals.",
                "In particular, agents arrive and depart at arbitrary (predefined) times and within these intervals use resources to execute tasks in finite-horizon MDPs.",
                "We address the problem of globally optimal resource scheduling, where the objective is to find an allocation of resources to the agents across time that maximizes the sum of the expected rewards that they obtain.",
                "In this context, our main contribution is a mixed-integerprogramming formulation of the scheduling problem that chooses globally optimal resource assignments, starting times, and execution horizons for all agents (within their arrival1220 978-81-904262-7-5 (RPS) c 2007 IFAAMAS departure intervals).",
                "We analyze and empirically compare two flavors of the scheduling problem: one, where agents have static resource assignments within their finite-horizon MDPs, and another, where resources can be dynamically reallocated between agents at every time step.",
                "In the rest of the paper, we first lay down the necessary groundwork in Section 2 and then introduce our model and formal problem statement in Section 3.",
                "In Section 4.2, we describe our main result, the optimization program for globally optimal resource scheduling.",
                "Following the discussion of our experimental results on a job-scheduling problem in Section 5, we conclude in Section 6 with a discussion of possible extensions and generalizations of our method. 2.",
                "BACKGROUND Similarly to the model used in previous work on resourceallocation with MDP-induced preferences [6, 7], we define the value of a set of resources to an agent as the value of the best MDP policy that is realizable, given those resources.",
                "However, since the focus of our work is on scheduling problems, and a large part of the optimization problem is to decide how resources are allocated in time among agents with finite arrival and departure times, we model the agents planning problems as finite-horizon MDPs, in contrast to previous work that used infinite-horizon discounted MDPs.",
                "In the rest of this section, we first introduce some necessary background on finite-horizon MDPs and present a linear-programming formulation that serves as the basis for our solution algorithm developed in Section 4.",
                "We also outline the standard methods for combinatorial resource scheduling with flat resource values, which serve as a comparison benchmark for the new model developed here. 2.1 Markov Decision Processes A stationary, finite-domain, discrete-time MDP (see, for example, [13] for a thorough and detailed development) can be described as S, A, p, r , where: S is a finite set of system states; A is a finite set of actions that are available to the agent; p is a stationary stochastic transition function, where p(σ|s, a) is the probability of transitioning to state σ upon executing action a in state s; r is a stationary reward function, where r(s, a) specifies the reward obtained upon executing action a in state s. Given such an MDP, a decision problem under a finite horizon T is to choose an optimal action at every time step to maximize the expected value of the total reward accrued during the agents (finite) lifetime.",
                "The agents optimal policy is then a function of current state s and the time until the horizon.",
                "An optimal policy for such a problem is to act greedily with respect to the optimal value function, defined recursively by the following system of finite-time Bellman equations [2]: v(s, t) = max a r(s, a) + X σ p(σ|s, a)v(σ, t + 1), ∀s ∈ S, t ∈ [1, T − 1]; v(s, T) = 0, ∀s ∈ S; where v(s, t) is the optimal value of being in state s at time t ∈ [1, T].",
                "This optimal value function can be easily computed using dynamic programming, leading to the following optimal policy π, where π(s, a, t) is the probability of executing action a in state s at time t: π(s, a, t) = ( 1, a = argmaxa r(s, a) + P σ p(σ|s, a)v(σ, t + 1), 0, otherwise.",
                "The above is the most common way of computing the optimal value function (and therefore an optimal policy) for a finite-horizon MDP.",
                "However, we can also formulate the problem as the following linear program (similarly to the dual LP for infinite-horizon discounted MDPs [13, 6, 7]): max X s X a r(s, a) X t x(s, a, t) subject to: X a x(σ, a, t + 1) = X s,a p(σ|s, a)x(s, a, t) ∀σ, t ∈ [1, T − 1]; X a x(s, a, 1) = α(s), ∀s ∈ S; (1) where α(s) is the initial distribution over the state space, and x is the (non-stationary) occupation measure (x(s, a, t) ∈ [0, 1] is the total expected number of times action a is executed in state s at time t).",
                "An optimal (non-stationary) policy is obtained from the occupation measure as follows: π(s, a, t) = x(s, a, t)/ X a x(s, a, t) ∀s ∈ S, t ∈ [1, T]. (2) Note that the standard unconstrained finite-horizon MDP, as described above, always has a uniformly-optimal solution (optimal for any initial distribution α(s)).",
                "Therefore, an optimal policy can be obtained by using an arbitrary constant α(s) > 0 (in particular, α(s) = 1 will result in x(s, a, t) = π(s, a, t)).",
                "However, for MDPs with resource constraints (as defined below in Section 3), uniformly-optimal policies do not in general exist.",
                "In such cases, α becomes a part of the problem input, and a resulting policy is only optimal for that particular α.",
                "This result is well known for infinite-horizon MDPs with various types of constraints [1, 6], and it also holds for our finite-horizon model, which can be easily established via a line of reasoning completely analogous to the arguments in [6]. 2.2 Combinatorial Resource Scheduling A straightforward approach to resource scheduling for a set of agents M, whose values for the resources are induced by stochastic planning problems (in our case, finite-horizon MDPs) would be to have each agent enumerate all possible resource assignments over time and, for each one, compute its value by solving the corresponding MDP.",
                "Then, each agent would provide valuations for each possible resource bundle over time to a centralized coordinator, who would compute the optimal resource assignments across time based on these valuations.",
                "When resources can be allocated at different times to different agents, each agent must submit valuations for every combination of possible time horizons.",
                "Let each agent m ∈ M execute its MDP within the arrival-departure time interval τ ∈ [τa m, τd m].",
                "Hence, agent m will execute an MDP with time horizon no greater than Tm = τd m−τa m+1.",
                "Let bτ be the global time horizon for the problem, before which all of the agents MDPs must finish.",
                "We assume τd m < bτ, ∀m ∈ M. The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1221 For the scheduling problem where agents have static resource requirements within their finite-horizon MDPs, the agents provide a valuation for each resource bundle for each possible time horizon (from [1, Tm]) that they may use.",
                "Let Ω be the set of resources to be allocated among the agents.",
                "An agent will get at most one resource bundle for one of the time horizons.",
                "Let the variable ψ ∈ Ψm enumerate all possible pairs of resource bundles and time horizons for agent m, so there are 2|Ω| × Tm values for ψ (the space of bundles is exponential in the number of resource types |Ω|).",
                "The agent m must provide a value vψ m for each ψ, and the coordinator will allocate at most one ψ (resource, time horizon) pair to each agent.",
                "This allocation is expressed as an indicator variable zψ m ∈ {0, 1} that shows whether ψ is assigned to agent m. For time τ and resource ω, the function nm(ψ, τ, ω) ∈ {0, 1} indicates whether the bundle in ψ uses resource ω at time τ (we make the assumption that agents have binary resource requirements).",
                "This allocation problem is NP-complete, even when considering only a single time step, and its difficulty increases significantly with multiple time steps because of the increasing number of values of ψ.",
                "The problem of finding an optimal allocation that satisfies the global constraint that the amount of each resource ω allocated to all agents does not exceed the available amount bϕ(ω) can be expressed as the following integer program: max X m∈M X ψ∈Ψm zψ mvψ m subject to: X ψ∈Ψm zψ m ≤ 1, ∀m ∈ M; X m∈M X ψ∈Ψm zψ mnm(ψ, τ, ω) ≤ bϕ(ω), ∀τ ∈ [1, bτ], ∀ω ∈ Ω; (3) The first constraint in equation 3 says that no agent can receive more than one bundle, and the second constraint ensures that the total assignment of resource ω does not, at any time, exceed the resource bound.",
                "For the scheduling problem where the agents are able to dynamically reallocate resources, each agent must specify a value for every combination of bundles and time steps within its time horizon.",
                "Let the variable ψ ∈ Ψm in this case enumerate all possible resource bundles for which at most one bundle may be assigned to agent m at each time step.",
                "Therefore, in this case there are P t∈[1,Tm](2|Ω| )t ∼ 2|Ω|Tm possibilities of resource bundles assigned to different time slots, for the Tm different time horizons.",
                "The same set of equations (3) can be used to solve this dynamic scheduling problem, but the integer program is different because of the difference in how ψ is defined.",
                "In this case, the number of ψ values is exponential in each agents planning horizon Tm, resulting in a much larger program.",
                "This straightforward approach to solving both of these scheduling problems requires an enumeration and solution of either 2|Ω| Tm (static allocation) or P t∈[1,Tm] 2|Ω|t (dynamic reallocation) MDPs for each agent, which very quickly becomes intractable with the growth of the number of resources |Ω| or the time horizon Tm. 3.",
                "MODEL AND PROBLEM STATEMENT We now formally introduce our model of the resourcescheduling problem.",
                "The problem input consists of the following components: • M, Ω, bϕ, τa m, τd m, bτ are as defined above in Section 2.2. • {Θm} = {S, A, pm, rm, αm} are the MDPs of all agents m ∈ M. Without loss of generality, we assume that state and action spaces of all agents are the same, but each has its own transition function pm, reward function rm, and initial conditions αm. • ϕm : A×Ω → {0, 1} is the mapping of actions to resources for agent m. ϕm(a, ω) indicates whether action a of agent m needs resource ω.",
                "An agent m that receives a set of resources that does not include resource ω cannot execute in its MDP policy any action a for which ϕm(a, ω) = 0.",
                "We assume all resource requirements are binary; as discussed below in Section 6, this assumption is not limiting.",
                "Given the above input, the optimization problem we consider is to find the globally optimal-maximizing the sum of expected rewards-mapping of resources to agents for all time steps: Δ : τ × M × Ω → {0, 1}.",
                "A solution is feasible if the corresponding assignment of resources to the agents does not violate the global resource constraint: X m Δm(τ, ω) ≤ bϕ(ω), ∀ω ∈ Ω, τ ∈ [1, bτ]. (4) We consider two flavors of the resource-scheduling problem.",
                "The first formulation restricts resource assignments to the space where the allocation of resources to each agent is static during the agents lifetime.",
                "The second formulation allows reassignment of resources between agents at every time step within their lifetimes.",
                "Figure 1 depicts a resource-scheduling problem with three agents M = {m1, m2, m3}, three resources Ω = {ω1, ω2, ω3}, and a global problem horizon of bτ = 11.",
                "The agents arrival and departure times are shown as gray boxes and are {1, 6}, {3, 7}, and {2, 11}, respectively.",
                "A solution to this problem is shown via horizontal bars within each agents box, where the bars correspond to the allocation of the three resource types.",
                "Figure 1a shows a solution to a static scheduling problem.",
                "According to the shown solution, agent m1 begins the execution of its MDP at time τ = 1 and has a lock on all three resources until it finishes execution at time τ = 3.",
                "Note that agent m1 relinquishes its hold on the resources before its announced departure time of τd m1 = 6, ostensibly because other agents can utilize the resources more effectively.",
                "Thus, at time τ = 4, resources ω1 and ω3 are allocated to agent m2, who then uses them to execute its MDP (using only actions supported by resources ω1 and ω3) until time τ = 7.",
                "Agent m3 holds resource ω3 during the interval τ ∈ [4, 10].",
                "Figure 1b shows a possible solution to the dynamic version of the same problem.",
                "There, resources can be reallocated between agents at every time step.",
                "For example, agent m1 gives up its use of resource ω2 at time τ = 2, although it continues the execution of its MDP until time τ = 6.",
                "Notice that an agent is not allowed to stop and restart its MDP, so agent m1 is only able to continue executing in the interval τ ∈ [3, 4] if it has actions that do not require any resources (ϕm(a, ω) = 0).",
                "Clearly, the model and problem statement described above make a number of assumptions about the problem and the desired solution properties.",
                "We discuss some of those assumptions and their implications in Section 6. 1222 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) (a) (b) Figure 1: Illustration of a solution to a resource-scheduling problem with three agents and three resources: a) static resource assignments (resource assignments are constant within agents lifetimes; b) dynamic assignment (resource assignments are allowed to change at every time step). 4.",
                "RESOURCE SCHEDULING Our resource-scheduling algorithm proceeds in two stages.",
                "First, we perform a preprocessing step that augments the agent MDPs; this process is described in Section 4.1.",
                "Second, using these augmented MDPs we construct a global optimization problem, which is described in Section 4.2. 4.1 Augmenting Agents MDPs In the model described in the previous section, we assume that if an agent does not possess the necessary resources to perform actions in its MDP, its execution is halted and the agent leaves the system.",
                "In other words, the MDPs cannot be paused and resumed.",
                "For example, in the problem shown in Figure 1a, agent m1 releases all resources after time τ = 3, at which point the execution of its MDP is halted.",
                "Similarly, agents m2 and m3 only execute their MDPs in the intervals τ ∈ [4, 6] and τ ∈ [4, 10], respectively.",
                "Therefore, an important part of the global decision-making problem is to decide the window of time during which each of the agents is active (i.e., executing its MDP).",
                "To accomplish this, we augment each agents MDP with two new states (start and finish states sb , sf , respectively) and a new start/stop action a∗ , as illustrated in Figure 2.",
                "The idea is that an agent stays in the start state sb until it is ready to execute its MDP, at which point it performs the start/stop action a∗ and transitions into the state space of the original MDP with the transition probability that corresponds to the original initial distribution α(s).",
                "For example, in Figure 1a, for agent m2 this would happen at time τ = 4.",
                "Once the agent gets to the end of its activity window (time τ = 6 for agent m2 in Figure 1a), it performs the start/stop action, which takes it into the sink finish state sf at time τ = 7.",
                "More precisely, given an MDP S, A, pm, rm, αm , we define an augmented MDP S , A , pm, rm, αm as follows: S = S ∪ sb ∪ sf ; A = A ∪ a∗ ; p (s|sb , a∗ ) = α(s), ∀s ∈ S; p (sb |sb , a) = 1.0, ∀a ∈ A; p (sf |s, a∗ ) = 1.0, ∀s ∈ S; p (σ|s, a) = p(σ|s, a), ∀s, σ ∈ S, a ∈ A; r (sb , a) = r (sf , a) = 0, ∀a ∈ A ; r (s, a) = r(s, a), ∀s ∈ S, a ∈ A; α (sb ) = 1; α (s) = 0, ∀s ∈ S; where all non-specified transition probabilities are assumed to be zero.",
                "Further, in order to account for the new starting state, we begin the MDP one time-step earlier, setting τa m ← τa m − 1.",
                "This will not affect the resource allocation due to the resource constraints only being enforced for the original MDP states, as will be discussed in the next section.",
                "For example, the augmented MDPs shown in Figure 2b (which starts in state sb at time τ = 2) would be constructed from an MDP with original arrival time τ = 3.",
                "Figure 2b also shows a sample trajectory through the state space: the agent starts in state sb , transitions into the state space S of the original MDP, and finally exists into the sink state sf .",
                "Note that if we wanted to model a problem where agents could pause their MDPs at arbitrary time steps (which might be useful for domains where dynamic reallocation is possible), we could easily accomplish this by including an extra action that transitions from each state to itself with zero reward. 4.2 MILP for Resource Scheduling Given a set of augmented MDPs, as defined above, the goal of this section is to formulate a global optimization program that solves the resource-scheduling problem.",
                "In this section and below, all MDPs are assumed to be the augmented MDPs as defined in Section 4.1.",
                "Our approach is similar to the idea used in [6]: we begin with the linear-program formulation of agents MDPs (1) and augment it with constraints that ensure that the corresponding resource allocation across agents and time is valid.",
                "The resulting optimization problem then simultaneously solves the agents MDPs and resource-scheduling problems.",
                "In the rest of this section, we incrementally develop a mixed integer program (MILP) that achieves this.",
                "In the absence of resource constraints, the agents finitehorizon MDPs are completely independent, and the globally optimal solution can be trivially obtained via the following LP, which is simply an aggregation of single-agent finitehorizon LPs: max X m X s X a rm(s, a) X t xm(s, a, t) subject to: X a xm(σ, a, t + 1) = X s,a pm(σ|s, a)xm(s, a, t), ∀m ∈ M, σ ∈ S, t ∈ [1, Tm − 1]; X a xm(s, a, 1) = αm(s), ∀m ∈ M, s ∈ S; (12) where xm(s, a, t) is the occupation measure of agent m, and The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1223 (a) (b) Figure 2: Illustration of augmenting an MDP to allow for variable starting and stopping times: a) (left) the original two-state MDP with a single action; (right) the augmented MDP with new states sb and sf and the new action a∗ (note that the origianl transitions are not changed in the augmentation process); b) the augmented MDP displayed as a trajectory through time (grey lines indicate all transitions, while black lines indicate a given trajectory.",
                "Objective Function (sum of expected rewards over all agents) max X m X s X a rm(s, a) X t xm(s, a, t) (5) Meaning Implication Linear Constraints Tie x to θ.",
                "Agent is only active when occupation measure is nonzero in original MDP states. θm(τ) = 0 =⇒ xm(s, a, τ −τa m+1) = 0 ∀s /∈ {sb , sf }, a ∈ A X s/∈{sb,sf } X a xm(s, a, t) ≤ θm(τa m + t − 1) ∀m ∈ M, ∀t ∈ [1, Tm] (6) Agent can only be active in τ ∈ (τa m, τd m) θm(τ) = 0 ∀m ∈ M, τ /∈ (τa m, τd m) (7) Cannot use resources when not active θm(τ) = 0 =⇒ Δm(τ, ω) = 0 ∀τ ∈ [0, bτ], ω ∈ Ω Δm(τ, ω) ≤ θm(τ) ∀m ∈ M, τ ∈ [0, bτ], ω ∈ Ω (8) Tie x to Δ (nonzero x forces corresponding Δ to be nonzero.)",
                "Δm(τ, ω) = 0, ϕm(a, ω) = 1 =⇒ xm(s, a, τ − τa m + 1) = 0 ∀s /∈ {sb , sf } 1/|A| X a ϕm(a, ω) X s/∈{sb,sf } xm(s, a, t) ≤ Δm(t + τa m − 1, ω) ∀m ∈ M, ω ∈ Ω, t ∈ [1, Tm] (9) Resource bounds X m Δm(τ, ω) ≤ bϕ(ω) ∀ω ∈ Ω, τ ∈ [0, bτ] (10) Agent cannot change resources while active.",
                "Only enabled for scheduling with static assignments. θm(τ) = 1 and θm(τ + 1) = 1 =⇒ Δm(τ, ω) = Δm(τ + 1, ω) Δm(τ, ω) − Z(1 − θm(τ + 1)) ≤ Δm(τ + 1, ω) + Z(1 − θm(τ)) Δm(τ, ω) + Z(1 − θm(τ + 1)) ≥ Δm(τ + 1, ω) − Z(1 − θm(τ)) ∀m ∈ M, ω ∈ Ω, τ ∈ [0, bτ] (11) Table 1: MILP for globally optimal resource scheduling.",
                "Tm = τd m − τa m + 1 is the time horizon for the agents MDP.",
                "Using this LP as a basis, we augment it with constraints that ensure that the resource usage implied by the agents occupation measures {xm} does not violate the global resource requirements bϕ at any time step τ ∈ [0, bτ].",
                "To formulate these resource constraints, we use the following binary variables: • Δm(τ, ω) = {0, 1}, ∀m ∈ M, τ ∈ [0, bτ], ω ∈ Ω, which serve as indicator variables that define whether agent m possesses resource ω at time τ.",
                "These are analogous to the static indicator variables used in the one-shot static resource-allocation problem in [6]. • θm = {0, 1}, ∀m ∈ M, τ ∈ [0, bτ] are indicator variables that specify whether agent m is active (i.e., executing its MDP) at time τ.",
                "The meaning of resource-usage variables Δ is illustrated in Figure 1: Δm(τ, ω) = 1 only if resource ω is allocated to agent m at time τ.",
                "The meaning of the activity indicators θ is illustrated in Figure 2b: when agent m is in either the start state sb or the finish state sf , the corresponding θm = 0, but once the agent becomes active and enters one of the other states, we set θm = 1 .",
                "This meaning of θ can be enforced with a linear constraint that synchronizes the values of the agents occupation measures xm and the activity 1224 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) indicators θ, as shown in (6) in Table 1.",
                "Another constraint we have to add-because the activity indicators θ are defined on the global timeline τ-is to enforce the fact that the agent is inactive outside of its arrivaldeparture window.",
                "This is accomplished by constraint (7) in Table 1.",
                "Furthermore, agents should not be using resources while they are inactive.",
                "This constraint can also be enforced via a linear inequality on θ and Δ, as shown in (8).",
                "Constraint (6) sets the value of θ to match the policy defined by the occupation measure xm.",
                "In a similar fashion, we have to make sure that the resource-usage variables Δ are also synchronized with the occupation measure xm.",
                "This is done via constraint (9) in Table 1, which is nearly identical to the analogous constraint from [6].",
                "After implementing the above constraint, which enforces the meaning of Δ, we add a constraint that ensures that the agents resource usage never exceeds the amounts of available resources.",
                "This condition is also trivially expressed as a linear inequality (10) in Table 1.",
                "Finally, for the problem formulation where resource assignments are static during a lifetime of an agent, we add a constraint that ensures that the resource-usage variables Δ do not change their value while the agent is active (θ = 1).",
                "This is accomplished via the linear constraint (11), where Z ≥ 2 is a constant that is used to turn off the constraints when θm(τ) = 0 or θm(τ + 1) = 0.",
                "This constraint is not used for the dynamic problem formulation, where resources can be reallocated between agents at every time step.",
                "To summarize, Table 1 together with the conservationof-flow constraints from (12) defines the MILP that simultaneously computes an optimal resource assignment for all agents across time as well as optimal finite-horizon MDP policies that are valid under that resource assignment.",
                "As a rough measure of the complexity of this MILP, let us consider the number of optimization variables and constraints.",
                "Let TM = P Tm = P m(τa m − τd m + 1) be the sum of the lengths of the arrival-departure windows across all agents.",
                "Then, the number of optimization variables is: TM + bτ|M||Ω| + bτ|M|, TM of which are continuous (xm), and bτ|M||Ω| + bτ|M| are binary (Δ and θ).",
                "However, notice that all but TM|M| of the θ are set to zero by constraint (7), which also immediately forces all but TM|M||Ω| of the Δ to be zero via the constraints (8).",
                "The number of constraints (not including the degenerate constraints in (7)) in the MILP is: TM + TM|Ω| + bτ|Ω| + bτ|M||Ω|.",
                "Despite the fact that the complexity of the MILP is, in the worst case, exponential1 in the number of binary variables, the complexity of this MILP is significantly (exponentially) lower than that of the MILP with flat utility functions, described in Section 2.2.",
                "This result echos the efficiency gains reported in [6] for single-shot resource-allocation problems, but is much more pronounced, because of the explosion of the flat utility representation due to the temporal aspect of the problem (recall the prohibitive complexity of the combinatorial optimization in Section 2.2).",
                "We empirically analyze the performance of this method in Section 5. 1 Strictly speaking, solving MILPs to optimality is NPcomplete in the number of integer variables. 5.",
                "EXPERIMENTAL RESULTS Although the complexity of solving MILPs is in the worst case exponential in the number of integer variables, there are many efficient methods for solving MILPs that allow our algorithm to scale well for parameters common to resource allocation and scheduling problems.",
                "In particular, this section introduces a problem domain-the repairshop problem-used to empirically evaluate our algorithms scalability in terms of the number of agents |M|, the number of shared resources |Ω|, and the varied lengths of global time bτ during which agents may enter and exit the system.",
                "The repairshop problem is a simple parameterized MDP adopting the metaphor of a vehicular repair shop.",
                "Agents in the repair shop are mechanics with a number of independent tasks that yield reward only when completed.",
                "In our MDP model of this system, actions taken to advance through the state space are only allowed if the agent holds certain resources that are publicly available to the shop.",
                "These resources are in finite supply, and optimal policies for the shop will determine when each agent may hold the limited resources to take actions and earn individual rewards.",
                "Each task to be completed is associated with a single action, although the agent is required to repeat the action numerous times before completing the task and earning a reward.",
                "This model was parameterized in terms of the number of agents in the system, the number of different types of resources that could be linked to necessary actions, a global time during which agents are allowed to arrive and depart, and a maximum length for the number of time steps an agent may remain in the system.",
                "All datapoints in our experiments were obtained with 20 evaluations using CPLEX to solve the MILPs on a Pentium4 computer with 2Gb of RAM.",
                "Trials were conducted on both the static and the dynamic version of the resourcescheduling problem, as defined earlier.",
                "Figure 3 shows the runtime and policy value for independent modifications to the parameter set.",
                "The top row shows how the solution time for the MILP scales as we increase the number of agents |M|, the global time horizon bτ, and the number of resources |Ω|.",
                "Increasing the number of agents leads to exponential complexity scaling, which is to be expected for an NP-complete problem.",
                "However, increasing the global time limit bτ or the total number of resource types |Ω|-while holding the number of agents constantdoes not lead to decreased performance.",
                "This occurs because the problems get easier as they become under-constrained, which is also a common phenomenon for NP-complete problems.",
                "We also observe that the solution to the dynamic version of the problem can often be computed much faster than the static version.",
                "The bottom row of Figure 3 shows the joint policy value of the policies that correspond to the computed optimal resource-allocation schedules.",
                "We can observe that the dynamic version yields higher reward (as expected, since the reward for the dynamic version is always no less than the reward of the static version).",
                "We should point out that these graphs should not be viewed as a measure of performance of two different algorithms (both algorithms produce optimal solutions but to different problems), but rather as observations about how the quality of optimal solutions change as more flexibility is allowed in the reallocation of resources.",
                "Figure 4 shows runtime and policy value for trials in which common input variables are scaled together.",
                "This allows The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1225 2 4 6 8 10 10 −3 10 −2 10 −1 10 0 10 1 10 2 10 3 10 4 Number of Agents |M| CPUTime,sec |Ω| = 5, τ = 50 static dynamic 50 100 150 200 10 −2 10 −1 10 0 10 1 10 2 10 3 Global Time Boundary τ CPUTime,sec |M| = 5, |Ω| = 5 static dynamic 10 20 30 40 50 10 −2 10 −1 10 0 10 1 10 2 Number of Resources |Ω| CPUTime,sec |M| = 5, τ = 50 static dynamic 2 4 6 8 10 200 400 600 800 1000 1200 1400 1600 Number of Agents |M| Value |Ω| = 5, τ = 50 static dynamic 50 100 150 200 400 500 600 700 800 900 1000 1100 1200 1300 1400 Global Time Boundary τ Value |M| = 5, |Ω| = 5 static dynamic 10 20 30 40 50 500 600 700 800 900 1000 1100 1200 1300 1400 Number of Resources |Ω| Value |M| = 5, τ = 50 static dynamic Figure 3: Evaluation of our MILP for variable numbers of agents (column 1), lengths of global-time window (column 2), and numbers of resource types (column 3).",
                "Top row shows CPU time, and bottom row shows the joint reward of agents MDP policies.",
                "Error bars show the 1st and 3rd quartiles (25% and 75%). 2 4 6 8 10 10 −3 10 −2 10 −1 10 0 10 1 10 2 10 3 Number of Agents |M| CPUTime,sec τ = 10|M| static dynamic 2 4 6 8 10 10 −3 10 −2 10 −1 10 0 10 1 10 2 10 3 10 4 Number of Agents |M| CPUTime,sec |Ω| = 2|M| static dynamic 2 4 6 8 10 10 −3 10 −2 10 −1 10 0 10 1 10 2 10 3 10 4 Number of Agents |M| CPUTime,sec |Ω| = 5|M| static dynamic 2 4 6 8 10 200 400 600 800 1000 1200 1400 1600 1800 2000 2200 Number of Agents |M| Value τ = 10|M| static dynamic 2 4 6 8 10 200 400 600 800 1000 1200 1400 1600 1800 2000 Number of Agents |M| Value |Ω| = 2|M| static dynamic 2 4 6 8 10 0 500 1000 1500 2000 2500 Number of Agents |M| Value |Ω| = 5|M| static dynamic Figure 4: Evaluation of our MILP using correlated input variables.",
                "The left column tracks the performance and CPU time as the number of agents and global-time window increase together (bτ = 10|M|).",
                "The middle and the right column track the performance and CPU time as the number of resources and the number of agents increase together as |Ω| = 2|M| and |Ω| = 5|M|, respectively.",
                "Error bars show the 1st and 3rd quartiles (25% and 75%). 1226 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) us to explore domains where the total number of agents scales proportionally to the total number of resource types or the global time horizon, while keeping constant the average agent density (per unit of global time) or the average number of resources per agent (which commonly occurs in real-life applications).",
                "Overall, we believe that these experimental results indicate that our MILP formulation can be used to effectively solve resource-scheduling problems of nontrivial size. 6.",
                "DISCUSSION AND CONCLUSIONS Throughout the paper, we have made a number of assumptions in our model and solution algorithm; we discuss their implications below. • Continual execution.",
                "We assume that once an agent stops executing its MDP (transitions into state sf ), it exits the system and cannot return.",
                "It is easy to relax this assumption for domains where agents MDPs can be paused and restarted.",
                "All that is required is to include an additional pause action which transitions from a given state back to itself, and has zero reward. • Indifference to start time.",
                "We used a reward model where agents rewards depend only on the time horizon of their MDPs and not the global start time.",
                "This is a consequence of our MDP-augmentation procedure from Section 4.1.",
                "It is easy to extend the model so that the agents incur an explicit penalty for idling by assigning a non-zero negative reward to the start state sb . • Binary resource requirements.",
                "For simplicity, we have assumed that resource costs are binary: ϕm(a, ω) = {0, 1}, but our results generalize in a straightforward manner to non-binary resource mappings, analogously to the procedure used in [5]. • Cooperative agents.",
                "The optimization procedure discussed in this paper was developed in the context of cooperative agents, but it can also be used to design a mechanism for scheduling resources among self-interested agents.",
                "This optimization procedure can be embedded in a VickreyClarke-Groves auction, completely analogously to the way it was done in [7].",
                "In fact, all the results of [7] about the properties of the auction and information privacy directly carry over to the scheduling domain discussed in this paper, requiring only slight modifications to deal with finitehorizon MDPs. • Known, deterministic arrival and departure times.",
                "Finally, we have assumed that agents arrival and departure times (τa m and τd m) are deterministic and known a priori.",
                "This assumption is fundamental to our solution method.",
                "While there are many domains where this assumption is valid, in many cases agents arrive and depart dynamically and their arrival and departure times can only be predicted probabilistically, leading to online resource-allocation problems.",
                "In particular, in the case of self-interested agents, this becomes an interesting version of an online-mechanism-design problem [11, 12].",
                "In summary, we have presented an MILP formulation for the combinatorial resource-scheduling problem where agents values for possible resource assignments are defined by finitehorizon MDPs.",
                "This result extends previous work ([6, 7]) on static one-shot resource allocation under MDP-induced preferences to resource-scheduling problems with a temporal aspect.",
                "As such, this work takes a step in the direction of designing an online mechanism for agents with combinatorial resource preferences induced by stochastic planning problems.",
                "Relaxing the assumption about deterministic arrival and departure times of the agents is a focus of our future work.",
                "We would like to thank the anonymous reviewers for their insightful comments and suggestions. 7.",
                "REFERENCES [1] E. Altman and A. Shwartz.",
                "Adaptive control of constrained Markov chains: Criteria and policies.",
                "Annals of Operations Research, special issue on Markov Decision Processes, 28:101-134, 1991. [2] R. Bellman.",
                "Dynamic Programming.",
                "Princeton University Press, 1957. [3] C. Boutilier.",
                "Solving concisely expressed combinatorial auction problems.",
                "In Proc. of AAAI-02, pages 359-366, 2002. [4] C. Boutilier and H. H. Hoos.",
                "Bidding languages for combinatorial auctions.",
                "In Proc. of IJCAI-01, pages 1211-1217, 2001. [5] D. Dolgov.",
                "Integrated Resource Allocation and Planning in Stochastic Multiagent Environments.",
                "PhD thesis, Computer Science Department, University of Michigan, February 2006. [6] D. A. Dolgov and E. H. Durfee.",
                "Optimal resource allocation and policy formulation in loosely-coupled Markov decision processes.",
                "In Proc. of ICAPS-04, pages 315-324, June 2004. [7] D. A. Dolgov and E. H. Durfee.",
                "Computationally efficient combinatorial auctions for resource allocation in weakly-coupled MDPs.",
                "In Proc. of AAMAS-05, New York, NY, USA, 2005.",
                "ACM Press. [8] D. A. Dolgov and E. H. Durfee.",
                "Resource allocation among agents with preferences induced by factored MDPs.",
                "In Proc. of AAMAS-06, 2006. [9] K. Larson and T. Sandholm.",
                "Mechanism design and deliberative agents.",
                "In Proc. of AAMAS-05, pages 650-656, New York, NY, USA, 2005.",
                "ACM Press. [10] N. Nisan.",
                "Bidding and allocation in combinatorial auctions.",
                "In Electronic Commerce, 2000. [11] D. C. Parkes and S. Singh.",
                "An MDP-based approach to Online Mechanism Design.",
                "In Proc. of the Seventeenths Annual Conference on Neural Information Processing Systems (NIPS-03), 2003. [12] D. C. Parkes, S. Singh, and D. Yanovsky.",
                "Approximately efficient online mechanism design.",
                "In Proc. of the Eighteenths Annual Conference on Neural Information Processing Systems (NIPS-04), 2004. [13] M. L. Puterman.",
                "Markov Decision Processes.",
                "John Wiley & Sons, New York, 1994. [14] M. H. Rothkopf, A. Pekec, and R. M. Harstad.",
                "Computationally manageable combinational auctions.",
                "Management Science, 44(8):1131-1147, 1998. [15] T. Sandholm.",
                "An algorithm for optimal winner determination in combinatorial auctions.",
                "In Proc. of IJCAI-99, pages 542-547, San Francisco, CA, USA, 1999.",
                "Morgan Kaufmann Publishers Inc.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1227"
            ],
            "original_annotated_samples": [
                "In particular, if an agent uses resources to act in a stochastic environment, its utility function can be naturally modeled with a <br>markov decision process</br>, whose action set is parameterized by the available resources."
            ],
            "translated_annotated_samples": [
                "En particular, si un agente utiliza recursos para actuar en un entorno estocástico, su función de utilidad puede ser modelada de forma natural con un <br>proceso de decisión de Markov</br>, cuyo conjunto de acciones está parametrizado por los recursos disponibles."
            ],
            "translated_text": "La programación de recursos combinatoria para MDPs multiagentes. Dmitri A. Dolgov, Michael R. James y Michael E. Samples. Grupo de Investigación en IA y Robótica, Centro Técnico de Toyota en EE. UU. {ddolgov, michael.r.james, michael.samples}@gmail.com. RESUMEN La programación óptima de recursos en sistemas multiagentes es una tarea computacionalmente desafiante, especialmente cuando los valores de los recursos no son aditivos. Consideramos el problema combinatorio de programar el uso de múltiples recursos entre agentes que operan en entornos estocásticos, modelados como procesos de decisión de Markov (MDPs). En los últimos años, se han desarrollado algoritmos eficientes de asignación de recursos para agentes con valores de recursos inducidos por MDPs. Sin embargo, este trabajo previo se ha centrado en problemas de asignación de recursos estáticos donde los recursos se distribuyen una vez y luego se utilizan en MDPs de horizonte infinito. Extendemos esos modelos existentes al problema de la programación de recursos combinatoria, donde los agentes persisten solo por períodos finitos entre sus tiempos de llegada y salida (predefinidos), requiriendo recursos solo durante esos períodos de tiempo. Proporcionamos un procedimiento computacionalmente eficiente para calcular asignaciones de recursos óptimas a nivel global a agentes a lo largo del tiempo. Ilustramos y analizamos empíricamente el método en el contexto de un dominio de programación de trabajos estocásticos. Categorías y Descriptores de Asignaturas I.2.8 [Inteligencia Artificial]: Resolución de Problemas, Métodos de Control y Búsqueda; I.2.11 [Inteligencia Artificial]: Inteligencia Artificial Distribuida-Sistemas Multiagente Términos Generales Algoritmos, Rendimiento, Diseño 1. INTRODUCCIÓN Las tareas de asignación óptima de recursos y programación son ubicuas en sistemas multiagentes, pero resolver tales problemas de optimización puede ser computacionalmente difícil, debido a una serie de factores. En particular, cuando el valor de un conjunto de recursos para un agente no es aditivo (como suele ser el caso con recursos que son sustitutos o complementos), la función de utilidad podría tener que definirse en un espacio exponencialmente grande de combinaciones de recursos, lo que rápidamente se vuelve computacionalmente intratable. Además, incluso cuando cada agente tiene una función de utilidad que es distinta de cero solo en un pequeño subconjunto de los posibles conjuntos de recursos, obtener una asignación óptima sigue siendo computacionalmente prohibitivo, ya que el problema se vuelve NP-completo [14]. Tales problemas computacionales han dado lugar recientemente a varias líneas de trabajo en el uso de modelos compactos de las preferencias de los agentes. Una idea es utilizar cualquier estructura presente en las funciones de utilidad para representarlas de forma compacta, por ejemplo, a través de fórmulas lógicas [15, 10, 4, 3]. Una alternativa es modelar directamente los mecanismos que definen las funciones de utilidad de los agentes y realizar la asignación de recursos directamente con estos modelos [9]. Una forma de lograr esto es modelar los procesos mediante los cuales un agente podría utilizar los recursos y definir la función de utilidad como la ganancia de estos procesos. En particular, si un agente utiliza recursos para actuar en un entorno estocástico, su función de utilidad puede ser modelada de forma natural con un <br>proceso de decisión de Markov</br>, cuyo conjunto de acciones está parametrizado por los recursos disponibles. Esta representación puede ser utilizada para construir algoritmos de asignación de recursos muy eficientes que resultan en una aceleración exponencial sobre un problema de optimización directa con representaciones planas de preferencias combinatorias [6, 7, 8]. Sin embargo, este trabajo existente sobre asignación de recursos con preferencias inducidas por MDPs parametrizados por recursos hace una suposición de que los recursos se asignan solo una vez y luego son utilizados por los agentes de forma independiente dentro de sus MDPs de horizonte infinito. Esta suposición de que no es posible la reasignación de recursos puede ser limitante en dominios donde los agentes llegan y se van dinámicamente. En este artículo, ampliamos el trabajo sobre asignación de recursos bajo preferencias inducidas por MDP a problemas de programación en tiempo discreto, donde los agentes están presentes en el sistema durante intervalos de tiempo finitos y solo pueden utilizar recursos dentro de estos intervalos. En particular, los agentes llegan y parten en momentos arbitrarios (predefinidos) y dentro de estos intervalos utilizan recursos para ejecutar tareas en MDPs de horizonte finito. Abordamos el problema de la programación de recursos óptima a nivel global, donde el objetivo es encontrar una asignación de recursos a los agentes a lo largo del tiempo que maximice la suma de las recompensas esperadas que obtienen. En este contexto, nuestra principal contribución es una formulación de programación entera mixta del problema de programación que elige asignaciones de recursos globalmente óptimas, tiempos de inicio y horizontes de ejecución para todos los agentes (dentro de sus intervalos de llegada y salida). Analizamos y comparamos empíricamente dos variantes del problema de programación: uno, donde los agentes tienen asignaciones de recursos estáticas dentro de sus MDPs de horizonte finito, y otro, donde los recursos pueden ser realocados dinámicamente entre agentes en cada paso de tiempo. En el resto del documento, primero establecemos las bases necesarias en la Sección 2 y luego presentamos nuestro modelo y declaración formal del problema en la Sección 3. En la Sección 4.2, describimos nuestro resultado principal, el programa de optimización para la programación de recursos globalmente óptima. Tras la discusión de nuestros resultados experimentales sobre un problema de programación de trabajos en la Sección 5, concluimos en la Sección 6 con una discusión de posibles extensiones y generalizaciones de nuestro método. De manera similar al modelo utilizado en trabajos anteriores sobre asignación de recursos con preferencias inducidas por MDP [6, 7], definimos el valor de un conjunto de recursos para un agente como el valor de la mejor política MDP que es realizable, dadas esos recursos. Sin embargo, dado que el enfoque de nuestro trabajo se centra en problemas de programación, y una gran parte del problema de optimización consiste en decidir cómo se asignan los recursos en el tiempo entre agentes con tiempos finitos de llegada y salida, modelamos los problemas de planificación de los agentes como MDPs de horizonte finito, en contraste con trabajos anteriores que utilizaron MDPs descontados de horizonte infinito. En el resto de esta sección, primero presentamos algo de antecedentes necesarios sobre MDPs de horizonte finito y presentamos una formulación de programación lineal que sirve como base para nuestro algoritmo de solución desarrollado en la Sección 4. También describimos los métodos estándar para la programación de recursos combinatorios con valores de recursos planos, que sirven como punto de referencia de comparación para el nuevo modelo desarrollado aquí. 2.1 Procesos de Decisión de Markov Un MDP (Proceso de Decisión de Markov) estacionario, de dominio finito y discreto en el tiempo (ver, por ejemplo, [13] para un desarrollo detallado y exhaustivo) se puede describir como S, A, p, r, donde: S es un conjunto finito de estados del sistema; A es un conjunto finito de acciones disponibles para el agente; p es una función de transición estocástica estacionaria, donde p(σ|s, a) es la probabilidad de transicionar al estado σ al ejecutar la acción a en el estado s; r es una función de recompensa estacionaria, donde r(s, a) especifica la recompensa obtenida al ejecutar la acción a en el estado s. Dado un MDP de este tipo, un problema de decisión bajo un horizonte finito T consiste en elegir una acción óptima en cada paso de tiempo para maximizar el valor esperado de la recompensa total acumulada durante la vida (finita) de los agentes. La política óptima del agente es entonces una función del estado actual s y el tiempo hasta el horizonte. Una política óptima para tal problema es actuar de forma codiciosa con respecto a la función de valor óptimo, definida de forma recursiva por el siguiente sistema de ecuaciones de Bellman de tiempo finito [2]: v(s, t) = max a r(s, a) + X σ p(σ|s, a)v(σ, t + 1), ∀s ∈ S, t ∈ [1, T − 1]; v(s, T) = 0, ∀s ∈ S; donde v(s, t) es el valor óptimo de estar en el estado s en el tiempo t ∈ [1, T]. Esta función de valor óptimo se puede calcular fácilmente utilizando programación dinámica, lo que conduce a la siguiente política óptima π, donde π(s, a, t) es la probabilidad de ejecutar la acción a en el estado s en el tiempo t: π(s, a, t) = (1, a = argmaxa r(s, a) + P σ p(σ|s, a)v(σ, t + 1), 0, en caso contrario. La anterior es la forma más común de calcular la función de valor óptimo (y por lo tanto una política óptima) para un MDP de horizonte finito. Sin embargo, también podemos formular el problema como el siguiente programa lineal (similarmente al LP dual para MDPs descontados de horizonte infinito [13, 6, 7]): max X s X a r(s, a) X t x(s, a, t) sujeto a: X a x(σ, a, t + 1) = X s,a p(σ|s, a)x(s, a, t) ∀σ, t ∈ [1, T − 1]; X a x(s, a, 1) = α(s), ∀s ∈ S; (1) donde α(s) es la distribución inicial sobre el espacio de estados, y x es la medida de ocupación (no estacionaria) (x(s, a, t) ∈ [0, 1] es el número total esperado de veces que se ejecuta la acción a en el estado s en el tiempo t). Una política óptima (no estacionaria) se obtiene a partir de la medida de ocupación de la siguiente manera: π(s, a, t) = x(s, a, t)/ X a x(s, a, t) ∀s ∈ S, t ∈ [1, T]. (2) Tenga en cuenta que el MDP estándar de horizonte finito sin restricciones, como se describe anteriormente, siempre tiene una solución uniformemente óptima (óptima para cualquier distribución inicial α(s)). Por lo tanto, una política óptima se puede obtener utilizando una constante arbitraria α(s) > 0 (en particular, α(s) = 1 resultará en x(s, a, t) = π(s, a, t)). Sin embargo, para los MDPs con restricciones de recursos (como se definen a continuación en la Sección 3), las políticas óptimas uniformemente no existen en general. En tales casos, α se convierte en parte de la entrada del problema, y una política resultante solo es óptima para ese α en particular. Este resultado es bien conocido para MDPs de horizonte infinito con varios tipos de restricciones [1, 6], y también se cumple para nuestro modelo de horizonte finito, el cual puede establecerse fácilmente a través de una línea de razonamiento completamente análoga a los argumentos en [6]. 2.2 Programación de Recursos Combinatoria Un enfoque directo para la programación de recursos para un conjunto de agentes M, cuyos valores para los recursos son inducidos por problemas de planificación estocástica (en nuestro caso, MDPs de horizonte finito) sería que cada agente enumerara todas las asignaciones de recursos posibles a lo largo del tiempo y, para cada una, calcular su valor resolviendo el MDP correspondiente. Entonces, cada agente proporcionaría valoraciones para cada posible conjunto de recursos a lo largo del tiempo a un coordinador centralizado, quien calcularía las asignaciones óptimas de recursos a lo largo del tiempo basándose en estas valoraciones. Cuando los recursos pueden asignarse en diferentes momentos a diferentes agentes, cada agente debe presentar valoraciones para cada combinación de posibles horizontes temporales. Que cada agente m ∈ M ejecute su MDP dentro del intervalo de tiempo de llegada-salida τ ∈ [τa m, τd m]. Por lo tanto, el agente m ejecutará un MDP con un horizonte temporal no mayor que Tm = τd m−τa m+1. Que bτ sea el horizonte temporal global para el problema, antes del cual todos los MDP de los agentes deben finalizar. Suponemos que τd m < bτ, ∀m ∈ M. El Sexto Congreso Internacional. En el problema de programación donde los agentes tienen requisitos de recursos estáticos dentro de sus MDPs de horizonte finito, los agentes proporcionan una valoración para cada conjunto de recursos para cada horizonte de tiempo posible (desde [1, Tm]) que puedan utilizar. Sea Ω el conjunto de recursos a ser asignados entre los agentes. Un agente recibirá como máximo un paquete de recursos para uno de los horizontes temporales. Que la variable ψ ∈ Ψm enumere todos los posibles pares de conjuntos de recursos y horizontes temporales para el agente m, por lo que hay 2|Ω| × Tm valores para ψ (el espacio de conjuntos es exponencial en el número de tipos de recursos |Ω|). El agente m debe proporcionar un valor vψ m para cada ψ, y el coordinador asignará como máximo un par (recurso, horizonte temporal) ψ a cada agente. Esta asignación se expresa como una variable indicadora zψ m ∈ {0, 1} que muestra si ψ está asignado al agente m. Para el tiempo τ y el recurso ω, la función nm(ψ, τ, ω) ∈ {0, 1} indica si el paquete en ψ utiliza el recurso ω en el tiempo τ (hacemos la suposición de que los agentes tienen requisitos de recursos binarios). Este problema de asignación es NP-completo, incluso al considerar solo un paso de tiempo, y su dificultad aumenta significativamente con múltiples pasos de tiempo debido al creciente número de valores de ψ. El problema de encontrar una asignación óptima que satisfaga la restricción global de que la cantidad de cada recurso ω asignada a todos los agentes no exceda la cantidad disponible bϕ(ω) se puede expresar como el siguiente programa entero: max X m∈M X ψ∈Ψm zψ mvψ m sujeto a: X ψ∈Ψm zψ m ≤ 1, ∀m ∈ M; X m∈M X ψ∈Ψm zψ mnm(ψ, τ, ω) ≤ bϕ(ω), ∀τ ∈ [1, bτ], ∀ω ∈ Ω; (3) La primera restricción en la ecuación 3 dice que ningún agente puede recibir más de un paquete, y la segunda restricción asegura que la asignación total del recurso ω no exceda, en ningún momento, el límite del recurso. Para el problema de programación en el que los agentes pueden reasignar dinámicamente recursos, cada agente debe especificar un valor para cada combinación de paquetes y pasos de tiempo dentro de su horizonte temporal. Que la variable ψ ∈ Ψm en este caso enumere todos los posibles conjuntos de recursos para los cuales a lo sumo un conjunto puede ser asignado al agente m en cada paso de tiempo. Por lo tanto, en este caso hay P t∈[1,Tm](2|Ω| )t ∼ 2|Ω|Tm posibilidades de combinaciones de recursos asignadas a diferentes intervalos de tiempo, para los Tm horizontes de tiempo diferentes. El mismo conjunto de ecuaciones (3) se puede utilizar para resolver este problema de programación dinámica, pero el programa entero es diferente debido a la diferencia en cómo se define ψ. En este caso, el número de valores de ψ es exponencial en el horizonte de planificación Tm de cada agente, lo que resulta en un programa mucho más grande. Este enfoque directo para resolver ambos problemas de programación requiere una enumeración y solución de 2|Ω| Tm (asignación estática) o P t∈[1,Tm] 2|Ω|t (reubicación dinámica) MDPs para cada agente, lo cual rápidamente se vuelve intratable con el crecimiento del número de recursos |Ω| o el horizonte de tiempo Tm. MODELO Y DECLARACIÓN DEL PROBLEMA Ahora presentamos formalmente nuestro modelo del problema de programación de recursos. La entrada del problema consiste en los siguientes componentes: • M, Ω, bϕ, τa m, τd m, bτ están definidos arriba en la Sección 2.2. • {Θm} = {S, A, pm, rm, αm} son los MDPs de todos los agentes m ∈ M. Sin pérdida de generalidad, asumimos que los espacios de estados y acciones de todos los agentes son iguales, pero cada uno tiene su propia función de transición pm, función de recompensa rm, y condiciones iniciales αm. • ϕm : A×Ω → {0, 1} es la asignación de acciones a recursos para el agente m. ϕm(a, ω) indica si la acción a del agente m necesita el recurso ω. Un agente m que recibe un conjunto de recursos que no incluye el recurso ω no puede ejecutar en su política MDP ninguna acción a para la cual ϕm(a, ω) = 0. Suponemos que todos los requisitos de recursos son binarios; como se discute más adelante en la Sección 6, esta suposición no es limitante. Dado el input anterior, el problema de optimización que consideramos es encontrar la asignación globalmente óptima que maximice la suma de recompensas esperadas de recursos a agentes para todos los pasos de tiempo: Δ : τ × M × Ω → {0, 1}. Una solución es factible si la asignación correspondiente de recursos a los agentes no viola la restricción global de recursos: X m Δm(τ, ω) ≤ bϕ(ω), ∀ω ∈ Ω, τ ∈ [1, bτ]. Consideramos dos variantes del problema de programación de recursos. La primera formulación restringe las asignaciones de recursos al espacio donde la asignación de recursos a cada agente es estática durante la vida de los agentes. La segunda formulación permite la reasignación de recursos entre agentes en cada paso de tiempo durante sus vidas. La Figura 1 representa un problema de programación de recursos con tres agentes M = {m1, m2, m3}, tres recursos Ω = {ω1, ω2, ω3}, y un horizonte de problema global de bτ = 11. Los tiempos de llegada y salida de los agentes se muestran como cajas grises y son {1, 6}, {3, 7} y {2, 11}, respectivamente. Una solución a este problema se muestra a través de barras horizontales dentro de cada casilla de agentes, donde las barras corresponden a la asignación de los tres tipos de recursos. La Figura 1a muestra una solución a un problema de programación estática. Según la solución mostrada, el agente m1 comienza la ejecución de su MDP en el tiempo τ = 1 y tiene un bloqueo en los tres recursos hasta que finaliza la ejecución en el tiempo τ = 3. Se debe tener en cuenta que el agente m1 renuncia a su control sobre los recursos antes de su hora de salida anunciada de τd m1 = 6, aparentemente porque otros agentes pueden utilizar los recursos de manera más efectiva. Por lo tanto, en el tiempo τ = 4, los recursos ω1 y ω3 se asignan al agente m2, quien luego los utiliza para ejecutar su MDP (utilizando solo acciones admitidas por los recursos ω1 y ω3) hasta el tiempo τ = 7. El agente m3 posee el recurso ω3 durante el intervalo τ ∈ [4, 10]. La Figura 1b muestra una posible solución a la versión dinámica del mismo problema. Allí, los recursos pueden ser reasignados entre agentes en cada paso de tiempo. Por ejemplo, el agente m1 renuncia a su uso del recurso ω2 en el tiempo τ = 2, aunque continúa la ejecución de su MDP hasta el tiempo τ = 6. Ten en cuenta que a un agente no se le permite detener y reiniciar su MDP, por lo que el agente m1 solo puede continuar ejecutando en el intervalo τ ∈ [3, 4] si tiene acciones que no requieren recursos (ϕm(a, ω) = 0). Claramente, el modelo y la declaración del problema descritos anteriormente hacen una serie de suposiciones sobre el problema y las propiedades de la solución deseada. Discutimos algunas de esas suposiciones y sus implicaciones en la Sección 6. 1222 La Sexta Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) (a) (b) Figura 1: Ilustración de una solución a un problema de programación de recursos con tres agentes y tres recursos: a) asignaciones de recursos estáticas (las asignaciones de recursos son constantes durante la vida de los agentes); b) asignación dinámica (las asignaciones de recursos pueden cambiar en cada paso de tiempo). 4. PROGRAMACIÓN DE RECURSOS Nuestro algoritmo de programación de recursos se lleva a cabo en dos etapas. Primero, realizamos un paso de preprocesamiento que aumenta los MDP de los agentes; este proceso se describe en la Sección 4.1. Segundo, utilizando estos MDPs aumentados construimos un problema de optimización global, el cual se describe en la Sección 4.2. 4.1 Aumentando los MDPs de los Agentes En el modelo descrito en la sección anterior, asumimos que si un agente no posee los recursos necesarios para realizar acciones en su MDP, su ejecución se detiene y el agente abandona el sistema. En otras palabras, los MDPs no pueden ser pausados y reanudados. Por ejemplo, en el problema mostrado en la Figura 1a, el agente m1 libera todos los recursos después del tiempo τ = 3, momento en el cual la ejecución de su MDP se detiene. De manera similar, los agentes m2 y m3 solo ejecutan sus MDPs en los intervalos τ ∈ [4, 6] y τ ∈ [4, 10], respectivamente. Por lo tanto, una parte importante del problema de toma de decisiones global es decidir la ventana de tiempo durante la cual cada uno de los agentes está activo (es decir, ejecutando su MDP). Para lograr esto, ampliamos el MDP de cada agente con dos nuevos estados (estados de inicio y final sb, sf, respectivamente) y una nueva acción de inicio/parada a∗, como se ilustra en la Figura 2. La idea es que un agente permanezca en el estado inicial sb hasta que esté listo para ejecutar su MDP, momento en el que realiza la acción de inicio/parada a∗ y transita al espacio de estados del MDP original con la probabilidad de transición que corresponde a la distribución inicial original α(s). Por ejemplo, en la Figura 1a, para el agente m2 esto ocurriría en el tiempo τ = 4. Una vez que el agente llega al final de su ventana de actividad (tiempo τ = 6 para el agente m2 en la Figura 1a), realiza la acción de inicio/parada, lo que lo lleva al estado final de finalización del sumidero sf en el tiempo τ = 7. Más precisamente, dado un MDP S, A, pm, rm, αm, definimos un MDP aumentado S, A, pm, rm, αm de la siguiente manera: S = S ∪ sb ∪ sf; A = A ∪ a∗; p(s|sb, a∗) = α(s), ∀s ∈ S; p(sb|sb, a) = 1.0, ∀a ∈ A; p(sf|s, a∗) = 1.0, ∀s ∈ S; p(σ|s, a) = p(σ|s, a), ∀s, σ ∈ S, a ∈ A; r(sb, a) = r(sf, a) = 0, ∀a ∈ A; r(s, a) = r(s, a), ∀s ∈ S, a ∈ A; α(sb) = 1; α(s) = 0, ∀s ∈ S; donde se asume que todas las probabilidades de transición no especificadas son cero. Además, para tener en cuenta el nuevo estado inicial, comenzamos el MDP un paso de tiempo antes, estableciendo τa m ← τa m − 1. Esto no afectará la asignación de recursos debido a que las restricciones de recursos solo se aplican a los estados originales del MDP, como se discutirá en la siguiente sección. Por ejemplo, los MDPs aumentados mostrados en la Figura 2b (que comienza en el estado sb en el tiempo τ = 2) se construirían a partir de un MDP con tiempo de llegada original τ = 3. La Figura 2b también muestra una trayectoria de muestra a través del espacio de estados: el agente comienza en el estado sb, transita al espacio de estados S del MDP original y finalmente sale al estado de sumidero sf. Cabe destacar que si quisiéramos modelar un problema en el que los agentes pudieran pausar sus MDP en pasos de tiempo arbitrarios (lo cual podría ser útil para dominios donde la reasignación dinámica es posible), podríamos lograrlo fácilmente incluyendo una acción adicional que transiciona de cada estado a sí mismo con una recompensa de cero. 4.2 MILP para la programación de recursos Dado un conjunto de MDPs aumentados, como se define arriba, el objetivo de esta sección es formular un programa de optimización global que resuelva el problema de programación de recursos. En esta sección y a continuación, se asume que todos los MDPs son los MDPs aumentados tal como se definen en la Sección 4.1. Nuestro enfoque es similar a la idea utilizada en [6]: comenzamos con la formulación del programa lineal de los MDP de los agentes (1) y lo complementamos con restricciones que aseguran que la asignación de recursos correspondiente entre agentes y tiempo sea válida. El problema de optimización resultante resuelve simultáneamente los MDP de los agentes y los problemas de programación de recursos. En el resto de esta sección, desarrollamos de forma incremental un programa de enteros mixtos (MILP) que logra esto. En ausencia de restricciones de recursos, los MDPs de horizonte finito de los agentes son completamente independientes, y la solución óptima global puede obtenerse trivialmente a través del siguiente LP, que es simplemente una agregación de LPs de horizonte finito de un solo agente: max X m X s X a rm(s, a) X t xm(s, a, t) sujeto a: X a xm(σ, a, t + 1) = X s,a pm(σ|s, a)xm(s, a, t), ∀m ∈ M, σ ∈ S, t ∈ [1, Tm − 1]; X a xm(s, a, 1) = αm(s), ∀m ∈ M, s ∈ S; (12) donde xm(s, a, t) es la medida de ocupación del agente m, y The Sixth Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 1223 (a) (b) Figura 2: Ilustración de la ampliación de un MDP para permitir tiempos de inicio y finalización variables: a) (izquierda) el MDP original de dos estados con una sola acción; (derecha) el MDP ampliado con nuevos estados sb y sf y la nueva acción a∗ (nota que las transiciones originales no se modifican en el proceso de ampliación); b) el MDP ampl augmentado mostrado como una trayectoria a través del tiempo (las líneas grises indican todas las transiciones, mientras que las líneas negras indican una trayectoria dada). Función objetivo (suma de recompensas esperadas de todos los agentes) máx X m X s X a rm(s, a) X t xm(s, a, t) (5) Implicación de significado Restricciones lineales Atar x a θ. El agente solo está activo cuando la medida de ocupación es distinta de cero en los estados originales del MDP. θm(τ) = 0 =⇒ xm(s, a, τ −τa m+1) = 0 ∀s /∈ {sb , sf }, a ∈ A X s/∈{sb,sf } X a xm(s, a, t) ≤ θm(τa m + t − 1) ∀m ∈ M, ∀t ∈ [1, Tm] (6) El agente solo puede estar activo en τ ∈ (τa m, τd m) θm(τ) = 0 ∀m ∈ M, τ /∈ (τa m, τd m) (7) No se pueden utilizar recursos cuando no está activo θm(τ) = 0 =⇒ Δm(τ, ω) = 0 ∀τ ∈ [0, bτ], ω ∈ Ω Δm(τ, ω) ≤ θm(τ) ∀m ∈ M, τ ∈ [0, bτ], ω ∈ Ω (8) Vincular x a Δ (x distinto de cero obliga a que el correspondiente Δ sea distinto de cero). Δm(τ, ω) = 0, ϕm(a, ω) = 1 =⇒ xm(s, a, τ − τa m + 1) = 0 ∀s /∈ {sb , sf } 1/|A| X a ϕm(a, ω) X s/∈{sb,sf } xm(s, a, t) ≤ Δm(t + τa m − 1, ω) ∀m ∈ M, ω ∈ Ω, t ∈ [1, Tm] (9) Límites de recursos X m Δm(τ, ω) ≤ bϕ(ω) ∀ω ∈ Ω, τ ∈ [0, bτ] (10) El agente no puede cambiar los recursos mientras está activo. Solo habilitado para programación con asignaciones estáticas. θm(τ) = 1 y θm(τ + 1) = 1 =⇒ Δm(τ, ω) = Δm(τ + 1, ω) Δm(τ, ω) − Z(1 − θm(τ + 1)) ≤ Δm(τ + 1, ω) + Z(1 − θm(τ)) Δm(τ, ω) + Z(1 − θm(τ + 1)) ≥ Δm(τ + 1, ω) − Z(1 − θm(τ)) ∀m ∈ M, ω ∈ Ω, τ ∈ [0, bτ] (11) Tabla 1: MILP para programación óptima global de recursos. Tm = τd m − τa m + 1 es el horizonte temporal para los agentes MDP. Usando este LP como base, lo ampliamos con restricciones que aseguran que el uso de recursos implicado por las medidas de ocupación de los agentes {xm} no viola los requisitos globales de recursos bϕ en ningún paso de tiempo τ ∈ [0, bτ]. Para formular estas restricciones de recursos, utilizamos las siguientes variables binarias: • Δm(τ, ω) = {0, 1}, ∀m ∈ M, τ ∈ [0, bτ], ω ∈ Ω, que sirven como variables indicadoras que definen si el agente m posee el recurso ω en el tiempo τ. Estos son análogos a las variables indicadoras estáticas utilizadas en el problema de asignación de recursos estáticos de una sola vez en [6]. • θm = {0, 1}, ∀m ∈ M, τ ∈ [0, bτ] son variables indicadoras que especifican si el agente m está activo (es decir, ejecutando su MDP) en el tiempo τ. El significado de las variables de uso de recursos Δ se ilustra en la Figura 1: Δm(τ, ω) = 1 solo si el recurso ω se asigna al agente m en el tiempo τ. El significado de los indicadores de actividad θ se ilustra en la Figura 2b: cuando el agente m se encuentra en el estado inicial sb o en el estado final sf, el θm correspondiente es igual a 0, pero una vez que el agente se vuelve activo y entra en uno de los otros estados, establecemos θm = 1. Este significado de θ puede ser impuesto con una restricción lineal que sincroniza los valores de las medidas de ocupación de los agentes xm y la actividad 1224 The Sixth Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) indicadores θ, como se muestra en (6) en la Tabla 1. Otra restricción que debemos agregar, debido a que los indicadores de actividad θ están definidos en la línea de tiempo global τ, es hacer cumplir el hecho de que el agente está inactivo fuera de su ventana de llegada y salida. Esto se logra mediante la restricción (7) en la Tabla 1. Además, los agentes no deben estar utilizando recursos mientras están inactivos. Esta restricción también puede ser impuesta a través de una desigualdad lineal en θ y Δ, como se muestra en (8). La restricción (6) establece el valor de θ para que coincida con la política definida por la medida de ocupación xm. De manera similar, debemos asegurarnos de que las variables de uso de recursos Δ estén también sincronizadas con la medida de ocupación xm. Esto se realiza a través de la restricción (9) en la Tabla 1, que es casi idéntica a la restricción análoga de [6]. Después de implementar la restricción anterior, que refuerza el significado de Δ, agregamos una restricción que garantiza que el uso de recursos de los agentes nunca exceda los montos de recursos disponibles. Esta condición también se expresa trivialmente como una desigualdad lineal (10) en la Tabla 1. Finalmente, para la formulación del problema en la que las asignaciones de recursos son estáticas durante la vida de un agente, agregamos una restricción que garantiza que las variables de uso de recursos Δ no cambien su valor mientras el agente está activo (θ = 1). Esto se logra a través de la restricción lineal (11), donde Z ≥ 2 es una constante que se utiliza para desactivar las restricciones cuando θm(τ) = 0 o θm(τ + 1) = 0. Esta restricción no se utiliza para la formulación del problema dinámico, donde los recursos pueden ser reasignados entre agentes en cada paso de tiempo. En resumen, la Tabla 1 junto con las restricciones de conservación de flujo de (12) define el MILP que calcula simultáneamente una asignación óptima de recursos para todos los agentes a lo largo del tiempo, así como políticas óptimas de MDP de horizonte finito que son válidas bajo esa asignación de recursos. Como una medida aproximada de la complejidad de este MILP, consideremos el número de variables de optimización y restricciones. Sea TM = P Tm = P m(τa m − τd m + 1) la suma de las longitudes de las ventanas de llegada-salida a través de todos los agentes. Entonces, el número de variables de optimización es: TM + bτ|M||Ω| + bτ|M|, de las cuales TM son continuas (xm), y bτ|M||Ω| + bτ|M| son binarias (Δ y θ). Sin embargo, hay que tener en cuenta que todos menos TM|M| de los θ se establecen en cero por la restricción (7), lo que también obliga inmediatamente a que todos menos TM|M||Ω| de los Δ sean cero a través de las restricciones (8). El número de restricciones (sin incluir las restricciones degeneradas en (7)) en el MILP es: TM + TM|Ω| + bτ|Ω| + bτ|M||Ω|. A pesar de que la complejidad del MILP es, en el peor de los casos, exponencial en el número de variables binarias, la complejidad de este MILP es significativamente (exponencialmente) menor que la del MILP con funciones de utilidad planas, descrito en la Sección 2.2. Este resultado refleja las ganancias de eficiencia reportadas en [6] para problemas de asignación de recursos de un solo disparo, pero es mucho más pronunciado, debido a la explosión de la representación de utilidad plana debido al aspecto temporal del problema (recuerde la complejidad prohibitiva de la optimización combinatoria en la Sección 2.2). Analizamos empíricamente el rendimiento de este método en la Sección 5.1. Estrictamente hablando, resolver MILPs hasta la optimalidad es NP-completo en el número de variables enteras. RESULTADOS EXPERIMENTALES Aunque la complejidad de resolver MILPs es en el peor de los casos exponencial en el número de variables enteras, existen muchos métodos eficientes para resolver MILPs que permiten que nuestro algoritmo se adapte bien a parámetros comunes en problemas de asignación de recursos y programación. En particular, esta sección introduce un dominio de problema: el problema del taller de reparación, que se utiliza para evaluar empíricamente la escalabilidad de nuestros algoritmos en términos del número de agentes |M|, el número de recursos compartidos |Ω| y las diferentes longitudes de tiempo global bτ durante las cuales los agentes pueden entrar y salir del sistema. El problema del taller de reparaciones es un MDP parametrizado simple que adopta la metáfora de un taller de reparaciones de vehículos. Los agentes en el taller de reparación son mecánicos con una serie de tareas independientes que solo generan recompensa cuando se completan. En nuestro modelo MDP de este sistema, las acciones tomadas para avanzar a través del espacio de estados solo están permitidas si el agente posee ciertos recursos que están públicamente disponibles para la tienda. Estos recursos son de suministro finito, y las políticas óptimas para la tienda determinarán cuándo cada agente puede retener los recursos limitados para tomar acciones y obtener recompensas individuales. Cada tarea a completar está asociada con una sola acción, aunque se requiere que el agente repita la acción numerosas veces antes de completar la tarea y obtener una recompensa. Este modelo fue parametrizado en términos del número de agentes en el sistema, el número de diferentes tipos de recursos que podrían estar vinculados a acciones necesarias, un tiempo global durante el cual se permite a los agentes llegar y partir, y una longitud máxima para el número de pasos de tiempo que un agente puede permanecer en el sistema. Todos los puntos de datos en nuestros experimentos se obtuvieron con 20 evaluaciones utilizando CPLEX para resolver los MILPs en una computadora Pentium4 con 2Gb de RAM. Se realizaron pruebas tanto en la versión estática como en la dinámica del problema de programación de recursos, tal como se definió anteriormente. La Figura 3 muestra el tiempo de ejecución y el valor de la política para modificaciones independientes en el conjunto de parámetros. La fila superior muestra cómo el tiempo de solución para el MILP escala a medida que aumentamos el número de agentes |M|, el horizonte de tiempo global bτ y el número de recursos |Ω|. Aumentar el número de agentes conduce a una escalabilidad exponencial en complejidad, lo cual es de esperarse para un problema NP-completo. Sin embargo, aumentar el límite de tiempo global bτ o el número total de tipos de recursos |Ω|, manteniendo constante el número de agentes, no conduce a una disminución en el rendimiento. Esto ocurre porque los problemas se vuelven más fáciles a medida que se vuelven sub-determinados, lo cual también es un fenómeno común en problemas NP-completos. También observamos que la solución a la versión dinámica del problema a menudo se puede calcular mucho más rápido que la versión estática. La fila inferior de la Figura 3 muestra el valor de la política conjunta de las políticas que corresponden a los horarios óptimos de asignación de recursos calculados. Podemos observar que la versión dinámica produce una recompensa más alta (como era de esperar, ya que la recompensa de la versión dinámica siempre es igual o mayor que la recompensa de la versión estática). Deberíamos señalar que estos gráficos no deben ser vistos como una medida del rendimiento de dos algoritmos diferentes (ambos algoritmos producen soluciones óptimas pero para problemas distintos), sino más bien como observaciones sobre cómo cambia la calidad de las soluciones óptimas a medida que se permite más flexibilidad en la reasignación de recursos. La Figura 4 muestra el tiempo de ejecución y el valor de la política para las pruebas en las que las variables de entrada comunes se escalan juntas. Esto permite la Sexta Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 1225 2 4 6 8 10 10 −3 10 −2 10 −1 10 0 10 1 10 2 10 3 10 4 Número de Agentes |M| Tiempo de CPU, seg |Ω| = 5, τ = 50 estático dinámico 50 100 150 200 10 −2 10 −1 10 0 10 1 10 2 10 3 Límite de Tiempo Global τ Tiempo de CPU, seg |M| = 5, |Ω| = 5 estático dinámico 10 20 30 40 50 10 −2 10 −1 10 0 10 1 10 2 Número de Recursos |Ω| Tiempo de CPU, seg |M| = 5, τ = 50 estático dinámico 2 4 6 8 10 200 400 600 800 1000 1200 1400 1600 Número de Agentes |M| Valor |Ω| = 5, τ = 50 estático dinámico 50 100 150 200 400 500 600 700 800 900 1000 1100 1200 1300 1400 Límite de Tiempo Global τ Valor |M| = 5, |Ω| = 5 estático dinámico 10 20 30 40 50 500 600 700 800 900 1000 1100 1200 1300 1400 Número de Recursos |Ω| Valor |M| = 5, τ = 50 estático dinámico Figura 3: Evaluación de nuestro MILP para números variables de agentes (columna 1), longitudes de ventana de tiempo global (columna 2) y números de tipos de recursos (columna 3). La fila superior muestra el tiempo de CPU, y la fila inferior muestra la recompensa conjunta de las políticas MDP de los agentes. Las barras de error muestran los cuartiles 1ro y 3ro (25% y 75%). 2 4 6 8 10 10 −3 10 −2 10 −1 10 0 10 1 10 2 10 3 Número de Agentes |M| Tiempo de CPU, seg τ = 10|M| estático dinámico 2 4 6 8 10 10 −3 10 −2 10 −1 10 0 10 1 10 2 10 3 10 4 Número de Agentes |M| Tiempo de CPU, seg |Ω| = 2|M| estático dinámico 2 4 6 8 10 10 −3 10 −2 10 −1 10 0 10 1 10 2 10 3 10 4 Número de Agentes |M| Tiempo de CPU, seg |Ω| = 5|M| estático dinámico 2 4 6 8 10 200 400 600 800 1000 1200 1400 1600 1800 2000 2200 Número de Agentes |M| Valor τ = 10|M| estático dinámico 2 4 6 8 10 200 400 600 800 1000 1200 1400 1600 1800 2000 Número de Agentes |M| Valor |Ω| = 2|M| estático dinámico 2 4 6 8 10 0 500 1000 1500 2000 2500 Número de Agentes |M| Valor |Ω| = 5|M| estático dinámico Figura 4: Evaluación de nuestro MILP utilizando variables de entrada correlacionadas. La columna izquierda sigue el rendimiento y el tiempo de CPU a medida que aumenta el número de agentes y la ventana de tiempo global juntos (bτ = 10|M|). La columna del medio y la columna de la derecha siguen el rendimiento y el tiempo de CPU a medida que aumenta el número de recursos y el número de agentes juntos, con |Ω| = 2|M| y |Ω| = 5|M|, respectivamente. Las barras de error muestran el primer y tercer cuartil (25% y 75%). 1226 The Sixth Intl. La Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) nos permite explorar dominios donde el número total de agentes escala proporcionalmente al número total de tipos de recursos o al horizonte temporal global, manteniendo constante la densidad promedio de agentes (por unidad de tiempo global) o el número promedio de recursos por agente (lo cual comúnmente ocurre en aplicaciones de la vida real). En general, creemos que estos resultados experimentales indican que nuestra formulación MILP puede ser utilizada para resolver de manera efectiva problemas de programación de recursos de tamaño no trivial. DISCUSIÓN Y CONCLUSIONES A lo largo del documento, hemos realizado una serie de suposiciones en nuestro modelo y algoritmo de solución; discutimos sus implicaciones a continuación. • Ejecución continua. Suponemos que una vez que un agente deja de ejecutar su MDP (transiciona al estado sf), sale del sistema y no puede regresar. Es fácil relajar esta suposición para dominios donde los agentes de MDP pueden ser pausados y reiniciados. Todo lo que se requiere es incluir una acción de pausa adicional que haga la transición desde un estado dado de regreso a sí mismo, y que tenga una recompensa de cero. • Indiferencia al tiempo de inicio. Utilizamos un modelo de recompensa donde las recompensas de los agentes dependen solo del horizonte temporal de sus MDP y no del tiempo de inicio global. Esta es una consecuencia de nuestro procedimiento de aumento de MDP de la Sección 4.1. Es fácil extender el modelo para que los agentes incurran en una penalización explícita por inactividad al asignar una recompensa negativa no nula al estado inicial sb. • Requisitos de recursos binarios. Para simplificar, hemos asumido que los costos de recursos son binarios: ϕm(a, ω) = {0, 1}, pero nuestros resultados se generalizan de manera directa a asignaciones de recursos no binarias, de manera análoga al procedimiento utilizado en [5]. • Agentes cooperativos. El procedimiento de optimización discutido en este artículo fue desarrollado en el contexto de agentes cooperativos, pero también puede ser utilizado para diseñar un mecanismo para programar recursos entre agentes egoístas. Este procedimiento de optimización puede ser incorporado en una subasta de Vickrey-Clarke-Groves, de manera completamente análoga a como se hizo en [7]. De hecho, todos los resultados de [7] sobre las propiedades de la subasta y la privacidad de la información se aplican directamente al dominio de programación discutido en este documento, requiriendo solo ligeras modificaciones para tratar con MDPs de horizonte finito. • Tiempos de llegada y salida conocidos y deterministas. Finalmente, hemos asumido que los tiempos de llegada y salida de los agentes (τa m y τd m) son deterministas y conocidos de antemano. Esta suposición es fundamental para nuestro método de solución. Si bien hay muchos dominios donde esta suposición es válida, en muchos casos los agentes llegan y se van de forma dinámica y sus tiempos de llegada y salida solo pueden predecirse de manera probabilística, lo que lleva a problemas de asignación de recursos en línea. En particular, en el caso de agentes con interés propio, esto se convierte en una versión interesante de un problema de diseño de mecanismos en línea [11, 12]. En resumen, hemos presentado una formulación MILP para el problema combinatorio de programación de recursos donde los valores de los agentes para posibles asignaciones de recursos están definidos por MDPs de horizonte finito. Este resultado amplía el trabajo previo ([6, 7]) sobre asignación estática de recursos de una sola vez bajo preferencias inducidas por MDP a problemas de programación de recursos con un aspecto temporal. Por lo tanto, este trabajo da un paso en la dirección de diseñar un mecanismo en línea para agentes con preferencias de recursos combinatorios inducidas por problemas de planificación estocástica. Relajar la suposición sobre los tiempos deterministas de llegada y salida de los agentes es un enfoque de nuestro trabajo futuro. Nos gustaría agradecer a los revisores anónimos por sus comentarios y sugerencias perspicaces. 7. REFERENCIAS [1] E. Altman y A. Shwartz. Control adaptativo de cadenas de Markov restringidas: Criterios y políticas. Anales de Investigación de Operaciones, número especial sobre Procesos de Decisión de Markov, 28:101-134, 1991. [2] R. Bellman. Programación dinámica. Princeton University Press, 1957. [3] C. Boutilier. \n\nPrensa de la Universidad de Princeton, 1957. [3] C. Boutilier. Resolviendo problemas de subasta combinatoria expresados de manera concisa. En Proc. de AAAI-02, páginas 359-366, 2002. [4] C. Boutilier y H. H. Hoos. Subastando idiomas para subastas combinatorias. En Proc. de IJCAI-01, páginas 1211-1217, 2001. [5] D. Dolgov. Asignación integrada de recursos y planificación en entornos estocásticos de múltiples agentes. Tesis doctoral, Departamento de Ciencias de la Computación, Universidad de Michigan, febrero de 2006. [6] D. A. Dolgov y E. H. Durfee. Asignación óptima de recursos y formulación de políticas en procesos de decisión de Markov débilmente acoplados. En Proc. de ICAPS-04, páginas 315-324, junio de 2004. [7] D. A. Dolgov y E. H. Durfee. Subastas combinatorias computacionalmente eficientes para asignación de recursos en MDPs débilmente acoplados. En Proc. de AAMAS-05, Nueva York, NY, EE. UU., 2005. ACM Press. [8] D. A. Dolgov y E. H. Durfee. Asignación de recursos entre agentes con preferencias inducidas por MDP factorizados. En Proc. de AAMAS-06, 2006. [9] K. Larson y T. Sandholm. Diseño de mecanismos y agentes deliberativos. En Proc. de AAMAS-05, páginas 650-656, Nueva York, NY, EE. UU., 2005. ACM Press. [10] N. Nisan. \n\nACM Press. [10] N. Nisan. Subasta y asignación en subastas combinatorias. En Comercio Electrónico, 2000. [11] D. C. Parkes y S. Singh. Un enfoque basado en MDP para el Diseño de Mecanismos en Línea. En Proc. de la Decimoséptima Conferencia Anual sobre Sistemas de Procesamiento de Información Neural (NIPS-03), 2003. [12] D. C. Parkes, S. Singh y D. Yanovsky. Diseño de mecanismos en línea aproximadamente eficiente. En Actas de la Decimoctava Conferencia Anual sobre Sistemas de Procesamiento de Información Neural (NIPS-04), 2004. [13] M. L. Puterman. Procesos de Decisión de Markov. John Wiley & Sons, Nueva York, 1994. [14] M. H. Rothkopf, A. Pekec y R. M. Harstad. Subastas combinatorias manejables computacionalmente. Ciencia de la Gestión, 44(8):1131-1147, 1998. [15] T. Sandholm. Un algoritmo para la determinación óptima del ganador en subastas combinatorias. En Proc. de IJCAI-99, páginas 542-547, San Francisco, CA, EE. UU., 1999. Morgan Kaufmann Publishers Inc. - Editores Morgan Kaufmann Inc. El Sexto Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 1227 ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "resource allocation": {
            "translated_key": "",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Combinatorial Resource Scheduling for Multiagent MDPs Dmitri A. Dolgov, Michael R. James, and Michael E. Samples AI and Robotics Group Technical Research, Toyota Technical Center USA {ddolgov, michael.r.james, michael.samples}@gmail.com ABSTRACT Optimal resource scheduling in multiagent systems is a computationally challenging task, particularly when the values of resources are not additive.",
                "We consider the combinatorial problem of scheduling the usage of multiple resources among agents that operate in stochastic environments, modeled as Markov decision processes (MDPs).",
                "In recent years, efficient resource-allocation algorithms have been developed for agents with resource values induced by MDPs.",
                "However, this prior work has focused on static resource-allocation problems where resources are distributed once and then utilized in infinite-horizon MDPs.",
                "We extend those existing models to the problem of combinatorial resource scheduling, where agents persist only for finite periods between their (predefined) arrival and departure times, requiring resources only for those time periods.",
                "We provide a computationally efficient procedure for computing globally optimal resource assignments to agents over time.",
                "We illustrate and empirically analyze the method in the context of a stochastic jobscheduling domain.",
                "Categories and Subject Descriptors I.2.8 [Artificial Intelligence]: Problem Solving, Control Methods, and Search; I.2.11 [Artificial Intelligence]: Distributed Artificial Intelligence-Multiagent systems General Terms Algorithms, Performance, Design 1.",
                "INTRODUCTION The tasks of optimal <br>resource allocation</br> and scheduling are ubiquitous in multiagent systems, but solving such optimization problems can be computationally difficult, due to a number of factors.",
                "In particular, when the value of a set of resources to an agent is not additive (as is often the case with resources that are substitutes or complements), the utility function might have to be defined on an exponentially large space of resource bundles, which very quickly becomes computationally intractable.",
                "Further, even when each agent has a utility function that is nonzero only on a small subset of the possible resource bundles, obtaining optimal allocation is still computationally prohibitive, as the problem becomes NP-complete [14].",
                "Such computational issues have recently spawned several threads of work in using compact models of agents preferences.",
                "One idea is to use any structure present in utility functions to represent them compactly, via, for example, logical formulas [15, 10, 4, 3].",
                "An alternative is to directly model the mechanisms that define the agents utility functions and perform <br>resource allocation</br> directly with these models [9].",
                "A way of accomplishing this is to model the processes by which an agent might utilize the resources and define the utility function as the payoff of these processes.",
                "In particular, if an agent uses resources to act in a stochastic environment, its utility function can be naturally modeled with a Markov decision process, whose action set is parameterized by the available resources.",
                "This representation can then be used to construct very efficient resource-allocation algorithms that lead to an exponential speedup over a straightforward optimization problem with flat representations of combinatorial preferences [6, 7, 8].",
                "However, this existing work on <br>resource allocation</br> with preferences induced by resource-parameterized MDPs makes an assumption that the resources are only allocated once and are then utilized by the agents independently within their infinite-horizon MDPs.",
                "This assumption that no reallocation of resources is possible can be limiting in domains where agents arrive and depart dynamically.",
                "In this paper, we extend the work on <br>resource allocation</br> under MDP-induced preferences to discrete-time scheduling problems, where agents are present in the system for finite time intervals and can only use resources within these intervals.",
                "In particular, agents arrive and depart at arbitrary (predefined) times and within these intervals use resources to execute tasks in finite-horizon MDPs.",
                "We address the problem of globally optimal resource scheduling, where the objective is to find an allocation of resources to the agents across time that maximizes the sum of the expected rewards that they obtain.",
                "In this context, our main contribution is a mixed-integerprogramming formulation of the scheduling problem that chooses globally optimal resource assignments, starting times, and execution horizons for all agents (within their arrival1220 978-81-904262-7-5 (RPS) c 2007 IFAAMAS departure intervals).",
                "We analyze and empirically compare two flavors of the scheduling problem: one, where agents have static resource assignments within their finite-horizon MDPs, and another, where resources can be dynamically reallocated between agents at every time step.",
                "In the rest of the paper, we first lay down the necessary groundwork in Section 2 and then introduce our model and formal problem statement in Section 3.",
                "In Section 4.2, we describe our main result, the optimization program for globally optimal resource scheduling.",
                "Following the discussion of our experimental results on a job-scheduling problem in Section 5, we conclude in Section 6 with a discussion of possible extensions and generalizations of our method. 2.",
                "BACKGROUND Similarly to the model used in previous work on resourceallocation with MDP-induced preferences [6, 7], we define the value of a set of resources to an agent as the value of the best MDP policy that is realizable, given those resources.",
                "However, since the focus of our work is on scheduling problems, and a large part of the optimization problem is to decide how resources are allocated in time among agents with finite arrival and departure times, we model the agents planning problems as finite-horizon MDPs, in contrast to previous work that used infinite-horizon discounted MDPs.",
                "In the rest of this section, we first introduce some necessary background on finite-horizon MDPs and present a linear-programming formulation that serves as the basis for our solution algorithm developed in Section 4.",
                "We also outline the standard methods for combinatorial resource scheduling with flat resource values, which serve as a comparison benchmark for the new model developed here. 2.1 Markov Decision Processes A stationary, finite-domain, discrete-time MDP (see, for example, [13] for a thorough and detailed development) can be described as S, A, p, r , where: S is a finite set of system states; A is a finite set of actions that are available to the agent; p is a stationary stochastic transition function, where p(σ|s, a) is the probability of transitioning to state σ upon executing action a in state s; r is a stationary reward function, where r(s, a) specifies the reward obtained upon executing action a in state s. Given such an MDP, a decision problem under a finite horizon T is to choose an optimal action at every time step to maximize the expected value of the total reward accrued during the agents (finite) lifetime.",
                "The agents optimal policy is then a function of current state s and the time until the horizon.",
                "An optimal policy for such a problem is to act greedily with respect to the optimal value function, defined recursively by the following system of finite-time Bellman equations [2]: v(s, t) = max a r(s, a) + X σ p(σ|s, a)v(σ, t + 1), ∀s ∈ S, t ∈ [1, T − 1]; v(s, T) = 0, ∀s ∈ S; where v(s, t) is the optimal value of being in state s at time t ∈ [1, T].",
                "This optimal value function can be easily computed using dynamic programming, leading to the following optimal policy π, where π(s, a, t) is the probability of executing action a in state s at time t: π(s, a, t) = ( 1, a = argmaxa r(s, a) + P σ p(σ|s, a)v(σ, t + 1), 0, otherwise.",
                "The above is the most common way of computing the optimal value function (and therefore an optimal policy) for a finite-horizon MDP.",
                "However, we can also formulate the problem as the following linear program (similarly to the dual LP for infinite-horizon discounted MDPs [13, 6, 7]): max X s X a r(s, a) X t x(s, a, t) subject to: X a x(σ, a, t + 1) = X s,a p(σ|s, a)x(s, a, t) ∀σ, t ∈ [1, T − 1]; X a x(s, a, 1) = α(s), ∀s ∈ S; (1) where α(s) is the initial distribution over the state space, and x is the (non-stationary) occupation measure (x(s, a, t) ∈ [0, 1] is the total expected number of times action a is executed in state s at time t).",
                "An optimal (non-stationary) policy is obtained from the occupation measure as follows: π(s, a, t) = x(s, a, t)/ X a x(s, a, t) ∀s ∈ S, t ∈ [1, T]. (2) Note that the standard unconstrained finite-horizon MDP, as described above, always has a uniformly-optimal solution (optimal for any initial distribution α(s)).",
                "Therefore, an optimal policy can be obtained by using an arbitrary constant α(s) > 0 (in particular, α(s) = 1 will result in x(s, a, t) = π(s, a, t)).",
                "However, for MDPs with resource constraints (as defined below in Section 3), uniformly-optimal policies do not in general exist.",
                "In such cases, α becomes a part of the problem input, and a resulting policy is only optimal for that particular α.",
                "This result is well known for infinite-horizon MDPs with various types of constraints [1, 6], and it also holds for our finite-horizon model, which can be easily established via a line of reasoning completely analogous to the arguments in [6]. 2.2 Combinatorial Resource Scheduling A straightforward approach to resource scheduling for a set of agents M, whose values for the resources are induced by stochastic planning problems (in our case, finite-horizon MDPs) would be to have each agent enumerate all possible resource assignments over time and, for each one, compute its value by solving the corresponding MDP.",
                "Then, each agent would provide valuations for each possible resource bundle over time to a centralized coordinator, who would compute the optimal resource assignments across time based on these valuations.",
                "When resources can be allocated at different times to different agents, each agent must submit valuations for every combination of possible time horizons.",
                "Let each agent m ∈ M execute its MDP within the arrival-departure time interval τ ∈ [τa m, τd m].",
                "Hence, agent m will execute an MDP with time horizon no greater than Tm = τd m−τa m+1.",
                "Let bτ be the global time horizon for the problem, before which all of the agents MDPs must finish.",
                "We assume τd m < bτ, ∀m ∈ M. The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1221 For the scheduling problem where agents have static resource requirements within their finite-horizon MDPs, the agents provide a valuation for each resource bundle for each possible time horizon (from [1, Tm]) that they may use.",
                "Let Ω be the set of resources to be allocated among the agents.",
                "An agent will get at most one resource bundle for one of the time horizons.",
                "Let the variable ψ ∈ Ψm enumerate all possible pairs of resource bundles and time horizons for agent m, so there are 2|Ω| × Tm values for ψ (the space of bundles is exponential in the number of resource types |Ω|).",
                "The agent m must provide a value vψ m for each ψ, and the coordinator will allocate at most one ψ (resource, time horizon) pair to each agent.",
                "This allocation is expressed as an indicator variable zψ m ∈ {0, 1} that shows whether ψ is assigned to agent m. For time τ and resource ω, the function nm(ψ, τ, ω) ∈ {0, 1} indicates whether the bundle in ψ uses resource ω at time τ (we make the assumption that agents have binary resource requirements).",
                "This allocation problem is NP-complete, even when considering only a single time step, and its difficulty increases significantly with multiple time steps because of the increasing number of values of ψ.",
                "The problem of finding an optimal allocation that satisfies the global constraint that the amount of each resource ω allocated to all agents does not exceed the available amount bϕ(ω) can be expressed as the following integer program: max X m∈M X ψ∈Ψm zψ mvψ m subject to: X ψ∈Ψm zψ m ≤ 1, ∀m ∈ M; X m∈M X ψ∈Ψm zψ mnm(ψ, τ, ω) ≤ bϕ(ω), ∀τ ∈ [1, bτ], ∀ω ∈ Ω; (3) The first constraint in equation 3 says that no agent can receive more than one bundle, and the second constraint ensures that the total assignment of resource ω does not, at any time, exceed the resource bound.",
                "For the scheduling problem where the agents are able to dynamically reallocate resources, each agent must specify a value for every combination of bundles and time steps within its time horizon.",
                "Let the variable ψ ∈ Ψm in this case enumerate all possible resource bundles for which at most one bundle may be assigned to agent m at each time step.",
                "Therefore, in this case there are P t∈[1,Tm](2|Ω| )t ∼ 2|Ω|Tm possibilities of resource bundles assigned to different time slots, for the Tm different time horizons.",
                "The same set of equations (3) can be used to solve this dynamic scheduling problem, but the integer program is different because of the difference in how ψ is defined.",
                "In this case, the number of ψ values is exponential in each agents planning horizon Tm, resulting in a much larger program.",
                "This straightforward approach to solving both of these scheduling problems requires an enumeration and solution of either 2|Ω| Tm (static allocation) or P t∈[1,Tm] 2|Ω|t (dynamic reallocation) MDPs for each agent, which very quickly becomes intractable with the growth of the number of resources |Ω| or the time horizon Tm. 3.",
                "MODEL AND PROBLEM STATEMENT We now formally introduce our model of the resourcescheduling problem.",
                "The problem input consists of the following components: • M, Ω, bϕ, τa m, τd m, bτ are as defined above in Section 2.2. • {Θm} = {S, A, pm, rm, αm} are the MDPs of all agents m ∈ M. Without loss of generality, we assume that state and action spaces of all agents are the same, but each has its own transition function pm, reward function rm, and initial conditions αm. • ϕm : A×Ω → {0, 1} is the mapping of actions to resources for agent m. ϕm(a, ω) indicates whether action a of agent m needs resource ω.",
                "An agent m that receives a set of resources that does not include resource ω cannot execute in its MDP policy any action a for which ϕm(a, ω) = 0.",
                "We assume all resource requirements are binary; as discussed below in Section 6, this assumption is not limiting.",
                "Given the above input, the optimization problem we consider is to find the globally optimal-maximizing the sum of expected rewards-mapping of resources to agents for all time steps: Δ : τ × M × Ω → {0, 1}.",
                "A solution is feasible if the corresponding assignment of resources to the agents does not violate the global resource constraint: X m Δm(τ, ω) ≤ bϕ(ω), ∀ω ∈ Ω, τ ∈ [1, bτ]. (4) We consider two flavors of the resource-scheduling problem.",
                "The first formulation restricts resource assignments to the space where the allocation of resources to each agent is static during the agents lifetime.",
                "The second formulation allows reassignment of resources between agents at every time step within their lifetimes.",
                "Figure 1 depicts a resource-scheduling problem with three agents M = {m1, m2, m3}, three resources Ω = {ω1, ω2, ω3}, and a global problem horizon of bτ = 11.",
                "The agents arrival and departure times are shown as gray boxes and are {1, 6}, {3, 7}, and {2, 11}, respectively.",
                "A solution to this problem is shown via horizontal bars within each agents box, where the bars correspond to the allocation of the three resource types.",
                "Figure 1a shows a solution to a static scheduling problem.",
                "According to the shown solution, agent m1 begins the execution of its MDP at time τ = 1 and has a lock on all three resources until it finishes execution at time τ = 3.",
                "Note that agent m1 relinquishes its hold on the resources before its announced departure time of τd m1 = 6, ostensibly because other agents can utilize the resources more effectively.",
                "Thus, at time τ = 4, resources ω1 and ω3 are allocated to agent m2, who then uses them to execute its MDP (using only actions supported by resources ω1 and ω3) until time τ = 7.",
                "Agent m3 holds resource ω3 during the interval τ ∈ [4, 10].",
                "Figure 1b shows a possible solution to the dynamic version of the same problem.",
                "There, resources can be reallocated between agents at every time step.",
                "For example, agent m1 gives up its use of resource ω2 at time τ = 2, although it continues the execution of its MDP until time τ = 6.",
                "Notice that an agent is not allowed to stop and restart its MDP, so agent m1 is only able to continue executing in the interval τ ∈ [3, 4] if it has actions that do not require any resources (ϕm(a, ω) = 0).",
                "Clearly, the model and problem statement described above make a number of assumptions about the problem and the desired solution properties.",
                "We discuss some of those assumptions and their implications in Section 6. 1222 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) (a) (b) Figure 1: Illustration of a solution to a resource-scheduling problem with three agents and three resources: a) static resource assignments (resource assignments are constant within agents lifetimes; b) dynamic assignment (resource assignments are allowed to change at every time step). 4.",
                "RESOURCE SCHEDULING Our resource-scheduling algorithm proceeds in two stages.",
                "First, we perform a preprocessing step that augments the agent MDPs; this process is described in Section 4.1.",
                "Second, using these augmented MDPs we construct a global optimization problem, which is described in Section 4.2. 4.1 Augmenting Agents MDPs In the model described in the previous section, we assume that if an agent does not possess the necessary resources to perform actions in its MDP, its execution is halted and the agent leaves the system.",
                "In other words, the MDPs cannot be paused and resumed.",
                "For example, in the problem shown in Figure 1a, agent m1 releases all resources after time τ = 3, at which point the execution of its MDP is halted.",
                "Similarly, agents m2 and m3 only execute their MDPs in the intervals τ ∈ [4, 6] and τ ∈ [4, 10], respectively.",
                "Therefore, an important part of the global decision-making problem is to decide the window of time during which each of the agents is active (i.e., executing its MDP).",
                "To accomplish this, we augment each agents MDP with two new states (start and finish states sb , sf , respectively) and a new start/stop action a∗ , as illustrated in Figure 2.",
                "The idea is that an agent stays in the start state sb until it is ready to execute its MDP, at which point it performs the start/stop action a∗ and transitions into the state space of the original MDP with the transition probability that corresponds to the original initial distribution α(s).",
                "For example, in Figure 1a, for agent m2 this would happen at time τ = 4.",
                "Once the agent gets to the end of its activity window (time τ = 6 for agent m2 in Figure 1a), it performs the start/stop action, which takes it into the sink finish state sf at time τ = 7.",
                "More precisely, given an MDP S, A, pm, rm, αm , we define an augmented MDP S , A , pm, rm, αm as follows: S = S ∪ sb ∪ sf ; A = A ∪ a∗ ; p (s|sb , a∗ ) = α(s), ∀s ∈ S; p (sb |sb , a) = 1.0, ∀a ∈ A; p (sf |s, a∗ ) = 1.0, ∀s ∈ S; p (σ|s, a) = p(σ|s, a), ∀s, σ ∈ S, a ∈ A; r (sb , a) = r (sf , a) = 0, ∀a ∈ A ; r (s, a) = r(s, a), ∀s ∈ S, a ∈ A; α (sb ) = 1; α (s) = 0, ∀s ∈ S; where all non-specified transition probabilities are assumed to be zero.",
                "Further, in order to account for the new starting state, we begin the MDP one time-step earlier, setting τa m ← τa m − 1.",
                "This will not affect the <br>resource allocation</br> due to the resource constraints only being enforced for the original MDP states, as will be discussed in the next section.",
                "For example, the augmented MDPs shown in Figure 2b (which starts in state sb at time τ = 2) would be constructed from an MDP with original arrival time τ = 3.",
                "Figure 2b also shows a sample trajectory through the state space: the agent starts in state sb , transitions into the state space S of the original MDP, and finally exists into the sink state sf .",
                "Note that if we wanted to model a problem where agents could pause their MDPs at arbitrary time steps (which might be useful for domains where dynamic reallocation is possible), we could easily accomplish this by including an extra action that transitions from each state to itself with zero reward. 4.2 MILP for Resource Scheduling Given a set of augmented MDPs, as defined above, the goal of this section is to formulate a global optimization program that solves the resource-scheduling problem.",
                "In this section and below, all MDPs are assumed to be the augmented MDPs as defined in Section 4.1.",
                "Our approach is similar to the idea used in [6]: we begin with the linear-program formulation of agents MDPs (1) and augment it with constraints that ensure that the corresponding <br>resource allocation</br> across agents and time is valid.",
                "The resulting optimization problem then simultaneously solves the agents MDPs and resource-scheduling problems.",
                "In the rest of this section, we incrementally develop a mixed integer program (MILP) that achieves this.",
                "In the absence of resource constraints, the agents finitehorizon MDPs are completely independent, and the globally optimal solution can be trivially obtained via the following LP, which is simply an aggregation of single-agent finitehorizon LPs: max X m X s X a rm(s, a) X t xm(s, a, t) subject to: X a xm(σ, a, t + 1) = X s,a pm(σ|s, a)xm(s, a, t), ∀m ∈ M, σ ∈ S, t ∈ [1, Tm − 1]; X a xm(s, a, 1) = αm(s), ∀m ∈ M, s ∈ S; (12) where xm(s, a, t) is the occupation measure of agent m, and The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1223 (a) (b) Figure 2: Illustration of augmenting an MDP to allow for variable starting and stopping times: a) (left) the original two-state MDP with a single action; (right) the augmented MDP with new states sb and sf and the new action a∗ (note that the origianl transitions are not changed in the augmentation process); b) the augmented MDP displayed as a trajectory through time (grey lines indicate all transitions, while black lines indicate a given trajectory.",
                "Objective Function (sum of expected rewards over all agents) max X m X s X a rm(s, a) X t xm(s, a, t) (5) Meaning Implication Linear Constraints Tie x to θ.",
                "Agent is only active when occupation measure is nonzero in original MDP states. θm(τ) = 0 =⇒ xm(s, a, τ −τa m+1) = 0 ∀s /∈ {sb , sf }, a ∈ A X s/∈{sb,sf } X a xm(s, a, t) ≤ θm(τa m + t − 1) ∀m ∈ M, ∀t ∈ [1, Tm] (6) Agent can only be active in τ ∈ (τa m, τd m) θm(τ) = 0 ∀m ∈ M, τ /∈ (τa m, τd m) (7) Cannot use resources when not active θm(τ) = 0 =⇒ Δm(τ, ω) = 0 ∀τ ∈ [0, bτ], ω ∈ Ω Δm(τ, ω) ≤ θm(τ) ∀m ∈ M, τ ∈ [0, bτ], ω ∈ Ω (8) Tie x to Δ (nonzero x forces corresponding Δ to be nonzero.)",
                "Δm(τ, ω) = 0, ϕm(a, ω) = 1 =⇒ xm(s, a, τ − τa m + 1) = 0 ∀s /∈ {sb , sf } 1/|A| X a ϕm(a, ω) X s/∈{sb,sf } xm(s, a, t) ≤ Δm(t + τa m − 1, ω) ∀m ∈ M, ω ∈ Ω, t ∈ [1, Tm] (9) Resource bounds X m Δm(τ, ω) ≤ bϕ(ω) ∀ω ∈ Ω, τ ∈ [0, bτ] (10) Agent cannot change resources while active.",
                "Only enabled for scheduling with static assignments. θm(τ) = 1 and θm(τ + 1) = 1 =⇒ Δm(τ, ω) = Δm(τ + 1, ω) Δm(τ, ω) − Z(1 − θm(τ + 1)) ≤ Δm(τ + 1, ω) + Z(1 − θm(τ)) Δm(τ, ω) + Z(1 − θm(τ + 1)) ≥ Δm(τ + 1, ω) − Z(1 − θm(τ)) ∀m ∈ M, ω ∈ Ω, τ ∈ [0, bτ] (11) Table 1: MILP for globally optimal resource scheduling.",
                "Tm = τd m − τa m + 1 is the time horizon for the agents MDP.",
                "Using this LP as a basis, we augment it with constraints that ensure that the resource usage implied by the agents occupation measures {xm} does not violate the global resource requirements bϕ at any time step τ ∈ [0, bτ].",
                "To formulate these resource constraints, we use the following binary variables: • Δm(τ, ω) = {0, 1}, ∀m ∈ M, τ ∈ [0, bτ], ω ∈ Ω, which serve as indicator variables that define whether agent m possesses resource ω at time τ.",
                "These are analogous to the static indicator variables used in the one-shot static resource-allocation problem in [6]. • θm = {0, 1}, ∀m ∈ M, τ ∈ [0, bτ] are indicator variables that specify whether agent m is active (i.e., executing its MDP) at time τ.",
                "The meaning of resource-usage variables Δ is illustrated in Figure 1: Δm(τ, ω) = 1 only if resource ω is allocated to agent m at time τ.",
                "The meaning of the activity indicators θ is illustrated in Figure 2b: when agent m is in either the start state sb or the finish state sf , the corresponding θm = 0, but once the agent becomes active and enters one of the other states, we set θm = 1 .",
                "This meaning of θ can be enforced with a linear constraint that synchronizes the values of the agents occupation measures xm and the activity 1224 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) indicators θ, as shown in (6) in Table 1.",
                "Another constraint we have to add-because the activity indicators θ are defined on the global timeline τ-is to enforce the fact that the agent is inactive outside of its arrivaldeparture window.",
                "This is accomplished by constraint (7) in Table 1.",
                "Furthermore, agents should not be using resources while they are inactive.",
                "This constraint can also be enforced via a linear inequality on θ and Δ, as shown in (8).",
                "Constraint (6) sets the value of θ to match the policy defined by the occupation measure xm.",
                "In a similar fashion, we have to make sure that the resource-usage variables Δ are also synchronized with the occupation measure xm.",
                "This is done via constraint (9) in Table 1, which is nearly identical to the analogous constraint from [6].",
                "After implementing the above constraint, which enforces the meaning of Δ, we add a constraint that ensures that the agents resource usage never exceeds the amounts of available resources.",
                "This condition is also trivially expressed as a linear inequality (10) in Table 1.",
                "Finally, for the problem formulation where resource assignments are static during a lifetime of an agent, we add a constraint that ensures that the resource-usage variables Δ do not change their value while the agent is active (θ = 1).",
                "This is accomplished via the linear constraint (11), where Z ≥ 2 is a constant that is used to turn off the constraints when θm(τ) = 0 or θm(τ + 1) = 0.",
                "This constraint is not used for the dynamic problem formulation, where resources can be reallocated between agents at every time step.",
                "To summarize, Table 1 together with the conservationof-flow constraints from (12) defines the MILP that simultaneously computes an optimal resource assignment for all agents across time as well as optimal finite-horizon MDP policies that are valid under that resource assignment.",
                "As a rough measure of the complexity of this MILP, let us consider the number of optimization variables and constraints.",
                "Let TM = P Tm = P m(τa m − τd m + 1) be the sum of the lengths of the arrival-departure windows across all agents.",
                "Then, the number of optimization variables is: TM + bτ|M||Ω| + bτ|M|, TM of which are continuous (xm), and bτ|M||Ω| + bτ|M| are binary (Δ and θ).",
                "However, notice that all but TM|M| of the θ are set to zero by constraint (7), which also immediately forces all but TM|M||Ω| of the Δ to be zero via the constraints (8).",
                "The number of constraints (not including the degenerate constraints in (7)) in the MILP is: TM + TM|Ω| + bτ|Ω| + bτ|M||Ω|.",
                "Despite the fact that the complexity of the MILP is, in the worst case, exponential1 in the number of binary variables, the complexity of this MILP is significantly (exponentially) lower than that of the MILP with flat utility functions, described in Section 2.2.",
                "This result echos the efficiency gains reported in [6] for single-shot resource-allocation problems, but is much more pronounced, because of the explosion of the flat utility representation due to the temporal aspect of the problem (recall the prohibitive complexity of the combinatorial optimization in Section 2.2).",
                "We empirically analyze the performance of this method in Section 5. 1 Strictly speaking, solving MILPs to optimality is NPcomplete in the number of integer variables. 5.",
                "EXPERIMENTAL RESULTS Although the complexity of solving MILPs is in the worst case exponential in the number of integer variables, there are many efficient methods for solving MILPs that allow our algorithm to scale well for parameters common to <br>resource allocation</br> and scheduling problems.",
                "In particular, this section introduces a problem domain-the repairshop problem-used to empirically evaluate our algorithms scalability in terms of the number of agents |M|, the number of shared resources |Ω|, and the varied lengths of global time bτ during which agents may enter and exit the system.",
                "The repairshop problem is a simple parameterized MDP adopting the metaphor of a vehicular repair shop.",
                "Agents in the repair shop are mechanics with a number of independent tasks that yield reward only when completed.",
                "In our MDP model of this system, actions taken to advance through the state space are only allowed if the agent holds certain resources that are publicly available to the shop.",
                "These resources are in finite supply, and optimal policies for the shop will determine when each agent may hold the limited resources to take actions and earn individual rewards.",
                "Each task to be completed is associated with a single action, although the agent is required to repeat the action numerous times before completing the task and earning a reward.",
                "This model was parameterized in terms of the number of agents in the system, the number of different types of resources that could be linked to necessary actions, a global time during which agents are allowed to arrive and depart, and a maximum length for the number of time steps an agent may remain in the system.",
                "All datapoints in our experiments were obtained with 20 evaluations using CPLEX to solve the MILPs on a Pentium4 computer with 2Gb of RAM.",
                "Trials were conducted on both the static and the dynamic version of the resourcescheduling problem, as defined earlier.",
                "Figure 3 shows the runtime and policy value for independent modifications to the parameter set.",
                "The top row shows how the solution time for the MILP scales as we increase the number of agents |M|, the global time horizon bτ, and the number of resources |Ω|.",
                "Increasing the number of agents leads to exponential complexity scaling, which is to be expected for an NP-complete problem.",
                "However, increasing the global time limit bτ or the total number of resource types |Ω|-while holding the number of agents constantdoes not lead to decreased performance.",
                "This occurs because the problems get easier as they become under-constrained, which is also a common phenomenon for NP-complete problems.",
                "We also observe that the solution to the dynamic version of the problem can often be computed much faster than the static version.",
                "The bottom row of Figure 3 shows the joint policy value of the policies that correspond to the computed optimal resource-allocation schedules.",
                "We can observe that the dynamic version yields higher reward (as expected, since the reward for the dynamic version is always no less than the reward of the static version).",
                "We should point out that these graphs should not be viewed as a measure of performance of two different algorithms (both algorithms produce optimal solutions but to different problems), but rather as observations about how the quality of optimal solutions change as more flexibility is allowed in the reallocation of resources.",
                "Figure 4 shows runtime and policy value for trials in which common input variables are scaled together.",
                "This allows The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1225 2 4 6 8 10 10 −3 10 −2 10 −1 10 0 10 1 10 2 10 3 10 4 Number of Agents |M| CPUTime,sec |Ω| = 5, τ = 50 static dynamic 50 100 150 200 10 −2 10 −1 10 0 10 1 10 2 10 3 Global Time Boundary τ CPUTime,sec |M| = 5, |Ω| = 5 static dynamic 10 20 30 40 50 10 −2 10 −1 10 0 10 1 10 2 Number of Resources |Ω| CPUTime,sec |M| = 5, τ = 50 static dynamic 2 4 6 8 10 200 400 600 800 1000 1200 1400 1600 Number of Agents |M| Value |Ω| = 5, τ = 50 static dynamic 50 100 150 200 400 500 600 700 800 900 1000 1100 1200 1300 1400 Global Time Boundary τ Value |M| = 5, |Ω| = 5 static dynamic 10 20 30 40 50 500 600 700 800 900 1000 1100 1200 1300 1400 Number of Resources |Ω| Value |M| = 5, τ = 50 static dynamic Figure 3: Evaluation of our MILP for variable numbers of agents (column 1), lengths of global-time window (column 2), and numbers of resource types (column 3).",
                "Top row shows CPU time, and bottom row shows the joint reward of agents MDP policies.",
                "Error bars show the 1st and 3rd quartiles (25% and 75%). 2 4 6 8 10 10 −3 10 −2 10 −1 10 0 10 1 10 2 10 3 Number of Agents |M| CPUTime,sec τ = 10|M| static dynamic 2 4 6 8 10 10 −3 10 −2 10 −1 10 0 10 1 10 2 10 3 10 4 Number of Agents |M| CPUTime,sec |Ω| = 2|M| static dynamic 2 4 6 8 10 10 −3 10 −2 10 −1 10 0 10 1 10 2 10 3 10 4 Number of Agents |M| CPUTime,sec |Ω| = 5|M| static dynamic 2 4 6 8 10 200 400 600 800 1000 1200 1400 1600 1800 2000 2200 Number of Agents |M| Value τ = 10|M| static dynamic 2 4 6 8 10 200 400 600 800 1000 1200 1400 1600 1800 2000 Number of Agents |M| Value |Ω| = 2|M| static dynamic 2 4 6 8 10 0 500 1000 1500 2000 2500 Number of Agents |M| Value |Ω| = 5|M| static dynamic Figure 4: Evaluation of our MILP using correlated input variables.",
                "The left column tracks the performance and CPU time as the number of agents and global-time window increase together (bτ = 10|M|).",
                "The middle and the right column track the performance and CPU time as the number of resources and the number of agents increase together as |Ω| = 2|M| and |Ω| = 5|M|, respectively.",
                "Error bars show the 1st and 3rd quartiles (25% and 75%). 1226 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) us to explore domains where the total number of agents scales proportionally to the total number of resource types or the global time horizon, while keeping constant the average agent density (per unit of global time) or the average number of resources per agent (which commonly occurs in real-life applications).",
                "Overall, we believe that these experimental results indicate that our MILP formulation can be used to effectively solve resource-scheduling problems of nontrivial size. 6.",
                "DISCUSSION AND CONCLUSIONS Throughout the paper, we have made a number of assumptions in our model and solution algorithm; we discuss their implications below. • Continual execution.",
                "We assume that once an agent stops executing its MDP (transitions into state sf ), it exits the system and cannot return.",
                "It is easy to relax this assumption for domains where agents MDPs can be paused and restarted.",
                "All that is required is to include an additional pause action which transitions from a given state back to itself, and has zero reward. • Indifference to start time.",
                "We used a reward model where agents rewards depend only on the time horizon of their MDPs and not the global start time.",
                "This is a consequence of our MDP-augmentation procedure from Section 4.1.",
                "It is easy to extend the model so that the agents incur an explicit penalty for idling by assigning a non-zero negative reward to the start state sb . • Binary resource requirements.",
                "For simplicity, we have assumed that resource costs are binary: ϕm(a, ω) = {0, 1}, but our results generalize in a straightforward manner to non-binary resource mappings, analogously to the procedure used in [5]. • Cooperative agents.",
                "The optimization procedure discussed in this paper was developed in the context of cooperative agents, but it can also be used to design a mechanism for scheduling resources among self-interested agents.",
                "This optimization procedure can be embedded in a VickreyClarke-Groves auction, completely analogously to the way it was done in [7].",
                "In fact, all the results of [7] about the properties of the auction and information privacy directly carry over to the scheduling domain discussed in this paper, requiring only slight modifications to deal with finitehorizon MDPs. • Known, deterministic arrival and departure times.",
                "Finally, we have assumed that agents arrival and departure times (τa m and τd m) are deterministic and known a priori.",
                "This assumption is fundamental to our solution method.",
                "While there are many domains where this assumption is valid, in many cases agents arrive and depart dynamically and their arrival and departure times can only be predicted probabilistically, leading to online resource-allocation problems.",
                "In particular, in the case of self-interested agents, this becomes an interesting version of an online-mechanism-design problem [11, 12].",
                "In summary, we have presented an MILP formulation for the combinatorial resource-scheduling problem where agents values for possible resource assignments are defined by finitehorizon MDPs.",
                "This result extends previous work ([6, 7]) on static one-shot <br>resource allocation</br> under MDP-induced preferences to resource-scheduling problems with a temporal aspect.",
                "As such, this work takes a step in the direction of designing an online mechanism for agents with combinatorial resource preferences induced by stochastic planning problems.",
                "Relaxing the assumption about deterministic arrival and departure times of the agents is a focus of our future work.",
                "We would like to thank the anonymous reviewers for their insightful comments and suggestions. 7.",
                "REFERENCES [1] E. Altman and A. Shwartz.",
                "Adaptive control of constrained Markov chains: Criteria and policies.",
                "Annals of Operations Research, special issue on Markov Decision Processes, 28:101-134, 1991. [2] R. Bellman.",
                "Dynamic Programming.",
                "Princeton University Press, 1957. [3] C. Boutilier.",
                "Solving concisely expressed combinatorial auction problems.",
                "In Proc. of AAAI-02, pages 359-366, 2002. [4] C. Boutilier and H. H. Hoos.",
                "Bidding languages for combinatorial auctions.",
                "In Proc. of IJCAI-01, pages 1211-1217, 2001. [5] D. Dolgov.",
                "Integrated <br>resource allocation</br> and Planning in Stochastic Multiagent Environments.",
                "PhD thesis, Computer Science Department, University of Michigan, February 2006. [6] D. A. Dolgov and E. H. Durfee.",
                "Optimal <br>resource allocation</br> and policy formulation in loosely-coupled Markov decision processes.",
                "In Proc. of ICAPS-04, pages 315-324, June 2004. [7] D. A. Dolgov and E. H. Durfee.",
                "Computationally efficient combinatorial auctions for <br>resource allocation</br> in weakly-coupled MDPs.",
                "In Proc. of AAMAS-05, New York, NY, USA, 2005.",
                "ACM Press. [8] D. A. Dolgov and E. H. Durfee.",
                "<br>resource allocation</br> among agents with preferences induced by factored MDPs.",
                "In Proc. of AAMAS-06, 2006. [9] K. Larson and T. Sandholm.",
                "Mechanism design and deliberative agents.",
                "In Proc. of AAMAS-05, pages 650-656, New York, NY, USA, 2005.",
                "ACM Press. [10] N. Nisan.",
                "Bidding and allocation in combinatorial auctions.",
                "In Electronic Commerce, 2000. [11] D. C. Parkes and S. Singh.",
                "An MDP-based approach to Online Mechanism Design.",
                "In Proc. of the Seventeenths Annual Conference on Neural Information Processing Systems (NIPS-03), 2003. [12] D. C. Parkes, S. Singh, and D. Yanovsky.",
                "Approximately efficient online mechanism design.",
                "In Proc. of the Eighteenths Annual Conference on Neural Information Processing Systems (NIPS-04), 2004. [13] M. L. Puterman.",
                "Markov Decision Processes.",
                "John Wiley & Sons, New York, 1994. [14] M. H. Rothkopf, A. Pekec, and R. M. Harstad.",
                "Computationally manageable combinational auctions.",
                "Management Science, 44(8):1131-1147, 1998. [15] T. Sandholm.",
                "An algorithm for optimal winner determination in combinatorial auctions.",
                "In Proc. of IJCAI-99, pages 542-547, San Francisco, CA, USA, 1999.",
                "Morgan Kaufmann Publishers Inc.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1227"
            ],
            "original_annotated_samples": [
                "INTRODUCTION The tasks of optimal <br>resource allocation</br> and scheduling are ubiquitous in multiagent systems, but solving such optimization problems can be computationally difficult, due to a number of factors.",
                "An alternative is to directly model the mechanisms that define the agents utility functions and perform <br>resource allocation</br> directly with these models [9].",
                "However, this existing work on <br>resource allocation</br> with preferences induced by resource-parameterized MDPs makes an assumption that the resources are only allocated once and are then utilized by the agents independently within their infinite-horizon MDPs.",
                "In this paper, we extend the work on <br>resource allocation</br> under MDP-induced preferences to discrete-time scheduling problems, where agents are present in the system for finite time intervals and can only use resources within these intervals.",
                "This will not affect the <br>resource allocation</br> due to the resource constraints only being enforced for the original MDP states, as will be discussed in the next section."
            ],
            "translated_annotated_samples": [
                "INTRODUCCIÓN Las tareas de <br>asignación óptima de recursos</br> y programación son ubicuas en sistemas multiagentes, pero resolver tales problemas de optimización puede ser computacionalmente difícil, debido a una serie de factores.",
                "Una alternativa es modelar directamente los mecanismos que definen las funciones de utilidad de los agentes y realizar la <br>asignación de recursos</br> directamente con estos modelos [9].",
                "Sin embargo, este trabajo existente sobre <br>asignación de recursos</br> con preferencias inducidas por MDPs parametrizados por recursos hace una suposición de que los recursos se asignan solo una vez y luego son utilizados por los agentes de forma independiente dentro de sus MDPs de horizonte infinito.",
                "En este artículo, ampliamos el trabajo sobre <br>asignación de recursos</br> bajo preferencias inducidas por MDP a problemas de programación en tiempo discreto, donde los agentes están presentes en el sistema durante intervalos de tiempo finitos y solo pueden utilizar recursos dentro de estos intervalos.",
                "Esto no afectará la <br>asignación de recursos</br> debido a que las restricciones de recursos solo se aplican a los estados originales del MDP, como se discutirá en la siguiente sección."
            ],
            "translated_text": "La programación de recursos combinatoria para MDPs multiagentes. Dmitri A. Dolgov, Michael R. James y Michael E. Samples. Grupo de Investigación en IA y Robótica, Centro Técnico de Toyota en EE. UU. {ddolgov, michael.r.james, michael.samples}@gmail.com. RESUMEN La programación óptima de recursos en sistemas multiagentes es una tarea computacionalmente desafiante, especialmente cuando los valores de los recursos no son aditivos. Consideramos el problema combinatorio de programar el uso de múltiples recursos entre agentes que operan en entornos estocásticos, modelados como procesos de decisión de Markov (MDPs). En los últimos años, se han desarrollado algoritmos eficientes de asignación de recursos para agentes con valores de recursos inducidos por MDPs. Sin embargo, este trabajo previo se ha centrado en problemas de asignación de recursos estáticos donde los recursos se distribuyen una vez y luego se utilizan en MDPs de horizonte infinito. Extendemos esos modelos existentes al problema de la programación de recursos combinatoria, donde los agentes persisten solo por períodos finitos entre sus tiempos de llegada y salida (predefinidos), requiriendo recursos solo durante esos períodos de tiempo. Proporcionamos un procedimiento computacionalmente eficiente para calcular asignaciones de recursos óptimas a nivel global a agentes a lo largo del tiempo. Ilustramos y analizamos empíricamente el método en el contexto de un dominio de programación de trabajos estocásticos. Categorías y Descriptores de Asignaturas I.2.8 [Inteligencia Artificial]: Resolución de Problemas, Métodos de Control y Búsqueda; I.2.11 [Inteligencia Artificial]: Inteligencia Artificial Distribuida-Sistemas Multiagente Términos Generales Algoritmos, Rendimiento, Diseño 1. INTRODUCCIÓN Las tareas de <br>asignación óptima de recursos</br> y programación son ubicuas en sistemas multiagentes, pero resolver tales problemas de optimización puede ser computacionalmente difícil, debido a una serie de factores. En particular, cuando el valor de un conjunto de recursos para un agente no es aditivo (como suele ser el caso con recursos que son sustitutos o complementos), la función de utilidad podría tener que definirse en un espacio exponencialmente grande de combinaciones de recursos, lo que rápidamente se vuelve computacionalmente intratable. Además, incluso cuando cada agente tiene una función de utilidad que es distinta de cero solo en un pequeño subconjunto de los posibles conjuntos de recursos, obtener una asignación óptima sigue siendo computacionalmente prohibitivo, ya que el problema se vuelve NP-completo [14]. Tales problemas computacionales han dado lugar recientemente a varias líneas de trabajo en el uso de modelos compactos de las preferencias de los agentes. Una idea es utilizar cualquier estructura presente en las funciones de utilidad para representarlas de forma compacta, por ejemplo, a través de fórmulas lógicas [15, 10, 4, 3]. Una alternativa es modelar directamente los mecanismos que definen las funciones de utilidad de los agentes y realizar la <br>asignación de recursos</br> directamente con estos modelos [9]. Una forma de lograr esto es modelar los procesos mediante los cuales un agente podría utilizar los recursos y definir la función de utilidad como la ganancia de estos procesos. En particular, si un agente utiliza recursos para actuar en un entorno estocástico, su función de utilidad puede ser modelada de forma natural con un proceso de decisión de Markov, cuyo conjunto de acciones está parametrizado por los recursos disponibles. Esta representación puede ser utilizada para construir algoritmos de asignación de recursos muy eficientes que resultan en una aceleración exponencial sobre un problema de optimización directa con representaciones planas de preferencias combinatorias [6, 7, 8]. Sin embargo, este trabajo existente sobre <br>asignación de recursos</br> con preferencias inducidas por MDPs parametrizados por recursos hace una suposición de que los recursos se asignan solo una vez y luego son utilizados por los agentes de forma independiente dentro de sus MDPs de horizonte infinito. Esta suposición de que no es posible la reasignación de recursos puede ser limitante en dominios donde los agentes llegan y se van dinámicamente. En este artículo, ampliamos el trabajo sobre <br>asignación de recursos</br> bajo preferencias inducidas por MDP a problemas de programación en tiempo discreto, donde los agentes están presentes en el sistema durante intervalos de tiempo finitos y solo pueden utilizar recursos dentro de estos intervalos. En particular, los agentes llegan y parten en momentos arbitrarios (predefinidos) y dentro de estos intervalos utilizan recursos para ejecutar tareas en MDPs de horizonte finito. Abordamos el problema de la programación de recursos óptima a nivel global, donde el objetivo es encontrar una asignación de recursos a los agentes a lo largo del tiempo que maximice la suma de las recompensas esperadas que obtienen. En este contexto, nuestra principal contribución es una formulación de programación entera mixta del problema de programación que elige asignaciones de recursos globalmente óptimas, tiempos de inicio y horizontes de ejecución para todos los agentes (dentro de sus intervalos de llegada y salida). Analizamos y comparamos empíricamente dos variantes del problema de programación: uno, donde los agentes tienen asignaciones de recursos estáticas dentro de sus MDPs de horizonte finito, y otro, donde los recursos pueden ser realocados dinámicamente entre agentes en cada paso de tiempo. En el resto del documento, primero establecemos las bases necesarias en la Sección 2 y luego presentamos nuestro modelo y declaración formal del problema en la Sección 3. En la Sección 4.2, describimos nuestro resultado principal, el programa de optimización para la programación de recursos globalmente óptima. Tras la discusión de nuestros resultados experimentales sobre un problema de programación de trabajos en la Sección 5, concluimos en la Sección 6 con una discusión de posibles extensiones y generalizaciones de nuestro método. De manera similar al modelo utilizado en trabajos anteriores sobre asignación de recursos con preferencias inducidas por MDP [6, 7], definimos el valor de un conjunto de recursos para un agente como el valor de la mejor política MDP que es realizable, dadas esos recursos. Sin embargo, dado que el enfoque de nuestro trabajo se centra en problemas de programación, y una gran parte del problema de optimización consiste en decidir cómo se asignan los recursos en el tiempo entre agentes con tiempos finitos de llegada y salida, modelamos los problemas de planificación de los agentes como MDPs de horizonte finito, en contraste con trabajos anteriores que utilizaron MDPs descontados de horizonte infinito. En el resto de esta sección, primero presentamos algo de antecedentes necesarios sobre MDPs de horizonte finito y presentamos una formulación de programación lineal que sirve como base para nuestro algoritmo de solución desarrollado en la Sección 4. También describimos los métodos estándar para la programación de recursos combinatorios con valores de recursos planos, que sirven como punto de referencia de comparación para el nuevo modelo desarrollado aquí. 2.1 Procesos de Decisión de Markov Un MDP (Proceso de Decisión de Markov) estacionario, de dominio finito y discreto en el tiempo (ver, por ejemplo, [13] para un desarrollo detallado y exhaustivo) se puede describir como S, A, p, r, donde: S es un conjunto finito de estados del sistema; A es un conjunto finito de acciones disponibles para el agente; p es una función de transición estocástica estacionaria, donde p(σ|s, a) es la probabilidad de transicionar al estado σ al ejecutar la acción a en el estado s; r es una función de recompensa estacionaria, donde r(s, a) especifica la recompensa obtenida al ejecutar la acción a en el estado s. Dado un MDP de este tipo, un problema de decisión bajo un horizonte finito T consiste en elegir una acción óptima en cada paso de tiempo para maximizar el valor esperado de la recompensa total acumulada durante la vida (finita) de los agentes. La política óptima del agente es entonces una función del estado actual s y el tiempo hasta el horizonte. Una política óptima para tal problema es actuar de forma codiciosa con respecto a la función de valor óptimo, definida de forma recursiva por el siguiente sistema de ecuaciones de Bellman de tiempo finito [2]: v(s, t) = max a r(s, a) + X σ p(σ|s, a)v(σ, t + 1), ∀s ∈ S, t ∈ [1, T − 1]; v(s, T) = 0, ∀s ∈ S; donde v(s, t) es el valor óptimo de estar en el estado s en el tiempo t ∈ [1, T]. Esta función de valor óptimo se puede calcular fácilmente utilizando programación dinámica, lo que conduce a la siguiente política óptima π, donde π(s, a, t) es la probabilidad de ejecutar la acción a en el estado s en el tiempo t: π(s, a, t) = (1, a = argmaxa r(s, a) + P σ p(σ|s, a)v(σ, t + 1), 0, en caso contrario. La anterior es la forma más común de calcular la función de valor óptimo (y por lo tanto una política óptima) para un MDP de horizonte finito. Sin embargo, también podemos formular el problema como el siguiente programa lineal (similarmente al LP dual para MDPs descontados de horizonte infinito [13, 6, 7]): max X s X a r(s, a) X t x(s, a, t) sujeto a: X a x(σ, a, t + 1) = X s,a p(σ|s, a)x(s, a, t) ∀σ, t ∈ [1, T − 1]; X a x(s, a, 1) = α(s), ∀s ∈ S; (1) donde α(s) es la distribución inicial sobre el espacio de estados, y x es la medida de ocupación (no estacionaria) (x(s, a, t) ∈ [0, 1] es el número total esperado de veces que se ejecuta la acción a en el estado s en el tiempo t). Una política óptima (no estacionaria) se obtiene a partir de la medida de ocupación de la siguiente manera: π(s, a, t) = x(s, a, t)/ X a x(s, a, t) ∀s ∈ S, t ∈ [1, T]. (2) Tenga en cuenta que el MDP estándar de horizonte finito sin restricciones, como se describe anteriormente, siempre tiene una solución uniformemente óptima (óptima para cualquier distribución inicial α(s)). Por lo tanto, una política óptima se puede obtener utilizando una constante arbitraria α(s) > 0 (en particular, α(s) = 1 resultará en x(s, a, t) = π(s, a, t)). Sin embargo, para los MDPs con restricciones de recursos (como se definen a continuación en la Sección 3), las políticas óptimas uniformemente no existen en general. En tales casos, α se convierte en parte de la entrada del problema, y una política resultante solo es óptima para ese α en particular. Este resultado es bien conocido para MDPs de horizonte infinito con varios tipos de restricciones [1, 6], y también se cumple para nuestro modelo de horizonte finito, el cual puede establecerse fácilmente a través de una línea de razonamiento completamente análoga a los argumentos en [6]. 2.2 Programación de Recursos Combinatoria Un enfoque directo para la programación de recursos para un conjunto de agentes M, cuyos valores para los recursos son inducidos por problemas de planificación estocástica (en nuestro caso, MDPs de horizonte finito) sería que cada agente enumerara todas las asignaciones de recursos posibles a lo largo del tiempo y, para cada una, calcular su valor resolviendo el MDP correspondiente. Entonces, cada agente proporcionaría valoraciones para cada posible conjunto de recursos a lo largo del tiempo a un coordinador centralizado, quien calcularía las asignaciones óptimas de recursos a lo largo del tiempo basándose en estas valoraciones. Cuando los recursos pueden asignarse en diferentes momentos a diferentes agentes, cada agente debe presentar valoraciones para cada combinación de posibles horizontes temporales. Que cada agente m ∈ M ejecute su MDP dentro del intervalo de tiempo de llegada-salida τ ∈ [τa m, τd m]. Por lo tanto, el agente m ejecutará un MDP con un horizonte temporal no mayor que Tm = τd m−τa m+1. Que bτ sea el horizonte temporal global para el problema, antes del cual todos los MDP de los agentes deben finalizar. Suponemos que τd m < bτ, ∀m ∈ M. El Sexto Congreso Internacional. En el problema de programación donde los agentes tienen requisitos de recursos estáticos dentro de sus MDPs de horizonte finito, los agentes proporcionan una valoración para cada conjunto de recursos para cada horizonte de tiempo posible (desde [1, Tm]) que puedan utilizar. Sea Ω el conjunto de recursos a ser asignados entre los agentes. Un agente recibirá como máximo un paquete de recursos para uno de los horizontes temporales. Que la variable ψ ∈ Ψm enumere todos los posibles pares de conjuntos de recursos y horizontes temporales para el agente m, por lo que hay 2|Ω| × Tm valores para ψ (el espacio de conjuntos es exponencial en el número de tipos de recursos |Ω|). El agente m debe proporcionar un valor vψ m para cada ψ, y el coordinador asignará como máximo un par (recurso, horizonte temporal) ψ a cada agente. Esta asignación se expresa como una variable indicadora zψ m ∈ {0, 1} que muestra si ψ está asignado al agente m. Para el tiempo τ y el recurso ω, la función nm(ψ, τ, ω) ∈ {0, 1} indica si el paquete en ψ utiliza el recurso ω en el tiempo τ (hacemos la suposición de que los agentes tienen requisitos de recursos binarios). Este problema de asignación es NP-completo, incluso al considerar solo un paso de tiempo, y su dificultad aumenta significativamente con múltiples pasos de tiempo debido al creciente número de valores de ψ. El problema de encontrar una asignación óptima que satisfaga la restricción global de que la cantidad de cada recurso ω asignada a todos los agentes no exceda la cantidad disponible bϕ(ω) se puede expresar como el siguiente programa entero: max X m∈M X ψ∈Ψm zψ mvψ m sujeto a: X ψ∈Ψm zψ m ≤ 1, ∀m ∈ M; X m∈M X ψ∈Ψm zψ mnm(ψ, τ, ω) ≤ bϕ(ω), ∀τ ∈ [1, bτ], ∀ω ∈ Ω; (3) La primera restricción en la ecuación 3 dice que ningún agente puede recibir más de un paquete, y la segunda restricción asegura que la asignación total del recurso ω no exceda, en ningún momento, el límite del recurso. Para el problema de programación en el que los agentes pueden reasignar dinámicamente recursos, cada agente debe especificar un valor para cada combinación de paquetes y pasos de tiempo dentro de su horizonte temporal. Que la variable ψ ∈ Ψm en este caso enumere todos los posibles conjuntos de recursos para los cuales a lo sumo un conjunto puede ser asignado al agente m en cada paso de tiempo. Por lo tanto, en este caso hay P t∈[1,Tm](2|Ω| )t ∼ 2|Ω|Tm posibilidades de combinaciones de recursos asignadas a diferentes intervalos de tiempo, para los Tm horizontes de tiempo diferentes. El mismo conjunto de ecuaciones (3) se puede utilizar para resolver este problema de programación dinámica, pero el programa entero es diferente debido a la diferencia en cómo se define ψ. En este caso, el número de valores de ψ es exponencial en el horizonte de planificación Tm de cada agente, lo que resulta en un programa mucho más grande. Este enfoque directo para resolver ambos problemas de programación requiere una enumeración y solución de 2|Ω| Tm (asignación estática) o P t∈[1,Tm] 2|Ω|t (reubicación dinámica) MDPs para cada agente, lo cual rápidamente se vuelve intratable con el crecimiento del número de recursos |Ω| o el horizonte de tiempo Tm. MODELO Y DECLARACIÓN DEL PROBLEMA Ahora presentamos formalmente nuestro modelo del problema de programación de recursos. La entrada del problema consiste en los siguientes componentes: • M, Ω, bϕ, τa m, τd m, bτ están definidos arriba en la Sección 2.2. • {Θm} = {S, A, pm, rm, αm} son los MDPs de todos los agentes m ∈ M. Sin pérdida de generalidad, asumimos que los espacios de estados y acciones de todos los agentes son iguales, pero cada uno tiene su propia función de transición pm, función de recompensa rm, y condiciones iniciales αm. • ϕm : A×Ω → {0, 1} es la asignación de acciones a recursos para el agente m. ϕm(a, ω) indica si la acción a del agente m necesita el recurso ω. Un agente m que recibe un conjunto de recursos que no incluye el recurso ω no puede ejecutar en su política MDP ninguna acción a para la cual ϕm(a, ω) = 0. Suponemos que todos los requisitos de recursos son binarios; como se discute más adelante en la Sección 6, esta suposición no es limitante. Dado el input anterior, el problema de optimización que consideramos es encontrar la asignación globalmente óptima que maximice la suma de recompensas esperadas de recursos a agentes para todos los pasos de tiempo: Δ : τ × M × Ω → {0, 1}. Una solución es factible si la asignación correspondiente de recursos a los agentes no viola la restricción global de recursos: X m Δm(τ, ω) ≤ bϕ(ω), ∀ω ∈ Ω, τ ∈ [1, bτ]. Consideramos dos variantes del problema de programación de recursos. La primera formulación restringe las asignaciones de recursos al espacio donde la asignación de recursos a cada agente es estática durante la vida de los agentes. La segunda formulación permite la reasignación de recursos entre agentes en cada paso de tiempo durante sus vidas. La Figura 1 representa un problema de programación de recursos con tres agentes M = {m1, m2, m3}, tres recursos Ω = {ω1, ω2, ω3}, y un horizonte de problema global de bτ = 11. Los tiempos de llegada y salida de los agentes se muestran como cajas grises y son {1, 6}, {3, 7} y {2, 11}, respectivamente. Una solución a este problema se muestra a través de barras horizontales dentro de cada casilla de agentes, donde las barras corresponden a la asignación de los tres tipos de recursos. La Figura 1a muestra una solución a un problema de programación estática. Según la solución mostrada, el agente m1 comienza la ejecución de su MDP en el tiempo τ = 1 y tiene un bloqueo en los tres recursos hasta que finaliza la ejecución en el tiempo τ = 3. Se debe tener en cuenta que el agente m1 renuncia a su control sobre los recursos antes de su hora de salida anunciada de τd m1 = 6, aparentemente porque otros agentes pueden utilizar los recursos de manera más efectiva. Por lo tanto, en el tiempo τ = 4, los recursos ω1 y ω3 se asignan al agente m2, quien luego los utiliza para ejecutar su MDP (utilizando solo acciones admitidas por los recursos ω1 y ω3) hasta el tiempo τ = 7. El agente m3 posee el recurso ω3 durante el intervalo τ ∈ [4, 10]. La Figura 1b muestra una posible solución a la versión dinámica del mismo problema. Allí, los recursos pueden ser reasignados entre agentes en cada paso de tiempo. Por ejemplo, el agente m1 renuncia a su uso del recurso ω2 en el tiempo τ = 2, aunque continúa la ejecución de su MDP hasta el tiempo τ = 6. Ten en cuenta que a un agente no se le permite detener y reiniciar su MDP, por lo que el agente m1 solo puede continuar ejecutando en el intervalo τ ∈ [3, 4] si tiene acciones que no requieren recursos (ϕm(a, ω) = 0). Claramente, el modelo y la declaración del problema descritos anteriormente hacen una serie de suposiciones sobre el problema y las propiedades de la solución deseada. Discutimos algunas de esas suposiciones y sus implicaciones en la Sección 6. 1222 La Sexta Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) (a) (b) Figura 1: Ilustración de una solución a un problema de programación de recursos con tres agentes y tres recursos: a) asignaciones de recursos estáticas (las asignaciones de recursos son constantes durante la vida de los agentes); b) asignación dinámica (las asignaciones de recursos pueden cambiar en cada paso de tiempo). 4. PROGRAMACIÓN DE RECURSOS Nuestro algoritmo de programación de recursos se lleva a cabo en dos etapas. Primero, realizamos un paso de preprocesamiento que aumenta los MDP de los agentes; este proceso se describe en la Sección 4.1. Segundo, utilizando estos MDPs aumentados construimos un problema de optimización global, el cual se describe en la Sección 4.2. 4.1 Aumentando los MDPs de los Agentes En el modelo descrito en la sección anterior, asumimos que si un agente no posee los recursos necesarios para realizar acciones en su MDP, su ejecución se detiene y el agente abandona el sistema. En otras palabras, los MDPs no pueden ser pausados y reanudados. Por ejemplo, en el problema mostrado en la Figura 1a, el agente m1 libera todos los recursos después del tiempo τ = 3, momento en el cual la ejecución de su MDP se detiene. De manera similar, los agentes m2 y m3 solo ejecutan sus MDPs en los intervalos τ ∈ [4, 6] y τ ∈ [4, 10], respectivamente. Por lo tanto, una parte importante del problema de toma de decisiones global es decidir la ventana de tiempo durante la cual cada uno de los agentes está activo (es decir, ejecutando su MDP). Para lograr esto, ampliamos el MDP de cada agente con dos nuevos estados (estados de inicio y final sb, sf, respectivamente) y una nueva acción de inicio/parada a∗, como se ilustra en la Figura 2. La idea es que un agente permanezca en el estado inicial sb hasta que esté listo para ejecutar su MDP, momento en el que realiza la acción de inicio/parada a∗ y transita al espacio de estados del MDP original con la probabilidad de transición que corresponde a la distribución inicial original α(s). Por ejemplo, en la Figura 1a, para el agente m2 esto ocurriría en el tiempo τ = 4. Una vez que el agente llega al final de su ventana de actividad (tiempo τ = 6 para el agente m2 en la Figura 1a), realiza la acción de inicio/parada, lo que lo lleva al estado final de finalización del sumidero sf en el tiempo τ = 7. Más precisamente, dado un MDP S, A, pm, rm, αm, definimos un MDP aumentado S, A, pm, rm, αm de la siguiente manera: S = S ∪ sb ∪ sf; A = A ∪ a∗; p(s|sb, a∗) = α(s), ∀s ∈ S; p(sb|sb, a) = 1.0, ∀a ∈ A; p(sf|s, a∗) = 1.0, ∀s ∈ S; p(σ|s, a) = p(σ|s, a), ∀s, σ ∈ S, a ∈ A; r(sb, a) = r(sf, a) = 0, ∀a ∈ A; r(s, a) = r(s, a), ∀s ∈ S, a ∈ A; α(sb) = 1; α(s) = 0, ∀s ∈ S; donde se asume que todas las probabilidades de transición no especificadas son cero. Además, para tener en cuenta el nuevo estado inicial, comenzamos el MDP un paso de tiempo antes, estableciendo τa m ← τa m − 1. Esto no afectará la <br>asignación de recursos</br> debido a que las restricciones de recursos solo se aplican a los estados originales del MDP, como se discutirá en la siguiente sección. ",
            "candidates": [],
            "error": [
                [
                    "asignación óptima de recursos",
                    "asignación de recursos",
                    "asignación de recursos",
                    "asignación de recursos",
                    "asignación de recursos"
                ]
            ]
        },
        "scheduling": {
            "translated_key": "",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Combinatorial Resource <br>scheduling</br> for Multiagent MDPs Dmitri A. Dolgov, Michael R. James, and Michael E. Samples AI and Robotics Group Technical Research, Toyota Technical Center USA {ddolgov, michael.r.james, michael.samples}@gmail.com ABSTRACT Optimal resource <br>scheduling</br> in multiagent systems is a computationally challenging task, particularly when the values of resources are not additive.",
                "We consider the combinatorial problem of <br>scheduling</br> the usage of multiple resources among agents that operate in stochastic environments, modeled as Markov decision processes (MDPs).",
                "In recent years, efficient resource-allocation algorithms have been developed for agents with resource values induced by MDPs.",
                "However, this prior work has focused on static resource-allocation problems where resources are distributed once and then utilized in infinite-horizon MDPs.",
                "We extend those existing models to the problem of combinatorial resource <br>scheduling</br>, where agents persist only for finite periods between their (predefined) arrival and departure times, requiring resources only for those time periods.",
                "We provide a computationally efficient procedure for computing globally optimal resource assignments to agents over time.",
                "We illustrate and empirically analyze the method in the context of a stochastic jobscheduling domain.",
                "Categories and Subject Descriptors I.2.8 [Artificial Intelligence]: Problem Solving, Control Methods, and Search; I.2.11 [Artificial Intelligence]: Distributed Artificial Intelligence-Multiagent systems General Terms Algorithms, Performance, Design 1.",
                "INTRODUCTION The tasks of optimal resource allocation and <br>scheduling</br> are ubiquitous in multiagent systems, but solving such optimization problems can be computationally difficult, due to a number of factors.",
                "In particular, when the value of a set of resources to an agent is not additive (as is often the case with resources that are substitutes or complements), the utility function might have to be defined on an exponentially large space of resource bundles, which very quickly becomes computationally intractable.",
                "Further, even when each agent has a utility function that is nonzero only on a small subset of the possible resource bundles, obtaining optimal allocation is still computationally prohibitive, as the problem becomes NP-complete [14].",
                "Such computational issues have recently spawned several threads of work in using compact models of agents preferences.",
                "One idea is to use any structure present in utility functions to represent them compactly, via, for example, logical formulas [15, 10, 4, 3].",
                "An alternative is to directly model the mechanisms that define the agents utility functions and perform resource allocation directly with these models [9].",
                "A way of accomplishing this is to model the processes by which an agent might utilize the resources and define the utility function as the payoff of these processes.",
                "In particular, if an agent uses resources to act in a stochastic environment, its utility function can be naturally modeled with a Markov decision process, whose action set is parameterized by the available resources.",
                "This representation can then be used to construct very efficient resource-allocation algorithms that lead to an exponential speedup over a straightforward optimization problem with flat representations of combinatorial preferences [6, 7, 8].",
                "However, this existing work on resource allocation with preferences induced by resource-parameterized MDPs makes an assumption that the resources are only allocated once and are then utilized by the agents independently within their infinite-horizon MDPs.",
                "This assumption that no reallocation of resources is possible can be limiting in domains where agents arrive and depart dynamically.",
                "In this paper, we extend the work on resource allocation under MDP-induced preferences to discrete-time <br>scheduling</br> problems, where agents are present in the system for finite time intervals and can only use resources within these intervals.",
                "In particular, agents arrive and depart at arbitrary (predefined) times and within these intervals use resources to execute tasks in finite-horizon MDPs.",
                "We address the problem of globally optimal resource <br>scheduling</br>, where the objective is to find an allocation of resources to the agents across time that maximizes the sum of the expected rewards that they obtain.",
                "In this context, our main contribution is a mixed-integerprogramming formulation of the <br>scheduling</br> problem that chooses globally optimal resource assignments, starting times, and execution horizons for all agents (within their arrival1220 978-81-904262-7-5 (RPS) c 2007 IFAAMAS departure intervals).",
                "We analyze and empirically compare two flavors of the <br>scheduling</br> problem: one, where agents have static resource assignments within their finite-horizon MDPs, and another, where resources can be dynamically reallocated between agents at every time step.",
                "In the rest of the paper, we first lay down the necessary groundwork in Section 2 and then introduce our model and formal problem statement in Section 3.",
                "In Section 4.2, we describe our main result, the optimization program for globally optimal resource <br>scheduling</br>.",
                "Following the discussion of our experimental results on a job-<br>scheduling</br> problem in Section 5, we conclude in Section 6 with a discussion of possible extensions and generalizations of our method. 2.",
                "BACKGROUND Similarly to the model used in previous work on resourceallocation with MDP-induced preferences [6, 7], we define the value of a set of resources to an agent as the value of the best MDP policy that is realizable, given those resources.",
                "However, since the focus of our work is on <br>scheduling</br> problems, and a large part of the optimization problem is to decide how resources are allocated in time among agents with finite arrival and departure times, we model the agents planning problems as finite-horizon MDPs, in contrast to previous work that used infinite-horizon discounted MDPs.",
                "In the rest of this section, we first introduce some necessary background on finite-horizon MDPs and present a linear-programming formulation that serves as the basis for our solution algorithm developed in Section 4.",
                "We also outline the standard methods for combinatorial resource <br>scheduling</br> with flat resource values, which serve as a comparison benchmark for the new model developed here. 2.1 Markov Decision Processes A stationary, finite-domain, discrete-time MDP (see, for example, [13] for a thorough and detailed development) can be described as S, A, p, r , where: S is a finite set of system states; A is a finite set of actions that are available to the agent; p is a stationary stochastic transition function, where p(σ|s, a) is the probability of transitioning to state σ upon executing action a in state s; r is a stationary reward function, where r(s, a) specifies the reward obtained upon executing action a in state s. Given such an MDP, a decision problem under a finite horizon T is to choose an optimal action at every time step to maximize the expected value of the total reward accrued during the agents (finite) lifetime.",
                "The agents optimal policy is then a function of current state s and the time until the horizon.",
                "An optimal policy for such a problem is to act greedily with respect to the optimal value function, defined recursively by the following system of finite-time Bellman equations [2]: v(s, t) = max a r(s, a) + X σ p(σ|s, a)v(σ, t + 1), ∀s ∈ S, t ∈ [1, T − 1]; v(s, T) = 0, ∀s ∈ S; where v(s, t) is the optimal value of being in state s at time t ∈ [1, T].",
                "This optimal value function can be easily computed using dynamic programming, leading to the following optimal policy π, where π(s, a, t) is the probability of executing action a in state s at time t: π(s, a, t) = ( 1, a = argmaxa r(s, a) + P σ p(σ|s, a)v(σ, t + 1), 0, otherwise.",
                "The above is the most common way of computing the optimal value function (and therefore an optimal policy) for a finite-horizon MDP.",
                "However, we can also formulate the problem as the following linear program (similarly to the dual LP for infinite-horizon discounted MDPs [13, 6, 7]): max X s X a r(s, a) X t x(s, a, t) subject to: X a x(σ, a, t + 1) = X s,a p(σ|s, a)x(s, a, t) ∀σ, t ∈ [1, T − 1]; X a x(s, a, 1) = α(s), ∀s ∈ S; (1) where α(s) is the initial distribution over the state space, and x is the (non-stationary) occupation measure (x(s, a, t) ∈ [0, 1] is the total expected number of times action a is executed in state s at time t).",
                "An optimal (non-stationary) policy is obtained from the occupation measure as follows: π(s, a, t) = x(s, a, t)/ X a x(s, a, t) ∀s ∈ S, t ∈ [1, T]. (2) Note that the standard unconstrained finite-horizon MDP, as described above, always has a uniformly-optimal solution (optimal for any initial distribution α(s)).",
                "Therefore, an optimal policy can be obtained by using an arbitrary constant α(s) > 0 (in particular, α(s) = 1 will result in x(s, a, t) = π(s, a, t)).",
                "However, for MDPs with resource constraints (as defined below in Section 3), uniformly-optimal policies do not in general exist.",
                "In such cases, α becomes a part of the problem input, and a resulting policy is only optimal for that particular α.",
                "This result is well known for infinite-horizon MDPs with various types of constraints [1, 6], and it also holds for our finite-horizon model, which can be easily established via a line of reasoning completely analogous to the arguments in [6]. 2.2 Combinatorial Resource <br>scheduling</br> A straightforward approach to resource <br>scheduling</br> for a set of agents M, whose values for the resources are induced by stochastic planning problems (in our case, finite-horizon MDPs) would be to have each agent enumerate all possible resource assignments over time and, for each one, compute its value by solving the corresponding MDP.",
                "Then, each agent would provide valuations for each possible resource bundle over time to a centralized coordinator, who would compute the optimal resource assignments across time based on these valuations.",
                "When resources can be allocated at different times to different agents, each agent must submit valuations for every combination of possible time horizons.",
                "Let each agent m ∈ M execute its MDP within the arrival-departure time interval τ ∈ [τa m, τd m].",
                "Hence, agent m will execute an MDP with time horizon no greater than Tm = τd m−τa m+1.",
                "Let bτ be the global time horizon for the problem, before which all of the agents MDPs must finish.",
                "We assume τd m < bτ, ∀m ∈ M. The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1221 For the <br>scheduling</br> problem where agents have static resource requirements within their finite-horizon MDPs, the agents provide a valuation for each resource bundle for each possible time horizon (from [1, Tm]) that they may use.",
                "Let Ω be the set of resources to be allocated among the agents.",
                "An agent will get at most one resource bundle for one of the time horizons.",
                "Let the variable ψ ∈ Ψm enumerate all possible pairs of resource bundles and time horizons for agent m, so there are 2|Ω| × Tm values for ψ (the space of bundles is exponential in the number of resource types |Ω|).",
                "The agent m must provide a value vψ m for each ψ, and the coordinator will allocate at most one ψ (resource, time horizon) pair to each agent.",
                "This allocation is expressed as an indicator variable zψ m ∈ {0, 1} that shows whether ψ is assigned to agent m. For time τ and resource ω, the function nm(ψ, τ, ω) ∈ {0, 1} indicates whether the bundle in ψ uses resource ω at time τ (we make the assumption that agents have binary resource requirements).",
                "This allocation problem is NP-complete, even when considering only a single time step, and its difficulty increases significantly with multiple time steps because of the increasing number of values of ψ.",
                "The problem of finding an optimal allocation that satisfies the global constraint that the amount of each resource ω allocated to all agents does not exceed the available amount bϕ(ω) can be expressed as the following integer program: max X m∈M X ψ∈Ψm zψ mvψ m subject to: X ψ∈Ψm zψ m ≤ 1, ∀m ∈ M; X m∈M X ψ∈Ψm zψ mnm(ψ, τ, ω) ≤ bϕ(ω), ∀τ ∈ [1, bτ], ∀ω ∈ Ω; (3) The first constraint in equation 3 says that no agent can receive more than one bundle, and the second constraint ensures that the total assignment of resource ω does not, at any time, exceed the resource bound.",
                "For the <br>scheduling</br> problem where the agents are able to dynamically reallocate resources, each agent must specify a value for every combination of bundles and time steps within its time horizon.",
                "Let the variable ψ ∈ Ψm in this case enumerate all possible resource bundles for which at most one bundle may be assigned to agent m at each time step.",
                "Therefore, in this case there are P t∈[1,Tm](2|Ω| )t ∼ 2|Ω|Tm possibilities of resource bundles assigned to different time slots, for the Tm different time horizons.",
                "The same set of equations (3) can be used to solve this dynamic <br>scheduling</br> problem, but the integer program is different because of the difference in how ψ is defined.",
                "In this case, the number of ψ values is exponential in each agents planning horizon Tm, resulting in a much larger program.",
                "This straightforward approach to solving both of these <br>scheduling</br> problems requires an enumeration and solution of either 2|Ω| Tm (static allocation) or P t∈[1,Tm] 2|Ω|t (dynamic reallocation) MDPs for each agent, which very quickly becomes intractable with the growth of the number of resources |Ω| or the time horizon Tm. 3.",
                "MODEL AND PROBLEM STATEMENT We now formally introduce our model of the resourcescheduling problem.",
                "The problem input consists of the following components: • M, Ω, bϕ, τa m, τd m, bτ are as defined above in Section 2.2. • {Θm} = {S, A, pm, rm, αm} are the MDPs of all agents m ∈ M. Without loss of generality, we assume that state and action spaces of all agents are the same, but each has its own transition function pm, reward function rm, and initial conditions αm. • ϕm : A×Ω → {0, 1} is the mapping of actions to resources for agent m. ϕm(a, ω) indicates whether action a of agent m needs resource ω.",
                "An agent m that receives a set of resources that does not include resource ω cannot execute in its MDP policy any action a for which ϕm(a, ω) = 0.",
                "We assume all resource requirements are binary; as discussed below in Section 6, this assumption is not limiting.",
                "Given the above input, the optimization problem we consider is to find the globally optimal-maximizing the sum of expected rewards-mapping of resources to agents for all time steps: Δ : τ × M × Ω → {0, 1}.",
                "A solution is feasible if the corresponding assignment of resources to the agents does not violate the global resource constraint: X m Δm(τ, ω) ≤ bϕ(ω), ∀ω ∈ Ω, τ ∈ [1, bτ]. (4) We consider two flavors of the resource-<br>scheduling</br> problem.",
                "The first formulation restricts resource assignments to the space where the allocation of resources to each agent is static during the agents lifetime.",
                "The second formulation allows reassignment of resources between agents at every time step within their lifetimes.",
                "Figure 1 depicts a resource-<br>scheduling</br> problem with three agents M = {m1, m2, m3}, three resources Ω = {ω1, ω2, ω3}, and a global problem horizon of bτ = 11.",
                "The agents arrival and departure times are shown as gray boxes and are {1, 6}, {3, 7}, and {2, 11}, respectively.",
                "A solution to this problem is shown via horizontal bars within each agents box, where the bars correspond to the allocation of the three resource types.",
                "Figure 1a shows a solution to a static <br>scheduling</br> problem.",
                "According to the shown solution, agent m1 begins the execution of its MDP at time τ = 1 and has a lock on all three resources until it finishes execution at time τ = 3.",
                "Note that agent m1 relinquishes its hold on the resources before its announced departure time of τd m1 = 6, ostensibly because other agents can utilize the resources more effectively.",
                "Thus, at time τ = 4, resources ω1 and ω3 are allocated to agent m2, who then uses them to execute its MDP (using only actions supported by resources ω1 and ω3) until time τ = 7.",
                "Agent m3 holds resource ω3 during the interval τ ∈ [4, 10].",
                "Figure 1b shows a possible solution to the dynamic version of the same problem.",
                "There, resources can be reallocated between agents at every time step.",
                "For example, agent m1 gives up its use of resource ω2 at time τ = 2, although it continues the execution of its MDP until time τ = 6.",
                "Notice that an agent is not allowed to stop and restart its MDP, so agent m1 is only able to continue executing in the interval τ ∈ [3, 4] if it has actions that do not require any resources (ϕm(a, ω) = 0).",
                "Clearly, the model and problem statement described above make a number of assumptions about the problem and the desired solution properties.",
                "We discuss some of those assumptions and their implications in Section 6. 1222 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) (a) (b) Figure 1: Illustration of a solution to a resource-<br>scheduling</br> problem with three agents and three resources: a) static resource assignments (resource assignments are constant within agents lifetimes; b) dynamic assignment (resource assignments are allowed to change at every time step). 4.",
                "RESOURCE <br>scheduling</br> Our resource-<br>scheduling</br> algorithm proceeds in two stages.",
                "First, we perform a preprocessing step that augments the agent MDPs; this process is described in Section 4.1.",
                "Second, using these augmented MDPs we construct a global optimization problem, which is described in Section 4.2. 4.1 Augmenting Agents MDPs In the model described in the previous section, we assume that if an agent does not possess the necessary resources to perform actions in its MDP, its execution is halted and the agent leaves the system.",
                "In other words, the MDPs cannot be paused and resumed.",
                "For example, in the problem shown in Figure 1a, agent m1 releases all resources after time τ = 3, at which point the execution of its MDP is halted.",
                "Similarly, agents m2 and m3 only execute their MDPs in the intervals τ ∈ [4, 6] and τ ∈ [4, 10], respectively.",
                "Therefore, an important part of the global decision-making problem is to decide the window of time during which each of the agents is active (i.e., executing its MDP).",
                "To accomplish this, we augment each agents MDP with two new states (start and finish states sb , sf , respectively) and a new start/stop action a∗ , as illustrated in Figure 2.",
                "The idea is that an agent stays in the start state sb until it is ready to execute its MDP, at which point it performs the start/stop action a∗ and transitions into the state space of the original MDP with the transition probability that corresponds to the original initial distribution α(s).",
                "For example, in Figure 1a, for agent m2 this would happen at time τ = 4.",
                "Once the agent gets to the end of its activity window (time τ = 6 for agent m2 in Figure 1a), it performs the start/stop action, which takes it into the sink finish state sf at time τ = 7.",
                "More precisely, given an MDP S, A, pm, rm, αm , we define an augmented MDP S , A , pm, rm, αm as follows: S = S ∪ sb ∪ sf ; A = A ∪ a∗ ; p (s|sb , a∗ ) = α(s), ∀s ∈ S; p (sb |sb , a) = 1.0, ∀a ∈ A; p (sf |s, a∗ ) = 1.0, ∀s ∈ S; p (σ|s, a) = p(σ|s, a), ∀s, σ ∈ S, a ∈ A; r (sb , a) = r (sf , a) = 0, ∀a ∈ A ; r (s, a) = r(s, a), ∀s ∈ S, a ∈ A; α (sb ) = 1; α (s) = 0, ∀s ∈ S; where all non-specified transition probabilities are assumed to be zero.",
                "Further, in order to account for the new starting state, we begin the MDP one time-step earlier, setting τa m ← τa m − 1.",
                "This will not affect the resource allocation due to the resource constraints only being enforced for the original MDP states, as will be discussed in the next section.",
                "For example, the augmented MDPs shown in Figure 2b (which starts in state sb at time τ = 2) would be constructed from an MDP with original arrival time τ = 3.",
                "Figure 2b also shows a sample trajectory through the state space: the agent starts in state sb , transitions into the state space S of the original MDP, and finally exists into the sink state sf .",
                "Note that if we wanted to model a problem where agents could pause their MDPs at arbitrary time steps (which might be useful for domains where dynamic reallocation is possible), we could easily accomplish this by including an extra action that transitions from each state to itself with zero reward. 4.2 MILP for Resource <br>scheduling</br> Given a set of augmented MDPs, as defined above, the goal of this section is to formulate a global optimization program that solves the resource-<br>scheduling</br> problem.",
                "In this section and below, all MDPs are assumed to be the augmented MDPs as defined in Section 4.1.",
                "Our approach is similar to the idea used in [6]: we begin with the linear-program formulation of agents MDPs (1) and augment it with constraints that ensure that the corresponding resource allocation across agents and time is valid.",
                "The resulting optimization problem then simultaneously solves the agents MDPs and resource-<br>scheduling</br> problems.",
                "In the rest of this section, we incrementally develop a mixed integer program (MILP) that achieves this.",
                "In the absence of resource constraints, the agents finitehorizon MDPs are completely independent, and the globally optimal solution can be trivially obtained via the following LP, which is simply an aggregation of single-agent finitehorizon LPs: max X m X s X a rm(s, a) X t xm(s, a, t) subject to: X a xm(σ, a, t + 1) = X s,a pm(σ|s, a)xm(s, a, t), ∀m ∈ M, σ ∈ S, t ∈ [1, Tm − 1]; X a xm(s, a, 1) = αm(s), ∀m ∈ M, s ∈ S; (12) where xm(s, a, t) is the occupation measure of agent m, and The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1223 (a) (b) Figure 2: Illustration of augmenting an MDP to allow for variable starting and stopping times: a) (left) the original two-state MDP with a single action; (right) the augmented MDP with new states sb and sf and the new action a∗ (note that the origianl transitions are not changed in the augmentation process); b) the augmented MDP displayed as a trajectory through time (grey lines indicate all transitions, while black lines indicate a given trajectory.",
                "Objective Function (sum of expected rewards over all agents) max X m X s X a rm(s, a) X t xm(s, a, t) (5) Meaning Implication Linear Constraints Tie x to θ.",
                "Agent is only active when occupation measure is nonzero in original MDP states. θm(τ) = 0 =⇒ xm(s, a, τ −τa m+1) = 0 ∀s /∈ {sb , sf }, a ∈ A X s/∈{sb,sf } X a xm(s, a, t) ≤ θm(τa m + t − 1) ∀m ∈ M, ∀t ∈ [1, Tm] (6) Agent can only be active in τ ∈ (τa m, τd m) θm(τ) = 0 ∀m ∈ M, τ /∈ (τa m, τd m) (7) Cannot use resources when not active θm(τ) = 0 =⇒ Δm(τ, ω) = 0 ∀τ ∈ [0, bτ], ω ∈ Ω Δm(τ, ω) ≤ θm(τ) ∀m ∈ M, τ ∈ [0, bτ], ω ∈ Ω (8) Tie x to Δ (nonzero x forces corresponding Δ to be nonzero.)",
                "Δm(τ, ω) = 0, ϕm(a, ω) = 1 =⇒ xm(s, a, τ − τa m + 1) = 0 ∀s /∈ {sb , sf } 1/|A| X a ϕm(a, ω) X s/∈{sb,sf } xm(s, a, t) ≤ Δm(t + τa m − 1, ω) ∀m ∈ M, ω ∈ Ω, t ∈ [1, Tm] (9) Resource bounds X m Δm(τ, ω) ≤ bϕ(ω) ∀ω ∈ Ω, τ ∈ [0, bτ] (10) Agent cannot change resources while active.",
                "Only enabled for <br>scheduling</br> with static assignments. θm(τ) = 1 and θm(τ + 1) = 1 =⇒ Δm(τ, ω) = Δm(τ + 1, ω) Δm(τ, ω) − Z(1 − θm(τ + 1)) ≤ Δm(τ + 1, ω) + Z(1 − θm(τ)) Δm(τ, ω) + Z(1 − θm(τ + 1)) ≥ Δm(τ + 1, ω) − Z(1 − θm(τ)) ∀m ∈ M, ω ∈ Ω, τ ∈ [0, bτ] (11) Table 1: MILP for globally optimal resource <br>scheduling</br>.",
                "Tm = τd m − τa m + 1 is the time horizon for the agents MDP.",
                "Using this LP as a basis, we augment it with constraints that ensure that the resource usage implied by the agents occupation measures {xm} does not violate the global resource requirements bϕ at any time step τ ∈ [0, bτ].",
                "To formulate these resource constraints, we use the following binary variables: • Δm(τ, ω) = {0, 1}, ∀m ∈ M, τ ∈ [0, bτ], ω ∈ Ω, which serve as indicator variables that define whether agent m possesses resource ω at time τ.",
                "These are analogous to the static indicator variables used in the one-shot static resource-allocation problem in [6]. • θm = {0, 1}, ∀m ∈ M, τ ∈ [0, bτ] are indicator variables that specify whether agent m is active (i.e., executing its MDP) at time τ.",
                "The meaning of resource-usage variables Δ is illustrated in Figure 1: Δm(τ, ω) = 1 only if resource ω is allocated to agent m at time τ.",
                "The meaning of the activity indicators θ is illustrated in Figure 2b: when agent m is in either the start state sb or the finish state sf , the corresponding θm = 0, but once the agent becomes active and enters one of the other states, we set θm = 1 .",
                "This meaning of θ can be enforced with a linear constraint that synchronizes the values of the agents occupation measures xm and the activity 1224 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) indicators θ, as shown in (6) in Table 1.",
                "Another constraint we have to add-because the activity indicators θ are defined on the global timeline τ-is to enforce the fact that the agent is inactive outside of its arrivaldeparture window.",
                "This is accomplished by constraint (7) in Table 1.",
                "Furthermore, agents should not be using resources while they are inactive.",
                "This constraint can also be enforced via a linear inequality on θ and Δ, as shown in (8).",
                "Constraint (6) sets the value of θ to match the policy defined by the occupation measure xm.",
                "In a similar fashion, we have to make sure that the resource-usage variables Δ are also synchronized with the occupation measure xm.",
                "This is done via constraint (9) in Table 1, which is nearly identical to the analogous constraint from [6].",
                "After implementing the above constraint, which enforces the meaning of Δ, we add a constraint that ensures that the agents resource usage never exceeds the amounts of available resources.",
                "This condition is also trivially expressed as a linear inequality (10) in Table 1.",
                "Finally, for the problem formulation where resource assignments are static during a lifetime of an agent, we add a constraint that ensures that the resource-usage variables Δ do not change their value while the agent is active (θ = 1).",
                "This is accomplished via the linear constraint (11), where Z ≥ 2 is a constant that is used to turn off the constraints when θm(τ) = 0 or θm(τ + 1) = 0.",
                "This constraint is not used for the dynamic problem formulation, where resources can be reallocated between agents at every time step.",
                "To summarize, Table 1 together with the conservationof-flow constraints from (12) defines the MILP that simultaneously computes an optimal resource assignment for all agents across time as well as optimal finite-horizon MDP policies that are valid under that resource assignment.",
                "As a rough measure of the complexity of this MILP, let us consider the number of optimization variables and constraints.",
                "Let TM = P Tm = P m(τa m − τd m + 1) be the sum of the lengths of the arrival-departure windows across all agents.",
                "Then, the number of optimization variables is: TM + bτ|M||Ω| + bτ|M|, TM of which are continuous (xm), and bτ|M||Ω| + bτ|M| are binary (Δ and θ).",
                "However, notice that all but TM|M| of the θ are set to zero by constraint (7), which also immediately forces all but TM|M||Ω| of the Δ to be zero via the constraints (8).",
                "The number of constraints (not including the degenerate constraints in (7)) in the MILP is: TM + TM|Ω| + bτ|Ω| + bτ|M||Ω|.",
                "Despite the fact that the complexity of the MILP is, in the worst case, exponential1 in the number of binary variables, the complexity of this MILP is significantly (exponentially) lower than that of the MILP with flat utility functions, described in Section 2.2.",
                "This result echos the efficiency gains reported in [6] for single-shot resource-allocation problems, but is much more pronounced, because of the explosion of the flat utility representation due to the temporal aspect of the problem (recall the prohibitive complexity of the combinatorial optimization in Section 2.2).",
                "We empirically analyze the performance of this method in Section 5. 1 Strictly speaking, solving MILPs to optimality is NPcomplete in the number of integer variables. 5.",
                "EXPERIMENTAL RESULTS Although the complexity of solving MILPs is in the worst case exponential in the number of integer variables, there are many efficient methods for solving MILPs that allow our algorithm to scale well for parameters common to resource allocation and <br>scheduling</br> problems.",
                "In particular, this section introduces a problem domain-the repairshop problem-used to empirically evaluate our algorithms scalability in terms of the number of agents |M|, the number of shared resources |Ω|, and the varied lengths of global time bτ during which agents may enter and exit the system.",
                "The repairshop problem is a simple parameterized MDP adopting the metaphor of a vehicular repair shop.",
                "Agents in the repair shop are mechanics with a number of independent tasks that yield reward only when completed.",
                "In our MDP model of this system, actions taken to advance through the state space are only allowed if the agent holds certain resources that are publicly available to the shop.",
                "These resources are in finite supply, and optimal policies for the shop will determine when each agent may hold the limited resources to take actions and earn individual rewards.",
                "Each task to be completed is associated with a single action, although the agent is required to repeat the action numerous times before completing the task and earning a reward.",
                "This model was parameterized in terms of the number of agents in the system, the number of different types of resources that could be linked to necessary actions, a global time during which agents are allowed to arrive and depart, and a maximum length for the number of time steps an agent may remain in the system.",
                "All datapoints in our experiments were obtained with 20 evaluations using CPLEX to solve the MILPs on a Pentium4 computer with 2Gb of RAM.",
                "Trials were conducted on both the static and the dynamic version of the resourcescheduling problem, as defined earlier.",
                "Figure 3 shows the runtime and policy value for independent modifications to the parameter set.",
                "The top row shows how the solution time for the MILP scales as we increase the number of agents |M|, the global time horizon bτ, and the number of resources |Ω|.",
                "Increasing the number of agents leads to exponential complexity scaling, which is to be expected for an NP-complete problem.",
                "However, increasing the global time limit bτ or the total number of resource types |Ω|-while holding the number of agents constantdoes not lead to decreased performance.",
                "This occurs because the problems get easier as they become under-constrained, which is also a common phenomenon for NP-complete problems.",
                "We also observe that the solution to the dynamic version of the problem can often be computed much faster than the static version.",
                "The bottom row of Figure 3 shows the joint policy value of the policies that correspond to the computed optimal resource-allocation schedules.",
                "We can observe that the dynamic version yields higher reward (as expected, since the reward for the dynamic version is always no less than the reward of the static version).",
                "We should point out that these graphs should not be viewed as a measure of performance of two different algorithms (both algorithms produce optimal solutions but to different problems), but rather as observations about how the quality of optimal solutions change as more flexibility is allowed in the reallocation of resources.",
                "Figure 4 shows runtime and policy value for trials in which common input variables are scaled together.",
                "This allows The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1225 2 4 6 8 10 10 −3 10 −2 10 −1 10 0 10 1 10 2 10 3 10 4 Number of Agents |M| CPUTime,sec |Ω| = 5, τ = 50 static dynamic 50 100 150 200 10 −2 10 −1 10 0 10 1 10 2 10 3 Global Time Boundary τ CPUTime,sec |M| = 5, |Ω| = 5 static dynamic 10 20 30 40 50 10 −2 10 −1 10 0 10 1 10 2 Number of Resources |Ω| CPUTime,sec |M| = 5, τ = 50 static dynamic 2 4 6 8 10 200 400 600 800 1000 1200 1400 1600 Number of Agents |M| Value |Ω| = 5, τ = 50 static dynamic 50 100 150 200 400 500 600 700 800 900 1000 1100 1200 1300 1400 Global Time Boundary τ Value |M| = 5, |Ω| = 5 static dynamic 10 20 30 40 50 500 600 700 800 900 1000 1100 1200 1300 1400 Number of Resources |Ω| Value |M| = 5, τ = 50 static dynamic Figure 3: Evaluation of our MILP for variable numbers of agents (column 1), lengths of global-time window (column 2), and numbers of resource types (column 3).",
                "Top row shows CPU time, and bottom row shows the joint reward of agents MDP policies.",
                "Error bars show the 1st and 3rd quartiles (25% and 75%). 2 4 6 8 10 10 −3 10 −2 10 −1 10 0 10 1 10 2 10 3 Number of Agents |M| CPUTime,sec τ = 10|M| static dynamic 2 4 6 8 10 10 −3 10 −2 10 −1 10 0 10 1 10 2 10 3 10 4 Number of Agents |M| CPUTime,sec |Ω| = 2|M| static dynamic 2 4 6 8 10 10 −3 10 −2 10 −1 10 0 10 1 10 2 10 3 10 4 Number of Agents |M| CPUTime,sec |Ω| = 5|M| static dynamic 2 4 6 8 10 200 400 600 800 1000 1200 1400 1600 1800 2000 2200 Number of Agents |M| Value τ = 10|M| static dynamic 2 4 6 8 10 200 400 600 800 1000 1200 1400 1600 1800 2000 Number of Agents |M| Value |Ω| = 2|M| static dynamic 2 4 6 8 10 0 500 1000 1500 2000 2500 Number of Agents |M| Value |Ω| = 5|M| static dynamic Figure 4: Evaluation of our MILP using correlated input variables.",
                "The left column tracks the performance and CPU time as the number of agents and global-time window increase together (bτ = 10|M|).",
                "The middle and the right column track the performance and CPU time as the number of resources and the number of agents increase together as |Ω| = 2|M| and |Ω| = 5|M|, respectively.",
                "Error bars show the 1st and 3rd quartiles (25% and 75%). 1226 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) us to explore domains where the total number of agents scales proportionally to the total number of resource types or the global time horizon, while keeping constant the average agent density (per unit of global time) or the average number of resources per agent (which commonly occurs in real-life applications).",
                "Overall, we believe that these experimental results indicate that our MILP formulation can be used to effectively solve resource-<br>scheduling</br> problems of nontrivial size. 6.",
                "DISCUSSION AND CONCLUSIONS Throughout the paper, we have made a number of assumptions in our model and solution algorithm; we discuss their implications below. • Continual execution.",
                "We assume that once an agent stops executing its MDP (transitions into state sf ), it exits the system and cannot return.",
                "It is easy to relax this assumption for domains where agents MDPs can be paused and restarted.",
                "All that is required is to include an additional pause action which transitions from a given state back to itself, and has zero reward. • Indifference to start time.",
                "We used a reward model where agents rewards depend only on the time horizon of their MDPs and not the global start time.",
                "This is a consequence of our MDP-augmentation procedure from Section 4.1.",
                "It is easy to extend the model so that the agents incur an explicit penalty for idling by assigning a non-zero negative reward to the start state sb . • Binary resource requirements.",
                "For simplicity, we have assumed that resource costs are binary: ϕm(a, ω) = {0, 1}, but our results generalize in a straightforward manner to non-binary resource mappings, analogously to the procedure used in [5]. • Cooperative agents.",
                "The optimization procedure discussed in this paper was developed in the context of cooperative agents, but it can also be used to design a mechanism for <br>scheduling</br> resources among self-interested agents.",
                "This optimization procedure can be embedded in a VickreyClarke-Groves auction, completely analogously to the way it was done in [7].",
                "In fact, all the results of [7] about the properties of the auction and information privacy directly carry over to the <br>scheduling</br> domain discussed in this paper, requiring only slight modifications to deal with finitehorizon MDPs. • Known, deterministic arrival and departure times.",
                "Finally, we have assumed that agents arrival and departure times (τa m and τd m) are deterministic and known a priori.",
                "This assumption is fundamental to our solution method.",
                "While there are many domains where this assumption is valid, in many cases agents arrive and depart dynamically and their arrival and departure times can only be predicted probabilistically, leading to online resource-allocation problems.",
                "In particular, in the case of self-interested agents, this becomes an interesting version of an online-mechanism-design problem [11, 12].",
                "In summary, we have presented an MILP formulation for the combinatorial resource-<br>scheduling</br> problem where agents values for possible resource assignments are defined by finitehorizon MDPs.",
                "This result extends previous work ([6, 7]) on static one-shot resource allocation under MDP-induced preferences to resource-<br>scheduling</br> problems with a temporal aspect.",
                "As such, this work takes a step in the direction of designing an online mechanism for agents with combinatorial resource preferences induced by stochastic planning problems.",
                "Relaxing the assumption about deterministic arrival and departure times of the agents is a focus of our future work.",
                "We would like to thank the anonymous reviewers for their insightful comments and suggestions. 7.",
                "REFERENCES [1] E. Altman and A. Shwartz.",
                "Adaptive control of constrained Markov chains: Criteria and policies.",
                "Annals of Operations Research, special issue on Markov Decision Processes, 28:101-134, 1991. [2] R. Bellman.",
                "Dynamic Programming.",
                "Princeton University Press, 1957. [3] C. Boutilier.",
                "Solving concisely expressed combinatorial auction problems.",
                "In Proc. of AAAI-02, pages 359-366, 2002. [4] C. Boutilier and H. H. Hoos.",
                "Bidding languages for combinatorial auctions.",
                "In Proc. of IJCAI-01, pages 1211-1217, 2001. [5] D. Dolgov.",
                "Integrated Resource Allocation and Planning in Stochastic Multiagent Environments.",
                "PhD thesis, Computer Science Department, University of Michigan, February 2006. [6] D. A. Dolgov and E. H. Durfee.",
                "Optimal resource allocation and policy formulation in loosely-coupled Markov decision processes.",
                "In Proc. of ICAPS-04, pages 315-324, June 2004. [7] D. A. Dolgov and E. H. Durfee.",
                "Computationally efficient combinatorial auctions for resource allocation in weakly-coupled MDPs.",
                "In Proc. of AAMAS-05, New York, NY, USA, 2005.",
                "ACM Press. [8] D. A. Dolgov and E. H. Durfee.",
                "Resource allocation among agents with preferences induced by factored MDPs.",
                "In Proc. of AAMAS-06, 2006. [9] K. Larson and T. Sandholm.",
                "Mechanism design and deliberative agents.",
                "In Proc. of AAMAS-05, pages 650-656, New York, NY, USA, 2005.",
                "ACM Press. [10] N. Nisan.",
                "Bidding and allocation in combinatorial auctions.",
                "In Electronic Commerce, 2000. [11] D. C. Parkes and S. Singh.",
                "An MDP-based approach to Online Mechanism Design.",
                "In Proc. of the Seventeenths Annual Conference on Neural Information Processing Systems (NIPS-03), 2003. [12] D. C. Parkes, S. Singh, and D. Yanovsky.",
                "Approximately efficient online mechanism design.",
                "In Proc. of the Eighteenths Annual Conference on Neural Information Processing Systems (NIPS-04), 2004. [13] M. L. Puterman.",
                "Markov Decision Processes.",
                "John Wiley & Sons, New York, 1994. [14] M. H. Rothkopf, A. Pekec, and R. M. Harstad.",
                "Computationally manageable combinational auctions.",
                "Management Science, 44(8):1131-1147, 1998. [15] T. Sandholm.",
                "An algorithm for optimal winner determination in combinatorial auctions.",
                "In Proc. of IJCAI-99, pages 542-547, San Francisco, CA, USA, 1999.",
                "Morgan Kaufmann Publishers Inc.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1227"
            ],
            "original_annotated_samples": [
                "Combinatorial Resource <br>scheduling</br> for Multiagent MDPs Dmitri A. Dolgov, Michael R. James, and Michael E. Samples AI and Robotics Group Technical Research, Toyota Technical Center USA {ddolgov, michael.r.james, michael.samples}@gmail.com ABSTRACT Optimal resource <br>scheduling</br> in multiagent systems is a computationally challenging task, particularly when the values of resources are not additive.",
                "We consider the combinatorial problem of <br>scheduling</br> the usage of multiple resources among agents that operate in stochastic environments, modeled as Markov decision processes (MDPs).",
                "We extend those existing models to the problem of combinatorial resource <br>scheduling</br>, where agents persist only for finite periods between their (predefined) arrival and departure times, requiring resources only for those time periods.",
                "INTRODUCTION The tasks of optimal resource allocation and <br>scheduling</br> are ubiquitous in multiagent systems, but solving such optimization problems can be computationally difficult, due to a number of factors.",
                "In this paper, we extend the work on resource allocation under MDP-induced preferences to discrete-time <br>scheduling</br> problems, where agents are present in the system for finite time intervals and can only use resources within these intervals."
            ],
            "translated_annotated_samples": [
                "La <br>programación de recursos</br> combinatoria para MDPs multiagentes. Dmitri A. Dolgov, Michael R. James y Michael E. Samples. Grupo de Investigación en IA y Robótica, Centro Técnico de Toyota en EE. UU. {ddolgov, michael.r.james, michael.samples}@gmail.com. RESUMEN La programación óptima de recursos en sistemas multiagentes es una tarea computacionalmente desafiante, especialmente cuando los valores de los recursos no son aditivos.",
                "Consideramos el problema combinatorio de <br>programar el uso</br> de múltiples recursos entre agentes que operan en entornos estocásticos, modelados como procesos de decisión de Markov (MDPs).",
                "Extendemos esos modelos existentes al problema de la <br>programación de recursos combinatoria</br>, donde los agentes persisten solo por períodos finitos entre sus tiempos de llegada y salida (predefinidos), requiriendo recursos solo durante esos períodos de tiempo.",
                "INTRODUCCIÓN Las tareas de asignación óptima de recursos y <br>programación</br> son ubicuas en sistemas multiagentes, pero resolver tales problemas de optimización puede ser computacionalmente difícil, debido a una serie de factores.",
                "En este artículo, ampliamos el trabajo sobre asignación de recursos bajo preferencias inducidas por MDP a problemas de <br>programación</br> en tiempo discreto, donde los agentes están presentes en el sistema durante intervalos de tiempo finitos y solo pueden utilizar recursos dentro de estos intervalos."
            ],
            "translated_text": "La <br>programación de recursos</br> combinatoria para MDPs multiagentes. Dmitri A. Dolgov, Michael R. James y Michael E. Samples. Grupo de Investigación en IA y Robótica, Centro Técnico de Toyota en EE. UU. {ddolgov, michael.r.james, michael.samples}@gmail.com. RESUMEN La programación óptima de recursos en sistemas multiagentes es una tarea computacionalmente desafiante, especialmente cuando los valores de los recursos no son aditivos. Consideramos el problema combinatorio de <br>programar el uso</br> de múltiples recursos entre agentes que operan en entornos estocásticos, modelados como procesos de decisión de Markov (MDPs). En los últimos años, se han desarrollado algoritmos eficientes de asignación de recursos para agentes con valores de recursos inducidos por MDPs. Sin embargo, este trabajo previo se ha centrado en problemas de asignación de recursos estáticos donde los recursos se distribuyen una vez y luego se utilizan en MDPs de horizonte infinito. Extendemos esos modelos existentes al problema de la <br>programación de recursos combinatoria</br>, donde los agentes persisten solo por períodos finitos entre sus tiempos de llegada y salida (predefinidos), requiriendo recursos solo durante esos períodos de tiempo. Proporcionamos un procedimiento computacionalmente eficiente para calcular asignaciones de recursos óptimas a nivel global a agentes a lo largo del tiempo. Ilustramos y analizamos empíricamente el método en el contexto de un dominio de programación de trabajos estocásticos. Categorías y Descriptores de Asignaturas I.2.8 [Inteligencia Artificial]: Resolución de Problemas, Métodos de Control y Búsqueda; I.2.11 [Inteligencia Artificial]: Inteligencia Artificial Distribuida-Sistemas Multiagente Términos Generales Algoritmos, Rendimiento, Diseño 1. INTRODUCCIÓN Las tareas de asignación óptima de recursos y <br>programación</br> son ubicuas en sistemas multiagentes, pero resolver tales problemas de optimización puede ser computacionalmente difícil, debido a una serie de factores. En particular, cuando el valor de un conjunto de recursos para un agente no es aditivo (como suele ser el caso con recursos que son sustitutos o complementos), la función de utilidad podría tener que definirse en un espacio exponencialmente grande de combinaciones de recursos, lo que rápidamente se vuelve computacionalmente intratable. Además, incluso cuando cada agente tiene una función de utilidad que es distinta de cero solo en un pequeño subconjunto de los posibles conjuntos de recursos, obtener una asignación óptima sigue siendo computacionalmente prohibitivo, ya que el problema se vuelve NP-completo [14]. Tales problemas computacionales han dado lugar recientemente a varias líneas de trabajo en el uso de modelos compactos de las preferencias de los agentes. Una idea es utilizar cualquier estructura presente en las funciones de utilidad para representarlas de forma compacta, por ejemplo, a través de fórmulas lógicas [15, 10, 4, 3]. Una alternativa es modelar directamente los mecanismos que definen las funciones de utilidad de los agentes y realizar la asignación de recursos directamente con estos modelos [9]. Una forma de lograr esto es modelar los procesos mediante los cuales un agente podría utilizar los recursos y definir la función de utilidad como la ganancia de estos procesos. En particular, si un agente utiliza recursos para actuar en un entorno estocástico, su función de utilidad puede ser modelada de forma natural con un proceso de decisión de Markov, cuyo conjunto de acciones está parametrizado por los recursos disponibles. Esta representación puede ser utilizada para construir algoritmos de asignación de recursos muy eficientes que resultan en una aceleración exponencial sobre un problema de optimización directa con representaciones planas de preferencias combinatorias [6, 7, 8]. Sin embargo, este trabajo existente sobre asignación de recursos con preferencias inducidas por MDPs parametrizados por recursos hace una suposición de que los recursos se asignan solo una vez y luego son utilizados por los agentes de forma independiente dentro de sus MDPs de horizonte infinito. Esta suposición de que no es posible la reasignación de recursos puede ser limitante en dominios donde los agentes llegan y se van dinámicamente. En este artículo, ampliamos el trabajo sobre asignación de recursos bajo preferencias inducidas por MDP a problemas de <br>programación</br> en tiempo discreto, donde los agentes están presentes en el sistema durante intervalos de tiempo finitos y solo pueden utilizar recursos dentro de estos intervalos. ",
            "candidates": [],
            "error": [
                [
                    "programación de recursos",
                    "programar el uso",
                    "programación de recursos combinatoria",
                    "programación",
                    "programación"
                ]
            ]
        },
        "optimization problem": {
            "translated_key": "problema de optimización",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Combinatorial Resource Scheduling for Multiagent MDPs Dmitri A. Dolgov, Michael R. James, and Michael E. Samples AI and Robotics Group Technical Research, Toyota Technical Center USA {ddolgov, michael.r.james, michael.samples}@gmail.com ABSTRACT Optimal resource scheduling in multiagent systems is a computationally challenging task, particularly when the values of resources are not additive.",
                "We consider the combinatorial problem of scheduling the usage of multiple resources among agents that operate in stochastic environments, modeled as Markov decision processes (MDPs).",
                "In recent years, efficient resource-allocation algorithms have been developed for agents with resource values induced by MDPs.",
                "However, this prior work has focused on static resource-allocation problems where resources are distributed once and then utilized in infinite-horizon MDPs.",
                "We extend those existing models to the problem of combinatorial resource scheduling, where agents persist only for finite periods between their (predefined) arrival and departure times, requiring resources only for those time periods.",
                "We provide a computationally efficient procedure for computing globally optimal resource assignments to agents over time.",
                "We illustrate and empirically analyze the method in the context of a stochastic jobscheduling domain.",
                "Categories and Subject Descriptors I.2.8 [Artificial Intelligence]: Problem Solving, Control Methods, and Search; I.2.11 [Artificial Intelligence]: Distributed Artificial Intelligence-Multiagent systems General Terms Algorithms, Performance, Design 1.",
                "INTRODUCTION The tasks of optimal resource allocation and scheduling are ubiquitous in multiagent systems, but solving such optimization problems can be computationally difficult, due to a number of factors.",
                "In particular, when the value of a set of resources to an agent is not additive (as is often the case with resources that are substitutes or complements), the utility function might have to be defined on an exponentially large space of resource bundles, which very quickly becomes computationally intractable.",
                "Further, even when each agent has a utility function that is nonzero only on a small subset of the possible resource bundles, obtaining optimal allocation is still computationally prohibitive, as the problem becomes NP-complete [14].",
                "Such computational issues have recently spawned several threads of work in using compact models of agents preferences.",
                "One idea is to use any structure present in utility functions to represent them compactly, via, for example, logical formulas [15, 10, 4, 3].",
                "An alternative is to directly model the mechanisms that define the agents utility functions and perform resource allocation directly with these models [9].",
                "A way of accomplishing this is to model the processes by which an agent might utilize the resources and define the utility function as the payoff of these processes.",
                "In particular, if an agent uses resources to act in a stochastic environment, its utility function can be naturally modeled with a Markov decision process, whose action set is parameterized by the available resources.",
                "This representation can then be used to construct very efficient resource-allocation algorithms that lead to an exponential speedup over a straightforward <br>optimization problem</br> with flat representations of combinatorial preferences [6, 7, 8].",
                "However, this existing work on resource allocation with preferences induced by resource-parameterized MDPs makes an assumption that the resources are only allocated once and are then utilized by the agents independently within their infinite-horizon MDPs.",
                "This assumption that no reallocation of resources is possible can be limiting in domains where agents arrive and depart dynamically.",
                "In this paper, we extend the work on resource allocation under MDP-induced preferences to discrete-time scheduling problems, where agents are present in the system for finite time intervals and can only use resources within these intervals.",
                "In particular, agents arrive and depart at arbitrary (predefined) times and within these intervals use resources to execute tasks in finite-horizon MDPs.",
                "We address the problem of globally optimal resource scheduling, where the objective is to find an allocation of resources to the agents across time that maximizes the sum of the expected rewards that they obtain.",
                "In this context, our main contribution is a mixed-integerprogramming formulation of the scheduling problem that chooses globally optimal resource assignments, starting times, and execution horizons for all agents (within their arrival1220 978-81-904262-7-5 (RPS) c 2007 IFAAMAS departure intervals).",
                "We analyze and empirically compare two flavors of the scheduling problem: one, where agents have static resource assignments within their finite-horizon MDPs, and another, where resources can be dynamically reallocated between agents at every time step.",
                "In the rest of the paper, we first lay down the necessary groundwork in Section 2 and then introduce our model and formal problem statement in Section 3.",
                "In Section 4.2, we describe our main result, the optimization program for globally optimal resource scheduling.",
                "Following the discussion of our experimental results on a job-scheduling problem in Section 5, we conclude in Section 6 with a discussion of possible extensions and generalizations of our method. 2.",
                "BACKGROUND Similarly to the model used in previous work on resourceallocation with MDP-induced preferences [6, 7], we define the value of a set of resources to an agent as the value of the best MDP policy that is realizable, given those resources.",
                "However, since the focus of our work is on scheduling problems, and a large part of the <br>optimization problem</br> is to decide how resources are allocated in time among agents with finite arrival and departure times, we model the agents planning problems as finite-horizon MDPs, in contrast to previous work that used infinite-horizon discounted MDPs.",
                "In the rest of this section, we first introduce some necessary background on finite-horizon MDPs and present a linear-programming formulation that serves as the basis for our solution algorithm developed in Section 4.",
                "We also outline the standard methods for combinatorial resource scheduling with flat resource values, which serve as a comparison benchmark for the new model developed here. 2.1 Markov Decision Processes A stationary, finite-domain, discrete-time MDP (see, for example, [13] for a thorough and detailed development) can be described as S, A, p, r , where: S is a finite set of system states; A is a finite set of actions that are available to the agent; p is a stationary stochastic transition function, where p(σ|s, a) is the probability of transitioning to state σ upon executing action a in state s; r is a stationary reward function, where r(s, a) specifies the reward obtained upon executing action a in state s. Given such an MDP, a decision problem under a finite horizon T is to choose an optimal action at every time step to maximize the expected value of the total reward accrued during the agents (finite) lifetime.",
                "The agents optimal policy is then a function of current state s and the time until the horizon.",
                "An optimal policy for such a problem is to act greedily with respect to the optimal value function, defined recursively by the following system of finite-time Bellman equations [2]: v(s, t) = max a r(s, a) + X σ p(σ|s, a)v(σ, t + 1), ∀s ∈ S, t ∈ [1, T − 1]; v(s, T) = 0, ∀s ∈ S; where v(s, t) is the optimal value of being in state s at time t ∈ [1, T].",
                "This optimal value function can be easily computed using dynamic programming, leading to the following optimal policy π, where π(s, a, t) is the probability of executing action a in state s at time t: π(s, a, t) = ( 1, a = argmaxa r(s, a) + P σ p(σ|s, a)v(σ, t + 1), 0, otherwise.",
                "The above is the most common way of computing the optimal value function (and therefore an optimal policy) for a finite-horizon MDP.",
                "However, we can also formulate the problem as the following linear program (similarly to the dual LP for infinite-horizon discounted MDPs [13, 6, 7]): max X s X a r(s, a) X t x(s, a, t) subject to: X a x(σ, a, t + 1) = X s,a p(σ|s, a)x(s, a, t) ∀σ, t ∈ [1, T − 1]; X a x(s, a, 1) = α(s), ∀s ∈ S; (1) where α(s) is the initial distribution over the state space, and x is the (non-stationary) occupation measure (x(s, a, t) ∈ [0, 1] is the total expected number of times action a is executed in state s at time t).",
                "An optimal (non-stationary) policy is obtained from the occupation measure as follows: π(s, a, t) = x(s, a, t)/ X a x(s, a, t) ∀s ∈ S, t ∈ [1, T]. (2) Note that the standard unconstrained finite-horizon MDP, as described above, always has a uniformly-optimal solution (optimal for any initial distribution α(s)).",
                "Therefore, an optimal policy can be obtained by using an arbitrary constant α(s) > 0 (in particular, α(s) = 1 will result in x(s, a, t) = π(s, a, t)).",
                "However, for MDPs with resource constraints (as defined below in Section 3), uniformly-optimal policies do not in general exist.",
                "In such cases, α becomes a part of the problem input, and a resulting policy is only optimal for that particular α.",
                "This result is well known for infinite-horizon MDPs with various types of constraints [1, 6], and it also holds for our finite-horizon model, which can be easily established via a line of reasoning completely analogous to the arguments in [6]. 2.2 Combinatorial Resource Scheduling A straightforward approach to resource scheduling for a set of agents M, whose values for the resources are induced by stochastic planning problems (in our case, finite-horizon MDPs) would be to have each agent enumerate all possible resource assignments over time and, for each one, compute its value by solving the corresponding MDP.",
                "Then, each agent would provide valuations for each possible resource bundle over time to a centralized coordinator, who would compute the optimal resource assignments across time based on these valuations.",
                "When resources can be allocated at different times to different agents, each agent must submit valuations for every combination of possible time horizons.",
                "Let each agent m ∈ M execute its MDP within the arrival-departure time interval τ ∈ [τa m, τd m].",
                "Hence, agent m will execute an MDP with time horizon no greater than Tm = τd m−τa m+1.",
                "Let bτ be the global time horizon for the problem, before which all of the agents MDPs must finish.",
                "We assume τd m < bτ, ∀m ∈ M. The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1221 For the scheduling problem where agents have static resource requirements within their finite-horizon MDPs, the agents provide a valuation for each resource bundle for each possible time horizon (from [1, Tm]) that they may use.",
                "Let Ω be the set of resources to be allocated among the agents.",
                "An agent will get at most one resource bundle for one of the time horizons.",
                "Let the variable ψ ∈ Ψm enumerate all possible pairs of resource bundles and time horizons for agent m, so there are 2|Ω| × Tm values for ψ (the space of bundles is exponential in the number of resource types |Ω|).",
                "The agent m must provide a value vψ m for each ψ, and the coordinator will allocate at most one ψ (resource, time horizon) pair to each agent.",
                "This allocation is expressed as an indicator variable zψ m ∈ {0, 1} that shows whether ψ is assigned to agent m. For time τ and resource ω, the function nm(ψ, τ, ω) ∈ {0, 1} indicates whether the bundle in ψ uses resource ω at time τ (we make the assumption that agents have binary resource requirements).",
                "This allocation problem is NP-complete, even when considering only a single time step, and its difficulty increases significantly with multiple time steps because of the increasing number of values of ψ.",
                "The problem of finding an optimal allocation that satisfies the global constraint that the amount of each resource ω allocated to all agents does not exceed the available amount bϕ(ω) can be expressed as the following integer program: max X m∈M X ψ∈Ψm zψ mvψ m subject to: X ψ∈Ψm zψ m ≤ 1, ∀m ∈ M; X m∈M X ψ∈Ψm zψ mnm(ψ, τ, ω) ≤ bϕ(ω), ∀τ ∈ [1, bτ], ∀ω ∈ Ω; (3) The first constraint in equation 3 says that no agent can receive more than one bundle, and the second constraint ensures that the total assignment of resource ω does not, at any time, exceed the resource bound.",
                "For the scheduling problem where the agents are able to dynamically reallocate resources, each agent must specify a value for every combination of bundles and time steps within its time horizon.",
                "Let the variable ψ ∈ Ψm in this case enumerate all possible resource bundles for which at most one bundle may be assigned to agent m at each time step.",
                "Therefore, in this case there are P t∈[1,Tm](2|Ω| )t ∼ 2|Ω|Tm possibilities of resource bundles assigned to different time slots, for the Tm different time horizons.",
                "The same set of equations (3) can be used to solve this dynamic scheduling problem, but the integer program is different because of the difference in how ψ is defined.",
                "In this case, the number of ψ values is exponential in each agents planning horizon Tm, resulting in a much larger program.",
                "This straightforward approach to solving both of these scheduling problems requires an enumeration and solution of either 2|Ω| Tm (static allocation) or P t∈[1,Tm] 2|Ω|t (dynamic reallocation) MDPs for each agent, which very quickly becomes intractable with the growth of the number of resources |Ω| or the time horizon Tm. 3.",
                "MODEL AND PROBLEM STATEMENT We now formally introduce our model of the resourcescheduling problem.",
                "The problem input consists of the following components: • M, Ω, bϕ, τa m, τd m, bτ are as defined above in Section 2.2. • {Θm} = {S, A, pm, rm, αm} are the MDPs of all agents m ∈ M. Without loss of generality, we assume that state and action spaces of all agents are the same, but each has its own transition function pm, reward function rm, and initial conditions αm. • ϕm : A×Ω → {0, 1} is the mapping of actions to resources for agent m. ϕm(a, ω) indicates whether action a of agent m needs resource ω.",
                "An agent m that receives a set of resources that does not include resource ω cannot execute in its MDP policy any action a for which ϕm(a, ω) = 0.",
                "We assume all resource requirements are binary; as discussed below in Section 6, this assumption is not limiting.",
                "Given the above input, the <br>optimization problem</br> we consider is to find the globally optimal-maximizing the sum of expected rewards-mapping of resources to agents for all time steps: Δ : τ × M × Ω → {0, 1}.",
                "A solution is feasible if the corresponding assignment of resources to the agents does not violate the global resource constraint: X m Δm(τ, ω) ≤ bϕ(ω), ∀ω ∈ Ω, τ ∈ [1, bτ]. (4) We consider two flavors of the resource-scheduling problem.",
                "The first formulation restricts resource assignments to the space where the allocation of resources to each agent is static during the agents lifetime.",
                "The second formulation allows reassignment of resources between agents at every time step within their lifetimes.",
                "Figure 1 depicts a resource-scheduling problem with three agents M = {m1, m2, m3}, three resources Ω = {ω1, ω2, ω3}, and a global problem horizon of bτ = 11.",
                "The agents arrival and departure times are shown as gray boxes and are {1, 6}, {3, 7}, and {2, 11}, respectively.",
                "A solution to this problem is shown via horizontal bars within each agents box, where the bars correspond to the allocation of the three resource types.",
                "Figure 1a shows a solution to a static scheduling problem.",
                "According to the shown solution, agent m1 begins the execution of its MDP at time τ = 1 and has a lock on all three resources until it finishes execution at time τ = 3.",
                "Note that agent m1 relinquishes its hold on the resources before its announced departure time of τd m1 = 6, ostensibly because other agents can utilize the resources more effectively.",
                "Thus, at time τ = 4, resources ω1 and ω3 are allocated to agent m2, who then uses them to execute its MDP (using only actions supported by resources ω1 and ω3) until time τ = 7.",
                "Agent m3 holds resource ω3 during the interval τ ∈ [4, 10].",
                "Figure 1b shows a possible solution to the dynamic version of the same problem.",
                "There, resources can be reallocated between agents at every time step.",
                "For example, agent m1 gives up its use of resource ω2 at time τ = 2, although it continues the execution of its MDP until time τ = 6.",
                "Notice that an agent is not allowed to stop and restart its MDP, so agent m1 is only able to continue executing in the interval τ ∈ [3, 4] if it has actions that do not require any resources (ϕm(a, ω) = 0).",
                "Clearly, the model and problem statement described above make a number of assumptions about the problem and the desired solution properties.",
                "We discuss some of those assumptions and their implications in Section 6. 1222 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) (a) (b) Figure 1: Illustration of a solution to a resource-scheduling problem with three agents and three resources: a) static resource assignments (resource assignments are constant within agents lifetimes; b) dynamic assignment (resource assignments are allowed to change at every time step). 4.",
                "RESOURCE SCHEDULING Our resource-scheduling algorithm proceeds in two stages.",
                "First, we perform a preprocessing step that augments the agent MDPs; this process is described in Section 4.1.",
                "Second, using these augmented MDPs we construct a global <br>optimization problem</br>, which is described in Section 4.2. 4.1 Augmenting Agents MDPs In the model described in the previous section, we assume that if an agent does not possess the necessary resources to perform actions in its MDP, its execution is halted and the agent leaves the system.",
                "In other words, the MDPs cannot be paused and resumed.",
                "For example, in the problem shown in Figure 1a, agent m1 releases all resources after time τ = 3, at which point the execution of its MDP is halted.",
                "Similarly, agents m2 and m3 only execute their MDPs in the intervals τ ∈ [4, 6] and τ ∈ [4, 10], respectively.",
                "Therefore, an important part of the global decision-making problem is to decide the window of time during which each of the agents is active (i.e., executing its MDP).",
                "To accomplish this, we augment each agents MDP with two new states (start and finish states sb , sf , respectively) and a new start/stop action a∗ , as illustrated in Figure 2.",
                "The idea is that an agent stays in the start state sb until it is ready to execute its MDP, at which point it performs the start/stop action a∗ and transitions into the state space of the original MDP with the transition probability that corresponds to the original initial distribution α(s).",
                "For example, in Figure 1a, for agent m2 this would happen at time τ = 4.",
                "Once the agent gets to the end of its activity window (time τ = 6 for agent m2 in Figure 1a), it performs the start/stop action, which takes it into the sink finish state sf at time τ = 7.",
                "More precisely, given an MDP S, A, pm, rm, αm , we define an augmented MDP S , A , pm, rm, αm as follows: S = S ∪ sb ∪ sf ; A = A ∪ a∗ ; p (s|sb , a∗ ) = α(s), ∀s ∈ S; p (sb |sb , a) = 1.0, ∀a ∈ A; p (sf |s, a∗ ) = 1.0, ∀s ∈ S; p (σ|s, a) = p(σ|s, a), ∀s, σ ∈ S, a ∈ A; r (sb , a) = r (sf , a) = 0, ∀a ∈ A ; r (s, a) = r(s, a), ∀s ∈ S, a ∈ A; α (sb ) = 1; α (s) = 0, ∀s ∈ S; where all non-specified transition probabilities are assumed to be zero.",
                "Further, in order to account for the new starting state, we begin the MDP one time-step earlier, setting τa m ← τa m − 1.",
                "This will not affect the resource allocation due to the resource constraints only being enforced for the original MDP states, as will be discussed in the next section.",
                "For example, the augmented MDPs shown in Figure 2b (which starts in state sb at time τ = 2) would be constructed from an MDP with original arrival time τ = 3.",
                "Figure 2b also shows a sample trajectory through the state space: the agent starts in state sb , transitions into the state space S of the original MDP, and finally exists into the sink state sf .",
                "Note that if we wanted to model a problem where agents could pause their MDPs at arbitrary time steps (which might be useful for domains where dynamic reallocation is possible), we could easily accomplish this by including an extra action that transitions from each state to itself with zero reward. 4.2 MILP for Resource Scheduling Given a set of augmented MDPs, as defined above, the goal of this section is to formulate a global optimization program that solves the resource-scheduling problem.",
                "In this section and below, all MDPs are assumed to be the augmented MDPs as defined in Section 4.1.",
                "Our approach is similar to the idea used in [6]: we begin with the linear-program formulation of agents MDPs (1) and augment it with constraints that ensure that the corresponding resource allocation across agents and time is valid.",
                "The resulting <br>optimization problem</br> then simultaneously solves the agents MDPs and resource-scheduling problems.",
                "In the rest of this section, we incrementally develop a mixed integer program (MILP) that achieves this.",
                "In the absence of resource constraints, the agents finitehorizon MDPs are completely independent, and the globally optimal solution can be trivially obtained via the following LP, which is simply an aggregation of single-agent finitehorizon LPs: max X m X s X a rm(s, a) X t xm(s, a, t) subject to: X a xm(σ, a, t + 1) = X s,a pm(σ|s, a)xm(s, a, t), ∀m ∈ M, σ ∈ S, t ∈ [1, Tm − 1]; X a xm(s, a, 1) = αm(s), ∀m ∈ M, s ∈ S; (12) where xm(s, a, t) is the occupation measure of agent m, and The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1223 (a) (b) Figure 2: Illustration of augmenting an MDP to allow for variable starting and stopping times: a) (left) the original two-state MDP with a single action; (right) the augmented MDP with new states sb and sf and the new action a∗ (note that the origianl transitions are not changed in the augmentation process); b) the augmented MDP displayed as a trajectory through time (grey lines indicate all transitions, while black lines indicate a given trajectory.",
                "Objective Function (sum of expected rewards over all agents) max X m X s X a rm(s, a) X t xm(s, a, t) (5) Meaning Implication Linear Constraints Tie x to θ.",
                "Agent is only active when occupation measure is nonzero in original MDP states. θm(τ) = 0 =⇒ xm(s, a, τ −τa m+1) = 0 ∀s /∈ {sb , sf }, a ∈ A X s/∈{sb,sf } X a xm(s, a, t) ≤ θm(τa m + t − 1) ∀m ∈ M, ∀t ∈ [1, Tm] (6) Agent can only be active in τ ∈ (τa m, τd m) θm(τ) = 0 ∀m ∈ M, τ /∈ (τa m, τd m) (7) Cannot use resources when not active θm(τ) = 0 =⇒ Δm(τ, ω) = 0 ∀τ ∈ [0, bτ], ω ∈ Ω Δm(τ, ω) ≤ θm(τ) ∀m ∈ M, τ ∈ [0, bτ], ω ∈ Ω (8) Tie x to Δ (nonzero x forces corresponding Δ to be nonzero.)",
                "Δm(τ, ω) = 0, ϕm(a, ω) = 1 =⇒ xm(s, a, τ − τa m + 1) = 0 ∀s /∈ {sb , sf } 1/|A| X a ϕm(a, ω) X s/∈{sb,sf } xm(s, a, t) ≤ Δm(t + τa m − 1, ω) ∀m ∈ M, ω ∈ Ω, t ∈ [1, Tm] (9) Resource bounds X m Δm(τ, ω) ≤ bϕ(ω) ∀ω ∈ Ω, τ ∈ [0, bτ] (10) Agent cannot change resources while active.",
                "Only enabled for scheduling with static assignments. θm(τ) = 1 and θm(τ + 1) = 1 =⇒ Δm(τ, ω) = Δm(τ + 1, ω) Δm(τ, ω) − Z(1 − θm(τ + 1)) ≤ Δm(τ + 1, ω) + Z(1 − θm(τ)) Δm(τ, ω) + Z(1 − θm(τ + 1)) ≥ Δm(τ + 1, ω) − Z(1 − θm(τ)) ∀m ∈ M, ω ∈ Ω, τ ∈ [0, bτ] (11) Table 1: MILP for globally optimal resource scheduling.",
                "Tm = τd m − τa m + 1 is the time horizon for the agents MDP.",
                "Using this LP as a basis, we augment it with constraints that ensure that the resource usage implied by the agents occupation measures {xm} does not violate the global resource requirements bϕ at any time step τ ∈ [0, bτ].",
                "To formulate these resource constraints, we use the following binary variables: • Δm(τ, ω) = {0, 1}, ∀m ∈ M, τ ∈ [0, bτ], ω ∈ Ω, which serve as indicator variables that define whether agent m possesses resource ω at time τ.",
                "These are analogous to the static indicator variables used in the one-shot static resource-allocation problem in [6]. • θm = {0, 1}, ∀m ∈ M, τ ∈ [0, bτ] are indicator variables that specify whether agent m is active (i.e., executing its MDP) at time τ.",
                "The meaning of resource-usage variables Δ is illustrated in Figure 1: Δm(τ, ω) = 1 only if resource ω is allocated to agent m at time τ.",
                "The meaning of the activity indicators θ is illustrated in Figure 2b: when agent m is in either the start state sb or the finish state sf , the corresponding θm = 0, but once the agent becomes active and enters one of the other states, we set θm = 1 .",
                "This meaning of θ can be enforced with a linear constraint that synchronizes the values of the agents occupation measures xm and the activity 1224 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) indicators θ, as shown in (6) in Table 1.",
                "Another constraint we have to add-because the activity indicators θ are defined on the global timeline τ-is to enforce the fact that the agent is inactive outside of its arrivaldeparture window.",
                "This is accomplished by constraint (7) in Table 1.",
                "Furthermore, agents should not be using resources while they are inactive.",
                "This constraint can also be enforced via a linear inequality on θ and Δ, as shown in (8).",
                "Constraint (6) sets the value of θ to match the policy defined by the occupation measure xm.",
                "In a similar fashion, we have to make sure that the resource-usage variables Δ are also synchronized with the occupation measure xm.",
                "This is done via constraint (9) in Table 1, which is nearly identical to the analogous constraint from [6].",
                "After implementing the above constraint, which enforces the meaning of Δ, we add a constraint that ensures that the agents resource usage never exceeds the amounts of available resources.",
                "This condition is also trivially expressed as a linear inequality (10) in Table 1.",
                "Finally, for the problem formulation where resource assignments are static during a lifetime of an agent, we add a constraint that ensures that the resource-usage variables Δ do not change their value while the agent is active (θ = 1).",
                "This is accomplished via the linear constraint (11), where Z ≥ 2 is a constant that is used to turn off the constraints when θm(τ) = 0 or θm(τ + 1) = 0.",
                "This constraint is not used for the dynamic problem formulation, where resources can be reallocated between agents at every time step.",
                "To summarize, Table 1 together with the conservationof-flow constraints from (12) defines the MILP that simultaneously computes an optimal resource assignment for all agents across time as well as optimal finite-horizon MDP policies that are valid under that resource assignment.",
                "As a rough measure of the complexity of this MILP, let us consider the number of optimization variables and constraints.",
                "Let TM = P Tm = P m(τa m − τd m + 1) be the sum of the lengths of the arrival-departure windows across all agents.",
                "Then, the number of optimization variables is: TM + bτ|M||Ω| + bτ|M|, TM of which are continuous (xm), and bτ|M||Ω| + bτ|M| are binary (Δ and θ).",
                "However, notice that all but TM|M| of the θ are set to zero by constraint (7), which also immediately forces all but TM|M||Ω| of the Δ to be zero via the constraints (8).",
                "The number of constraints (not including the degenerate constraints in (7)) in the MILP is: TM + TM|Ω| + bτ|Ω| + bτ|M||Ω|.",
                "Despite the fact that the complexity of the MILP is, in the worst case, exponential1 in the number of binary variables, the complexity of this MILP is significantly (exponentially) lower than that of the MILP with flat utility functions, described in Section 2.2.",
                "This result echos the efficiency gains reported in [6] for single-shot resource-allocation problems, but is much more pronounced, because of the explosion of the flat utility representation due to the temporal aspect of the problem (recall the prohibitive complexity of the combinatorial optimization in Section 2.2).",
                "We empirically analyze the performance of this method in Section 5. 1 Strictly speaking, solving MILPs to optimality is NPcomplete in the number of integer variables. 5.",
                "EXPERIMENTAL RESULTS Although the complexity of solving MILPs is in the worst case exponential in the number of integer variables, there are many efficient methods for solving MILPs that allow our algorithm to scale well for parameters common to resource allocation and scheduling problems.",
                "In particular, this section introduces a problem domain-the repairshop problem-used to empirically evaluate our algorithms scalability in terms of the number of agents |M|, the number of shared resources |Ω|, and the varied lengths of global time bτ during which agents may enter and exit the system.",
                "The repairshop problem is a simple parameterized MDP adopting the metaphor of a vehicular repair shop.",
                "Agents in the repair shop are mechanics with a number of independent tasks that yield reward only when completed.",
                "In our MDP model of this system, actions taken to advance through the state space are only allowed if the agent holds certain resources that are publicly available to the shop.",
                "These resources are in finite supply, and optimal policies for the shop will determine when each agent may hold the limited resources to take actions and earn individual rewards.",
                "Each task to be completed is associated with a single action, although the agent is required to repeat the action numerous times before completing the task and earning a reward.",
                "This model was parameterized in terms of the number of agents in the system, the number of different types of resources that could be linked to necessary actions, a global time during which agents are allowed to arrive and depart, and a maximum length for the number of time steps an agent may remain in the system.",
                "All datapoints in our experiments were obtained with 20 evaluations using CPLEX to solve the MILPs on a Pentium4 computer with 2Gb of RAM.",
                "Trials were conducted on both the static and the dynamic version of the resourcescheduling problem, as defined earlier.",
                "Figure 3 shows the runtime and policy value for independent modifications to the parameter set.",
                "The top row shows how the solution time for the MILP scales as we increase the number of agents |M|, the global time horizon bτ, and the number of resources |Ω|.",
                "Increasing the number of agents leads to exponential complexity scaling, which is to be expected for an NP-complete problem.",
                "However, increasing the global time limit bτ or the total number of resource types |Ω|-while holding the number of agents constantdoes not lead to decreased performance.",
                "This occurs because the problems get easier as they become under-constrained, which is also a common phenomenon for NP-complete problems.",
                "We also observe that the solution to the dynamic version of the problem can often be computed much faster than the static version.",
                "The bottom row of Figure 3 shows the joint policy value of the policies that correspond to the computed optimal resource-allocation schedules.",
                "We can observe that the dynamic version yields higher reward (as expected, since the reward for the dynamic version is always no less than the reward of the static version).",
                "We should point out that these graphs should not be viewed as a measure of performance of two different algorithms (both algorithms produce optimal solutions but to different problems), but rather as observations about how the quality of optimal solutions change as more flexibility is allowed in the reallocation of resources.",
                "Figure 4 shows runtime and policy value for trials in which common input variables are scaled together.",
                "This allows The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1225 2 4 6 8 10 10 −3 10 −2 10 −1 10 0 10 1 10 2 10 3 10 4 Number of Agents |M| CPUTime,sec |Ω| = 5, τ = 50 static dynamic 50 100 150 200 10 −2 10 −1 10 0 10 1 10 2 10 3 Global Time Boundary τ CPUTime,sec |M| = 5, |Ω| = 5 static dynamic 10 20 30 40 50 10 −2 10 −1 10 0 10 1 10 2 Number of Resources |Ω| CPUTime,sec |M| = 5, τ = 50 static dynamic 2 4 6 8 10 200 400 600 800 1000 1200 1400 1600 Number of Agents |M| Value |Ω| = 5, τ = 50 static dynamic 50 100 150 200 400 500 600 700 800 900 1000 1100 1200 1300 1400 Global Time Boundary τ Value |M| = 5, |Ω| = 5 static dynamic 10 20 30 40 50 500 600 700 800 900 1000 1100 1200 1300 1400 Number of Resources |Ω| Value |M| = 5, τ = 50 static dynamic Figure 3: Evaluation of our MILP for variable numbers of agents (column 1), lengths of global-time window (column 2), and numbers of resource types (column 3).",
                "Top row shows CPU time, and bottom row shows the joint reward of agents MDP policies.",
                "Error bars show the 1st and 3rd quartiles (25% and 75%). 2 4 6 8 10 10 −3 10 −2 10 −1 10 0 10 1 10 2 10 3 Number of Agents |M| CPUTime,sec τ = 10|M| static dynamic 2 4 6 8 10 10 −3 10 −2 10 −1 10 0 10 1 10 2 10 3 10 4 Number of Agents |M| CPUTime,sec |Ω| = 2|M| static dynamic 2 4 6 8 10 10 −3 10 −2 10 −1 10 0 10 1 10 2 10 3 10 4 Number of Agents |M| CPUTime,sec |Ω| = 5|M| static dynamic 2 4 6 8 10 200 400 600 800 1000 1200 1400 1600 1800 2000 2200 Number of Agents |M| Value τ = 10|M| static dynamic 2 4 6 8 10 200 400 600 800 1000 1200 1400 1600 1800 2000 Number of Agents |M| Value |Ω| = 2|M| static dynamic 2 4 6 8 10 0 500 1000 1500 2000 2500 Number of Agents |M| Value |Ω| = 5|M| static dynamic Figure 4: Evaluation of our MILP using correlated input variables.",
                "The left column tracks the performance and CPU time as the number of agents and global-time window increase together (bτ = 10|M|).",
                "The middle and the right column track the performance and CPU time as the number of resources and the number of agents increase together as |Ω| = 2|M| and |Ω| = 5|M|, respectively.",
                "Error bars show the 1st and 3rd quartiles (25% and 75%). 1226 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) us to explore domains where the total number of agents scales proportionally to the total number of resource types or the global time horizon, while keeping constant the average agent density (per unit of global time) or the average number of resources per agent (which commonly occurs in real-life applications).",
                "Overall, we believe that these experimental results indicate that our MILP formulation can be used to effectively solve resource-scheduling problems of nontrivial size. 6.",
                "DISCUSSION AND CONCLUSIONS Throughout the paper, we have made a number of assumptions in our model and solution algorithm; we discuss their implications below. • Continual execution.",
                "We assume that once an agent stops executing its MDP (transitions into state sf ), it exits the system and cannot return.",
                "It is easy to relax this assumption for domains where agents MDPs can be paused and restarted.",
                "All that is required is to include an additional pause action which transitions from a given state back to itself, and has zero reward. • Indifference to start time.",
                "We used a reward model where agents rewards depend only on the time horizon of their MDPs and not the global start time.",
                "This is a consequence of our MDP-augmentation procedure from Section 4.1.",
                "It is easy to extend the model so that the agents incur an explicit penalty for idling by assigning a non-zero negative reward to the start state sb . • Binary resource requirements.",
                "For simplicity, we have assumed that resource costs are binary: ϕm(a, ω) = {0, 1}, but our results generalize in a straightforward manner to non-binary resource mappings, analogously to the procedure used in [5]. • Cooperative agents.",
                "The optimization procedure discussed in this paper was developed in the context of cooperative agents, but it can also be used to design a mechanism for scheduling resources among self-interested agents.",
                "This optimization procedure can be embedded in a VickreyClarke-Groves auction, completely analogously to the way it was done in [7].",
                "In fact, all the results of [7] about the properties of the auction and information privacy directly carry over to the scheduling domain discussed in this paper, requiring only slight modifications to deal with finitehorizon MDPs. • Known, deterministic arrival and departure times.",
                "Finally, we have assumed that agents arrival and departure times (τa m and τd m) are deterministic and known a priori.",
                "This assumption is fundamental to our solution method.",
                "While there are many domains where this assumption is valid, in many cases agents arrive and depart dynamically and their arrival and departure times can only be predicted probabilistically, leading to online resource-allocation problems.",
                "In particular, in the case of self-interested agents, this becomes an interesting version of an online-mechanism-design problem [11, 12].",
                "In summary, we have presented an MILP formulation for the combinatorial resource-scheduling problem where agents values for possible resource assignments are defined by finitehorizon MDPs.",
                "This result extends previous work ([6, 7]) on static one-shot resource allocation under MDP-induced preferences to resource-scheduling problems with a temporal aspect.",
                "As such, this work takes a step in the direction of designing an online mechanism for agents with combinatorial resource preferences induced by stochastic planning problems.",
                "Relaxing the assumption about deterministic arrival and departure times of the agents is a focus of our future work.",
                "We would like to thank the anonymous reviewers for their insightful comments and suggestions. 7.",
                "REFERENCES [1] E. Altman and A. Shwartz.",
                "Adaptive control of constrained Markov chains: Criteria and policies.",
                "Annals of Operations Research, special issue on Markov Decision Processes, 28:101-134, 1991. [2] R. Bellman.",
                "Dynamic Programming.",
                "Princeton University Press, 1957. [3] C. Boutilier.",
                "Solving concisely expressed combinatorial auction problems.",
                "In Proc. of AAAI-02, pages 359-366, 2002. [4] C. Boutilier and H. H. Hoos.",
                "Bidding languages for combinatorial auctions.",
                "In Proc. of IJCAI-01, pages 1211-1217, 2001. [5] D. Dolgov.",
                "Integrated Resource Allocation and Planning in Stochastic Multiagent Environments.",
                "PhD thesis, Computer Science Department, University of Michigan, February 2006. [6] D. A. Dolgov and E. H. Durfee.",
                "Optimal resource allocation and policy formulation in loosely-coupled Markov decision processes.",
                "In Proc. of ICAPS-04, pages 315-324, June 2004. [7] D. A. Dolgov and E. H. Durfee.",
                "Computationally efficient combinatorial auctions for resource allocation in weakly-coupled MDPs.",
                "In Proc. of AAMAS-05, New York, NY, USA, 2005.",
                "ACM Press. [8] D. A. Dolgov and E. H. Durfee.",
                "Resource allocation among agents with preferences induced by factored MDPs.",
                "In Proc. of AAMAS-06, 2006. [9] K. Larson and T. Sandholm.",
                "Mechanism design and deliberative agents.",
                "In Proc. of AAMAS-05, pages 650-656, New York, NY, USA, 2005.",
                "ACM Press. [10] N. Nisan.",
                "Bidding and allocation in combinatorial auctions.",
                "In Electronic Commerce, 2000. [11] D. C. Parkes and S. Singh.",
                "An MDP-based approach to Online Mechanism Design.",
                "In Proc. of the Seventeenths Annual Conference on Neural Information Processing Systems (NIPS-03), 2003. [12] D. C. Parkes, S. Singh, and D. Yanovsky.",
                "Approximately efficient online mechanism design.",
                "In Proc. of the Eighteenths Annual Conference on Neural Information Processing Systems (NIPS-04), 2004. [13] M. L. Puterman.",
                "Markov Decision Processes.",
                "John Wiley & Sons, New York, 1994. [14] M. H. Rothkopf, A. Pekec, and R. M. Harstad.",
                "Computationally manageable combinational auctions.",
                "Management Science, 44(8):1131-1147, 1998. [15] T. Sandholm.",
                "An algorithm for optimal winner determination in combinatorial auctions.",
                "In Proc. of IJCAI-99, pages 542-547, San Francisco, CA, USA, 1999.",
                "Morgan Kaufmann Publishers Inc.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1227"
            ],
            "original_annotated_samples": [
                "This representation can then be used to construct very efficient resource-allocation algorithms that lead to an exponential speedup over a straightforward <br>optimization problem</br> with flat representations of combinatorial preferences [6, 7, 8].",
                "However, since the focus of our work is on scheduling problems, and a large part of the <br>optimization problem</br> is to decide how resources are allocated in time among agents with finite arrival and departure times, we model the agents planning problems as finite-horizon MDPs, in contrast to previous work that used infinite-horizon discounted MDPs.",
                "Given the above input, the <br>optimization problem</br> we consider is to find the globally optimal-maximizing the sum of expected rewards-mapping of resources to agents for all time steps: Δ : τ × M × Ω → {0, 1}.",
                "Second, using these augmented MDPs we construct a global <br>optimization problem</br>, which is described in Section 4.2. 4.1 Augmenting Agents MDPs In the model described in the previous section, we assume that if an agent does not possess the necessary resources to perform actions in its MDP, its execution is halted and the agent leaves the system.",
                "The resulting <br>optimization problem</br> then simultaneously solves the agents MDPs and resource-scheduling problems."
            ],
            "translated_annotated_samples": [
                "Esta representación puede ser utilizada para construir algoritmos de asignación de recursos muy eficientes que resultan en una aceleración exponencial sobre un <br>problema de optimización</br> directa con representaciones planas de preferencias combinatorias [6, 7, 8].",
                "Sin embargo, dado que el enfoque de nuestro trabajo se centra en problemas de programación, y una gran parte del <br>problema de optimización</br> consiste en decidir cómo se asignan los recursos en el tiempo entre agentes con tiempos finitos de llegada y salida, modelamos los problemas de planificación de los agentes como MDPs de horizonte finito, en contraste con trabajos anteriores que utilizaron MDPs descontados de horizonte infinito.",
                "Dado el input anterior, el <br>problema de optimización</br> que consideramos es encontrar la asignación globalmente óptima que maximice la suma de recompensas esperadas de recursos a agentes para todos los pasos de tiempo: Δ : τ × M × Ω → {0, 1}.",
                "Segundo, utilizando estos MDPs aumentados construimos un <br>problema de optimización</br> global, el cual se describe en la Sección 4.2. 4.1 Aumentando los MDPs de los Agentes En el modelo descrito en la sección anterior, asumimos que si un agente no posee los recursos necesarios para realizar acciones en su MDP, su ejecución se detiene y el agente abandona el sistema.",
                "El <br>problema de optimización</br> resultante resuelve simultáneamente los MDP de los agentes y los problemas de programación de recursos."
            ],
            "translated_text": "La programación de recursos combinatoria para MDPs multiagentes. Dmitri A. Dolgov, Michael R. James y Michael E. Samples. Grupo de Investigación en IA y Robótica, Centro Técnico de Toyota en EE. UU. {ddolgov, michael.r.james, michael.samples}@gmail.com. RESUMEN La programación óptima de recursos en sistemas multiagentes es una tarea computacionalmente desafiante, especialmente cuando los valores de los recursos no son aditivos. Consideramos el problema combinatorio de programar el uso de múltiples recursos entre agentes que operan en entornos estocásticos, modelados como procesos de decisión de Markov (MDPs). En los últimos años, se han desarrollado algoritmos eficientes de asignación de recursos para agentes con valores de recursos inducidos por MDPs. Sin embargo, este trabajo previo se ha centrado en problemas de asignación de recursos estáticos donde los recursos se distribuyen una vez y luego se utilizan en MDPs de horizonte infinito. Extendemos esos modelos existentes al problema de la programación de recursos combinatoria, donde los agentes persisten solo por períodos finitos entre sus tiempos de llegada y salida (predefinidos), requiriendo recursos solo durante esos períodos de tiempo. Proporcionamos un procedimiento computacionalmente eficiente para calcular asignaciones de recursos óptimas a nivel global a agentes a lo largo del tiempo. Ilustramos y analizamos empíricamente el método en el contexto de un dominio de programación de trabajos estocásticos. Categorías y Descriptores de Asignaturas I.2.8 [Inteligencia Artificial]: Resolución de Problemas, Métodos de Control y Búsqueda; I.2.11 [Inteligencia Artificial]: Inteligencia Artificial Distribuida-Sistemas Multiagente Términos Generales Algoritmos, Rendimiento, Diseño 1. INTRODUCCIÓN Las tareas de asignación óptima de recursos y programación son ubicuas en sistemas multiagentes, pero resolver tales problemas de optimización puede ser computacionalmente difícil, debido a una serie de factores. En particular, cuando el valor de un conjunto de recursos para un agente no es aditivo (como suele ser el caso con recursos que son sustitutos o complementos), la función de utilidad podría tener que definirse en un espacio exponencialmente grande de combinaciones de recursos, lo que rápidamente se vuelve computacionalmente intratable. Además, incluso cuando cada agente tiene una función de utilidad que es distinta de cero solo en un pequeño subconjunto de los posibles conjuntos de recursos, obtener una asignación óptima sigue siendo computacionalmente prohibitivo, ya que el problema se vuelve NP-completo [14]. Tales problemas computacionales han dado lugar recientemente a varias líneas de trabajo en el uso de modelos compactos de las preferencias de los agentes. Una idea es utilizar cualquier estructura presente en las funciones de utilidad para representarlas de forma compacta, por ejemplo, a través de fórmulas lógicas [15, 10, 4, 3]. Una alternativa es modelar directamente los mecanismos que definen las funciones de utilidad de los agentes y realizar la asignación de recursos directamente con estos modelos [9]. Una forma de lograr esto es modelar los procesos mediante los cuales un agente podría utilizar los recursos y definir la función de utilidad como la ganancia de estos procesos. En particular, si un agente utiliza recursos para actuar en un entorno estocástico, su función de utilidad puede ser modelada de forma natural con un proceso de decisión de Markov, cuyo conjunto de acciones está parametrizado por los recursos disponibles. Esta representación puede ser utilizada para construir algoritmos de asignación de recursos muy eficientes que resultan en una aceleración exponencial sobre un <br>problema de optimización</br> directa con representaciones planas de preferencias combinatorias [6, 7, 8]. Sin embargo, este trabajo existente sobre asignación de recursos con preferencias inducidas por MDPs parametrizados por recursos hace una suposición de que los recursos se asignan solo una vez y luego son utilizados por los agentes de forma independiente dentro de sus MDPs de horizonte infinito. Esta suposición de que no es posible la reasignación de recursos puede ser limitante en dominios donde los agentes llegan y se van dinámicamente. En este artículo, ampliamos el trabajo sobre asignación de recursos bajo preferencias inducidas por MDP a problemas de programación en tiempo discreto, donde los agentes están presentes en el sistema durante intervalos de tiempo finitos y solo pueden utilizar recursos dentro de estos intervalos. En particular, los agentes llegan y parten en momentos arbitrarios (predefinidos) y dentro de estos intervalos utilizan recursos para ejecutar tareas en MDPs de horizonte finito. Abordamos el problema de la programación de recursos óptima a nivel global, donde el objetivo es encontrar una asignación de recursos a los agentes a lo largo del tiempo que maximice la suma de las recompensas esperadas que obtienen. En este contexto, nuestra principal contribución es una formulación de programación entera mixta del problema de programación que elige asignaciones de recursos globalmente óptimas, tiempos de inicio y horizontes de ejecución para todos los agentes (dentro de sus intervalos de llegada y salida). Analizamos y comparamos empíricamente dos variantes del problema de programación: uno, donde los agentes tienen asignaciones de recursos estáticas dentro de sus MDPs de horizonte finito, y otro, donde los recursos pueden ser realocados dinámicamente entre agentes en cada paso de tiempo. En el resto del documento, primero establecemos las bases necesarias en la Sección 2 y luego presentamos nuestro modelo y declaración formal del problema en la Sección 3. En la Sección 4.2, describimos nuestro resultado principal, el programa de optimización para la programación de recursos globalmente óptima. Tras la discusión de nuestros resultados experimentales sobre un problema de programación de trabajos en la Sección 5, concluimos en la Sección 6 con una discusión de posibles extensiones y generalizaciones de nuestro método. De manera similar al modelo utilizado en trabajos anteriores sobre asignación de recursos con preferencias inducidas por MDP [6, 7], definimos el valor de un conjunto de recursos para un agente como el valor de la mejor política MDP que es realizable, dadas esos recursos. Sin embargo, dado que el enfoque de nuestro trabajo se centra en problemas de programación, y una gran parte del <br>problema de optimización</br> consiste en decidir cómo se asignan los recursos en el tiempo entre agentes con tiempos finitos de llegada y salida, modelamos los problemas de planificación de los agentes como MDPs de horizonte finito, en contraste con trabajos anteriores que utilizaron MDPs descontados de horizonte infinito. En el resto de esta sección, primero presentamos algo de antecedentes necesarios sobre MDPs de horizonte finito y presentamos una formulación de programación lineal que sirve como base para nuestro algoritmo de solución desarrollado en la Sección 4. También describimos los métodos estándar para la programación de recursos combinatorios con valores de recursos planos, que sirven como punto de referencia de comparación para el nuevo modelo desarrollado aquí. 2.1 Procesos de Decisión de Markov Un MDP (Proceso de Decisión de Markov) estacionario, de dominio finito y discreto en el tiempo (ver, por ejemplo, [13] para un desarrollo detallado y exhaustivo) se puede describir como S, A, p, r, donde: S es un conjunto finito de estados del sistema; A es un conjunto finito de acciones disponibles para el agente; p es una función de transición estocástica estacionaria, donde p(σ|s, a) es la probabilidad de transicionar al estado σ al ejecutar la acción a en el estado s; r es una función de recompensa estacionaria, donde r(s, a) especifica la recompensa obtenida al ejecutar la acción a en el estado s. Dado un MDP de este tipo, un problema de decisión bajo un horizonte finito T consiste en elegir una acción óptima en cada paso de tiempo para maximizar el valor esperado de la recompensa total acumulada durante la vida (finita) de los agentes. La política óptima del agente es entonces una función del estado actual s y el tiempo hasta el horizonte. Una política óptima para tal problema es actuar de forma codiciosa con respecto a la función de valor óptimo, definida de forma recursiva por el siguiente sistema de ecuaciones de Bellman de tiempo finito [2]: v(s, t) = max a r(s, a) + X σ p(σ|s, a)v(σ, t + 1), ∀s ∈ S, t ∈ [1, T − 1]; v(s, T) = 0, ∀s ∈ S; donde v(s, t) es el valor óptimo de estar en el estado s en el tiempo t ∈ [1, T]. Esta función de valor óptimo se puede calcular fácilmente utilizando programación dinámica, lo que conduce a la siguiente política óptima π, donde π(s, a, t) es la probabilidad de ejecutar la acción a en el estado s en el tiempo t: π(s, a, t) = (1, a = argmaxa r(s, a) + P σ p(σ|s, a)v(σ, t + 1), 0, en caso contrario. La anterior es la forma más común de calcular la función de valor óptimo (y por lo tanto una política óptima) para un MDP de horizonte finito. Sin embargo, también podemos formular el problema como el siguiente programa lineal (similarmente al LP dual para MDPs descontados de horizonte infinito [13, 6, 7]): max X s X a r(s, a) X t x(s, a, t) sujeto a: X a x(σ, a, t + 1) = X s,a p(σ|s, a)x(s, a, t) ∀σ, t ∈ [1, T − 1]; X a x(s, a, 1) = α(s), ∀s ∈ S; (1) donde α(s) es la distribución inicial sobre el espacio de estados, y x es la medida de ocupación (no estacionaria) (x(s, a, t) ∈ [0, 1] es el número total esperado de veces que se ejecuta la acción a en el estado s en el tiempo t). Una política óptima (no estacionaria) se obtiene a partir de la medida de ocupación de la siguiente manera: π(s, a, t) = x(s, a, t)/ X a x(s, a, t) ∀s ∈ S, t ∈ [1, T]. (2) Tenga en cuenta que el MDP estándar de horizonte finito sin restricciones, como se describe anteriormente, siempre tiene una solución uniformemente óptima (óptima para cualquier distribución inicial α(s)). Por lo tanto, una política óptima se puede obtener utilizando una constante arbitraria α(s) > 0 (en particular, α(s) = 1 resultará en x(s, a, t) = π(s, a, t)). Sin embargo, para los MDPs con restricciones de recursos (como se definen a continuación en la Sección 3), las políticas óptimas uniformemente no existen en general. En tales casos, α se convierte en parte de la entrada del problema, y una política resultante solo es óptima para ese α en particular. Este resultado es bien conocido para MDPs de horizonte infinito con varios tipos de restricciones [1, 6], y también se cumple para nuestro modelo de horizonte finito, el cual puede establecerse fácilmente a través de una línea de razonamiento completamente análoga a los argumentos en [6]. 2.2 Programación de Recursos Combinatoria Un enfoque directo para la programación de recursos para un conjunto de agentes M, cuyos valores para los recursos son inducidos por problemas de planificación estocástica (en nuestro caso, MDPs de horizonte finito) sería que cada agente enumerara todas las asignaciones de recursos posibles a lo largo del tiempo y, para cada una, calcular su valor resolviendo el MDP correspondiente. Entonces, cada agente proporcionaría valoraciones para cada posible conjunto de recursos a lo largo del tiempo a un coordinador centralizado, quien calcularía las asignaciones óptimas de recursos a lo largo del tiempo basándose en estas valoraciones. Cuando los recursos pueden asignarse en diferentes momentos a diferentes agentes, cada agente debe presentar valoraciones para cada combinación de posibles horizontes temporales. Que cada agente m ∈ M ejecute su MDP dentro del intervalo de tiempo de llegada-salida τ ∈ [τa m, τd m]. Por lo tanto, el agente m ejecutará un MDP con un horizonte temporal no mayor que Tm = τd m−τa m+1. Que bτ sea el horizonte temporal global para el problema, antes del cual todos los MDP de los agentes deben finalizar. Suponemos que τd m < bτ, ∀m ∈ M. El Sexto Congreso Internacional. En el problema de programación donde los agentes tienen requisitos de recursos estáticos dentro de sus MDPs de horizonte finito, los agentes proporcionan una valoración para cada conjunto de recursos para cada horizonte de tiempo posible (desde [1, Tm]) que puedan utilizar. Sea Ω el conjunto de recursos a ser asignados entre los agentes. Un agente recibirá como máximo un paquete de recursos para uno de los horizontes temporales. Que la variable ψ ∈ Ψm enumere todos los posibles pares de conjuntos de recursos y horizontes temporales para el agente m, por lo que hay 2|Ω| × Tm valores para ψ (el espacio de conjuntos es exponencial en el número de tipos de recursos |Ω|). El agente m debe proporcionar un valor vψ m para cada ψ, y el coordinador asignará como máximo un par (recurso, horizonte temporal) ψ a cada agente. Esta asignación se expresa como una variable indicadora zψ m ∈ {0, 1} que muestra si ψ está asignado al agente m. Para el tiempo τ y el recurso ω, la función nm(ψ, τ, ω) ∈ {0, 1} indica si el paquete en ψ utiliza el recurso ω en el tiempo τ (hacemos la suposición de que los agentes tienen requisitos de recursos binarios). Este problema de asignación es NP-completo, incluso al considerar solo un paso de tiempo, y su dificultad aumenta significativamente con múltiples pasos de tiempo debido al creciente número de valores de ψ. El problema de encontrar una asignación óptima que satisfaga la restricción global de que la cantidad de cada recurso ω asignada a todos los agentes no exceda la cantidad disponible bϕ(ω) se puede expresar como el siguiente programa entero: max X m∈M X ψ∈Ψm zψ mvψ m sujeto a: X ψ∈Ψm zψ m ≤ 1, ∀m ∈ M; X m∈M X ψ∈Ψm zψ mnm(ψ, τ, ω) ≤ bϕ(ω), ∀τ ∈ [1, bτ], ∀ω ∈ Ω; (3) La primera restricción en la ecuación 3 dice que ningún agente puede recibir más de un paquete, y la segunda restricción asegura que la asignación total del recurso ω no exceda, en ningún momento, el límite del recurso. Para el problema de programación en el que los agentes pueden reasignar dinámicamente recursos, cada agente debe especificar un valor para cada combinación de paquetes y pasos de tiempo dentro de su horizonte temporal. Que la variable ψ ∈ Ψm en este caso enumere todos los posibles conjuntos de recursos para los cuales a lo sumo un conjunto puede ser asignado al agente m en cada paso de tiempo. Por lo tanto, en este caso hay P t∈[1,Tm](2|Ω| )t ∼ 2|Ω|Tm posibilidades de combinaciones de recursos asignadas a diferentes intervalos de tiempo, para los Tm horizontes de tiempo diferentes. El mismo conjunto de ecuaciones (3) se puede utilizar para resolver este problema de programación dinámica, pero el programa entero es diferente debido a la diferencia en cómo se define ψ. En este caso, el número de valores de ψ es exponencial en el horizonte de planificación Tm de cada agente, lo que resulta en un programa mucho más grande. Este enfoque directo para resolver ambos problemas de programación requiere una enumeración y solución de 2|Ω| Tm (asignación estática) o P t∈[1,Tm] 2|Ω|t (reubicación dinámica) MDPs para cada agente, lo cual rápidamente se vuelve intratable con el crecimiento del número de recursos |Ω| o el horizonte de tiempo Tm. MODELO Y DECLARACIÓN DEL PROBLEMA Ahora presentamos formalmente nuestro modelo del problema de programación de recursos. La entrada del problema consiste en los siguientes componentes: • M, Ω, bϕ, τa m, τd m, bτ están definidos arriba en la Sección 2.2. • {Θm} = {S, A, pm, rm, αm} son los MDPs de todos los agentes m ∈ M. Sin pérdida de generalidad, asumimos que los espacios de estados y acciones de todos los agentes son iguales, pero cada uno tiene su propia función de transición pm, función de recompensa rm, y condiciones iniciales αm. • ϕm : A×Ω → {0, 1} es la asignación de acciones a recursos para el agente m. ϕm(a, ω) indica si la acción a del agente m necesita el recurso ω. Un agente m que recibe un conjunto de recursos que no incluye el recurso ω no puede ejecutar en su política MDP ninguna acción a para la cual ϕm(a, ω) = 0. Suponemos que todos los requisitos de recursos son binarios; como se discute más adelante en la Sección 6, esta suposición no es limitante. Dado el input anterior, el <br>problema de optimización</br> que consideramos es encontrar la asignación globalmente óptima que maximice la suma de recompensas esperadas de recursos a agentes para todos los pasos de tiempo: Δ : τ × M × Ω → {0, 1}. Una solución es factible si la asignación correspondiente de recursos a los agentes no viola la restricción global de recursos: X m Δm(τ, ω) ≤ bϕ(ω), ∀ω ∈ Ω, τ ∈ [1, bτ]. Consideramos dos variantes del problema de programación de recursos. La primera formulación restringe las asignaciones de recursos al espacio donde la asignación de recursos a cada agente es estática durante la vida de los agentes. La segunda formulación permite la reasignación de recursos entre agentes en cada paso de tiempo durante sus vidas. La Figura 1 representa un problema de programación de recursos con tres agentes M = {m1, m2, m3}, tres recursos Ω = {ω1, ω2, ω3}, y un horizonte de problema global de bτ = 11. Los tiempos de llegada y salida de los agentes se muestran como cajas grises y son {1, 6}, {3, 7} y {2, 11}, respectivamente. Una solución a este problema se muestra a través de barras horizontales dentro de cada casilla de agentes, donde las barras corresponden a la asignación de los tres tipos de recursos. La Figura 1a muestra una solución a un problema de programación estática. Según la solución mostrada, el agente m1 comienza la ejecución de su MDP en el tiempo τ = 1 y tiene un bloqueo en los tres recursos hasta que finaliza la ejecución en el tiempo τ = 3. Se debe tener en cuenta que el agente m1 renuncia a su control sobre los recursos antes de su hora de salida anunciada de τd m1 = 6, aparentemente porque otros agentes pueden utilizar los recursos de manera más efectiva. Por lo tanto, en el tiempo τ = 4, los recursos ω1 y ω3 se asignan al agente m2, quien luego los utiliza para ejecutar su MDP (utilizando solo acciones admitidas por los recursos ω1 y ω3) hasta el tiempo τ = 7. El agente m3 posee el recurso ω3 durante el intervalo τ ∈ [4, 10]. La Figura 1b muestra una posible solución a la versión dinámica del mismo problema. Allí, los recursos pueden ser reasignados entre agentes en cada paso de tiempo. Por ejemplo, el agente m1 renuncia a su uso del recurso ω2 en el tiempo τ = 2, aunque continúa la ejecución de su MDP hasta el tiempo τ = 6. Ten en cuenta que a un agente no se le permite detener y reiniciar su MDP, por lo que el agente m1 solo puede continuar ejecutando en el intervalo τ ∈ [3, 4] si tiene acciones que no requieren recursos (ϕm(a, ω) = 0). Claramente, el modelo y la declaración del problema descritos anteriormente hacen una serie de suposiciones sobre el problema y las propiedades de la solución deseada. Discutimos algunas de esas suposiciones y sus implicaciones en la Sección 6. 1222 La Sexta Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) (a) (b) Figura 1: Ilustración de una solución a un problema de programación de recursos con tres agentes y tres recursos: a) asignaciones de recursos estáticas (las asignaciones de recursos son constantes durante la vida de los agentes); b) asignación dinámica (las asignaciones de recursos pueden cambiar en cada paso de tiempo). 4. PROGRAMACIÓN DE RECURSOS Nuestro algoritmo de programación de recursos se lleva a cabo en dos etapas. Primero, realizamos un paso de preprocesamiento que aumenta los MDP de los agentes; este proceso se describe en la Sección 4.1. Segundo, utilizando estos MDPs aumentados construimos un <br>problema de optimización</br> global, el cual se describe en la Sección 4.2. 4.1 Aumentando los MDPs de los Agentes En el modelo descrito en la sección anterior, asumimos que si un agente no posee los recursos necesarios para realizar acciones en su MDP, su ejecución se detiene y el agente abandona el sistema. En otras palabras, los MDPs no pueden ser pausados y reanudados. Por ejemplo, en el problema mostrado en la Figura 1a, el agente m1 libera todos los recursos después del tiempo τ = 3, momento en el cual la ejecución de su MDP se detiene. De manera similar, los agentes m2 y m3 solo ejecutan sus MDPs en los intervalos τ ∈ [4, 6] y τ ∈ [4, 10], respectivamente. Por lo tanto, una parte importante del problema de toma de decisiones global es decidir la ventana de tiempo durante la cual cada uno de los agentes está activo (es decir, ejecutando su MDP). Para lograr esto, ampliamos el MDP de cada agente con dos nuevos estados (estados de inicio y final sb, sf, respectivamente) y una nueva acción de inicio/parada a∗, como se ilustra en la Figura 2. La idea es que un agente permanezca en el estado inicial sb hasta que esté listo para ejecutar su MDP, momento en el que realiza la acción de inicio/parada a∗ y transita al espacio de estados del MDP original con la probabilidad de transición que corresponde a la distribución inicial original α(s). Por ejemplo, en la Figura 1a, para el agente m2 esto ocurriría en el tiempo τ = 4. Una vez que el agente llega al final de su ventana de actividad (tiempo τ = 6 para el agente m2 en la Figura 1a), realiza la acción de inicio/parada, lo que lo lleva al estado final de finalización del sumidero sf en el tiempo τ = 7. Más precisamente, dado un MDP S, A, pm, rm, αm, definimos un MDP aumentado S, A, pm, rm, αm de la siguiente manera: S = S ∪ sb ∪ sf; A = A ∪ a∗; p(s|sb, a∗) = α(s), ∀s ∈ S; p(sb|sb, a) = 1.0, ∀a ∈ A; p(sf|s, a∗) = 1.0, ∀s ∈ S; p(σ|s, a) = p(σ|s, a), ∀s, σ ∈ S, a ∈ A; r(sb, a) = r(sf, a) = 0, ∀a ∈ A; r(s, a) = r(s, a), ∀s ∈ S, a ∈ A; α(sb) = 1; α(s) = 0, ∀s ∈ S; donde se asume que todas las probabilidades de transición no especificadas son cero. Además, para tener en cuenta el nuevo estado inicial, comenzamos el MDP un paso de tiempo antes, estableciendo τa m ← τa m − 1. Esto no afectará la asignación de recursos debido a que las restricciones de recursos solo se aplican a los estados originales del MDP, como se discutirá en la siguiente sección. Por ejemplo, los MDPs aumentados mostrados en la Figura 2b (que comienza en el estado sb en el tiempo τ = 2) se construirían a partir de un MDP con tiempo de llegada original τ = 3. La Figura 2b también muestra una trayectoria de muestra a través del espacio de estados: el agente comienza en el estado sb, transita al espacio de estados S del MDP original y finalmente sale al estado de sumidero sf. Cabe destacar que si quisiéramos modelar un problema en el que los agentes pudieran pausar sus MDP en pasos de tiempo arbitrarios (lo cual podría ser útil para dominios donde la reasignación dinámica es posible), podríamos lograrlo fácilmente incluyendo una acción adicional que transiciona de cada estado a sí mismo con una recompensa de cero. 4.2 MILP para la programación de recursos Dado un conjunto de MDPs aumentados, como se define arriba, el objetivo de esta sección es formular un programa de optimización global que resuelva el problema de programación de recursos. En esta sección y a continuación, se asume que todos los MDPs son los MDPs aumentados tal como se definen en la Sección 4.1. Nuestro enfoque es similar a la idea utilizada en [6]: comenzamos con la formulación del programa lineal de los MDP de los agentes (1) y lo complementamos con restricciones que aseguran que la asignación de recursos correspondiente entre agentes y tiempo sea válida. El <br>problema de optimización</br> resultante resuelve simultáneamente los MDP de los agentes y los problemas de programación de recursos. ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "utility function": {
            "translated_key": "función de utilidad",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Combinatorial Resource Scheduling for Multiagent MDPs Dmitri A. Dolgov, Michael R. James, and Michael E. Samples AI and Robotics Group Technical Research, Toyota Technical Center USA {ddolgov, michael.r.james, michael.samples}@gmail.com ABSTRACT Optimal resource scheduling in multiagent systems is a computationally challenging task, particularly when the values of resources are not additive.",
                "We consider the combinatorial problem of scheduling the usage of multiple resources among agents that operate in stochastic environments, modeled as Markov decision processes (MDPs).",
                "In recent years, efficient resource-allocation algorithms have been developed for agents with resource values induced by MDPs.",
                "However, this prior work has focused on static resource-allocation problems where resources are distributed once and then utilized in infinite-horizon MDPs.",
                "We extend those existing models to the problem of combinatorial resource scheduling, where agents persist only for finite periods between their (predefined) arrival and departure times, requiring resources only for those time periods.",
                "We provide a computationally efficient procedure for computing globally optimal resource assignments to agents over time.",
                "We illustrate and empirically analyze the method in the context of a stochastic jobscheduling domain.",
                "Categories and Subject Descriptors I.2.8 [Artificial Intelligence]: Problem Solving, Control Methods, and Search; I.2.11 [Artificial Intelligence]: Distributed Artificial Intelligence-Multiagent systems General Terms Algorithms, Performance, Design 1.",
                "INTRODUCTION The tasks of optimal resource allocation and scheduling are ubiquitous in multiagent systems, but solving such optimization problems can be computationally difficult, due to a number of factors.",
                "In particular, when the value of a set of resources to an agent is not additive (as is often the case with resources that are substitutes or complements), the <br>utility function</br> might have to be defined on an exponentially large space of resource bundles, which very quickly becomes computationally intractable.",
                "Further, even when each agent has a <br>utility function</br> that is nonzero only on a small subset of the possible resource bundles, obtaining optimal allocation is still computationally prohibitive, as the problem becomes NP-complete [14].",
                "Such computational issues have recently spawned several threads of work in using compact models of agents preferences.",
                "One idea is to use any structure present in utility functions to represent them compactly, via, for example, logical formulas [15, 10, 4, 3].",
                "An alternative is to directly model the mechanisms that define the agents utility functions and perform resource allocation directly with these models [9].",
                "A way of accomplishing this is to model the processes by which an agent might utilize the resources and define the <br>utility function</br> as the payoff of these processes.",
                "In particular, if an agent uses resources to act in a stochastic environment, its <br>utility function</br> can be naturally modeled with a Markov decision process, whose action set is parameterized by the available resources.",
                "This representation can then be used to construct very efficient resource-allocation algorithms that lead to an exponential speedup over a straightforward optimization problem with flat representations of combinatorial preferences [6, 7, 8].",
                "However, this existing work on resource allocation with preferences induced by resource-parameterized MDPs makes an assumption that the resources are only allocated once and are then utilized by the agents independently within their infinite-horizon MDPs.",
                "This assumption that no reallocation of resources is possible can be limiting in domains where agents arrive and depart dynamically.",
                "In this paper, we extend the work on resource allocation under MDP-induced preferences to discrete-time scheduling problems, where agents are present in the system for finite time intervals and can only use resources within these intervals.",
                "In particular, agents arrive and depart at arbitrary (predefined) times and within these intervals use resources to execute tasks in finite-horizon MDPs.",
                "We address the problem of globally optimal resource scheduling, where the objective is to find an allocation of resources to the agents across time that maximizes the sum of the expected rewards that they obtain.",
                "In this context, our main contribution is a mixed-integerprogramming formulation of the scheduling problem that chooses globally optimal resource assignments, starting times, and execution horizons for all agents (within their arrival1220 978-81-904262-7-5 (RPS) c 2007 IFAAMAS departure intervals).",
                "We analyze and empirically compare two flavors of the scheduling problem: one, where agents have static resource assignments within their finite-horizon MDPs, and another, where resources can be dynamically reallocated between agents at every time step.",
                "In the rest of the paper, we first lay down the necessary groundwork in Section 2 and then introduce our model and formal problem statement in Section 3.",
                "In Section 4.2, we describe our main result, the optimization program for globally optimal resource scheduling.",
                "Following the discussion of our experimental results on a job-scheduling problem in Section 5, we conclude in Section 6 with a discussion of possible extensions and generalizations of our method. 2.",
                "BACKGROUND Similarly to the model used in previous work on resourceallocation with MDP-induced preferences [6, 7], we define the value of a set of resources to an agent as the value of the best MDP policy that is realizable, given those resources.",
                "However, since the focus of our work is on scheduling problems, and a large part of the optimization problem is to decide how resources are allocated in time among agents with finite arrival and departure times, we model the agents planning problems as finite-horizon MDPs, in contrast to previous work that used infinite-horizon discounted MDPs.",
                "In the rest of this section, we first introduce some necessary background on finite-horizon MDPs and present a linear-programming formulation that serves as the basis for our solution algorithm developed in Section 4.",
                "We also outline the standard methods for combinatorial resource scheduling with flat resource values, which serve as a comparison benchmark for the new model developed here. 2.1 Markov Decision Processes A stationary, finite-domain, discrete-time MDP (see, for example, [13] for a thorough and detailed development) can be described as S, A, p, r , where: S is a finite set of system states; A is a finite set of actions that are available to the agent; p is a stationary stochastic transition function, where p(σ|s, a) is the probability of transitioning to state σ upon executing action a in state s; r is a stationary reward function, where r(s, a) specifies the reward obtained upon executing action a in state s. Given such an MDP, a decision problem under a finite horizon T is to choose an optimal action at every time step to maximize the expected value of the total reward accrued during the agents (finite) lifetime.",
                "The agents optimal policy is then a function of current state s and the time until the horizon.",
                "An optimal policy for such a problem is to act greedily with respect to the optimal value function, defined recursively by the following system of finite-time Bellman equations [2]: v(s, t) = max a r(s, a) + X σ p(σ|s, a)v(σ, t + 1), ∀s ∈ S, t ∈ [1, T − 1]; v(s, T) = 0, ∀s ∈ S; where v(s, t) is the optimal value of being in state s at time t ∈ [1, T].",
                "This optimal value function can be easily computed using dynamic programming, leading to the following optimal policy π, where π(s, a, t) is the probability of executing action a in state s at time t: π(s, a, t) = ( 1, a = argmaxa r(s, a) + P σ p(σ|s, a)v(σ, t + 1), 0, otherwise.",
                "The above is the most common way of computing the optimal value function (and therefore an optimal policy) for a finite-horizon MDP.",
                "However, we can also formulate the problem as the following linear program (similarly to the dual LP for infinite-horizon discounted MDPs [13, 6, 7]): max X s X a r(s, a) X t x(s, a, t) subject to: X a x(σ, a, t + 1) = X s,a p(σ|s, a)x(s, a, t) ∀σ, t ∈ [1, T − 1]; X a x(s, a, 1) = α(s), ∀s ∈ S; (1) where α(s) is the initial distribution over the state space, and x is the (non-stationary) occupation measure (x(s, a, t) ∈ [0, 1] is the total expected number of times action a is executed in state s at time t).",
                "An optimal (non-stationary) policy is obtained from the occupation measure as follows: π(s, a, t) = x(s, a, t)/ X a x(s, a, t) ∀s ∈ S, t ∈ [1, T]. (2) Note that the standard unconstrained finite-horizon MDP, as described above, always has a uniformly-optimal solution (optimal for any initial distribution α(s)).",
                "Therefore, an optimal policy can be obtained by using an arbitrary constant α(s) > 0 (in particular, α(s) = 1 will result in x(s, a, t) = π(s, a, t)).",
                "However, for MDPs with resource constraints (as defined below in Section 3), uniformly-optimal policies do not in general exist.",
                "In such cases, α becomes a part of the problem input, and a resulting policy is only optimal for that particular α.",
                "This result is well known for infinite-horizon MDPs with various types of constraints [1, 6], and it also holds for our finite-horizon model, which can be easily established via a line of reasoning completely analogous to the arguments in [6]. 2.2 Combinatorial Resource Scheduling A straightforward approach to resource scheduling for a set of agents M, whose values for the resources are induced by stochastic planning problems (in our case, finite-horizon MDPs) would be to have each agent enumerate all possible resource assignments over time and, for each one, compute its value by solving the corresponding MDP.",
                "Then, each agent would provide valuations for each possible resource bundle over time to a centralized coordinator, who would compute the optimal resource assignments across time based on these valuations.",
                "When resources can be allocated at different times to different agents, each agent must submit valuations for every combination of possible time horizons.",
                "Let each agent m ∈ M execute its MDP within the arrival-departure time interval τ ∈ [τa m, τd m].",
                "Hence, agent m will execute an MDP with time horizon no greater than Tm = τd m−τa m+1.",
                "Let bτ be the global time horizon for the problem, before which all of the agents MDPs must finish.",
                "We assume τd m < bτ, ∀m ∈ M. The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1221 For the scheduling problem where agents have static resource requirements within their finite-horizon MDPs, the agents provide a valuation for each resource bundle for each possible time horizon (from [1, Tm]) that they may use.",
                "Let Ω be the set of resources to be allocated among the agents.",
                "An agent will get at most one resource bundle for one of the time horizons.",
                "Let the variable ψ ∈ Ψm enumerate all possible pairs of resource bundles and time horizons for agent m, so there are 2|Ω| × Tm values for ψ (the space of bundles is exponential in the number of resource types |Ω|).",
                "The agent m must provide a value vψ m for each ψ, and the coordinator will allocate at most one ψ (resource, time horizon) pair to each agent.",
                "This allocation is expressed as an indicator variable zψ m ∈ {0, 1} that shows whether ψ is assigned to agent m. For time τ and resource ω, the function nm(ψ, τ, ω) ∈ {0, 1} indicates whether the bundle in ψ uses resource ω at time τ (we make the assumption that agents have binary resource requirements).",
                "This allocation problem is NP-complete, even when considering only a single time step, and its difficulty increases significantly with multiple time steps because of the increasing number of values of ψ.",
                "The problem of finding an optimal allocation that satisfies the global constraint that the amount of each resource ω allocated to all agents does not exceed the available amount bϕ(ω) can be expressed as the following integer program: max X m∈M X ψ∈Ψm zψ mvψ m subject to: X ψ∈Ψm zψ m ≤ 1, ∀m ∈ M; X m∈M X ψ∈Ψm zψ mnm(ψ, τ, ω) ≤ bϕ(ω), ∀τ ∈ [1, bτ], ∀ω ∈ Ω; (3) The first constraint in equation 3 says that no agent can receive more than one bundle, and the second constraint ensures that the total assignment of resource ω does not, at any time, exceed the resource bound.",
                "For the scheduling problem where the agents are able to dynamically reallocate resources, each agent must specify a value for every combination of bundles and time steps within its time horizon.",
                "Let the variable ψ ∈ Ψm in this case enumerate all possible resource bundles for which at most one bundle may be assigned to agent m at each time step.",
                "Therefore, in this case there are P t∈[1,Tm](2|Ω| )t ∼ 2|Ω|Tm possibilities of resource bundles assigned to different time slots, for the Tm different time horizons.",
                "The same set of equations (3) can be used to solve this dynamic scheduling problem, but the integer program is different because of the difference in how ψ is defined.",
                "In this case, the number of ψ values is exponential in each agents planning horizon Tm, resulting in a much larger program.",
                "This straightforward approach to solving both of these scheduling problems requires an enumeration and solution of either 2|Ω| Tm (static allocation) or P t∈[1,Tm] 2|Ω|t (dynamic reallocation) MDPs for each agent, which very quickly becomes intractable with the growth of the number of resources |Ω| or the time horizon Tm. 3.",
                "MODEL AND PROBLEM STATEMENT We now formally introduce our model of the resourcescheduling problem.",
                "The problem input consists of the following components: • M, Ω, bϕ, τa m, τd m, bτ are as defined above in Section 2.2. • {Θm} = {S, A, pm, rm, αm} are the MDPs of all agents m ∈ M. Without loss of generality, we assume that state and action spaces of all agents are the same, but each has its own transition function pm, reward function rm, and initial conditions αm. • ϕm : A×Ω → {0, 1} is the mapping of actions to resources for agent m. ϕm(a, ω) indicates whether action a of agent m needs resource ω.",
                "An agent m that receives a set of resources that does not include resource ω cannot execute in its MDP policy any action a for which ϕm(a, ω) = 0.",
                "We assume all resource requirements are binary; as discussed below in Section 6, this assumption is not limiting.",
                "Given the above input, the optimization problem we consider is to find the globally optimal-maximizing the sum of expected rewards-mapping of resources to agents for all time steps: Δ : τ × M × Ω → {0, 1}.",
                "A solution is feasible if the corresponding assignment of resources to the agents does not violate the global resource constraint: X m Δm(τ, ω) ≤ bϕ(ω), ∀ω ∈ Ω, τ ∈ [1, bτ]. (4) We consider two flavors of the resource-scheduling problem.",
                "The first formulation restricts resource assignments to the space where the allocation of resources to each agent is static during the agents lifetime.",
                "The second formulation allows reassignment of resources between agents at every time step within their lifetimes.",
                "Figure 1 depicts a resource-scheduling problem with three agents M = {m1, m2, m3}, three resources Ω = {ω1, ω2, ω3}, and a global problem horizon of bτ = 11.",
                "The agents arrival and departure times are shown as gray boxes and are {1, 6}, {3, 7}, and {2, 11}, respectively.",
                "A solution to this problem is shown via horizontal bars within each agents box, where the bars correspond to the allocation of the three resource types.",
                "Figure 1a shows a solution to a static scheduling problem.",
                "According to the shown solution, agent m1 begins the execution of its MDP at time τ = 1 and has a lock on all three resources until it finishes execution at time τ = 3.",
                "Note that agent m1 relinquishes its hold on the resources before its announced departure time of τd m1 = 6, ostensibly because other agents can utilize the resources more effectively.",
                "Thus, at time τ = 4, resources ω1 and ω3 are allocated to agent m2, who then uses them to execute its MDP (using only actions supported by resources ω1 and ω3) until time τ = 7.",
                "Agent m3 holds resource ω3 during the interval τ ∈ [4, 10].",
                "Figure 1b shows a possible solution to the dynamic version of the same problem.",
                "There, resources can be reallocated between agents at every time step.",
                "For example, agent m1 gives up its use of resource ω2 at time τ = 2, although it continues the execution of its MDP until time τ = 6.",
                "Notice that an agent is not allowed to stop and restart its MDP, so agent m1 is only able to continue executing in the interval τ ∈ [3, 4] if it has actions that do not require any resources (ϕm(a, ω) = 0).",
                "Clearly, the model and problem statement described above make a number of assumptions about the problem and the desired solution properties.",
                "We discuss some of those assumptions and their implications in Section 6. 1222 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) (a) (b) Figure 1: Illustration of a solution to a resource-scheduling problem with three agents and three resources: a) static resource assignments (resource assignments are constant within agents lifetimes; b) dynamic assignment (resource assignments are allowed to change at every time step). 4.",
                "RESOURCE SCHEDULING Our resource-scheduling algorithm proceeds in two stages.",
                "First, we perform a preprocessing step that augments the agent MDPs; this process is described in Section 4.1.",
                "Second, using these augmented MDPs we construct a global optimization problem, which is described in Section 4.2. 4.1 Augmenting Agents MDPs In the model described in the previous section, we assume that if an agent does not possess the necessary resources to perform actions in its MDP, its execution is halted and the agent leaves the system.",
                "In other words, the MDPs cannot be paused and resumed.",
                "For example, in the problem shown in Figure 1a, agent m1 releases all resources after time τ = 3, at which point the execution of its MDP is halted.",
                "Similarly, agents m2 and m3 only execute their MDPs in the intervals τ ∈ [4, 6] and τ ∈ [4, 10], respectively.",
                "Therefore, an important part of the global decision-making problem is to decide the window of time during which each of the agents is active (i.e., executing its MDP).",
                "To accomplish this, we augment each agents MDP with two new states (start and finish states sb , sf , respectively) and a new start/stop action a∗ , as illustrated in Figure 2.",
                "The idea is that an agent stays in the start state sb until it is ready to execute its MDP, at which point it performs the start/stop action a∗ and transitions into the state space of the original MDP with the transition probability that corresponds to the original initial distribution α(s).",
                "For example, in Figure 1a, for agent m2 this would happen at time τ = 4.",
                "Once the agent gets to the end of its activity window (time τ = 6 for agent m2 in Figure 1a), it performs the start/stop action, which takes it into the sink finish state sf at time τ = 7.",
                "More precisely, given an MDP S, A, pm, rm, αm , we define an augmented MDP S , A , pm, rm, αm as follows: S = S ∪ sb ∪ sf ; A = A ∪ a∗ ; p (s|sb , a∗ ) = α(s), ∀s ∈ S; p (sb |sb , a) = 1.0, ∀a ∈ A; p (sf |s, a∗ ) = 1.0, ∀s ∈ S; p (σ|s, a) = p(σ|s, a), ∀s, σ ∈ S, a ∈ A; r (sb , a) = r (sf , a) = 0, ∀a ∈ A ; r (s, a) = r(s, a), ∀s ∈ S, a ∈ A; α (sb ) = 1; α (s) = 0, ∀s ∈ S; where all non-specified transition probabilities are assumed to be zero.",
                "Further, in order to account for the new starting state, we begin the MDP one time-step earlier, setting τa m ← τa m − 1.",
                "This will not affect the resource allocation due to the resource constraints only being enforced for the original MDP states, as will be discussed in the next section.",
                "For example, the augmented MDPs shown in Figure 2b (which starts in state sb at time τ = 2) would be constructed from an MDP with original arrival time τ = 3.",
                "Figure 2b also shows a sample trajectory through the state space: the agent starts in state sb , transitions into the state space S of the original MDP, and finally exists into the sink state sf .",
                "Note that if we wanted to model a problem where agents could pause their MDPs at arbitrary time steps (which might be useful for domains where dynamic reallocation is possible), we could easily accomplish this by including an extra action that transitions from each state to itself with zero reward. 4.2 MILP for Resource Scheduling Given a set of augmented MDPs, as defined above, the goal of this section is to formulate a global optimization program that solves the resource-scheduling problem.",
                "In this section and below, all MDPs are assumed to be the augmented MDPs as defined in Section 4.1.",
                "Our approach is similar to the idea used in [6]: we begin with the linear-program formulation of agents MDPs (1) and augment it with constraints that ensure that the corresponding resource allocation across agents and time is valid.",
                "The resulting optimization problem then simultaneously solves the agents MDPs and resource-scheduling problems.",
                "In the rest of this section, we incrementally develop a mixed integer program (MILP) that achieves this.",
                "In the absence of resource constraints, the agents finitehorizon MDPs are completely independent, and the globally optimal solution can be trivially obtained via the following LP, which is simply an aggregation of single-agent finitehorizon LPs: max X m X s X a rm(s, a) X t xm(s, a, t) subject to: X a xm(σ, a, t + 1) = X s,a pm(σ|s, a)xm(s, a, t), ∀m ∈ M, σ ∈ S, t ∈ [1, Tm − 1]; X a xm(s, a, 1) = αm(s), ∀m ∈ M, s ∈ S; (12) where xm(s, a, t) is the occupation measure of agent m, and The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1223 (a) (b) Figure 2: Illustration of augmenting an MDP to allow for variable starting and stopping times: a) (left) the original two-state MDP with a single action; (right) the augmented MDP with new states sb and sf and the new action a∗ (note that the origianl transitions are not changed in the augmentation process); b) the augmented MDP displayed as a trajectory through time (grey lines indicate all transitions, while black lines indicate a given trajectory.",
                "Objective Function (sum of expected rewards over all agents) max X m X s X a rm(s, a) X t xm(s, a, t) (5) Meaning Implication Linear Constraints Tie x to θ.",
                "Agent is only active when occupation measure is nonzero in original MDP states. θm(τ) = 0 =⇒ xm(s, a, τ −τa m+1) = 0 ∀s /∈ {sb , sf }, a ∈ A X s/∈{sb,sf } X a xm(s, a, t) ≤ θm(τa m + t − 1) ∀m ∈ M, ∀t ∈ [1, Tm] (6) Agent can only be active in τ ∈ (τa m, τd m) θm(τ) = 0 ∀m ∈ M, τ /∈ (τa m, τd m) (7) Cannot use resources when not active θm(τ) = 0 =⇒ Δm(τ, ω) = 0 ∀τ ∈ [0, bτ], ω ∈ Ω Δm(τ, ω) ≤ θm(τ) ∀m ∈ M, τ ∈ [0, bτ], ω ∈ Ω (8) Tie x to Δ (nonzero x forces corresponding Δ to be nonzero.)",
                "Δm(τ, ω) = 0, ϕm(a, ω) = 1 =⇒ xm(s, a, τ − τa m + 1) = 0 ∀s /∈ {sb , sf } 1/|A| X a ϕm(a, ω) X s/∈{sb,sf } xm(s, a, t) ≤ Δm(t + τa m − 1, ω) ∀m ∈ M, ω ∈ Ω, t ∈ [1, Tm] (9) Resource bounds X m Δm(τ, ω) ≤ bϕ(ω) ∀ω ∈ Ω, τ ∈ [0, bτ] (10) Agent cannot change resources while active.",
                "Only enabled for scheduling with static assignments. θm(τ) = 1 and θm(τ + 1) = 1 =⇒ Δm(τ, ω) = Δm(τ + 1, ω) Δm(τ, ω) − Z(1 − θm(τ + 1)) ≤ Δm(τ + 1, ω) + Z(1 − θm(τ)) Δm(τ, ω) + Z(1 − θm(τ + 1)) ≥ Δm(τ + 1, ω) − Z(1 − θm(τ)) ∀m ∈ M, ω ∈ Ω, τ ∈ [0, bτ] (11) Table 1: MILP for globally optimal resource scheduling.",
                "Tm = τd m − τa m + 1 is the time horizon for the agents MDP.",
                "Using this LP as a basis, we augment it with constraints that ensure that the resource usage implied by the agents occupation measures {xm} does not violate the global resource requirements bϕ at any time step τ ∈ [0, bτ].",
                "To formulate these resource constraints, we use the following binary variables: • Δm(τ, ω) = {0, 1}, ∀m ∈ M, τ ∈ [0, bτ], ω ∈ Ω, which serve as indicator variables that define whether agent m possesses resource ω at time τ.",
                "These are analogous to the static indicator variables used in the one-shot static resource-allocation problem in [6]. • θm = {0, 1}, ∀m ∈ M, τ ∈ [0, bτ] are indicator variables that specify whether agent m is active (i.e., executing its MDP) at time τ.",
                "The meaning of resource-usage variables Δ is illustrated in Figure 1: Δm(τ, ω) = 1 only if resource ω is allocated to agent m at time τ.",
                "The meaning of the activity indicators θ is illustrated in Figure 2b: when agent m is in either the start state sb or the finish state sf , the corresponding θm = 0, but once the agent becomes active and enters one of the other states, we set θm = 1 .",
                "This meaning of θ can be enforced with a linear constraint that synchronizes the values of the agents occupation measures xm and the activity 1224 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) indicators θ, as shown in (6) in Table 1.",
                "Another constraint we have to add-because the activity indicators θ are defined on the global timeline τ-is to enforce the fact that the agent is inactive outside of its arrivaldeparture window.",
                "This is accomplished by constraint (7) in Table 1.",
                "Furthermore, agents should not be using resources while they are inactive.",
                "This constraint can also be enforced via a linear inequality on θ and Δ, as shown in (8).",
                "Constraint (6) sets the value of θ to match the policy defined by the occupation measure xm.",
                "In a similar fashion, we have to make sure that the resource-usage variables Δ are also synchronized with the occupation measure xm.",
                "This is done via constraint (9) in Table 1, which is nearly identical to the analogous constraint from [6].",
                "After implementing the above constraint, which enforces the meaning of Δ, we add a constraint that ensures that the agents resource usage never exceeds the amounts of available resources.",
                "This condition is also trivially expressed as a linear inequality (10) in Table 1.",
                "Finally, for the problem formulation where resource assignments are static during a lifetime of an agent, we add a constraint that ensures that the resource-usage variables Δ do not change their value while the agent is active (θ = 1).",
                "This is accomplished via the linear constraint (11), where Z ≥ 2 is a constant that is used to turn off the constraints when θm(τ) = 0 or θm(τ + 1) = 0.",
                "This constraint is not used for the dynamic problem formulation, where resources can be reallocated between agents at every time step.",
                "To summarize, Table 1 together with the conservationof-flow constraints from (12) defines the MILP that simultaneously computes an optimal resource assignment for all agents across time as well as optimal finite-horizon MDP policies that are valid under that resource assignment.",
                "As a rough measure of the complexity of this MILP, let us consider the number of optimization variables and constraints.",
                "Let TM = P Tm = P m(τa m − τd m + 1) be the sum of the lengths of the arrival-departure windows across all agents.",
                "Then, the number of optimization variables is: TM + bτ|M||Ω| + bτ|M|, TM of which are continuous (xm), and bτ|M||Ω| + bτ|M| are binary (Δ and θ).",
                "However, notice that all but TM|M| of the θ are set to zero by constraint (7), which also immediately forces all but TM|M||Ω| of the Δ to be zero via the constraints (8).",
                "The number of constraints (not including the degenerate constraints in (7)) in the MILP is: TM + TM|Ω| + bτ|Ω| + bτ|M||Ω|.",
                "Despite the fact that the complexity of the MILP is, in the worst case, exponential1 in the number of binary variables, the complexity of this MILP is significantly (exponentially) lower than that of the MILP with flat utility functions, described in Section 2.2.",
                "This result echos the efficiency gains reported in [6] for single-shot resource-allocation problems, but is much more pronounced, because of the explosion of the flat utility representation due to the temporal aspect of the problem (recall the prohibitive complexity of the combinatorial optimization in Section 2.2).",
                "We empirically analyze the performance of this method in Section 5. 1 Strictly speaking, solving MILPs to optimality is NPcomplete in the number of integer variables. 5.",
                "EXPERIMENTAL RESULTS Although the complexity of solving MILPs is in the worst case exponential in the number of integer variables, there are many efficient methods for solving MILPs that allow our algorithm to scale well for parameters common to resource allocation and scheduling problems.",
                "In particular, this section introduces a problem domain-the repairshop problem-used to empirically evaluate our algorithms scalability in terms of the number of agents |M|, the number of shared resources |Ω|, and the varied lengths of global time bτ during which agents may enter and exit the system.",
                "The repairshop problem is a simple parameterized MDP adopting the metaphor of a vehicular repair shop.",
                "Agents in the repair shop are mechanics with a number of independent tasks that yield reward only when completed.",
                "In our MDP model of this system, actions taken to advance through the state space are only allowed if the agent holds certain resources that are publicly available to the shop.",
                "These resources are in finite supply, and optimal policies for the shop will determine when each agent may hold the limited resources to take actions and earn individual rewards.",
                "Each task to be completed is associated with a single action, although the agent is required to repeat the action numerous times before completing the task and earning a reward.",
                "This model was parameterized in terms of the number of agents in the system, the number of different types of resources that could be linked to necessary actions, a global time during which agents are allowed to arrive and depart, and a maximum length for the number of time steps an agent may remain in the system.",
                "All datapoints in our experiments were obtained with 20 evaluations using CPLEX to solve the MILPs on a Pentium4 computer with 2Gb of RAM.",
                "Trials were conducted on both the static and the dynamic version of the resourcescheduling problem, as defined earlier.",
                "Figure 3 shows the runtime and policy value for independent modifications to the parameter set.",
                "The top row shows how the solution time for the MILP scales as we increase the number of agents |M|, the global time horizon bτ, and the number of resources |Ω|.",
                "Increasing the number of agents leads to exponential complexity scaling, which is to be expected for an NP-complete problem.",
                "However, increasing the global time limit bτ or the total number of resource types |Ω|-while holding the number of agents constantdoes not lead to decreased performance.",
                "This occurs because the problems get easier as they become under-constrained, which is also a common phenomenon for NP-complete problems.",
                "We also observe that the solution to the dynamic version of the problem can often be computed much faster than the static version.",
                "The bottom row of Figure 3 shows the joint policy value of the policies that correspond to the computed optimal resource-allocation schedules.",
                "We can observe that the dynamic version yields higher reward (as expected, since the reward for the dynamic version is always no less than the reward of the static version).",
                "We should point out that these graphs should not be viewed as a measure of performance of two different algorithms (both algorithms produce optimal solutions but to different problems), but rather as observations about how the quality of optimal solutions change as more flexibility is allowed in the reallocation of resources.",
                "Figure 4 shows runtime and policy value for trials in which common input variables are scaled together.",
                "This allows The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1225 2 4 6 8 10 10 −3 10 −2 10 −1 10 0 10 1 10 2 10 3 10 4 Number of Agents |M| CPUTime,sec |Ω| = 5, τ = 50 static dynamic 50 100 150 200 10 −2 10 −1 10 0 10 1 10 2 10 3 Global Time Boundary τ CPUTime,sec |M| = 5, |Ω| = 5 static dynamic 10 20 30 40 50 10 −2 10 −1 10 0 10 1 10 2 Number of Resources |Ω| CPUTime,sec |M| = 5, τ = 50 static dynamic 2 4 6 8 10 200 400 600 800 1000 1200 1400 1600 Number of Agents |M| Value |Ω| = 5, τ = 50 static dynamic 50 100 150 200 400 500 600 700 800 900 1000 1100 1200 1300 1400 Global Time Boundary τ Value |M| = 5, |Ω| = 5 static dynamic 10 20 30 40 50 500 600 700 800 900 1000 1100 1200 1300 1400 Number of Resources |Ω| Value |M| = 5, τ = 50 static dynamic Figure 3: Evaluation of our MILP for variable numbers of agents (column 1), lengths of global-time window (column 2), and numbers of resource types (column 3).",
                "Top row shows CPU time, and bottom row shows the joint reward of agents MDP policies.",
                "Error bars show the 1st and 3rd quartiles (25% and 75%). 2 4 6 8 10 10 −3 10 −2 10 −1 10 0 10 1 10 2 10 3 Number of Agents |M| CPUTime,sec τ = 10|M| static dynamic 2 4 6 8 10 10 −3 10 −2 10 −1 10 0 10 1 10 2 10 3 10 4 Number of Agents |M| CPUTime,sec |Ω| = 2|M| static dynamic 2 4 6 8 10 10 −3 10 −2 10 −1 10 0 10 1 10 2 10 3 10 4 Number of Agents |M| CPUTime,sec |Ω| = 5|M| static dynamic 2 4 6 8 10 200 400 600 800 1000 1200 1400 1600 1800 2000 2200 Number of Agents |M| Value τ = 10|M| static dynamic 2 4 6 8 10 200 400 600 800 1000 1200 1400 1600 1800 2000 Number of Agents |M| Value |Ω| = 2|M| static dynamic 2 4 6 8 10 0 500 1000 1500 2000 2500 Number of Agents |M| Value |Ω| = 5|M| static dynamic Figure 4: Evaluation of our MILP using correlated input variables.",
                "The left column tracks the performance and CPU time as the number of agents and global-time window increase together (bτ = 10|M|).",
                "The middle and the right column track the performance and CPU time as the number of resources and the number of agents increase together as |Ω| = 2|M| and |Ω| = 5|M|, respectively.",
                "Error bars show the 1st and 3rd quartiles (25% and 75%). 1226 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) us to explore domains where the total number of agents scales proportionally to the total number of resource types or the global time horizon, while keeping constant the average agent density (per unit of global time) or the average number of resources per agent (which commonly occurs in real-life applications).",
                "Overall, we believe that these experimental results indicate that our MILP formulation can be used to effectively solve resource-scheduling problems of nontrivial size. 6.",
                "DISCUSSION AND CONCLUSIONS Throughout the paper, we have made a number of assumptions in our model and solution algorithm; we discuss their implications below. • Continual execution.",
                "We assume that once an agent stops executing its MDP (transitions into state sf ), it exits the system and cannot return.",
                "It is easy to relax this assumption for domains where agents MDPs can be paused and restarted.",
                "All that is required is to include an additional pause action which transitions from a given state back to itself, and has zero reward. • Indifference to start time.",
                "We used a reward model where agents rewards depend only on the time horizon of their MDPs and not the global start time.",
                "This is a consequence of our MDP-augmentation procedure from Section 4.1.",
                "It is easy to extend the model so that the agents incur an explicit penalty for idling by assigning a non-zero negative reward to the start state sb . • Binary resource requirements.",
                "For simplicity, we have assumed that resource costs are binary: ϕm(a, ω) = {0, 1}, but our results generalize in a straightforward manner to non-binary resource mappings, analogously to the procedure used in [5]. • Cooperative agents.",
                "The optimization procedure discussed in this paper was developed in the context of cooperative agents, but it can also be used to design a mechanism for scheduling resources among self-interested agents.",
                "This optimization procedure can be embedded in a VickreyClarke-Groves auction, completely analogously to the way it was done in [7].",
                "In fact, all the results of [7] about the properties of the auction and information privacy directly carry over to the scheduling domain discussed in this paper, requiring only slight modifications to deal with finitehorizon MDPs. • Known, deterministic arrival and departure times.",
                "Finally, we have assumed that agents arrival and departure times (τa m and τd m) are deterministic and known a priori.",
                "This assumption is fundamental to our solution method.",
                "While there are many domains where this assumption is valid, in many cases agents arrive and depart dynamically and their arrival and departure times can only be predicted probabilistically, leading to online resource-allocation problems.",
                "In particular, in the case of self-interested agents, this becomes an interesting version of an online-mechanism-design problem [11, 12].",
                "In summary, we have presented an MILP formulation for the combinatorial resource-scheduling problem where agents values for possible resource assignments are defined by finitehorizon MDPs.",
                "This result extends previous work ([6, 7]) on static one-shot resource allocation under MDP-induced preferences to resource-scheduling problems with a temporal aspect.",
                "As such, this work takes a step in the direction of designing an online mechanism for agents with combinatorial resource preferences induced by stochastic planning problems.",
                "Relaxing the assumption about deterministic arrival and departure times of the agents is a focus of our future work.",
                "We would like to thank the anonymous reviewers for their insightful comments and suggestions. 7.",
                "REFERENCES [1] E. Altman and A. Shwartz.",
                "Adaptive control of constrained Markov chains: Criteria and policies.",
                "Annals of Operations Research, special issue on Markov Decision Processes, 28:101-134, 1991. [2] R. Bellman.",
                "Dynamic Programming.",
                "Princeton University Press, 1957. [3] C. Boutilier.",
                "Solving concisely expressed combinatorial auction problems.",
                "In Proc. of AAAI-02, pages 359-366, 2002. [4] C. Boutilier and H. H. Hoos.",
                "Bidding languages for combinatorial auctions.",
                "In Proc. of IJCAI-01, pages 1211-1217, 2001. [5] D. Dolgov.",
                "Integrated Resource Allocation and Planning in Stochastic Multiagent Environments.",
                "PhD thesis, Computer Science Department, University of Michigan, February 2006. [6] D. A. Dolgov and E. H. Durfee.",
                "Optimal resource allocation and policy formulation in loosely-coupled Markov decision processes.",
                "In Proc. of ICAPS-04, pages 315-324, June 2004. [7] D. A. Dolgov and E. H. Durfee.",
                "Computationally efficient combinatorial auctions for resource allocation in weakly-coupled MDPs.",
                "In Proc. of AAMAS-05, New York, NY, USA, 2005.",
                "ACM Press. [8] D. A. Dolgov and E. H. Durfee.",
                "Resource allocation among agents with preferences induced by factored MDPs.",
                "In Proc. of AAMAS-06, 2006. [9] K. Larson and T. Sandholm.",
                "Mechanism design and deliberative agents.",
                "In Proc. of AAMAS-05, pages 650-656, New York, NY, USA, 2005.",
                "ACM Press. [10] N. Nisan.",
                "Bidding and allocation in combinatorial auctions.",
                "In Electronic Commerce, 2000. [11] D. C. Parkes and S. Singh.",
                "An MDP-based approach to Online Mechanism Design.",
                "In Proc. of the Seventeenths Annual Conference on Neural Information Processing Systems (NIPS-03), 2003. [12] D. C. Parkes, S. Singh, and D. Yanovsky.",
                "Approximately efficient online mechanism design.",
                "In Proc. of the Eighteenths Annual Conference on Neural Information Processing Systems (NIPS-04), 2004. [13] M. L. Puterman.",
                "Markov Decision Processes.",
                "John Wiley & Sons, New York, 1994. [14] M. H. Rothkopf, A. Pekec, and R. M. Harstad.",
                "Computationally manageable combinational auctions.",
                "Management Science, 44(8):1131-1147, 1998. [15] T. Sandholm.",
                "An algorithm for optimal winner determination in combinatorial auctions.",
                "In Proc. of IJCAI-99, pages 542-547, San Francisco, CA, USA, 1999.",
                "Morgan Kaufmann Publishers Inc.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1227"
            ],
            "original_annotated_samples": [
                "In particular, when the value of a set of resources to an agent is not additive (as is often the case with resources that are substitutes or complements), the <br>utility function</br> might have to be defined on an exponentially large space of resource bundles, which very quickly becomes computationally intractable.",
                "Further, even when each agent has a <br>utility function</br> that is nonzero only on a small subset of the possible resource bundles, obtaining optimal allocation is still computationally prohibitive, as the problem becomes NP-complete [14].",
                "A way of accomplishing this is to model the processes by which an agent might utilize the resources and define the <br>utility function</br> as the payoff of these processes.",
                "In particular, if an agent uses resources to act in a stochastic environment, its <br>utility function</br> can be naturally modeled with a Markov decision process, whose action set is parameterized by the available resources."
            ],
            "translated_annotated_samples": [
                "En particular, cuando el valor de un conjunto de recursos para un agente no es aditivo (como suele ser el caso con recursos que son sustitutos o complementos), la <br>función de utilidad</br> podría tener que definirse en un espacio exponencialmente grande de combinaciones de recursos, lo que rápidamente se vuelve computacionalmente intratable.",
                "Además, incluso cuando cada agente tiene una <br>función de utilidad</br> que es distinta de cero solo en un pequeño subconjunto de los posibles conjuntos de recursos, obtener una asignación óptima sigue siendo computacionalmente prohibitivo, ya que el problema se vuelve NP-completo [14].",
                "Una forma de lograr esto es modelar los procesos mediante los cuales un agente podría utilizar los recursos y definir la <br>función de utilidad</br> como la ganancia de estos procesos.",
                "En particular, si un agente utiliza recursos para actuar en un entorno estocástico, su <br>función de utilidad</br> puede ser modelada de forma natural con un proceso de decisión de Markov, cuyo conjunto de acciones está parametrizado por los recursos disponibles."
            ],
            "translated_text": "La programación de recursos combinatoria para MDPs multiagentes. Dmitri A. Dolgov, Michael R. James y Michael E. Samples. Grupo de Investigación en IA y Robótica, Centro Técnico de Toyota en EE. UU. {ddolgov, michael.r.james, michael.samples}@gmail.com. RESUMEN La programación óptima de recursos en sistemas multiagentes es una tarea computacionalmente desafiante, especialmente cuando los valores de los recursos no son aditivos. Consideramos el problema combinatorio de programar el uso de múltiples recursos entre agentes que operan en entornos estocásticos, modelados como procesos de decisión de Markov (MDPs). En los últimos años, se han desarrollado algoritmos eficientes de asignación de recursos para agentes con valores de recursos inducidos por MDPs. Sin embargo, este trabajo previo se ha centrado en problemas de asignación de recursos estáticos donde los recursos se distribuyen una vez y luego se utilizan en MDPs de horizonte infinito. Extendemos esos modelos existentes al problema de la programación de recursos combinatoria, donde los agentes persisten solo por períodos finitos entre sus tiempos de llegada y salida (predefinidos), requiriendo recursos solo durante esos períodos de tiempo. Proporcionamos un procedimiento computacionalmente eficiente para calcular asignaciones de recursos óptimas a nivel global a agentes a lo largo del tiempo. Ilustramos y analizamos empíricamente el método en el contexto de un dominio de programación de trabajos estocásticos. Categorías y Descriptores de Asignaturas I.2.8 [Inteligencia Artificial]: Resolución de Problemas, Métodos de Control y Búsqueda; I.2.11 [Inteligencia Artificial]: Inteligencia Artificial Distribuida-Sistemas Multiagente Términos Generales Algoritmos, Rendimiento, Diseño 1. INTRODUCCIÓN Las tareas de asignación óptima de recursos y programación son ubicuas en sistemas multiagentes, pero resolver tales problemas de optimización puede ser computacionalmente difícil, debido a una serie de factores. En particular, cuando el valor de un conjunto de recursos para un agente no es aditivo (como suele ser el caso con recursos que son sustitutos o complementos), la <br>función de utilidad</br> podría tener que definirse en un espacio exponencialmente grande de combinaciones de recursos, lo que rápidamente se vuelve computacionalmente intratable. Además, incluso cuando cada agente tiene una <br>función de utilidad</br> que es distinta de cero solo en un pequeño subconjunto de los posibles conjuntos de recursos, obtener una asignación óptima sigue siendo computacionalmente prohibitivo, ya que el problema se vuelve NP-completo [14]. Tales problemas computacionales han dado lugar recientemente a varias líneas de trabajo en el uso de modelos compactos de las preferencias de los agentes. Una idea es utilizar cualquier estructura presente en las funciones de utilidad para representarlas de forma compacta, por ejemplo, a través de fórmulas lógicas [15, 10, 4, 3]. Una alternativa es modelar directamente los mecanismos que definen las funciones de utilidad de los agentes y realizar la asignación de recursos directamente con estos modelos [9]. Una forma de lograr esto es modelar los procesos mediante los cuales un agente podría utilizar los recursos y definir la <br>función de utilidad</br> como la ganancia de estos procesos. En particular, si un agente utiliza recursos para actuar en un entorno estocástico, su <br>función de utilidad</br> puede ser modelada de forma natural con un proceso de decisión de Markov, cuyo conjunto de acciones está parametrizado por los recursos disponibles. Esta representación puede ser utilizada para construir algoritmos de asignación de recursos muy eficientes que resultan en una aceleración exponencial sobre un problema de optimización directa con representaciones planas de preferencias combinatorias [6, 7, 8]. Sin embargo, este trabajo existente sobre asignación de recursos con preferencias inducidas por MDPs parametrizados por recursos hace una suposición de que los recursos se asignan solo una vez y luego son utilizados por los agentes de forma independiente dentro de sus MDPs de horizonte infinito. Esta suposición de que no es posible la reasignación de recursos puede ser limitante en dominios donde los agentes llegan y se van dinámicamente. En este artículo, ampliamos el trabajo sobre asignación de recursos bajo preferencias inducidas por MDP a problemas de programación en tiempo discreto, donde los agentes están presentes en el sistema durante intervalos de tiempo finitos y solo pueden utilizar recursos dentro de estos intervalos. En particular, los agentes llegan y parten en momentos arbitrarios (predefinidos) y dentro de estos intervalos utilizan recursos para ejecutar tareas en MDPs de horizonte finito. Abordamos el problema de la programación de recursos óptima a nivel global, donde el objetivo es encontrar una asignación de recursos a los agentes a lo largo del tiempo que maximice la suma de las recompensas esperadas que obtienen. En este contexto, nuestra principal contribución es una formulación de programación entera mixta del problema de programación que elige asignaciones de recursos globalmente óptimas, tiempos de inicio y horizontes de ejecución para todos los agentes (dentro de sus intervalos de llegada y salida). Analizamos y comparamos empíricamente dos variantes del problema de programación: uno, donde los agentes tienen asignaciones de recursos estáticas dentro de sus MDPs de horizonte finito, y otro, donde los recursos pueden ser realocados dinámicamente entre agentes en cada paso de tiempo. En el resto del documento, primero establecemos las bases necesarias en la Sección 2 y luego presentamos nuestro modelo y declaración formal del problema en la Sección 3. En la Sección 4.2, describimos nuestro resultado principal, el programa de optimización para la programación de recursos globalmente óptima. Tras la discusión de nuestros resultados experimentales sobre un problema de programación de trabajos en la Sección 5, concluimos en la Sección 6 con una discusión de posibles extensiones y generalizaciones de nuestro método. De manera similar al modelo utilizado en trabajos anteriores sobre asignación de recursos con preferencias inducidas por MDP [6, 7], definimos el valor de un conjunto de recursos para un agente como el valor de la mejor política MDP que es realizable, dadas esos recursos. Sin embargo, dado que el enfoque de nuestro trabajo se centra en problemas de programación, y una gran parte del problema de optimización consiste en decidir cómo se asignan los recursos en el tiempo entre agentes con tiempos finitos de llegada y salida, modelamos los problemas de planificación de los agentes como MDPs de horizonte finito, en contraste con trabajos anteriores que utilizaron MDPs descontados de horizonte infinito. En el resto de esta sección, primero presentamos algo de antecedentes necesarios sobre MDPs de horizonte finito y presentamos una formulación de programación lineal que sirve como base para nuestro algoritmo de solución desarrollado en la Sección 4. También describimos los métodos estándar para la programación de recursos combinatorios con valores de recursos planos, que sirven como punto de referencia de comparación para el nuevo modelo desarrollado aquí. 2.1 Procesos de Decisión de Markov Un MDP (Proceso de Decisión de Markov) estacionario, de dominio finito y discreto en el tiempo (ver, por ejemplo, [13] para un desarrollo detallado y exhaustivo) se puede describir como S, A, p, r, donde: S es un conjunto finito de estados del sistema; A es un conjunto finito de acciones disponibles para el agente; p es una función de transición estocástica estacionaria, donde p(σ|s, a) es la probabilidad de transicionar al estado σ al ejecutar la acción a en el estado s; r es una función de recompensa estacionaria, donde r(s, a) especifica la recompensa obtenida al ejecutar la acción a en el estado s. Dado un MDP de este tipo, un problema de decisión bajo un horizonte finito T consiste en elegir una acción óptima en cada paso de tiempo para maximizar el valor esperado de la recompensa total acumulada durante la vida (finita) de los agentes. La política óptima del agente es entonces una función del estado actual s y el tiempo hasta el horizonte. Una política óptima para tal problema es actuar de forma codiciosa con respecto a la función de valor óptimo, definida de forma recursiva por el siguiente sistema de ecuaciones de Bellman de tiempo finito [2]: v(s, t) = max a r(s, a) + X σ p(σ|s, a)v(σ, t + 1), ∀s ∈ S, t ∈ [1, T − 1]; v(s, T) = 0, ∀s ∈ S; donde v(s, t) es el valor óptimo de estar en el estado s en el tiempo t ∈ [1, T]. Esta función de valor óptimo se puede calcular fácilmente utilizando programación dinámica, lo que conduce a la siguiente política óptima π, donde π(s, a, t) es la probabilidad de ejecutar la acción a en el estado s en el tiempo t: π(s, a, t) = (1, a = argmaxa r(s, a) + P σ p(σ|s, a)v(σ, t + 1), 0, en caso contrario. La anterior es la forma más común de calcular la función de valor óptimo (y por lo tanto una política óptima) para un MDP de horizonte finito. Sin embargo, también podemos formular el problema como el siguiente programa lineal (similarmente al LP dual para MDPs descontados de horizonte infinito [13, 6, 7]): max X s X a r(s, a) X t x(s, a, t) sujeto a: X a x(σ, a, t + 1) = X s,a p(σ|s, a)x(s, a, t) ∀σ, t ∈ [1, T − 1]; X a x(s, a, 1) = α(s), ∀s ∈ S; (1) donde α(s) es la distribución inicial sobre el espacio de estados, y x es la medida de ocupación (no estacionaria) (x(s, a, t) ∈ [0, 1] es el número total esperado de veces que se ejecuta la acción a en el estado s en el tiempo t). Una política óptima (no estacionaria) se obtiene a partir de la medida de ocupación de la siguiente manera: π(s, a, t) = x(s, a, t)/ X a x(s, a, t) ∀s ∈ S, t ∈ [1, T]. (2) Tenga en cuenta que el MDP estándar de horizonte finito sin restricciones, como se describe anteriormente, siempre tiene una solución uniformemente óptima (óptima para cualquier distribución inicial α(s)). Por lo tanto, una política óptima se puede obtener utilizando una constante arbitraria α(s) > 0 (en particular, α(s) = 1 resultará en x(s, a, t) = π(s, a, t)). Sin embargo, para los MDPs con restricciones de recursos (como se definen a continuación en la Sección 3), las políticas óptimas uniformemente no existen en general. En tales casos, α se convierte en parte de la entrada del problema, y una política resultante solo es óptima para ese α en particular. Este resultado es bien conocido para MDPs de horizonte infinito con varios tipos de restricciones [1, 6], y también se cumple para nuestro modelo de horizonte finito, el cual puede establecerse fácilmente a través de una línea de razonamiento completamente análoga a los argumentos en [6]. 2.2 Programación de Recursos Combinatoria Un enfoque directo para la programación de recursos para un conjunto de agentes M, cuyos valores para los recursos son inducidos por problemas de planificación estocástica (en nuestro caso, MDPs de horizonte finito) sería que cada agente enumerara todas las asignaciones de recursos posibles a lo largo del tiempo y, para cada una, calcular su valor resolviendo el MDP correspondiente. Entonces, cada agente proporcionaría valoraciones para cada posible conjunto de recursos a lo largo del tiempo a un coordinador centralizado, quien calcularía las asignaciones óptimas de recursos a lo largo del tiempo basándose en estas valoraciones. Cuando los recursos pueden asignarse en diferentes momentos a diferentes agentes, cada agente debe presentar valoraciones para cada combinación de posibles horizontes temporales. Que cada agente m ∈ M ejecute su MDP dentro del intervalo de tiempo de llegada-salida τ ∈ [τa m, τd m]. Por lo tanto, el agente m ejecutará un MDP con un horizonte temporal no mayor que Tm = τd m−τa m+1. Que bτ sea el horizonte temporal global para el problema, antes del cual todos los MDP de los agentes deben finalizar. Suponemos que τd m < bτ, ∀m ∈ M. El Sexto Congreso Internacional. En el problema de programación donde los agentes tienen requisitos de recursos estáticos dentro de sus MDPs de horizonte finito, los agentes proporcionan una valoración para cada conjunto de recursos para cada horizonte de tiempo posible (desde [1, Tm]) que puedan utilizar. Sea Ω el conjunto de recursos a ser asignados entre los agentes. Un agente recibirá como máximo un paquete de recursos para uno de los horizontes temporales. Que la variable ψ ∈ Ψm enumere todos los posibles pares de conjuntos de recursos y horizontes temporales para el agente m, por lo que hay 2|Ω| × Tm valores para ψ (el espacio de conjuntos es exponencial en el número de tipos de recursos |Ω|). El agente m debe proporcionar un valor vψ m para cada ψ, y el coordinador asignará como máximo un par (recurso, horizonte temporal) ψ a cada agente. Esta asignación se expresa como una variable indicadora zψ m ∈ {0, 1} que muestra si ψ está asignado al agente m. Para el tiempo τ y el recurso ω, la función nm(ψ, τ, ω) ∈ {0, 1} indica si el paquete en ψ utiliza el recurso ω en el tiempo τ (hacemos la suposición de que los agentes tienen requisitos de recursos binarios). Este problema de asignación es NP-completo, incluso al considerar solo un paso de tiempo, y su dificultad aumenta significativamente con múltiples pasos de tiempo debido al creciente número de valores de ψ. El problema de encontrar una asignación óptima que satisfaga la restricción global de que la cantidad de cada recurso ω asignada a todos los agentes no exceda la cantidad disponible bϕ(ω) se puede expresar como el siguiente programa entero: max X m∈M X ψ∈Ψm zψ mvψ m sujeto a: X ψ∈Ψm zψ m ≤ 1, ∀m ∈ M; X m∈M X ψ∈Ψm zψ mnm(ψ, τ, ω) ≤ bϕ(ω), ∀τ ∈ [1, bτ], ∀ω ∈ Ω; (3) La primera restricción en la ecuación 3 dice que ningún agente puede recibir más de un paquete, y la segunda restricción asegura que la asignación total del recurso ω no exceda, en ningún momento, el límite del recurso. Para el problema de programación en el que los agentes pueden reasignar dinámicamente recursos, cada agente debe especificar un valor para cada combinación de paquetes y pasos de tiempo dentro de su horizonte temporal. Que la variable ψ ∈ Ψm en este caso enumere todos los posibles conjuntos de recursos para los cuales a lo sumo un conjunto puede ser asignado al agente m en cada paso de tiempo. Por lo tanto, en este caso hay P t∈[1,Tm](2|Ω| )t ∼ 2|Ω|Tm posibilidades de combinaciones de recursos asignadas a diferentes intervalos de tiempo, para los Tm horizontes de tiempo diferentes. El mismo conjunto de ecuaciones (3) se puede utilizar para resolver este problema de programación dinámica, pero el programa entero es diferente debido a la diferencia en cómo se define ψ. En este caso, el número de valores de ψ es exponencial en el horizonte de planificación Tm de cada agente, lo que resulta en un programa mucho más grande. Este enfoque directo para resolver ambos problemas de programación requiere una enumeración y solución de 2|Ω| Tm (asignación estática) o P t∈[1,Tm] 2|Ω|t (reubicación dinámica) MDPs para cada agente, lo cual rápidamente se vuelve intratable con el crecimiento del número de recursos |Ω| o el horizonte de tiempo Tm. MODELO Y DECLARACIÓN DEL PROBLEMA Ahora presentamos formalmente nuestro modelo del problema de programación de recursos. La entrada del problema consiste en los siguientes componentes: • M, Ω, bϕ, τa m, τd m, bτ están definidos arriba en la Sección 2.2. • {Θm} = {S, A, pm, rm, αm} son los MDPs de todos los agentes m ∈ M. Sin pérdida de generalidad, asumimos que los espacios de estados y acciones de todos los agentes son iguales, pero cada uno tiene su propia función de transición pm, función de recompensa rm, y condiciones iniciales αm. • ϕm : A×Ω → {0, 1} es la asignación de acciones a recursos para el agente m. ϕm(a, ω) indica si la acción a del agente m necesita el recurso ω. Un agente m que recibe un conjunto de recursos que no incluye el recurso ω no puede ejecutar en su política MDP ninguna acción a para la cual ϕm(a, ω) = 0. Suponemos que todos los requisitos de recursos son binarios; como se discute más adelante en la Sección 6, esta suposición no es limitante. Dado el input anterior, el problema de optimización que consideramos es encontrar la asignación globalmente óptima que maximice la suma de recompensas esperadas de recursos a agentes para todos los pasos de tiempo: Δ : τ × M × Ω → {0, 1}. Una solución es factible si la asignación correspondiente de recursos a los agentes no viola la restricción global de recursos: X m Δm(τ, ω) ≤ bϕ(ω), ∀ω ∈ Ω, τ ∈ [1, bτ]. Consideramos dos variantes del problema de programación de recursos. La primera formulación restringe las asignaciones de recursos al espacio donde la asignación de recursos a cada agente es estática durante la vida de los agentes. La segunda formulación permite la reasignación de recursos entre agentes en cada paso de tiempo durante sus vidas. La Figura 1 representa un problema de programación de recursos con tres agentes M = {m1, m2, m3}, tres recursos Ω = {ω1, ω2, ω3}, y un horizonte de problema global de bτ = 11. Los tiempos de llegada y salida de los agentes se muestran como cajas grises y son {1, 6}, {3, 7} y {2, 11}, respectivamente. Una solución a este problema se muestra a través de barras horizontales dentro de cada casilla de agentes, donde las barras corresponden a la asignación de los tres tipos de recursos. La Figura 1a muestra una solución a un problema de programación estática. Según la solución mostrada, el agente m1 comienza la ejecución de su MDP en el tiempo τ = 1 y tiene un bloqueo en los tres recursos hasta que finaliza la ejecución en el tiempo τ = 3. Se debe tener en cuenta que el agente m1 renuncia a su control sobre los recursos antes de su hora de salida anunciada de τd m1 = 6, aparentemente porque otros agentes pueden utilizar los recursos de manera más efectiva. Por lo tanto, en el tiempo τ = 4, los recursos ω1 y ω3 se asignan al agente m2, quien luego los utiliza para ejecutar su MDP (utilizando solo acciones admitidas por los recursos ω1 y ω3) hasta el tiempo τ = 7. El agente m3 posee el recurso ω3 durante el intervalo τ ∈ [4, 10]. La Figura 1b muestra una posible solución a la versión dinámica del mismo problema. Allí, los recursos pueden ser reasignados entre agentes en cada paso de tiempo. Por ejemplo, el agente m1 renuncia a su uso del recurso ω2 en el tiempo τ = 2, aunque continúa la ejecución de su MDP hasta el tiempo τ = 6. Ten en cuenta que a un agente no se le permite detener y reiniciar su MDP, por lo que el agente m1 solo puede continuar ejecutando en el intervalo τ ∈ [3, 4] si tiene acciones que no requieren recursos (ϕm(a, ω) = 0). Claramente, el modelo y la declaración del problema descritos anteriormente hacen una serie de suposiciones sobre el problema y las propiedades de la solución deseada. Discutimos algunas de esas suposiciones y sus implicaciones en la Sección 6. 1222 La Sexta Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) (a) (b) Figura 1: Ilustración de una solución a un problema de programación de recursos con tres agentes y tres recursos: a) asignaciones de recursos estáticas (las asignaciones de recursos son constantes durante la vida de los agentes); b) asignación dinámica (las asignaciones de recursos pueden cambiar en cada paso de tiempo). 4. PROGRAMACIÓN DE RECURSOS Nuestro algoritmo de programación de recursos se lleva a cabo en dos etapas. Primero, realizamos un paso de preprocesamiento que aumenta los MDP de los agentes; este proceso se describe en la Sección 4.1. Segundo, utilizando estos MDPs aumentados construimos un problema de optimización global, el cual se describe en la Sección 4.2. 4.1 Aumentando los MDPs de los Agentes En el modelo descrito en la sección anterior, asumimos que si un agente no posee los recursos necesarios para realizar acciones en su MDP, su ejecución se detiene y el agente abandona el sistema. En otras palabras, los MDPs no pueden ser pausados y reanudados. Por ejemplo, en el problema mostrado en la Figura 1a, el agente m1 libera todos los recursos después del tiempo τ = 3, momento en el cual la ejecución de su MDP se detiene. De manera similar, los agentes m2 y m3 solo ejecutan sus MDPs en los intervalos τ ∈ [4, 6] y τ ∈ [4, 10], respectivamente. Por lo tanto, una parte importante del problema de toma de decisiones global es decidir la ventana de tiempo durante la cual cada uno de los agentes está activo (es decir, ejecutando su MDP). Para lograr esto, ampliamos el MDP de cada agente con dos nuevos estados (estados de inicio y final sb, sf, respectivamente) y una nueva acción de inicio/parada a∗, como se ilustra en la Figura 2. La idea es que un agente permanezca en el estado inicial sb hasta que esté listo para ejecutar su MDP, momento en el que realiza la acción de inicio/parada a∗ y transita al espacio de estados del MDP original con la probabilidad de transición que corresponde a la distribución inicial original α(s). Por ejemplo, en la Figura 1a, para el agente m2 esto ocurriría en el tiempo τ = 4. Una vez que el agente llega al final de su ventana de actividad (tiempo τ = 6 para el agente m2 en la Figura 1a), realiza la acción de inicio/parada, lo que lo lleva al estado final de finalización del sumidero sf en el tiempo τ = 7. Más precisamente, dado un MDP S, A, pm, rm, αm, definimos un MDP aumentado S, A, pm, rm, αm de la siguiente manera: S = S ∪ sb ∪ sf; A = A ∪ a∗; p(s|sb, a∗) = α(s), ∀s ∈ S; p(sb|sb, a) = 1.0, ∀a ∈ A; p(sf|s, a∗) = 1.0, ∀s ∈ S; p(σ|s, a) = p(σ|s, a), ∀s, σ ∈ S, a ∈ A; r(sb, a) = r(sf, a) = 0, ∀a ∈ A; r(s, a) = r(s, a), ∀s ∈ S, a ∈ A; α(sb) = 1; α(s) = 0, ∀s ∈ S; donde se asume que todas las probabilidades de transición no especificadas son cero. Además, para tener en cuenta el nuevo estado inicial, comenzamos el MDP un paso de tiempo antes, estableciendo τa m ← τa m − 1. Esto no afectará la asignación de recursos debido a que las restricciones de recursos solo se aplican a los estados originales del MDP, como se discutirá en la siguiente sección. Por ejemplo, los MDPs aumentados mostrados en la Figura 2b (que comienza en el estado sb en el tiempo τ = 2) se construirían a partir de un MDP con tiempo de llegada original τ = 3. La Figura 2b también muestra una trayectoria de muestra a través del espacio de estados: el agente comienza en el estado sb, transita al espacio de estados S del MDP original y finalmente sale al estado de sumidero sf. Cabe destacar que si quisiéramos modelar un problema en el que los agentes pudieran pausar sus MDP en pasos de tiempo arbitrarios (lo cual podría ser útil para dominios donde la reasignación dinámica es posible), podríamos lograrlo fácilmente incluyendo una acción adicional que transiciona de cada estado a sí mismo con una recompensa de cero. 4.2 MILP para la programación de recursos Dado un conjunto de MDPs aumentados, como se define arriba, el objetivo de esta sección es formular un programa de optimización global que resuelva el problema de programación de recursos. En esta sección y a continuación, se asume que todos los MDPs son los MDPs aumentados tal como se definen en la Sección 4.1. Nuestro enfoque es similar a la idea utilizada en [6]: comenzamos con la formulación del programa lineal de los MDP de los agentes (1) y lo complementamos con restricciones que aseguran que la asignación de recursos correspondiente entre agentes y tiempo sea válida. El problema de optimización resultante resuelve simultáneamente los MDP de los agentes y los problemas de programación de recursos. En el resto de esta sección, desarrollamos de forma incremental un programa de enteros mixtos (MILP) que logra esto. En ausencia de restricciones de recursos, los MDPs de horizonte finito de los agentes son completamente independientes, y la solución óptima global puede obtenerse trivialmente a través del siguiente LP, que es simplemente una agregación de LPs de horizonte finito de un solo agente: max X m X s X a rm(s, a) X t xm(s, a, t) sujeto a: X a xm(σ, a, t + 1) = X s,a pm(σ|s, a)xm(s, a, t), ∀m ∈ M, σ ∈ S, t ∈ [1, Tm − 1]; X a xm(s, a, 1) = αm(s), ∀m ∈ M, s ∈ S; (12) donde xm(s, a, t) es la medida de ocupación del agente m, y The Sixth Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 1223 (a) (b) Figura 2: Ilustración de la ampliación de un MDP para permitir tiempos de inicio y finalización variables: a) (izquierda) el MDP original de dos estados con una sola acción; (derecha) el MDP ampliado con nuevos estados sb y sf y la nueva acción a∗ (nota que las transiciones originales no se modifican en el proceso de ampliación); b) el MDP ampl augmentado mostrado como una trayectoria a través del tiempo (las líneas grises indican todas las transiciones, mientras que las líneas negras indican una trayectoria dada). Función objetivo (suma de recompensas esperadas de todos los agentes) máx X m X s X a rm(s, a) X t xm(s, a, t) (5) Implicación de significado Restricciones lineales Atar x a θ. El agente solo está activo cuando la medida de ocupación es distinta de cero en los estados originales del MDP. θm(τ) = 0 =⇒ xm(s, a, τ −τa m+1) = 0 ∀s /∈ {sb , sf }, a ∈ A X s/∈{sb,sf } X a xm(s, a, t) ≤ θm(τa m + t − 1) ∀m ∈ M, ∀t ∈ [1, Tm] (6) El agente solo puede estar activo en τ ∈ (τa m, τd m) θm(τ) = 0 ∀m ∈ M, τ /∈ (τa m, τd m) (7) No se pueden utilizar recursos cuando no está activo θm(τ) = 0 =⇒ Δm(τ, ω) = 0 ∀τ ∈ [0, bτ], ω ∈ Ω Δm(τ, ω) ≤ θm(τ) ∀m ∈ M, τ ∈ [0, bτ], ω ∈ Ω (8) Vincular x a Δ (x distinto de cero obliga a que el correspondiente Δ sea distinto de cero). Δm(τ, ω) = 0, ϕm(a, ω) = 1 =⇒ xm(s, a, τ − τa m + 1) = 0 ∀s /∈ {sb , sf } 1/|A| X a ϕm(a, ω) X s/∈{sb,sf } xm(s, a, t) ≤ Δm(t + τa m − 1, ω) ∀m ∈ M, ω ∈ Ω, t ∈ [1, Tm] (9) Límites de recursos X m Δm(τ, ω) ≤ bϕ(ω) ∀ω ∈ Ω, τ ∈ [0, bτ] (10) El agente no puede cambiar los recursos mientras está activo. Solo habilitado para programación con asignaciones estáticas. θm(τ) = 1 y θm(τ + 1) = 1 =⇒ Δm(τ, ω) = Δm(τ + 1, ω) Δm(τ, ω) − Z(1 − θm(τ + 1)) ≤ Δm(τ + 1, ω) + Z(1 − θm(τ)) Δm(τ, ω) + Z(1 − θm(τ + 1)) ≥ Δm(τ + 1, ω) − Z(1 − θm(τ)) ∀m ∈ M, ω ∈ Ω, τ ∈ [0, bτ] (11) Tabla 1: MILP para programación óptima global de recursos. Tm = τd m − τa m + 1 es el horizonte temporal para los agentes MDP. Usando este LP como base, lo ampliamos con restricciones que aseguran que el uso de recursos implicado por las medidas de ocupación de los agentes {xm} no viola los requisitos globales de recursos bϕ en ningún paso de tiempo τ ∈ [0, bτ]. Para formular estas restricciones de recursos, utilizamos las siguientes variables binarias: • Δm(τ, ω) = {0, 1}, ∀m ∈ M, τ ∈ [0, bτ], ω ∈ Ω, que sirven como variables indicadoras que definen si el agente m posee el recurso ω en el tiempo τ. Estos son análogos a las variables indicadoras estáticas utilizadas en el problema de asignación de recursos estáticos de una sola vez en [6]. • θm = {0, 1}, ∀m ∈ M, τ ∈ [0, bτ] son variables indicadoras que especifican si el agente m está activo (es decir, ejecutando su MDP) en el tiempo τ. El significado de las variables de uso de recursos Δ se ilustra en la Figura 1: Δm(τ, ω) = 1 solo si el recurso ω se asigna al agente m en el tiempo τ. El significado de los indicadores de actividad θ se ilustra en la Figura 2b: cuando el agente m se encuentra en el estado inicial sb o en el estado final sf, el θm correspondiente es igual a 0, pero una vez que el agente se vuelve activo y entra en uno de los otros estados, establecemos θm = 1. Este significado de θ puede ser impuesto con una restricción lineal que sincroniza los valores de las medidas de ocupación de los agentes xm y la actividad 1224 The Sixth Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) indicadores θ, como se muestra en (6) en la Tabla 1. Otra restricción que debemos agregar, debido a que los indicadores de actividad θ están definidos en la línea de tiempo global τ, es hacer cumplir el hecho de que el agente está inactivo fuera de su ventana de llegada y salida. Esto se logra mediante la restricción (7) en la Tabla 1. Además, los agentes no deben estar utilizando recursos mientras están inactivos. Esta restricción también puede ser impuesta a través de una desigualdad lineal en θ y Δ, como se muestra en (8). La restricción (6) establece el valor de θ para que coincida con la política definida por la medida de ocupación xm. De manera similar, debemos asegurarnos de que las variables de uso de recursos Δ estén también sincronizadas con la medida de ocupación xm. Esto se realiza a través de la restricción (9) en la Tabla 1, que es casi idéntica a la restricción análoga de [6]. Después de implementar la restricción anterior, que refuerza el significado de Δ, agregamos una restricción que garantiza que el uso de recursos de los agentes nunca exceda los montos de recursos disponibles. Esta condición también se expresa trivialmente como una desigualdad lineal (10) en la Tabla 1. Finalmente, para la formulación del problema en la que las asignaciones de recursos son estáticas durante la vida de un agente, agregamos una restricción que garantiza que las variables de uso de recursos Δ no cambien su valor mientras el agente está activo (θ = 1). Esto se logra a través de la restricción lineal (11), donde Z ≥ 2 es una constante que se utiliza para desactivar las restricciones cuando θm(τ) = 0 o θm(τ + 1) = 0. Esta restricción no se utiliza para la formulación del problema dinámico, donde los recursos pueden ser reasignados entre agentes en cada paso de tiempo. En resumen, la Tabla 1 junto con las restricciones de conservación de flujo de (12) define el MILP que calcula simultáneamente una asignación óptima de recursos para todos los agentes a lo largo del tiempo, así como políticas óptimas de MDP de horizonte finito que son válidas bajo esa asignación de recursos. Como una medida aproximada de la complejidad de este MILP, consideremos el número de variables de optimización y restricciones. Sea TM = P Tm = P m(τa m − τd m + 1) la suma de las longitudes de las ventanas de llegada-salida a través de todos los agentes. Entonces, el número de variables de optimización es: TM + bτ|M||Ω| + bτ|M|, de las cuales TM son continuas (xm), y bτ|M||Ω| + bτ|M| son binarias (Δ y θ). Sin embargo, hay que tener en cuenta que todos menos TM|M| de los θ se establecen en cero por la restricción (7), lo que también obliga inmediatamente a que todos menos TM|M||Ω| de los Δ sean cero a través de las restricciones (8). El número de restricciones (sin incluir las restricciones degeneradas en (7)) en el MILP es: TM + TM|Ω| + bτ|Ω| + bτ|M||Ω|. A pesar de que la complejidad del MILP es, en el peor de los casos, exponencial en el número de variables binarias, la complejidad de este MILP es significativamente (exponencialmente) menor que la del MILP con funciones de utilidad planas, descrito en la Sección 2.2. Este resultado refleja las ganancias de eficiencia reportadas en [6] para problemas de asignación de recursos de un solo disparo, pero es mucho más pronunciado, debido a la explosión de la representación de utilidad plana debido al aspecto temporal del problema (recuerde la complejidad prohibitiva de la optimización combinatoria en la Sección 2.2). Analizamos empíricamente el rendimiento de este método en la Sección 5.1. Estrictamente hablando, resolver MILPs hasta la optimalidad es NP-completo en el número de variables enteras. RESULTADOS EXPERIMENTALES Aunque la complejidad de resolver MILPs es en el peor de los casos exponencial en el número de variables enteras, existen muchos métodos eficientes para resolver MILPs que permiten que nuestro algoritmo se adapte bien a parámetros comunes en problemas de asignación de recursos y programación. En particular, esta sección introduce un dominio de problema: el problema del taller de reparación, que se utiliza para evaluar empíricamente la escalabilidad de nuestros algoritmos en términos del número de agentes |M|, el número de recursos compartidos |Ω| y las diferentes longitudes de tiempo global bτ durante las cuales los agentes pueden entrar y salir del sistema. El problema del taller de reparaciones es un MDP parametrizado simple que adopta la metáfora de un taller de reparaciones de vehículos. Los agentes en el taller de reparación son mecánicos con una serie de tareas independientes que solo generan recompensa cuando se completan. En nuestro modelo MDP de este sistema, las acciones tomadas para avanzar a través del espacio de estados solo están permitidas si el agente posee ciertos recursos que están públicamente disponibles para la tienda. Estos recursos son de suministro finito, y las políticas óptimas para la tienda determinarán cuándo cada agente puede retener los recursos limitados para tomar acciones y obtener recompensas individuales. Cada tarea a completar está asociada con una sola acción, aunque se requiere que el agente repita la acción numerosas veces antes de completar la tarea y obtener una recompensa. Este modelo fue parametrizado en términos del número de agentes en el sistema, el número de diferentes tipos de recursos que podrían estar vinculados a acciones necesarias, un tiempo global durante el cual se permite a los agentes llegar y partir, y una longitud máxima para el número de pasos de tiempo que un agente puede permanecer en el sistema. Todos los puntos de datos en nuestros experimentos se obtuvieron con 20 evaluaciones utilizando CPLEX para resolver los MILPs en una computadora Pentium4 con 2Gb de RAM. Se realizaron pruebas tanto en la versión estática como en la dinámica del problema de programación de recursos, tal como se definió anteriormente. La Figura 3 muestra el tiempo de ejecución y el valor de la política para modificaciones independientes en el conjunto de parámetros. La fila superior muestra cómo el tiempo de solución para el MILP escala a medida que aumentamos el número de agentes |M|, el horizonte de tiempo global bτ y el número de recursos |Ω|. Aumentar el número de agentes conduce a una escalabilidad exponencial en complejidad, lo cual es de esperarse para un problema NP-completo. Sin embargo, aumentar el límite de tiempo global bτ o el número total de tipos de recursos |Ω|, manteniendo constante el número de agentes, no conduce a una disminución en el rendimiento. Esto ocurre porque los problemas se vuelven más fáciles a medida que se vuelven sub-determinados, lo cual también es un fenómeno común en problemas NP-completos. También observamos que la solución a la versión dinámica del problema a menudo se puede calcular mucho más rápido que la versión estática. La fila inferior de la Figura 3 muestra el valor de la política conjunta de las políticas que corresponden a los horarios óptimos de asignación de recursos calculados. Podemos observar que la versión dinámica produce una recompensa más alta (como era de esperar, ya que la recompensa de la versión dinámica siempre es igual o mayor que la recompensa de la versión estática). Deberíamos señalar que estos gráficos no deben ser vistos como una medida del rendimiento de dos algoritmos diferentes (ambos algoritmos producen soluciones óptimas pero para problemas distintos), sino más bien como observaciones sobre cómo cambia la calidad de las soluciones óptimas a medida que se permite más flexibilidad en la reasignación de recursos. La Figura 4 muestra el tiempo de ejecución y el valor de la política para las pruebas en las que las variables de entrada comunes se escalan juntas. Esto permite la Sexta Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 1225 2 4 6 8 10 10 −3 10 −2 10 −1 10 0 10 1 10 2 10 3 10 4 Número de Agentes |M| Tiempo de CPU, seg |Ω| = 5, τ = 50 estático dinámico 50 100 150 200 10 −2 10 −1 10 0 10 1 10 2 10 3 Límite de Tiempo Global τ Tiempo de CPU, seg |M| = 5, |Ω| = 5 estático dinámico 10 20 30 40 50 10 −2 10 −1 10 0 10 1 10 2 Número de Recursos |Ω| Tiempo de CPU, seg |M| = 5, τ = 50 estático dinámico 2 4 6 8 10 200 400 600 800 1000 1200 1400 1600 Número de Agentes |M| Valor |Ω| = 5, τ = 50 estático dinámico 50 100 150 200 400 500 600 700 800 900 1000 1100 1200 1300 1400 Límite de Tiempo Global τ Valor |M| = 5, |Ω| = 5 estático dinámico 10 20 30 40 50 500 600 700 800 900 1000 1100 1200 1300 1400 Número de Recursos |Ω| Valor |M| = 5, τ = 50 estático dinámico Figura 3: Evaluación de nuestro MILP para números variables de agentes (columna 1), longitudes de ventana de tiempo global (columna 2) y números de tipos de recursos (columna 3). La fila superior muestra el tiempo de CPU, y la fila inferior muestra la recompensa conjunta de las políticas MDP de los agentes. Las barras de error muestran los cuartiles 1ro y 3ro (25% y 75%). 2 4 6 8 10 10 −3 10 −2 10 −1 10 0 10 1 10 2 10 3 Número de Agentes |M| Tiempo de CPU, seg τ = 10|M| estático dinámico 2 4 6 8 10 10 −3 10 −2 10 −1 10 0 10 1 10 2 10 3 10 4 Número de Agentes |M| Tiempo de CPU, seg |Ω| = 2|M| estático dinámico 2 4 6 8 10 10 −3 10 −2 10 −1 10 0 10 1 10 2 10 3 10 4 Número de Agentes |M| Tiempo de CPU, seg |Ω| = 5|M| estático dinámico 2 4 6 8 10 200 400 600 800 1000 1200 1400 1600 1800 2000 2200 Número de Agentes |M| Valor τ = 10|M| estático dinámico 2 4 6 8 10 200 400 600 800 1000 1200 1400 1600 1800 2000 Número de Agentes |M| Valor |Ω| = 2|M| estático dinámico 2 4 6 8 10 0 500 1000 1500 2000 2500 Número de Agentes |M| Valor |Ω| = 5|M| estático dinámico Figura 4: Evaluación de nuestro MILP utilizando variables de entrada correlacionadas. La columna izquierda sigue el rendimiento y el tiempo de CPU a medida que aumenta el número de agentes y la ventana de tiempo global juntos (bτ = 10|M|). La columna del medio y la columna de la derecha siguen el rendimiento y el tiempo de CPU a medida que aumenta el número de recursos y el número de agentes juntos, con |Ω| = 2|M| y |Ω| = 5|M|, respectivamente. Las barras de error muestran el primer y tercer cuartil (25% y 75%). 1226 The Sixth Intl. La Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) nos permite explorar dominios donde el número total de agentes escala proporcionalmente al número total de tipos de recursos o al horizonte temporal global, manteniendo constante la densidad promedio de agentes (por unidad de tiempo global) o el número promedio de recursos por agente (lo cual comúnmente ocurre en aplicaciones de la vida real). En general, creemos que estos resultados experimentales indican que nuestra formulación MILP puede ser utilizada para resolver de manera efectiva problemas de programación de recursos de tamaño no trivial. DISCUSIÓN Y CONCLUSIONES A lo largo del documento, hemos realizado una serie de suposiciones en nuestro modelo y algoritmo de solución; discutimos sus implicaciones a continuación. • Ejecución continua. Suponemos que una vez que un agente deja de ejecutar su MDP (transiciona al estado sf), sale del sistema y no puede regresar. Es fácil relajar esta suposición para dominios donde los agentes de MDP pueden ser pausados y reiniciados. Todo lo que se requiere es incluir una acción de pausa adicional que haga la transición desde un estado dado de regreso a sí mismo, y que tenga una recompensa de cero. • Indiferencia al tiempo de inicio. Utilizamos un modelo de recompensa donde las recompensas de los agentes dependen solo del horizonte temporal de sus MDP y no del tiempo de inicio global. Esta es una consecuencia de nuestro procedimiento de aumento de MDP de la Sección 4.1. Es fácil extender el modelo para que los agentes incurran en una penalización explícita por inactividad al asignar una recompensa negativa no nula al estado inicial sb. • Requisitos de recursos binarios. Para simplificar, hemos asumido que los costos de recursos son binarios: ϕm(a, ω) = {0, 1}, pero nuestros resultados se generalizan de manera directa a asignaciones de recursos no binarias, de manera análoga al procedimiento utilizado en [5]. • Agentes cooperativos. El procedimiento de optimización discutido en este artículo fue desarrollado en el contexto de agentes cooperativos, pero también puede ser utilizado para diseñar un mecanismo para programar recursos entre agentes egoístas. Este procedimiento de optimización puede ser incorporado en una subasta de Vickrey-Clarke-Groves, de manera completamente análoga a como se hizo en [7]. De hecho, todos los resultados de [7] sobre las propiedades de la subasta y la privacidad de la información se aplican directamente al dominio de programación discutido en este documento, requiriendo solo ligeras modificaciones para tratar con MDPs de horizonte finito. • Tiempos de llegada y salida conocidos y deterministas. Finalmente, hemos asumido que los tiempos de llegada y salida de los agentes (τa m y τd m) son deterministas y conocidos de antemano. Esta suposición es fundamental para nuestro método de solución. Si bien hay muchos dominios donde esta suposición es válida, en muchos casos los agentes llegan y se van de forma dinámica y sus tiempos de llegada y salida solo pueden predecirse de manera probabilística, lo que lleva a problemas de asignación de recursos en línea. En particular, en el caso de agentes con interés propio, esto se convierte en una versión interesante de un problema de diseño de mecanismos en línea [11, 12]. En resumen, hemos presentado una formulación MILP para el problema combinatorio de programación de recursos donde los valores de los agentes para posibles asignaciones de recursos están definidos por MDPs de horizonte finito. Este resultado amplía el trabajo previo ([6, 7]) sobre asignación estática de recursos de una sola vez bajo preferencias inducidas por MDP a problemas de programación de recursos con un aspecto temporal. Por lo tanto, este trabajo da un paso en la dirección de diseñar un mecanismo en línea para agentes con preferencias de recursos combinatorios inducidas por problemas de planificación estocástica. Relajar la suposición sobre los tiempos deterministas de llegada y salida de los agentes es un enfoque de nuestro trabajo futuro. Nos gustaría agradecer a los revisores anónimos por sus comentarios y sugerencias perspicaces. 7. REFERENCIAS [1] E. Altman y A. Shwartz. Control adaptativo de cadenas de Markov restringidas: Criterios y políticas. Anales de Investigación de Operaciones, número especial sobre Procesos de Decisión de Markov, 28:101-134, 1991. [2] R. Bellman. Programación dinámica. Princeton University Press, 1957. [3] C. Boutilier. \n\nPrensa de la Universidad de Princeton, 1957. [3] C. Boutilier. Resolviendo problemas de subasta combinatoria expresados de manera concisa. En Proc. de AAAI-02, páginas 359-366, 2002. [4] C. Boutilier y H. H. Hoos. Subastando idiomas para subastas combinatorias. En Proc. de IJCAI-01, páginas 1211-1217, 2001. [5] D. Dolgov. Asignación integrada de recursos y planificación en entornos estocásticos de múltiples agentes. Tesis doctoral, Departamento de Ciencias de la Computación, Universidad de Michigan, febrero de 2006. [6] D. A. Dolgov y E. H. Durfee. Asignación óptima de recursos y formulación de políticas en procesos de decisión de Markov débilmente acoplados. En Proc. de ICAPS-04, páginas 315-324, junio de 2004. [7] D. A. Dolgov y E. H. Durfee. Subastas combinatorias computacionalmente eficientes para asignación de recursos en MDPs débilmente acoplados. En Proc. de AAMAS-05, Nueva York, NY, EE. UU., 2005. ACM Press. [8] D. A. Dolgov y E. H. Durfee. Asignación de recursos entre agentes con preferencias inducidas por MDP factorizados. En Proc. de AAMAS-06, 2006. [9] K. Larson y T. Sandholm. Diseño de mecanismos y agentes deliberativos. En Proc. de AAMAS-05, páginas 650-656, Nueva York, NY, EE. UU., 2005. ACM Press. [10] N. Nisan. \n\nACM Press. [10] N. Nisan. Subasta y asignación en subastas combinatorias. En Comercio Electrónico, 2000. [11] D. C. Parkes y S. Singh. Un enfoque basado en MDP para el Diseño de Mecanismos en Línea. En Proc. de la Decimoséptima Conferencia Anual sobre Sistemas de Procesamiento de Información Neural (NIPS-03), 2003. [12] D. C. Parkes, S. Singh y D. Yanovsky. Diseño de mecanismos en línea aproximadamente eficiente. En Actas de la Decimoctava Conferencia Anual sobre Sistemas de Procesamiento de Información Neural (NIPS-04), 2004. [13] M. L. Puterman. Procesos de Decisión de Markov. John Wiley & Sons, Nueva York, 1994. [14] M. H. Rothkopf, A. Pekec y R. M. Harstad. Subastas combinatorias manejables computacionalmente. Ciencia de la Gestión, 44(8):1131-1147, 1998. [15] T. Sandholm. Un algoritmo para la determinación óptima del ganador en subastas combinatorias. En Proc. de IJCAI-99, páginas 542-547, San Francisco, CA, EE. UU., 1999. Morgan Kaufmann Publishers Inc. - Editores Morgan Kaufmann Inc. El Sexto Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 1227 ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "optimal allocation": {
            "translated_key": "asignación óptima",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Combinatorial Resource Scheduling for Multiagent MDPs Dmitri A. Dolgov, Michael R. James, and Michael E. Samples AI and Robotics Group Technical Research, Toyota Technical Center USA {ddolgov, michael.r.james, michael.samples}@gmail.com ABSTRACT Optimal resource scheduling in multiagent systems is a computationally challenging task, particularly when the values of resources are not additive.",
                "We consider the combinatorial problem of scheduling the usage of multiple resources among agents that operate in stochastic environments, modeled as Markov decision processes (MDPs).",
                "In recent years, efficient resource-allocation algorithms have been developed for agents with resource values induced by MDPs.",
                "However, this prior work has focused on static resource-allocation problems where resources are distributed once and then utilized in infinite-horizon MDPs.",
                "We extend those existing models to the problem of combinatorial resource scheduling, where agents persist only for finite periods between their (predefined) arrival and departure times, requiring resources only for those time periods.",
                "We provide a computationally efficient procedure for computing globally optimal resource assignments to agents over time.",
                "We illustrate and empirically analyze the method in the context of a stochastic jobscheduling domain.",
                "Categories and Subject Descriptors I.2.8 [Artificial Intelligence]: Problem Solving, Control Methods, and Search; I.2.11 [Artificial Intelligence]: Distributed Artificial Intelligence-Multiagent systems General Terms Algorithms, Performance, Design 1.",
                "INTRODUCTION The tasks of optimal resource allocation and scheduling are ubiquitous in multiagent systems, but solving such optimization problems can be computationally difficult, due to a number of factors.",
                "In particular, when the value of a set of resources to an agent is not additive (as is often the case with resources that are substitutes or complements), the utility function might have to be defined on an exponentially large space of resource bundles, which very quickly becomes computationally intractable.",
                "Further, even when each agent has a utility function that is nonzero only on a small subset of the possible resource bundles, obtaining <br>optimal allocation</br> is still computationally prohibitive, as the problem becomes NP-complete [14].",
                "Such computational issues have recently spawned several threads of work in using compact models of agents preferences.",
                "One idea is to use any structure present in utility functions to represent them compactly, via, for example, logical formulas [15, 10, 4, 3].",
                "An alternative is to directly model the mechanisms that define the agents utility functions and perform resource allocation directly with these models [9].",
                "A way of accomplishing this is to model the processes by which an agent might utilize the resources and define the utility function as the payoff of these processes.",
                "In particular, if an agent uses resources to act in a stochastic environment, its utility function can be naturally modeled with a Markov decision process, whose action set is parameterized by the available resources.",
                "This representation can then be used to construct very efficient resource-allocation algorithms that lead to an exponential speedup over a straightforward optimization problem with flat representations of combinatorial preferences [6, 7, 8].",
                "However, this existing work on resource allocation with preferences induced by resource-parameterized MDPs makes an assumption that the resources are only allocated once and are then utilized by the agents independently within their infinite-horizon MDPs.",
                "This assumption that no reallocation of resources is possible can be limiting in domains where agents arrive and depart dynamically.",
                "In this paper, we extend the work on resource allocation under MDP-induced preferences to discrete-time scheduling problems, where agents are present in the system for finite time intervals and can only use resources within these intervals.",
                "In particular, agents arrive and depart at arbitrary (predefined) times and within these intervals use resources to execute tasks in finite-horizon MDPs.",
                "We address the problem of globally optimal resource scheduling, where the objective is to find an allocation of resources to the agents across time that maximizes the sum of the expected rewards that they obtain.",
                "In this context, our main contribution is a mixed-integerprogramming formulation of the scheduling problem that chooses globally optimal resource assignments, starting times, and execution horizons for all agents (within their arrival1220 978-81-904262-7-5 (RPS) c 2007 IFAAMAS departure intervals).",
                "We analyze and empirically compare two flavors of the scheduling problem: one, where agents have static resource assignments within their finite-horizon MDPs, and another, where resources can be dynamically reallocated between agents at every time step.",
                "In the rest of the paper, we first lay down the necessary groundwork in Section 2 and then introduce our model and formal problem statement in Section 3.",
                "In Section 4.2, we describe our main result, the optimization program for globally optimal resource scheduling.",
                "Following the discussion of our experimental results on a job-scheduling problem in Section 5, we conclude in Section 6 with a discussion of possible extensions and generalizations of our method. 2.",
                "BACKGROUND Similarly to the model used in previous work on resourceallocation with MDP-induced preferences [6, 7], we define the value of a set of resources to an agent as the value of the best MDP policy that is realizable, given those resources.",
                "However, since the focus of our work is on scheduling problems, and a large part of the optimization problem is to decide how resources are allocated in time among agents with finite arrival and departure times, we model the agents planning problems as finite-horizon MDPs, in contrast to previous work that used infinite-horizon discounted MDPs.",
                "In the rest of this section, we first introduce some necessary background on finite-horizon MDPs and present a linear-programming formulation that serves as the basis for our solution algorithm developed in Section 4.",
                "We also outline the standard methods for combinatorial resource scheduling with flat resource values, which serve as a comparison benchmark for the new model developed here. 2.1 Markov Decision Processes A stationary, finite-domain, discrete-time MDP (see, for example, [13] for a thorough and detailed development) can be described as S, A, p, r , where: S is a finite set of system states; A is a finite set of actions that are available to the agent; p is a stationary stochastic transition function, where p(σ|s, a) is the probability of transitioning to state σ upon executing action a in state s; r is a stationary reward function, where r(s, a) specifies the reward obtained upon executing action a in state s. Given such an MDP, a decision problem under a finite horizon T is to choose an optimal action at every time step to maximize the expected value of the total reward accrued during the agents (finite) lifetime.",
                "The agents optimal policy is then a function of current state s and the time until the horizon.",
                "An optimal policy for such a problem is to act greedily with respect to the optimal value function, defined recursively by the following system of finite-time Bellman equations [2]: v(s, t) = max a r(s, a) + X σ p(σ|s, a)v(σ, t + 1), ∀s ∈ S, t ∈ [1, T − 1]; v(s, T) = 0, ∀s ∈ S; where v(s, t) is the optimal value of being in state s at time t ∈ [1, T].",
                "This optimal value function can be easily computed using dynamic programming, leading to the following optimal policy π, where π(s, a, t) is the probability of executing action a in state s at time t: π(s, a, t) = ( 1, a = argmaxa r(s, a) + P σ p(σ|s, a)v(σ, t + 1), 0, otherwise.",
                "The above is the most common way of computing the optimal value function (and therefore an optimal policy) for a finite-horizon MDP.",
                "However, we can also formulate the problem as the following linear program (similarly to the dual LP for infinite-horizon discounted MDPs [13, 6, 7]): max X s X a r(s, a) X t x(s, a, t) subject to: X a x(σ, a, t + 1) = X s,a p(σ|s, a)x(s, a, t) ∀σ, t ∈ [1, T − 1]; X a x(s, a, 1) = α(s), ∀s ∈ S; (1) where α(s) is the initial distribution over the state space, and x is the (non-stationary) occupation measure (x(s, a, t) ∈ [0, 1] is the total expected number of times action a is executed in state s at time t).",
                "An optimal (non-stationary) policy is obtained from the occupation measure as follows: π(s, a, t) = x(s, a, t)/ X a x(s, a, t) ∀s ∈ S, t ∈ [1, T]. (2) Note that the standard unconstrained finite-horizon MDP, as described above, always has a uniformly-optimal solution (optimal for any initial distribution α(s)).",
                "Therefore, an optimal policy can be obtained by using an arbitrary constant α(s) > 0 (in particular, α(s) = 1 will result in x(s, a, t) = π(s, a, t)).",
                "However, for MDPs with resource constraints (as defined below in Section 3), uniformly-optimal policies do not in general exist.",
                "In such cases, α becomes a part of the problem input, and a resulting policy is only optimal for that particular α.",
                "This result is well known for infinite-horizon MDPs with various types of constraints [1, 6], and it also holds for our finite-horizon model, which can be easily established via a line of reasoning completely analogous to the arguments in [6]. 2.2 Combinatorial Resource Scheduling A straightforward approach to resource scheduling for a set of agents M, whose values for the resources are induced by stochastic planning problems (in our case, finite-horizon MDPs) would be to have each agent enumerate all possible resource assignments over time and, for each one, compute its value by solving the corresponding MDP.",
                "Then, each agent would provide valuations for each possible resource bundle over time to a centralized coordinator, who would compute the optimal resource assignments across time based on these valuations.",
                "When resources can be allocated at different times to different agents, each agent must submit valuations for every combination of possible time horizons.",
                "Let each agent m ∈ M execute its MDP within the arrival-departure time interval τ ∈ [τa m, τd m].",
                "Hence, agent m will execute an MDP with time horizon no greater than Tm = τd m−τa m+1.",
                "Let bτ be the global time horizon for the problem, before which all of the agents MDPs must finish.",
                "We assume τd m < bτ, ∀m ∈ M. The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1221 For the scheduling problem where agents have static resource requirements within their finite-horizon MDPs, the agents provide a valuation for each resource bundle for each possible time horizon (from [1, Tm]) that they may use.",
                "Let Ω be the set of resources to be allocated among the agents.",
                "An agent will get at most one resource bundle for one of the time horizons.",
                "Let the variable ψ ∈ Ψm enumerate all possible pairs of resource bundles and time horizons for agent m, so there are 2|Ω| × Tm values for ψ (the space of bundles is exponential in the number of resource types |Ω|).",
                "The agent m must provide a value vψ m for each ψ, and the coordinator will allocate at most one ψ (resource, time horizon) pair to each agent.",
                "This allocation is expressed as an indicator variable zψ m ∈ {0, 1} that shows whether ψ is assigned to agent m. For time τ and resource ω, the function nm(ψ, τ, ω) ∈ {0, 1} indicates whether the bundle in ψ uses resource ω at time τ (we make the assumption that agents have binary resource requirements).",
                "This allocation problem is NP-complete, even when considering only a single time step, and its difficulty increases significantly with multiple time steps because of the increasing number of values of ψ.",
                "The problem of finding an <br>optimal allocation</br> that satisfies the global constraint that the amount of each resource ω allocated to all agents does not exceed the available amount bϕ(ω) can be expressed as the following integer program: max X m∈M X ψ∈Ψm zψ mvψ m subject to: X ψ∈Ψm zψ m ≤ 1, ∀m ∈ M; X m∈M X ψ∈Ψm zψ mnm(ψ, τ, ω) ≤ bϕ(ω), ∀τ ∈ [1, bτ], ∀ω ∈ Ω; (3) The first constraint in equation 3 says that no agent can receive more than one bundle, and the second constraint ensures that the total assignment of resource ω does not, at any time, exceed the resource bound.",
                "For the scheduling problem where the agents are able to dynamically reallocate resources, each agent must specify a value for every combination of bundles and time steps within its time horizon.",
                "Let the variable ψ ∈ Ψm in this case enumerate all possible resource bundles for which at most one bundle may be assigned to agent m at each time step.",
                "Therefore, in this case there are P t∈[1,Tm](2|Ω| )t ∼ 2|Ω|Tm possibilities of resource bundles assigned to different time slots, for the Tm different time horizons.",
                "The same set of equations (3) can be used to solve this dynamic scheduling problem, but the integer program is different because of the difference in how ψ is defined.",
                "In this case, the number of ψ values is exponential in each agents planning horizon Tm, resulting in a much larger program.",
                "This straightforward approach to solving both of these scheduling problems requires an enumeration and solution of either 2|Ω| Tm (static allocation) or P t∈[1,Tm] 2|Ω|t (dynamic reallocation) MDPs for each agent, which very quickly becomes intractable with the growth of the number of resources |Ω| or the time horizon Tm. 3.",
                "MODEL AND PROBLEM STATEMENT We now formally introduce our model of the resourcescheduling problem.",
                "The problem input consists of the following components: • M, Ω, bϕ, τa m, τd m, bτ are as defined above in Section 2.2. • {Θm} = {S, A, pm, rm, αm} are the MDPs of all agents m ∈ M. Without loss of generality, we assume that state and action spaces of all agents are the same, but each has its own transition function pm, reward function rm, and initial conditions αm. • ϕm : A×Ω → {0, 1} is the mapping of actions to resources for agent m. ϕm(a, ω) indicates whether action a of agent m needs resource ω.",
                "An agent m that receives a set of resources that does not include resource ω cannot execute in its MDP policy any action a for which ϕm(a, ω) = 0.",
                "We assume all resource requirements are binary; as discussed below in Section 6, this assumption is not limiting.",
                "Given the above input, the optimization problem we consider is to find the globally optimal-maximizing the sum of expected rewards-mapping of resources to agents for all time steps: Δ : τ × M × Ω → {0, 1}.",
                "A solution is feasible if the corresponding assignment of resources to the agents does not violate the global resource constraint: X m Δm(τ, ω) ≤ bϕ(ω), ∀ω ∈ Ω, τ ∈ [1, bτ]. (4) We consider two flavors of the resource-scheduling problem.",
                "The first formulation restricts resource assignments to the space where the allocation of resources to each agent is static during the agents lifetime.",
                "The second formulation allows reassignment of resources between agents at every time step within their lifetimes.",
                "Figure 1 depicts a resource-scheduling problem with three agents M = {m1, m2, m3}, three resources Ω = {ω1, ω2, ω3}, and a global problem horizon of bτ = 11.",
                "The agents arrival and departure times are shown as gray boxes and are {1, 6}, {3, 7}, and {2, 11}, respectively.",
                "A solution to this problem is shown via horizontal bars within each agents box, where the bars correspond to the allocation of the three resource types.",
                "Figure 1a shows a solution to a static scheduling problem.",
                "According to the shown solution, agent m1 begins the execution of its MDP at time τ = 1 and has a lock on all three resources until it finishes execution at time τ = 3.",
                "Note that agent m1 relinquishes its hold on the resources before its announced departure time of τd m1 = 6, ostensibly because other agents can utilize the resources more effectively.",
                "Thus, at time τ = 4, resources ω1 and ω3 are allocated to agent m2, who then uses them to execute its MDP (using only actions supported by resources ω1 and ω3) until time τ = 7.",
                "Agent m3 holds resource ω3 during the interval τ ∈ [4, 10].",
                "Figure 1b shows a possible solution to the dynamic version of the same problem.",
                "There, resources can be reallocated between agents at every time step.",
                "For example, agent m1 gives up its use of resource ω2 at time τ = 2, although it continues the execution of its MDP until time τ = 6.",
                "Notice that an agent is not allowed to stop and restart its MDP, so agent m1 is only able to continue executing in the interval τ ∈ [3, 4] if it has actions that do not require any resources (ϕm(a, ω) = 0).",
                "Clearly, the model and problem statement described above make a number of assumptions about the problem and the desired solution properties.",
                "We discuss some of those assumptions and their implications in Section 6. 1222 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) (a) (b) Figure 1: Illustration of a solution to a resource-scheduling problem with three agents and three resources: a) static resource assignments (resource assignments are constant within agents lifetimes; b) dynamic assignment (resource assignments are allowed to change at every time step). 4.",
                "RESOURCE SCHEDULING Our resource-scheduling algorithm proceeds in two stages.",
                "First, we perform a preprocessing step that augments the agent MDPs; this process is described in Section 4.1.",
                "Second, using these augmented MDPs we construct a global optimization problem, which is described in Section 4.2. 4.1 Augmenting Agents MDPs In the model described in the previous section, we assume that if an agent does not possess the necessary resources to perform actions in its MDP, its execution is halted and the agent leaves the system.",
                "In other words, the MDPs cannot be paused and resumed.",
                "For example, in the problem shown in Figure 1a, agent m1 releases all resources after time τ = 3, at which point the execution of its MDP is halted.",
                "Similarly, agents m2 and m3 only execute their MDPs in the intervals τ ∈ [4, 6] and τ ∈ [4, 10], respectively.",
                "Therefore, an important part of the global decision-making problem is to decide the window of time during which each of the agents is active (i.e., executing its MDP).",
                "To accomplish this, we augment each agents MDP with two new states (start and finish states sb , sf , respectively) and a new start/stop action a∗ , as illustrated in Figure 2.",
                "The idea is that an agent stays in the start state sb until it is ready to execute its MDP, at which point it performs the start/stop action a∗ and transitions into the state space of the original MDP with the transition probability that corresponds to the original initial distribution α(s).",
                "For example, in Figure 1a, for agent m2 this would happen at time τ = 4.",
                "Once the agent gets to the end of its activity window (time τ = 6 for agent m2 in Figure 1a), it performs the start/stop action, which takes it into the sink finish state sf at time τ = 7.",
                "More precisely, given an MDP S, A, pm, rm, αm , we define an augmented MDP S , A , pm, rm, αm as follows: S = S ∪ sb ∪ sf ; A = A ∪ a∗ ; p (s|sb , a∗ ) = α(s), ∀s ∈ S; p (sb |sb , a) = 1.0, ∀a ∈ A; p (sf |s, a∗ ) = 1.0, ∀s ∈ S; p (σ|s, a) = p(σ|s, a), ∀s, σ ∈ S, a ∈ A; r (sb , a) = r (sf , a) = 0, ∀a ∈ A ; r (s, a) = r(s, a), ∀s ∈ S, a ∈ A; α (sb ) = 1; α (s) = 0, ∀s ∈ S; where all non-specified transition probabilities are assumed to be zero.",
                "Further, in order to account for the new starting state, we begin the MDP one time-step earlier, setting τa m ← τa m − 1.",
                "This will not affect the resource allocation due to the resource constraints only being enforced for the original MDP states, as will be discussed in the next section.",
                "For example, the augmented MDPs shown in Figure 2b (which starts in state sb at time τ = 2) would be constructed from an MDP with original arrival time τ = 3.",
                "Figure 2b also shows a sample trajectory through the state space: the agent starts in state sb , transitions into the state space S of the original MDP, and finally exists into the sink state sf .",
                "Note that if we wanted to model a problem where agents could pause their MDPs at arbitrary time steps (which might be useful for domains where dynamic reallocation is possible), we could easily accomplish this by including an extra action that transitions from each state to itself with zero reward. 4.2 MILP for Resource Scheduling Given a set of augmented MDPs, as defined above, the goal of this section is to formulate a global optimization program that solves the resource-scheduling problem.",
                "In this section and below, all MDPs are assumed to be the augmented MDPs as defined in Section 4.1.",
                "Our approach is similar to the idea used in [6]: we begin with the linear-program formulation of agents MDPs (1) and augment it with constraints that ensure that the corresponding resource allocation across agents and time is valid.",
                "The resulting optimization problem then simultaneously solves the agents MDPs and resource-scheduling problems.",
                "In the rest of this section, we incrementally develop a mixed integer program (MILP) that achieves this.",
                "In the absence of resource constraints, the agents finitehorizon MDPs are completely independent, and the globally optimal solution can be trivially obtained via the following LP, which is simply an aggregation of single-agent finitehorizon LPs: max X m X s X a rm(s, a) X t xm(s, a, t) subject to: X a xm(σ, a, t + 1) = X s,a pm(σ|s, a)xm(s, a, t), ∀m ∈ M, σ ∈ S, t ∈ [1, Tm − 1]; X a xm(s, a, 1) = αm(s), ∀m ∈ M, s ∈ S; (12) where xm(s, a, t) is the occupation measure of agent m, and The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1223 (a) (b) Figure 2: Illustration of augmenting an MDP to allow for variable starting and stopping times: a) (left) the original two-state MDP with a single action; (right) the augmented MDP with new states sb and sf and the new action a∗ (note that the origianl transitions are not changed in the augmentation process); b) the augmented MDP displayed as a trajectory through time (grey lines indicate all transitions, while black lines indicate a given trajectory.",
                "Objective Function (sum of expected rewards over all agents) max X m X s X a rm(s, a) X t xm(s, a, t) (5) Meaning Implication Linear Constraints Tie x to θ.",
                "Agent is only active when occupation measure is nonzero in original MDP states. θm(τ) = 0 =⇒ xm(s, a, τ −τa m+1) = 0 ∀s /∈ {sb , sf }, a ∈ A X s/∈{sb,sf } X a xm(s, a, t) ≤ θm(τa m + t − 1) ∀m ∈ M, ∀t ∈ [1, Tm] (6) Agent can only be active in τ ∈ (τa m, τd m) θm(τ) = 0 ∀m ∈ M, τ /∈ (τa m, τd m) (7) Cannot use resources when not active θm(τ) = 0 =⇒ Δm(τ, ω) = 0 ∀τ ∈ [0, bτ], ω ∈ Ω Δm(τ, ω) ≤ θm(τ) ∀m ∈ M, τ ∈ [0, bτ], ω ∈ Ω (8) Tie x to Δ (nonzero x forces corresponding Δ to be nonzero.)",
                "Δm(τ, ω) = 0, ϕm(a, ω) = 1 =⇒ xm(s, a, τ − τa m + 1) = 0 ∀s /∈ {sb , sf } 1/|A| X a ϕm(a, ω) X s/∈{sb,sf } xm(s, a, t) ≤ Δm(t + τa m − 1, ω) ∀m ∈ M, ω ∈ Ω, t ∈ [1, Tm] (9) Resource bounds X m Δm(τ, ω) ≤ bϕ(ω) ∀ω ∈ Ω, τ ∈ [0, bτ] (10) Agent cannot change resources while active.",
                "Only enabled for scheduling with static assignments. θm(τ) = 1 and θm(τ + 1) = 1 =⇒ Δm(τ, ω) = Δm(τ + 1, ω) Δm(τ, ω) − Z(1 − θm(τ + 1)) ≤ Δm(τ + 1, ω) + Z(1 − θm(τ)) Δm(τ, ω) + Z(1 − θm(τ + 1)) ≥ Δm(τ + 1, ω) − Z(1 − θm(τ)) ∀m ∈ M, ω ∈ Ω, τ ∈ [0, bτ] (11) Table 1: MILP for globally optimal resource scheduling.",
                "Tm = τd m − τa m + 1 is the time horizon for the agents MDP.",
                "Using this LP as a basis, we augment it with constraints that ensure that the resource usage implied by the agents occupation measures {xm} does not violate the global resource requirements bϕ at any time step τ ∈ [0, bτ].",
                "To formulate these resource constraints, we use the following binary variables: • Δm(τ, ω) = {0, 1}, ∀m ∈ M, τ ∈ [0, bτ], ω ∈ Ω, which serve as indicator variables that define whether agent m possesses resource ω at time τ.",
                "These are analogous to the static indicator variables used in the one-shot static resource-allocation problem in [6]. • θm = {0, 1}, ∀m ∈ M, τ ∈ [0, bτ] are indicator variables that specify whether agent m is active (i.e., executing its MDP) at time τ.",
                "The meaning of resource-usage variables Δ is illustrated in Figure 1: Δm(τ, ω) = 1 only if resource ω is allocated to agent m at time τ.",
                "The meaning of the activity indicators θ is illustrated in Figure 2b: when agent m is in either the start state sb or the finish state sf , the corresponding θm = 0, but once the agent becomes active and enters one of the other states, we set θm = 1 .",
                "This meaning of θ can be enforced with a linear constraint that synchronizes the values of the agents occupation measures xm and the activity 1224 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) indicators θ, as shown in (6) in Table 1.",
                "Another constraint we have to add-because the activity indicators θ are defined on the global timeline τ-is to enforce the fact that the agent is inactive outside of its arrivaldeparture window.",
                "This is accomplished by constraint (7) in Table 1.",
                "Furthermore, agents should not be using resources while they are inactive.",
                "This constraint can also be enforced via a linear inequality on θ and Δ, as shown in (8).",
                "Constraint (6) sets the value of θ to match the policy defined by the occupation measure xm.",
                "In a similar fashion, we have to make sure that the resource-usage variables Δ are also synchronized with the occupation measure xm.",
                "This is done via constraint (9) in Table 1, which is nearly identical to the analogous constraint from [6].",
                "After implementing the above constraint, which enforces the meaning of Δ, we add a constraint that ensures that the agents resource usage never exceeds the amounts of available resources.",
                "This condition is also trivially expressed as a linear inequality (10) in Table 1.",
                "Finally, for the problem formulation where resource assignments are static during a lifetime of an agent, we add a constraint that ensures that the resource-usage variables Δ do not change their value while the agent is active (θ = 1).",
                "This is accomplished via the linear constraint (11), where Z ≥ 2 is a constant that is used to turn off the constraints when θm(τ) = 0 or θm(τ + 1) = 0.",
                "This constraint is not used for the dynamic problem formulation, where resources can be reallocated between agents at every time step.",
                "To summarize, Table 1 together with the conservationof-flow constraints from (12) defines the MILP that simultaneously computes an optimal resource assignment for all agents across time as well as optimal finite-horizon MDP policies that are valid under that resource assignment.",
                "As a rough measure of the complexity of this MILP, let us consider the number of optimization variables and constraints.",
                "Let TM = P Tm = P m(τa m − τd m + 1) be the sum of the lengths of the arrival-departure windows across all agents.",
                "Then, the number of optimization variables is: TM + bτ|M||Ω| + bτ|M|, TM of which are continuous (xm), and bτ|M||Ω| + bτ|M| are binary (Δ and θ).",
                "However, notice that all but TM|M| of the θ are set to zero by constraint (7), which also immediately forces all but TM|M||Ω| of the Δ to be zero via the constraints (8).",
                "The number of constraints (not including the degenerate constraints in (7)) in the MILP is: TM + TM|Ω| + bτ|Ω| + bτ|M||Ω|.",
                "Despite the fact that the complexity of the MILP is, in the worst case, exponential1 in the number of binary variables, the complexity of this MILP is significantly (exponentially) lower than that of the MILP with flat utility functions, described in Section 2.2.",
                "This result echos the efficiency gains reported in [6] for single-shot resource-allocation problems, but is much more pronounced, because of the explosion of the flat utility representation due to the temporal aspect of the problem (recall the prohibitive complexity of the combinatorial optimization in Section 2.2).",
                "We empirically analyze the performance of this method in Section 5. 1 Strictly speaking, solving MILPs to optimality is NPcomplete in the number of integer variables. 5.",
                "EXPERIMENTAL RESULTS Although the complexity of solving MILPs is in the worst case exponential in the number of integer variables, there are many efficient methods for solving MILPs that allow our algorithm to scale well for parameters common to resource allocation and scheduling problems.",
                "In particular, this section introduces a problem domain-the repairshop problem-used to empirically evaluate our algorithms scalability in terms of the number of agents |M|, the number of shared resources |Ω|, and the varied lengths of global time bτ during which agents may enter and exit the system.",
                "The repairshop problem is a simple parameterized MDP adopting the metaphor of a vehicular repair shop.",
                "Agents in the repair shop are mechanics with a number of independent tasks that yield reward only when completed.",
                "In our MDP model of this system, actions taken to advance through the state space are only allowed if the agent holds certain resources that are publicly available to the shop.",
                "These resources are in finite supply, and optimal policies for the shop will determine when each agent may hold the limited resources to take actions and earn individual rewards.",
                "Each task to be completed is associated with a single action, although the agent is required to repeat the action numerous times before completing the task and earning a reward.",
                "This model was parameterized in terms of the number of agents in the system, the number of different types of resources that could be linked to necessary actions, a global time during which agents are allowed to arrive and depart, and a maximum length for the number of time steps an agent may remain in the system.",
                "All datapoints in our experiments were obtained with 20 evaluations using CPLEX to solve the MILPs on a Pentium4 computer with 2Gb of RAM.",
                "Trials were conducted on both the static and the dynamic version of the resourcescheduling problem, as defined earlier.",
                "Figure 3 shows the runtime and policy value for independent modifications to the parameter set.",
                "The top row shows how the solution time for the MILP scales as we increase the number of agents |M|, the global time horizon bτ, and the number of resources |Ω|.",
                "Increasing the number of agents leads to exponential complexity scaling, which is to be expected for an NP-complete problem.",
                "However, increasing the global time limit bτ or the total number of resource types |Ω|-while holding the number of agents constantdoes not lead to decreased performance.",
                "This occurs because the problems get easier as they become under-constrained, which is also a common phenomenon for NP-complete problems.",
                "We also observe that the solution to the dynamic version of the problem can often be computed much faster than the static version.",
                "The bottom row of Figure 3 shows the joint policy value of the policies that correspond to the computed optimal resource-allocation schedules.",
                "We can observe that the dynamic version yields higher reward (as expected, since the reward for the dynamic version is always no less than the reward of the static version).",
                "We should point out that these graphs should not be viewed as a measure of performance of two different algorithms (both algorithms produce optimal solutions but to different problems), but rather as observations about how the quality of optimal solutions change as more flexibility is allowed in the reallocation of resources.",
                "Figure 4 shows runtime and policy value for trials in which common input variables are scaled together.",
                "This allows The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1225 2 4 6 8 10 10 −3 10 −2 10 −1 10 0 10 1 10 2 10 3 10 4 Number of Agents |M| CPUTime,sec |Ω| = 5, τ = 50 static dynamic 50 100 150 200 10 −2 10 −1 10 0 10 1 10 2 10 3 Global Time Boundary τ CPUTime,sec |M| = 5, |Ω| = 5 static dynamic 10 20 30 40 50 10 −2 10 −1 10 0 10 1 10 2 Number of Resources |Ω| CPUTime,sec |M| = 5, τ = 50 static dynamic 2 4 6 8 10 200 400 600 800 1000 1200 1400 1600 Number of Agents |M| Value |Ω| = 5, τ = 50 static dynamic 50 100 150 200 400 500 600 700 800 900 1000 1100 1200 1300 1400 Global Time Boundary τ Value |M| = 5, |Ω| = 5 static dynamic 10 20 30 40 50 500 600 700 800 900 1000 1100 1200 1300 1400 Number of Resources |Ω| Value |M| = 5, τ = 50 static dynamic Figure 3: Evaluation of our MILP for variable numbers of agents (column 1), lengths of global-time window (column 2), and numbers of resource types (column 3).",
                "Top row shows CPU time, and bottom row shows the joint reward of agents MDP policies.",
                "Error bars show the 1st and 3rd quartiles (25% and 75%). 2 4 6 8 10 10 −3 10 −2 10 −1 10 0 10 1 10 2 10 3 Number of Agents |M| CPUTime,sec τ = 10|M| static dynamic 2 4 6 8 10 10 −3 10 −2 10 −1 10 0 10 1 10 2 10 3 10 4 Number of Agents |M| CPUTime,sec |Ω| = 2|M| static dynamic 2 4 6 8 10 10 −3 10 −2 10 −1 10 0 10 1 10 2 10 3 10 4 Number of Agents |M| CPUTime,sec |Ω| = 5|M| static dynamic 2 4 6 8 10 200 400 600 800 1000 1200 1400 1600 1800 2000 2200 Number of Agents |M| Value τ = 10|M| static dynamic 2 4 6 8 10 200 400 600 800 1000 1200 1400 1600 1800 2000 Number of Agents |M| Value |Ω| = 2|M| static dynamic 2 4 6 8 10 0 500 1000 1500 2000 2500 Number of Agents |M| Value |Ω| = 5|M| static dynamic Figure 4: Evaluation of our MILP using correlated input variables.",
                "The left column tracks the performance and CPU time as the number of agents and global-time window increase together (bτ = 10|M|).",
                "The middle and the right column track the performance and CPU time as the number of resources and the number of agents increase together as |Ω| = 2|M| and |Ω| = 5|M|, respectively.",
                "Error bars show the 1st and 3rd quartiles (25% and 75%). 1226 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) us to explore domains where the total number of agents scales proportionally to the total number of resource types or the global time horizon, while keeping constant the average agent density (per unit of global time) or the average number of resources per agent (which commonly occurs in real-life applications).",
                "Overall, we believe that these experimental results indicate that our MILP formulation can be used to effectively solve resource-scheduling problems of nontrivial size. 6.",
                "DISCUSSION AND CONCLUSIONS Throughout the paper, we have made a number of assumptions in our model and solution algorithm; we discuss their implications below. • Continual execution.",
                "We assume that once an agent stops executing its MDP (transitions into state sf ), it exits the system and cannot return.",
                "It is easy to relax this assumption for domains where agents MDPs can be paused and restarted.",
                "All that is required is to include an additional pause action which transitions from a given state back to itself, and has zero reward. • Indifference to start time.",
                "We used a reward model where agents rewards depend only on the time horizon of their MDPs and not the global start time.",
                "This is a consequence of our MDP-augmentation procedure from Section 4.1.",
                "It is easy to extend the model so that the agents incur an explicit penalty for idling by assigning a non-zero negative reward to the start state sb . • Binary resource requirements.",
                "For simplicity, we have assumed that resource costs are binary: ϕm(a, ω) = {0, 1}, but our results generalize in a straightforward manner to non-binary resource mappings, analogously to the procedure used in [5]. • Cooperative agents.",
                "The optimization procedure discussed in this paper was developed in the context of cooperative agents, but it can also be used to design a mechanism for scheduling resources among self-interested agents.",
                "This optimization procedure can be embedded in a VickreyClarke-Groves auction, completely analogously to the way it was done in [7].",
                "In fact, all the results of [7] about the properties of the auction and information privacy directly carry over to the scheduling domain discussed in this paper, requiring only slight modifications to deal with finitehorizon MDPs. • Known, deterministic arrival and departure times.",
                "Finally, we have assumed that agents arrival and departure times (τa m and τd m) are deterministic and known a priori.",
                "This assumption is fundamental to our solution method.",
                "While there are many domains where this assumption is valid, in many cases agents arrive and depart dynamically and their arrival and departure times can only be predicted probabilistically, leading to online resource-allocation problems.",
                "In particular, in the case of self-interested agents, this becomes an interesting version of an online-mechanism-design problem [11, 12].",
                "In summary, we have presented an MILP formulation for the combinatorial resource-scheduling problem where agents values for possible resource assignments are defined by finitehorizon MDPs.",
                "This result extends previous work ([6, 7]) on static one-shot resource allocation under MDP-induced preferences to resource-scheduling problems with a temporal aspect.",
                "As such, this work takes a step in the direction of designing an online mechanism for agents with combinatorial resource preferences induced by stochastic planning problems.",
                "Relaxing the assumption about deterministic arrival and departure times of the agents is a focus of our future work.",
                "We would like to thank the anonymous reviewers for their insightful comments and suggestions. 7.",
                "REFERENCES [1] E. Altman and A. Shwartz.",
                "Adaptive control of constrained Markov chains: Criteria and policies.",
                "Annals of Operations Research, special issue on Markov Decision Processes, 28:101-134, 1991. [2] R. Bellman.",
                "Dynamic Programming.",
                "Princeton University Press, 1957. [3] C. Boutilier.",
                "Solving concisely expressed combinatorial auction problems.",
                "In Proc. of AAAI-02, pages 359-366, 2002. [4] C. Boutilier and H. H. Hoos.",
                "Bidding languages for combinatorial auctions.",
                "In Proc. of IJCAI-01, pages 1211-1217, 2001. [5] D. Dolgov.",
                "Integrated Resource Allocation and Planning in Stochastic Multiagent Environments.",
                "PhD thesis, Computer Science Department, University of Michigan, February 2006. [6] D. A. Dolgov and E. H. Durfee.",
                "Optimal resource allocation and policy formulation in loosely-coupled Markov decision processes.",
                "In Proc. of ICAPS-04, pages 315-324, June 2004. [7] D. A. Dolgov and E. H. Durfee.",
                "Computationally efficient combinatorial auctions for resource allocation in weakly-coupled MDPs.",
                "In Proc. of AAMAS-05, New York, NY, USA, 2005.",
                "ACM Press. [8] D. A. Dolgov and E. H. Durfee.",
                "Resource allocation among agents with preferences induced by factored MDPs.",
                "In Proc. of AAMAS-06, 2006. [9] K. Larson and T. Sandholm.",
                "Mechanism design and deliberative agents.",
                "In Proc. of AAMAS-05, pages 650-656, New York, NY, USA, 2005.",
                "ACM Press. [10] N. Nisan.",
                "Bidding and allocation in combinatorial auctions.",
                "In Electronic Commerce, 2000. [11] D. C. Parkes and S. Singh.",
                "An MDP-based approach to Online Mechanism Design.",
                "In Proc. of the Seventeenths Annual Conference on Neural Information Processing Systems (NIPS-03), 2003. [12] D. C. Parkes, S. Singh, and D. Yanovsky.",
                "Approximately efficient online mechanism design.",
                "In Proc. of the Eighteenths Annual Conference on Neural Information Processing Systems (NIPS-04), 2004. [13] M. L. Puterman.",
                "Markov Decision Processes.",
                "John Wiley & Sons, New York, 1994. [14] M. H. Rothkopf, A. Pekec, and R. M. Harstad.",
                "Computationally manageable combinational auctions.",
                "Management Science, 44(8):1131-1147, 1998. [15] T. Sandholm.",
                "An algorithm for optimal winner determination in combinatorial auctions.",
                "In Proc. of IJCAI-99, pages 542-547, San Francisco, CA, USA, 1999.",
                "Morgan Kaufmann Publishers Inc.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1227"
            ],
            "original_annotated_samples": [
                "Further, even when each agent has a utility function that is nonzero only on a small subset of the possible resource bundles, obtaining <br>optimal allocation</br> is still computationally prohibitive, as the problem becomes NP-complete [14].",
                "The problem of finding an <br>optimal allocation</br> that satisfies the global constraint that the amount of each resource ω allocated to all agents does not exceed the available amount bϕ(ω) can be expressed as the following integer program: max X m∈M X ψ∈Ψm zψ mvψ m subject to: X ψ∈Ψm zψ m ≤ 1, ∀m ∈ M; X m∈M X ψ∈Ψm zψ mnm(ψ, τ, ω) ≤ bϕ(ω), ∀τ ∈ [1, bτ], ∀ω ∈ Ω; (3) The first constraint in equation 3 says that no agent can receive more than one bundle, and the second constraint ensures that the total assignment of resource ω does not, at any time, exceed the resource bound."
            ],
            "translated_annotated_samples": [
                "Además, incluso cuando cada agente tiene una función de utilidad que es distinta de cero solo en un pequeño subconjunto de los posibles conjuntos de recursos, obtener una <br>asignación óptima</br> sigue siendo computacionalmente prohibitivo, ya que el problema se vuelve NP-completo [14].",
                "El problema de encontrar una <br>asignación óptima</br> que satisfaga la restricción global de que la cantidad de cada recurso ω asignada a todos los agentes no exceda la cantidad disponible bϕ(ω) se puede expresar como el siguiente programa entero: max X m∈M X ψ∈Ψm zψ mvψ m sujeto a: X ψ∈Ψm zψ m ≤ 1, ∀m ∈ M; X m∈M X ψ∈Ψm zψ mnm(ψ, τ, ω) ≤ bϕ(ω), ∀τ ∈ [1, bτ], ∀ω ∈ Ω; (3) La primera restricción en la ecuación 3 dice que ningún agente puede recibir más de un paquete, y la segunda restricción asegura que la asignación total del recurso ω no exceda, en ningún momento, el límite del recurso."
            ],
            "translated_text": "La programación de recursos combinatoria para MDPs multiagentes. Dmitri A. Dolgov, Michael R. James y Michael E. Samples. Grupo de Investigación en IA y Robótica, Centro Técnico de Toyota en EE. UU. {ddolgov, michael.r.james, michael.samples}@gmail.com. RESUMEN La programación óptima de recursos en sistemas multiagentes es una tarea computacionalmente desafiante, especialmente cuando los valores de los recursos no son aditivos. Consideramos el problema combinatorio de programar el uso de múltiples recursos entre agentes que operan en entornos estocásticos, modelados como procesos de decisión de Markov (MDPs). En los últimos años, se han desarrollado algoritmos eficientes de asignación de recursos para agentes con valores de recursos inducidos por MDPs. Sin embargo, este trabajo previo se ha centrado en problemas de asignación de recursos estáticos donde los recursos se distribuyen una vez y luego se utilizan en MDPs de horizonte infinito. Extendemos esos modelos existentes al problema de la programación de recursos combinatoria, donde los agentes persisten solo por períodos finitos entre sus tiempos de llegada y salida (predefinidos), requiriendo recursos solo durante esos períodos de tiempo. Proporcionamos un procedimiento computacionalmente eficiente para calcular asignaciones de recursos óptimas a nivel global a agentes a lo largo del tiempo. Ilustramos y analizamos empíricamente el método en el contexto de un dominio de programación de trabajos estocásticos. Categorías y Descriptores de Asignaturas I.2.8 [Inteligencia Artificial]: Resolución de Problemas, Métodos de Control y Búsqueda; I.2.11 [Inteligencia Artificial]: Inteligencia Artificial Distribuida-Sistemas Multiagente Términos Generales Algoritmos, Rendimiento, Diseño 1. INTRODUCCIÓN Las tareas de asignación óptima de recursos y programación son ubicuas en sistemas multiagentes, pero resolver tales problemas de optimización puede ser computacionalmente difícil, debido a una serie de factores. En particular, cuando el valor de un conjunto de recursos para un agente no es aditivo (como suele ser el caso con recursos que son sustitutos o complementos), la función de utilidad podría tener que definirse en un espacio exponencialmente grande de combinaciones de recursos, lo que rápidamente se vuelve computacionalmente intratable. Además, incluso cuando cada agente tiene una función de utilidad que es distinta de cero solo en un pequeño subconjunto de los posibles conjuntos de recursos, obtener una <br>asignación óptima</br> sigue siendo computacionalmente prohibitivo, ya que el problema se vuelve NP-completo [14]. Tales problemas computacionales han dado lugar recientemente a varias líneas de trabajo en el uso de modelos compactos de las preferencias de los agentes. Una idea es utilizar cualquier estructura presente en las funciones de utilidad para representarlas de forma compacta, por ejemplo, a través de fórmulas lógicas [15, 10, 4, 3]. Una alternativa es modelar directamente los mecanismos que definen las funciones de utilidad de los agentes y realizar la asignación de recursos directamente con estos modelos [9]. Una forma de lograr esto es modelar los procesos mediante los cuales un agente podría utilizar los recursos y definir la función de utilidad como la ganancia de estos procesos. En particular, si un agente utiliza recursos para actuar en un entorno estocástico, su función de utilidad puede ser modelada de forma natural con un proceso de decisión de Markov, cuyo conjunto de acciones está parametrizado por los recursos disponibles. Esta representación puede ser utilizada para construir algoritmos de asignación de recursos muy eficientes que resultan en una aceleración exponencial sobre un problema de optimización directa con representaciones planas de preferencias combinatorias [6, 7, 8]. Sin embargo, este trabajo existente sobre asignación de recursos con preferencias inducidas por MDPs parametrizados por recursos hace una suposición de que los recursos se asignan solo una vez y luego son utilizados por los agentes de forma independiente dentro de sus MDPs de horizonte infinito. Esta suposición de que no es posible la reasignación de recursos puede ser limitante en dominios donde los agentes llegan y se van dinámicamente. En este artículo, ampliamos el trabajo sobre asignación de recursos bajo preferencias inducidas por MDP a problemas de programación en tiempo discreto, donde los agentes están presentes en el sistema durante intervalos de tiempo finitos y solo pueden utilizar recursos dentro de estos intervalos. En particular, los agentes llegan y parten en momentos arbitrarios (predefinidos) y dentro de estos intervalos utilizan recursos para ejecutar tareas en MDPs de horizonte finito. Abordamos el problema de la programación de recursos óptima a nivel global, donde el objetivo es encontrar una asignación de recursos a los agentes a lo largo del tiempo que maximice la suma de las recompensas esperadas que obtienen. En este contexto, nuestra principal contribución es una formulación de programación entera mixta del problema de programación que elige asignaciones de recursos globalmente óptimas, tiempos de inicio y horizontes de ejecución para todos los agentes (dentro de sus intervalos de llegada y salida). Analizamos y comparamos empíricamente dos variantes del problema de programación: uno, donde los agentes tienen asignaciones de recursos estáticas dentro de sus MDPs de horizonte finito, y otro, donde los recursos pueden ser realocados dinámicamente entre agentes en cada paso de tiempo. En el resto del documento, primero establecemos las bases necesarias en la Sección 2 y luego presentamos nuestro modelo y declaración formal del problema en la Sección 3. En la Sección 4.2, describimos nuestro resultado principal, el programa de optimización para la programación de recursos globalmente óptima. Tras la discusión de nuestros resultados experimentales sobre un problema de programación de trabajos en la Sección 5, concluimos en la Sección 6 con una discusión de posibles extensiones y generalizaciones de nuestro método. De manera similar al modelo utilizado en trabajos anteriores sobre asignación de recursos con preferencias inducidas por MDP [6, 7], definimos el valor de un conjunto de recursos para un agente como el valor de la mejor política MDP que es realizable, dadas esos recursos. Sin embargo, dado que el enfoque de nuestro trabajo se centra en problemas de programación, y una gran parte del problema de optimización consiste en decidir cómo se asignan los recursos en el tiempo entre agentes con tiempos finitos de llegada y salida, modelamos los problemas de planificación de los agentes como MDPs de horizonte finito, en contraste con trabajos anteriores que utilizaron MDPs descontados de horizonte infinito. En el resto de esta sección, primero presentamos algo de antecedentes necesarios sobre MDPs de horizonte finito y presentamos una formulación de programación lineal que sirve como base para nuestro algoritmo de solución desarrollado en la Sección 4. También describimos los métodos estándar para la programación de recursos combinatorios con valores de recursos planos, que sirven como punto de referencia de comparación para el nuevo modelo desarrollado aquí. 2.1 Procesos de Decisión de Markov Un MDP (Proceso de Decisión de Markov) estacionario, de dominio finito y discreto en el tiempo (ver, por ejemplo, [13] para un desarrollo detallado y exhaustivo) se puede describir como S, A, p, r, donde: S es un conjunto finito de estados del sistema; A es un conjunto finito de acciones disponibles para el agente; p es una función de transición estocástica estacionaria, donde p(σ|s, a) es la probabilidad de transicionar al estado σ al ejecutar la acción a en el estado s; r es una función de recompensa estacionaria, donde r(s, a) especifica la recompensa obtenida al ejecutar la acción a en el estado s. Dado un MDP de este tipo, un problema de decisión bajo un horizonte finito T consiste en elegir una acción óptima en cada paso de tiempo para maximizar el valor esperado de la recompensa total acumulada durante la vida (finita) de los agentes. La política óptima del agente es entonces una función del estado actual s y el tiempo hasta el horizonte. Una política óptima para tal problema es actuar de forma codiciosa con respecto a la función de valor óptimo, definida de forma recursiva por el siguiente sistema de ecuaciones de Bellman de tiempo finito [2]: v(s, t) = max a r(s, a) + X σ p(σ|s, a)v(σ, t + 1), ∀s ∈ S, t ∈ [1, T − 1]; v(s, T) = 0, ∀s ∈ S; donde v(s, t) es el valor óptimo de estar en el estado s en el tiempo t ∈ [1, T]. Esta función de valor óptimo se puede calcular fácilmente utilizando programación dinámica, lo que conduce a la siguiente política óptima π, donde π(s, a, t) es la probabilidad de ejecutar la acción a en el estado s en el tiempo t: π(s, a, t) = (1, a = argmaxa r(s, a) + P σ p(σ|s, a)v(σ, t + 1), 0, en caso contrario. La anterior es la forma más común de calcular la función de valor óptimo (y por lo tanto una política óptima) para un MDP de horizonte finito. Sin embargo, también podemos formular el problema como el siguiente programa lineal (similarmente al LP dual para MDPs descontados de horizonte infinito [13, 6, 7]): max X s X a r(s, a) X t x(s, a, t) sujeto a: X a x(σ, a, t + 1) = X s,a p(σ|s, a)x(s, a, t) ∀σ, t ∈ [1, T − 1]; X a x(s, a, 1) = α(s), ∀s ∈ S; (1) donde α(s) es la distribución inicial sobre el espacio de estados, y x es la medida de ocupación (no estacionaria) (x(s, a, t) ∈ [0, 1] es el número total esperado de veces que se ejecuta la acción a en el estado s en el tiempo t). Una política óptima (no estacionaria) se obtiene a partir de la medida de ocupación de la siguiente manera: π(s, a, t) = x(s, a, t)/ X a x(s, a, t) ∀s ∈ S, t ∈ [1, T]. (2) Tenga en cuenta que el MDP estándar de horizonte finito sin restricciones, como se describe anteriormente, siempre tiene una solución uniformemente óptima (óptima para cualquier distribución inicial α(s)). Por lo tanto, una política óptima se puede obtener utilizando una constante arbitraria α(s) > 0 (en particular, α(s) = 1 resultará en x(s, a, t) = π(s, a, t)). Sin embargo, para los MDPs con restricciones de recursos (como se definen a continuación en la Sección 3), las políticas óptimas uniformemente no existen en general. En tales casos, α se convierte en parte de la entrada del problema, y una política resultante solo es óptima para ese α en particular. Este resultado es bien conocido para MDPs de horizonte infinito con varios tipos de restricciones [1, 6], y también se cumple para nuestro modelo de horizonte finito, el cual puede establecerse fácilmente a través de una línea de razonamiento completamente análoga a los argumentos en [6]. 2.2 Programación de Recursos Combinatoria Un enfoque directo para la programación de recursos para un conjunto de agentes M, cuyos valores para los recursos son inducidos por problemas de planificación estocástica (en nuestro caso, MDPs de horizonte finito) sería que cada agente enumerara todas las asignaciones de recursos posibles a lo largo del tiempo y, para cada una, calcular su valor resolviendo el MDP correspondiente. Entonces, cada agente proporcionaría valoraciones para cada posible conjunto de recursos a lo largo del tiempo a un coordinador centralizado, quien calcularía las asignaciones óptimas de recursos a lo largo del tiempo basándose en estas valoraciones. Cuando los recursos pueden asignarse en diferentes momentos a diferentes agentes, cada agente debe presentar valoraciones para cada combinación de posibles horizontes temporales. Que cada agente m ∈ M ejecute su MDP dentro del intervalo de tiempo de llegada-salida τ ∈ [τa m, τd m]. Por lo tanto, el agente m ejecutará un MDP con un horizonte temporal no mayor que Tm = τd m−τa m+1. Que bτ sea el horizonte temporal global para el problema, antes del cual todos los MDP de los agentes deben finalizar. Suponemos que τd m < bτ, ∀m ∈ M. El Sexto Congreso Internacional. En el problema de programación donde los agentes tienen requisitos de recursos estáticos dentro de sus MDPs de horizonte finito, los agentes proporcionan una valoración para cada conjunto de recursos para cada horizonte de tiempo posible (desde [1, Tm]) que puedan utilizar. Sea Ω el conjunto de recursos a ser asignados entre los agentes. Un agente recibirá como máximo un paquete de recursos para uno de los horizontes temporales. Que la variable ψ ∈ Ψm enumere todos los posibles pares de conjuntos de recursos y horizontes temporales para el agente m, por lo que hay 2|Ω| × Tm valores para ψ (el espacio de conjuntos es exponencial en el número de tipos de recursos |Ω|). El agente m debe proporcionar un valor vψ m para cada ψ, y el coordinador asignará como máximo un par (recurso, horizonte temporal) ψ a cada agente. Esta asignación se expresa como una variable indicadora zψ m ∈ {0, 1} que muestra si ψ está asignado al agente m. Para el tiempo τ y el recurso ω, la función nm(ψ, τ, ω) ∈ {0, 1} indica si el paquete en ψ utiliza el recurso ω en el tiempo τ (hacemos la suposición de que los agentes tienen requisitos de recursos binarios). Este problema de asignación es NP-completo, incluso al considerar solo un paso de tiempo, y su dificultad aumenta significativamente con múltiples pasos de tiempo debido al creciente número de valores de ψ. El problema de encontrar una <br>asignación óptima</br> que satisfaga la restricción global de que la cantidad de cada recurso ω asignada a todos los agentes no exceda la cantidad disponible bϕ(ω) se puede expresar como el siguiente programa entero: max X m∈M X ψ∈Ψm zψ mvψ m sujeto a: X ψ∈Ψm zψ m ≤ 1, ∀m ∈ M; X m∈M X ψ∈Ψm zψ mnm(ψ, τ, ω) ≤ bϕ(ω), ∀τ ∈ [1, bτ], ∀ω ∈ Ω; (3) La primera restricción en la ecuación 3 dice que ningún agente puede recibir más de un paquete, y la segunda restricción asegura que la asignación total del recurso ω no exceda, en ningún momento, el límite del recurso. Para el problema de programación en el que los agentes pueden reasignar dinámicamente recursos, cada agente debe especificar un valor para cada combinación de paquetes y pasos de tiempo dentro de su horizonte temporal. Que la variable ψ ∈ Ψm en este caso enumere todos los posibles conjuntos de recursos para los cuales a lo sumo un conjunto puede ser asignado al agente m en cada paso de tiempo. Por lo tanto, en este caso hay P t∈[1,Tm](2|Ω| )t ∼ 2|Ω|Tm posibilidades de combinaciones de recursos asignadas a diferentes intervalos de tiempo, para los Tm horizontes de tiempo diferentes. El mismo conjunto de ecuaciones (3) se puede utilizar para resolver este problema de programación dinámica, pero el programa entero es diferente debido a la diferencia en cómo se define ψ. En este caso, el número de valores de ψ es exponencial en el horizonte de planificación Tm de cada agente, lo que resulta en un programa mucho más grande. Este enfoque directo para resolver ambos problemas de programación requiere una enumeración y solución de 2|Ω| Tm (asignación estática) o P t∈[1,Tm] 2|Ω|t (reubicación dinámica) MDPs para cada agente, lo cual rápidamente se vuelve intratable con el crecimiento del número de recursos |Ω| o el horizonte de tiempo Tm. MODELO Y DECLARACIÓN DEL PROBLEMA Ahora presentamos formalmente nuestro modelo del problema de programación de recursos. La entrada del problema consiste en los siguientes componentes: • M, Ω, bϕ, τa m, τd m, bτ están definidos arriba en la Sección 2.2. • {Θm} = {S, A, pm, rm, αm} son los MDPs de todos los agentes m ∈ M. Sin pérdida de generalidad, asumimos que los espacios de estados y acciones de todos los agentes son iguales, pero cada uno tiene su propia función de transición pm, función de recompensa rm, y condiciones iniciales αm. • ϕm : A×Ω → {0, 1} es la asignación de acciones a recursos para el agente m. ϕm(a, ω) indica si la acción a del agente m necesita el recurso ω. Un agente m que recibe un conjunto de recursos que no incluye el recurso ω no puede ejecutar en su política MDP ninguna acción a para la cual ϕm(a, ω) = 0. Suponemos que todos los requisitos de recursos son binarios; como se discute más adelante en la Sección 6, esta suposición no es limitante. Dado el input anterior, el problema de optimización que consideramos es encontrar la asignación globalmente óptima que maximice la suma de recompensas esperadas de recursos a agentes para todos los pasos de tiempo: Δ : τ × M × Ω → {0, 1}. Una solución es factible si la asignación correspondiente de recursos a los agentes no viola la restricción global de recursos: X m Δm(τ, ω) ≤ bϕ(ω), ∀ω ∈ Ω, τ ∈ [1, bτ]. Consideramos dos variantes del problema de programación de recursos. La primera formulación restringe las asignaciones de recursos al espacio donde la asignación de recursos a cada agente es estática durante la vida de los agentes. La segunda formulación permite la reasignación de recursos entre agentes en cada paso de tiempo durante sus vidas. La Figura 1 representa un problema de programación de recursos con tres agentes M = {m1, m2, m3}, tres recursos Ω = {ω1, ω2, ω3}, y un horizonte de problema global de bτ = 11. Los tiempos de llegada y salida de los agentes se muestran como cajas grises y son {1, 6}, {3, 7} y {2, 11}, respectivamente. Una solución a este problema se muestra a través de barras horizontales dentro de cada casilla de agentes, donde las barras corresponden a la asignación de los tres tipos de recursos. La Figura 1a muestra una solución a un problema de programación estática. Según la solución mostrada, el agente m1 comienza la ejecución de su MDP en el tiempo τ = 1 y tiene un bloqueo en los tres recursos hasta que finaliza la ejecución en el tiempo τ = 3. Se debe tener en cuenta que el agente m1 renuncia a su control sobre los recursos antes de su hora de salida anunciada de τd m1 = 6, aparentemente porque otros agentes pueden utilizar los recursos de manera más efectiva. Por lo tanto, en el tiempo τ = 4, los recursos ω1 y ω3 se asignan al agente m2, quien luego los utiliza para ejecutar su MDP (utilizando solo acciones admitidas por los recursos ω1 y ω3) hasta el tiempo τ = 7. El agente m3 posee el recurso ω3 durante el intervalo τ ∈ [4, 10]. La Figura 1b muestra una posible solución a la versión dinámica del mismo problema. Allí, los recursos pueden ser reasignados entre agentes en cada paso de tiempo. Por ejemplo, el agente m1 renuncia a su uso del recurso ω2 en el tiempo τ = 2, aunque continúa la ejecución de su MDP hasta el tiempo τ = 6. Ten en cuenta que a un agente no se le permite detener y reiniciar su MDP, por lo que el agente m1 solo puede continuar ejecutando en el intervalo τ ∈ [3, 4] si tiene acciones que no requieren recursos (ϕm(a, ω) = 0). Claramente, el modelo y la declaración del problema descritos anteriormente hacen una serie de suposiciones sobre el problema y las propiedades de la solución deseada. Discutimos algunas de esas suposiciones y sus implicaciones en la Sección 6. 1222 La Sexta Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) (a) (b) Figura 1: Ilustración de una solución a un problema de programación de recursos con tres agentes y tres recursos: a) asignaciones de recursos estáticas (las asignaciones de recursos son constantes durante la vida de los agentes); b) asignación dinámica (las asignaciones de recursos pueden cambiar en cada paso de tiempo). 4. PROGRAMACIÓN DE RECURSOS Nuestro algoritmo de programación de recursos se lleva a cabo en dos etapas. Primero, realizamos un paso de preprocesamiento que aumenta los MDP de los agentes; este proceso se describe en la Sección 4.1. Segundo, utilizando estos MDPs aumentados construimos un problema de optimización global, el cual se describe en la Sección 4.2. 4.1 Aumentando los MDPs de los Agentes En el modelo descrito en la sección anterior, asumimos que si un agente no posee los recursos necesarios para realizar acciones en su MDP, su ejecución se detiene y el agente abandona el sistema. En otras palabras, los MDPs no pueden ser pausados y reanudados. Por ejemplo, en el problema mostrado en la Figura 1a, el agente m1 libera todos los recursos después del tiempo τ = 3, momento en el cual la ejecución de su MDP se detiene. De manera similar, los agentes m2 y m3 solo ejecutan sus MDPs en los intervalos τ ∈ [4, 6] y τ ∈ [4, 10], respectivamente. Por lo tanto, una parte importante del problema de toma de decisiones global es decidir la ventana de tiempo durante la cual cada uno de los agentes está activo (es decir, ejecutando su MDP). Para lograr esto, ampliamos el MDP de cada agente con dos nuevos estados (estados de inicio y final sb, sf, respectivamente) y una nueva acción de inicio/parada a∗, como se ilustra en la Figura 2. La idea es que un agente permanezca en el estado inicial sb hasta que esté listo para ejecutar su MDP, momento en el que realiza la acción de inicio/parada a∗ y transita al espacio de estados del MDP original con la probabilidad de transición que corresponde a la distribución inicial original α(s). Por ejemplo, en la Figura 1a, para el agente m2 esto ocurriría en el tiempo τ = 4. Una vez que el agente llega al final de su ventana de actividad (tiempo τ = 6 para el agente m2 en la Figura 1a), realiza la acción de inicio/parada, lo que lo lleva al estado final de finalización del sumidero sf en el tiempo τ = 7. Más precisamente, dado un MDP S, A, pm, rm, αm, definimos un MDP aumentado S, A, pm, rm, αm de la siguiente manera: S = S ∪ sb ∪ sf; A = A ∪ a∗; p(s|sb, a∗) = α(s), ∀s ∈ S; p(sb|sb, a) = 1.0, ∀a ∈ A; p(sf|s, a∗) = 1.0, ∀s ∈ S; p(σ|s, a) = p(σ|s, a), ∀s, σ ∈ S, a ∈ A; r(sb, a) = r(sf, a) = 0, ∀a ∈ A; r(s, a) = r(s, a), ∀s ∈ S, a ∈ A; α(sb) = 1; α(s) = 0, ∀s ∈ S; donde se asume que todas las probabilidades de transición no especificadas son cero. Además, para tener en cuenta el nuevo estado inicial, comenzamos el MDP un paso de tiempo antes, estableciendo τa m ← τa m − 1. Esto no afectará la asignación de recursos debido a que las restricciones de recursos solo se aplican a los estados originales del MDP, como se discutirá en la siguiente sección. Por ejemplo, los MDPs aumentados mostrados en la Figura 2b (que comienza en el estado sb en el tiempo τ = 2) se construirían a partir de un MDP con tiempo de llegada original τ = 3. La Figura 2b también muestra una trayectoria de muestra a través del espacio de estados: el agente comienza en el estado sb, transita al espacio de estados S del MDP original y finalmente sale al estado de sumidero sf. Cabe destacar que si quisiéramos modelar un problema en el que los agentes pudieran pausar sus MDP en pasos de tiempo arbitrarios (lo cual podría ser útil para dominios donde la reasignación dinámica es posible), podríamos lograrlo fácilmente incluyendo una acción adicional que transiciona de cada estado a sí mismo con una recompensa de cero. 4.2 MILP para la programación de recursos Dado un conjunto de MDPs aumentados, como se define arriba, el objetivo de esta sección es formular un programa de optimización global que resuelva el problema de programación de recursos. En esta sección y a continuación, se asume que todos los MDPs son los MDPs aumentados tal como se definen en la Sección 4.1. Nuestro enfoque es similar a la idea utilizada en [6]: comenzamos con la formulación del programa lineal de los MDP de los agentes (1) y lo complementamos con restricciones que aseguran que la asignación de recursos correspondiente entre agentes y tiempo sea válida. El problema de optimización resultante resuelve simultáneamente los MDP de los agentes y los problemas de programación de recursos. En el resto de esta sección, desarrollamos de forma incremental un programa de enteros mixtos (MILP) que logra esto. En ausencia de restricciones de recursos, los MDPs de horizonte finito de los agentes son completamente independientes, y la solución óptima global puede obtenerse trivialmente a través del siguiente LP, que es simplemente una agregación de LPs de horizonte finito de un solo agente: max X m X s X a rm(s, a) X t xm(s, a, t) sujeto a: X a xm(σ, a, t + 1) = X s,a pm(σ|s, a)xm(s, a, t), ∀m ∈ M, σ ∈ S, t ∈ [1, Tm − 1]; X a xm(s, a, 1) = αm(s), ∀m ∈ M, s ∈ S; (12) donde xm(s, a, t) es la medida de ocupación del agente m, y The Sixth Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 1223 (a) (b) Figura 2: Ilustración de la ampliación de un MDP para permitir tiempos de inicio y finalización variables: a) (izquierda) el MDP original de dos estados con una sola acción; (derecha) el MDP ampliado con nuevos estados sb y sf y la nueva acción a∗ (nota que las transiciones originales no se modifican en el proceso de ampliación); b) el MDP ampl augmentado mostrado como una trayectoria a través del tiempo (las líneas grises indican todas las transiciones, mientras que las líneas negras indican una trayectoria dada). Función objetivo (suma de recompensas esperadas de todos los agentes) máx X m X s X a rm(s, a) X t xm(s, a, t) (5) Implicación de significado Restricciones lineales Atar x a θ. El agente solo está activo cuando la medida de ocupación es distinta de cero en los estados originales del MDP. θm(τ) = 0 =⇒ xm(s, a, τ −τa m+1) = 0 ∀s /∈ {sb , sf }, a ∈ A X s/∈{sb,sf } X a xm(s, a, t) ≤ θm(τa m + t − 1) ∀m ∈ M, ∀t ∈ [1, Tm] (6) El agente solo puede estar activo en τ ∈ (τa m, τd m) θm(τ) = 0 ∀m ∈ M, τ /∈ (τa m, τd m) (7) No se pueden utilizar recursos cuando no está activo θm(τ) = 0 =⇒ Δm(τ, ω) = 0 ∀τ ∈ [0, bτ], ω ∈ Ω Δm(τ, ω) ≤ θm(τ) ∀m ∈ M, τ ∈ [0, bτ], ω ∈ Ω (8) Vincular x a Δ (x distinto de cero obliga a que el correspondiente Δ sea distinto de cero). Δm(τ, ω) = 0, ϕm(a, ω) = 1 =⇒ xm(s, a, τ − τa m + 1) = 0 ∀s /∈ {sb , sf } 1/|A| X a ϕm(a, ω) X s/∈{sb,sf } xm(s, a, t) ≤ Δm(t + τa m − 1, ω) ∀m ∈ M, ω ∈ Ω, t ∈ [1, Tm] (9) Límites de recursos X m Δm(τ, ω) ≤ bϕ(ω) ∀ω ∈ Ω, τ ∈ [0, bτ] (10) El agente no puede cambiar los recursos mientras está activo. Solo habilitado para programación con asignaciones estáticas. θm(τ) = 1 y θm(τ + 1) = 1 =⇒ Δm(τ, ω) = Δm(τ + 1, ω) Δm(τ, ω) − Z(1 − θm(τ + 1)) ≤ Δm(τ + 1, ω) + Z(1 − θm(τ)) Δm(τ, ω) + Z(1 − θm(τ + 1)) ≥ Δm(τ + 1, ω) − Z(1 − θm(τ)) ∀m ∈ M, ω ∈ Ω, τ ∈ [0, bτ] (11) Tabla 1: MILP para programación óptima global de recursos. Tm = τd m − τa m + 1 es el horizonte temporal para los agentes MDP. Usando este LP como base, lo ampliamos con restricciones que aseguran que el uso de recursos implicado por las medidas de ocupación de los agentes {xm} no viola los requisitos globales de recursos bϕ en ningún paso de tiempo τ ∈ [0, bτ]. Para formular estas restricciones de recursos, utilizamos las siguientes variables binarias: • Δm(τ, ω) = {0, 1}, ∀m ∈ M, τ ∈ [0, bτ], ω ∈ Ω, que sirven como variables indicadoras que definen si el agente m posee el recurso ω en el tiempo τ. Estos son análogos a las variables indicadoras estáticas utilizadas en el problema de asignación de recursos estáticos de una sola vez en [6]. • θm = {0, 1}, ∀m ∈ M, τ ∈ [0, bτ] son variables indicadoras que especifican si el agente m está activo (es decir, ejecutando su MDP) en el tiempo τ. El significado de las variables de uso de recursos Δ se ilustra en la Figura 1: Δm(τ, ω) = 1 solo si el recurso ω se asigna al agente m en el tiempo τ. El significado de los indicadores de actividad θ se ilustra en la Figura 2b: cuando el agente m se encuentra en el estado inicial sb o en el estado final sf, el θm correspondiente es igual a 0, pero una vez que el agente se vuelve activo y entra en uno de los otros estados, establecemos θm = 1. Este significado de θ puede ser impuesto con una restricción lineal que sincroniza los valores de las medidas de ocupación de los agentes xm y la actividad 1224 The Sixth Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) indicadores θ, como se muestra en (6) en la Tabla 1. Otra restricción que debemos agregar, debido a que los indicadores de actividad θ están definidos en la línea de tiempo global τ, es hacer cumplir el hecho de que el agente está inactivo fuera de su ventana de llegada y salida. Esto se logra mediante la restricción (7) en la Tabla 1. Además, los agentes no deben estar utilizando recursos mientras están inactivos. Esta restricción también puede ser impuesta a través de una desigualdad lineal en θ y Δ, como se muestra en (8). La restricción (6) establece el valor de θ para que coincida con la política definida por la medida de ocupación xm. De manera similar, debemos asegurarnos de que las variables de uso de recursos Δ estén también sincronizadas con la medida de ocupación xm. Esto se realiza a través de la restricción (9) en la Tabla 1, que es casi idéntica a la restricción análoga de [6]. Después de implementar la restricción anterior, que refuerza el significado de Δ, agregamos una restricción que garantiza que el uso de recursos de los agentes nunca exceda los montos de recursos disponibles. Esta condición también se expresa trivialmente como una desigualdad lineal (10) en la Tabla 1. Finalmente, para la formulación del problema en la que las asignaciones de recursos son estáticas durante la vida de un agente, agregamos una restricción que garantiza que las variables de uso de recursos Δ no cambien su valor mientras el agente está activo (θ = 1). Esto se logra a través de la restricción lineal (11), donde Z ≥ 2 es una constante que se utiliza para desactivar las restricciones cuando θm(τ) = 0 o θm(τ + 1) = 0. Esta restricción no se utiliza para la formulación del problema dinámico, donde los recursos pueden ser reasignados entre agentes en cada paso de tiempo. En resumen, la Tabla 1 junto con las restricciones de conservación de flujo de (12) define el MILP que calcula simultáneamente una asignación óptima de recursos para todos los agentes a lo largo del tiempo, así como políticas óptimas de MDP de horizonte finito que son válidas bajo esa asignación de recursos. Como una medida aproximada de la complejidad de este MILP, consideremos el número de variables de optimización y restricciones. Sea TM = P Tm = P m(τa m − τd m + 1) la suma de las longitudes de las ventanas de llegada-salida a través de todos los agentes. Entonces, el número de variables de optimización es: TM + bτ|M||Ω| + bτ|M|, de las cuales TM son continuas (xm), y bτ|M||Ω| + bτ|M| son binarias (Δ y θ). Sin embargo, hay que tener en cuenta que todos menos TM|M| de los θ se establecen en cero por la restricción (7), lo que también obliga inmediatamente a que todos menos TM|M||Ω| de los Δ sean cero a través de las restricciones (8). El número de restricciones (sin incluir las restricciones degeneradas en (7)) en el MILP es: TM + TM|Ω| + bτ|Ω| + bτ|M||Ω|. A pesar de que la complejidad del MILP es, en el peor de los casos, exponencial en el número de variables binarias, la complejidad de este MILP es significativamente (exponencialmente) menor que la del MILP con funciones de utilidad planas, descrito en la Sección 2.2. Este resultado refleja las ganancias de eficiencia reportadas en [6] para problemas de asignación de recursos de un solo disparo, pero es mucho más pronunciado, debido a la explosión de la representación de utilidad plana debido al aspecto temporal del problema (recuerde la complejidad prohibitiva de la optimización combinatoria en la Sección 2.2). Analizamos empíricamente el rendimiento de este método en la Sección 5.1. Estrictamente hablando, resolver MILPs hasta la optimalidad es NP-completo en el número de variables enteras. RESULTADOS EXPERIMENTALES Aunque la complejidad de resolver MILPs es en el peor de los casos exponencial en el número de variables enteras, existen muchos métodos eficientes para resolver MILPs que permiten que nuestro algoritmo se adapte bien a parámetros comunes en problemas de asignación de recursos y programación. En particular, esta sección introduce un dominio de problema: el problema del taller de reparación, que se utiliza para evaluar empíricamente la escalabilidad de nuestros algoritmos en términos del número de agentes |M|, el número de recursos compartidos |Ω| y las diferentes longitudes de tiempo global bτ durante las cuales los agentes pueden entrar y salir del sistema. El problema del taller de reparaciones es un MDP parametrizado simple que adopta la metáfora de un taller de reparaciones de vehículos. Los agentes en el taller de reparación son mecánicos con una serie de tareas independientes que solo generan recompensa cuando se completan. En nuestro modelo MDP de este sistema, las acciones tomadas para avanzar a través del espacio de estados solo están permitidas si el agente posee ciertos recursos que están públicamente disponibles para la tienda. Estos recursos son de suministro finito, y las políticas óptimas para la tienda determinarán cuándo cada agente puede retener los recursos limitados para tomar acciones y obtener recompensas individuales. Cada tarea a completar está asociada con una sola acción, aunque se requiere que el agente repita la acción numerosas veces antes de completar la tarea y obtener una recompensa. Este modelo fue parametrizado en términos del número de agentes en el sistema, el número de diferentes tipos de recursos que podrían estar vinculados a acciones necesarias, un tiempo global durante el cual se permite a los agentes llegar y partir, y una longitud máxima para el número de pasos de tiempo que un agente puede permanecer en el sistema. Todos los puntos de datos en nuestros experimentos se obtuvieron con 20 evaluaciones utilizando CPLEX para resolver los MILPs en una computadora Pentium4 con 2Gb de RAM. Se realizaron pruebas tanto en la versión estática como en la dinámica del problema de programación de recursos, tal como se definió anteriormente. La Figura 3 muestra el tiempo de ejecución y el valor de la política para modificaciones independientes en el conjunto de parámetros. La fila superior muestra cómo el tiempo de solución para el MILP escala a medida que aumentamos el número de agentes |M|, el horizonte de tiempo global bτ y el número de recursos |Ω|. Aumentar el número de agentes conduce a una escalabilidad exponencial en complejidad, lo cual es de esperarse para un problema NP-completo. Sin embargo, aumentar el límite de tiempo global bτ o el número total de tipos de recursos |Ω|, manteniendo constante el número de agentes, no conduce a una disminución en el rendimiento. Esto ocurre porque los problemas se vuelven más fáciles a medida que se vuelven sub-determinados, lo cual también es un fenómeno común en problemas NP-completos. También observamos que la solución a la versión dinámica del problema a menudo se puede calcular mucho más rápido que la versión estática. La fila inferior de la Figura 3 muestra el valor de la política conjunta de las políticas que corresponden a los horarios óptimos de asignación de recursos calculados. Podemos observar que la versión dinámica produce una recompensa más alta (como era de esperar, ya que la recompensa de la versión dinámica siempre es igual o mayor que la recompensa de la versión estática). Deberíamos señalar que estos gráficos no deben ser vistos como una medida del rendimiento de dos algoritmos diferentes (ambos algoritmos producen soluciones óptimas pero para problemas distintos), sino más bien como observaciones sobre cómo cambia la calidad de las soluciones óptimas a medida que se permite más flexibilidad en la reasignación de recursos. La Figura 4 muestra el tiempo de ejecución y el valor de la política para las pruebas en las que las variables de entrada comunes se escalan juntas. Esto permite la Sexta Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 1225 2 4 6 8 10 10 −3 10 −2 10 −1 10 0 10 1 10 2 10 3 10 4 Número de Agentes |M| Tiempo de CPU, seg |Ω| = 5, τ = 50 estático dinámico 50 100 150 200 10 −2 10 −1 10 0 10 1 10 2 10 3 Límite de Tiempo Global τ Tiempo de CPU, seg |M| = 5, |Ω| = 5 estático dinámico 10 20 30 40 50 10 −2 10 −1 10 0 10 1 10 2 Número de Recursos |Ω| Tiempo de CPU, seg |M| = 5, τ = 50 estático dinámico 2 4 6 8 10 200 400 600 800 1000 1200 1400 1600 Número de Agentes |M| Valor |Ω| = 5, τ = 50 estático dinámico 50 100 150 200 400 500 600 700 800 900 1000 1100 1200 1300 1400 Límite de Tiempo Global τ Valor |M| = 5, |Ω| = 5 estático dinámico 10 20 30 40 50 500 600 700 800 900 1000 1100 1200 1300 1400 Número de Recursos |Ω| Valor |M| = 5, τ = 50 estático dinámico Figura 3: Evaluación de nuestro MILP para números variables de agentes (columna 1), longitudes de ventana de tiempo global (columna 2) y números de tipos de recursos (columna 3). La fila superior muestra el tiempo de CPU, y la fila inferior muestra la recompensa conjunta de las políticas MDP de los agentes. Las barras de error muestran los cuartiles 1ro y 3ro (25% y 75%). 2 4 6 8 10 10 −3 10 −2 10 −1 10 0 10 1 10 2 10 3 Número de Agentes |M| Tiempo de CPU, seg τ = 10|M| estático dinámico 2 4 6 8 10 10 −3 10 −2 10 −1 10 0 10 1 10 2 10 3 10 4 Número de Agentes |M| Tiempo de CPU, seg |Ω| = 2|M| estático dinámico 2 4 6 8 10 10 −3 10 −2 10 −1 10 0 10 1 10 2 10 3 10 4 Número de Agentes |M| Tiempo de CPU, seg |Ω| = 5|M| estático dinámico 2 4 6 8 10 200 400 600 800 1000 1200 1400 1600 1800 2000 2200 Número de Agentes |M| Valor τ = 10|M| estático dinámico 2 4 6 8 10 200 400 600 800 1000 1200 1400 1600 1800 2000 Número de Agentes |M| Valor |Ω| = 2|M| estático dinámico 2 4 6 8 10 0 500 1000 1500 2000 2500 Número de Agentes |M| Valor |Ω| = 5|M| estático dinámico Figura 4: Evaluación de nuestro MILP utilizando variables de entrada correlacionadas. La columna izquierda sigue el rendimiento y el tiempo de CPU a medida que aumenta el número de agentes y la ventana de tiempo global juntos (bτ = 10|M|). La columna del medio y la columna de la derecha siguen el rendimiento y el tiempo de CPU a medida que aumenta el número de recursos y el número de agentes juntos, con |Ω| = 2|M| y |Ω| = 5|M|, respectivamente. Las barras de error muestran el primer y tercer cuartil (25% y 75%). 1226 The Sixth Intl. La Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) nos permite explorar dominios donde el número total de agentes escala proporcionalmente al número total de tipos de recursos o al horizonte temporal global, manteniendo constante la densidad promedio de agentes (por unidad de tiempo global) o el número promedio de recursos por agente (lo cual comúnmente ocurre en aplicaciones de la vida real). En general, creemos que estos resultados experimentales indican que nuestra formulación MILP puede ser utilizada para resolver de manera efectiva problemas de programación de recursos de tamaño no trivial. DISCUSIÓN Y CONCLUSIONES A lo largo del documento, hemos realizado una serie de suposiciones en nuestro modelo y algoritmo de solución; discutimos sus implicaciones a continuación. • Ejecución continua. Suponemos que una vez que un agente deja de ejecutar su MDP (transiciona al estado sf), sale del sistema y no puede regresar. Es fácil relajar esta suposición para dominios donde los agentes de MDP pueden ser pausados y reiniciados. Todo lo que se requiere es incluir una acción de pausa adicional que haga la transición desde un estado dado de regreso a sí mismo, y que tenga una recompensa de cero. • Indiferencia al tiempo de inicio. Utilizamos un modelo de recompensa donde las recompensas de los agentes dependen solo del horizonte temporal de sus MDP y no del tiempo de inicio global. Esta es una consecuencia de nuestro procedimiento de aumento de MDP de la Sección 4.1. Es fácil extender el modelo para que los agentes incurran en una penalización explícita por inactividad al asignar una recompensa negativa no nula al estado inicial sb. • Requisitos de recursos binarios. Para simplificar, hemos asumido que los costos de recursos son binarios: ϕm(a, ω) = {0, 1}, pero nuestros resultados se generalizan de manera directa a asignaciones de recursos no binarias, de manera análoga al procedimiento utilizado en [5]. • Agentes cooperativos. El procedimiento de optimización discutido en este artículo fue desarrollado en el contexto de agentes cooperativos, pero también puede ser utilizado para diseñar un mecanismo para programar recursos entre agentes egoístas. Este procedimiento de optimización puede ser incorporado en una subasta de Vickrey-Clarke-Groves, de manera completamente análoga a como se hizo en [7]. De hecho, todos los resultados de [7] sobre las propiedades de la subasta y la privacidad de la información se aplican directamente al dominio de programación discutido en este documento, requiriendo solo ligeras modificaciones para tratar con MDPs de horizonte finito. • Tiempos de llegada y salida conocidos y deterministas. Finalmente, hemos asumido que los tiempos de llegada y salida de los agentes (τa m y τd m) son deterministas y conocidos de antemano. Esta suposición es fundamental para nuestro método de solución. Si bien hay muchos dominios donde esta suposición es válida, en muchos casos los agentes llegan y se van de forma dinámica y sus tiempos de llegada y salida solo pueden predecirse de manera probabilística, lo que lleva a problemas de asignación de recursos en línea. En particular, en el caso de agentes con interés propio, esto se convierte en una versión interesante de un problema de diseño de mecanismos en línea [11, 12]. En resumen, hemos presentado una formulación MILP para el problema combinatorio de programación de recursos donde los valores de los agentes para posibles asignaciones de recursos están definidos por MDPs de horizonte finito. Este resultado amplía el trabajo previo ([6, 7]) sobre asignación estática de recursos de una sola vez bajo preferencias inducidas por MDP a problemas de programación de recursos con un aspecto temporal. Por lo tanto, este trabajo da un paso en la dirección de diseñar un mecanismo en línea para agentes con preferencias de recursos combinatorios inducidas por problemas de planificación estocástica. Relajar la suposición sobre los tiempos deterministas de llegada y salida de los agentes es un enfoque de nuestro trabajo futuro. Nos gustaría agradecer a los revisores anónimos por sus comentarios y sugerencias perspicaces. 7. REFERENCIAS [1] E. Altman y A. Shwartz. Control adaptativo de cadenas de Markov restringidas: Criterios y políticas. Anales de Investigación de Operaciones, número especial sobre Procesos de Decisión de Markov, 28:101-134, 1991. [2] R. Bellman. Programación dinámica. Princeton University Press, 1957. [3] C. Boutilier. \n\nPrensa de la Universidad de Princeton, 1957. [3] C. Boutilier. Resolviendo problemas de subasta combinatoria expresados de manera concisa. En Proc. de AAAI-02, páginas 359-366, 2002. [4] C. Boutilier y H. H. Hoos. Subastando idiomas para subastas combinatorias. En Proc. de IJCAI-01, páginas 1211-1217, 2001. [5] D. Dolgov. Asignación integrada de recursos y planificación en entornos estocásticos de múltiples agentes. Tesis doctoral, Departamento de Ciencias de la Computación, Universidad de Michigan, febrero de 2006. [6] D. A. Dolgov y E. H. Durfee. Asignación óptima de recursos y formulación de políticas en procesos de decisión de Markov débilmente acoplados. En Proc. de ICAPS-04, páginas 315-324, junio de 2004. [7] D. A. Dolgov y E. H. Durfee. Subastas combinatorias computacionalmente eficientes para asignación de recursos en MDPs débilmente acoplados. En Proc. de AAMAS-05, Nueva York, NY, EE. UU., 2005. ACM Press. [8] D. A. Dolgov y E. H. Durfee. Asignación de recursos entre agentes con preferencias inducidas por MDP factorizados. En Proc. de AAMAS-06, 2006. [9] K. Larson y T. Sandholm. Diseño de mecanismos y agentes deliberativos. En Proc. de AAMAS-05, páginas 650-656, Nueva York, NY, EE. UU., 2005. ACM Press. [10] N. Nisan. \n\nACM Press. [10] N. Nisan. Subasta y asignación en subastas combinatorias. En Comercio Electrónico, 2000. [11] D. C. Parkes y S. Singh. Un enfoque basado en MDP para el Diseño de Mecanismos en Línea. En Proc. de la Decimoséptima Conferencia Anual sobre Sistemas de Procesamiento de Información Neural (NIPS-03), 2003. [12] D. C. Parkes, S. Singh y D. Yanovsky. Diseño de mecanismos en línea aproximadamente eficiente. En Actas de la Decimoctava Conferencia Anual sobre Sistemas de Procesamiento de Información Neural (NIPS-04), 2004. [13] M. L. Puterman. Procesos de Decisión de Markov. John Wiley & Sons, Nueva York, 1994. [14] M. H. Rothkopf, A. Pekec y R. M. Harstad. Subastas combinatorias manejables computacionalmente. Ciencia de la Gestión, 44(8):1131-1147, 1998. [15] T. Sandholm. Un algoritmo para la determinación óptima del ganador en subastas combinatorias. En Proc. de IJCAI-99, páginas 542-547, San Francisco, CA, EE. UU., 1999. Morgan Kaufmann Publishers Inc. - Editores Morgan Kaufmann Inc. El Sexto Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 1227 ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "discrete-time scheduling problem": {
            "translated_key": "problemas de programación en tiempo discreto",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Combinatorial Resource Scheduling for Multiagent MDPs Dmitri A. Dolgov, Michael R. James, and Michael E. Samples AI and Robotics Group Technical Research, Toyota Technical Center USA {ddolgov, michael.r.james, michael.samples}@gmail.com ABSTRACT Optimal resource scheduling in multiagent systems is a computationally challenging task, particularly when the values of resources are not additive.",
                "We consider the combinatorial problem of scheduling the usage of multiple resources among agents that operate in stochastic environments, modeled as Markov decision processes (MDPs).",
                "In recent years, efficient resource-allocation algorithms have been developed for agents with resource values induced by MDPs.",
                "However, this prior work has focused on static resource-allocation problems where resources are distributed once and then utilized in infinite-horizon MDPs.",
                "We extend those existing models to the problem of combinatorial resource scheduling, where agents persist only for finite periods between their (predefined) arrival and departure times, requiring resources only for those time periods.",
                "We provide a computationally efficient procedure for computing globally optimal resource assignments to agents over time.",
                "We illustrate and empirically analyze the method in the context of a stochastic jobscheduling domain.",
                "Categories and Subject Descriptors I.2.8 [Artificial Intelligence]: Problem Solving, Control Methods, and Search; I.2.11 [Artificial Intelligence]: Distributed Artificial Intelligence-Multiagent systems General Terms Algorithms, Performance, Design 1.",
                "INTRODUCTION The tasks of optimal resource allocation and scheduling are ubiquitous in multiagent systems, but solving such optimization problems can be computationally difficult, due to a number of factors.",
                "In particular, when the value of a set of resources to an agent is not additive (as is often the case with resources that are substitutes or complements), the utility function might have to be defined on an exponentially large space of resource bundles, which very quickly becomes computationally intractable.",
                "Further, even when each agent has a utility function that is nonzero only on a small subset of the possible resource bundles, obtaining optimal allocation is still computationally prohibitive, as the problem becomes NP-complete [14].",
                "Such computational issues have recently spawned several threads of work in using compact models of agents preferences.",
                "One idea is to use any structure present in utility functions to represent them compactly, via, for example, logical formulas [15, 10, 4, 3].",
                "An alternative is to directly model the mechanisms that define the agents utility functions and perform resource allocation directly with these models [9].",
                "A way of accomplishing this is to model the processes by which an agent might utilize the resources and define the utility function as the payoff of these processes.",
                "In particular, if an agent uses resources to act in a stochastic environment, its utility function can be naturally modeled with a Markov decision process, whose action set is parameterized by the available resources.",
                "This representation can then be used to construct very efficient resource-allocation algorithms that lead to an exponential speedup over a straightforward optimization problem with flat representations of combinatorial preferences [6, 7, 8].",
                "However, this existing work on resource allocation with preferences induced by resource-parameterized MDPs makes an assumption that the resources are only allocated once and are then utilized by the agents independently within their infinite-horizon MDPs.",
                "This assumption that no reallocation of resources is possible can be limiting in domains where agents arrive and depart dynamically.",
                "In this paper, we extend the work on resource allocation under MDP-induced preferences to <br>discrete-time scheduling problem</br>s, where agents are present in the system for finite time intervals and can only use resources within these intervals.",
                "In particular, agents arrive and depart at arbitrary (predefined) times and within these intervals use resources to execute tasks in finite-horizon MDPs.",
                "We address the problem of globally optimal resource scheduling, where the objective is to find an allocation of resources to the agents across time that maximizes the sum of the expected rewards that they obtain.",
                "In this context, our main contribution is a mixed-integerprogramming formulation of the scheduling problem that chooses globally optimal resource assignments, starting times, and execution horizons for all agents (within their arrival1220 978-81-904262-7-5 (RPS) c 2007 IFAAMAS departure intervals).",
                "We analyze and empirically compare two flavors of the scheduling problem: one, where agents have static resource assignments within their finite-horizon MDPs, and another, where resources can be dynamically reallocated between agents at every time step.",
                "In the rest of the paper, we first lay down the necessary groundwork in Section 2 and then introduce our model and formal problem statement in Section 3.",
                "In Section 4.2, we describe our main result, the optimization program for globally optimal resource scheduling.",
                "Following the discussion of our experimental results on a job-scheduling problem in Section 5, we conclude in Section 6 with a discussion of possible extensions and generalizations of our method. 2.",
                "BACKGROUND Similarly to the model used in previous work on resourceallocation with MDP-induced preferences [6, 7], we define the value of a set of resources to an agent as the value of the best MDP policy that is realizable, given those resources.",
                "However, since the focus of our work is on scheduling problems, and a large part of the optimization problem is to decide how resources are allocated in time among agents with finite arrival and departure times, we model the agents planning problems as finite-horizon MDPs, in contrast to previous work that used infinite-horizon discounted MDPs.",
                "In the rest of this section, we first introduce some necessary background on finite-horizon MDPs and present a linear-programming formulation that serves as the basis for our solution algorithm developed in Section 4.",
                "We also outline the standard methods for combinatorial resource scheduling with flat resource values, which serve as a comparison benchmark for the new model developed here. 2.1 Markov Decision Processes A stationary, finite-domain, discrete-time MDP (see, for example, [13] for a thorough and detailed development) can be described as S, A, p, r , where: S is a finite set of system states; A is a finite set of actions that are available to the agent; p is a stationary stochastic transition function, where p(σ|s, a) is the probability of transitioning to state σ upon executing action a in state s; r is a stationary reward function, where r(s, a) specifies the reward obtained upon executing action a in state s. Given such an MDP, a decision problem under a finite horizon T is to choose an optimal action at every time step to maximize the expected value of the total reward accrued during the agents (finite) lifetime.",
                "The agents optimal policy is then a function of current state s and the time until the horizon.",
                "An optimal policy for such a problem is to act greedily with respect to the optimal value function, defined recursively by the following system of finite-time Bellman equations [2]: v(s, t) = max a r(s, a) + X σ p(σ|s, a)v(σ, t + 1), ∀s ∈ S, t ∈ [1, T − 1]; v(s, T) = 0, ∀s ∈ S; where v(s, t) is the optimal value of being in state s at time t ∈ [1, T].",
                "This optimal value function can be easily computed using dynamic programming, leading to the following optimal policy π, where π(s, a, t) is the probability of executing action a in state s at time t: π(s, a, t) = ( 1, a = argmaxa r(s, a) + P σ p(σ|s, a)v(σ, t + 1), 0, otherwise.",
                "The above is the most common way of computing the optimal value function (and therefore an optimal policy) for a finite-horizon MDP.",
                "However, we can also formulate the problem as the following linear program (similarly to the dual LP for infinite-horizon discounted MDPs [13, 6, 7]): max X s X a r(s, a) X t x(s, a, t) subject to: X a x(σ, a, t + 1) = X s,a p(σ|s, a)x(s, a, t) ∀σ, t ∈ [1, T − 1]; X a x(s, a, 1) = α(s), ∀s ∈ S; (1) where α(s) is the initial distribution over the state space, and x is the (non-stationary) occupation measure (x(s, a, t) ∈ [0, 1] is the total expected number of times action a is executed in state s at time t).",
                "An optimal (non-stationary) policy is obtained from the occupation measure as follows: π(s, a, t) = x(s, a, t)/ X a x(s, a, t) ∀s ∈ S, t ∈ [1, T]. (2) Note that the standard unconstrained finite-horizon MDP, as described above, always has a uniformly-optimal solution (optimal for any initial distribution α(s)).",
                "Therefore, an optimal policy can be obtained by using an arbitrary constant α(s) > 0 (in particular, α(s) = 1 will result in x(s, a, t) = π(s, a, t)).",
                "However, for MDPs with resource constraints (as defined below in Section 3), uniformly-optimal policies do not in general exist.",
                "In such cases, α becomes a part of the problem input, and a resulting policy is only optimal for that particular α.",
                "This result is well known for infinite-horizon MDPs with various types of constraints [1, 6], and it also holds for our finite-horizon model, which can be easily established via a line of reasoning completely analogous to the arguments in [6]. 2.2 Combinatorial Resource Scheduling A straightforward approach to resource scheduling for a set of agents M, whose values for the resources are induced by stochastic planning problems (in our case, finite-horizon MDPs) would be to have each agent enumerate all possible resource assignments over time and, for each one, compute its value by solving the corresponding MDP.",
                "Then, each agent would provide valuations for each possible resource bundle over time to a centralized coordinator, who would compute the optimal resource assignments across time based on these valuations.",
                "When resources can be allocated at different times to different agents, each agent must submit valuations for every combination of possible time horizons.",
                "Let each agent m ∈ M execute its MDP within the arrival-departure time interval τ ∈ [τa m, τd m].",
                "Hence, agent m will execute an MDP with time horizon no greater than Tm = τd m−τa m+1.",
                "Let bτ be the global time horizon for the problem, before which all of the agents MDPs must finish.",
                "We assume τd m < bτ, ∀m ∈ M. The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1221 For the scheduling problem where agents have static resource requirements within their finite-horizon MDPs, the agents provide a valuation for each resource bundle for each possible time horizon (from [1, Tm]) that they may use.",
                "Let Ω be the set of resources to be allocated among the agents.",
                "An agent will get at most one resource bundle for one of the time horizons.",
                "Let the variable ψ ∈ Ψm enumerate all possible pairs of resource bundles and time horizons for agent m, so there are 2|Ω| × Tm values for ψ (the space of bundles is exponential in the number of resource types |Ω|).",
                "The agent m must provide a value vψ m for each ψ, and the coordinator will allocate at most one ψ (resource, time horizon) pair to each agent.",
                "This allocation is expressed as an indicator variable zψ m ∈ {0, 1} that shows whether ψ is assigned to agent m. For time τ and resource ω, the function nm(ψ, τ, ω) ∈ {0, 1} indicates whether the bundle in ψ uses resource ω at time τ (we make the assumption that agents have binary resource requirements).",
                "This allocation problem is NP-complete, even when considering only a single time step, and its difficulty increases significantly with multiple time steps because of the increasing number of values of ψ.",
                "The problem of finding an optimal allocation that satisfies the global constraint that the amount of each resource ω allocated to all agents does not exceed the available amount bϕ(ω) can be expressed as the following integer program: max X m∈M X ψ∈Ψm zψ mvψ m subject to: X ψ∈Ψm zψ m ≤ 1, ∀m ∈ M; X m∈M X ψ∈Ψm zψ mnm(ψ, τ, ω) ≤ bϕ(ω), ∀τ ∈ [1, bτ], ∀ω ∈ Ω; (3) The first constraint in equation 3 says that no agent can receive more than one bundle, and the second constraint ensures that the total assignment of resource ω does not, at any time, exceed the resource bound.",
                "For the scheduling problem where the agents are able to dynamically reallocate resources, each agent must specify a value for every combination of bundles and time steps within its time horizon.",
                "Let the variable ψ ∈ Ψm in this case enumerate all possible resource bundles for which at most one bundle may be assigned to agent m at each time step.",
                "Therefore, in this case there are P t∈[1,Tm](2|Ω| )t ∼ 2|Ω|Tm possibilities of resource bundles assigned to different time slots, for the Tm different time horizons.",
                "The same set of equations (3) can be used to solve this dynamic scheduling problem, but the integer program is different because of the difference in how ψ is defined.",
                "In this case, the number of ψ values is exponential in each agents planning horizon Tm, resulting in a much larger program.",
                "This straightforward approach to solving both of these scheduling problems requires an enumeration and solution of either 2|Ω| Tm (static allocation) or P t∈[1,Tm] 2|Ω|t (dynamic reallocation) MDPs for each agent, which very quickly becomes intractable with the growth of the number of resources |Ω| or the time horizon Tm. 3.",
                "MODEL AND PROBLEM STATEMENT We now formally introduce our model of the resourcescheduling problem.",
                "The problem input consists of the following components: • M, Ω, bϕ, τa m, τd m, bτ are as defined above in Section 2.2. • {Θm} = {S, A, pm, rm, αm} are the MDPs of all agents m ∈ M. Without loss of generality, we assume that state and action spaces of all agents are the same, but each has its own transition function pm, reward function rm, and initial conditions αm. • ϕm : A×Ω → {0, 1} is the mapping of actions to resources for agent m. ϕm(a, ω) indicates whether action a of agent m needs resource ω.",
                "An agent m that receives a set of resources that does not include resource ω cannot execute in its MDP policy any action a for which ϕm(a, ω) = 0.",
                "We assume all resource requirements are binary; as discussed below in Section 6, this assumption is not limiting.",
                "Given the above input, the optimization problem we consider is to find the globally optimal-maximizing the sum of expected rewards-mapping of resources to agents for all time steps: Δ : τ × M × Ω → {0, 1}.",
                "A solution is feasible if the corresponding assignment of resources to the agents does not violate the global resource constraint: X m Δm(τ, ω) ≤ bϕ(ω), ∀ω ∈ Ω, τ ∈ [1, bτ]. (4) We consider two flavors of the resource-scheduling problem.",
                "The first formulation restricts resource assignments to the space where the allocation of resources to each agent is static during the agents lifetime.",
                "The second formulation allows reassignment of resources between agents at every time step within their lifetimes.",
                "Figure 1 depicts a resource-scheduling problem with three agents M = {m1, m2, m3}, three resources Ω = {ω1, ω2, ω3}, and a global problem horizon of bτ = 11.",
                "The agents arrival and departure times are shown as gray boxes and are {1, 6}, {3, 7}, and {2, 11}, respectively.",
                "A solution to this problem is shown via horizontal bars within each agents box, where the bars correspond to the allocation of the three resource types.",
                "Figure 1a shows a solution to a static scheduling problem.",
                "According to the shown solution, agent m1 begins the execution of its MDP at time τ = 1 and has a lock on all three resources until it finishes execution at time τ = 3.",
                "Note that agent m1 relinquishes its hold on the resources before its announced departure time of τd m1 = 6, ostensibly because other agents can utilize the resources more effectively.",
                "Thus, at time τ = 4, resources ω1 and ω3 are allocated to agent m2, who then uses them to execute its MDP (using only actions supported by resources ω1 and ω3) until time τ = 7.",
                "Agent m3 holds resource ω3 during the interval τ ∈ [4, 10].",
                "Figure 1b shows a possible solution to the dynamic version of the same problem.",
                "There, resources can be reallocated between agents at every time step.",
                "For example, agent m1 gives up its use of resource ω2 at time τ = 2, although it continues the execution of its MDP until time τ = 6.",
                "Notice that an agent is not allowed to stop and restart its MDP, so agent m1 is only able to continue executing in the interval τ ∈ [3, 4] if it has actions that do not require any resources (ϕm(a, ω) = 0).",
                "Clearly, the model and problem statement described above make a number of assumptions about the problem and the desired solution properties.",
                "We discuss some of those assumptions and their implications in Section 6. 1222 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) (a) (b) Figure 1: Illustration of a solution to a resource-scheduling problem with three agents and three resources: a) static resource assignments (resource assignments are constant within agents lifetimes; b) dynamic assignment (resource assignments are allowed to change at every time step). 4.",
                "RESOURCE SCHEDULING Our resource-scheduling algorithm proceeds in two stages.",
                "First, we perform a preprocessing step that augments the agent MDPs; this process is described in Section 4.1.",
                "Second, using these augmented MDPs we construct a global optimization problem, which is described in Section 4.2. 4.1 Augmenting Agents MDPs In the model described in the previous section, we assume that if an agent does not possess the necessary resources to perform actions in its MDP, its execution is halted and the agent leaves the system.",
                "In other words, the MDPs cannot be paused and resumed.",
                "For example, in the problem shown in Figure 1a, agent m1 releases all resources after time τ = 3, at which point the execution of its MDP is halted.",
                "Similarly, agents m2 and m3 only execute their MDPs in the intervals τ ∈ [4, 6] and τ ∈ [4, 10], respectively.",
                "Therefore, an important part of the global decision-making problem is to decide the window of time during which each of the agents is active (i.e., executing its MDP).",
                "To accomplish this, we augment each agents MDP with two new states (start and finish states sb , sf , respectively) and a new start/stop action a∗ , as illustrated in Figure 2.",
                "The idea is that an agent stays in the start state sb until it is ready to execute its MDP, at which point it performs the start/stop action a∗ and transitions into the state space of the original MDP with the transition probability that corresponds to the original initial distribution α(s).",
                "For example, in Figure 1a, for agent m2 this would happen at time τ = 4.",
                "Once the agent gets to the end of its activity window (time τ = 6 for agent m2 in Figure 1a), it performs the start/stop action, which takes it into the sink finish state sf at time τ = 7.",
                "More precisely, given an MDP S, A, pm, rm, αm , we define an augmented MDP S , A , pm, rm, αm as follows: S = S ∪ sb ∪ sf ; A = A ∪ a∗ ; p (s|sb , a∗ ) = α(s), ∀s ∈ S; p (sb |sb , a) = 1.0, ∀a ∈ A; p (sf |s, a∗ ) = 1.0, ∀s ∈ S; p (σ|s, a) = p(σ|s, a), ∀s, σ ∈ S, a ∈ A; r (sb , a) = r (sf , a) = 0, ∀a ∈ A ; r (s, a) = r(s, a), ∀s ∈ S, a ∈ A; α (sb ) = 1; α (s) = 0, ∀s ∈ S; where all non-specified transition probabilities are assumed to be zero.",
                "Further, in order to account for the new starting state, we begin the MDP one time-step earlier, setting τa m ← τa m − 1.",
                "This will not affect the resource allocation due to the resource constraints only being enforced for the original MDP states, as will be discussed in the next section.",
                "For example, the augmented MDPs shown in Figure 2b (which starts in state sb at time τ = 2) would be constructed from an MDP with original arrival time τ = 3.",
                "Figure 2b also shows a sample trajectory through the state space: the agent starts in state sb , transitions into the state space S of the original MDP, and finally exists into the sink state sf .",
                "Note that if we wanted to model a problem where agents could pause their MDPs at arbitrary time steps (which might be useful for domains where dynamic reallocation is possible), we could easily accomplish this by including an extra action that transitions from each state to itself with zero reward. 4.2 MILP for Resource Scheduling Given a set of augmented MDPs, as defined above, the goal of this section is to formulate a global optimization program that solves the resource-scheduling problem.",
                "In this section and below, all MDPs are assumed to be the augmented MDPs as defined in Section 4.1.",
                "Our approach is similar to the idea used in [6]: we begin with the linear-program formulation of agents MDPs (1) and augment it with constraints that ensure that the corresponding resource allocation across agents and time is valid.",
                "The resulting optimization problem then simultaneously solves the agents MDPs and resource-scheduling problems.",
                "In the rest of this section, we incrementally develop a mixed integer program (MILP) that achieves this.",
                "In the absence of resource constraints, the agents finitehorizon MDPs are completely independent, and the globally optimal solution can be trivially obtained via the following LP, which is simply an aggregation of single-agent finitehorizon LPs: max X m X s X a rm(s, a) X t xm(s, a, t) subject to: X a xm(σ, a, t + 1) = X s,a pm(σ|s, a)xm(s, a, t), ∀m ∈ M, σ ∈ S, t ∈ [1, Tm − 1]; X a xm(s, a, 1) = αm(s), ∀m ∈ M, s ∈ S; (12) where xm(s, a, t) is the occupation measure of agent m, and The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1223 (a) (b) Figure 2: Illustration of augmenting an MDP to allow for variable starting and stopping times: a) (left) the original two-state MDP with a single action; (right) the augmented MDP with new states sb and sf and the new action a∗ (note that the origianl transitions are not changed in the augmentation process); b) the augmented MDP displayed as a trajectory through time (grey lines indicate all transitions, while black lines indicate a given trajectory.",
                "Objective Function (sum of expected rewards over all agents) max X m X s X a rm(s, a) X t xm(s, a, t) (5) Meaning Implication Linear Constraints Tie x to θ.",
                "Agent is only active when occupation measure is nonzero in original MDP states. θm(τ) = 0 =⇒ xm(s, a, τ −τa m+1) = 0 ∀s /∈ {sb , sf }, a ∈ A X s/∈{sb,sf } X a xm(s, a, t) ≤ θm(τa m + t − 1) ∀m ∈ M, ∀t ∈ [1, Tm] (6) Agent can only be active in τ ∈ (τa m, τd m) θm(τ) = 0 ∀m ∈ M, τ /∈ (τa m, τd m) (7) Cannot use resources when not active θm(τ) = 0 =⇒ Δm(τ, ω) = 0 ∀τ ∈ [0, bτ], ω ∈ Ω Δm(τ, ω) ≤ θm(τ) ∀m ∈ M, τ ∈ [0, bτ], ω ∈ Ω (8) Tie x to Δ (nonzero x forces corresponding Δ to be nonzero.)",
                "Δm(τ, ω) = 0, ϕm(a, ω) = 1 =⇒ xm(s, a, τ − τa m + 1) = 0 ∀s /∈ {sb , sf } 1/|A| X a ϕm(a, ω) X s/∈{sb,sf } xm(s, a, t) ≤ Δm(t + τa m − 1, ω) ∀m ∈ M, ω ∈ Ω, t ∈ [1, Tm] (9) Resource bounds X m Δm(τ, ω) ≤ bϕ(ω) ∀ω ∈ Ω, τ ∈ [0, bτ] (10) Agent cannot change resources while active.",
                "Only enabled for scheduling with static assignments. θm(τ) = 1 and θm(τ + 1) = 1 =⇒ Δm(τ, ω) = Δm(τ + 1, ω) Δm(τ, ω) − Z(1 − θm(τ + 1)) ≤ Δm(τ + 1, ω) + Z(1 − θm(τ)) Δm(τ, ω) + Z(1 − θm(τ + 1)) ≥ Δm(τ + 1, ω) − Z(1 − θm(τ)) ∀m ∈ M, ω ∈ Ω, τ ∈ [0, bτ] (11) Table 1: MILP for globally optimal resource scheduling.",
                "Tm = τd m − τa m + 1 is the time horizon for the agents MDP.",
                "Using this LP as a basis, we augment it with constraints that ensure that the resource usage implied by the agents occupation measures {xm} does not violate the global resource requirements bϕ at any time step τ ∈ [0, bτ].",
                "To formulate these resource constraints, we use the following binary variables: • Δm(τ, ω) = {0, 1}, ∀m ∈ M, τ ∈ [0, bτ], ω ∈ Ω, which serve as indicator variables that define whether agent m possesses resource ω at time τ.",
                "These are analogous to the static indicator variables used in the one-shot static resource-allocation problem in [6]. • θm = {0, 1}, ∀m ∈ M, τ ∈ [0, bτ] are indicator variables that specify whether agent m is active (i.e., executing its MDP) at time τ.",
                "The meaning of resource-usage variables Δ is illustrated in Figure 1: Δm(τ, ω) = 1 only if resource ω is allocated to agent m at time τ.",
                "The meaning of the activity indicators θ is illustrated in Figure 2b: when agent m is in either the start state sb or the finish state sf , the corresponding θm = 0, but once the agent becomes active and enters one of the other states, we set θm = 1 .",
                "This meaning of θ can be enforced with a linear constraint that synchronizes the values of the agents occupation measures xm and the activity 1224 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) indicators θ, as shown in (6) in Table 1.",
                "Another constraint we have to add-because the activity indicators θ are defined on the global timeline τ-is to enforce the fact that the agent is inactive outside of its arrivaldeparture window.",
                "This is accomplished by constraint (7) in Table 1.",
                "Furthermore, agents should not be using resources while they are inactive.",
                "This constraint can also be enforced via a linear inequality on θ and Δ, as shown in (8).",
                "Constraint (6) sets the value of θ to match the policy defined by the occupation measure xm.",
                "In a similar fashion, we have to make sure that the resource-usage variables Δ are also synchronized with the occupation measure xm.",
                "This is done via constraint (9) in Table 1, which is nearly identical to the analogous constraint from [6].",
                "After implementing the above constraint, which enforces the meaning of Δ, we add a constraint that ensures that the agents resource usage never exceeds the amounts of available resources.",
                "This condition is also trivially expressed as a linear inequality (10) in Table 1.",
                "Finally, for the problem formulation where resource assignments are static during a lifetime of an agent, we add a constraint that ensures that the resource-usage variables Δ do not change their value while the agent is active (θ = 1).",
                "This is accomplished via the linear constraint (11), where Z ≥ 2 is a constant that is used to turn off the constraints when θm(τ) = 0 or θm(τ + 1) = 0.",
                "This constraint is not used for the dynamic problem formulation, where resources can be reallocated between agents at every time step.",
                "To summarize, Table 1 together with the conservationof-flow constraints from (12) defines the MILP that simultaneously computes an optimal resource assignment for all agents across time as well as optimal finite-horizon MDP policies that are valid under that resource assignment.",
                "As a rough measure of the complexity of this MILP, let us consider the number of optimization variables and constraints.",
                "Let TM = P Tm = P m(τa m − τd m + 1) be the sum of the lengths of the arrival-departure windows across all agents.",
                "Then, the number of optimization variables is: TM + bτ|M||Ω| + bτ|M|, TM of which are continuous (xm), and bτ|M||Ω| + bτ|M| are binary (Δ and θ).",
                "However, notice that all but TM|M| of the θ are set to zero by constraint (7), which also immediately forces all but TM|M||Ω| of the Δ to be zero via the constraints (8).",
                "The number of constraints (not including the degenerate constraints in (7)) in the MILP is: TM + TM|Ω| + bτ|Ω| + bτ|M||Ω|.",
                "Despite the fact that the complexity of the MILP is, in the worst case, exponential1 in the number of binary variables, the complexity of this MILP is significantly (exponentially) lower than that of the MILP with flat utility functions, described in Section 2.2.",
                "This result echos the efficiency gains reported in [6] for single-shot resource-allocation problems, but is much more pronounced, because of the explosion of the flat utility representation due to the temporal aspect of the problem (recall the prohibitive complexity of the combinatorial optimization in Section 2.2).",
                "We empirically analyze the performance of this method in Section 5. 1 Strictly speaking, solving MILPs to optimality is NPcomplete in the number of integer variables. 5.",
                "EXPERIMENTAL RESULTS Although the complexity of solving MILPs is in the worst case exponential in the number of integer variables, there are many efficient methods for solving MILPs that allow our algorithm to scale well for parameters common to resource allocation and scheduling problems.",
                "In particular, this section introduces a problem domain-the repairshop problem-used to empirically evaluate our algorithms scalability in terms of the number of agents |M|, the number of shared resources |Ω|, and the varied lengths of global time bτ during which agents may enter and exit the system.",
                "The repairshop problem is a simple parameterized MDP adopting the metaphor of a vehicular repair shop.",
                "Agents in the repair shop are mechanics with a number of independent tasks that yield reward only when completed.",
                "In our MDP model of this system, actions taken to advance through the state space are only allowed if the agent holds certain resources that are publicly available to the shop.",
                "These resources are in finite supply, and optimal policies for the shop will determine when each agent may hold the limited resources to take actions and earn individual rewards.",
                "Each task to be completed is associated with a single action, although the agent is required to repeat the action numerous times before completing the task and earning a reward.",
                "This model was parameterized in terms of the number of agents in the system, the number of different types of resources that could be linked to necessary actions, a global time during which agents are allowed to arrive and depart, and a maximum length for the number of time steps an agent may remain in the system.",
                "All datapoints in our experiments were obtained with 20 evaluations using CPLEX to solve the MILPs on a Pentium4 computer with 2Gb of RAM.",
                "Trials were conducted on both the static and the dynamic version of the resourcescheduling problem, as defined earlier.",
                "Figure 3 shows the runtime and policy value for independent modifications to the parameter set.",
                "The top row shows how the solution time for the MILP scales as we increase the number of agents |M|, the global time horizon bτ, and the number of resources |Ω|.",
                "Increasing the number of agents leads to exponential complexity scaling, which is to be expected for an NP-complete problem.",
                "However, increasing the global time limit bτ or the total number of resource types |Ω|-while holding the number of agents constantdoes not lead to decreased performance.",
                "This occurs because the problems get easier as they become under-constrained, which is also a common phenomenon for NP-complete problems.",
                "We also observe that the solution to the dynamic version of the problem can often be computed much faster than the static version.",
                "The bottom row of Figure 3 shows the joint policy value of the policies that correspond to the computed optimal resource-allocation schedules.",
                "We can observe that the dynamic version yields higher reward (as expected, since the reward for the dynamic version is always no less than the reward of the static version).",
                "We should point out that these graphs should not be viewed as a measure of performance of two different algorithms (both algorithms produce optimal solutions but to different problems), but rather as observations about how the quality of optimal solutions change as more flexibility is allowed in the reallocation of resources.",
                "Figure 4 shows runtime and policy value for trials in which common input variables are scaled together.",
                "This allows The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1225 2 4 6 8 10 10 −3 10 −2 10 −1 10 0 10 1 10 2 10 3 10 4 Number of Agents |M| CPUTime,sec |Ω| = 5, τ = 50 static dynamic 50 100 150 200 10 −2 10 −1 10 0 10 1 10 2 10 3 Global Time Boundary τ CPUTime,sec |M| = 5, |Ω| = 5 static dynamic 10 20 30 40 50 10 −2 10 −1 10 0 10 1 10 2 Number of Resources |Ω| CPUTime,sec |M| = 5, τ = 50 static dynamic 2 4 6 8 10 200 400 600 800 1000 1200 1400 1600 Number of Agents |M| Value |Ω| = 5, τ = 50 static dynamic 50 100 150 200 400 500 600 700 800 900 1000 1100 1200 1300 1400 Global Time Boundary τ Value |M| = 5, |Ω| = 5 static dynamic 10 20 30 40 50 500 600 700 800 900 1000 1100 1200 1300 1400 Number of Resources |Ω| Value |M| = 5, τ = 50 static dynamic Figure 3: Evaluation of our MILP for variable numbers of agents (column 1), lengths of global-time window (column 2), and numbers of resource types (column 3).",
                "Top row shows CPU time, and bottom row shows the joint reward of agents MDP policies.",
                "Error bars show the 1st and 3rd quartiles (25% and 75%). 2 4 6 8 10 10 −3 10 −2 10 −1 10 0 10 1 10 2 10 3 Number of Agents |M| CPUTime,sec τ = 10|M| static dynamic 2 4 6 8 10 10 −3 10 −2 10 −1 10 0 10 1 10 2 10 3 10 4 Number of Agents |M| CPUTime,sec |Ω| = 2|M| static dynamic 2 4 6 8 10 10 −3 10 −2 10 −1 10 0 10 1 10 2 10 3 10 4 Number of Agents |M| CPUTime,sec |Ω| = 5|M| static dynamic 2 4 6 8 10 200 400 600 800 1000 1200 1400 1600 1800 2000 2200 Number of Agents |M| Value τ = 10|M| static dynamic 2 4 6 8 10 200 400 600 800 1000 1200 1400 1600 1800 2000 Number of Agents |M| Value |Ω| = 2|M| static dynamic 2 4 6 8 10 0 500 1000 1500 2000 2500 Number of Agents |M| Value |Ω| = 5|M| static dynamic Figure 4: Evaluation of our MILP using correlated input variables.",
                "The left column tracks the performance and CPU time as the number of agents and global-time window increase together (bτ = 10|M|).",
                "The middle and the right column track the performance and CPU time as the number of resources and the number of agents increase together as |Ω| = 2|M| and |Ω| = 5|M|, respectively.",
                "Error bars show the 1st and 3rd quartiles (25% and 75%). 1226 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) us to explore domains where the total number of agents scales proportionally to the total number of resource types or the global time horizon, while keeping constant the average agent density (per unit of global time) or the average number of resources per agent (which commonly occurs in real-life applications).",
                "Overall, we believe that these experimental results indicate that our MILP formulation can be used to effectively solve resource-scheduling problems of nontrivial size. 6.",
                "DISCUSSION AND CONCLUSIONS Throughout the paper, we have made a number of assumptions in our model and solution algorithm; we discuss their implications below. • Continual execution.",
                "We assume that once an agent stops executing its MDP (transitions into state sf ), it exits the system and cannot return.",
                "It is easy to relax this assumption for domains where agents MDPs can be paused and restarted.",
                "All that is required is to include an additional pause action which transitions from a given state back to itself, and has zero reward. • Indifference to start time.",
                "We used a reward model where agents rewards depend only on the time horizon of their MDPs and not the global start time.",
                "This is a consequence of our MDP-augmentation procedure from Section 4.1.",
                "It is easy to extend the model so that the agents incur an explicit penalty for idling by assigning a non-zero negative reward to the start state sb . • Binary resource requirements.",
                "For simplicity, we have assumed that resource costs are binary: ϕm(a, ω) = {0, 1}, but our results generalize in a straightforward manner to non-binary resource mappings, analogously to the procedure used in [5]. • Cooperative agents.",
                "The optimization procedure discussed in this paper was developed in the context of cooperative agents, but it can also be used to design a mechanism for scheduling resources among self-interested agents.",
                "This optimization procedure can be embedded in a VickreyClarke-Groves auction, completely analogously to the way it was done in [7].",
                "In fact, all the results of [7] about the properties of the auction and information privacy directly carry over to the scheduling domain discussed in this paper, requiring only slight modifications to deal with finitehorizon MDPs. • Known, deterministic arrival and departure times.",
                "Finally, we have assumed that agents arrival and departure times (τa m and τd m) are deterministic and known a priori.",
                "This assumption is fundamental to our solution method.",
                "While there are many domains where this assumption is valid, in many cases agents arrive and depart dynamically and their arrival and departure times can only be predicted probabilistically, leading to online resource-allocation problems.",
                "In particular, in the case of self-interested agents, this becomes an interesting version of an online-mechanism-design problem [11, 12].",
                "In summary, we have presented an MILP formulation for the combinatorial resource-scheduling problem where agents values for possible resource assignments are defined by finitehorizon MDPs.",
                "This result extends previous work ([6, 7]) on static one-shot resource allocation under MDP-induced preferences to resource-scheduling problems with a temporal aspect.",
                "As such, this work takes a step in the direction of designing an online mechanism for agents with combinatorial resource preferences induced by stochastic planning problems.",
                "Relaxing the assumption about deterministic arrival and departure times of the agents is a focus of our future work.",
                "We would like to thank the anonymous reviewers for their insightful comments and suggestions. 7.",
                "REFERENCES [1] E. Altman and A. Shwartz.",
                "Adaptive control of constrained Markov chains: Criteria and policies.",
                "Annals of Operations Research, special issue on Markov Decision Processes, 28:101-134, 1991. [2] R. Bellman.",
                "Dynamic Programming.",
                "Princeton University Press, 1957. [3] C. Boutilier.",
                "Solving concisely expressed combinatorial auction problems.",
                "In Proc. of AAAI-02, pages 359-366, 2002. [4] C. Boutilier and H. H. Hoos.",
                "Bidding languages for combinatorial auctions.",
                "In Proc. of IJCAI-01, pages 1211-1217, 2001. [5] D. Dolgov.",
                "Integrated Resource Allocation and Planning in Stochastic Multiagent Environments.",
                "PhD thesis, Computer Science Department, University of Michigan, February 2006. [6] D. A. Dolgov and E. H. Durfee.",
                "Optimal resource allocation and policy formulation in loosely-coupled Markov decision processes.",
                "In Proc. of ICAPS-04, pages 315-324, June 2004. [7] D. A. Dolgov and E. H. Durfee.",
                "Computationally efficient combinatorial auctions for resource allocation in weakly-coupled MDPs.",
                "In Proc. of AAMAS-05, New York, NY, USA, 2005.",
                "ACM Press. [8] D. A. Dolgov and E. H. Durfee.",
                "Resource allocation among agents with preferences induced by factored MDPs.",
                "In Proc. of AAMAS-06, 2006. [9] K. Larson and T. Sandholm.",
                "Mechanism design and deliberative agents.",
                "In Proc. of AAMAS-05, pages 650-656, New York, NY, USA, 2005.",
                "ACM Press. [10] N. Nisan.",
                "Bidding and allocation in combinatorial auctions.",
                "In Electronic Commerce, 2000. [11] D. C. Parkes and S. Singh.",
                "An MDP-based approach to Online Mechanism Design.",
                "In Proc. of the Seventeenths Annual Conference on Neural Information Processing Systems (NIPS-03), 2003. [12] D. C. Parkes, S. Singh, and D. Yanovsky.",
                "Approximately efficient online mechanism design.",
                "In Proc. of the Eighteenths Annual Conference on Neural Information Processing Systems (NIPS-04), 2004. [13] M. L. Puterman.",
                "Markov Decision Processes.",
                "John Wiley & Sons, New York, 1994. [14] M. H. Rothkopf, A. Pekec, and R. M. Harstad.",
                "Computationally manageable combinational auctions.",
                "Management Science, 44(8):1131-1147, 1998. [15] T. Sandholm.",
                "An algorithm for optimal winner determination in combinatorial auctions.",
                "In Proc. of IJCAI-99, pages 542-547, San Francisco, CA, USA, 1999.",
                "Morgan Kaufmann Publishers Inc.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1227"
            ],
            "original_annotated_samples": [
                "In this paper, we extend the work on resource allocation under MDP-induced preferences to <br>discrete-time scheduling problem</br>s, where agents are present in the system for finite time intervals and can only use resources within these intervals."
            ],
            "translated_annotated_samples": [
                "En este artículo, ampliamos el trabajo sobre asignación de recursos bajo preferencias inducidas por MDP a <br>problemas de programación en tiempo discreto</br>, donde los agentes están presentes en el sistema durante intervalos de tiempo finitos y solo pueden utilizar recursos dentro de estos intervalos."
            ],
            "translated_text": "La programación de recursos combinatoria para MDPs multiagentes. Dmitri A. Dolgov, Michael R. James y Michael E. Samples. Grupo de Investigación en IA y Robótica, Centro Técnico de Toyota en EE. UU. {ddolgov, michael.r.james, michael.samples}@gmail.com. RESUMEN La programación óptima de recursos en sistemas multiagentes es una tarea computacionalmente desafiante, especialmente cuando los valores de los recursos no son aditivos. Consideramos el problema combinatorio de programar el uso de múltiples recursos entre agentes que operan en entornos estocásticos, modelados como procesos de decisión de Markov (MDPs). En los últimos años, se han desarrollado algoritmos eficientes de asignación de recursos para agentes con valores de recursos inducidos por MDPs. Sin embargo, este trabajo previo se ha centrado en problemas de asignación de recursos estáticos donde los recursos se distribuyen una vez y luego se utilizan en MDPs de horizonte infinito. Extendemos esos modelos existentes al problema de la programación de recursos combinatoria, donde los agentes persisten solo por períodos finitos entre sus tiempos de llegada y salida (predefinidos), requiriendo recursos solo durante esos períodos de tiempo. Proporcionamos un procedimiento computacionalmente eficiente para calcular asignaciones de recursos óptimas a nivel global a agentes a lo largo del tiempo. Ilustramos y analizamos empíricamente el método en el contexto de un dominio de programación de trabajos estocásticos. Categorías y Descriptores de Asignaturas I.2.8 [Inteligencia Artificial]: Resolución de Problemas, Métodos de Control y Búsqueda; I.2.11 [Inteligencia Artificial]: Inteligencia Artificial Distribuida-Sistemas Multiagente Términos Generales Algoritmos, Rendimiento, Diseño 1. INTRODUCCIÓN Las tareas de asignación óptima de recursos y programación son ubicuas en sistemas multiagentes, pero resolver tales problemas de optimización puede ser computacionalmente difícil, debido a una serie de factores. En particular, cuando el valor de un conjunto de recursos para un agente no es aditivo (como suele ser el caso con recursos que son sustitutos o complementos), la función de utilidad podría tener que definirse en un espacio exponencialmente grande de combinaciones de recursos, lo que rápidamente se vuelve computacionalmente intratable. Además, incluso cuando cada agente tiene una función de utilidad que es distinta de cero solo en un pequeño subconjunto de los posibles conjuntos de recursos, obtener una asignación óptima sigue siendo computacionalmente prohibitivo, ya que el problema se vuelve NP-completo [14]. Tales problemas computacionales han dado lugar recientemente a varias líneas de trabajo en el uso de modelos compactos de las preferencias de los agentes. Una idea es utilizar cualquier estructura presente en las funciones de utilidad para representarlas de forma compacta, por ejemplo, a través de fórmulas lógicas [15, 10, 4, 3]. Una alternativa es modelar directamente los mecanismos que definen las funciones de utilidad de los agentes y realizar la asignación de recursos directamente con estos modelos [9]. Una forma de lograr esto es modelar los procesos mediante los cuales un agente podría utilizar los recursos y definir la función de utilidad como la ganancia de estos procesos. En particular, si un agente utiliza recursos para actuar en un entorno estocástico, su función de utilidad puede ser modelada de forma natural con un proceso de decisión de Markov, cuyo conjunto de acciones está parametrizado por los recursos disponibles. Esta representación puede ser utilizada para construir algoritmos de asignación de recursos muy eficientes que resultan en una aceleración exponencial sobre un problema de optimización directa con representaciones planas de preferencias combinatorias [6, 7, 8]. Sin embargo, este trabajo existente sobre asignación de recursos con preferencias inducidas por MDPs parametrizados por recursos hace una suposición de que los recursos se asignan solo una vez y luego son utilizados por los agentes de forma independiente dentro de sus MDPs de horizonte infinito. Esta suposición de que no es posible la reasignación de recursos puede ser limitante en dominios donde los agentes llegan y se van dinámicamente. En este artículo, ampliamos el trabajo sobre asignación de recursos bajo preferencias inducidas por MDP a <br>problemas de programación en tiempo discreto</br>, donde los agentes están presentes en el sistema durante intervalos de tiempo finitos y solo pueden utilizar recursos dentro de estos intervalos. En particular, los agentes llegan y parten en momentos arbitrarios (predefinidos) y dentro de estos intervalos utilizan recursos para ejecutar tareas en MDPs de horizonte finito. Abordamos el problema de la programación de recursos óptima a nivel global, donde el objetivo es encontrar una asignación de recursos a los agentes a lo largo del tiempo que maximice la suma de las recompensas esperadas que obtienen. En este contexto, nuestra principal contribución es una formulación de programación entera mixta del problema de programación que elige asignaciones de recursos globalmente óptimas, tiempos de inicio y horizontes de ejecución para todos los agentes (dentro de sus intervalos de llegada y salida). Analizamos y comparamos empíricamente dos variantes del problema de programación: uno, donde los agentes tienen asignaciones de recursos estáticas dentro de sus MDPs de horizonte finito, y otro, donde los recursos pueden ser realocados dinámicamente entre agentes en cada paso de tiempo. En el resto del documento, primero establecemos las bases necesarias en la Sección 2 y luego presentamos nuestro modelo y declaración formal del problema en la Sección 3. En la Sección 4.2, describimos nuestro resultado principal, el programa de optimización para la programación de recursos globalmente óptima. Tras la discusión de nuestros resultados experimentales sobre un problema de programación de trabajos en la Sección 5, concluimos en la Sección 6 con una discusión de posibles extensiones y generalizaciones de nuestro método. De manera similar al modelo utilizado en trabajos anteriores sobre asignación de recursos con preferencias inducidas por MDP [6, 7], definimos el valor de un conjunto de recursos para un agente como el valor de la mejor política MDP que es realizable, dadas esos recursos. Sin embargo, dado que el enfoque de nuestro trabajo se centra en problemas de programación, y una gran parte del problema de optimización consiste en decidir cómo se asignan los recursos en el tiempo entre agentes con tiempos finitos de llegada y salida, modelamos los problemas de planificación de los agentes como MDPs de horizonte finito, en contraste con trabajos anteriores que utilizaron MDPs descontados de horizonte infinito. En el resto de esta sección, primero presentamos algo de antecedentes necesarios sobre MDPs de horizonte finito y presentamos una formulación de programación lineal que sirve como base para nuestro algoritmo de solución desarrollado en la Sección 4. También describimos los métodos estándar para la programación de recursos combinatorios con valores de recursos planos, que sirven como punto de referencia de comparación para el nuevo modelo desarrollado aquí. 2.1 Procesos de Decisión de Markov Un MDP (Proceso de Decisión de Markov) estacionario, de dominio finito y discreto en el tiempo (ver, por ejemplo, [13] para un desarrollo detallado y exhaustivo) se puede describir como S, A, p, r, donde: S es un conjunto finito de estados del sistema; A es un conjunto finito de acciones disponibles para el agente; p es una función de transición estocástica estacionaria, donde p(σ|s, a) es la probabilidad de transicionar al estado σ al ejecutar la acción a en el estado s; r es una función de recompensa estacionaria, donde r(s, a) especifica la recompensa obtenida al ejecutar la acción a en el estado s. Dado un MDP de este tipo, un problema de decisión bajo un horizonte finito T consiste en elegir una acción óptima en cada paso de tiempo para maximizar el valor esperado de la recompensa total acumulada durante la vida (finita) de los agentes. La política óptima del agente es entonces una función del estado actual s y el tiempo hasta el horizonte. Una política óptima para tal problema es actuar de forma codiciosa con respecto a la función de valor óptimo, definida de forma recursiva por el siguiente sistema de ecuaciones de Bellman de tiempo finito [2]: v(s, t) = max a r(s, a) + X σ p(σ|s, a)v(σ, t + 1), ∀s ∈ S, t ∈ [1, T − 1]; v(s, T) = 0, ∀s ∈ S; donde v(s, t) es el valor óptimo de estar en el estado s en el tiempo t ∈ [1, T]. Esta función de valor óptimo se puede calcular fácilmente utilizando programación dinámica, lo que conduce a la siguiente política óptima π, donde π(s, a, t) es la probabilidad de ejecutar la acción a en el estado s en el tiempo t: π(s, a, t) = (1, a = argmaxa r(s, a) + P σ p(σ|s, a)v(σ, t + 1), 0, en caso contrario. La anterior es la forma más común de calcular la función de valor óptimo (y por lo tanto una política óptima) para un MDP de horizonte finito. Sin embargo, también podemos formular el problema como el siguiente programa lineal (similarmente al LP dual para MDPs descontados de horizonte infinito [13, 6, 7]): max X s X a r(s, a) X t x(s, a, t) sujeto a: X a x(σ, a, t + 1) = X s,a p(σ|s, a)x(s, a, t) ∀σ, t ∈ [1, T − 1]; X a x(s, a, 1) = α(s), ∀s ∈ S; (1) donde α(s) es la distribución inicial sobre el espacio de estados, y x es la medida de ocupación (no estacionaria) (x(s, a, t) ∈ [0, 1] es el número total esperado de veces que se ejecuta la acción a en el estado s en el tiempo t). Una política óptima (no estacionaria) se obtiene a partir de la medida de ocupación de la siguiente manera: π(s, a, t) = x(s, a, t)/ X a x(s, a, t) ∀s ∈ S, t ∈ [1, T]. (2) Tenga en cuenta que el MDP estándar de horizonte finito sin restricciones, como se describe anteriormente, siempre tiene una solución uniformemente óptima (óptima para cualquier distribución inicial α(s)). Por lo tanto, una política óptima se puede obtener utilizando una constante arbitraria α(s) > 0 (en particular, α(s) = 1 resultará en x(s, a, t) = π(s, a, t)). Sin embargo, para los MDPs con restricciones de recursos (como se definen a continuación en la Sección 3), las políticas óptimas uniformemente no existen en general. En tales casos, α se convierte en parte de la entrada del problema, y una política resultante solo es óptima para ese α en particular. Este resultado es bien conocido para MDPs de horizonte infinito con varios tipos de restricciones [1, 6], y también se cumple para nuestro modelo de horizonte finito, el cual puede establecerse fácilmente a través de una línea de razonamiento completamente análoga a los argumentos en [6]. 2.2 Programación de Recursos Combinatoria Un enfoque directo para la programación de recursos para un conjunto de agentes M, cuyos valores para los recursos son inducidos por problemas de planificación estocástica (en nuestro caso, MDPs de horizonte finito) sería que cada agente enumerara todas las asignaciones de recursos posibles a lo largo del tiempo y, para cada una, calcular su valor resolviendo el MDP correspondiente. Entonces, cada agente proporcionaría valoraciones para cada posible conjunto de recursos a lo largo del tiempo a un coordinador centralizado, quien calcularía las asignaciones óptimas de recursos a lo largo del tiempo basándose en estas valoraciones. Cuando los recursos pueden asignarse en diferentes momentos a diferentes agentes, cada agente debe presentar valoraciones para cada combinación de posibles horizontes temporales. Que cada agente m ∈ M ejecute su MDP dentro del intervalo de tiempo de llegada-salida τ ∈ [τa m, τd m]. Por lo tanto, el agente m ejecutará un MDP con un horizonte temporal no mayor que Tm = τd m−τa m+1. Que bτ sea el horizonte temporal global para el problema, antes del cual todos los MDP de los agentes deben finalizar. Suponemos que τd m < bτ, ∀m ∈ M. El Sexto Congreso Internacional. En el problema de programación donde los agentes tienen requisitos de recursos estáticos dentro de sus MDPs de horizonte finito, los agentes proporcionan una valoración para cada conjunto de recursos para cada horizonte de tiempo posible (desde [1, Tm]) que puedan utilizar. Sea Ω el conjunto de recursos a ser asignados entre los agentes. Un agente recibirá como máximo un paquete de recursos para uno de los horizontes temporales. Que la variable ψ ∈ Ψm enumere todos los posibles pares de conjuntos de recursos y horizontes temporales para el agente m, por lo que hay 2|Ω| × Tm valores para ψ (el espacio de conjuntos es exponencial en el número de tipos de recursos |Ω|). El agente m debe proporcionar un valor vψ m para cada ψ, y el coordinador asignará como máximo un par (recurso, horizonte temporal) ψ a cada agente. Esta asignación se expresa como una variable indicadora zψ m ∈ {0, 1} que muestra si ψ está asignado al agente m. Para el tiempo τ y el recurso ω, la función nm(ψ, τ, ω) ∈ {0, 1} indica si el paquete en ψ utiliza el recurso ω en el tiempo τ (hacemos la suposición de que los agentes tienen requisitos de recursos binarios). Este problema de asignación es NP-completo, incluso al considerar solo un paso de tiempo, y su dificultad aumenta significativamente con múltiples pasos de tiempo debido al creciente número de valores de ψ. El problema de encontrar una asignación óptima que satisfaga la restricción global de que la cantidad de cada recurso ω asignada a todos los agentes no exceda la cantidad disponible bϕ(ω) se puede expresar como el siguiente programa entero: max X m∈M X ψ∈Ψm zψ mvψ m sujeto a: X ψ∈Ψm zψ m ≤ 1, ∀m ∈ M; X m∈M X ψ∈Ψm zψ mnm(ψ, τ, ω) ≤ bϕ(ω), ∀τ ∈ [1, bτ], ∀ω ∈ Ω; (3) La primera restricción en la ecuación 3 dice que ningún agente puede recibir más de un paquete, y la segunda restricción asegura que la asignación total del recurso ω no exceda, en ningún momento, el límite del recurso. Para el problema de programación en el que los agentes pueden reasignar dinámicamente recursos, cada agente debe especificar un valor para cada combinación de paquetes y pasos de tiempo dentro de su horizonte temporal. Que la variable ψ ∈ Ψm en este caso enumere todos los posibles conjuntos de recursos para los cuales a lo sumo un conjunto puede ser asignado al agente m en cada paso de tiempo. Por lo tanto, en este caso hay P t∈[1,Tm](2|Ω| )t ∼ 2|Ω|Tm posibilidades de combinaciones de recursos asignadas a diferentes intervalos de tiempo, para los Tm horizontes de tiempo diferentes. El mismo conjunto de ecuaciones (3) se puede utilizar para resolver este problema de programación dinámica, pero el programa entero es diferente debido a la diferencia en cómo se define ψ. En este caso, el número de valores de ψ es exponencial en el horizonte de planificación Tm de cada agente, lo que resulta en un programa mucho más grande. Este enfoque directo para resolver ambos problemas de programación requiere una enumeración y solución de 2|Ω| Tm (asignación estática) o P t∈[1,Tm] 2|Ω|t (reubicación dinámica) MDPs para cada agente, lo cual rápidamente se vuelve intratable con el crecimiento del número de recursos |Ω| o el horizonte de tiempo Tm. MODELO Y DECLARACIÓN DEL PROBLEMA Ahora presentamos formalmente nuestro modelo del problema de programación de recursos. La entrada del problema consiste en los siguientes componentes: • M, Ω, bϕ, τa m, τd m, bτ están definidos arriba en la Sección 2.2. • {Θm} = {S, A, pm, rm, αm} son los MDPs de todos los agentes m ∈ M. Sin pérdida de generalidad, asumimos que los espacios de estados y acciones de todos los agentes son iguales, pero cada uno tiene su propia función de transición pm, función de recompensa rm, y condiciones iniciales αm. • ϕm : A×Ω → {0, 1} es la asignación de acciones a recursos para el agente m. ϕm(a, ω) indica si la acción a del agente m necesita el recurso ω. Un agente m que recibe un conjunto de recursos que no incluye el recurso ω no puede ejecutar en su política MDP ninguna acción a para la cual ϕm(a, ω) = 0. Suponemos que todos los requisitos de recursos son binarios; como se discute más adelante en la Sección 6, esta suposición no es limitante. Dado el input anterior, el problema de optimización que consideramos es encontrar la asignación globalmente óptima que maximice la suma de recompensas esperadas de recursos a agentes para todos los pasos de tiempo: Δ : τ × M × Ω → {0, 1}. Una solución es factible si la asignación correspondiente de recursos a los agentes no viola la restricción global de recursos: X m Δm(τ, ω) ≤ bϕ(ω), ∀ω ∈ Ω, τ ∈ [1, bτ]. Consideramos dos variantes del problema de programación de recursos. La primera formulación restringe las asignaciones de recursos al espacio donde la asignación de recursos a cada agente es estática durante la vida de los agentes. La segunda formulación permite la reasignación de recursos entre agentes en cada paso de tiempo durante sus vidas. La Figura 1 representa un problema de programación de recursos con tres agentes M = {m1, m2, m3}, tres recursos Ω = {ω1, ω2, ω3}, y un horizonte de problema global de bτ = 11. Los tiempos de llegada y salida de los agentes se muestran como cajas grises y son {1, 6}, {3, 7} y {2, 11}, respectivamente. Una solución a este problema se muestra a través de barras horizontales dentro de cada casilla de agentes, donde las barras corresponden a la asignación de los tres tipos de recursos. La Figura 1a muestra una solución a un problema de programación estática. Según la solución mostrada, el agente m1 comienza la ejecución de su MDP en el tiempo τ = 1 y tiene un bloqueo en los tres recursos hasta que finaliza la ejecución en el tiempo τ = 3. Se debe tener en cuenta que el agente m1 renuncia a su control sobre los recursos antes de su hora de salida anunciada de τd m1 = 6, aparentemente porque otros agentes pueden utilizar los recursos de manera más efectiva. Por lo tanto, en el tiempo τ = 4, los recursos ω1 y ω3 se asignan al agente m2, quien luego los utiliza para ejecutar su MDP (utilizando solo acciones admitidas por los recursos ω1 y ω3) hasta el tiempo τ = 7. El agente m3 posee el recurso ω3 durante el intervalo τ ∈ [4, 10]. La Figura 1b muestra una posible solución a la versión dinámica del mismo problema. Allí, los recursos pueden ser reasignados entre agentes en cada paso de tiempo. Por ejemplo, el agente m1 renuncia a su uso del recurso ω2 en el tiempo τ = 2, aunque continúa la ejecución de su MDP hasta el tiempo τ = 6. Ten en cuenta que a un agente no se le permite detener y reiniciar su MDP, por lo que el agente m1 solo puede continuar ejecutando en el intervalo τ ∈ [3, 4] si tiene acciones que no requieren recursos (ϕm(a, ω) = 0). Claramente, el modelo y la declaración del problema descritos anteriormente hacen una serie de suposiciones sobre el problema y las propiedades de la solución deseada. Discutimos algunas de esas suposiciones y sus implicaciones en la Sección 6. 1222 La Sexta Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) (a) (b) Figura 1: Ilustración de una solución a un problema de programación de recursos con tres agentes y tres recursos: a) asignaciones de recursos estáticas (las asignaciones de recursos son constantes durante la vida de los agentes); b) asignación dinámica (las asignaciones de recursos pueden cambiar en cada paso de tiempo). 4. PROGRAMACIÓN DE RECURSOS Nuestro algoritmo de programación de recursos se lleva a cabo en dos etapas. Primero, realizamos un paso de preprocesamiento que aumenta los MDP de los agentes; este proceso se describe en la Sección 4.1. Segundo, utilizando estos MDPs aumentados construimos un problema de optimización global, el cual se describe en la Sección 4.2. 4.1 Aumentando los MDPs de los Agentes En el modelo descrito en la sección anterior, asumimos que si un agente no posee los recursos necesarios para realizar acciones en su MDP, su ejecución se detiene y el agente abandona el sistema. En otras palabras, los MDPs no pueden ser pausados y reanudados. Por ejemplo, en el problema mostrado en la Figura 1a, el agente m1 libera todos los recursos después del tiempo τ = 3, momento en el cual la ejecución de su MDP se detiene. De manera similar, los agentes m2 y m3 solo ejecutan sus MDPs en los intervalos τ ∈ [4, 6] y τ ∈ [4, 10], respectivamente. Por lo tanto, una parte importante del problema de toma de decisiones global es decidir la ventana de tiempo durante la cual cada uno de los agentes está activo (es decir, ejecutando su MDP). Para lograr esto, ampliamos el MDP de cada agente con dos nuevos estados (estados de inicio y final sb, sf, respectivamente) y una nueva acción de inicio/parada a∗, como se ilustra en la Figura 2. La idea es que un agente permanezca en el estado inicial sb hasta que esté listo para ejecutar su MDP, momento en el que realiza la acción de inicio/parada a∗ y transita al espacio de estados del MDP original con la probabilidad de transición que corresponde a la distribución inicial original α(s). Por ejemplo, en la Figura 1a, para el agente m2 esto ocurriría en el tiempo τ = 4. Una vez que el agente llega al final de su ventana de actividad (tiempo τ = 6 para el agente m2 en la Figura 1a), realiza la acción de inicio/parada, lo que lo lleva al estado final de finalización del sumidero sf en el tiempo τ = 7. Más precisamente, dado un MDP S, A, pm, rm, αm, definimos un MDP aumentado S, A, pm, rm, αm de la siguiente manera: S = S ∪ sb ∪ sf; A = A ∪ a∗; p(s|sb, a∗) = α(s), ∀s ∈ S; p(sb|sb, a) = 1.0, ∀a ∈ A; p(sf|s, a∗) = 1.0, ∀s ∈ S; p(σ|s, a) = p(σ|s, a), ∀s, σ ∈ S, a ∈ A; r(sb, a) = r(sf, a) = 0, ∀a ∈ A; r(s, a) = r(s, a), ∀s ∈ S, a ∈ A; α(sb) = 1; α(s) = 0, ∀s ∈ S; donde se asume que todas las probabilidades de transición no especificadas son cero. Además, para tener en cuenta el nuevo estado inicial, comenzamos el MDP un paso de tiempo antes, estableciendo τa m ← τa m − 1. Esto no afectará la asignación de recursos debido a que las restricciones de recursos solo se aplican a los estados originales del MDP, como se discutirá en la siguiente sección. Por ejemplo, los MDPs aumentados mostrados en la Figura 2b (que comienza en el estado sb en el tiempo τ = 2) se construirían a partir de un MDP con tiempo de llegada original τ = 3. La Figura 2b también muestra una trayectoria de muestra a través del espacio de estados: el agente comienza en el estado sb, transita al espacio de estados S del MDP original y finalmente sale al estado de sumidero sf. Cabe destacar que si quisiéramos modelar un problema en el que los agentes pudieran pausar sus MDP en pasos de tiempo arbitrarios (lo cual podría ser útil para dominios donde la reasignación dinámica es posible), podríamos lograrlo fácilmente incluyendo una acción adicional que transiciona de cada estado a sí mismo con una recompensa de cero. 4.2 MILP para la programación de recursos Dado un conjunto de MDPs aumentados, como se define arriba, el objetivo de esta sección es formular un programa de optimización global que resuelva el problema de programación de recursos. En esta sección y a continuación, se asume que todos los MDPs son los MDPs aumentados tal como se definen en la Sección 4.1. Nuestro enfoque es similar a la idea utilizada en [6]: comenzamos con la formulación del programa lineal de los MDP de los agentes (1) y lo complementamos con restricciones que aseguran que la asignación de recursos correspondiente entre agentes y tiempo sea válida. El problema de optimización resultante resuelve simultáneamente los MDP de los agentes y los problemas de programación de recursos. En el resto de esta sección, desarrollamos de forma incremental un programa de enteros mixtos (MILP) que logra esto. En ausencia de restricciones de recursos, los MDPs de horizonte finito de los agentes son completamente independientes, y la solución óptima global puede obtenerse trivialmente a través del siguiente LP, que es simplemente una agregación de LPs de horizonte finito de un solo agente: max X m X s X a rm(s, a) X t xm(s, a, t) sujeto a: X a xm(σ, a, t + 1) = X s,a pm(σ|s, a)xm(s, a, t), ∀m ∈ M, σ ∈ S, t ∈ [1, Tm − 1]; X a xm(s, a, 1) = αm(s), ∀m ∈ M, s ∈ S; (12) donde xm(s, a, t) es la medida de ocupación del agente m, y The Sixth Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 1223 (a) (b) Figura 2: Ilustración de la ampliación de un MDP para permitir tiempos de inicio y finalización variables: a) (izquierda) el MDP original de dos estados con una sola acción; (derecha) el MDP ampliado con nuevos estados sb y sf y la nueva acción a∗ (nota que las transiciones originales no se modifican en el proceso de ampliación); b) el MDP ampl augmentado mostrado como una trayectoria a través del tiempo (las líneas grises indican todas las transiciones, mientras que las líneas negras indican una trayectoria dada). Función objetivo (suma de recompensas esperadas de todos los agentes) máx X m X s X a rm(s, a) X t xm(s, a, t) (5) Implicación de significado Restricciones lineales Atar x a θ. El agente solo está activo cuando la medida de ocupación es distinta de cero en los estados originales del MDP. θm(τ) = 0 =⇒ xm(s, a, τ −τa m+1) = 0 ∀s /∈ {sb , sf }, a ∈ A X s/∈{sb,sf } X a xm(s, a, t) ≤ θm(τa m + t − 1) ∀m ∈ M, ∀t ∈ [1, Tm] (6) El agente solo puede estar activo en τ ∈ (τa m, τd m) θm(τ) = 0 ∀m ∈ M, τ /∈ (τa m, τd m) (7) No se pueden utilizar recursos cuando no está activo θm(τ) = 0 =⇒ Δm(τ, ω) = 0 ∀τ ∈ [0, bτ], ω ∈ Ω Δm(τ, ω) ≤ θm(τ) ∀m ∈ M, τ ∈ [0, bτ], ω ∈ Ω (8) Vincular x a Δ (x distinto de cero obliga a que el correspondiente Δ sea distinto de cero). Δm(τ, ω) = 0, ϕm(a, ω) = 1 =⇒ xm(s, a, τ − τa m + 1) = 0 ∀s /∈ {sb , sf } 1/|A| X a ϕm(a, ω) X s/∈{sb,sf } xm(s, a, t) ≤ Δm(t + τa m − 1, ω) ∀m ∈ M, ω ∈ Ω, t ∈ [1, Tm] (9) Límites de recursos X m Δm(τ, ω) ≤ bϕ(ω) ∀ω ∈ Ω, τ ∈ [0, bτ] (10) El agente no puede cambiar los recursos mientras está activo. Solo habilitado para programación con asignaciones estáticas. θm(τ) = 1 y θm(τ + 1) = 1 =⇒ Δm(τ, ω) = Δm(τ + 1, ω) Δm(τ, ω) − Z(1 − θm(τ + 1)) ≤ Δm(τ + 1, ω) + Z(1 − θm(τ)) Δm(τ, ω) + Z(1 − θm(τ + 1)) ≥ Δm(τ + 1, ω) − Z(1 − θm(τ)) ∀m ∈ M, ω ∈ Ω, τ ∈ [0, bτ] (11) Tabla 1: MILP para programación óptima global de recursos. Tm = τd m − τa m + 1 es el horizonte temporal para los agentes MDP. Usando este LP como base, lo ampliamos con restricciones que aseguran que el uso de recursos implicado por las medidas de ocupación de los agentes {xm} no viola los requisitos globales de recursos bϕ en ningún paso de tiempo τ ∈ [0, bτ]. Para formular estas restricciones de recursos, utilizamos las siguientes variables binarias: • Δm(τ, ω) = {0, 1}, ∀m ∈ M, τ ∈ [0, bτ], ω ∈ Ω, que sirven como variables indicadoras que definen si el agente m posee el recurso ω en el tiempo τ. Estos son análogos a las variables indicadoras estáticas utilizadas en el problema de asignación de recursos estáticos de una sola vez en [6]. • θm = {0, 1}, ∀m ∈ M, τ ∈ [0, bτ] son variables indicadoras que especifican si el agente m está activo (es decir, ejecutando su MDP) en el tiempo τ. El significado de las variables de uso de recursos Δ se ilustra en la Figura 1: Δm(τ, ω) = 1 solo si el recurso ω se asigna al agente m en el tiempo τ. El significado de los indicadores de actividad θ se ilustra en la Figura 2b: cuando el agente m se encuentra en el estado inicial sb o en el estado final sf, el θm correspondiente es igual a 0, pero una vez que el agente se vuelve activo y entra en uno de los otros estados, establecemos θm = 1. Este significado de θ puede ser impuesto con una restricción lineal que sincroniza los valores de las medidas de ocupación de los agentes xm y la actividad 1224 The Sixth Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) indicadores θ, como se muestra en (6) en la Tabla 1. Otra restricción que debemos agregar, debido a que los indicadores de actividad θ están definidos en la línea de tiempo global τ, es hacer cumplir el hecho de que el agente está inactivo fuera de su ventana de llegada y salida. Esto se logra mediante la restricción (7) en la Tabla 1. Además, los agentes no deben estar utilizando recursos mientras están inactivos. Esta restricción también puede ser impuesta a través de una desigualdad lineal en θ y Δ, como se muestra en (8). La restricción (6) establece el valor de θ para que coincida con la política definida por la medida de ocupación xm. De manera similar, debemos asegurarnos de que las variables de uso de recursos Δ estén también sincronizadas con la medida de ocupación xm. Esto se realiza a través de la restricción (9) en la Tabla 1, que es casi idéntica a la restricción análoga de [6]. Después de implementar la restricción anterior, que refuerza el significado de Δ, agregamos una restricción que garantiza que el uso de recursos de los agentes nunca exceda los montos de recursos disponibles. Esta condición también se expresa trivialmente como una desigualdad lineal (10) en la Tabla 1. Finalmente, para la formulación del problema en la que las asignaciones de recursos son estáticas durante la vida de un agente, agregamos una restricción que garantiza que las variables de uso de recursos Δ no cambien su valor mientras el agente está activo (θ = 1). Esto se logra a través de la restricción lineal (11), donde Z ≥ 2 es una constante que se utiliza para desactivar las restricciones cuando θm(τ) = 0 o θm(τ + 1) = 0. Esta restricción no se utiliza para la formulación del problema dinámico, donde los recursos pueden ser reasignados entre agentes en cada paso de tiempo. En resumen, la Tabla 1 junto con las restricciones de conservación de flujo de (12) define el MILP que calcula simultáneamente una asignación óptima de recursos para todos los agentes a lo largo del tiempo, así como políticas óptimas de MDP de horizonte finito que son válidas bajo esa asignación de recursos. Como una medida aproximada de la complejidad de este MILP, consideremos el número de variables de optimización y restricciones. Sea TM = P Tm = P m(τa m − τd m + 1) la suma de las longitudes de las ventanas de llegada-salida a través de todos los agentes. Entonces, el número de variables de optimización es: TM + bτ|M||Ω| + bτ|M|, de las cuales TM son continuas (xm), y bτ|M||Ω| + bτ|M| son binarias (Δ y θ). Sin embargo, hay que tener en cuenta que todos menos TM|M| de los θ se establecen en cero por la restricción (7), lo que también obliga inmediatamente a que todos menos TM|M||Ω| de los Δ sean cero a través de las restricciones (8). El número de restricciones (sin incluir las restricciones degeneradas en (7)) en el MILP es: TM + TM|Ω| + bτ|Ω| + bτ|M||Ω|. A pesar de que la complejidad del MILP es, en el peor de los casos, exponencial en el número de variables binarias, la complejidad de este MILP es significativamente (exponencialmente) menor que la del MILP con funciones de utilidad planas, descrito en la Sección 2.2. Este resultado refleja las ganancias de eficiencia reportadas en [6] para problemas de asignación de recursos de un solo disparo, pero es mucho más pronunciado, debido a la explosión de la representación de utilidad plana debido al aspecto temporal del problema (recuerde la complejidad prohibitiva de la optimización combinatoria en la Sección 2.2). Analizamos empíricamente el rendimiento de este método en la Sección 5.1. Estrictamente hablando, resolver MILPs hasta la optimalidad es NP-completo en el número de variables enteras. RESULTADOS EXPERIMENTALES Aunque la complejidad de resolver MILPs es en el peor de los casos exponencial en el número de variables enteras, existen muchos métodos eficientes para resolver MILPs que permiten que nuestro algoritmo se adapte bien a parámetros comunes en problemas de asignación de recursos y programación. En particular, esta sección introduce un dominio de problema: el problema del taller de reparación, que se utiliza para evaluar empíricamente la escalabilidad de nuestros algoritmos en términos del número de agentes |M|, el número de recursos compartidos |Ω| y las diferentes longitudes de tiempo global bτ durante las cuales los agentes pueden entrar y salir del sistema. El problema del taller de reparaciones es un MDP parametrizado simple que adopta la metáfora de un taller de reparaciones de vehículos. Los agentes en el taller de reparación son mecánicos con una serie de tareas independientes que solo generan recompensa cuando se completan. En nuestro modelo MDP de este sistema, las acciones tomadas para avanzar a través del espacio de estados solo están permitidas si el agente posee ciertos recursos que están públicamente disponibles para la tienda. Estos recursos son de suministro finito, y las políticas óptimas para la tienda determinarán cuándo cada agente puede retener los recursos limitados para tomar acciones y obtener recompensas individuales. Cada tarea a completar está asociada con una sola acción, aunque se requiere que el agente repita la acción numerosas veces antes de completar la tarea y obtener una recompensa. Este modelo fue parametrizado en términos del número de agentes en el sistema, el número de diferentes tipos de recursos que podrían estar vinculados a acciones necesarias, un tiempo global durante el cual se permite a los agentes llegar y partir, y una longitud máxima para el número de pasos de tiempo que un agente puede permanecer en el sistema. Todos los puntos de datos en nuestros experimentos se obtuvieron con 20 evaluaciones utilizando CPLEX para resolver los MILPs en una computadora Pentium4 con 2Gb de RAM. Se realizaron pruebas tanto en la versión estática como en la dinámica del problema de programación de recursos, tal como se definió anteriormente. La Figura 3 muestra el tiempo de ejecución y el valor de la política para modificaciones independientes en el conjunto de parámetros. La fila superior muestra cómo el tiempo de solución para el MILP escala a medida que aumentamos el número de agentes |M|, el horizonte de tiempo global bτ y el número de recursos |Ω|. Aumentar el número de agentes conduce a una escalabilidad exponencial en complejidad, lo cual es de esperarse para un problema NP-completo. Sin embargo, aumentar el límite de tiempo global bτ o el número total de tipos de recursos |Ω|, manteniendo constante el número de agentes, no conduce a una disminución en el rendimiento. Esto ocurre porque los problemas se vuelven más fáciles a medida que se vuelven sub-determinados, lo cual también es un fenómeno común en problemas NP-completos. También observamos que la solución a la versión dinámica del problema a menudo se puede calcular mucho más rápido que la versión estática. La fila inferior de la Figura 3 muestra el valor de la política conjunta de las políticas que corresponden a los horarios óptimos de asignación de recursos calculados. Podemos observar que la versión dinámica produce una recompensa más alta (como era de esperar, ya que la recompensa de la versión dinámica siempre es igual o mayor que la recompensa de la versión estática). Deberíamos señalar que estos gráficos no deben ser vistos como una medida del rendimiento de dos algoritmos diferentes (ambos algoritmos producen soluciones óptimas pero para problemas distintos), sino más bien como observaciones sobre cómo cambia la calidad de las soluciones óptimas a medida que se permite más flexibilidad en la reasignación de recursos. La Figura 4 muestra el tiempo de ejecución y el valor de la política para las pruebas en las que las variables de entrada comunes se escalan juntas. Esto permite la Sexta Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 1225 2 4 6 8 10 10 −3 10 −2 10 −1 10 0 10 1 10 2 10 3 10 4 Número de Agentes |M| Tiempo de CPU, seg |Ω| = 5, τ = 50 estático dinámico 50 100 150 200 10 −2 10 −1 10 0 10 1 10 2 10 3 Límite de Tiempo Global τ Tiempo de CPU, seg |M| = 5, |Ω| = 5 estático dinámico 10 20 30 40 50 10 −2 10 −1 10 0 10 1 10 2 Número de Recursos |Ω| Tiempo de CPU, seg |M| = 5, τ = 50 estático dinámico 2 4 6 8 10 200 400 600 800 1000 1200 1400 1600 Número de Agentes |M| Valor |Ω| = 5, τ = 50 estático dinámico 50 100 150 200 400 500 600 700 800 900 1000 1100 1200 1300 1400 Límite de Tiempo Global τ Valor |M| = 5, |Ω| = 5 estático dinámico 10 20 30 40 50 500 600 700 800 900 1000 1100 1200 1300 1400 Número de Recursos |Ω| Valor |M| = 5, τ = 50 estático dinámico Figura 3: Evaluación de nuestro MILP para números variables de agentes (columna 1), longitudes de ventana de tiempo global (columna 2) y números de tipos de recursos (columna 3). La fila superior muestra el tiempo de CPU, y la fila inferior muestra la recompensa conjunta de las políticas MDP de los agentes. Las barras de error muestran los cuartiles 1ro y 3ro (25% y 75%). 2 4 6 8 10 10 −3 10 −2 10 −1 10 0 10 1 10 2 10 3 Número de Agentes |M| Tiempo de CPU, seg τ = 10|M| estático dinámico 2 4 6 8 10 10 −3 10 −2 10 −1 10 0 10 1 10 2 10 3 10 4 Número de Agentes |M| Tiempo de CPU, seg |Ω| = 2|M| estático dinámico 2 4 6 8 10 10 −3 10 −2 10 −1 10 0 10 1 10 2 10 3 10 4 Número de Agentes |M| Tiempo de CPU, seg |Ω| = 5|M| estático dinámico 2 4 6 8 10 200 400 600 800 1000 1200 1400 1600 1800 2000 2200 Número de Agentes |M| Valor τ = 10|M| estático dinámico 2 4 6 8 10 200 400 600 800 1000 1200 1400 1600 1800 2000 Número de Agentes |M| Valor |Ω| = 2|M| estático dinámico 2 4 6 8 10 0 500 1000 1500 2000 2500 Número de Agentes |M| Valor |Ω| = 5|M| estático dinámico Figura 4: Evaluación de nuestro MILP utilizando variables de entrada correlacionadas. La columna izquierda sigue el rendimiento y el tiempo de CPU a medida que aumenta el número de agentes y la ventana de tiempo global juntos (bτ = 10|M|). La columna del medio y la columna de la derecha siguen el rendimiento y el tiempo de CPU a medida que aumenta el número de recursos y el número de agentes juntos, con |Ω| = 2|M| y |Ω| = 5|M|, respectivamente. Las barras de error muestran el primer y tercer cuartil (25% y 75%). 1226 The Sixth Intl. La Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) nos permite explorar dominios donde el número total de agentes escala proporcionalmente al número total de tipos de recursos o al horizonte temporal global, manteniendo constante la densidad promedio de agentes (por unidad de tiempo global) o el número promedio de recursos por agente (lo cual comúnmente ocurre en aplicaciones de la vida real). En general, creemos que estos resultados experimentales indican que nuestra formulación MILP puede ser utilizada para resolver de manera efectiva problemas de programación de recursos de tamaño no trivial. DISCUSIÓN Y CONCLUSIONES A lo largo del documento, hemos realizado una serie de suposiciones en nuestro modelo y algoritmo de solución; discutimos sus implicaciones a continuación. • Ejecución continua. Suponemos que una vez que un agente deja de ejecutar su MDP (transiciona al estado sf), sale del sistema y no puede regresar. Es fácil relajar esta suposición para dominios donde los agentes de MDP pueden ser pausados y reiniciados. Todo lo que se requiere es incluir una acción de pausa adicional que haga la transición desde un estado dado de regreso a sí mismo, y que tenga una recompensa de cero. • Indiferencia al tiempo de inicio. Utilizamos un modelo de recompensa donde las recompensas de los agentes dependen solo del horizonte temporal de sus MDP y no del tiempo de inicio global. Esta es una consecuencia de nuestro procedimiento de aumento de MDP de la Sección 4.1. Es fácil extender el modelo para que los agentes incurran en una penalización explícita por inactividad al asignar una recompensa negativa no nula al estado inicial sb. • Requisitos de recursos binarios. Para simplificar, hemos asumido que los costos de recursos son binarios: ϕm(a, ω) = {0, 1}, pero nuestros resultados se generalizan de manera directa a asignaciones de recursos no binarias, de manera análoga al procedimiento utilizado en [5]. • Agentes cooperativos. El procedimiento de optimización discutido en este artículo fue desarrollado en el contexto de agentes cooperativos, pero también puede ser utilizado para diseñar un mecanismo para programar recursos entre agentes egoístas. Este procedimiento de optimización puede ser incorporado en una subasta de Vickrey-Clarke-Groves, de manera completamente análoga a como se hizo en [7]. De hecho, todos los resultados de [7] sobre las propiedades de la subasta y la privacidad de la información se aplican directamente al dominio de programación discutido en este documento, requiriendo solo ligeras modificaciones para tratar con MDPs de horizonte finito. • Tiempos de llegada y salida conocidos y deterministas. Finalmente, hemos asumido que los tiempos de llegada y salida de los agentes (τa m y τd m) son deterministas y conocidos de antemano. Esta suposición es fundamental para nuestro método de solución. Si bien hay muchos dominios donde esta suposición es válida, en muchos casos los agentes llegan y se van de forma dinámica y sus tiempos de llegada y salida solo pueden predecirse de manera probabilística, lo que lleva a problemas de asignación de recursos en línea. En particular, en el caso de agentes con interés propio, esto se convierte en una versión interesante de un problema de diseño de mecanismos en línea [11, 12]. En resumen, hemos presentado una formulación MILP para el problema combinatorio de programación de recursos donde los valores de los agentes para posibles asignaciones de recursos están definidos por MDPs de horizonte finito. Este resultado amplía el trabajo previo ([6, 7]) sobre asignación estática de recursos de una sola vez bajo preferencias inducidas por MDP a problemas de programación de recursos con un aspecto temporal. Por lo tanto, este trabajo da un paso en la dirección de diseñar un mecanismo en línea para agentes con preferencias de recursos combinatorios inducidas por problemas de planificación estocástica. Relajar la suposición sobre los tiempos deterministas de llegada y salida de los agentes es un enfoque de nuestro trabajo futuro. Nos gustaría agradecer a los revisores anónimos por sus comentarios y sugerencias perspicaces. 7. REFERENCIAS [1] E. Altman y A. Shwartz. Control adaptativo de cadenas de Markov restringidas: Criterios y políticas. Anales de Investigación de Operaciones, número especial sobre Procesos de Decisión de Markov, 28:101-134, 1991. [2] R. Bellman. Programación dinámica. Princeton University Press, 1957. [3] C. Boutilier. \n\nPrensa de la Universidad de Princeton, 1957. [3] C. Boutilier. Resolviendo problemas de subasta combinatoria expresados de manera concisa. En Proc. de AAAI-02, páginas 359-366, 2002. [4] C. Boutilier y H. H. Hoos. Subastando idiomas para subastas combinatorias. En Proc. de IJCAI-01, páginas 1211-1217, 2001. [5] D. Dolgov. Asignación integrada de recursos y planificación en entornos estocásticos de múltiples agentes. Tesis doctoral, Departamento de Ciencias de la Computación, Universidad de Michigan, febrero de 2006. [6] D. A. Dolgov y E. H. Durfee. Asignación óptima de recursos y formulación de políticas en procesos de decisión de Markov débilmente acoplados. En Proc. de ICAPS-04, páginas 315-324, junio de 2004. [7] D. A. Dolgov y E. H. Durfee. Subastas combinatorias computacionalmente eficientes para asignación de recursos en MDPs débilmente acoplados. En Proc. de AAMAS-05, Nueva York, NY, EE. UU., 2005. ACM Press. [8] D. A. Dolgov y E. H. Durfee. Asignación de recursos entre agentes con preferencias inducidas por MDP factorizados. En Proc. de AAMAS-06, 2006. [9] K. Larson y T. Sandholm. Diseño de mecanismos y agentes deliberativos. En Proc. de AAMAS-05, páginas 650-656, Nueva York, NY, EE. UU., 2005. ACM Press. [10] N. Nisan. \n\nACM Press. [10] N. Nisan. Subasta y asignación en subastas combinatorias. En Comercio Electrónico, 2000. [11] D. C. Parkes y S. Singh. Un enfoque basado en MDP para el Diseño de Mecanismos en Línea. En Proc. de la Decimoséptima Conferencia Anual sobre Sistemas de Procesamiento de Información Neural (NIPS-03), 2003. [12] D. C. Parkes, S. Singh y D. Yanovsky. Diseño de mecanismos en línea aproximadamente eficiente. En Actas de la Decimoctava Conferencia Anual sobre Sistemas de Procesamiento de Información Neural (NIPS-04), 2004. [13] M. L. Puterman. Procesos de Decisión de Markov. John Wiley & Sons, Nueva York, 1994. [14] M. H. Rothkopf, A. Pekec y R. M. Harstad. Subastas combinatorias manejables computacionalmente. Ciencia de la Gestión, 44(8):1131-1147, 1998. [15] T. Sandholm. Un algoritmo para la determinación óptima del ganador en subastas combinatorias. En Proc. de IJCAI-99, páginas 542-547, San Francisco, CA, EE. UU., 1999. Morgan Kaufmann Publishers Inc. - Editores Morgan Kaufmann Inc. El Sexto Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 1227 ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "resource-scheduling algorithm": {
            "translated_key": "algoritmo de programación de recursos",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Combinatorial Resource Scheduling for Multiagent MDPs Dmitri A. Dolgov, Michael R. James, and Michael E. Samples AI and Robotics Group Technical Research, Toyota Technical Center USA {ddolgov, michael.r.james, michael.samples}@gmail.com ABSTRACT Optimal resource scheduling in multiagent systems is a computationally challenging task, particularly when the values of resources are not additive.",
                "We consider the combinatorial problem of scheduling the usage of multiple resources among agents that operate in stochastic environments, modeled as Markov decision processes (MDPs).",
                "In recent years, efficient resource-allocation algorithms have been developed for agents with resource values induced by MDPs.",
                "However, this prior work has focused on static resource-allocation problems where resources are distributed once and then utilized in infinite-horizon MDPs.",
                "We extend those existing models to the problem of combinatorial resource scheduling, where agents persist only for finite periods between their (predefined) arrival and departure times, requiring resources only for those time periods.",
                "We provide a computationally efficient procedure for computing globally optimal resource assignments to agents over time.",
                "We illustrate and empirically analyze the method in the context of a stochastic jobscheduling domain.",
                "Categories and Subject Descriptors I.2.8 [Artificial Intelligence]: Problem Solving, Control Methods, and Search; I.2.11 [Artificial Intelligence]: Distributed Artificial Intelligence-Multiagent systems General Terms Algorithms, Performance, Design 1.",
                "INTRODUCTION The tasks of optimal resource allocation and scheduling are ubiquitous in multiagent systems, but solving such optimization problems can be computationally difficult, due to a number of factors.",
                "In particular, when the value of a set of resources to an agent is not additive (as is often the case with resources that are substitutes or complements), the utility function might have to be defined on an exponentially large space of resource bundles, which very quickly becomes computationally intractable.",
                "Further, even when each agent has a utility function that is nonzero only on a small subset of the possible resource bundles, obtaining optimal allocation is still computationally prohibitive, as the problem becomes NP-complete [14].",
                "Such computational issues have recently spawned several threads of work in using compact models of agents preferences.",
                "One idea is to use any structure present in utility functions to represent them compactly, via, for example, logical formulas [15, 10, 4, 3].",
                "An alternative is to directly model the mechanisms that define the agents utility functions and perform resource allocation directly with these models [9].",
                "A way of accomplishing this is to model the processes by which an agent might utilize the resources and define the utility function as the payoff of these processes.",
                "In particular, if an agent uses resources to act in a stochastic environment, its utility function can be naturally modeled with a Markov decision process, whose action set is parameterized by the available resources.",
                "This representation can then be used to construct very efficient resource-allocation algorithms that lead to an exponential speedup over a straightforward optimization problem with flat representations of combinatorial preferences [6, 7, 8].",
                "However, this existing work on resource allocation with preferences induced by resource-parameterized MDPs makes an assumption that the resources are only allocated once and are then utilized by the agents independently within their infinite-horizon MDPs.",
                "This assumption that no reallocation of resources is possible can be limiting in domains where agents arrive and depart dynamically.",
                "In this paper, we extend the work on resource allocation under MDP-induced preferences to discrete-time scheduling problems, where agents are present in the system for finite time intervals and can only use resources within these intervals.",
                "In particular, agents arrive and depart at arbitrary (predefined) times and within these intervals use resources to execute tasks in finite-horizon MDPs.",
                "We address the problem of globally optimal resource scheduling, where the objective is to find an allocation of resources to the agents across time that maximizes the sum of the expected rewards that they obtain.",
                "In this context, our main contribution is a mixed-integerprogramming formulation of the scheduling problem that chooses globally optimal resource assignments, starting times, and execution horizons for all agents (within their arrival1220 978-81-904262-7-5 (RPS) c 2007 IFAAMAS departure intervals).",
                "We analyze and empirically compare two flavors of the scheduling problem: one, where agents have static resource assignments within their finite-horizon MDPs, and another, where resources can be dynamically reallocated between agents at every time step.",
                "In the rest of the paper, we first lay down the necessary groundwork in Section 2 and then introduce our model and formal problem statement in Section 3.",
                "In Section 4.2, we describe our main result, the optimization program for globally optimal resource scheduling.",
                "Following the discussion of our experimental results on a job-scheduling problem in Section 5, we conclude in Section 6 with a discussion of possible extensions and generalizations of our method. 2.",
                "BACKGROUND Similarly to the model used in previous work on resourceallocation with MDP-induced preferences [6, 7], we define the value of a set of resources to an agent as the value of the best MDP policy that is realizable, given those resources.",
                "However, since the focus of our work is on scheduling problems, and a large part of the optimization problem is to decide how resources are allocated in time among agents with finite arrival and departure times, we model the agents planning problems as finite-horizon MDPs, in contrast to previous work that used infinite-horizon discounted MDPs.",
                "In the rest of this section, we first introduce some necessary background on finite-horizon MDPs and present a linear-programming formulation that serves as the basis for our solution algorithm developed in Section 4.",
                "We also outline the standard methods for combinatorial resource scheduling with flat resource values, which serve as a comparison benchmark for the new model developed here. 2.1 Markov Decision Processes A stationary, finite-domain, discrete-time MDP (see, for example, [13] for a thorough and detailed development) can be described as S, A, p, r , where: S is a finite set of system states; A is a finite set of actions that are available to the agent; p is a stationary stochastic transition function, where p(σ|s, a) is the probability of transitioning to state σ upon executing action a in state s; r is a stationary reward function, where r(s, a) specifies the reward obtained upon executing action a in state s. Given such an MDP, a decision problem under a finite horizon T is to choose an optimal action at every time step to maximize the expected value of the total reward accrued during the agents (finite) lifetime.",
                "The agents optimal policy is then a function of current state s and the time until the horizon.",
                "An optimal policy for such a problem is to act greedily with respect to the optimal value function, defined recursively by the following system of finite-time Bellman equations [2]: v(s, t) = max a r(s, a) + X σ p(σ|s, a)v(σ, t + 1), ∀s ∈ S, t ∈ [1, T − 1]; v(s, T) = 0, ∀s ∈ S; where v(s, t) is the optimal value of being in state s at time t ∈ [1, T].",
                "This optimal value function can be easily computed using dynamic programming, leading to the following optimal policy π, where π(s, a, t) is the probability of executing action a in state s at time t: π(s, a, t) = ( 1, a = argmaxa r(s, a) + P σ p(σ|s, a)v(σ, t + 1), 0, otherwise.",
                "The above is the most common way of computing the optimal value function (and therefore an optimal policy) for a finite-horizon MDP.",
                "However, we can also formulate the problem as the following linear program (similarly to the dual LP for infinite-horizon discounted MDPs [13, 6, 7]): max X s X a r(s, a) X t x(s, a, t) subject to: X a x(σ, a, t + 1) = X s,a p(σ|s, a)x(s, a, t) ∀σ, t ∈ [1, T − 1]; X a x(s, a, 1) = α(s), ∀s ∈ S; (1) where α(s) is the initial distribution over the state space, and x is the (non-stationary) occupation measure (x(s, a, t) ∈ [0, 1] is the total expected number of times action a is executed in state s at time t).",
                "An optimal (non-stationary) policy is obtained from the occupation measure as follows: π(s, a, t) = x(s, a, t)/ X a x(s, a, t) ∀s ∈ S, t ∈ [1, T]. (2) Note that the standard unconstrained finite-horizon MDP, as described above, always has a uniformly-optimal solution (optimal for any initial distribution α(s)).",
                "Therefore, an optimal policy can be obtained by using an arbitrary constant α(s) > 0 (in particular, α(s) = 1 will result in x(s, a, t) = π(s, a, t)).",
                "However, for MDPs with resource constraints (as defined below in Section 3), uniformly-optimal policies do not in general exist.",
                "In such cases, α becomes a part of the problem input, and a resulting policy is only optimal for that particular α.",
                "This result is well known for infinite-horizon MDPs with various types of constraints [1, 6], and it also holds for our finite-horizon model, which can be easily established via a line of reasoning completely analogous to the arguments in [6]. 2.2 Combinatorial Resource Scheduling A straightforward approach to resource scheduling for a set of agents M, whose values for the resources are induced by stochastic planning problems (in our case, finite-horizon MDPs) would be to have each agent enumerate all possible resource assignments over time and, for each one, compute its value by solving the corresponding MDP.",
                "Then, each agent would provide valuations for each possible resource bundle over time to a centralized coordinator, who would compute the optimal resource assignments across time based on these valuations.",
                "When resources can be allocated at different times to different agents, each agent must submit valuations for every combination of possible time horizons.",
                "Let each agent m ∈ M execute its MDP within the arrival-departure time interval τ ∈ [τa m, τd m].",
                "Hence, agent m will execute an MDP with time horizon no greater than Tm = τd m−τa m+1.",
                "Let bτ be the global time horizon for the problem, before which all of the agents MDPs must finish.",
                "We assume τd m < bτ, ∀m ∈ M. The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1221 For the scheduling problem where agents have static resource requirements within their finite-horizon MDPs, the agents provide a valuation for each resource bundle for each possible time horizon (from [1, Tm]) that they may use.",
                "Let Ω be the set of resources to be allocated among the agents.",
                "An agent will get at most one resource bundle for one of the time horizons.",
                "Let the variable ψ ∈ Ψm enumerate all possible pairs of resource bundles and time horizons for agent m, so there are 2|Ω| × Tm values for ψ (the space of bundles is exponential in the number of resource types |Ω|).",
                "The agent m must provide a value vψ m for each ψ, and the coordinator will allocate at most one ψ (resource, time horizon) pair to each agent.",
                "This allocation is expressed as an indicator variable zψ m ∈ {0, 1} that shows whether ψ is assigned to agent m. For time τ and resource ω, the function nm(ψ, τ, ω) ∈ {0, 1} indicates whether the bundle in ψ uses resource ω at time τ (we make the assumption that agents have binary resource requirements).",
                "This allocation problem is NP-complete, even when considering only a single time step, and its difficulty increases significantly with multiple time steps because of the increasing number of values of ψ.",
                "The problem of finding an optimal allocation that satisfies the global constraint that the amount of each resource ω allocated to all agents does not exceed the available amount bϕ(ω) can be expressed as the following integer program: max X m∈M X ψ∈Ψm zψ mvψ m subject to: X ψ∈Ψm zψ m ≤ 1, ∀m ∈ M; X m∈M X ψ∈Ψm zψ mnm(ψ, τ, ω) ≤ bϕ(ω), ∀τ ∈ [1, bτ], ∀ω ∈ Ω; (3) The first constraint in equation 3 says that no agent can receive more than one bundle, and the second constraint ensures that the total assignment of resource ω does not, at any time, exceed the resource bound.",
                "For the scheduling problem where the agents are able to dynamically reallocate resources, each agent must specify a value for every combination of bundles and time steps within its time horizon.",
                "Let the variable ψ ∈ Ψm in this case enumerate all possible resource bundles for which at most one bundle may be assigned to agent m at each time step.",
                "Therefore, in this case there are P t∈[1,Tm](2|Ω| )t ∼ 2|Ω|Tm possibilities of resource bundles assigned to different time slots, for the Tm different time horizons.",
                "The same set of equations (3) can be used to solve this dynamic scheduling problem, but the integer program is different because of the difference in how ψ is defined.",
                "In this case, the number of ψ values is exponential in each agents planning horizon Tm, resulting in a much larger program.",
                "This straightforward approach to solving both of these scheduling problems requires an enumeration and solution of either 2|Ω| Tm (static allocation) or P t∈[1,Tm] 2|Ω|t (dynamic reallocation) MDPs for each agent, which very quickly becomes intractable with the growth of the number of resources |Ω| or the time horizon Tm. 3.",
                "MODEL AND PROBLEM STATEMENT We now formally introduce our model of the resourcescheduling problem.",
                "The problem input consists of the following components: • M, Ω, bϕ, τa m, τd m, bτ are as defined above in Section 2.2. • {Θm} = {S, A, pm, rm, αm} are the MDPs of all agents m ∈ M. Without loss of generality, we assume that state and action spaces of all agents are the same, but each has its own transition function pm, reward function rm, and initial conditions αm. • ϕm : A×Ω → {0, 1} is the mapping of actions to resources for agent m. ϕm(a, ω) indicates whether action a of agent m needs resource ω.",
                "An agent m that receives a set of resources that does not include resource ω cannot execute in its MDP policy any action a for which ϕm(a, ω) = 0.",
                "We assume all resource requirements are binary; as discussed below in Section 6, this assumption is not limiting.",
                "Given the above input, the optimization problem we consider is to find the globally optimal-maximizing the sum of expected rewards-mapping of resources to agents for all time steps: Δ : τ × M × Ω → {0, 1}.",
                "A solution is feasible if the corresponding assignment of resources to the agents does not violate the global resource constraint: X m Δm(τ, ω) ≤ bϕ(ω), ∀ω ∈ Ω, τ ∈ [1, bτ]. (4) We consider two flavors of the resource-scheduling problem.",
                "The first formulation restricts resource assignments to the space where the allocation of resources to each agent is static during the agents lifetime.",
                "The second formulation allows reassignment of resources between agents at every time step within their lifetimes.",
                "Figure 1 depicts a resource-scheduling problem with three agents M = {m1, m2, m3}, three resources Ω = {ω1, ω2, ω3}, and a global problem horizon of bτ = 11.",
                "The agents arrival and departure times are shown as gray boxes and are {1, 6}, {3, 7}, and {2, 11}, respectively.",
                "A solution to this problem is shown via horizontal bars within each agents box, where the bars correspond to the allocation of the three resource types.",
                "Figure 1a shows a solution to a static scheduling problem.",
                "According to the shown solution, agent m1 begins the execution of its MDP at time τ = 1 and has a lock on all three resources until it finishes execution at time τ = 3.",
                "Note that agent m1 relinquishes its hold on the resources before its announced departure time of τd m1 = 6, ostensibly because other agents can utilize the resources more effectively.",
                "Thus, at time τ = 4, resources ω1 and ω3 are allocated to agent m2, who then uses them to execute its MDP (using only actions supported by resources ω1 and ω3) until time τ = 7.",
                "Agent m3 holds resource ω3 during the interval τ ∈ [4, 10].",
                "Figure 1b shows a possible solution to the dynamic version of the same problem.",
                "There, resources can be reallocated between agents at every time step.",
                "For example, agent m1 gives up its use of resource ω2 at time τ = 2, although it continues the execution of its MDP until time τ = 6.",
                "Notice that an agent is not allowed to stop and restart its MDP, so agent m1 is only able to continue executing in the interval τ ∈ [3, 4] if it has actions that do not require any resources (ϕm(a, ω) = 0).",
                "Clearly, the model and problem statement described above make a number of assumptions about the problem and the desired solution properties.",
                "We discuss some of those assumptions and their implications in Section 6. 1222 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) (a) (b) Figure 1: Illustration of a solution to a resource-scheduling problem with three agents and three resources: a) static resource assignments (resource assignments are constant within agents lifetimes; b) dynamic assignment (resource assignments are allowed to change at every time step). 4.",
                "RESOURCE SCHEDULING Our <br>resource-scheduling algorithm</br> proceeds in two stages.",
                "First, we perform a preprocessing step that augments the agent MDPs; this process is described in Section 4.1.",
                "Second, using these augmented MDPs we construct a global optimization problem, which is described in Section 4.2. 4.1 Augmenting Agents MDPs In the model described in the previous section, we assume that if an agent does not possess the necessary resources to perform actions in its MDP, its execution is halted and the agent leaves the system.",
                "In other words, the MDPs cannot be paused and resumed.",
                "For example, in the problem shown in Figure 1a, agent m1 releases all resources after time τ = 3, at which point the execution of its MDP is halted.",
                "Similarly, agents m2 and m3 only execute their MDPs in the intervals τ ∈ [4, 6] and τ ∈ [4, 10], respectively.",
                "Therefore, an important part of the global decision-making problem is to decide the window of time during which each of the agents is active (i.e., executing its MDP).",
                "To accomplish this, we augment each agents MDP with two new states (start and finish states sb , sf , respectively) and a new start/stop action a∗ , as illustrated in Figure 2.",
                "The idea is that an agent stays in the start state sb until it is ready to execute its MDP, at which point it performs the start/stop action a∗ and transitions into the state space of the original MDP with the transition probability that corresponds to the original initial distribution α(s).",
                "For example, in Figure 1a, for agent m2 this would happen at time τ = 4.",
                "Once the agent gets to the end of its activity window (time τ = 6 for agent m2 in Figure 1a), it performs the start/stop action, which takes it into the sink finish state sf at time τ = 7.",
                "More precisely, given an MDP S, A, pm, rm, αm , we define an augmented MDP S , A , pm, rm, αm as follows: S = S ∪ sb ∪ sf ; A = A ∪ a∗ ; p (s|sb , a∗ ) = α(s), ∀s ∈ S; p (sb |sb , a) = 1.0, ∀a ∈ A; p (sf |s, a∗ ) = 1.0, ∀s ∈ S; p (σ|s, a) = p(σ|s, a), ∀s, σ ∈ S, a ∈ A; r (sb , a) = r (sf , a) = 0, ∀a ∈ A ; r (s, a) = r(s, a), ∀s ∈ S, a ∈ A; α (sb ) = 1; α (s) = 0, ∀s ∈ S; where all non-specified transition probabilities are assumed to be zero.",
                "Further, in order to account for the new starting state, we begin the MDP one time-step earlier, setting τa m ← τa m − 1.",
                "This will not affect the resource allocation due to the resource constraints only being enforced for the original MDP states, as will be discussed in the next section.",
                "For example, the augmented MDPs shown in Figure 2b (which starts in state sb at time τ = 2) would be constructed from an MDP with original arrival time τ = 3.",
                "Figure 2b also shows a sample trajectory through the state space: the agent starts in state sb , transitions into the state space S of the original MDP, and finally exists into the sink state sf .",
                "Note that if we wanted to model a problem where agents could pause their MDPs at arbitrary time steps (which might be useful for domains where dynamic reallocation is possible), we could easily accomplish this by including an extra action that transitions from each state to itself with zero reward. 4.2 MILP for Resource Scheduling Given a set of augmented MDPs, as defined above, the goal of this section is to formulate a global optimization program that solves the resource-scheduling problem.",
                "In this section and below, all MDPs are assumed to be the augmented MDPs as defined in Section 4.1.",
                "Our approach is similar to the idea used in [6]: we begin with the linear-program formulation of agents MDPs (1) and augment it with constraints that ensure that the corresponding resource allocation across agents and time is valid.",
                "The resulting optimization problem then simultaneously solves the agents MDPs and resource-scheduling problems.",
                "In the rest of this section, we incrementally develop a mixed integer program (MILP) that achieves this.",
                "In the absence of resource constraints, the agents finitehorizon MDPs are completely independent, and the globally optimal solution can be trivially obtained via the following LP, which is simply an aggregation of single-agent finitehorizon LPs: max X m X s X a rm(s, a) X t xm(s, a, t) subject to: X a xm(σ, a, t + 1) = X s,a pm(σ|s, a)xm(s, a, t), ∀m ∈ M, σ ∈ S, t ∈ [1, Tm − 1]; X a xm(s, a, 1) = αm(s), ∀m ∈ M, s ∈ S; (12) where xm(s, a, t) is the occupation measure of agent m, and The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1223 (a) (b) Figure 2: Illustration of augmenting an MDP to allow for variable starting and stopping times: a) (left) the original two-state MDP with a single action; (right) the augmented MDP with new states sb and sf and the new action a∗ (note that the origianl transitions are not changed in the augmentation process); b) the augmented MDP displayed as a trajectory through time (grey lines indicate all transitions, while black lines indicate a given trajectory.",
                "Objective Function (sum of expected rewards over all agents) max X m X s X a rm(s, a) X t xm(s, a, t) (5) Meaning Implication Linear Constraints Tie x to θ.",
                "Agent is only active when occupation measure is nonzero in original MDP states. θm(τ) = 0 =⇒ xm(s, a, τ −τa m+1) = 0 ∀s /∈ {sb , sf }, a ∈ A X s/∈{sb,sf } X a xm(s, a, t) ≤ θm(τa m + t − 1) ∀m ∈ M, ∀t ∈ [1, Tm] (6) Agent can only be active in τ ∈ (τa m, τd m) θm(τ) = 0 ∀m ∈ M, τ /∈ (τa m, τd m) (7) Cannot use resources when not active θm(τ) = 0 =⇒ Δm(τ, ω) = 0 ∀τ ∈ [0, bτ], ω ∈ Ω Δm(τ, ω) ≤ θm(τ) ∀m ∈ M, τ ∈ [0, bτ], ω ∈ Ω (8) Tie x to Δ (nonzero x forces corresponding Δ to be nonzero.)",
                "Δm(τ, ω) = 0, ϕm(a, ω) = 1 =⇒ xm(s, a, τ − τa m + 1) = 0 ∀s /∈ {sb , sf } 1/|A| X a ϕm(a, ω) X s/∈{sb,sf } xm(s, a, t) ≤ Δm(t + τa m − 1, ω) ∀m ∈ M, ω ∈ Ω, t ∈ [1, Tm] (9) Resource bounds X m Δm(τ, ω) ≤ bϕ(ω) ∀ω ∈ Ω, τ ∈ [0, bτ] (10) Agent cannot change resources while active.",
                "Only enabled for scheduling with static assignments. θm(τ) = 1 and θm(τ + 1) = 1 =⇒ Δm(τ, ω) = Δm(τ + 1, ω) Δm(τ, ω) − Z(1 − θm(τ + 1)) ≤ Δm(τ + 1, ω) + Z(1 − θm(τ)) Δm(τ, ω) + Z(1 − θm(τ + 1)) ≥ Δm(τ + 1, ω) − Z(1 − θm(τ)) ∀m ∈ M, ω ∈ Ω, τ ∈ [0, bτ] (11) Table 1: MILP for globally optimal resource scheduling.",
                "Tm = τd m − τa m + 1 is the time horizon for the agents MDP.",
                "Using this LP as a basis, we augment it with constraints that ensure that the resource usage implied by the agents occupation measures {xm} does not violate the global resource requirements bϕ at any time step τ ∈ [0, bτ].",
                "To formulate these resource constraints, we use the following binary variables: • Δm(τ, ω) = {0, 1}, ∀m ∈ M, τ ∈ [0, bτ], ω ∈ Ω, which serve as indicator variables that define whether agent m possesses resource ω at time τ.",
                "These are analogous to the static indicator variables used in the one-shot static resource-allocation problem in [6]. • θm = {0, 1}, ∀m ∈ M, τ ∈ [0, bτ] are indicator variables that specify whether agent m is active (i.e., executing its MDP) at time τ.",
                "The meaning of resource-usage variables Δ is illustrated in Figure 1: Δm(τ, ω) = 1 only if resource ω is allocated to agent m at time τ.",
                "The meaning of the activity indicators θ is illustrated in Figure 2b: when agent m is in either the start state sb or the finish state sf , the corresponding θm = 0, but once the agent becomes active and enters one of the other states, we set θm = 1 .",
                "This meaning of θ can be enforced with a linear constraint that synchronizes the values of the agents occupation measures xm and the activity 1224 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) indicators θ, as shown in (6) in Table 1.",
                "Another constraint we have to add-because the activity indicators θ are defined on the global timeline τ-is to enforce the fact that the agent is inactive outside of its arrivaldeparture window.",
                "This is accomplished by constraint (7) in Table 1.",
                "Furthermore, agents should not be using resources while they are inactive.",
                "This constraint can also be enforced via a linear inequality on θ and Δ, as shown in (8).",
                "Constraint (6) sets the value of θ to match the policy defined by the occupation measure xm.",
                "In a similar fashion, we have to make sure that the resource-usage variables Δ are also synchronized with the occupation measure xm.",
                "This is done via constraint (9) in Table 1, which is nearly identical to the analogous constraint from [6].",
                "After implementing the above constraint, which enforces the meaning of Δ, we add a constraint that ensures that the agents resource usage never exceeds the amounts of available resources.",
                "This condition is also trivially expressed as a linear inequality (10) in Table 1.",
                "Finally, for the problem formulation where resource assignments are static during a lifetime of an agent, we add a constraint that ensures that the resource-usage variables Δ do not change their value while the agent is active (θ = 1).",
                "This is accomplished via the linear constraint (11), where Z ≥ 2 is a constant that is used to turn off the constraints when θm(τ) = 0 or θm(τ + 1) = 0.",
                "This constraint is not used for the dynamic problem formulation, where resources can be reallocated between agents at every time step.",
                "To summarize, Table 1 together with the conservationof-flow constraints from (12) defines the MILP that simultaneously computes an optimal resource assignment for all agents across time as well as optimal finite-horizon MDP policies that are valid under that resource assignment.",
                "As a rough measure of the complexity of this MILP, let us consider the number of optimization variables and constraints.",
                "Let TM = P Tm = P m(τa m − τd m + 1) be the sum of the lengths of the arrival-departure windows across all agents.",
                "Then, the number of optimization variables is: TM + bτ|M||Ω| + bτ|M|, TM of which are continuous (xm), and bτ|M||Ω| + bτ|M| are binary (Δ and θ).",
                "However, notice that all but TM|M| of the θ are set to zero by constraint (7), which also immediately forces all but TM|M||Ω| of the Δ to be zero via the constraints (8).",
                "The number of constraints (not including the degenerate constraints in (7)) in the MILP is: TM + TM|Ω| + bτ|Ω| + bτ|M||Ω|.",
                "Despite the fact that the complexity of the MILP is, in the worst case, exponential1 in the number of binary variables, the complexity of this MILP is significantly (exponentially) lower than that of the MILP with flat utility functions, described in Section 2.2.",
                "This result echos the efficiency gains reported in [6] for single-shot resource-allocation problems, but is much more pronounced, because of the explosion of the flat utility representation due to the temporal aspect of the problem (recall the prohibitive complexity of the combinatorial optimization in Section 2.2).",
                "We empirically analyze the performance of this method in Section 5. 1 Strictly speaking, solving MILPs to optimality is NPcomplete in the number of integer variables. 5.",
                "EXPERIMENTAL RESULTS Although the complexity of solving MILPs is in the worst case exponential in the number of integer variables, there are many efficient methods for solving MILPs that allow our algorithm to scale well for parameters common to resource allocation and scheduling problems.",
                "In particular, this section introduces a problem domain-the repairshop problem-used to empirically evaluate our algorithms scalability in terms of the number of agents |M|, the number of shared resources |Ω|, and the varied lengths of global time bτ during which agents may enter and exit the system.",
                "The repairshop problem is a simple parameterized MDP adopting the metaphor of a vehicular repair shop.",
                "Agents in the repair shop are mechanics with a number of independent tasks that yield reward only when completed.",
                "In our MDP model of this system, actions taken to advance through the state space are only allowed if the agent holds certain resources that are publicly available to the shop.",
                "These resources are in finite supply, and optimal policies for the shop will determine when each agent may hold the limited resources to take actions and earn individual rewards.",
                "Each task to be completed is associated with a single action, although the agent is required to repeat the action numerous times before completing the task and earning a reward.",
                "This model was parameterized in terms of the number of agents in the system, the number of different types of resources that could be linked to necessary actions, a global time during which agents are allowed to arrive and depart, and a maximum length for the number of time steps an agent may remain in the system.",
                "All datapoints in our experiments were obtained with 20 evaluations using CPLEX to solve the MILPs on a Pentium4 computer with 2Gb of RAM.",
                "Trials were conducted on both the static and the dynamic version of the resourcescheduling problem, as defined earlier.",
                "Figure 3 shows the runtime and policy value for independent modifications to the parameter set.",
                "The top row shows how the solution time for the MILP scales as we increase the number of agents |M|, the global time horizon bτ, and the number of resources |Ω|.",
                "Increasing the number of agents leads to exponential complexity scaling, which is to be expected for an NP-complete problem.",
                "However, increasing the global time limit bτ or the total number of resource types |Ω|-while holding the number of agents constantdoes not lead to decreased performance.",
                "This occurs because the problems get easier as they become under-constrained, which is also a common phenomenon for NP-complete problems.",
                "We also observe that the solution to the dynamic version of the problem can often be computed much faster than the static version.",
                "The bottom row of Figure 3 shows the joint policy value of the policies that correspond to the computed optimal resource-allocation schedules.",
                "We can observe that the dynamic version yields higher reward (as expected, since the reward for the dynamic version is always no less than the reward of the static version).",
                "We should point out that these graphs should not be viewed as a measure of performance of two different algorithms (both algorithms produce optimal solutions but to different problems), but rather as observations about how the quality of optimal solutions change as more flexibility is allowed in the reallocation of resources.",
                "Figure 4 shows runtime and policy value for trials in which common input variables are scaled together.",
                "This allows The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1225 2 4 6 8 10 10 −3 10 −2 10 −1 10 0 10 1 10 2 10 3 10 4 Number of Agents |M| CPUTime,sec |Ω| = 5, τ = 50 static dynamic 50 100 150 200 10 −2 10 −1 10 0 10 1 10 2 10 3 Global Time Boundary τ CPUTime,sec |M| = 5, |Ω| = 5 static dynamic 10 20 30 40 50 10 −2 10 −1 10 0 10 1 10 2 Number of Resources |Ω| CPUTime,sec |M| = 5, τ = 50 static dynamic 2 4 6 8 10 200 400 600 800 1000 1200 1400 1600 Number of Agents |M| Value |Ω| = 5, τ = 50 static dynamic 50 100 150 200 400 500 600 700 800 900 1000 1100 1200 1300 1400 Global Time Boundary τ Value |M| = 5, |Ω| = 5 static dynamic 10 20 30 40 50 500 600 700 800 900 1000 1100 1200 1300 1400 Number of Resources |Ω| Value |M| = 5, τ = 50 static dynamic Figure 3: Evaluation of our MILP for variable numbers of agents (column 1), lengths of global-time window (column 2), and numbers of resource types (column 3).",
                "Top row shows CPU time, and bottom row shows the joint reward of agents MDP policies.",
                "Error bars show the 1st and 3rd quartiles (25% and 75%). 2 4 6 8 10 10 −3 10 −2 10 −1 10 0 10 1 10 2 10 3 Number of Agents |M| CPUTime,sec τ = 10|M| static dynamic 2 4 6 8 10 10 −3 10 −2 10 −1 10 0 10 1 10 2 10 3 10 4 Number of Agents |M| CPUTime,sec |Ω| = 2|M| static dynamic 2 4 6 8 10 10 −3 10 −2 10 −1 10 0 10 1 10 2 10 3 10 4 Number of Agents |M| CPUTime,sec |Ω| = 5|M| static dynamic 2 4 6 8 10 200 400 600 800 1000 1200 1400 1600 1800 2000 2200 Number of Agents |M| Value τ = 10|M| static dynamic 2 4 6 8 10 200 400 600 800 1000 1200 1400 1600 1800 2000 Number of Agents |M| Value |Ω| = 2|M| static dynamic 2 4 6 8 10 0 500 1000 1500 2000 2500 Number of Agents |M| Value |Ω| = 5|M| static dynamic Figure 4: Evaluation of our MILP using correlated input variables.",
                "The left column tracks the performance and CPU time as the number of agents and global-time window increase together (bτ = 10|M|).",
                "The middle and the right column track the performance and CPU time as the number of resources and the number of agents increase together as |Ω| = 2|M| and |Ω| = 5|M|, respectively.",
                "Error bars show the 1st and 3rd quartiles (25% and 75%). 1226 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) us to explore domains where the total number of agents scales proportionally to the total number of resource types or the global time horizon, while keeping constant the average agent density (per unit of global time) or the average number of resources per agent (which commonly occurs in real-life applications).",
                "Overall, we believe that these experimental results indicate that our MILP formulation can be used to effectively solve resource-scheduling problems of nontrivial size. 6.",
                "DISCUSSION AND CONCLUSIONS Throughout the paper, we have made a number of assumptions in our model and solution algorithm; we discuss their implications below. • Continual execution.",
                "We assume that once an agent stops executing its MDP (transitions into state sf ), it exits the system and cannot return.",
                "It is easy to relax this assumption for domains where agents MDPs can be paused and restarted.",
                "All that is required is to include an additional pause action which transitions from a given state back to itself, and has zero reward. • Indifference to start time.",
                "We used a reward model where agents rewards depend only on the time horizon of their MDPs and not the global start time.",
                "This is a consequence of our MDP-augmentation procedure from Section 4.1.",
                "It is easy to extend the model so that the agents incur an explicit penalty for idling by assigning a non-zero negative reward to the start state sb . • Binary resource requirements.",
                "For simplicity, we have assumed that resource costs are binary: ϕm(a, ω) = {0, 1}, but our results generalize in a straightforward manner to non-binary resource mappings, analogously to the procedure used in [5]. • Cooperative agents.",
                "The optimization procedure discussed in this paper was developed in the context of cooperative agents, but it can also be used to design a mechanism for scheduling resources among self-interested agents.",
                "This optimization procedure can be embedded in a VickreyClarke-Groves auction, completely analogously to the way it was done in [7].",
                "In fact, all the results of [7] about the properties of the auction and information privacy directly carry over to the scheduling domain discussed in this paper, requiring only slight modifications to deal with finitehorizon MDPs. • Known, deterministic arrival and departure times.",
                "Finally, we have assumed that agents arrival and departure times (τa m and τd m) are deterministic and known a priori.",
                "This assumption is fundamental to our solution method.",
                "While there are many domains where this assumption is valid, in many cases agents arrive and depart dynamically and their arrival and departure times can only be predicted probabilistically, leading to online resource-allocation problems.",
                "In particular, in the case of self-interested agents, this becomes an interesting version of an online-mechanism-design problem [11, 12].",
                "In summary, we have presented an MILP formulation for the combinatorial resource-scheduling problem where agents values for possible resource assignments are defined by finitehorizon MDPs.",
                "This result extends previous work ([6, 7]) on static one-shot resource allocation under MDP-induced preferences to resource-scheduling problems with a temporal aspect.",
                "As such, this work takes a step in the direction of designing an online mechanism for agents with combinatorial resource preferences induced by stochastic planning problems.",
                "Relaxing the assumption about deterministic arrival and departure times of the agents is a focus of our future work.",
                "We would like to thank the anonymous reviewers for their insightful comments and suggestions. 7.",
                "REFERENCES [1] E. Altman and A. Shwartz.",
                "Adaptive control of constrained Markov chains: Criteria and policies.",
                "Annals of Operations Research, special issue on Markov Decision Processes, 28:101-134, 1991. [2] R. Bellman.",
                "Dynamic Programming.",
                "Princeton University Press, 1957. [3] C. Boutilier.",
                "Solving concisely expressed combinatorial auction problems.",
                "In Proc. of AAAI-02, pages 359-366, 2002. [4] C. Boutilier and H. H. Hoos.",
                "Bidding languages for combinatorial auctions.",
                "In Proc. of IJCAI-01, pages 1211-1217, 2001. [5] D. Dolgov.",
                "Integrated Resource Allocation and Planning in Stochastic Multiagent Environments.",
                "PhD thesis, Computer Science Department, University of Michigan, February 2006. [6] D. A. Dolgov and E. H. Durfee.",
                "Optimal resource allocation and policy formulation in loosely-coupled Markov decision processes.",
                "In Proc. of ICAPS-04, pages 315-324, June 2004. [7] D. A. Dolgov and E. H. Durfee.",
                "Computationally efficient combinatorial auctions for resource allocation in weakly-coupled MDPs.",
                "In Proc. of AAMAS-05, New York, NY, USA, 2005.",
                "ACM Press. [8] D. A. Dolgov and E. H. Durfee.",
                "Resource allocation among agents with preferences induced by factored MDPs.",
                "In Proc. of AAMAS-06, 2006. [9] K. Larson and T. Sandholm.",
                "Mechanism design and deliberative agents.",
                "In Proc. of AAMAS-05, pages 650-656, New York, NY, USA, 2005.",
                "ACM Press. [10] N. Nisan.",
                "Bidding and allocation in combinatorial auctions.",
                "In Electronic Commerce, 2000. [11] D. C. Parkes and S. Singh.",
                "An MDP-based approach to Online Mechanism Design.",
                "In Proc. of the Seventeenths Annual Conference on Neural Information Processing Systems (NIPS-03), 2003. [12] D. C. Parkes, S. Singh, and D. Yanovsky.",
                "Approximately efficient online mechanism design.",
                "In Proc. of the Eighteenths Annual Conference on Neural Information Processing Systems (NIPS-04), 2004. [13] M. L. Puterman.",
                "Markov Decision Processes.",
                "John Wiley & Sons, New York, 1994. [14] M. H. Rothkopf, A. Pekec, and R. M. Harstad.",
                "Computationally manageable combinational auctions.",
                "Management Science, 44(8):1131-1147, 1998. [15] T. Sandholm.",
                "An algorithm for optimal winner determination in combinatorial auctions.",
                "In Proc. of IJCAI-99, pages 542-547, San Francisco, CA, USA, 1999.",
                "Morgan Kaufmann Publishers Inc.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1227"
            ],
            "original_annotated_samples": [
                "RESOURCE SCHEDULING Our <br>resource-scheduling algorithm</br> proceeds in two stages."
            ],
            "translated_annotated_samples": [
                "PROGRAMACIÓN DE RECURSOS Nuestro <br>algoritmo de programación de recursos</br> se lleva a cabo en dos etapas."
            ],
            "translated_text": "La programación de recursos combinatoria para MDPs multiagentes. Dmitri A. Dolgov, Michael R. James y Michael E. Samples. Grupo de Investigación en IA y Robótica, Centro Técnico de Toyota en EE. UU. {ddolgov, michael.r.james, michael.samples}@gmail.com. RESUMEN La programación óptima de recursos en sistemas multiagentes es una tarea computacionalmente desafiante, especialmente cuando los valores de los recursos no son aditivos. Consideramos el problema combinatorio de programar el uso de múltiples recursos entre agentes que operan en entornos estocásticos, modelados como procesos de decisión de Markov (MDPs). En los últimos años, se han desarrollado algoritmos eficientes de asignación de recursos para agentes con valores de recursos inducidos por MDPs. Sin embargo, este trabajo previo se ha centrado en problemas de asignación de recursos estáticos donde los recursos se distribuyen una vez y luego se utilizan en MDPs de horizonte infinito. Extendemos esos modelos existentes al problema de la programación de recursos combinatoria, donde los agentes persisten solo por períodos finitos entre sus tiempos de llegada y salida (predefinidos), requiriendo recursos solo durante esos períodos de tiempo. Proporcionamos un procedimiento computacionalmente eficiente para calcular asignaciones de recursos óptimas a nivel global a agentes a lo largo del tiempo. Ilustramos y analizamos empíricamente el método en el contexto de un dominio de programación de trabajos estocásticos. Categorías y Descriptores de Asignaturas I.2.8 [Inteligencia Artificial]: Resolución de Problemas, Métodos de Control y Búsqueda; I.2.11 [Inteligencia Artificial]: Inteligencia Artificial Distribuida-Sistemas Multiagente Términos Generales Algoritmos, Rendimiento, Diseño 1. INTRODUCCIÓN Las tareas de asignación óptima de recursos y programación son ubicuas en sistemas multiagentes, pero resolver tales problemas de optimización puede ser computacionalmente difícil, debido a una serie de factores. En particular, cuando el valor de un conjunto de recursos para un agente no es aditivo (como suele ser el caso con recursos que son sustitutos o complementos), la función de utilidad podría tener que definirse en un espacio exponencialmente grande de combinaciones de recursos, lo que rápidamente se vuelve computacionalmente intratable. Además, incluso cuando cada agente tiene una función de utilidad que es distinta de cero solo en un pequeño subconjunto de los posibles conjuntos de recursos, obtener una asignación óptima sigue siendo computacionalmente prohibitivo, ya que el problema se vuelve NP-completo [14]. Tales problemas computacionales han dado lugar recientemente a varias líneas de trabajo en el uso de modelos compactos de las preferencias de los agentes. Una idea es utilizar cualquier estructura presente en las funciones de utilidad para representarlas de forma compacta, por ejemplo, a través de fórmulas lógicas [15, 10, 4, 3]. Una alternativa es modelar directamente los mecanismos que definen las funciones de utilidad de los agentes y realizar la asignación de recursos directamente con estos modelos [9]. Una forma de lograr esto es modelar los procesos mediante los cuales un agente podría utilizar los recursos y definir la función de utilidad como la ganancia de estos procesos. En particular, si un agente utiliza recursos para actuar en un entorno estocástico, su función de utilidad puede ser modelada de forma natural con un proceso de decisión de Markov, cuyo conjunto de acciones está parametrizado por los recursos disponibles. Esta representación puede ser utilizada para construir algoritmos de asignación de recursos muy eficientes que resultan en una aceleración exponencial sobre un problema de optimización directa con representaciones planas de preferencias combinatorias [6, 7, 8]. Sin embargo, este trabajo existente sobre asignación de recursos con preferencias inducidas por MDPs parametrizados por recursos hace una suposición de que los recursos se asignan solo una vez y luego son utilizados por los agentes de forma independiente dentro de sus MDPs de horizonte infinito. Esta suposición de que no es posible la reasignación de recursos puede ser limitante en dominios donde los agentes llegan y se van dinámicamente. En este artículo, ampliamos el trabajo sobre asignación de recursos bajo preferencias inducidas por MDP a problemas de programación en tiempo discreto, donde los agentes están presentes en el sistema durante intervalos de tiempo finitos y solo pueden utilizar recursos dentro de estos intervalos. En particular, los agentes llegan y parten en momentos arbitrarios (predefinidos) y dentro de estos intervalos utilizan recursos para ejecutar tareas en MDPs de horizonte finito. Abordamos el problema de la programación de recursos óptima a nivel global, donde el objetivo es encontrar una asignación de recursos a los agentes a lo largo del tiempo que maximice la suma de las recompensas esperadas que obtienen. En este contexto, nuestra principal contribución es una formulación de programación entera mixta del problema de programación que elige asignaciones de recursos globalmente óptimas, tiempos de inicio y horizontes de ejecución para todos los agentes (dentro de sus intervalos de llegada y salida). Analizamos y comparamos empíricamente dos variantes del problema de programación: uno, donde los agentes tienen asignaciones de recursos estáticas dentro de sus MDPs de horizonte finito, y otro, donde los recursos pueden ser realocados dinámicamente entre agentes en cada paso de tiempo. En el resto del documento, primero establecemos las bases necesarias en la Sección 2 y luego presentamos nuestro modelo y declaración formal del problema en la Sección 3. En la Sección 4.2, describimos nuestro resultado principal, el programa de optimización para la programación de recursos globalmente óptima. Tras la discusión de nuestros resultados experimentales sobre un problema de programación de trabajos en la Sección 5, concluimos en la Sección 6 con una discusión de posibles extensiones y generalizaciones de nuestro método. De manera similar al modelo utilizado en trabajos anteriores sobre asignación de recursos con preferencias inducidas por MDP [6, 7], definimos el valor de un conjunto de recursos para un agente como el valor de la mejor política MDP que es realizable, dadas esos recursos. Sin embargo, dado que el enfoque de nuestro trabajo se centra en problemas de programación, y una gran parte del problema de optimización consiste en decidir cómo se asignan los recursos en el tiempo entre agentes con tiempos finitos de llegada y salida, modelamos los problemas de planificación de los agentes como MDPs de horizonte finito, en contraste con trabajos anteriores que utilizaron MDPs descontados de horizonte infinito. En el resto de esta sección, primero presentamos algo de antecedentes necesarios sobre MDPs de horizonte finito y presentamos una formulación de programación lineal que sirve como base para nuestro algoritmo de solución desarrollado en la Sección 4. También describimos los métodos estándar para la programación de recursos combinatorios con valores de recursos planos, que sirven como punto de referencia de comparación para el nuevo modelo desarrollado aquí. 2.1 Procesos de Decisión de Markov Un MDP (Proceso de Decisión de Markov) estacionario, de dominio finito y discreto en el tiempo (ver, por ejemplo, [13] para un desarrollo detallado y exhaustivo) se puede describir como S, A, p, r, donde: S es un conjunto finito de estados del sistema; A es un conjunto finito de acciones disponibles para el agente; p es una función de transición estocástica estacionaria, donde p(σ|s, a) es la probabilidad de transicionar al estado σ al ejecutar la acción a en el estado s; r es una función de recompensa estacionaria, donde r(s, a) especifica la recompensa obtenida al ejecutar la acción a en el estado s. Dado un MDP de este tipo, un problema de decisión bajo un horizonte finito T consiste en elegir una acción óptima en cada paso de tiempo para maximizar el valor esperado de la recompensa total acumulada durante la vida (finita) de los agentes. La política óptima del agente es entonces una función del estado actual s y el tiempo hasta el horizonte. Una política óptima para tal problema es actuar de forma codiciosa con respecto a la función de valor óptimo, definida de forma recursiva por el siguiente sistema de ecuaciones de Bellman de tiempo finito [2]: v(s, t) = max a r(s, a) + X σ p(σ|s, a)v(σ, t + 1), ∀s ∈ S, t ∈ [1, T − 1]; v(s, T) = 0, ∀s ∈ S; donde v(s, t) es el valor óptimo de estar en el estado s en el tiempo t ∈ [1, T]. Esta función de valor óptimo se puede calcular fácilmente utilizando programación dinámica, lo que conduce a la siguiente política óptima π, donde π(s, a, t) es la probabilidad de ejecutar la acción a en el estado s en el tiempo t: π(s, a, t) = (1, a = argmaxa r(s, a) + P σ p(σ|s, a)v(σ, t + 1), 0, en caso contrario. La anterior es la forma más común de calcular la función de valor óptimo (y por lo tanto una política óptima) para un MDP de horizonte finito. Sin embargo, también podemos formular el problema como el siguiente programa lineal (similarmente al LP dual para MDPs descontados de horizonte infinito [13, 6, 7]): max X s X a r(s, a) X t x(s, a, t) sujeto a: X a x(σ, a, t + 1) = X s,a p(σ|s, a)x(s, a, t) ∀σ, t ∈ [1, T − 1]; X a x(s, a, 1) = α(s), ∀s ∈ S; (1) donde α(s) es la distribución inicial sobre el espacio de estados, y x es la medida de ocupación (no estacionaria) (x(s, a, t) ∈ [0, 1] es el número total esperado de veces que se ejecuta la acción a en el estado s en el tiempo t). Una política óptima (no estacionaria) se obtiene a partir de la medida de ocupación de la siguiente manera: π(s, a, t) = x(s, a, t)/ X a x(s, a, t) ∀s ∈ S, t ∈ [1, T]. (2) Tenga en cuenta que el MDP estándar de horizonte finito sin restricciones, como se describe anteriormente, siempre tiene una solución uniformemente óptima (óptima para cualquier distribución inicial α(s)). Por lo tanto, una política óptima se puede obtener utilizando una constante arbitraria α(s) > 0 (en particular, α(s) = 1 resultará en x(s, a, t) = π(s, a, t)). Sin embargo, para los MDPs con restricciones de recursos (como se definen a continuación en la Sección 3), las políticas óptimas uniformemente no existen en general. En tales casos, α se convierte en parte de la entrada del problema, y una política resultante solo es óptima para ese α en particular. Este resultado es bien conocido para MDPs de horizonte infinito con varios tipos de restricciones [1, 6], y también se cumple para nuestro modelo de horizonte finito, el cual puede establecerse fácilmente a través de una línea de razonamiento completamente análoga a los argumentos en [6]. 2.2 Programación de Recursos Combinatoria Un enfoque directo para la programación de recursos para un conjunto de agentes M, cuyos valores para los recursos son inducidos por problemas de planificación estocástica (en nuestro caso, MDPs de horizonte finito) sería que cada agente enumerara todas las asignaciones de recursos posibles a lo largo del tiempo y, para cada una, calcular su valor resolviendo el MDP correspondiente. Entonces, cada agente proporcionaría valoraciones para cada posible conjunto de recursos a lo largo del tiempo a un coordinador centralizado, quien calcularía las asignaciones óptimas de recursos a lo largo del tiempo basándose en estas valoraciones. Cuando los recursos pueden asignarse en diferentes momentos a diferentes agentes, cada agente debe presentar valoraciones para cada combinación de posibles horizontes temporales. Que cada agente m ∈ M ejecute su MDP dentro del intervalo de tiempo de llegada-salida τ ∈ [τa m, τd m]. Por lo tanto, el agente m ejecutará un MDP con un horizonte temporal no mayor que Tm = τd m−τa m+1. Que bτ sea el horizonte temporal global para el problema, antes del cual todos los MDP de los agentes deben finalizar. Suponemos que τd m < bτ, ∀m ∈ M. El Sexto Congreso Internacional. En el problema de programación donde los agentes tienen requisitos de recursos estáticos dentro de sus MDPs de horizonte finito, los agentes proporcionan una valoración para cada conjunto de recursos para cada horizonte de tiempo posible (desde [1, Tm]) que puedan utilizar. Sea Ω el conjunto de recursos a ser asignados entre los agentes. Un agente recibirá como máximo un paquete de recursos para uno de los horizontes temporales. Que la variable ψ ∈ Ψm enumere todos los posibles pares de conjuntos de recursos y horizontes temporales para el agente m, por lo que hay 2|Ω| × Tm valores para ψ (el espacio de conjuntos es exponencial en el número de tipos de recursos |Ω|). El agente m debe proporcionar un valor vψ m para cada ψ, y el coordinador asignará como máximo un par (recurso, horizonte temporal) ψ a cada agente. Esta asignación se expresa como una variable indicadora zψ m ∈ {0, 1} que muestra si ψ está asignado al agente m. Para el tiempo τ y el recurso ω, la función nm(ψ, τ, ω) ∈ {0, 1} indica si el paquete en ψ utiliza el recurso ω en el tiempo τ (hacemos la suposición de que los agentes tienen requisitos de recursos binarios). Este problema de asignación es NP-completo, incluso al considerar solo un paso de tiempo, y su dificultad aumenta significativamente con múltiples pasos de tiempo debido al creciente número de valores de ψ. El problema de encontrar una asignación óptima que satisfaga la restricción global de que la cantidad de cada recurso ω asignada a todos los agentes no exceda la cantidad disponible bϕ(ω) se puede expresar como el siguiente programa entero: max X m∈M X ψ∈Ψm zψ mvψ m sujeto a: X ψ∈Ψm zψ m ≤ 1, ∀m ∈ M; X m∈M X ψ∈Ψm zψ mnm(ψ, τ, ω) ≤ bϕ(ω), ∀τ ∈ [1, bτ], ∀ω ∈ Ω; (3) La primera restricción en la ecuación 3 dice que ningún agente puede recibir más de un paquete, y la segunda restricción asegura que la asignación total del recurso ω no exceda, en ningún momento, el límite del recurso. Para el problema de programación en el que los agentes pueden reasignar dinámicamente recursos, cada agente debe especificar un valor para cada combinación de paquetes y pasos de tiempo dentro de su horizonte temporal. Que la variable ψ ∈ Ψm en este caso enumere todos los posibles conjuntos de recursos para los cuales a lo sumo un conjunto puede ser asignado al agente m en cada paso de tiempo. Por lo tanto, en este caso hay P t∈[1,Tm](2|Ω| )t ∼ 2|Ω|Tm posibilidades de combinaciones de recursos asignadas a diferentes intervalos de tiempo, para los Tm horizontes de tiempo diferentes. El mismo conjunto de ecuaciones (3) se puede utilizar para resolver este problema de programación dinámica, pero el programa entero es diferente debido a la diferencia en cómo se define ψ. En este caso, el número de valores de ψ es exponencial en el horizonte de planificación Tm de cada agente, lo que resulta en un programa mucho más grande. Este enfoque directo para resolver ambos problemas de programación requiere una enumeración y solución de 2|Ω| Tm (asignación estática) o P t∈[1,Tm] 2|Ω|t (reubicación dinámica) MDPs para cada agente, lo cual rápidamente se vuelve intratable con el crecimiento del número de recursos |Ω| o el horizonte de tiempo Tm. MODELO Y DECLARACIÓN DEL PROBLEMA Ahora presentamos formalmente nuestro modelo del problema de programación de recursos. La entrada del problema consiste en los siguientes componentes: • M, Ω, bϕ, τa m, τd m, bτ están definidos arriba en la Sección 2.2. • {Θm} = {S, A, pm, rm, αm} son los MDPs de todos los agentes m ∈ M. Sin pérdida de generalidad, asumimos que los espacios de estados y acciones de todos los agentes son iguales, pero cada uno tiene su propia función de transición pm, función de recompensa rm, y condiciones iniciales αm. • ϕm : A×Ω → {0, 1} es la asignación de acciones a recursos para el agente m. ϕm(a, ω) indica si la acción a del agente m necesita el recurso ω. Un agente m que recibe un conjunto de recursos que no incluye el recurso ω no puede ejecutar en su política MDP ninguna acción a para la cual ϕm(a, ω) = 0. Suponemos que todos los requisitos de recursos son binarios; como se discute más adelante en la Sección 6, esta suposición no es limitante. Dado el input anterior, el problema de optimización que consideramos es encontrar la asignación globalmente óptima que maximice la suma de recompensas esperadas de recursos a agentes para todos los pasos de tiempo: Δ : τ × M × Ω → {0, 1}. Una solución es factible si la asignación correspondiente de recursos a los agentes no viola la restricción global de recursos: X m Δm(τ, ω) ≤ bϕ(ω), ∀ω ∈ Ω, τ ∈ [1, bτ]. Consideramos dos variantes del problema de programación de recursos. La primera formulación restringe las asignaciones de recursos al espacio donde la asignación de recursos a cada agente es estática durante la vida de los agentes. La segunda formulación permite la reasignación de recursos entre agentes en cada paso de tiempo durante sus vidas. La Figura 1 representa un problema de programación de recursos con tres agentes M = {m1, m2, m3}, tres recursos Ω = {ω1, ω2, ω3}, y un horizonte de problema global de bτ = 11. Los tiempos de llegada y salida de los agentes se muestran como cajas grises y son {1, 6}, {3, 7} y {2, 11}, respectivamente. Una solución a este problema se muestra a través de barras horizontales dentro de cada casilla de agentes, donde las barras corresponden a la asignación de los tres tipos de recursos. La Figura 1a muestra una solución a un problema de programación estática. Según la solución mostrada, el agente m1 comienza la ejecución de su MDP en el tiempo τ = 1 y tiene un bloqueo en los tres recursos hasta que finaliza la ejecución en el tiempo τ = 3. Se debe tener en cuenta que el agente m1 renuncia a su control sobre los recursos antes de su hora de salida anunciada de τd m1 = 6, aparentemente porque otros agentes pueden utilizar los recursos de manera más efectiva. Por lo tanto, en el tiempo τ = 4, los recursos ω1 y ω3 se asignan al agente m2, quien luego los utiliza para ejecutar su MDP (utilizando solo acciones admitidas por los recursos ω1 y ω3) hasta el tiempo τ = 7. El agente m3 posee el recurso ω3 durante el intervalo τ ∈ [4, 10]. La Figura 1b muestra una posible solución a la versión dinámica del mismo problema. Allí, los recursos pueden ser reasignados entre agentes en cada paso de tiempo. Por ejemplo, el agente m1 renuncia a su uso del recurso ω2 en el tiempo τ = 2, aunque continúa la ejecución de su MDP hasta el tiempo τ = 6. Ten en cuenta que a un agente no se le permite detener y reiniciar su MDP, por lo que el agente m1 solo puede continuar ejecutando en el intervalo τ ∈ [3, 4] si tiene acciones que no requieren recursos (ϕm(a, ω) = 0). Claramente, el modelo y la declaración del problema descritos anteriormente hacen una serie de suposiciones sobre el problema y las propiedades de la solución deseada. Discutimos algunas de esas suposiciones y sus implicaciones en la Sección 6. 1222 La Sexta Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) (a) (b) Figura 1: Ilustración de una solución a un problema de programación de recursos con tres agentes y tres recursos: a) asignaciones de recursos estáticas (las asignaciones de recursos son constantes durante la vida de los agentes); b) asignación dinámica (las asignaciones de recursos pueden cambiar en cada paso de tiempo). 4. PROGRAMACIÓN DE RECURSOS Nuestro <br>algoritmo de programación de recursos</br> se lleva a cabo en dos etapas. Primero, realizamos un paso de preprocesamiento que aumenta los MDP de los agentes; este proceso se describe en la Sección 4.1. Segundo, utilizando estos MDPs aumentados construimos un problema de optimización global, el cual se describe en la Sección 4.2. 4.1 Aumentando los MDPs de los Agentes En el modelo descrito en la sección anterior, asumimos que si un agente no posee los recursos necesarios para realizar acciones en su MDP, su ejecución se detiene y el agente abandona el sistema. En otras palabras, los MDPs no pueden ser pausados y reanudados. Por ejemplo, en el problema mostrado en la Figura 1a, el agente m1 libera todos los recursos después del tiempo τ = 3, momento en el cual la ejecución de su MDP se detiene. De manera similar, los agentes m2 y m3 solo ejecutan sus MDPs en los intervalos τ ∈ [4, 6] y τ ∈ [4, 10], respectivamente. Por lo tanto, una parte importante del problema de toma de decisiones global es decidir la ventana de tiempo durante la cual cada uno de los agentes está activo (es decir, ejecutando su MDP). Para lograr esto, ampliamos el MDP de cada agente con dos nuevos estados (estados de inicio y final sb, sf, respectivamente) y una nueva acción de inicio/parada a∗, como se ilustra en la Figura 2. La idea es que un agente permanezca en el estado inicial sb hasta que esté listo para ejecutar su MDP, momento en el que realiza la acción de inicio/parada a∗ y transita al espacio de estados del MDP original con la probabilidad de transición que corresponde a la distribución inicial original α(s). Por ejemplo, en la Figura 1a, para el agente m2 esto ocurriría en el tiempo τ = 4. Una vez que el agente llega al final de su ventana de actividad (tiempo τ = 6 para el agente m2 en la Figura 1a), realiza la acción de inicio/parada, lo que lo lleva al estado final de finalización del sumidero sf en el tiempo τ = 7. Más precisamente, dado un MDP S, A, pm, rm, αm, definimos un MDP aumentado S, A, pm, rm, αm de la siguiente manera: S = S ∪ sb ∪ sf; A = A ∪ a∗; p(s|sb, a∗) = α(s), ∀s ∈ S; p(sb|sb, a) = 1.0, ∀a ∈ A; p(sf|s, a∗) = 1.0, ∀s ∈ S; p(σ|s, a) = p(σ|s, a), ∀s, σ ∈ S, a ∈ A; r(sb, a) = r(sf, a) = 0, ∀a ∈ A; r(s, a) = r(s, a), ∀s ∈ S, a ∈ A; α(sb) = 1; α(s) = 0, ∀s ∈ S; donde se asume que todas las probabilidades de transición no especificadas son cero. Además, para tener en cuenta el nuevo estado inicial, comenzamos el MDP un paso de tiempo antes, estableciendo τa m ← τa m − 1. Esto no afectará la asignación de recursos debido a que las restricciones de recursos solo se aplican a los estados originales del MDP, como se discutirá en la siguiente sección. Por ejemplo, los MDPs aumentados mostrados en la Figura 2b (que comienza en el estado sb en el tiempo τ = 2) se construirían a partir de un MDP con tiempo de llegada original τ = 3. La Figura 2b también muestra una trayectoria de muestra a través del espacio de estados: el agente comienza en el estado sb, transita al espacio de estados S del MDP original y finalmente sale al estado de sumidero sf. Cabe destacar que si quisiéramos modelar un problema en el que los agentes pudieran pausar sus MDP en pasos de tiempo arbitrarios (lo cual podría ser útil para dominios donde la reasignación dinámica es posible), podríamos lograrlo fácilmente incluyendo una acción adicional que transiciona de cada estado a sí mismo con una recompensa de cero. 4.2 MILP para la programación de recursos Dado un conjunto de MDPs aumentados, como se define arriba, el objetivo de esta sección es formular un programa de optimización global que resuelva el problema de programación de recursos. En esta sección y a continuación, se asume que todos los MDPs son los MDPs aumentados tal como se definen en la Sección 4.1. Nuestro enfoque es similar a la idea utilizada en [6]: comenzamos con la formulación del programa lineal de los MDP de los agentes (1) y lo complementamos con restricciones que aseguran que la asignación de recursos correspondiente entre agentes y tiempo sea válida. El problema de optimización resultante resuelve simultáneamente los MDP de los agentes y los problemas de programación de recursos. En el resto de esta sección, desarrollamos de forma incremental un programa de enteros mixtos (MILP) que logra esto. En ausencia de restricciones de recursos, los MDPs de horizonte finito de los agentes son completamente independientes, y la solución óptima global puede obtenerse trivialmente a través del siguiente LP, que es simplemente una agregación de LPs de horizonte finito de un solo agente: max X m X s X a rm(s, a) X t xm(s, a, t) sujeto a: X a xm(σ, a, t + 1) = X s,a pm(σ|s, a)xm(s, a, t), ∀m ∈ M, σ ∈ S, t ∈ [1, Tm − 1]; X a xm(s, a, 1) = αm(s), ∀m ∈ M, s ∈ S; (12) donde xm(s, a, t) es la medida de ocupación del agente m, y The Sixth Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 1223 (a) (b) Figura 2: Ilustración de la ampliación de un MDP para permitir tiempos de inicio y finalización variables: a) (izquierda) el MDP original de dos estados con una sola acción; (derecha) el MDP ampliado con nuevos estados sb y sf y la nueva acción a∗ (nota que las transiciones originales no se modifican en el proceso de ampliación); b) el MDP ampl augmentado mostrado como una trayectoria a través del tiempo (las líneas grises indican todas las transiciones, mientras que las líneas negras indican una trayectoria dada). Función objetivo (suma de recompensas esperadas de todos los agentes) máx X m X s X a rm(s, a) X t xm(s, a, t) (5) Implicación de significado Restricciones lineales Atar x a θ. El agente solo está activo cuando la medida de ocupación es distinta de cero en los estados originales del MDP. θm(τ) = 0 =⇒ xm(s, a, τ −τa m+1) = 0 ∀s /∈ {sb , sf }, a ∈ A X s/∈{sb,sf } X a xm(s, a, t) ≤ θm(τa m + t − 1) ∀m ∈ M, ∀t ∈ [1, Tm] (6) El agente solo puede estar activo en τ ∈ (τa m, τd m) θm(τ) = 0 ∀m ∈ M, τ /∈ (τa m, τd m) (7) No se pueden utilizar recursos cuando no está activo θm(τ) = 0 =⇒ Δm(τ, ω) = 0 ∀τ ∈ [0, bτ], ω ∈ Ω Δm(τ, ω) ≤ θm(τ) ∀m ∈ M, τ ∈ [0, bτ], ω ∈ Ω (8) Vincular x a Δ (x distinto de cero obliga a que el correspondiente Δ sea distinto de cero). Δm(τ, ω) = 0, ϕm(a, ω) = 1 =⇒ xm(s, a, τ − τa m + 1) = 0 ∀s /∈ {sb , sf } 1/|A| X a ϕm(a, ω) X s/∈{sb,sf } xm(s, a, t) ≤ Δm(t + τa m − 1, ω) ∀m ∈ M, ω ∈ Ω, t ∈ [1, Tm] (9) Límites de recursos X m Δm(τ, ω) ≤ bϕ(ω) ∀ω ∈ Ω, τ ∈ [0, bτ] (10) El agente no puede cambiar los recursos mientras está activo. Solo habilitado para programación con asignaciones estáticas. θm(τ) = 1 y θm(τ + 1) = 1 =⇒ Δm(τ, ω) = Δm(τ + 1, ω) Δm(τ, ω) − Z(1 − θm(τ + 1)) ≤ Δm(τ + 1, ω) + Z(1 − θm(τ)) Δm(τ, ω) + Z(1 − θm(τ + 1)) ≥ Δm(τ + 1, ω) − Z(1 − θm(τ)) ∀m ∈ M, ω ∈ Ω, τ ∈ [0, bτ] (11) Tabla 1: MILP para programación óptima global de recursos. Tm = τd m − τa m + 1 es el horizonte temporal para los agentes MDP. Usando este LP como base, lo ampliamos con restricciones que aseguran que el uso de recursos implicado por las medidas de ocupación de los agentes {xm} no viola los requisitos globales de recursos bϕ en ningún paso de tiempo τ ∈ [0, bτ]. Para formular estas restricciones de recursos, utilizamos las siguientes variables binarias: • Δm(τ, ω) = {0, 1}, ∀m ∈ M, τ ∈ [0, bτ], ω ∈ Ω, que sirven como variables indicadoras que definen si el agente m posee el recurso ω en el tiempo τ. Estos son análogos a las variables indicadoras estáticas utilizadas en el problema de asignación de recursos estáticos de una sola vez en [6]. • θm = {0, 1}, ∀m ∈ M, τ ∈ [0, bτ] son variables indicadoras que especifican si el agente m está activo (es decir, ejecutando su MDP) en el tiempo τ. El significado de las variables de uso de recursos Δ se ilustra en la Figura 1: Δm(τ, ω) = 1 solo si el recurso ω se asigna al agente m en el tiempo τ. El significado de los indicadores de actividad θ se ilustra en la Figura 2b: cuando el agente m se encuentra en el estado inicial sb o en el estado final sf, el θm correspondiente es igual a 0, pero una vez que el agente se vuelve activo y entra en uno de los otros estados, establecemos θm = 1. Este significado de θ puede ser impuesto con una restricción lineal que sincroniza los valores de las medidas de ocupación de los agentes xm y la actividad 1224 The Sixth Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) indicadores θ, como se muestra en (6) en la Tabla 1. Otra restricción que debemos agregar, debido a que los indicadores de actividad θ están definidos en la línea de tiempo global τ, es hacer cumplir el hecho de que el agente está inactivo fuera de su ventana de llegada y salida. Esto se logra mediante la restricción (7) en la Tabla 1. Además, los agentes no deben estar utilizando recursos mientras están inactivos. Esta restricción también puede ser impuesta a través de una desigualdad lineal en θ y Δ, como se muestra en (8). La restricción (6) establece el valor de θ para que coincida con la política definida por la medida de ocupación xm. De manera similar, debemos asegurarnos de que las variables de uso de recursos Δ estén también sincronizadas con la medida de ocupación xm. Esto se realiza a través de la restricción (9) en la Tabla 1, que es casi idéntica a la restricción análoga de [6]. Después de implementar la restricción anterior, que refuerza el significado de Δ, agregamos una restricción que garantiza que el uso de recursos de los agentes nunca exceda los montos de recursos disponibles. Esta condición también se expresa trivialmente como una desigualdad lineal (10) en la Tabla 1. Finalmente, para la formulación del problema en la que las asignaciones de recursos son estáticas durante la vida de un agente, agregamos una restricción que garantiza que las variables de uso de recursos Δ no cambien su valor mientras el agente está activo (θ = 1). Esto se logra a través de la restricción lineal (11), donde Z ≥ 2 es una constante que se utiliza para desactivar las restricciones cuando θm(τ) = 0 o θm(τ + 1) = 0. Esta restricción no se utiliza para la formulación del problema dinámico, donde los recursos pueden ser reasignados entre agentes en cada paso de tiempo. En resumen, la Tabla 1 junto con las restricciones de conservación de flujo de (12) define el MILP que calcula simultáneamente una asignación óptima de recursos para todos los agentes a lo largo del tiempo, así como políticas óptimas de MDP de horizonte finito que son válidas bajo esa asignación de recursos. Como una medida aproximada de la complejidad de este MILP, consideremos el número de variables de optimización y restricciones. Sea TM = P Tm = P m(τa m − τd m + 1) la suma de las longitudes de las ventanas de llegada-salida a través de todos los agentes. Entonces, el número de variables de optimización es: TM + bτ|M||Ω| + bτ|M|, de las cuales TM son continuas (xm), y bτ|M||Ω| + bτ|M| son binarias (Δ y θ). Sin embargo, hay que tener en cuenta que todos menos TM|M| de los θ se establecen en cero por la restricción (7), lo que también obliga inmediatamente a que todos menos TM|M||Ω| de los Δ sean cero a través de las restricciones (8). El número de restricciones (sin incluir las restricciones degeneradas en (7)) en el MILP es: TM + TM|Ω| + bτ|Ω| + bτ|M||Ω|. A pesar de que la complejidad del MILP es, en el peor de los casos, exponencial en el número de variables binarias, la complejidad de este MILP es significativamente (exponencialmente) menor que la del MILP con funciones de utilidad planas, descrito en la Sección 2.2. Este resultado refleja las ganancias de eficiencia reportadas en [6] para problemas de asignación de recursos de un solo disparo, pero es mucho más pronunciado, debido a la explosión de la representación de utilidad plana debido al aspecto temporal del problema (recuerde la complejidad prohibitiva de la optimización combinatoria en la Sección 2.2). Analizamos empíricamente el rendimiento de este método en la Sección 5.1. Estrictamente hablando, resolver MILPs hasta la optimalidad es NP-completo en el número de variables enteras. RESULTADOS EXPERIMENTALES Aunque la complejidad de resolver MILPs es en el peor de los casos exponencial en el número de variables enteras, existen muchos métodos eficientes para resolver MILPs que permiten que nuestro algoritmo se adapte bien a parámetros comunes en problemas de asignación de recursos y programación. En particular, esta sección introduce un dominio de problema: el problema del taller de reparación, que se utiliza para evaluar empíricamente la escalabilidad de nuestros algoritmos en términos del número de agentes |M|, el número de recursos compartidos |Ω| y las diferentes longitudes de tiempo global bτ durante las cuales los agentes pueden entrar y salir del sistema. El problema del taller de reparaciones es un MDP parametrizado simple que adopta la metáfora de un taller de reparaciones de vehículos. Los agentes en el taller de reparación son mecánicos con una serie de tareas independientes que solo generan recompensa cuando se completan. En nuestro modelo MDP de este sistema, las acciones tomadas para avanzar a través del espacio de estados solo están permitidas si el agente posee ciertos recursos que están públicamente disponibles para la tienda. Estos recursos son de suministro finito, y las políticas óptimas para la tienda determinarán cuándo cada agente puede retener los recursos limitados para tomar acciones y obtener recompensas individuales. Cada tarea a completar está asociada con una sola acción, aunque se requiere que el agente repita la acción numerosas veces antes de completar la tarea y obtener una recompensa. Este modelo fue parametrizado en términos del número de agentes en el sistema, el número de diferentes tipos de recursos que podrían estar vinculados a acciones necesarias, un tiempo global durante el cual se permite a los agentes llegar y partir, y una longitud máxima para el número de pasos de tiempo que un agente puede permanecer en el sistema. Todos los puntos de datos en nuestros experimentos se obtuvieron con 20 evaluaciones utilizando CPLEX para resolver los MILPs en una computadora Pentium4 con 2Gb de RAM. Se realizaron pruebas tanto en la versión estática como en la dinámica del problema de programación de recursos, tal como se definió anteriormente. La Figura 3 muestra el tiempo de ejecución y el valor de la política para modificaciones independientes en el conjunto de parámetros. La fila superior muestra cómo el tiempo de solución para el MILP escala a medida que aumentamos el número de agentes |M|, el horizonte de tiempo global bτ y el número de recursos |Ω|. Aumentar el número de agentes conduce a una escalabilidad exponencial en complejidad, lo cual es de esperarse para un problema NP-completo. Sin embargo, aumentar el límite de tiempo global bτ o el número total de tipos de recursos |Ω|, manteniendo constante el número de agentes, no conduce a una disminución en el rendimiento. Esto ocurre porque los problemas se vuelven más fáciles a medida que se vuelven sub-determinados, lo cual también es un fenómeno común en problemas NP-completos. También observamos que la solución a la versión dinámica del problema a menudo se puede calcular mucho más rápido que la versión estática. La fila inferior de la Figura 3 muestra el valor de la política conjunta de las políticas que corresponden a los horarios óptimos de asignación de recursos calculados. Podemos observar que la versión dinámica produce una recompensa más alta (como era de esperar, ya que la recompensa de la versión dinámica siempre es igual o mayor que la recompensa de la versión estática). Deberíamos señalar que estos gráficos no deben ser vistos como una medida del rendimiento de dos algoritmos diferentes (ambos algoritmos producen soluciones óptimas pero para problemas distintos), sino más bien como observaciones sobre cómo cambia la calidad de las soluciones óptimas a medida que se permite más flexibilidad en la reasignación de recursos. La Figura 4 muestra el tiempo de ejecución y el valor de la política para las pruebas en las que las variables de entrada comunes se escalan juntas. Esto permite la Sexta Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 1225 2 4 6 8 10 10 −3 10 −2 10 −1 10 0 10 1 10 2 10 3 10 4 Número de Agentes |M| Tiempo de CPU, seg |Ω| = 5, τ = 50 estático dinámico 50 100 150 200 10 −2 10 −1 10 0 10 1 10 2 10 3 Límite de Tiempo Global τ Tiempo de CPU, seg |M| = 5, |Ω| = 5 estático dinámico 10 20 30 40 50 10 −2 10 −1 10 0 10 1 10 2 Número de Recursos |Ω| Tiempo de CPU, seg |M| = 5, τ = 50 estático dinámico 2 4 6 8 10 200 400 600 800 1000 1200 1400 1600 Número de Agentes |M| Valor |Ω| = 5, τ = 50 estático dinámico 50 100 150 200 400 500 600 700 800 900 1000 1100 1200 1300 1400 Límite de Tiempo Global τ Valor |M| = 5, |Ω| = 5 estático dinámico 10 20 30 40 50 500 600 700 800 900 1000 1100 1200 1300 1400 Número de Recursos |Ω| Valor |M| = 5, τ = 50 estático dinámico Figura 3: Evaluación de nuestro MILP para números variables de agentes (columna 1), longitudes de ventana de tiempo global (columna 2) y números de tipos de recursos (columna 3). La fila superior muestra el tiempo de CPU, y la fila inferior muestra la recompensa conjunta de las políticas MDP de los agentes. Las barras de error muestran los cuartiles 1ro y 3ro (25% y 75%). 2 4 6 8 10 10 −3 10 −2 10 −1 10 0 10 1 10 2 10 3 Número de Agentes |M| Tiempo de CPU, seg τ = 10|M| estático dinámico 2 4 6 8 10 10 −3 10 −2 10 −1 10 0 10 1 10 2 10 3 10 4 Número de Agentes |M| Tiempo de CPU, seg |Ω| = 2|M| estático dinámico 2 4 6 8 10 10 −3 10 −2 10 −1 10 0 10 1 10 2 10 3 10 4 Número de Agentes |M| Tiempo de CPU, seg |Ω| = 5|M| estático dinámico 2 4 6 8 10 200 400 600 800 1000 1200 1400 1600 1800 2000 2200 Número de Agentes |M| Valor τ = 10|M| estático dinámico 2 4 6 8 10 200 400 600 800 1000 1200 1400 1600 1800 2000 Número de Agentes |M| Valor |Ω| = 2|M| estático dinámico 2 4 6 8 10 0 500 1000 1500 2000 2500 Número de Agentes |M| Valor |Ω| = 5|M| estático dinámico Figura 4: Evaluación de nuestro MILP utilizando variables de entrada correlacionadas. La columna izquierda sigue el rendimiento y el tiempo de CPU a medida que aumenta el número de agentes y la ventana de tiempo global juntos (bτ = 10|M|). La columna del medio y la columna de la derecha siguen el rendimiento y el tiempo de CPU a medida que aumenta el número de recursos y el número de agentes juntos, con |Ω| = 2|M| y |Ω| = 5|M|, respectivamente. Las barras de error muestran el primer y tercer cuartil (25% y 75%). 1226 The Sixth Intl. La Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) nos permite explorar dominios donde el número total de agentes escala proporcionalmente al número total de tipos de recursos o al horizonte temporal global, manteniendo constante la densidad promedio de agentes (por unidad de tiempo global) o el número promedio de recursos por agente (lo cual comúnmente ocurre en aplicaciones de la vida real). En general, creemos que estos resultados experimentales indican que nuestra formulación MILP puede ser utilizada para resolver de manera efectiva problemas de programación de recursos de tamaño no trivial. DISCUSIÓN Y CONCLUSIONES A lo largo del documento, hemos realizado una serie de suposiciones en nuestro modelo y algoritmo de solución; discutimos sus implicaciones a continuación. • Ejecución continua. Suponemos que una vez que un agente deja de ejecutar su MDP (transiciona al estado sf), sale del sistema y no puede regresar. Es fácil relajar esta suposición para dominios donde los agentes de MDP pueden ser pausados y reiniciados. Todo lo que se requiere es incluir una acción de pausa adicional que haga la transición desde un estado dado de regreso a sí mismo, y que tenga una recompensa de cero. • Indiferencia al tiempo de inicio. Utilizamos un modelo de recompensa donde las recompensas de los agentes dependen solo del horizonte temporal de sus MDP y no del tiempo de inicio global. Esta es una consecuencia de nuestro procedimiento de aumento de MDP de la Sección 4.1. Es fácil extender el modelo para que los agentes incurran en una penalización explícita por inactividad al asignar una recompensa negativa no nula al estado inicial sb. • Requisitos de recursos binarios. Para simplificar, hemos asumido que los costos de recursos son binarios: ϕm(a, ω) = {0, 1}, pero nuestros resultados se generalizan de manera directa a asignaciones de recursos no binarias, de manera análoga al procedimiento utilizado en [5]. • Agentes cooperativos. El procedimiento de optimización discutido en este artículo fue desarrollado en el contexto de agentes cooperativos, pero también puede ser utilizado para diseñar un mecanismo para programar recursos entre agentes egoístas. Este procedimiento de optimización puede ser incorporado en una subasta de Vickrey-Clarke-Groves, de manera completamente análoga a como se hizo en [7]. De hecho, todos los resultados de [7] sobre las propiedades de la subasta y la privacidad de la información se aplican directamente al dominio de programación discutido en este documento, requiriendo solo ligeras modificaciones para tratar con MDPs de horizonte finito. • Tiempos de llegada y salida conocidos y deterministas. Finalmente, hemos asumido que los tiempos de llegada y salida de los agentes (τa m y τd m) son deterministas y conocidos de antemano. Esta suposición es fundamental para nuestro método de solución. Si bien hay muchos dominios donde esta suposición es válida, en muchos casos los agentes llegan y se van de forma dinámica y sus tiempos de llegada y salida solo pueden predecirse de manera probabilística, lo que lleva a problemas de asignación de recursos en línea. En particular, en el caso de agentes con interés propio, esto se convierte en una versión interesante de un problema de diseño de mecanismos en línea [11, 12]. En resumen, hemos presentado una formulación MILP para el problema combinatorio de programación de recursos donde los valores de los agentes para posibles asignaciones de recursos están definidos por MDPs de horizonte finito. Este resultado amplía el trabajo previo ([6, 7]) sobre asignación estática de recursos de una sola vez bajo preferencias inducidas por MDP a problemas de programación de recursos con un aspecto temporal. Por lo tanto, este trabajo da un paso en la dirección de diseñar un mecanismo en línea para agentes con preferencias de recursos combinatorios inducidas por problemas de planificación estocástica. Relajar la suposición sobre los tiempos deterministas de llegada y salida de los agentes es un enfoque de nuestro trabajo futuro. Nos gustaría agradecer a los revisores anónimos por sus comentarios y sugerencias perspicaces. 7. REFERENCIAS [1] E. Altman y A. Shwartz. Control adaptativo de cadenas de Markov restringidas: Criterios y políticas. Anales de Investigación de Operaciones, número especial sobre Procesos de Decisión de Markov, 28:101-134, 1991. [2] R. Bellman. Programación dinámica. Princeton University Press, 1957. [3] C. Boutilier. \n\nPrensa de la Universidad de Princeton, 1957. [3] C. Boutilier. Resolviendo problemas de subasta combinatoria expresados de manera concisa. En Proc. de AAAI-02, páginas 359-366, 2002. [4] C. Boutilier y H. H. Hoos. Subastando idiomas para subastas combinatorias. En Proc. de IJCAI-01, páginas 1211-1217, 2001. [5] D. Dolgov. Asignación integrada de recursos y planificación en entornos estocásticos de múltiples agentes. Tesis doctoral, Departamento de Ciencias de la Computación, Universidad de Michigan, febrero de 2006. [6] D. A. Dolgov y E. H. Durfee. Asignación óptima de recursos y formulación de políticas en procesos de decisión de Markov débilmente acoplados. En Proc. de ICAPS-04, páginas 315-324, junio de 2004. [7] D. A. Dolgov y E. H. Durfee. Subastas combinatorias computacionalmente eficientes para asignación de recursos en MDPs débilmente acoplados. En Proc. de AAMAS-05, Nueva York, NY, EE. UU., 2005. ACM Press. [8] D. A. Dolgov y E. H. Durfee. Asignación de recursos entre agentes con preferencias inducidas por MDP factorizados. En Proc. de AAMAS-06, 2006. [9] K. Larson y T. Sandholm. Diseño de mecanismos y agentes deliberativos. En Proc. de AAMAS-05, páginas 650-656, Nueva York, NY, EE. UU., 2005. ACM Press. [10] N. Nisan. \n\nACM Press. [10] N. Nisan. Subasta y asignación en subastas combinatorias. En Comercio Electrónico, 2000. [11] D. C. Parkes y S. Singh. Un enfoque basado en MDP para el Diseño de Mecanismos en Línea. En Proc. de la Decimoséptima Conferencia Anual sobre Sistemas de Procesamiento de Información Neural (NIPS-03), 2003. [12] D. C. Parkes, S. Singh y D. Yanovsky. Diseño de mecanismos en línea aproximadamente eficiente. En Actas de la Decimoctava Conferencia Anual sobre Sistemas de Procesamiento de Información Neural (NIPS-04), 2004. [13] M. L. Puterman. Procesos de Decisión de Markov. John Wiley & Sons, Nueva York, 1994. [14] M. H. Rothkopf, A. Pekec y R. M. Harstad. Subastas combinatorias manejables computacionalmente. Ciencia de la Gestión, 44(8):1131-1147, 1998. [15] T. Sandholm. Un algoritmo para la determinación óptima del ganador en subastas combinatorias. En Proc. de IJCAI-99, páginas 542-547, San Francisco, CA, EE. UU., 1999. Morgan Kaufmann Publishers Inc. - Editores Morgan Kaufmann Inc. El Sexto Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 1227 ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "resource-scheduling": {
            "translated_key": "programación de recursos",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Combinatorial Resource Scheduling for Multiagent MDPs Dmitri A. Dolgov, Michael R. James, and Michael E. Samples AI and Robotics Group Technical Research, Toyota Technical Center USA {ddolgov, michael.r.james, michael.samples}@gmail.com ABSTRACT Optimal resource scheduling in multiagent systems is a computationally challenging task, particularly when the values of resources are not additive.",
                "We consider the combinatorial problem of scheduling the usage of multiple resources among agents that operate in stochastic environments, modeled as Markov decision processes (MDPs).",
                "In recent years, efficient resource-allocation algorithms have been developed for agents with resource values induced by MDPs.",
                "However, this prior work has focused on static resource-allocation problems where resources are distributed once and then utilized in infinite-horizon MDPs.",
                "We extend those existing models to the problem of combinatorial resource scheduling, where agents persist only for finite periods between their (predefined) arrival and departure times, requiring resources only for those time periods.",
                "We provide a computationally efficient procedure for computing globally optimal resource assignments to agents over time.",
                "We illustrate and empirically analyze the method in the context of a stochastic jobscheduling domain.",
                "Categories and Subject Descriptors I.2.8 [Artificial Intelligence]: Problem Solving, Control Methods, and Search; I.2.11 [Artificial Intelligence]: Distributed Artificial Intelligence-Multiagent systems General Terms Algorithms, Performance, Design 1.",
                "INTRODUCTION The tasks of optimal resource allocation and scheduling are ubiquitous in multiagent systems, but solving such optimization problems can be computationally difficult, due to a number of factors.",
                "In particular, when the value of a set of resources to an agent is not additive (as is often the case with resources that are substitutes or complements), the utility function might have to be defined on an exponentially large space of resource bundles, which very quickly becomes computationally intractable.",
                "Further, even when each agent has a utility function that is nonzero only on a small subset of the possible resource bundles, obtaining optimal allocation is still computationally prohibitive, as the problem becomes NP-complete [14].",
                "Such computational issues have recently spawned several threads of work in using compact models of agents preferences.",
                "One idea is to use any structure present in utility functions to represent them compactly, via, for example, logical formulas [15, 10, 4, 3].",
                "An alternative is to directly model the mechanisms that define the agents utility functions and perform resource allocation directly with these models [9].",
                "A way of accomplishing this is to model the processes by which an agent might utilize the resources and define the utility function as the payoff of these processes.",
                "In particular, if an agent uses resources to act in a stochastic environment, its utility function can be naturally modeled with a Markov decision process, whose action set is parameterized by the available resources.",
                "This representation can then be used to construct very efficient resource-allocation algorithms that lead to an exponential speedup over a straightforward optimization problem with flat representations of combinatorial preferences [6, 7, 8].",
                "However, this existing work on resource allocation with preferences induced by resource-parameterized MDPs makes an assumption that the resources are only allocated once and are then utilized by the agents independently within their infinite-horizon MDPs.",
                "This assumption that no reallocation of resources is possible can be limiting in domains where agents arrive and depart dynamically.",
                "In this paper, we extend the work on resource allocation under MDP-induced preferences to discrete-time scheduling problems, where agents are present in the system for finite time intervals and can only use resources within these intervals.",
                "In particular, agents arrive and depart at arbitrary (predefined) times and within these intervals use resources to execute tasks in finite-horizon MDPs.",
                "We address the problem of globally optimal resource scheduling, where the objective is to find an allocation of resources to the agents across time that maximizes the sum of the expected rewards that they obtain.",
                "In this context, our main contribution is a mixed-integerprogramming formulation of the scheduling problem that chooses globally optimal resource assignments, starting times, and execution horizons for all agents (within their arrival1220 978-81-904262-7-5 (RPS) c 2007 IFAAMAS departure intervals).",
                "We analyze and empirically compare two flavors of the scheduling problem: one, where agents have static resource assignments within their finite-horizon MDPs, and another, where resources can be dynamically reallocated between agents at every time step.",
                "In the rest of the paper, we first lay down the necessary groundwork in Section 2 and then introduce our model and formal problem statement in Section 3.",
                "In Section 4.2, we describe our main result, the optimization program for globally optimal resource scheduling.",
                "Following the discussion of our experimental results on a job-scheduling problem in Section 5, we conclude in Section 6 with a discussion of possible extensions and generalizations of our method. 2.",
                "BACKGROUND Similarly to the model used in previous work on resourceallocation with MDP-induced preferences [6, 7], we define the value of a set of resources to an agent as the value of the best MDP policy that is realizable, given those resources.",
                "However, since the focus of our work is on scheduling problems, and a large part of the optimization problem is to decide how resources are allocated in time among agents with finite arrival and departure times, we model the agents planning problems as finite-horizon MDPs, in contrast to previous work that used infinite-horizon discounted MDPs.",
                "In the rest of this section, we first introduce some necessary background on finite-horizon MDPs and present a linear-programming formulation that serves as the basis for our solution algorithm developed in Section 4.",
                "We also outline the standard methods for combinatorial resource scheduling with flat resource values, which serve as a comparison benchmark for the new model developed here. 2.1 Markov Decision Processes A stationary, finite-domain, discrete-time MDP (see, for example, [13] for a thorough and detailed development) can be described as S, A, p, r , where: S is a finite set of system states; A is a finite set of actions that are available to the agent; p is a stationary stochastic transition function, where p(σ|s, a) is the probability of transitioning to state σ upon executing action a in state s; r is a stationary reward function, where r(s, a) specifies the reward obtained upon executing action a in state s. Given such an MDP, a decision problem under a finite horizon T is to choose an optimal action at every time step to maximize the expected value of the total reward accrued during the agents (finite) lifetime.",
                "The agents optimal policy is then a function of current state s and the time until the horizon.",
                "An optimal policy for such a problem is to act greedily with respect to the optimal value function, defined recursively by the following system of finite-time Bellman equations [2]: v(s, t) = max a r(s, a) + X σ p(σ|s, a)v(σ, t + 1), ∀s ∈ S, t ∈ [1, T − 1]; v(s, T) = 0, ∀s ∈ S; where v(s, t) is the optimal value of being in state s at time t ∈ [1, T].",
                "This optimal value function can be easily computed using dynamic programming, leading to the following optimal policy π, where π(s, a, t) is the probability of executing action a in state s at time t: π(s, a, t) = ( 1, a = argmaxa r(s, a) + P σ p(σ|s, a)v(σ, t + 1), 0, otherwise.",
                "The above is the most common way of computing the optimal value function (and therefore an optimal policy) for a finite-horizon MDP.",
                "However, we can also formulate the problem as the following linear program (similarly to the dual LP for infinite-horizon discounted MDPs [13, 6, 7]): max X s X a r(s, a) X t x(s, a, t) subject to: X a x(σ, a, t + 1) = X s,a p(σ|s, a)x(s, a, t) ∀σ, t ∈ [1, T − 1]; X a x(s, a, 1) = α(s), ∀s ∈ S; (1) where α(s) is the initial distribution over the state space, and x is the (non-stationary) occupation measure (x(s, a, t) ∈ [0, 1] is the total expected number of times action a is executed in state s at time t).",
                "An optimal (non-stationary) policy is obtained from the occupation measure as follows: π(s, a, t) = x(s, a, t)/ X a x(s, a, t) ∀s ∈ S, t ∈ [1, T]. (2) Note that the standard unconstrained finite-horizon MDP, as described above, always has a uniformly-optimal solution (optimal for any initial distribution α(s)).",
                "Therefore, an optimal policy can be obtained by using an arbitrary constant α(s) > 0 (in particular, α(s) = 1 will result in x(s, a, t) = π(s, a, t)).",
                "However, for MDPs with resource constraints (as defined below in Section 3), uniformly-optimal policies do not in general exist.",
                "In such cases, α becomes a part of the problem input, and a resulting policy is only optimal for that particular α.",
                "This result is well known for infinite-horizon MDPs with various types of constraints [1, 6], and it also holds for our finite-horizon model, which can be easily established via a line of reasoning completely analogous to the arguments in [6]. 2.2 Combinatorial Resource Scheduling A straightforward approach to resource scheduling for a set of agents M, whose values for the resources are induced by stochastic planning problems (in our case, finite-horizon MDPs) would be to have each agent enumerate all possible resource assignments over time and, for each one, compute its value by solving the corresponding MDP.",
                "Then, each agent would provide valuations for each possible resource bundle over time to a centralized coordinator, who would compute the optimal resource assignments across time based on these valuations.",
                "When resources can be allocated at different times to different agents, each agent must submit valuations for every combination of possible time horizons.",
                "Let each agent m ∈ M execute its MDP within the arrival-departure time interval τ ∈ [τa m, τd m].",
                "Hence, agent m will execute an MDP with time horizon no greater than Tm = τd m−τa m+1.",
                "Let bτ be the global time horizon for the problem, before which all of the agents MDPs must finish.",
                "We assume τd m < bτ, ∀m ∈ M. The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1221 For the scheduling problem where agents have static resource requirements within their finite-horizon MDPs, the agents provide a valuation for each resource bundle for each possible time horizon (from [1, Tm]) that they may use.",
                "Let Ω be the set of resources to be allocated among the agents.",
                "An agent will get at most one resource bundle for one of the time horizons.",
                "Let the variable ψ ∈ Ψm enumerate all possible pairs of resource bundles and time horizons for agent m, so there are 2|Ω| × Tm values for ψ (the space of bundles is exponential in the number of resource types |Ω|).",
                "The agent m must provide a value vψ m for each ψ, and the coordinator will allocate at most one ψ (resource, time horizon) pair to each agent.",
                "This allocation is expressed as an indicator variable zψ m ∈ {0, 1} that shows whether ψ is assigned to agent m. For time τ and resource ω, the function nm(ψ, τ, ω) ∈ {0, 1} indicates whether the bundle in ψ uses resource ω at time τ (we make the assumption that agents have binary resource requirements).",
                "This allocation problem is NP-complete, even when considering only a single time step, and its difficulty increases significantly with multiple time steps because of the increasing number of values of ψ.",
                "The problem of finding an optimal allocation that satisfies the global constraint that the amount of each resource ω allocated to all agents does not exceed the available amount bϕ(ω) can be expressed as the following integer program: max X m∈M X ψ∈Ψm zψ mvψ m subject to: X ψ∈Ψm zψ m ≤ 1, ∀m ∈ M; X m∈M X ψ∈Ψm zψ mnm(ψ, τ, ω) ≤ bϕ(ω), ∀τ ∈ [1, bτ], ∀ω ∈ Ω; (3) The first constraint in equation 3 says that no agent can receive more than one bundle, and the second constraint ensures that the total assignment of resource ω does not, at any time, exceed the resource bound.",
                "For the scheduling problem where the agents are able to dynamically reallocate resources, each agent must specify a value for every combination of bundles and time steps within its time horizon.",
                "Let the variable ψ ∈ Ψm in this case enumerate all possible resource bundles for which at most one bundle may be assigned to agent m at each time step.",
                "Therefore, in this case there are P t∈[1,Tm](2|Ω| )t ∼ 2|Ω|Tm possibilities of resource bundles assigned to different time slots, for the Tm different time horizons.",
                "The same set of equations (3) can be used to solve this dynamic scheduling problem, but the integer program is different because of the difference in how ψ is defined.",
                "In this case, the number of ψ values is exponential in each agents planning horizon Tm, resulting in a much larger program.",
                "This straightforward approach to solving both of these scheduling problems requires an enumeration and solution of either 2|Ω| Tm (static allocation) or P t∈[1,Tm] 2|Ω|t (dynamic reallocation) MDPs for each agent, which very quickly becomes intractable with the growth of the number of resources |Ω| or the time horizon Tm. 3.",
                "MODEL AND PROBLEM STATEMENT We now formally introduce our model of the resourcescheduling problem.",
                "The problem input consists of the following components: • M, Ω, bϕ, τa m, τd m, bτ are as defined above in Section 2.2. • {Θm} = {S, A, pm, rm, αm} are the MDPs of all agents m ∈ M. Without loss of generality, we assume that state and action spaces of all agents are the same, but each has its own transition function pm, reward function rm, and initial conditions αm. • ϕm : A×Ω → {0, 1} is the mapping of actions to resources for agent m. ϕm(a, ω) indicates whether action a of agent m needs resource ω.",
                "An agent m that receives a set of resources that does not include resource ω cannot execute in its MDP policy any action a for which ϕm(a, ω) = 0.",
                "We assume all resource requirements are binary; as discussed below in Section 6, this assumption is not limiting.",
                "Given the above input, the optimization problem we consider is to find the globally optimal-maximizing the sum of expected rewards-mapping of resources to agents for all time steps: Δ : τ × M × Ω → {0, 1}.",
                "A solution is feasible if the corresponding assignment of resources to the agents does not violate the global resource constraint: X m Δm(τ, ω) ≤ bϕ(ω), ∀ω ∈ Ω, τ ∈ [1, bτ]. (4) We consider two flavors of the <br>resource-scheduling</br> problem.",
                "The first formulation restricts resource assignments to the space where the allocation of resources to each agent is static during the agents lifetime.",
                "The second formulation allows reassignment of resources between agents at every time step within their lifetimes.",
                "Figure 1 depicts a <br>resource-scheduling</br> problem with three agents M = {m1, m2, m3}, three resources Ω = {ω1, ω2, ω3}, and a global problem horizon of bτ = 11.",
                "The agents arrival and departure times are shown as gray boxes and are {1, 6}, {3, 7}, and {2, 11}, respectively.",
                "A solution to this problem is shown via horizontal bars within each agents box, where the bars correspond to the allocation of the three resource types.",
                "Figure 1a shows a solution to a static scheduling problem.",
                "According to the shown solution, agent m1 begins the execution of its MDP at time τ = 1 and has a lock on all three resources until it finishes execution at time τ = 3.",
                "Note that agent m1 relinquishes its hold on the resources before its announced departure time of τd m1 = 6, ostensibly because other agents can utilize the resources more effectively.",
                "Thus, at time τ = 4, resources ω1 and ω3 are allocated to agent m2, who then uses them to execute its MDP (using only actions supported by resources ω1 and ω3) until time τ = 7.",
                "Agent m3 holds resource ω3 during the interval τ ∈ [4, 10].",
                "Figure 1b shows a possible solution to the dynamic version of the same problem.",
                "There, resources can be reallocated between agents at every time step.",
                "For example, agent m1 gives up its use of resource ω2 at time τ = 2, although it continues the execution of its MDP until time τ = 6.",
                "Notice that an agent is not allowed to stop and restart its MDP, so agent m1 is only able to continue executing in the interval τ ∈ [3, 4] if it has actions that do not require any resources (ϕm(a, ω) = 0).",
                "Clearly, the model and problem statement described above make a number of assumptions about the problem and the desired solution properties.",
                "We discuss some of those assumptions and their implications in Section 6. 1222 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) (a) (b) Figure 1: Illustration of a solution to a <br>resource-scheduling</br> problem with three agents and three resources: a) static resource assignments (resource assignments are constant within agents lifetimes; b) dynamic assignment (resource assignments are allowed to change at every time step). 4.",
                "RESOURCE SCHEDULING Our <br>resource-scheduling</br> algorithm proceeds in two stages.",
                "First, we perform a preprocessing step that augments the agent MDPs; this process is described in Section 4.1.",
                "Second, using these augmented MDPs we construct a global optimization problem, which is described in Section 4.2. 4.1 Augmenting Agents MDPs In the model described in the previous section, we assume that if an agent does not possess the necessary resources to perform actions in its MDP, its execution is halted and the agent leaves the system.",
                "In other words, the MDPs cannot be paused and resumed.",
                "For example, in the problem shown in Figure 1a, agent m1 releases all resources after time τ = 3, at which point the execution of its MDP is halted.",
                "Similarly, agents m2 and m3 only execute their MDPs in the intervals τ ∈ [4, 6] and τ ∈ [4, 10], respectively.",
                "Therefore, an important part of the global decision-making problem is to decide the window of time during which each of the agents is active (i.e., executing its MDP).",
                "To accomplish this, we augment each agents MDP with two new states (start and finish states sb , sf , respectively) and a new start/stop action a∗ , as illustrated in Figure 2.",
                "The idea is that an agent stays in the start state sb until it is ready to execute its MDP, at which point it performs the start/stop action a∗ and transitions into the state space of the original MDP with the transition probability that corresponds to the original initial distribution α(s).",
                "For example, in Figure 1a, for agent m2 this would happen at time τ = 4.",
                "Once the agent gets to the end of its activity window (time τ = 6 for agent m2 in Figure 1a), it performs the start/stop action, which takes it into the sink finish state sf at time τ = 7.",
                "More precisely, given an MDP S, A, pm, rm, αm , we define an augmented MDP S , A , pm, rm, αm as follows: S = S ∪ sb ∪ sf ; A = A ∪ a∗ ; p (s|sb , a∗ ) = α(s), ∀s ∈ S; p (sb |sb , a) = 1.0, ∀a ∈ A; p (sf |s, a∗ ) = 1.0, ∀s ∈ S; p (σ|s, a) = p(σ|s, a), ∀s, σ ∈ S, a ∈ A; r (sb , a) = r (sf , a) = 0, ∀a ∈ A ; r (s, a) = r(s, a), ∀s ∈ S, a ∈ A; α (sb ) = 1; α (s) = 0, ∀s ∈ S; where all non-specified transition probabilities are assumed to be zero.",
                "Further, in order to account for the new starting state, we begin the MDP one time-step earlier, setting τa m ← τa m − 1.",
                "This will not affect the resource allocation due to the resource constraints only being enforced for the original MDP states, as will be discussed in the next section.",
                "For example, the augmented MDPs shown in Figure 2b (which starts in state sb at time τ = 2) would be constructed from an MDP with original arrival time τ = 3.",
                "Figure 2b also shows a sample trajectory through the state space: the agent starts in state sb , transitions into the state space S of the original MDP, and finally exists into the sink state sf .",
                "Note that if we wanted to model a problem where agents could pause their MDPs at arbitrary time steps (which might be useful for domains where dynamic reallocation is possible), we could easily accomplish this by including an extra action that transitions from each state to itself with zero reward. 4.2 MILP for Resource Scheduling Given a set of augmented MDPs, as defined above, the goal of this section is to formulate a global optimization program that solves the <br>resource-scheduling</br> problem.",
                "In this section and below, all MDPs are assumed to be the augmented MDPs as defined in Section 4.1.",
                "Our approach is similar to the idea used in [6]: we begin with the linear-program formulation of agents MDPs (1) and augment it with constraints that ensure that the corresponding resource allocation across agents and time is valid.",
                "The resulting optimization problem then simultaneously solves the agents MDPs and <br>resource-scheduling</br> problems.",
                "In the rest of this section, we incrementally develop a mixed integer program (MILP) that achieves this.",
                "In the absence of resource constraints, the agents finitehorizon MDPs are completely independent, and the globally optimal solution can be trivially obtained via the following LP, which is simply an aggregation of single-agent finitehorizon LPs: max X m X s X a rm(s, a) X t xm(s, a, t) subject to: X a xm(σ, a, t + 1) = X s,a pm(σ|s, a)xm(s, a, t), ∀m ∈ M, σ ∈ S, t ∈ [1, Tm − 1]; X a xm(s, a, 1) = αm(s), ∀m ∈ M, s ∈ S; (12) where xm(s, a, t) is the occupation measure of agent m, and The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1223 (a) (b) Figure 2: Illustration of augmenting an MDP to allow for variable starting and stopping times: a) (left) the original two-state MDP with a single action; (right) the augmented MDP with new states sb and sf and the new action a∗ (note that the origianl transitions are not changed in the augmentation process); b) the augmented MDP displayed as a trajectory through time (grey lines indicate all transitions, while black lines indicate a given trajectory.",
                "Objective Function (sum of expected rewards over all agents) max X m X s X a rm(s, a) X t xm(s, a, t) (5) Meaning Implication Linear Constraints Tie x to θ.",
                "Agent is only active when occupation measure is nonzero in original MDP states. θm(τ) = 0 =⇒ xm(s, a, τ −τa m+1) = 0 ∀s /∈ {sb , sf }, a ∈ A X s/∈{sb,sf } X a xm(s, a, t) ≤ θm(τa m + t − 1) ∀m ∈ M, ∀t ∈ [1, Tm] (6) Agent can only be active in τ ∈ (τa m, τd m) θm(τ) = 0 ∀m ∈ M, τ /∈ (τa m, τd m) (7) Cannot use resources when not active θm(τ) = 0 =⇒ Δm(τ, ω) = 0 ∀τ ∈ [0, bτ], ω ∈ Ω Δm(τ, ω) ≤ θm(τ) ∀m ∈ M, τ ∈ [0, bτ], ω ∈ Ω (8) Tie x to Δ (nonzero x forces corresponding Δ to be nonzero.)",
                "Δm(τ, ω) = 0, ϕm(a, ω) = 1 =⇒ xm(s, a, τ − τa m + 1) = 0 ∀s /∈ {sb , sf } 1/|A| X a ϕm(a, ω) X s/∈{sb,sf } xm(s, a, t) ≤ Δm(t + τa m − 1, ω) ∀m ∈ M, ω ∈ Ω, t ∈ [1, Tm] (9) Resource bounds X m Δm(τ, ω) ≤ bϕ(ω) ∀ω ∈ Ω, τ ∈ [0, bτ] (10) Agent cannot change resources while active.",
                "Only enabled for scheduling with static assignments. θm(τ) = 1 and θm(τ + 1) = 1 =⇒ Δm(τ, ω) = Δm(τ + 1, ω) Δm(τ, ω) − Z(1 − θm(τ + 1)) ≤ Δm(τ + 1, ω) + Z(1 − θm(τ)) Δm(τ, ω) + Z(1 − θm(τ + 1)) ≥ Δm(τ + 1, ω) − Z(1 − θm(τ)) ∀m ∈ M, ω ∈ Ω, τ ∈ [0, bτ] (11) Table 1: MILP for globally optimal resource scheduling.",
                "Tm = τd m − τa m + 1 is the time horizon for the agents MDP.",
                "Using this LP as a basis, we augment it with constraints that ensure that the resource usage implied by the agents occupation measures {xm} does not violate the global resource requirements bϕ at any time step τ ∈ [0, bτ].",
                "To formulate these resource constraints, we use the following binary variables: • Δm(τ, ω) = {0, 1}, ∀m ∈ M, τ ∈ [0, bτ], ω ∈ Ω, which serve as indicator variables that define whether agent m possesses resource ω at time τ.",
                "These are analogous to the static indicator variables used in the one-shot static resource-allocation problem in [6]. • θm = {0, 1}, ∀m ∈ M, τ ∈ [0, bτ] are indicator variables that specify whether agent m is active (i.e., executing its MDP) at time τ.",
                "The meaning of resource-usage variables Δ is illustrated in Figure 1: Δm(τ, ω) = 1 only if resource ω is allocated to agent m at time τ.",
                "The meaning of the activity indicators θ is illustrated in Figure 2b: when agent m is in either the start state sb or the finish state sf , the corresponding θm = 0, but once the agent becomes active and enters one of the other states, we set θm = 1 .",
                "This meaning of θ can be enforced with a linear constraint that synchronizes the values of the agents occupation measures xm and the activity 1224 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) indicators θ, as shown in (6) in Table 1.",
                "Another constraint we have to add-because the activity indicators θ are defined on the global timeline τ-is to enforce the fact that the agent is inactive outside of its arrivaldeparture window.",
                "This is accomplished by constraint (7) in Table 1.",
                "Furthermore, agents should not be using resources while they are inactive.",
                "This constraint can also be enforced via a linear inequality on θ and Δ, as shown in (8).",
                "Constraint (6) sets the value of θ to match the policy defined by the occupation measure xm.",
                "In a similar fashion, we have to make sure that the resource-usage variables Δ are also synchronized with the occupation measure xm.",
                "This is done via constraint (9) in Table 1, which is nearly identical to the analogous constraint from [6].",
                "After implementing the above constraint, which enforces the meaning of Δ, we add a constraint that ensures that the agents resource usage never exceeds the amounts of available resources.",
                "This condition is also trivially expressed as a linear inequality (10) in Table 1.",
                "Finally, for the problem formulation where resource assignments are static during a lifetime of an agent, we add a constraint that ensures that the resource-usage variables Δ do not change their value while the agent is active (θ = 1).",
                "This is accomplished via the linear constraint (11), where Z ≥ 2 is a constant that is used to turn off the constraints when θm(τ) = 0 or θm(τ + 1) = 0.",
                "This constraint is not used for the dynamic problem formulation, where resources can be reallocated between agents at every time step.",
                "To summarize, Table 1 together with the conservationof-flow constraints from (12) defines the MILP that simultaneously computes an optimal resource assignment for all agents across time as well as optimal finite-horizon MDP policies that are valid under that resource assignment.",
                "As a rough measure of the complexity of this MILP, let us consider the number of optimization variables and constraints.",
                "Let TM = P Tm = P m(τa m − τd m + 1) be the sum of the lengths of the arrival-departure windows across all agents.",
                "Then, the number of optimization variables is: TM + bτ|M||Ω| + bτ|M|, TM of which are continuous (xm), and bτ|M||Ω| + bτ|M| are binary (Δ and θ).",
                "However, notice that all but TM|M| of the θ are set to zero by constraint (7), which also immediately forces all but TM|M||Ω| of the Δ to be zero via the constraints (8).",
                "The number of constraints (not including the degenerate constraints in (7)) in the MILP is: TM + TM|Ω| + bτ|Ω| + bτ|M||Ω|.",
                "Despite the fact that the complexity of the MILP is, in the worst case, exponential1 in the number of binary variables, the complexity of this MILP is significantly (exponentially) lower than that of the MILP with flat utility functions, described in Section 2.2.",
                "This result echos the efficiency gains reported in [6] for single-shot resource-allocation problems, but is much more pronounced, because of the explosion of the flat utility representation due to the temporal aspect of the problem (recall the prohibitive complexity of the combinatorial optimization in Section 2.2).",
                "We empirically analyze the performance of this method in Section 5. 1 Strictly speaking, solving MILPs to optimality is NPcomplete in the number of integer variables. 5.",
                "EXPERIMENTAL RESULTS Although the complexity of solving MILPs is in the worst case exponential in the number of integer variables, there are many efficient methods for solving MILPs that allow our algorithm to scale well for parameters common to resource allocation and scheduling problems.",
                "In particular, this section introduces a problem domain-the repairshop problem-used to empirically evaluate our algorithms scalability in terms of the number of agents |M|, the number of shared resources |Ω|, and the varied lengths of global time bτ during which agents may enter and exit the system.",
                "The repairshop problem is a simple parameterized MDP adopting the metaphor of a vehicular repair shop.",
                "Agents in the repair shop are mechanics with a number of independent tasks that yield reward only when completed.",
                "In our MDP model of this system, actions taken to advance through the state space are only allowed if the agent holds certain resources that are publicly available to the shop.",
                "These resources are in finite supply, and optimal policies for the shop will determine when each agent may hold the limited resources to take actions and earn individual rewards.",
                "Each task to be completed is associated with a single action, although the agent is required to repeat the action numerous times before completing the task and earning a reward.",
                "This model was parameterized in terms of the number of agents in the system, the number of different types of resources that could be linked to necessary actions, a global time during which agents are allowed to arrive and depart, and a maximum length for the number of time steps an agent may remain in the system.",
                "All datapoints in our experiments were obtained with 20 evaluations using CPLEX to solve the MILPs on a Pentium4 computer with 2Gb of RAM.",
                "Trials were conducted on both the static and the dynamic version of the resourcescheduling problem, as defined earlier.",
                "Figure 3 shows the runtime and policy value for independent modifications to the parameter set.",
                "The top row shows how the solution time for the MILP scales as we increase the number of agents |M|, the global time horizon bτ, and the number of resources |Ω|.",
                "Increasing the number of agents leads to exponential complexity scaling, which is to be expected for an NP-complete problem.",
                "However, increasing the global time limit bτ or the total number of resource types |Ω|-while holding the number of agents constantdoes not lead to decreased performance.",
                "This occurs because the problems get easier as they become under-constrained, which is also a common phenomenon for NP-complete problems.",
                "We also observe that the solution to the dynamic version of the problem can often be computed much faster than the static version.",
                "The bottom row of Figure 3 shows the joint policy value of the policies that correspond to the computed optimal resource-allocation schedules.",
                "We can observe that the dynamic version yields higher reward (as expected, since the reward for the dynamic version is always no less than the reward of the static version).",
                "We should point out that these graphs should not be viewed as a measure of performance of two different algorithms (both algorithms produce optimal solutions but to different problems), but rather as observations about how the quality of optimal solutions change as more flexibility is allowed in the reallocation of resources.",
                "Figure 4 shows runtime and policy value for trials in which common input variables are scaled together.",
                "This allows The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1225 2 4 6 8 10 10 −3 10 −2 10 −1 10 0 10 1 10 2 10 3 10 4 Number of Agents |M| CPUTime,sec |Ω| = 5, τ = 50 static dynamic 50 100 150 200 10 −2 10 −1 10 0 10 1 10 2 10 3 Global Time Boundary τ CPUTime,sec |M| = 5, |Ω| = 5 static dynamic 10 20 30 40 50 10 −2 10 −1 10 0 10 1 10 2 Number of Resources |Ω| CPUTime,sec |M| = 5, τ = 50 static dynamic 2 4 6 8 10 200 400 600 800 1000 1200 1400 1600 Number of Agents |M| Value |Ω| = 5, τ = 50 static dynamic 50 100 150 200 400 500 600 700 800 900 1000 1100 1200 1300 1400 Global Time Boundary τ Value |M| = 5, |Ω| = 5 static dynamic 10 20 30 40 50 500 600 700 800 900 1000 1100 1200 1300 1400 Number of Resources |Ω| Value |M| = 5, τ = 50 static dynamic Figure 3: Evaluation of our MILP for variable numbers of agents (column 1), lengths of global-time window (column 2), and numbers of resource types (column 3).",
                "Top row shows CPU time, and bottom row shows the joint reward of agents MDP policies.",
                "Error bars show the 1st and 3rd quartiles (25% and 75%). 2 4 6 8 10 10 −3 10 −2 10 −1 10 0 10 1 10 2 10 3 Number of Agents |M| CPUTime,sec τ = 10|M| static dynamic 2 4 6 8 10 10 −3 10 −2 10 −1 10 0 10 1 10 2 10 3 10 4 Number of Agents |M| CPUTime,sec |Ω| = 2|M| static dynamic 2 4 6 8 10 10 −3 10 −2 10 −1 10 0 10 1 10 2 10 3 10 4 Number of Agents |M| CPUTime,sec |Ω| = 5|M| static dynamic 2 4 6 8 10 200 400 600 800 1000 1200 1400 1600 1800 2000 2200 Number of Agents |M| Value τ = 10|M| static dynamic 2 4 6 8 10 200 400 600 800 1000 1200 1400 1600 1800 2000 Number of Agents |M| Value |Ω| = 2|M| static dynamic 2 4 6 8 10 0 500 1000 1500 2000 2500 Number of Agents |M| Value |Ω| = 5|M| static dynamic Figure 4: Evaluation of our MILP using correlated input variables.",
                "The left column tracks the performance and CPU time as the number of agents and global-time window increase together (bτ = 10|M|).",
                "The middle and the right column track the performance and CPU time as the number of resources and the number of agents increase together as |Ω| = 2|M| and |Ω| = 5|M|, respectively.",
                "Error bars show the 1st and 3rd quartiles (25% and 75%). 1226 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) us to explore domains where the total number of agents scales proportionally to the total number of resource types or the global time horizon, while keeping constant the average agent density (per unit of global time) or the average number of resources per agent (which commonly occurs in real-life applications).",
                "Overall, we believe that these experimental results indicate that our MILP formulation can be used to effectively solve <br>resource-scheduling</br> problems of nontrivial size. 6.",
                "DISCUSSION AND CONCLUSIONS Throughout the paper, we have made a number of assumptions in our model and solution algorithm; we discuss their implications below. • Continual execution.",
                "We assume that once an agent stops executing its MDP (transitions into state sf ), it exits the system and cannot return.",
                "It is easy to relax this assumption for domains where agents MDPs can be paused and restarted.",
                "All that is required is to include an additional pause action which transitions from a given state back to itself, and has zero reward. • Indifference to start time.",
                "We used a reward model where agents rewards depend only on the time horizon of their MDPs and not the global start time.",
                "This is a consequence of our MDP-augmentation procedure from Section 4.1.",
                "It is easy to extend the model so that the agents incur an explicit penalty for idling by assigning a non-zero negative reward to the start state sb . • Binary resource requirements.",
                "For simplicity, we have assumed that resource costs are binary: ϕm(a, ω) = {0, 1}, but our results generalize in a straightforward manner to non-binary resource mappings, analogously to the procedure used in [5]. • Cooperative agents.",
                "The optimization procedure discussed in this paper was developed in the context of cooperative agents, but it can also be used to design a mechanism for scheduling resources among self-interested agents.",
                "This optimization procedure can be embedded in a VickreyClarke-Groves auction, completely analogously to the way it was done in [7].",
                "In fact, all the results of [7] about the properties of the auction and information privacy directly carry over to the scheduling domain discussed in this paper, requiring only slight modifications to deal with finitehorizon MDPs. • Known, deterministic arrival and departure times.",
                "Finally, we have assumed that agents arrival and departure times (τa m and τd m) are deterministic and known a priori.",
                "This assumption is fundamental to our solution method.",
                "While there are many domains where this assumption is valid, in many cases agents arrive and depart dynamically and their arrival and departure times can only be predicted probabilistically, leading to online resource-allocation problems.",
                "In particular, in the case of self-interested agents, this becomes an interesting version of an online-mechanism-design problem [11, 12].",
                "In summary, we have presented an MILP formulation for the combinatorial <br>resource-scheduling</br> problem where agents values for possible resource assignments are defined by finitehorizon MDPs.",
                "This result extends previous work ([6, 7]) on static one-shot resource allocation under MDP-induced preferences to <br>resource-scheduling</br> problems with a temporal aspect.",
                "As such, this work takes a step in the direction of designing an online mechanism for agents with combinatorial resource preferences induced by stochastic planning problems.",
                "Relaxing the assumption about deterministic arrival and departure times of the agents is a focus of our future work.",
                "We would like to thank the anonymous reviewers for their insightful comments and suggestions. 7.",
                "REFERENCES [1] E. Altman and A. Shwartz.",
                "Adaptive control of constrained Markov chains: Criteria and policies.",
                "Annals of Operations Research, special issue on Markov Decision Processes, 28:101-134, 1991. [2] R. Bellman.",
                "Dynamic Programming.",
                "Princeton University Press, 1957. [3] C. Boutilier.",
                "Solving concisely expressed combinatorial auction problems.",
                "In Proc. of AAAI-02, pages 359-366, 2002. [4] C. Boutilier and H. H. Hoos.",
                "Bidding languages for combinatorial auctions.",
                "In Proc. of IJCAI-01, pages 1211-1217, 2001. [5] D. Dolgov.",
                "Integrated Resource Allocation and Planning in Stochastic Multiagent Environments.",
                "PhD thesis, Computer Science Department, University of Michigan, February 2006. [6] D. A. Dolgov and E. H. Durfee.",
                "Optimal resource allocation and policy formulation in loosely-coupled Markov decision processes.",
                "In Proc. of ICAPS-04, pages 315-324, June 2004. [7] D. A. Dolgov and E. H. Durfee.",
                "Computationally efficient combinatorial auctions for resource allocation in weakly-coupled MDPs.",
                "In Proc. of AAMAS-05, New York, NY, USA, 2005.",
                "ACM Press. [8] D. A. Dolgov and E. H. Durfee.",
                "Resource allocation among agents with preferences induced by factored MDPs.",
                "In Proc. of AAMAS-06, 2006. [9] K. Larson and T. Sandholm.",
                "Mechanism design and deliberative agents.",
                "In Proc. of AAMAS-05, pages 650-656, New York, NY, USA, 2005.",
                "ACM Press. [10] N. Nisan.",
                "Bidding and allocation in combinatorial auctions.",
                "In Electronic Commerce, 2000. [11] D. C. Parkes and S. Singh.",
                "An MDP-based approach to Online Mechanism Design.",
                "In Proc. of the Seventeenths Annual Conference on Neural Information Processing Systems (NIPS-03), 2003. [12] D. C. Parkes, S. Singh, and D. Yanovsky.",
                "Approximately efficient online mechanism design.",
                "In Proc. of the Eighteenths Annual Conference on Neural Information Processing Systems (NIPS-04), 2004. [13] M. L. Puterman.",
                "Markov Decision Processes.",
                "John Wiley & Sons, New York, 1994. [14] M. H. Rothkopf, A. Pekec, and R. M. Harstad.",
                "Computationally manageable combinational auctions.",
                "Management Science, 44(8):1131-1147, 1998. [15] T. Sandholm.",
                "An algorithm for optimal winner determination in combinatorial auctions.",
                "In Proc. of IJCAI-99, pages 542-547, San Francisco, CA, USA, 1999.",
                "Morgan Kaufmann Publishers Inc.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1227"
            ],
            "original_annotated_samples": [
                "A solution is feasible if the corresponding assignment of resources to the agents does not violate the global resource constraint: X m Δm(τ, ω) ≤ bϕ(ω), ∀ω ∈ Ω, τ ∈ [1, bτ]. (4) We consider two flavors of the <br>resource-scheduling</br> problem.",
                "Figure 1 depicts a <br>resource-scheduling</br> problem with three agents M = {m1, m2, m3}, three resources Ω = {ω1, ω2, ω3}, and a global problem horizon of bτ = 11.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) (a) (b) Figure 1: Illustration of a solution to a <br>resource-scheduling</br> problem with three agents and three resources: a) static resource assignments (resource assignments are constant within agents lifetimes; b) dynamic assignment (resource assignments are allowed to change at every time step). 4.",
                "RESOURCE SCHEDULING Our <br>resource-scheduling</br> algorithm proceeds in two stages.",
                "Note that if we wanted to model a problem where agents could pause their MDPs at arbitrary time steps (which might be useful for domains where dynamic reallocation is possible), we could easily accomplish this by including an extra action that transitions from each state to itself with zero reward. 4.2 MILP for Resource Scheduling Given a set of augmented MDPs, as defined above, the goal of this section is to formulate a global optimization program that solves the <br>resource-scheduling</br> problem."
            ],
            "translated_annotated_samples": [
                "Una solución es factible si la asignación correspondiente de recursos a los agentes no viola la restricción global de recursos: X m Δm(τ, ω) ≤ bϕ(ω), ∀ω ∈ Ω, τ ∈ [1, bτ]. Consideramos dos variantes del problema de <br>programación de recursos</br>.",
                "La Figura 1 representa un problema de <br>programación de recursos</br> con tres agentes M = {m1, m2, m3}, tres recursos Ω = {ω1, ω2, ω3}, y un horizonte de problema global de bτ = 11.",
                "Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) (a) (b) Figura 1: Ilustración de una solución a un problema de <br>programación de recursos</br> con tres agentes y tres recursos: a) asignaciones de recursos estáticas (las asignaciones de recursos son constantes durante la vida de los agentes); b) asignación dinámica (las asignaciones de recursos pueden cambiar en cada paso de tiempo). 4.",
                "PROGRAMACIÓN DE RECURSOS Nuestro algoritmo de <br>programación de recursos</br> se lleva a cabo en dos etapas.",
                "Cabe destacar que si quisiéramos modelar un problema en el que los agentes pudieran pausar sus MDP en pasos de tiempo arbitrarios (lo cual podría ser útil para dominios donde la reasignación dinámica es posible), podríamos lograrlo fácilmente incluyendo una acción adicional que transiciona de cada estado a sí mismo con una recompensa de cero. 4.2 MILP para la <br>programación de recursos</br> Dado un conjunto de MDPs aumentados, como se define arriba, el objetivo de esta sección es formular un programa de optimización global que resuelva el problema de <br>programación de recursos</br>."
            ],
            "translated_text": "La programación de recursos combinatoria para MDPs multiagentes. Dmitri A. Dolgov, Michael R. James y Michael E. Samples. Grupo de Investigación en IA y Robótica, Centro Técnico de Toyota en EE. UU. {ddolgov, michael.r.james, michael.samples}@gmail.com. RESUMEN La programación óptima de recursos en sistemas multiagentes es una tarea computacionalmente desafiante, especialmente cuando los valores de los recursos no son aditivos. Consideramos el problema combinatorio de programar el uso de múltiples recursos entre agentes que operan en entornos estocásticos, modelados como procesos de decisión de Markov (MDPs). En los últimos años, se han desarrollado algoritmos eficientes de asignación de recursos para agentes con valores de recursos inducidos por MDPs. Sin embargo, este trabajo previo se ha centrado en problemas de asignación de recursos estáticos donde los recursos se distribuyen una vez y luego se utilizan en MDPs de horizonte infinito. Extendemos esos modelos existentes al problema de la programación de recursos combinatoria, donde los agentes persisten solo por períodos finitos entre sus tiempos de llegada y salida (predefinidos), requiriendo recursos solo durante esos períodos de tiempo. Proporcionamos un procedimiento computacionalmente eficiente para calcular asignaciones de recursos óptimas a nivel global a agentes a lo largo del tiempo. Ilustramos y analizamos empíricamente el método en el contexto de un dominio de programación de trabajos estocásticos. Categorías y Descriptores de Asignaturas I.2.8 [Inteligencia Artificial]: Resolución de Problemas, Métodos de Control y Búsqueda; I.2.11 [Inteligencia Artificial]: Inteligencia Artificial Distribuida-Sistemas Multiagente Términos Generales Algoritmos, Rendimiento, Diseño 1. INTRODUCCIÓN Las tareas de asignación óptima de recursos y programación son ubicuas en sistemas multiagentes, pero resolver tales problemas de optimización puede ser computacionalmente difícil, debido a una serie de factores. En particular, cuando el valor de un conjunto de recursos para un agente no es aditivo (como suele ser el caso con recursos que son sustitutos o complementos), la función de utilidad podría tener que definirse en un espacio exponencialmente grande de combinaciones de recursos, lo que rápidamente se vuelve computacionalmente intratable. Además, incluso cuando cada agente tiene una función de utilidad que es distinta de cero solo en un pequeño subconjunto de los posibles conjuntos de recursos, obtener una asignación óptima sigue siendo computacionalmente prohibitivo, ya que el problema se vuelve NP-completo [14]. Tales problemas computacionales han dado lugar recientemente a varias líneas de trabajo en el uso de modelos compactos de las preferencias de los agentes. Una idea es utilizar cualquier estructura presente en las funciones de utilidad para representarlas de forma compacta, por ejemplo, a través de fórmulas lógicas [15, 10, 4, 3]. Una alternativa es modelar directamente los mecanismos que definen las funciones de utilidad de los agentes y realizar la asignación de recursos directamente con estos modelos [9]. Una forma de lograr esto es modelar los procesos mediante los cuales un agente podría utilizar los recursos y definir la función de utilidad como la ganancia de estos procesos. En particular, si un agente utiliza recursos para actuar en un entorno estocástico, su función de utilidad puede ser modelada de forma natural con un proceso de decisión de Markov, cuyo conjunto de acciones está parametrizado por los recursos disponibles. Esta representación puede ser utilizada para construir algoritmos de asignación de recursos muy eficientes que resultan en una aceleración exponencial sobre un problema de optimización directa con representaciones planas de preferencias combinatorias [6, 7, 8]. Sin embargo, este trabajo existente sobre asignación de recursos con preferencias inducidas por MDPs parametrizados por recursos hace una suposición de que los recursos se asignan solo una vez y luego son utilizados por los agentes de forma independiente dentro de sus MDPs de horizonte infinito. Esta suposición de que no es posible la reasignación de recursos puede ser limitante en dominios donde los agentes llegan y se van dinámicamente. En este artículo, ampliamos el trabajo sobre asignación de recursos bajo preferencias inducidas por MDP a problemas de programación en tiempo discreto, donde los agentes están presentes en el sistema durante intervalos de tiempo finitos y solo pueden utilizar recursos dentro de estos intervalos. En particular, los agentes llegan y parten en momentos arbitrarios (predefinidos) y dentro de estos intervalos utilizan recursos para ejecutar tareas en MDPs de horizonte finito. Abordamos el problema de la programación de recursos óptima a nivel global, donde el objetivo es encontrar una asignación de recursos a los agentes a lo largo del tiempo que maximice la suma de las recompensas esperadas que obtienen. En este contexto, nuestra principal contribución es una formulación de programación entera mixta del problema de programación que elige asignaciones de recursos globalmente óptimas, tiempos de inicio y horizontes de ejecución para todos los agentes (dentro de sus intervalos de llegada y salida). Analizamos y comparamos empíricamente dos variantes del problema de programación: uno, donde los agentes tienen asignaciones de recursos estáticas dentro de sus MDPs de horizonte finito, y otro, donde los recursos pueden ser realocados dinámicamente entre agentes en cada paso de tiempo. En el resto del documento, primero establecemos las bases necesarias en la Sección 2 y luego presentamos nuestro modelo y declaración formal del problema en la Sección 3. En la Sección 4.2, describimos nuestro resultado principal, el programa de optimización para la programación de recursos globalmente óptima. Tras la discusión de nuestros resultados experimentales sobre un problema de programación de trabajos en la Sección 5, concluimos en la Sección 6 con una discusión de posibles extensiones y generalizaciones de nuestro método. De manera similar al modelo utilizado en trabajos anteriores sobre asignación de recursos con preferencias inducidas por MDP [6, 7], definimos el valor de un conjunto de recursos para un agente como el valor de la mejor política MDP que es realizable, dadas esos recursos. Sin embargo, dado que el enfoque de nuestro trabajo se centra en problemas de programación, y una gran parte del problema de optimización consiste en decidir cómo se asignan los recursos en el tiempo entre agentes con tiempos finitos de llegada y salida, modelamos los problemas de planificación de los agentes como MDPs de horizonte finito, en contraste con trabajos anteriores que utilizaron MDPs descontados de horizonte infinito. En el resto de esta sección, primero presentamos algo de antecedentes necesarios sobre MDPs de horizonte finito y presentamos una formulación de programación lineal que sirve como base para nuestro algoritmo de solución desarrollado en la Sección 4. También describimos los métodos estándar para la programación de recursos combinatorios con valores de recursos planos, que sirven como punto de referencia de comparación para el nuevo modelo desarrollado aquí. 2.1 Procesos de Decisión de Markov Un MDP (Proceso de Decisión de Markov) estacionario, de dominio finito y discreto en el tiempo (ver, por ejemplo, [13] para un desarrollo detallado y exhaustivo) se puede describir como S, A, p, r, donde: S es un conjunto finito de estados del sistema; A es un conjunto finito de acciones disponibles para el agente; p es una función de transición estocástica estacionaria, donde p(σ|s, a) es la probabilidad de transicionar al estado σ al ejecutar la acción a en el estado s; r es una función de recompensa estacionaria, donde r(s, a) especifica la recompensa obtenida al ejecutar la acción a en el estado s. Dado un MDP de este tipo, un problema de decisión bajo un horizonte finito T consiste en elegir una acción óptima en cada paso de tiempo para maximizar el valor esperado de la recompensa total acumulada durante la vida (finita) de los agentes. La política óptima del agente es entonces una función del estado actual s y el tiempo hasta el horizonte. Una política óptima para tal problema es actuar de forma codiciosa con respecto a la función de valor óptimo, definida de forma recursiva por el siguiente sistema de ecuaciones de Bellman de tiempo finito [2]: v(s, t) = max a r(s, a) + X σ p(σ|s, a)v(σ, t + 1), ∀s ∈ S, t ∈ [1, T − 1]; v(s, T) = 0, ∀s ∈ S; donde v(s, t) es el valor óptimo de estar en el estado s en el tiempo t ∈ [1, T]. Esta función de valor óptimo se puede calcular fácilmente utilizando programación dinámica, lo que conduce a la siguiente política óptima π, donde π(s, a, t) es la probabilidad de ejecutar la acción a en el estado s en el tiempo t: π(s, a, t) = (1, a = argmaxa r(s, a) + P σ p(σ|s, a)v(σ, t + 1), 0, en caso contrario. La anterior es la forma más común de calcular la función de valor óptimo (y por lo tanto una política óptima) para un MDP de horizonte finito. Sin embargo, también podemos formular el problema como el siguiente programa lineal (similarmente al LP dual para MDPs descontados de horizonte infinito [13, 6, 7]): max X s X a r(s, a) X t x(s, a, t) sujeto a: X a x(σ, a, t + 1) = X s,a p(σ|s, a)x(s, a, t) ∀σ, t ∈ [1, T − 1]; X a x(s, a, 1) = α(s), ∀s ∈ S; (1) donde α(s) es la distribución inicial sobre el espacio de estados, y x es la medida de ocupación (no estacionaria) (x(s, a, t) ∈ [0, 1] es el número total esperado de veces que se ejecuta la acción a en el estado s en el tiempo t). Una política óptima (no estacionaria) se obtiene a partir de la medida de ocupación de la siguiente manera: π(s, a, t) = x(s, a, t)/ X a x(s, a, t) ∀s ∈ S, t ∈ [1, T]. (2) Tenga en cuenta que el MDP estándar de horizonte finito sin restricciones, como se describe anteriormente, siempre tiene una solución uniformemente óptima (óptima para cualquier distribución inicial α(s)). Por lo tanto, una política óptima se puede obtener utilizando una constante arbitraria α(s) > 0 (en particular, α(s) = 1 resultará en x(s, a, t) = π(s, a, t)). Sin embargo, para los MDPs con restricciones de recursos (como se definen a continuación en la Sección 3), las políticas óptimas uniformemente no existen en general. En tales casos, α se convierte en parte de la entrada del problema, y una política resultante solo es óptima para ese α en particular. Este resultado es bien conocido para MDPs de horizonte infinito con varios tipos de restricciones [1, 6], y también se cumple para nuestro modelo de horizonte finito, el cual puede establecerse fácilmente a través de una línea de razonamiento completamente análoga a los argumentos en [6]. 2.2 Programación de Recursos Combinatoria Un enfoque directo para la programación de recursos para un conjunto de agentes M, cuyos valores para los recursos son inducidos por problemas de planificación estocástica (en nuestro caso, MDPs de horizonte finito) sería que cada agente enumerara todas las asignaciones de recursos posibles a lo largo del tiempo y, para cada una, calcular su valor resolviendo el MDP correspondiente. Entonces, cada agente proporcionaría valoraciones para cada posible conjunto de recursos a lo largo del tiempo a un coordinador centralizado, quien calcularía las asignaciones óptimas de recursos a lo largo del tiempo basándose en estas valoraciones. Cuando los recursos pueden asignarse en diferentes momentos a diferentes agentes, cada agente debe presentar valoraciones para cada combinación de posibles horizontes temporales. Que cada agente m ∈ M ejecute su MDP dentro del intervalo de tiempo de llegada-salida τ ∈ [τa m, τd m]. Por lo tanto, el agente m ejecutará un MDP con un horizonte temporal no mayor que Tm = τd m−τa m+1. Que bτ sea el horizonte temporal global para el problema, antes del cual todos los MDP de los agentes deben finalizar. Suponemos que τd m < bτ, ∀m ∈ M. El Sexto Congreso Internacional. En el problema de programación donde los agentes tienen requisitos de recursos estáticos dentro de sus MDPs de horizonte finito, los agentes proporcionan una valoración para cada conjunto de recursos para cada horizonte de tiempo posible (desde [1, Tm]) que puedan utilizar. Sea Ω el conjunto de recursos a ser asignados entre los agentes. Un agente recibirá como máximo un paquete de recursos para uno de los horizontes temporales. Que la variable ψ ∈ Ψm enumere todos los posibles pares de conjuntos de recursos y horizontes temporales para el agente m, por lo que hay 2|Ω| × Tm valores para ψ (el espacio de conjuntos es exponencial en el número de tipos de recursos |Ω|). El agente m debe proporcionar un valor vψ m para cada ψ, y el coordinador asignará como máximo un par (recurso, horizonte temporal) ψ a cada agente. Esta asignación se expresa como una variable indicadora zψ m ∈ {0, 1} que muestra si ψ está asignado al agente m. Para el tiempo τ y el recurso ω, la función nm(ψ, τ, ω) ∈ {0, 1} indica si el paquete en ψ utiliza el recurso ω en el tiempo τ (hacemos la suposición de que los agentes tienen requisitos de recursos binarios). Este problema de asignación es NP-completo, incluso al considerar solo un paso de tiempo, y su dificultad aumenta significativamente con múltiples pasos de tiempo debido al creciente número de valores de ψ. El problema de encontrar una asignación óptima que satisfaga la restricción global de que la cantidad de cada recurso ω asignada a todos los agentes no exceda la cantidad disponible bϕ(ω) se puede expresar como el siguiente programa entero: max X m∈M X ψ∈Ψm zψ mvψ m sujeto a: X ψ∈Ψm zψ m ≤ 1, ∀m ∈ M; X m∈M X ψ∈Ψm zψ mnm(ψ, τ, ω) ≤ bϕ(ω), ∀τ ∈ [1, bτ], ∀ω ∈ Ω; (3) La primera restricción en la ecuación 3 dice que ningún agente puede recibir más de un paquete, y la segunda restricción asegura que la asignación total del recurso ω no exceda, en ningún momento, el límite del recurso. Para el problema de programación en el que los agentes pueden reasignar dinámicamente recursos, cada agente debe especificar un valor para cada combinación de paquetes y pasos de tiempo dentro de su horizonte temporal. Que la variable ψ ∈ Ψm en este caso enumere todos los posibles conjuntos de recursos para los cuales a lo sumo un conjunto puede ser asignado al agente m en cada paso de tiempo. Por lo tanto, en este caso hay P t∈[1,Tm](2|Ω| )t ∼ 2|Ω|Tm posibilidades de combinaciones de recursos asignadas a diferentes intervalos de tiempo, para los Tm horizontes de tiempo diferentes. El mismo conjunto de ecuaciones (3) se puede utilizar para resolver este problema de programación dinámica, pero el programa entero es diferente debido a la diferencia en cómo se define ψ. En este caso, el número de valores de ψ es exponencial en el horizonte de planificación Tm de cada agente, lo que resulta en un programa mucho más grande. Este enfoque directo para resolver ambos problemas de programación requiere una enumeración y solución de 2|Ω| Tm (asignación estática) o P t∈[1,Tm] 2|Ω|t (reubicación dinámica) MDPs para cada agente, lo cual rápidamente se vuelve intratable con el crecimiento del número de recursos |Ω| o el horizonte de tiempo Tm. MODELO Y DECLARACIÓN DEL PROBLEMA Ahora presentamos formalmente nuestro modelo del problema de programación de recursos. La entrada del problema consiste en los siguientes componentes: • M, Ω, bϕ, τa m, τd m, bτ están definidos arriba en la Sección 2.2. • {Θm} = {S, A, pm, rm, αm} son los MDPs de todos los agentes m ∈ M. Sin pérdida de generalidad, asumimos que los espacios de estados y acciones de todos los agentes son iguales, pero cada uno tiene su propia función de transición pm, función de recompensa rm, y condiciones iniciales αm. • ϕm : A×Ω → {0, 1} es la asignación de acciones a recursos para el agente m. ϕm(a, ω) indica si la acción a del agente m necesita el recurso ω. Un agente m que recibe un conjunto de recursos que no incluye el recurso ω no puede ejecutar en su política MDP ninguna acción a para la cual ϕm(a, ω) = 0. Suponemos que todos los requisitos de recursos son binarios; como se discute más adelante en la Sección 6, esta suposición no es limitante. Dado el input anterior, el problema de optimización que consideramos es encontrar la asignación globalmente óptima que maximice la suma de recompensas esperadas de recursos a agentes para todos los pasos de tiempo: Δ : τ × M × Ω → {0, 1}. Una solución es factible si la asignación correspondiente de recursos a los agentes no viola la restricción global de recursos: X m Δm(τ, ω) ≤ bϕ(ω), ∀ω ∈ Ω, τ ∈ [1, bτ]. Consideramos dos variantes del problema de <br>programación de recursos</br>. La primera formulación restringe las asignaciones de recursos al espacio donde la asignación de recursos a cada agente es estática durante la vida de los agentes. La segunda formulación permite la reasignación de recursos entre agentes en cada paso de tiempo durante sus vidas. La Figura 1 representa un problema de <br>programación de recursos</br> con tres agentes M = {m1, m2, m3}, tres recursos Ω = {ω1, ω2, ω3}, y un horizonte de problema global de bτ = 11. Los tiempos de llegada y salida de los agentes se muestran como cajas grises y son {1, 6}, {3, 7} y {2, 11}, respectivamente. Una solución a este problema se muestra a través de barras horizontales dentro de cada casilla de agentes, donde las barras corresponden a la asignación de los tres tipos de recursos. La Figura 1a muestra una solución a un problema de programación estática. Según la solución mostrada, el agente m1 comienza la ejecución de su MDP en el tiempo τ = 1 y tiene un bloqueo en los tres recursos hasta que finaliza la ejecución en el tiempo τ = 3. Se debe tener en cuenta que el agente m1 renuncia a su control sobre los recursos antes de su hora de salida anunciada de τd m1 = 6, aparentemente porque otros agentes pueden utilizar los recursos de manera más efectiva. Por lo tanto, en el tiempo τ = 4, los recursos ω1 y ω3 se asignan al agente m2, quien luego los utiliza para ejecutar su MDP (utilizando solo acciones admitidas por los recursos ω1 y ω3) hasta el tiempo τ = 7. El agente m3 posee el recurso ω3 durante el intervalo τ ∈ [4, 10]. La Figura 1b muestra una posible solución a la versión dinámica del mismo problema. Allí, los recursos pueden ser reasignados entre agentes en cada paso de tiempo. Por ejemplo, el agente m1 renuncia a su uso del recurso ω2 en el tiempo τ = 2, aunque continúa la ejecución de su MDP hasta el tiempo τ = 6. Ten en cuenta que a un agente no se le permite detener y reiniciar su MDP, por lo que el agente m1 solo puede continuar ejecutando en el intervalo τ ∈ [3, 4] si tiene acciones que no requieren recursos (ϕm(a, ω) = 0). Claramente, el modelo y la declaración del problema descritos anteriormente hacen una serie de suposiciones sobre el problema y las propiedades de la solución deseada. Discutimos algunas de esas suposiciones y sus implicaciones en la Sección 6. 1222 La Sexta Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) (a) (b) Figura 1: Ilustración de una solución a un problema de <br>programación de recursos</br> con tres agentes y tres recursos: a) asignaciones de recursos estáticas (las asignaciones de recursos son constantes durante la vida de los agentes); b) asignación dinámica (las asignaciones de recursos pueden cambiar en cada paso de tiempo). 4. PROGRAMACIÓN DE RECURSOS Nuestro algoritmo de <br>programación de recursos</br> se lleva a cabo en dos etapas. Primero, realizamos un paso de preprocesamiento que aumenta los MDP de los agentes; este proceso se describe en la Sección 4.1. Segundo, utilizando estos MDPs aumentados construimos un problema de optimización global, el cual se describe en la Sección 4.2. 4.1 Aumentando los MDPs de los Agentes En el modelo descrito en la sección anterior, asumimos que si un agente no posee los recursos necesarios para realizar acciones en su MDP, su ejecución se detiene y el agente abandona el sistema. En otras palabras, los MDPs no pueden ser pausados y reanudados. Por ejemplo, en el problema mostrado en la Figura 1a, el agente m1 libera todos los recursos después del tiempo τ = 3, momento en el cual la ejecución de su MDP se detiene. De manera similar, los agentes m2 y m3 solo ejecutan sus MDPs en los intervalos τ ∈ [4, 6] y τ ∈ [4, 10], respectivamente. Por lo tanto, una parte importante del problema de toma de decisiones global es decidir la ventana de tiempo durante la cual cada uno de los agentes está activo (es decir, ejecutando su MDP). Para lograr esto, ampliamos el MDP de cada agente con dos nuevos estados (estados de inicio y final sb, sf, respectivamente) y una nueva acción de inicio/parada a∗, como se ilustra en la Figura 2. La idea es que un agente permanezca en el estado inicial sb hasta que esté listo para ejecutar su MDP, momento en el que realiza la acción de inicio/parada a∗ y transita al espacio de estados del MDP original con la probabilidad de transición que corresponde a la distribución inicial original α(s). Por ejemplo, en la Figura 1a, para el agente m2 esto ocurriría en el tiempo τ = 4. Una vez que el agente llega al final de su ventana de actividad (tiempo τ = 6 para el agente m2 en la Figura 1a), realiza la acción de inicio/parada, lo que lo lleva al estado final de finalización del sumidero sf en el tiempo τ = 7. Más precisamente, dado un MDP S, A, pm, rm, αm, definimos un MDP aumentado S, A, pm, rm, αm de la siguiente manera: S = S ∪ sb ∪ sf; A = A ∪ a∗; p(s|sb, a∗) = α(s), ∀s ∈ S; p(sb|sb, a) = 1.0, ∀a ∈ A; p(sf|s, a∗) = 1.0, ∀s ∈ S; p(σ|s, a) = p(σ|s, a), ∀s, σ ∈ S, a ∈ A; r(sb, a) = r(sf, a) = 0, ∀a ∈ A; r(s, a) = r(s, a), ∀s ∈ S, a ∈ A; α(sb) = 1; α(s) = 0, ∀s ∈ S; donde se asume que todas las probabilidades de transición no especificadas son cero. Además, para tener en cuenta el nuevo estado inicial, comenzamos el MDP un paso de tiempo antes, estableciendo τa m ← τa m − 1. Esto no afectará la asignación de recursos debido a que las restricciones de recursos solo se aplican a los estados originales del MDP, como se discutirá en la siguiente sección. Por ejemplo, los MDPs aumentados mostrados en la Figura 2b (que comienza en el estado sb en el tiempo τ = 2) se construirían a partir de un MDP con tiempo de llegada original τ = 3. La Figura 2b también muestra una trayectoria de muestra a través del espacio de estados: el agente comienza en el estado sb, transita al espacio de estados S del MDP original y finalmente sale al estado de sumidero sf. Cabe destacar que si quisiéramos modelar un problema en el que los agentes pudieran pausar sus MDP en pasos de tiempo arbitrarios (lo cual podría ser útil para dominios donde la reasignación dinámica es posible), podríamos lograrlo fácilmente incluyendo una acción adicional que transiciona de cada estado a sí mismo con una recompensa de cero. 4.2 MILP para la <br>programación de recursos</br> Dado un conjunto de MDPs aumentados, como se define arriba, el objetivo de esta sección es formular un programa de optimización global que resuelva el problema de <br>programación de recursos</br>. ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "task and resource allocation in agent system": {
            "translated_key": "Asignación de tareas y recursos en un sistema de agentes",
            "is_in_text": false,
            "original_annotated_sentences": [
                "Combinatorial Resource Scheduling for Multiagent MDPs Dmitri A. Dolgov, Michael R. James, and Michael E. Samples AI and Robotics Group Technical Research, Toyota Technical Center USA {ddolgov, michael.r.james, michael.samples}@gmail.com ABSTRACT Optimal resource scheduling in multiagent systems is a computationally challenging task, particularly when the values of resources are not additive.",
                "We consider the combinatorial problem of scheduling the usage of multiple resources among agents that operate in stochastic environments, modeled as Markov decision processes (MDPs).",
                "In recent years, efficient resource-allocation algorithms have been developed for agents with resource values induced by MDPs.",
                "However, this prior work has focused on static resource-allocation problems where resources are distributed once and then utilized in infinite-horizon MDPs.",
                "We extend those existing models to the problem of combinatorial resource scheduling, where agents persist only for finite periods between their (predefined) arrival and departure times, requiring resources only for those time periods.",
                "We provide a computationally efficient procedure for computing globally optimal resource assignments to agents over time.",
                "We illustrate and empirically analyze the method in the context of a stochastic jobscheduling domain.",
                "Categories and Subject Descriptors I.2.8 [Artificial Intelligence]: Problem Solving, Control Methods, and Search; I.2.11 [Artificial Intelligence]: Distributed Artificial Intelligence-Multiagent systems General Terms Algorithms, Performance, Design 1.",
                "INTRODUCTION The tasks of optimal resource allocation and scheduling are ubiquitous in multiagent systems, but solving such optimization problems can be computationally difficult, due to a number of factors.",
                "In particular, when the value of a set of resources to an agent is not additive (as is often the case with resources that are substitutes or complements), the utility function might have to be defined on an exponentially large space of resource bundles, which very quickly becomes computationally intractable.",
                "Further, even when each agent has a utility function that is nonzero only on a small subset of the possible resource bundles, obtaining optimal allocation is still computationally prohibitive, as the problem becomes NP-complete [14].",
                "Such computational issues have recently spawned several threads of work in using compact models of agents preferences.",
                "One idea is to use any structure present in utility functions to represent them compactly, via, for example, logical formulas [15, 10, 4, 3].",
                "An alternative is to directly model the mechanisms that define the agents utility functions and perform resource allocation directly with these models [9].",
                "A way of accomplishing this is to model the processes by which an agent might utilize the resources and define the utility function as the payoff of these processes.",
                "In particular, if an agent uses resources to act in a stochastic environment, its utility function can be naturally modeled with a Markov decision process, whose action set is parameterized by the available resources.",
                "This representation can then be used to construct very efficient resource-allocation algorithms that lead to an exponential speedup over a straightforward optimization problem with flat representations of combinatorial preferences [6, 7, 8].",
                "However, this existing work on resource allocation with preferences induced by resource-parameterized MDPs makes an assumption that the resources are only allocated once and are then utilized by the agents independently within their infinite-horizon MDPs.",
                "This assumption that no reallocation of resources is possible can be limiting in domains where agents arrive and depart dynamically.",
                "In this paper, we extend the work on resource allocation under MDP-induced preferences to discrete-time scheduling problems, where agents are present in the system for finite time intervals and can only use resources within these intervals.",
                "In particular, agents arrive and depart at arbitrary (predefined) times and within these intervals use resources to execute tasks in finite-horizon MDPs.",
                "We address the problem of globally optimal resource scheduling, where the objective is to find an allocation of resources to the agents across time that maximizes the sum of the expected rewards that they obtain.",
                "In this context, our main contribution is a mixed-integerprogramming formulation of the scheduling problem that chooses globally optimal resource assignments, starting times, and execution horizons for all agents (within their arrival1220 978-81-904262-7-5 (RPS) c 2007 IFAAMAS departure intervals).",
                "We analyze and empirically compare two flavors of the scheduling problem: one, where agents have static resource assignments within their finite-horizon MDPs, and another, where resources can be dynamically reallocated between agents at every time step.",
                "In the rest of the paper, we first lay down the necessary groundwork in Section 2 and then introduce our model and formal problem statement in Section 3.",
                "In Section 4.2, we describe our main result, the optimization program for globally optimal resource scheduling.",
                "Following the discussion of our experimental results on a job-scheduling problem in Section 5, we conclude in Section 6 with a discussion of possible extensions and generalizations of our method. 2.",
                "BACKGROUND Similarly to the model used in previous work on resourceallocation with MDP-induced preferences [6, 7], we define the value of a set of resources to an agent as the value of the best MDP policy that is realizable, given those resources.",
                "However, since the focus of our work is on scheduling problems, and a large part of the optimization problem is to decide how resources are allocated in time among agents with finite arrival and departure times, we model the agents planning problems as finite-horizon MDPs, in contrast to previous work that used infinite-horizon discounted MDPs.",
                "In the rest of this section, we first introduce some necessary background on finite-horizon MDPs and present a linear-programming formulation that serves as the basis for our solution algorithm developed in Section 4.",
                "We also outline the standard methods for combinatorial resource scheduling with flat resource values, which serve as a comparison benchmark for the new model developed here. 2.1 Markov Decision Processes A stationary, finite-domain, discrete-time MDP (see, for example, [13] for a thorough and detailed development) can be described as S, A, p, r , where: S is a finite set of system states; A is a finite set of actions that are available to the agent; p is a stationary stochastic transition function, where p(σ|s, a) is the probability of transitioning to state σ upon executing action a in state s; r is a stationary reward function, where r(s, a) specifies the reward obtained upon executing action a in state s. Given such an MDP, a decision problem under a finite horizon T is to choose an optimal action at every time step to maximize the expected value of the total reward accrued during the agents (finite) lifetime.",
                "The agents optimal policy is then a function of current state s and the time until the horizon.",
                "An optimal policy for such a problem is to act greedily with respect to the optimal value function, defined recursively by the following system of finite-time Bellman equations [2]: v(s, t) = max a r(s, a) + X σ p(σ|s, a)v(σ, t + 1), ∀s ∈ S, t ∈ [1, T − 1]; v(s, T) = 0, ∀s ∈ S; where v(s, t) is the optimal value of being in state s at time t ∈ [1, T].",
                "This optimal value function can be easily computed using dynamic programming, leading to the following optimal policy π, where π(s, a, t) is the probability of executing action a in state s at time t: π(s, a, t) = ( 1, a = argmaxa r(s, a) + P σ p(σ|s, a)v(σ, t + 1), 0, otherwise.",
                "The above is the most common way of computing the optimal value function (and therefore an optimal policy) for a finite-horizon MDP.",
                "However, we can also formulate the problem as the following linear program (similarly to the dual LP for infinite-horizon discounted MDPs [13, 6, 7]): max X s X a r(s, a) X t x(s, a, t) subject to: X a x(σ, a, t + 1) = X s,a p(σ|s, a)x(s, a, t) ∀σ, t ∈ [1, T − 1]; X a x(s, a, 1) = α(s), ∀s ∈ S; (1) where α(s) is the initial distribution over the state space, and x is the (non-stationary) occupation measure (x(s, a, t) ∈ [0, 1] is the total expected number of times action a is executed in state s at time t).",
                "An optimal (non-stationary) policy is obtained from the occupation measure as follows: π(s, a, t) = x(s, a, t)/ X a x(s, a, t) ∀s ∈ S, t ∈ [1, T]. (2) Note that the standard unconstrained finite-horizon MDP, as described above, always has a uniformly-optimal solution (optimal for any initial distribution α(s)).",
                "Therefore, an optimal policy can be obtained by using an arbitrary constant α(s) > 0 (in particular, α(s) = 1 will result in x(s, a, t) = π(s, a, t)).",
                "However, for MDPs with resource constraints (as defined below in Section 3), uniformly-optimal policies do not in general exist.",
                "In such cases, α becomes a part of the problem input, and a resulting policy is only optimal for that particular α.",
                "This result is well known for infinite-horizon MDPs with various types of constraints [1, 6], and it also holds for our finite-horizon model, which can be easily established via a line of reasoning completely analogous to the arguments in [6]. 2.2 Combinatorial Resource Scheduling A straightforward approach to resource scheduling for a set of agents M, whose values for the resources are induced by stochastic planning problems (in our case, finite-horizon MDPs) would be to have each agent enumerate all possible resource assignments over time and, for each one, compute its value by solving the corresponding MDP.",
                "Then, each agent would provide valuations for each possible resource bundle over time to a centralized coordinator, who would compute the optimal resource assignments across time based on these valuations.",
                "When resources can be allocated at different times to different agents, each agent must submit valuations for every combination of possible time horizons.",
                "Let each agent m ∈ M execute its MDP within the arrival-departure time interval τ ∈ [τa m, τd m].",
                "Hence, agent m will execute an MDP with time horizon no greater than Tm = τd m−τa m+1.",
                "Let bτ be the global time horizon for the problem, before which all of the agents MDPs must finish.",
                "We assume τd m < bτ, ∀m ∈ M. The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1221 For the scheduling problem where agents have static resource requirements within their finite-horizon MDPs, the agents provide a valuation for each resource bundle for each possible time horizon (from [1, Tm]) that they may use.",
                "Let Ω be the set of resources to be allocated among the agents.",
                "An agent will get at most one resource bundle for one of the time horizons.",
                "Let the variable ψ ∈ Ψm enumerate all possible pairs of resource bundles and time horizons for agent m, so there are 2|Ω| × Tm values for ψ (the space of bundles is exponential in the number of resource types |Ω|).",
                "The agent m must provide a value vψ m for each ψ, and the coordinator will allocate at most one ψ (resource, time horizon) pair to each agent.",
                "This allocation is expressed as an indicator variable zψ m ∈ {0, 1} that shows whether ψ is assigned to agent m. For time τ and resource ω, the function nm(ψ, τ, ω) ∈ {0, 1} indicates whether the bundle in ψ uses resource ω at time τ (we make the assumption that agents have binary resource requirements).",
                "This allocation problem is NP-complete, even when considering only a single time step, and its difficulty increases significantly with multiple time steps because of the increasing number of values of ψ.",
                "The problem of finding an optimal allocation that satisfies the global constraint that the amount of each resource ω allocated to all agents does not exceed the available amount bϕ(ω) can be expressed as the following integer program: max X m∈M X ψ∈Ψm zψ mvψ m subject to: X ψ∈Ψm zψ m ≤ 1, ∀m ∈ M; X m∈M X ψ∈Ψm zψ mnm(ψ, τ, ω) ≤ bϕ(ω), ∀τ ∈ [1, bτ], ∀ω ∈ Ω; (3) The first constraint in equation 3 says that no agent can receive more than one bundle, and the second constraint ensures that the total assignment of resource ω does not, at any time, exceed the resource bound.",
                "For the scheduling problem where the agents are able to dynamically reallocate resources, each agent must specify a value for every combination of bundles and time steps within its time horizon.",
                "Let the variable ψ ∈ Ψm in this case enumerate all possible resource bundles for which at most one bundle may be assigned to agent m at each time step.",
                "Therefore, in this case there are P t∈[1,Tm](2|Ω| )t ∼ 2|Ω|Tm possibilities of resource bundles assigned to different time slots, for the Tm different time horizons.",
                "The same set of equations (3) can be used to solve this dynamic scheduling problem, but the integer program is different because of the difference in how ψ is defined.",
                "In this case, the number of ψ values is exponential in each agents planning horizon Tm, resulting in a much larger program.",
                "This straightforward approach to solving both of these scheduling problems requires an enumeration and solution of either 2|Ω| Tm (static allocation) or P t∈[1,Tm] 2|Ω|t (dynamic reallocation) MDPs for each agent, which very quickly becomes intractable with the growth of the number of resources |Ω| or the time horizon Tm. 3.",
                "MODEL AND PROBLEM STATEMENT We now formally introduce our model of the resourcescheduling problem.",
                "The problem input consists of the following components: • M, Ω, bϕ, τa m, τd m, bτ are as defined above in Section 2.2. • {Θm} = {S, A, pm, rm, αm} are the MDPs of all agents m ∈ M. Without loss of generality, we assume that state and action spaces of all agents are the same, but each has its own transition function pm, reward function rm, and initial conditions αm. • ϕm : A×Ω → {0, 1} is the mapping of actions to resources for agent m. ϕm(a, ω) indicates whether action a of agent m needs resource ω.",
                "An agent m that receives a set of resources that does not include resource ω cannot execute in its MDP policy any action a for which ϕm(a, ω) = 0.",
                "We assume all resource requirements are binary; as discussed below in Section 6, this assumption is not limiting.",
                "Given the above input, the optimization problem we consider is to find the globally optimal-maximizing the sum of expected rewards-mapping of resources to agents for all time steps: Δ : τ × M × Ω → {0, 1}.",
                "A solution is feasible if the corresponding assignment of resources to the agents does not violate the global resource constraint: X m Δm(τ, ω) ≤ bϕ(ω), ∀ω ∈ Ω, τ ∈ [1, bτ]. (4) We consider two flavors of the resource-scheduling problem.",
                "The first formulation restricts resource assignments to the space where the allocation of resources to each agent is static during the agents lifetime.",
                "The second formulation allows reassignment of resources between agents at every time step within their lifetimes.",
                "Figure 1 depicts a resource-scheduling problem with three agents M = {m1, m2, m3}, three resources Ω = {ω1, ω2, ω3}, and a global problem horizon of bτ = 11.",
                "The agents arrival and departure times are shown as gray boxes and are {1, 6}, {3, 7}, and {2, 11}, respectively.",
                "A solution to this problem is shown via horizontal bars within each agents box, where the bars correspond to the allocation of the three resource types.",
                "Figure 1a shows a solution to a static scheduling problem.",
                "According to the shown solution, agent m1 begins the execution of its MDP at time τ = 1 and has a lock on all three resources until it finishes execution at time τ = 3.",
                "Note that agent m1 relinquishes its hold on the resources before its announced departure time of τd m1 = 6, ostensibly because other agents can utilize the resources more effectively.",
                "Thus, at time τ = 4, resources ω1 and ω3 are allocated to agent m2, who then uses them to execute its MDP (using only actions supported by resources ω1 and ω3) until time τ = 7.",
                "Agent m3 holds resource ω3 during the interval τ ∈ [4, 10].",
                "Figure 1b shows a possible solution to the dynamic version of the same problem.",
                "There, resources can be reallocated between agents at every time step.",
                "For example, agent m1 gives up its use of resource ω2 at time τ = 2, although it continues the execution of its MDP until time τ = 6.",
                "Notice that an agent is not allowed to stop and restart its MDP, so agent m1 is only able to continue executing in the interval τ ∈ [3, 4] if it has actions that do not require any resources (ϕm(a, ω) = 0).",
                "Clearly, the model and problem statement described above make a number of assumptions about the problem and the desired solution properties.",
                "We discuss some of those assumptions and their implications in Section 6. 1222 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) (a) (b) Figure 1: Illustration of a solution to a resource-scheduling problem with three agents and three resources: a) static resource assignments (resource assignments are constant within agents lifetimes; b) dynamic assignment (resource assignments are allowed to change at every time step). 4.",
                "RESOURCE SCHEDULING Our resource-scheduling algorithm proceeds in two stages.",
                "First, we perform a preprocessing step that augments the agent MDPs; this process is described in Section 4.1.",
                "Second, using these augmented MDPs we construct a global optimization problem, which is described in Section 4.2. 4.1 Augmenting Agents MDPs In the model described in the previous section, we assume that if an agent does not possess the necessary resources to perform actions in its MDP, its execution is halted and the agent leaves the system.",
                "In other words, the MDPs cannot be paused and resumed.",
                "For example, in the problem shown in Figure 1a, agent m1 releases all resources after time τ = 3, at which point the execution of its MDP is halted.",
                "Similarly, agents m2 and m3 only execute their MDPs in the intervals τ ∈ [4, 6] and τ ∈ [4, 10], respectively.",
                "Therefore, an important part of the global decision-making problem is to decide the window of time during which each of the agents is active (i.e., executing its MDP).",
                "To accomplish this, we augment each agents MDP with two new states (start and finish states sb , sf , respectively) and a new start/stop action a∗ , as illustrated in Figure 2.",
                "The idea is that an agent stays in the start state sb until it is ready to execute its MDP, at which point it performs the start/stop action a∗ and transitions into the state space of the original MDP with the transition probability that corresponds to the original initial distribution α(s).",
                "For example, in Figure 1a, for agent m2 this would happen at time τ = 4.",
                "Once the agent gets to the end of its activity window (time τ = 6 for agent m2 in Figure 1a), it performs the start/stop action, which takes it into the sink finish state sf at time τ = 7.",
                "More precisely, given an MDP S, A, pm, rm, αm , we define an augmented MDP S , A , pm, rm, αm as follows: S = S ∪ sb ∪ sf ; A = A ∪ a∗ ; p (s|sb , a∗ ) = α(s), ∀s ∈ S; p (sb |sb , a) = 1.0, ∀a ∈ A; p (sf |s, a∗ ) = 1.0, ∀s ∈ S; p (σ|s, a) = p(σ|s, a), ∀s, σ ∈ S, a ∈ A; r (sb , a) = r (sf , a) = 0, ∀a ∈ A ; r (s, a) = r(s, a), ∀s ∈ S, a ∈ A; α (sb ) = 1; α (s) = 0, ∀s ∈ S; where all non-specified transition probabilities are assumed to be zero.",
                "Further, in order to account for the new starting state, we begin the MDP one time-step earlier, setting τa m ← τa m − 1.",
                "This will not affect the resource allocation due to the resource constraints only being enforced for the original MDP states, as will be discussed in the next section.",
                "For example, the augmented MDPs shown in Figure 2b (which starts in state sb at time τ = 2) would be constructed from an MDP with original arrival time τ = 3.",
                "Figure 2b also shows a sample trajectory through the state space: the agent starts in state sb , transitions into the state space S of the original MDP, and finally exists into the sink state sf .",
                "Note that if we wanted to model a problem where agents could pause their MDPs at arbitrary time steps (which might be useful for domains where dynamic reallocation is possible), we could easily accomplish this by including an extra action that transitions from each state to itself with zero reward. 4.2 MILP for Resource Scheduling Given a set of augmented MDPs, as defined above, the goal of this section is to formulate a global optimization program that solves the resource-scheduling problem.",
                "In this section and below, all MDPs are assumed to be the augmented MDPs as defined in Section 4.1.",
                "Our approach is similar to the idea used in [6]: we begin with the linear-program formulation of agents MDPs (1) and augment it with constraints that ensure that the corresponding resource allocation across agents and time is valid.",
                "The resulting optimization problem then simultaneously solves the agents MDPs and resource-scheduling problems.",
                "In the rest of this section, we incrementally develop a mixed integer program (MILP) that achieves this.",
                "In the absence of resource constraints, the agents finitehorizon MDPs are completely independent, and the globally optimal solution can be trivially obtained via the following LP, which is simply an aggregation of single-agent finitehorizon LPs: max X m X s X a rm(s, a) X t xm(s, a, t) subject to: X a xm(σ, a, t + 1) = X s,a pm(σ|s, a)xm(s, a, t), ∀m ∈ M, σ ∈ S, t ∈ [1, Tm − 1]; X a xm(s, a, 1) = αm(s), ∀m ∈ M, s ∈ S; (12) where xm(s, a, t) is the occupation measure of agent m, and The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1223 (a) (b) Figure 2: Illustration of augmenting an MDP to allow for variable starting and stopping times: a) (left) the original two-state MDP with a single action; (right) the augmented MDP with new states sb and sf and the new action a∗ (note that the origianl transitions are not changed in the augmentation process); b) the augmented MDP displayed as a trajectory through time (grey lines indicate all transitions, while black lines indicate a given trajectory.",
                "Objective Function (sum of expected rewards over all agents) max X m X s X a rm(s, a) X t xm(s, a, t) (5) Meaning Implication Linear Constraints Tie x to θ.",
                "Agent is only active when occupation measure is nonzero in original MDP states. θm(τ) = 0 =⇒ xm(s, a, τ −τa m+1) = 0 ∀s /∈ {sb , sf }, a ∈ A X s/∈{sb,sf } X a xm(s, a, t) ≤ θm(τa m + t − 1) ∀m ∈ M, ∀t ∈ [1, Tm] (6) Agent can only be active in τ ∈ (τa m, τd m) θm(τ) = 0 ∀m ∈ M, τ /∈ (τa m, τd m) (7) Cannot use resources when not active θm(τ) = 0 =⇒ Δm(τ, ω) = 0 ∀τ ∈ [0, bτ], ω ∈ Ω Δm(τ, ω) ≤ θm(τ) ∀m ∈ M, τ ∈ [0, bτ], ω ∈ Ω (8) Tie x to Δ (nonzero x forces corresponding Δ to be nonzero.)",
                "Δm(τ, ω) = 0, ϕm(a, ω) = 1 =⇒ xm(s, a, τ − τa m + 1) = 0 ∀s /∈ {sb , sf } 1/|A| X a ϕm(a, ω) X s/∈{sb,sf } xm(s, a, t) ≤ Δm(t + τa m − 1, ω) ∀m ∈ M, ω ∈ Ω, t ∈ [1, Tm] (9) Resource bounds X m Δm(τ, ω) ≤ bϕ(ω) ∀ω ∈ Ω, τ ∈ [0, bτ] (10) Agent cannot change resources while active.",
                "Only enabled for scheduling with static assignments. θm(τ) = 1 and θm(τ + 1) = 1 =⇒ Δm(τ, ω) = Δm(τ + 1, ω) Δm(τ, ω) − Z(1 − θm(τ + 1)) ≤ Δm(τ + 1, ω) + Z(1 − θm(τ)) Δm(τ, ω) + Z(1 − θm(τ + 1)) ≥ Δm(τ + 1, ω) − Z(1 − θm(τ)) ∀m ∈ M, ω ∈ Ω, τ ∈ [0, bτ] (11) Table 1: MILP for globally optimal resource scheduling.",
                "Tm = τd m − τa m + 1 is the time horizon for the agents MDP.",
                "Using this LP as a basis, we augment it with constraints that ensure that the resource usage implied by the agents occupation measures {xm} does not violate the global resource requirements bϕ at any time step τ ∈ [0, bτ].",
                "To formulate these resource constraints, we use the following binary variables: • Δm(τ, ω) = {0, 1}, ∀m ∈ M, τ ∈ [0, bτ], ω ∈ Ω, which serve as indicator variables that define whether agent m possesses resource ω at time τ.",
                "These are analogous to the static indicator variables used in the one-shot static resource-allocation problem in [6]. • θm = {0, 1}, ∀m ∈ M, τ ∈ [0, bτ] are indicator variables that specify whether agent m is active (i.e., executing its MDP) at time τ.",
                "The meaning of resource-usage variables Δ is illustrated in Figure 1: Δm(τ, ω) = 1 only if resource ω is allocated to agent m at time τ.",
                "The meaning of the activity indicators θ is illustrated in Figure 2b: when agent m is in either the start state sb or the finish state sf , the corresponding θm = 0, but once the agent becomes active and enters one of the other states, we set θm = 1 .",
                "This meaning of θ can be enforced with a linear constraint that synchronizes the values of the agents occupation measures xm and the activity 1224 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) indicators θ, as shown in (6) in Table 1.",
                "Another constraint we have to add-because the activity indicators θ are defined on the global timeline τ-is to enforce the fact that the agent is inactive outside of its arrivaldeparture window.",
                "This is accomplished by constraint (7) in Table 1.",
                "Furthermore, agents should not be using resources while they are inactive.",
                "This constraint can also be enforced via a linear inequality on θ and Δ, as shown in (8).",
                "Constraint (6) sets the value of θ to match the policy defined by the occupation measure xm.",
                "In a similar fashion, we have to make sure that the resource-usage variables Δ are also synchronized with the occupation measure xm.",
                "This is done via constraint (9) in Table 1, which is nearly identical to the analogous constraint from [6].",
                "After implementing the above constraint, which enforces the meaning of Δ, we add a constraint that ensures that the agents resource usage never exceeds the amounts of available resources.",
                "This condition is also trivially expressed as a linear inequality (10) in Table 1.",
                "Finally, for the problem formulation where resource assignments are static during a lifetime of an agent, we add a constraint that ensures that the resource-usage variables Δ do not change their value while the agent is active (θ = 1).",
                "This is accomplished via the linear constraint (11), where Z ≥ 2 is a constant that is used to turn off the constraints when θm(τ) = 0 or θm(τ + 1) = 0.",
                "This constraint is not used for the dynamic problem formulation, where resources can be reallocated between agents at every time step.",
                "To summarize, Table 1 together with the conservationof-flow constraints from (12) defines the MILP that simultaneously computes an optimal resource assignment for all agents across time as well as optimal finite-horizon MDP policies that are valid under that resource assignment.",
                "As a rough measure of the complexity of this MILP, let us consider the number of optimization variables and constraints.",
                "Let TM = P Tm = P m(τa m − τd m + 1) be the sum of the lengths of the arrival-departure windows across all agents.",
                "Then, the number of optimization variables is: TM + bτ|M||Ω| + bτ|M|, TM of which are continuous (xm), and bτ|M||Ω| + bτ|M| are binary (Δ and θ).",
                "However, notice that all but TM|M| of the θ are set to zero by constraint (7), which also immediately forces all but TM|M||Ω| of the Δ to be zero via the constraints (8).",
                "The number of constraints (not including the degenerate constraints in (7)) in the MILP is: TM + TM|Ω| + bτ|Ω| + bτ|M||Ω|.",
                "Despite the fact that the complexity of the MILP is, in the worst case, exponential1 in the number of binary variables, the complexity of this MILP is significantly (exponentially) lower than that of the MILP with flat utility functions, described in Section 2.2.",
                "This result echos the efficiency gains reported in [6] for single-shot resource-allocation problems, but is much more pronounced, because of the explosion of the flat utility representation due to the temporal aspect of the problem (recall the prohibitive complexity of the combinatorial optimization in Section 2.2).",
                "We empirically analyze the performance of this method in Section 5. 1 Strictly speaking, solving MILPs to optimality is NPcomplete in the number of integer variables. 5.",
                "EXPERIMENTAL RESULTS Although the complexity of solving MILPs is in the worst case exponential in the number of integer variables, there are many efficient methods for solving MILPs that allow our algorithm to scale well for parameters common to resource allocation and scheduling problems.",
                "In particular, this section introduces a problem domain-the repairshop problem-used to empirically evaluate our algorithms scalability in terms of the number of agents |M|, the number of shared resources |Ω|, and the varied lengths of global time bτ during which agents may enter and exit the system.",
                "The repairshop problem is a simple parameterized MDP adopting the metaphor of a vehicular repair shop.",
                "Agents in the repair shop are mechanics with a number of independent tasks that yield reward only when completed.",
                "In our MDP model of this system, actions taken to advance through the state space are only allowed if the agent holds certain resources that are publicly available to the shop.",
                "These resources are in finite supply, and optimal policies for the shop will determine when each agent may hold the limited resources to take actions and earn individual rewards.",
                "Each task to be completed is associated with a single action, although the agent is required to repeat the action numerous times before completing the task and earning a reward.",
                "This model was parameterized in terms of the number of agents in the system, the number of different types of resources that could be linked to necessary actions, a global time during which agents are allowed to arrive and depart, and a maximum length for the number of time steps an agent may remain in the system.",
                "All datapoints in our experiments were obtained with 20 evaluations using CPLEX to solve the MILPs on a Pentium4 computer with 2Gb of RAM.",
                "Trials were conducted on both the static and the dynamic version of the resourcescheduling problem, as defined earlier.",
                "Figure 3 shows the runtime and policy value for independent modifications to the parameter set.",
                "The top row shows how the solution time for the MILP scales as we increase the number of agents |M|, the global time horizon bτ, and the number of resources |Ω|.",
                "Increasing the number of agents leads to exponential complexity scaling, which is to be expected for an NP-complete problem.",
                "However, increasing the global time limit bτ or the total number of resource types |Ω|-while holding the number of agents constantdoes not lead to decreased performance.",
                "This occurs because the problems get easier as they become under-constrained, which is also a common phenomenon for NP-complete problems.",
                "We also observe that the solution to the dynamic version of the problem can often be computed much faster than the static version.",
                "The bottom row of Figure 3 shows the joint policy value of the policies that correspond to the computed optimal resource-allocation schedules.",
                "We can observe that the dynamic version yields higher reward (as expected, since the reward for the dynamic version is always no less than the reward of the static version).",
                "We should point out that these graphs should not be viewed as a measure of performance of two different algorithms (both algorithms produce optimal solutions but to different problems), but rather as observations about how the quality of optimal solutions change as more flexibility is allowed in the reallocation of resources.",
                "Figure 4 shows runtime and policy value for trials in which common input variables are scaled together.",
                "This allows The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1225 2 4 6 8 10 10 −3 10 −2 10 −1 10 0 10 1 10 2 10 3 10 4 Number of Agents |M| CPUTime,sec |Ω| = 5, τ = 50 static dynamic 50 100 150 200 10 −2 10 −1 10 0 10 1 10 2 10 3 Global Time Boundary τ CPUTime,sec |M| = 5, |Ω| = 5 static dynamic 10 20 30 40 50 10 −2 10 −1 10 0 10 1 10 2 Number of Resources |Ω| CPUTime,sec |M| = 5, τ = 50 static dynamic 2 4 6 8 10 200 400 600 800 1000 1200 1400 1600 Number of Agents |M| Value |Ω| = 5, τ = 50 static dynamic 50 100 150 200 400 500 600 700 800 900 1000 1100 1200 1300 1400 Global Time Boundary τ Value |M| = 5, |Ω| = 5 static dynamic 10 20 30 40 50 500 600 700 800 900 1000 1100 1200 1300 1400 Number of Resources |Ω| Value |M| = 5, τ = 50 static dynamic Figure 3: Evaluation of our MILP for variable numbers of agents (column 1), lengths of global-time window (column 2), and numbers of resource types (column 3).",
                "Top row shows CPU time, and bottom row shows the joint reward of agents MDP policies.",
                "Error bars show the 1st and 3rd quartiles (25% and 75%). 2 4 6 8 10 10 −3 10 −2 10 −1 10 0 10 1 10 2 10 3 Number of Agents |M| CPUTime,sec τ = 10|M| static dynamic 2 4 6 8 10 10 −3 10 −2 10 −1 10 0 10 1 10 2 10 3 10 4 Number of Agents |M| CPUTime,sec |Ω| = 2|M| static dynamic 2 4 6 8 10 10 −3 10 −2 10 −1 10 0 10 1 10 2 10 3 10 4 Number of Agents |M| CPUTime,sec |Ω| = 5|M| static dynamic 2 4 6 8 10 200 400 600 800 1000 1200 1400 1600 1800 2000 2200 Number of Agents |M| Value τ = 10|M| static dynamic 2 4 6 8 10 200 400 600 800 1000 1200 1400 1600 1800 2000 Number of Agents |M| Value |Ω| = 2|M| static dynamic 2 4 6 8 10 0 500 1000 1500 2000 2500 Number of Agents |M| Value |Ω| = 5|M| static dynamic Figure 4: Evaluation of our MILP using correlated input variables.",
                "The left column tracks the performance and CPU time as the number of agents and global-time window increase together (bτ = 10|M|).",
                "The middle and the right column track the performance and CPU time as the number of resources and the number of agents increase together as |Ω| = 2|M| and |Ω| = 5|M|, respectively.",
                "Error bars show the 1st and 3rd quartiles (25% and 75%). 1226 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) us to explore domains where the total number of agents scales proportionally to the total number of resource types or the global time horizon, while keeping constant the average agent density (per unit of global time) or the average number of resources per agent (which commonly occurs in real-life applications).",
                "Overall, we believe that these experimental results indicate that our MILP formulation can be used to effectively solve resource-scheduling problems of nontrivial size. 6.",
                "DISCUSSION AND CONCLUSIONS Throughout the paper, we have made a number of assumptions in our model and solution algorithm; we discuss their implications below. • Continual execution.",
                "We assume that once an agent stops executing its MDP (transitions into state sf ), it exits the system and cannot return.",
                "It is easy to relax this assumption for domains where agents MDPs can be paused and restarted.",
                "All that is required is to include an additional pause action which transitions from a given state back to itself, and has zero reward. • Indifference to start time.",
                "We used a reward model where agents rewards depend only on the time horizon of their MDPs and not the global start time.",
                "This is a consequence of our MDP-augmentation procedure from Section 4.1.",
                "It is easy to extend the model so that the agents incur an explicit penalty for idling by assigning a non-zero negative reward to the start state sb . • Binary resource requirements.",
                "For simplicity, we have assumed that resource costs are binary: ϕm(a, ω) = {0, 1}, but our results generalize in a straightforward manner to non-binary resource mappings, analogously to the procedure used in [5]. • Cooperative agents.",
                "The optimization procedure discussed in this paper was developed in the context of cooperative agents, but it can also be used to design a mechanism for scheduling resources among self-interested agents.",
                "This optimization procedure can be embedded in a VickreyClarke-Groves auction, completely analogously to the way it was done in [7].",
                "In fact, all the results of [7] about the properties of the auction and information privacy directly carry over to the scheduling domain discussed in this paper, requiring only slight modifications to deal with finitehorizon MDPs. • Known, deterministic arrival and departure times.",
                "Finally, we have assumed that agents arrival and departure times (τa m and τd m) are deterministic and known a priori.",
                "This assumption is fundamental to our solution method.",
                "While there are many domains where this assumption is valid, in many cases agents arrive and depart dynamically and their arrival and departure times can only be predicted probabilistically, leading to online resource-allocation problems.",
                "In particular, in the case of self-interested agents, this becomes an interesting version of an online-mechanism-design problem [11, 12].",
                "In summary, we have presented an MILP formulation for the combinatorial resource-scheduling problem where agents values for possible resource assignments are defined by finitehorizon MDPs.",
                "This result extends previous work ([6, 7]) on static one-shot resource allocation under MDP-induced preferences to resource-scheduling problems with a temporal aspect.",
                "As such, this work takes a step in the direction of designing an online mechanism for agents with combinatorial resource preferences induced by stochastic planning problems.",
                "Relaxing the assumption about deterministic arrival and departure times of the agents is a focus of our future work.",
                "We would like to thank the anonymous reviewers for their insightful comments and suggestions. 7.",
                "REFERENCES [1] E. Altman and A. Shwartz.",
                "Adaptive control of constrained Markov chains: Criteria and policies.",
                "Annals of Operations Research, special issue on Markov Decision Processes, 28:101-134, 1991. [2] R. Bellman.",
                "Dynamic Programming.",
                "Princeton University Press, 1957. [3] C. Boutilier.",
                "Solving concisely expressed combinatorial auction problems.",
                "In Proc. of AAAI-02, pages 359-366, 2002. [4] C. Boutilier and H. H. Hoos.",
                "Bidding languages for combinatorial auctions.",
                "In Proc. of IJCAI-01, pages 1211-1217, 2001. [5] D. Dolgov.",
                "Integrated Resource Allocation and Planning in Stochastic Multiagent Environments.",
                "PhD thesis, Computer Science Department, University of Michigan, February 2006. [6] D. A. Dolgov and E. H. Durfee.",
                "Optimal resource allocation and policy formulation in loosely-coupled Markov decision processes.",
                "In Proc. of ICAPS-04, pages 315-324, June 2004. [7] D. A. Dolgov and E. H. Durfee.",
                "Computationally efficient combinatorial auctions for resource allocation in weakly-coupled MDPs.",
                "In Proc. of AAMAS-05, New York, NY, USA, 2005.",
                "ACM Press. [8] D. A. Dolgov and E. H. Durfee.",
                "Resource allocation among agents with preferences induced by factored MDPs.",
                "In Proc. of AAMAS-06, 2006. [9] K. Larson and T. Sandholm.",
                "Mechanism design and deliberative agents.",
                "In Proc. of AAMAS-05, pages 650-656, New York, NY, USA, 2005.",
                "ACM Press. [10] N. Nisan.",
                "Bidding and allocation in combinatorial auctions.",
                "In Electronic Commerce, 2000. [11] D. C. Parkes and S. Singh.",
                "An MDP-based approach to Online Mechanism Design.",
                "In Proc. of the Seventeenths Annual Conference on Neural Information Processing Systems (NIPS-03), 2003. [12] D. C. Parkes, S. Singh, and D. Yanovsky.",
                "Approximately efficient online mechanism design.",
                "In Proc. of the Eighteenths Annual Conference on Neural Information Processing Systems (NIPS-04), 2004. [13] M. L. Puterman.",
                "Markov Decision Processes.",
                "John Wiley & Sons, New York, 1994. [14] M. H. Rothkopf, A. Pekec, and R. M. Harstad.",
                "Computationally manageable combinational auctions.",
                "Management Science, 44(8):1131-1147, 1998. [15] T. Sandholm.",
                "An algorithm for optimal winner determination in combinatorial auctions.",
                "In Proc. of IJCAI-99, pages 542-547, San Francisco, CA, USA, 1999.",
                "Morgan Kaufmann Publishers Inc.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1227"
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [],
            "translated_text": "",
            "candidates": [],
            "error": [
                []
            ]
        },
        "multiagent plan": {
            "translated_key": "plan multiagente",
            "is_in_text": false,
            "original_annotated_sentences": [
                "Combinatorial Resource Scheduling for Multiagent MDPs Dmitri A. Dolgov, Michael R. James, and Michael E. Samples AI and Robotics Group Technical Research, Toyota Technical Center USA {ddolgov, michael.r.james, michael.samples}@gmail.com ABSTRACT Optimal resource scheduling in multiagent systems is a computationally challenging task, particularly when the values of resources are not additive.",
                "We consider the combinatorial problem of scheduling the usage of multiple resources among agents that operate in stochastic environments, modeled as Markov decision processes (MDPs).",
                "In recent years, efficient resource-allocation algorithms have been developed for agents with resource values induced by MDPs.",
                "However, this prior work has focused on static resource-allocation problems where resources are distributed once and then utilized in infinite-horizon MDPs.",
                "We extend those existing models to the problem of combinatorial resource scheduling, where agents persist only for finite periods between their (predefined) arrival and departure times, requiring resources only for those time periods.",
                "We provide a computationally efficient procedure for computing globally optimal resource assignments to agents over time.",
                "We illustrate and empirically analyze the method in the context of a stochastic jobscheduling domain.",
                "Categories and Subject Descriptors I.2.8 [Artificial Intelligence]: Problem Solving, Control Methods, and Search; I.2.11 [Artificial Intelligence]: Distributed Artificial Intelligence-Multiagent systems General Terms Algorithms, Performance, Design 1.",
                "INTRODUCTION The tasks of optimal resource allocation and scheduling are ubiquitous in multiagent systems, but solving such optimization problems can be computationally difficult, due to a number of factors.",
                "In particular, when the value of a set of resources to an agent is not additive (as is often the case with resources that are substitutes or complements), the utility function might have to be defined on an exponentially large space of resource bundles, which very quickly becomes computationally intractable.",
                "Further, even when each agent has a utility function that is nonzero only on a small subset of the possible resource bundles, obtaining optimal allocation is still computationally prohibitive, as the problem becomes NP-complete [14].",
                "Such computational issues have recently spawned several threads of work in using compact models of agents preferences.",
                "One idea is to use any structure present in utility functions to represent them compactly, via, for example, logical formulas [15, 10, 4, 3].",
                "An alternative is to directly model the mechanisms that define the agents utility functions and perform resource allocation directly with these models [9].",
                "A way of accomplishing this is to model the processes by which an agent might utilize the resources and define the utility function as the payoff of these processes.",
                "In particular, if an agent uses resources to act in a stochastic environment, its utility function can be naturally modeled with a Markov decision process, whose action set is parameterized by the available resources.",
                "This representation can then be used to construct very efficient resource-allocation algorithms that lead to an exponential speedup over a straightforward optimization problem with flat representations of combinatorial preferences [6, 7, 8].",
                "However, this existing work on resource allocation with preferences induced by resource-parameterized MDPs makes an assumption that the resources are only allocated once and are then utilized by the agents independently within their infinite-horizon MDPs.",
                "This assumption that no reallocation of resources is possible can be limiting in domains where agents arrive and depart dynamically.",
                "In this paper, we extend the work on resource allocation under MDP-induced preferences to discrete-time scheduling problems, where agents are present in the system for finite time intervals and can only use resources within these intervals.",
                "In particular, agents arrive and depart at arbitrary (predefined) times and within these intervals use resources to execute tasks in finite-horizon MDPs.",
                "We address the problem of globally optimal resource scheduling, where the objective is to find an allocation of resources to the agents across time that maximizes the sum of the expected rewards that they obtain.",
                "In this context, our main contribution is a mixed-integerprogramming formulation of the scheduling problem that chooses globally optimal resource assignments, starting times, and execution horizons for all agents (within their arrival1220 978-81-904262-7-5 (RPS) c 2007 IFAAMAS departure intervals).",
                "We analyze and empirically compare two flavors of the scheduling problem: one, where agents have static resource assignments within their finite-horizon MDPs, and another, where resources can be dynamically reallocated between agents at every time step.",
                "In the rest of the paper, we first lay down the necessary groundwork in Section 2 and then introduce our model and formal problem statement in Section 3.",
                "In Section 4.2, we describe our main result, the optimization program for globally optimal resource scheduling.",
                "Following the discussion of our experimental results on a job-scheduling problem in Section 5, we conclude in Section 6 with a discussion of possible extensions and generalizations of our method. 2.",
                "BACKGROUND Similarly to the model used in previous work on resourceallocation with MDP-induced preferences [6, 7], we define the value of a set of resources to an agent as the value of the best MDP policy that is realizable, given those resources.",
                "However, since the focus of our work is on scheduling problems, and a large part of the optimization problem is to decide how resources are allocated in time among agents with finite arrival and departure times, we model the agents planning problems as finite-horizon MDPs, in contrast to previous work that used infinite-horizon discounted MDPs.",
                "In the rest of this section, we first introduce some necessary background on finite-horizon MDPs and present a linear-programming formulation that serves as the basis for our solution algorithm developed in Section 4.",
                "We also outline the standard methods for combinatorial resource scheduling with flat resource values, which serve as a comparison benchmark for the new model developed here. 2.1 Markov Decision Processes A stationary, finite-domain, discrete-time MDP (see, for example, [13] for a thorough and detailed development) can be described as S, A, p, r , where: S is a finite set of system states; A is a finite set of actions that are available to the agent; p is a stationary stochastic transition function, where p(σ|s, a) is the probability of transitioning to state σ upon executing action a in state s; r is a stationary reward function, where r(s, a) specifies the reward obtained upon executing action a in state s. Given such an MDP, a decision problem under a finite horizon T is to choose an optimal action at every time step to maximize the expected value of the total reward accrued during the agents (finite) lifetime.",
                "The agents optimal policy is then a function of current state s and the time until the horizon.",
                "An optimal policy for such a problem is to act greedily with respect to the optimal value function, defined recursively by the following system of finite-time Bellman equations [2]: v(s, t) = max a r(s, a) + X σ p(σ|s, a)v(σ, t + 1), ∀s ∈ S, t ∈ [1, T − 1]; v(s, T) = 0, ∀s ∈ S; where v(s, t) is the optimal value of being in state s at time t ∈ [1, T].",
                "This optimal value function can be easily computed using dynamic programming, leading to the following optimal policy π, where π(s, a, t) is the probability of executing action a in state s at time t: π(s, a, t) = ( 1, a = argmaxa r(s, a) + P σ p(σ|s, a)v(σ, t + 1), 0, otherwise.",
                "The above is the most common way of computing the optimal value function (and therefore an optimal policy) for a finite-horizon MDP.",
                "However, we can also formulate the problem as the following linear program (similarly to the dual LP for infinite-horizon discounted MDPs [13, 6, 7]): max X s X a r(s, a) X t x(s, a, t) subject to: X a x(σ, a, t + 1) = X s,a p(σ|s, a)x(s, a, t) ∀σ, t ∈ [1, T − 1]; X a x(s, a, 1) = α(s), ∀s ∈ S; (1) where α(s) is the initial distribution over the state space, and x is the (non-stationary) occupation measure (x(s, a, t) ∈ [0, 1] is the total expected number of times action a is executed in state s at time t).",
                "An optimal (non-stationary) policy is obtained from the occupation measure as follows: π(s, a, t) = x(s, a, t)/ X a x(s, a, t) ∀s ∈ S, t ∈ [1, T]. (2) Note that the standard unconstrained finite-horizon MDP, as described above, always has a uniformly-optimal solution (optimal for any initial distribution α(s)).",
                "Therefore, an optimal policy can be obtained by using an arbitrary constant α(s) > 0 (in particular, α(s) = 1 will result in x(s, a, t) = π(s, a, t)).",
                "However, for MDPs with resource constraints (as defined below in Section 3), uniformly-optimal policies do not in general exist.",
                "In such cases, α becomes a part of the problem input, and a resulting policy is only optimal for that particular α.",
                "This result is well known for infinite-horizon MDPs with various types of constraints [1, 6], and it also holds for our finite-horizon model, which can be easily established via a line of reasoning completely analogous to the arguments in [6]. 2.2 Combinatorial Resource Scheduling A straightforward approach to resource scheduling for a set of agents M, whose values for the resources are induced by stochastic planning problems (in our case, finite-horizon MDPs) would be to have each agent enumerate all possible resource assignments over time and, for each one, compute its value by solving the corresponding MDP.",
                "Then, each agent would provide valuations for each possible resource bundle over time to a centralized coordinator, who would compute the optimal resource assignments across time based on these valuations.",
                "When resources can be allocated at different times to different agents, each agent must submit valuations for every combination of possible time horizons.",
                "Let each agent m ∈ M execute its MDP within the arrival-departure time interval τ ∈ [τa m, τd m].",
                "Hence, agent m will execute an MDP with time horizon no greater than Tm = τd m−τa m+1.",
                "Let bτ be the global time horizon for the problem, before which all of the agents MDPs must finish.",
                "We assume τd m < bτ, ∀m ∈ M. The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1221 For the scheduling problem where agents have static resource requirements within their finite-horizon MDPs, the agents provide a valuation for each resource bundle for each possible time horizon (from [1, Tm]) that they may use.",
                "Let Ω be the set of resources to be allocated among the agents.",
                "An agent will get at most one resource bundle for one of the time horizons.",
                "Let the variable ψ ∈ Ψm enumerate all possible pairs of resource bundles and time horizons for agent m, so there are 2|Ω| × Tm values for ψ (the space of bundles is exponential in the number of resource types |Ω|).",
                "The agent m must provide a value vψ m for each ψ, and the coordinator will allocate at most one ψ (resource, time horizon) pair to each agent.",
                "This allocation is expressed as an indicator variable zψ m ∈ {0, 1} that shows whether ψ is assigned to agent m. For time τ and resource ω, the function nm(ψ, τ, ω) ∈ {0, 1} indicates whether the bundle in ψ uses resource ω at time τ (we make the assumption that agents have binary resource requirements).",
                "This allocation problem is NP-complete, even when considering only a single time step, and its difficulty increases significantly with multiple time steps because of the increasing number of values of ψ.",
                "The problem of finding an optimal allocation that satisfies the global constraint that the amount of each resource ω allocated to all agents does not exceed the available amount bϕ(ω) can be expressed as the following integer program: max X m∈M X ψ∈Ψm zψ mvψ m subject to: X ψ∈Ψm zψ m ≤ 1, ∀m ∈ M; X m∈M X ψ∈Ψm zψ mnm(ψ, τ, ω) ≤ bϕ(ω), ∀τ ∈ [1, bτ], ∀ω ∈ Ω; (3) The first constraint in equation 3 says that no agent can receive more than one bundle, and the second constraint ensures that the total assignment of resource ω does not, at any time, exceed the resource bound.",
                "For the scheduling problem where the agents are able to dynamically reallocate resources, each agent must specify a value for every combination of bundles and time steps within its time horizon.",
                "Let the variable ψ ∈ Ψm in this case enumerate all possible resource bundles for which at most one bundle may be assigned to agent m at each time step.",
                "Therefore, in this case there are P t∈[1,Tm](2|Ω| )t ∼ 2|Ω|Tm possibilities of resource bundles assigned to different time slots, for the Tm different time horizons.",
                "The same set of equations (3) can be used to solve this dynamic scheduling problem, but the integer program is different because of the difference in how ψ is defined.",
                "In this case, the number of ψ values is exponential in each agents planning horizon Tm, resulting in a much larger program.",
                "This straightforward approach to solving both of these scheduling problems requires an enumeration and solution of either 2|Ω| Tm (static allocation) or P t∈[1,Tm] 2|Ω|t (dynamic reallocation) MDPs for each agent, which very quickly becomes intractable with the growth of the number of resources |Ω| or the time horizon Tm. 3.",
                "MODEL AND PROBLEM STATEMENT We now formally introduce our model of the resourcescheduling problem.",
                "The problem input consists of the following components: • M, Ω, bϕ, τa m, τd m, bτ are as defined above in Section 2.2. • {Θm} = {S, A, pm, rm, αm} are the MDPs of all agents m ∈ M. Without loss of generality, we assume that state and action spaces of all agents are the same, but each has its own transition function pm, reward function rm, and initial conditions αm. • ϕm : A×Ω → {0, 1} is the mapping of actions to resources for agent m. ϕm(a, ω) indicates whether action a of agent m needs resource ω.",
                "An agent m that receives a set of resources that does not include resource ω cannot execute in its MDP policy any action a for which ϕm(a, ω) = 0.",
                "We assume all resource requirements are binary; as discussed below in Section 6, this assumption is not limiting.",
                "Given the above input, the optimization problem we consider is to find the globally optimal-maximizing the sum of expected rewards-mapping of resources to agents for all time steps: Δ : τ × M × Ω → {0, 1}.",
                "A solution is feasible if the corresponding assignment of resources to the agents does not violate the global resource constraint: X m Δm(τ, ω) ≤ bϕ(ω), ∀ω ∈ Ω, τ ∈ [1, bτ]. (4) We consider two flavors of the resource-scheduling problem.",
                "The first formulation restricts resource assignments to the space where the allocation of resources to each agent is static during the agents lifetime.",
                "The second formulation allows reassignment of resources between agents at every time step within their lifetimes.",
                "Figure 1 depicts a resource-scheduling problem with three agents M = {m1, m2, m3}, three resources Ω = {ω1, ω2, ω3}, and a global problem horizon of bτ = 11.",
                "The agents arrival and departure times are shown as gray boxes and are {1, 6}, {3, 7}, and {2, 11}, respectively.",
                "A solution to this problem is shown via horizontal bars within each agents box, where the bars correspond to the allocation of the three resource types.",
                "Figure 1a shows a solution to a static scheduling problem.",
                "According to the shown solution, agent m1 begins the execution of its MDP at time τ = 1 and has a lock on all three resources until it finishes execution at time τ = 3.",
                "Note that agent m1 relinquishes its hold on the resources before its announced departure time of τd m1 = 6, ostensibly because other agents can utilize the resources more effectively.",
                "Thus, at time τ = 4, resources ω1 and ω3 are allocated to agent m2, who then uses them to execute its MDP (using only actions supported by resources ω1 and ω3) until time τ = 7.",
                "Agent m3 holds resource ω3 during the interval τ ∈ [4, 10].",
                "Figure 1b shows a possible solution to the dynamic version of the same problem.",
                "There, resources can be reallocated between agents at every time step.",
                "For example, agent m1 gives up its use of resource ω2 at time τ = 2, although it continues the execution of its MDP until time τ = 6.",
                "Notice that an agent is not allowed to stop and restart its MDP, so agent m1 is only able to continue executing in the interval τ ∈ [3, 4] if it has actions that do not require any resources (ϕm(a, ω) = 0).",
                "Clearly, the model and problem statement described above make a number of assumptions about the problem and the desired solution properties.",
                "We discuss some of those assumptions and their implications in Section 6. 1222 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) (a) (b) Figure 1: Illustration of a solution to a resource-scheduling problem with three agents and three resources: a) static resource assignments (resource assignments are constant within agents lifetimes; b) dynamic assignment (resource assignments are allowed to change at every time step). 4.",
                "RESOURCE SCHEDULING Our resource-scheduling algorithm proceeds in two stages.",
                "First, we perform a preprocessing step that augments the agent MDPs; this process is described in Section 4.1.",
                "Second, using these augmented MDPs we construct a global optimization problem, which is described in Section 4.2. 4.1 Augmenting Agents MDPs In the model described in the previous section, we assume that if an agent does not possess the necessary resources to perform actions in its MDP, its execution is halted and the agent leaves the system.",
                "In other words, the MDPs cannot be paused and resumed.",
                "For example, in the problem shown in Figure 1a, agent m1 releases all resources after time τ = 3, at which point the execution of its MDP is halted.",
                "Similarly, agents m2 and m3 only execute their MDPs in the intervals τ ∈ [4, 6] and τ ∈ [4, 10], respectively.",
                "Therefore, an important part of the global decision-making problem is to decide the window of time during which each of the agents is active (i.e., executing its MDP).",
                "To accomplish this, we augment each agents MDP with two new states (start and finish states sb , sf , respectively) and a new start/stop action a∗ , as illustrated in Figure 2.",
                "The idea is that an agent stays in the start state sb until it is ready to execute its MDP, at which point it performs the start/stop action a∗ and transitions into the state space of the original MDP with the transition probability that corresponds to the original initial distribution α(s).",
                "For example, in Figure 1a, for agent m2 this would happen at time τ = 4.",
                "Once the agent gets to the end of its activity window (time τ = 6 for agent m2 in Figure 1a), it performs the start/stop action, which takes it into the sink finish state sf at time τ = 7.",
                "More precisely, given an MDP S, A, pm, rm, αm , we define an augmented MDP S , A , pm, rm, αm as follows: S = S ∪ sb ∪ sf ; A = A ∪ a∗ ; p (s|sb , a∗ ) = α(s), ∀s ∈ S; p (sb |sb , a) = 1.0, ∀a ∈ A; p (sf |s, a∗ ) = 1.0, ∀s ∈ S; p (σ|s, a) = p(σ|s, a), ∀s, σ ∈ S, a ∈ A; r (sb , a) = r (sf , a) = 0, ∀a ∈ A ; r (s, a) = r(s, a), ∀s ∈ S, a ∈ A; α (sb ) = 1; α (s) = 0, ∀s ∈ S; where all non-specified transition probabilities are assumed to be zero.",
                "Further, in order to account for the new starting state, we begin the MDP one time-step earlier, setting τa m ← τa m − 1.",
                "This will not affect the resource allocation due to the resource constraints only being enforced for the original MDP states, as will be discussed in the next section.",
                "For example, the augmented MDPs shown in Figure 2b (which starts in state sb at time τ = 2) would be constructed from an MDP with original arrival time τ = 3.",
                "Figure 2b also shows a sample trajectory through the state space: the agent starts in state sb , transitions into the state space S of the original MDP, and finally exists into the sink state sf .",
                "Note that if we wanted to model a problem where agents could pause their MDPs at arbitrary time steps (which might be useful for domains where dynamic reallocation is possible), we could easily accomplish this by including an extra action that transitions from each state to itself with zero reward. 4.2 MILP for Resource Scheduling Given a set of augmented MDPs, as defined above, the goal of this section is to formulate a global optimization program that solves the resource-scheduling problem.",
                "In this section and below, all MDPs are assumed to be the augmented MDPs as defined in Section 4.1.",
                "Our approach is similar to the idea used in [6]: we begin with the linear-program formulation of agents MDPs (1) and augment it with constraints that ensure that the corresponding resource allocation across agents and time is valid.",
                "The resulting optimization problem then simultaneously solves the agents MDPs and resource-scheduling problems.",
                "In the rest of this section, we incrementally develop a mixed integer program (MILP) that achieves this.",
                "In the absence of resource constraints, the agents finitehorizon MDPs are completely independent, and the globally optimal solution can be trivially obtained via the following LP, which is simply an aggregation of single-agent finitehorizon LPs: max X m X s X a rm(s, a) X t xm(s, a, t) subject to: X a xm(σ, a, t + 1) = X s,a pm(σ|s, a)xm(s, a, t), ∀m ∈ M, σ ∈ S, t ∈ [1, Tm − 1]; X a xm(s, a, 1) = αm(s), ∀m ∈ M, s ∈ S; (12) where xm(s, a, t) is the occupation measure of agent m, and The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1223 (a) (b) Figure 2: Illustration of augmenting an MDP to allow for variable starting and stopping times: a) (left) the original two-state MDP with a single action; (right) the augmented MDP with new states sb and sf and the new action a∗ (note that the origianl transitions are not changed in the augmentation process); b) the augmented MDP displayed as a trajectory through time (grey lines indicate all transitions, while black lines indicate a given trajectory.",
                "Objective Function (sum of expected rewards over all agents) max X m X s X a rm(s, a) X t xm(s, a, t) (5) Meaning Implication Linear Constraints Tie x to θ.",
                "Agent is only active when occupation measure is nonzero in original MDP states. θm(τ) = 0 =⇒ xm(s, a, τ −τa m+1) = 0 ∀s /∈ {sb , sf }, a ∈ A X s/∈{sb,sf } X a xm(s, a, t) ≤ θm(τa m + t − 1) ∀m ∈ M, ∀t ∈ [1, Tm] (6) Agent can only be active in τ ∈ (τa m, τd m) θm(τ) = 0 ∀m ∈ M, τ /∈ (τa m, τd m) (7) Cannot use resources when not active θm(τ) = 0 =⇒ Δm(τ, ω) = 0 ∀τ ∈ [0, bτ], ω ∈ Ω Δm(τ, ω) ≤ θm(τ) ∀m ∈ M, τ ∈ [0, bτ], ω ∈ Ω (8) Tie x to Δ (nonzero x forces corresponding Δ to be nonzero.)",
                "Δm(τ, ω) = 0, ϕm(a, ω) = 1 =⇒ xm(s, a, τ − τa m + 1) = 0 ∀s /∈ {sb , sf } 1/|A| X a ϕm(a, ω) X s/∈{sb,sf } xm(s, a, t) ≤ Δm(t + τa m − 1, ω) ∀m ∈ M, ω ∈ Ω, t ∈ [1, Tm] (9) Resource bounds X m Δm(τ, ω) ≤ bϕ(ω) ∀ω ∈ Ω, τ ∈ [0, bτ] (10) Agent cannot change resources while active.",
                "Only enabled for scheduling with static assignments. θm(τ) = 1 and θm(τ + 1) = 1 =⇒ Δm(τ, ω) = Δm(τ + 1, ω) Δm(τ, ω) − Z(1 − θm(τ + 1)) ≤ Δm(τ + 1, ω) + Z(1 − θm(τ)) Δm(τ, ω) + Z(1 − θm(τ + 1)) ≥ Δm(τ + 1, ω) − Z(1 − θm(τ)) ∀m ∈ M, ω ∈ Ω, τ ∈ [0, bτ] (11) Table 1: MILP for globally optimal resource scheduling.",
                "Tm = τd m − τa m + 1 is the time horizon for the agents MDP.",
                "Using this LP as a basis, we augment it with constraints that ensure that the resource usage implied by the agents occupation measures {xm} does not violate the global resource requirements bϕ at any time step τ ∈ [0, bτ].",
                "To formulate these resource constraints, we use the following binary variables: • Δm(τ, ω) = {0, 1}, ∀m ∈ M, τ ∈ [0, bτ], ω ∈ Ω, which serve as indicator variables that define whether agent m possesses resource ω at time τ.",
                "These are analogous to the static indicator variables used in the one-shot static resource-allocation problem in [6]. • θm = {0, 1}, ∀m ∈ M, τ ∈ [0, bτ] are indicator variables that specify whether agent m is active (i.e., executing its MDP) at time τ.",
                "The meaning of resource-usage variables Δ is illustrated in Figure 1: Δm(τ, ω) = 1 only if resource ω is allocated to agent m at time τ.",
                "The meaning of the activity indicators θ is illustrated in Figure 2b: when agent m is in either the start state sb or the finish state sf , the corresponding θm = 0, but once the agent becomes active and enters one of the other states, we set θm = 1 .",
                "This meaning of θ can be enforced with a linear constraint that synchronizes the values of the agents occupation measures xm and the activity 1224 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) indicators θ, as shown in (6) in Table 1.",
                "Another constraint we have to add-because the activity indicators θ are defined on the global timeline τ-is to enforce the fact that the agent is inactive outside of its arrivaldeparture window.",
                "This is accomplished by constraint (7) in Table 1.",
                "Furthermore, agents should not be using resources while they are inactive.",
                "This constraint can also be enforced via a linear inequality on θ and Δ, as shown in (8).",
                "Constraint (6) sets the value of θ to match the policy defined by the occupation measure xm.",
                "In a similar fashion, we have to make sure that the resource-usage variables Δ are also synchronized with the occupation measure xm.",
                "This is done via constraint (9) in Table 1, which is nearly identical to the analogous constraint from [6].",
                "After implementing the above constraint, which enforces the meaning of Δ, we add a constraint that ensures that the agents resource usage never exceeds the amounts of available resources.",
                "This condition is also trivially expressed as a linear inequality (10) in Table 1.",
                "Finally, for the problem formulation where resource assignments are static during a lifetime of an agent, we add a constraint that ensures that the resource-usage variables Δ do not change their value while the agent is active (θ = 1).",
                "This is accomplished via the linear constraint (11), where Z ≥ 2 is a constant that is used to turn off the constraints when θm(τ) = 0 or θm(τ + 1) = 0.",
                "This constraint is not used for the dynamic problem formulation, where resources can be reallocated between agents at every time step.",
                "To summarize, Table 1 together with the conservationof-flow constraints from (12) defines the MILP that simultaneously computes an optimal resource assignment for all agents across time as well as optimal finite-horizon MDP policies that are valid under that resource assignment.",
                "As a rough measure of the complexity of this MILP, let us consider the number of optimization variables and constraints.",
                "Let TM = P Tm = P m(τa m − τd m + 1) be the sum of the lengths of the arrival-departure windows across all agents.",
                "Then, the number of optimization variables is: TM + bτ|M||Ω| + bτ|M|, TM of which are continuous (xm), and bτ|M||Ω| + bτ|M| are binary (Δ and θ).",
                "However, notice that all but TM|M| of the θ are set to zero by constraint (7), which also immediately forces all but TM|M||Ω| of the Δ to be zero via the constraints (8).",
                "The number of constraints (not including the degenerate constraints in (7)) in the MILP is: TM + TM|Ω| + bτ|Ω| + bτ|M||Ω|.",
                "Despite the fact that the complexity of the MILP is, in the worst case, exponential1 in the number of binary variables, the complexity of this MILP is significantly (exponentially) lower than that of the MILP with flat utility functions, described in Section 2.2.",
                "This result echos the efficiency gains reported in [6] for single-shot resource-allocation problems, but is much more pronounced, because of the explosion of the flat utility representation due to the temporal aspect of the problem (recall the prohibitive complexity of the combinatorial optimization in Section 2.2).",
                "We empirically analyze the performance of this method in Section 5. 1 Strictly speaking, solving MILPs to optimality is NPcomplete in the number of integer variables. 5.",
                "EXPERIMENTAL RESULTS Although the complexity of solving MILPs is in the worst case exponential in the number of integer variables, there are many efficient methods for solving MILPs that allow our algorithm to scale well for parameters common to resource allocation and scheduling problems.",
                "In particular, this section introduces a problem domain-the repairshop problem-used to empirically evaluate our algorithms scalability in terms of the number of agents |M|, the number of shared resources |Ω|, and the varied lengths of global time bτ during which agents may enter and exit the system.",
                "The repairshop problem is a simple parameterized MDP adopting the metaphor of a vehicular repair shop.",
                "Agents in the repair shop are mechanics with a number of independent tasks that yield reward only when completed.",
                "In our MDP model of this system, actions taken to advance through the state space are only allowed if the agent holds certain resources that are publicly available to the shop.",
                "These resources are in finite supply, and optimal policies for the shop will determine when each agent may hold the limited resources to take actions and earn individual rewards.",
                "Each task to be completed is associated with a single action, although the agent is required to repeat the action numerous times before completing the task and earning a reward.",
                "This model was parameterized in terms of the number of agents in the system, the number of different types of resources that could be linked to necessary actions, a global time during which agents are allowed to arrive and depart, and a maximum length for the number of time steps an agent may remain in the system.",
                "All datapoints in our experiments were obtained with 20 evaluations using CPLEX to solve the MILPs on a Pentium4 computer with 2Gb of RAM.",
                "Trials were conducted on both the static and the dynamic version of the resourcescheduling problem, as defined earlier.",
                "Figure 3 shows the runtime and policy value for independent modifications to the parameter set.",
                "The top row shows how the solution time for the MILP scales as we increase the number of agents |M|, the global time horizon bτ, and the number of resources |Ω|.",
                "Increasing the number of agents leads to exponential complexity scaling, which is to be expected for an NP-complete problem.",
                "However, increasing the global time limit bτ or the total number of resource types |Ω|-while holding the number of agents constantdoes not lead to decreased performance.",
                "This occurs because the problems get easier as they become under-constrained, which is also a common phenomenon for NP-complete problems.",
                "We also observe that the solution to the dynamic version of the problem can often be computed much faster than the static version.",
                "The bottom row of Figure 3 shows the joint policy value of the policies that correspond to the computed optimal resource-allocation schedules.",
                "We can observe that the dynamic version yields higher reward (as expected, since the reward for the dynamic version is always no less than the reward of the static version).",
                "We should point out that these graphs should not be viewed as a measure of performance of two different algorithms (both algorithms produce optimal solutions but to different problems), but rather as observations about how the quality of optimal solutions change as more flexibility is allowed in the reallocation of resources.",
                "Figure 4 shows runtime and policy value for trials in which common input variables are scaled together.",
                "This allows The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1225 2 4 6 8 10 10 −3 10 −2 10 −1 10 0 10 1 10 2 10 3 10 4 Number of Agents |M| CPUTime,sec |Ω| = 5, τ = 50 static dynamic 50 100 150 200 10 −2 10 −1 10 0 10 1 10 2 10 3 Global Time Boundary τ CPUTime,sec |M| = 5, |Ω| = 5 static dynamic 10 20 30 40 50 10 −2 10 −1 10 0 10 1 10 2 Number of Resources |Ω| CPUTime,sec |M| = 5, τ = 50 static dynamic 2 4 6 8 10 200 400 600 800 1000 1200 1400 1600 Number of Agents |M| Value |Ω| = 5, τ = 50 static dynamic 50 100 150 200 400 500 600 700 800 900 1000 1100 1200 1300 1400 Global Time Boundary τ Value |M| = 5, |Ω| = 5 static dynamic 10 20 30 40 50 500 600 700 800 900 1000 1100 1200 1300 1400 Number of Resources |Ω| Value |M| = 5, τ = 50 static dynamic Figure 3: Evaluation of our MILP for variable numbers of agents (column 1), lengths of global-time window (column 2), and numbers of resource types (column 3).",
                "Top row shows CPU time, and bottom row shows the joint reward of agents MDP policies.",
                "Error bars show the 1st and 3rd quartiles (25% and 75%). 2 4 6 8 10 10 −3 10 −2 10 −1 10 0 10 1 10 2 10 3 Number of Agents |M| CPUTime,sec τ = 10|M| static dynamic 2 4 6 8 10 10 −3 10 −2 10 −1 10 0 10 1 10 2 10 3 10 4 Number of Agents |M| CPUTime,sec |Ω| = 2|M| static dynamic 2 4 6 8 10 10 −3 10 −2 10 −1 10 0 10 1 10 2 10 3 10 4 Number of Agents |M| CPUTime,sec |Ω| = 5|M| static dynamic 2 4 6 8 10 200 400 600 800 1000 1200 1400 1600 1800 2000 2200 Number of Agents |M| Value τ = 10|M| static dynamic 2 4 6 8 10 200 400 600 800 1000 1200 1400 1600 1800 2000 Number of Agents |M| Value |Ω| = 2|M| static dynamic 2 4 6 8 10 0 500 1000 1500 2000 2500 Number of Agents |M| Value |Ω| = 5|M| static dynamic Figure 4: Evaluation of our MILP using correlated input variables.",
                "The left column tracks the performance and CPU time as the number of agents and global-time window increase together (bτ = 10|M|).",
                "The middle and the right column track the performance and CPU time as the number of resources and the number of agents increase together as |Ω| = 2|M| and |Ω| = 5|M|, respectively.",
                "Error bars show the 1st and 3rd quartiles (25% and 75%). 1226 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) us to explore domains where the total number of agents scales proportionally to the total number of resource types or the global time horizon, while keeping constant the average agent density (per unit of global time) or the average number of resources per agent (which commonly occurs in real-life applications).",
                "Overall, we believe that these experimental results indicate that our MILP formulation can be used to effectively solve resource-scheduling problems of nontrivial size. 6.",
                "DISCUSSION AND CONCLUSIONS Throughout the paper, we have made a number of assumptions in our model and solution algorithm; we discuss their implications below. • Continual execution.",
                "We assume that once an agent stops executing its MDP (transitions into state sf ), it exits the system and cannot return.",
                "It is easy to relax this assumption for domains where agents MDPs can be paused and restarted.",
                "All that is required is to include an additional pause action which transitions from a given state back to itself, and has zero reward. • Indifference to start time.",
                "We used a reward model where agents rewards depend only on the time horizon of their MDPs and not the global start time.",
                "This is a consequence of our MDP-augmentation procedure from Section 4.1.",
                "It is easy to extend the model so that the agents incur an explicit penalty for idling by assigning a non-zero negative reward to the start state sb . • Binary resource requirements.",
                "For simplicity, we have assumed that resource costs are binary: ϕm(a, ω) = {0, 1}, but our results generalize in a straightforward manner to non-binary resource mappings, analogously to the procedure used in [5]. • Cooperative agents.",
                "The optimization procedure discussed in this paper was developed in the context of cooperative agents, but it can also be used to design a mechanism for scheduling resources among self-interested agents.",
                "This optimization procedure can be embedded in a VickreyClarke-Groves auction, completely analogously to the way it was done in [7].",
                "In fact, all the results of [7] about the properties of the auction and information privacy directly carry over to the scheduling domain discussed in this paper, requiring only slight modifications to deal with finitehorizon MDPs. • Known, deterministic arrival and departure times.",
                "Finally, we have assumed that agents arrival and departure times (τa m and τd m) are deterministic and known a priori.",
                "This assumption is fundamental to our solution method.",
                "While there are many domains where this assumption is valid, in many cases agents arrive and depart dynamically and their arrival and departure times can only be predicted probabilistically, leading to online resource-allocation problems.",
                "In particular, in the case of self-interested agents, this becomes an interesting version of an online-mechanism-design problem [11, 12].",
                "In summary, we have presented an MILP formulation for the combinatorial resource-scheduling problem where agents values for possible resource assignments are defined by finitehorizon MDPs.",
                "This result extends previous work ([6, 7]) on static one-shot resource allocation under MDP-induced preferences to resource-scheduling problems with a temporal aspect.",
                "As such, this work takes a step in the direction of designing an online mechanism for agents with combinatorial resource preferences induced by stochastic planning problems.",
                "Relaxing the assumption about deterministic arrival and departure times of the agents is a focus of our future work.",
                "We would like to thank the anonymous reviewers for their insightful comments and suggestions. 7.",
                "REFERENCES [1] E. Altman and A. Shwartz.",
                "Adaptive control of constrained Markov chains: Criteria and policies.",
                "Annals of Operations Research, special issue on Markov Decision Processes, 28:101-134, 1991. [2] R. Bellman.",
                "Dynamic Programming.",
                "Princeton University Press, 1957. [3] C. Boutilier.",
                "Solving concisely expressed combinatorial auction problems.",
                "In Proc. of AAAI-02, pages 359-366, 2002. [4] C. Boutilier and H. H. Hoos.",
                "Bidding languages for combinatorial auctions.",
                "In Proc. of IJCAI-01, pages 1211-1217, 2001. [5] D. Dolgov.",
                "Integrated Resource Allocation and Planning in Stochastic Multiagent Environments.",
                "PhD thesis, Computer Science Department, University of Michigan, February 2006. [6] D. A. Dolgov and E. H. Durfee.",
                "Optimal resource allocation and policy formulation in loosely-coupled Markov decision processes.",
                "In Proc. of ICAPS-04, pages 315-324, June 2004. [7] D. A. Dolgov and E. H. Durfee.",
                "Computationally efficient combinatorial auctions for resource allocation in weakly-coupled MDPs.",
                "In Proc. of AAMAS-05, New York, NY, USA, 2005.",
                "ACM Press. [8] D. A. Dolgov and E. H. Durfee.",
                "Resource allocation among agents with preferences induced by factored MDPs.",
                "In Proc. of AAMAS-06, 2006. [9] K. Larson and T. Sandholm.",
                "Mechanism design and deliberative agents.",
                "In Proc. of AAMAS-05, pages 650-656, New York, NY, USA, 2005.",
                "ACM Press. [10] N. Nisan.",
                "Bidding and allocation in combinatorial auctions.",
                "In Electronic Commerce, 2000. [11] D. C. Parkes and S. Singh.",
                "An MDP-based approach to Online Mechanism Design.",
                "In Proc. of the Seventeenths Annual Conference on Neural Information Processing Systems (NIPS-03), 2003. [12] D. C. Parkes, S. Singh, and D. Yanovsky.",
                "Approximately efficient online mechanism design.",
                "In Proc. of the Eighteenths Annual Conference on Neural Information Processing Systems (NIPS-04), 2004. [13] M. L. Puterman.",
                "Markov Decision Processes.",
                "John Wiley & Sons, New York, 1994. [14] M. H. Rothkopf, A. Pekec, and R. M. Harstad.",
                "Computationally manageable combinational auctions.",
                "Management Science, 44(8):1131-1147, 1998. [15] T. Sandholm.",
                "An algorithm for optimal winner determination in combinatorial auctions.",
                "In Proc. of IJCAI-99, pages 542-547, San Francisco, CA, USA, 1999.",
                "Morgan Kaufmann Publishers Inc.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1227"
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [],
            "translated_text": "",
            "candidates": [],
            "error": [
                []
            ]
        }
    }
}