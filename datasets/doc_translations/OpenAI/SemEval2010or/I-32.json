{
    "id": "I-32",
    "original_text": "An Adversarial Environment Model for Bounded Rational Agents in Zero-Sum Interactions Inon Zuckerman1 , Sarit Kraus1 , Jeffrey S. Rosenschein2 , Gal Kaminka1 1 Department of Computer Science 2 The School of Engineering Bar-Ilan University and Computer Science Ramat-Gan, Israel Hebrew University, Jerusalem, Israel {zukermi,sarit,galk}@cs.biu.ac.il jeff@cs.huji.ac.il ABSTRACT Multiagent environments are often not cooperative nor collaborative; in many cases, agents have conflicting interests, leading to adversarial interactions. This paper presents a formal Adversarial Environment model for bounded rational agents operating in a zero-sum environment. In such environments, attempts to use classical utility-based search methods can raise a variety of difficulties (e.g., implicitly modeling the opponent as an omniscient utility maximizer, rather than leveraging a more nuanced, explicit opponent model). We define an Adversarial Environment by describing the mental states of an agent in such an environment. We then present behavioral axioms that are intended to serve as design principles for building such adversarial agents. We explore the application of our approach by analyzing log files of completed Connect-Four games, and present an empirical analysis of the axioms appropriateness. Categories and Subject Descriptors I.2.11 [Artificial Intelligence]: Distributed Artificial Intelligence-Intelligent agents,Multiagent Systems; I.2.4 [Artificial Intelligence]: Knowledge Representation Formalisms and Methods -Modal logic General Terms Design, Theory 1. INTRODUCTION Early research in multiagent systems (MAS) considered cooperative groups of agents; because individual agents had limited resources, or limited access to information (e.g., limited processing power, limited sensor coverage), they worked together by design to solve problems that individually they could not solve, or at least could not solve as efficiently. MAS research, however, soon began to consider interacting agents with individuated interests, as representatives of different humans or organizations with non-identical interests. When interactions are guided by diverse interests, participants may have to overcome disagreements, uncooperative interactions, and even intentional attempts to damage one another. When these types of interactions occur, environments require appropriate behavior from the agents situated in them. We call these environments Adversarial Environments, and call the clashing agents Adversaries. Models of cooperation and teamwork have been extensively explored in MAS through the axiomatization of mental states (e.g., [8, 4, 5]). However, none of this research dealt with adversarial domains and their implications for agent behavior. Our paper addresses this issue by providing a formal, axiomatized mental state model for a subset of adversarial domains, namely simple zero-sum adversarial environments. Simple zero-sum encounters exist of course in various twoplayer games (e.g., Chess, Checkers), but they also exist in n-player games (e.g., Risk, Diplomacy), auctions for a single good, and elsewhere. In these latter environments especially, using a utility-based adversarial search (such as the Min-Max algorithm) does not always provide an adequate solution; the payoff function might be quite complex or difficult to quantify, and there are natural computational limitations on bounded rational agents. In addition, traditional search methods (like Min-Max) do not make use of a model of the opponent, which has proven to be a valuable addition to adversarial planning [9, 3, 11]. In this paper, we develop a formal, axiomatized model for bounded rational agents that are situated in a zero-sum adversarial environment. The model uses different modality operators, and its main foundations are the SharedPlans [4] model for collaborative behavior. We explore environment properties and the mental states of agents to derive behavioral axioms; these behavioral axioms constitute a formal model that serves as a specification and design guideline for agent design in such settings. We then investigate the behavior of our model empirically using the Connect-Four board game. We show that this game conforms to our environment definition, and analyze players behavior using a large set of completed match log 550 978-81-904262-7-5 (RPS) c 2007 IFAAMAS files. In addition, we use the results presented in [9] to discuss the importance of opponent modeling in our ConnectFour adversarial domain. The paper proceeds as follows. Section 2 presents the models formalization. Section 3 presents the empirical analysis and its results. We discuss related work in Section 4, and conclude and present future directions in Section 5. 2. ADVERSARIAL ENVIRONMENTS The adversarial environment model (denoted as AE) is intended to guide the design of agents by providing a specification of the capabilities and mental attitudes of an agent in an adversarial environment. We focus here on specific types of adversarial environments, specified as follows: 1. Zero-Sum Interactions: positive and negative utilities of all agents sum to zero; 2. Simple AEs: all agents in the environment are adversarial agents; 3. Bilateral AEs: AEs with exactly two agents; 4. Multilateral AEs: AEs of three or more agents. We will work on both bilateral and multilateral instantiations of zero-sum and simple environments. In particular, our adversarial environment model will deal with interactions that consist of N agents (N ≥ 2), where all agents are adversaries, and only one agent can succeed. Examples of such environments range from board games (e.g., Chess, Connect-Four, and Diplomacy) to certain economic environments (e.g., N-bidder auctions over a single good). 2.1 Model Overview Our approach is to formalize the mental attitudes and behaviors of a single adversarial agent; we consider how a single agent perceives the AE. The following list specifies the conditions and mental states of an agent in a simple, zero-sum AE: 1. The agent has an individual intention that its own goal will be completed; 2. The agent has an individual belief that it and its adversaries are pursuing full conflicting goals (defined below)there can be only one winner; 3. The agent has an individual belief that each adversary has an intention to complete its own full conflicting goal; 4. The agent has an individual belief in the (partial) profile of its adversaries. Item 3 is required, since it might be the case that some agent has a full conflicting goal, and is currently considering adopting the intention to complete it, but is, as of yet, not committed to achieving it. This might occur because the agent has not yet deliberated about the effects that adopting that intention might have on the other intentions it is currently holding. In such cases, it might not consider itself to even be in an adversarial environment. Item 4 states that the agent should hold some belief about the profiles of its adversaries. The profile represents all the knowledge the agent has about its adversary: its weaknesses, strategic capabilities, goals, intentions, trustworthiness, and more. It can be given explicitly or can be learned from observations of past encounters. 2.2 Model Definitions for Mental States We use Grosz and Krauss definitions of the modal operators, predicates, and meta-predicates, as defined in their SharedPlan formalization [4]. We recall here some of the predicates and operators that are used in that formalization: Int.To(Ai, α, Tn, Tα, C) represents Ais intentions at time Tn to do an action α at time Tα in the context of C. Int.Th(Ai, prop, Tn, Tprop, C) represents Ais intentions at time Tn that a certain proposition prop holds at time Tprop in the context of C. The potential intention operators, Pot.Int.To(...) and Pot.Int.Th(...), are used to represent the mental state when an agent considers adopting an intention, but has not deliberated about the interaction of the other intentions it holds. The operator Bel(Ai, f, Tf ) represents agent Ai believing in the statement expressed in formula f, at time Tf . MB(A, f, Tf ) represents mutual belief for a group of agents A. A snapshot of the system finds our environment to be in some state e ∈ E of environmental variable states, and each adversary in any LAi ∈ L of possible local states. At any given time step, the system will be in some world w of the set of all possible worlds w ∈ W, where w = E×LA1 ×LA2 × ...LAn , and n is the number of adversaries. For example, in a Texas Holdem poker game, an agents local state might be its own set of cards (which is unknown to its adversary) while the environment will consist of the betting pot and the community cards (which are visible to both players). A utility function under this formalization is defined as a mapping from a possible world w ∈ W to an element in , which expresses the desirability of the world, from a single agent perspective. We usually normalize the range to [0,1], where 0 represents the least desirable possible world, and 1 is the most desirable world. The implementation of the utility function is dependent on the domain in question. The following list specifies new predicates, functions, variables, and constants used in conjunction with the original definitions for the adversarial environment formalization: 1. φ is a null action (the agent does not do anything). 2. GAi is the set of agent Ais goals. Each goal is a set of predicates whose satisfaction makes the goal complete (we use G∗ Ai ∈ GAi to represent an arbitrary goal of agent Ai). 3. gAi is the set of agent Ais subgoals. Subgoals are predicates whose satisfaction represents an important milestone toward achievement of the full goal. gG∗ Ai ⊆ gAi is the set of subgoals that are important to the completion of goal G∗ Ai (we will use g∗ G∗ Ai ∈ gG∗ Ai to represent an arbitrary subgoal). 4. P Aj Ai is the profile object agent Ai holds about agent Aj. 5. CA is a general set of actions for all agents in A which are derived from the environments constraints. CAi ⊆ CA is the set of agent Ais possible actions. 6. Do(Ai, α, Tα, w) holds when Ai performs action α over time interval Tα in world w. 7. Achieve(G∗ Ai , α, w) is true when goal G∗ Ai is achieved following the completion of action α in world w ∈ W, where α ∈ CAi . 8. Profile(Ai, PAi Ai ) is true when agent Ai holds an object profile for agent Aj. Definition 1. Full conflict (FulConf ) describes a zerosum interaction where only a single goal of the goals in conflict can be completed. FulConf(G∗ Ai , G∗ Aj ) ⇒ (∃α ∈ CAi , ∀w, β ∈ CAj ) (Achieve(G∗ Ai , α, w) ⇒ ¬Achieve(G∗ Aj , β, w)) ∨ (∃β ∈ CAj , ∀w, α ∈ CAi )(Achieve(G∗ Aj , β, w) ⇒ ¬Achieve(G∗ Ai , α, w)) Definition 2. Adversarial Knowledge (AdvKnow) is a function returning a value which represents the amount of The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 551 knowledge agent Ai has on the profile of agent Aj, at time Tn. The higher the value, the more knowledge agent Ai has. AdvKnow : P Aj Ai × Tn → Definition 3. Eval - This evaluation function returns an estimated expected utility value for an agent in A, after completing an action from CA in some world state w. Eval : A × CA × w → Definition 4. TrH - (Threshold) is a numerical constant in the [0,1] range that represents an evaluation function (Eval) threshold value. An action that yields an estimated utility evaluation above the TrH is regarded as a highly beneficial action. The Eval value is an estimation and not the real utility function, which is usually unknown. Using the real utility value for a rational agent would easily yield the best outcome for that agent. However, agents usually do not have the real utility functions, but rather a heuristic estimate of it. There are two important properties that should hold for the evaluation function: Property 1. The evaluation function should state that the most desirable world state is one in which the goal is achieved. Therefore, after the goal has been satisfied, there can be no future action that can put the agent in a world state with higher Eval value. (∀Ai, G∗ Ai , α, β ∈ CAi , w ∈ W) Achieve(G∗ Ai , α, w) ⇒ Eval(Ai, α, w) ≥ Eval(Ai, β, w) Property 2. The evaluation function should project an action that causes a completion of a goal or a subgoal to a value which is greater than TrH (a highly beneficial action). (∀Ai, G∗ Ai ∈ GAi , α ∈ CAi , w ∈ W, g∗ GAi ∈ gGAi ) Achieve(G∗ Ai , α, w) ∨ Achieve(g∗ GAi , α, w) ⇒ Eval(Ai, α, w) ≥ TrH. Definition 5. SetAction We define a set action (SetAction) as a set of action operations (either complex or basic actions) from some action sets CAi and CAj which, according to agent Ais belief, are attached together by a temporal and consequential relationship, forming a chain of events (action, and its following consequent action). (∀α1 , . . . , αu ∈ CAi , β1 , . . . , βv ∈ CAj , w ∈ W) SetAction(α1 , . . . , αu , β1 , . . . , βv , w) ⇒ ((Do(Ai, α1 , Tα1 , w) ⇒ Do(Aj, β1 , Tβ1 , w)) ⇒ Do(Ai, α2 , Tα2 , w) ⇒ . . . ⇒ Do(Ai, αu , Tαu , w)) The consequential relation might exist due to various environmental constraints (when one action forces the adversary to respond with a specific action) or due to the agents knowledge about the profile of its adversary. Property 3. As the knowledge we have about our adversary increases we will have additional beliefs about its behavior in different situations which in turn creates new set actions. Formally, if our AdvKnow at time Tn+1 is greater than AdvKnow at time Tn, then every SetAction known at time Tn is also known at time Tn+1. AdvKnow(P Aj Ai , Tn+1) > AdvKnow(P Aj Ai , Tn) ⇒ (∀α1 , . . . , αu ∈ CAi , β1 , . . . , βv ∈ CAj ) Bel(Aag, SetAction(α1 , . . . , αu , β1 , . . . , βv ), Tn) ⇒ Bel(Aag, SetAction(α1 , . . . , αu , β1 , . . . , βv ), Tn+1) 2.3 The Environment Formulation The following axioms provide the formal definition for a simple, zero-sum Adversarial Environment (AE). Satisfaction of these axioms means that the agent is situated in such an environment. It provides specifications for agent Aag to interact with its set of adversaries A with respect to goals G∗ Aag and G∗ A at time TCo at some world state w. AE(Aag, A, G∗ Aag , A1, . . . , Ak, G∗ A1 , . . . , G∗ Ak , Tn, w) 1. Aag has an Int.Th his goal would be completed: (∃α ∈ CAag , Tα) Int.Th(Aag, Achieve(G∗ Aag , α), Tn, Tα, AE) 2. Aag believes that it and each of its adversaries Ao are pursuing full conflicting goals: (∀Ao ∈ {A1, . . . , Ak}) Bel(Aag, FulConf(G∗ Aag , G∗ Ao ), Tn) 3. Aag believes that each of his adversaries in Ao has the Int.Th his conflicting goal G∗ Aoi will be completed: (∀Ao ∈ {A1, . . . , Ak})(∃β ∈ CAo , Tβ) Bel(Aag, Int.Th(Ao, Achieve(G∗ Ao , β), TCo, Tβ, AE), Tn) 4. Aag has beliefs about the (partial) profiles of its adversaries (∀Ao ∈ {A1, . . . , Ak}) (∃PAo Aag ∈ PAag )Bel(Aag, Profile(Ao, PAo Aag ), Tn) To build an agent that will be able to operate successfully within such an AE, we must specify behavioral guidelines for its interactions. Using a naive Eval maximization strategy to a certain search depth will not always yield satisfactory results for several reasons: (1) the search horizon problem when searching for a fixed depth; (2) the strong assumption of an optimally rational, unbounded resources adversary; (3) using an estimated evaluation function which will not give optimal results in all world states, and can be exploited [9]. The following axioms specify the behavioral principles that can be used to differentiate between successful and less successful agents in the above Adversarial Environment. Those axioms should be used as specification principles when designing and implementing agents that should be able to perform well in such Adversarial Environments. The behavioral axioms represent situations in which the agent will adopt potential intentions to (Pot.Int.To(...)) perform an action, which will typically require some means-end reasoning to select a possible course of action. This reasoning will lead to the adoption of an Int.To(...) (see [4]). A1. Goal Achieving Axiom. The first axiom is the simplest case; when the agent Aag believes that it is one action (α) away from achieving his conflicting goal G∗ Aag , it should adopt the potential intention to do α and complete its goal. (∀Aag, α ∈ CAag , Tn, Tα, w ∈ W) (Bel(Aag, Do(Aag, α, Tα, w) ⇒ Achieve(G∗ Aag , α, w)) ⇒ Pot.Int.To(Aag, α, Tn, Tα, w) This somewhat trivial behavior is the first and strongest axiom. In any situation, when the agent is an action away from completing the goal, it should complete the action. Any fair Eval function would naturally classify α as the maximal value action (property 1). However, without explicit axiomatization of such behavior there might be situations where the agent will decide on taking another action for various reasons, due to its bounded decision resources. A2. Preventive Act Axiom. Being in an adversarial situation, agent Aag might decide to take actions that will damage one of its adversarys plans to complete its goal, even if those actions do not explicitly advance Aag towards its conflicting goal G∗ Aag . Such preventive action will take place when agent Aag has a belief about the possibility of its adversary Ao doing an action β that will give it a high 552 The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) utility evaluation value (> TrH). Believing that taking action α will prevent the opponent from doing its β, it will adopt a potential intention to do α. (∀Aag, Ao ∈ A, α ∈ CAag , β ∈ CAo , Tn, Tβ, w ∈ W) (Bel(Aag, Do(Ao, β, Tβ, w) ∧ Eval(Ao, β, w) > TrH, Tn) ∧ Bel(Aag, Do(Aag, α, Tα, w) ⇒ ¬Do(Ao, β, Tβ, w), Tn) ⇒ Pot.Int.To(Aag, α, Tn, Tα, w) This axiom is a basic component of any adversarial environment. For example, looking at a Chess board game, a player could realize that it is about to be checkmated by its opponent, thus making a preventive move. Another example is a Connect Four game: when a player has a row of three chips, its opponent must block it, or lose. A specific instance of A1 occurs when the adversary is one action away from achieving its goal, and immediate preventive action needs to be taken by the agent. Formally, we have the same beliefs as stated above, with a changed belief that doing action β will cause agent Ao to achieve its goal. Proposition 1: Prevent or lose case. (∀Aag, Ao ∈ A, α ∈ CAag , β ∈ CAo , G∗ Ao , Tn, Tα, Tβ, w ∈ W) Bel(Aag, Do(Ao, β, Tβ, w) ⇒ Achieve(G∗ Ao , β, w), Tn) ∧ Bel(Aag, Do(Aag, α, Tα, w) ⇒ ¬Do(Ao, β, Tβ, w)) ⇒ Pot.Int.To(Aag, α, Tn, Tα, w) Sketch of proof: Proposition 1 can be easily derived from axiom A1 and the property 2 of the Eval function, which states that any action that causes a completion of a goal is a highly beneficial action. The preventive act behavior will occur implicitly when the Eval function is equal to the real world utility function. However, being bounded rational agents and dealing with an estimated evaluation function we need to explicitly axiomatize such behavior, for it will not always occur implicitly from the evaluation function. A3. Suboptimal Tactical Move Axiom. In many scenarios a situation may occur where an agent will decide not to take the current most beneficial action it can take (the action with the maximal utility evaluation value), because it believes that taking another action (with lower utility evaluation value) might yield (depending on the adversarys response) a future possibility for a highly beneficial action. This will occur most often when the Eval function is inaccurate and differs by a large extent from the Utility function. Put formally, agent Aag believes in a certain SetAction that will evolve according to its initial action and will yield a high beneficial value (> TrH) solely for it. (∀Aag, Ao ∈ A, Tn, w ∈ W) (∃α1 , . . . , αu ∈ CAi , β1 , . . . , βv ∈ CAj , Tα1 ) Bel(Aag, SetAction(α1 , . . . , αu , β1 , . . . , βv ), Tn) ∧ Bel(Aag, Eval(Ao, βv , w) < TrH < Eval(Aag, αu , w), Tn) ⇒ Pot.Int.To(Aag, α1 , Tn, Tα1 , w) An agent might believe that a chain of events will occur for various reasons due to the inevitable nature of the domain. For example, in Chess, we often observe the following: a move causes a check position, which in turn limits the opponents moves to avoiding the check, to which the first player might react with another check, and so on. The agent might also believe in a chain of events based on its knowledge of its adversarys profile, which allows it to foresee the adversarys movements with high accuracy. A4. Profile Detection Axiom. The agent can adjust its adversarys profiles by observations and pattern study (specifically, if there are repeated encounters with the same adversary). However, instead of waiting for profile information to be revealed, an agent can also initiate actions that will force its adversary to react in a way that will reveal profile knowledge about it. Formally, the axiom states that if all actions (γ) are not highly beneficial actions (< TrH), the agent can do action α in time Tα if it believes that it will result in a non-highly beneficial action β from its adversary, which in turn teaches it about the adversarys profile, i.e., gives a higher AdvKnow(P Aj Ai , Tβ). (∀Aag, Ao ∈ A, α ∈ CAag , β ∈ CAo , Tn, Tα, Tβ, w ∈ W) Bel(Aag, (∀γ ∈ CAag )Eval(Aag, γ, w) < TrH, Tn) ∧ Bel(Aag, Do(Aag, α, Tα, w) ⇒ Do(Ao, β, Tβ, w), Tn) ∧ Bel(Aag, Eval(Ao, β, w) < TrH) ∧ Bel(Aag, AdvKnow(P Aj Ai , Tβ) > AdvKnow(P Aj Ai , Tn), Tn) ⇒ Pot.Int.To(Aag, α, Tn, Tα, w) For example, going back to the Chess board game scenario, consider starting a game versus an opponent about whom we know nothing, not even if it is a human or a computerized opponent. We might start playing a strategy that will be suitable versus an average opponent, and adjust our game according to its level of play. A5. Alliance Formation Axiom The following behavioral axiom is relevant only in a multilateral instantiation of the adversarial environment (obviously, an alliance cannot be formed in a bilateral, zero-sum encounter). In different situations during a multilateral interaction, a group of agents might believe that it is in their best interests to form a temporary alliance. Such an alliance is an agreement that constrains its members behavior, but is believed by its members to enable them to achieve a higher utility value than the one achievable outside of the alliance. As an example, we can look at the classical Risk board game, where each player has an individual goal of being the sole conquerer of the world, a zero-sum game. However, in order to achieve this goal, it might be strategically wise to make short-term ceasefire agreements with other players, or to join forces and attack an opponent who is stronger than the rest. An alliances terms defines the way its members should act. It is a set of predicates, denoted as Terms, that is agreed upon by the alliance members, and should remain true for the duration of the alliance. For example, the set Terms in the Risk scenario, could contain the following predicates: 1. Alliance members will not attack each other on territories X, Y and Z; 2. Alliance members will contribute C units per turn for attacking adversary Ao; 3. Members are obligated to stay as part of the alliance until time Tk or until adversarys Ao army is smaller than Q. The set Terms specifies inter-group constraints on each of the alliance members (∀Aal i ∈ Aal ⊆ A) set of actions Cal i ⊆ C. Definition 6. Al val - the total evaluation value that agent Ai will achieve while being part of Aal is the sum of Evali (Eval for Ai) of each of Aal j Eval values after taking their own α actions (via the agent(α) predicate): Al val(Ai, Cal , Aal , w) = α∈Cal Evali(Aal j , agent(α), w) Definition 7. Al TrH - is a number representing an Al val The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 553 threshold; above it, the alliance can be said to be a highly beneficial alliance. The value of Al TrH will be calculated dynamically according to the progress of the interaction, as can be seen in [7]. After an alliance is formed, its members are now working in their normal adversarial environment, as well as according to the mental states and axioms required for their interactions as part of the alliance. The following Alliance model (AL) specifies the conditions under which the group Aal can be said to be in an alliance and working with a new and constrained set of actions Cal , at time Tn. AL(Aal , Cal , w, Tn) 1. Aal has a MB that all members are part of Aal : MB(Aal , (∀Aal i ∈ Aal )member(Aal i , Aal ), Tn) 2. Aal has a MB that the group be maintained: MB(Aal , (∀Aal i ∈ Aal )Int.Th (Ai, member(Ai, Aal ), Tn, Tn+1, Co), Tn) 3. Aal has a MB that being members gives them high utility value: MB(Aal , (∀Aal i ∈ Aal )Al val(Aal i , Cal , Aal , w) ≥ Al TrH, Tn) Members profiles are a crucial part of successful alliances. We assume that agents that have more accurate profiles of their adversaries will be more successful in such environments. Such agents will be able to predict when a member is about to breach the alliances contract (item 2 in the above model), and take counter measures (when item 3 will falsify). The robustness of the alliance is in part a function of its members trustfulness measure, objective position estimation, and other profile properties. We should note that an agent can simultaneously be part of more than one alliance. Such a temporary alliance, where the group members do not have a joint goal but act collaboratively for the interest of their own individual goals, is classified as a Treatment Group by modern psychologists [12] (in contrast to a Task Group, where its members have a joint goal). The Shared Activity model as presented in [5] modeled Treatment Group behavior using the same SharedPlans formalization. When comparing both definitions of an alliance and a Treatment Group we found an unsurprising resemblance between both models: the environment models definitions are almost identical (see SAs definitions in [5]), and their Selfish-Act and Cooperative Act axioms conform to our adversarial agents behavior. The main distinction between both models is the integration of a Helpful-behavior act axiom, in the Shared Activity which cannot be part of ours. This axiom states that an agent will consider taking action that will lower its Eval value (to a certain lower bound), if it believes that a group partner will gain a significant benefit. Such behavior cannot occur in a pure adversarial environment (as a zero-sum game is), where the alliance members are constantly on watch to manipulate their alliance to their own advantage. A6. Evaluation Maximization Axiom. In a case when all other axioms are inapplicable, we will proceed with the action that maximizes the heuristic value as computed in the Eval function. (∀Aag, Ao ∈ A, α ∈ Cag, Tn, w ∈ W) Bel(Aag, (∀γ ∈ Cag)Eval(Aag, α, w) ≥ Eval(Aag, γ, w), Tn) ⇒ Pot.Int.To(Aag, α, Tn, Tα, w) T1. Optimality on Eval = Utility The above axiomatic model handles situations where the Utility is unknown and the agents are bounded rational agents. The following theorem shows that in bilateral interactions, where the agents have the real Utility function (i.e., Eval = Utility) and are rational agents, the axioms provide the same optimal result as classic adversarial search (e.g., Min-Max). Theorem 1. Let Ae ag be an unbounded rational AE agent using the Eval heuristic evaluation function, Au ag be the same agent using the true Utility function, and Ao be a sole unbounded utility-based rational adversary. Given that Eval = Utility: (∀α ∈ CAu ag , α ∈ CAe ag , Tn, w ∈ W) Pot.Int.To(Au ag, α, Tn, Tα, w) → Pot.Int.To(Ae ag, α , Tn, Tα, w) ∧ ((α = α ) ∨ (Utility(Au ag, α, w) = Eval(Ae ag, α , w))) Sketch of proof - Given that Au ag has the real utility function and unbounded resources, it can generate the full game tree and run the optimal MinMax algorithm to choose the highest utility value action, which we denote by, α. The proof will show that Ae ag, using the AE axioms, will select the same or equal utility α (when there is more than one action with the same max utility) when Eval = Utility. (A1) Goal achieving axiom - suppose there is an α such that its completion will achieve Au ags goal. It will obtain the highest utility by Min-Max for Au ag. The Ae ag agent will select α or another action with the same utility value via A1. If such α does not exist, Ae ag cannot apply this axiom, and proceeds to A2. (A2) Preventive act axiom - (1) Looking at the basic case (see Prop1), if there is a β which leads Ao to achieve its goal, then a preventive action α will yield the highest utility for Au ag. Au ag will choose it through the utility, while Ae ag will choose it through A2. (2) In the general case, β is a highly beneficial action for Ao, thus yields low utility for Au ag, which will guide it to select an α that will prevent β, while Ae ag will choose it through A2.1 If such β does not exist for Ao, then A2 is not applicable, and Ae ag can proceed to A3. (A3) Suboptimal tactical move axiom - When using a heuristic Eval function, Ae ag has a partial belief in the profile of its adversary (item 4 in AE model), which may lead it to believe in SetActions (Prop1). In our case, Ae ag is holding a full profile on its optimal adversary and knows that Ao will behave optimally according to the real utility values on the complete search tree, therefore, any belief about suboptimal SetAction cannot exist, yielding this axiom inapplicable. Ae ag will proceed to A4. (A4) Profile detection axiom - Given that Ae ag has the full profile of Ao, none of Ae ags actions can increase its knowledge. That axiom will not be applied, and the agent will proceed with A6 (A5 will be disregarded because the interaction is bilateral). (A6) Evaluation maximization axiom - This axiom will select the max Eval for Ae ag. Given that Eval = Utility, the same α that was selected by Au ag will be selected. 3. EVALUATION The main purpose of our experimental analysis is to evaluate the models behavior and performance in a real adversarial environment. This section investigates whether bounded 1 A case where following the completion of β there exists a γ which gives high utility for Agent Au ag, cannot occur because Ao uses the same utility, and γs existence will cause it to classify β as a low utility action. 554 The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) rational agents situated in such adversarial environments will be better off applying our suggested behavioral axioms. 3.1 The Domain To explore the use of the above model and its behavioral axioms, we decided to use the Connect-Four game as our adversarial environment. Connect-Four is a 2-player, zerosum game which is played using a 6x7 matrix-like board. Each turn, a player drops a disc into one of the 7 columns (the set of 21 discs is usually colored yellow for player 1 and red for player 2; we will use White and Black respectively to avoid confusion). The winner is the first player to complete a horizontal, vertical, or diagonal set of four discs with its color. On very rare occasions, the game might end in a tie if all the empty grids are filled, but no player managed to create a 4-disc set. The Connect-Four game was solved in [1], where it is shown that the first player (playing with the white discs) can force a win by starting in the middle column (column 4) and playing optimally However, the optimal strategy is very complex, and difficult to follow even for complex bounded rational agents, such as human players. Before we can proceed checking agent behavior, we must first verify that the domain conforms to the adversarial environments definition as given above (which the behavioral axioms are based on). First, when playing a Connect-Four game, the agent has an intention to win the game (item 1). Second (item 2), our agent believes that in Connect-Four there can only be one winner (or no winner at all in the rare occurrence of a tie). In addition, our agent believes that its opponent to the game will try to win (item 3), and we hope it has some partial knowledge (item 4) about its adversary (this knowledge can vary from nothing, through simple facts such as age, to strategies and weaknesses). Of course, not all Connect-Four encounters are adversarial. For example, when a parent is playing the game with its child, the following situation might occur: the child, having a strong incentive to win, treats the environment as adversarial (it intends to win, understands that there can only be one winner, and believes that its parent is trying to beat him). However, the parents point of view might see the environment as an educational one, where its goal is not to win the game, but to cause enjoyment or practice strategic reasoning. In such an educational environment, a new set of behavioral axioms might be more beneficial to the parents goals than our suggested adversarial behavioral axioms. 3.2 Axiom Analysis After showing that the Connect-Four game is indeed a zero-sum, bilateral adversarial environment, the next step is to look at players behaviors during the game and check whether behaving according to our model does improve performance. To do so we have collected log files from completed Connect-Four games that were played by human players over the Internet. Our collected log file data came from Play by eMail (PBeM) sites. These are web sites that host email games, where each move is taken by an email exchange between the server and the players. Many such sites archives contain real competitive interactions, and also maintain a ranking system for their members. Most of the data we used can be found in [6]. As can be learned from [1], Connect-Four has an optimal strategy and a considerable advantage for the player who starts the game (which we call the White player). We will concentrate in our analysis on the second players moves (to be called Black). The White player, being the first to act, has the so-called initiative advantage. Having the advantage and a good strategy will keep the Black player busy reacting to its moves, instead of initiating threats. A threat is a combination of three discs of the same color, with an empty spot for the fourth winning disk. An open threat is a threat that can be realized in the opponents next move. In order for the Black player to win, it must somehow turn the tide, take the advantage and start presenting threats to the White player. We will explore Black players behavior and their conformance to our axioms. To do so, we built an application that reads log files and analyzes the Black players moves. The application contains two main components: (1) a Min-Max algorithm for evaluation of moves; (2) open threats detector for the discovering of open threats. The Min-Max algorithm will work to a given depth, d and for each move α will output the heuristic value for the next action taken by the player as written in the log file, h(α), alongside the maximum heuristic value, maxh(α), that could be achieved prior to taking the move (obviously, if h(α) = maxh(α), then the player did not do the optimal move heuristically). The threat detectors job is to notify if some action was taken in order to block an open threat (not blocking an open threat will probably cause the player to lose in the opponents next move). The heuristic function used by Min-Max to evaluate the players utility is the following function, which is simple to compute, yet provides a reasonable challenge to human opponents: Definition 8. Let Group be an adjacent set of four squares that are horizontal, vertical, or diagonal. Groupn b (Groupn w) be a Group with n pieces of the black (white) color and 4−n empty squares. h = ((Group1 b ∗α)+(Group2 b ∗β)+(Group3 b ∗γ)+(Group4 b ∗∞)) − ((Group1 w ∗α)+(Group2 w ∗β)+(Group3 w ∗γ)+(Group4 w ∗∞)) The values of α, β and δ can vary to form any desired linear combination; however, it is important to value them with the α < β < δ ordering in mind (we used 1, 4, and 8 as their respective values). Groups of 4 discs of the same color means victory, thus discovery of such a group will result in ∞ to ensure an extreme value. We now use our estimated evaluation function to evaluate the Black players actions during the Connect-Four adversarial interaction. Each game from the log file was input into the application, which processed and output a reformatted log file containing the h value of the current move, the maxh value that could be achieved, and a notification if an open threat was detected. A total of 123 games were analyzed (57 with White winning, and 66 with Black winning). A few additional games were manually ignored in the experiment, due to these problems: a player abandoning the game while the outcome is not final, or a blunt irrational move in the early stages of the game (e.g., not blocking an obvious winning group in the first opening moves). In addition, a single tie game was also removed. The simulator was run to a search depth of 3 moves. We now proceed to analyze the games with respect to each behavioral axiom. The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 555 Table 1: Average heuristic difference analysis Black losses Black Won Avg minh -17.62 -12.02 Avg 3 lowest h moves (min3 h) -13.20 -8.70 3.2.1 Affirming the Suboptimal tactical move axiom The following section presents the heuristic evaluations of the Min-Max algorithm for each action, and checks the amount and extent of suboptimal tactical actions and their implications on performance. Table 1 shows results and insights from the games heuristic analysis, when search depth equals 3 (this search depth was selected for the results to be comparable to [9], see Section 3.2.3). The tables heuristic data is the difference between the present maximal heuristic value and the heuristic value of the action that was eventually taken by the player (i.e., the closer the number is to 0, the closer the action was to the maximum heuristic action). The first row presents the difference values of the action that had the maximal difference value among all the Black players actions in a given game, as averaged over all Blacks winning and losing games (see respective columns). In games in which the Black player loses, its average difference value was -17.62, while in games in which the Black player won, its average was -12.02. The second row expands the analysis by considering the 3 highest heuristic difference actions, and averaging them. In that case, we notice an average heuristic difference of 5 points between games which the Black player loses and games in which it wins. Nevertheless, the importance of those numbers is that they allowed us to take an educated guess on a threshold number of 11.5, as the value of the TrH constant, which differentiates between normal actions and highly beneficial ones. After finding an approximated TrH constant, we can proceed with an analysis of the importance of suboptimal moves. To do so we took the subset of games in which the minimum heuristic difference value for Blacks actions was 11.5. As presented in Table 2, we can see the different min3 h average of the 3 largest ranges and the respective percentage of games won. The first row shows that the Black player won only 12% of the games in which the average of its 3 highest heuristically difference actions (min3 h) was smaller than the suggested threshold, TrH = 11.5. The second row shows a surprising result: it seems that when min3 h > −4 the Black player rarely wins. Intuition would suggest that games in which the action evaluation values were closer to the maximal values will result in more winning games for Black. However, it seems that in the Connect-Four domain, merely responding with somewhat easily expected actions, without initiating a few surprising and suboptimal moves, does not yield good results. The last row sums up the main insights from the analysis; most of Blacks wins (83%) came when its min3 h was in the range of -11.5 to -4. A close inspection of those Black winning games shows the following pattern behind the numbers: after standard opening moves, Black suddenly drops a disc into an isolated column, which seems a waste of a move. White continues to build its threats, while usually disregarding Blacks last move, which in turn uses the isolated disc as an anchor for a future winning threat. The results show that it was beneficial for the Black player Table 2: Blacks winnings percentages % of games min3 h < −11.5 12% min3 h > −4 5% −11.5 ≤ min3 h ≤ −4 83% to take suboptimal actions and not give the current highest possible heuristic value, but will not be too harmful for its position (i.e., will not give high beneficial value to its adversary). As it turned out, learning the threshold is an important aspect of success: taking wildly risky moves (min3 h < −11.5) or trying to avoid them (min3 h > −4) reduces the Black players winning chances by a large margin. 3.2.2 Affirming the Profile Monitoring Axiom In the task of showing the importance of monitoring ones adversaries profiles, our log files could not be used because they did not contain repeated interactions between players, which are needed to infer the players knowledge about their adversaries. However, the importance of opponent modeling and its use in attaining tactical advantages was already studied in various domains ([3, 9] are good examples). In a recent paper, Markovitch and Reger [9] explored the notion of learning and exploitation of opponent weakness in competitive interactions. They apply simple learning strategies by analyzing examples from past interactions in a specific domain. They also used the Connect-Four adversarial domain, which can now be used to understand the importance of monitoring the adversarys profile. Following the presentation of their theoretical model, they describe an extensive empirical study and check the agents performance after learning the weakness model with past examples. One of the domains used as a competitive environment was the same Connect-Four game (Checkers was the second domain). Their heuristic function was identical to ours with three different variations (H1, H2, and H3) that are distinguished from one another in their linear combination coefficient values. The search depth for the players was 3 (as in our analysis). Their extensive experiments check and compare various learning strategies, risk factors, predefined feature sets and usage methods. The bottom line is that the Connect-Four domain shows an improvement from a 0.556 winning rate before modeling to a 0.69 after modeling (page 22). Their conclusions, showing improved performance when holding and using the adversarys model, justify the effort to monitor the adversary profile for continuous and repeated interactions. An additional point that came up in their experiments is the following: after the opponent weakness model has been learned, the authors describe different methods of integrating the opponent weakness model into the agents decision strategy. Nevertheless, regardless of the specific method they chose to work with, all integration methods might cause the agent to take suboptimal decisions; it might cause the agent to prefer actions that are suboptimal at the present decision junction, but which might cause the opponent to react in accordance with its weakness model (as represented by our agent) which in turn will be beneficial for us in the future. The agents behavior, as demonstrated in [9] further confirms and strengthens our Suboptimal Tactical Axiom as discussed in the previous section. 556 The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 3.2.3 Additional Insights The need for the Goal Achieving, Preventive Act, and Evaluation Maximization axioms are obvious, and need no further verification. However, even with respect to those axioms, a few interesting insights came up in the log analysis. The Goal achieving and Preventive Act axioms, though theoretically trivial, seem to provide some challenge to a human player. In the initial inspection of the logs, we encountered few games2 where a player, for inexplicable reasons, did not block the other from winning or failed to execute its own winning move. We can blame those faults on the humans lack of attention, or a typing error in its move reply; nevertheless, those errors might occur in bounded rational agents, and the appropriate behavior needs to be axiomatized. A typical Connect-Four game revolves around generating threats and blocking them. In our analysis we looked for explicit preventive actions, i.e., moves that block a group of 3 discs, or that remove a future threat (in our limited search horizon). We found that in 83% of the total games there was at least one preventive action taken by the Black player. It was also found that Black averaged 2.8 preventive actions per game on the games in which it lost, while averaging 1.5 preventive actions per game when winning. It seems that Black requires 1 or 2 preventive actions to build its initial taking position, before starting to present threats. If it did not manage to win, it will usually prevent an extra threat or two before succumbing to White. 4. RELATED WORK Much research deals with the axiomatization of teamwork and mental states of individuals: some models use knowledge and belief [10], others have models of goals and intentions [8, 4]. However, all these formal theories deal with agent teamwork and cooperation. As far as we know, our model is the first to provide a formalized model for explicit adversarial environments and agents behavior in it. The classical Min-Max adversarial search algorithm was the first attempt to integrate the opponent into the search space with a weak assumption of an optimally playing opponent. Since then, much effort has gone into integrating the opponent model into the decision procedure to predict future behavior. The M∗ algorithm presented by Carmel and Markovitch [2] showed a method of incorporating opponent models into adversary search, while in [3] they used learning to provide a more accurate opponent model in a 2player repeated game environment, where agents strategies were modeled as finite automata. Additional Adversarial planning work was done by Willmott et al. [13], which provided an adversarial planning approach to the game of GO. The research mentioned above dealt with adversarial search and the integration of opponent models into classical utilitybased search methods. That work shows the importance of opponent modeling and the ability to exploit it to an agents advantage. However, the basic limitations of those search methods still apply; our model tries to overcome those limitations by presenting a formal model for a new, mental state-based adversarial specification. 5. CONCLUSIONS We presented an Adversarial Environment model for a 2 These were later removed from the final analysis. bounded rational agent that is situated in an N-player, zerosum environment. We used the SharedPlans formalization to define the model and the axioms that agents can apply as behavioral guidelines. The model is meant to be used as a guideline for designing agents that need to operate in such adversarial environments. We presented empirical results, based on ConnectFour log file analysis, that exemplify the model and the axioms for a bilateral instance of the environment. The results we presented are a first step towards an expanded model that will cover all types of adversarial environments, for example, environments that are non-zero-sum, and environments that contain natural agents that are not part of the direct conflict. Those challenges and more will be dealt with in future research. 6. ACKNOWLEDGMENT This research was supported in part by Israel Science Foundation grants #1211/04 and #898/05. 7. REFERENCES [1] L. V. Allis. A knowledge-based approach of Connect-Four - the game is solved: White wins. Masters thesis, Free University, Amsterdam, The Netherlands, 1988. [2] D. Carmel and S. Markovitch. Incorporating opponent models into adversary search. In Proceedings of the Thirteenth National Conference on Artificial Intelligence, pages 120-125, Portland, OR, 1996. [3] D. Carmel and S. Markovitch. Opponent modeling in multi-agent systems. In G. Weiß and S. Sen, editors, Adaptation and Learning in Multi-Agent Systems, pages 40-52. Springer-Verlag, 1996. [4] B. J. Grosz and S. Kraus. Collaborative plans for complex group action. Artificial Intelligence, 86(2):269-357, 1996. [5] M. Hadad, G. Kaminka, G. Armon, and S. Kraus. Supporting collaborative activity. In Proc. of AAAI-2005, pages 83-88, Pittsburgh, 2005. [6] http://www.gamerz.net/˜pbmserv/. [7] S. Kraus and D. Lehmann. Designing and building a negotiating automated agent. Computational Intelligence, 11:132-171, 1995. [8] H. J. Levesque, P. R. Cohen, and J. H. T. Nunes. On acting together. In Proc. of AAAI-90, pages 94-99, Boston, MA, 1990. [9] S. Markovitch and R. Reger. Learning and exploiting relative weaknesses of opponent agents. Autonomous Agents and Multi-Agent Systems, 10(2):103-130, 2005. [10] Y. M. Ronald Fagin, Joseph Y. Halpern and M. Y. Vardi. Reasoning about knowledge. MIT Press, Cambridge, Mass., 1995. [11] P. Thagard. Adversarial problem solving: Modeling an oponent using explanatory coherence. Cognitive Science, 16(1):123-149, 1992. [12] R. W. Toseland and R. F. Rivas. An Introduction to Group Work Practice. Prentice Hall, Englewood Cliffs, NJ, 2nd edition edition, 1995. [13] S. Willmott, J. Richardson, A. Bundy, and J. Levine. An adversarial planning approach to Go. Lecture Notes in Computer Science, 1558:93-112, 1999. The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 557",
    "original_translation": "Un modelo de entorno adversarial para agentes racionales limitados en interacciones de suma cero Inon Zuckerman1, Sarit Kraus1, Jeffrey S. Rosenschein2, Gal Kaminka1 1 Departamento de Ciencias de la Computación 2 Escuela de Ingeniería Universidad Bar-Ilan y Ciencias de la Computación Ramat-Gan, Israel Universidad Hebrea, Jerusalén, Israel {zukermi, sarit, galk}@cs.biu.ac.il jeff@cs.huji.ac.il RESUMEN Los entornos multiagentes a menudo no son cooperativos ni colaborativos; en muchos casos, los agentes tienen intereses conflictivos, lo que lleva a interacciones adversariales. Este documento presenta un modelo formal de Entorno Adversarial para agentes racionales acotados que operan en un entorno de suma cero. En tales entornos, los intentos de utilizar métodos de búsqueda basados en utilidad clásica pueden plantear una variedad de dificultades (por ejemplo, modelar implícitamente al oponente como un maximizador de utilidad omnisciente, en lugar de aprovechar un modelo de oponente más matizado y explícito). Definimos un Entorno Adversario al describir los estados mentales de un agente en dicho entorno. Luego presentamos axiomas de comportamiento que están destinados a servir como principios de diseño para construir agentes adversarios. Exploramos la aplicación de nuestro enfoque mediante el análisis de archivos de registro de juegos de Conecta Cuatro completados, y presentamos un análisis empírico de la adecuación de los axiomas. Categorías y Descriptores de Asignaturas I.2.11 [Inteligencia Artificial]: Inteligencia Artificial Distribuida-Agentes Inteligentes, Sistemas Multiagente; I.2.4 [Inteligencia Artificial]: Formalismos y Métodos de Representación del Conocimiento - Lógica Modal Términos Generales Diseño, Teoría 1. Introducción: En las primeras investigaciones en sistemas multiagente (SMA) se consideraban grupos cooperativos de agentes; debido a que los agentes individuales tenían recursos limitados o acceso limitado a la información (por ejemplo, potencia de procesamiento limitada, cobertura sensorial limitada), trabajaban juntos por diseño para resolver problemas que individualmente no podían resolver, o al menos no podían resolver de manera eficiente. Sin embargo, la investigación de MAS pronto comenzó a considerar agentes interactivos con intereses individualizados, como representantes de diferentes humanos u organizaciones con intereses no idénticos. Cuando las interacciones son guiadas por intereses diversos, los participantes pueden tener que superar desacuerdos, interacciones no cooperativas e incluso intentos intencionales de dañarse mutuamente. Cuando ocurren este tipo de interacciones, los entornos requieren un comportamiento apropiado por parte de los agentes situados en ellos. Llamamos a estos entornos Entornos Adversarios, y llamamos a los agentes en conflicto Adversarios. Los modelos de cooperación y trabajo en equipo han sido ampliamente explorados en MAS a través de la axiomatización de estados mentales (por ejemplo, [8, 4, 5]). Sin embargo, ninguna de estas investigaciones abordó los dominios adversarios y sus implicaciones para el comportamiento de los agentes. Nuestro artículo aborda este problema al proporcionar un modelo formal y axiomatizado de estados mentales para un subconjunto de dominios adversariales, específicamente entornos adversariales simples de suma cero. Los encuentros de suma cero simples existen, por supuesto, en varios juegos de dos jugadores (por ejemplo, Ajedrez, Damas), pero también existen en juegos de n jugadores (por ejemplo, Risk, Diplomacia), subastas de un solo bien y en otros lugares. En estos entornos recientes en particular, el uso de una búsqueda adversarial basada en utilidad (como el algoritmo Min-Max) no siempre proporciona una solución adecuada; la función de recompensa podría ser bastante compleja o difícil de cuantificar, y existen limitaciones computacionales naturales en agentes racionales acotados. Además, los métodos de búsqueda tradicionales (como Min-Max) no hacen uso de un modelo del oponente, lo cual ha demostrado ser una adición valiosa a la planificación adversarial [9, 3, 11]. En este artículo, desarrollamos un modelo formal y axiomatizado para agentes racionales limitados que se encuentran en un entorno adversarial de suma cero. El modelo utiliza diferentes operadores de modalidad, y sus principales fundamentos son el modelo SharedPlans [4] para el comportamiento colaborativo. Exploramos las propiedades del entorno y los estados mentales de los agentes para derivar axiomas de comportamiento; estos axiomas de comportamiento constituyen un modelo formal que sirve como especificación y guía de diseño para el diseño de agentes en tales entornos. Luego investigamos el comportamiento de nuestro modelo empíricamente utilizando el juego de mesa Conecta Cuatro. Mostramos que este juego se ajusta a nuestra definición de entorno, y analizamos el comportamiento de los jugadores utilizando un gran conjunto de registros de partidas completadas 550 978-81-904262-7-5 (RPS) c 2007 archivos de IFAAMAS. Además, utilizamos los resultados presentados en [9] para discutir la importancia del modelado del oponente en nuestro dominio adversarial de Conecta Cuatro. El artículo continúa de la siguiente manera. La sección 2 presenta la formalización de los modelos. La sección 3 presenta el análisis empírico y sus resultados. Discutimos el trabajo relacionado en la Sección 4, y concluimos y presentamos las direcciones futuras en la Sección 5. ENTORNOS ADVERSARIOS El modelo de entorno adversario (denominado como AE) está destinado a guiar el diseño de agentes al proporcionar una especificación de las capacidades y actitudes mentales de un agente en un entorno adversario. Nos enfocamos aquí en tipos específicos de entornos adversarios, especificados de la siguiente manera: 1. Interacciones de suma cero: las utilidades positivas y negativas de todos los agentes suman cero; 2. Agentes adversarios simples: todos los agentes en el entorno son agentes adversarios; 3. AEs bilaterales: AEs con exactamente dos agentes; 4. AEs multilaterales: AEs de tres o más agentes. Trabajaremos en instanciaciones tanto bilaterales como multilaterales de entornos de suma cero y simples. En particular, nuestro modelo de entorno adversarial tratará con interacciones que consisten en N agentes (N ≥ 2), donde todos los agentes son adversarios y solo un agente puede tener éxito. Ejemplos de tales entornos van desde juegos de mesa (por ejemplo, Ajedrez, Conecta Cuatro y Diplomacia) hasta ciertos entornos económicos (por ejemplo, subastas de N postores sobre un único bien). 2.1 Resumen del Modelo Nuestro enfoque es formalizar las actitudes mentales y comportamientos de un único agente adversario; consideramos cómo un único agente percibe el AE. La siguiente lista especifica las condiciones y estados mentales de un agente en un AE simple de suma cero: 1. El agente tiene la intención individual de que su propio objetivo se complete; 2. El agente tiene la creencia individual de que él y sus adversarios persiguen objetivos completamente opuestos (definidos a continuación) y que solo puede haber un ganador; 3. El agente tiene la creencia individual de que cada adversario tiene la intención de completar su propio objetivo conflictivo completo; 4. El agente tiene una creencia individual en el perfil (parcial) de sus adversarios. El ítem 3 es necesario, ya que podría ser el caso de que algún agente tenga un objetivo completamente conflictivo y esté considerando adoptar la intención de completarlo, pero aún no está comprometido a lograrlo. Esto podría ocurrir porque el agente aún no ha reflexionado sobre los efectos que adoptar esa intención podría tener en las otras intenciones que actualmente mantiene. En tales casos, es posible que ni siquiera se considere en un entorno adversarial. El ítem 4 establece que el agente debe tener alguna creencia sobre los perfiles de sus adversarios. El perfil representa todo el conocimiento que el agente tiene sobre su adversario: sus debilidades, capacidades estratégicas, objetivos, intenciones, confiabilidad y más. Puede ser dado explícitamente o puede ser aprendido a partir de observaciones de encuentros pasados. 2.2 Definiciones de modelos para estados mentales Utilizamos las definiciones de los operadores modales, predicados y metapredicados de Grosz y Krauss, tal como se definen en su formalización de SharedPlan [4]. Recordamos aquí algunos de los predicados y operadores que se utilizan en esa formalización: Int.To(Ai, α, Tn, Tα, C) representa las intenciones de Ai en el tiempo Tn de realizar una acción α en el tiempo Tα en el contexto de C. Int.Th(Ai, prop, Tn, Tprop, C) representa las intenciones de Ai en el tiempo Tn de que una cierta proposición prop se cumpla en el tiempo Tprop en el contexto de C. Los operadores de intención potencial, Pot.Int.To(...) y Pot.Int.Th(...), se utilizan para representar el estado mental cuando un agente considera adoptar una intención, pero no ha deliberado sobre la interacción de las otras intenciones que tiene. El operador Bel(Ai, f, Tf) representa al agente Ai creyendo en la afirmación expresada en la fórmula f, en el tiempo Tf. MB(A, f, Tf) representa la creencia mutua para un grupo de agentes A. Una instantánea del sistema encuentra que nuestro entorno se encuentra en algún estado e ∈ E de estados variables ambientales, y cada adversario en cualquier LAi ∈ L de posibles estados locales. En cualquier paso de tiempo dado, el sistema estará en algún mundo w del conjunto de todos los mundos posibles w ∈ W, donde w = E×LA1 ×LA2 × ...LAn, y n es el número de adversarios. Por ejemplo, en un juego de póker Texas Holdem, el estado local de un agente podría ser su propio conjunto de cartas (que es desconocido para su adversario) mientras que el entorno consistirá en el bote de apuestas y las cartas comunitarias (que son visibles para ambos jugadores). Una función de utilidad bajo esta formalización se define como un mapeo de un mundo posible w ∈ W a un elemento en , que expresa la deseabilidad del mundo, desde la perspectiva de un único agente. Normalmente normalizamos el rango a [0,1], donde 0 representa el mundo menos deseable posible, y 1 es el mundo más deseable. La implementación de la función de utilidad depende del dominio en cuestión. La siguiente lista especifica nuevos predicados, funciones, variables y constantes utilizadas en conjunto con las definiciones originales para la formalización del entorno adversarial: 1. φ es una acción nula (el agente no hace nada). 2. GAi es el conjunto de objetivos del agente Ai. Cada objetivo es un conjunto de predicados cuya satisfacción hace que el objetivo esté completo (usamos G∗ Ai ∈ GAi para representar un objetivo arbitrario del agente Ai). 3. gAi es el conjunto de subobjetivos del agente Ai. Los subobjetivos son predicados cuya satisfacción representa un hito importante hacia el logro del objetivo completo. gG∗ Ai ⊆ gAi es el conjunto de subobjetivos que son importantes para la finalización del objetivo G∗ Ai (usaremos g∗ G∗ Ai ∈ gG∗ Ai para representar un subobjetivo arbitrario). 4. P Aj Ai es el objeto de perfil que el agente Ai tiene sobre el agente Aj. 5. CA es un conjunto general de acciones para todos los agentes en A que se derivan de las restricciones del entorno. CAi ⊆ CA es el conjunto de acciones posibles del agente Ai. La expresión Do(Ai, α, Tα, w) se cumple cuando Ai realiza la acción α durante el intervalo de tiempo Tα en el mundo w. 7. La afirmación Achieve(G∗ Ai , α, w) es verdadera cuando se logra el objetivo G∗ Ai siguiendo la finalización de la acción α en el mundo w ∈ W, donde α ∈ CAi. El perfil (Ai, PAi Ai) es verdadero cuando el agente Ai tiene un perfil de objeto para el agente Aj. Definición 1. El conflicto total (FulConf) describe una interacción de suma cero donde solo se puede completar un solo objetivo de los objetivos en conflicto. FulConf(G∗ Ai , G∗ Aj ) ⇒ (∃α ∈ CAi , ∀w, β ∈ CAj ) (Achieve(G∗ Ai , α, w) ⇒ ¬Achieve(G∗ Aj , β, w)) ∨ (∃β ∈ CAj , ∀w, α ∈ CAi )(Achieve(G∗ Aj , β, w) ⇒ ¬Achieve(G∗ Ai , α, w)) Definición 2. El Conocimiento Adversarial (AdvKnow) es una función que devuelve un valor que representa la cantidad de The Sixth Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 551 agente de conocimiento Ai tiene sobre el perfil del agente Aj, en el tiempo Tn. Cuanto mayor sea el valor, más conocimiento tiene el agente Ai. AdvKnow: P Aj Ai × Tn → Definición 3. Eval: Esta función de evaluación devuelve un valor de utilidad esperada estimado para un agente en A, después de completar una acción de CA en algún estado del mundo w. Eval: A × CA × w → Definición 4. TrH - (Umbral) es una constante numérica en el rango [0,1] que representa un valor umbral de la función de evaluación (Eval). Una acción que produce una evaluación de utilidad estimada por encima del TrH se considera una acción altamente beneficiosa. El valor Eval es una estimación y no la función de utilidad real, la cual suele ser desconocida. Utilizar el valor real de utilidad para un agente racional fácilmente produciría el mejor resultado para ese agente. Sin embargo, los agentes generalmente no tienen las funciones de utilidad reales, sino más bien una estimación heurística de las mismas. Existen dos propiedades importantes que deben cumplir para la función de evaluación: Propiedad 1. La función de evaluación debería indicar que el estado del mundo más deseable es aquel en el que se logra el objetivo. Por lo tanto, una vez que se ha satisfecho el objetivo, no puede haber ninguna acción futura que pueda colocar al agente en un estado del mundo con un valor de Eval más alto. (∀Ai, G∗ Ai , α, β ∈ CAi , w ∈ W) Lograr(G∗ Ai , α, w) ⇒ Eval(Ai, α, w) ≥ Eval(Ai, β, w) Propiedad 2. La función de evaluación debe proyectar una acción que cause la finalización de un objetivo o subobjetivo a un valor que sea mayor que TrH (una acción altamente beneficiosa). (∀Ai, G∗ Ai ∈ GAi , α ∈ CAi , w ∈ W, g∗ GAi ∈ gGAi ) Lograr(G∗ Ai , α, w) ∨ Lograr(g∗ GAi , α, w) ⇒ Eval(Ai, α, w) ≥ TrH. Definición 5. La acción de conjunto Definimos una acción de conjunto (SetAction) como un conjunto de operaciones de acción (ya sean acciones complejas o básicas) de algunos conjuntos de acción CAi y CAj que, según la creencia del agente Ai, están unidas por una relación temporal y consecuente, formando una cadena de eventos (acción y su acción consecuente siguiente). (∀α1 , . . . , αu ∈ CAi , β1 , . . . , βv ∈ CAj , w ∈ W) SetAction(α1 , . . . , αu , β1 , . . . , βv , w) ⇒ ((Do(Ai, α1 , Tα1 , w) ⇒ Do(Aj, β1 , Tβ1 , w)) ⇒ Do(Ai, α2 , Tα2 , w) ⇒ . . . ⇒ Do(Ai, αu , Tαu , w)) La relación consecuente podría existir debido a varias restricciones ambientales (cuando una acción obliga al adversario a responder con una acción específica) o debido al conocimiento de los agentes sobre el perfil de su adversario. Propiedad 3. A medida que aumenta el conocimiento que tenemos sobre nuestro adversario, tendremos creencias adicionales sobre su comportamiento en diferentes situaciones, lo que a su vez crea un nuevo conjunto de acciones. Formalmente, si nuestro AdvKnow en el tiempo Tn+1 es mayor que AdvKnow en el tiempo Tn, entonces cada SetAction conocido en el tiempo Tn también es conocido en el tiempo Tn+1. AdvKnow(P Aj Ai , Tn+1) > AdvKnow(P Aj Ai , Tn) ⇒ (∀α1 , . . . , αu ∈ CAi , β1 , . . . , βv ∈ CAj ) Bel(Aag, SetAction(α1 , . . . , αu , β1 , . . . , βv ), Tn) ⇒ Bel(Aag, SetAction(α1 , . . . , αu , β1 , . . . , βv ), Tn+1) 2.3 La formulación del entorno Los siguientes axiomas proporcionan la definición formal para un Entorno Adversarial (AE) simple y de suma cero. La satisfacción de estos axiomas significa que el agente está situado en un entorno así. Proporciona especificaciones para que el agente Aag interactúe con su conjunto de adversarios A con respecto a los objetivos G∗ Aag y G∗ A en el tiempo TCo en algún estado del mundo w. AE(Aag, A, G∗ Aag, A1, . . . , Ak, G∗ A1, . . . , G∗ Ak, Tn, w) 1. Aag tiene un Int.Th su objetivo se completaría: (∃α ∈ CAag , Tα) Int.Th(Aag, Lograr(G∗ Aag , α), Tn, Tα, AE) 2. Aag cree que él y cada uno de sus adversarios Ao están persiguiendo metas completamente conflictivas: (∀Ao ∈ {A1, . . . , Ak}) Bel(Aag, FulConf(G∗ Aag , G∗ Ao ), Tn) 3. Aag cree que cada uno de sus adversarios en Ao tiene la Int.Th su objetivo en conflicto G∗ Aoi se completará: (∀Ao ∈ {A1, . . . , Ak})(∃β ∈ CAo , Tβ) Bel(Aag, Int.Th(Ao, Lograr(G∗ Ao , β), TCo, Tβ, AE), Tn) 4. Para construir un agente que pueda operar con éxito dentro de un entorno de adversarios estratégicos, debemos especificar pautas de comportamiento para sus interacciones. Utilizar una estrategia de maximización de Eval ingenua hasta cierta profundidad de búsqueda no siempre dará resultados satisfactorios por varias razones: (1) el problema del horizonte de búsqueda al buscar a una profundidad fija; (2) la fuerte suposición de un adversario óptimamente racional con recursos ilimitados; (3) utilizar una función de evaluación estimada que no dará resultados óptimos en todos los estados del mundo, y que puede ser explotada [9]. Los siguientes axiomas especifican los principios de comportamiento que se pueden utilizar para diferenciar entre agentes exitosos y menos exitosos en el entorno adversarial mencionado anteriormente. Esos axiomas deben ser utilizados como principios de especificación al diseñar e implementar agentes que deben ser capaces de desempeñarse bien en entornos adversarios. Los axiomas conductuales representan situaciones en las que el agente adoptará intenciones potenciales para (Pot.Int.To(...)) realizar una acción, lo que típicamente requerirá un razonamiento de medios-fines para seleccionar un posible curso de acción. Este razonamiento llevará a la adopción de un Int.To(...) (ver [4]). A1. Axioma para lograr metas. El primer axioma es el caso más simple; cuando el agente Aag cree que está a una acción (α) de lograr su objetivo en conflicto G∗ Aag, debería adoptar la intención potencial de hacer α y completar su objetivo. (∀Aag, α ∈ CAag, Tn, Tα, w ∈ W) (Bel(Aag, Do(Aag, α, Tα, w) ⇒ Achieve(G∗ Aag, α, w)) ⇒ Pot.Int.To(Aag, α, Tn, Tα, w) Este comportamiento algo trivial es el primer y más fuerte axioma. En cualquier situación, cuando el agente está a una acción de completar el objetivo, debería completar la acción. Cualquier función de evaluación justa clasificaría naturalmente a α como la acción de valor máximo (propiedad 1). Sin embargo, sin una axiomatización explícita de dicho comportamiento, podría haber situaciones en las que el agente decida tomar otra acción por diversas razones, debido a sus recursos de decisión limitados. A2. Axioma del Acto Preventivo. En una situación adversarial, el agente Aag podría decidir tomar acciones que dañen uno de los planes de su adversario para completar su objetivo, incluso si esas acciones no avanzan explícitamente a Aag hacia su objetivo en conflicto G∗ Aag. Tal acción preventiva tendrá lugar cuando el agente Aag tenga la creencia sobre la posibilidad de que su adversario Ao realice una acción β que le otorgará un alto 552 The Sixth Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) valor de evaluación de utilidad (> TrH). Creer que tomar la acción α evitará que el oponente realice su β, adoptará una intención potencial de hacer α. (∀Aag, Ao ∈ A, α ∈ CAag , β ∈ CAo , Tn, Tβ, w ∈ W) (Bel(Aag, Do(Ao, β, Tβ, w) ∧ Eval(Ao, β, w) > TrH, Tn) ∧ Bel(Aag, Do(Aag, α, Tα, w) ⇒ ¬Do(Ao, β, Tβ, w), Tn) ⇒ Pot.Int.To(Aag, α, Tn, Tα, w) Este axioma es un componente básico de cualquier entorno adversarial. Por ejemplo, al observar un juego de ajedrez, un jugador podría darse cuenta de que está a punto de ser jaque mateado por su oponente, por lo tanto, realiza una jugada preventiva. Otro ejemplo es el juego de Cuatro en Línea: cuando un jugador tiene una fila de tres fichas, su oponente debe bloquearla o perder. Una instancia específica de A1 ocurre cuando el adversario está a una acción de lograr su objetivo, y la acción preventiva inmediata debe ser tomada por el agente. Formalmente, tenemos las mismas creencias que se mencionaron anteriormente, con la creencia modificada de que realizar la acción β hará que el agente Ao logre su objetivo. Proposición 1: Caso de prevención o pérdida. (∀Aag, Ao ∈ A, α ∈ CAag , β ∈ CAo , G∗ Ao , Tn, Tα, Tβ, w ∈ W) Bel(Aag, Do(Ao, β, Tβ, w) ⇒ Achieve(G∗ Ao , β, w), Tn) ∧ Bel(Aag, Do(Aag, α, Tα, w) ⇒ ¬Do(Ao, β, Tβ, w)) ⇒ Pot.Int.To(Aag, α, Tn, Tα, w) Esquema de prueba: La Proposición 1 puede derivarse fácilmente del axioma A1 y de la propiedad 2 de la función Eval, que establece que cualquier acción que cause la finalización de un objetivo es una acción altamente beneficiosa. El comportamiento del acto preventivo ocurrirá implícitamente cuando la función Eval sea igual a la función de utilidad del mundo real. Sin embargo, al ser agentes racionales limitados y al tratar con una función de evaluación estimada, necesitamos axiomatizar explícitamente dicho comportamiento, ya que no siempre ocurrirá implícitamente a partir de la función de evaluación. A3. Axioma de Movimiento Táctico Subóptimo. En muchos escenarios, puede ocurrir una situación en la que un agente decida no tomar la acción más beneficiosa actual que puede tomar (la acción con el valor de evaluación de utilidad máximo), porque cree que tomar otra acción (con un valor de evaluación de utilidad más bajo) podría generar (dependiendo de la respuesta del adversario) una posibilidad futura para una acción altamente beneficiosa. Esto ocurrirá con mayor frecuencia cuando la función Eval sea inexacta y difiera en gran medida de la función de Utilidad. De manera formal, el agente Aag cree en una cierta SetAction que evolucionará de acuerdo con su acción inicial y producirá un valor beneficioso alto (> TrH) únicamente para él. (∀Aag, Ao ∈ A, Tn, w ∈ W) (∃α1 , . . . , αu ∈ CAi , β1 , . . . , βv ∈ CAj , Tα1 ) Bel(Aag, SetAction(α1 , . . . , αu , β1 , . . . , βv ), Tn) ∧ Bel(Aag, Eval(Ao, βv , w) < TrH < Eval(Aag, αu , w), Tn) ⇒ Pot.Int.To(Aag, α1 , Tn, Tα1 , w) Un agente podría creer que una cadena de eventos ocurrirá por diversas razones debido a la naturaleza inevitable del dominio. Por ejemplo, en el ajedrez, a menudo observamos lo siguiente: una jugada provoca una posición de jaque, lo que a su vez limita los movimientos del oponente para evitar el jaque, a lo que el primer jugador podría reaccionar con otro jaque, y así sucesivamente. El agente también podría creer en una cadena de eventos basada en su conocimiento del perfil de su adversario, lo que le permite prever los movimientos del adversario con alta precisión. A4. Axioma de Detección de Perfiles. El agente puede ajustar los perfiles de sus adversarios mediante observaciones y estudio de patrones (específicamente, si hay encuentros repetidos con el mismo adversario). Sin embargo, en lugar de esperar a que se revele la información del perfil, un agente también puede iniciar acciones que obligarán a su adversario a reaccionar de una manera que revele conocimiento del perfil sobre él. Formalmente, el axioma establece que si todas las acciones (γ) no son acciones altamente beneficiosas (< TrH), el agente puede realizar la acción α en el tiempo Tα si cree que resultará en una acción no altamente beneficiosa β por parte de su adversario, lo que a su vez le enseña sobre el perfil del adversario, es decir, proporciona un mayor AdvKnow(P Aj Ai , Tβ). (∀Aag, Ao ∈ A, α ∈ CAag , β ∈ CAo , Tn, Tα, Tβ, w ∈ W) Bel(Aag, (∀γ ∈ CAag )Eval(Aag, γ, w) < TrH, Tn) ∧ Bel(Aag, Do(Aag, α, Tα, w) ⇒ Do(Ao, β, Tβ, w), Tn) ∧ Bel(Aag, Eval(Ao, β, w) < TrH) ∧ Bel(Aag, AdvKnow(P Aj Ai , Tβ) > AdvKnow(P Aj Ai , Tn), Tn) ⇒ Pot.Int.To(Aag, α, Tn, Tα, w) Por ejemplo, volviendo al escenario del juego de ajedrez, consideremos comenzar una partida contra un oponente del que no sabemos nada, ni siquiera si es humano o un oponente computarizado. Podríamos comenzar a jugar una estrategia que sea adecuada frente a un oponente promedio, y ajustar nuestro juego de acuerdo a su nivel de juego. A5. Axioma de Formación de Alianzas El siguiente axioma de comportamiento es relevante únicamente en una instancia multilateral del entorno adversarial (obviamente, una alianza no puede formarse en un encuentro bilateral de suma cero). En diferentes situaciones durante una interacción multilateral, un grupo de agentes podría creer que es de su interés formar una alianza temporal. Una alianza de este tipo es un acuerdo que limita el comportamiento de sus miembros, pero se cree por parte de estos que les permite alcanzar un valor de utilidad mayor al que podrían lograr fuera de la alianza. Como ejemplo, podemos observar el clásico juego de mesa Risk, donde cada jugador tiene como objetivo individual ser el único conquistador del mundo, un juego de suma cero. Sin embargo, para lograr este objetivo, podría ser estratégicamente sabio hacer acuerdos de alto el fuego a corto plazo con otros jugadores, o unirse y atacar a un oponente que sea más fuerte que el resto. Los términos de una alianza definen la forma en que sus miembros deben actuar. Es un conjunto de predicados, denominados Términos, que es acordado por los miembros de la alianza y debe mantenerse verdadero durante la duración de la alianza. Por ejemplo, el conjunto de términos en el escenario de riesgo podría contener los siguientes predicados: 1. Los miembros de la alianza no se atacarán entre sí en los territorios X, Y y Z; 2. Los miembros de la alianza contribuirán C unidades por turno para atacar al adversario Ao; 3. Los miembros están obligados a permanecer como parte de la alianza hasta el tiempo Tk o hasta que el ejército adversario Ao sea más pequeño que Q. El conjunto de términos especifica restricciones intergrupales en cada uno de los miembros de la alianza (∀Aal i ∈ Aal ⊆ A) conjunto de acciones Cal i ⊆ C. Definición 6. El valor total de evaluación que el agente Ai logrará al ser parte de Aal es la suma de Evali (Eval para Ai) de cada uno de los valores de evaluación de Aal j después de tomar sus propias acciones α (a través del predicado del agente(α)): Al val(Ai, Cal, Aal, w) = α∈Cal Evali(Aal j, agente(α), w) Definición 7. Al TrH - es un número que representa un valor de Al en la Sexta Conferencia Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 553 umbral; por encima de él, se puede decir que la alianza es una alianza altamente beneficiosa. El valor de Al TrH se calculará dinámicamente de acuerdo con el progreso de la interacción, como se puede ver en [7]. Después de formarse una alianza, sus miembros ahora están trabajando en su entorno adversarial normal, así como de acuerdo con los estados mentales y axiomas requeridos para sus interacciones como parte de la alianza. El siguiente modelo de Alianza (AL) especifica las condiciones bajo las cuales el grupo Aal puede considerarse en una alianza y trabajando con un nuevo y limitado conjunto de acciones Cal, en el tiempo Tn. AL(Aal , Cal , w, Tn) 1. \n\nAL(Aal , Cal , w, Tn) 1. Aal tiene un MB en el que todos los miembros son parte de Aal: MB(Aal, (∀Aal i ∈ Aal)miembro(Aal i, Aal), Tn) 2. Aal tiene una MB que el grupo se mantenga: MB(Aal , (∀Aal i ∈ Aal )Int.Th (Ai, miembro(Ai, Aal ), Tn, Tn+1, Co), Tn) 3. Aal tiene un beneficio marginal que al ser miembros les otorga un alto valor de utilidad: MB(Aal , (∀Aal i ∈ Aal )Al val(Aal i , Cal , Aal , w) ≥ Al TrH, Tn) Los perfiles de los miembros son una parte crucial de las alianzas exitosas. Suponemos que los agentes que tienen perfiles más precisos de sus adversarios serán más exitosos en tales entornos. Tales agentes podrán predecir cuándo un miembro está a punto de violar el contrato de alianzas (elemento 2 en el modelo anterior) y tomar medidas correctivas (cuando el elemento 3 se falsificará). La robustez de la alianza depende en parte de la medida de confianza de sus miembros, la estimación de su posición objetiva y otras propiedades de su perfil. Debemos tener en cuenta que un agente puede formar parte simultáneamente de más de una alianza. Una alianza temporal, donde los miembros del grupo no tienen un objetivo común pero actúan colaborativamente en interés de sus propias metas individuales, es clasificada como un Grupo de Tratamiento por los psicólogos modernos (en contraste con un Grupo de Tarea, donde sus miembros tienen un objetivo común). El modelo de Actividad Compartida presentado en [5] modeló el comportamiento del Grupo de Tratamiento utilizando la misma formalización de Planes Compartidos. Al comparar ambas definiciones de una alianza y un Grupo de Tratamiento, encontramos una similitud sorprendente entre ambos modelos: las definiciones de los modelos de entorno son casi idénticas (ver las definiciones de SAs en [5]), y sus axiomas de Acto Egoísta y Acto Cooperativo se ajustan al comportamiento de nuestros agentes adversarios. La principal distinción entre ambos modelos es la integración de un axioma de acto de comportamiento útil, en la Actividad Compartida que no puede formar parte de la nuestra. Este axioma establece que un agente considerará tomar una acción que disminuirá su valor de Evaluación (hasta un cierto límite inferior), si cree que un compañero del grupo obtendrá un beneficio significativo. Dicho comportamiento no puede ocurrir en un entorno puramente adversarial (como un juego de suma cero), donde los miembros de la alianza están constantemente alerta para manipular su alianza en su propio beneficio. A6. Axioma de Maximización de la Evaluación. En un caso en el que todos los demás axiomas no sean aplicables, procederemos con la acción que maximice el valor heurístico calculado en la función Eval. (∀Aag, Ao ∈ A, α ∈ Cag, Tn, w ∈ W) Bel(Aag, (∀γ ∈ Cag)Eval(Aag, α, w) ≥ Eval(Aag, γ, w), Tn) ⇒ Pot.Int.To(Aag, α, Tn, Tα, w) T1. Optimalidad en Eval = Utilidad El modelo axiomático anterior aborda situaciones en las que la Utilidad es desconocida y los agentes son agentes racionales limitados. El siguiente teorema muestra que en interacciones bilaterales, donde los agentes tienen la función de Utilidad real (es decir, Eval = Utilidad) y son agentes racionales, los axiomas proporcionan el mismo resultado óptimo que la búsqueda adversarial clásica (por ejemplo, Min-Max). Teorema 1. Que Ae ag sea un agente AE racional ilimitado que utiliza la función de evaluación heurística Eval, Au ag sea el mismo agente utilizando la verdadera función de utilidad y Ao sea un único adversario racional ilimitado basado en utilidad. Dado que Eval = Utilidad: (∀α ∈ CAu ag , α ∈ CAe ag , Tn, w ∈ W) Pot.Int.To(Au ag, α, Tn, Tα, w) → Pot.Int.To(Ae ag, α , Tn, Tα, w) ∧ ((α = α ) ∨ (Utilidad(Au ag, α, w) = Eval(Ae ag, α , w))) Esquema de prueba - Dado que Au ag tiene la función de utilidad real y recursos ilimitados, puede generar el árbol de juego completo y ejecutar el algoritmo MinMax óptimo para elegir la acción de mayor valor de utilidad, que denotamos como α. La prueba mostrará que Ae ag, utilizando los axiomas AE, seleccionará la misma utilidad α o igual (cuando hay más de una acción con la misma utilidad máxima) cuando Eval = Utilidad. (A1) Axioma de logro de objetivo - supongamos que hay un α tal que su cumplimiento logrará el objetivo Au ags. Obtendrá la utilidad más alta mediante Min-Max para Au ag. El agente Ae seleccionará α u otra acción con el mismo valor de utilidad a través de A1. Si tal α no existe, Ae ag no puede aplicar este axioma y procede a A2. (A2) Axioma de la acción preventiva - (1) Observando el caso base (ver Prop1), si hay un β que lleva a Ao a alcanzar su objetivo, entonces una acción preventiva α producirá la mayor utilidad para Au ag. Au ag lo elegirá a través de la utilidad, mientras que Ae ag lo elegirá a través de A2. En el caso general, β es una acción altamente beneficiosa para Ao, lo que resulta en una baja utilidad para Au ag, lo que lo guiará a seleccionar un α que evitará β, mientras que Ae ag lo elegirá a través de A2.1 Si dicho β no existe para Ao, entonces A2 no es aplicable, y Ae ag puede proceder a A3. (A3) Axioma de movimiento táctico subóptimo: al usar una función de evaluación heurística, Ae ag tiene una creencia parcial en el perfil de su adversario (elemento 4 en el modelo AE), lo que puede llevarlo a creer en SetActions (Prop1). En nuestro caso, Ae ag tiene un perfil completo de su adversario óptimo y sabe que Ao se comportará de manera óptima según los valores reales de utilidad en el árbol de búsqueda completo, por lo tanto, cualquier creencia sobre una SetAction subóptima no puede existir, lo que hace que este axioma sea inaplicable. Ae ag procederá a A4. (A4) Axioma de detección de perfil - Dado que Ae ag tiene el perfil completo de Ao, ninguna de las acciones de Ae ag puede aumentar su conocimiento. Ese axioma no se aplicará, y el agente procederá con A6 (A5 será ignorado porque la interacción es bilateral). (A6) Axioma de maximización de la evaluación: este axioma seleccionará la máxima Evaluación para Ae ag. Dado que Eval = Utilidad, se seleccionará el mismo α que fue seleccionado por Au ag. 3. EVALUACIÓN El propósito principal de nuestro análisis experimental es evaluar el comportamiento y rendimiento de los modelos en un entorno adversarial real. Esta sección investiga si un caso acotado 1 en el que, tras la finalización de β, exista un γ que proporcione una alta utilidad para el Agente Au ag, no puede ocurrir porque Ao utiliza la misma utilidad, y la existencia de γ hará que clasifique a β como una acción de baja utilidad. 554 The Sixth Intl. En la Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07), los agentes racionales situados en entornos adversariales se beneficiarán más al aplicar nuestros axiomas de comportamiento sugeridos. 3.1 El Dominio Para explorar el uso del modelo anterior y sus axiomas de comportamiento, decidimos utilizar el juego de Conecta Cuatro como nuestro entorno adversarial. Connect-Four es un juego de 2 jugadores, de suma cero, que se juega utilizando un tablero similar a una matriz de 6x7. En cada turno, un jugador coloca un disco en una de las 7 columnas (el conjunto de 21 discos suele ser de color amarillo para el jugador 1 y rojo para el jugador 2; usaremos Blanco y Negro respectivamente para evitar confusiones). El ganador es el primer jugador en completar un conjunto de cuatro discos de su color en horizontal, vertical o diagonal. En ocasiones muy raras, el juego podría terminar en empate si todos los cuadros vacíos están llenos, pero ningún jugador logró crear un conjunto de 4 discos. El juego de Conecta Cuatro fue resuelto en [1], donde se muestra que el primer jugador (jugando con los discos blancos) puede forzar una victoria comenzando en la columna del medio (columna 4) y jugando de manera óptima. Sin embargo, la estrategia óptima es muy compleja y difícil de seguir incluso para agentes racionales limitados complejos, como los jugadores humanos. Antes de que podamos proceder a verificar el comportamiento del agente, primero debemos verificar que el dominio se ajusta a la definición de entornos adversarios mencionada anteriormente (en la que se basan los axiomas de comportamiento). Primero, al jugar al juego de Conecta Cuatro, el agente tiene la intención de ganar el juego (ítem 1). Segundo (ítem 2), nuestro agente cree que en Conecta Cuatro solo puede haber un ganador (o ningún ganador en la rara ocasión de un empate). Además, nuestro agente cree que su oponente en el juego intentará ganar (ítem 3), y esperamos que tenga algún conocimiento parcial (ítem 4) sobre su adversario (este conocimiento puede variar desde nada, pasando por hechos simples como la edad, hasta estrategias y debilidades). Por supuesto, no todos los encuentros de Conecta Cuatro son adversariales. Por ejemplo, cuando un padre está jugando el juego con su hijo, la siguiente situación podría ocurrir: el niño, con un fuerte incentivo para ganar, trata el entorno como adversarial (intenta ganar, entiende que solo puede haber un ganador y cree que su padre está tratando de vencerlo). Sin embargo, desde el punto de vista de los padres, el entorno podría ser visto como educativo, donde el objetivo no es ganar el juego, sino causar disfrute o practicar el razonamiento estratégico. En un entorno educativo como este, un nuevo conjunto de axiomas de comportamiento podría ser más beneficioso para los objetivos de los padres que nuestros axiomas de comportamiento adversarial sugeridos. 3.2 Análisis de Axiomas Después de demostrar que el juego de Conecta Cuatro es de hecho un entorno adversarial bilateral de suma cero, el siguiente paso es observar el comportamiento de los jugadores durante el juego y verificar si comportarse de acuerdo con nuestro modelo mejora el rendimiento. Para hacerlo, hemos recopilado archivos de registro de juegos de Conecta Cuatro completados que fueron jugados por jugadores humanos a través de Internet. Nuestros datos de archivo de registro recopilados provienen de sitios de Juego por Correo Electrónico (PBeM). Estos son sitios web que alojan juegos por correo electrónico, donde cada movimiento se realiza a través de un intercambio de correos electrónicos entre el servidor y los jugadores. Muchos de estos sitios de archivos contienen interacciones competitivas reales y también mantienen un sistema de clasificación para sus miembros. La mayoría de los datos que utilizamos se pueden encontrar en [6]. Como se puede aprender de [1], Conecta Cuatro tiene una estrategia óptima y una ventaja considerable para el jugador que comienza el juego (a quien llamamos jugador Blanco). Nos concentraremos en nuestro análisis en los movimientos de los segundos jugadores (a los que llamaremos Negras). El jugador Blanco, al ser el primero en actuar, tiene la llamada ventaja de la iniciativa. Tener la ventaja y una buena estrategia mantendrá al jugador Negro ocupado reaccionando a sus movimientos, en lugar de iniciar amenazas. Una amenaza es una combinación de tres discos del mismo color, con un espacio vacío para el cuarto disco ganador. Una amenaza abierta es una amenaza que puede materializarse en el próximo movimiento del oponente. Para que el jugador Negro gane, debe de alguna manera cambiar el rumbo, tomar la ventaja y comenzar a presentar amenazas al jugador Blanco. Exploraremos el comportamiento de los jugadores negros y su conformidad con nuestros axiomas. Para hacerlo, construimos una aplicación que lee archivos de registro y analiza los movimientos de los jugadores Negros. La aplicación contiene dos componentes principales: (1) un algoritmo Min-Max para la evaluación de movimientos; (2) un detector de amenazas abiertas para descubrir amenazas abiertas. El algoritmo Min-Max funcionará hasta una profundidad d dada y, para cada movimiento α, devolverá el valor heurístico para la próxima acción tomada por el jugador según lo escrito en el archivo de registro, h(α), junto con el valor heurístico máximo, maxh(α), que podría lograrse antes de realizar el movimiento (obviamente, si h(α) = maxh(α), entonces el jugador no realizó el movimiento óptimo desde el punto de vista heurístico). El trabajo de los detectores de amenazas es notificar si se tomó alguna acción para bloquear una amenaza abierta (no bloquear una amenaza abierta probablemente hará que el jugador pierda en el próximo movimiento del oponente). La función heurística utilizada por Min-Max para evaluar la utilidad de los jugadores es la siguiente función, que es simple de calcular, pero proporciona un desafío razonable a los oponentes humanos: Definición 8. Sea Grupo un conjunto adyacente de cuatro cuadrados que son horizontales, verticales o diagonales. El Grupo n (Grupo w) es un Grupo con n piezas de color negro (blanco) y 4−n casillas vacías. h = ((Grupo1 negro ∗α)+(Grupo2 negro ∗β)+(Grupo3 negro ∗γ)+(Grupo4 negro ∗∞)) − ((Grupo1 blanco ∗α)+(Grupo2 blanco ∗β)+(Grupo3 blanco ∗γ)+(Grupo4 blanco ∗∞)) Los valores de α, β y δ pueden variar para formar cualquier combinación lineal deseada; sin embargo, es importante asignarles valores teniendo en cuenta el orden α < β < δ (usamos 1, 4 y 8 como sus valores respectivos). Grupos de 4 discos del mismo color significan victoria, por lo tanto, el descubrimiento de dicho grupo resultará en ∞ para asegurar un valor extremo. Ahora utilizamos nuestra función de evaluación estimada para evaluar las acciones de los jugadores Negros durante la interacción adversarial de Conecta Cuatro. Cada juego del archivo de registro fue introducido en la aplicación, la cual procesó y generó un archivo de registro reformateado que contenía el valor h del movimiento actual, el valor maxh que podría ser alcanzado y una notificación si se detectaba una amenaza abierta. Un total de 123 partidas fueron analizadas (57 con victoria de las blancas y 66 con victoria de las negras). Algunos juegos adicionales fueron ignorados manualmente en el experimento, debido a estos problemas: un jugador abandonando el juego mientras el resultado no es definitivo, o una jugada irracional y contundente en las primeras etapas del juego (por ejemplo, no bloquear un grupo obviamente ganador en las primeras jugadas de apertura). Además, también se eliminó un empate. El simulador se ejecutó con una profundidad de búsqueda de 3 movimientos. Ahora procedemos a analizar los juegos con respecto a cada axioma de comportamiento. El Sexto Internacional. La siguiente sección presenta las evaluaciones heurísticas del algoritmo Min-Max para cada acción, y verifica la cantidad y el alcance de las acciones tácticas subóptimas y sus implicaciones en el rendimiento. La Tabla 1 muestra los resultados y las percepciones del análisis heurístico de los juegos, cuando la profundidad de búsqueda es igual a 3 (esta profundidad de búsqueda fue seleccionada para que los resultados sean comparables a [9], ver Sección 3.2.3). La información heurística de las tablas es la diferencia entre el valor heurístico máximo presente y el valor heurístico de la acción que finalmente tomó el jugador (es decir, cuanto más cercano sea el número a 0, más cercana estuvo la acción a la acción heurística máxima). La primera fila presenta los valores de diferencia de la acción que tuvo el valor de diferencia máximo entre todas las acciones de los jugadores Negros en un juego dado, promediado en todos los juegos ganados y perdidos por los Negros (ver columnas respectivas). En los juegos en los que el jugador Negro pierde, su valor promedio de diferencia fue de -17.62, mientras que en los juegos en los que el jugador Negro ganó, su promedio fue de -12.02. La segunda fila amplía el análisis al considerar las 3 acciones con mayor diferencia heurística, y promediarlas. En ese caso, notamos una diferencia heurística promedio de 5 puntos entre los juegos en los que el jugador Negro pierde y los juegos en los que gana. Sin embargo, la importancia de esos números es que nos permitieron hacer una suposición educada sobre un número umbral de 11.5, como el valor de la constante TrH, que diferencia entre acciones normales y altamente beneficiosas. Después de encontrar una constante TrH aproximada, podemos proceder con un análisis de la importancia de los movimientos subóptimos. Para hacerlo, tomamos el subconjunto de juegos en los que el valor mínimo de diferencia heurística para las acciones de las negras era de 11.5. Como se presenta en la Tabla 2, podemos ver el promedio de 3 horas mínimo de los 3 mayores rangos y el porcentaje respectivo de juegos ganados. La primera fila muestra que el jugador Negro ganó solo el 12% de los juegos en los que el promedio de sus 3 acciones con mayor diferencia heurística (min3 h) era menor que el umbral sugerido, TrH = 11.5. La segunda fila muestra un resultado sorprendente: parece que cuando min3 h > −4, el jugador Negro rara vez gana. La intuición sugeriría que los juegos en los que los valores de evaluación de la acción estén más cerca de los valores máximos resultarán en más juegos ganados para las piezas negras. Sin embargo, parece que en el dominio de Conecta Cuatro, simplemente responder con acciones algo fácilmente esperadas, sin iniciar algunos movimientos sorprendentes y subóptimos, no produce buenos resultados. La última fila resume las principales conclusiones del análisis; la mayoría de las victorias de Blacks (83%) se produjeron cuando su min3 h estaba en el rango de -11.5 a -4. Una inspección detallada de esos juegos ganados por las fichas negras muestra el siguiente patrón detrás de los números: después de los movimientos de apertura estándar, las fichas negras de repente colocan una ficha en una columna aislada, lo cual parece ser un desperdicio de un movimiento. El blanco sigue construyendo sus amenazas, generalmente ignorando el último movimiento de las negras, que a su vez utiliza el disco aislado como ancla para una futura amenaza ganadora. Los resultados muestran que fue beneficioso para el jugador Negro en la Tabla 2: Porcentajes de ganancias de los Negros % de juegos min3 h < −11.5 12% min3 h > −4 5% −11.5 ≤ min3 h ≤ −4 83% tomar acciones subóptimas y no dar el valor heurístico más alto posible en ese momento, pero no será demasiado perjudicial para su posición (es decir, no dará un alto valor beneficioso a su adversario). Resultó que aprender el umbral es un aspecto importante del éxito: realizar movimientos extremadamente arriesgados (min3 h < −11.5) o intentar evitarlos (min3 h > −4) reduce significativamente las posibilidades de ganar de los jugadores negros. 3.2.2 Afirmación del Axioma de Monitoreo de Perfiles En la tarea de demostrar la importancia de monitorear los perfiles de los adversarios, nuestros archivos de registro no pudieron ser utilizados porque no contenían interacciones repetidas entre jugadores, que son necesarias para inferir el conocimiento de los jugadores sobre sus adversarios. Sin embargo, la importancia del modelado del oponente y su uso para obtener ventajas tácticas ya ha sido estudiada en varios ámbitos ([3, 9] son buenos ejemplos). En un artículo reciente, Markovitch y Reger [9] exploraron la noción de aprendizaje y explotación de la debilidad del oponente en interacciones competitivas. Aplican estrategias de aprendizaje simples analizando ejemplos de interacciones pasadas en un dominio específico. También utilizaron el dominio adversarial de Conecta Cuatro, que ahora se puede utilizar para comprender la importancia de monitorear el perfil del adversario. Tras la presentación de su modelo teórico, describen un extenso estudio empírico y verifican el rendimiento de los agentes después de aprender el modelo de debilidades con ejemplos pasados. Uno de los dominios utilizados como entorno competitivo fue el mismo juego de Conecta Cuatro (las Damas fueron el segundo dominio). Su función heurística era idéntica a la nuestra con tres variaciones diferentes (H1, H2 y H3) que se distinguen entre sí por los valores de sus coeficientes de combinación lineal. La profundidad de búsqueda para los jugadores fue de 3 (como en nuestro análisis). Sus extensos experimentos verifican y comparan diversas estrategias de aprendizaje, factores de riesgo, conjuntos de características predefinidas y métodos de uso. La conclusión es que el dominio de Conecta Cuatro muestra una mejora de una tasa de victoria de 0.556 antes de modelar a 0.69 después de modelar (página 22). Sus conclusiones, que muestran un rendimiento mejorado al sostener y utilizar el modelo del adversario, justifican el esfuerzo de monitorear el perfil del adversario para interacciones continuas y repetidas. Un punto adicional que surgió en sus experimentos es el siguiente: después de que se haya aprendido el modelo de debilidad del oponente, los autores describen diferentes métodos para integrar el modelo de debilidad del oponente en la estrategia de decisión de los agentes. Sin embargo, independientemente del método específico que elijan para trabajar, todos los métodos de integración podrían hacer que el agente tome decisiones subóptimas; podría hacer que el agente prefiera acciones que son subóptimas en el momento actual de la decisión, pero que podrían hacer que el oponente reaccione de acuerdo con su modelo de debilidad (como lo representa nuestro agente), lo cual a su vez será beneficioso para nosotros en el futuro. El comportamiento de los agentes, como se demuestra en [9], confirma y refuerza aún más nuestro Axioma Táctico Subóptimo discutido en la sección anterior. 556 The Sixth Intl. La necesidad de los axiomas de Logro de Objetivos, Acto Preventivo y Maximización de la Evaluación es evidente y no requiere más verificación. Sin embargo, incluso con respecto a esos axiomas, surgieron algunas ideas interesantes en el análisis de registros. Los axiomas de Logro y Prevención de Actos, aunque teóricamente triviales, parecen presentar un desafío para un jugador humano. En la inspección inicial de los registros, nos encontramos con algunos juegos donde un jugador, por razones inexplicables, no bloqueó al otro para que ganara o no logró ejecutar su propia jugada ganadora. Podemos atribuir esas fallas a la falta de atención de los humanos, o a un error de escritura en su respuesta al movimiento; sin embargo, esos errores podrían ocurrir en agentes racionales limitados, y el comportamiento apropiado debe ser axiomatizado. Un juego típico de Cuatro en Línea gira en torno a generar amenazas y bloquearlas. En nuestro análisis buscamos acciones preventivas explícitas, es decir, movimientos que bloqueen un grupo de 3 discos o que eliminen una amenaza futura (en nuestro horizonte de búsqueda limitado). Encontramos que en el 83% de los juegos totales, hubo al menos una acción preventiva tomada por el jugador Negro. También se encontró que Black promedió 2.8 acciones preventivas por juego en los juegos en los que perdió, mientras que promedió 1.5 acciones preventivas por juego al ganar. Parece que Black necesita 1 o 2 acciones preventivas para construir su posición inicial de toma, antes de comenzar a presentar amenazas. Si no logra ganar, generalmente evitará una amenaza adicional o dos antes de sucumbir ante el Blanco. 4. TRABAJO RELACIONADO Mucha investigación se ocupa de la axiomatización del trabajo en equipo y los estados mentales de los individuos: algunos modelos utilizan el conocimiento y la creencia [10], otros tienen modelos de metas e intenciones [8, 4]. Sin embargo, todas estas teorías formales tratan sobre el trabajo en equipo y la cooperación de agentes. Hasta donde sabemos, nuestro modelo es el primero en proporcionar un modelo formalizado para entornos adversarios explícitos y el comportamiento de los agentes en él. El algoritmo de búsqueda adversarial clásico Min-Max fue el primer intento de integrar al oponente en el espacio de búsqueda con la suposición débil de un oponente que juega de manera óptima. Desde entonces, se ha dedicado mucho esfuerzo a integrar el modelo del oponente en el procedimiento de toma de decisiones para predecir el comportamiento futuro. El algoritmo M∗ presentado por Carmel y Markovitch [2] mostró un método para incorporar modelos de oponentes en la búsqueda de adversarios, mientras que en [3] utilizaron el aprendizaje para proporcionar un modelo de oponente más preciso en un entorno de juego repetido de 2 jugadores, donde las estrategias de los agentes se modelaron como autómatas finitos. El trabajo adicional de planificación adversarial fue realizado por Willmott y colaboradores [13], quienes proporcionaron un enfoque de planificación adversarial para el juego de GO. La investigación mencionada anteriormente trató sobre la búsqueda adversarial y la integración de modelos de oponentes en métodos de búsqueda de utilidad clásicos. Ese trabajo muestra la importancia del modelado del oponente y la capacidad de explotarlo en beneficio de un agente. Sin embargo, las limitaciones básicas de esos métodos de búsqueda siguen aplicándose; nuestro modelo intenta superar esas limitaciones presentando un modelo formal para una nueva especificación adversarial basada en estados mentales. 5. CONCLUSIONES Presentamos un modelo de Entorno Adversarial para un agente racional acotado que está situado en un entorno de N jugadores, de suma cero. Estos fueron posteriormente eliminados del análisis final. Utilizamos la formalización de SharedPlans para definir el modelo y los axiomas que los agentes pueden aplicar como pautas de comportamiento. El modelo está destinado a ser utilizado como una guía para diseñar agentes que necesitan operar en entornos adversos. Presentamos resultados empíricos, basados en el análisis del archivo de registro de ConnectFour, que ejemplifican el modelo y los axiomas para una instancia bilateral del entorno. Los resultados que presentamos son un primer paso hacia un modelo ampliado que abarcará todos los tipos de entornos adversarios, por ejemplo, entornos que no son de suma cero y entornos que contienen agentes naturales que no forman parte del conflicto directo. Esos desafíos y más serán abordados en futuras investigaciones. 6. AGRADECIMIENTO Esta investigación fue apoyada en parte por las becas de la Fundación de Ciencia de Israel #1211/04 y #898/05. 7. REFERENCIAS [1] L. V. Allis. Un enfoque basado en el conocimiento de Conecta Cuatro: el juego está resuelto, gana el jugador Blanco. Tesis de maestría, Universidad Libre de Ámsterdam, Países Bajos, 1988. [2] D. Carmel y S. Markovitch. Incorporando modelos de oponentes en la búsqueda de adversarios. En Actas de la Decimotercera Conferencia Nacional de Inteligencia Artificial, páginas 120-125, Portland, Oregón, 1996. [3] D. Carmel y S. Markovitch. Modelado de oponentes en sistemas multiagente. En G. Weiß y S. Sen, editores, Adaptación y Aprendizaje en Sistemas Multiagente, páginas 40-52. Springer-Verlag, 1996. [4] B. J. Grosz y S. Kraus. Planes colaborativos para acciones grupales complejas. Inteligencia Artificial, 86(2):269-357, 1996. [5] M. Hadad, G. Kaminka, G. Armon y S. Kraus. Apoyando la actividad colaborativa. En Proc. de AAAI-2005, páginas 83-88, Pittsburgh, 2005. [6] http://www.gamerz.net/˜pbmserv/. [7] S. Kraus y D. Lehmann. Diseñando y construyendo un agente automatizado de negociación. Inteligencia Computacional, 11:132-171, 1995. [8] H. J. Levesque, P. R. Cohen y J. H. T. Nunes. Actuar juntos. En Proc. de AAAI-90, páginas 94-99, Boston, MA, 1990. [9] S. Markovitch y R. Reger. Aprender y aprovechar las debilidades relativas de los agentes oponentes. Agentes Autónomos y Sistemas Multiagente, 10(2):103-130, 2005. [10] Y. M. Ronald Fagin, Joseph Y. Halpern y M. Y. Vardi. Razonamiento sobre el conocimiento. MIT Press, Cambridge, Mass., 1995. [11] P. Thagard.\nMIT Press, Cambridge, Massachusetts, 1995. [11] P. Thagard. Resolución de problemas adversariales: Modelado de un oponente utilizando coherencia explicativa. Ciencia Cognitiva, 16(1):123-149, 1992. [12] R. W. Toseland y R. F. Rivas. Una introducción a la práctica del trabajo en grupo. Prentice Hall, Englewood Cliffs, NJ, segunda edición, 1995. [13] S. Willmott, J. Richardson, A. Bundy y J. Levine. Un enfoque de planificación adversarial para el juego de Go. Notas de conferencia en Ciencias de la Computación, 1558:93-112, 1999. El Sexto Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 557",
    "original_sentences": [
        "An Adversarial Environment Model for Bounded Rational Agents in Zero-Sum Interactions Inon Zuckerman1 , Sarit Kraus1 , Jeffrey S. Rosenschein2 , Gal Kaminka1 1 Department of Computer Science 2 The School of Engineering Bar-Ilan University and Computer Science Ramat-Gan, Israel Hebrew University, Jerusalem, Israel {zukermi,sarit,galk}@cs.biu.ac.il jeff@cs.huji.ac.il ABSTRACT Multiagent environments are often not cooperative nor collaborative; in many cases, agents have conflicting interests, leading to adversarial interactions.",
        "This paper presents a formal Adversarial Environment model for bounded rational agents operating in a zero-sum environment.",
        "In such environments, attempts to use classical utility-based search methods can raise a variety of difficulties (e.g., implicitly modeling the opponent as an omniscient utility maximizer, rather than leveraging a more nuanced, explicit opponent model).",
        "We define an Adversarial Environment by describing the mental states of an agent in such an environment.",
        "We then present behavioral axioms that are intended to serve as design principles for building such adversarial agents.",
        "We explore the application of our approach by analyzing log files of completed Connect-Four games, and present an empirical analysis of the axioms appropriateness.",
        "Categories and Subject Descriptors I.2.11 [Artificial Intelligence]: Distributed Artificial Intelligence-Intelligent agents,Multiagent Systems; I.2.4 [Artificial Intelligence]: Knowledge Representation Formalisms and Methods -Modal logic General Terms Design, Theory 1.",
        "INTRODUCTION Early research in multiagent systems (MAS) considered cooperative groups of agents; because individual agents had limited resources, or limited access to information (e.g., limited processing power, limited sensor coverage), they worked together by design to solve problems that individually they could not solve, or at least could not solve as efficiently.",
        "MAS research, however, soon began to consider interacting agents with individuated interests, as representatives of different humans or organizations with non-identical interests.",
        "When interactions are guided by diverse interests, participants may have to overcome disagreements, uncooperative interactions, and even intentional attempts to damage one another.",
        "When these types of interactions occur, environments require appropriate behavior from the agents situated in them.",
        "We call these environments Adversarial Environments, and call the clashing agents Adversaries.",
        "Models of cooperation and teamwork have been extensively explored in MAS through the axiomatization of mental states (e.g., [8, 4, 5]).",
        "However, none of this research dealt with adversarial domains and their implications for agent behavior.",
        "Our paper addresses this issue by providing a formal, axiomatized mental state model for a subset of adversarial domains, namely simple zero-sum adversarial environments.",
        "Simple zero-sum encounters exist of course in various twoplayer games (e.g., Chess, Checkers), but they also exist in n-player games (e.g., Risk, Diplomacy), auctions for a single good, and elsewhere.",
        "In these latter environments especially, using a utility-based adversarial search (such as the Min-Max algorithm) does not always provide an adequate solution; the payoff function might be quite complex or difficult to quantify, and there are natural computational limitations on bounded rational agents.",
        "In addition, traditional search methods (like Min-Max) do not make use of a model of the opponent, which has proven to be a valuable addition to adversarial planning [9, 3, 11].",
        "In this paper, we develop a formal, axiomatized model for bounded rational agents that are situated in a zero-sum adversarial environment.",
        "The model uses different modality operators, and its main foundations are the SharedPlans [4] model for collaborative behavior.",
        "We explore environment properties and the mental states of agents to derive behavioral axioms; these behavioral axioms constitute a formal model that serves as a specification and design guideline for agent design in such settings.",
        "We then investigate the behavior of our model empirically using the Connect-Four board game.",
        "We show that this game conforms to our environment definition, and analyze players behavior using a large set of completed match log 550 978-81-904262-7-5 (RPS) c 2007 IFAAMAS files.",
        "In addition, we use the results presented in [9] to discuss the importance of opponent modeling in our ConnectFour adversarial domain.",
        "The paper proceeds as follows.",
        "Section 2 presents the models formalization.",
        "Section 3 presents the empirical analysis and its results.",
        "We discuss related work in Section 4, and conclude and present future directions in Section 5. 2.",
        "ADVERSARIAL ENVIRONMENTS The adversarial environment model (denoted as AE) is intended to guide the design of agents by providing a specification of the capabilities and mental attitudes of an agent in an adversarial environment.",
        "We focus here on specific types of adversarial environments, specified as follows: 1.",
        "Zero-Sum Interactions: positive and negative utilities of all agents sum to zero; 2.",
        "Simple AEs: all agents in the environment are adversarial agents; 3.",
        "Bilateral AEs: AEs with exactly two agents; 4.",
        "Multilateral AEs: AEs of three or more agents.",
        "We will work on both bilateral and multilateral instantiations of zero-sum and simple environments.",
        "In particular, our adversarial environment model will deal with interactions that consist of N agents (N ≥ 2), where all agents are adversaries, and only one agent can succeed.",
        "Examples of such environments range from board games (e.g., Chess, Connect-Four, and Diplomacy) to certain economic environments (e.g., N-bidder auctions over a single good). 2.1 Model Overview Our approach is to formalize the mental attitudes and behaviors of a single adversarial agent; we consider how a single agent perceives the AE.",
        "The following list specifies the conditions and mental states of an agent in a simple, zero-sum AE: 1.",
        "The agent has an individual intention that its own goal will be completed; 2.",
        "The agent has an individual belief that it and its adversaries are pursuing full conflicting goals (defined below)there can be only one winner; 3.",
        "The agent has an individual belief that each adversary has an intention to complete its own full conflicting goal; 4.",
        "The agent has an individual belief in the (partial) profile of its adversaries.",
        "Item 3 is required, since it might be the case that some agent has a full conflicting goal, and is currently considering adopting the intention to complete it, but is, as of yet, not committed to achieving it.",
        "This might occur because the agent has not yet deliberated about the effects that adopting that intention might have on the other intentions it is currently holding.",
        "In such cases, it might not consider itself to even be in an adversarial environment.",
        "Item 4 states that the agent should hold some belief about the profiles of its adversaries.",
        "The profile represents all the knowledge the agent has about its adversary: its weaknesses, strategic capabilities, goals, intentions, trustworthiness, and more.",
        "It can be given explicitly or can be learned from observations of past encounters. 2.2 Model Definitions for Mental States We use Grosz and Krauss definitions of the modal operators, predicates, and meta-predicates, as defined in their SharedPlan formalization [4].",
        "We recall here some of the predicates and operators that are used in that formalization: Int.To(Ai, α, Tn, Tα, C) represents Ais intentions at time Tn to do an action α at time Tα in the context of C. Int.Th(Ai, prop, Tn, Tprop, C) represents Ais intentions at time Tn that a certain proposition prop holds at time Tprop in the context of C. The potential intention operators, Pot.Int.To(...) and Pot.Int.Th(...), are used to represent the mental state when an agent considers adopting an intention, but has not deliberated about the interaction of the other intentions it holds.",
        "The operator Bel(Ai, f, Tf ) represents agent Ai believing in the statement expressed in formula f, at time Tf .",
        "MB(A, f, Tf ) represents mutual belief for a group of agents A.",
        "A snapshot of the system finds our environment to be in some state e ∈ E of environmental variable states, and each adversary in any LAi ∈ L of possible local states.",
        "At any given time step, the system will be in some world w of the set of all possible worlds w ∈ W, where w = E×LA1 ×LA2 × ...LAn , and n is the number of adversaries.",
        "For example, in a Texas Holdem poker game, an agents local state might be its own set of cards (which is unknown to its adversary) while the environment will consist of the betting pot and the community cards (which are visible to both players).",
        "A utility function under this formalization is defined as a mapping from a possible world w ∈ W to an element in , which expresses the desirability of the world, from a single agent perspective.",
        "We usually normalize the range to [0,1], where 0 represents the least desirable possible world, and 1 is the most desirable world.",
        "The implementation of the utility function is dependent on the domain in question.",
        "The following list specifies new predicates, functions, variables, and constants used in conjunction with the original definitions for the adversarial environment formalization: 1. φ is a null action (the agent does not do anything). 2.",
        "GAi is the set of agent Ais goals.",
        "Each goal is a set of predicates whose satisfaction makes the goal complete (we use G∗ Ai ∈ GAi to represent an arbitrary goal of agent Ai). 3. gAi is the set of agent Ais subgoals.",
        "Subgoals are predicates whose satisfaction represents an important milestone toward achievement of the full goal. gG∗ Ai ⊆ gAi is the set of subgoals that are important to the completion of goal G∗ Ai (we will use g∗ G∗ Ai ∈ gG∗ Ai to represent an arbitrary subgoal). 4.",
        "P Aj Ai is the profile object agent Ai holds about agent Aj. 5.",
        "CA is a general set of actions for all agents in A which are derived from the environments constraints.",
        "CAi ⊆ CA is the set of agent Ais possible actions. 6.",
        "Do(Ai, α, Tα, w) holds when Ai performs action α over time interval Tα in world w. 7.",
        "Achieve(G∗ Ai , α, w) is true when goal G∗ Ai is achieved following the completion of action α in world w ∈ W, where α ∈ CAi . 8.",
        "Profile(Ai, PAi Ai ) is true when agent Ai holds an object profile for agent Aj.",
        "Definition 1.",
        "Full conflict (FulConf ) describes a zerosum interaction where only a single goal of the goals in conflict can be completed.",
        "FulConf(G∗ Ai , G∗ Aj ) ⇒ (∃α ∈ CAi , ∀w, β ∈ CAj ) (Achieve(G∗ Ai , α, w) ⇒ ¬Achieve(G∗ Aj , β, w)) ∨ (∃β ∈ CAj , ∀w, α ∈ CAi )(Achieve(G∗ Aj , β, w) ⇒ ¬Achieve(G∗ Ai , α, w)) Definition 2.",
        "Adversarial Knowledge (AdvKnow) is a function returning a value which represents the amount of The Sixth Intl.",
        "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 551 knowledge agent Ai has on the profile of agent Aj, at time Tn.",
        "The higher the value, the more knowledge agent Ai has.",
        "AdvKnow : P Aj Ai × Tn → Definition 3.",
        "Eval - This evaluation function returns an estimated expected utility value for an agent in A, after completing an action from CA in some world state w. Eval : A × CA × w → Definition 4.",
        "TrH - (Threshold) is a numerical constant in the [0,1] range that represents an evaluation function (Eval) threshold value.",
        "An action that yields an estimated utility evaluation above the TrH is regarded as a highly beneficial action.",
        "The Eval value is an estimation and not the real utility function, which is usually unknown.",
        "Using the real utility value for a rational agent would easily yield the best outcome for that agent.",
        "However, agents usually do not have the real utility functions, but rather a heuristic estimate of it.",
        "There are two important properties that should hold for the evaluation function: Property 1.",
        "The evaluation function should state that the most desirable world state is one in which the goal is achieved.",
        "Therefore, after the goal has been satisfied, there can be no future action that can put the agent in a world state with higher Eval value. (∀Ai, G∗ Ai , α, β ∈ CAi , w ∈ W) Achieve(G∗ Ai , α, w) ⇒ Eval(Ai, α, w) ≥ Eval(Ai, β, w) Property 2.",
        "The evaluation function should project an action that causes a completion of a goal or a subgoal to a value which is greater than TrH (a highly beneficial action). (∀Ai, G∗ Ai ∈ GAi , α ∈ CAi , w ∈ W, g∗ GAi ∈ gGAi ) Achieve(G∗ Ai , α, w) ∨ Achieve(g∗ GAi , α, w) ⇒ Eval(Ai, α, w) ≥ TrH.",
        "Definition 5.",
        "SetAction We define a set action (SetAction) as a set of action operations (either complex or basic actions) from some action sets CAi and CAj which, according to agent Ais belief, are attached together by a temporal and consequential relationship, forming a chain of events (action, and its following consequent action). (∀α1 , . . . , αu ∈ CAi , β1 , . . . , βv ∈ CAj , w ∈ W) SetAction(α1 , . . . , αu , β1 , . . . , βv , w) ⇒ ((Do(Ai, α1 , Tα1 , w) ⇒ Do(Aj, β1 , Tβ1 , w)) ⇒ Do(Ai, α2 , Tα2 , w) ⇒ . . . ⇒ Do(Ai, αu , Tαu , w)) The consequential relation might exist due to various environmental constraints (when one action forces the adversary to respond with a specific action) or due to the agents knowledge about the profile of its adversary.",
        "Property 3.",
        "As the knowledge we have about our adversary increases we will have additional beliefs about its behavior in different situations which in turn creates new set actions.",
        "Formally, if our AdvKnow at time Tn+1 is greater than AdvKnow at time Tn, then every SetAction known at time Tn is also known at time Tn+1.",
        "AdvKnow(P Aj Ai , Tn+1) > AdvKnow(P Aj Ai , Tn) ⇒ (∀α1 , . . . , αu ∈ CAi , β1 , . . . , βv ∈ CAj ) Bel(Aag, SetAction(α1 , . . . , αu , β1 , . . . , βv ), Tn) ⇒ Bel(Aag, SetAction(α1 , . . . , αu , β1 , . . . , βv ), Tn+1) 2.3 The Environment Formulation The following axioms provide the formal definition for a simple, zero-sum Adversarial Environment (AE).",
        "Satisfaction of these axioms means that the agent is situated in such an environment.",
        "It provides specifications for agent Aag to interact with its set of adversaries A with respect to goals G∗ Aag and G∗ A at time TCo at some world state w. AE(Aag, A, G∗ Aag , A1, . . . , Ak, G∗ A1 , . . . , G∗ Ak , Tn, w) 1.",
        "Aag has an Int.Th his goal would be completed: (∃α ∈ CAag , Tα) Int.Th(Aag, Achieve(G∗ Aag , α), Tn, Tα, AE) 2.",
        "Aag believes that it and each of its adversaries Ao are pursuing full conflicting goals: (∀Ao ∈ {A1, . . . , Ak}) Bel(Aag, FulConf(G∗ Aag , G∗ Ao ), Tn) 3.",
        "Aag believes that each of his adversaries in Ao has the Int.Th his conflicting goal G∗ Aoi will be completed: (∀Ao ∈ {A1, . . . , Ak})(∃β ∈ CAo , Tβ) Bel(Aag, Int.Th(Ao, Achieve(G∗ Ao , β), TCo, Tβ, AE), Tn) 4.",
        "Aag has beliefs about the (partial) profiles of its adversaries (∀Ao ∈ {A1, . . . , Ak}) (∃PAo Aag ∈ PAag )Bel(Aag, Profile(Ao, PAo Aag ), Tn) To build an agent that will be able to operate successfully within such an AE, we must specify behavioral guidelines for its interactions.",
        "Using a naive Eval maximization strategy to a certain search depth will not always yield satisfactory results for several reasons: (1) the search horizon problem when searching for a fixed depth; (2) the strong assumption of an optimally rational, unbounded resources adversary; (3) using an estimated evaluation function which will not give optimal results in all world states, and can be exploited [9].",
        "The following axioms specify the behavioral principles that can be used to differentiate between successful and less successful agents in the above Adversarial Environment.",
        "Those axioms should be used as specification principles when designing and implementing agents that should be able to perform well in such Adversarial Environments.",
        "The behavioral axioms represent situations in which the agent will adopt potential intentions to (Pot.Int.To(...)) perform an action, which will typically require some means-end reasoning to select a possible course of action.",
        "This reasoning will lead to the adoption of an Int.To(...) (see [4]).",
        "A1.",
        "Goal Achieving Axiom.",
        "The first axiom is the simplest case; when the agent Aag believes that it is one action (α) away from achieving his conflicting goal G∗ Aag , it should adopt the potential intention to do α and complete its goal. (∀Aag, α ∈ CAag , Tn, Tα, w ∈ W) (Bel(Aag, Do(Aag, α, Tα, w) ⇒ Achieve(G∗ Aag , α, w)) ⇒ Pot.Int.To(Aag, α, Tn, Tα, w) This somewhat trivial behavior is the first and strongest axiom.",
        "In any situation, when the agent is an action away from completing the goal, it should complete the action.",
        "Any fair Eval function would naturally classify α as the maximal value action (property 1).",
        "However, without explicit axiomatization of such behavior there might be situations where the agent will decide on taking another action for various reasons, due to its bounded decision resources.",
        "A2.",
        "Preventive Act Axiom.",
        "Being in an adversarial situation, agent Aag might decide to take actions that will damage one of its adversarys plans to complete its goal, even if those actions do not explicitly advance Aag towards its conflicting goal G∗ Aag .",
        "Such preventive action will take place when agent Aag has a belief about the possibility of its adversary Ao doing an action β that will give it a high 552 The Sixth Intl.",
        "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) utility evaluation value (> TrH).",
        "Believing that taking action α will prevent the opponent from doing its β, it will adopt a potential intention to do α. (∀Aag, Ao ∈ A, α ∈ CAag , β ∈ CAo , Tn, Tβ, w ∈ W) (Bel(Aag, Do(Ao, β, Tβ, w) ∧ Eval(Ao, β, w) > TrH, Tn) ∧ Bel(Aag, Do(Aag, α, Tα, w) ⇒ ¬Do(Ao, β, Tβ, w), Tn) ⇒ Pot.Int.To(Aag, α, Tn, Tα, w) This axiom is a basic component of any adversarial environment.",
        "For example, looking at a Chess board game, a player could realize that it is about to be checkmated by its opponent, thus making a preventive move.",
        "Another example is a Connect Four game: when a player has a row of three chips, its opponent must block it, or lose.",
        "A specific instance of A1 occurs when the adversary is one action away from achieving its goal, and immediate preventive action needs to be taken by the agent.",
        "Formally, we have the same beliefs as stated above, with a changed belief that doing action β will cause agent Ao to achieve its goal.",
        "Proposition 1: Prevent or lose case. (∀Aag, Ao ∈ A, α ∈ CAag , β ∈ CAo , G∗ Ao , Tn, Tα, Tβ, w ∈ W) Bel(Aag, Do(Ao, β, Tβ, w) ⇒ Achieve(G∗ Ao , β, w), Tn) ∧ Bel(Aag, Do(Aag, α, Tα, w) ⇒ ¬Do(Ao, β, Tβ, w)) ⇒ Pot.Int.To(Aag, α, Tn, Tα, w) Sketch of proof: Proposition 1 can be easily derived from axiom A1 and the property 2 of the Eval function, which states that any action that causes a completion of a goal is a highly beneficial action.",
        "The preventive act behavior will occur implicitly when the Eval function is equal to the real world utility function.",
        "However, being bounded rational agents and dealing with an estimated evaluation function we need to explicitly axiomatize such behavior, for it will not always occur implicitly from the evaluation function.",
        "A3.",
        "Suboptimal Tactical Move Axiom.",
        "In many scenarios a situation may occur where an agent will decide not to take the current most beneficial action it can take (the action with the maximal utility evaluation value), because it believes that taking another action (with lower utility evaluation value) might yield (depending on the adversarys response) a future possibility for a highly beneficial action.",
        "This will occur most often when the Eval function is inaccurate and differs by a large extent from the Utility function.",
        "Put formally, agent Aag believes in a certain SetAction that will evolve according to its initial action and will yield a high beneficial value (> TrH) solely for it. (∀Aag, Ao ∈ A, Tn, w ∈ W) (∃α1 , . . . , αu ∈ CAi , β1 , . . . , βv ∈ CAj , Tα1 ) Bel(Aag, SetAction(α1 , . . . , αu , β1 , . . . , βv ), Tn) ∧ Bel(Aag, Eval(Ao, βv , w) < TrH < Eval(Aag, αu , w), Tn) ⇒ Pot.Int.To(Aag, α1 , Tn, Tα1 , w) An agent might believe that a chain of events will occur for various reasons due to the inevitable nature of the domain.",
        "For example, in Chess, we often observe the following: a move causes a check position, which in turn limits the opponents moves to avoiding the check, to which the first player might react with another check, and so on.",
        "The agent might also believe in a chain of events based on its knowledge of its adversarys profile, which allows it to foresee the adversarys movements with high accuracy.",
        "A4.",
        "Profile Detection Axiom.",
        "The agent can adjust its adversarys profiles by observations and pattern study (specifically, if there are repeated encounters with the same adversary).",
        "However, instead of waiting for profile information to be revealed, an agent can also initiate actions that will force its adversary to react in a way that will reveal profile knowledge about it.",
        "Formally, the axiom states that if all actions (γ) are not highly beneficial actions (< TrH), the agent can do action α in time Tα if it believes that it will result in a non-highly beneficial action β from its adversary, which in turn teaches it about the adversarys profile, i.e., gives a higher AdvKnow(P Aj Ai , Tβ). (∀Aag, Ao ∈ A, α ∈ CAag , β ∈ CAo , Tn, Tα, Tβ, w ∈ W) Bel(Aag, (∀γ ∈ CAag )Eval(Aag, γ, w) < TrH, Tn) ∧ Bel(Aag, Do(Aag, α, Tα, w) ⇒ Do(Ao, β, Tβ, w), Tn) ∧ Bel(Aag, Eval(Ao, β, w) < TrH) ∧ Bel(Aag, AdvKnow(P Aj Ai , Tβ) > AdvKnow(P Aj Ai , Tn), Tn) ⇒ Pot.Int.To(Aag, α, Tn, Tα, w) For example, going back to the Chess board game scenario, consider starting a game versus an opponent about whom we know nothing, not even if it is a human or a computerized opponent.",
        "We might start playing a strategy that will be suitable versus an average opponent, and adjust our game according to its level of play.",
        "A5.",
        "Alliance Formation Axiom The following behavioral axiom is relevant only in a multilateral instantiation of the adversarial environment (obviously, an alliance cannot be formed in a bilateral, zero-sum encounter).",
        "In different situations during a multilateral interaction, a group of agents might believe that it is in their best interests to form a temporary alliance.",
        "Such an alliance is an agreement that constrains its members behavior, but is believed by its members to enable them to achieve a higher utility value than the one achievable outside of the alliance.",
        "As an example, we can look at the classical Risk board game, where each player has an individual goal of being the sole conquerer of the world, a zero-sum game.",
        "However, in order to achieve this goal, it might be strategically wise to make short-term ceasefire agreements with other players, or to join forces and attack an opponent who is stronger than the rest.",
        "An alliances terms defines the way its members should act.",
        "It is a set of predicates, denoted as Terms, that is agreed upon by the alliance members, and should remain true for the duration of the alliance.",
        "For example, the set Terms in the Risk scenario, could contain the following predicates: 1.",
        "Alliance members will not attack each other on territories X, Y and Z; 2.",
        "Alliance members will contribute C units per turn for attacking adversary Ao; 3.",
        "Members are obligated to stay as part of the alliance until time Tk or until adversarys Ao army is smaller than Q.",
        "The set Terms specifies inter-group constraints on each of the alliance members (∀Aal i ∈ Aal ⊆ A) set of actions Cal i ⊆ C. Definition 6.",
        "Al val - the total evaluation value that agent Ai will achieve while being part of Aal is the sum of Evali (Eval for Ai) of each of Aal j Eval values after taking their own α actions (via the agent(α) predicate): Al val(Ai, Cal , Aal , w) = α∈Cal Evali(Aal j , agent(α), w) Definition 7.",
        "Al TrH - is a number representing an Al val The Sixth Intl.",
        "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 553 threshold; above it, the alliance can be said to be a highly beneficial alliance.",
        "The value of Al TrH will be calculated dynamically according to the progress of the interaction, as can be seen in [7].",
        "After an alliance is formed, its members are now working in their normal adversarial environment, as well as according to the mental states and axioms required for their interactions as part of the alliance.",
        "The following Alliance model (AL) specifies the conditions under which the group Aal can be said to be in an alliance and working with a new and constrained set of actions Cal , at time Tn.",
        "AL(Aal , Cal , w, Tn) 1.",
        "Aal has a MB that all members are part of Aal : MB(Aal , (∀Aal i ∈ Aal )member(Aal i , Aal ), Tn) 2.",
        "Aal has a MB that the group be maintained: MB(Aal , (∀Aal i ∈ Aal )Int.Th (Ai, member(Ai, Aal ), Tn, Tn+1, Co), Tn) 3.",
        "Aal has a MB that being members gives them high utility value: MB(Aal , (∀Aal i ∈ Aal )Al val(Aal i , Cal , Aal , w) ≥ Al TrH, Tn) Members profiles are a crucial part of successful alliances.",
        "We assume that agents that have more accurate profiles of their adversaries will be more successful in such environments.",
        "Such agents will be able to predict when a member is about to breach the alliances contract (item 2 in the above model), and take counter measures (when item 3 will falsify).",
        "The robustness of the alliance is in part a function of its members trustfulness measure, objective position estimation, and other profile properties.",
        "We should note that an agent can simultaneously be part of more than one alliance.",
        "Such a temporary alliance, where the group members do not have a joint goal but act collaboratively for the interest of their own individual goals, is classified as a Treatment Group by modern psychologists [12] (in contrast to a Task Group, where its members have a joint goal).",
        "The Shared Activity model as presented in [5] modeled Treatment Group behavior using the same SharedPlans formalization.",
        "When comparing both definitions of an alliance and a Treatment Group we found an unsurprising resemblance between both models: the environment models definitions are almost identical (see SAs definitions in [5]), and their Selfish-Act and Cooperative Act axioms conform to our adversarial agents behavior.",
        "The main distinction between both models is the integration of a Helpful-behavior act axiom, in the Shared Activity which cannot be part of ours.",
        "This axiom states that an agent will consider taking action that will lower its Eval value (to a certain lower bound), if it believes that a group partner will gain a significant benefit.",
        "Such behavior cannot occur in a pure adversarial environment (as a zero-sum game is), where the alliance members are constantly on watch to manipulate their alliance to their own advantage.",
        "A6.",
        "Evaluation Maximization Axiom.",
        "In a case when all other axioms are inapplicable, we will proceed with the action that maximizes the heuristic value as computed in the Eval function. (∀Aag, Ao ∈ A, α ∈ Cag, Tn, w ∈ W) Bel(Aag, (∀γ ∈ Cag)Eval(Aag, α, w) ≥ Eval(Aag, γ, w), Tn) ⇒ Pot.Int.To(Aag, α, Tn, Tα, w) T1.",
        "Optimality on Eval = Utility The above axiomatic model handles situations where the Utility is unknown and the agents are bounded rational agents.",
        "The following theorem shows that in bilateral interactions, where the agents have the real Utility function (i.e., Eval = Utility) and are rational agents, the axioms provide the same optimal result as classic adversarial search (e.g., Min-Max).",
        "Theorem 1.",
        "Let Ae ag be an unbounded rational AE agent using the Eval heuristic evaluation function, Au ag be the same agent using the true Utility function, and Ao be a sole unbounded utility-based rational adversary.",
        "Given that Eval = Utility: (∀α ∈ CAu ag , α ∈ CAe ag , Tn, w ∈ W) Pot.Int.To(Au ag, α, Tn, Tα, w) → Pot.Int.To(Ae ag, α , Tn, Tα, w) ∧ ((α = α ) ∨ (Utility(Au ag, α, w) = Eval(Ae ag, α , w))) Sketch of proof - Given that Au ag has the real utility function and unbounded resources, it can generate the full game tree and run the optimal MinMax algorithm to choose the highest utility value action, which we denote by, α.",
        "The proof will show that Ae ag, using the AE axioms, will select the same or equal utility α (when there is more than one action with the same max utility) when Eval = Utility. (A1) Goal achieving axiom - suppose there is an α such that its completion will achieve Au ags goal.",
        "It will obtain the highest utility by Min-Max for Au ag.",
        "The Ae ag agent will select α or another action with the same utility value via A1.",
        "If such α does not exist, Ae ag cannot apply this axiom, and proceeds to A2. (A2) Preventive act axiom - (1) Looking at the basic case (see Prop1), if there is a β which leads Ao to achieve its goal, then a preventive action α will yield the highest utility for Au ag.",
        "Au ag will choose it through the utility, while Ae ag will choose it through A2. (2) In the general case, β is a highly beneficial action for Ao, thus yields low utility for Au ag, which will guide it to select an α that will prevent β, while Ae ag will choose it through A2.1 If such β does not exist for Ao, then A2 is not applicable, and Ae ag can proceed to A3. (A3) Suboptimal tactical move axiom - When using a heuristic Eval function, Ae ag has a partial belief in the profile of its adversary (item 4 in AE model), which may lead it to believe in SetActions (Prop1).",
        "In our case, Ae ag is holding a full profile on its optimal adversary and knows that Ao will behave optimally according to the real utility values on the complete search tree, therefore, any belief about suboptimal SetAction cannot exist, yielding this axiom inapplicable.",
        "Ae ag will proceed to A4. (A4) Profile detection axiom - Given that Ae ag has the full profile of Ao, none of Ae ags actions can increase its knowledge.",
        "That axiom will not be applied, and the agent will proceed with A6 (A5 will be disregarded because the interaction is bilateral). (A6) Evaluation maximization axiom - This axiom will select the max Eval for Ae ag.",
        "Given that Eval = Utility, the same α that was selected by Au ag will be selected. 3.",
        "EVALUATION The main purpose of our experimental analysis is to evaluate the models behavior and performance in a real adversarial environment.",
        "This section investigates whether bounded 1 A case where following the completion of β there exists a γ which gives high utility for Agent Au ag, cannot occur because Ao uses the same utility, and γs existence will cause it to classify β as a low utility action. 554 The Sixth Intl.",
        "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) rational agents situated in such adversarial environments will be better off applying our suggested behavioral axioms. 3.1 The Domain To explore the use of the above model and its behavioral axioms, we decided to use the Connect-Four game as our adversarial environment.",
        "Connect-Four is a 2-player, zerosum game which is played using a 6x7 matrix-like board.",
        "Each turn, a player drops a disc into one of the 7 columns (the set of 21 discs is usually colored yellow for player 1 and red for player 2; we will use White and Black respectively to avoid confusion).",
        "The winner is the first player to complete a horizontal, vertical, or diagonal set of four discs with its color.",
        "On very rare occasions, the game might end in a tie if all the empty grids are filled, but no player managed to create a 4-disc set.",
        "The Connect-Four game was solved in [1], where it is shown that the first player (playing with the white discs) can force a win by starting in the middle column (column 4) and playing optimally However, the optimal strategy is very complex, and difficult to follow even for complex bounded rational agents, such as human players.",
        "Before we can proceed checking agent behavior, we must first verify that the domain conforms to the adversarial environments definition as given above (which the behavioral axioms are based on).",
        "First, when playing a Connect-Four game, the agent has an intention to win the game (item 1).",
        "Second (item 2), our agent believes that in Connect-Four there can only be one winner (or no winner at all in the rare occurrence of a tie).",
        "In addition, our agent believes that its opponent to the game will try to win (item 3), and we hope it has some partial knowledge (item 4) about its adversary (this knowledge can vary from nothing, through simple facts such as age, to strategies and weaknesses).",
        "Of course, not all Connect-Four encounters are adversarial.",
        "For example, when a parent is playing the game with its child, the following situation might occur: the child, having a strong incentive to win, treats the environment as adversarial (it intends to win, understands that there can only be one winner, and believes that its parent is trying to beat him).",
        "However, the parents point of view might see the environment as an educational one, where its goal is not to win the game, but to cause enjoyment or practice strategic reasoning.",
        "In such an educational environment, a new set of behavioral axioms might be more beneficial to the parents goals than our suggested adversarial behavioral axioms. 3.2 Axiom Analysis After showing that the Connect-Four game is indeed a zero-sum, bilateral adversarial environment, the next step is to look at players behaviors during the game and check whether behaving according to our model does improve performance.",
        "To do so we have collected log files from completed Connect-Four games that were played by human players over the Internet.",
        "Our collected log file data came from Play by eMail (PBeM) sites.",
        "These are web sites that host email games, where each move is taken by an email exchange between the server and the players.",
        "Many such sites archives contain real competitive interactions, and also maintain a ranking system for their members.",
        "Most of the data we used can be found in [6].",
        "As can be learned from [1], Connect-Four has an optimal strategy and a considerable advantage for the player who starts the game (which we call the White player).",
        "We will concentrate in our analysis on the second players moves (to be called Black).",
        "The White player, being the first to act, has the so-called initiative advantage.",
        "Having the advantage and a good strategy will keep the Black player busy reacting to its moves, instead of initiating threats.",
        "A threat is a combination of three discs of the same color, with an empty spot for the fourth winning disk.",
        "An open threat is a threat that can be realized in the opponents next move.",
        "In order for the Black player to win, it must somehow turn the tide, take the advantage and start presenting threats to the White player.",
        "We will explore Black players behavior and their conformance to our axioms.",
        "To do so, we built an application that reads log files and analyzes the Black players moves.",
        "The application contains two main components: (1) a Min-Max algorithm for evaluation of moves; (2) open threats detector for the discovering of open threats.",
        "The Min-Max algorithm will work to a given depth, d and for each move α will output the heuristic value for the next action taken by the player as written in the log file, h(α), alongside the maximum heuristic value, maxh(α), that could be achieved prior to taking the move (obviously, if h(α) = maxh(α), then the player did not do the optimal move heuristically).",
        "The threat detectors job is to notify if some action was taken in order to block an open threat (not blocking an open threat will probably cause the player to lose in the opponents next move).",
        "The heuristic function used by Min-Max to evaluate the players utility is the following function, which is simple to compute, yet provides a reasonable challenge to human opponents: Definition 8.",
        "Let Group be an adjacent set of four squares that are horizontal, vertical, or diagonal.",
        "Groupn b (Groupn w) be a Group with n pieces of the black (white) color and 4−n empty squares. h = ((Group1 b ∗α)+(Group2 b ∗β)+(Group3 b ∗γ)+(Group4 b ∗∞)) − ((Group1 w ∗α)+(Group2 w ∗β)+(Group3 w ∗γ)+(Group4 w ∗∞)) The values of α, β and δ can vary to form any desired linear combination; however, it is important to value them with the α < β < δ ordering in mind (we used 1, 4, and 8 as their respective values).",
        "Groups of 4 discs of the same color means victory, thus discovery of such a group will result in ∞ to ensure an extreme value.",
        "We now use our estimated evaluation function to evaluate the Black players actions during the Connect-Four adversarial interaction.",
        "Each game from the log file was input into the application, which processed and output a reformatted log file containing the h value of the current move, the maxh value that could be achieved, and a notification if an open threat was detected.",
        "A total of 123 games were analyzed (57 with White winning, and 66 with Black winning).",
        "A few additional games were manually ignored in the experiment, due to these problems: a player abandoning the game while the outcome is not final, or a blunt irrational move in the early stages of the game (e.g., not blocking an obvious winning group in the first opening moves).",
        "In addition, a single tie game was also removed.",
        "The simulator was run to a search depth of 3 moves.",
        "We now proceed to analyze the games with respect to each behavioral axiom.",
        "The Sixth Intl.",
        "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 555 Table 1: Average heuristic difference analysis Black losses Black Won Avg minh -17.62 -12.02 Avg 3 lowest h moves (min3 h) -13.20 -8.70 3.2.1 Affirming the Suboptimal tactical move axiom The following section presents the heuristic evaluations of the Min-Max algorithm for each action, and checks the amount and extent of suboptimal tactical actions and their implications on performance.",
        "Table 1 shows results and insights from the games heuristic analysis, when search depth equals 3 (this search depth was selected for the results to be comparable to [9], see Section 3.2.3).",
        "The tables heuristic data is the difference between the present maximal heuristic value and the heuristic value of the action that was eventually taken by the player (i.e., the closer the number is to 0, the closer the action was to the maximum heuristic action).",
        "The first row presents the difference values of the action that had the maximal difference value among all the Black players actions in a given game, as averaged over all Blacks winning and losing games (see respective columns).",
        "In games in which the Black player loses, its average difference value was -17.62, while in games in which the Black player won, its average was -12.02.",
        "The second row expands the analysis by considering the 3 highest heuristic difference actions, and averaging them.",
        "In that case, we notice an average heuristic difference of 5 points between games which the Black player loses and games in which it wins.",
        "Nevertheless, the importance of those numbers is that they allowed us to take an educated guess on a threshold number of 11.5, as the value of the TrH constant, which differentiates between normal actions and highly beneficial ones.",
        "After finding an approximated TrH constant, we can proceed with an analysis of the importance of suboptimal moves.",
        "To do so we took the subset of games in which the minimum heuristic difference value for Blacks actions was 11.5.",
        "As presented in Table 2, we can see the different min3 h average of the 3 largest ranges and the respective percentage of games won.",
        "The first row shows that the Black player won only 12% of the games in which the average of its 3 highest heuristically difference actions (min3 h) was smaller than the suggested threshold, TrH = 11.5.",
        "The second row shows a surprising result: it seems that when min3 h > −4 the Black player rarely wins.",
        "Intuition would suggest that games in which the action evaluation values were closer to the maximal values will result in more winning games for Black.",
        "However, it seems that in the Connect-Four domain, merely responding with somewhat easily expected actions, without initiating a few surprising and suboptimal moves, does not yield good results.",
        "The last row sums up the main insights from the analysis; most of Blacks wins (83%) came when its min3 h was in the range of -11.5 to -4.",
        "A close inspection of those Black winning games shows the following pattern behind the numbers: after standard opening moves, Black suddenly drops a disc into an isolated column, which seems a waste of a move.",
        "White continues to build its threats, while usually disregarding Blacks last move, which in turn uses the isolated disc as an anchor for a future winning threat.",
        "The results show that it was beneficial for the Black player Table 2: Blacks winnings percentages % of games min3 h < −11.5 12% min3 h > −4 5% −11.5 ≤ min3 h ≤ −4 83% to take suboptimal actions and not give the current highest possible heuristic value, but will not be too harmful for its position (i.e., will not give high beneficial value to its adversary).",
        "As it turned out, learning the threshold is an important aspect of success: taking wildly risky moves (min3 h < −11.5) or trying to avoid them (min3 h > −4) reduces the Black players winning chances by a large margin. 3.2.2 Affirming the Profile Monitoring Axiom In the task of showing the importance of monitoring ones adversaries profiles, our log files could not be used because they did not contain repeated interactions between players, which are needed to infer the players knowledge about their adversaries.",
        "However, the importance of opponent modeling and its use in attaining tactical advantages was already studied in various domains ([3, 9] are good examples).",
        "In a recent paper, Markovitch and Reger [9] explored the notion of learning and exploitation of opponent weakness in competitive interactions.",
        "They apply simple learning strategies by analyzing examples from past interactions in a specific domain.",
        "They also used the Connect-Four adversarial domain, which can now be used to understand the importance of monitoring the adversarys profile.",
        "Following the presentation of their theoretical model, they describe an extensive empirical study and check the agents performance after learning the weakness model with past examples.",
        "One of the domains used as a competitive environment was the same Connect-Four game (Checkers was the second domain).",
        "Their heuristic function was identical to ours with three different variations (H1, H2, and H3) that are distinguished from one another in their linear combination coefficient values.",
        "The search depth for the players was 3 (as in our analysis).",
        "Their extensive experiments check and compare various learning strategies, risk factors, predefined feature sets and usage methods.",
        "The bottom line is that the Connect-Four domain shows an improvement from a 0.556 winning rate before modeling to a 0.69 after modeling (page 22).",
        "Their conclusions, showing improved performance when holding and using the adversarys model, justify the effort to monitor the adversary profile for continuous and repeated interactions.",
        "An additional point that came up in their experiments is the following: after the opponent weakness model has been learned, the authors describe different methods of integrating the opponent weakness model into the agents decision strategy.",
        "Nevertheless, regardless of the specific method they chose to work with, all integration methods might cause the agent to take suboptimal decisions; it might cause the agent to prefer actions that are suboptimal at the present decision junction, but which might cause the opponent to react in accordance with its weakness model (as represented by our agent) which in turn will be beneficial for us in the future.",
        "The agents behavior, as demonstrated in [9] further confirms and strengthens our Suboptimal Tactical Axiom as discussed in the previous section. 556 The Sixth Intl.",
        "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 3.2.3 Additional Insights The need for the Goal Achieving, Preventive Act, and Evaluation Maximization axioms are obvious, and need no further verification.",
        "However, even with respect to those axioms, a few interesting insights came up in the log analysis.",
        "The Goal achieving and Preventive Act axioms, though theoretically trivial, seem to provide some challenge to a human player.",
        "In the initial inspection of the logs, we encountered few games2 where a player, for inexplicable reasons, did not block the other from winning or failed to execute its own winning move.",
        "We can blame those faults on the humans lack of attention, or a typing error in its move reply; nevertheless, those errors might occur in bounded rational agents, and the appropriate behavior needs to be axiomatized.",
        "A typical Connect-Four game revolves around generating threats and blocking them.",
        "In our analysis we looked for explicit preventive actions, i.e., moves that block a group of 3 discs, or that remove a future threat (in our limited search horizon).",
        "We found that in 83% of the total games there was at least one preventive action taken by the Black player.",
        "It was also found that Black averaged 2.8 preventive actions per game on the games in which it lost, while averaging 1.5 preventive actions per game when winning.",
        "It seems that Black requires 1 or 2 preventive actions to build its initial taking position, before starting to present threats.",
        "If it did not manage to win, it will usually prevent an extra threat or two before succumbing to White. 4.",
        "RELATED WORK Much research deals with the axiomatization of teamwork and mental states of individuals: some models use knowledge and belief [10], others have models of goals and intentions [8, 4].",
        "However, all these formal theories deal with agent teamwork and cooperation.",
        "As far as we know, our model is the first to provide a formalized model for explicit adversarial environments and agents behavior in it.",
        "The classical Min-Max adversarial search algorithm was the first attempt to integrate the opponent into the search space with a weak assumption of an optimally playing opponent.",
        "Since then, much effort has gone into integrating the opponent model into the decision procedure to predict future behavior.",
        "The M∗ algorithm presented by Carmel and Markovitch [2] showed a method of incorporating opponent models into adversary search, while in [3] they used learning to provide a more accurate opponent model in a 2player repeated game environment, where agents strategies were modeled as finite automata.",
        "Additional Adversarial planning work was done by Willmott et al. [13], which provided an adversarial planning approach to the game of GO.",
        "The research mentioned above dealt with adversarial search and the integration of opponent models into classical utilitybased search methods.",
        "That work shows the importance of opponent modeling and the ability to exploit it to an agents advantage.",
        "However, the basic limitations of those search methods still apply; our model tries to overcome those limitations by presenting a formal model for a new, mental state-based adversarial specification. 5.",
        "CONCLUSIONS We presented an Adversarial Environment model for a 2 These were later removed from the final analysis. bounded rational agent that is situated in an N-player, zerosum environment.",
        "We used the SharedPlans formalization to define the model and the axioms that agents can apply as behavioral guidelines.",
        "The model is meant to be used as a guideline for designing agents that need to operate in such adversarial environments.",
        "We presented empirical results, based on ConnectFour log file analysis, that exemplify the model and the axioms for a bilateral instance of the environment.",
        "The results we presented are a first step towards an expanded model that will cover all types of adversarial environments, for example, environments that are non-zero-sum, and environments that contain natural agents that are not part of the direct conflict.",
        "Those challenges and more will be dealt with in future research. 6.",
        "ACKNOWLEDGMENT This research was supported in part by Israel Science Foundation grants #1211/04 and #898/05. 7.",
        "REFERENCES [1] L. V. Allis.",
        "A knowledge-based approach of Connect-Four - the game is solved: White wins.",
        "Masters thesis, Free University, Amsterdam, The Netherlands, 1988. [2] D. Carmel and S. Markovitch.",
        "Incorporating opponent models into adversary search.",
        "In Proceedings of the Thirteenth National Conference on Artificial Intelligence, pages 120-125, Portland, OR, 1996. [3] D. Carmel and S. Markovitch.",
        "Opponent modeling in multi-agent systems.",
        "In G. Weiß and S. Sen, editors, Adaptation and Learning in Multi-Agent Systems, pages 40-52.",
        "Springer-Verlag, 1996. [4] B. J. Grosz and S. Kraus.",
        "Collaborative plans for complex group action.",
        "Artificial Intelligence, 86(2):269-357, 1996. [5] M. Hadad, G. Kaminka, G. Armon, and S. Kraus.",
        "Supporting collaborative activity.",
        "In Proc. of AAAI-2005, pages 83-88, Pittsburgh, 2005. [6] http://www.gamerz.net/˜pbmserv/. [7] S. Kraus and D. Lehmann.",
        "Designing and building a negotiating automated agent.",
        "Computational Intelligence, 11:132-171, 1995. [8] H. J. Levesque, P. R. Cohen, and J. H. T. Nunes.",
        "On acting together.",
        "In Proc. of AAAI-90, pages 94-99, Boston, MA, 1990. [9] S. Markovitch and R. Reger.",
        "Learning and exploiting relative weaknesses of opponent agents.",
        "Autonomous Agents and Multi-Agent Systems, 10(2):103-130, 2005. [10] Y. M. Ronald Fagin, Joseph Y. Halpern and M. Y. Vardi.",
        "Reasoning about knowledge.",
        "MIT Press, Cambridge, Mass., 1995. [11] P. Thagard.",
        "Adversarial problem solving: Modeling an oponent using explanatory coherence.",
        "Cognitive Science, 16(1):123-149, 1992. [12] R. W. Toseland and R. F. Rivas.",
        "An Introduction to Group Work Practice.",
        "Prentice Hall, Englewood Cliffs, NJ, 2nd edition edition, 1995. [13] S. Willmott, J. Richardson, A. Bundy, and J. Levine.",
        "An adversarial planning approach to Go.",
        "Lecture Notes in Computer Science, 1558:93-112, 1999.",
        "The Sixth Intl.",
        "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 557"
    ],
    "translated_text_sentences": [
        "Un modelo de entorno adversarial para agentes racionales limitados en interacciones de suma cero Inon Zuckerman1, Sarit Kraus1, Jeffrey S. Rosenschein2, Gal Kaminka1 1 Departamento de Ciencias de la Computación 2 Escuela de Ingeniería Universidad Bar-Ilan y Ciencias de la Computación Ramat-Gan, Israel Universidad Hebrea, Jerusalén, Israel {zukermi, sarit, galk}@cs.biu.ac.il jeff@cs.huji.ac.il RESUMEN Los entornos multiagentes a menudo no son cooperativos ni colaborativos; en muchos casos, los agentes tienen intereses conflictivos, lo que lleva a interacciones adversariales.",
        "Este documento presenta un modelo formal de Entorno Adversarial para agentes racionales acotados que operan en un entorno de suma cero.",
        "En tales entornos, los intentos de utilizar métodos de búsqueda basados en utilidad clásica pueden plantear una variedad de dificultades (por ejemplo, modelar implícitamente al oponente como un maximizador de utilidad omnisciente, en lugar de aprovechar un modelo de oponente más matizado y explícito).",
        "Definimos un Entorno Adversario al describir los estados mentales de un agente en dicho entorno.",
        "Luego presentamos axiomas de comportamiento que están destinados a servir como principios de diseño para construir agentes adversarios.",
        "Exploramos la aplicación de nuestro enfoque mediante el análisis de archivos de registro de juegos de Conecta Cuatro completados, y presentamos un análisis empírico de la adecuación de los axiomas.",
        "Categorías y Descriptores de Asignaturas I.2.11 [Inteligencia Artificial]: Inteligencia Artificial Distribuida-Agentes Inteligentes, Sistemas Multiagente; I.2.4 [Inteligencia Artificial]: Formalismos y Métodos de Representación del Conocimiento - Lógica Modal Términos Generales Diseño, Teoría 1.",
        "Introducción: En las primeras investigaciones en sistemas multiagente (SMA) se consideraban grupos cooperativos de agentes; debido a que los agentes individuales tenían recursos limitados o acceso limitado a la información (por ejemplo, potencia de procesamiento limitada, cobertura sensorial limitada), trabajaban juntos por diseño para resolver problemas que individualmente no podían resolver, o al menos no podían resolver de manera eficiente.",
        "Sin embargo, la investigación de MAS pronto comenzó a considerar agentes interactivos con intereses individualizados, como representantes de diferentes humanos u organizaciones con intereses no idénticos.",
        "Cuando las interacciones son guiadas por intereses diversos, los participantes pueden tener que superar desacuerdos, interacciones no cooperativas e incluso intentos intencionales de dañarse mutuamente.",
        "Cuando ocurren este tipo de interacciones, los entornos requieren un comportamiento apropiado por parte de los agentes situados en ellos.",
        "Llamamos a estos entornos Entornos Adversarios, y llamamos a los agentes en conflicto Adversarios.",
        "Los modelos de cooperación y trabajo en equipo han sido ampliamente explorados en MAS a través de la axiomatización de estados mentales (por ejemplo, [8, 4, 5]).",
        "Sin embargo, ninguna de estas investigaciones abordó los dominios adversarios y sus implicaciones para el comportamiento de los agentes.",
        "Nuestro artículo aborda este problema al proporcionar un modelo formal y axiomatizado de estados mentales para un subconjunto de dominios adversariales, específicamente entornos adversariales simples de suma cero.",
        "Los encuentros de suma cero simples existen, por supuesto, en varios juegos de dos jugadores (por ejemplo, Ajedrez, Damas), pero también existen en juegos de n jugadores (por ejemplo, Risk, Diplomacia), subastas de un solo bien y en otros lugares.",
        "En estos entornos recientes en particular, el uso de una búsqueda adversarial basada en utilidad (como el algoritmo Min-Max) no siempre proporciona una solución adecuada; la función de recompensa podría ser bastante compleja o difícil de cuantificar, y existen limitaciones computacionales naturales en agentes racionales acotados.",
        "Además, los métodos de búsqueda tradicionales (como Min-Max) no hacen uso de un modelo del oponente, lo cual ha demostrado ser una adición valiosa a la planificación adversarial [9, 3, 11].",
        "En este artículo, desarrollamos un modelo formal y axiomatizado para agentes racionales limitados que se encuentran en un entorno adversarial de suma cero.",
        "El modelo utiliza diferentes operadores de modalidad, y sus principales fundamentos son el modelo SharedPlans [4] para el comportamiento colaborativo.",
        "Exploramos las propiedades del entorno y los estados mentales de los agentes para derivar axiomas de comportamiento; estos axiomas de comportamiento constituyen un modelo formal que sirve como especificación y guía de diseño para el diseño de agentes en tales entornos.",
        "Luego investigamos el comportamiento de nuestro modelo empíricamente utilizando el juego de mesa Conecta Cuatro.",
        "Mostramos que este juego se ajusta a nuestra definición de entorno, y analizamos el comportamiento de los jugadores utilizando un gran conjunto de registros de partidas completadas 550 978-81-904262-7-5 (RPS) c 2007 archivos de IFAAMAS.",
        "Además, utilizamos los resultados presentados en [9] para discutir la importancia del modelado del oponente en nuestro dominio adversarial de Conecta Cuatro.",
        "El artículo continúa de la siguiente manera.",
        "La sección 2 presenta la formalización de los modelos.",
        "La sección 3 presenta el análisis empírico y sus resultados.",
        "Discutimos el trabajo relacionado en la Sección 4, y concluimos y presentamos las direcciones futuras en la Sección 5.",
        "ENTORNOS ADVERSARIOS El modelo de entorno adversario (denominado como AE) está destinado a guiar el diseño de agentes al proporcionar una especificación de las capacidades y actitudes mentales de un agente en un entorno adversario.",
        "Nos enfocamos aquí en tipos específicos de entornos adversarios, especificados de la siguiente manera: 1.",
        "Interacciones de suma cero: las utilidades positivas y negativas de todos los agentes suman cero; 2.",
        "Agentes adversarios simples: todos los agentes en el entorno son agentes adversarios; 3.",
        "AEs bilaterales: AEs con exactamente dos agentes; 4.",
        "AEs multilaterales: AEs de tres o más agentes.",
        "Trabajaremos en instanciaciones tanto bilaterales como multilaterales de entornos de suma cero y simples.",
        "En particular, nuestro modelo de entorno adversarial tratará con interacciones que consisten en N agentes (N ≥ 2), donde todos los agentes son adversarios y solo un agente puede tener éxito.",
        "Ejemplos de tales entornos van desde juegos de mesa (por ejemplo, Ajedrez, Conecta Cuatro y Diplomacia) hasta ciertos entornos económicos (por ejemplo, subastas de N postores sobre un único bien). 2.1 Resumen del Modelo Nuestro enfoque es formalizar las actitudes mentales y comportamientos de un único agente adversario; consideramos cómo un único agente percibe el AE.",
        "La siguiente lista especifica las condiciones y estados mentales de un agente en un AE simple de suma cero: 1.",
        "El agente tiene la intención individual de que su propio objetivo se complete; 2.",
        "El agente tiene la creencia individual de que él y sus adversarios persiguen objetivos completamente opuestos (definidos a continuación) y que solo puede haber un ganador; 3.",
        "El agente tiene la creencia individual de que cada adversario tiene la intención de completar su propio objetivo conflictivo completo; 4.",
        "El agente tiene una creencia individual en el perfil (parcial) de sus adversarios.",
        "El ítem 3 es necesario, ya que podría ser el caso de que algún agente tenga un objetivo completamente conflictivo y esté considerando adoptar la intención de completarlo, pero aún no está comprometido a lograrlo.",
        "Esto podría ocurrir porque el agente aún no ha reflexionado sobre los efectos que adoptar esa intención podría tener en las otras intenciones que actualmente mantiene.",
        "En tales casos, es posible que ni siquiera se considere en un entorno adversarial.",
        "El ítem 4 establece que el agente debe tener alguna creencia sobre los perfiles de sus adversarios.",
        "El perfil representa todo el conocimiento que el agente tiene sobre su adversario: sus debilidades, capacidades estratégicas, objetivos, intenciones, confiabilidad y más.",
        "Puede ser dado explícitamente o puede ser aprendido a partir de observaciones de encuentros pasados. 2.2 Definiciones de modelos para estados mentales Utilizamos las definiciones de los operadores modales, predicados y metapredicados de Grosz y Krauss, tal como se definen en su formalización de SharedPlan [4].",
        "Recordamos aquí algunos de los predicados y operadores que se utilizan en esa formalización: Int.To(Ai, α, Tn, Tα, C) representa las intenciones de Ai en el tiempo Tn de realizar una acción α en el tiempo Tα en el contexto de C. Int.Th(Ai, prop, Tn, Tprop, C) representa las intenciones de Ai en el tiempo Tn de que una cierta proposición prop se cumpla en el tiempo Tprop en el contexto de C. Los operadores de intención potencial, Pot.Int.To(...) y Pot.Int.Th(...), se utilizan para representar el estado mental cuando un agente considera adoptar una intención, pero no ha deliberado sobre la interacción de las otras intenciones que tiene.",
        "El operador Bel(Ai, f, Tf) representa al agente Ai creyendo en la afirmación expresada en la fórmula f, en el tiempo Tf.",
        "MB(A, f, Tf) representa la creencia mutua para un grupo de agentes A.",
        "Una instantánea del sistema encuentra que nuestro entorno se encuentra en algún estado e ∈ E de estados variables ambientales, y cada adversario en cualquier LAi ∈ L de posibles estados locales.",
        "En cualquier paso de tiempo dado, el sistema estará en algún mundo w del conjunto de todos los mundos posibles w ∈ W, donde w = E×LA1 ×LA2 × ...LAn, y n es el número de adversarios.",
        "Por ejemplo, en un juego de póker Texas Holdem, el estado local de un agente podría ser su propio conjunto de cartas (que es desconocido para su adversario) mientras que el entorno consistirá en el bote de apuestas y las cartas comunitarias (que son visibles para ambos jugadores).",
        "Una función de utilidad bajo esta formalización se define como un mapeo de un mundo posible w ∈ W a un elemento en , que expresa la deseabilidad del mundo, desde la perspectiva de un único agente.",
        "Normalmente normalizamos el rango a [0,1], donde 0 representa el mundo menos deseable posible, y 1 es el mundo más deseable.",
        "La implementación de la función de utilidad depende del dominio en cuestión.",
        "La siguiente lista especifica nuevos predicados, funciones, variables y constantes utilizadas en conjunto con las definiciones originales para la formalización del entorno adversarial: 1. φ es una acción nula (el agente no hace nada). 2.",
        "GAi es el conjunto de objetivos del agente Ai.",
        "Cada objetivo es un conjunto de predicados cuya satisfacción hace que el objetivo esté completo (usamos G∗ Ai ∈ GAi para representar un objetivo arbitrario del agente Ai). 3. gAi es el conjunto de subobjetivos del agente Ai.",
        "Los subobjetivos son predicados cuya satisfacción representa un hito importante hacia el logro del objetivo completo. gG∗ Ai ⊆ gAi es el conjunto de subobjetivos que son importantes para la finalización del objetivo G∗ Ai (usaremos g∗ G∗ Ai ∈ gG∗ Ai para representar un subobjetivo arbitrario). 4.",
        "P Aj Ai es el objeto de perfil que el agente Ai tiene sobre el agente Aj. 5.",
        "CA es un conjunto general de acciones para todos los agentes en A que se derivan de las restricciones del entorno.",
        "CAi ⊆ CA es el conjunto de acciones posibles del agente Ai.",
        "La expresión Do(Ai, α, Tα, w) se cumple cuando Ai realiza la acción α durante el intervalo de tiempo Tα en el mundo w. 7.",
        "La afirmación Achieve(G∗ Ai , α, w) es verdadera cuando se logra el objetivo G∗ Ai siguiendo la finalización de la acción α en el mundo w ∈ W, donde α ∈ CAi.",
        "El perfil (Ai, PAi Ai) es verdadero cuando el agente Ai tiene un perfil de objeto para el agente Aj.",
        "Definición 1.",
        "El conflicto total (FulConf) describe una interacción de suma cero donde solo se puede completar un solo objetivo de los objetivos en conflicto.",
        "FulConf(G∗ Ai , G∗ Aj ) ⇒ (∃α ∈ CAi , ∀w, β ∈ CAj ) (Achieve(G∗ Ai , α, w) ⇒ ¬Achieve(G∗ Aj , β, w)) ∨ (∃β ∈ CAj , ∀w, α ∈ CAi )(Achieve(G∗ Aj , β, w) ⇒ ¬Achieve(G∗ Ai , α, w)) Definición 2.",
        "El Conocimiento Adversarial (AdvKnow) es una función que devuelve un valor que representa la cantidad de The Sixth Intl.",
        "Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 551 agente de conocimiento Ai tiene sobre el perfil del agente Aj, en el tiempo Tn.",
        "Cuanto mayor sea el valor, más conocimiento tiene el agente Ai.",
        "AdvKnow: P Aj Ai × Tn → Definición 3.",
        "Eval: Esta función de evaluación devuelve un valor de utilidad esperada estimado para un agente en A, después de completar una acción de CA en algún estado del mundo w. Eval: A × CA × w → Definición 4.",
        "TrH - (Umbral) es una constante numérica en el rango [0,1] que representa un valor umbral de la función de evaluación (Eval).",
        "Una acción que produce una evaluación de utilidad estimada por encima del TrH se considera una acción altamente beneficiosa.",
        "El valor Eval es una estimación y no la función de utilidad real, la cual suele ser desconocida.",
        "Utilizar el valor real de utilidad para un agente racional fácilmente produciría el mejor resultado para ese agente.",
        "Sin embargo, los agentes generalmente no tienen las funciones de utilidad reales, sino más bien una estimación heurística de las mismas.",
        "Existen dos propiedades importantes que deben cumplir para la función de evaluación: Propiedad 1.",
        "La función de evaluación debería indicar que el estado del mundo más deseable es aquel en el que se logra el objetivo.",
        "Por lo tanto, una vez que se ha satisfecho el objetivo, no puede haber ninguna acción futura que pueda colocar al agente en un estado del mundo con un valor de Eval más alto. (∀Ai, G∗ Ai , α, β ∈ CAi , w ∈ W) Lograr(G∗ Ai , α, w) ⇒ Eval(Ai, α, w) ≥ Eval(Ai, β, w) Propiedad 2.",
        "La función de evaluación debe proyectar una acción que cause la finalización de un objetivo o subobjetivo a un valor que sea mayor que TrH (una acción altamente beneficiosa). (∀Ai, G∗ Ai ∈ GAi , α ∈ CAi , w ∈ W, g∗ GAi ∈ gGAi ) Lograr(G∗ Ai , α, w) ∨ Lograr(g∗ GAi , α, w) ⇒ Eval(Ai, α, w) ≥ TrH.",
        "Definición 5.",
        "La acción de conjunto Definimos una acción de conjunto (SetAction) como un conjunto de operaciones de acción (ya sean acciones complejas o básicas) de algunos conjuntos de acción CAi y CAj que, según la creencia del agente Ai, están unidas por una relación temporal y consecuente, formando una cadena de eventos (acción y su acción consecuente siguiente). (∀α1 , . . . , αu ∈ CAi , β1 , . . . , βv ∈ CAj , w ∈ W) SetAction(α1 , . . . , αu , β1 , . . . , βv , w) ⇒ ((Do(Ai, α1 , Tα1 , w) ⇒ Do(Aj, β1 , Tβ1 , w)) ⇒ Do(Ai, α2 , Tα2 , w) ⇒ . . . ⇒ Do(Ai, αu , Tαu , w)) La relación consecuente podría existir debido a varias restricciones ambientales (cuando una acción obliga al adversario a responder con una acción específica) o debido al conocimiento de los agentes sobre el perfil de su adversario.",
        "Propiedad 3.",
        "A medida que aumenta el conocimiento que tenemos sobre nuestro adversario, tendremos creencias adicionales sobre su comportamiento en diferentes situaciones, lo que a su vez crea un nuevo conjunto de acciones.",
        "Formalmente, si nuestro AdvKnow en el tiempo Tn+1 es mayor que AdvKnow en el tiempo Tn, entonces cada SetAction conocido en el tiempo Tn también es conocido en el tiempo Tn+1.",
        "AdvKnow(P Aj Ai , Tn+1) > AdvKnow(P Aj Ai , Tn) ⇒ (∀α1 , . . . , αu ∈ CAi , β1 , . . . , βv ∈ CAj ) Bel(Aag, SetAction(α1 , . . . , αu , β1 , . . . , βv ), Tn) ⇒ Bel(Aag, SetAction(α1 , . . . , αu , β1 , . . . , βv ), Tn+1) 2.3 La formulación del entorno Los siguientes axiomas proporcionan la definición formal para un Entorno Adversarial (AE) simple y de suma cero.",
        "La satisfacción de estos axiomas significa que el agente está situado en un entorno así.",
        "Proporciona especificaciones para que el agente Aag interactúe con su conjunto de adversarios A con respecto a los objetivos G∗ Aag y G∗ A en el tiempo TCo en algún estado del mundo w. AE(Aag, A, G∗ Aag, A1, . . . , Ak, G∗ A1, . . . , G∗ Ak, Tn, w) 1.",
        "Aag tiene un Int.Th su objetivo se completaría: (∃α ∈ CAag , Tα) Int.Th(Aag, Lograr(G∗ Aag , α), Tn, Tα, AE) 2.",
        "Aag cree que él y cada uno de sus adversarios Ao están persiguiendo metas completamente conflictivas: (∀Ao ∈ {A1, . . . , Ak}) Bel(Aag, FulConf(G∗ Aag , G∗ Ao ), Tn) 3.",
        "Aag cree que cada uno de sus adversarios en Ao tiene la Int.Th su objetivo en conflicto G∗ Aoi se completará: (∀Ao ∈ {A1, . . . , Ak})(∃β ∈ CAo , Tβ) Bel(Aag, Int.Th(Ao, Lograr(G∗ Ao , β), TCo, Tβ, AE), Tn) 4.",
        "Para construir un agente que pueda operar con éxito dentro de un entorno de adversarios estratégicos, debemos especificar pautas de comportamiento para sus interacciones.",
        "Utilizar una estrategia de maximización de Eval ingenua hasta cierta profundidad de búsqueda no siempre dará resultados satisfactorios por varias razones: (1) el problema del horizonte de búsqueda al buscar a una profundidad fija; (2) la fuerte suposición de un adversario óptimamente racional con recursos ilimitados; (3) utilizar una función de evaluación estimada que no dará resultados óptimos en todos los estados del mundo, y que puede ser explotada [9].",
        "Los siguientes axiomas especifican los principios de comportamiento que se pueden utilizar para diferenciar entre agentes exitosos y menos exitosos en el entorno adversarial mencionado anteriormente.",
        "Esos axiomas deben ser utilizados como principios de especificación al diseñar e implementar agentes que deben ser capaces de desempeñarse bien en entornos adversarios.",
        "Los axiomas conductuales representan situaciones en las que el agente adoptará intenciones potenciales para (Pot.Int.To(...)) realizar una acción, lo que típicamente requerirá un razonamiento de medios-fines para seleccionar un posible curso de acción.",
        "Este razonamiento llevará a la adopción de un Int.To(...) (ver [4]).",
        "A1.",
        "Axioma para lograr metas.",
        "El primer axioma es el caso más simple; cuando el agente Aag cree que está a una acción (α) de lograr su objetivo en conflicto G∗ Aag, debería adoptar la intención potencial de hacer α y completar su objetivo. (∀Aag, α ∈ CAag, Tn, Tα, w ∈ W) (Bel(Aag, Do(Aag, α, Tα, w) ⇒ Achieve(G∗ Aag, α, w)) ⇒ Pot.Int.To(Aag, α, Tn, Tα, w) Este comportamiento algo trivial es el primer y más fuerte axioma.",
        "En cualquier situación, cuando el agente está a una acción de completar el objetivo, debería completar la acción.",
        "Cualquier función de evaluación justa clasificaría naturalmente a α como la acción de valor máximo (propiedad 1).",
        "Sin embargo, sin una axiomatización explícita de dicho comportamiento, podría haber situaciones en las que el agente decida tomar otra acción por diversas razones, debido a sus recursos de decisión limitados.",
        "A2.",
        "Axioma del Acto Preventivo.",
        "En una situación adversarial, el agente Aag podría decidir tomar acciones que dañen uno de los planes de su adversario para completar su objetivo, incluso si esas acciones no avanzan explícitamente a Aag hacia su objetivo en conflicto G∗ Aag.",
        "Tal acción preventiva tendrá lugar cuando el agente Aag tenga la creencia sobre la posibilidad de que su adversario Ao realice una acción β que le otorgará un alto 552 The Sixth Intl.",
        "Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) valor de evaluación de utilidad (> TrH).",
        "Creer que tomar la acción α evitará que el oponente realice su β, adoptará una intención potencial de hacer α. (∀Aag, Ao ∈ A, α ∈ CAag , β ∈ CAo , Tn, Tβ, w ∈ W) (Bel(Aag, Do(Ao, β, Tβ, w) ∧ Eval(Ao, β, w) > TrH, Tn) ∧ Bel(Aag, Do(Aag, α, Tα, w) ⇒ ¬Do(Ao, β, Tβ, w), Tn) ⇒ Pot.Int.To(Aag, α, Tn, Tα, w) Este axioma es un componente básico de cualquier entorno adversarial.",
        "Por ejemplo, al observar un juego de ajedrez, un jugador podría darse cuenta de que está a punto de ser jaque mateado por su oponente, por lo tanto, realiza una jugada preventiva.",
        "Otro ejemplo es el juego de Cuatro en Línea: cuando un jugador tiene una fila de tres fichas, su oponente debe bloquearla o perder.",
        "Una instancia específica de A1 ocurre cuando el adversario está a una acción de lograr su objetivo, y la acción preventiva inmediata debe ser tomada por el agente.",
        "Formalmente, tenemos las mismas creencias que se mencionaron anteriormente, con la creencia modificada de que realizar la acción β hará que el agente Ao logre su objetivo.",
        "Proposición 1: Caso de prevención o pérdida. (∀Aag, Ao ∈ A, α ∈ CAag , β ∈ CAo , G∗ Ao , Tn, Tα, Tβ, w ∈ W) Bel(Aag, Do(Ao, β, Tβ, w) ⇒ Achieve(G∗ Ao , β, w), Tn) ∧ Bel(Aag, Do(Aag, α, Tα, w) ⇒ ¬Do(Ao, β, Tβ, w)) ⇒ Pot.Int.To(Aag, α, Tn, Tα, w) Esquema de prueba: La Proposición 1 puede derivarse fácilmente del axioma A1 y de la propiedad 2 de la función Eval, que establece que cualquier acción que cause la finalización de un objetivo es una acción altamente beneficiosa.",
        "El comportamiento del acto preventivo ocurrirá implícitamente cuando la función Eval sea igual a la función de utilidad del mundo real.",
        "Sin embargo, al ser agentes racionales limitados y al tratar con una función de evaluación estimada, necesitamos axiomatizar explícitamente dicho comportamiento, ya que no siempre ocurrirá implícitamente a partir de la función de evaluación.",
        "A3.",
        "Axioma de Movimiento Táctico Subóptimo.",
        "En muchos escenarios, puede ocurrir una situación en la que un agente decida no tomar la acción más beneficiosa actual que puede tomar (la acción con el valor de evaluación de utilidad máximo), porque cree que tomar otra acción (con un valor de evaluación de utilidad más bajo) podría generar (dependiendo de la respuesta del adversario) una posibilidad futura para una acción altamente beneficiosa.",
        "Esto ocurrirá con mayor frecuencia cuando la función Eval sea inexacta y difiera en gran medida de la función de Utilidad.",
        "De manera formal, el agente Aag cree en una cierta SetAction que evolucionará de acuerdo con su acción inicial y producirá un valor beneficioso alto (> TrH) únicamente para él. (∀Aag, Ao ∈ A, Tn, w ∈ W) (∃α1 , . . . , αu ∈ CAi , β1 , . . . , βv ∈ CAj , Tα1 ) Bel(Aag, SetAction(α1 , . . . , αu , β1 , . . . , βv ), Tn) ∧ Bel(Aag, Eval(Ao, βv , w) < TrH < Eval(Aag, αu , w), Tn) ⇒ Pot.Int.To(Aag, α1 , Tn, Tα1 , w) Un agente podría creer que una cadena de eventos ocurrirá por diversas razones debido a la naturaleza inevitable del dominio.",
        "Por ejemplo, en el ajedrez, a menudo observamos lo siguiente: una jugada provoca una posición de jaque, lo que a su vez limita los movimientos del oponente para evitar el jaque, a lo que el primer jugador podría reaccionar con otro jaque, y así sucesivamente.",
        "El agente también podría creer en una cadena de eventos basada en su conocimiento del perfil de su adversario, lo que le permite prever los movimientos del adversario con alta precisión.",
        "A4.",
        "Axioma de Detección de Perfiles.",
        "El agente puede ajustar los perfiles de sus adversarios mediante observaciones y estudio de patrones (específicamente, si hay encuentros repetidos con el mismo adversario).",
        "Sin embargo, en lugar de esperar a que se revele la información del perfil, un agente también puede iniciar acciones que obligarán a su adversario a reaccionar de una manera que revele conocimiento del perfil sobre él.",
        "Formalmente, el axioma establece que si todas las acciones (γ) no son acciones altamente beneficiosas (< TrH), el agente puede realizar la acción α en el tiempo Tα si cree que resultará en una acción no altamente beneficiosa β por parte de su adversario, lo que a su vez le enseña sobre el perfil del adversario, es decir, proporciona un mayor AdvKnow(P Aj Ai , Tβ). (∀Aag, Ao ∈ A, α ∈ CAag , β ∈ CAo , Tn, Tα, Tβ, w ∈ W) Bel(Aag, (∀γ ∈ CAag )Eval(Aag, γ, w) < TrH, Tn) ∧ Bel(Aag, Do(Aag, α, Tα, w) ⇒ Do(Ao, β, Tβ, w), Tn) ∧ Bel(Aag, Eval(Ao, β, w) < TrH) ∧ Bel(Aag, AdvKnow(P Aj Ai , Tβ) > AdvKnow(P Aj Ai , Tn), Tn) ⇒ Pot.Int.To(Aag, α, Tn, Tα, w) Por ejemplo, volviendo al escenario del juego de ajedrez, consideremos comenzar una partida contra un oponente del que no sabemos nada, ni siquiera si es humano o un oponente computarizado.",
        "Podríamos comenzar a jugar una estrategia que sea adecuada frente a un oponente promedio, y ajustar nuestro juego de acuerdo a su nivel de juego.",
        "A5.",
        "Axioma de Formación de Alianzas El siguiente axioma de comportamiento es relevante únicamente en una instancia multilateral del entorno adversarial (obviamente, una alianza no puede formarse en un encuentro bilateral de suma cero).",
        "En diferentes situaciones durante una interacción multilateral, un grupo de agentes podría creer que es de su interés formar una alianza temporal.",
        "Una alianza de este tipo es un acuerdo que limita el comportamiento de sus miembros, pero se cree por parte de estos que les permite alcanzar un valor de utilidad mayor al que podrían lograr fuera de la alianza.",
        "Como ejemplo, podemos observar el clásico juego de mesa Risk, donde cada jugador tiene como objetivo individual ser el único conquistador del mundo, un juego de suma cero.",
        "Sin embargo, para lograr este objetivo, podría ser estratégicamente sabio hacer acuerdos de alto el fuego a corto plazo con otros jugadores, o unirse y atacar a un oponente que sea más fuerte que el resto.",
        "Los términos de una alianza definen la forma en que sus miembros deben actuar.",
        "Es un conjunto de predicados, denominados Términos, que es acordado por los miembros de la alianza y debe mantenerse verdadero durante la duración de la alianza.",
        "Por ejemplo, el conjunto de términos en el escenario de riesgo podría contener los siguientes predicados: 1.",
        "Los miembros de la alianza no se atacarán entre sí en los territorios X, Y y Z; 2.",
        "Los miembros de la alianza contribuirán C unidades por turno para atacar al adversario Ao; 3.",
        "Los miembros están obligados a permanecer como parte de la alianza hasta el tiempo Tk o hasta que el ejército adversario Ao sea más pequeño que Q.",
        "El conjunto de términos especifica restricciones intergrupales en cada uno de los miembros de la alianza (∀Aal i ∈ Aal ⊆ A) conjunto de acciones Cal i ⊆ C. Definición 6.",
        "El valor total de evaluación que el agente Ai logrará al ser parte de Aal es la suma de Evali (Eval para Ai) de cada uno de los valores de evaluación de Aal j después de tomar sus propias acciones α (a través del predicado del agente(α)): Al val(Ai, Cal, Aal, w) = α∈Cal Evali(Aal j, agente(α), w) Definición 7.",
        "Al TrH - es un número que representa un valor de Al en la Sexta Conferencia Internacional.",
        "Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 553 umbral; por encima de él, se puede decir que la alianza es una alianza altamente beneficiosa.",
        "El valor de Al TrH se calculará dinámicamente de acuerdo con el progreso de la interacción, como se puede ver en [7].",
        "Después de formarse una alianza, sus miembros ahora están trabajando en su entorno adversarial normal, así como de acuerdo con los estados mentales y axiomas requeridos para sus interacciones como parte de la alianza.",
        "El siguiente modelo de Alianza (AL) especifica las condiciones bajo las cuales el grupo Aal puede considerarse en una alianza y trabajando con un nuevo y limitado conjunto de acciones Cal, en el tiempo Tn.",
        "AL(Aal , Cal , w, Tn) 1. \n\nAL(Aal , Cal , w, Tn) 1.",
        "Aal tiene un MB en el que todos los miembros son parte de Aal: MB(Aal, (∀Aal i ∈ Aal)miembro(Aal i, Aal), Tn) 2.",
        "Aal tiene una MB que el grupo se mantenga: MB(Aal , (∀Aal i ∈ Aal )Int.Th (Ai, miembro(Ai, Aal ), Tn, Tn+1, Co), Tn) 3.",
        "Aal tiene un beneficio marginal que al ser miembros les otorga un alto valor de utilidad: MB(Aal , (∀Aal i ∈ Aal )Al val(Aal i , Cal , Aal , w) ≥ Al TrH, Tn) Los perfiles de los miembros son una parte crucial de las alianzas exitosas.",
        "Suponemos que los agentes que tienen perfiles más precisos de sus adversarios serán más exitosos en tales entornos.",
        "Tales agentes podrán predecir cuándo un miembro está a punto de violar el contrato de alianzas (elemento 2 en el modelo anterior) y tomar medidas correctivas (cuando el elemento 3 se falsificará).",
        "La robustez de la alianza depende en parte de la medida de confianza de sus miembros, la estimación de su posición objetiva y otras propiedades de su perfil.",
        "Debemos tener en cuenta que un agente puede formar parte simultáneamente de más de una alianza.",
        "Una alianza temporal, donde los miembros del grupo no tienen un objetivo común pero actúan colaborativamente en interés de sus propias metas individuales, es clasificada como un Grupo de Tratamiento por los psicólogos modernos (en contraste con un Grupo de Tarea, donde sus miembros tienen un objetivo común).",
        "El modelo de Actividad Compartida presentado en [5] modeló el comportamiento del Grupo de Tratamiento utilizando la misma formalización de Planes Compartidos.",
        "Al comparar ambas definiciones de una alianza y un Grupo de Tratamiento, encontramos una similitud sorprendente entre ambos modelos: las definiciones de los modelos de entorno son casi idénticas (ver las definiciones de SAs en [5]), y sus axiomas de Acto Egoísta y Acto Cooperativo se ajustan al comportamiento de nuestros agentes adversarios.",
        "La principal distinción entre ambos modelos es la integración de un axioma de acto de comportamiento útil, en la Actividad Compartida que no puede formar parte de la nuestra.",
        "Este axioma establece que un agente considerará tomar una acción que disminuirá su valor de Evaluación (hasta un cierto límite inferior), si cree que un compañero del grupo obtendrá un beneficio significativo.",
        "Dicho comportamiento no puede ocurrir en un entorno puramente adversarial (como un juego de suma cero), donde los miembros de la alianza están constantemente alerta para manipular su alianza en su propio beneficio.",
        "A6.",
        "Axioma de Maximización de la Evaluación.",
        "En un caso en el que todos los demás axiomas no sean aplicables, procederemos con la acción que maximice el valor heurístico calculado en la función Eval. (∀Aag, Ao ∈ A, α ∈ Cag, Tn, w ∈ W) Bel(Aag, (∀γ ∈ Cag)Eval(Aag, α, w) ≥ Eval(Aag, γ, w), Tn) ⇒ Pot.Int.To(Aag, α, Tn, Tα, w) T1.",
        "Optimalidad en Eval = Utilidad El modelo axiomático anterior aborda situaciones en las que la Utilidad es desconocida y los agentes son agentes racionales limitados.",
        "El siguiente teorema muestra que en interacciones bilaterales, donde los agentes tienen la función de Utilidad real (es decir, Eval = Utilidad) y son agentes racionales, los axiomas proporcionan el mismo resultado óptimo que la búsqueda adversarial clásica (por ejemplo, Min-Max).",
        "Teorema 1.",
        "Que Ae ag sea un agente AE racional ilimitado que utiliza la función de evaluación heurística Eval, Au ag sea el mismo agente utilizando la verdadera función de utilidad y Ao sea un único adversario racional ilimitado basado en utilidad.",
        "Dado que Eval = Utilidad: (∀α ∈ CAu ag , α ∈ CAe ag , Tn, w ∈ W) Pot.Int.To(Au ag, α, Tn, Tα, w) → Pot.Int.To(Ae ag, α , Tn, Tα, w) ∧ ((α = α ) ∨ (Utilidad(Au ag, α, w) = Eval(Ae ag, α , w))) Esquema de prueba - Dado que Au ag tiene la función de utilidad real y recursos ilimitados, puede generar el árbol de juego completo y ejecutar el algoritmo MinMax óptimo para elegir la acción de mayor valor de utilidad, que denotamos como α.",
        "La prueba mostrará que Ae ag, utilizando los axiomas AE, seleccionará la misma utilidad α o igual (cuando hay más de una acción con la misma utilidad máxima) cuando Eval = Utilidad. (A1) Axioma de logro de objetivo - supongamos que hay un α tal que su cumplimiento logrará el objetivo Au ags.",
        "Obtendrá la utilidad más alta mediante Min-Max para Au ag.",
        "El agente Ae seleccionará α u otra acción con el mismo valor de utilidad a través de A1.",
        "Si tal α no existe, Ae ag no puede aplicar este axioma y procede a A2. (A2) Axioma de la acción preventiva - (1) Observando el caso base (ver Prop1), si hay un β que lleva a Ao a alcanzar su objetivo, entonces una acción preventiva α producirá la mayor utilidad para Au ag.",
        "Au ag lo elegirá a través de la utilidad, mientras que Ae ag lo elegirá a través de A2. En el caso general, β es una acción altamente beneficiosa para Ao, lo que resulta en una baja utilidad para Au ag, lo que lo guiará a seleccionar un α que evitará β, mientras que Ae ag lo elegirá a través de A2.1 Si dicho β no existe para Ao, entonces A2 no es aplicable, y Ae ag puede proceder a A3. (A3) Axioma de movimiento táctico subóptimo: al usar una función de evaluación heurística, Ae ag tiene una creencia parcial en el perfil de su adversario (elemento 4 en el modelo AE), lo que puede llevarlo a creer en SetActions (Prop1).",
        "En nuestro caso, Ae ag tiene un perfil completo de su adversario óptimo y sabe que Ao se comportará de manera óptima según los valores reales de utilidad en el árbol de búsqueda completo, por lo tanto, cualquier creencia sobre una SetAction subóptima no puede existir, lo que hace que este axioma sea inaplicable.",
        "Ae ag procederá a A4. (A4) Axioma de detección de perfil - Dado que Ae ag tiene el perfil completo de Ao, ninguna de las acciones de Ae ag puede aumentar su conocimiento.",
        "Ese axioma no se aplicará, y el agente procederá con A6 (A5 será ignorado porque la interacción es bilateral). (A6) Axioma de maximización de la evaluación: este axioma seleccionará la máxima Evaluación para Ae ag.",
        "Dado que Eval = Utilidad, se seleccionará el mismo α que fue seleccionado por Au ag. 3.",
        "EVALUACIÓN El propósito principal de nuestro análisis experimental es evaluar el comportamiento y rendimiento de los modelos en un entorno adversarial real.",
        "Esta sección investiga si un caso acotado 1 en el que, tras la finalización de β, exista un γ que proporcione una alta utilidad para el Agente Au ag, no puede ocurrir porque Ao utiliza la misma utilidad, y la existencia de γ hará que clasifique a β como una acción de baja utilidad. 554 The Sixth Intl.",
        "En la Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07), los agentes racionales situados en entornos adversariales se beneficiarán más al aplicar nuestros axiomas de comportamiento sugeridos. 3.1 El Dominio Para explorar el uso del modelo anterior y sus axiomas de comportamiento, decidimos utilizar el juego de Conecta Cuatro como nuestro entorno adversarial.",
        "Connect-Four es un juego de 2 jugadores, de suma cero, que se juega utilizando un tablero similar a una matriz de 6x7.",
        "En cada turno, un jugador coloca un disco en una de las 7 columnas (el conjunto de 21 discos suele ser de color amarillo para el jugador 1 y rojo para el jugador 2; usaremos Blanco y Negro respectivamente para evitar confusiones).",
        "El ganador es el primer jugador en completar un conjunto de cuatro discos de su color en horizontal, vertical o diagonal.",
        "En ocasiones muy raras, el juego podría terminar en empate si todos los cuadros vacíos están llenos, pero ningún jugador logró crear un conjunto de 4 discos.",
        "El juego de Conecta Cuatro fue resuelto en [1], donde se muestra que el primer jugador (jugando con los discos blancos) puede forzar una victoria comenzando en la columna del medio (columna 4) y jugando de manera óptima. Sin embargo, la estrategia óptima es muy compleja y difícil de seguir incluso para agentes racionales limitados complejos, como los jugadores humanos.",
        "Antes de que podamos proceder a verificar el comportamiento del agente, primero debemos verificar que el dominio se ajusta a la definición de entornos adversarios mencionada anteriormente (en la que se basan los axiomas de comportamiento).",
        "Primero, al jugar al juego de Conecta Cuatro, el agente tiene la intención de ganar el juego (ítem 1).",
        "Segundo (ítem 2), nuestro agente cree que en Conecta Cuatro solo puede haber un ganador (o ningún ganador en la rara ocasión de un empate).",
        "Además, nuestro agente cree que su oponente en el juego intentará ganar (ítem 3), y esperamos que tenga algún conocimiento parcial (ítem 4) sobre su adversario (este conocimiento puede variar desde nada, pasando por hechos simples como la edad, hasta estrategias y debilidades).",
        "Por supuesto, no todos los encuentros de Conecta Cuatro son adversariales.",
        "Por ejemplo, cuando un padre está jugando el juego con su hijo, la siguiente situación podría ocurrir: el niño, con un fuerte incentivo para ganar, trata el entorno como adversarial (intenta ganar, entiende que solo puede haber un ganador y cree que su padre está tratando de vencerlo).",
        "Sin embargo, desde el punto de vista de los padres, el entorno podría ser visto como educativo, donde el objetivo no es ganar el juego, sino causar disfrute o practicar el razonamiento estratégico.",
        "En un entorno educativo como este, un nuevo conjunto de axiomas de comportamiento podría ser más beneficioso para los objetivos de los padres que nuestros axiomas de comportamiento adversarial sugeridos. 3.2 Análisis de Axiomas Después de demostrar que el juego de Conecta Cuatro es de hecho un entorno adversarial bilateral de suma cero, el siguiente paso es observar el comportamiento de los jugadores durante el juego y verificar si comportarse de acuerdo con nuestro modelo mejora el rendimiento.",
        "Para hacerlo, hemos recopilado archivos de registro de juegos de Conecta Cuatro completados que fueron jugados por jugadores humanos a través de Internet.",
        "Nuestros datos de archivo de registro recopilados provienen de sitios de Juego por Correo Electrónico (PBeM).",
        "Estos son sitios web que alojan juegos por correo electrónico, donde cada movimiento se realiza a través de un intercambio de correos electrónicos entre el servidor y los jugadores.",
        "Muchos de estos sitios de archivos contienen interacciones competitivas reales y también mantienen un sistema de clasificación para sus miembros.",
        "La mayoría de los datos que utilizamos se pueden encontrar en [6].",
        "Como se puede aprender de [1], Conecta Cuatro tiene una estrategia óptima y una ventaja considerable para el jugador que comienza el juego (a quien llamamos jugador Blanco).",
        "Nos concentraremos en nuestro análisis en los movimientos de los segundos jugadores (a los que llamaremos Negras).",
        "El jugador Blanco, al ser el primero en actuar, tiene la llamada ventaja de la iniciativa.",
        "Tener la ventaja y una buena estrategia mantendrá al jugador Negro ocupado reaccionando a sus movimientos, en lugar de iniciar amenazas.",
        "Una amenaza es una combinación de tres discos del mismo color, con un espacio vacío para el cuarto disco ganador.",
        "Una amenaza abierta es una amenaza que puede materializarse en el próximo movimiento del oponente.",
        "Para que el jugador Negro gane, debe de alguna manera cambiar el rumbo, tomar la ventaja y comenzar a presentar amenazas al jugador Blanco.",
        "Exploraremos el comportamiento de los jugadores negros y su conformidad con nuestros axiomas.",
        "Para hacerlo, construimos una aplicación que lee archivos de registro y analiza los movimientos de los jugadores Negros.",
        "La aplicación contiene dos componentes principales: (1) un algoritmo Min-Max para la evaluación de movimientos; (2) un detector de amenazas abiertas para descubrir amenazas abiertas.",
        "El algoritmo Min-Max funcionará hasta una profundidad d dada y, para cada movimiento α, devolverá el valor heurístico para la próxima acción tomada por el jugador según lo escrito en el archivo de registro, h(α), junto con el valor heurístico máximo, maxh(α), que podría lograrse antes de realizar el movimiento (obviamente, si h(α) = maxh(α), entonces el jugador no realizó el movimiento óptimo desde el punto de vista heurístico).",
        "El trabajo de los detectores de amenazas es notificar si se tomó alguna acción para bloquear una amenaza abierta (no bloquear una amenaza abierta probablemente hará que el jugador pierda en el próximo movimiento del oponente).",
        "La función heurística utilizada por Min-Max para evaluar la utilidad de los jugadores es la siguiente función, que es simple de calcular, pero proporciona un desafío razonable a los oponentes humanos: Definición 8.",
        "Sea Grupo un conjunto adyacente de cuatro cuadrados que son horizontales, verticales o diagonales.",
        "El Grupo n (Grupo w) es un Grupo con n piezas de color negro (blanco) y 4−n casillas vacías. h = ((Grupo1 negro ∗α)+(Grupo2 negro ∗β)+(Grupo3 negro ∗γ)+(Grupo4 negro ∗∞)) − ((Grupo1 blanco ∗α)+(Grupo2 blanco ∗β)+(Grupo3 blanco ∗γ)+(Grupo4 blanco ∗∞)) Los valores de α, β y δ pueden variar para formar cualquier combinación lineal deseada; sin embargo, es importante asignarles valores teniendo en cuenta el orden α < β < δ (usamos 1, 4 y 8 como sus valores respectivos).",
        "Grupos de 4 discos del mismo color significan victoria, por lo tanto, el descubrimiento de dicho grupo resultará en ∞ para asegurar un valor extremo.",
        "Ahora utilizamos nuestra función de evaluación estimada para evaluar las acciones de los jugadores Negros durante la interacción adversarial de Conecta Cuatro.",
        "Cada juego del archivo de registro fue introducido en la aplicación, la cual procesó y generó un archivo de registro reformateado que contenía el valor h del movimiento actual, el valor maxh que podría ser alcanzado y una notificación si se detectaba una amenaza abierta.",
        "Un total de 123 partidas fueron analizadas (57 con victoria de las blancas y 66 con victoria de las negras).",
        "Algunos juegos adicionales fueron ignorados manualmente en el experimento, debido a estos problemas: un jugador abandonando el juego mientras el resultado no es definitivo, o una jugada irracional y contundente en las primeras etapas del juego (por ejemplo, no bloquear un grupo obviamente ganador en las primeras jugadas de apertura).",
        "Además, también se eliminó un empate.",
        "El simulador se ejecutó con una profundidad de búsqueda de 3 movimientos.",
        "Ahora procedemos a analizar los juegos con respecto a cada axioma de comportamiento.",
        "El Sexto Internacional.",
        "La siguiente sección presenta las evaluaciones heurísticas del algoritmo Min-Max para cada acción, y verifica la cantidad y el alcance de las acciones tácticas subóptimas y sus implicaciones en el rendimiento.",
        "La Tabla 1 muestra los resultados y las percepciones del análisis heurístico de los juegos, cuando la profundidad de búsqueda es igual a 3 (esta profundidad de búsqueda fue seleccionada para que los resultados sean comparables a [9], ver Sección 3.2.3).",
        "La información heurística de las tablas es la diferencia entre el valor heurístico máximo presente y el valor heurístico de la acción que finalmente tomó el jugador (es decir, cuanto más cercano sea el número a 0, más cercana estuvo la acción a la acción heurística máxima).",
        "La primera fila presenta los valores de diferencia de la acción que tuvo el valor de diferencia máximo entre todas las acciones de los jugadores Negros en un juego dado, promediado en todos los juegos ganados y perdidos por los Negros (ver columnas respectivas).",
        "En los juegos en los que el jugador Negro pierde, su valor promedio de diferencia fue de -17.62, mientras que en los juegos en los que el jugador Negro ganó, su promedio fue de -12.02.",
        "La segunda fila amplía el análisis al considerar las 3 acciones con mayor diferencia heurística, y promediarlas.",
        "En ese caso, notamos una diferencia heurística promedio de 5 puntos entre los juegos en los que el jugador Negro pierde y los juegos en los que gana.",
        "Sin embargo, la importancia de esos números es que nos permitieron hacer una suposición educada sobre un número umbral de 11.5, como el valor de la constante TrH, que diferencia entre acciones normales y altamente beneficiosas.",
        "Después de encontrar una constante TrH aproximada, podemos proceder con un análisis de la importancia de los movimientos subóptimos.",
        "Para hacerlo, tomamos el subconjunto de juegos en los que el valor mínimo de diferencia heurística para las acciones de las negras era de 11.5.",
        "Como se presenta en la Tabla 2, podemos ver el promedio de 3 horas mínimo de los 3 mayores rangos y el porcentaje respectivo de juegos ganados.",
        "La primera fila muestra que el jugador Negro ganó solo el 12% de los juegos en los que el promedio de sus 3 acciones con mayor diferencia heurística (min3 h) era menor que el umbral sugerido, TrH = 11.5.",
        "La segunda fila muestra un resultado sorprendente: parece que cuando min3 h > −4, el jugador Negro rara vez gana.",
        "La intuición sugeriría que los juegos en los que los valores de evaluación de la acción estén más cerca de los valores máximos resultarán en más juegos ganados para las piezas negras.",
        "Sin embargo, parece que en el dominio de Conecta Cuatro, simplemente responder con acciones algo fácilmente esperadas, sin iniciar algunos movimientos sorprendentes y subóptimos, no produce buenos resultados.",
        "La última fila resume las principales conclusiones del análisis; la mayoría de las victorias de Blacks (83%) se produjeron cuando su min3 h estaba en el rango de -11.5 a -4.",
        "Una inspección detallada de esos juegos ganados por las fichas negras muestra el siguiente patrón detrás de los números: después de los movimientos de apertura estándar, las fichas negras de repente colocan una ficha en una columna aislada, lo cual parece ser un desperdicio de un movimiento.",
        "El blanco sigue construyendo sus amenazas, generalmente ignorando el último movimiento de las negras, que a su vez utiliza el disco aislado como ancla para una futura amenaza ganadora.",
        "Los resultados muestran que fue beneficioso para el jugador Negro en la Tabla 2: Porcentajes de ganancias de los Negros % de juegos min3 h < −11.5 12% min3 h > −4 5% −11.5 ≤ min3 h ≤ −4 83% tomar acciones subóptimas y no dar el valor heurístico más alto posible en ese momento, pero no será demasiado perjudicial para su posición (es decir, no dará un alto valor beneficioso a su adversario).",
        "Resultó que aprender el umbral es un aspecto importante del éxito: realizar movimientos extremadamente arriesgados (min3 h < −11.5) o intentar evitarlos (min3 h > −4) reduce significativamente las posibilidades de ganar de los jugadores negros. 3.2.2 Afirmación del Axioma de Monitoreo de Perfiles En la tarea de demostrar la importancia de monitorear los perfiles de los adversarios, nuestros archivos de registro no pudieron ser utilizados porque no contenían interacciones repetidas entre jugadores, que son necesarias para inferir el conocimiento de los jugadores sobre sus adversarios.",
        "Sin embargo, la importancia del modelado del oponente y su uso para obtener ventajas tácticas ya ha sido estudiada en varios ámbitos ([3, 9] son buenos ejemplos).",
        "En un artículo reciente, Markovitch y Reger [9] exploraron la noción de aprendizaje y explotación de la debilidad del oponente en interacciones competitivas.",
        "Aplican estrategias de aprendizaje simples analizando ejemplos de interacciones pasadas en un dominio específico.",
        "También utilizaron el dominio adversarial de Conecta Cuatro, que ahora se puede utilizar para comprender la importancia de monitorear el perfil del adversario.",
        "Tras la presentación de su modelo teórico, describen un extenso estudio empírico y verifican el rendimiento de los agentes después de aprender el modelo de debilidades con ejemplos pasados.",
        "Uno de los dominios utilizados como entorno competitivo fue el mismo juego de Conecta Cuatro (las Damas fueron el segundo dominio).",
        "Su función heurística era idéntica a la nuestra con tres variaciones diferentes (H1, H2 y H3) que se distinguen entre sí por los valores de sus coeficientes de combinación lineal.",
        "La profundidad de búsqueda para los jugadores fue de 3 (como en nuestro análisis).",
        "Sus extensos experimentos verifican y comparan diversas estrategias de aprendizaje, factores de riesgo, conjuntos de características predefinidas y métodos de uso.",
        "La conclusión es que el dominio de Conecta Cuatro muestra una mejora de una tasa de victoria de 0.556 antes de modelar a 0.69 después de modelar (página 22).",
        "Sus conclusiones, que muestran un rendimiento mejorado al sostener y utilizar el modelo del adversario, justifican el esfuerzo de monitorear el perfil del adversario para interacciones continuas y repetidas.",
        "Un punto adicional que surgió en sus experimentos es el siguiente: después de que se haya aprendido el modelo de debilidad del oponente, los autores describen diferentes métodos para integrar el modelo de debilidad del oponente en la estrategia de decisión de los agentes.",
        "Sin embargo, independientemente del método específico que elijan para trabajar, todos los métodos de integración podrían hacer que el agente tome decisiones subóptimas; podría hacer que el agente prefiera acciones que son subóptimas en el momento actual de la decisión, pero que podrían hacer que el oponente reaccione de acuerdo con su modelo de debilidad (como lo representa nuestro agente), lo cual a su vez será beneficioso para nosotros en el futuro.",
        "El comportamiento de los agentes, como se demuestra en [9], confirma y refuerza aún más nuestro Axioma Táctico Subóptimo discutido en la sección anterior. 556 The Sixth Intl.",
        "La necesidad de los axiomas de Logro de Objetivos, Acto Preventivo y Maximización de la Evaluación es evidente y no requiere más verificación.",
        "Sin embargo, incluso con respecto a esos axiomas, surgieron algunas ideas interesantes en el análisis de registros.",
        "Los axiomas de Logro y Prevención de Actos, aunque teóricamente triviales, parecen presentar un desafío para un jugador humano.",
        "En la inspección inicial de los registros, nos encontramos con algunos juegos donde un jugador, por razones inexplicables, no bloqueó al otro para que ganara o no logró ejecutar su propia jugada ganadora.",
        "Podemos atribuir esas fallas a la falta de atención de los humanos, o a un error de escritura en su respuesta al movimiento; sin embargo, esos errores podrían ocurrir en agentes racionales limitados, y el comportamiento apropiado debe ser axiomatizado.",
        "Un juego típico de Cuatro en Línea gira en torno a generar amenazas y bloquearlas.",
        "En nuestro análisis buscamos acciones preventivas explícitas, es decir, movimientos que bloqueen un grupo de 3 discos o que eliminen una amenaza futura (en nuestro horizonte de búsqueda limitado).",
        "Encontramos que en el 83% de los juegos totales, hubo al menos una acción preventiva tomada por el jugador Negro.",
        "También se encontró que Black promedió 2.8 acciones preventivas por juego en los juegos en los que perdió, mientras que promedió 1.5 acciones preventivas por juego al ganar.",
        "Parece que Black necesita 1 o 2 acciones preventivas para construir su posición inicial de toma, antes de comenzar a presentar amenazas.",
        "Si no logra ganar, generalmente evitará una amenaza adicional o dos antes de sucumbir ante el Blanco. 4.",
        "TRABAJO RELACIONADO Mucha investigación se ocupa de la axiomatización del trabajo en equipo y los estados mentales de los individuos: algunos modelos utilizan el conocimiento y la creencia [10], otros tienen modelos de metas e intenciones [8, 4].",
        "Sin embargo, todas estas teorías formales tratan sobre el trabajo en equipo y la cooperación de agentes.",
        "Hasta donde sabemos, nuestro modelo es el primero en proporcionar un modelo formalizado para entornos adversarios explícitos y el comportamiento de los agentes en él.",
        "El algoritmo de búsqueda adversarial clásico Min-Max fue el primer intento de integrar al oponente en el espacio de búsqueda con la suposición débil de un oponente que juega de manera óptima.",
        "Desde entonces, se ha dedicado mucho esfuerzo a integrar el modelo del oponente en el procedimiento de toma de decisiones para predecir el comportamiento futuro.",
        "El algoritmo M∗ presentado por Carmel y Markovitch [2] mostró un método para incorporar modelos de oponentes en la búsqueda de adversarios, mientras que en [3] utilizaron el aprendizaje para proporcionar un modelo de oponente más preciso en un entorno de juego repetido de 2 jugadores, donde las estrategias de los agentes se modelaron como autómatas finitos.",
        "El trabajo adicional de planificación adversarial fue realizado por Willmott y colaboradores [13], quienes proporcionaron un enfoque de planificación adversarial para el juego de GO.",
        "La investigación mencionada anteriormente trató sobre la búsqueda adversarial y la integración de modelos de oponentes en métodos de búsqueda de utilidad clásicos.",
        "Ese trabajo muestra la importancia del modelado del oponente y la capacidad de explotarlo en beneficio de un agente.",
        "Sin embargo, las limitaciones básicas de esos métodos de búsqueda siguen aplicándose; nuestro modelo intenta superar esas limitaciones presentando un modelo formal para una nueva especificación adversarial basada en estados mentales. 5.",
        "CONCLUSIONES Presentamos un modelo de Entorno Adversarial para un agente racional acotado que está situado en un entorno de N jugadores, de suma cero. Estos fueron posteriormente eliminados del análisis final.",
        "Utilizamos la formalización de SharedPlans para definir el modelo y los axiomas que los agentes pueden aplicar como pautas de comportamiento.",
        "El modelo está destinado a ser utilizado como una guía para diseñar agentes que necesitan operar en entornos adversos.",
        "Presentamos resultados empíricos, basados en el análisis del archivo de registro de ConnectFour, que ejemplifican el modelo y los axiomas para una instancia bilateral del entorno.",
        "Los resultados que presentamos son un primer paso hacia un modelo ampliado que abarcará todos los tipos de entornos adversarios, por ejemplo, entornos que no son de suma cero y entornos que contienen agentes naturales que no forman parte del conflicto directo.",
        "Esos desafíos y más serán abordados en futuras investigaciones. 6.",
        "AGRADECIMIENTO Esta investigación fue apoyada en parte por las becas de la Fundación de Ciencia de Israel #1211/04 y #898/05. 7.",
        "REFERENCIAS [1] L. V. Allis.",
        "Un enfoque basado en el conocimiento de Conecta Cuatro: el juego está resuelto, gana el jugador Blanco.",
        "Tesis de maestría, Universidad Libre de Ámsterdam, Países Bajos, 1988. [2] D. Carmel y S. Markovitch.",
        "Incorporando modelos de oponentes en la búsqueda de adversarios.",
        "En Actas de la Decimotercera Conferencia Nacional de Inteligencia Artificial, páginas 120-125, Portland, Oregón, 1996. [3] D. Carmel y S. Markovitch.",
        "Modelado de oponentes en sistemas multiagente.",
        "En G. Weiß y S. Sen, editores, Adaptación y Aprendizaje en Sistemas Multiagente, páginas 40-52.",
        "Springer-Verlag, 1996. [4] B. J. Grosz y S. Kraus.",
        "Planes colaborativos para acciones grupales complejas.",
        "Inteligencia Artificial, 86(2):269-357, 1996. [5] M. Hadad, G. Kaminka, G. Armon y S. Kraus.",
        "Apoyando la actividad colaborativa.",
        "En Proc. de AAAI-2005, páginas 83-88, Pittsburgh, 2005. [6] http://www.gamerz.net/˜pbmserv/. [7] S. Kraus y D. Lehmann.",
        "Diseñando y construyendo un agente automatizado de negociación.",
        "Inteligencia Computacional, 11:132-171, 1995. [8] H. J. Levesque, P. R. Cohen y J. H. T. Nunes.",
        "Actuar juntos.",
        "En Proc. de AAAI-90, páginas 94-99, Boston, MA, 1990. [9] S. Markovitch y R. Reger.",
        "Aprender y aprovechar las debilidades relativas de los agentes oponentes.",
        "Agentes Autónomos y Sistemas Multiagente, 10(2):103-130, 2005. [10] Y. M. Ronald Fagin, Joseph Y. Halpern y M. Y. Vardi.",
        "Razonamiento sobre el conocimiento.",
        "MIT Press, Cambridge, Mass., 1995. [11] P. Thagard.\nMIT Press, Cambridge, Massachusetts, 1995. [11] P. Thagard.",
        "Resolución de problemas adversariales: Modelado de un oponente utilizando coherencia explicativa.",
        "Ciencia Cognitiva, 16(1):123-149, 1992. [12] R. W. Toseland y R. F. Rivas.",
        "Una introducción a la práctica del trabajo en grupo.",
        "Prentice Hall, Englewood Cliffs, NJ, segunda edición, 1995. [13] S. Willmott, J. Richardson, A. Bundy y J. Levine.",
        "Un enfoque de planificación adversarial para el juego de Go.",
        "Notas de conferencia en Ciencias de la Computación, 1558:93-112, 1999.",
        "El Sexto Internacional.",
        "Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 557"
    ],
    "error_count": 5,
    "keys": {
        "multiagent environment": {
            "translated_key": "entorno multiagente",
            "is_in_text": false,
            "original_annotated_sentences": [
                "An Adversarial Environment Model for Bounded Rational Agents in Zero-Sum Interactions Inon Zuckerman1 , Sarit Kraus1 , Jeffrey S. Rosenschein2 , Gal Kaminka1 1 Department of Computer Science 2 The School of Engineering Bar-Ilan University and Computer Science Ramat-Gan, Israel Hebrew University, Jerusalem, Israel {zukermi,sarit,galk}@cs.biu.ac.il jeff@cs.huji.ac.il ABSTRACT Multiagent environments are often not cooperative nor collaborative; in many cases, agents have conflicting interests, leading to adversarial interactions.",
                "This paper presents a formal Adversarial Environment model for bounded rational agents operating in a zero-sum environment.",
                "In such environments, attempts to use classical utility-based search methods can raise a variety of difficulties (e.g., implicitly modeling the opponent as an omniscient utility maximizer, rather than leveraging a more nuanced, explicit opponent model).",
                "We define an Adversarial Environment by describing the mental states of an agent in such an environment.",
                "We then present behavioral axioms that are intended to serve as design principles for building such adversarial agents.",
                "We explore the application of our approach by analyzing log files of completed Connect-Four games, and present an empirical analysis of the axioms appropriateness.",
                "Categories and Subject Descriptors I.2.11 [Artificial Intelligence]: Distributed Artificial Intelligence-Intelligent agents,Multiagent Systems; I.2.4 [Artificial Intelligence]: Knowledge Representation Formalisms and Methods -Modal logic General Terms Design, Theory 1.",
                "INTRODUCTION Early research in multiagent systems (MAS) considered cooperative groups of agents; because individual agents had limited resources, or limited access to information (e.g., limited processing power, limited sensor coverage), they worked together by design to solve problems that individually they could not solve, or at least could not solve as efficiently.",
                "MAS research, however, soon began to consider interacting agents with individuated interests, as representatives of different humans or organizations with non-identical interests.",
                "When interactions are guided by diverse interests, participants may have to overcome disagreements, uncooperative interactions, and even intentional attempts to damage one another.",
                "When these types of interactions occur, environments require appropriate behavior from the agents situated in them.",
                "We call these environments Adversarial Environments, and call the clashing agents Adversaries.",
                "Models of cooperation and teamwork have been extensively explored in MAS through the axiomatization of mental states (e.g., [8, 4, 5]).",
                "However, none of this research dealt with adversarial domains and their implications for agent behavior.",
                "Our paper addresses this issue by providing a formal, axiomatized mental state model for a subset of adversarial domains, namely simple zero-sum adversarial environments.",
                "Simple zero-sum encounters exist of course in various twoplayer games (e.g., Chess, Checkers), but they also exist in n-player games (e.g., Risk, Diplomacy), auctions for a single good, and elsewhere.",
                "In these latter environments especially, using a utility-based adversarial search (such as the Min-Max algorithm) does not always provide an adequate solution; the payoff function might be quite complex or difficult to quantify, and there are natural computational limitations on bounded rational agents.",
                "In addition, traditional search methods (like Min-Max) do not make use of a model of the opponent, which has proven to be a valuable addition to adversarial planning [9, 3, 11].",
                "In this paper, we develop a formal, axiomatized model for bounded rational agents that are situated in a zero-sum adversarial environment.",
                "The model uses different modality operators, and its main foundations are the SharedPlans [4] model for collaborative behavior.",
                "We explore environment properties and the mental states of agents to derive behavioral axioms; these behavioral axioms constitute a formal model that serves as a specification and design guideline for agent design in such settings.",
                "We then investigate the behavior of our model empirically using the Connect-Four board game.",
                "We show that this game conforms to our environment definition, and analyze players behavior using a large set of completed match log 550 978-81-904262-7-5 (RPS) c 2007 IFAAMAS files.",
                "In addition, we use the results presented in [9] to discuss the importance of opponent modeling in our ConnectFour adversarial domain.",
                "The paper proceeds as follows.",
                "Section 2 presents the models formalization.",
                "Section 3 presents the empirical analysis and its results.",
                "We discuss related work in Section 4, and conclude and present future directions in Section 5. 2.",
                "ADVERSARIAL ENVIRONMENTS The adversarial environment model (denoted as AE) is intended to guide the design of agents by providing a specification of the capabilities and mental attitudes of an agent in an adversarial environment.",
                "We focus here on specific types of adversarial environments, specified as follows: 1.",
                "Zero-Sum Interactions: positive and negative utilities of all agents sum to zero; 2.",
                "Simple AEs: all agents in the environment are adversarial agents; 3.",
                "Bilateral AEs: AEs with exactly two agents; 4.",
                "Multilateral AEs: AEs of three or more agents.",
                "We will work on both bilateral and multilateral instantiations of zero-sum and simple environments.",
                "In particular, our adversarial environment model will deal with interactions that consist of N agents (N ≥ 2), where all agents are adversaries, and only one agent can succeed.",
                "Examples of such environments range from board games (e.g., Chess, Connect-Four, and Diplomacy) to certain economic environments (e.g., N-bidder auctions over a single good). 2.1 Model Overview Our approach is to formalize the mental attitudes and behaviors of a single adversarial agent; we consider how a single agent perceives the AE.",
                "The following list specifies the conditions and mental states of an agent in a simple, zero-sum AE: 1.",
                "The agent has an individual intention that its own goal will be completed; 2.",
                "The agent has an individual belief that it and its adversaries are pursuing full conflicting goals (defined below)there can be only one winner; 3.",
                "The agent has an individual belief that each adversary has an intention to complete its own full conflicting goal; 4.",
                "The agent has an individual belief in the (partial) profile of its adversaries.",
                "Item 3 is required, since it might be the case that some agent has a full conflicting goal, and is currently considering adopting the intention to complete it, but is, as of yet, not committed to achieving it.",
                "This might occur because the agent has not yet deliberated about the effects that adopting that intention might have on the other intentions it is currently holding.",
                "In such cases, it might not consider itself to even be in an adversarial environment.",
                "Item 4 states that the agent should hold some belief about the profiles of its adversaries.",
                "The profile represents all the knowledge the agent has about its adversary: its weaknesses, strategic capabilities, goals, intentions, trustworthiness, and more.",
                "It can be given explicitly or can be learned from observations of past encounters. 2.2 Model Definitions for Mental States We use Grosz and Krauss definitions of the modal operators, predicates, and meta-predicates, as defined in their SharedPlan formalization [4].",
                "We recall here some of the predicates and operators that are used in that formalization: Int.To(Ai, α, Tn, Tα, C) represents Ais intentions at time Tn to do an action α at time Tα in the context of C. Int.Th(Ai, prop, Tn, Tprop, C) represents Ais intentions at time Tn that a certain proposition prop holds at time Tprop in the context of C. The potential intention operators, Pot.Int.To(...) and Pot.Int.Th(...), are used to represent the mental state when an agent considers adopting an intention, but has not deliberated about the interaction of the other intentions it holds.",
                "The operator Bel(Ai, f, Tf ) represents agent Ai believing in the statement expressed in formula f, at time Tf .",
                "MB(A, f, Tf ) represents mutual belief for a group of agents A.",
                "A snapshot of the system finds our environment to be in some state e ∈ E of environmental variable states, and each adversary in any LAi ∈ L of possible local states.",
                "At any given time step, the system will be in some world w of the set of all possible worlds w ∈ W, where w = E×LA1 ×LA2 × ...LAn , and n is the number of adversaries.",
                "For example, in a Texas Holdem poker game, an agents local state might be its own set of cards (which is unknown to its adversary) while the environment will consist of the betting pot and the community cards (which are visible to both players).",
                "A utility function under this formalization is defined as a mapping from a possible world w ∈ W to an element in , which expresses the desirability of the world, from a single agent perspective.",
                "We usually normalize the range to [0,1], where 0 represents the least desirable possible world, and 1 is the most desirable world.",
                "The implementation of the utility function is dependent on the domain in question.",
                "The following list specifies new predicates, functions, variables, and constants used in conjunction with the original definitions for the adversarial environment formalization: 1. φ is a null action (the agent does not do anything). 2.",
                "GAi is the set of agent Ais goals.",
                "Each goal is a set of predicates whose satisfaction makes the goal complete (we use G∗ Ai ∈ GAi to represent an arbitrary goal of agent Ai). 3. gAi is the set of agent Ais subgoals.",
                "Subgoals are predicates whose satisfaction represents an important milestone toward achievement of the full goal. gG∗ Ai ⊆ gAi is the set of subgoals that are important to the completion of goal G∗ Ai (we will use g∗ G∗ Ai ∈ gG∗ Ai to represent an arbitrary subgoal). 4.",
                "P Aj Ai is the profile object agent Ai holds about agent Aj. 5.",
                "CA is a general set of actions for all agents in A which are derived from the environments constraints.",
                "CAi ⊆ CA is the set of agent Ais possible actions. 6.",
                "Do(Ai, α, Tα, w) holds when Ai performs action α over time interval Tα in world w. 7.",
                "Achieve(G∗ Ai , α, w) is true when goal G∗ Ai is achieved following the completion of action α in world w ∈ W, where α ∈ CAi . 8.",
                "Profile(Ai, PAi Ai ) is true when agent Ai holds an object profile for agent Aj.",
                "Definition 1.",
                "Full conflict (FulConf ) describes a zerosum interaction where only a single goal of the goals in conflict can be completed.",
                "FulConf(G∗ Ai , G∗ Aj ) ⇒ (∃α ∈ CAi , ∀w, β ∈ CAj ) (Achieve(G∗ Ai , α, w) ⇒ ¬Achieve(G∗ Aj , β, w)) ∨ (∃β ∈ CAj , ∀w, α ∈ CAi )(Achieve(G∗ Aj , β, w) ⇒ ¬Achieve(G∗ Ai , α, w)) Definition 2.",
                "Adversarial Knowledge (AdvKnow) is a function returning a value which represents the amount of The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 551 knowledge agent Ai has on the profile of agent Aj, at time Tn.",
                "The higher the value, the more knowledge agent Ai has.",
                "AdvKnow : P Aj Ai × Tn → Definition 3.",
                "Eval - This evaluation function returns an estimated expected utility value for an agent in A, after completing an action from CA in some world state w. Eval : A × CA × w → Definition 4.",
                "TrH - (Threshold) is a numerical constant in the [0,1] range that represents an evaluation function (Eval) threshold value.",
                "An action that yields an estimated utility evaluation above the TrH is regarded as a highly beneficial action.",
                "The Eval value is an estimation and not the real utility function, which is usually unknown.",
                "Using the real utility value for a rational agent would easily yield the best outcome for that agent.",
                "However, agents usually do not have the real utility functions, but rather a heuristic estimate of it.",
                "There are two important properties that should hold for the evaluation function: Property 1.",
                "The evaluation function should state that the most desirable world state is one in which the goal is achieved.",
                "Therefore, after the goal has been satisfied, there can be no future action that can put the agent in a world state with higher Eval value. (∀Ai, G∗ Ai , α, β ∈ CAi , w ∈ W) Achieve(G∗ Ai , α, w) ⇒ Eval(Ai, α, w) ≥ Eval(Ai, β, w) Property 2.",
                "The evaluation function should project an action that causes a completion of a goal or a subgoal to a value which is greater than TrH (a highly beneficial action). (∀Ai, G∗ Ai ∈ GAi , α ∈ CAi , w ∈ W, g∗ GAi ∈ gGAi ) Achieve(G∗ Ai , α, w) ∨ Achieve(g∗ GAi , α, w) ⇒ Eval(Ai, α, w) ≥ TrH.",
                "Definition 5.",
                "SetAction We define a set action (SetAction) as a set of action operations (either complex or basic actions) from some action sets CAi and CAj which, according to agent Ais belief, are attached together by a temporal and consequential relationship, forming a chain of events (action, and its following consequent action). (∀α1 , . . . , αu ∈ CAi , β1 , . . . , βv ∈ CAj , w ∈ W) SetAction(α1 , . . . , αu , β1 , . . . , βv , w) ⇒ ((Do(Ai, α1 , Tα1 , w) ⇒ Do(Aj, β1 , Tβ1 , w)) ⇒ Do(Ai, α2 , Tα2 , w) ⇒ . . . ⇒ Do(Ai, αu , Tαu , w)) The consequential relation might exist due to various environmental constraints (when one action forces the adversary to respond with a specific action) or due to the agents knowledge about the profile of its adversary.",
                "Property 3.",
                "As the knowledge we have about our adversary increases we will have additional beliefs about its behavior in different situations which in turn creates new set actions.",
                "Formally, if our AdvKnow at time Tn+1 is greater than AdvKnow at time Tn, then every SetAction known at time Tn is also known at time Tn+1.",
                "AdvKnow(P Aj Ai , Tn+1) > AdvKnow(P Aj Ai , Tn) ⇒ (∀α1 , . . . , αu ∈ CAi , β1 , . . . , βv ∈ CAj ) Bel(Aag, SetAction(α1 , . . . , αu , β1 , . . . , βv ), Tn) ⇒ Bel(Aag, SetAction(α1 , . . . , αu , β1 , . . . , βv ), Tn+1) 2.3 The Environment Formulation The following axioms provide the formal definition for a simple, zero-sum Adversarial Environment (AE).",
                "Satisfaction of these axioms means that the agent is situated in such an environment.",
                "It provides specifications for agent Aag to interact with its set of adversaries A with respect to goals G∗ Aag and G∗ A at time TCo at some world state w. AE(Aag, A, G∗ Aag , A1, . . . , Ak, G∗ A1 , . . . , G∗ Ak , Tn, w) 1.",
                "Aag has an Int.Th his goal would be completed: (∃α ∈ CAag , Tα) Int.Th(Aag, Achieve(G∗ Aag , α), Tn, Tα, AE) 2.",
                "Aag believes that it and each of its adversaries Ao are pursuing full conflicting goals: (∀Ao ∈ {A1, . . . , Ak}) Bel(Aag, FulConf(G∗ Aag , G∗ Ao ), Tn) 3.",
                "Aag believes that each of his adversaries in Ao has the Int.Th his conflicting goal G∗ Aoi will be completed: (∀Ao ∈ {A1, . . . , Ak})(∃β ∈ CAo , Tβ) Bel(Aag, Int.Th(Ao, Achieve(G∗ Ao , β), TCo, Tβ, AE), Tn) 4.",
                "Aag has beliefs about the (partial) profiles of its adversaries (∀Ao ∈ {A1, . . . , Ak}) (∃PAo Aag ∈ PAag )Bel(Aag, Profile(Ao, PAo Aag ), Tn) To build an agent that will be able to operate successfully within such an AE, we must specify behavioral guidelines for its interactions.",
                "Using a naive Eval maximization strategy to a certain search depth will not always yield satisfactory results for several reasons: (1) the search horizon problem when searching for a fixed depth; (2) the strong assumption of an optimally rational, unbounded resources adversary; (3) using an estimated evaluation function which will not give optimal results in all world states, and can be exploited [9].",
                "The following axioms specify the behavioral principles that can be used to differentiate between successful and less successful agents in the above Adversarial Environment.",
                "Those axioms should be used as specification principles when designing and implementing agents that should be able to perform well in such Adversarial Environments.",
                "The behavioral axioms represent situations in which the agent will adopt potential intentions to (Pot.Int.To(...)) perform an action, which will typically require some means-end reasoning to select a possible course of action.",
                "This reasoning will lead to the adoption of an Int.To(...) (see [4]).",
                "A1.",
                "Goal Achieving Axiom.",
                "The first axiom is the simplest case; when the agent Aag believes that it is one action (α) away from achieving his conflicting goal G∗ Aag , it should adopt the potential intention to do α and complete its goal. (∀Aag, α ∈ CAag , Tn, Tα, w ∈ W) (Bel(Aag, Do(Aag, α, Tα, w) ⇒ Achieve(G∗ Aag , α, w)) ⇒ Pot.Int.To(Aag, α, Tn, Tα, w) This somewhat trivial behavior is the first and strongest axiom.",
                "In any situation, when the agent is an action away from completing the goal, it should complete the action.",
                "Any fair Eval function would naturally classify α as the maximal value action (property 1).",
                "However, without explicit axiomatization of such behavior there might be situations where the agent will decide on taking another action for various reasons, due to its bounded decision resources.",
                "A2.",
                "Preventive Act Axiom.",
                "Being in an adversarial situation, agent Aag might decide to take actions that will damage one of its adversarys plans to complete its goal, even if those actions do not explicitly advance Aag towards its conflicting goal G∗ Aag .",
                "Such preventive action will take place when agent Aag has a belief about the possibility of its adversary Ao doing an action β that will give it a high 552 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) utility evaluation value (> TrH).",
                "Believing that taking action α will prevent the opponent from doing its β, it will adopt a potential intention to do α. (∀Aag, Ao ∈ A, α ∈ CAag , β ∈ CAo , Tn, Tβ, w ∈ W) (Bel(Aag, Do(Ao, β, Tβ, w) ∧ Eval(Ao, β, w) > TrH, Tn) ∧ Bel(Aag, Do(Aag, α, Tα, w) ⇒ ¬Do(Ao, β, Tβ, w), Tn) ⇒ Pot.Int.To(Aag, α, Tn, Tα, w) This axiom is a basic component of any adversarial environment.",
                "For example, looking at a Chess board game, a player could realize that it is about to be checkmated by its opponent, thus making a preventive move.",
                "Another example is a Connect Four game: when a player has a row of three chips, its opponent must block it, or lose.",
                "A specific instance of A1 occurs when the adversary is one action away from achieving its goal, and immediate preventive action needs to be taken by the agent.",
                "Formally, we have the same beliefs as stated above, with a changed belief that doing action β will cause agent Ao to achieve its goal.",
                "Proposition 1: Prevent or lose case. (∀Aag, Ao ∈ A, α ∈ CAag , β ∈ CAo , G∗ Ao , Tn, Tα, Tβ, w ∈ W) Bel(Aag, Do(Ao, β, Tβ, w) ⇒ Achieve(G∗ Ao , β, w), Tn) ∧ Bel(Aag, Do(Aag, α, Tα, w) ⇒ ¬Do(Ao, β, Tβ, w)) ⇒ Pot.Int.To(Aag, α, Tn, Tα, w) Sketch of proof: Proposition 1 can be easily derived from axiom A1 and the property 2 of the Eval function, which states that any action that causes a completion of a goal is a highly beneficial action.",
                "The preventive act behavior will occur implicitly when the Eval function is equal to the real world utility function.",
                "However, being bounded rational agents and dealing with an estimated evaluation function we need to explicitly axiomatize such behavior, for it will not always occur implicitly from the evaluation function.",
                "A3.",
                "Suboptimal Tactical Move Axiom.",
                "In many scenarios a situation may occur where an agent will decide not to take the current most beneficial action it can take (the action with the maximal utility evaluation value), because it believes that taking another action (with lower utility evaluation value) might yield (depending on the adversarys response) a future possibility for a highly beneficial action.",
                "This will occur most often when the Eval function is inaccurate and differs by a large extent from the Utility function.",
                "Put formally, agent Aag believes in a certain SetAction that will evolve according to its initial action and will yield a high beneficial value (> TrH) solely for it. (∀Aag, Ao ∈ A, Tn, w ∈ W) (∃α1 , . . . , αu ∈ CAi , β1 , . . . , βv ∈ CAj , Tα1 ) Bel(Aag, SetAction(α1 , . . . , αu , β1 , . . . , βv ), Tn) ∧ Bel(Aag, Eval(Ao, βv , w) < TrH < Eval(Aag, αu , w), Tn) ⇒ Pot.Int.To(Aag, α1 , Tn, Tα1 , w) An agent might believe that a chain of events will occur for various reasons due to the inevitable nature of the domain.",
                "For example, in Chess, we often observe the following: a move causes a check position, which in turn limits the opponents moves to avoiding the check, to which the first player might react with another check, and so on.",
                "The agent might also believe in a chain of events based on its knowledge of its adversarys profile, which allows it to foresee the adversarys movements with high accuracy.",
                "A4.",
                "Profile Detection Axiom.",
                "The agent can adjust its adversarys profiles by observations and pattern study (specifically, if there are repeated encounters with the same adversary).",
                "However, instead of waiting for profile information to be revealed, an agent can also initiate actions that will force its adversary to react in a way that will reveal profile knowledge about it.",
                "Formally, the axiom states that if all actions (γ) are not highly beneficial actions (< TrH), the agent can do action α in time Tα if it believes that it will result in a non-highly beneficial action β from its adversary, which in turn teaches it about the adversarys profile, i.e., gives a higher AdvKnow(P Aj Ai , Tβ). (∀Aag, Ao ∈ A, α ∈ CAag , β ∈ CAo , Tn, Tα, Tβ, w ∈ W) Bel(Aag, (∀γ ∈ CAag )Eval(Aag, γ, w) < TrH, Tn) ∧ Bel(Aag, Do(Aag, α, Tα, w) ⇒ Do(Ao, β, Tβ, w), Tn) ∧ Bel(Aag, Eval(Ao, β, w) < TrH) ∧ Bel(Aag, AdvKnow(P Aj Ai , Tβ) > AdvKnow(P Aj Ai , Tn), Tn) ⇒ Pot.Int.To(Aag, α, Tn, Tα, w) For example, going back to the Chess board game scenario, consider starting a game versus an opponent about whom we know nothing, not even if it is a human or a computerized opponent.",
                "We might start playing a strategy that will be suitable versus an average opponent, and adjust our game according to its level of play.",
                "A5.",
                "Alliance Formation Axiom The following behavioral axiom is relevant only in a multilateral instantiation of the adversarial environment (obviously, an alliance cannot be formed in a bilateral, zero-sum encounter).",
                "In different situations during a multilateral interaction, a group of agents might believe that it is in their best interests to form a temporary alliance.",
                "Such an alliance is an agreement that constrains its members behavior, but is believed by its members to enable them to achieve a higher utility value than the one achievable outside of the alliance.",
                "As an example, we can look at the classical Risk board game, where each player has an individual goal of being the sole conquerer of the world, a zero-sum game.",
                "However, in order to achieve this goal, it might be strategically wise to make short-term ceasefire agreements with other players, or to join forces and attack an opponent who is stronger than the rest.",
                "An alliances terms defines the way its members should act.",
                "It is a set of predicates, denoted as Terms, that is agreed upon by the alliance members, and should remain true for the duration of the alliance.",
                "For example, the set Terms in the Risk scenario, could contain the following predicates: 1.",
                "Alliance members will not attack each other on territories X, Y and Z; 2.",
                "Alliance members will contribute C units per turn for attacking adversary Ao; 3.",
                "Members are obligated to stay as part of the alliance until time Tk or until adversarys Ao army is smaller than Q.",
                "The set Terms specifies inter-group constraints on each of the alliance members (∀Aal i ∈ Aal ⊆ A) set of actions Cal i ⊆ C. Definition 6.",
                "Al val - the total evaluation value that agent Ai will achieve while being part of Aal is the sum of Evali (Eval for Ai) of each of Aal j Eval values after taking their own α actions (via the agent(α) predicate): Al val(Ai, Cal , Aal , w) = α∈Cal Evali(Aal j , agent(α), w) Definition 7.",
                "Al TrH - is a number representing an Al val The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 553 threshold; above it, the alliance can be said to be a highly beneficial alliance.",
                "The value of Al TrH will be calculated dynamically according to the progress of the interaction, as can be seen in [7].",
                "After an alliance is formed, its members are now working in their normal adversarial environment, as well as according to the mental states and axioms required for their interactions as part of the alliance.",
                "The following Alliance model (AL) specifies the conditions under which the group Aal can be said to be in an alliance and working with a new and constrained set of actions Cal , at time Tn.",
                "AL(Aal , Cal , w, Tn) 1.",
                "Aal has a MB that all members are part of Aal : MB(Aal , (∀Aal i ∈ Aal )member(Aal i , Aal ), Tn) 2.",
                "Aal has a MB that the group be maintained: MB(Aal , (∀Aal i ∈ Aal )Int.Th (Ai, member(Ai, Aal ), Tn, Tn+1, Co), Tn) 3.",
                "Aal has a MB that being members gives them high utility value: MB(Aal , (∀Aal i ∈ Aal )Al val(Aal i , Cal , Aal , w) ≥ Al TrH, Tn) Members profiles are a crucial part of successful alliances.",
                "We assume that agents that have more accurate profiles of their adversaries will be more successful in such environments.",
                "Such agents will be able to predict when a member is about to breach the alliances contract (item 2 in the above model), and take counter measures (when item 3 will falsify).",
                "The robustness of the alliance is in part a function of its members trustfulness measure, objective position estimation, and other profile properties.",
                "We should note that an agent can simultaneously be part of more than one alliance.",
                "Such a temporary alliance, where the group members do not have a joint goal but act collaboratively for the interest of their own individual goals, is classified as a Treatment Group by modern psychologists [12] (in contrast to a Task Group, where its members have a joint goal).",
                "The Shared Activity model as presented in [5] modeled Treatment Group behavior using the same SharedPlans formalization.",
                "When comparing both definitions of an alliance and a Treatment Group we found an unsurprising resemblance between both models: the environment models definitions are almost identical (see SAs definitions in [5]), and their Selfish-Act and Cooperative Act axioms conform to our adversarial agents behavior.",
                "The main distinction between both models is the integration of a Helpful-behavior act axiom, in the Shared Activity which cannot be part of ours.",
                "This axiom states that an agent will consider taking action that will lower its Eval value (to a certain lower bound), if it believes that a group partner will gain a significant benefit.",
                "Such behavior cannot occur in a pure adversarial environment (as a zero-sum game is), where the alliance members are constantly on watch to manipulate their alliance to their own advantage.",
                "A6.",
                "Evaluation Maximization Axiom.",
                "In a case when all other axioms are inapplicable, we will proceed with the action that maximizes the heuristic value as computed in the Eval function. (∀Aag, Ao ∈ A, α ∈ Cag, Tn, w ∈ W) Bel(Aag, (∀γ ∈ Cag)Eval(Aag, α, w) ≥ Eval(Aag, γ, w), Tn) ⇒ Pot.Int.To(Aag, α, Tn, Tα, w) T1.",
                "Optimality on Eval = Utility The above axiomatic model handles situations where the Utility is unknown and the agents are bounded rational agents.",
                "The following theorem shows that in bilateral interactions, where the agents have the real Utility function (i.e., Eval = Utility) and are rational agents, the axioms provide the same optimal result as classic adversarial search (e.g., Min-Max).",
                "Theorem 1.",
                "Let Ae ag be an unbounded rational AE agent using the Eval heuristic evaluation function, Au ag be the same agent using the true Utility function, and Ao be a sole unbounded utility-based rational adversary.",
                "Given that Eval = Utility: (∀α ∈ CAu ag , α ∈ CAe ag , Tn, w ∈ W) Pot.Int.To(Au ag, α, Tn, Tα, w) → Pot.Int.To(Ae ag, α , Tn, Tα, w) ∧ ((α = α ) ∨ (Utility(Au ag, α, w) = Eval(Ae ag, α , w))) Sketch of proof - Given that Au ag has the real utility function and unbounded resources, it can generate the full game tree and run the optimal MinMax algorithm to choose the highest utility value action, which we denote by, α.",
                "The proof will show that Ae ag, using the AE axioms, will select the same or equal utility α (when there is more than one action with the same max utility) when Eval = Utility. (A1) Goal achieving axiom - suppose there is an α such that its completion will achieve Au ags goal.",
                "It will obtain the highest utility by Min-Max for Au ag.",
                "The Ae ag agent will select α or another action with the same utility value via A1.",
                "If such α does not exist, Ae ag cannot apply this axiom, and proceeds to A2. (A2) Preventive act axiom - (1) Looking at the basic case (see Prop1), if there is a β which leads Ao to achieve its goal, then a preventive action α will yield the highest utility for Au ag.",
                "Au ag will choose it through the utility, while Ae ag will choose it through A2. (2) In the general case, β is a highly beneficial action for Ao, thus yields low utility for Au ag, which will guide it to select an α that will prevent β, while Ae ag will choose it through A2.1 If such β does not exist for Ao, then A2 is not applicable, and Ae ag can proceed to A3. (A3) Suboptimal tactical move axiom - When using a heuristic Eval function, Ae ag has a partial belief in the profile of its adversary (item 4 in AE model), which may lead it to believe in SetActions (Prop1).",
                "In our case, Ae ag is holding a full profile on its optimal adversary and knows that Ao will behave optimally according to the real utility values on the complete search tree, therefore, any belief about suboptimal SetAction cannot exist, yielding this axiom inapplicable.",
                "Ae ag will proceed to A4. (A4) Profile detection axiom - Given that Ae ag has the full profile of Ao, none of Ae ags actions can increase its knowledge.",
                "That axiom will not be applied, and the agent will proceed with A6 (A5 will be disregarded because the interaction is bilateral). (A6) Evaluation maximization axiom - This axiom will select the max Eval for Ae ag.",
                "Given that Eval = Utility, the same α that was selected by Au ag will be selected. 3.",
                "EVALUATION The main purpose of our experimental analysis is to evaluate the models behavior and performance in a real adversarial environment.",
                "This section investigates whether bounded 1 A case where following the completion of β there exists a γ which gives high utility for Agent Au ag, cannot occur because Ao uses the same utility, and γs existence will cause it to classify β as a low utility action. 554 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) rational agents situated in such adversarial environments will be better off applying our suggested behavioral axioms. 3.1 The Domain To explore the use of the above model and its behavioral axioms, we decided to use the Connect-Four game as our adversarial environment.",
                "Connect-Four is a 2-player, zerosum game which is played using a 6x7 matrix-like board.",
                "Each turn, a player drops a disc into one of the 7 columns (the set of 21 discs is usually colored yellow for player 1 and red for player 2; we will use White and Black respectively to avoid confusion).",
                "The winner is the first player to complete a horizontal, vertical, or diagonal set of four discs with its color.",
                "On very rare occasions, the game might end in a tie if all the empty grids are filled, but no player managed to create a 4-disc set.",
                "The Connect-Four game was solved in [1], where it is shown that the first player (playing with the white discs) can force a win by starting in the middle column (column 4) and playing optimally However, the optimal strategy is very complex, and difficult to follow even for complex bounded rational agents, such as human players.",
                "Before we can proceed checking agent behavior, we must first verify that the domain conforms to the adversarial environments definition as given above (which the behavioral axioms are based on).",
                "First, when playing a Connect-Four game, the agent has an intention to win the game (item 1).",
                "Second (item 2), our agent believes that in Connect-Four there can only be one winner (or no winner at all in the rare occurrence of a tie).",
                "In addition, our agent believes that its opponent to the game will try to win (item 3), and we hope it has some partial knowledge (item 4) about its adversary (this knowledge can vary from nothing, through simple facts such as age, to strategies and weaknesses).",
                "Of course, not all Connect-Four encounters are adversarial.",
                "For example, when a parent is playing the game with its child, the following situation might occur: the child, having a strong incentive to win, treats the environment as adversarial (it intends to win, understands that there can only be one winner, and believes that its parent is trying to beat him).",
                "However, the parents point of view might see the environment as an educational one, where its goal is not to win the game, but to cause enjoyment or practice strategic reasoning.",
                "In such an educational environment, a new set of behavioral axioms might be more beneficial to the parents goals than our suggested adversarial behavioral axioms. 3.2 Axiom Analysis After showing that the Connect-Four game is indeed a zero-sum, bilateral adversarial environment, the next step is to look at players behaviors during the game and check whether behaving according to our model does improve performance.",
                "To do so we have collected log files from completed Connect-Four games that were played by human players over the Internet.",
                "Our collected log file data came from Play by eMail (PBeM) sites.",
                "These are web sites that host email games, where each move is taken by an email exchange between the server and the players.",
                "Many such sites archives contain real competitive interactions, and also maintain a ranking system for their members.",
                "Most of the data we used can be found in [6].",
                "As can be learned from [1], Connect-Four has an optimal strategy and a considerable advantage for the player who starts the game (which we call the White player).",
                "We will concentrate in our analysis on the second players moves (to be called Black).",
                "The White player, being the first to act, has the so-called initiative advantage.",
                "Having the advantage and a good strategy will keep the Black player busy reacting to its moves, instead of initiating threats.",
                "A threat is a combination of three discs of the same color, with an empty spot for the fourth winning disk.",
                "An open threat is a threat that can be realized in the opponents next move.",
                "In order for the Black player to win, it must somehow turn the tide, take the advantage and start presenting threats to the White player.",
                "We will explore Black players behavior and their conformance to our axioms.",
                "To do so, we built an application that reads log files and analyzes the Black players moves.",
                "The application contains two main components: (1) a Min-Max algorithm for evaluation of moves; (2) open threats detector for the discovering of open threats.",
                "The Min-Max algorithm will work to a given depth, d and for each move α will output the heuristic value for the next action taken by the player as written in the log file, h(α), alongside the maximum heuristic value, maxh(α), that could be achieved prior to taking the move (obviously, if h(α) = maxh(α), then the player did not do the optimal move heuristically).",
                "The threat detectors job is to notify if some action was taken in order to block an open threat (not blocking an open threat will probably cause the player to lose in the opponents next move).",
                "The heuristic function used by Min-Max to evaluate the players utility is the following function, which is simple to compute, yet provides a reasonable challenge to human opponents: Definition 8.",
                "Let Group be an adjacent set of four squares that are horizontal, vertical, or diagonal.",
                "Groupn b (Groupn w) be a Group with n pieces of the black (white) color and 4−n empty squares. h = ((Group1 b ∗α)+(Group2 b ∗β)+(Group3 b ∗γ)+(Group4 b ∗∞)) − ((Group1 w ∗α)+(Group2 w ∗β)+(Group3 w ∗γ)+(Group4 w ∗∞)) The values of α, β and δ can vary to form any desired linear combination; however, it is important to value them with the α < β < δ ordering in mind (we used 1, 4, and 8 as their respective values).",
                "Groups of 4 discs of the same color means victory, thus discovery of such a group will result in ∞ to ensure an extreme value.",
                "We now use our estimated evaluation function to evaluate the Black players actions during the Connect-Four adversarial interaction.",
                "Each game from the log file was input into the application, which processed and output a reformatted log file containing the h value of the current move, the maxh value that could be achieved, and a notification if an open threat was detected.",
                "A total of 123 games were analyzed (57 with White winning, and 66 with Black winning).",
                "A few additional games were manually ignored in the experiment, due to these problems: a player abandoning the game while the outcome is not final, or a blunt irrational move in the early stages of the game (e.g., not blocking an obvious winning group in the first opening moves).",
                "In addition, a single tie game was also removed.",
                "The simulator was run to a search depth of 3 moves.",
                "We now proceed to analyze the games with respect to each behavioral axiom.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 555 Table 1: Average heuristic difference analysis Black losses Black Won Avg minh -17.62 -12.02 Avg 3 lowest h moves (min3 h) -13.20 -8.70 3.2.1 Affirming the Suboptimal tactical move axiom The following section presents the heuristic evaluations of the Min-Max algorithm for each action, and checks the amount and extent of suboptimal tactical actions and their implications on performance.",
                "Table 1 shows results and insights from the games heuristic analysis, when search depth equals 3 (this search depth was selected for the results to be comparable to [9], see Section 3.2.3).",
                "The tables heuristic data is the difference between the present maximal heuristic value and the heuristic value of the action that was eventually taken by the player (i.e., the closer the number is to 0, the closer the action was to the maximum heuristic action).",
                "The first row presents the difference values of the action that had the maximal difference value among all the Black players actions in a given game, as averaged over all Blacks winning and losing games (see respective columns).",
                "In games in which the Black player loses, its average difference value was -17.62, while in games in which the Black player won, its average was -12.02.",
                "The second row expands the analysis by considering the 3 highest heuristic difference actions, and averaging them.",
                "In that case, we notice an average heuristic difference of 5 points between games which the Black player loses and games in which it wins.",
                "Nevertheless, the importance of those numbers is that they allowed us to take an educated guess on a threshold number of 11.5, as the value of the TrH constant, which differentiates between normal actions and highly beneficial ones.",
                "After finding an approximated TrH constant, we can proceed with an analysis of the importance of suboptimal moves.",
                "To do so we took the subset of games in which the minimum heuristic difference value for Blacks actions was 11.5.",
                "As presented in Table 2, we can see the different min3 h average of the 3 largest ranges and the respective percentage of games won.",
                "The first row shows that the Black player won only 12% of the games in which the average of its 3 highest heuristically difference actions (min3 h) was smaller than the suggested threshold, TrH = 11.5.",
                "The second row shows a surprising result: it seems that when min3 h > −4 the Black player rarely wins.",
                "Intuition would suggest that games in which the action evaluation values were closer to the maximal values will result in more winning games for Black.",
                "However, it seems that in the Connect-Four domain, merely responding with somewhat easily expected actions, without initiating a few surprising and suboptimal moves, does not yield good results.",
                "The last row sums up the main insights from the analysis; most of Blacks wins (83%) came when its min3 h was in the range of -11.5 to -4.",
                "A close inspection of those Black winning games shows the following pattern behind the numbers: after standard opening moves, Black suddenly drops a disc into an isolated column, which seems a waste of a move.",
                "White continues to build its threats, while usually disregarding Blacks last move, which in turn uses the isolated disc as an anchor for a future winning threat.",
                "The results show that it was beneficial for the Black player Table 2: Blacks winnings percentages % of games min3 h < −11.5 12% min3 h > −4 5% −11.5 ≤ min3 h ≤ −4 83% to take suboptimal actions and not give the current highest possible heuristic value, but will not be too harmful for its position (i.e., will not give high beneficial value to its adversary).",
                "As it turned out, learning the threshold is an important aspect of success: taking wildly risky moves (min3 h < −11.5) or trying to avoid them (min3 h > −4) reduces the Black players winning chances by a large margin. 3.2.2 Affirming the Profile Monitoring Axiom In the task of showing the importance of monitoring ones adversaries profiles, our log files could not be used because they did not contain repeated interactions between players, which are needed to infer the players knowledge about their adversaries.",
                "However, the importance of opponent modeling and its use in attaining tactical advantages was already studied in various domains ([3, 9] are good examples).",
                "In a recent paper, Markovitch and Reger [9] explored the notion of learning and exploitation of opponent weakness in competitive interactions.",
                "They apply simple learning strategies by analyzing examples from past interactions in a specific domain.",
                "They also used the Connect-Four adversarial domain, which can now be used to understand the importance of monitoring the adversarys profile.",
                "Following the presentation of their theoretical model, they describe an extensive empirical study and check the agents performance after learning the weakness model with past examples.",
                "One of the domains used as a competitive environment was the same Connect-Four game (Checkers was the second domain).",
                "Their heuristic function was identical to ours with three different variations (H1, H2, and H3) that are distinguished from one another in their linear combination coefficient values.",
                "The search depth for the players was 3 (as in our analysis).",
                "Their extensive experiments check and compare various learning strategies, risk factors, predefined feature sets and usage methods.",
                "The bottom line is that the Connect-Four domain shows an improvement from a 0.556 winning rate before modeling to a 0.69 after modeling (page 22).",
                "Their conclusions, showing improved performance when holding and using the adversarys model, justify the effort to monitor the adversary profile for continuous and repeated interactions.",
                "An additional point that came up in their experiments is the following: after the opponent weakness model has been learned, the authors describe different methods of integrating the opponent weakness model into the agents decision strategy.",
                "Nevertheless, regardless of the specific method they chose to work with, all integration methods might cause the agent to take suboptimal decisions; it might cause the agent to prefer actions that are suboptimal at the present decision junction, but which might cause the opponent to react in accordance with its weakness model (as represented by our agent) which in turn will be beneficial for us in the future.",
                "The agents behavior, as demonstrated in [9] further confirms and strengthens our Suboptimal Tactical Axiom as discussed in the previous section. 556 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 3.2.3 Additional Insights The need for the Goal Achieving, Preventive Act, and Evaluation Maximization axioms are obvious, and need no further verification.",
                "However, even with respect to those axioms, a few interesting insights came up in the log analysis.",
                "The Goal achieving and Preventive Act axioms, though theoretically trivial, seem to provide some challenge to a human player.",
                "In the initial inspection of the logs, we encountered few games2 where a player, for inexplicable reasons, did not block the other from winning or failed to execute its own winning move.",
                "We can blame those faults on the humans lack of attention, or a typing error in its move reply; nevertheless, those errors might occur in bounded rational agents, and the appropriate behavior needs to be axiomatized.",
                "A typical Connect-Four game revolves around generating threats and blocking them.",
                "In our analysis we looked for explicit preventive actions, i.e., moves that block a group of 3 discs, or that remove a future threat (in our limited search horizon).",
                "We found that in 83% of the total games there was at least one preventive action taken by the Black player.",
                "It was also found that Black averaged 2.8 preventive actions per game on the games in which it lost, while averaging 1.5 preventive actions per game when winning.",
                "It seems that Black requires 1 or 2 preventive actions to build its initial taking position, before starting to present threats.",
                "If it did not manage to win, it will usually prevent an extra threat or two before succumbing to White. 4.",
                "RELATED WORK Much research deals with the axiomatization of teamwork and mental states of individuals: some models use knowledge and belief [10], others have models of goals and intentions [8, 4].",
                "However, all these formal theories deal with agent teamwork and cooperation.",
                "As far as we know, our model is the first to provide a formalized model for explicit adversarial environments and agents behavior in it.",
                "The classical Min-Max adversarial search algorithm was the first attempt to integrate the opponent into the search space with a weak assumption of an optimally playing opponent.",
                "Since then, much effort has gone into integrating the opponent model into the decision procedure to predict future behavior.",
                "The M∗ algorithm presented by Carmel and Markovitch [2] showed a method of incorporating opponent models into adversary search, while in [3] they used learning to provide a more accurate opponent model in a 2player repeated game environment, where agents strategies were modeled as finite automata.",
                "Additional Adversarial planning work was done by Willmott et al. [13], which provided an adversarial planning approach to the game of GO.",
                "The research mentioned above dealt with adversarial search and the integration of opponent models into classical utilitybased search methods.",
                "That work shows the importance of opponent modeling and the ability to exploit it to an agents advantage.",
                "However, the basic limitations of those search methods still apply; our model tries to overcome those limitations by presenting a formal model for a new, mental state-based adversarial specification. 5.",
                "CONCLUSIONS We presented an Adversarial Environment model for a 2 These were later removed from the final analysis. bounded rational agent that is situated in an N-player, zerosum environment.",
                "We used the SharedPlans formalization to define the model and the axioms that agents can apply as behavioral guidelines.",
                "The model is meant to be used as a guideline for designing agents that need to operate in such adversarial environments.",
                "We presented empirical results, based on ConnectFour log file analysis, that exemplify the model and the axioms for a bilateral instance of the environment.",
                "The results we presented are a first step towards an expanded model that will cover all types of adversarial environments, for example, environments that are non-zero-sum, and environments that contain natural agents that are not part of the direct conflict.",
                "Those challenges and more will be dealt with in future research. 6.",
                "ACKNOWLEDGMENT This research was supported in part by Israel Science Foundation grants #1211/04 and #898/05. 7.",
                "REFERENCES [1] L. V. Allis.",
                "A knowledge-based approach of Connect-Four - the game is solved: White wins.",
                "Masters thesis, Free University, Amsterdam, The Netherlands, 1988. [2] D. Carmel and S. Markovitch.",
                "Incorporating opponent models into adversary search.",
                "In Proceedings of the Thirteenth National Conference on Artificial Intelligence, pages 120-125, Portland, OR, 1996. [3] D. Carmel and S. Markovitch.",
                "Opponent modeling in multi-agent systems.",
                "In G. Weiß and S. Sen, editors, Adaptation and Learning in Multi-Agent Systems, pages 40-52.",
                "Springer-Verlag, 1996. [4] B. J. Grosz and S. Kraus.",
                "Collaborative plans for complex group action.",
                "Artificial Intelligence, 86(2):269-357, 1996. [5] M. Hadad, G. Kaminka, G. Armon, and S. Kraus.",
                "Supporting collaborative activity.",
                "In Proc. of AAAI-2005, pages 83-88, Pittsburgh, 2005. [6] http://www.gamerz.net/˜pbmserv/. [7] S. Kraus and D. Lehmann.",
                "Designing and building a negotiating automated agent.",
                "Computational Intelligence, 11:132-171, 1995. [8] H. J. Levesque, P. R. Cohen, and J. H. T. Nunes.",
                "On acting together.",
                "In Proc. of AAAI-90, pages 94-99, Boston, MA, 1990. [9] S. Markovitch and R. Reger.",
                "Learning and exploiting relative weaknesses of opponent agents.",
                "Autonomous Agents and Multi-Agent Systems, 10(2):103-130, 2005. [10] Y. M. Ronald Fagin, Joseph Y. Halpern and M. Y. Vardi.",
                "Reasoning about knowledge.",
                "MIT Press, Cambridge, Mass., 1995. [11] P. Thagard.",
                "Adversarial problem solving: Modeling an oponent using explanatory coherence.",
                "Cognitive Science, 16(1):123-149, 1992. [12] R. W. Toseland and R. F. Rivas.",
                "An Introduction to Group Work Practice.",
                "Prentice Hall, Englewood Cliffs, NJ, 2nd edition edition, 1995. [13] S. Willmott, J. Richardson, A. Bundy, and J. Levine.",
                "An adversarial planning approach to Go.",
                "Lecture Notes in Computer Science, 1558:93-112, 1999.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 557"
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [],
            "translated_text": "",
            "candidates": [],
            "error": [
                []
            ]
        },
        "adversarial interaction": {
            "translated_key": "interacción adversarial",
            "is_in_text": true,
            "original_annotated_sentences": [
                "An Adversarial Environment Model for Bounded Rational Agents in Zero-Sum Interactions Inon Zuckerman1 , Sarit Kraus1 , Jeffrey S. Rosenschein2 , Gal Kaminka1 1 Department of Computer Science 2 The School of Engineering Bar-Ilan University and Computer Science Ramat-Gan, Israel Hebrew University, Jerusalem, Israel {zukermi,sarit,galk}@cs.biu.ac.il jeff@cs.huji.ac.il ABSTRACT Multiagent environments are often not cooperative nor collaborative; in many cases, agents have conflicting interests, leading to adversarial interactions.",
                "This paper presents a formal Adversarial Environment model for bounded rational agents operating in a zero-sum environment.",
                "In such environments, attempts to use classical utility-based search methods can raise a variety of difficulties (e.g., implicitly modeling the opponent as an omniscient utility maximizer, rather than leveraging a more nuanced, explicit opponent model).",
                "We define an Adversarial Environment by describing the mental states of an agent in such an environment.",
                "We then present behavioral axioms that are intended to serve as design principles for building such adversarial agents.",
                "We explore the application of our approach by analyzing log files of completed Connect-Four games, and present an empirical analysis of the axioms appropriateness.",
                "Categories and Subject Descriptors I.2.11 [Artificial Intelligence]: Distributed Artificial Intelligence-Intelligent agents,Multiagent Systems; I.2.4 [Artificial Intelligence]: Knowledge Representation Formalisms and Methods -Modal logic General Terms Design, Theory 1.",
                "INTRODUCTION Early research in multiagent systems (MAS) considered cooperative groups of agents; because individual agents had limited resources, or limited access to information (e.g., limited processing power, limited sensor coverage), they worked together by design to solve problems that individually they could not solve, or at least could not solve as efficiently.",
                "MAS research, however, soon began to consider interacting agents with individuated interests, as representatives of different humans or organizations with non-identical interests.",
                "When interactions are guided by diverse interests, participants may have to overcome disagreements, uncooperative interactions, and even intentional attempts to damage one another.",
                "When these types of interactions occur, environments require appropriate behavior from the agents situated in them.",
                "We call these environments Adversarial Environments, and call the clashing agents Adversaries.",
                "Models of cooperation and teamwork have been extensively explored in MAS through the axiomatization of mental states (e.g., [8, 4, 5]).",
                "However, none of this research dealt with adversarial domains and their implications for agent behavior.",
                "Our paper addresses this issue by providing a formal, axiomatized mental state model for a subset of adversarial domains, namely simple zero-sum adversarial environments.",
                "Simple zero-sum encounters exist of course in various twoplayer games (e.g., Chess, Checkers), but they also exist in n-player games (e.g., Risk, Diplomacy), auctions for a single good, and elsewhere.",
                "In these latter environments especially, using a utility-based adversarial search (such as the Min-Max algorithm) does not always provide an adequate solution; the payoff function might be quite complex or difficult to quantify, and there are natural computational limitations on bounded rational agents.",
                "In addition, traditional search methods (like Min-Max) do not make use of a model of the opponent, which has proven to be a valuable addition to adversarial planning [9, 3, 11].",
                "In this paper, we develop a formal, axiomatized model for bounded rational agents that are situated in a zero-sum adversarial environment.",
                "The model uses different modality operators, and its main foundations are the SharedPlans [4] model for collaborative behavior.",
                "We explore environment properties and the mental states of agents to derive behavioral axioms; these behavioral axioms constitute a formal model that serves as a specification and design guideline for agent design in such settings.",
                "We then investigate the behavior of our model empirically using the Connect-Four board game.",
                "We show that this game conforms to our environment definition, and analyze players behavior using a large set of completed match log 550 978-81-904262-7-5 (RPS) c 2007 IFAAMAS files.",
                "In addition, we use the results presented in [9] to discuss the importance of opponent modeling in our ConnectFour adversarial domain.",
                "The paper proceeds as follows.",
                "Section 2 presents the models formalization.",
                "Section 3 presents the empirical analysis and its results.",
                "We discuss related work in Section 4, and conclude and present future directions in Section 5. 2.",
                "ADVERSARIAL ENVIRONMENTS The adversarial environment model (denoted as AE) is intended to guide the design of agents by providing a specification of the capabilities and mental attitudes of an agent in an adversarial environment.",
                "We focus here on specific types of adversarial environments, specified as follows: 1.",
                "Zero-Sum Interactions: positive and negative utilities of all agents sum to zero; 2.",
                "Simple AEs: all agents in the environment are adversarial agents; 3.",
                "Bilateral AEs: AEs with exactly two agents; 4.",
                "Multilateral AEs: AEs of three or more agents.",
                "We will work on both bilateral and multilateral instantiations of zero-sum and simple environments.",
                "In particular, our adversarial environment model will deal with interactions that consist of N agents (N ≥ 2), where all agents are adversaries, and only one agent can succeed.",
                "Examples of such environments range from board games (e.g., Chess, Connect-Four, and Diplomacy) to certain economic environments (e.g., N-bidder auctions over a single good). 2.1 Model Overview Our approach is to formalize the mental attitudes and behaviors of a single adversarial agent; we consider how a single agent perceives the AE.",
                "The following list specifies the conditions and mental states of an agent in a simple, zero-sum AE: 1.",
                "The agent has an individual intention that its own goal will be completed; 2.",
                "The agent has an individual belief that it and its adversaries are pursuing full conflicting goals (defined below)there can be only one winner; 3.",
                "The agent has an individual belief that each adversary has an intention to complete its own full conflicting goal; 4.",
                "The agent has an individual belief in the (partial) profile of its adversaries.",
                "Item 3 is required, since it might be the case that some agent has a full conflicting goal, and is currently considering adopting the intention to complete it, but is, as of yet, not committed to achieving it.",
                "This might occur because the agent has not yet deliberated about the effects that adopting that intention might have on the other intentions it is currently holding.",
                "In such cases, it might not consider itself to even be in an adversarial environment.",
                "Item 4 states that the agent should hold some belief about the profiles of its adversaries.",
                "The profile represents all the knowledge the agent has about its adversary: its weaknesses, strategic capabilities, goals, intentions, trustworthiness, and more.",
                "It can be given explicitly or can be learned from observations of past encounters. 2.2 Model Definitions for Mental States We use Grosz and Krauss definitions of the modal operators, predicates, and meta-predicates, as defined in their SharedPlan formalization [4].",
                "We recall here some of the predicates and operators that are used in that formalization: Int.To(Ai, α, Tn, Tα, C) represents Ais intentions at time Tn to do an action α at time Tα in the context of C. Int.Th(Ai, prop, Tn, Tprop, C) represents Ais intentions at time Tn that a certain proposition prop holds at time Tprop in the context of C. The potential intention operators, Pot.Int.To(...) and Pot.Int.Th(...), are used to represent the mental state when an agent considers adopting an intention, but has not deliberated about the interaction of the other intentions it holds.",
                "The operator Bel(Ai, f, Tf ) represents agent Ai believing in the statement expressed in formula f, at time Tf .",
                "MB(A, f, Tf ) represents mutual belief for a group of agents A.",
                "A snapshot of the system finds our environment to be in some state e ∈ E of environmental variable states, and each adversary in any LAi ∈ L of possible local states.",
                "At any given time step, the system will be in some world w of the set of all possible worlds w ∈ W, where w = E×LA1 ×LA2 × ...LAn , and n is the number of adversaries.",
                "For example, in a Texas Holdem poker game, an agents local state might be its own set of cards (which is unknown to its adversary) while the environment will consist of the betting pot and the community cards (which are visible to both players).",
                "A utility function under this formalization is defined as a mapping from a possible world w ∈ W to an element in , which expresses the desirability of the world, from a single agent perspective.",
                "We usually normalize the range to [0,1], where 0 represents the least desirable possible world, and 1 is the most desirable world.",
                "The implementation of the utility function is dependent on the domain in question.",
                "The following list specifies new predicates, functions, variables, and constants used in conjunction with the original definitions for the adversarial environment formalization: 1. φ is a null action (the agent does not do anything). 2.",
                "GAi is the set of agent Ais goals.",
                "Each goal is a set of predicates whose satisfaction makes the goal complete (we use G∗ Ai ∈ GAi to represent an arbitrary goal of agent Ai). 3. gAi is the set of agent Ais subgoals.",
                "Subgoals are predicates whose satisfaction represents an important milestone toward achievement of the full goal. gG∗ Ai ⊆ gAi is the set of subgoals that are important to the completion of goal G∗ Ai (we will use g∗ G∗ Ai ∈ gG∗ Ai to represent an arbitrary subgoal). 4.",
                "P Aj Ai is the profile object agent Ai holds about agent Aj. 5.",
                "CA is a general set of actions for all agents in A which are derived from the environments constraints.",
                "CAi ⊆ CA is the set of agent Ais possible actions. 6.",
                "Do(Ai, α, Tα, w) holds when Ai performs action α over time interval Tα in world w. 7.",
                "Achieve(G∗ Ai , α, w) is true when goal G∗ Ai is achieved following the completion of action α in world w ∈ W, where α ∈ CAi . 8.",
                "Profile(Ai, PAi Ai ) is true when agent Ai holds an object profile for agent Aj.",
                "Definition 1.",
                "Full conflict (FulConf ) describes a zerosum interaction where only a single goal of the goals in conflict can be completed.",
                "FulConf(G∗ Ai , G∗ Aj ) ⇒ (∃α ∈ CAi , ∀w, β ∈ CAj ) (Achieve(G∗ Ai , α, w) ⇒ ¬Achieve(G∗ Aj , β, w)) ∨ (∃β ∈ CAj , ∀w, α ∈ CAi )(Achieve(G∗ Aj , β, w) ⇒ ¬Achieve(G∗ Ai , α, w)) Definition 2.",
                "Adversarial Knowledge (AdvKnow) is a function returning a value which represents the amount of The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 551 knowledge agent Ai has on the profile of agent Aj, at time Tn.",
                "The higher the value, the more knowledge agent Ai has.",
                "AdvKnow : P Aj Ai × Tn → Definition 3.",
                "Eval - This evaluation function returns an estimated expected utility value for an agent in A, after completing an action from CA in some world state w. Eval : A × CA × w → Definition 4.",
                "TrH - (Threshold) is a numerical constant in the [0,1] range that represents an evaluation function (Eval) threshold value.",
                "An action that yields an estimated utility evaluation above the TrH is regarded as a highly beneficial action.",
                "The Eval value is an estimation and not the real utility function, which is usually unknown.",
                "Using the real utility value for a rational agent would easily yield the best outcome for that agent.",
                "However, agents usually do not have the real utility functions, but rather a heuristic estimate of it.",
                "There are two important properties that should hold for the evaluation function: Property 1.",
                "The evaluation function should state that the most desirable world state is one in which the goal is achieved.",
                "Therefore, after the goal has been satisfied, there can be no future action that can put the agent in a world state with higher Eval value. (∀Ai, G∗ Ai , α, β ∈ CAi , w ∈ W) Achieve(G∗ Ai , α, w) ⇒ Eval(Ai, α, w) ≥ Eval(Ai, β, w) Property 2.",
                "The evaluation function should project an action that causes a completion of a goal or a subgoal to a value which is greater than TrH (a highly beneficial action). (∀Ai, G∗ Ai ∈ GAi , α ∈ CAi , w ∈ W, g∗ GAi ∈ gGAi ) Achieve(G∗ Ai , α, w) ∨ Achieve(g∗ GAi , α, w) ⇒ Eval(Ai, α, w) ≥ TrH.",
                "Definition 5.",
                "SetAction We define a set action (SetAction) as a set of action operations (either complex or basic actions) from some action sets CAi and CAj which, according to agent Ais belief, are attached together by a temporal and consequential relationship, forming a chain of events (action, and its following consequent action). (∀α1 , . . . , αu ∈ CAi , β1 , . . . , βv ∈ CAj , w ∈ W) SetAction(α1 , . . . , αu , β1 , . . . , βv , w) ⇒ ((Do(Ai, α1 , Tα1 , w) ⇒ Do(Aj, β1 , Tβ1 , w)) ⇒ Do(Ai, α2 , Tα2 , w) ⇒ . . . ⇒ Do(Ai, αu , Tαu , w)) The consequential relation might exist due to various environmental constraints (when one action forces the adversary to respond with a specific action) or due to the agents knowledge about the profile of its adversary.",
                "Property 3.",
                "As the knowledge we have about our adversary increases we will have additional beliefs about its behavior in different situations which in turn creates new set actions.",
                "Formally, if our AdvKnow at time Tn+1 is greater than AdvKnow at time Tn, then every SetAction known at time Tn is also known at time Tn+1.",
                "AdvKnow(P Aj Ai , Tn+1) > AdvKnow(P Aj Ai , Tn) ⇒ (∀α1 , . . . , αu ∈ CAi , β1 , . . . , βv ∈ CAj ) Bel(Aag, SetAction(α1 , . . . , αu , β1 , . . . , βv ), Tn) ⇒ Bel(Aag, SetAction(α1 , . . . , αu , β1 , . . . , βv ), Tn+1) 2.3 The Environment Formulation The following axioms provide the formal definition for a simple, zero-sum Adversarial Environment (AE).",
                "Satisfaction of these axioms means that the agent is situated in such an environment.",
                "It provides specifications for agent Aag to interact with its set of adversaries A with respect to goals G∗ Aag and G∗ A at time TCo at some world state w. AE(Aag, A, G∗ Aag , A1, . . . , Ak, G∗ A1 , . . . , G∗ Ak , Tn, w) 1.",
                "Aag has an Int.Th his goal would be completed: (∃α ∈ CAag , Tα) Int.Th(Aag, Achieve(G∗ Aag , α), Tn, Tα, AE) 2.",
                "Aag believes that it and each of its adversaries Ao are pursuing full conflicting goals: (∀Ao ∈ {A1, . . . , Ak}) Bel(Aag, FulConf(G∗ Aag , G∗ Ao ), Tn) 3.",
                "Aag believes that each of his adversaries in Ao has the Int.Th his conflicting goal G∗ Aoi will be completed: (∀Ao ∈ {A1, . . . , Ak})(∃β ∈ CAo , Tβ) Bel(Aag, Int.Th(Ao, Achieve(G∗ Ao , β), TCo, Tβ, AE), Tn) 4.",
                "Aag has beliefs about the (partial) profiles of its adversaries (∀Ao ∈ {A1, . . . , Ak}) (∃PAo Aag ∈ PAag )Bel(Aag, Profile(Ao, PAo Aag ), Tn) To build an agent that will be able to operate successfully within such an AE, we must specify behavioral guidelines for its interactions.",
                "Using a naive Eval maximization strategy to a certain search depth will not always yield satisfactory results for several reasons: (1) the search horizon problem when searching for a fixed depth; (2) the strong assumption of an optimally rational, unbounded resources adversary; (3) using an estimated evaluation function which will not give optimal results in all world states, and can be exploited [9].",
                "The following axioms specify the behavioral principles that can be used to differentiate between successful and less successful agents in the above Adversarial Environment.",
                "Those axioms should be used as specification principles when designing and implementing agents that should be able to perform well in such Adversarial Environments.",
                "The behavioral axioms represent situations in which the agent will adopt potential intentions to (Pot.Int.To(...)) perform an action, which will typically require some means-end reasoning to select a possible course of action.",
                "This reasoning will lead to the adoption of an Int.To(...) (see [4]).",
                "A1.",
                "Goal Achieving Axiom.",
                "The first axiom is the simplest case; when the agent Aag believes that it is one action (α) away from achieving his conflicting goal G∗ Aag , it should adopt the potential intention to do α and complete its goal. (∀Aag, α ∈ CAag , Tn, Tα, w ∈ W) (Bel(Aag, Do(Aag, α, Tα, w) ⇒ Achieve(G∗ Aag , α, w)) ⇒ Pot.Int.To(Aag, α, Tn, Tα, w) This somewhat trivial behavior is the first and strongest axiom.",
                "In any situation, when the agent is an action away from completing the goal, it should complete the action.",
                "Any fair Eval function would naturally classify α as the maximal value action (property 1).",
                "However, without explicit axiomatization of such behavior there might be situations where the agent will decide on taking another action for various reasons, due to its bounded decision resources.",
                "A2.",
                "Preventive Act Axiom.",
                "Being in an adversarial situation, agent Aag might decide to take actions that will damage one of its adversarys plans to complete its goal, even if those actions do not explicitly advance Aag towards its conflicting goal G∗ Aag .",
                "Such preventive action will take place when agent Aag has a belief about the possibility of its adversary Ao doing an action β that will give it a high 552 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) utility evaluation value (> TrH).",
                "Believing that taking action α will prevent the opponent from doing its β, it will adopt a potential intention to do α. (∀Aag, Ao ∈ A, α ∈ CAag , β ∈ CAo , Tn, Tβ, w ∈ W) (Bel(Aag, Do(Ao, β, Tβ, w) ∧ Eval(Ao, β, w) > TrH, Tn) ∧ Bel(Aag, Do(Aag, α, Tα, w) ⇒ ¬Do(Ao, β, Tβ, w), Tn) ⇒ Pot.Int.To(Aag, α, Tn, Tα, w) This axiom is a basic component of any adversarial environment.",
                "For example, looking at a Chess board game, a player could realize that it is about to be checkmated by its opponent, thus making a preventive move.",
                "Another example is a Connect Four game: when a player has a row of three chips, its opponent must block it, or lose.",
                "A specific instance of A1 occurs when the adversary is one action away from achieving its goal, and immediate preventive action needs to be taken by the agent.",
                "Formally, we have the same beliefs as stated above, with a changed belief that doing action β will cause agent Ao to achieve its goal.",
                "Proposition 1: Prevent or lose case. (∀Aag, Ao ∈ A, α ∈ CAag , β ∈ CAo , G∗ Ao , Tn, Tα, Tβ, w ∈ W) Bel(Aag, Do(Ao, β, Tβ, w) ⇒ Achieve(G∗ Ao , β, w), Tn) ∧ Bel(Aag, Do(Aag, α, Tα, w) ⇒ ¬Do(Ao, β, Tβ, w)) ⇒ Pot.Int.To(Aag, α, Tn, Tα, w) Sketch of proof: Proposition 1 can be easily derived from axiom A1 and the property 2 of the Eval function, which states that any action that causes a completion of a goal is a highly beneficial action.",
                "The preventive act behavior will occur implicitly when the Eval function is equal to the real world utility function.",
                "However, being bounded rational agents and dealing with an estimated evaluation function we need to explicitly axiomatize such behavior, for it will not always occur implicitly from the evaluation function.",
                "A3.",
                "Suboptimal Tactical Move Axiom.",
                "In many scenarios a situation may occur where an agent will decide not to take the current most beneficial action it can take (the action with the maximal utility evaluation value), because it believes that taking another action (with lower utility evaluation value) might yield (depending on the adversarys response) a future possibility for a highly beneficial action.",
                "This will occur most often when the Eval function is inaccurate and differs by a large extent from the Utility function.",
                "Put formally, agent Aag believes in a certain SetAction that will evolve according to its initial action and will yield a high beneficial value (> TrH) solely for it. (∀Aag, Ao ∈ A, Tn, w ∈ W) (∃α1 , . . . , αu ∈ CAi , β1 , . . . , βv ∈ CAj , Tα1 ) Bel(Aag, SetAction(α1 , . . . , αu , β1 , . . . , βv ), Tn) ∧ Bel(Aag, Eval(Ao, βv , w) < TrH < Eval(Aag, αu , w), Tn) ⇒ Pot.Int.To(Aag, α1 , Tn, Tα1 , w) An agent might believe that a chain of events will occur for various reasons due to the inevitable nature of the domain.",
                "For example, in Chess, we often observe the following: a move causes a check position, which in turn limits the opponents moves to avoiding the check, to which the first player might react with another check, and so on.",
                "The agent might also believe in a chain of events based on its knowledge of its adversarys profile, which allows it to foresee the adversarys movements with high accuracy.",
                "A4.",
                "Profile Detection Axiom.",
                "The agent can adjust its adversarys profiles by observations and pattern study (specifically, if there are repeated encounters with the same adversary).",
                "However, instead of waiting for profile information to be revealed, an agent can also initiate actions that will force its adversary to react in a way that will reveal profile knowledge about it.",
                "Formally, the axiom states that if all actions (γ) are not highly beneficial actions (< TrH), the agent can do action α in time Tα if it believes that it will result in a non-highly beneficial action β from its adversary, which in turn teaches it about the adversarys profile, i.e., gives a higher AdvKnow(P Aj Ai , Tβ). (∀Aag, Ao ∈ A, α ∈ CAag , β ∈ CAo , Tn, Tα, Tβ, w ∈ W) Bel(Aag, (∀γ ∈ CAag )Eval(Aag, γ, w) < TrH, Tn) ∧ Bel(Aag, Do(Aag, α, Tα, w) ⇒ Do(Ao, β, Tβ, w), Tn) ∧ Bel(Aag, Eval(Ao, β, w) < TrH) ∧ Bel(Aag, AdvKnow(P Aj Ai , Tβ) > AdvKnow(P Aj Ai , Tn), Tn) ⇒ Pot.Int.To(Aag, α, Tn, Tα, w) For example, going back to the Chess board game scenario, consider starting a game versus an opponent about whom we know nothing, not even if it is a human or a computerized opponent.",
                "We might start playing a strategy that will be suitable versus an average opponent, and adjust our game according to its level of play.",
                "A5.",
                "Alliance Formation Axiom The following behavioral axiom is relevant only in a multilateral instantiation of the adversarial environment (obviously, an alliance cannot be formed in a bilateral, zero-sum encounter).",
                "In different situations during a multilateral interaction, a group of agents might believe that it is in their best interests to form a temporary alliance.",
                "Such an alliance is an agreement that constrains its members behavior, but is believed by its members to enable them to achieve a higher utility value than the one achievable outside of the alliance.",
                "As an example, we can look at the classical Risk board game, where each player has an individual goal of being the sole conquerer of the world, a zero-sum game.",
                "However, in order to achieve this goal, it might be strategically wise to make short-term ceasefire agreements with other players, or to join forces and attack an opponent who is stronger than the rest.",
                "An alliances terms defines the way its members should act.",
                "It is a set of predicates, denoted as Terms, that is agreed upon by the alliance members, and should remain true for the duration of the alliance.",
                "For example, the set Terms in the Risk scenario, could contain the following predicates: 1.",
                "Alliance members will not attack each other on territories X, Y and Z; 2.",
                "Alliance members will contribute C units per turn for attacking adversary Ao; 3.",
                "Members are obligated to stay as part of the alliance until time Tk or until adversarys Ao army is smaller than Q.",
                "The set Terms specifies inter-group constraints on each of the alliance members (∀Aal i ∈ Aal ⊆ A) set of actions Cal i ⊆ C. Definition 6.",
                "Al val - the total evaluation value that agent Ai will achieve while being part of Aal is the sum of Evali (Eval for Ai) of each of Aal j Eval values after taking their own α actions (via the agent(α) predicate): Al val(Ai, Cal , Aal , w) = α∈Cal Evali(Aal j , agent(α), w) Definition 7.",
                "Al TrH - is a number representing an Al val The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 553 threshold; above it, the alliance can be said to be a highly beneficial alliance.",
                "The value of Al TrH will be calculated dynamically according to the progress of the interaction, as can be seen in [7].",
                "After an alliance is formed, its members are now working in their normal adversarial environment, as well as according to the mental states and axioms required for their interactions as part of the alliance.",
                "The following Alliance model (AL) specifies the conditions under which the group Aal can be said to be in an alliance and working with a new and constrained set of actions Cal , at time Tn.",
                "AL(Aal , Cal , w, Tn) 1.",
                "Aal has a MB that all members are part of Aal : MB(Aal , (∀Aal i ∈ Aal )member(Aal i , Aal ), Tn) 2.",
                "Aal has a MB that the group be maintained: MB(Aal , (∀Aal i ∈ Aal )Int.Th (Ai, member(Ai, Aal ), Tn, Tn+1, Co), Tn) 3.",
                "Aal has a MB that being members gives them high utility value: MB(Aal , (∀Aal i ∈ Aal )Al val(Aal i , Cal , Aal , w) ≥ Al TrH, Tn) Members profiles are a crucial part of successful alliances.",
                "We assume that agents that have more accurate profiles of their adversaries will be more successful in such environments.",
                "Such agents will be able to predict when a member is about to breach the alliances contract (item 2 in the above model), and take counter measures (when item 3 will falsify).",
                "The robustness of the alliance is in part a function of its members trustfulness measure, objective position estimation, and other profile properties.",
                "We should note that an agent can simultaneously be part of more than one alliance.",
                "Such a temporary alliance, where the group members do not have a joint goal but act collaboratively for the interest of their own individual goals, is classified as a Treatment Group by modern psychologists [12] (in contrast to a Task Group, where its members have a joint goal).",
                "The Shared Activity model as presented in [5] modeled Treatment Group behavior using the same SharedPlans formalization.",
                "When comparing both definitions of an alliance and a Treatment Group we found an unsurprising resemblance between both models: the environment models definitions are almost identical (see SAs definitions in [5]), and their Selfish-Act and Cooperative Act axioms conform to our adversarial agents behavior.",
                "The main distinction between both models is the integration of a Helpful-behavior act axiom, in the Shared Activity which cannot be part of ours.",
                "This axiom states that an agent will consider taking action that will lower its Eval value (to a certain lower bound), if it believes that a group partner will gain a significant benefit.",
                "Such behavior cannot occur in a pure adversarial environment (as a zero-sum game is), where the alliance members are constantly on watch to manipulate their alliance to their own advantage.",
                "A6.",
                "Evaluation Maximization Axiom.",
                "In a case when all other axioms are inapplicable, we will proceed with the action that maximizes the heuristic value as computed in the Eval function. (∀Aag, Ao ∈ A, α ∈ Cag, Tn, w ∈ W) Bel(Aag, (∀γ ∈ Cag)Eval(Aag, α, w) ≥ Eval(Aag, γ, w), Tn) ⇒ Pot.Int.To(Aag, α, Tn, Tα, w) T1.",
                "Optimality on Eval = Utility The above axiomatic model handles situations where the Utility is unknown and the agents are bounded rational agents.",
                "The following theorem shows that in bilateral interactions, where the agents have the real Utility function (i.e., Eval = Utility) and are rational agents, the axioms provide the same optimal result as classic adversarial search (e.g., Min-Max).",
                "Theorem 1.",
                "Let Ae ag be an unbounded rational AE agent using the Eval heuristic evaluation function, Au ag be the same agent using the true Utility function, and Ao be a sole unbounded utility-based rational adversary.",
                "Given that Eval = Utility: (∀α ∈ CAu ag , α ∈ CAe ag , Tn, w ∈ W) Pot.Int.To(Au ag, α, Tn, Tα, w) → Pot.Int.To(Ae ag, α , Tn, Tα, w) ∧ ((α = α ) ∨ (Utility(Au ag, α, w) = Eval(Ae ag, α , w))) Sketch of proof - Given that Au ag has the real utility function and unbounded resources, it can generate the full game tree and run the optimal MinMax algorithm to choose the highest utility value action, which we denote by, α.",
                "The proof will show that Ae ag, using the AE axioms, will select the same or equal utility α (when there is more than one action with the same max utility) when Eval = Utility. (A1) Goal achieving axiom - suppose there is an α such that its completion will achieve Au ags goal.",
                "It will obtain the highest utility by Min-Max for Au ag.",
                "The Ae ag agent will select α or another action with the same utility value via A1.",
                "If such α does not exist, Ae ag cannot apply this axiom, and proceeds to A2. (A2) Preventive act axiom - (1) Looking at the basic case (see Prop1), if there is a β which leads Ao to achieve its goal, then a preventive action α will yield the highest utility for Au ag.",
                "Au ag will choose it through the utility, while Ae ag will choose it through A2. (2) In the general case, β is a highly beneficial action for Ao, thus yields low utility for Au ag, which will guide it to select an α that will prevent β, while Ae ag will choose it through A2.1 If such β does not exist for Ao, then A2 is not applicable, and Ae ag can proceed to A3. (A3) Suboptimal tactical move axiom - When using a heuristic Eval function, Ae ag has a partial belief in the profile of its adversary (item 4 in AE model), which may lead it to believe in SetActions (Prop1).",
                "In our case, Ae ag is holding a full profile on its optimal adversary and knows that Ao will behave optimally according to the real utility values on the complete search tree, therefore, any belief about suboptimal SetAction cannot exist, yielding this axiom inapplicable.",
                "Ae ag will proceed to A4. (A4) Profile detection axiom - Given that Ae ag has the full profile of Ao, none of Ae ags actions can increase its knowledge.",
                "That axiom will not be applied, and the agent will proceed with A6 (A5 will be disregarded because the interaction is bilateral). (A6) Evaluation maximization axiom - This axiom will select the max Eval for Ae ag.",
                "Given that Eval = Utility, the same α that was selected by Au ag will be selected. 3.",
                "EVALUATION The main purpose of our experimental analysis is to evaluate the models behavior and performance in a real adversarial environment.",
                "This section investigates whether bounded 1 A case where following the completion of β there exists a γ which gives high utility for Agent Au ag, cannot occur because Ao uses the same utility, and γs existence will cause it to classify β as a low utility action. 554 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) rational agents situated in such adversarial environments will be better off applying our suggested behavioral axioms. 3.1 The Domain To explore the use of the above model and its behavioral axioms, we decided to use the Connect-Four game as our adversarial environment.",
                "Connect-Four is a 2-player, zerosum game which is played using a 6x7 matrix-like board.",
                "Each turn, a player drops a disc into one of the 7 columns (the set of 21 discs is usually colored yellow for player 1 and red for player 2; we will use White and Black respectively to avoid confusion).",
                "The winner is the first player to complete a horizontal, vertical, or diagonal set of four discs with its color.",
                "On very rare occasions, the game might end in a tie if all the empty grids are filled, but no player managed to create a 4-disc set.",
                "The Connect-Four game was solved in [1], where it is shown that the first player (playing with the white discs) can force a win by starting in the middle column (column 4) and playing optimally However, the optimal strategy is very complex, and difficult to follow even for complex bounded rational agents, such as human players.",
                "Before we can proceed checking agent behavior, we must first verify that the domain conforms to the adversarial environments definition as given above (which the behavioral axioms are based on).",
                "First, when playing a Connect-Four game, the agent has an intention to win the game (item 1).",
                "Second (item 2), our agent believes that in Connect-Four there can only be one winner (or no winner at all in the rare occurrence of a tie).",
                "In addition, our agent believes that its opponent to the game will try to win (item 3), and we hope it has some partial knowledge (item 4) about its adversary (this knowledge can vary from nothing, through simple facts such as age, to strategies and weaknesses).",
                "Of course, not all Connect-Four encounters are adversarial.",
                "For example, when a parent is playing the game with its child, the following situation might occur: the child, having a strong incentive to win, treats the environment as adversarial (it intends to win, understands that there can only be one winner, and believes that its parent is trying to beat him).",
                "However, the parents point of view might see the environment as an educational one, where its goal is not to win the game, but to cause enjoyment or practice strategic reasoning.",
                "In such an educational environment, a new set of behavioral axioms might be more beneficial to the parents goals than our suggested adversarial behavioral axioms. 3.2 Axiom Analysis After showing that the Connect-Four game is indeed a zero-sum, bilateral adversarial environment, the next step is to look at players behaviors during the game and check whether behaving according to our model does improve performance.",
                "To do so we have collected log files from completed Connect-Four games that were played by human players over the Internet.",
                "Our collected log file data came from Play by eMail (PBeM) sites.",
                "These are web sites that host email games, where each move is taken by an email exchange between the server and the players.",
                "Many such sites archives contain real competitive interactions, and also maintain a ranking system for their members.",
                "Most of the data we used can be found in [6].",
                "As can be learned from [1], Connect-Four has an optimal strategy and a considerable advantage for the player who starts the game (which we call the White player).",
                "We will concentrate in our analysis on the second players moves (to be called Black).",
                "The White player, being the first to act, has the so-called initiative advantage.",
                "Having the advantage and a good strategy will keep the Black player busy reacting to its moves, instead of initiating threats.",
                "A threat is a combination of three discs of the same color, with an empty spot for the fourth winning disk.",
                "An open threat is a threat that can be realized in the opponents next move.",
                "In order for the Black player to win, it must somehow turn the tide, take the advantage and start presenting threats to the White player.",
                "We will explore Black players behavior and their conformance to our axioms.",
                "To do so, we built an application that reads log files and analyzes the Black players moves.",
                "The application contains two main components: (1) a Min-Max algorithm for evaluation of moves; (2) open threats detector for the discovering of open threats.",
                "The Min-Max algorithm will work to a given depth, d and for each move α will output the heuristic value for the next action taken by the player as written in the log file, h(α), alongside the maximum heuristic value, maxh(α), that could be achieved prior to taking the move (obviously, if h(α) = maxh(α), then the player did not do the optimal move heuristically).",
                "The threat detectors job is to notify if some action was taken in order to block an open threat (not blocking an open threat will probably cause the player to lose in the opponents next move).",
                "The heuristic function used by Min-Max to evaluate the players utility is the following function, which is simple to compute, yet provides a reasonable challenge to human opponents: Definition 8.",
                "Let Group be an adjacent set of four squares that are horizontal, vertical, or diagonal.",
                "Groupn b (Groupn w) be a Group with n pieces of the black (white) color and 4−n empty squares. h = ((Group1 b ∗α)+(Group2 b ∗β)+(Group3 b ∗γ)+(Group4 b ∗∞)) − ((Group1 w ∗α)+(Group2 w ∗β)+(Group3 w ∗γ)+(Group4 w ∗∞)) The values of α, β and δ can vary to form any desired linear combination; however, it is important to value them with the α < β < δ ordering in mind (we used 1, 4, and 8 as their respective values).",
                "Groups of 4 discs of the same color means victory, thus discovery of such a group will result in ∞ to ensure an extreme value.",
                "We now use our estimated evaluation function to evaluate the Black players actions during the Connect-Four <br>adversarial interaction</br>.",
                "Each game from the log file was input into the application, which processed and output a reformatted log file containing the h value of the current move, the maxh value that could be achieved, and a notification if an open threat was detected.",
                "A total of 123 games were analyzed (57 with White winning, and 66 with Black winning).",
                "A few additional games were manually ignored in the experiment, due to these problems: a player abandoning the game while the outcome is not final, or a blunt irrational move in the early stages of the game (e.g., not blocking an obvious winning group in the first opening moves).",
                "In addition, a single tie game was also removed.",
                "The simulator was run to a search depth of 3 moves.",
                "We now proceed to analyze the games with respect to each behavioral axiom.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 555 Table 1: Average heuristic difference analysis Black losses Black Won Avg minh -17.62 -12.02 Avg 3 lowest h moves (min3 h) -13.20 -8.70 3.2.1 Affirming the Suboptimal tactical move axiom The following section presents the heuristic evaluations of the Min-Max algorithm for each action, and checks the amount and extent of suboptimal tactical actions and their implications on performance.",
                "Table 1 shows results and insights from the games heuristic analysis, when search depth equals 3 (this search depth was selected for the results to be comparable to [9], see Section 3.2.3).",
                "The tables heuristic data is the difference between the present maximal heuristic value and the heuristic value of the action that was eventually taken by the player (i.e., the closer the number is to 0, the closer the action was to the maximum heuristic action).",
                "The first row presents the difference values of the action that had the maximal difference value among all the Black players actions in a given game, as averaged over all Blacks winning and losing games (see respective columns).",
                "In games in which the Black player loses, its average difference value was -17.62, while in games in which the Black player won, its average was -12.02.",
                "The second row expands the analysis by considering the 3 highest heuristic difference actions, and averaging them.",
                "In that case, we notice an average heuristic difference of 5 points between games which the Black player loses and games in which it wins.",
                "Nevertheless, the importance of those numbers is that they allowed us to take an educated guess on a threshold number of 11.5, as the value of the TrH constant, which differentiates between normal actions and highly beneficial ones.",
                "After finding an approximated TrH constant, we can proceed with an analysis of the importance of suboptimal moves.",
                "To do so we took the subset of games in which the minimum heuristic difference value for Blacks actions was 11.5.",
                "As presented in Table 2, we can see the different min3 h average of the 3 largest ranges and the respective percentage of games won.",
                "The first row shows that the Black player won only 12% of the games in which the average of its 3 highest heuristically difference actions (min3 h) was smaller than the suggested threshold, TrH = 11.5.",
                "The second row shows a surprising result: it seems that when min3 h > −4 the Black player rarely wins.",
                "Intuition would suggest that games in which the action evaluation values were closer to the maximal values will result in more winning games for Black.",
                "However, it seems that in the Connect-Four domain, merely responding with somewhat easily expected actions, without initiating a few surprising and suboptimal moves, does not yield good results.",
                "The last row sums up the main insights from the analysis; most of Blacks wins (83%) came when its min3 h was in the range of -11.5 to -4.",
                "A close inspection of those Black winning games shows the following pattern behind the numbers: after standard opening moves, Black suddenly drops a disc into an isolated column, which seems a waste of a move.",
                "White continues to build its threats, while usually disregarding Blacks last move, which in turn uses the isolated disc as an anchor for a future winning threat.",
                "The results show that it was beneficial for the Black player Table 2: Blacks winnings percentages % of games min3 h < −11.5 12% min3 h > −4 5% −11.5 ≤ min3 h ≤ −4 83% to take suboptimal actions and not give the current highest possible heuristic value, but will not be too harmful for its position (i.e., will not give high beneficial value to its adversary).",
                "As it turned out, learning the threshold is an important aspect of success: taking wildly risky moves (min3 h < −11.5) or trying to avoid them (min3 h > −4) reduces the Black players winning chances by a large margin. 3.2.2 Affirming the Profile Monitoring Axiom In the task of showing the importance of monitoring ones adversaries profiles, our log files could not be used because they did not contain repeated interactions between players, which are needed to infer the players knowledge about their adversaries.",
                "However, the importance of opponent modeling and its use in attaining tactical advantages was already studied in various domains ([3, 9] are good examples).",
                "In a recent paper, Markovitch and Reger [9] explored the notion of learning and exploitation of opponent weakness in competitive interactions.",
                "They apply simple learning strategies by analyzing examples from past interactions in a specific domain.",
                "They also used the Connect-Four adversarial domain, which can now be used to understand the importance of monitoring the adversarys profile.",
                "Following the presentation of their theoretical model, they describe an extensive empirical study and check the agents performance after learning the weakness model with past examples.",
                "One of the domains used as a competitive environment was the same Connect-Four game (Checkers was the second domain).",
                "Their heuristic function was identical to ours with three different variations (H1, H2, and H3) that are distinguished from one another in their linear combination coefficient values.",
                "The search depth for the players was 3 (as in our analysis).",
                "Their extensive experiments check and compare various learning strategies, risk factors, predefined feature sets and usage methods.",
                "The bottom line is that the Connect-Four domain shows an improvement from a 0.556 winning rate before modeling to a 0.69 after modeling (page 22).",
                "Their conclusions, showing improved performance when holding and using the adversarys model, justify the effort to monitor the adversary profile for continuous and repeated interactions.",
                "An additional point that came up in their experiments is the following: after the opponent weakness model has been learned, the authors describe different methods of integrating the opponent weakness model into the agents decision strategy.",
                "Nevertheless, regardless of the specific method they chose to work with, all integration methods might cause the agent to take suboptimal decisions; it might cause the agent to prefer actions that are suboptimal at the present decision junction, but which might cause the opponent to react in accordance with its weakness model (as represented by our agent) which in turn will be beneficial for us in the future.",
                "The agents behavior, as demonstrated in [9] further confirms and strengthens our Suboptimal Tactical Axiom as discussed in the previous section. 556 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 3.2.3 Additional Insights The need for the Goal Achieving, Preventive Act, and Evaluation Maximization axioms are obvious, and need no further verification.",
                "However, even with respect to those axioms, a few interesting insights came up in the log analysis.",
                "The Goal achieving and Preventive Act axioms, though theoretically trivial, seem to provide some challenge to a human player.",
                "In the initial inspection of the logs, we encountered few games2 where a player, for inexplicable reasons, did not block the other from winning or failed to execute its own winning move.",
                "We can blame those faults on the humans lack of attention, or a typing error in its move reply; nevertheless, those errors might occur in bounded rational agents, and the appropriate behavior needs to be axiomatized.",
                "A typical Connect-Four game revolves around generating threats and blocking them.",
                "In our analysis we looked for explicit preventive actions, i.e., moves that block a group of 3 discs, or that remove a future threat (in our limited search horizon).",
                "We found that in 83% of the total games there was at least one preventive action taken by the Black player.",
                "It was also found that Black averaged 2.8 preventive actions per game on the games in which it lost, while averaging 1.5 preventive actions per game when winning.",
                "It seems that Black requires 1 or 2 preventive actions to build its initial taking position, before starting to present threats.",
                "If it did not manage to win, it will usually prevent an extra threat or two before succumbing to White. 4.",
                "RELATED WORK Much research deals with the axiomatization of teamwork and mental states of individuals: some models use knowledge and belief [10], others have models of goals and intentions [8, 4].",
                "However, all these formal theories deal with agent teamwork and cooperation.",
                "As far as we know, our model is the first to provide a formalized model for explicit adversarial environments and agents behavior in it.",
                "The classical Min-Max adversarial search algorithm was the first attempt to integrate the opponent into the search space with a weak assumption of an optimally playing opponent.",
                "Since then, much effort has gone into integrating the opponent model into the decision procedure to predict future behavior.",
                "The M∗ algorithm presented by Carmel and Markovitch [2] showed a method of incorporating opponent models into adversary search, while in [3] they used learning to provide a more accurate opponent model in a 2player repeated game environment, where agents strategies were modeled as finite automata.",
                "Additional Adversarial planning work was done by Willmott et al. [13], which provided an adversarial planning approach to the game of GO.",
                "The research mentioned above dealt with adversarial search and the integration of opponent models into classical utilitybased search methods.",
                "That work shows the importance of opponent modeling and the ability to exploit it to an agents advantage.",
                "However, the basic limitations of those search methods still apply; our model tries to overcome those limitations by presenting a formal model for a new, mental state-based adversarial specification. 5.",
                "CONCLUSIONS We presented an Adversarial Environment model for a 2 These were later removed from the final analysis. bounded rational agent that is situated in an N-player, zerosum environment.",
                "We used the SharedPlans formalization to define the model and the axioms that agents can apply as behavioral guidelines.",
                "The model is meant to be used as a guideline for designing agents that need to operate in such adversarial environments.",
                "We presented empirical results, based on ConnectFour log file analysis, that exemplify the model and the axioms for a bilateral instance of the environment.",
                "The results we presented are a first step towards an expanded model that will cover all types of adversarial environments, for example, environments that are non-zero-sum, and environments that contain natural agents that are not part of the direct conflict.",
                "Those challenges and more will be dealt with in future research. 6.",
                "ACKNOWLEDGMENT This research was supported in part by Israel Science Foundation grants #1211/04 and #898/05. 7.",
                "REFERENCES [1] L. V. Allis.",
                "A knowledge-based approach of Connect-Four - the game is solved: White wins.",
                "Masters thesis, Free University, Amsterdam, The Netherlands, 1988. [2] D. Carmel and S. Markovitch.",
                "Incorporating opponent models into adversary search.",
                "In Proceedings of the Thirteenth National Conference on Artificial Intelligence, pages 120-125, Portland, OR, 1996. [3] D. Carmel and S. Markovitch.",
                "Opponent modeling in multi-agent systems.",
                "In G. Weiß and S. Sen, editors, Adaptation and Learning in Multi-Agent Systems, pages 40-52.",
                "Springer-Verlag, 1996. [4] B. J. Grosz and S. Kraus.",
                "Collaborative plans for complex group action.",
                "Artificial Intelligence, 86(2):269-357, 1996. [5] M. Hadad, G. Kaminka, G. Armon, and S. Kraus.",
                "Supporting collaborative activity.",
                "In Proc. of AAAI-2005, pages 83-88, Pittsburgh, 2005. [6] http://www.gamerz.net/˜pbmserv/. [7] S. Kraus and D. Lehmann.",
                "Designing and building a negotiating automated agent.",
                "Computational Intelligence, 11:132-171, 1995. [8] H. J. Levesque, P. R. Cohen, and J. H. T. Nunes.",
                "On acting together.",
                "In Proc. of AAAI-90, pages 94-99, Boston, MA, 1990. [9] S. Markovitch and R. Reger.",
                "Learning and exploiting relative weaknesses of opponent agents.",
                "Autonomous Agents and Multi-Agent Systems, 10(2):103-130, 2005. [10] Y. M. Ronald Fagin, Joseph Y. Halpern and M. Y. Vardi.",
                "Reasoning about knowledge.",
                "MIT Press, Cambridge, Mass., 1995. [11] P. Thagard.",
                "Adversarial problem solving: Modeling an oponent using explanatory coherence.",
                "Cognitive Science, 16(1):123-149, 1992. [12] R. W. Toseland and R. F. Rivas.",
                "An Introduction to Group Work Practice.",
                "Prentice Hall, Englewood Cliffs, NJ, 2nd edition edition, 1995. [13] S. Willmott, J. Richardson, A. Bundy, and J. Levine.",
                "An adversarial planning approach to Go.",
                "Lecture Notes in Computer Science, 1558:93-112, 1999.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 557"
            ],
            "original_annotated_samples": [
                "We now use our estimated evaluation function to evaluate the Black players actions during the Connect-Four <br>adversarial interaction</br>."
            ],
            "translated_annotated_samples": [
                "Ahora utilizamos nuestra función de evaluación estimada para evaluar las acciones de los jugadores Negros durante la <br>interacción adversarial</br> de Conecta Cuatro."
            ],
            "translated_text": "Un modelo de entorno adversarial para agentes racionales limitados en interacciones de suma cero Inon Zuckerman1, Sarit Kraus1, Jeffrey S. Rosenschein2, Gal Kaminka1 1 Departamento de Ciencias de la Computación 2 Escuela de Ingeniería Universidad Bar-Ilan y Ciencias de la Computación Ramat-Gan, Israel Universidad Hebrea, Jerusalén, Israel {zukermi, sarit, galk}@cs.biu.ac.il jeff@cs.huji.ac.il RESUMEN Los entornos multiagentes a menudo no son cooperativos ni colaborativos; en muchos casos, los agentes tienen intereses conflictivos, lo que lleva a interacciones adversariales. Este documento presenta un modelo formal de Entorno Adversarial para agentes racionales acotados que operan en un entorno de suma cero. En tales entornos, los intentos de utilizar métodos de búsqueda basados en utilidad clásica pueden plantear una variedad de dificultades (por ejemplo, modelar implícitamente al oponente como un maximizador de utilidad omnisciente, en lugar de aprovechar un modelo de oponente más matizado y explícito). Definimos un Entorno Adversario al describir los estados mentales de un agente en dicho entorno. Luego presentamos axiomas de comportamiento que están destinados a servir como principios de diseño para construir agentes adversarios. Exploramos la aplicación de nuestro enfoque mediante el análisis de archivos de registro de juegos de Conecta Cuatro completados, y presentamos un análisis empírico de la adecuación de los axiomas. Categorías y Descriptores de Asignaturas I.2.11 [Inteligencia Artificial]: Inteligencia Artificial Distribuida-Agentes Inteligentes, Sistemas Multiagente; I.2.4 [Inteligencia Artificial]: Formalismos y Métodos de Representación del Conocimiento - Lógica Modal Términos Generales Diseño, Teoría 1. Introducción: En las primeras investigaciones en sistemas multiagente (SMA) se consideraban grupos cooperativos de agentes; debido a que los agentes individuales tenían recursos limitados o acceso limitado a la información (por ejemplo, potencia de procesamiento limitada, cobertura sensorial limitada), trabajaban juntos por diseño para resolver problemas que individualmente no podían resolver, o al menos no podían resolver de manera eficiente. Sin embargo, la investigación de MAS pronto comenzó a considerar agentes interactivos con intereses individualizados, como representantes de diferentes humanos u organizaciones con intereses no idénticos. Cuando las interacciones son guiadas por intereses diversos, los participantes pueden tener que superar desacuerdos, interacciones no cooperativas e incluso intentos intencionales de dañarse mutuamente. Cuando ocurren este tipo de interacciones, los entornos requieren un comportamiento apropiado por parte de los agentes situados en ellos. Llamamos a estos entornos Entornos Adversarios, y llamamos a los agentes en conflicto Adversarios. Los modelos de cooperación y trabajo en equipo han sido ampliamente explorados en MAS a través de la axiomatización de estados mentales (por ejemplo, [8, 4, 5]). Sin embargo, ninguna de estas investigaciones abordó los dominios adversarios y sus implicaciones para el comportamiento de los agentes. Nuestro artículo aborda este problema al proporcionar un modelo formal y axiomatizado de estados mentales para un subconjunto de dominios adversariales, específicamente entornos adversariales simples de suma cero. Los encuentros de suma cero simples existen, por supuesto, en varios juegos de dos jugadores (por ejemplo, Ajedrez, Damas), pero también existen en juegos de n jugadores (por ejemplo, Risk, Diplomacia), subastas de un solo bien y en otros lugares. En estos entornos recientes en particular, el uso de una búsqueda adversarial basada en utilidad (como el algoritmo Min-Max) no siempre proporciona una solución adecuada; la función de recompensa podría ser bastante compleja o difícil de cuantificar, y existen limitaciones computacionales naturales en agentes racionales acotados. Además, los métodos de búsqueda tradicionales (como Min-Max) no hacen uso de un modelo del oponente, lo cual ha demostrado ser una adición valiosa a la planificación adversarial [9, 3, 11]. En este artículo, desarrollamos un modelo formal y axiomatizado para agentes racionales limitados que se encuentran en un entorno adversarial de suma cero. El modelo utiliza diferentes operadores de modalidad, y sus principales fundamentos son el modelo SharedPlans [4] para el comportamiento colaborativo. Exploramos las propiedades del entorno y los estados mentales de los agentes para derivar axiomas de comportamiento; estos axiomas de comportamiento constituyen un modelo formal que sirve como especificación y guía de diseño para el diseño de agentes en tales entornos. Luego investigamos el comportamiento de nuestro modelo empíricamente utilizando el juego de mesa Conecta Cuatro. Mostramos que este juego se ajusta a nuestra definición de entorno, y analizamos el comportamiento de los jugadores utilizando un gran conjunto de registros de partidas completadas 550 978-81-904262-7-5 (RPS) c 2007 archivos de IFAAMAS. Además, utilizamos los resultados presentados en [9] para discutir la importancia del modelado del oponente en nuestro dominio adversarial de Conecta Cuatro. El artículo continúa de la siguiente manera. La sección 2 presenta la formalización de los modelos. La sección 3 presenta el análisis empírico y sus resultados. Discutimos el trabajo relacionado en la Sección 4, y concluimos y presentamos las direcciones futuras en la Sección 5. ENTORNOS ADVERSARIOS El modelo de entorno adversario (denominado como AE) está destinado a guiar el diseño de agentes al proporcionar una especificación de las capacidades y actitudes mentales de un agente en un entorno adversario. Nos enfocamos aquí en tipos específicos de entornos adversarios, especificados de la siguiente manera: 1. Interacciones de suma cero: las utilidades positivas y negativas de todos los agentes suman cero; 2. Agentes adversarios simples: todos los agentes en el entorno son agentes adversarios; 3. AEs bilaterales: AEs con exactamente dos agentes; 4. AEs multilaterales: AEs de tres o más agentes. Trabajaremos en instanciaciones tanto bilaterales como multilaterales de entornos de suma cero y simples. En particular, nuestro modelo de entorno adversarial tratará con interacciones que consisten en N agentes (N ≥ 2), donde todos los agentes son adversarios y solo un agente puede tener éxito. Ejemplos de tales entornos van desde juegos de mesa (por ejemplo, Ajedrez, Conecta Cuatro y Diplomacia) hasta ciertos entornos económicos (por ejemplo, subastas de N postores sobre un único bien). 2.1 Resumen del Modelo Nuestro enfoque es formalizar las actitudes mentales y comportamientos de un único agente adversario; consideramos cómo un único agente percibe el AE. La siguiente lista especifica las condiciones y estados mentales de un agente en un AE simple de suma cero: 1. El agente tiene la intención individual de que su propio objetivo se complete; 2. El agente tiene la creencia individual de que él y sus adversarios persiguen objetivos completamente opuestos (definidos a continuación) y que solo puede haber un ganador; 3. El agente tiene la creencia individual de que cada adversario tiene la intención de completar su propio objetivo conflictivo completo; 4. El agente tiene una creencia individual en el perfil (parcial) de sus adversarios. El ítem 3 es necesario, ya que podría ser el caso de que algún agente tenga un objetivo completamente conflictivo y esté considerando adoptar la intención de completarlo, pero aún no está comprometido a lograrlo. Esto podría ocurrir porque el agente aún no ha reflexionado sobre los efectos que adoptar esa intención podría tener en las otras intenciones que actualmente mantiene. En tales casos, es posible que ni siquiera se considere en un entorno adversarial. El ítem 4 establece que el agente debe tener alguna creencia sobre los perfiles de sus adversarios. El perfil representa todo el conocimiento que el agente tiene sobre su adversario: sus debilidades, capacidades estratégicas, objetivos, intenciones, confiabilidad y más. Puede ser dado explícitamente o puede ser aprendido a partir de observaciones de encuentros pasados. 2.2 Definiciones de modelos para estados mentales Utilizamos las definiciones de los operadores modales, predicados y metapredicados de Grosz y Krauss, tal como se definen en su formalización de SharedPlan [4]. Recordamos aquí algunos de los predicados y operadores que se utilizan en esa formalización: Int.To(Ai, α, Tn, Tα, C) representa las intenciones de Ai en el tiempo Tn de realizar una acción α en el tiempo Tα en el contexto de C. Int.Th(Ai, prop, Tn, Tprop, C) representa las intenciones de Ai en el tiempo Tn de que una cierta proposición prop se cumpla en el tiempo Tprop en el contexto de C. Los operadores de intención potencial, Pot.Int.To(...) y Pot.Int.Th(...), se utilizan para representar el estado mental cuando un agente considera adoptar una intención, pero no ha deliberado sobre la interacción de las otras intenciones que tiene. El operador Bel(Ai, f, Tf) representa al agente Ai creyendo en la afirmación expresada en la fórmula f, en el tiempo Tf. MB(A, f, Tf) representa la creencia mutua para un grupo de agentes A. Una instantánea del sistema encuentra que nuestro entorno se encuentra en algún estado e ∈ E de estados variables ambientales, y cada adversario en cualquier LAi ∈ L de posibles estados locales. En cualquier paso de tiempo dado, el sistema estará en algún mundo w del conjunto de todos los mundos posibles w ∈ W, donde w = E×LA1 ×LA2 × ...LAn, y n es el número de adversarios. Por ejemplo, en un juego de póker Texas Holdem, el estado local de un agente podría ser su propio conjunto de cartas (que es desconocido para su adversario) mientras que el entorno consistirá en el bote de apuestas y las cartas comunitarias (que son visibles para ambos jugadores). Una función de utilidad bajo esta formalización se define como un mapeo de un mundo posible w ∈ W a un elemento en , que expresa la deseabilidad del mundo, desde la perspectiva de un único agente. Normalmente normalizamos el rango a [0,1], donde 0 representa el mundo menos deseable posible, y 1 es el mundo más deseable. La implementación de la función de utilidad depende del dominio en cuestión. La siguiente lista especifica nuevos predicados, funciones, variables y constantes utilizadas en conjunto con las definiciones originales para la formalización del entorno adversarial: 1. φ es una acción nula (el agente no hace nada). 2. GAi es el conjunto de objetivos del agente Ai. Cada objetivo es un conjunto de predicados cuya satisfacción hace que el objetivo esté completo (usamos G∗ Ai ∈ GAi para representar un objetivo arbitrario del agente Ai). 3. gAi es el conjunto de subobjetivos del agente Ai. Los subobjetivos son predicados cuya satisfacción representa un hito importante hacia el logro del objetivo completo. gG∗ Ai ⊆ gAi es el conjunto de subobjetivos que son importantes para la finalización del objetivo G∗ Ai (usaremos g∗ G∗ Ai ∈ gG∗ Ai para representar un subobjetivo arbitrario). 4. P Aj Ai es el objeto de perfil que el agente Ai tiene sobre el agente Aj. 5. CA es un conjunto general de acciones para todos los agentes en A que se derivan de las restricciones del entorno. CAi ⊆ CA es el conjunto de acciones posibles del agente Ai. La expresión Do(Ai, α, Tα, w) se cumple cuando Ai realiza la acción α durante el intervalo de tiempo Tα en el mundo w. 7. La afirmación Achieve(G∗ Ai , α, w) es verdadera cuando se logra el objetivo G∗ Ai siguiendo la finalización de la acción α en el mundo w ∈ W, donde α ∈ CAi. El perfil (Ai, PAi Ai) es verdadero cuando el agente Ai tiene un perfil de objeto para el agente Aj. Definición 1. El conflicto total (FulConf) describe una interacción de suma cero donde solo se puede completar un solo objetivo de los objetivos en conflicto. FulConf(G∗ Ai , G∗ Aj ) ⇒ (∃α ∈ CAi , ∀w, β ∈ CAj ) (Achieve(G∗ Ai , α, w) ⇒ ¬Achieve(G∗ Aj , β, w)) ∨ (∃β ∈ CAj , ∀w, α ∈ CAi )(Achieve(G∗ Aj , β, w) ⇒ ¬Achieve(G∗ Ai , α, w)) Definición 2. El Conocimiento Adversarial (AdvKnow) es una función que devuelve un valor que representa la cantidad de The Sixth Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 551 agente de conocimiento Ai tiene sobre el perfil del agente Aj, en el tiempo Tn. Cuanto mayor sea el valor, más conocimiento tiene el agente Ai. AdvKnow: P Aj Ai × Tn → Definición 3. Eval: Esta función de evaluación devuelve un valor de utilidad esperada estimado para un agente en A, después de completar una acción de CA en algún estado del mundo w. Eval: A × CA × w → Definición 4. TrH - (Umbral) es una constante numérica en el rango [0,1] que representa un valor umbral de la función de evaluación (Eval). Una acción que produce una evaluación de utilidad estimada por encima del TrH se considera una acción altamente beneficiosa. El valor Eval es una estimación y no la función de utilidad real, la cual suele ser desconocida. Utilizar el valor real de utilidad para un agente racional fácilmente produciría el mejor resultado para ese agente. Sin embargo, los agentes generalmente no tienen las funciones de utilidad reales, sino más bien una estimación heurística de las mismas. Existen dos propiedades importantes que deben cumplir para la función de evaluación: Propiedad 1. La función de evaluación debería indicar que el estado del mundo más deseable es aquel en el que se logra el objetivo. Por lo tanto, una vez que se ha satisfecho el objetivo, no puede haber ninguna acción futura que pueda colocar al agente en un estado del mundo con un valor de Eval más alto. (∀Ai, G∗ Ai , α, β ∈ CAi , w ∈ W) Lograr(G∗ Ai , α, w) ⇒ Eval(Ai, α, w) ≥ Eval(Ai, β, w) Propiedad 2. La función de evaluación debe proyectar una acción que cause la finalización de un objetivo o subobjetivo a un valor que sea mayor que TrH (una acción altamente beneficiosa). (∀Ai, G∗ Ai ∈ GAi , α ∈ CAi , w ∈ W, g∗ GAi ∈ gGAi ) Lograr(G∗ Ai , α, w) ∨ Lograr(g∗ GAi , α, w) ⇒ Eval(Ai, α, w) ≥ TrH. Definición 5. La acción de conjunto Definimos una acción de conjunto (SetAction) como un conjunto de operaciones de acción (ya sean acciones complejas o básicas) de algunos conjuntos de acción CAi y CAj que, según la creencia del agente Ai, están unidas por una relación temporal y consecuente, formando una cadena de eventos (acción y su acción consecuente siguiente). (∀α1 , . . . , αu ∈ CAi , β1 , . . . , βv ∈ CAj , w ∈ W) SetAction(α1 , . . . , αu , β1 , . . . , βv , w) ⇒ ((Do(Ai, α1 , Tα1 , w) ⇒ Do(Aj, β1 , Tβ1 , w)) ⇒ Do(Ai, α2 , Tα2 , w) ⇒ . . . ⇒ Do(Ai, αu , Tαu , w)) La relación consecuente podría existir debido a varias restricciones ambientales (cuando una acción obliga al adversario a responder con una acción específica) o debido al conocimiento de los agentes sobre el perfil de su adversario. Propiedad 3. A medida que aumenta el conocimiento que tenemos sobre nuestro adversario, tendremos creencias adicionales sobre su comportamiento en diferentes situaciones, lo que a su vez crea un nuevo conjunto de acciones. Formalmente, si nuestro AdvKnow en el tiempo Tn+1 es mayor que AdvKnow en el tiempo Tn, entonces cada SetAction conocido en el tiempo Tn también es conocido en el tiempo Tn+1. AdvKnow(P Aj Ai , Tn+1) > AdvKnow(P Aj Ai , Tn) ⇒ (∀α1 , . . . , αu ∈ CAi , β1 , . . . , βv ∈ CAj ) Bel(Aag, SetAction(α1 , . . . , αu , β1 , . . . , βv ), Tn) ⇒ Bel(Aag, SetAction(α1 , . . . , αu , β1 , . . . , βv ), Tn+1) 2.3 La formulación del entorno Los siguientes axiomas proporcionan la definición formal para un Entorno Adversarial (AE) simple y de suma cero. La satisfacción de estos axiomas significa que el agente está situado en un entorno así. Proporciona especificaciones para que el agente Aag interactúe con su conjunto de adversarios A con respecto a los objetivos G∗ Aag y G∗ A en el tiempo TCo en algún estado del mundo w. AE(Aag, A, G∗ Aag, A1, . . . , Ak, G∗ A1, . . . , G∗ Ak, Tn, w) 1. Aag tiene un Int.Th su objetivo se completaría: (∃α ∈ CAag , Tα) Int.Th(Aag, Lograr(G∗ Aag , α), Tn, Tα, AE) 2. Aag cree que él y cada uno de sus adversarios Ao están persiguiendo metas completamente conflictivas: (∀Ao ∈ {A1, . . . , Ak}) Bel(Aag, FulConf(G∗ Aag , G∗ Ao ), Tn) 3. Aag cree que cada uno de sus adversarios en Ao tiene la Int.Th su objetivo en conflicto G∗ Aoi se completará: (∀Ao ∈ {A1, . . . , Ak})(∃β ∈ CAo , Tβ) Bel(Aag, Int.Th(Ao, Lograr(G∗ Ao , β), TCo, Tβ, AE), Tn) 4. Para construir un agente que pueda operar con éxito dentro de un entorno de adversarios estratégicos, debemos especificar pautas de comportamiento para sus interacciones. Utilizar una estrategia de maximización de Eval ingenua hasta cierta profundidad de búsqueda no siempre dará resultados satisfactorios por varias razones: (1) el problema del horizonte de búsqueda al buscar a una profundidad fija; (2) la fuerte suposición de un adversario óptimamente racional con recursos ilimitados; (3) utilizar una función de evaluación estimada que no dará resultados óptimos en todos los estados del mundo, y que puede ser explotada [9]. Los siguientes axiomas especifican los principios de comportamiento que se pueden utilizar para diferenciar entre agentes exitosos y menos exitosos en el entorno adversarial mencionado anteriormente. Esos axiomas deben ser utilizados como principios de especificación al diseñar e implementar agentes que deben ser capaces de desempeñarse bien en entornos adversarios. Los axiomas conductuales representan situaciones en las que el agente adoptará intenciones potenciales para (Pot.Int.To(...)) realizar una acción, lo que típicamente requerirá un razonamiento de medios-fines para seleccionar un posible curso de acción. Este razonamiento llevará a la adopción de un Int.To(...) (ver [4]). A1. Axioma para lograr metas. El primer axioma es el caso más simple; cuando el agente Aag cree que está a una acción (α) de lograr su objetivo en conflicto G∗ Aag, debería adoptar la intención potencial de hacer α y completar su objetivo. (∀Aag, α ∈ CAag, Tn, Tα, w ∈ W) (Bel(Aag, Do(Aag, α, Tα, w) ⇒ Achieve(G∗ Aag, α, w)) ⇒ Pot.Int.To(Aag, α, Tn, Tα, w) Este comportamiento algo trivial es el primer y más fuerte axioma. En cualquier situación, cuando el agente está a una acción de completar el objetivo, debería completar la acción. Cualquier función de evaluación justa clasificaría naturalmente a α como la acción de valor máximo (propiedad 1). Sin embargo, sin una axiomatización explícita de dicho comportamiento, podría haber situaciones en las que el agente decida tomar otra acción por diversas razones, debido a sus recursos de decisión limitados. A2. Axioma del Acto Preventivo. En una situación adversarial, el agente Aag podría decidir tomar acciones que dañen uno de los planes de su adversario para completar su objetivo, incluso si esas acciones no avanzan explícitamente a Aag hacia su objetivo en conflicto G∗ Aag. Tal acción preventiva tendrá lugar cuando el agente Aag tenga la creencia sobre la posibilidad de que su adversario Ao realice una acción β que le otorgará un alto 552 The Sixth Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) valor de evaluación de utilidad (> TrH). Creer que tomar la acción α evitará que el oponente realice su β, adoptará una intención potencial de hacer α. (∀Aag, Ao ∈ A, α ∈ CAag , β ∈ CAo , Tn, Tβ, w ∈ W) (Bel(Aag, Do(Ao, β, Tβ, w) ∧ Eval(Ao, β, w) > TrH, Tn) ∧ Bel(Aag, Do(Aag, α, Tα, w) ⇒ ¬Do(Ao, β, Tβ, w), Tn) ⇒ Pot.Int.To(Aag, α, Tn, Tα, w) Este axioma es un componente básico de cualquier entorno adversarial. Por ejemplo, al observar un juego de ajedrez, un jugador podría darse cuenta de que está a punto de ser jaque mateado por su oponente, por lo tanto, realiza una jugada preventiva. Otro ejemplo es el juego de Cuatro en Línea: cuando un jugador tiene una fila de tres fichas, su oponente debe bloquearla o perder. Una instancia específica de A1 ocurre cuando el adversario está a una acción de lograr su objetivo, y la acción preventiva inmediata debe ser tomada por el agente. Formalmente, tenemos las mismas creencias que se mencionaron anteriormente, con la creencia modificada de que realizar la acción β hará que el agente Ao logre su objetivo. Proposición 1: Caso de prevención o pérdida. (∀Aag, Ao ∈ A, α ∈ CAag , β ∈ CAo , G∗ Ao , Tn, Tα, Tβ, w ∈ W) Bel(Aag, Do(Ao, β, Tβ, w) ⇒ Achieve(G∗ Ao , β, w), Tn) ∧ Bel(Aag, Do(Aag, α, Tα, w) ⇒ ¬Do(Ao, β, Tβ, w)) ⇒ Pot.Int.To(Aag, α, Tn, Tα, w) Esquema de prueba: La Proposición 1 puede derivarse fácilmente del axioma A1 y de la propiedad 2 de la función Eval, que establece que cualquier acción que cause la finalización de un objetivo es una acción altamente beneficiosa. El comportamiento del acto preventivo ocurrirá implícitamente cuando la función Eval sea igual a la función de utilidad del mundo real. Sin embargo, al ser agentes racionales limitados y al tratar con una función de evaluación estimada, necesitamos axiomatizar explícitamente dicho comportamiento, ya que no siempre ocurrirá implícitamente a partir de la función de evaluación. A3. Axioma de Movimiento Táctico Subóptimo. En muchos escenarios, puede ocurrir una situación en la que un agente decida no tomar la acción más beneficiosa actual que puede tomar (la acción con el valor de evaluación de utilidad máximo), porque cree que tomar otra acción (con un valor de evaluación de utilidad más bajo) podría generar (dependiendo de la respuesta del adversario) una posibilidad futura para una acción altamente beneficiosa. Esto ocurrirá con mayor frecuencia cuando la función Eval sea inexacta y difiera en gran medida de la función de Utilidad. De manera formal, el agente Aag cree en una cierta SetAction que evolucionará de acuerdo con su acción inicial y producirá un valor beneficioso alto (> TrH) únicamente para él. (∀Aag, Ao ∈ A, Tn, w ∈ W) (∃α1 , . . . , αu ∈ CAi , β1 , . . . , βv ∈ CAj , Tα1 ) Bel(Aag, SetAction(α1 , . . . , αu , β1 , . . . , βv ), Tn) ∧ Bel(Aag, Eval(Ao, βv , w) < TrH < Eval(Aag, αu , w), Tn) ⇒ Pot.Int.To(Aag, α1 , Tn, Tα1 , w) Un agente podría creer que una cadena de eventos ocurrirá por diversas razones debido a la naturaleza inevitable del dominio. Por ejemplo, en el ajedrez, a menudo observamos lo siguiente: una jugada provoca una posición de jaque, lo que a su vez limita los movimientos del oponente para evitar el jaque, a lo que el primer jugador podría reaccionar con otro jaque, y así sucesivamente. El agente también podría creer en una cadena de eventos basada en su conocimiento del perfil de su adversario, lo que le permite prever los movimientos del adversario con alta precisión. A4. Axioma de Detección de Perfiles. El agente puede ajustar los perfiles de sus adversarios mediante observaciones y estudio de patrones (específicamente, si hay encuentros repetidos con el mismo adversario). Sin embargo, en lugar de esperar a que se revele la información del perfil, un agente también puede iniciar acciones que obligarán a su adversario a reaccionar de una manera que revele conocimiento del perfil sobre él. Formalmente, el axioma establece que si todas las acciones (γ) no son acciones altamente beneficiosas (< TrH), el agente puede realizar la acción α en el tiempo Tα si cree que resultará en una acción no altamente beneficiosa β por parte de su adversario, lo que a su vez le enseña sobre el perfil del adversario, es decir, proporciona un mayor AdvKnow(P Aj Ai , Tβ). (∀Aag, Ao ∈ A, α ∈ CAag , β ∈ CAo , Tn, Tα, Tβ, w ∈ W) Bel(Aag, (∀γ ∈ CAag )Eval(Aag, γ, w) < TrH, Tn) ∧ Bel(Aag, Do(Aag, α, Tα, w) ⇒ Do(Ao, β, Tβ, w), Tn) ∧ Bel(Aag, Eval(Ao, β, w) < TrH) ∧ Bel(Aag, AdvKnow(P Aj Ai , Tβ) > AdvKnow(P Aj Ai , Tn), Tn) ⇒ Pot.Int.To(Aag, α, Tn, Tα, w) Por ejemplo, volviendo al escenario del juego de ajedrez, consideremos comenzar una partida contra un oponente del que no sabemos nada, ni siquiera si es humano o un oponente computarizado. Podríamos comenzar a jugar una estrategia que sea adecuada frente a un oponente promedio, y ajustar nuestro juego de acuerdo a su nivel de juego. A5. Axioma de Formación de Alianzas El siguiente axioma de comportamiento es relevante únicamente en una instancia multilateral del entorno adversarial (obviamente, una alianza no puede formarse en un encuentro bilateral de suma cero). En diferentes situaciones durante una interacción multilateral, un grupo de agentes podría creer que es de su interés formar una alianza temporal. Una alianza de este tipo es un acuerdo que limita el comportamiento de sus miembros, pero se cree por parte de estos que les permite alcanzar un valor de utilidad mayor al que podrían lograr fuera de la alianza. Como ejemplo, podemos observar el clásico juego de mesa Risk, donde cada jugador tiene como objetivo individual ser el único conquistador del mundo, un juego de suma cero. Sin embargo, para lograr este objetivo, podría ser estratégicamente sabio hacer acuerdos de alto el fuego a corto plazo con otros jugadores, o unirse y atacar a un oponente que sea más fuerte que el resto. Los términos de una alianza definen la forma en que sus miembros deben actuar. Es un conjunto de predicados, denominados Términos, que es acordado por los miembros de la alianza y debe mantenerse verdadero durante la duración de la alianza. Por ejemplo, el conjunto de términos en el escenario de riesgo podría contener los siguientes predicados: 1. Los miembros de la alianza no se atacarán entre sí en los territorios X, Y y Z; 2. Los miembros de la alianza contribuirán C unidades por turno para atacar al adversario Ao; 3. Los miembros están obligados a permanecer como parte de la alianza hasta el tiempo Tk o hasta que el ejército adversario Ao sea más pequeño que Q. El conjunto de términos especifica restricciones intergrupales en cada uno de los miembros de la alianza (∀Aal i ∈ Aal ⊆ A) conjunto de acciones Cal i ⊆ C. Definición 6. El valor total de evaluación que el agente Ai logrará al ser parte de Aal es la suma de Evali (Eval para Ai) de cada uno de los valores de evaluación de Aal j después de tomar sus propias acciones α (a través del predicado del agente(α)): Al val(Ai, Cal, Aal, w) = α∈Cal Evali(Aal j, agente(α), w) Definición 7. Al TrH - es un número que representa un valor de Al en la Sexta Conferencia Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 553 umbral; por encima de él, se puede decir que la alianza es una alianza altamente beneficiosa. El valor de Al TrH se calculará dinámicamente de acuerdo con el progreso de la interacción, como se puede ver en [7]. Después de formarse una alianza, sus miembros ahora están trabajando en su entorno adversarial normal, así como de acuerdo con los estados mentales y axiomas requeridos para sus interacciones como parte de la alianza. El siguiente modelo de Alianza (AL) especifica las condiciones bajo las cuales el grupo Aal puede considerarse en una alianza y trabajando con un nuevo y limitado conjunto de acciones Cal, en el tiempo Tn. AL(Aal , Cal , w, Tn) 1. \n\nAL(Aal , Cal , w, Tn) 1. Aal tiene un MB en el que todos los miembros son parte de Aal: MB(Aal, (∀Aal i ∈ Aal)miembro(Aal i, Aal), Tn) 2. Aal tiene una MB que el grupo se mantenga: MB(Aal , (∀Aal i ∈ Aal )Int.Th (Ai, miembro(Ai, Aal ), Tn, Tn+1, Co), Tn) 3. Aal tiene un beneficio marginal que al ser miembros les otorga un alto valor de utilidad: MB(Aal , (∀Aal i ∈ Aal )Al val(Aal i , Cal , Aal , w) ≥ Al TrH, Tn) Los perfiles de los miembros son una parte crucial de las alianzas exitosas. Suponemos que los agentes que tienen perfiles más precisos de sus adversarios serán más exitosos en tales entornos. Tales agentes podrán predecir cuándo un miembro está a punto de violar el contrato de alianzas (elemento 2 en el modelo anterior) y tomar medidas correctivas (cuando el elemento 3 se falsificará). La robustez de la alianza depende en parte de la medida de confianza de sus miembros, la estimación de su posición objetiva y otras propiedades de su perfil. Debemos tener en cuenta que un agente puede formar parte simultáneamente de más de una alianza. Una alianza temporal, donde los miembros del grupo no tienen un objetivo común pero actúan colaborativamente en interés de sus propias metas individuales, es clasificada como un Grupo de Tratamiento por los psicólogos modernos (en contraste con un Grupo de Tarea, donde sus miembros tienen un objetivo común). El modelo de Actividad Compartida presentado en [5] modeló el comportamiento del Grupo de Tratamiento utilizando la misma formalización de Planes Compartidos. Al comparar ambas definiciones de una alianza y un Grupo de Tratamiento, encontramos una similitud sorprendente entre ambos modelos: las definiciones de los modelos de entorno son casi idénticas (ver las definiciones de SAs en [5]), y sus axiomas de Acto Egoísta y Acto Cooperativo se ajustan al comportamiento de nuestros agentes adversarios. La principal distinción entre ambos modelos es la integración de un axioma de acto de comportamiento útil, en la Actividad Compartida que no puede formar parte de la nuestra. Este axioma establece que un agente considerará tomar una acción que disminuirá su valor de Evaluación (hasta un cierto límite inferior), si cree que un compañero del grupo obtendrá un beneficio significativo. Dicho comportamiento no puede ocurrir en un entorno puramente adversarial (como un juego de suma cero), donde los miembros de la alianza están constantemente alerta para manipular su alianza en su propio beneficio. A6. Axioma de Maximización de la Evaluación. En un caso en el que todos los demás axiomas no sean aplicables, procederemos con la acción que maximice el valor heurístico calculado en la función Eval. (∀Aag, Ao ∈ A, α ∈ Cag, Tn, w ∈ W) Bel(Aag, (∀γ ∈ Cag)Eval(Aag, α, w) ≥ Eval(Aag, γ, w), Tn) ⇒ Pot.Int.To(Aag, α, Tn, Tα, w) T1. Optimalidad en Eval = Utilidad El modelo axiomático anterior aborda situaciones en las que la Utilidad es desconocida y los agentes son agentes racionales limitados. El siguiente teorema muestra que en interacciones bilaterales, donde los agentes tienen la función de Utilidad real (es decir, Eval = Utilidad) y son agentes racionales, los axiomas proporcionan el mismo resultado óptimo que la búsqueda adversarial clásica (por ejemplo, Min-Max). Teorema 1. Que Ae ag sea un agente AE racional ilimitado que utiliza la función de evaluación heurística Eval, Au ag sea el mismo agente utilizando la verdadera función de utilidad y Ao sea un único adversario racional ilimitado basado en utilidad. Dado que Eval = Utilidad: (∀α ∈ CAu ag , α ∈ CAe ag , Tn, w ∈ W) Pot.Int.To(Au ag, α, Tn, Tα, w) → Pot.Int.To(Ae ag, α , Tn, Tα, w) ∧ ((α = α ) ∨ (Utilidad(Au ag, α, w) = Eval(Ae ag, α , w))) Esquema de prueba - Dado que Au ag tiene la función de utilidad real y recursos ilimitados, puede generar el árbol de juego completo y ejecutar el algoritmo MinMax óptimo para elegir la acción de mayor valor de utilidad, que denotamos como α. La prueba mostrará que Ae ag, utilizando los axiomas AE, seleccionará la misma utilidad α o igual (cuando hay más de una acción con la misma utilidad máxima) cuando Eval = Utilidad. (A1) Axioma de logro de objetivo - supongamos que hay un α tal que su cumplimiento logrará el objetivo Au ags. Obtendrá la utilidad más alta mediante Min-Max para Au ag. El agente Ae seleccionará α u otra acción con el mismo valor de utilidad a través de A1. Si tal α no existe, Ae ag no puede aplicar este axioma y procede a A2. (A2) Axioma de la acción preventiva - (1) Observando el caso base (ver Prop1), si hay un β que lleva a Ao a alcanzar su objetivo, entonces una acción preventiva α producirá la mayor utilidad para Au ag. Au ag lo elegirá a través de la utilidad, mientras que Ae ag lo elegirá a través de A2. En el caso general, β es una acción altamente beneficiosa para Ao, lo que resulta en una baja utilidad para Au ag, lo que lo guiará a seleccionar un α que evitará β, mientras que Ae ag lo elegirá a través de A2.1 Si dicho β no existe para Ao, entonces A2 no es aplicable, y Ae ag puede proceder a A3. (A3) Axioma de movimiento táctico subóptimo: al usar una función de evaluación heurística, Ae ag tiene una creencia parcial en el perfil de su adversario (elemento 4 en el modelo AE), lo que puede llevarlo a creer en SetActions (Prop1). En nuestro caso, Ae ag tiene un perfil completo de su adversario óptimo y sabe que Ao se comportará de manera óptima según los valores reales de utilidad en el árbol de búsqueda completo, por lo tanto, cualquier creencia sobre una SetAction subóptima no puede existir, lo que hace que este axioma sea inaplicable. Ae ag procederá a A4. (A4) Axioma de detección de perfil - Dado que Ae ag tiene el perfil completo de Ao, ninguna de las acciones de Ae ag puede aumentar su conocimiento. Ese axioma no se aplicará, y el agente procederá con A6 (A5 será ignorado porque la interacción es bilateral). (A6) Axioma de maximización de la evaluación: este axioma seleccionará la máxima Evaluación para Ae ag. Dado que Eval = Utilidad, se seleccionará el mismo α que fue seleccionado por Au ag. 3. EVALUACIÓN El propósito principal de nuestro análisis experimental es evaluar el comportamiento y rendimiento de los modelos en un entorno adversarial real. Esta sección investiga si un caso acotado 1 en el que, tras la finalización de β, exista un γ que proporcione una alta utilidad para el Agente Au ag, no puede ocurrir porque Ao utiliza la misma utilidad, y la existencia de γ hará que clasifique a β como una acción de baja utilidad. 554 The Sixth Intl. En la Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07), los agentes racionales situados en entornos adversariales se beneficiarán más al aplicar nuestros axiomas de comportamiento sugeridos. 3.1 El Dominio Para explorar el uso del modelo anterior y sus axiomas de comportamiento, decidimos utilizar el juego de Conecta Cuatro como nuestro entorno adversarial. Connect-Four es un juego de 2 jugadores, de suma cero, que se juega utilizando un tablero similar a una matriz de 6x7. En cada turno, un jugador coloca un disco en una de las 7 columnas (el conjunto de 21 discos suele ser de color amarillo para el jugador 1 y rojo para el jugador 2; usaremos Blanco y Negro respectivamente para evitar confusiones). El ganador es el primer jugador en completar un conjunto de cuatro discos de su color en horizontal, vertical o diagonal. En ocasiones muy raras, el juego podría terminar en empate si todos los cuadros vacíos están llenos, pero ningún jugador logró crear un conjunto de 4 discos. El juego de Conecta Cuatro fue resuelto en [1], donde se muestra que el primer jugador (jugando con los discos blancos) puede forzar una victoria comenzando en la columna del medio (columna 4) y jugando de manera óptima. Sin embargo, la estrategia óptima es muy compleja y difícil de seguir incluso para agentes racionales limitados complejos, como los jugadores humanos. Antes de que podamos proceder a verificar el comportamiento del agente, primero debemos verificar que el dominio se ajusta a la definición de entornos adversarios mencionada anteriormente (en la que se basan los axiomas de comportamiento). Primero, al jugar al juego de Conecta Cuatro, el agente tiene la intención de ganar el juego (ítem 1). Segundo (ítem 2), nuestro agente cree que en Conecta Cuatro solo puede haber un ganador (o ningún ganador en la rara ocasión de un empate). Además, nuestro agente cree que su oponente en el juego intentará ganar (ítem 3), y esperamos que tenga algún conocimiento parcial (ítem 4) sobre su adversario (este conocimiento puede variar desde nada, pasando por hechos simples como la edad, hasta estrategias y debilidades). Por supuesto, no todos los encuentros de Conecta Cuatro son adversariales. Por ejemplo, cuando un padre está jugando el juego con su hijo, la siguiente situación podría ocurrir: el niño, con un fuerte incentivo para ganar, trata el entorno como adversarial (intenta ganar, entiende que solo puede haber un ganador y cree que su padre está tratando de vencerlo). Sin embargo, desde el punto de vista de los padres, el entorno podría ser visto como educativo, donde el objetivo no es ganar el juego, sino causar disfrute o practicar el razonamiento estratégico. En un entorno educativo como este, un nuevo conjunto de axiomas de comportamiento podría ser más beneficioso para los objetivos de los padres que nuestros axiomas de comportamiento adversarial sugeridos. 3.2 Análisis de Axiomas Después de demostrar que el juego de Conecta Cuatro es de hecho un entorno adversarial bilateral de suma cero, el siguiente paso es observar el comportamiento de los jugadores durante el juego y verificar si comportarse de acuerdo con nuestro modelo mejora el rendimiento. Para hacerlo, hemos recopilado archivos de registro de juegos de Conecta Cuatro completados que fueron jugados por jugadores humanos a través de Internet. Nuestros datos de archivo de registro recopilados provienen de sitios de Juego por Correo Electrónico (PBeM). Estos son sitios web que alojan juegos por correo electrónico, donde cada movimiento se realiza a través de un intercambio de correos electrónicos entre el servidor y los jugadores. Muchos de estos sitios de archivos contienen interacciones competitivas reales y también mantienen un sistema de clasificación para sus miembros. La mayoría de los datos que utilizamos se pueden encontrar en [6]. Como se puede aprender de [1], Conecta Cuatro tiene una estrategia óptima y una ventaja considerable para el jugador que comienza el juego (a quien llamamos jugador Blanco). Nos concentraremos en nuestro análisis en los movimientos de los segundos jugadores (a los que llamaremos Negras). El jugador Blanco, al ser el primero en actuar, tiene la llamada ventaja de la iniciativa. Tener la ventaja y una buena estrategia mantendrá al jugador Negro ocupado reaccionando a sus movimientos, en lugar de iniciar amenazas. Una amenaza es una combinación de tres discos del mismo color, con un espacio vacío para el cuarto disco ganador. Una amenaza abierta es una amenaza que puede materializarse en el próximo movimiento del oponente. Para que el jugador Negro gane, debe de alguna manera cambiar el rumbo, tomar la ventaja y comenzar a presentar amenazas al jugador Blanco. Exploraremos el comportamiento de los jugadores negros y su conformidad con nuestros axiomas. Para hacerlo, construimos una aplicación que lee archivos de registro y analiza los movimientos de los jugadores Negros. La aplicación contiene dos componentes principales: (1) un algoritmo Min-Max para la evaluación de movimientos; (2) un detector de amenazas abiertas para descubrir amenazas abiertas. El algoritmo Min-Max funcionará hasta una profundidad d dada y, para cada movimiento α, devolverá el valor heurístico para la próxima acción tomada por el jugador según lo escrito en el archivo de registro, h(α), junto con el valor heurístico máximo, maxh(α), que podría lograrse antes de realizar el movimiento (obviamente, si h(α) = maxh(α), entonces el jugador no realizó el movimiento óptimo desde el punto de vista heurístico). El trabajo de los detectores de amenazas es notificar si se tomó alguna acción para bloquear una amenaza abierta (no bloquear una amenaza abierta probablemente hará que el jugador pierda en el próximo movimiento del oponente). La función heurística utilizada por Min-Max para evaluar la utilidad de los jugadores es la siguiente función, que es simple de calcular, pero proporciona un desafío razonable a los oponentes humanos: Definición 8. Sea Grupo un conjunto adyacente de cuatro cuadrados que son horizontales, verticales o diagonales. El Grupo n (Grupo w) es un Grupo con n piezas de color negro (blanco) y 4−n casillas vacías. h = ((Grupo1 negro ∗α)+(Grupo2 negro ∗β)+(Grupo3 negro ∗γ)+(Grupo4 negro ∗∞)) − ((Grupo1 blanco ∗α)+(Grupo2 blanco ∗β)+(Grupo3 blanco ∗γ)+(Grupo4 blanco ∗∞)) Los valores de α, β y δ pueden variar para formar cualquier combinación lineal deseada; sin embargo, es importante asignarles valores teniendo en cuenta el orden α < β < δ (usamos 1, 4 y 8 como sus valores respectivos). Grupos de 4 discos del mismo color significan victoria, por lo tanto, el descubrimiento de dicho grupo resultará en ∞ para asegurar un valor extremo. Ahora utilizamos nuestra función de evaluación estimada para evaluar las acciones de los jugadores Negros durante la <br>interacción adversarial</br> de Conecta Cuatro. Cada juego del archivo de registro fue introducido en la aplicación, la cual procesó y generó un archivo de registro reformateado que contenía el valor h del movimiento actual, el valor maxh que podría ser alcanzado y una notificación si se detectaba una amenaza abierta. Un total de 123 partidas fueron analizadas (57 con victoria de las blancas y 66 con victoria de las negras). Algunos juegos adicionales fueron ignorados manualmente en el experimento, debido a estos problemas: un jugador abandonando el juego mientras el resultado no es definitivo, o una jugada irracional y contundente en las primeras etapas del juego (por ejemplo, no bloquear un grupo obviamente ganador en las primeras jugadas de apertura). Además, también se eliminó un empate. El simulador se ejecutó con una profundidad de búsqueda de 3 movimientos. Ahora procedemos a analizar los juegos con respecto a cada axioma de comportamiento. El Sexto Internacional. La siguiente sección presenta las evaluaciones heurísticas del algoritmo Min-Max para cada acción, y verifica la cantidad y el alcance de las acciones tácticas subóptimas y sus implicaciones en el rendimiento. La Tabla 1 muestra los resultados y las percepciones del análisis heurístico de los juegos, cuando la profundidad de búsqueda es igual a 3 (esta profundidad de búsqueda fue seleccionada para que los resultados sean comparables a [9], ver Sección 3.2.3). La información heurística de las tablas es la diferencia entre el valor heurístico máximo presente y el valor heurístico de la acción que finalmente tomó el jugador (es decir, cuanto más cercano sea el número a 0, más cercana estuvo la acción a la acción heurística máxima). La primera fila presenta los valores de diferencia de la acción que tuvo el valor de diferencia máximo entre todas las acciones de los jugadores Negros en un juego dado, promediado en todos los juegos ganados y perdidos por los Negros (ver columnas respectivas). En los juegos en los que el jugador Negro pierde, su valor promedio de diferencia fue de -17.62, mientras que en los juegos en los que el jugador Negro ganó, su promedio fue de -12.02. La segunda fila amplía el análisis al considerar las 3 acciones con mayor diferencia heurística, y promediarlas. En ese caso, notamos una diferencia heurística promedio de 5 puntos entre los juegos en los que el jugador Negro pierde y los juegos en los que gana. Sin embargo, la importancia de esos números es que nos permitieron hacer una suposición educada sobre un número umbral de 11.5, como el valor de la constante TrH, que diferencia entre acciones normales y altamente beneficiosas. Después de encontrar una constante TrH aproximada, podemos proceder con un análisis de la importancia de los movimientos subóptimos. Para hacerlo, tomamos el subconjunto de juegos en los que el valor mínimo de diferencia heurística para las acciones de las negras era de 11.5. Como se presenta en la Tabla 2, podemos ver el promedio de 3 horas mínimo de los 3 mayores rangos y el porcentaje respectivo de juegos ganados. La primera fila muestra que el jugador Negro ganó solo el 12% de los juegos en los que el promedio de sus 3 acciones con mayor diferencia heurística (min3 h) era menor que el umbral sugerido, TrH = 11.5. La segunda fila muestra un resultado sorprendente: parece que cuando min3 h > −4, el jugador Negro rara vez gana. La intuición sugeriría que los juegos en los que los valores de evaluación de la acción estén más cerca de los valores máximos resultarán en más juegos ganados para las piezas negras. Sin embargo, parece que en el dominio de Conecta Cuatro, simplemente responder con acciones algo fácilmente esperadas, sin iniciar algunos movimientos sorprendentes y subóptimos, no produce buenos resultados. La última fila resume las principales conclusiones del análisis; la mayoría de las victorias de Blacks (83%) se produjeron cuando su min3 h estaba en el rango de -11.5 a -4. Una inspección detallada de esos juegos ganados por las fichas negras muestra el siguiente patrón detrás de los números: después de los movimientos de apertura estándar, las fichas negras de repente colocan una ficha en una columna aislada, lo cual parece ser un desperdicio de un movimiento. El blanco sigue construyendo sus amenazas, generalmente ignorando el último movimiento de las negras, que a su vez utiliza el disco aislado como ancla para una futura amenaza ganadora. Los resultados muestran que fue beneficioso para el jugador Negro en la Tabla 2: Porcentajes de ganancias de los Negros % de juegos min3 h < −11.5 12% min3 h > −4 5% −11.5 ≤ min3 h ≤ −4 83% tomar acciones subóptimas y no dar el valor heurístico más alto posible en ese momento, pero no será demasiado perjudicial para su posición (es decir, no dará un alto valor beneficioso a su adversario). Resultó que aprender el umbral es un aspecto importante del éxito: realizar movimientos extremadamente arriesgados (min3 h < −11.5) o intentar evitarlos (min3 h > −4) reduce significativamente las posibilidades de ganar de los jugadores negros. 3.2.2 Afirmación del Axioma de Monitoreo de Perfiles En la tarea de demostrar la importancia de monitorear los perfiles de los adversarios, nuestros archivos de registro no pudieron ser utilizados porque no contenían interacciones repetidas entre jugadores, que son necesarias para inferir el conocimiento de los jugadores sobre sus adversarios. Sin embargo, la importancia del modelado del oponente y su uso para obtener ventajas tácticas ya ha sido estudiada en varios ámbitos ([3, 9] son buenos ejemplos). En un artículo reciente, Markovitch y Reger [9] exploraron la noción de aprendizaje y explotación de la debilidad del oponente en interacciones competitivas. Aplican estrategias de aprendizaje simples analizando ejemplos de interacciones pasadas en un dominio específico. También utilizaron el dominio adversarial de Conecta Cuatro, que ahora se puede utilizar para comprender la importancia de monitorear el perfil del adversario. Tras la presentación de su modelo teórico, describen un extenso estudio empírico y verifican el rendimiento de los agentes después de aprender el modelo de debilidades con ejemplos pasados. Uno de los dominios utilizados como entorno competitivo fue el mismo juego de Conecta Cuatro (las Damas fueron el segundo dominio). Su función heurística era idéntica a la nuestra con tres variaciones diferentes (H1, H2 y H3) que se distinguen entre sí por los valores de sus coeficientes de combinación lineal. La profundidad de búsqueda para los jugadores fue de 3 (como en nuestro análisis). Sus extensos experimentos verifican y comparan diversas estrategias de aprendizaje, factores de riesgo, conjuntos de características predefinidas y métodos de uso. La conclusión es que el dominio de Conecta Cuatro muestra una mejora de una tasa de victoria de 0.556 antes de modelar a 0.69 después de modelar (página 22). Sus conclusiones, que muestran un rendimiento mejorado al sostener y utilizar el modelo del adversario, justifican el esfuerzo de monitorear el perfil del adversario para interacciones continuas y repetidas. Un punto adicional que surgió en sus experimentos es el siguiente: después de que se haya aprendido el modelo de debilidad del oponente, los autores describen diferentes métodos para integrar el modelo de debilidad del oponente en la estrategia de decisión de los agentes. Sin embargo, independientemente del método específico que elijan para trabajar, todos los métodos de integración podrían hacer que el agente tome decisiones subóptimas; podría hacer que el agente prefiera acciones que son subóptimas en el momento actual de la decisión, pero que podrían hacer que el oponente reaccione de acuerdo con su modelo de debilidad (como lo representa nuestro agente), lo cual a su vez será beneficioso para nosotros en el futuro. El comportamiento de los agentes, como se demuestra en [9], confirma y refuerza aún más nuestro Axioma Táctico Subóptimo discutido en la sección anterior. 556 The Sixth Intl. La necesidad de los axiomas de Logro de Objetivos, Acto Preventivo y Maximización de la Evaluación es evidente y no requiere más verificación. Sin embargo, incluso con respecto a esos axiomas, surgieron algunas ideas interesantes en el análisis de registros. Los axiomas de Logro y Prevención de Actos, aunque teóricamente triviales, parecen presentar un desafío para un jugador humano. En la inspección inicial de los registros, nos encontramos con algunos juegos donde un jugador, por razones inexplicables, no bloqueó al otro para que ganara o no logró ejecutar su propia jugada ganadora. Podemos atribuir esas fallas a la falta de atención de los humanos, o a un error de escritura en su respuesta al movimiento; sin embargo, esos errores podrían ocurrir en agentes racionales limitados, y el comportamiento apropiado debe ser axiomatizado. Un juego típico de Cuatro en Línea gira en torno a generar amenazas y bloquearlas. En nuestro análisis buscamos acciones preventivas explícitas, es decir, movimientos que bloqueen un grupo de 3 discos o que eliminen una amenaza futura (en nuestro horizonte de búsqueda limitado). Encontramos que en el 83% de los juegos totales, hubo al menos una acción preventiva tomada por el jugador Negro. También se encontró que Black promedió 2.8 acciones preventivas por juego en los juegos en los que perdió, mientras que promedió 1.5 acciones preventivas por juego al ganar. Parece que Black necesita 1 o 2 acciones preventivas para construir su posición inicial de toma, antes de comenzar a presentar amenazas. Si no logra ganar, generalmente evitará una amenaza adicional o dos antes de sucumbir ante el Blanco. 4. TRABAJO RELACIONADO Mucha investigación se ocupa de la axiomatización del trabajo en equipo y los estados mentales de los individuos: algunos modelos utilizan el conocimiento y la creencia [10], otros tienen modelos de metas e intenciones [8, 4]. Sin embargo, todas estas teorías formales tratan sobre el trabajo en equipo y la cooperación de agentes. Hasta donde sabemos, nuestro modelo es el primero en proporcionar un modelo formalizado para entornos adversarios explícitos y el comportamiento de los agentes en él. El algoritmo de búsqueda adversarial clásico Min-Max fue el primer intento de integrar al oponente en el espacio de búsqueda con la suposición débil de un oponente que juega de manera óptima. Desde entonces, se ha dedicado mucho esfuerzo a integrar el modelo del oponente en el procedimiento de toma de decisiones para predecir el comportamiento futuro. El algoritmo M∗ presentado por Carmel y Markovitch [2] mostró un método para incorporar modelos de oponentes en la búsqueda de adversarios, mientras que en [3] utilizaron el aprendizaje para proporcionar un modelo de oponente más preciso en un entorno de juego repetido de 2 jugadores, donde las estrategias de los agentes se modelaron como autómatas finitos. El trabajo adicional de planificación adversarial fue realizado por Willmott y colaboradores [13], quienes proporcionaron un enfoque de planificación adversarial para el juego de GO. La investigación mencionada anteriormente trató sobre la búsqueda adversarial y la integración de modelos de oponentes en métodos de búsqueda de utilidad clásicos. Ese trabajo muestra la importancia del modelado del oponente y la capacidad de explotarlo en beneficio de un agente. Sin embargo, las limitaciones básicas de esos métodos de búsqueda siguen aplicándose; nuestro modelo intenta superar esas limitaciones presentando un modelo formal para una nueva especificación adversarial basada en estados mentales. 5. CONCLUSIONES Presentamos un modelo de Entorno Adversarial para un agente racional acotado que está situado en un entorno de N jugadores, de suma cero. Estos fueron posteriormente eliminados del análisis final. Utilizamos la formalización de SharedPlans para definir el modelo y los axiomas que los agentes pueden aplicar como pautas de comportamiento. El modelo está destinado a ser utilizado como una guía para diseñar agentes que necesitan operar en entornos adversos. Presentamos resultados empíricos, basados en el análisis del archivo de registro de ConnectFour, que ejemplifican el modelo y los axiomas para una instancia bilateral del entorno. Los resultados que presentamos son un primer paso hacia un modelo ampliado que abarcará todos los tipos de entornos adversarios, por ejemplo, entornos que no son de suma cero y entornos que contienen agentes naturales que no forman parte del conflicto directo. Esos desafíos y más serán abordados en futuras investigaciones. 6. AGRADECIMIENTO Esta investigación fue apoyada en parte por las becas de la Fundación de Ciencia de Israel #1211/04 y #898/05. 7. REFERENCIAS [1] L. V. Allis. Un enfoque basado en el conocimiento de Conecta Cuatro: el juego está resuelto, gana el jugador Blanco. Tesis de maestría, Universidad Libre de Ámsterdam, Países Bajos, 1988. [2] D. Carmel y S. Markovitch. Incorporando modelos de oponentes en la búsqueda de adversarios. En Actas de la Decimotercera Conferencia Nacional de Inteligencia Artificial, páginas 120-125, Portland, Oregón, 1996. [3] D. Carmel y S. Markovitch. Modelado de oponentes en sistemas multiagente. En G. Weiß y S. Sen, editores, Adaptación y Aprendizaje en Sistemas Multiagente, páginas 40-52. Springer-Verlag, 1996. [4] B. J. Grosz y S. Kraus. Planes colaborativos para acciones grupales complejas. Inteligencia Artificial, 86(2):269-357, 1996. [5] M. Hadad, G. Kaminka, G. Armon y S. Kraus. Apoyando la actividad colaborativa. En Proc. de AAAI-2005, páginas 83-88, Pittsburgh, 2005. [6] http://www.gamerz.net/˜pbmserv/. [7] S. Kraus y D. Lehmann. Diseñando y construyendo un agente automatizado de negociación. Inteligencia Computacional, 11:132-171, 1995. [8] H. J. Levesque, P. R. Cohen y J. H. T. Nunes. Actuar juntos. En Proc. de AAAI-90, páginas 94-99, Boston, MA, 1990. [9] S. Markovitch y R. Reger. Aprender y aprovechar las debilidades relativas de los agentes oponentes. Agentes Autónomos y Sistemas Multiagente, 10(2):103-130, 2005. [10] Y. M. Ronald Fagin, Joseph Y. Halpern y M. Y. Vardi. Razonamiento sobre el conocimiento. MIT Press, Cambridge, Mass., 1995. [11] P. Thagard.\nMIT Press, Cambridge, Massachusetts, 1995. [11] P. Thagard. Resolución de problemas adversariales: Modelado de un oponente utilizando coherencia explicativa. Ciencia Cognitiva, 16(1):123-149, 1992. [12] R. W. Toseland y R. F. Rivas. Una introducción a la práctica del trabajo en grupo. Prentice Hall, Englewood Cliffs, NJ, segunda edición, 1995. [13] S. Willmott, J. Richardson, A. Bundy y J. Levine. Un enfoque de planificación adversarial para el juego de Go. Notas de conferencia en Ciencias de la Computación, 1558:93-112, 1999. El Sexto Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 557 ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "adversarial environment": {
            "translated_key": "",
            "is_in_text": true,
            "original_annotated_sentences": [
                "An <br>adversarial environment</br> Model for Bounded Rational Agents in Zero-Sum Interactions Inon Zuckerman1 , Sarit Kraus1 , Jeffrey S. Rosenschein2 , Gal Kaminka1 1 Department of Computer Science 2 The School of Engineering Bar-Ilan University and Computer Science Ramat-Gan, Israel Hebrew University, Jerusalem, Israel {zukermi,sarit,galk}@cs.biu.ac.il jeff@cs.huji.ac.il ABSTRACT Multiagent environments are often not cooperative nor collaborative; in many cases, agents have conflicting interests, leading to adversarial interactions.",
                "This paper presents a formal <br>adversarial environment</br> model for bounded rational agents operating in a zero-sum environment.",
                "In such environments, attempts to use classical utility-based search methods can raise a variety of difficulties (e.g., implicitly modeling the opponent as an omniscient utility maximizer, rather than leveraging a more nuanced, explicit opponent model).",
                "We define an <br>adversarial environment</br> by describing the mental states of an agent in such an environment.",
                "We then present behavioral axioms that are intended to serve as design principles for building such adversarial agents.",
                "We explore the application of our approach by analyzing log files of completed Connect-Four games, and present an empirical analysis of the axioms appropriateness.",
                "Categories and Subject Descriptors I.2.11 [Artificial Intelligence]: Distributed Artificial Intelligence-Intelligent agents,Multiagent Systems; I.2.4 [Artificial Intelligence]: Knowledge Representation Formalisms and Methods -Modal logic General Terms Design, Theory 1.",
                "INTRODUCTION Early research in multiagent systems (MAS) considered cooperative groups of agents; because individual agents had limited resources, or limited access to information (e.g., limited processing power, limited sensor coverage), they worked together by design to solve problems that individually they could not solve, or at least could not solve as efficiently.",
                "MAS research, however, soon began to consider interacting agents with individuated interests, as representatives of different humans or organizations with non-identical interests.",
                "When interactions are guided by diverse interests, participants may have to overcome disagreements, uncooperative interactions, and even intentional attempts to damage one another.",
                "When these types of interactions occur, environments require appropriate behavior from the agents situated in them.",
                "We call these environments Adversarial Environments, and call the clashing agents Adversaries.",
                "Models of cooperation and teamwork have been extensively explored in MAS through the axiomatization of mental states (e.g., [8, 4, 5]).",
                "However, none of this research dealt with adversarial domains and their implications for agent behavior.",
                "Our paper addresses this issue by providing a formal, axiomatized mental state model for a subset of adversarial domains, namely simple zero-sum adversarial environments.",
                "Simple zero-sum encounters exist of course in various twoplayer games (e.g., Chess, Checkers), but they also exist in n-player games (e.g., Risk, Diplomacy), auctions for a single good, and elsewhere.",
                "In these latter environments especially, using a utility-based adversarial search (such as the Min-Max algorithm) does not always provide an adequate solution; the payoff function might be quite complex or difficult to quantify, and there are natural computational limitations on bounded rational agents.",
                "In addition, traditional search methods (like Min-Max) do not make use of a model of the opponent, which has proven to be a valuable addition to adversarial planning [9, 3, 11].",
                "In this paper, we develop a formal, axiomatized model for bounded rational agents that are situated in a zero-sum <br>adversarial environment</br>.",
                "The model uses different modality operators, and its main foundations are the SharedPlans [4] model for collaborative behavior.",
                "We explore environment properties and the mental states of agents to derive behavioral axioms; these behavioral axioms constitute a formal model that serves as a specification and design guideline for agent design in such settings.",
                "We then investigate the behavior of our model empirically using the Connect-Four board game.",
                "We show that this game conforms to our environment definition, and analyze players behavior using a large set of completed match log 550 978-81-904262-7-5 (RPS) c 2007 IFAAMAS files.",
                "In addition, we use the results presented in [9] to discuss the importance of opponent modeling in our ConnectFour adversarial domain.",
                "The paper proceeds as follows.",
                "Section 2 presents the models formalization.",
                "Section 3 presents the empirical analysis and its results.",
                "We discuss related work in Section 4, and conclude and present future directions in Section 5. 2.",
                "ADVERSARIAL ENVIRONMENTS The <br>adversarial environment</br> model (denoted as AE) is intended to guide the design of agents by providing a specification of the capabilities and mental attitudes of an agent in an <br>adversarial environment</br>.",
                "We focus here on specific types of adversarial environments, specified as follows: 1.",
                "Zero-Sum Interactions: positive and negative utilities of all agents sum to zero; 2.",
                "Simple AEs: all agents in the environment are adversarial agents; 3.",
                "Bilateral AEs: AEs with exactly two agents; 4.",
                "Multilateral AEs: AEs of three or more agents.",
                "We will work on both bilateral and multilateral instantiations of zero-sum and simple environments.",
                "In particular, our <br>adversarial environment</br> model will deal with interactions that consist of N agents (N ≥ 2), where all agents are adversaries, and only one agent can succeed.",
                "Examples of such environments range from board games (e.g., Chess, Connect-Four, and Diplomacy) to certain economic environments (e.g., N-bidder auctions over a single good). 2.1 Model Overview Our approach is to formalize the mental attitudes and behaviors of a single adversarial agent; we consider how a single agent perceives the AE.",
                "The following list specifies the conditions and mental states of an agent in a simple, zero-sum AE: 1.",
                "The agent has an individual intention that its own goal will be completed; 2.",
                "The agent has an individual belief that it and its adversaries are pursuing full conflicting goals (defined below)there can be only one winner; 3.",
                "The agent has an individual belief that each adversary has an intention to complete its own full conflicting goal; 4.",
                "The agent has an individual belief in the (partial) profile of its adversaries.",
                "Item 3 is required, since it might be the case that some agent has a full conflicting goal, and is currently considering adopting the intention to complete it, but is, as of yet, not committed to achieving it.",
                "This might occur because the agent has not yet deliberated about the effects that adopting that intention might have on the other intentions it is currently holding.",
                "In such cases, it might not consider itself to even be in an <br>adversarial environment</br>.",
                "Item 4 states that the agent should hold some belief about the profiles of its adversaries.",
                "The profile represents all the knowledge the agent has about its adversary: its weaknesses, strategic capabilities, goals, intentions, trustworthiness, and more.",
                "It can be given explicitly or can be learned from observations of past encounters. 2.2 Model Definitions for Mental States We use Grosz and Krauss definitions of the modal operators, predicates, and meta-predicates, as defined in their SharedPlan formalization [4].",
                "We recall here some of the predicates and operators that are used in that formalization: Int.To(Ai, α, Tn, Tα, C) represents Ais intentions at time Tn to do an action α at time Tα in the context of C. Int.Th(Ai, prop, Tn, Tprop, C) represents Ais intentions at time Tn that a certain proposition prop holds at time Tprop in the context of C. The potential intention operators, Pot.Int.To(...) and Pot.Int.Th(...), are used to represent the mental state when an agent considers adopting an intention, but has not deliberated about the interaction of the other intentions it holds.",
                "The operator Bel(Ai, f, Tf ) represents agent Ai believing in the statement expressed in formula f, at time Tf .",
                "MB(A, f, Tf ) represents mutual belief for a group of agents A.",
                "A snapshot of the system finds our environment to be in some state e ∈ E of environmental variable states, and each adversary in any LAi ∈ L of possible local states.",
                "At any given time step, the system will be in some world w of the set of all possible worlds w ∈ W, where w = E×LA1 ×LA2 × ...LAn , and n is the number of adversaries.",
                "For example, in a Texas Holdem poker game, an agents local state might be its own set of cards (which is unknown to its adversary) while the environment will consist of the betting pot and the community cards (which are visible to both players).",
                "A utility function under this formalization is defined as a mapping from a possible world w ∈ W to an element in , which expresses the desirability of the world, from a single agent perspective.",
                "We usually normalize the range to [0,1], where 0 represents the least desirable possible world, and 1 is the most desirable world.",
                "The implementation of the utility function is dependent on the domain in question.",
                "The following list specifies new predicates, functions, variables, and constants used in conjunction with the original definitions for the <br>adversarial environment</br> formalization: 1. φ is a null action (the agent does not do anything). 2.",
                "GAi is the set of agent Ais goals.",
                "Each goal is a set of predicates whose satisfaction makes the goal complete (we use G∗ Ai ∈ GAi to represent an arbitrary goal of agent Ai). 3. gAi is the set of agent Ais subgoals.",
                "Subgoals are predicates whose satisfaction represents an important milestone toward achievement of the full goal. gG∗ Ai ⊆ gAi is the set of subgoals that are important to the completion of goal G∗ Ai (we will use g∗ G∗ Ai ∈ gG∗ Ai to represent an arbitrary subgoal). 4.",
                "P Aj Ai is the profile object agent Ai holds about agent Aj. 5.",
                "CA is a general set of actions for all agents in A which are derived from the environments constraints.",
                "CAi ⊆ CA is the set of agent Ais possible actions. 6.",
                "Do(Ai, α, Tα, w) holds when Ai performs action α over time interval Tα in world w. 7.",
                "Achieve(G∗ Ai , α, w) is true when goal G∗ Ai is achieved following the completion of action α in world w ∈ W, where α ∈ CAi . 8.",
                "Profile(Ai, PAi Ai ) is true when agent Ai holds an object profile for agent Aj.",
                "Definition 1.",
                "Full conflict (FulConf ) describes a zerosum interaction where only a single goal of the goals in conflict can be completed.",
                "FulConf(G∗ Ai , G∗ Aj ) ⇒ (∃α ∈ CAi , ∀w, β ∈ CAj ) (Achieve(G∗ Ai , α, w) ⇒ ¬Achieve(G∗ Aj , β, w)) ∨ (∃β ∈ CAj , ∀w, α ∈ CAi )(Achieve(G∗ Aj , β, w) ⇒ ¬Achieve(G∗ Ai , α, w)) Definition 2.",
                "Adversarial Knowledge (AdvKnow) is a function returning a value which represents the amount of The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 551 knowledge agent Ai has on the profile of agent Aj, at time Tn.",
                "The higher the value, the more knowledge agent Ai has.",
                "AdvKnow : P Aj Ai × Tn → Definition 3.",
                "Eval - This evaluation function returns an estimated expected utility value for an agent in A, after completing an action from CA in some world state w. Eval : A × CA × w → Definition 4.",
                "TrH - (Threshold) is a numerical constant in the [0,1] range that represents an evaluation function (Eval) threshold value.",
                "An action that yields an estimated utility evaluation above the TrH is regarded as a highly beneficial action.",
                "The Eval value is an estimation and not the real utility function, which is usually unknown.",
                "Using the real utility value for a rational agent would easily yield the best outcome for that agent.",
                "However, agents usually do not have the real utility functions, but rather a heuristic estimate of it.",
                "There are two important properties that should hold for the evaluation function: Property 1.",
                "The evaluation function should state that the most desirable world state is one in which the goal is achieved.",
                "Therefore, after the goal has been satisfied, there can be no future action that can put the agent in a world state with higher Eval value. (∀Ai, G∗ Ai , α, β ∈ CAi , w ∈ W) Achieve(G∗ Ai , α, w) ⇒ Eval(Ai, α, w) ≥ Eval(Ai, β, w) Property 2.",
                "The evaluation function should project an action that causes a completion of a goal or a subgoal to a value which is greater than TrH (a highly beneficial action). (∀Ai, G∗ Ai ∈ GAi , α ∈ CAi , w ∈ W, g∗ GAi ∈ gGAi ) Achieve(G∗ Ai , α, w) ∨ Achieve(g∗ GAi , α, w) ⇒ Eval(Ai, α, w) ≥ TrH.",
                "Definition 5.",
                "SetAction We define a set action (SetAction) as a set of action operations (either complex or basic actions) from some action sets CAi and CAj which, according to agent Ais belief, are attached together by a temporal and consequential relationship, forming a chain of events (action, and its following consequent action). (∀α1 , . . . , αu ∈ CAi , β1 , . . . , βv ∈ CAj , w ∈ W) SetAction(α1 , . . . , αu , β1 , . . . , βv , w) ⇒ ((Do(Ai, α1 , Tα1 , w) ⇒ Do(Aj, β1 , Tβ1 , w)) ⇒ Do(Ai, α2 , Tα2 , w) ⇒ . . . ⇒ Do(Ai, αu , Tαu , w)) The consequential relation might exist due to various environmental constraints (when one action forces the adversary to respond with a specific action) or due to the agents knowledge about the profile of its adversary.",
                "Property 3.",
                "As the knowledge we have about our adversary increases we will have additional beliefs about its behavior in different situations which in turn creates new set actions.",
                "Formally, if our AdvKnow at time Tn+1 is greater than AdvKnow at time Tn, then every SetAction known at time Tn is also known at time Tn+1.",
                "AdvKnow(P Aj Ai , Tn+1) > AdvKnow(P Aj Ai , Tn) ⇒ (∀α1 , . . . , αu ∈ CAi , β1 , . . . , βv ∈ CAj ) Bel(Aag, SetAction(α1 , . . . , αu , β1 , . . . , βv ), Tn) ⇒ Bel(Aag, SetAction(α1 , . . . , αu , β1 , . . . , βv ), Tn+1) 2.3 The Environment Formulation The following axioms provide the formal definition for a simple, zero-sum <br>adversarial environment</br> (AE).",
                "Satisfaction of these axioms means that the agent is situated in such an environment.",
                "It provides specifications for agent Aag to interact with its set of adversaries A with respect to goals G∗ Aag and G∗ A at time TCo at some world state w. AE(Aag, A, G∗ Aag , A1, . . . , Ak, G∗ A1 , . . . , G∗ Ak , Tn, w) 1.",
                "Aag has an Int.Th his goal would be completed: (∃α ∈ CAag , Tα) Int.Th(Aag, Achieve(G∗ Aag , α), Tn, Tα, AE) 2.",
                "Aag believes that it and each of its adversaries Ao are pursuing full conflicting goals: (∀Ao ∈ {A1, . . . , Ak}) Bel(Aag, FulConf(G∗ Aag , G∗ Ao ), Tn) 3.",
                "Aag believes that each of his adversaries in Ao has the Int.Th his conflicting goal G∗ Aoi will be completed: (∀Ao ∈ {A1, . . . , Ak})(∃β ∈ CAo , Tβ) Bel(Aag, Int.Th(Ao, Achieve(G∗ Ao , β), TCo, Tβ, AE), Tn) 4.",
                "Aag has beliefs about the (partial) profiles of its adversaries (∀Ao ∈ {A1, . . . , Ak}) (∃PAo Aag ∈ PAag )Bel(Aag, Profile(Ao, PAo Aag ), Tn) To build an agent that will be able to operate successfully within such an AE, we must specify behavioral guidelines for its interactions.",
                "Using a naive Eval maximization strategy to a certain search depth will not always yield satisfactory results for several reasons: (1) the search horizon problem when searching for a fixed depth; (2) the strong assumption of an optimally rational, unbounded resources adversary; (3) using an estimated evaluation function which will not give optimal results in all world states, and can be exploited [9].",
                "The following axioms specify the behavioral principles that can be used to differentiate between successful and less successful agents in the above <br>adversarial environment</br>.",
                "Those axioms should be used as specification principles when designing and implementing agents that should be able to perform well in such Adversarial Environments.",
                "The behavioral axioms represent situations in which the agent will adopt potential intentions to (Pot.Int.To(...)) perform an action, which will typically require some means-end reasoning to select a possible course of action.",
                "This reasoning will lead to the adoption of an Int.To(...) (see [4]).",
                "A1.",
                "Goal Achieving Axiom.",
                "The first axiom is the simplest case; when the agent Aag believes that it is one action (α) away from achieving his conflicting goal G∗ Aag , it should adopt the potential intention to do α and complete its goal. (∀Aag, α ∈ CAag , Tn, Tα, w ∈ W) (Bel(Aag, Do(Aag, α, Tα, w) ⇒ Achieve(G∗ Aag , α, w)) ⇒ Pot.Int.To(Aag, α, Tn, Tα, w) This somewhat trivial behavior is the first and strongest axiom.",
                "In any situation, when the agent is an action away from completing the goal, it should complete the action.",
                "Any fair Eval function would naturally classify α as the maximal value action (property 1).",
                "However, without explicit axiomatization of such behavior there might be situations where the agent will decide on taking another action for various reasons, due to its bounded decision resources.",
                "A2.",
                "Preventive Act Axiom.",
                "Being in an adversarial situation, agent Aag might decide to take actions that will damage one of its adversarys plans to complete its goal, even if those actions do not explicitly advance Aag towards its conflicting goal G∗ Aag .",
                "Such preventive action will take place when agent Aag has a belief about the possibility of its adversary Ao doing an action β that will give it a high 552 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) utility evaluation value (> TrH).",
                "Believing that taking action α will prevent the opponent from doing its β, it will adopt a potential intention to do α. (∀Aag, Ao ∈ A, α ∈ CAag , β ∈ CAo , Tn, Tβ, w ∈ W) (Bel(Aag, Do(Ao, β, Tβ, w) ∧ Eval(Ao, β, w) > TrH, Tn) ∧ Bel(Aag, Do(Aag, α, Tα, w) ⇒ ¬Do(Ao, β, Tβ, w), Tn) ⇒ Pot.Int.To(Aag, α, Tn, Tα, w) This axiom is a basic component of any <br>adversarial environment</br>.",
                "For example, looking at a Chess board game, a player could realize that it is about to be checkmated by its opponent, thus making a preventive move.",
                "Another example is a Connect Four game: when a player has a row of three chips, its opponent must block it, or lose.",
                "A specific instance of A1 occurs when the adversary is one action away from achieving its goal, and immediate preventive action needs to be taken by the agent.",
                "Formally, we have the same beliefs as stated above, with a changed belief that doing action β will cause agent Ao to achieve its goal.",
                "Proposition 1: Prevent or lose case. (∀Aag, Ao ∈ A, α ∈ CAag , β ∈ CAo , G∗ Ao , Tn, Tα, Tβ, w ∈ W) Bel(Aag, Do(Ao, β, Tβ, w) ⇒ Achieve(G∗ Ao , β, w), Tn) ∧ Bel(Aag, Do(Aag, α, Tα, w) ⇒ ¬Do(Ao, β, Tβ, w)) ⇒ Pot.Int.To(Aag, α, Tn, Tα, w) Sketch of proof: Proposition 1 can be easily derived from axiom A1 and the property 2 of the Eval function, which states that any action that causes a completion of a goal is a highly beneficial action.",
                "The preventive act behavior will occur implicitly when the Eval function is equal to the real world utility function.",
                "However, being bounded rational agents and dealing with an estimated evaluation function we need to explicitly axiomatize such behavior, for it will not always occur implicitly from the evaluation function.",
                "A3.",
                "Suboptimal Tactical Move Axiom.",
                "In many scenarios a situation may occur where an agent will decide not to take the current most beneficial action it can take (the action with the maximal utility evaluation value), because it believes that taking another action (with lower utility evaluation value) might yield (depending on the adversarys response) a future possibility for a highly beneficial action.",
                "This will occur most often when the Eval function is inaccurate and differs by a large extent from the Utility function.",
                "Put formally, agent Aag believes in a certain SetAction that will evolve according to its initial action and will yield a high beneficial value (> TrH) solely for it. (∀Aag, Ao ∈ A, Tn, w ∈ W) (∃α1 , . . . , αu ∈ CAi , β1 , . . . , βv ∈ CAj , Tα1 ) Bel(Aag, SetAction(α1 , . . . , αu , β1 , . . . , βv ), Tn) ∧ Bel(Aag, Eval(Ao, βv , w) < TrH < Eval(Aag, αu , w), Tn) ⇒ Pot.Int.To(Aag, α1 , Tn, Tα1 , w) An agent might believe that a chain of events will occur for various reasons due to the inevitable nature of the domain.",
                "For example, in Chess, we often observe the following: a move causes a check position, which in turn limits the opponents moves to avoiding the check, to which the first player might react with another check, and so on.",
                "The agent might also believe in a chain of events based on its knowledge of its adversarys profile, which allows it to foresee the adversarys movements with high accuracy.",
                "A4.",
                "Profile Detection Axiom.",
                "The agent can adjust its adversarys profiles by observations and pattern study (specifically, if there are repeated encounters with the same adversary).",
                "However, instead of waiting for profile information to be revealed, an agent can also initiate actions that will force its adversary to react in a way that will reveal profile knowledge about it.",
                "Formally, the axiom states that if all actions (γ) are not highly beneficial actions (< TrH), the agent can do action α in time Tα if it believes that it will result in a non-highly beneficial action β from its adversary, which in turn teaches it about the adversarys profile, i.e., gives a higher AdvKnow(P Aj Ai , Tβ). (∀Aag, Ao ∈ A, α ∈ CAag , β ∈ CAo , Tn, Tα, Tβ, w ∈ W) Bel(Aag, (∀γ ∈ CAag )Eval(Aag, γ, w) < TrH, Tn) ∧ Bel(Aag, Do(Aag, α, Tα, w) ⇒ Do(Ao, β, Tβ, w), Tn) ∧ Bel(Aag, Eval(Ao, β, w) < TrH) ∧ Bel(Aag, AdvKnow(P Aj Ai , Tβ) > AdvKnow(P Aj Ai , Tn), Tn) ⇒ Pot.Int.To(Aag, α, Tn, Tα, w) For example, going back to the Chess board game scenario, consider starting a game versus an opponent about whom we know nothing, not even if it is a human or a computerized opponent.",
                "We might start playing a strategy that will be suitable versus an average opponent, and adjust our game according to its level of play.",
                "A5.",
                "Alliance Formation Axiom The following behavioral axiom is relevant only in a multilateral instantiation of the <br>adversarial environment</br> (obviously, an alliance cannot be formed in a bilateral, zero-sum encounter).",
                "In different situations during a multilateral interaction, a group of agents might believe that it is in their best interests to form a temporary alliance.",
                "Such an alliance is an agreement that constrains its members behavior, but is believed by its members to enable them to achieve a higher utility value than the one achievable outside of the alliance.",
                "As an example, we can look at the classical Risk board game, where each player has an individual goal of being the sole conquerer of the world, a zero-sum game.",
                "However, in order to achieve this goal, it might be strategically wise to make short-term ceasefire agreements with other players, or to join forces and attack an opponent who is stronger than the rest.",
                "An alliances terms defines the way its members should act.",
                "It is a set of predicates, denoted as Terms, that is agreed upon by the alliance members, and should remain true for the duration of the alliance.",
                "For example, the set Terms in the Risk scenario, could contain the following predicates: 1.",
                "Alliance members will not attack each other on territories X, Y and Z; 2.",
                "Alliance members will contribute C units per turn for attacking adversary Ao; 3.",
                "Members are obligated to stay as part of the alliance until time Tk or until adversarys Ao army is smaller than Q.",
                "The set Terms specifies inter-group constraints on each of the alliance members (∀Aal i ∈ Aal ⊆ A) set of actions Cal i ⊆ C. Definition 6.",
                "Al val - the total evaluation value that agent Ai will achieve while being part of Aal is the sum of Evali (Eval for Ai) of each of Aal j Eval values after taking their own α actions (via the agent(α) predicate): Al val(Ai, Cal , Aal , w) = α∈Cal Evali(Aal j , agent(α), w) Definition 7.",
                "Al TrH - is a number representing an Al val The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 553 threshold; above it, the alliance can be said to be a highly beneficial alliance.",
                "The value of Al TrH will be calculated dynamically according to the progress of the interaction, as can be seen in [7].",
                "After an alliance is formed, its members are now working in their normal <br>adversarial environment</br>, as well as according to the mental states and axioms required for their interactions as part of the alliance.",
                "The following Alliance model (AL) specifies the conditions under which the group Aal can be said to be in an alliance and working with a new and constrained set of actions Cal , at time Tn.",
                "AL(Aal , Cal , w, Tn) 1.",
                "Aal has a MB that all members are part of Aal : MB(Aal , (∀Aal i ∈ Aal )member(Aal i , Aal ), Tn) 2.",
                "Aal has a MB that the group be maintained: MB(Aal , (∀Aal i ∈ Aal )Int.Th (Ai, member(Ai, Aal ), Tn, Tn+1, Co), Tn) 3.",
                "Aal has a MB that being members gives them high utility value: MB(Aal , (∀Aal i ∈ Aal )Al val(Aal i , Cal , Aal , w) ≥ Al TrH, Tn) Members profiles are a crucial part of successful alliances.",
                "We assume that agents that have more accurate profiles of their adversaries will be more successful in such environments.",
                "Such agents will be able to predict when a member is about to breach the alliances contract (item 2 in the above model), and take counter measures (when item 3 will falsify).",
                "The robustness of the alliance is in part a function of its members trustfulness measure, objective position estimation, and other profile properties.",
                "We should note that an agent can simultaneously be part of more than one alliance.",
                "Such a temporary alliance, where the group members do not have a joint goal but act collaboratively for the interest of their own individual goals, is classified as a Treatment Group by modern psychologists [12] (in contrast to a Task Group, where its members have a joint goal).",
                "The Shared Activity model as presented in [5] modeled Treatment Group behavior using the same SharedPlans formalization.",
                "When comparing both definitions of an alliance and a Treatment Group we found an unsurprising resemblance between both models: the environment models definitions are almost identical (see SAs definitions in [5]), and their Selfish-Act and Cooperative Act axioms conform to our adversarial agents behavior.",
                "The main distinction between both models is the integration of a Helpful-behavior act axiom, in the Shared Activity which cannot be part of ours.",
                "This axiom states that an agent will consider taking action that will lower its Eval value (to a certain lower bound), if it believes that a group partner will gain a significant benefit.",
                "Such behavior cannot occur in a pure <br>adversarial environment</br> (as a zero-sum game is), where the alliance members are constantly on watch to manipulate their alliance to their own advantage.",
                "A6.",
                "Evaluation Maximization Axiom.",
                "In a case when all other axioms are inapplicable, we will proceed with the action that maximizes the heuristic value as computed in the Eval function. (∀Aag, Ao ∈ A, α ∈ Cag, Tn, w ∈ W) Bel(Aag, (∀γ ∈ Cag)Eval(Aag, α, w) ≥ Eval(Aag, γ, w), Tn) ⇒ Pot.Int.To(Aag, α, Tn, Tα, w) T1.",
                "Optimality on Eval = Utility The above axiomatic model handles situations where the Utility is unknown and the agents are bounded rational agents.",
                "The following theorem shows that in bilateral interactions, where the agents have the real Utility function (i.e., Eval = Utility) and are rational agents, the axioms provide the same optimal result as classic adversarial search (e.g., Min-Max).",
                "Theorem 1.",
                "Let Ae ag be an unbounded rational AE agent using the Eval heuristic evaluation function, Au ag be the same agent using the true Utility function, and Ao be a sole unbounded utility-based rational adversary.",
                "Given that Eval = Utility: (∀α ∈ CAu ag , α ∈ CAe ag , Tn, w ∈ W) Pot.Int.To(Au ag, α, Tn, Tα, w) → Pot.Int.To(Ae ag, α , Tn, Tα, w) ∧ ((α = α ) ∨ (Utility(Au ag, α, w) = Eval(Ae ag, α , w))) Sketch of proof - Given that Au ag has the real utility function and unbounded resources, it can generate the full game tree and run the optimal MinMax algorithm to choose the highest utility value action, which we denote by, α.",
                "The proof will show that Ae ag, using the AE axioms, will select the same or equal utility α (when there is more than one action with the same max utility) when Eval = Utility. (A1) Goal achieving axiom - suppose there is an α such that its completion will achieve Au ags goal.",
                "It will obtain the highest utility by Min-Max for Au ag.",
                "The Ae ag agent will select α or another action with the same utility value via A1.",
                "If such α does not exist, Ae ag cannot apply this axiom, and proceeds to A2. (A2) Preventive act axiom - (1) Looking at the basic case (see Prop1), if there is a β which leads Ao to achieve its goal, then a preventive action α will yield the highest utility for Au ag.",
                "Au ag will choose it through the utility, while Ae ag will choose it through A2. (2) In the general case, β is a highly beneficial action for Ao, thus yields low utility for Au ag, which will guide it to select an α that will prevent β, while Ae ag will choose it through A2.1 If such β does not exist for Ao, then A2 is not applicable, and Ae ag can proceed to A3. (A3) Suboptimal tactical move axiom - When using a heuristic Eval function, Ae ag has a partial belief in the profile of its adversary (item 4 in AE model), which may lead it to believe in SetActions (Prop1).",
                "In our case, Ae ag is holding a full profile on its optimal adversary and knows that Ao will behave optimally according to the real utility values on the complete search tree, therefore, any belief about suboptimal SetAction cannot exist, yielding this axiom inapplicable.",
                "Ae ag will proceed to A4. (A4) Profile detection axiom - Given that Ae ag has the full profile of Ao, none of Ae ags actions can increase its knowledge.",
                "That axiom will not be applied, and the agent will proceed with A6 (A5 will be disregarded because the interaction is bilateral). (A6) Evaluation maximization axiom - This axiom will select the max Eval for Ae ag.",
                "Given that Eval = Utility, the same α that was selected by Au ag will be selected. 3.",
                "EVALUATION The main purpose of our experimental analysis is to evaluate the models behavior and performance in a real <br>adversarial environment</br>.",
                "This section investigates whether bounded 1 A case where following the completion of β there exists a γ which gives high utility for Agent Au ag, cannot occur because Ao uses the same utility, and γs existence will cause it to classify β as a low utility action. 554 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) rational agents situated in such adversarial environments will be better off applying our suggested behavioral axioms. 3.1 The Domain To explore the use of the above model and its behavioral axioms, we decided to use the Connect-Four game as our <br>adversarial environment</br>.",
                "Connect-Four is a 2-player, zerosum game which is played using a 6x7 matrix-like board.",
                "Each turn, a player drops a disc into one of the 7 columns (the set of 21 discs is usually colored yellow for player 1 and red for player 2; we will use White and Black respectively to avoid confusion).",
                "The winner is the first player to complete a horizontal, vertical, or diagonal set of four discs with its color.",
                "On very rare occasions, the game might end in a tie if all the empty grids are filled, but no player managed to create a 4-disc set.",
                "The Connect-Four game was solved in [1], where it is shown that the first player (playing with the white discs) can force a win by starting in the middle column (column 4) and playing optimally However, the optimal strategy is very complex, and difficult to follow even for complex bounded rational agents, such as human players.",
                "Before we can proceed checking agent behavior, we must first verify that the domain conforms to the adversarial environments definition as given above (which the behavioral axioms are based on).",
                "First, when playing a Connect-Four game, the agent has an intention to win the game (item 1).",
                "Second (item 2), our agent believes that in Connect-Four there can only be one winner (or no winner at all in the rare occurrence of a tie).",
                "In addition, our agent believes that its opponent to the game will try to win (item 3), and we hope it has some partial knowledge (item 4) about its adversary (this knowledge can vary from nothing, through simple facts such as age, to strategies and weaknesses).",
                "Of course, not all Connect-Four encounters are adversarial.",
                "For example, when a parent is playing the game with its child, the following situation might occur: the child, having a strong incentive to win, treats the environment as adversarial (it intends to win, understands that there can only be one winner, and believes that its parent is trying to beat him).",
                "However, the parents point of view might see the environment as an educational one, where its goal is not to win the game, but to cause enjoyment or practice strategic reasoning.",
                "In such an educational environment, a new set of behavioral axioms might be more beneficial to the parents goals than our suggested adversarial behavioral axioms. 3.2 Axiom Analysis After showing that the Connect-Four game is indeed a zero-sum, bilateral <br>adversarial environment</br>, the next step is to look at players behaviors during the game and check whether behaving according to our model does improve performance.",
                "To do so we have collected log files from completed Connect-Four games that were played by human players over the Internet.",
                "Our collected log file data came from Play by eMail (PBeM) sites.",
                "These are web sites that host email games, where each move is taken by an email exchange between the server and the players.",
                "Many such sites archives contain real competitive interactions, and also maintain a ranking system for their members.",
                "Most of the data we used can be found in [6].",
                "As can be learned from [1], Connect-Four has an optimal strategy and a considerable advantage for the player who starts the game (which we call the White player).",
                "We will concentrate in our analysis on the second players moves (to be called Black).",
                "The White player, being the first to act, has the so-called initiative advantage.",
                "Having the advantage and a good strategy will keep the Black player busy reacting to its moves, instead of initiating threats.",
                "A threat is a combination of three discs of the same color, with an empty spot for the fourth winning disk.",
                "An open threat is a threat that can be realized in the opponents next move.",
                "In order for the Black player to win, it must somehow turn the tide, take the advantage and start presenting threats to the White player.",
                "We will explore Black players behavior and their conformance to our axioms.",
                "To do so, we built an application that reads log files and analyzes the Black players moves.",
                "The application contains two main components: (1) a Min-Max algorithm for evaluation of moves; (2) open threats detector for the discovering of open threats.",
                "The Min-Max algorithm will work to a given depth, d and for each move α will output the heuristic value for the next action taken by the player as written in the log file, h(α), alongside the maximum heuristic value, maxh(α), that could be achieved prior to taking the move (obviously, if h(α) = maxh(α), then the player did not do the optimal move heuristically).",
                "The threat detectors job is to notify if some action was taken in order to block an open threat (not blocking an open threat will probably cause the player to lose in the opponents next move).",
                "The heuristic function used by Min-Max to evaluate the players utility is the following function, which is simple to compute, yet provides a reasonable challenge to human opponents: Definition 8.",
                "Let Group be an adjacent set of four squares that are horizontal, vertical, or diagonal.",
                "Groupn b (Groupn w) be a Group with n pieces of the black (white) color and 4−n empty squares. h = ((Group1 b ∗α)+(Group2 b ∗β)+(Group3 b ∗γ)+(Group4 b ∗∞)) − ((Group1 w ∗α)+(Group2 w ∗β)+(Group3 w ∗γ)+(Group4 w ∗∞)) The values of α, β and δ can vary to form any desired linear combination; however, it is important to value them with the α < β < δ ordering in mind (we used 1, 4, and 8 as their respective values).",
                "Groups of 4 discs of the same color means victory, thus discovery of such a group will result in ∞ to ensure an extreme value.",
                "We now use our estimated evaluation function to evaluate the Black players actions during the Connect-Four adversarial interaction.",
                "Each game from the log file was input into the application, which processed and output a reformatted log file containing the h value of the current move, the maxh value that could be achieved, and a notification if an open threat was detected.",
                "A total of 123 games were analyzed (57 with White winning, and 66 with Black winning).",
                "A few additional games were manually ignored in the experiment, due to these problems: a player abandoning the game while the outcome is not final, or a blunt irrational move in the early stages of the game (e.g., not blocking an obvious winning group in the first opening moves).",
                "In addition, a single tie game was also removed.",
                "The simulator was run to a search depth of 3 moves.",
                "We now proceed to analyze the games with respect to each behavioral axiom.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 555 Table 1: Average heuristic difference analysis Black losses Black Won Avg minh -17.62 -12.02 Avg 3 lowest h moves (min3 h) -13.20 -8.70 3.2.1 Affirming the Suboptimal tactical move axiom The following section presents the heuristic evaluations of the Min-Max algorithm for each action, and checks the amount and extent of suboptimal tactical actions and their implications on performance.",
                "Table 1 shows results and insights from the games heuristic analysis, when search depth equals 3 (this search depth was selected for the results to be comparable to [9], see Section 3.2.3).",
                "The tables heuristic data is the difference between the present maximal heuristic value and the heuristic value of the action that was eventually taken by the player (i.e., the closer the number is to 0, the closer the action was to the maximum heuristic action).",
                "The first row presents the difference values of the action that had the maximal difference value among all the Black players actions in a given game, as averaged over all Blacks winning and losing games (see respective columns).",
                "In games in which the Black player loses, its average difference value was -17.62, while in games in which the Black player won, its average was -12.02.",
                "The second row expands the analysis by considering the 3 highest heuristic difference actions, and averaging them.",
                "In that case, we notice an average heuristic difference of 5 points between games which the Black player loses and games in which it wins.",
                "Nevertheless, the importance of those numbers is that they allowed us to take an educated guess on a threshold number of 11.5, as the value of the TrH constant, which differentiates between normal actions and highly beneficial ones.",
                "After finding an approximated TrH constant, we can proceed with an analysis of the importance of suboptimal moves.",
                "To do so we took the subset of games in which the minimum heuristic difference value for Blacks actions was 11.5.",
                "As presented in Table 2, we can see the different min3 h average of the 3 largest ranges and the respective percentage of games won.",
                "The first row shows that the Black player won only 12% of the games in which the average of its 3 highest heuristically difference actions (min3 h) was smaller than the suggested threshold, TrH = 11.5.",
                "The second row shows a surprising result: it seems that when min3 h > −4 the Black player rarely wins.",
                "Intuition would suggest that games in which the action evaluation values were closer to the maximal values will result in more winning games for Black.",
                "However, it seems that in the Connect-Four domain, merely responding with somewhat easily expected actions, without initiating a few surprising and suboptimal moves, does not yield good results.",
                "The last row sums up the main insights from the analysis; most of Blacks wins (83%) came when its min3 h was in the range of -11.5 to -4.",
                "A close inspection of those Black winning games shows the following pattern behind the numbers: after standard opening moves, Black suddenly drops a disc into an isolated column, which seems a waste of a move.",
                "White continues to build its threats, while usually disregarding Blacks last move, which in turn uses the isolated disc as an anchor for a future winning threat.",
                "The results show that it was beneficial for the Black player Table 2: Blacks winnings percentages % of games min3 h < −11.5 12% min3 h > −4 5% −11.5 ≤ min3 h ≤ −4 83% to take suboptimal actions and not give the current highest possible heuristic value, but will not be too harmful for its position (i.e., will not give high beneficial value to its adversary).",
                "As it turned out, learning the threshold is an important aspect of success: taking wildly risky moves (min3 h < −11.5) or trying to avoid them (min3 h > −4) reduces the Black players winning chances by a large margin. 3.2.2 Affirming the Profile Monitoring Axiom In the task of showing the importance of monitoring ones adversaries profiles, our log files could not be used because they did not contain repeated interactions between players, which are needed to infer the players knowledge about their adversaries.",
                "However, the importance of opponent modeling and its use in attaining tactical advantages was already studied in various domains ([3, 9] are good examples).",
                "In a recent paper, Markovitch and Reger [9] explored the notion of learning and exploitation of opponent weakness in competitive interactions.",
                "They apply simple learning strategies by analyzing examples from past interactions in a specific domain.",
                "They also used the Connect-Four adversarial domain, which can now be used to understand the importance of monitoring the adversarys profile.",
                "Following the presentation of their theoretical model, they describe an extensive empirical study and check the agents performance after learning the weakness model with past examples.",
                "One of the domains used as a competitive environment was the same Connect-Four game (Checkers was the second domain).",
                "Their heuristic function was identical to ours with three different variations (H1, H2, and H3) that are distinguished from one another in their linear combination coefficient values.",
                "The search depth for the players was 3 (as in our analysis).",
                "Their extensive experiments check and compare various learning strategies, risk factors, predefined feature sets and usage methods.",
                "The bottom line is that the Connect-Four domain shows an improvement from a 0.556 winning rate before modeling to a 0.69 after modeling (page 22).",
                "Their conclusions, showing improved performance when holding and using the adversarys model, justify the effort to monitor the adversary profile for continuous and repeated interactions.",
                "An additional point that came up in their experiments is the following: after the opponent weakness model has been learned, the authors describe different methods of integrating the opponent weakness model into the agents decision strategy.",
                "Nevertheless, regardless of the specific method they chose to work with, all integration methods might cause the agent to take suboptimal decisions; it might cause the agent to prefer actions that are suboptimal at the present decision junction, but which might cause the opponent to react in accordance with its weakness model (as represented by our agent) which in turn will be beneficial for us in the future.",
                "The agents behavior, as demonstrated in [9] further confirms and strengthens our Suboptimal Tactical Axiom as discussed in the previous section. 556 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 3.2.3 Additional Insights The need for the Goal Achieving, Preventive Act, and Evaluation Maximization axioms are obvious, and need no further verification.",
                "However, even with respect to those axioms, a few interesting insights came up in the log analysis.",
                "The Goal achieving and Preventive Act axioms, though theoretically trivial, seem to provide some challenge to a human player.",
                "In the initial inspection of the logs, we encountered few games2 where a player, for inexplicable reasons, did not block the other from winning or failed to execute its own winning move.",
                "We can blame those faults on the humans lack of attention, or a typing error in its move reply; nevertheless, those errors might occur in bounded rational agents, and the appropriate behavior needs to be axiomatized.",
                "A typical Connect-Four game revolves around generating threats and blocking them.",
                "In our analysis we looked for explicit preventive actions, i.e., moves that block a group of 3 discs, or that remove a future threat (in our limited search horizon).",
                "We found that in 83% of the total games there was at least one preventive action taken by the Black player.",
                "It was also found that Black averaged 2.8 preventive actions per game on the games in which it lost, while averaging 1.5 preventive actions per game when winning.",
                "It seems that Black requires 1 or 2 preventive actions to build its initial taking position, before starting to present threats.",
                "If it did not manage to win, it will usually prevent an extra threat or two before succumbing to White. 4.",
                "RELATED WORK Much research deals with the axiomatization of teamwork and mental states of individuals: some models use knowledge and belief [10], others have models of goals and intentions [8, 4].",
                "However, all these formal theories deal with agent teamwork and cooperation.",
                "As far as we know, our model is the first to provide a formalized model for explicit adversarial environments and agents behavior in it.",
                "The classical Min-Max adversarial search algorithm was the first attempt to integrate the opponent into the search space with a weak assumption of an optimally playing opponent.",
                "Since then, much effort has gone into integrating the opponent model into the decision procedure to predict future behavior.",
                "The M∗ algorithm presented by Carmel and Markovitch [2] showed a method of incorporating opponent models into adversary search, while in [3] they used learning to provide a more accurate opponent model in a 2player repeated game environment, where agents strategies were modeled as finite automata.",
                "Additional Adversarial planning work was done by Willmott et al. [13], which provided an adversarial planning approach to the game of GO.",
                "The research mentioned above dealt with adversarial search and the integration of opponent models into classical utilitybased search methods.",
                "That work shows the importance of opponent modeling and the ability to exploit it to an agents advantage.",
                "However, the basic limitations of those search methods still apply; our model tries to overcome those limitations by presenting a formal model for a new, mental state-based adversarial specification. 5.",
                "CONCLUSIONS We presented an <br>adversarial environment</br> model for a 2 These were later removed from the final analysis. bounded rational agent that is situated in an N-player, zerosum environment.",
                "We used the SharedPlans formalization to define the model and the axioms that agents can apply as behavioral guidelines.",
                "The model is meant to be used as a guideline for designing agents that need to operate in such adversarial environments.",
                "We presented empirical results, based on ConnectFour log file analysis, that exemplify the model and the axioms for a bilateral instance of the environment.",
                "The results we presented are a first step towards an expanded model that will cover all types of adversarial environments, for example, environments that are non-zero-sum, and environments that contain natural agents that are not part of the direct conflict.",
                "Those challenges and more will be dealt with in future research. 6.",
                "ACKNOWLEDGMENT This research was supported in part by Israel Science Foundation grants #1211/04 and #898/05. 7.",
                "REFERENCES [1] L. V. Allis.",
                "A knowledge-based approach of Connect-Four - the game is solved: White wins.",
                "Masters thesis, Free University, Amsterdam, The Netherlands, 1988. [2] D. Carmel and S. Markovitch.",
                "Incorporating opponent models into adversary search.",
                "In Proceedings of the Thirteenth National Conference on Artificial Intelligence, pages 120-125, Portland, OR, 1996. [3] D. Carmel and S. Markovitch.",
                "Opponent modeling in multi-agent systems.",
                "In G. Weiß and S. Sen, editors, Adaptation and Learning in Multi-Agent Systems, pages 40-52.",
                "Springer-Verlag, 1996. [4] B. J. Grosz and S. Kraus.",
                "Collaborative plans for complex group action.",
                "Artificial Intelligence, 86(2):269-357, 1996. [5] M. Hadad, G. Kaminka, G. Armon, and S. Kraus.",
                "Supporting collaborative activity.",
                "In Proc. of AAAI-2005, pages 83-88, Pittsburgh, 2005. [6] http://www.gamerz.net/˜pbmserv/. [7] S. Kraus and D. Lehmann.",
                "Designing and building a negotiating automated agent.",
                "Computational Intelligence, 11:132-171, 1995. [8] H. J. Levesque, P. R. Cohen, and J. H. T. Nunes.",
                "On acting together.",
                "In Proc. of AAAI-90, pages 94-99, Boston, MA, 1990. [9] S. Markovitch and R. Reger.",
                "Learning and exploiting relative weaknesses of opponent agents.",
                "Autonomous Agents and Multi-Agent Systems, 10(2):103-130, 2005. [10] Y. M. Ronald Fagin, Joseph Y. Halpern and M. Y. Vardi.",
                "Reasoning about knowledge.",
                "MIT Press, Cambridge, Mass., 1995. [11] P. Thagard.",
                "Adversarial problem solving: Modeling an oponent using explanatory coherence.",
                "Cognitive Science, 16(1):123-149, 1992. [12] R. W. Toseland and R. F. Rivas.",
                "An Introduction to Group Work Practice.",
                "Prentice Hall, Englewood Cliffs, NJ, 2nd edition edition, 1995. [13] S. Willmott, J. Richardson, A. Bundy, and J. Levine.",
                "An adversarial planning approach to Go.",
                "Lecture Notes in Computer Science, 1558:93-112, 1999.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 557"
            ],
            "original_annotated_samples": [
                "An <br>adversarial environment</br> Model for Bounded Rational Agents in Zero-Sum Interactions Inon Zuckerman1 , Sarit Kraus1 , Jeffrey S. Rosenschein2 , Gal Kaminka1 1 Department of Computer Science 2 The School of Engineering Bar-Ilan University and Computer Science Ramat-Gan, Israel Hebrew University, Jerusalem, Israel {zukermi,sarit,galk}@cs.biu.ac.il jeff@cs.huji.ac.il ABSTRACT Multiagent environments are often not cooperative nor collaborative; in many cases, agents have conflicting interests, leading to adversarial interactions.",
                "This paper presents a formal <br>adversarial environment</br> model for bounded rational agents operating in a zero-sum environment.",
                "We define an <br>adversarial environment</br> by describing the mental states of an agent in such an environment.",
                "In this paper, we develop a formal, axiomatized model for bounded rational agents that are situated in a zero-sum <br>adversarial environment</br>.",
                "ADVERSARIAL ENVIRONMENTS The <br>adversarial environment</br> model (denoted as AE) is intended to guide the design of agents by providing a specification of the capabilities and mental attitudes of an agent in an <br>adversarial environment</br>."
            ],
            "translated_annotated_samples": [
                "Un modelo de <br>entorno adversarial</br> para agentes racionales limitados en interacciones de suma cero Inon Zuckerman1, Sarit Kraus1, Jeffrey S. Rosenschein2, Gal Kaminka1 1 Departamento de Ciencias de la Computación 2 Escuela de Ingeniería Universidad Bar-Ilan y Ciencias de la Computación Ramat-Gan, Israel Universidad Hebrea, Jerusalén, Israel {zukermi, sarit, galk}@cs.biu.ac.il jeff@cs.huji.ac.il RESUMEN Los entornos multiagentes a menudo no son cooperativos ni colaborativos; en muchos casos, los agentes tienen intereses conflictivos, lo que lleva a interacciones adversariales.",
                "Este documento presenta un modelo formal de <br>Entorno Adversarial</br> para agentes racionales acotados que operan en un entorno de suma cero.",
                "Definimos un <br>Entorno Adversario</br> al describir los estados mentales de un agente en dicho entorno.",
                "En este artículo, desarrollamos un modelo formal y axiomatizado para agentes racionales limitados que se encuentran en un <br>entorno adversarial</br> de suma cero.",
                "ENTORNOS ADVERSARIOS El modelo de <br>entorno adversario</br> (denominado como AE) está destinado a guiar el diseño de agentes al proporcionar una especificación de las capacidades y actitudes mentales de un agente en un <br>entorno adversario</br>."
            ],
            "translated_text": "Un modelo de <br>entorno adversarial</br> para agentes racionales limitados en interacciones de suma cero Inon Zuckerman1, Sarit Kraus1, Jeffrey S. Rosenschein2, Gal Kaminka1 1 Departamento de Ciencias de la Computación 2 Escuela de Ingeniería Universidad Bar-Ilan y Ciencias de la Computación Ramat-Gan, Israel Universidad Hebrea, Jerusalén, Israel {zukermi, sarit, galk}@cs.biu.ac.il jeff@cs.huji.ac.il RESUMEN Los entornos multiagentes a menudo no son cooperativos ni colaborativos; en muchos casos, los agentes tienen intereses conflictivos, lo que lleva a interacciones adversariales. Este documento presenta un modelo formal de <br>Entorno Adversarial</br> para agentes racionales acotados que operan en un entorno de suma cero. En tales entornos, los intentos de utilizar métodos de búsqueda basados en utilidad clásica pueden plantear una variedad de dificultades (por ejemplo, modelar implícitamente al oponente como un maximizador de utilidad omnisciente, en lugar de aprovechar un modelo de oponente más matizado y explícito). Definimos un <br>Entorno Adversario</br> al describir los estados mentales de un agente en dicho entorno. Luego presentamos axiomas de comportamiento que están destinados a servir como principios de diseño para construir agentes adversarios. Exploramos la aplicación de nuestro enfoque mediante el análisis de archivos de registro de juegos de Conecta Cuatro completados, y presentamos un análisis empírico de la adecuación de los axiomas. Categorías y Descriptores de Asignaturas I.2.11 [Inteligencia Artificial]: Inteligencia Artificial Distribuida-Agentes Inteligentes, Sistemas Multiagente; I.2.4 [Inteligencia Artificial]: Formalismos y Métodos de Representación del Conocimiento - Lógica Modal Términos Generales Diseño, Teoría 1. Introducción: En las primeras investigaciones en sistemas multiagente (SMA) se consideraban grupos cooperativos de agentes; debido a que los agentes individuales tenían recursos limitados o acceso limitado a la información (por ejemplo, potencia de procesamiento limitada, cobertura sensorial limitada), trabajaban juntos por diseño para resolver problemas que individualmente no podían resolver, o al menos no podían resolver de manera eficiente. Sin embargo, la investigación de MAS pronto comenzó a considerar agentes interactivos con intereses individualizados, como representantes de diferentes humanos u organizaciones con intereses no idénticos. Cuando las interacciones son guiadas por intereses diversos, los participantes pueden tener que superar desacuerdos, interacciones no cooperativas e incluso intentos intencionales de dañarse mutuamente. Cuando ocurren este tipo de interacciones, los entornos requieren un comportamiento apropiado por parte de los agentes situados en ellos. Llamamos a estos entornos Entornos Adversarios, y llamamos a los agentes en conflicto Adversarios. Los modelos de cooperación y trabajo en equipo han sido ampliamente explorados en MAS a través de la axiomatización de estados mentales (por ejemplo, [8, 4, 5]). Sin embargo, ninguna de estas investigaciones abordó los dominios adversarios y sus implicaciones para el comportamiento de los agentes. Nuestro artículo aborda este problema al proporcionar un modelo formal y axiomatizado de estados mentales para un subconjunto de dominios adversariales, específicamente entornos adversariales simples de suma cero. Los encuentros de suma cero simples existen, por supuesto, en varios juegos de dos jugadores (por ejemplo, Ajedrez, Damas), pero también existen en juegos de n jugadores (por ejemplo, Risk, Diplomacia), subastas de un solo bien y en otros lugares. En estos entornos recientes en particular, el uso de una búsqueda adversarial basada en utilidad (como el algoritmo Min-Max) no siempre proporciona una solución adecuada; la función de recompensa podría ser bastante compleja o difícil de cuantificar, y existen limitaciones computacionales naturales en agentes racionales acotados. Además, los métodos de búsqueda tradicionales (como Min-Max) no hacen uso de un modelo del oponente, lo cual ha demostrado ser una adición valiosa a la planificación adversarial [9, 3, 11]. En este artículo, desarrollamos un modelo formal y axiomatizado para agentes racionales limitados que se encuentran en un <br>entorno adversarial</br> de suma cero. El modelo utiliza diferentes operadores de modalidad, y sus principales fundamentos son el modelo SharedPlans [4] para el comportamiento colaborativo. Exploramos las propiedades del entorno y los estados mentales de los agentes para derivar axiomas de comportamiento; estos axiomas de comportamiento constituyen un modelo formal que sirve como especificación y guía de diseño para el diseño de agentes en tales entornos. Luego investigamos el comportamiento de nuestro modelo empíricamente utilizando el juego de mesa Conecta Cuatro. Mostramos que este juego se ajusta a nuestra definición de entorno, y analizamos el comportamiento de los jugadores utilizando un gran conjunto de registros de partidas completadas 550 978-81-904262-7-5 (RPS) c 2007 archivos de IFAAMAS. Además, utilizamos los resultados presentados en [9] para discutir la importancia del modelado del oponente en nuestro dominio adversarial de Conecta Cuatro. El artículo continúa de la siguiente manera. La sección 2 presenta la formalización de los modelos. La sección 3 presenta el análisis empírico y sus resultados. Discutimos el trabajo relacionado en la Sección 4, y concluimos y presentamos las direcciones futuras en la Sección 5. ENTORNOS ADVERSARIOS El modelo de <br>entorno adversario</br> (denominado como AE) está destinado a guiar el diseño de agentes al proporcionar una especificación de las capacidades y actitudes mentales de un agente en un <br>entorno adversario</br>. ",
            "candidates": [],
            "error": [
                [
                    "entorno adversarial",
                    "Entorno Adversarial",
                    "Entorno Adversario",
                    "entorno adversarial",
                    "entorno adversario",
                    "entorno adversario"
                ]
            ]
        },
        "behavioral axiom": {
            "translated_key": "axioma de comportamiento",
            "is_in_text": true,
            "original_annotated_sentences": [
                "An Adversarial Environment Model for Bounded Rational Agents in Zero-Sum Interactions Inon Zuckerman1 , Sarit Kraus1 , Jeffrey S. Rosenschein2 , Gal Kaminka1 1 Department of Computer Science 2 The School of Engineering Bar-Ilan University and Computer Science Ramat-Gan, Israel Hebrew University, Jerusalem, Israel {zukermi,sarit,galk}@cs.biu.ac.il jeff@cs.huji.ac.il ABSTRACT Multiagent environments are often not cooperative nor collaborative; in many cases, agents have conflicting interests, leading to adversarial interactions.",
                "This paper presents a formal Adversarial Environment model for bounded rational agents operating in a zero-sum environment.",
                "In such environments, attempts to use classical utility-based search methods can raise a variety of difficulties (e.g., implicitly modeling the opponent as an omniscient utility maximizer, rather than leveraging a more nuanced, explicit opponent model).",
                "We define an Adversarial Environment by describing the mental states of an agent in such an environment.",
                "We then present behavioral axioms that are intended to serve as design principles for building such adversarial agents.",
                "We explore the application of our approach by analyzing log files of completed Connect-Four games, and present an empirical analysis of the axioms appropriateness.",
                "Categories and Subject Descriptors I.2.11 [Artificial Intelligence]: Distributed Artificial Intelligence-Intelligent agents,Multiagent Systems; I.2.4 [Artificial Intelligence]: Knowledge Representation Formalisms and Methods -Modal logic General Terms Design, Theory 1.",
                "INTRODUCTION Early research in multiagent systems (MAS) considered cooperative groups of agents; because individual agents had limited resources, or limited access to information (e.g., limited processing power, limited sensor coverage), they worked together by design to solve problems that individually they could not solve, or at least could not solve as efficiently.",
                "MAS research, however, soon began to consider interacting agents with individuated interests, as representatives of different humans or organizations with non-identical interests.",
                "When interactions are guided by diverse interests, participants may have to overcome disagreements, uncooperative interactions, and even intentional attempts to damage one another.",
                "When these types of interactions occur, environments require appropriate behavior from the agents situated in them.",
                "We call these environments Adversarial Environments, and call the clashing agents Adversaries.",
                "Models of cooperation and teamwork have been extensively explored in MAS through the axiomatization of mental states (e.g., [8, 4, 5]).",
                "However, none of this research dealt with adversarial domains and their implications for agent behavior.",
                "Our paper addresses this issue by providing a formal, axiomatized mental state model for a subset of adversarial domains, namely simple zero-sum adversarial environments.",
                "Simple zero-sum encounters exist of course in various twoplayer games (e.g., Chess, Checkers), but they also exist in n-player games (e.g., Risk, Diplomacy), auctions for a single good, and elsewhere.",
                "In these latter environments especially, using a utility-based adversarial search (such as the Min-Max algorithm) does not always provide an adequate solution; the payoff function might be quite complex or difficult to quantify, and there are natural computational limitations on bounded rational agents.",
                "In addition, traditional search methods (like Min-Max) do not make use of a model of the opponent, which has proven to be a valuable addition to adversarial planning [9, 3, 11].",
                "In this paper, we develop a formal, axiomatized model for bounded rational agents that are situated in a zero-sum adversarial environment.",
                "The model uses different modality operators, and its main foundations are the SharedPlans [4] model for collaborative behavior.",
                "We explore environment properties and the mental states of agents to derive behavioral axioms; these behavioral axioms constitute a formal model that serves as a specification and design guideline for agent design in such settings.",
                "We then investigate the behavior of our model empirically using the Connect-Four board game.",
                "We show that this game conforms to our environment definition, and analyze players behavior using a large set of completed match log 550 978-81-904262-7-5 (RPS) c 2007 IFAAMAS files.",
                "In addition, we use the results presented in [9] to discuss the importance of opponent modeling in our ConnectFour adversarial domain.",
                "The paper proceeds as follows.",
                "Section 2 presents the models formalization.",
                "Section 3 presents the empirical analysis and its results.",
                "We discuss related work in Section 4, and conclude and present future directions in Section 5. 2.",
                "ADVERSARIAL ENVIRONMENTS The adversarial environment model (denoted as AE) is intended to guide the design of agents by providing a specification of the capabilities and mental attitudes of an agent in an adversarial environment.",
                "We focus here on specific types of adversarial environments, specified as follows: 1.",
                "Zero-Sum Interactions: positive and negative utilities of all agents sum to zero; 2.",
                "Simple AEs: all agents in the environment are adversarial agents; 3.",
                "Bilateral AEs: AEs with exactly two agents; 4.",
                "Multilateral AEs: AEs of three or more agents.",
                "We will work on both bilateral and multilateral instantiations of zero-sum and simple environments.",
                "In particular, our adversarial environment model will deal with interactions that consist of N agents (N ≥ 2), where all agents are adversaries, and only one agent can succeed.",
                "Examples of such environments range from board games (e.g., Chess, Connect-Four, and Diplomacy) to certain economic environments (e.g., N-bidder auctions over a single good). 2.1 Model Overview Our approach is to formalize the mental attitudes and behaviors of a single adversarial agent; we consider how a single agent perceives the AE.",
                "The following list specifies the conditions and mental states of an agent in a simple, zero-sum AE: 1.",
                "The agent has an individual intention that its own goal will be completed; 2.",
                "The agent has an individual belief that it and its adversaries are pursuing full conflicting goals (defined below)there can be only one winner; 3.",
                "The agent has an individual belief that each adversary has an intention to complete its own full conflicting goal; 4.",
                "The agent has an individual belief in the (partial) profile of its adversaries.",
                "Item 3 is required, since it might be the case that some agent has a full conflicting goal, and is currently considering adopting the intention to complete it, but is, as of yet, not committed to achieving it.",
                "This might occur because the agent has not yet deliberated about the effects that adopting that intention might have on the other intentions it is currently holding.",
                "In such cases, it might not consider itself to even be in an adversarial environment.",
                "Item 4 states that the agent should hold some belief about the profiles of its adversaries.",
                "The profile represents all the knowledge the agent has about its adversary: its weaknesses, strategic capabilities, goals, intentions, trustworthiness, and more.",
                "It can be given explicitly or can be learned from observations of past encounters. 2.2 Model Definitions for Mental States We use Grosz and Krauss definitions of the modal operators, predicates, and meta-predicates, as defined in their SharedPlan formalization [4].",
                "We recall here some of the predicates and operators that are used in that formalization: Int.To(Ai, α, Tn, Tα, C) represents Ais intentions at time Tn to do an action α at time Tα in the context of C. Int.Th(Ai, prop, Tn, Tprop, C) represents Ais intentions at time Tn that a certain proposition prop holds at time Tprop in the context of C. The potential intention operators, Pot.Int.To(...) and Pot.Int.Th(...), are used to represent the mental state when an agent considers adopting an intention, but has not deliberated about the interaction of the other intentions it holds.",
                "The operator Bel(Ai, f, Tf ) represents agent Ai believing in the statement expressed in formula f, at time Tf .",
                "MB(A, f, Tf ) represents mutual belief for a group of agents A.",
                "A snapshot of the system finds our environment to be in some state e ∈ E of environmental variable states, and each adversary in any LAi ∈ L of possible local states.",
                "At any given time step, the system will be in some world w of the set of all possible worlds w ∈ W, where w = E×LA1 ×LA2 × ...LAn , and n is the number of adversaries.",
                "For example, in a Texas Holdem poker game, an agents local state might be its own set of cards (which is unknown to its adversary) while the environment will consist of the betting pot and the community cards (which are visible to both players).",
                "A utility function under this formalization is defined as a mapping from a possible world w ∈ W to an element in , which expresses the desirability of the world, from a single agent perspective.",
                "We usually normalize the range to [0,1], where 0 represents the least desirable possible world, and 1 is the most desirable world.",
                "The implementation of the utility function is dependent on the domain in question.",
                "The following list specifies new predicates, functions, variables, and constants used in conjunction with the original definitions for the adversarial environment formalization: 1. φ is a null action (the agent does not do anything). 2.",
                "GAi is the set of agent Ais goals.",
                "Each goal is a set of predicates whose satisfaction makes the goal complete (we use G∗ Ai ∈ GAi to represent an arbitrary goal of agent Ai). 3. gAi is the set of agent Ais subgoals.",
                "Subgoals are predicates whose satisfaction represents an important milestone toward achievement of the full goal. gG∗ Ai ⊆ gAi is the set of subgoals that are important to the completion of goal G∗ Ai (we will use g∗ G∗ Ai ∈ gG∗ Ai to represent an arbitrary subgoal). 4.",
                "P Aj Ai is the profile object agent Ai holds about agent Aj. 5.",
                "CA is a general set of actions for all agents in A which are derived from the environments constraints.",
                "CAi ⊆ CA is the set of agent Ais possible actions. 6.",
                "Do(Ai, α, Tα, w) holds when Ai performs action α over time interval Tα in world w. 7.",
                "Achieve(G∗ Ai , α, w) is true when goal G∗ Ai is achieved following the completion of action α in world w ∈ W, where α ∈ CAi . 8.",
                "Profile(Ai, PAi Ai ) is true when agent Ai holds an object profile for agent Aj.",
                "Definition 1.",
                "Full conflict (FulConf ) describes a zerosum interaction where only a single goal of the goals in conflict can be completed.",
                "FulConf(G∗ Ai , G∗ Aj ) ⇒ (∃α ∈ CAi , ∀w, β ∈ CAj ) (Achieve(G∗ Ai , α, w) ⇒ ¬Achieve(G∗ Aj , β, w)) ∨ (∃β ∈ CAj , ∀w, α ∈ CAi )(Achieve(G∗ Aj , β, w) ⇒ ¬Achieve(G∗ Ai , α, w)) Definition 2.",
                "Adversarial Knowledge (AdvKnow) is a function returning a value which represents the amount of The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 551 knowledge agent Ai has on the profile of agent Aj, at time Tn.",
                "The higher the value, the more knowledge agent Ai has.",
                "AdvKnow : P Aj Ai × Tn → Definition 3.",
                "Eval - This evaluation function returns an estimated expected utility value for an agent in A, after completing an action from CA in some world state w. Eval : A × CA × w → Definition 4.",
                "TrH - (Threshold) is a numerical constant in the [0,1] range that represents an evaluation function (Eval) threshold value.",
                "An action that yields an estimated utility evaluation above the TrH is regarded as a highly beneficial action.",
                "The Eval value is an estimation and not the real utility function, which is usually unknown.",
                "Using the real utility value for a rational agent would easily yield the best outcome for that agent.",
                "However, agents usually do not have the real utility functions, but rather a heuristic estimate of it.",
                "There are two important properties that should hold for the evaluation function: Property 1.",
                "The evaluation function should state that the most desirable world state is one in which the goal is achieved.",
                "Therefore, after the goal has been satisfied, there can be no future action that can put the agent in a world state with higher Eval value. (∀Ai, G∗ Ai , α, β ∈ CAi , w ∈ W) Achieve(G∗ Ai , α, w) ⇒ Eval(Ai, α, w) ≥ Eval(Ai, β, w) Property 2.",
                "The evaluation function should project an action that causes a completion of a goal or a subgoal to a value which is greater than TrH (a highly beneficial action). (∀Ai, G∗ Ai ∈ GAi , α ∈ CAi , w ∈ W, g∗ GAi ∈ gGAi ) Achieve(G∗ Ai , α, w) ∨ Achieve(g∗ GAi , α, w) ⇒ Eval(Ai, α, w) ≥ TrH.",
                "Definition 5.",
                "SetAction We define a set action (SetAction) as a set of action operations (either complex or basic actions) from some action sets CAi and CAj which, according to agent Ais belief, are attached together by a temporal and consequential relationship, forming a chain of events (action, and its following consequent action). (∀α1 , . . . , αu ∈ CAi , β1 , . . . , βv ∈ CAj , w ∈ W) SetAction(α1 , . . . , αu , β1 , . . . , βv , w) ⇒ ((Do(Ai, α1 , Tα1 , w) ⇒ Do(Aj, β1 , Tβ1 , w)) ⇒ Do(Ai, α2 , Tα2 , w) ⇒ . . . ⇒ Do(Ai, αu , Tαu , w)) The consequential relation might exist due to various environmental constraints (when one action forces the adversary to respond with a specific action) or due to the agents knowledge about the profile of its adversary.",
                "Property 3.",
                "As the knowledge we have about our adversary increases we will have additional beliefs about its behavior in different situations which in turn creates new set actions.",
                "Formally, if our AdvKnow at time Tn+1 is greater than AdvKnow at time Tn, then every SetAction known at time Tn is also known at time Tn+1.",
                "AdvKnow(P Aj Ai , Tn+1) > AdvKnow(P Aj Ai , Tn) ⇒ (∀α1 , . . . , αu ∈ CAi , β1 , . . . , βv ∈ CAj ) Bel(Aag, SetAction(α1 , . . . , αu , β1 , . . . , βv ), Tn) ⇒ Bel(Aag, SetAction(α1 , . . . , αu , β1 , . . . , βv ), Tn+1) 2.3 The Environment Formulation The following axioms provide the formal definition for a simple, zero-sum Adversarial Environment (AE).",
                "Satisfaction of these axioms means that the agent is situated in such an environment.",
                "It provides specifications for agent Aag to interact with its set of adversaries A with respect to goals G∗ Aag and G∗ A at time TCo at some world state w. AE(Aag, A, G∗ Aag , A1, . . . , Ak, G∗ A1 , . . . , G∗ Ak , Tn, w) 1.",
                "Aag has an Int.Th his goal would be completed: (∃α ∈ CAag , Tα) Int.Th(Aag, Achieve(G∗ Aag , α), Tn, Tα, AE) 2.",
                "Aag believes that it and each of its adversaries Ao are pursuing full conflicting goals: (∀Ao ∈ {A1, . . . , Ak}) Bel(Aag, FulConf(G∗ Aag , G∗ Ao ), Tn) 3.",
                "Aag believes that each of his adversaries in Ao has the Int.Th his conflicting goal G∗ Aoi will be completed: (∀Ao ∈ {A1, . . . , Ak})(∃β ∈ CAo , Tβ) Bel(Aag, Int.Th(Ao, Achieve(G∗ Ao , β), TCo, Tβ, AE), Tn) 4.",
                "Aag has beliefs about the (partial) profiles of its adversaries (∀Ao ∈ {A1, . . . , Ak}) (∃PAo Aag ∈ PAag )Bel(Aag, Profile(Ao, PAo Aag ), Tn) To build an agent that will be able to operate successfully within such an AE, we must specify behavioral guidelines for its interactions.",
                "Using a naive Eval maximization strategy to a certain search depth will not always yield satisfactory results for several reasons: (1) the search horizon problem when searching for a fixed depth; (2) the strong assumption of an optimally rational, unbounded resources adversary; (3) using an estimated evaluation function which will not give optimal results in all world states, and can be exploited [9].",
                "The following axioms specify the behavioral principles that can be used to differentiate between successful and less successful agents in the above Adversarial Environment.",
                "Those axioms should be used as specification principles when designing and implementing agents that should be able to perform well in such Adversarial Environments.",
                "The behavioral axioms represent situations in which the agent will adopt potential intentions to (Pot.Int.To(...)) perform an action, which will typically require some means-end reasoning to select a possible course of action.",
                "This reasoning will lead to the adoption of an Int.To(...) (see [4]).",
                "A1.",
                "Goal Achieving Axiom.",
                "The first axiom is the simplest case; when the agent Aag believes that it is one action (α) away from achieving his conflicting goal G∗ Aag , it should adopt the potential intention to do α and complete its goal. (∀Aag, α ∈ CAag , Tn, Tα, w ∈ W) (Bel(Aag, Do(Aag, α, Tα, w) ⇒ Achieve(G∗ Aag , α, w)) ⇒ Pot.Int.To(Aag, α, Tn, Tα, w) This somewhat trivial behavior is the first and strongest axiom.",
                "In any situation, when the agent is an action away from completing the goal, it should complete the action.",
                "Any fair Eval function would naturally classify α as the maximal value action (property 1).",
                "However, without explicit axiomatization of such behavior there might be situations where the agent will decide on taking another action for various reasons, due to its bounded decision resources.",
                "A2.",
                "Preventive Act Axiom.",
                "Being in an adversarial situation, agent Aag might decide to take actions that will damage one of its adversarys plans to complete its goal, even if those actions do not explicitly advance Aag towards its conflicting goal G∗ Aag .",
                "Such preventive action will take place when agent Aag has a belief about the possibility of its adversary Ao doing an action β that will give it a high 552 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) utility evaluation value (> TrH).",
                "Believing that taking action α will prevent the opponent from doing its β, it will adopt a potential intention to do α. (∀Aag, Ao ∈ A, α ∈ CAag , β ∈ CAo , Tn, Tβ, w ∈ W) (Bel(Aag, Do(Ao, β, Tβ, w) ∧ Eval(Ao, β, w) > TrH, Tn) ∧ Bel(Aag, Do(Aag, α, Tα, w) ⇒ ¬Do(Ao, β, Tβ, w), Tn) ⇒ Pot.Int.To(Aag, α, Tn, Tα, w) This axiom is a basic component of any adversarial environment.",
                "For example, looking at a Chess board game, a player could realize that it is about to be checkmated by its opponent, thus making a preventive move.",
                "Another example is a Connect Four game: when a player has a row of three chips, its opponent must block it, or lose.",
                "A specific instance of A1 occurs when the adversary is one action away from achieving its goal, and immediate preventive action needs to be taken by the agent.",
                "Formally, we have the same beliefs as stated above, with a changed belief that doing action β will cause agent Ao to achieve its goal.",
                "Proposition 1: Prevent or lose case. (∀Aag, Ao ∈ A, α ∈ CAag , β ∈ CAo , G∗ Ao , Tn, Tα, Tβ, w ∈ W) Bel(Aag, Do(Ao, β, Tβ, w) ⇒ Achieve(G∗ Ao , β, w), Tn) ∧ Bel(Aag, Do(Aag, α, Tα, w) ⇒ ¬Do(Ao, β, Tβ, w)) ⇒ Pot.Int.To(Aag, α, Tn, Tα, w) Sketch of proof: Proposition 1 can be easily derived from axiom A1 and the property 2 of the Eval function, which states that any action that causes a completion of a goal is a highly beneficial action.",
                "The preventive act behavior will occur implicitly when the Eval function is equal to the real world utility function.",
                "However, being bounded rational agents and dealing with an estimated evaluation function we need to explicitly axiomatize such behavior, for it will not always occur implicitly from the evaluation function.",
                "A3.",
                "Suboptimal Tactical Move Axiom.",
                "In many scenarios a situation may occur where an agent will decide not to take the current most beneficial action it can take (the action with the maximal utility evaluation value), because it believes that taking another action (with lower utility evaluation value) might yield (depending on the adversarys response) a future possibility for a highly beneficial action.",
                "This will occur most often when the Eval function is inaccurate and differs by a large extent from the Utility function.",
                "Put formally, agent Aag believes in a certain SetAction that will evolve according to its initial action and will yield a high beneficial value (> TrH) solely for it. (∀Aag, Ao ∈ A, Tn, w ∈ W) (∃α1 , . . . , αu ∈ CAi , β1 , . . . , βv ∈ CAj , Tα1 ) Bel(Aag, SetAction(α1 , . . . , αu , β1 , . . . , βv ), Tn) ∧ Bel(Aag, Eval(Ao, βv , w) < TrH < Eval(Aag, αu , w), Tn) ⇒ Pot.Int.To(Aag, α1 , Tn, Tα1 , w) An agent might believe that a chain of events will occur for various reasons due to the inevitable nature of the domain.",
                "For example, in Chess, we often observe the following: a move causes a check position, which in turn limits the opponents moves to avoiding the check, to which the first player might react with another check, and so on.",
                "The agent might also believe in a chain of events based on its knowledge of its adversarys profile, which allows it to foresee the adversarys movements with high accuracy.",
                "A4.",
                "Profile Detection Axiom.",
                "The agent can adjust its adversarys profiles by observations and pattern study (specifically, if there are repeated encounters with the same adversary).",
                "However, instead of waiting for profile information to be revealed, an agent can also initiate actions that will force its adversary to react in a way that will reveal profile knowledge about it.",
                "Formally, the axiom states that if all actions (γ) are not highly beneficial actions (< TrH), the agent can do action α in time Tα if it believes that it will result in a non-highly beneficial action β from its adversary, which in turn teaches it about the adversarys profile, i.e., gives a higher AdvKnow(P Aj Ai , Tβ). (∀Aag, Ao ∈ A, α ∈ CAag , β ∈ CAo , Tn, Tα, Tβ, w ∈ W) Bel(Aag, (∀γ ∈ CAag )Eval(Aag, γ, w) < TrH, Tn) ∧ Bel(Aag, Do(Aag, α, Tα, w) ⇒ Do(Ao, β, Tβ, w), Tn) ∧ Bel(Aag, Eval(Ao, β, w) < TrH) ∧ Bel(Aag, AdvKnow(P Aj Ai , Tβ) > AdvKnow(P Aj Ai , Tn), Tn) ⇒ Pot.Int.To(Aag, α, Tn, Tα, w) For example, going back to the Chess board game scenario, consider starting a game versus an opponent about whom we know nothing, not even if it is a human or a computerized opponent.",
                "We might start playing a strategy that will be suitable versus an average opponent, and adjust our game according to its level of play.",
                "A5.",
                "Alliance Formation Axiom The following <br>behavioral axiom</br> is relevant only in a multilateral instantiation of the adversarial environment (obviously, an alliance cannot be formed in a bilateral, zero-sum encounter).",
                "In different situations during a multilateral interaction, a group of agents might believe that it is in their best interests to form a temporary alliance.",
                "Such an alliance is an agreement that constrains its members behavior, but is believed by its members to enable them to achieve a higher utility value than the one achievable outside of the alliance.",
                "As an example, we can look at the classical Risk board game, where each player has an individual goal of being the sole conquerer of the world, a zero-sum game.",
                "However, in order to achieve this goal, it might be strategically wise to make short-term ceasefire agreements with other players, or to join forces and attack an opponent who is stronger than the rest.",
                "An alliances terms defines the way its members should act.",
                "It is a set of predicates, denoted as Terms, that is agreed upon by the alliance members, and should remain true for the duration of the alliance.",
                "For example, the set Terms in the Risk scenario, could contain the following predicates: 1.",
                "Alliance members will not attack each other on territories X, Y and Z; 2.",
                "Alliance members will contribute C units per turn for attacking adversary Ao; 3.",
                "Members are obligated to stay as part of the alliance until time Tk or until adversarys Ao army is smaller than Q.",
                "The set Terms specifies inter-group constraints on each of the alliance members (∀Aal i ∈ Aal ⊆ A) set of actions Cal i ⊆ C. Definition 6.",
                "Al val - the total evaluation value that agent Ai will achieve while being part of Aal is the sum of Evali (Eval for Ai) of each of Aal j Eval values after taking their own α actions (via the agent(α) predicate): Al val(Ai, Cal , Aal , w) = α∈Cal Evali(Aal j , agent(α), w) Definition 7.",
                "Al TrH - is a number representing an Al val The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 553 threshold; above it, the alliance can be said to be a highly beneficial alliance.",
                "The value of Al TrH will be calculated dynamically according to the progress of the interaction, as can be seen in [7].",
                "After an alliance is formed, its members are now working in their normal adversarial environment, as well as according to the mental states and axioms required for their interactions as part of the alliance.",
                "The following Alliance model (AL) specifies the conditions under which the group Aal can be said to be in an alliance and working with a new and constrained set of actions Cal , at time Tn.",
                "AL(Aal , Cal , w, Tn) 1.",
                "Aal has a MB that all members are part of Aal : MB(Aal , (∀Aal i ∈ Aal )member(Aal i , Aal ), Tn) 2.",
                "Aal has a MB that the group be maintained: MB(Aal , (∀Aal i ∈ Aal )Int.Th (Ai, member(Ai, Aal ), Tn, Tn+1, Co), Tn) 3.",
                "Aal has a MB that being members gives them high utility value: MB(Aal , (∀Aal i ∈ Aal )Al val(Aal i , Cal , Aal , w) ≥ Al TrH, Tn) Members profiles are a crucial part of successful alliances.",
                "We assume that agents that have more accurate profiles of their adversaries will be more successful in such environments.",
                "Such agents will be able to predict when a member is about to breach the alliances contract (item 2 in the above model), and take counter measures (when item 3 will falsify).",
                "The robustness of the alliance is in part a function of its members trustfulness measure, objective position estimation, and other profile properties.",
                "We should note that an agent can simultaneously be part of more than one alliance.",
                "Such a temporary alliance, where the group members do not have a joint goal but act collaboratively for the interest of their own individual goals, is classified as a Treatment Group by modern psychologists [12] (in contrast to a Task Group, where its members have a joint goal).",
                "The Shared Activity model as presented in [5] modeled Treatment Group behavior using the same SharedPlans formalization.",
                "When comparing both definitions of an alliance and a Treatment Group we found an unsurprising resemblance between both models: the environment models definitions are almost identical (see SAs definitions in [5]), and their Selfish-Act and Cooperative Act axioms conform to our adversarial agents behavior.",
                "The main distinction between both models is the integration of a Helpful-behavior act axiom, in the Shared Activity which cannot be part of ours.",
                "This axiom states that an agent will consider taking action that will lower its Eval value (to a certain lower bound), if it believes that a group partner will gain a significant benefit.",
                "Such behavior cannot occur in a pure adversarial environment (as a zero-sum game is), where the alliance members are constantly on watch to manipulate their alliance to their own advantage.",
                "A6.",
                "Evaluation Maximization Axiom.",
                "In a case when all other axioms are inapplicable, we will proceed with the action that maximizes the heuristic value as computed in the Eval function. (∀Aag, Ao ∈ A, α ∈ Cag, Tn, w ∈ W) Bel(Aag, (∀γ ∈ Cag)Eval(Aag, α, w) ≥ Eval(Aag, γ, w), Tn) ⇒ Pot.Int.To(Aag, α, Tn, Tα, w) T1.",
                "Optimality on Eval = Utility The above axiomatic model handles situations where the Utility is unknown and the agents are bounded rational agents.",
                "The following theorem shows that in bilateral interactions, where the agents have the real Utility function (i.e., Eval = Utility) and are rational agents, the axioms provide the same optimal result as classic adversarial search (e.g., Min-Max).",
                "Theorem 1.",
                "Let Ae ag be an unbounded rational AE agent using the Eval heuristic evaluation function, Au ag be the same agent using the true Utility function, and Ao be a sole unbounded utility-based rational adversary.",
                "Given that Eval = Utility: (∀α ∈ CAu ag , α ∈ CAe ag , Tn, w ∈ W) Pot.Int.To(Au ag, α, Tn, Tα, w) → Pot.Int.To(Ae ag, α , Tn, Tα, w) ∧ ((α = α ) ∨ (Utility(Au ag, α, w) = Eval(Ae ag, α , w))) Sketch of proof - Given that Au ag has the real utility function and unbounded resources, it can generate the full game tree and run the optimal MinMax algorithm to choose the highest utility value action, which we denote by, α.",
                "The proof will show that Ae ag, using the AE axioms, will select the same or equal utility α (when there is more than one action with the same max utility) when Eval = Utility. (A1) Goal achieving axiom - suppose there is an α such that its completion will achieve Au ags goal.",
                "It will obtain the highest utility by Min-Max for Au ag.",
                "The Ae ag agent will select α or another action with the same utility value via A1.",
                "If such α does not exist, Ae ag cannot apply this axiom, and proceeds to A2. (A2) Preventive act axiom - (1) Looking at the basic case (see Prop1), if there is a β which leads Ao to achieve its goal, then a preventive action α will yield the highest utility for Au ag.",
                "Au ag will choose it through the utility, while Ae ag will choose it through A2. (2) In the general case, β is a highly beneficial action for Ao, thus yields low utility for Au ag, which will guide it to select an α that will prevent β, while Ae ag will choose it through A2.1 If such β does not exist for Ao, then A2 is not applicable, and Ae ag can proceed to A3. (A3) Suboptimal tactical move axiom - When using a heuristic Eval function, Ae ag has a partial belief in the profile of its adversary (item 4 in AE model), which may lead it to believe in SetActions (Prop1).",
                "In our case, Ae ag is holding a full profile on its optimal adversary and knows that Ao will behave optimally according to the real utility values on the complete search tree, therefore, any belief about suboptimal SetAction cannot exist, yielding this axiom inapplicable.",
                "Ae ag will proceed to A4. (A4) Profile detection axiom - Given that Ae ag has the full profile of Ao, none of Ae ags actions can increase its knowledge.",
                "That axiom will not be applied, and the agent will proceed with A6 (A5 will be disregarded because the interaction is bilateral). (A6) Evaluation maximization axiom - This axiom will select the max Eval for Ae ag.",
                "Given that Eval = Utility, the same α that was selected by Au ag will be selected. 3.",
                "EVALUATION The main purpose of our experimental analysis is to evaluate the models behavior and performance in a real adversarial environment.",
                "This section investigates whether bounded 1 A case where following the completion of β there exists a γ which gives high utility for Agent Au ag, cannot occur because Ao uses the same utility, and γs existence will cause it to classify β as a low utility action. 554 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) rational agents situated in such adversarial environments will be better off applying our suggested behavioral axioms. 3.1 The Domain To explore the use of the above model and its behavioral axioms, we decided to use the Connect-Four game as our adversarial environment.",
                "Connect-Four is a 2-player, zerosum game which is played using a 6x7 matrix-like board.",
                "Each turn, a player drops a disc into one of the 7 columns (the set of 21 discs is usually colored yellow for player 1 and red for player 2; we will use White and Black respectively to avoid confusion).",
                "The winner is the first player to complete a horizontal, vertical, or diagonal set of four discs with its color.",
                "On very rare occasions, the game might end in a tie if all the empty grids are filled, but no player managed to create a 4-disc set.",
                "The Connect-Four game was solved in [1], where it is shown that the first player (playing with the white discs) can force a win by starting in the middle column (column 4) and playing optimally However, the optimal strategy is very complex, and difficult to follow even for complex bounded rational agents, such as human players.",
                "Before we can proceed checking agent behavior, we must first verify that the domain conforms to the adversarial environments definition as given above (which the behavioral axioms are based on).",
                "First, when playing a Connect-Four game, the agent has an intention to win the game (item 1).",
                "Second (item 2), our agent believes that in Connect-Four there can only be one winner (or no winner at all in the rare occurrence of a tie).",
                "In addition, our agent believes that its opponent to the game will try to win (item 3), and we hope it has some partial knowledge (item 4) about its adversary (this knowledge can vary from nothing, through simple facts such as age, to strategies and weaknesses).",
                "Of course, not all Connect-Four encounters are adversarial.",
                "For example, when a parent is playing the game with its child, the following situation might occur: the child, having a strong incentive to win, treats the environment as adversarial (it intends to win, understands that there can only be one winner, and believes that its parent is trying to beat him).",
                "However, the parents point of view might see the environment as an educational one, where its goal is not to win the game, but to cause enjoyment or practice strategic reasoning.",
                "In such an educational environment, a new set of behavioral axioms might be more beneficial to the parents goals than our suggested adversarial behavioral axioms. 3.2 Axiom Analysis After showing that the Connect-Four game is indeed a zero-sum, bilateral adversarial environment, the next step is to look at players behaviors during the game and check whether behaving according to our model does improve performance.",
                "To do so we have collected log files from completed Connect-Four games that were played by human players over the Internet.",
                "Our collected log file data came from Play by eMail (PBeM) sites.",
                "These are web sites that host email games, where each move is taken by an email exchange between the server and the players.",
                "Many such sites archives contain real competitive interactions, and also maintain a ranking system for their members.",
                "Most of the data we used can be found in [6].",
                "As can be learned from [1], Connect-Four has an optimal strategy and a considerable advantage for the player who starts the game (which we call the White player).",
                "We will concentrate in our analysis on the second players moves (to be called Black).",
                "The White player, being the first to act, has the so-called initiative advantage.",
                "Having the advantage and a good strategy will keep the Black player busy reacting to its moves, instead of initiating threats.",
                "A threat is a combination of three discs of the same color, with an empty spot for the fourth winning disk.",
                "An open threat is a threat that can be realized in the opponents next move.",
                "In order for the Black player to win, it must somehow turn the tide, take the advantage and start presenting threats to the White player.",
                "We will explore Black players behavior and their conformance to our axioms.",
                "To do so, we built an application that reads log files and analyzes the Black players moves.",
                "The application contains two main components: (1) a Min-Max algorithm for evaluation of moves; (2) open threats detector for the discovering of open threats.",
                "The Min-Max algorithm will work to a given depth, d and for each move α will output the heuristic value for the next action taken by the player as written in the log file, h(α), alongside the maximum heuristic value, maxh(α), that could be achieved prior to taking the move (obviously, if h(α) = maxh(α), then the player did not do the optimal move heuristically).",
                "The threat detectors job is to notify if some action was taken in order to block an open threat (not blocking an open threat will probably cause the player to lose in the opponents next move).",
                "The heuristic function used by Min-Max to evaluate the players utility is the following function, which is simple to compute, yet provides a reasonable challenge to human opponents: Definition 8.",
                "Let Group be an adjacent set of four squares that are horizontal, vertical, or diagonal.",
                "Groupn b (Groupn w) be a Group with n pieces of the black (white) color and 4−n empty squares. h = ((Group1 b ∗α)+(Group2 b ∗β)+(Group3 b ∗γ)+(Group4 b ∗∞)) − ((Group1 w ∗α)+(Group2 w ∗β)+(Group3 w ∗γ)+(Group4 w ∗∞)) The values of α, β and δ can vary to form any desired linear combination; however, it is important to value them with the α < β < δ ordering in mind (we used 1, 4, and 8 as their respective values).",
                "Groups of 4 discs of the same color means victory, thus discovery of such a group will result in ∞ to ensure an extreme value.",
                "We now use our estimated evaluation function to evaluate the Black players actions during the Connect-Four adversarial interaction.",
                "Each game from the log file was input into the application, which processed and output a reformatted log file containing the h value of the current move, the maxh value that could be achieved, and a notification if an open threat was detected.",
                "A total of 123 games were analyzed (57 with White winning, and 66 with Black winning).",
                "A few additional games were manually ignored in the experiment, due to these problems: a player abandoning the game while the outcome is not final, or a blunt irrational move in the early stages of the game (e.g., not blocking an obvious winning group in the first opening moves).",
                "In addition, a single tie game was also removed.",
                "The simulator was run to a search depth of 3 moves.",
                "We now proceed to analyze the games with respect to each <br>behavioral axiom</br>.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 555 Table 1: Average heuristic difference analysis Black losses Black Won Avg minh -17.62 -12.02 Avg 3 lowest h moves (min3 h) -13.20 -8.70 3.2.1 Affirming the Suboptimal tactical move axiom The following section presents the heuristic evaluations of the Min-Max algorithm for each action, and checks the amount and extent of suboptimal tactical actions and their implications on performance.",
                "Table 1 shows results and insights from the games heuristic analysis, when search depth equals 3 (this search depth was selected for the results to be comparable to [9], see Section 3.2.3).",
                "The tables heuristic data is the difference between the present maximal heuristic value and the heuristic value of the action that was eventually taken by the player (i.e., the closer the number is to 0, the closer the action was to the maximum heuristic action).",
                "The first row presents the difference values of the action that had the maximal difference value among all the Black players actions in a given game, as averaged over all Blacks winning and losing games (see respective columns).",
                "In games in which the Black player loses, its average difference value was -17.62, while in games in which the Black player won, its average was -12.02.",
                "The second row expands the analysis by considering the 3 highest heuristic difference actions, and averaging them.",
                "In that case, we notice an average heuristic difference of 5 points between games which the Black player loses and games in which it wins.",
                "Nevertheless, the importance of those numbers is that they allowed us to take an educated guess on a threshold number of 11.5, as the value of the TrH constant, which differentiates between normal actions and highly beneficial ones.",
                "After finding an approximated TrH constant, we can proceed with an analysis of the importance of suboptimal moves.",
                "To do so we took the subset of games in which the minimum heuristic difference value for Blacks actions was 11.5.",
                "As presented in Table 2, we can see the different min3 h average of the 3 largest ranges and the respective percentage of games won.",
                "The first row shows that the Black player won only 12% of the games in which the average of its 3 highest heuristically difference actions (min3 h) was smaller than the suggested threshold, TrH = 11.5.",
                "The second row shows a surprising result: it seems that when min3 h > −4 the Black player rarely wins.",
                "Intuition would suggest that games in which the action evaluation values were closer to the maximal values will result in more winning games for Black.",
                "However, it seems that in the Connect-Four domain, merely responding with somewhat easily expected actions, without initiating a few surprising and suboptimal moves, does not yield good results.",
                "The last row sums up the main insights from the analysis; most of Blacks wins (83%) came when its min3 h was in the range of -11.5 to -4.",
                "A close inspection of those Black winning games shows the following pattern behind the numbers: after standard opening moves, Black suddenly drops a disc into an isolated column, which seems a waste of a move.",
                "White continues to build its threats, while usually disregarding Blacks last move, which in turn uses the isolated disc as an anchor for a future winning threat.",
                "The results show that it was beneficial for the Black player Table 2: Blacks winnings percentages % of games min3 h < −11.5 12% min3 h > −4 5% −11.5 ≤ min3 h ≤ −4 83% to take suboptimal actions and not give the current highest possible heuristic value, but will not be too harmful for its position (i.e., will not give high beneficial value to its adversary).",
                "As it turned out, learning the threshold is an important aspect of success: taking wildly risky moves (min3 h < −11.5) or trying to avoid them (min3 h > −4) reduces the Black players winning chances by a large margin. 3.2.2 Affirming the Profile Monitoring Axiom In the task of showing the importance of monitoring ones adversaries profiles, our log files could not be used because they did not contain repeated interactions between players, which are needed to infer the players knowledge about their adversaries.",
                "However, the importance of opponent modeling and its use in attaining tactical advantages was already studied in various domains ([3, 9] are good examples).",
                "In a recent paper, Markovitch and Reger [9] explored the notion of learning and exploitation of opponent weakness in competitive interactions.",
                "They apply simple learning strategies by analyzing examples from past interactions in a specific domain.",
                "They also used the Connect-Four adversarial domain, which can now be used to understand the importance of monitoring the adversarys profile.",
                "Following the presentation of their theoretical model, they describe an extensive empirical study and check the agents performance after learning the weakness model with past examples.",
                "One of the domains used as a competitive environment was the same Connect-Four game (Checkers was the second domain).",
                "Their heuristic function was identical to ours with three different variations (H1, H2, and H3) that are distinguished from one another in their linear combination coefficient values.",
                "The search depth for the players was 3 (as in our analysis).",
                "Their extensive experiments check and compare various learning strategies, risk factors, predefined feature sets and usage methods.",
                "The bottom line is that the Connect-Four domain shows an improvement from a 0.556 winning rate before modeling to a 0.69 after modeling (page 22).",
                "Their conclusions, showing improved performance when holding and using the adversarys model, justify the effort to monitor the adversary profile for continuous and repeated interactions.",
                "An additional point that came up in their experiments is the following: after the opponent weakness model has been learned, the authors describe different methods of integrating the opponent weakness model into the agents decision strategy.",
                "Nevertheless, regardless of the specific method they chose to work with, all integration methods might cause the agent to take suboptimal decisions; it might cause the agent to prefer actions that are suboptimal at the present decision junction, but which might cause the opponent to react in accordance with its weakness model (as represented by our agent) which in turn will be beneficial for us in the future.",
                "The agents behavior, as demonstrated in [9] further confirms and strengthens our Suboptimal Tactical Axiom as discussed in the previous section. 556 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 3.2.3 Additional Insights The need for the Goal Achieving, Preventive Act, and Evaluation Maximization axioms are obvious, and need no further verification.",
                "However, even with respect to those axioms, a few interesting insights came up in the log analysis.",
                "The Goal achieving and Preventive Act axioms, though theoretically trivial, seem to provide some challenge to a human player.",
                "In the initial inspection of the logs, we encountered few games2 where a player, for inexplicable reasons, did not block the other from winning or failed to execute its own winning move.",
                "We can blame those faults on the humans lack of attention, or a typing error in its move reply; nevertheless, those errors might occur in bounded rational agents, and the appropriate behavior needs to be axiomatized.",
                "A typical Connect-Four game revolves around generating threats and blocking them.",
                "In our analysis we looked for explicit preventive actions, i.e., moves that block a group of 3 discs, or that remove a future threat (in our limited search horizon).",
                "We found that in 83% of the total games there was at least one preventive action taken by the Black player.",
                "It was also found that Black averaged 2.8 preventive actions per game on the games in which it lost, while averaging 1.5 preventive actions per game when winning.",
                "It seems that Black requires 1 or 2 preventive actions to build its initial taking position, before starting to present threats.",
                "If it did not manage to win, it will usually prevent an extra threat or two before succumbing to White. 4.",
                "RELATED WORK Much research deals with the axiomatization of teamwork and mental states of individuals: some models use knowledge and belief [10], others have models of goals and intentions [8, 4].",
                "However, all these formal theories deal with agent teamwork and cooperation.",
                "As far as we know, our model is the first to provide a formalized model for explicit adversarial environments and agents behavior in it.",
                "The classical Min-Max adversarial search algorithm was the first attempt to integrate the opponent into the search space with a weak assumption of an optimally playing opponent.",
                "Since then, much effort has gone into integrating the opponent model into the decision procedure to predict future behavior.",
                "The M∗ algorithm presented by Carmel and Markovitch [2] showed a method of incorporating opponent models into adversary search, while in [3] they used learning to provide a more accurate opponent model in a 2player repeated game environment, where agents strategies were modeled as finite automata.",
                "Additional Adversarial planning work was done by Willmott et al. [13], which provided an adversarial planning approach to the game of GO.",
                "The research mentioned above dealt with adversarial search and the integration of opponent models into classical utilitybased search methods.",
                "That work shows the importance of opponent modeling and the ability to exploit it to an agents advantage.",
                "However, the basic limitations of those search methods still apply; our model tries to overcome those limitations by presenting a formal model for a new, mental state-based adversarial specification. 5.",
                "CONCLUSIONS We presented an Adversarial Environment model for a 2 These were later removed from the final analysis. bounded rational agent that is situated in an N-player, zerosum environment.",
                "We used the SharedPlans formalization to define the model and the axioms that agents can apply as behavioral guidelines.",
                "The model is meant to be used as a guideline for designing agents that need to operate in such adversarial environments.",
                "We presented empirical results, based on ConnectFour log file analysis, that exemplify the model and the axioms for a bilateral instance of the environment.",
                "The results we presented are a first step towards an expanded model that will cover all types of adversarial environments, for example, environments that are non-zero-sum, and environments that contain natural agents that are not part of the direct conflict.",
                "Those challenges and more will be dealt with in future research. 6.",
                "ACKNOWLEDGMENT This research was supported in part by Israel Science Foundation grants #1211/04 and #898/05. 7.",
                "REFERENCES [1] L. V. Allis.",
                "A knowledge-based approach of Connect-Four - the game is solved: White wins.",
                "Masters thesis, Free University, Amsterdam, The Netherlands, 1988. [2] D. Carmel and S. Markovitch.",
                "Incorporating opponent models into adversary search.",
                "In Proceedings of the Thirteenth National Conference on Artificial Intelligence, pages 120-125, Portland, OR, 1996. [3] D. Carmel and S. Markovitch.",
                "Opponent modeling in multi-agent systems.",
                "In G. Weiß and S. Sen, editors, Adaptation and Learning in Multi-Agent Systems, pages 40-52.",
                "Springer-Verlag, 1996. [4] B. J. Grosz and S. Kraus.",
                "Collaborative plans for complex group action.",
                "Artificial Intelligence, 86(2):269-357, 1996. [5] M. Hadad, G. Kaminka, G. Armon, and S. Kraus.",
                "Supporting collaborative activity.",
                "In Proc. of AAAI-2005, pages 83-88, Pittsburgh, 2005. [6] http://www.gamerz.net/˜pbmserv/. [7] S. Kraus and D. Lehmann.",
                "Designing and building a negotiating automated agent.",
                "Computational Intelligence, 11:132-171, 1995. [8] H. J. Levesque, P. R. Cohen, and J. H. T. Nunes.",
                "On acting together.",
                "In Proc. of AAAI-90, pages 94-99, Boston, MA, 1990. [9] S. Markovitch and R. Reger.",
                "Learning and exploiting relative weaknesses of opponent agents.",
                "Autonomous Agents and Multi-Agent Systems, 10(2):103-130, 2005. [10] Y. M. Ronald Fagin, Joseph Y. Halpern and M. Y. Vardi.",
                "Reasoning about knowledge.",
                "MIT Press, Cambridge, Mass., 1995. [11] P. Thagard.",
                "Adversarial problem solving: Modeling an oponent using explanatory coherence.",
                "Cognitive Science, 16(1):123-149, 1992. [12] R. W. Toseland and R. F. Rivas.",
                "An Introduction to Group Work Practice.",
                "Prentice Hall, Englewood Cliffs, NJ, 2nd edition edition, 1995. [13] S. Willmott, J. Richardson, A. Bundy, and J. Levine.",
                "An adversarial planning approach to Go.",
                "Lecture Notes in Computer Science, 1558:93-112, 1999.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 557"
            ],
            "original_annotated_samples": [
                "Alliance Formation Axiom The following <br>behavioral axiom</br> is relevant only in a multilateral instantiation of the adversarial environment (obviously, an alliance cannot be formed in a bilateral, zero-sum encounter).",
                "We now proceed to analyze the games with respect to each <br>behavioral axiom</br>."
            ],
            "translated_annotated_samples": [
                "Axioma de Formación de Alianzas El siguiente <br>axioma de comportamiento</br> es relevante únicamente en una instancia multilateral del entorno adversarial (obviamente, una alianza no puede formarse en un encuentro bilateral de suma cero).",
                "Ahora procedemos a analizar los juegos con respecto a cada <br>axioma de comportamiento</br>."
            ],
            "translated_text": "Un modelo de entorno adversarial para agentes racionales limitados en interacciones de suma cero Inon Zuckerman1, Sarit Kraus1, Jeffrey S. Rosenschein2, Gal Kaminka1 1 Departamento de Ciencias de la Computación 2 Escuela de Ingeniería Universidad Bar-Ilan y Ciencias de la Computación Ramat-Gan, Israel Universidad Hebrea, Jerusalén, Israel {zukermi, sarit, galk}@cs.biu.ac.il jeff@cs.huji.ac.il RESUMEN Los entornos multiagentes a menudo no son cooperativos ni colaborativos; en muchos casos, los agentes tienen intereses conflictivos, lo que lleva a interacciones adversariales. Este documento presenta un modelo formal de Entorno Adversarial para agentes racionales acotados que operan en un entorno de suma cero. En tales entornos, los intentos de utilizar métodos de búsqueda basados en utilidad clásica pueden plantear una variedad de dificultades (por ejemplo, modelar implícitamente al oponente como un maximizador de utilidad omnisciente, en lugar de aprovechar un modelo de oponente más matizado y explícito). Definimos un Entorno Adversario al describir los estados mentales de un agente en dicho entorno. Luego presentamos axiomas de comportamiento que están destinados a servir como principios de diseño para construir agentes adversarios. Exploramos la aplicación de nuestro enfoque mediante el análisis de archivos de registro de juegos de Conecta Cuatro completados, y presentamos un análisis empírico de la adecuación de los axiomas. Categorías y Descriptores de Asignaturas I.2.11 [Inteligencia Artificial]: Inteligencia Artificial Distribuida-Agentes Inteligentes, Sistemas Multiagente; I.2.4 [Inteligencia Artificial]: Formalismos y Métodos de Representación del Conocimiento - Lógica Modal Términos Generales Diseño, Teoría 1. Introducción: En las primeras investigaciones en sistemas multiagente (SMA) se consideraban grupos cooperativos de agentes; debido a que los agentes individuales tenían recursos limitados o acceso limitado a la información (por ejemplo, potencia de procesamiento limitada, cobertura sensorial limitada), trabajaban juntos por diseño para resolver problemas que individualmente no podían resolver, o al menos no podían resolver de manera eficiente. Sin embargo, la investigación de MAS pronto comenzó a considerar agentes interactivos con intereses individualizados, como representantes de diferentes humanos u organizaciones con intereses no idénticos. Cuando las interacciones son guiadas por intereses diversos, los participantes pueden tener que superar desacuerdos, interacciones no cooperativas e incluso intentos intencionales de dañarse mutuamente. Cuando ocurren este tipo de interacciones, los entornos requieren un comportamiento apropiado por parte de los agentes situados en ellos. Llamamos a estos entornos Entornos Adversarios, y llamamos a los agentes en conflicto Adversarios. Los modelos de cooperación y trabajo en equipo han sido ampliamente explorados en MAS a través de la axiomatización de estados mentales (por ejemplo, [8, 4, 5]). Sin embargo, ninguna de estas investigaciones abordó los dominios adversarios y sus implicaciones para el comportamiento de los agentes. Nuestro artículo aborda este problema al proporcionar un modelo formal y axiomatizado de estados mentales para un subconjunto de dominios adversariales, específicamente entornos adversariales simples de suma cero. Los encuentros de suma cero simples existen, por supuesto, en varios juegos de dos jugadores (por ejemplo, Ajedrez, Damas), pero también existen en juegos de n jugadores (por ejemplo, Risk, Diplomacia), subastas de un solo bien y en otros lugares. En estos entornos recientes en particular, el uso de una búsqueda adversarial basada en utilidad (como el algoritmo Min-Max) no siempre proporciona una solución adecuada; la función de recompensa podría ser bastante compleja o difícil de cuantificar, y existen limitaciones computacionales naturales en agentes racionales acotados. Además, los métodos de búsqueda tradicionales (como Min-Max) no hacen uso de un modelo del oponente, lo cual ha demostrado ser una adición valiosa a la planificación adversarial [9, 3, 11]. En este artículo, desarrollamos un modelo formal y axiomatizado para agentes racionales limitados que se encuentran en un entorno adversarial de suma cero. El modelo utiliza diferentes operadores de modalidad, y sus principales fundamentos son el modelo SharedPlans [4] para el comportamiento colaborativo. Exploramos las propiedades del entorno y los estados mentales de los agentes para derivar axiomas de comportamiento; estos axiomas de comportamiento constituyen un modelo formal que sirve como especificación y guía de diseño para el diseño de agentes en tales entornos. Luego investigamos el comportamiento de nuestro modelo empíricamente utilizando el juego de mesa Conecta Cuatro. Mostramos que este juego se ajusta a nuestra definición de entorno, y analizamos el comportamiento de los jugadores utilizando un gran conjunto de registros de partidas completadas 550 978-81-904262-7-5 (RPS) c 2007 archivos de IFAAMAS. Además, utilizamos los resultados presentados en [9] para discutir la importancia del modelado del oponente en nuestro dominio adversarial de Conecta Cuatro. El artículo continúa de la siguiente manera. La sección 2 presenta la formalización de los modelos. La sección 3 presenta el análisis empírico y sus resultados. Discutimos el trabajo relacionado en la Sección 4, y concluimos y presentamos las direcciones futuras en la Sección 5. ENTORNOS ADVERSARIOS El modelo de entorno adversario (denominado como AE) está destinado a guiar el diseño de agentes al proporcionar una especificación de las capacidades y actitudes mentales de un agente en un entorno adversario. Nos enfocamos aquí en tipos específicos de entornos adversarios, especificados de la siguiente manera: 1. Interacciones de suma cero: las utilidades positivas y negativas de todos los agentes suman cero; 2. Agentes adversarios simples: todos los agentes en el entorno son agentes adversarios; 3. AEs bilaterales: AEs con exactamente dos agentes; 4. AEs multilaterales: AEs de tres o más agentes. Trabajaremos en instanciaciones tanto bilaterales como multilaterales de entornos de suma cero y simples. En particular, nuestro modelo de entorno adversarial tratará con interacciones que consisten en N agentes (N ≥ 2), donde todos los agentes son adversarios y solo un agente puede tener éxito. Ejemplos de tales entornos van desde juegos de mesa (por ejemplo, Ajedrez, Conecta Cuatro y Diplomacia) hasta ciertos entornos económicos (por ejemplo, subastas de N postores sobre un único bien). 2.1 Resumen del Modelo Nuestro enfoque es formalizar las actitudes mentales y comportamientos de un único agente adversario; consideramos cómo un único agente percibe el AE. La siguiente lista especifica las condiciones y estados mentales de un agente en un AE simple de suma cero: 1. El agente tiene la intención individual de que su propio objetivo se complete; 2. El agente tiene la creencia individual de que él y sus adversarios persiguen objetivos completamente opuestos (definidos a continuación) y que solo puede haber un ganador; 3. El agente tiene la creencia individual de que cada adversario tiene la intención de completar su propio objetivo conflictivo completo; 4. El agente tiene una creencia individual en el perfil (parcial) de sus adversarios. El ítem 3 es necesario, ya que podría ser el caso de que algún agente tenga un objetivo completamente conflictivo y esté considerando adoptar la intención de completarlo, pero aún no está comprometido a lograrlo. Esto podría ocurrir porque el agente aún no ha reflexionado sobre los efectos que adoptar esa intención podría tener en las otras intenciones que actualmente mantiene. En tales casos, es posible que ni siquiera se considere en un entorno adversarial. El ítem 4 establece que el agente debe tener alguna creencia sobre los perfiles de sus adversarios. El perfil representa todo el conocimiento que el agente tiene sobre su adversario: sus debilidades, capacidades estratégicas, objetivos, intenciones, confiabilidad y más. Puede ser dado explícitamente o puede ser aprendido a partir de observaciones de encuentros pasados. 2.2 Definiciones de modelos para estados mentales Utilizamos las definiciones de los operadores modales, predicados y metapredicados de Grosz y Krauss, tal como se definen en su formalización de SharedPlan [4]. Recordamos aquí algunos de los predicados y operadores que se utilizan en esa formalización: Int.To(Ai, α, Tn, Tα, C) representa las intenciones de Ai en el tiempo Tn de realizar una acción α en el tiempo Tα en el contexto de C. Int.Th(Ai, prop, Tn, Tprop, C) representa las intenciones de Ai en el tiempo Tn de que una cierta proposición prop se cumpla en el tiempo Tprop en el contexto de C. Los operadores de intención potencial, Pot.Int.To(...) y Pot.Int.Th(...), se utilizan para representar el estado mental cuando un agente considera adoptar una intención, pero no ha deliberado sobre la interacción de las otras intenciones que tiene. El operador Bel(Ai, f, Tf) representa al agente Ai creyendo en la afirmación expresada en la fórmula f, en el tiempo Tf. MB(A, f, Tf) representa la creencia mutua para un grupo de agentes A. Una instantánea del sistema encuentra que nuestro entorno se encuentra en algún estado e ∈ E de estados variables ambientales, y cada adversario en cualquier LAi ∈ L de posibles estados locales. En cualquier paso de tiempo dado, el sistema estará en algún mundo w del conjunto de todos los mundos posibles w ∈ W, donde w = E×LA1 ×LA2 × ...LAn, y n es el número de adversarios. Por ejemplo, en un juego de póker Texas Holdem, el estado local de un agente podría ser su propio conjunto de cartas (que es desconocido para su adversario) mientras que el entorno consistirá en el bote de apuestas y las cartas comunitarias (que son visibles para ambos jugadores). Una función de utilidad bajo esta formalización se define como un mapeo de un mundo posible w ∈ W a un elemento en , que expresa la deseabilidad del mundo, desde la perspectiva de un único agente. Normalmente normalizamos el rango a [0,1], donde 0 representa el mundo menos deseable posible, y 1 es el mundo más deseable. La implementación de la función de utilidad depende del dominio en cuestión. La siguiente lista especifica nuevos predicados, funciones, variables y constantes utilizadas en conjunto con las definiciones originales para la formalización del entorno adversarial: 1. φ es una acción nula (el agente no hace nada). 2. GAi es el conjunto de objetivos del agente Ai. Cada objetivo es un conjunto de predicados cuya satisfacción hace que el objetivo esté completo (usamos G∗ Ai ∈ GAi para representar un objetivo arbitrario del agente Ai). 3. gAi es el conjunto de subobjetivos del agente Ai. Los subobjetivos son predicados cuya satisfacción representa un hito importante hacia el logro del objetivo completo. gG∗ Ai ⊆ gAi es el conjunto de subobjetivos que son importantes para la finalización del objetivo G∗ Ai (usaremos g∗ G∗ Ai ∈ gG∗ Ai para representar un subobjetivo arbitrario). 4. P Aj Ai es el objeto de perfil que el agente Ai tiene sobre el agente Aj. 5. CA es un conjunto general de acciones para todos los agentes en A que se derivan de las restricciones del entorno. CAi ⊆ CA es el conjunto de acciones posibles del agente Ai. La expresión Do(Ai, α, Tα, w) se cumple cuando Ai realiza la acción α durante el intervalo de tiempo Tα en el mundo w. 7. La afirmación Achieve(G∗ Ai , α, w) es verdadera cuando se logra el objetivo G∗ Ai siguiendo la finalización de la acción α en el mundo w ∈ W, donde α ∈ CAi. El perfil (Ai, PAi Ai) es verdadero cuando el agente Ai tiene un perfil de objeto para el agente Aj. Definición 1. El conflicto total (FulConf) describe una interacción de suma cero donde solo se puede completar un solo objetivo de los objetivos en conflicto. FulConf(G∗ Ai , G∗ Aj ) ⇒ (∃α ∈ CAi , ∀w, β ∈ CAj ) (Achieve(G∗ Ai , α, w) ⇒ ¬Achieve(G∗ Aj , β, w)) ∨ (∃β ∈ CAj , ∀w, α ∈ CAi )(Achieve(G∗ Aj , β, w) ⇒ ¬Achieve(G∗ Ai , α, w)) Definición 2. El Conocimiento Adversarial (AdvKnow) es una función que devuelve un valor que representa la cantidad de The Sixth Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 551 agente de conocimiento Ai tiene sobre el perfil del agente Aj, en el tiempo Tn. Cuanto mayor sea el valor, más conocimiento tiene el agente Ai. AdvKnow: P Aj Ai × Tn → Definición 3. Eval: Esta función de evaluación devuelve un valor de utilidad esperada estimado para un agente en A, después de completar una acción de CA en algún estado del mundo w. Eval: A × CA × w → Definición 4. TrH - (Umbral) es una constante numérica en el rango [0,1] que representa un valor umbral de la función de evaluación (Eval). Una acción que produce una evaluación de utilidad estimada por encima del TrH se considera una acción altamente beneficiosa. El valor Eval es una estimación y no la función de utilidad real, la cual suele ser desconocida. Utilizar el valor real de utilidad para un agente racional fácilmente produciría el mejor resultado para ese agente. Sin embargo, los agentes generalmente no tienen las funciones de utilidad reales, sino más bien una estimación heurística de las mismas. Existen dos propiedades importantes que deben cumplir para la función de evaluación: Propiedad 1. La función de evaluación debería indicar que el estado del mundo más deseable es aquel en el que se logra el objetivo. Por lo tanto, una vez que se ha satisfecho el objetivo, no puede haber ninguna acción futura que pueda colocar al agente en un estado del mundo con un valor de Eval más alto. (∀Ai, G∗ Ai , α, β ∈ CAi , w ∈ W) Lograr(G∗ Ai , α, w) ⇒ Eval(Ai, α, w) ≥ Eval(Ai, β, w) Propiedad 2. La función de evaluación debe proyectar una acción que cause la finalización de un objetivo o subobjetivo a un valor que sea mayor que TrH (una acción altamente beneficiosa). (∀Ai, G∗ Ai ∈ GAi , α ∈ CAi , w ∈ W, g∗ GAi ∈ gGAi ) Lograr(G∗ Ai , α, w) ∨ Lograr(g∗ GAi , α, w) ⇒ Eval(Ai, α, w) ≥ TrH. Definición 5. La acción de conjunto Definimos una acción de conjunto (SetAction) como un conjunto de operaciones de acción (ya sean acciones complejas o básicas) de algunos conjuntos de acción CAi y CAj que, según la creencia del agente Ai, están unidas por una relación temporal y consecuente, formando una cadena de eventos (acción y su acción consecuente siguiente). (∀α1 , . . . , αu ∈ CAi , β1 , . . . , βv ∈ CAj , w ∈ W) SetAction(α1 , . . . , αu , β1 , . . . , βv , w) ⇒ ((Do(Ai, α1 , Tα1 , w) ⇒ Do(Aj, β1 , Tβ1 , w)) ⇒ Do(Ai, α2 , Tα2 , w) ⇒ . . . ⇒ Do(Ai, αu , Tαu , w)) La relación consecuente podría existir debido a varias restricciones ambientales (cuando una acción obliga al adversario a responder con una acción específica) o debido al conocimiento de los agentes sobre el perfil de su adversario. Propiedad 3. A medida que aumenta el conocimiento que tenemos sobre nuestro adversario, tendremos creencias adicionales sobre su comportamiento en diferentes situaciones, lo que a su vez crea un nuevo conjunto de acciones. Formalmente, si nuestro AdvKnow en el tiempo Tn+1 es mayor que AdvKnow en el tiempo Tn, entonces cada SetAction conocido en el tiempo Tn también es conocido en el tiempo Tn+1. AdvKnow(P Aj Ai , Tn+1) > AdvKnow(P Aj Ai , Tn) ⇒ (∀α1 , . . . , αu ∈ CAi , β1 , . . . , βv ∈ CAj ) Bel(Aag, SetAction(α1 , . . . , αu , β1 , . . . , βv ), Tn) ⇒ Bel(Aag, SetAction(α1 , . . . , αu , β1 , . . . , βv ), Tn+1) 2.3 La formulación del entorno Los siguientes axiomas proporcionan la definición formal para un Entorno Adversarial (AE) simple y de suma cero. La satisfacción de estos axiomas significa que el agente está situado en un entorno así. Proporciona especificaciones para que el agente Aag interactúe con su conjunto de adversarios A con respecto a los objetivos G∗ Aag y G∗ A en el tiempo TCo en algún estado del mundo w. AE(Aag, A, G∗ Aag, A1, . . . , Ak, G∗ A1, . . . , G∗ Ak, Tn, w) 1. Aag tiene un Int.Th su objetivo se completaría: (∃α ∈ CAag , Tα) Int.Th(Aag, Lograr(G∗ Aag , α), Tn, Tα, AE) 2. Aag cree que él y cada uno de sus adversarios Ao están persiguiendo metas completamente conflictivas: (∀Ao ∈ {A1, . . . , Ak}) Bel(Aag, FulConf(G∗ Aag , G∗ Ao ), Tn) 3. Aag cree que cada uno de sus adversarios en Ao tiene la Int.Th su objetivo en conflicto G∗ Aoi se completará: (∀Ao ∈ {A1, . . . , Ak})(∃β ∈ CAo , Tβ) Bel(Aag, Int.Th(Ao, Lograr(G∗ Ao , β), TCo, Tβ, AE), Tn) 4. Para construir un agente que pueda operar con éxito dentro de un entorno de adversarios estratégicos, debemos especificar pautas de comportamiento para sus interacciones. Utilizar una estrategia de maximización de Eval ingenua hasta cierta profundidad de búsqueda no siempre dará resultados satisfactorios por varias razones: (1) el problema del horizonte de búsqueda al buscar a una profundidad fija; (2) la fuerte suposición de un adversario óptimamente racional con recursos ilimitados; (3) utilizar una función de evaluación estimada que no dará resultados óptimos en todos los estados del mundo, y que puede ser explotada [9]. Los siguientes axiomas especifican los principios de comportamiento que se pueden utilizar para diferenciar entre agentes exitosos y menos exitosos en el entorno adversarial mencionado anteriormente. Esos axiomas deben ser utilizados como principios de especificación al diseñar e implementar agentes que deben ser capaces de desempeñarse bien en entornos adversarios. Los axiomas conductuales representan situaciones en las que el agente adoptará intenciones potenciales para (Pot.Int.To(...)) realizar una acción, lo que típicamente requerirá un razonamiento de medios-fines para seleccionar un posible curso de acción. Este razonamiento llevará a la adopción de un Int.To(...) (ver [4]). A1. Axioma para lograr metas. El primer axioma es el caso más simple; cuando el agente Aag cree que está a una acción (α) de lograr su objetivo en conflicto G∗ Aag, debería adoptar la intención potencial de hacer α y completar su objetivo. (∀Aag, α ∈ CAag, Tn, Tα, w ∈ W) (Bel(Aag, Do(Aag, α, Tα, w) ⇒ Achieve(G∗ Aag, α, w)) ⇒ Pot.Int.To(Aag, α, Tn, Tα, w) Este comportamiento algo trivial es el primer y más fuerte axioma. En cualquier situación, cuando el agente está a una acción de completar el objetivo, debería completar la acción. Cualquier función de evaluación justa clasificaría naturalmente a α como la acción de valor máximo (propiedad 1). Sin embargo, sin una axiomatización explícita de dicho comportamiento, podría haber situaciones en las que el agente decida tomar otra acción por diversas razones, debido a sus recursos de decisión limitados. A2. Axioma del Acto Preventivo. En una situación adversarial, el agente Aag podría decidir tomar acciones que dañen uno de los planes de su adversario para completar su objetivo, incluso si esas acciones no avanzan explícitamente a Aag hacia su objetivo en conflicto G∗ Aag. Tal acción preventiva tendrá lugar cuando el agente Aag tenga la creencia sobre la posibilidad de que su adversario Ao realice una acción β que le otorgará un alto 552 The Sixth Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) valor de evaluación de utilidad (> TrH). Creer que tomar la acción α evitará que el oponente realice su β, adoptará una intención potencial de hacer α. (∀Aag, Ao ∈ A, α ∈ CAag , β ∈ CAo , Tn, Tβ, w ∈ W) (Bel(Aag, Do(Ao, β, Tβ, w) ∧ Eval(Ao, β, w) > TrH, Tn) ∧ Bel(Aag, Do(Aag, α, Tα, w) ⇒ ¬Do(Ao, β, Tβ, w), Tn) ⇒ Pot.Int.To(Aag, α, Tn, Tα, w) Este axioma es un componente básico de cualquier entorno adversarial. Por ejemplo, al observar un juego de ajedrez, un jugador podría darse cuenta de que está a punto de ser jaque mateado por su oponente, por lo tanto, realiza una jugada preventiva. Otro ejemplo es el juego de Cuatro en Línea: cuando un jugador tiene una fila de tres fichas, su oponente debe bloquearla o perder. Una instancia específica de A1 ocurre cuando el adversario está a una acción de lograr su objetivo, y la acción preventiva inmediata debe ser tomada por el agente. Formalmente, tenemos las mismas creencias que se mencionaron anteriormente, con la creencia modificada de que realizar la acción β hará que el agente Ao logre su objetivo. Proposición 1: Caso de prevención o pérdida. (∀Aag, Ao ∈ A, α ∈ CAag , β ∈ CAo , G∗ Ao , Tn, Tα, Tβ, w ∈ W) Bel(Aag, Do(Ao, β, Tβ, w) ⇒ Achieve(G∗ Ao , β, w), Tn) ∧ Bel(Aag, Do(Aag, α, Tα, w) ⇒ ¬Do(Ao, β, Tβ, w)) ⇒ Pot.Int.To(Aag, α, Tn, Tα, w) Esquema de prueba: La Proposición 1 puede derivarse fácilmente del axioma A1 y de la propiedad 2 de la función Eval, que establece que cualquier acción que cause la finalización de un objetivo es una acción altamente beneficiosa. El comportamiento del acto preventivo ocurrirá implícitamente cuando la función Eval sea igual a la función de utilidad del mundo real. Sin embargo, al ser agentes racionales limitados y al tratar con una función de evaluación estimada, necesitamos axiomatizar explícitamente dicho comportamiento, ya que no siempre ocurrirá implícitamente a partir de la función de evaluación. A3. Axioma de Movimiento Táctico Subóptimo. En muchos escenarios, puede ocurrir una situación en la que un agente decida no tomar la acción más beneficiosa actual que puede tomar (la acción con el valor de evaluación de utilidad máximo), porque cree que tomar otra acción (con un valor de evaluación de utilidad más bajo) podría generar (dependiendo de la respuesta del adversario) una posibilidad futura para una acción altamente beneficiosa. Esto ocurrirá con mayor frecuencia cuando la función Eval sea inexacta y difiera en gran medida de la función de Utilidad. De manera formal, el agente Aag cree en una cierta SetAction que evolucionará de acuerdo con su acción inicial y producirá un valor beneficioso alto (> TrH) únicamente para él. (∀Aag, Ao ∈ A, Tn, w ∈ W) (∃α1 , . . . , αu ∈ CAi , β1 , . . . , βv ∈ CAj , Tα1 ) Bel(Aag, SetAction(α1 , . . . , αu , β1 , . . . , βv ), Tn) ∧ Bel(Aag, Eval(Ao, βv , w) < TrH < Eval(Aag, αu , w), Tn) ⇒ Pot.Int.To(Aag, α1 , Tn, Tα1 , w) Un agente podría creer que una cadena de eventos ocurrirá por diversas razones debido a la naturaleza inevitable del dominio. Por ejemplo, en el ajedrez, a menudo observamos lo siguiente: una jugada provoca una posición de jaque, lo que a su vez limita los movimientos del oponente para evitar el jaque, a lo que el primer jugador podría reaccionar con otro jaque, y así sucesivamente. El agente también podría creer en una cadena de eventos basada en su conocimiento del perfil de su adversario, lo que le permite prever los movimientos del adversario con alta precisión. A4. Axioma de Detección de Perfiles. El agente puede ajustar los perfiles de sus adversarios mediante observaciones y estudio de patrones (específicamente, si hay encuentros repetidos con el mismo adversario). Sin embargo, en lugar de esperar a que se revele la información del perfil, un agente también puede iniciar acciones que obligarán a su adversario a reaccionar de una manera que revele conocimiento del perfil sobre él. Formalmente, el axioma establece que si todas las acciones (γ) no son acciones altamente beneficiosas (< TrH), el agente puede realizar la acción α en el tiempo Tα si cree que resultará en una acción no altamente beneficiosa β por parte de su adversario, lo que a su vez le enseña sobre el perfil del adversario, es decir, proporciona un mayor AdvKnow(P Aj Ai , Tβ). (∀Aag, Ao ∈ A, α ∈ CAag , β ∈ CAo , Tn, Tα, Tβ, w ∈ W) Bel(Aag, (∀γ ∈ CAag )Eval(Aag, γ, w) < TrH, Tn) ∧ Bel(Aag, Do(Aag, α, Tα, w) ⇒ Do(Ao, β, Tβ, w), Tn) ∧ Bel(Aag, Eval(Ao, β, w) < TrH) ∧ Bel(Aag, AdvKnow(P Aj Ai , Tβ) > AdvKnow(P Aj Ai , Tn), Tn) ⇒ Pot.Int.To(Aag, α, Tn, Tα, w) Por ejemplo, volviendo al escenario del juego de ajedrez, consideremos comenzar una partida contra un oponente del que no sabemos nada, ni siquiera si es humano o un oponente computarizado. Podríamos comenzar a jugar una estrategia que sea adecuada frente a un oponente promedio, y ajustar nuestro juego de acuerdo a su nivel de juego. A5. Axioma de Formación de Alianzas El siguiente <br>axioma de comportamiento</br> es relevante únicamente en una instancia multilateral del entorno adversarial (obviamente, una alianza no puede formarse en un encuentro bilateral de suma cero). En diferentes situaciones durante una interacción multilateral, un grupo de agentes podría creer que es de su interés formar una alianza temporal. Una alianza de este tipo es un acuerdo que limita el comportamiento de sus miembros, pero se cree por parte de estos que les permite alcanzar un valor de utilidad mayor al que podrían lograr fuera de la alianza. Como ejemplo, podemos observar el clásico juego de mesa Risk, donde cada jugador tiene como objetivo individual ser el único conquistador del mundo, un juego de suma cero. Sin embargo, para lograr este objetivo, podría ser estratégicamente sabio hacer acuerdos de alto el fuego a corto plazo con otros jugadores, o unirse y atacar a un oponente que sea más fuerte que el resto. Los términos de una alianza definen la forma en que sus miembros deben actuar. Es un conjunto de predicados, denominados Términos, que es acordado por los miembros de la alianza y debe mantenerse verdadero durante la duración de la alianza. Por ejemplo, el conjunto de términos en el escenario de riesgo podría contener los siguientes predicados: 1. Los miembros de la alianza no se atacarán entre sí en los territorios X, Y y Z; 2. Los miembros de la alianza contribuirán C unidades por turno para atacar al adversario Ao; 3. Los miembros están obligados a permanecer como parte de la alianza hasta el tiempo Tk o hasta que el ejército adversario Ao sea más pequeño que Q. El conjunto de términos especifica restricciones intergrupales en cada uno de los miembros de la alianza (∀Aal i ∈ Aal ⊆ A) conjunto de acciones Cal i ⊆ C. Definición 6. El valor total de evaluación que el agente Ai logrará al ser parte de Aal es la suma de Evali (Eval para Ai) de cada uno de los valores de evaluación de Aal j después de tomar sus propias acciones α (a través del predicado del agente(α)): Al val(Ai, Cal, Aal, w) = α∈Cal Evali(Aal j, agente(α), w) Definición 7. Al TrH - es un número que representa un valor de Al en la Sexta Conferencia Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 553 umbral; por encima de él, se puede decir que la alianza es una alianza altamente beneficiosa. El valor de Al TrH se calculará dinámicamente de acuerdo con el progreso de la interacción, como se puede ver en [7]. Después de formarse una alianza, sus miembros ahora están trabajando en su entorno adversarial normal, así como de acuerdo con los estados mentales y axiomas requeridos para sus interacciones como parte de la alianza. El siguiente modelo de Alianza (AL) especifica las condiciones bajo las cuales el grupo Aal puede considerarse en una alianza y trabajando con un nuevo y limitado conjunto de acciones Cal, en el tiempo Tn. AL(Aal , Cal , w, Tn) 1. \n\nAL(Aal , Cal , w, Tn) 1. Aal tiene un MB en el que todos los miembros son parte de Aal: MB(Aal, (∀Aal i ∈ Aal)miembro(Aal i, Aal), Tn) 2. Aal tiene una MB que el grupo se mantenga: MB(Aal , (∀Aal i ∈ Aal )Int.Th (Ai, miembro(Ai, Aal ), Tn, Tn+1, Co), Tn) 3. Aal tiene un beneficio marginal que al ser miembros les otorga un alto valor de utilidad: MB(Aal , (∀Aal i ∈ Aal )Al val(Aal i , Cal , Aal , w) ≥ Al TrH, Tn) Los perfiles de los miembros son una parte crucial de las alianzas exitosas. Suponemos que los agentes que tienen perfiles más precisos de sus adversarios serán más exitosos en tales entornos. Tales agentes podrán predecir cuándo un miembro está a punto de violar el contrato de alianzas (elemento 2 en el modelo anterior) y tomar medidas correctivas (cuando el elemento 3 se falsificará). La robustez de la alianza depende en parte de la medida de confianza de sus miembros, la estimación de su posición objetiva y otras propiedades de su perfil. Debemos tener en cuenta que un agente puede formar parte simultáneamente de más de una alianza. Una alianza temporal, donde los miembros del grupo no tienen un objetivo común pero actúan colaborativamente en interés de sus propias metas individuales, es clasificada como un Grupo de Tratamiento por los psicólogos modernos (en contraste con un Grupo de Tarea, donde sus miembros tienen un objetivo común). El modelo de Actividad Compartida presentado en [5] modeló el comportamiento del Grupo de Tratamiento utilizando la misma formalización de Planes Compartidos. Al comparar ambas definiciones de una alianza y un Grupo de Tratamiento, encontramos una similitud sorprendente entre ambos modelos: las definiciones de los modelos de entorno son casi idénticas (ver las definiciones de SAs en [5]), y sus axiomas de Acto Egoísta y Acto Cooperativo se ajustan al comportamiento de nuestros agentes adversarios. La principal distinción entre ambos modelos es la integración de un axioma de acto de comportamiento útil, en la Actividad Compartida que no puede formar parte de la nuestra. Este axioma establece que un agente considerará tomar una acción que disminuirá su valor de Evaluación (hasta un cierto límite inferior), si cree que un compañero del grupo obtendrá un beneficio significativo. Dicho comportamiento no puede ocurrir en un entorno puramente adversarial (como un juego de suma cero), donde los miembros de la alianza están constantemente alerta para manipular su alianza en su propio beneficio. A6. Axioma de Maximización de la Evaluación. En un caso en el que todos los demás axiomas no sean aplicables, procederemos con la acción que maximice el valor heurístico calculado en la función Eval. (∀Aag, Ao ∈ A, α ∈ Cag, Tn, w ∈ W) Bel(Aag, (∀γ ∈ Cag)Eval(Aag, α, w) ≥ Eval(Aag, γ, w), Tn) ⇒ Pot.Int.To(Aag, α, Tn, Tα, w) T1. Optimalidad en Eval = Utilidad El modelo axiomático anterior aborda situaciones en las que la Utilidad es desconocida y los agentes son agentes racionales limitados. El siguiente teorema muestra que en interacciones bilaterales, donde los agentes tienen la función de Utilidad real (es decir, Eval = Utilidad) y son agentes racionales, los axiomas proporcionan el mismo resultado óptimo que la búsqueda adversarial clásica (por ejemplo, Min-Max). Teorema 1. Que Ae ag sea un agente AE racional ilimitado que utiliza la función de evaluación heurística Eval, Au ag sea el mismo agente utilizando la verdadera función de utilidad y Ao sea un único adversario racional ilimitado basado en utilidad. Dado que Eval = Utilidad: (∀α ∈ CAu ag , α ∈ CAe ag , Tn, w ∈ W) Pot.Int.To(Au ag, α, Tn, Tα, w) → Pot.Int.To(Ae ag, α , Tn, Tα, w) ∧ ((α = α ) ∨ (Utilidad(Au ag, α, w) = Eval(Ae ag, α , w))) Esquema de prueba - Dado que Au ag tiene la función de utilidad real y recursos ilimitados, puede generar el árbol de juego completo y ejecutar el algoritmo MinMax óptimo para elegir la acción de mayor valor de utilidad, que denotamos como α. La prueba mostrará que Ae ag, utilizando los axiomas AE, seleccionará la misma utilidad α o igual (cuando hay más de una acción con la misma utilidad máxima) cuando Eval = Utilidad. (A1) Axioma de logro de objetivo - supongamos que hay un α tal que su cumplimiento logrará el objetivo Au ags. Obtendrá la utilidad más alta mediante Min-Max para Au ag. El agente Ae seleccionará α u otra acción con el mismo valor de utilidad a través de A1. Si tal α no existe, Ae ag no puede aplicar este axioma y procede a A2. (A2) Axioma de la acción preventiva - (1) Observando el caso base (ver Prop1), si hay un β que lleva a Ao a alcanzar su objetivo, entonces una acción preventiva α producirá la mayor utilidad para Au ag. Au ag lo elegirá a través de la utilidad, mientras que Ae ag lo elegirá a través de A2. En el caso general, β es una acción altamente beneficiosa para Ao, lo que resulta en una baja utilidad para Au ag, lo que lo guiará a seleccionar un α que evitará β, mientras que Ae ag lo elegirá a través de A2.1 Si dicho β no existe para Ao, entonces A2 no es aplicable, y Ae ag puede proceder a A3. (A3) Axioma de movimiento táctico subóptimo: al usar una función de evaluación heurística, Ae ag tiene una creencia parcial en el perfil de su adversario (elemento 4 en el modelo AE), lo que puede llevarlo a creer en SetActions (Prop1). En nuestro caso, Ae ag tiene un perfil completo de su adversario óptimo y sabe que Ao se comportará de manera óptima según los valores reales de utilidad en el árbol de búsqueda completo, por lo tanto, cualquier creencia sobre una SetAction subóptima no puede existir, lo que hace que este axioma sea inaplicable. Ae ag procederá a A4. (A4) Axioma de detección de perfil - Dado que Ae ag tiene el perfil completo de Ao, ninguna de las acciones de Ae ag puede aumentar su conocimiento. Ese axioma no se aplicará, y el agente procederá con A6 (A5 será ignorado porque la interacción es bilateral). (A6) Axioma de maximización de la evaluación: este axioma seleccionará la máxima Evaluación para Ae ag. Dado que Eval = Utilidad, se seleccionará el mismo α que fue seleccionado por Au ag. 3. EVALUACIÓN El propósito principal de nuestro análisis experimental es evaluar el comportamiento y rendimiento de los modelos en un entorno adversarial real. Esta sección investiga si un caso acotado 1 en el que, tras la finalización de β, exista un γ que proporcione una alta utilidad para el Agente Au ag, no puede ocurrir porque Ao utiliza la misma utilidad, y la existencia de γ hará que clasifique a β como una acción de baja utilidad. 554 The Sixth Intl. En la Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07), los agentes racionales situados en entornos adversariales se beneficiarán más al aplicar nuestros axiomas de comportamiento sugeridos. 3.1 El Dominio Para explorar el uso del modelo anterior y sus axiomas de comportamiento, decidimos utilizar el juego de Conecta Cuatro como nuestro entorno adversarial. Connect-Four es un juego de 2 jugadores, de suma cero, que se juega utilizando un tablero similar a una matriz de 6x7. En cada turno, un jugador coloca un disco en una de las 7 columnas (el conjunto de 21 discos suele ser de color amarillo para el jugador 1 y rojo para el jugador 2; usaremos Blanco y Negro respectivamente para evitar confusiones). El ganador es el primer jugador en completar un conjunto de cuatro discos de su color en horizontal, vertical o diagonal. En ocasiones muy raras, el juego podría terminar en empate si todos los cuadros vacíos están llenos, pero ningún jugador logró crear un conjunto de 4 discos. El juego de Conecta Cuatro fue resuelto en [1], donde se muestra que el primer jugador (jugando con los discos blancos) puede forzar una victoria comenzando en la columna del medio (columna 4) y jugando de manera óptima. Sin embargo, la estrategia óptima es muy compleja y difícil de seguir incluso para agentes racionales limitados complejos, como los jugadores humanos. Antes de que podamos proceder a verificar el comportamiento del agente, primero debemos verificar que el dominio se ajusta a la definición de entornos adversarios mencionada anteriormente (en la que se basan los axiomas de comportamiento). Primero, al jugar al juego de Conecta Cuatro, el agente tiene la intención de ganar el juego (ítem 1). Segundo (ítem 2), nuestro agente cree que en Conecta Cuatro solo puede haber un ganador (o ningún ganador en la rara ocasión de un empate). Además, nuestro agente cree que su oponente en el juego intentará ganar (ítem 3), y esperamos que tenga algún conocimiento parcial (ítem 4) sobre su adversario (este conocimiento puede variar desde nada, pasando por hechos simples como la edad, hasta estrategias y debilidades). Por supuesto, no todos los encuentros de Conecta Cuatro son adversariales. Por ejemplo, cuando un padre está jugando el juego con su hijo, la siguiente situación podría ocurrir: el niño, con un fuerte incentivo para ganar, trata el entorno como adversarial (intenta ganar, entiende que solo puede haber un ganador y cree que su padre está tratando de vencerlo). Sin embargo, desde el punto de vista de los padres, el entorno podría ser visto como educativo, donde el objetivo no es ganar el juego, sino causar disfrute o practicar el razonamiento estratégico. En un entorno educativo como este, un nuevo conjunto de axiomas de comportamiento podría ser más beneficioso para los objetivos de los padres que nuestros axiomas de comportamiento adversarial sugeridos. 3.2 Análisis de Axiomas Después de demostrar que el juego de Conecta Cuatro es de hecho un entorno adversarial bilateral de suma cero, el siguiente paso es observar el comportamiento de los jugadores durante el juego y verificar si comportarse de acuerdo con nuestro modelo mejora el rendimiento. Para hacerlo, hemos recopilado archivos de registro de juegos de Conecta Cuatro completados que fueron jugados por jugadores humanos a través de Internet. Nuestros datos de archivo de registro recopilados provienen de sitios de Juego por Correo Electrónico (PBeM). Estos son sitios web que alojan juegos por correo electrónico, donde cada movimiento se realiza a través de un intercambio de correos electrónicos entre el servidor y los jugadores. Muchos de estos sitios de archivos contienen interacciones competitivas reales y también mantienen un sistema de clasificación para sus miembros. La mayoría de los datos que utilizamos se pueden encontrar en [6]. Como se puede aprender de [1], Conecta Cuatro tiene una estrategia óptima y una ventaja considerable para el jugador que comienza el juego (a quien llamamos jugador Blanco). Nos concentraremos en nuestro análisis en los movimientos de los segundos jugadores (a los que llamaremos Negras). El jugador Blanco, al ser el primero en actuar, tiene la llamada ventaja de la iniciativa. Tener la ventaja y una buena estrategia mantendrá al jugador Negro ocupado reaccionando a sus movimientos, en lugar de iniciar amenazas. Una amenaza es una combinación de tres discos del mismo color, con un espacio vacío para el cuarto disco ganador. Una amenaza abierta es una amenaza que puede materializarse en el próximo movimiento del oponente. Para que el jugador Negro gane, debe de alguna manera cambiar el rumbo, tomar la ventaja y comenzar a presentar amenazas al jugador Blanco. Exploraremos el comportamiento de los jugadores negros y su conformidad con nuestros axiomas. Para hacerlo, construimos una aplicación que lee archivos de registro y analiza los movimientos de los jugadores Negros. La aplicación contiene dos componentes principales: (1) un algoritmo Min-Max para la evaluación de movimientos; (2) un detector de amenazas abiertas para descubrir amenazas abiertas. El algoritmo Min-Max funcionará hasta una profundidad d dada y, para cada movimiento α, devolverá el valor heurístico para la próxima acción tomada por el jugador según lo escrito en el archivo de registro, h(α), junto con el valor heurístico máximo, maxh(α), que podría lograrse antes de realizar el movimiento (obviamente, si h(α) = maxh(α), entonces el jugador no realizó el movimiento óptimo desde el punto de vista heurístico). El trabajo de los detectores de amenazas es notificar si se tomó alguna acción para bloquear una amenaza abierta (no bloquear una amenaza abierta probablemente hará que el jugador pierda en el próximo movimiento del oponente). La función heurística utilizada por Min-Max para evaluar la utilidad de los jugadores es la siguiente función, que es simple de calcular, pero proporciona un desafío razonable a los oponentes humanos: Definición 8. Sea Grupo un conjunto adyacente de cuatro cuadrados que son horizontales, verticales o diagonales. El Grupo n (Grupo w) es un Grupo con n piezas de color negro (blanco) y 4−n casillas vacías. h = ((Grupo1 negro ∗α)+(Grupo2 negro ∗β)+(Grupo3 negro ∗γ)+(Grupo4 negro ∗∞)) − ((Grupo1 blanco ∗α)+(Grupo2 blanco ∗β)+(Grupo3 blanco ∗γ)+(Grupo4 blanco ∗∞)) Los valores de α, β y δ pueden variar para formar cualquier combinación lineal deseada; sin embargo, es importante asignarles valores teniendo en cuenta el orden α < β < δ (usamos 1, 4 y 8 como sus valores respectivos). Grupos de 4 discos del mismo color significan victoria, por lo tanto, el descubrimiento de dicho grupo resultará en ∞ para asegurar un valor extremo. Ahora utilizamos nuestra función de evaluación estimada para evaluar las acciones de los jugadores Negros durante la interacción adversarial de Conecta Cuatro. Cada juego del archivo de registro fue introducido en la aplicación, la cual procesó y generó un archivo de registro reformateado que contenía el valor h del movimiento actual, el valor maxh que podría ser alcanzado y una notificación si se detectaba una amenaza abierta. Un total de 123 partidas fueron analizadas (57 con victoria de las blancas y 66 con victoria de las negras). Algunos juegos adicionales fueron ignorados manualmente en el experimento, debido a estos problemas: un jugador abandonando el juego mientras el resultado no es definitivo, o una jugada irracional y contundente en las primeras etapas del juego (por ejemplo, no bloquear un grupo obviamente ganador en las primeras jugadas de apertura). Además, también se eliminó un empate. El simulador se ejecutó con una profundidad de búsqueda de 3 movimientos. Ahora procedemos a analizar los juegos con respecto a cada <br>axioma de comportamiento</br>. El Sexto Internacional. La siguiente sección presenta las evaluaciones heurísticas del algoritmo Min-Max para cada acción, y verifica la cantidad y el alcance de las acciones tácticas subóptimas y sus implicaciones en el rendimiento. La Tabla 1 muestra los resultados y las percepciones del análisis heurístico de los juegos, cuando la profundidad de búsqueda es igual a 3 (esta profundidad de búsqueda fue seleccionada para que los resultados sean comparables a [9], ver Sección 3.2.3). La información heurística de las tablas es la diferencia entre el valor heurístico máximo presente y el valor heurístico de la acción que finalmente tomó el jugador (es decir, cuanto más cercano sea el número a 0, más cercana estuvo la acción a la acción heurística máxima). La primera fila presenta los valores de diferencia de la acción que tuvo el valor de diferencia máximo entre todas las acciones de los jugadores Negros en un juego dado, promediado en todos los juegos ganados y perdidos por los Negros (ver columnas respectivas). En los juegos en los que el jugador Negro pierde, su valor promedio de diferencia fue de -17.62, mientras que en los juegos en los que el jugador Negro ganó, su promedio fue de -12.02. La segunda fila amplía el análisis al considerar las 3 acciones con mayor diferencia heurística, y promediarlas. En ese caso, notamos una diferencia heurística promedio de 5 puntos entre los juegos en los que el jugador Negro pierde y los juegos en los que gana. Sin embargo, la importancia de esos números es que nos permitieron hacer una suposición educada sobre un número umbral de 11.5, como el valor de la constante TrH, que diferencia entre acciones normales y altamente beneficiosas. Después de encontrar una constante TrH aproximada, podemos proceder con un análisis de la importancia de los movimientos subóptimos. Para hacerlo, tomamos el subconjunto de juegos en los que el valor mínimo de diferencia heurística para las acciones de las negras era de 11.5. Como se presenta en la Tabla 2, podemos ver el promedio de 3 horas mínimo de los 3 mayores rangos y el porcentaje respectivo de juegos ganados. La primera fila muestra que el jugador Negro ganó solo el 12% de los juegos en los que el promedio de sus 3 acciones con mayor diferencia heurística (min3 h) era menor que el umbral sugerido, TrH = 11.5. La segunda fila muestra un resultado sorprendente: parece que cuando min3 h > −4, el jugador Negro rara vez gana. La intuición sugeriría que los juegos en los que los valores de evaluación de la acción estén más cerca de los valores máximos resultarán en más juegos ganados para las piezas negras. Sin embargo, parece que en el dominio de Conecta Cuatro, simplemente responder con acciones algo fácilmente esperadas, sin iniciar algunos movimientos sorprendentes y subóptimos, no produce buenos resultados. La última fila resume las principales conclusiones del análisis; la mayoría de las victorias de Blacks (83%) se produjeron cuando su min3 h estaba en el rango de -11.5 a -4. Una inspección detallada de esos juegos ganados por las fichas negras muestra el siguiente patrón detrás de los números: después de los movimientos de apertura estándar, las fichas negras de repente colocan una ficha en una columna aislada, lo cual parece ser un desperdicio de un movimiento. El blanco sigue construyendo sus amenazas, generalmente ignorando el último movimiento de las negras, que a su vez utiliza el disco aislado como ancla para una futura amenaza ganadora. Los resultados muestran que fue beneficioso para el jugador Negro en la Tabla 2: Porcentajes de ganancias de los Negros % de juegos min3 h < −11.5 12% min3 h > −4 5% −11.5 ≤ min3 h ≤ −4 83% tomar acciones subóptimas y no dar el valor heurístico más alto posible en ese momento, pero no será demasiado perjudicial para su posición (es decir, no dará un alto valor beneficioso a su adversario). Resultó que aprender el umbral es un aspecto importante del éxito: realizar movimientos extremadamente arriesgados (min3 h < −11.5) o intentar evitarlos (min3 h > −4) reduce significativamente las posibilidades de ganar de los jugadores negros. 3.2.2 Afirmación del Axioma de Monitoreo de Perfiles En la tarea de demostrar la importancia de monitorear los perfiles de los adversarios, nuestros archivos de registro no pudieron ser utilizados porque no contenían interacciones repetidas entre jugadores, que son necesarias para inferir el conocimiento de los jugadores sobre sus adversarios. Sin embargo, la importancia del modelado del oponente y su uso para obtener ventajas tácticas ya ha sido estudiada en varios ámbitos ([3, 9] son buenos ejemplos). En un artículo reciente, Markovitch y Reger [9] exploraron la noción de aprendizaje y explotación de la debilidad del oponente en interacciones competitivas. Aplican estrategias de aprendizaje simples analizando ejemplos de interacciones pasadas en un dominio específico. También utilizaron el dominio adversarial de Conecta Cuatro, que ahora se puede utilizar para comprender la importancia de monitorear el perfil del adversario. Tras la presentación de su modelo teórico, describen un extenso estudio empírico y verifican el rendimiento de los agentes después de aprender el modelo de debilidades con ejemplos pasados. Uno de los dominios utilizados como entorno competitivo fue el mismo juego de Conecta Cuatro (las Damas fueron el segundo dominio). Su función heurística era idéntica a la nuestra con tres variaciones diferentes (H1, H2 y H3) que se distinguen entre sí por los valores de sus coeficientes de combinación lineal. La profundidad de búsqueda para los jugadores fue de 3 (como en nuestro análisis). Sus extensos experimentos verifican y comparan diversas estrategias de aprendizaje, factores de riesgo, conjuntos de características predefinidas y métodos de uso. La conclusión es que el dominio de Conecta Cuatro muestra una mejora de una tasa de victoria de 0.556 antes de modelar a 0.69 después de modelar (página 22). Sus conclusiones, que muestran un rendimiento mejorado al sostener y utilizar el modelo del adversario, justifican el esfuerzo de monitorear el perfil del adversario para interacciones continuas y repetidas. Un punto adicional que surgió en sus experimentos es el siguiente: después de que se haya aprendido el modelo de debilidad del oponente, los autores describen diferentes métodos para integrar el modelo de debilidad del oponente en la estrategia de decisión de los agentes. Sin embargo, independientemente del método específico que elijan para trabajar, todos los métodos de integración podrían hacer que el agente tome decisiones subóptimas; podría hacer que el agente prefiera acciones que son subóptimas en el momento actual de la decisión, pero que podrían hacer que el oponente reaccione de acuerdo con su modelo de debilidad (como lo representa nuestro agente), lo cual a su vez será beneficioso para nosotros en el futuro. El comportamiento de los agentes, como se demuestra en [9], confirma y refuerza aún más nuestro Axioma Táctico Subóptimo discutido en la sección anterior. 556 The Sixth Intl. La necesidad de los axiomas de Logro de Objetivos, Acto Preventivo y Maximización de la Evaluación es evidente y no requiere más verificación. Sin embargo, incluso con respecto a esos axiomas, surgieron algunas ideas interesantes en el análisis de registros. Los axiomas de Logro y Prevención de Actos, aunque teóricamente triviales, parecen presentar un desafío para un jugador humano. En la inspección inicial de los registros, nos encontramos con algunos juegos donde un jugador, por razones inexplicables, no bloqueó al otro para que ganara o no logró ejecutar su propia jugada ganadora. Podemos atribuir esas fallas a la falta de atención de los humanos, o a un error de escritura en su respuesta al movimiento; sin embargo, esos errores podrían ocurrir en agentes racionales limitados, y el comportamiento apropiado debe ser axiomatizado. Un juego típico de Cuatro en Línea gira en torno a generar amenazas y bloquearlas. En nuestro análisis buscamos acciones preventivas explícitas, es decir, movimientos que bloqueen un grupo de 3 discos o que eliminen una amenaza futura (en nuestro horizonte de búsqueda limitado). Encontramos que en el 83% de los juegos totales, hubo al menos una acción preventiva tomada por el jugador Negro. También se encontró que Black promedió 2.8 acciones preventivas por juego en los juegos en los que perdió, mientras que promedió 1.5 acciones preventivas por juego al ganar. Parece que Black necesita 1 o 2 acciones preventivas para construir su posición inicial de toma, antes de comenzar a presentar amenazas. Si no logra ganar, generalmente evitará una amenaza adicional o dos antes de sucumbir ante el Blanco. 4. TRABAJO RELACIONADO Mucha investigación se ocupa de la axiomatización del trabajo en equipo y los estados mentales de los individuos: algunos modelos utilizan el conocimiento y la creencia [10], otros tienen modelos de metas e intenciones [8, 4]. Sin embargo, todas estas teorías formales tratan sobre el trabajo en equipo y la cooperación de agentes. Hasta donde sabemos, nuestro modelo es el primero en proporcionar un modelo formalizado para entornos adversarios explícitos y el comportamiento de los agentes en él. El algoritmo de búsqueda adversarial clásico Min-Max fue el primer intento de integrar al oponente en el espacio de búsqueda con la suposición débil de un oponente que juega de manera óptima. Desde entonces, se ha dedicado mucho esfuerzo a integrar el modelo del oponente en el procedimiento de toma de decisiones para predecir el comportamiento futuro. El algoritmo M∗ presentado por Carmel y Markovitch [2] mostró un método para incorporar modelos de oponentes en la búsqueda de adversarios, mientras que en [3] utilizaron el aprendizaje para proporcionar un modelo de oponente más preciso en un entorno de juego repetido de 2 jugadores, donde las estrategias de los agentes se modelaron como autómatas finitos. El trabajo adicional de planificación adversarial fue realizado por Willmott y colaboradores [13], quienes proporcionaron un enfoque de planificación adversarial para el juego de GO. La investigación mencionada anteriormente trató sobre la búsqueda adversarial y la integración de modelos de oponentes en métodos de búsqueda de utilidad clásicos. Ese trabajo muestra la importancia del modelado del oponente y la capacidad de explotarlo en beneficio de un agente. Sin embargo, las limitaciones básicas de esos métodos de búsqueda siguen aplicándose; nuestro modelo intenta superar esas limitaciones presentando un modelo formal para una nueva especificación adversarial basada en estados mentales. 5. CONCLUSIONES Presentamos un modelo de Entorno Adversarial para un agente racional acotado que está situado en un entorno de N jugadores, de suma cero. Estos fueron posteriormente eliminados del análisis final. Utilizamos la formalización de SharedPlans para definir el modelo y los axiomas que los agentes pueden aplicar como pautas de comportamiento. El modelo está destinado a ser utilizado como una guía para diseñar agentes que necesitan operar en entornos adversos. Presentamos resultados empíricos, basados en el análisis del archivo de registro de ConnectFour, que ejemplifican el modelo y los axiomas para una instancia bilateral del entorno. Los resultados que presentamos son un primer paso hacia un modelo ampliado que abarcará todos los tipos de entornos adversarios, por ejemplo, entornos que no son de suma cero y entornos que contienen agentes naturales que no forman parte del conflicto directo. Esos desafíos y más serán abordados en futuras investigaciones. 6. AGRADECIMIENTO Esta investigación fue apoyada en parte por las becas de la Fundación de Ciencia de Israel #1211/04 y #898/05. 7. REFERENCIAS [1] L. V. Allis. Un enfoque basado en el conocimiento de Conecta Cuatro: el juego está resuelto, gana el jugador Blanco. Tesis de maestría, Universidad Libre de Ámsterdam, Países Bajos, 1988. [2] D. Carmel y S. Markovitch. Incorporando modelos de oponentes en la búsqueda de adversarios. En Actas de la Decimotercera Conferencia Nacional de Inteligencia Artificial, páginas 120-125, Portland, Oregón, 1996. [3] D. Carmel y S. Markovitch. Modelado de oponentes en sistemas multiagente. En G. Weiß y S. Sen, editores, Adaptación y Aprendizaje en Sistemas Multiagente, páginas 40-52. Springer-Verlag, 1996. [4] B. J. Grosz y S. Kraus. Planes colaborativos para acciones grupales complejas. Inteligencia Artificial, 86(2):269-357, 1996. [5] M. Hadad, G. Kaminka, G. Armon y S. Kraus. Apoyando la actividad colaborativa. En Proc. de AAAI-2005, páginas 83-88, Pittsburgh, 2005. [6] http://www.gamerz.net/˜pbmserv/. [7] S. Kraus y D. Lehmann. Diseñando y construyendo un agente automatizado de negociación. Inteligencia Computacional, 11:132-171, 1995. [8] H. J. Levesque, P. R. Cohen y J. H. T. Nunes. Actuar juntos. En Proc. de AAAI-90, páginas 94-99, Boston, MA, 1990. [9] S. Markovitch y R. Reger. Aprender y aprovechar las debilidades relativas de los agentes oponentes. Agentes Autónomos y Sistemas Multiagente, 10(2):103-130, 2005. [10] Y. M. Ronald Fagin, Joseph Y. Halpern y M. Y. Vardi. Razonamiento sobre el conocimiento. MIT Press, Cambridge, Mass., 1995. [11] P. Thagard.\nMIT Press, Cambridge, Massachusetts, 1995. [11] P. Thagard. Resolución de problemas adversariales: Modelado de un oponente utilizando coherencia explicativa. Ciencia Cognitiva, 16(1):123-149, 1992. [12] R. W. Toseland y R. F. Rivas. Una introducción a la práctica del trabajo en grupo. Prentice Hall, Englewood Cliffs, NJ, segunda edición, 1995. [13] S. Willmott, J. Richardson, A. Bundy y J. Levine. Un enfoque de planificación adversarial para el juego de Go. Notas de conferencia en Ciencias de la Computación, 1558:93-112, 1999. El Sexto Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 557 ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "bilateral and multilateral instantiation": {
            "translated_key": "instanciaciones tanto bilaterales como multilaterales",
            "is_in_text": true,
            "original_annotated_sentences": [
                "An Adversarial Environment Model for Bounded Rational Agents in Zero-Sum Interactions Inon Zuckerman1 , Sarit Kraus1 , Jeffrey S. Rosenschein2 , Gal Kaminka1 1 Department of Computer Science 2 The School of Engineering Bar-Ilan University and Computer Science Ramat-Gan, Israel Hebrew University, Jerusalem, Israel {zukermi,sarit,galk}@cs.biu.ac.il jeff@cs.huji.ac.il ABSTRACT Multiagent environments are often not cooperative nor collaborative; in many cases, agents have conflicting interests, leading to adversarial interactions.",
                "This paper presents a formal Adversarial Environment model for bounded rational agents operating in a zero-sum environment.",
                "In such environments, attempts to use classical utility-based search methods can raise a variety of difficulties (e.g., implicitly modeling the opponent as an omniscient utility maximizer, rather than leveraging a more nuanced, explicit opponent model).",
                "We define an Adversarial Environment by describing the mental states of an agent in such an environment.",
                "We then present behavioral axioms that are intended to serve as design principles for building such adversarial agents.",
                "We explore the application of our approach by analyzing log files of completed Connect-Four games, and present an empirical analysis of the axioms appropriateness.",
                "Categories and Subject Descriptors I.2.11 [Artificial Intelligence]: Distributed Artificial Intelligence-Intelligent agents,Multiagent Systems; I.2.4 [Artificial Intelligence]: Knowledge Representation Formalisms and Methods -Modal logic General Terms Design, Theory 1.",
                "INTRODUCTION Early research in multiagent systems (MAS) considered cooperative groups of agents; because individual agents had limited resources, or limited access to information (e.g., limited processing power, limited sensor coverage), they worked together by design to solve problems that individually they could not solve, or at least could not solve as efficiently.",
                "MAS research, however, soon began to consider interacting agents with individuated interests, as representatives of different humans or organizations with non-identical interests.",
                "When interactions are guided by diverse interests, participants may have to overcome disagreements, uncooperative interactions, and even intentional attempts to damage one another.",
                "When these types of interactions occur, environments require appropriate behavior from the agents situated in them.",
                "We call these environments Adversarial Environments, and call the clashing agents Adversaries.",
                "Models of cooperation and teamwork have been extensively explored in MAS through the axiomatization of mental states (e.g., [8, 4, 5]).",
                "However, none of this research dealt with adversarial domains and their implications for agent behavior.",
                "Our paper addresses this issue by providing a formal, axiomatized mental state model for a subset of adversarial domains, namely simple zero-sum adversarial environments.",
                "Simple zero-sum encounters exist of course in various twoplayer games (e.g., Chess, Checkers), but they also exist in n-player games (e.g., Risk, Diplomacy), auctions for a single good, and elsewhere.",
                "In these latter environments especially, using a utility-based adversarial search (such as the Min-Max algorithm) does not always provide an adequate solution; the payoff function might be quite complex or difficult to quantify, and there are natural computational limitations on bounded rational agents.",
                "In addition, traditional search methods (like Min-Max) do not make use of a model of the opponent, which has proven to be a valuable addition to adversarial planning [9, 3, 11].",
                "In this paper, we develop a formal, axiomatized model for bounded rational agents that are situated in a zero-sum adversarial environment.",
                "The model uses different modality operators, and its main foundations are the SharedPlans [4] model for collaborative behavior.",
                "We explore environment properties and the mental states of agents to derive behavioral axioms; these behavioral axioms constitute a formal model that serves as a specification and design guideline for agent design in such settings.",
                "We then investigate the behavior of our model empirically using the Connect-Four board game.",
                "We show that this game conforms to our environment definition, and analyze players behavior using a large set of completed match log 550 978-81-904262-7-5 (RPS) c 2007 IFAAMAS files.",
                "In addition, we use the results presented in [9] to discuss the importance of opponent modeling in our ConnectFour adversarial domain.",
                "The paper proceeds as follows.",
                "Section 2 presents the models formalization.",
                "Section 3 presents the empirical analysis and its results.",
                "We discuss related work in Section 4, and conclude and present future directions in Section 5. 2.",
                "ADVERSARIAL ENVIRONMENTS The adversarial environment model (denoted as AE) is intended to guide the design of agents by providing a specification of the capabilities and mental attitudes of an agent in an adversarial environment.",
                "We focus here on specific types of adversarial environments, specified as follows: 1.",
                "Zero-Sum Interactions: positive and negative utilities of all agents sum to zero; 2.",
                "Simple AEs: all agents in the environment are adversarial agents; 3.",
                "Bilateral AEs: AEs with exactly two agents; 4.",
                "Multilateral AEs: AEs of three or more agents.",
                "We will work on both <br>bilateral and multilateral instantiation</br>s of zero-sum and simple environments.",
                "In particular, our adversarial environment model will deal with interactions that consist of N agents (N ≥ 2), where all agents are adversaries, and only one agent can succeed.",
                "Examples of such environments range from board games (e.g., Chess, Connect-Four, and Diplomacy) to certain economic environments (e.g., N-bidder auctions over a single good). 2.1 Model Overview Our approach is to formalize the mental attitudes and behaviors of a single adversarial agent; we consider how a single agent perceives the AE.",
                "The following list specifies the conditions and mental states of an agent in a simple, zero-sum AE: 1.",
                "The agent has an individual intention that its own goal will be completed; 2.",
                "The agent has an individual belief that it and its adversaries are pursuing full conflicting goals (defined below)there can be only one winner; 3.",
                "The agent has an individual belief that each adversary has an intention to complete its own full conflicting goal; 4.",
                "The agent has an individual belief in the (partial) profile of its adversaries.",
                "Item 3 is required, since it might be the case that some agent has a full conflicting goal, and is currently considering adopting the intention to complete it, but is, as of yet, not committed to achieving it.",
                "This might occur because the agent has not yet deliberated about the effects that adopting that intention might have on the other intentions it is currently holding.",
                "In such cases, it might not consider itself to even be in an adversarial environment.",
                "Item 4 states that the agent should hold some belief about the profiles of its adversaries.",
                "The profile represents all the knowledge the agent has about its adversary: its weaknesses, strategic capabilities, goals, intentions, trustworthiness, and more.",
                "It can be given explicitly or can be learned from observations of past encounters. 2.2 Model Definitions for Mental States We use Grosz and Krauss definitions of the modal operators, predicates, and meta-predicates, as defined in their SharedPlan formalization [4].",
                "We recall here some of the predicates and operators that are used in that formalization: Int.To(Ai, α, Tn, Tα, C) represents Ais intentions at time Tn to do an action α at time Tα in the context of C. Int.Th(Ai, prop, Tn, Tprop, C) represents Ais intentions at time Tn that a certain proposition prop holds at time Tprop in the context of C. The potential intention operators, Pot.Int.To(...) and Pot.Int.Th(...), are used to represent the mental state when an agent considers adopting an intention, but has not deliberated about the interaction of the other intentions it holds.",
                "The operator Bel(Ai, f, Tf ) represents agent Ai believing in the statement expressed in formula f, at time Tf .",
                "MB(A, f, Tf ) represents mutual belief for a group of agents A.",
                "A snapshot of the system finds our environment to be in some state e ∈ E of environmental variable states, and each adversary in any LAi ∈ L of possible local states.",
                "At any given time step, the system will be in some world w of the set of all possible worlds w ∈ W, where w = E×LA1 ×LA2 × ...LAn , and n is the number of adversaries.",
                "For example, in a Texas Holdem poker game, an agents local state might be its own set of cards (which is unknown to its adversary) while the environment will consist of the betting pot and the community cards (which are visible to both players).",
                "A utility function under this formalization is defined as a mapping from a possible world w ∈ W to an element in , which expresses the desirability of the world, from a single agent perspective.",
                "We usually normalize the range to [0,1], where 0 represents the least desirable possible world, and 1 is the most desirable world.",
                "The implementation of the utility function is dependent on the domain in question.",
                "The following list specifies new predicates, functions, variables, and constants used in conjunction with the original definitions for the adversarial environment formalization: 1. φ is a null action (the agent does not do anything). 2.",
                "GAi is the set of agent Ais goals.",
                "Each goal is a set of predicates whose satisfaction makes the goal complete (we use G∗ Ai ∈ GAi to represent an arbitrary goal of agent Ai). 3. gAi is the set of agent Ais subgoals.",
                "Subgoals are predicates whose satisfaction represents an important milestone toward achievement of the full goal. gG∗ Ai ⊆ gAi is the set of subgoals that are important to the completion of goal G∗ Ai (we will use g∗ G∗ Ai ∈ gG∗ Ai to represent an arbitrary subgoal). 4.",
                "P Aj Ai is the profile object agent Ai holds about agent Aj. 5.",
                "CA is a general set of actions for all agents in A which are derived from the environments constraints.",
                "CAi ⊆ CA is the set of agent Ais possible actions. 6.",
                "Do(Ai, α, Tα, w) holds when Ai performs action α over time interval Tα in world w. 7.",
                "Achieve(G∗ Ai , α, w) is true when goal G∗ Ai is achieved following the completion of action α in world w ∈ W, where α ∈ CAi . 8.",
                "Profile(Ai, PAi Ai ) is true when agent Ai holds an object profile for agent Aj.",
                "Definition 1.",
                "Full conflict (FulConf ) describes a zerosum interaction where only a single goal of the goals in conflict can be completed.",
                "FulConf(G∗ Ai , G∗ Aj ) ⇒ (∃α ∈ CAi , ∀w, β ∈ CAj ) (Achieve(G∗ Ai , α, w) ⇒ ¬Achieve(G∗ Aj , β, w)) ∨ (∃β ∈ CAj , ∀w, α ∈ CAi )(Achieve(G∗ Aj , β, w) ⇒ ¬Achieve(G∗ Ai , α, w)) Definition 2.",
                "Adversarial Knowledge (AdvKnow) is a function returning a value which represents the amount of The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 551 knowledge agent Ai has on the profile of agent Aj, at time Tn.",
                "The higher the value, the more knowledge agent Ai has.",
                "AdvKnow : P Aj Ai × Tn → Definition 3.",
                "Eval - This evaluation function returns an estimated expected utility value for an agent in A, after completing an action from CA in some world state w. Eval : A × CA × w → Definition 4.",
                "TrH - (Threshold) is a numerical constant in the [0,1] range that represents an evaluation function (Eval) threshold value.",
                "An action that yields an estimated utility evaluation above the TrH is regarded as a highly beneficial action.",
                "The Eval value is an estimation and not the real utility function, which is usually unknown.",
                "Using the real utility value for a rational agent would easily yield the best outcome for that agent.",
                "However, agents usually do not have the real utility functions, but rather a heuristic estimate of it.",
                "There are two important properties that should hold for the evaluation function: Property 1.",
                "The evaluation function should state that the most desirable world state is one in which the goal is achieved.",
                "Therefore, after the goal has been satisfied, there can be no future action that can put the agent in a world state with higher Eval value. (∀Ai, G∗ Ai , α, β ∈ CAi , w ∈ W) Achieve(G∗ Ai , α, w) ⇒ Eval(Ai, α, w) ≥ Eval(Ai, β, w) Property 2.",
                "The evaluation function should project an action that causes a completion of a goal or a subgoal to a value which is greater than TrH (a highly beneficial action). (∀Ai, G∗ Ai ∈ GAi , α ∈ CAi , w ∈ W, g∗ GAi ∈ gGAi ) Achieve(G∗ Ai , α, w) ∨ Achieve(g∗ GAi , α, w) ⇒ Eval(Ai, α, w) ≥ TrH.",
                "Definition 5.",
                "SetAction We define a set action (SetAction) as a set of action operations (either complex or basic actions) from some action sets CAi and CAj which, according to agent Ais belief, are attached together by a temporal and consequential relationship, forming a chain of events (action, and its following consequent action). (∀α1 , . . . , αu ∈ CAi , β1 , . . . , βv ∈ CAj , w ∈ W) SetAction(α1 , . . . , αu , β1 , . . . , βv , w) ⇒ ((Do(Ai, α1 , Tα1 , w) ⇒ Do(Aj, β1 , Tβ1 , w)) ⇒ Do(Ai, α2 , Tα2 , w) ⇒ . . . ⇒ Do(Ai, αu , Tαu , w)) The consequential relation might exist due to various environmental constraints (when one action forces the adversary to respond with a specific action) or due to the agents knowledge about the profile of its adversary.",
                "Property 3.",
                "As the knowledge we have about our adversary increases we will have additional beliefs about its behavior in different situations which in turn creates new set actions.",
                "Formally, if our AdvKnow at time Tn+1 is greater than AdvKnow at time Tn, then every SetAction known at time Tn is also known at time Tn+1.",
                "AdvKnow(P Aj Ai , Tn+1) > AdvKnow(P Aj Ai , Tn) ⇒ (∀α1 , . . . , αu ∈ CAi , β1 , . . . , βv ∈ CAj ) Bel(Aag, SetAction(α1 , . . . , αu , β1 , . . . , βv ), Tn) ⇒ Bel(Aag, SetAction(α1 , . . . , αu , β1 , . . . , βv ), Tn+1) 2.3 The Environment Formulation The following axioms provide the formal definition for a simple, zero-sum Adversarial Environment (AE).",
                "Satisfaction of these axioms means that the agent is situated in such an environment.",
                "It provides specifications for agent Aag to interact with its set of adversaries A with respect to goals G∗ Aag and G∗ A at time TCo at some world state w. AE(Aag, A, G∗ Aag , A1, . . . , Ak, G∗ A1 , . . . , G∗ Ak , Tn, w) 1.",
                "Aag has an Int.Th his goal would be completed: (∃α ∈ CAag , Tα) Int.Th(Aag, Achieve(G∗ Aag , α), Tn, Tα, AE) 2.",
                "Aag believes that it and each of its adversaries Ao are pursuing full conflicting goals: (∀Ao ∈ {A1, . . . , Ak}) Bel(Aag, FulConf(G∗ Aag , G∗ Ao ), Tn) 3.",
                "Aag believes that each of his adversaries in Ao has the Int.Th his conflicting goal G∗ Aoi will be completed: (∀Ao ∈ {A1, . . . , Ak})(∃β ∈ CAo , Tβ) Bel(Aag, Int.Th(Ao, Achieve(G∗ Ao , β), TCo, Tβ, AE), Tn) 4.",
                "Aag has beliefs about the (partial) profiles of its adversaries (∀Ao ∈ {A1, . . . , Ak}) (∃PAo Aag ∈ PAag )Bel(Aag, Profile(Ao, PAo Aag ), Tn) To build an agent that will be able to operate successfully within such an AE, we must specify behavioral guidelines for its interactions.",
                "Using a naive Eval maximization strategy to a certain search depth will not always yield satisfactory results for several reasons: (1) the search horizon problem when searching for a fixed depth; (2) the strong assumption of an optimally rational, unbounded resources adversary; (3) using an estimated evaluation function which will not give optimal results in all world states, and can be exploited [9].",
                "The following axioms specify the behavioral principles that can be used to differentiate between successful and less successful agents in the above Adversarial Environment.",
                "Those axioms should be used as specification principles when designing and implementing agents that should be able to perform well in such Adversarial Environments.",
                "The behavioral axioms represent situations in which the agent will adopt potential intentions to (Pot.Int.To(...)) perform an action, which will typically require some means-end reasoning to select a possible course of action.",
                "This reasoning will lead to the adoption of an Int.To(...) (see [4]).",
                "A1.",
                "Goal Achieving Axiom.",
                "The first axiom is the simplest case; when the agent Aag believes that it is one action (α) away from achieving his conflicting goal G∗ Aag , it should adopt the potential intention to do α and complete its goal. (∀Aag, α ∈ CAag , Tn, Tα, w ∈ W) (Bel(Aag, Do(Aag, α, Tα, w) ⇒ Achieve(G∗ Aag , α, w)) ⇒ Pot.Int.To(Aag, α, Tn, Tα, w) This somewhat trivial behavior is the first and strongest axiom.",
                "In any situation, when the agent is an action away from completing the goal, it should complete the action.",
                "Any fair Eval function would naturally classify α as the maximal value action (property 1).",
                "However, without explicit axiomatization of such behavior there might be situations where the agent will decide on taking another action for various reasons, due to its bounded decision resources.",
                "A2.",
                "Preventive Act Axiom.",
                "Being in an adversarial situation, agent Aag might decide to take actions that will damage one of its adversarys plans to complete its goal, even if those actions do not explicitly advance Aag towards its conflicting goal G∗ Aag .",
                "Such preventive action will take place when agent Aag has a belief about the possibility of its adversary Ao doing an action β that will give it a high 552 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) utility evaluation value (> TrH).",
                "Believing that taking action α will prevent the opponent from doing its β, it will adopt a potential intention to do α. (∀Aag, Ao ∈ A, α ∈ CAag , β ∈ CAo , Tn, Tβ, w ∈ W) (Bel(Aag, Do(Ao, β, Tβ, w) ∧ Eval(Ao, β, w) > TrH, Tn) ∧ Bel(Aag, Do(Aag, α, Tα, w) ⇒ ¬Do(Ao, β, Tβ, w), Tn) ⇒ Pot.Int.To(Aag, α, Tn, Tα, w) This axiom is a basic component of any adversarial environment.",
                "For example, looking at a Chess board game, a player could realize that it is about to be checkmated by its opponent, thus making a preventive move.",
                "Another example is a Connect Four game: when a player has a row of three chips, its opponent must block it, or lose.",
                "A specific instance of A1 occurs when the adversary is one action away from achieving its goal, and immediate preventive action needs to be taken by the agent.",
                "Formally, we have the same beliefs as stated above, with a changed belief that doing action β will cause agent Ao to achieve its goal.",
                "Proposition 1: Prevent or lose case. (∀Aag, Ao ∈ A, α ∈ CAag , β ∈ CAo , G∗ Ao , Tn, Tα, Tβ, w ∈ W) Bel(Aag, Do(Ao, β, Tβ, w) ⇒ Achieve(G∗ Ao , β, w), Tn) ∧ Bel(Aag, Do(Aag, α, Tα, w) ⇒ ¬Do(Ao, β, Tβ, w)) ⇒ Pot.Int.To(Aag, α, Tn, Tα, w) Sketch of proof: Proposition 1 can be easily derived from axiom A1 and the property 2 of the Eval function, which states that any action that causes a completion of a goal is a highly beneficial action.",
                "The preventive act behavior will occur implicitly when the Eval function is equal to the real world utility function.",
                "However, being bounded rational agents and dealing with an estimated evaluation function we need to explicitly axiomatize such behavior, for it will not always occur implicitly from the evaluation function.",
                "A3.",
                "Suboptimal Tactical Move Axiom.",
                "In many scenarios a situation may occur where an agent will decide not to take the current most beneficial action it can take (the action with the maximal utility evaluation value), because it believes that taking another action (with lower utility evaluation value) might yield (depending on the adversarys response) a future possibility for a highly beneficial action.",
                "This will occur most often when the Eval function is inaccurate and differs by a large extent from the Utility function.",
                "Put formally, agent Aag believes in a certain SetAction that will evolve according to its initial action and will yield a high beneficial value (> TrH) solely for it. (∀Aag, Ao ∈ A, Tn, w ∈ W) (∃α1 , . . . , αu ∈ CAi , β1 , . . . , βv ∈ CAj , Tα1 ) Bel(Aag, SetAction(α1 , . . . , αu , β1 , . . . , βv ), Tn) ∧ Bel(Aag, Eval(Ao, βv , w) < TrH < Eval(Aag, αu , w), Tn) ⇒ Pot.Int.To(Aag, α1 , Tn, Tα1 , w) An agent might believe that a chain of events will occur for various reasons due to the inevitable nature of the domain.",
                "For example, in Chess, we often observe the following: a move causes a check position, which in turn limits the opponents moves to avoiding the check, to which the first player might react with another check, and so on.",
                "The agent might also believe in a chain of events based on its knowledge of its adversarys profile, which allows it to foresee the adversarys movements with high accuracy.",
                "A4.",
                "Profile Detection Axiom.",
                "The agent can adjust its adversarys profiles by observations and pattern study (specifically, if there are repeated encounters with the same adversary).",
                "However, instead of waiting for profile information to be revealed, an agent can also initiate actions that will force its adversary to react in a way that will reveal profile knowledge about it.",
                "Formally, the axiom states that if all actions (γ) are not highly beneficial actions (< TrH), the agent can do action α in time Tα if it believes that it will result in a non-highly beneficial action β from its adversary, which in turn teaches it about the adversarys profile, i.e., gives a higher AdvKnow(P Aj Ai , Tβ). (∀Aag, Ao ∈ A, α ∈ CAag , β ∈ CAo , Tn, Tα, Tβ, w ∈ W) Bel(Aag, (∀γ ∈ CAag )Eval(Aag, γ, w) < TrH, Tn) ∧ Bel(Aag, Do(Aag, α, Tα, w) ⇒ Do(Ao, β, Tβ, w), Tn) ∧ Bel(Aag, Eval(Ao, β, w) < TrH) ∧ Bel(Aag, AdvKnow(P Aj Ai , Tβ) > AdvKnow(P Aj Ai , Tn), Tn) ⇒ Pot.Int.To(Aag, α, Tn, Tα, w) For example, going back to the Chess board game scenario, consider starting a game versus an opponent about whom we know nothing, not even if it is a human or a computerized opponent.",
                "We might start playing a strategy that will be suitable versus an average opponent, and adjust our game according to its level of play.",
                "A5.",
                "Alliance Formation Axiom The following behavioral axiom is relevant only in a multilateral instantiation of the adversarial environment (obviously, an alliance cannot be formed in a bilateral, zero-sum encounter).",
                "In different situations during a multilateral interaction, a group of agents might believe that it is in their best interests to form a temporary alliance.",
                "Such an alliance is an agreement that constrains its members behavior, but is believed by its members to enable them to achieve a higher utility value than the one achievable outside of the alliance.",
                "As an example, we can look at the classical Risk board game, where each player has an individual goal of being the sole conquerer of the world, a zero-sum game.",
                "However, in order to achieve this goal, it might be strategically wise to make short-term ceasefire agreements with other players, or to join forces and attack an opponent who is stronger than the rest.",
                "An alliances terms defines the way its members should act.",
                "It is a set of predicates, denoted as Terms, that is agreed upon by the alliance members, and should remain true for the duration of the alliance.",
                "For example, the set Terms in the Risk scenario, could contain the following predicates: 1.",
                "Alliance members will not attack each other on territories X, Y and Z; 2.",
                "Alliance members will contribute C units per turn for attacking adversary Ao; 3.",
                "Members are obligated to stay as part of the alliance until time Tk or until adversarys Ao army is smaller than Q.",
                "The set Terms specifies inter-group constraints on each of the alliance members (∀Aal i ∈ Aal ⊆ A) set of actions Cal i ⊆ C. Definition 6.",
                "Al val - the total evaluation value that agent Ai will achieve while being part of Aal is the sum of Evali (Eval for Ai) of each of Aal j Eval values after taking their own α actions (via the agent(α) predicate): Al val(Ai, Cal , Aal , w) = α∈Cal Evali(Aal j , agent(α), w) Definition 7.",
                "Al TrH - is a number representing an Al val The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 553 threshold; above it, the alliance can be said to be a highly beneficial alliance.",
                "The value of Al TrH will be calculated dynamically according to the progress of the interaction, as can be seen in [7].",
                "After an alliance is formed, its members are now working in their normal adversarial environment, as well as according to the mental states and axioms required for their interactions as part of the alliance.",
                "The following Alliance model (AL) specifies the conditions under which the group Aal can be said to be in an alliance and working with a new and constrained set of actions Cal , at time Tn.",
                "AL(Aal , Cal , w, Tn) 1.",
                "Aal has a MB that all members are part of Aal : MB(Aal , (∀Aal i ∈ Aal )member(Aal i , Aal ), Tn) 2.",
                "Aal has a MB that the group be maintained: MB(Aal , (∀Aal i ∈ Aal )Int.Th (Ai, member(Ai, Aal ), Tn, Tn+1, Co), Tn) 3.",
                "Aal has a MB that being members gives them high utility value: MB(Aal , (∀Aal i ∈ Aal )Al val(Aal i , Cal , Aal , w) ≥ Al TrH, Tn) Members profiles are a crucial part of successful alliances.",
                "We assume that agents that have more accurate profiles of their adversaries will be more successful in such environments.",
                "Such agents will be able to predict when a member is about to breach the alliances contract (item 2 in the above model), and take counter measures (when item 3 will falsify).",
                "The robustness of the alliance is in part a function of its members trustfulness measure, objective position estimation, and other profile properties.",
                "We should note that an agent can simultaneously be part of more than one alliance.",
                "Such a temporary alliance, where the group members do not have a joint goal but act collaboratively for the interest of their own individual goals, is classified as a Treatment Group by modern psychologists [12] (in contrast to a Task Group, where its members have a joint goal).",
                "The Shared Activity model as presented in [5] modeled Treatment Group behavior using the same SharedPlans formalization.",
                "When comparing both definitions of an alliance and a Treatment Group we found an unsurprising resemblance between both models: the environment models definitions are almost identical (see SAs definitions in [5]), and their Selfish-Act and Cooperative Act axioms conform to our adversarial agents behavior.",
                "The main distinction between both models is the integration of a Helpful-behavior act axiom, in the Shared Activity which cannot be part of ours.",
                "This axiom states that an agent will consider taking action that will lower its Eval value (to a certain lower bound), if it believes that a group partner will gain a significant benefit.",
                "Such behavior cannot occur in a pure adversarial environment (as a zero-sum game is), where the alliance members are constantly on watch to manipulate their alliance to their own advantage.",
                "A6.",
                "Evaluation Maximization Axiom.",
                "In a case when all other axioms are inapplicable, we will proceed with the action that maximizes the heuristic value as computed in the Eval function. (∀Aag, Ao ∈ A, α ∈ Cag, Tn, w ∈ W) Bel(Aag, (∀γ ∈ Cag)Eval(Aag, α, w) ≥ Eval(Aag, γ, w), Tn) ⇒ Pot.Int.To(Aag, α, Tn, Tα, w) T1.",
                "Optimality on Eval = Utility The above axiomatic model handles situations where the Utility is unknown and the agents are bounded rational agents.",
                "The following theorem shows that in bilateral interactions, where the agents have the real Utility function (i.e., Eval = Utility) and are rational agents, the axioms provide the same optimal result as classic adversarial search (e.g., Min-Max).",
                "Theorem 1.",
                "Let Ae ag be an unbounded rational AE agent using the Eval heuristic evaluation function, Au ag be the same agent using the true Utility function, and Ao be a sole unbounded utility-based rational adversary.",
                "Given that Eval = Utility: (∀α ∈ CAu ag , α ∈ CAe ag , Tn, w ∈ W) Pot.Int.To(Au ag, α, Tn, Tα, w) → Pot.Int.To(Ae ag, α , Tn, Tα, w) ∧ ((α = α ) ∨ (Utility(Au ag, α, w) = Eval(Ae ag, α , w))) Sketch of proof - Given that Au ag has the real utility function and unbounded resources, it can generate the full game tree and run the optimal MinMax algorithm to choose the highest utility value action, which we denote by, α.",
                "The proof will show that Ae ag, using the AE axioms, will select the same or equal utility α (when there is more than one action with the same max utility) when Eval = Utility. (A1) Goal achieving axiom - suppose there is an α such that its completion will achieve Au ags goal.",
                "It will obtain the highest utility by Min-Max for Au ag.",
                "The Ae ag agent will select α or another action with the same utility value via A1.",
                "If such α does not exist, Ae ag cannot apply this axiom, and proceeds to A2. (A2) Preventive act axiom - (1) Looking at the basic case (see Prop1), if there is a β which leads Ao to achieve its goal, then a preventive action α will yield the highest utility for Au ag.",
                "Au ag will choose it through the utility, while Ae ag will choose it through A2. (2) In the general case, β is a highly beneficial action for Ao, thus yields low utility for Au ag, which will guide it to select an α that will prevent β, while Ae ag will choose it through A2.1 If such β does not exist for Ao, then A2 is not applicable, and Ae ag can proceed to A3. (A3) Suboptimal tactical move axiom - When using a heuristic Eval function, Ae ag has a partial belief in the profile of its adversary (item 4 in AE model), which may lead it to believe in SetActions (Prop1).",
                "In our case, Ae ag is holding a full profile on its optimal adversary and knows that Ao will behave optimally according to the real utility values on the complete search tree, therefore, any belief about suboptimal SetAction cannot exist, yielding this axiom inapplicable.",
                "Ae ag will proceed to A4. (A4) Profile detection axiom - Given that Ae ag has the full profile of Ao, none of Ae ags actions can increase its knowledge.",
                "That axiom will not be applied, and the agent will proceed with A6 (A5 will be disregarded because the interaction is bilateral). (A6) Evaluation maximization axiom - This axiom will select the max Eval for Ae ag.",
                "Given that Eval = Utility, the same α that was selected by Au ag will be selected. 3.",
                "EVALUATION The main purpose of our experimental analysis is to evaluate the models behavior and performance in a real adversarial environment.",
                "This section investigates whether bounded 1 A case where following the completion of β there exists a γ which gives high utility for Agent Au ag, cannot occur because Ao uses the same utility, and γs existence will cause it to classify β as a low utility action. 554 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) rational agents situated in such adversarial environments will be better off applying our suggested behavioral axioms. 3.1 The Domain To explore the use of the above model and its behavioral axioms, we decided to use the Connect-Four game as our adversarial environment.",
                "Connect-Four is a 2-player, zerosum game which is played using a 6x7 matrix-like board.",
                "Each turn, a player drops a disc into one of the 7 columns (the set of 21 discs is usually colored yellow for player 1 and red for player 2; we will use White and Black respectively to avoid confusion).",
                "The winner is the first player to complete a horizontal, vertical, or diagonal set of four discs with its color.",
                "On very rare occasions, the game might end in a tie if all the empty grids are filled, but no player managed to create a 4-disc set.",
                "The Connect-Four game was solved in [1], where it is shown that the first player (playing with the white discs) can force a win by starting in the middle column (column 4) and playing optimally However, the optimal strategy is very complex, and difficult to follow even for complex bounded rational agents, such as human players.",
                "Before we can proceed checking agent behavior, we must first verify that the domain conforms to the adversarial environments definition as given above (which the behavioral axioms are based on).",
                "First, when playing a Connect-Four game, the agent has an intention to win the game (item 1).",
                "Second (item 2), our agent believes that in Connect-Four there can only be one winner (or no winner at all in the rare occurrence of a tie).",
                "In addition, our agent believes that its opponent to the game will try to win (item 3), and we hope it has some partial knowledge (item 4) about its adversary (this knowledge can vary from nothing, through simple facts such as age, to strategies and weaknesses).",
                "Of course, not all Connect-Four encounters are adversarial.",
                "For example, when a parent is playing the game with its child, the following situation might occur: the child, having a strong incentive to win, treats the environment as adversarial (it intends to win, understands that there can only be one winner, and believes that its parent is trying to beat him).",
                "However, the parents point of view might see the environment as an educational one, where its goal is not to win the game, but to cause enjoyment or practice strategic reasoning.",
                "In such an educational environment, a new set of behavioral axioms might be more beneficial to the parents goals than our suggested adversarial behavioral axioms. 3.2 Axiom Analysis After showing that the Connect-Four game is indeed a zero-sum, bilateral adversarial environment, the next step is to look at players behaviors during the game and check whether behaving according to our model does improve performance.",
                "To do so we have collected log files from completed Connect-Four games that were played by human players over the Internet.",
                "Our collected log file data came from Play by eMail (PBeM) sites.",
                "These are web sites that host email games, where each move is taken by an email exchange between the server and the players.",
                "Many such sites archives contain real competitive interactions, and also maintain a ranking system for their members.",
                "Most of the data we used can be found in [6].",
                "As can be learned from [1], Connect-Four has an optimal strategy and a considerable advantage for the player who starts the game (which we call the White player).",
                "We will concentrate in our analysis on the second players moves (to be called Black).",
                "The White player, being the first to act, has the so-called initiative advantage.",
                "Having the advantage and a good strategy will keep the Black player busy reacting to its moves, instead of initiating threats.",
                "A threat is a combination of three discs of the same color, with an empty spot for the fourth winning disk.",
                "An open threat is a threat that can be realized in the opponents next move.",
                "In order for the Black player to win, it must somehow turn the tide, take the advantage and start presenting threats to the White player.",
                "We will explore Black players behavior and their conformance to our axioms.",
                "To do so, we built an application that reads log files and analyzes the Black players moves.",
                "The application contains two main components: (1) a Min-Max algorithm for evaluation of moves; (2) open threats detector for the discovering of open threats.",
                "The Min-Max algorithm will work to a given depth, d and for each move α will output the heuristic value for the next action taken by the player as written in the log file, h(α), alongside the maximum heuristic value, maxh(α), that could be achieved prior to taking the move (obviously, if h(α) = maxh(α), then the player did not do the optimal move heuristically).",
                "The threat detectors job is to notify if some action was taken in order to block an open threat (not blocking an open threat will probably cause the player to lose in the opponents next move).",
                "The heuristic function used by Min-Max to evaluate the players utility is the following function, which is simple to compute, yet provides a reasonable challenge to human opponents: Definition 8.",
                "Let Group be an adjacent set of four squares that are horizontal, vertical, or diagonal.",
                "Groupn b (Groupn w) be a Group with n pieces of the black (white) color and 4−n empty squares. h = ((Group1 b ∗α)+(Group2 b ∗β)+(Group3 b ∗γ)+(Group4 b ∗∞)) − ((Group1 w ∗α)+(Group2 w ∗β)+(Group3 w ∗γ)+(Group4 w ∗∞)) The values of α, β and δ can vary to form any desired linear combination; however, it is important to value them with the α < β < δ ordering in mind (we used 1, 4, and 8 as their respective values).",
                "Groups of 4 discs of the same color means victory, thus discovery of such a group will result in ∞ to ensure an extreme value.",
                "We now use our estimated evaluation function to evaluate the Black players actions during the Connect-Four adversarial interaction.",
                "Each game from the log file was input into the application, which processed and output a reformatted log file containing the h value of the current move, the maxh value that could be achieved, and a notification if an open threat was detected.",
                "A total of 123 games were analyzed (57 with White winning, and 66 with Black winning).",
                "A few additional games were manually ignored in the experiment, due to these problems: a player abandoning the game while the outcome is not final, or a blunt irrational move in the early stages of the game (e.g., not blocking an obvious winning group in the first opening moves).",
                "In addition, a single tie game was also removed.",
                "The simulator was run to a search depth of 3 moves.",
                "We now proceed to analyze the games with respect to each behavioral axiom.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 555 Table 1: Average heuristic difference analysis Black losses Black Won Avg minh -17.62 -12.02 Avg 3 lowest h moves (min3 h) -13.20 -8.70 3.2.1 Affirming the Suboptimal tactical move axiom The following section presents the heuristic evaluations of the Min-Max algorithm for each action, and checks the amount and extent of suboptimal tactical actions and their implications on performance.",
                "Table 1 shows results and insights from the games heuristic analysis, when search depth equals 3 (this search depth was selected for the results to be comparable to [9], see Section 3.2.3).",
                "The tables heuristic data is the difference between the present maximal heuristic value and the heuristic value of the action that was eventually taken by the player (i.e., the closer the number is to 0, the closer the action was to the maximum heuristic action).",
                "The first row presents the difference values of the action that had the maximal difference value among all the Black players actions in a given game, as averaged over all Blacks winning and losing games (see respective columns).",
                "In games in which the Black player loses, its average difference value was -17.62, while in games in which the Black player won, its average was -12.02.",
                "The second row expands the analysis by considering the 3 highest heuristic difference actions, and averaging them.",
                "In that case, we notice an average heuristic difference of 5 points between games which the Black player loses and games in which it wins.",
                "Nevertheless, the importance of those numbers is that they allowed us to take an educated guess on a threshold number of 11.5, as the value of the TrH constant, which differentiates between normal actions and highly beneficial ones.",
                "After finding an approximated TrH constant, we can proceed with an analysis of the importance of suboptimal moves.",
                "To do so we took the subset of games in which the minimum heuristic difference value for Blacks actions was 11.5.",
                "As presented in Table 2, we can see the different min3 h average of the 3 largest ranges and the respective percentage of games won.",
                "The first row shows that the Black player won only 12% of the games in which the average of its 3 highest heuristically difference actions (min3 h) was smaller than the suggested threshold, TrH = 11.5.",
                "The second row shows a surprising result: it seems that when min3 h > −4 the Black player rarely wins.",
                "Intuition would suggest that games in which the action evaluation values were closer to the maximal values will result in more winning games for Black.",
                "However, it seems that in the Connect-Four domain, merely responding with somewhat easily expected actions, without initiating a few surprising and suboptimal moves, does not yield good results.",
                "The last row sums up the main insights from the analysis; most of Blacks wins (83%) came when its min3 h was in the range of -11.5 to -4.",
                "A close inspection of those Black winning games shows the following pattern behind the numbers: after standard opening moves, Black suddenly drops a disc into an isolated column, which seems a waste of a move.",
                "White continues to build its threats, while usually disregarding Blacks last move, which in turn uses the isolated disc as an anchor for a future winning threat.",
                "The results show that it was beneficial for the Black player Table 2: Blacks winnings percentages % of games min3 h < −11.5 12% min3 h > −4 5% −11.5 ≤ min3 h ≤ −4 83% to take suboptimal actions and not give the current highest possible heuristic value, but will not be too harmful for its position (i.e., will not give high beneficial value to its adversary).",
                "As it turned out, learning the threshold is an important aspect of success: taking wildly risky moves (min3 h < −11.5) or trying to avoid them (min3 h > −4) reduces the Black players winning chances by a large margin. 3.2.2 Affirming the Profile Monitoring Axiom In the task of showing the importance of monitoring ones adversaries profiles, our log files could not be used because they did not contain repeated interactions between players, which are needed to infer the players knowledge about their adversaries.",
                "However, the importance of opponent modeling and its use in attaining tactical advantages was already studied in various domains ([3, 9] are good examples).",
                "In a recent paper, Markovitch and Reger [9] explored the notion of learning and exploitation of opponent weakness in competitive interactions.",
                "They apply simple learning strategies by analyzing examples from past interactions in a specific domain.",
                "They also used the Connect-Four adversarial domain, which can now be used to understand the importance of monitoring the adversarys profile.",
                "Following the presentation of their theoretical model, they describe an extensive empirical study and check the agents performance after learning the weakness model with past examples.",
                "One of the domains used as a competitive environment was the same Connect-Four game (Checkers was the second domain).",
                "Their heuristic function was identical to ours with three different variations (H1, H2, and H3) that are distinguished from one another in their linear combination coefficient values.",
                "The search depth for the players was 3 (as in our analysis).",
                "Their extensive experiments check and compare various learning strategies, risk factors, predefined feature sets and usage methods.",
                "The bottom line is that the Connect-Four domain shows an improvement from a 0.556 winning rate before modeling to a 0.69 after modeling (page 22).",
                "Their conclusions, showing improved performance when holding and using the adversarys model, justify the effort to monitor the adversary profile for continuous and repeated interactions.",
                "An additional point that came up in their experiments is the following: after the opponent weakness model has been learned, the authors describe different methods of integrating the opponent weakness model into the agents decision strategy.",
                "Nevertheless, regardless of the specific method they chose to work with, all integration methods might cause the agent to take suboptimal decisions; it might cause the agent to prefer actions that are suboptimal at the present decision junction, but which might cause the opponent to react in accordance with its weakness model (as represented by our agent) which in turn will be beneficial for us in the future.",
                "The agents behavior, as demonstrated in [9] further confirms and strengthens our Suboptimal Tactical Axiom as discussed in the previous section. 556 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 3.2.3 Additional Insights The need for the Goal Achieving, Preventive Act, and Evaluation Maximization axioms are obvious, and need no further verification.",
                "However, even with respect to those axioms, a few interesting insights came up in the log analysis.",
                "The Goal achieving and Preventive Act axioms, though theoretically trivial, seem to provide some challenge to a human player.",
                "In the initial inspection of the logs, we encountered few games2 where a player, for inexplicable reasons, did not block the other from winning or failed to execute its own winning move.",
                "We can blame those faults on the humans lack of attention, or a typing error in its move reply; nevertheless, those errors might occur in bounded rational agents, and the appropriate behavior needs to be axiomatized.",
                "A typical Connect-Four game revolves around generating threats and blocking them.",
                "In our analysis we looked for explicit preventive actions, i.e., moves that block a group of 3 discs, or that remove a future threat (in our limited search horizon).",
                "We found that in 83% of the total games there was at least one preventive action taken by the Black player.",
                "It was also found that Black averaged 2.8 preventive actions per game on the games in which it lost, while averaging 1.5 preventive actions per game when winning.",
                "It seems that Black requires 1 or 2 preventive actions to build its initial taking position, before starting to present threats.",
                "If it did not manage to win, it will usually prevent an extra threat or two before succumbing to White. 4.",
                "RELATED WORK Much research deals with the axiomatization of teamwork and mental states of individuals: some models use knowledge and belief [10], others have models of goals and intentions [8, 4].",
                "However, all these formal theories deal with agent teamwork and cooperation.",
                "As far as we know, our model is the first to provide a formalized model for explicit adversarial environments and agents behavior in it.",
                "The classical Min-Max adversarial search algorithm was the first attempt to integrate the opponent into the search space with a weak assumption of an optimally playing opponent.",
                "Since then, much effort has gone into integrating the opponent model into the decision procedure to predict future behavior.",
                "The M∗ algorithm presented by Carmel and Markovitch [2] showed a method of incorporating opponent models into adversary search, while in [3] they used learning to provide a more accurate opponent model in a 2player repeated game environment, where agents strategies were modeled as finite automata.",
                "Additional Adversarial planning work was done by Willmott et al. [13], which provided an adversarial planning approach to the game of GO.",
                "The research mentioned above dealt with adversarial search and the integration of opponent models into classical utilitybased search methods.",
                "That work shows the importance of opponent modeling and the ability to exploit it to an agents advantage.",
                "However, the basic limitations of those search methods still apply; our model tries to overcome those limitations by presenting a formal model for a new, mental state-based adversarial specification. 5.",
                "CONCLUSIONS We presented an Adversarial Environment model for a 2 These were later removed from the final analysis. bounded rational agent that is situated in an N-player, zerosum environment.",
                "We used the SharedPlans formalization to define the model and the axioms that agents can apply as behavioral guidelines.",
                "The model is meant to be used as a guideline for designing agents that need to operate in such adversarial environments.",
                "We presented empirical results, based on ConnectFour log file analysis, that exemplify the model and the axioms for a bilateral instance of the environment.",
                "The results we presented are a first step towards an expanded model that will cover all types of adversarial environments, for example, environments that are non-zero-sum, and environments that contain natural agents that are not part of the direct conflict.",
                "Those challenges and more will be dealt with in future research. 6.",
                "ACKNOWLEDGMENT This research was supported in part by Israel Science Foundation grants #1211/04 and #898/05. 7.",
                "REFERENCES [1] L. V. Allis.",
                "A knowledge-based approach of Connect-Four - the game is solved: White wins.",
                "Masters thesis, Free University, Amsterdam, The Netherlands, 1988. [2] D. Carmel and S. Markovitch.",
                "Incorporating opponent models into adversary search.",
                "In Proceedings of the Thirteenth National Conference on Artificial Intelligence, pages 120-125, Portland, OR, 1996. [3] D. Carmel and S. Markovitch.",
                "Opponent modeling in multi-agent systems.",
                "In G. Weiß and S. Sen, editors, Adaptation and Learning in Multi-Agent Systems, pages 40-52.",
                "Springer-Verlag, 1996. [4] B. J. Grosz and S. Kraus.",
                "Collaborative plans for complex group action.",
                "Artificial Intelligence, 86(2):269-357, 1996. [5] M. Hadad, G. Kaminka, G. Armon, and S. Kraus.",
                "Supporting collaborative activity.",
                "In Proc. of AAAI-2005, pages 83-88, Pittsburgh, 2005. [6] http://www.gamerz.net/˜pbmserv/. [7] S. Kraus and D. Lehmann.",
                "Designing and building a negotiating automated agent.",
                "Computational Intelligence, 11:132-171, 1995. [8] H. J. Levesque, P. R. Cohen, and J. H. T. Nunes.",
                "On acting together.",
                "In Proc. of AAAI-90, pages 94-99, Boston, MA, 1990. [9] S. Markovitch and R. Reger.",
                "Learning and exploiting relative weaknesses of opponent agents.",
                "Autonomous Agents and Multi-Agent Systems, 10(2):103-130, 2005. [10] Y. M. Ronald Fagin, Joseph Y. Halpern and M. Y. Vardi.",
                "Reasoning about knowledge.",
                "MIT Press, Cambridge, Mass., 1995. [11] P. Thagard.",
                "Adversarial problem solving: Modeling an oponent using explanatory coherence.",
                "Cognitive Science, 16(1):123-149, 1992. [12] R. W. Toseland and R. F. Rivas.",
                "An Introduction to Group Work Practice.",
                "Prentice Hall, Englewood Cliffs, NJ, 2nd edition edition, 1995. [13] S. Willmott, J. Richardson, A. Bundy, and J. Levine.",
                "An adversarial planning approach to Go.",
                "Lecture Notes in Computer Science, 1558:93-112, 1999.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 557"
            ],
            "original_annotated_samples": [
                "We will work on both <br>bilateral and multilateral instantiation</br>s of zero-sum and simple environments."
            ],
            "translated_annotated_samples": [
                "Trabajaremos en <br>instanciaciones tanto bilaterales como multilaterales</br> de entornos de suma cero y simples."
            ],
            "translated_text": "Un modelo de entorno adversarial para agentes racionales limitados en interacciones de suma cero Inon Zuckerman1, Sarit Kraus1, Jeffrey S. Rosenschein2, Gal Kaminka1 1 Departamento de Ciencias de la Computación 2 Escuela de Ingeniería Universidad Bar-Ilan y Ciencias de la Computación Ramat-Gan, Israel Universidad Hebrea, Jerusalén, Israel {zukermi, sarit, galk}@cs.biu.ac.il jeff@cs.huji.ac.il RESUMEN Los entornos multiagentes a menudo no son cooperativos ni colaborativos; en muchos casos, los agentes tienen intereses conflictivos, lo que lleva a interacciones adversariales. Este documento presenta un modelo formal de Entorno Adversarial para agentes racionales acotados que operan en un entorno de suma cero. En tales entornos, los intentos de utilizar métodos de búsqueda basados en utilidad clásica pueden plantear una variedad de dificultades (por ejemplo, modelar implícitamente al oponente como un maximizador de utilidad omnisciente, en lugar de aprovechar un modelo de oponente más matizado y explícito). Definimos un Entorno Adversario al describir los estados mentales de un agente en dicho entorno. Luego presentamos axiomas de comportamiento que están destinados a servir como principios de diseño para construir agentes adversarios. Exploramos la aplicación de nuestro enfoque mediante el análisis de archivos de registro de juegos de Conecta Cuatro completados, y presentamos un análisis empírico de la adecuación de los axiomas. Categorías y Descriptores de Asignaturas I.2.11 [Inteligencia Artificial]: Inteligencia Artificial Distribuida-Agentes Inteligentes, Sistemas Multiagente; I.2.4 [Inteligencia Artificial]: Formalismos y Métodos de Representación del Conocimiento - Lógica Modal Términos Generales Diseño, Teoría 1. Introducción: En las primeras investigaciones en sistemas multiagente (SMA) se consideraban grupos cooperativos de agentes; debido a que los agentes individuales tenían recursos limitados o acceso limitado a la información (por ejemplo, potencia de procesamiento limitada, cobertura sensorial limitada), trabajaban juntos por diseño para resolver problemas que individualmente no podían resolver, o al menos no podían resolver de manera eficiente. Sin embargo, la investigación de MAS pronto comenzó a considerar agentes interactivos con intereses individualizados, como representantes de diferentes humanos u organizaciones con intereses no idénticos. Cuando las interacciones son guiadas por intereses diversos, los participantes pueden tener que superar desacuerdos, interacciones no cooperativas e incluso intentos intencionales de dañarse mutuamente. Cuando ocurren este tipo de interacciones, los entornos requieren un comportamiento apropiado por parte de los agentes situados en ellos. Llamamos a estos entornos Entornos Adversarios, y llamamos a los agentes en conflicto Adversarios. Los modelos de cooperación y trabajo en equipo han sido ampliamente explorados en MAS a través de la axiomatización de estados mentales (por ejemplo, [8, 4, 5]). Sin embargo, ninguna de estas investigaciones abordó los dominios adversarios y sus implicaciones para el comportamiento de los agentes. Nuestro artículo aborda este problema al proporcionar un modelo formal y axiomatizado de estados mentales para un subconjunto de dominios adversariales, específicamente entornos adversariales simples de suma cero. Los encuentros de suma cero simples existen, por supuesto, en varios juegos de dos jugadores (por ejemplo, Ajedrez, Damas), pero también existen en juegos de n jugadores (por ejemplo, Risk, Diplomacia), subastas de un solo bien y en otros lugares. En estos entornos recientes en particular, el uso de una búsqueda adversarial basada en utilidad (como el algoritmo Min-Max) no siempre proporciona una solución adecuada; la función de recompensa podría ser bastante compleja o difícil de cuantificar, y existen limitaciones computacionales naturales en agentes racionales acotados. Además, los métodos de búsqueda tradicionales (como Min-Max) no hacen uso de un modelo del oponente, lo cual ha demostrado ser una adición valiosa a la planificación adversarial [9, 3, 11]. En este artículo, desarrollamos un modelo formal y axiomatizado para agentes racionales limitados que se encuentran en un entorno adversarial de suma cero. El modelo utiliza diferentes operadores de modalidad, y sus principales fundamentos son el modelo SharedPlans [4] para el comportamiento colaborativo. Exploramos las propiedades del entorno y los estados mentales de los agentes para derivar axiomas de comportamiento; estos axiomas de comportamiento constituyen un modelo formal que sirve como especificación y guía de diseño para el diseño de agentes en tales entornos. Luego investigamos el comportamiento de nuestro modelo empíricamente utilizando el juego de mesa Conecta Cuatro. Mostramos que este juego se ajusta a nuestra definición de entorno, y analizamos el comportamiento de los jugadores utilizando un gran conjunto de registros de partidas completadas 550 978-81-904262-7-5 (RPS) c 2007 archivos de IFAAMAS. Además, utilizamos los resultados presentados en [9] para discutir la importancia del modelado del oponente en nuestro dominio adversarial de Conecta Cuatro. El artículo continúa de la siguiente manera. La sección 2 presenta la formalización de los modelos. La sección 3 presenta el análisis empírico y sus resultados. Discutimos el trabajo relacionado en la Sección 4, y concluimos y presentamos las direcciones futuras en la Sección 5. ENTORNOS ADVERSARIOS El modelo de entorno adversario (denominado como AE) está destinado a guiar el diseño de agentes al proporcionar una especificación de las capacidades y actitudes mentales de un agente en un entorno adversario. Nos enfocamos aquí en tipos específicos de entornos adversarios, especificados de la siguiente manera: 1. Interacciones de suma cero: las utilidades positivas y negativas de todos los agentes suman cero; 2. Agentes adversarios simples: todos los agentes en el entorno son agentes adversarios; 3. AEs bilaterales: AEs con exactamente dos agentes; 4. AEs multilaterales: AEs de tres o más agentes. Trabajaremos en <br>instanciaciones tanto bilaterales como multilaterales</br> de entornos de suma cero y simples. En particular, nuestro modelo de entorno adversarial tratará con interacciones que consisten en N agentes (N ≥ 2), donde todos los agentes son adversarios y solo un agente puede tener éxito. Ejemplos de tales entornos van desde juegos de mesa (por ejemplo, Ajedrez, Conecta Cuatro y Diplomacia) hasta ciertos entornos económicos (por ejemplo, subastas de N postores sobre un único bien). 2.1 Resumen del Modelo Nuestro enfoque es formalizar las actitudes mentales y comportamientos de un único agente adversario; consideramos cómo un único agente percibe el AE. La siguiente lista especifica las condiciones y estados mentales de un agente en un AE simple de suma cero: 1. El agente tiene la intención individual de que su propio objetivo se complete; 2. El agente tiene la creencia individual de que él y sus adversarios persiguen objetivos completamente opuestos (definidos a continuación) y que solo puede haber un ganador; 3. El agente tiene la creencia individual de que cada adversario tiene la intención de completar su propio objetivo conflictivo completo; 4. El agente tiene una creencia individual en el perfil (parcial) de sus adversarios. El ítem 3 es necesario, ya que podría ser el caso de que algún agente tenga un objetivo completamente conflictivo y esté considerando adoptar la intención de completarlo, pero aún no está comprometido a lograrlo. Esto podría ocurrir porque el agente aún no ha reflexionado sobre los efectos que adoptar esa intención podría tener en las otras intenciones que actualmente mantiene. En tales casos, es posible que ni siquiera se considere en un entorno adversarial. El ítem 4 establece que el agente debe tener alguna creencia sobre los perfiles de sus adversarios. El perfil representa todo el conocimiento que el agente tiene sobre su adversario: sus debilidades, capacidades estratégicas, objetivos, intenciones, confiabilidad y más. Puede ser dado explícitamente o puede ser aprendido a partir de observaciones de encuentros pasados. 2.2 Definiciones de modelos para estados mentales Utilizamos las definiciones de los operadores modales, predicados y metapredicados de Grosz y Krauss, tal como se definen en su formalización de SharedPlan [4]. Recordamos aquí algunos de los predicados y operadores que se utilizan en esa formalización: Int.To(Ai, α, Tn, Tα, C) representa las intenciones de Ai en el tiempo Tn de realizar una acción α en el tiempo Tα en el contexto de C. Int.Th(Ai, prop, Tn, Tprop, C) representa las intenciones de Ai en el tiempo Tn de que una cierta proposición prop se cumpla en el tiempo Tprop en el contexto de C. Los operadores de intención potencial, Pot.Int.To(...) y Pot.Int.Th(...), se utilizan para representar el estado mental cuando un agente considera adoptar una intención, pero no ha deliberado sobre la interacción de las otras intenciones que tiene. El operador Bel(Ai, f, Tf) representa al agente Ai creyendo en la afirmación expresada en la fórmula f, en el tiempo Tf. MB(A, f, Tf) representa la creencia mutua para un grupo de agentes A. Una instantánea del sistema encuentra que nuestro entorno se encuentra en algún estado e ∈ E de estados variables ambientales, y cada adversario en cualquier LAi ∈ L de posibles estados locales. En cualquier paso de tiempo dado, el sistema estará en algún mundo w del conjunto de todos los mundos posibles w ∈ W, donde w = E×LA1 ×LA2 × ...LAn, y n es el número de adversarios. Por ejemplo, en un juego de póker Texas Holdem, el estado local de un agente podría ser su propio conjunto de cartas (que es desconocido para su adversario) mientras que el entorno consistirá en el bote de apuestas y las cartas comunitarias (que son visibles para ambos jugadores). Una función de utilidad bajo esta formalización se define como un mapeo de un mundo posible w ∈ W a un elemento en , que expresa la deseabilidad del mundo, desde la perspectiva de un único agente. Normalmente normalizamos el rango a [0,1], donde 0 representa el mundo menos deseable posible, y 1 es el mundo más deseable. La implementación de la función de utilidad depende del dominio en cuestión. La siguiente lista especifica nuevos predicados, funciones, variables y constantes utilizadas en conjunto con las definiciones originales para la formalización del entorno adversarial: 1. φ es una acción nula (el agente no hace nada). 2. GAi es el conjunto de objetivos del agente Ai. Cada objetivo es un conjunto de predicados cuya satisfacción hace que el objetivo esté completo (usamos G∗ Ai ∈ GAi para representar un objetivo arbitrario del agente Ai). 3. gAi es el conjunto de subobjetivos del agente Ai. Los subobjetivos son predicados cuya satisfacción representa un hito importante hacia el logro del objetivo completo. gG∗ Ai ⊆ gAi es el conjunto de subobjetivos que son importantes para la finalización del objetivo G∗ Ai (usaremos g∗ G∗ Ai ∈ gG∗ Ai para representar un subobjetivo arbitrario). 4. P Aj Ai es el objeto de perfil que el agente Ai tiene sobre el agente Aj. 5. CA es un conjunto general de acciones para todos los agentes en A que se derivan de las restricciones del entorno. CAi ⊆ CA es el conjunto de acciones posibles del agente Ai. La expresión Do(Ai, α, Tα, w) se cumple cuando Ai realiza la acción α durante el intervalo de tiempo Tα en el mundo w. 7. La afirmación Achieve(G∗ Ai , α, w) es verdadera cuando se logra el objetivo G∗ Ai siguiendo la finalización de la acción α en el mundo w ∈ W, donde α ∈ CAi. El perfil (Ai, PAi Ai) es verdadero cuando el agente Ai tiene un perfil de objeto para el agente Aj. Definición 1. El conflicto total (FulConf) describe una interacción de suma cero donde solo se puede completar un solo objetivo de los objetivos en conflicto. FulConf(G∗ Ai , G∗ Aj ) ⇒ (∃α ∈ CAi , ∀w, β ∈ CAj ) (Achieve(G∗ Ai , α, w) ⇒ ¬Achieve(G∗ Aj , β, w)) ∨ (∃β ∈ CAj , ∀w, α ∈ CAi )(Achieve(G∗ Aj , β, w) ⇒ ¬Achieve(G∗ Ai , α, w)) Definición 2. El Conocimiento Adversarial (AdvKnow) es una función que devuelve un valor que representa la cantidad de The Sixth Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 551 agente de conocimiento Ai tiene sobre el perfil del agente Aj, en el tiempo Tn. Cuanto mayor sea el valor, más conocimiento tiene el agente Ai. AdvKnow: P Aj Ai × Tn → Definición 3. Eval: Esta función de evaluación devuelve un valor de utilidad esperada estimado para un agente en A, después de completar una acción de CA en algún estado del mundo w. Eval: A × CA × w → Definición 4. TrH - (Umbral) es una constante numérica en el rango [0,1] que representa un valor umbral de la función de evaluación (Eval). Una acción que produce una evaluación de utilidad estimada por encima del TrH se considera una acción altamente beneficiosa. El valor Eval es una estimación y no la función de utilidad real, la cual suele ser desconocida. Utilizar el valor real de utilidad para un agente racional fácilmente produciría el mejor resultado para ese agente. Sin embargo, los agentes generalmente no tienen las funciones de utilidad reales, sino más bien una estimación heurística de las mismas. Existen dos propiedades importantes que deben cumplir para la función de evaluación: Propiedad 1. La función de evaluación debería indicar que el estado del mundo más deseable es aquel en el que se logra el objetivo. Por lo tanto, una vez que se ha satisfecho el objetivo, no puede haber ninguna acción futura que pueda colocar al agente en un estado del mundo con un valor de Eval más alto. (∀Ai, G∗ Ai , α, β ∈ CAi , w ∈ W) Lograr(G∗ Ai , α, w) ⇒ Eval(Ai, α, w) ≥ Eval(Ai, β, w) Propiedad 2. La función de evaluación debe proyectar una acción que cause la finalización de un objetivo o subobjetivo a un valor que sea mayor que TrH (una acción altamente beneficiosa). (∀Ai, G∗ Ai ∈ GAi , α ∈ CAi , w ∈ W, g∗ GAi ∈ gGAi ) Lograr(G∗ Ai , α, w) ∨ Lograr(g∗ GAi , α, w) ⇒ Eval(Ai, α, w) ≥ TrH. Definición 5. La acción de conjunto Definimos una acción de conjunto (SetAction) como un conjunto de operaciones de acción (ya sean acciones complejas o básicas) de algunos conjuntos de acción CAi y CAj que, según la creencia del agente Ai, están unidas por una relación temporal y consecuente, formando una cadena de eventos (acción y su acción consecuente siguiente). (∀α1 , . . . , αu ∈ CAi , β1 , . . . , βv ∈ CAj , w ∈ W) SetAction(α1 , . . . , αu , β1 , . . . , βv , w) ⇒ ((Do(Ai, α1 , Tα1 , w) ⇒ Do(Aj, β1 , Tβ1 , w)) ⇒ Do(Ai, α2 , Tα2 , w) ⇒ . . . ⇒ Do(Ai, αu , Tαu , w)) La relación consecuente podría existir debido a varias restricciones ambientales (cuando una acción obliga al adversario a responder con una acción específica) o debido al conocimiento de los agentes sobre el perfil de su adversario. Propiedad 3. A medida que aumenta el conocimiento que tenemos sobre nuestro adversario, tendremos creencias adicionales sobre su comportamiento en diferentes situaciones, lo que a su vez crea un nuevo conjunto de acciones. Formalmente, si nuestro AdvKnow en el tiempo Tn+1 es mayor que AdvKnow en el tiempo Tn, entonces cada SetAction conocido en el tiempo Tn también es conocido en el tiempo Tn+1. AdvKnow(P Aj Ai , Tn+1) > AdvKnow(P Aj Ai , Tn) ⇒ (∀α1 , . . . , αu ∈ CAi , β1 , . . . , βv ∈ CAj ) Bel(Aag, SetAction(α1 , . . . , αu , β1 , . . . , βv ), Tn) ⇒ Bel(Aag, SetAction(α1 , . . . , αu , β1 , . . . , βv ), Tn+1) 2.3 La formulación del entorno Los siguientes axiomas proporcionan la definición formal para un Entorno Adversarial (AE) simple y de suma cero. La satisfacción de estos axiomas significa que el agente está situado en un entorno así. Proporciona especificaciones para que el agente Aag interactúe con su conjunto de adversarios A con respecto a los objetivos G∗ Aag y G∗ A en el tiempo TCo en algún estado del mundo w. AE(Aag, A, G∗ Aag, A1, . . . , Ak, G∗ A1, . . . , G∗ Ak, Tn, w) 1. Aag tiene un Int.Th su objetivo se completaría: (∃α ∈ CAag , Tα) Int.Th(Aag, Lograr(G∗ Aag , α), Tn, Tα, AE) 2. Aag cree que él y cada uno de sus adversarios Ao están persiguiendo metas completamente conflictivas: (∀Ao ∈ {A1, . . . , Ak}) Bel(Aag, FulConf(G∗ Aag , G∗ Ao ), Tn) 3. Aag cree que cada uno de sus adversarios en Ao tiene la Int.Th su objetivo en conflicto G∗ Aoi se completará: (∀Ao ∈ {A1, . . . , Ak})(∃β ∈ CAo , Tβ) Bel(Aag, Int.Th(Ao, Lograr(G∗ Ao , β), TCo, Tβ, AE), Tn) 4. Para construir un agente que pueda operar con éxito dentro de un entorno de adversarios estratégicos, debemos especificar pautas de comportamiento para sus interacciones. Utilizar una estrategia de maximización de Eval ingenua hasta cierta profundidad de búsqueda no siempre dará resultados satisfactorios por varias razones: (1) el problema del horizonte de búsqueda al buscar a una profundidad fija; (2) la fuerte suposición de un adversario óptimamente racional con recursos ilimitados; (3) utilizar una función de evaluación estimada que no dará resultados óptimos en todos los estados del mundo, y que puede ser explotada [9]. Los siguientes axiomas especifican los principios de comportamiento que se pueden utilizar para diferenciar entre agentes exitosos y menos exitosos en el entorno adversarial mencionado anteriormente. Esos axiomas deben ser utilizados como principios de especificación al diseñar e implementar agentes que deben ser capaces de desempeñarse bien en entornos adversarios. Los axiomas conductuales representan situaciones en las que el agente adoptará intenciones potenciales para (Pot.Int.To(...)) realizar una acción, lo que típicamente requerirá un razonamiento de medios-fines para seleccionar un posible curso de acción. Este razonamiento llevará a la adopción de un Int.To(...) (ver [4]). A1. Axioma para lograr metas. El primer axioma es el caso más simple; cuando el agente Aag cree que está a una acción (α) de lograr su objetivo en conflicto G∗ Aag, debería adoptar la intención potencial de hacer α y completar su objetivo. (∀Aag, α ∈ CAag, Tn, Tα, w ∈ W) (Bel(Aag, Do(Aag, α, Tα, w) ⇒ Achieve(G∗ Aag, α, w)) ⇒ Pot.Int.To(Aag, α, Tn, Tα, w) Este comportamiento algo trivial es el primer y más fuerte axioma. En cualquier situación, cuando el agente está a una acción de completar el objetivo, debería completar la acción. Cualquier función de evaluación justa clasificaría naturalmente a α como la acción de valor máximo (propiedad 1). Sin embargo, sin una axiomatización explícita de dicho comportamiento, podría haber situaciones en las que el agente decida tomar otra acción por diversas razones, debido a sus recursos de decisión limitados. A2. Axioma del Acto Preventivo. En una situación adversarial, el agente Aag podría decidir tomar acciones que dañen uno de los planes de su adversario para completar su objetivo, incluso si esas acciones no avanzan explícitamente a Aag hacia su objetivo en conflicto G∗ Aag. Tal acción preventiva tendrá lugar cuando el agente Aag tenga la creencia sobre la posibilidad de que su adversario Ao realice una acción β que le otorgará un alto 552 The Sixth Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) valor de evaluación de utilidad (> TrH). Creer que tomar la acción α evitará que el oponente realice su β, adoptará una intención potencial de hacer α. (∀Aag, Ao ∈ A, α ∈ CAag , β ∈ CAo , Tn, Tβ, w ∈ W) (Bel(Aag, Do(Ao, β, Tβ, w) ∧ Eval(Ao, β, w) > TrH, Tn) ∧ Bel(Aag, Do(Aag, α, Tα, w) ⇒ ¬Do(Ao, β, Tβ, w), Tn) ⇒ Pot.Int.To(Aag, α, Tn, Tα, w) Este axioma es un componente básico de cualquier entorno adversarial. Por ejemplo, al observar un juego de ajedrez, un jugador podría darse cuenta de que está a punto de ser jaque mateado por su oponente, por lo tanto, realiza una jugada preventiva. Otro ejemplo es el juego de Cuatro en Línea: cuando un jugador tiene una fila de tres fichas, su oponente debe bloquearla o perder. Una instancia específica de A1 ocurre cuando el adversario está a una acción de lograr su objetivo, y la acción preventiva inmediata debe ser tomada por el agente. Formalmente, tenemos las mismas creencias que se mencionaron anteriormente, con la creencia modificada de que realizar la acción β hará que el agente Ao logre su objetivo. Proposición 1: Caso de prevención o pérdida. (∀Aag, Ao ∈ A, α ∈ CAag , β ∈ CAo , G∗ Ao , Tn, Tα, Tβ, w ∈ W) Bel(Aag, Do(Ao, β, Tβ, w) ⇒ Achieve(G∗ Ao , β, w), Tn) ∧ Bel(Aag, Do(Aag, α, Tα, w) ⇒ ¬Do(Ao, β, Tβ, w)) ⇒ Pot.Int.To(Aag, α, Tn, Tα, w) Esquema de prueba: La Proposición 1 puede derivarse fácilmente del axioma A1 y de la propiedad 2 de la función Eval, que establece que cualquier acción que cause la finalización de un objetivo es una acción altamente beneficiosa. El comportamiento del acto preventivo ocurrirá implícitamente cuando la función Eval sea igual a la función de utilidad del mundo real. Sin embargo, al ser agentes racionales limitados y al tratar con una función de evaluación estimada, necesitamos axiomatizar explícitamente dicho comportamiento, ya que no siempre ocurrirá implícitamente a partir de la función de evaluación. A3. Axioma de Movimiento Táctico Subóptimo. En muchos escenarios, puede ocurrir una situación en la que un agente decida no tomar la acción más beneficiosa actual que puede tomar (la acción con el valor de evaluación de utilidad máximo), porque cree que tomar otra acción (con un valor de evaluación de utilidad más bajo) podría generar (dependiendo de la respuesta del adversario) una posibilidad futura para una acción altamente beneficiosa. Esto ocurrirá con mayor frecuencia cuando la función Eval sea inexacta y difiera en gran medida de la función de Utilidad. De manera formal, el agente Aag cree en una cierta SetAction que evolucionará de acuerdo con su acción inicial y producirá un valor beneficioso alto (> TrH) únicamente para él. (∀Aag, Ao ∈ A, Tn, w ∈ W) (∃α1 , . . . , αu ∈ CAi , β1 , . . . , βv ∈ CAj , Tα1 ) Bel(Aag, SetAction(α1 , . . . , αu , β1 , . . . , βv ), Tn) ∧ Bel(Aag, Eval(Ao, βv , w) < TrH < Eval(Aag, αu , w), Tn) ⇒ Pot.Int.To(Aag, α1 , Tn, Tα1 , w) Un agente podría creer que una cadena de eventos ocurrirá por diversas razones debido a la naturaleza inevitable del dominio. Por ejemplo, en el ajedrez, a menudo observamos lo siguiente: una jugada provoca una posición de jaque, lo que a su vez limita los movimientos del oponente para evitar el jaque, a lo que el primer jugador podría reaccionar con otro jaque, y así sucesivamente. El agente también podría creer en una cadena de eventos basada en su conocimiento del perfil de su adversario, lo que le permite prever los movimientos del adversario con alta precisión. A4. Axioma de Detección de Perfiles. El agente puede ajustar los perfiles de sus adversarios mediante observaciones y estudio de patrones (específicamente, si hay encuentros repetidos con el mismo adversario). Sin embargo, en lugar de esperar a que se revele la información del perfil, un agente también puede iniciar acciones que obligarán a su adversario a reaccionar de una manera que revele conocimiento del perfil sobre él. Formalmente, el axioma establece que si todas las acciones (γ) no son acciones altamente beneficiosas (< TrH), el agente puede realizar la acción α en el tiempo Tα si cree que resultará en una acción no altamente beneficiosa β por parte de su adversario, lo que a su vez le enseña sobre el perfil del adversario, es decir, proporciona un mayor AdvKnow(P Aj Ai , Tβ). (∀Aag, Ao ∈ A, α ∈ CAag , β ∈ CAo , Tn, Tα, Tβ, w ∈ W) Bel(Aag, (∀γ ∈ CAag )Eval(Aag, γ, w) < TrH, Tn) ∧ Bel(Aag, Do(Aag, α, Tα, w) ⇒ Do(Ao, β, Tβ, w), Tn) ∧ Bel(Aag, Eval(Ao, β, w) < TrH) ∧ Bel(Aag, AdvKnow(P Aj Ai , Tβ) > AdvKnow(P Aj Ai , Tn), Tn) ⇒ Pot.Int.To(Aag, α, Tn, Tα, w) Por ejemplo, volviendo al escenario del juego de ajedrez, consideremos comenzar una partida contra un oponente del que no sabemos nada, ni siquiera si es humano o un oponente computarizado. Podríamos comenzar a jugar una estrategia que sea adecuada frente a un oponente promedio, y ajustar nuestro juego de acuerdo a su nivel de juego. A5. Axioma de Formación de Alianzas El siguiente axioma de comportamiento es relevante únicamente en una instancia multilateral del entorno adversarial (obviamente, una alianza no puede formarse en un encuentro bilateral de suma cero). En diferentes situaciones durante una interacción multilateral, un grupo de agentes podría creer que es de su interés formar una alianza temporal. Una alianza de este tipo es un acuerdo que limita el comportamiento de sus miembros, pero se cree por parte de estos que les permite alcanzar un valor de utilidad mayor al que podrían lograr fuera de la alianza. Como ejemplo, podemos observar el clásico juego de mesa Risk, donde cada jugador tiene como objetivo individual ser el único conquistador del mundo, un juego de suma cero. Sin embargo, para lograr este objetivo, podría ser estratégicamente sabio hacer acuerdos de alto el fuego a corto plazo con otros jugadores, o unirse y atacar a un oponente que sea más fuerte que el resto. Los términos de una alianza definen la forma en que sus miembros deben actuar. Es un conjunto de predicados, denominados Términos, que es acordado por los miembros de la alianza y debe mantenerse verdadero durante la duración de la alianza. Por ejemplo, el conjunto de términos en el escenario de riesgo podría contener los siguientes predicados: 1. Los miembros de la alianza no se atacarán entre sí en los territorios X, Y y Z; 2. Los miembros de la alianza contribuirán C unidades por turno para atacar al adversario Ao; 3. Los miembros están obligados a permanecer como parte de la alianza hasta el tiempo Tk o hasta que el ejército adversario Ao sea más pequeño que Q. El conjunto de términos especifica restricciones intergrupales en cada uno de los miembros de la alianza (∀Aal i ∈ Aal ⊆ A) conjunto de acciones Cal i ⊆ C. Definición 6. El valor total de evaluación que el agente Ai logrará al ser parte de Aal es la suma de Evali (Eval para Ai) de cada uno de los valores de evaluación de Aal j después de tomar sus propias acciones α (a través del predicado del agente(α)): Al val(Ai, Cal, Aal, w) = α∈Cal Evali(Aal j, agente(α), w) Definición 7. Al TrH - es un número que representa un valor de Al en la Sexta Conferencia Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 553 umbral; por encima de él, se puede decir que la alianza es una alianza altamente beneficiosa. El valor de Al TrH se calculará dinámicamente de acuerdo con el progreso de la interacción, como se puede ver en [7]. Después de formarse una alianza, sus miembros ahora están trabajando en su entorno adversarial normal, así como de acuerdo con los estados mentales y axiomas requeridos para sus interacciones como parte de la alianza. El siguiente modelo de Alianza (AL) especifica las condiciones bajo las cuales el grupo Aal puede considerarse en una alianza y trabajando con un nuevo y limitado conjunto de acciones Cal, en el tiempo Tn. AL(Aal , Cal , w, Tn) 1. \n\nAL(Aal , Cal , w, Tn) 1. Aal tiene un MB en el que todos los miembros son parte de Aal: MB(Aal, (∀Aal i ∈ Aal)miembro(Aal i, Aal), Tn) 2. Aal tiene una MB que el grupo se mantenga: MB(Aal , (∀Aal i ∈ Aal )Int.Th (Ai, miembro(Ai, Aal ), Tn, Tn+1, Co), Tn) 3. Aal tiene un beneficio marginal que al ser miembros les otorga un alto valor de utilidad: MB(Aal , (∀Aal i ∈ Aal )Al val(Aal i , Cal , Aal , w) ≥ Al TrH, Tn) Los perfiles de los miembros son una parte crucial de las alianzas exitosas. Suponemos que los agentes que tienen perfiles más precisos de sus adversarios serán más exitosos en tales entornos. Tales agentes podrán predecir cuándo un miembro está a punto de violar el contrato de alianzas (elemento 2 en el modelo anterior) y tomar medidas correctivas (cuando el elemento 3 se falsificará). La robustez de la alianza depende en parte de la medida de confianza de sus miembros, la estimación de su posición objetiva y otras propiedades de su perfil. Debemos tener en cuenta que un agente puede formar parte simultáneamente de más de una alianza. Una alianza temporal, donde los miembros del grupo no tienen un objetivo común pero actúan colaborativamente en interés de sus propias metas individuales, es clasificada como un Grupo de Tratamiento por los psicólogos modernos (en contraste con un Grupo de Tarea, donde sus miembros tienen un objetivo común). El modelo de Actividad Compartida presentado en [5] modeló el comportamiento del Grupo de Tratamiento utilizando la misma formalización de Planes Compartidos. Al comparar ambas definiciones de una alianza y un Grupo de Tratamiento, encontramos una similitud sorprendente entre ambos modelos: las definiciones de los modelos de entorno son casi idénticas (ver las definiciones de SAs en [5]), y sus axiomas de Acto Egoísta y Acto Cooperativo se ajustan al comportamiento de nuestros agentes adversarios. La principal distinción entre ambos modelos es la integración de un axioma de acto de comportamiento útil, en la Actividad Compartida que no puede formar parte de la nuestra. Este axioma establece que un agente considerará tomar una acción que disminuirá su valor de Evaluación (hasta un cierto límite inferior), si cree que un compañero del grupo obtendrá un beneficio significativo. Dicho comportamiento no puede ocurrir en un entorno puramente adversarial (como un juego de suma cero), donde los miembros de la alianza están constantemente alerta para manipular su alianza en su propio beneficio. A6. Axioma de Maximización de la Evaluación. En un caso en el que todos los demás axiomas no sean aplicables, procederemos con la acción que maximice el valor heurístico calculado en la función Eval. (∀Aag, Ao ∈ A, α ∈ Cag, Tn, w ∈ W) Bel(Aag, (∀γ ∈ Cag)Eval(Aag, α, w) ≥ Eval(Aag, γ, w), Tn) ⇒ Pot.Int.To(Aag, α, Tn, Tα, w) T1. Optimalidad en Eval = Utilidad El modelo axiomático anterior aborda situaciones en las que la Utilidad es desconocida y los agentes son agentes racionales limitados. El siguiente teorema muestra que en interacciones bilaterales, donde los agentes tienen la función de Utilidad real (es decir, Eval = Utilidad) y son agentes racionales, los axiomas proporcionan el mismo resultado óptimo que la búsqueda adversarial clásica (por ejemplo, Min-Max). Teorema 1. Que Ae ag sea un agente AE racional ilimitado que utiliza la función de evaluación heurística Eval, Au ag sea el mismo agente utilizando la verdadera función de utilidad y Ao sea un único adversario racional ilimitado basado en utilidad. Dado que Eval = Utilidad: (∀α ∈ CAu ag , α ∈ CAe ag , Tn, w ∈ W) Pot.Int.To(Au ag, α, Tn, Tα, w) → Pot.Int.To(Ae ag, α , Tn, Tα, w) ∧ ((α = α ) ∨ (Utilidad(Au ag, α, w) = Eval(Ae ag, α , w))) Esquema de prueba - Dado que Au ag tiene la función de utilidad real y recursos ilimitados, puede generar el árbol de juego completo y ejecutar el algoritmo MinMax óptimo para elegir la acción de mayor valor de utilidad, que denotamos como α. La prueba mostrará que Ae ag, utilizando los axiomas AE, seleccionará la misma utilidad α o igual (cuando hay más de una acción con la misma utilidad máxima) cuando Eval = Utilidad. (A1) Axioma de logro de objetivo - supongamos que hay un α tal que su cumplimiento logrará el objetivo Au ags. Obtendrá la utilidad más alta mediante Min-Max para Au ag. El agente Ae seleccionará α u otra acción con el mismo valor de utilidad a través de A1. Si tal α no existe, Ae ag no puede aplicar este axioma y procede a A2. (A2) Axioma de la acción preventiva - (1) Observando el caso base (ver Prop1), si hay un β que lleva a Ao a alcanzar su objetivo, entonces una acción preventiva α producirá la mayor utilidad para Au ag. Au ag lo elegirá a través de la utilidad, mientras que Ae ag lo elegirá a través de A2. En el caso general, β es una acción altamente beneficiosa para Ao, lo que resulta en una baja utilidad para Au ag, lo que lo guiará a seleccionar un α que evitará β, mientras que Ae ag lo elegirá a través de A2.1 Si dicho β no existe para Ao, entonces A2 no es aplicable, y Ae ag puede proceder a A3. (A3) Axioma de movimiento táctico subóptimo: al usar una función de evaluación heurística, Ae ag tiene una creencia parcial en el perfil de su adversario (elemento 4 en el modelo AE), lo que puede llevarlo a creer en SetActions (Prop1). En nuestro caso, Ae ag tiene un perfil completo de su adversario óptimo y sabe que Ao se comportará de manera óptima según los valores reales de utilidad en el árbol de búsqueda completo, por lo tanto, cualquier creencia sobre una SetAction subóptima no puede existir, lo que hace que este axioma sea inaplicable. Ae ag procederá a A4. (A4) Axioma de detección de perfil - Dado que Ae ag tiene el perfil completo de Ao, ninguna de las acciones de Ae ag puede aumentar su conocimiento. Ese axioma no se aplicará, y el agente procederá con A6 (A5 será ignorado porque la interacción es bilateral). (A6) Axioma de maximización de la evaluación: este axioma seleccionará la máxima Evaluación para Ae ag. Dado que Eval = Utilidad, se seleccionará el mismo α que fue seleccionado por Au ag. 3. EVALUACIÓN El propósito principal de nuestro análisis experimental es evaluar el comportamiento y rendimiento de los modelos en un entorno adversarial real. Esta sección investiga si un caso acotado 1 en el que, tras la finalización de β, exista un γ que proporcione una alta utilidad para el Agente Au ag, no puede ocurrir porque Ao utiliza la misma utilidad, y la existencia de γ hará que clasifique a β como una acción de baja utilidad. 554 The Sixth Intl. En la Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07), los agentes racionales situados en entornos adversariales se beneficiarán más al aplicar nuestros axiomas de comportamiento sugeridos. 3.1 El Dominio Para explorar el uso del modelo anterior y sus axiomas de comportamiento, decidimos utilizar el juego de Conecta Cuatro como nuestro entorno adversarial. Connect-Four es un juego de 2 jugadores, de suma cero, que se juega utilizando un tablero similar a una matriz de 6x7. En cada turno, un jugador coloca un disco en una de las 7 columnas (el conjunto de 21 discos suele ser de color amarillo para el jugador 1 y rojo para el jugador 2; usaremos Blanco y Negro respectivamente para evitar confusiones). El ganador es el primer jugador en completar un conjunto de cuatro discos de su color en horizontal, vertical o diagonal. En ocasiones muy raras, el juego podría terminar en empate si todos los cuadros vacíos están llenos, pero ningún jugador logró crear un conjunto de 4 discos. El juego de Conecta Cuatro fue resuelto en [1], donde se muestra que el primer jugador (jugando con los discos blancos) puede forzar una victoria comenzando en la columna del medio (columna 4) y jugando de manera óptima. Sin embargo, la estrategia óptima es muy compleja y difícil de seguir incluso para agentes racionales limitados complejos, como los jugadores humanos. Antes de que podamos proceder a verificar el comportamiento del agente, primero debemos verificar que el dominio se ajusta a la definición de entornos adversarios mencionada anteriormente (en la que se basan los axiomas de comportamiento). Primero, al jugar al juego de Conecta Cuatro, el agente tiene la intención de ganar el juego (ítem 1). Segundo (ítem 2), nuestro agente cree que en Conecta Cuatro solo puede haber un ganador (o ningún ganador en la rara ocasión de un empate). Además, nuestro agente cree que su oponente en el juego intentará ganar (ítem 3), y esperamos que tenga algún conocimiento parcial (ítem 4) sobre su adversario (este conocimiento puede variar desde nada, pasando por hechos simples como la edad, hasta estrategias y debilidades). Por supuesto, no todos los encuentros de Conecta Cuatro son adversariales. Por ejemplo, cuando un padre está jugando el juego con su hijo, la siguiente situación podría ocurrir: el niño, con un fuerte incentivo para ganar, trata el entorno como adversarial (intenta ganar, entiende que solo puede haber un ganador y cree que su padre está tratando de vencerlo). Sin embargo, desde el punto de vista de los padres, el entorno podría ser visto como educativo, donde el objetivo no es ganar el juego, sino causar disfrute o practicar el razonamiento estratégico. En un entorno educativo como este, un nuevo conjunto de axiomas de comportamiento podría ser más beneficioso para los objetivos de los padres que nuestros axiomas de comportamiento adversarial sugeridos. 3.2 Análisis de Axiomas Después de demostrar que el juego de Conecta Cuatro es de hecho un entorno adversarial bilateral de suma cero, el siguiente paso es observar el comportamiento de los jugadores durante el juego y verificar si comportarse de acuerdo con nuestro modelo mejora el rendimiento. Para hacerlo, hemos recopilado archivos de registro de juegos de Conecta Cuatro completados que fueron jugados por jugadores humanos a través de Internet. Nuestros datos de archivo de registro recopilados provienen de sitios de Juego por Correo Electrónico (PBeM). Estos son sitios web que alojan juegos por correo electrónico, donde cada movimiento se realiza a través de un intercambio de correos electrónicos entre el servidor y los jugadores. Muchos de estos sitios de archivos contienen interacciones competitivas reales y también mantienen un sistema de clasificación para sus miembros. La mayoría de los datos que utilizamos se pueden encontrar en [6]. Como se puede aprender de [1], Conecta Cuatro tiene una estrategia óptima y una ventaja considerable para el jugador que comienza el juego (a quien llamamos jugador Blanco). Nos concentraremos en nuestro análisis en los movimientos de los segundos jugadores (a los que llamaremos Negras). El jugador Blanco, al ser el primero en actuar, tiene la llamada ventaja de la iniciativa. Tener la ventaja y una buena estrategia mantendrá al jugador Negro ocupado reaccionando a sus movimientos, en lugar de iniciar amenazas. Una amenaza es una combinación de tres discos del mismo color, con un espacio vacío para el cuarto disco ganador. Una amenaza abierta es una amenaza que puede materializarse en el próximo movimiento del oponente. Para que el jugador Negro gane, debe de alguna manera cambiar el rumbo, tomar la ventaja y comenzar a presentar amenazas al jugador Blanco. Exploraremos el comportamiento de los jugadores negros y su conformidad con nuestros axiomas. Para hacerlo, construimos una aplicación que lee archivos de registro y analiza los movimientos de los jugadores Negros. La aplicación contiene dos componentes principales: (1) un algoritmo Min-Max para la evaluación de movimientos; (2) un detector de amenazas abiertas para descubrir amenazas abiertas. El algoritmo Min-Max funcionará hasta una profundidad d dada y, para cada movimiento α, devolverá el valor heurístico para la próxima acción tomada por el jugador según lo escrito en el archivo de registro, h(α), junto con el valor heurístico máximo, maxh(α), que podría lograrse antes de realizar el movimiento (obviamente, si h(α) = maxh(α), entonces el jugador no realizó el movimiento óptimo desde el punto de vista heurístico). El trabajo de los detectores de amenazas es notificar si se tomó alguna acción para bloquear una amenaza abierta (no bloquear una amenaza abierta probablemente hará que el jugador pierda en el próximo movimiento del oponente). La función heurística utilizada por Min-Max para evaluar la utilidad de los jugadores es la siguiente función, que es simple de calcular, pero proporciona un desafío razonable a los oponentes humanos: Definición 8. Sea Grupo un conjunto adyacente de cuatro cuadrados que son horizontales, verticales o diagonales. El Grupo n (Grupo w) es un Grupo con n piezas de color negro (blanco) y 4−n casillas vacías. h = ((Grupo1 negro ∗α)+(Grupo2 negro ∗β)+(Grupo3 negro ∗γ)+(Grupo4 negro ∗∞)) − ((Grupo1 blanco ∗α)+(Grupo2 blanco ∗β)+(Grupo3 blanco ∗γ)+(Grupo4 blanco ∗∞)) Los valores de α, β y δ pueden variar para formar cualquier combinación lineal deseada; sin embargo, es importante asignarles valores teniendo en cuenta el orden α < β < δ (usamos 1, 4 y 8 como sus valores respectivos). Grupos de 4 discos del mismo color significan victoria, por lo tanto, el descubrimiento de dicho grupo resultará en ∞ para asegurar un valor extremo. Ahora utilizamos nuestra función de evaluación estimada para evaluar las acciones de los jugadores Negros durante la interacción adversarial de Conecta Cuatro. Cada juego del archivo de registro fue introducido en la aplicación, la cual procesó y generó un archivo de registro reformateado que contenía el valor h del movimiento actual, el valor maxh que podría ser alcanzado y una notificación si se detectaba una amenaza abierta. Un total de 123 partidas fueron analizadas (57 con victoria de las blancas y 66 con victoria de las negras). Algunos juegos adicionales fueron ignorados manualmente en el experimento, debido a estos problemas: un jugador abandonando el juego mientras el resultado no es definitivo, o una jugada irracional y contundente en las primeras etapas del juego (por ejemplo, no bloquear un grupo obviamente ganador en las primeras jugadas de apertura). Además, también se eliminó un empate. El simulador se ejecutó con una profundidad de búsqueda de 3 movimientos. Ahora procedemos a analizar los juegos con respecto a cada axioma de comportamiento. El Sexto Internacional. La siguiente sección presenta las evaluaciones heurísticas del algoritmo Min-Max para cada acción, y verifica la cantidad y el alcance de las acciones tácticas subóptimas y sus implicaciones en el rendimiento. La Tabla 1 muestra los resultados y las percepciones del análisis heurístico de los juegos, cuando la profundidad de búsqueda es igual a 3 (esta profundidad de búsqueda fue seleccionada para que los resultados sean comparables a [9], ver Sección 3.2.3). La información heurística de las tablas es la diferencia entre el valor heurístico máximo presente y el valor heurístico de la acción que finalmente tomó el jugador (es decir, cuanto más cercano sea el número a 0, más cercana estuvo la acción a la acción heurística máxima). La primera fila presenta los valores de diferencia de la acción que tuvo el valor de diferencia máximo entre todas las acciones de los jugadores Negros en un juego dado, promediado en todos los juegos ganados y perdidos por los Negros (ver columnas respectivas). En los juegos en los que el jugador Negro pierde, su valor promedio de diferencia fue de -17.62, mientras que en los juegos en los que el jugador Negro ganó, su promedio fue de -12.02. La segunda fila amplía el análisis al considerar las 3 acciones con mayor diferencia heurística, y promediarlas. En ese caso, notamos una diferencia heurística promedio de 5 puntos entre los juegos en los que el jugador Negro pierde y los juegos en los que gana. Sin embargo, la importancia de esos números es que nos permitieron hacer una suposición educada sobre un número umbral de 11.5, como el valor de la constante TrH, que diferencia entre acciones normales y altamente beneficiosas. Después de encontrar una constante TrH aproximada, podemos proceder con un análisis de la importancia de los movimientos subóptimos. Para hacerlo, tomamos el subconjunto de juegos en los que el valor mínimo de diferencia heurística para las acciones de las negras era de 11.5. Como se presenta en la Tabla 2, podemos ver el promedio de 3 horas mínimo de los 3 mayores rangos y el porcentaje respectivo de juegos ganados. La primera fila muestra que el jugador Negro ganó solo el 12% de los juegos en los que el promedio de sus 3 acciones con mayor diferencia heurística (min3 h) era menor que el umbral sugerido, TrH = 11.5. La segunda fila muestra un resultado sorprendente: parece que cuando min3 h > −4, el jugador Negro rara vez gana. La intuición sugeriría que los juegos en los que los valores de evaluación de la acción estén más cerca de los valores máximos resultarán en más juegos ganados para las piezas negras. Sin embargo, parece que en el dominio de Conecta Cuatro, simplemente responder con acciones algo fácilmente esperadas, sin iniciar algunos movimientos sorprendentes y subóptimos, no produce buenos resultados. La última fila resume las principales conclusiones del análisis; la mayoría de las victorias de Blacks (83%) se produjeron cuando su min3 h estaba en el rango de -11.5 a -4. Una inspección detallada de esos juegos ganados por las fichas negras muestra el siguiente patrón detrás de los números: después de los movimientos de apertura estándar, las fichas negras de repente colocan una ficha en una columna aislada, lo cual parece ser un desperdicio de un movimiento. El blanco sigue construyendo sus amenazas, generalmente ignorando el último movimiento de las negras, que a su vez utiliza el disco aislado como ancla para una futura amenaza ganadora. Los resultados muestran que fue beneficioso para el jugador Negro en la Tabla 2: Porcentajes de ganancias de los Negros % de juegos min3 h < −11.5 12% min3 h > −4 5% −11.5 ≤ min3 h ≤ −4 83% tomar acciones subóptimas y no dar el valor heurístico más alto posible en ese momento, pero no será demasiado perjudicial para su posición (es decir, no dará un alto valor beneficioso a su adversario). Resultó que aprender el umbral es un aspecto importante del éxito: realizar movimientos extremadamente arriesgados (min3 h < −11.5) o intentar evitarlos (min3 h > −4) reduce significativamente las posibilidades de ganar de los jugadores negros. 3.2.2 Afirmación del Axioma de Monitoreo de Perfiles En la tarea de demostrar la importancia de monitorear los perfiles de los adversarios, nuestros archivos de registro no pudieron ser utilizados porque no contenían interacciones repetidas entre jugadores, que son necesarias para inferir el conocimiento de los jugadores sobre sus adversarios. Sin embargo, la importancia del modelado del oponente y su uso para obtener ventajas tácticas ya ha sido estudiada en varios ámbitos ([3, 9] son buenos ejemplos). En un artículo reciente, Markovitch y Reger [9] exploraron la noción de aprendizaje y explotación de la debilidad del oponente en interacciones competitivas. Aplican estrategias de aprendizaje simples analizando ejemplos de interacciones pasadas en un dominio específico. También utilizaron el dominio adversarial de Conecta Cuatro, que ahora se puede utilizar para comprender la importancia de monitorear el perfil del adversario. Tras la presentación de su modelo teórico, describen un extenso estudio empírico y verifican el rendimiento de los agentes después de aprender el modelo de debilidades con ejemplos pasados. Uno de los dominios utilizados como entorno competitivo fue el mismo juego de Conecta Cuatro (las Damas fueron el segundo dominio). Su función heurística era idéntica a la nuestra con tres variaciones diferentes (H1, H2 y H3) que se distinguen entre sí por los valores de sus coeficientes de combinación lineal. La profundidad de búsqueda para los jugadores fue de 3 (como en nuestro análisis). Sus extensos experimentos verifican y comparan diversas estrategias de aprendizaje, factores de riesgo, conjuntos de características predefinidas y métodos de uso. La conclusión es que el dominio de Conecta Cuatro muestra una mejora de una tasa de victoria de 0.556 antes de modelar a 0.69 después de modelar (página 22). Sus conclusiones, que muestran un rendimiento mejorado al sostener y utilizar el modelo del adversario, justifican el esfuerzo de monitorear el perfil del adversario para interacciones continuas y repetidas. Un punto adicional que surgió en sus experimentos es el siguiente: después de que se haya aprendido el modelo de debilidad del oponente, los autores describen diferentes métodos para integrar el modelo de debilidad del oponente en la estrategia de decisión de los agentes. Sin embargo, independientemente del método específico que elijan para trabajar, todos los métodos de integración podrían hacer que el agente tome decisiones subóptimas; podría hacer que el agente prefiera acciones que son subóptimas en el momento actual de la decisión, pero que podrían hacer que el oponente reaccione de acuerdo con su modelo de debilidad (como lo representa nuestro agente), lo cual a su vez será beneficioso para nosotros en el futuro. El comportamiento de los agentes, como se demuestra en [9], confirma y refuerza aún más nuestro Axioma Táctico Subóptimo discutido en la sección anterior. 556 The Sixth Intl. La necesidad de los axiomas de Logro de Objetivos, Acto Preventivo y Maximización de la Evaluación es evidente y no requiere más verificación. Sin embargo, incluso con respecto a esos axiomas, surgieron algunas ideas interesantes en el análisis de registros. Los axiomas de Logro y Prevención de Actos, aunque teóricamente triviales, parecen presentar un desafío para un jugador humano. En la inspección inicial de los registros, nos encontramos con algunos juegos donde un jugador, por razones inexplicables, no bloqueó al otro para que ganara o no logró ejecutar su propia jugada ganadora. Podemos atribuir esas fallas a la falta de atención de los humanos, o a un error de escritura en su respuesta al movimiento; sin embargo, esos errores podrían ocurrir en agentes racionales limitados, y el comportamiento apropiado debe ser axiomatizado. Un juego típico de Cuatro en Línea gira en torno a generar amenazas y bloquearlas. En nuestro análisis buscamos acciones preventivas explícitas, es decir, movimientos que bloqueen un grupo de 3 discos o que eliminen una amenaza futura (en nuestro horizonte de búsqueda limitado). Encontramos que en el 83% de los juegos totales, hubo al menos una acción preventiva tomada por el jugador Negro. También se encontró que Black promedió 2.8 acciones preventivas por juego en los juegos en los que perdió, mientras que promedió 1.5 acciones preventivas por juego al ganar. Parece que Black necesita 1 o 2 acciones preventivas para construir su posición inicial de toma, antes de comenzar a presentar amenazas. Si no logra ganar, generalmente evitará una amenaza adicional o dos antes de sucumbir ante el Blanco. 4. TRABAJO RELACIONADO Mucha investigación se ocupa de la axiomatización del trabajo en equipo y los estados mentales de los individuos: algunos modelos utilizan el conocimiento y la creencia [10], otros tienen modelos de metas e intenciones [8, 4]. Sin embargo, todas estas teorías formales tratan sobre el trabajo en equipo y la cooperación de agentes. Hasta donde sabemos, nuestro modelo es el primero en proporcionar un modelo formalizado para entornos adversarios explícitos y el comportamiento de los agentes en él. El algoritmo de búsqueda adversarial clásico Min-Max fue el primer intento de integrar al oponente en el espacio de búsqueda con la suposición débil de un oponente que juega de manera óptima. Desde entonces, se ha dedicado mucho esfuerzo a integrar el modelo del oponente en el procedimiento de toma de decisiones para predecir el comportamiento futuro. El algoritmo M∗ presentado por Carmel y Markovitch [2] mostró un método para incorporar modelos de oponentes en la búsqueda de adversarios, mientras que en [3] utilizaron el aprendizaje para proporcionar un modelo de oponente más preciso en un entorno de juego repetido de 2 jugadores, donde las estrategias de los agentes se modelaron como autómatas finitos. El trabajo adicional de planificación adversarial fue realizado por Willmott y colaboradores [13], quienes proporcionaron un enfoque de planificación adversarial para el juego de GO. La investigación mencionada anteriormente trató sobre la búsqueda adversarial y la integración de modelos de oponentes en métodos de búsqueda de utilidad clásicos. Ese trabajo muestra la importancia del modelado del oponente y la capacidad de explotarlo en beneficio de un agente. Sin embargo, las limitaciones básicas de esos métodos de búsqueda siguen aplicándose; nuestro modelo intenta superar esas limitaciones presentando un modelo formal para una nueva especificación adversarial basada en estados mentales. 5. CONCLUSIONES Presentamos un modelo de Entorno Adversarial para un agente racional acotado que está situado en un entorno de N jugadores, de suma cero. Estos fueron posteriormente eliminados del análisis final. Utilizamos la formalización de SharedPlans para definir el modelo y los axiomas que los agentes pueden aplicar como pautas de comportamiento. El modelo está destinado a ser utilizado como una guía para diseñar agentes que necesitan operar en entornos adversos. Presentamos resultados empíricos, basados en el análisis del archivo de registro de ConnectFour, que ejemplifican el modelo y los axiomas para una instancia bilateral del entorno. Los resultados que presentamos son un primer paso hacia un modelo ampliado que abarcará todos los tipos de entornos adversarios, por ejemplo, entornos que no son de suma cero y entornos que contienen agentes naturales que no forman parte del conflicto directo. Esos desafíos y más serán abordados en futuras investigaciones. 6. AGRADECIMIENTO Esta investigación fue apoyada en parte por las becas de la Fundación de Ciencia de Israel #1211/04 y #898/05. 7. REFERENCIAS [1] L. V. Allis. Un enfoque basado en el conocimiento de Conecta Cuatro: el juego está resuelto, gana el jugador Blanco. Tesis de maestría, Universidad Libre de Ámsterdam, Países Bajos, 1988. [2] D. Carmel y S. Markovitch. Incorporando modelos de oponentes en la búsqueda de adversarios. En Actas de la Decimotercera Conferencia Nacional de Inteligencia Artificial, páginas 120-125, Portland, Oregón, 1996. [3] D. Carmel y S. Markovitch. Modelado de oponentes en sistemas multiagente. En G. Weiß y S. Sen, editores, Adaptación y Aprendizaje en Sistemas Multiagente, páginas 40-52. Springer-Verlag, 1996. [4] B. J. Grosz y S. Kraus. Planes colaborativos para acciones grupales complejas. Inteligencia Artificial, 86(2):269-357, 1996. [5] M. Hadad, G. Kaminka, G. Armon y S. Kraus. Apoyando la actividad colaborativa. En Proc. de AAAI-2005, páginas 83-88, Pittsburgh, 2005. [6] http://www.gamerz.net/˜pbmserv/. [7] S. Kraus y D. Lehmann. Diseñando y construyendo un agente automatizado de negociación. Inteligencia Computacional, 11:132-171, 1995. [8] H. J. Levesque, P. R. Cohen y J. H. T. Nunes. Actuar juntos. En Proc. de AAAI-90, páginas 94-99, Boston, MA, 1990. [9] S. Markovitch y R. Reger. Aprender y aprovechar las debilidades relativas de los agentes oponentes. Agentes Autónomos y Sistemas Multiagente, 10(2):103-130, 2005. [10] Y. M. Ronald Fagin, Joseph Y. Halpern y M. Y. Vardi. Razonamiento sobre el conocimiento. MIT Press, Cambridge, Mass., 1995. [11] P. Thagard.\nMIT Press, Cambridge, Massachusetts, 1995. [11] P. Thagard. Resolución de problemas adversariales: Modelado de un oponente utilizando coherencia explicativa. Ciencia Cognitiva, 16(1):123-149, 1992. [12] R. W. Toseland y R. F. Rivas. Una introducción a la práctica del trabajo en grupo. Prentice Hall, Englewood Cliffs, NJ, segunda edición, 1995. [13] S. Willmott, J. Richardson, A. Bundy y J. Levine. Un enfoque de planificación adversarial para el juego de Go. Notas de conferencia en Ciencias de la Computación, 1558:93-112, 1999. El Sexto Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 557 ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "evaluation function": {
            "translated_key": "",
            "is_in_text": true,
            "original_annotated_sentences": [
                "An Adversarial Environment Model for Bounded Rational Agents in Zero-Sum Interactions Inon Zuckerman1 , Sarit Kraus1 , Jeffrey S. Rosenschein2 , Gal Kaminka1 1 Department of Computer Science 2 The School of Engineering Bar-Ilan University and Computer Science Ramat-Gan, Israel Hebrew University, Jerusalem, Israel {zukermi,sarit,galk}@cs.biu.ac.il jeff@cs.huji.ac.il ABSTRACT Multiagent environments are often not cooperative nor collaborative; in many cases, agents have conflicting interests, leading to adversarial interactions.",
                "This paper presents a formal Adversarial Environment model for bounded rational agents operating in a zero-sum environment.",
                "In such environments, attempts to use classical utility-based search methods can raise a variety of difficulties (e.g., implicitly modeling the opponent as an omniscient utility maximizer, rather than leveraging a more nuanced, explicit opponent model).",
                "We define an Adversarial Environment by describing the mental states of an agent in such an environment.",
                "We then present behavioral axioms that are intended to serve as design principles for building such adversarial agents.",
                "We explore the application of our approach by analyzing log files of completed Connect-Four games, and present an empirical analysis of the axioms appropriateness.",
                "Categories and Subject Descriptors I.2.11 [Artificial Intelligence]: Distributed Artificial Intelligence-Intelligent agents,Multiagent Systems; I.2.4 [Artificial Intelligence]: Knowledge Representation Formalisms and Methods -Modal logic General Terms Design, Theory 1.",
                "INTRODUCTION Early research in multiagent systems (MAS) considered cooperative groups of agents; because individual agents had limited resources, or limited access to information (e.g., limited processing power, limited sensor coverage), they worked together by design to solve problems that individually they could not solve, or at least could not solve as efficiently.",
                "MAS research, however, soon began to consider interacting agents with individuated interests, as representatives of different humans or organizations with non-identical interests.",
                "When interactions are guided by diverse interests, participants may have to overcome disagreements, uncooperative interactions, and even intentional attempts to damage one another.",
                "When these types of interactions occur, environments require appropriate behavior from the agents situated in them.",
                "We call these environments Adversarial Environments, and call the clashing agents Adversaries.",
                "Models of cooperation and teamwork have been extensively explored in MAS through the axiomatization of mental states (e.g., [8, 4, 5]).",
                "However, none of this research dealt with adversarial domains and their implications for agent behavior.",
                "Our paper addresses this issue by providing a formal, axiomatized mental state model for a subset of adversarial domains, namely simple zero-sum adversarial environments.",
                "Simple zero-sum encounters exist of course in various twoplayer games (e.g., Chess, Checkers), but they also exist in n-player games (e.g., Risk, Diplomacy), auctions for a single good, and elsewhere.",
                "In these latter environments especially, using a utility-based adversarial search (such as the Min-Max algorithm) does not always provide an adequate solution; the payoff function might be quite complex or difficult to quantify, and there are natural computational limitations on bounded rational agents.",
                "In addition, traditional search methods (like Min-Max) do not make use of a model of the opponent, which has proven to be a valuable addition to adversarial planning [9, 3, 11].",
                "In this paper, we develop a formal, axiomatized model for bounded rational agents that are situated in a zero-sum adversarial environment.",
                "The model uses different modality operators, and its main foundations are the SharedPlans [4] model for collaborative behavior.",
                "We explore environment properties and the mental states of agents to derive behavioral axioms; these behavioral axioms constitute a formal model that serves as a specification and design guideline for agent design in such settings.",
                "We then investigate the behavior of our model empirically using the Connect-Four board game.",
                "We show that this game conforms to our environment definition, and analyze players behavior using a large set of completed match log 550 978-81-904262-7-5 (RPS) c 2007 IFAAMAS files.",
                "In addition, we use the results presented in [9] to discuss the importance of opponent modeling in our ConnectFour adversarial domain.",
                "The paper proceeds as follows.",
                "Section 2 presents the models formalization.",
                "Section 3 presents the empirical analysis and its results.",
                "We discuss related work in Section 4, and conclude and present future directions in Section 5. 2.",
                "ADVERSARIAL ENVIRONMENTS The adversarial environment model (denoted as AE) is intended to guide the design of agents by providing a specification of the capabilities and mental attitudes of an agent in an adversarial environment.",
                "We focus here on specific types of adversarial environments, specified as follows: 1.",
                "Zero-Sum Interactions: positive and negative utilities of all agents sum to zero; 2.",
                "Simple AEs: all agents in the environment are adversarial agents; 3.",
                "Bilateral AEs: AEs with exactly two agents; 4.",
                "Multilateral AEs: AEs of three or more agents.",
                "We will work on both bilateral and multilateral instantiations of zero-sum and simple environments.",
                "In particular, our adversarial environment model will deal with interactions that consist of N agents (N ≥ 2), where all agents are adversaries, and only one agent can succeed.",
                "Examples of such environments range from board games (e.g., Chess, Connect-Four, and Diplomacy) to certain economic environments (e.g., N-bidder auctions over a single good). 2.1 Model Overview Our approach is to formalize the mental attitudes and behaviors of a single adversarial agent; we consider how a single agent perceives the AE.",
                "The following list specifies the conditions and mental states of an agent in a simple, zero-sum AE: 1.",
                "The agent has an individual intention that its own goal will be completed; 2.",
                "The agent has an individual belief that it and its adversaries are pursuing full conflicting goals (defined below)there can be only one winner; 3.",
                "The agent has an individual belief that each adversary has an intention to complete its own full conflicting goal; 4.",
                "The agent has an individual belief in the (partial) profile of its adversaries.",
                "Item 3 is required, since it might be the case that some agent has a full conflicting goal, and is currently considering adopting the intention to complete it, but is, as of yet, not committed to achieving it.",
                "This might occur because the agent has not yet deliberated about the effects that adopting that intention might have on the other intentions it is currently holding.",
                "In such cases, it might not consider itself to even be in an adversarial environment.",
                "Item 4 states that the agent should hold some belief about the profiles of its adversaries.",
                "The profile represents all the knowledge the agent has about its adversary: its weaknesses, strategic capabilities, goals, intentions, trustworthiness, and more.",
                "It can be given explicitly or can be learned from observations of past encounters. 2.2 Model Definitions for Mental States We use Grosz and Krauss definitions of the modal operators, predicates, and meta-predicates, as defined in their SharedPlan formalization [4].",
                "We recall here some of the predicates and operators that are used in that formalization: Int.To(Ai, α, Tn, Tα, C) represents Ais intentions at time Tn to do an action α at time Tα in the context of C. Int.Th(Ai, prop, Tn, Tprop, C) represents Ais intentions at time Tn that a certain proposition prop holds at time Tprop in the context of C. The potential intention operators, Pot.Int.To(...) and Pot.Int.Th(...), are used to represent the mental state when an agent considers adopting an intention, but has not deliberated about the interaction of the other intentions it holds.",
                "The operator Bel(Ai, f, Tf ) represents agent Ai believing in the statement expressed in formula f, at time Tf .",
                "MB(A, f, Tf ) represents mutual belief for a group of agents A.",
                "A snapshot of the system finds our environment to be in some state e ∈ E of environmental variable states, and each adversary in any LAi ∈ L of possible local states.",
                "At any given time step, the system will be in some world w of the set of all possible worlds w ∈ W, where w = E×LA1 ×LA2 × ...LAn , and n is the number of adversaries.",
                "For example, in a Texas Holdem poker game, an agents local state might be its own set of cards (which is unknown to its adversary) while the environment will consist of the betting pot and the community cards (which are visible to both players).",
                "A utility function under this formalization is defined as a mapping from a possible world w ∈ W to an element in , which expresses the desirability of the world, from a single agent perspective.",
                "We usually normalize the range to [0,1], where 0 represents the least desirable possible world, and 1 is the most desirable world.",
                "The implementation of the utility function is dependent on the domain in question.",
                "The following list specifies new predicates, functions, variables, and constants used in conjunction with the original definitions for the adversarial environment formalization: 1. φ is a null action (the agent does not do anything). 2.",
                "GAi is the set of agent Ais goals.",
                "Each goal is a set of predicates whose satisfaction makes the goal complete (we use G∗ Ai ∈ GAi to represent an arbitrary goal of agent Ai). 3. gAi is the set of agent Ais subgoals.",
                "Subgoals are predicates whose satisfaction represents an important milestone toward achievement of the full goal. gG∗ Ai ⊆ gAi is the set of subgoals that are important to the completion of goal G∗ Ai (we will use g∗ G∗ Ai ∈ gG∗ Ai to represent an arbitrary subgoal). 4.",
                "P Aj Ai is the profile object agent Ai holds about agent Aj. 5.",
                "CA is a general set of actions for all agents in A which are derived from the environments constraints.",
                "CAi ⊆ CA is the set of agent Ais possible actions. 6.",
                "Do(Ai, α, Tα, w) holds when Ai performs action α over time interval Tα in world w. 7.",
                "Achieve(G∗ Ai , α, w) is true when goal G∗ Ai is achieved following the completion of action α in world w ∈ W, where α ∈ CAi . 8.",
                "Profile(Ai, PAi Ai ) is true when agent Ai holds an object profile for agent Aj.",
                "Definition 1.",
                "Full conflict (FulConf ) describes a zerosum interaction where only a single goal of the goals in conflict can be completed.",
                "FulConf(G∗ Ai , G∗ Aj ) ⇒ (∃α ∈ CAi , ∀w, β ∈ CAj ) (Achieve(G∗ Ai , α, w) ⇒ ¬Achieve(G∗ Aj , β, w)) ∨ (∃β ∈ CAj , ∀w, α ∈ CAi )(Achieve(G∗ Aj , β, w) ⇒ ¬Achieve(G∗ Ai , α, w)) Definition 2.",
                "Adversarial Knowledge (AdvKnow) is a function returning a value which represents the amount of The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 551 knowledge agent Ai has on the profile of agent Aj, at time Tn.",
                "The higher the value, the more knowledge agent Ai has.",
                "AdvKnow : P Aj Ai × Tn → Definition 3.",
                "Eval - This <br>evaluation function</br> returns an estimated expected utility value for an agent in A, after completing an action from CA in some world state w. Eval : A × CA × w → Definition 4.",
                "TrH - (Threshold) is a numerical constant in the [0,1] range that represents an <br>evaluation function</br> (Eval) threshold value.",
                "An action that yields an estimated utility evaluation above the TrH is regarded as a highly beneficial action.",
                "The Eval value is an estimation and not the real utility function, which is usually unknown.",
                "Using the real utility value for a rational agent would easily yield the best outcome for that agent.",
                "However, agents usually do not have the real utility functions, but rather a heuristic estimate of it.",
                "There are two important properties that should hold for the <br>evaluation function</br>: Property 1.",
                "The <br>evaluation function</br> should state that the most desirable world state is one in which the goal is achieved.",
                "Therefore, after the goal has been satisfied, there can be no future action that can put the agent in a world state with higher Eval value. (∀Ai, G∗ Ai , α, β ∈ CAi , w ∈ W) Achieve(G∗ Ai , α, w) ⇒ Eval(Ai, α, w) ≥ Eval(Ai, β, w) Property 2.",
                "The <br>evaluation function</br> should project an action that causes a completion of a goal or a subgoal to a value which is greater than TrH (a highly beneficial action). (∀Ai, G∗ Ai ∈ GAi , α ∈ CAi , w ∈ W, g∗ GAi ∈ gGAi ) Achieve(G∗ Ai , α, w) ∨ Achieve(g∗ GAi , α, w) ⇒ Eval(Ai, α, w) ≥ TrH.",
                "Definition 5.",
                "SetAction We define a set action (SetAction) as a set of action operations (either complex or basic actions) from some action sets CAi and CAj which, according to agent Ais belief, are attached together by a temporal and consequential relationship, forming a chain of events (action, and its following consequent action). (∀α1 , . . . , αu ∈ CAi , β1 , . . . , βv ∈ CAj , w ∈ W) SetAction(α1 , . . . , αu , β1 , . . . , βv , w) ⇒ ((Do(Ai, α1 , Tα1 , w) ⇒ Do(Aj, β1 , Tβ1 , w)) ⇒ Do(Ai, α2 , Tα2 , w) ⇒ . . . ⇒ Do(Ai, αu , Tαu , w)) The consequential relation might exist due to various environmental constraints (when one action forces the adversary to respond with a specific action) or due to the agents knowledge about the profile of its adversary.",
                "Property 3.",
                "As the knowledge we have about our adversary increases we will have additional beliefs about its behavior in different situations which in turn creates new set actions.",
                "Formally, if our AdvKnow at time Tn+1 is greater than AdvKnow at time Tn, then every SetAction known at time Tn is also known at time Tn+1.",
                "AdvKnow(P Aj Ai , Tn+1) > AdvKnow(P Aj Ai , Tn) ⇒ (∀α1 , . . . , αu ∈ CAi , β1 , . . . , βv ∈ CAj ) Bel(Aag, SetAction(α1 , . . . , αu , β1 , . . . , βv ), Tn) ⇒ Bel(Aag, SetAction(α1 , . . . , αu , β1 , . . . , βv ), Tn+1) 2.3 The Environment Formulation The following axioms provide the formal definition for a simple, zero-sum Adversarial Environment (AE).",
                "Satisfaction of these axioms means that the agent is situated in such an environment.",
                "It provides specifications for agent Aag to interact with its set of adversaries A with respect to goals G∗ Aag and G∗ A at time TCo at some world state w. AE(Aag, A, G∗ Aag , A1, . . . , Ak, G∗ A1 , . . . , G∗ Ak , Tn, w) 1.",
                "Aag has an Int.Th his goal would be completed: (∃α ∈ CAag , Tα) Int.Th(Aag, Achieve(G∗ Aag , α), Tn, Tα, AE) 2.",
                "Aag believes that it and each of its adversaries Ao are pursuing full conflicting goals: (∀Ao ∈ {A1, . . . , Ak}) Bel(Aag, FulConf(G∗ Aag , G∗ Ao ), Tn) 3.",
                "Aag believes that each of his adversaries in Ao has the Int.Th his conflicting goal G∗ Aoi will be completed: (∀Ao ∈ {A1, . . . , Ak})(∃β ∈ CAo , Tβ) Bel(Aag, Int.Th(Ao, Achieve(G∗ Ao , β), TCo, Tβ, AE), Tn) 4.",
                "Aag has beliefs about the (partial) profiles of its adversaries (∀Ao ∈ {A1, . . . , Ak}) (∃PAo Aag ∈ PAag )Bel(Aag, Profile(Ao, PAo Aag ), Tn) To build an agent that will be able to operate successfully within such an AE, we must specify behavioral guidelines for its interactions.",
                "Using a naive Eval maximization strategy to a certain search depth will not always yield satisfactory results for several reasons: (1) the search horizon problem when searching for a fixed depth; (2) the strong assumption of an optimally rational, unbounded resources adversary; (3) using an estimated <br>evaluation function</br> which will not give optimal results in all world states, and can be exploited [9].",
                "The following axioms specify the behavioral principles that can be used to differentiate between successful and less successful agents in the above Adversarial Environment.",
                "Those axioms should be used as specification principles when designing and implementing agents that should be able to perform well in such Adversarial Environments.",
                "The behavioral axioms represent situations in which the agent will adopt potential intentions to (Pot.Int.To(...)) perform an action, which will typically require some means-end reasoning to select a possible course of action.",
                "This reasoning will lead to the adoption of an Int.To(...) (see [4]).",
                "A1.",
                "Goal Achieving Axiom.",
                "The first axiom is the simplest case; when the agent Aag believes that it is one action (α) away from achieving his conflicting goal G∗ Aag , it should adopt the potential intention to do α and complete its goal. (∀Aag, α ∈ CAag , Tn, Tα, w ∈ W) (Bel(Aag, Do(Aag, α, Tα, w) ⇒ Achieve(G∗ Aag , α, w)) ⇒ Pot.Int.To(Aag, α, Tn, Tα, w) This somewhat trivial behavior is the first and strongest axiom.",
                "In any situation, when the agent is an action away from completing the goal, it should complete the action.",
                "Any fair Eval function would naturally classify α as the maximal value action (property 1).",
                "However, without explicit axiomatization of such behavior there might be situations where the agent will decide on taking another action for various reasons, due to its bounded decision resources.",
                "A2.",
                "Preventive Act Axiom.",
                "Being in an adversarial situation, agent Aag might decide to take actions that will damage one of its adversarys plans to complete its goal, even if those actions do not explicitly advance Aag towards its conflicting goal G∗ Aag .",
                "Such preventive action will take place when agent Aag has a belief about the possibility of its adversary Ao doing an action β that will give it a high 552 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) utility evaluation value (> TrH).",
                "Believing that taking action α will prevent the opponent from doing its β, it will adopt a potential intention to do α. (∀Aag, Ao ∈ A, α ∈ CAag , β ∈ CAo , Tn, Tβ, w ∈ W) (Bel(Aag, Do(Ao, β, Tβ, w) ∧ Eval(Ao, β, w) > TrH, Tn) ∧ Bel(Aag, Do(Aag, α, Tα, w) ⇒ ¬Do(Ao, β, Tβ, w), Tn) ⇒ Pot.Int.To(Aag, α, Tn, Tα, w) This axiom is a basic component of any adversarial environment.",
                "For example, looking at a Chess board game, a player could realize that it is about to be checkmated by its opponent, thus making a preventive move.",
                "Another example is a Connect Four game: when a player has a row of three chips, its opponent must block it, or lose.",
                "A specific instance of A1 occurs when the adversary is one action away from achieving its goal, and immediate preventive action needs to be taken by the agent.",
                "Formally, we have the same beliefs as stated above, with a changed belief that doing action β will cause agent Ao to achieve its goal.",
                "Proposition 1: Prevent or lose case. (∀Aag, Ao ∈ A, α ∈ CAag , β ∈ CAo , G∗ Ao , Tn, Tα, Tβ, w ∈ W) Bel(Aag, Do(Ao, β, Tβ, w) ⇒ Achieve(G∗ Ao , β, w), Tn) ∧ Bel(Aag, Do(Aag, α, Tα, w) ⇒ ¬Do(Ao, β, Tβ, w)) ⇒ Pot.Int.To(Aag, α, Tn, Tα, w) Sketch of proof: Proposition 1 can be easily derived from axiom A1 and the property 2 of the Eval function, which states that any action that causes a completion of a goal is a highly beneficial action.",
                "The preventive act behavior will occur implicitly when the Eval function is equal to the real world utility function.",
                "However, being bounded rational agents and dealing with an estimated <br>evaluation function</br> we need to explicitly axiomatize such behavior, for it will not always occur implicitly from the <br>evaluation function</br>.",
                "A3.",
                "Suboptimal Tactical Move Axiom.",
                "In many scenarios a situation may occur where an agent will decide not to take the current most beneficial action it can take (the action with the maximal utility evaluation value), because it believes that taking another action (with lower utility evaluation value) might yield (depending on the adversarys response) a future possibility for a highly beneficial action.",
                "This will occur most often when the Eval function is inaccurate and differs by a large extent from the Utility function.",
                "Put formally, agent Aag believes in a certain SetAction that will evolve according to its initial action and will yield a high beneficial value (> TrH) solely for it. (∀Aag, Ao ∈ A, Tn, w ∈ W) (∃α1 , . . . , αu ∈ CAi , β1 , . . . , βv ∈ CAj , Tα1 ) Bel(Aag, SetAction(α1 , . . . , αu , β1 , . . . , βv ), Tn) ∧ Bel(Aag, Eval(Ao, βv , w) < TrH < Eval(Aag, αu , w), Tn) ⇒ Pot.Int.To(Aag, α1 , Tn, Tα1 , w) An agent might believe that a chain of events will occur for various reasons due to the inevitable nature of the domain.",
                "For example, in Chess, we often observe the following: a move causes a check position, which in turn limits the opponents moves to avoiding the check, to which the first player might react with another check, and so on.",
                "The agent might also believe in a chain of events based on its knowledge of its adversarys profile, which allows it to foresee the adversarys movements with high accuracy.",
                "A4.",
                "Profile Detection Axiom.",
                "The agent can adjust its adversarys profiles by observations and pattern study (specifically, if there are repeated encounters with the same adversary).",
                "However, instead of waiting for profile information to be revealed, an agent can also initiate actions that will force its adversary to react in a way that will reveal profile knowledge about it.",
                "Formally, the axiom states that if all actions (γ) are not highly beneficial actions (< TrH), the agent can do action α in time Tα if it believes that it will result in a non-highly beneficial action β from its adversary, which in turn teaches it about the adversarys profile, i.e., gives a higher AdvKnow(P Aj Ai , Tβ). (∀Aag, Ao ∈ A, α ∈ CAag , β ∈ CAo , Tn, Tα, Tβ, w ∈ W) Bel(Aag, (∀γ ∈ CAag )Eval(Aag, γ, w) < TrH, Tn) ∧ Bel(Aag, Do(Aag, α, Tα, w) ⇒ Do(Ao, β, Tβ, w), Tn) ∧ Bel(Aag, Eval(Ao, β, w) < TrH) ∧ Bel(Aag, AdvKnow(P Aj Ai , Tβ) > AdvKnow(P Aj Ai , Tn), Tn) ⇒ Pot.Int.To(Aag, α, Tn, Tα, w) For example, going back to the Chess board game scenario, consider starting a game versus an opponent about whom we know nothing, not even if it is a human or a computerized opponent.",
                "We might start playing a strategy that will be suitable versus an average opponent, and adjust our game according to its level of play.",
                "A5.",
                "Alliance Formation Axiom The following behavioral axiom is relevant only in a multilateral instantiation of the adversarial environment (obviously, an alliance cannot be formed in a bilateral, zero-sum encounter).",
                "In different situations during a multilateral interaction, a group of agents might believe that it is in their best interests to form a temporary alliance.",
                "Such an alliance is an agreement that constrains its members behavior, but is believed by its members to enable them to achieve a higher utility value than the one achievable outside of the alliance.",
                "As an example, we can look at the classical Risk board game, where each player has an individual goal of being the sole conquerer of the world, a zero-sum game.",
                "However, in order to achieve this goal, it might be strategically wise to make short-term ceasefire agreements with other players, or to join forces and attack an opponent who is stronger than the rest.",
                "An alliances terms defines the way its members should act.",
                "It is a set of predicates, denoted as Terms, that is agreed upon by the alliance members, and should remain true for the duration of the alliance.",
                "For example, the set Terms in the Risk scenario, could contain the following predicates: 1.",
                "Alliance members will not attack each other on territories X, Y and Z; 2.",
                "Alliance members will contribute C units per turn for attacking adversary Ao; 3.",
                "Members are obligated to stay as part of the alliance until time Tk or until adversarys Ao army is smaller than Q.",
                "The set Terms specifies inter-group constraints on each of the alliance members (∀Aal i ∈ Aal ⊆ A) set of actions Cal i ⊆ C. Definition 6.",
                "Al val - the total evaluation value that agent Ai will achieve while being part of Aal is the sum of Evali (Eval for Ai) of each of Aal j Eval values after taking their own α actions (via the agent(α) predicate): Al val(Ai, Cal , Aal , w) = α∈Cal Evali(Aal j , agent(α), w) Definition 7.",
                "Al TrH - is a number representing an Al val The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 553 threshold; above it, the alliance can be said to be a highly beneficial alliance.",
                "The value of Al TrH will be calculated dynamically according to the progress of the interaction, as can be seen in [7].",
                "After an alliance is formed, its members are now working in their normal adversarial environment, as well as according to the mental states and axioms required for their interactions as part of the alliance.",
                "The following Alliance model (AL) specifies the conditions under which the group Aal can be said to be in an alliance and working with a new and constrained set of actions Cal , at time Tn.",
                "AL(Aal , Cal , w, Tn) 1.",
                "Aal has a MB that all members are part of Aal : MB(Aal , (∀Aal i ∈ Aal )member(Aal i , Aal ), Tn) 2.",
                "Aal has a MB that the group be maintained: MB(Aal , (∀Aal i ∈ Aal )Int.Th (Ai, member(Ai, Aal ), Tn, Tn+1, Co), Tn) 3.",
                "Aal has a MB that being members gives them high utility value: MB(Aal , (∀Aal i ∈ Aal )Al val(Aal i , Cal , Aal , w) ≥ Al TrH, Tn) Members profiles are a crucial part of successful alliances.",
                "We assume that agents that have more accurate profiles of their adversaries will be more successful in such environments.",
                "Such agents will be able to predict when a member is about to breach the alliances contract (item 2 in the above model), and take counter measures (when item 3 will falsify).",
                "The robustness of the alliance is in part a function of its members trustfulness measure, objective position estimation, and other profile properties.",
                "We should note that an agent can simultaneously be part of more than one alliance.",
                "Such a temporary alliance, where the group members do not have a joint goal but act collaboratively for the interest of their own individual goals, is classified as a Treatment Group by modern psychologists [12] (in contrast to a Task Group, where its members have a joint goal).",
                "The Shared Activity model as presented in [5] modeled Treatment Group behavior using the same SharedPlans formalization.",
                "When comparing both definitions of an alliance and a Treatment Group we found an unsurprising resemblance between both models: the environment models definitions are almost identical (see SAs definitions in [5]), and their Selfish-Act and Cooperative Act axioms conform to our adversarial agents behavior.",
                "The main distinction between both models is the integration of a Helpful-behavior act axiom, in the Shared Activity which cannot be part of ours.",
                "This axiom states that an agent will consider taking action that will lower its Eval value (to a certain lower bound), if it believes that a group partner will gain a significant benefit.",
                "Such behavior cannot occur in a pure adversarial environment (as a zero-sum game is), where the alliance members are constantly on watch to manipulate their alliance to their own advantage.",
                "A6.",
                "Evaluation Maximization Axiom.",
                "In a case when all other axioms are inapplicable, we will proceed with the action that maximizes the heuristic value as computed in the Eval function. (∀Aag, Ao ∈ A, α ∈ Cag, Tn, w ∈ W) Bel(Aag, (∀γ ∈ Cag)Eval(Aag, α, w) ≥ Eval(Aag, γ, w), Tn) ⇒ Pot.Int.To(Aag, α, Tn, Tα, w) T1.",
                "Optimality on Eval = Utility The above axiomatic model handles situations where the Utility is unknown and the agents are bounded rational agents.",
                "The following theorem shows that in bilateral interactions, where the agents have the real Utility function (i.e., Eval = Utility) and are rational agents, the axioms provide the same optimal result as classic adversarial search (e.g., Min-Max).",
                "Theorem 1.",
                "Let Ae ag be an unbounded rational AE agent using the Eval heuristic <br>evaluation function</br>, Au ag be the same agent using the true Utility function, and Ao be a sole unbounded utility-based rational adversary.",
                "Given that Eval = Utility: (∀α ∈ CAu ag , α ∈ CAe ag , Tn, w ∈ W) Pot.Int.To(Au ag, α, Tn, Tα, w) → Pot.Int.To(Ae ag, α , Tn, Tα, w) ∧ ((α = α ) ∨ (Utility(Au ag, α, w) = Eval(Ae ag, α , w))) Sketch of proof - Given that Au ag has the real utility function and unbounded resources, it can generate the full game tree and run the optimal MinMax algorithm to choose the highest utility value action, which we denote by, α.",
                "The proof will show that Ae ag, using the AE axioms, will select the same or equal utility α (when there is more than one action with the same max utility) when Eval = Utility. (A1) Goal achieving axiom - suppose there is an α such that its completion will achieve Au ags goal.",
                "It will obtain the highest utility by Min-Max for Au ag.",
                "The Ae ag agent will select α or another action with the same utility value via A1.",
                "If such α does not exist, Ae ag cannot apply this axiom, and proceeds to A2. (A2) Preventive act axiom - (1) Looking at the basic case (see Prop1), if there is a β which leads Ao to achieve its goal, then a preventive action α will yield the highest utility for Au ag.",
                "Au ag will choose it through the utility, while Ae ag will choose it through A2. (2) In the general case, β is a highly beneficial action for Ao, thus yields low utility for Au ag, which will guide it to select an α that will prevent β, while Ae ag will choose it through A2.1 If such β does not exist for Ao, then A2 is not applicable, and Ae ag can proceed to A3. (A3) Suboptimal tactical move axiom - When using a heuristic Eval function, Ae ag has a partial belief in the profile of its adversary (item 4 in AE model), which may lead it to believe in SetActions (Prop1).",
                "In our case, Ae ag is holding a full profile on its optimal adversary and knows that Ao will behave optimally according to the real utility values on the complete search tree, therefore, any belief about suboptimal SetAction cannot exist, yielding this axiom inapplicable.",
                "Ae ag will proceed to A4. (A4) Profile detection axiom - Given that Ae ag has the full profile of Ao, none of Ae ags actions can increase its knowledge.",
                "That axiom will not be applied, and the agent will proceed with A6 (A5 will be disregarded because the interaction is bilateral). (A6) Evaluation maximization axiom - This axiom will select the max Eval for Ae ag.",
                "Given that Eval = Utility, the same α that was selected by Au ag will be selected. 3.",
                "EVALUATION The main purpose of our experimental analysis is to evaluate the models behavior and performance in a real adversarial environment.",
                "This section investigates whether bounded 1 A case where following the completion of β there exists a γ which gives high utility for Agent Au ag, cannot occur because Ao uses the same utility, and γs existence will cause it to classify β as a low utility action. 554 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) rational agents situated in such adversarial environments will be better off applying our suggested behavioral axioms. 3.1 The Domain To explore the use of the above model and its behavioral axioms, we decided to use the Connect-Four game as our adversarial environment.",
                "Connect-Four is a 2-player, zerosum game which is played using a 6x7 matrix-like board.",
                "Each turn, a player drops a disc into one of the 7 columns (the set of 21 discs is usually colored yellow for player 1 and red for player 2; we will use White and Black respectively to avoid confusion).",
                "The winner is the first player to complete a horizontal, vertical, or diagonal set of four discs with its color.",
                "On very rare occasions, the game might end in a tie if all the empty grids are filled, but no player managed to create a 4-disc set.",
                "The Connect-Four game was solved in [1], where it is shown that the first player (playing with the white discs) can force a win by starting in the middle column (column 4) and playing optimally However, the optimal strategy is very complex, and difficult to follow even for complex bounded rational agents, such as human players.",
                "Before we can proceed checking agent behavior, we must first verify that the domain conforms to the adversarial environments definition as given above (which the behavioral axioms are based on).",
                "First, when playing a Connect-Four game, the agent has an intention to win the game (item 1).",
                "Second (item 2), our agent believes that in Connect-Four there can only be one winner (or no winner at all in the rare occurrence of a tie).",
                "In addition, our agent believes that its opponent to the game will try to win (item 3), and we hope it has some partial knowledge (item 4) about its adversary (this knowledge can vary from nothing, through simple facts such as age, to strategies and weaknesses).",
                "Of course, not all Connect-Four encounters are adversarial.",
                "For example, when a parent is playing the game with its child, the following situation might occur: the child, having a strong incentive to win, treats the environment as adversarial (it intends to win, understands that there can only be one winner, and believes that its parent is trying to beat him).",
                "However, the parents point of view might see the environment as an educational one, where its goal is not to win the game, but to cause enjoyment or practice strategic reasoning.",
                "In such an educational environment, a new set of behavioral axioms might be more beneficial to the parents goals than our suggested adversarial behavioral axioms. 3.2 Axiom Analysis After showing that the Connect-Four game is indeed a zero-sum, bilateral adversarial environment, the next step is to look at players behaviors during the game and check whether behaving according to our model does improve performance.",
                "To do so we have collected log files from completed Connect-Four games that were played by human players over the Internet.",
                "Our collected log file data came from Play by eMail (PBeM) sites.",
                "These are web sites that host email games, where each move is taken by an email exchange between the server and the players.",
                "Many such sites archives contain real competitive interactions, and also maintain a ranking system for their members.",
                "Most of the data we used can be found in [6].",
                "As can be learned from [1], Connect-Four has an optimal strategy and a considerable advantage for the player who starts the game (which we call the White player).",
                "We will concentrate in our analysis on the second players moves (to be called Black).",
                "The White player, being the first to act, has the so-called initiative advantage.",
                "Having the advantage and a good strategy will keep the Black player busy reacting to its moves, instead of initiating threats.",
                "A threat is a combination of three discs of the same color, with an empty spot for the fourth winning disk.",
                "An open threat is a threat that can be realized in the opponents next move.",
                "In order for the Black player to win, it must somehow turn the tide, take the advantage and start presenting threats to the White player.",
                "We will explore Black players behavior and their conformance to our axioms.",
                "To do so, we built an application that reads log files and analyzes the Black players moves.",
                "The application contains two main components: (1) a Min-Max algorithm for evaluation of moves; (2) open threats detector for the discovering of open threats.",
                "The Min-Max algorithm will work to a given depth, d and for each move α will output the heuristic value for the next action taken by the player as written in the log file, h(α), alongside the maximum heuristic value, maxh(α), that could be achieved prior to taking the move (obviously, if h(α) = maxh(α), then the player did not do the optimal move heuristically).",
                "The threat detectors job is to notify if some action was taken in order to block an open threat (not blocking an open threat will probably cause the player to lose in the opponents next move).",
                "The heuristic function used by Min-Max to evaluate the players utility is the following function, which is simple to compute, yet provides a reasonable challenge to human opponents: Definition 8.",
                "Let Group be an adjacent set of four squares that are horizontal, vertical, or diagonal.",
                "Groupn b (Groupn w) be a Group with n pieces of the black (white) color and 4−n empty squares. h = ((Group1 b ∗α)+(Group2 b ∗β)+(Group3 b ∗γ)+(Group4 b ∗∞)) − ((Group1 w ∗α)+(Group2 w ∗β)+(Group3 w ∗γ)+(Group4 w ∗∞)) The values of α, β and δ can vary to form any desired linear combination; however, it is important to value them with the α < β < δ ordering in mind (we used 1, 4, and 8 as their respective values).",
                "Groups of 4 discs of the same color means victory, thus discovery of such a group will result in ∞ to ensure an extreme value.",
                "We now use our estimated <br>evaluation function</br> to evaluate the Black players actions during the Connect-Four adversarial interaction.",
                "Each game from the log file was input into the application, which processed and output a reformatted log file containing the h value of the current move, the maxh value that could be achieved, and a notification if an open threat was detected.",
                "A total of 123 games were analyzed (57 with White winning, and 66 with Black winning).",
                "A few additional games were manually ignored in the experiment, due to these problems: a player abandoning the game while the outcome is not final, or a blunt irrational move in the early stages of the game (e.g., not blocking an obvious winning group in the first opening moves).",
                "In addition, a single tie game was also removed.",
                "The simulator was run to a search depth of 3 moves.",
                "We now proceed to analyze the games with respect to each behavioral axiom.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 555 Table 1: Average heuristic difference analysis Black losses Black Won Avg minh -17.62 -12.02 Avg 3 lowest h moves (min3 h) -13.20 -8.70 3.2.1 Affirming the Suboptimal tactical move axiom The following section presents the heuristic evaluations of the Min-Max algorithm for each action, and checks the amount and extent of suboptimal tactical actions and their implications on performance.",
                "Table 1 shows results and insights from the games heuristic analysis, when search depth equals 3 (this search depth was selected for the results to be comparable to [9], see Section 3.2.3).",
                "The tables heuristic data is the difference between the present maximal heuristic value and the heuristic value of the action that was eventually taken by the player (i.e., the closer the number is to 0, the closer the action was to the maximum heuristic action).",
                "The first row presents the difference values of the action that had the maximal difference value among all the Black players actions in a given game, as averaged over all Blacks winning and losing games (see respective columns).",
                "In games in which the Black player loses, its average difference value was -17.62, while in games in which the Black player won, its average was -12.02.",
                "The second row expands the analysis by considering the 3 highest heuristic difference actions, and averaging them.",
                "In that case, we notice an average heuristic difference of 5 points between games which the Black player loses and games in which it wins.",
                "Nevertheless, the importance of those numbers is that they allowed us to take an educated guess on a threshold number of 11.5, as the value of the TrH constant, which differentiates between normal actions and highly beneficial ones.",
                "After finding an approximated TrH constant, we can proceed with an analysis of the importance of suboptimal moves.",
                "To do so we took the subset of games in which the minimum heuristic difference value for Blacks actions was 11.5.",
                "As presented in Table 2, we can see the different min3 h average of the 3 largest ranges and the respective percentage of games won.",
                "The first row shows that the Black player won only 12% of the games in which the average of its 3 highest heuristically difference actions (min3 h) was smaller than the suggested threshold, TrH = 11.5.",
                "The second row shows a surprising result: it seems that when min3 h > −4 the Black player rarely wins.",
                "Intuition would suggest that games in which the action evaluation values were closer to the maximal values will result in more winning games for Black.",
                "However, it seems that in the Connect-Four domain, merely responding with somewhat easily expected actions, without initiating a few surprising and suboptimal moves, does not yield good results.",
                "The last row sums up the main insights from the analysis; most of Blacks wins (83%) came when its min3 h was in the range of -11.5 to -4.",
                "A close inspection of those Black winning games shows the following pattern behind the numbers: after standard opening moves, Black suddenly drops a disc into an isolated column, which seems a waste of a move.",
                "White continues to build its threats, while usually disregarding Blacks last move, which in turn uses the isolated disc as an anchor for a future winning threat.",
                "The results show that it was beneficial for the Black player Table 2: Blacks winnings percentages % of games min3 h < −11.5 12% min3 h > −4 5% −11.5 ≤ min3 h ≤ −4 83% to take suboptimal actions and not give the current highest possible heuristic value, but will not be too harmful for its position (i.e., will not give high beneficial value to its adversary).",
                "As it turned out, learning the threshold is an important aspect of success: taking wildly risky moves (min3 h < −11.5) or trying to avoid them (min3 h > −4) reduces the Black players winning chances by a large margin. 3.2.2 Affirming the Profile Monitoring Axiom In the task of showing the importance of monitoring ones adversaries profiles, our log files could not be used because they did not contain repeated interactions between players, which are needed to infer the players knowledge about their adversaries.",
                "However, the importance of opponent modeling and its use in attaining tactical advantages was already studied in various domains ([3, 9] are good examples).",
                "In a recent paper, Markovitch and Reger [9] explored the notion of learning and exploitation of opponent weakness in competitive interactions.",
                "They apply simple learning strategies by analyzing examples from past interactions in a specific domain.",
                "They also used the Connect-Four adversarial domain, which can now be used to understand the importance of monitoring the adversarys profile.",
                "Following the presentation of their theoretical model, they describe an extensive empirical study and check the agents performance after learning the weakness model with past examples.",
                "One of the domains used as a competitive environment was the same Connect-Four game (Checkers was the second domain).",
                "Their heuristic function was identical to ours with three different variations (H1, H2, and H3) that are distinguished from one another in their linear combination coefficient values.",
                "The search depth for the players was 3 (as in our analysis).",
                "Their extensive experiments check and compare various learning strategies, risk factors, predefined feature sets and usage methods.",
                "The bottom line is that the Connect-Four domain shows an improvement from a 0.556 winning rate before modeling to a 0.69 after modeling (page 22).",
                "Their conclusions, showing improved performance when holding and using the adversarys model, justify the effort to monitor the adversary profile for continuous and repeated interactions.",
                "An additional point that came up in their experiments is the following: after the opponent weakness model has been learned, the authors describe different methods of integrating the opponent weakness model into the agents decision strategy.",
                "Nevertheless, regardless of the specific method they chose to work with, all integration methods might cause the agent to take suboptimal decisions; it might cause the agent to prefer actions that are suboptimal at the present decision junction, but which might cause the opponent to react in accordance with its weakness model (as represented by our agent) which in turn will be beneficial for us in the future.",
                "The agents behavior, as demonstrated in [9] further confirms and strengthens our Suboptimal Tactical Axiom as discussed in the previous section. 556 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 3.2.3 Additional Insights The need for the Goal Achieving, Preventive Act, and Evaluation Maximization axioms are obvious, and need no further verification.",
                "However, even with respect to those axioms, a few interesting insights came up in the log analysis.",
                "The Goal achieving and Preventive Act axioms, though theoretically trivial, seem to provide some challenge to a human player.",
                "In the initial inspection of the logs, we encountered few games2 where a player, for inexplicable reasons, did not block the other from winning or failed to execute its own winning move.",
                "We can blame those faults on the humans lack of attention, or a typing error in its move reply; nevertheless, those errors might occur in bounded rational agents, and the appropriate behavior needs to be axiomatized.",
                "A typical Connect-Four game revolves around generating threats and blocking them.",
                "In our analysis we looked for explicit preventive actions, i.e., moves that block a group of 3 discs, or that remove a future threat (in our limited search horizon).",
                "We found that in 83% of the total games there was at least one preventive action taken by the Black player.",
                "It was also found that Black averaged 2.8 preventive actions per game on the games in which it lost, while averaging 1.5 preventive actions per game when winning.",
                "It seems that Black requires 1 or 2 preventive actions to build its initial taking position, before starting to present threats.",
                "If it did not manage to win, it will usually prevent an extra threat or two before succumbing to White. 4.",
                "RELATED WORK Much research deals with the axiomatization of teamwork and mental states of individuals: some models use knowledge and belief [10], others have models of goals and intentions [8, 4].",
                "However, all these formal theories deal with agent teamwork and cooperation.",
                "As far as we know, our model is the first to provide a formalized model for explicit adversarial environments and agents behavior in it.",
                "The classical Min-Max adversarial search algorithm was the first attempt to integrate the opponent into the search space with a weak assumption of an optimally playing opponent.",
                "Since then, much effort has gone into integrating the opponent model into the decision procedure to predict future behavior.",
                "The M∗ algorithm presented by Carmel and Markovitch [2] showed a method of incorporating opponent models into adversary search, while in [3] they used learning to provide a more accurate opponent model in a 2player repeated game environment, where agents strategies were modeled as finite automata.",
                "Additional Adversarial planning work was done by Willmott et al. [13], which provided an adversarial planning approach to the game of GO.",
                "The research mentioned above dealt with adversarial search and the integration of opponent models into classical utilitybased search methods.",
                "That work shows the importance of opponent modeling and the ability to exploit it to an agents advantage.",
                "However, the basic limitations of those search methods still apply; our model tries to overcome those limitations by presenting a formal model for a new, mental state-based adversarial specification. 5.",
                "CONCLUSIONS We presented an Adversarial Environment model for a 2 These were later removed from the final analysis. bounded rational agent that is situated in an N-player, zerosum environment.",
                "We used the SharedPlans formalization to define the model and the axioms that agents can apply as behavioral guidelines.",
                "The model is meant to be used as a guideline for designing agents that need to operate in such adversarial environments.",
                "We presented empirical results, based on ConnectFour log file analysis, that exemplify the model and the axioms for a bilateral instance of the environment.",
                "The results we presented are a first step towards an expanded model that will cover all types of adversarial environments, for example, environments that are non-zero-sum, and environments that contain natural agents that are not part of the direct conflict.",
                "Those challenges and more will be dealt with in future research. 6.",
                "ACKNOWLEDGMENT This research was supported in part by Israel Science Foundation grants #1211/04 and #898/05. 7.",
                "REFERENCES [1] L. V. Allis.",
                "A knowledge-based approach of Connect-Four - the game is solved: White wins.",
                "Masters thesis, Free University, Amsterdam, The Netherlands, 1988. [2] D. Carmel and S. Markovitch.",
                "Incorporating opponent models into adversary search.",
                "In Proceedings of the Thirteenth National Conference on Artificial Intelligence, pages 120-125, Portland, OR, 1996. [3] D. Carmel and S. Markovitch.",
                "Opponent modeling in multi-agent systems.",
                "In G. Weiß and S. Sen, editors, Adaptation and Learning in Multi-Agent Systems, pages 40-52.",
                "Springer-Verlag, 1996. [4] B. J. Grosz and S. Kraus.",
                "Collaborative plans for complex group action.",
                "Artificial Intelligence, 86(2):269-357, 1996. [5] M. Hadad, G. Kaminka, G. Armon, and S. Kraus.",
                "Supporting collaborative activity.",
                "In Proc. of AAAI-2005, pages 83-88, Pittsburgh, 2005. [6] http://www.gamerz.net/˜pbmserv/. [7] S. Kraus and D. Lehmann.",
                "Designing and building a negotiating automated agent.",
                "Computational Intelligence, 11:132-171, 1995. [8] H. J. Levesque, P. R. Cohen, and J. H. T. Nunes.",
                "On acting together.",
                "In Proc. of AAAI-90, pages 94-99, Boston, MA, 1990. [9] S. Markovitch and R. Reger.",
                "Learning and exploiting relative weaknesses of opponent agents.",
                "Autonomous Agents and Multi-Agent Systems, 10(2):103-130, 2005. [10] Y. M. Ronald Fagin, Joseph Y. Halpern and M. Y. Vardi.",
                "Reasoning about knowledge.",
                "MIT Press, Cambridge, Mass., 1995. [11] P. Thagard.",
                "Adversarial problem solving: Modeling an oponent using explanatory coherence.",
                "Cognitive Science, 16(1):123-149, 1992. [12] R. W. Toseland and R. F. Rivas.",
                "An Introduction to Group Work Practice.",
                "Prentice Hall, Englewood Cliffs, NJ, 2nd edition edition, 1995. [13] S. Willmott, J. Richardson, A. Bundy, and J. Levine.",
                "An adversarial planning approach to Go.",
                "Lecture Notes in Computer Science, 1558:93-112, 1999.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 557"
            ],
            "original_annotated_samples": [
                "Eval - This <br>evaluation function</br> returns an estimated expected utility value for an agent in A, after completing an action from CA in some world state w. Eval : A × CA × w → Definition 4.",
                "TrH - (Threshold) is a numerical constant in the [0,1] range that represents an <br>evaluation function</br> (Eval) threshold value.",
                "There are two important properties that should hold for the <br>evaluation function</br>: Property 1.",
                "The <br>evaluation function</br> should state that the most desirable world state is one in which the goal is achieved.",
                "The <br>evaluation function</br> should project an action that causes a completion of a goal or a subgoal to a value which is greater than TrH (a highly beneficial action). (∀Ai, G∗ Ai ∈ GAi , α ∈ CAi , w ∈ W, g∗ GAi ∈ gGAi ) Achieve(G∗ Ai , α, w) ∨ Achieve(g∗ GAi , α, w) ⇒ Eval(Ai, α, w) ≥ TrH."
            ],
            "translated_annotated_samples": [
                "Eval: Esta <br>función de evaluación</br> devuelve un valor de utilidad esperada estimado para un agente en A, después de completar una acción de CA en algún estado del mundo w. Eval: A × CA × w → Definición 4.",
                "TrH - (Umbral) es una constante numérica en el rango [0,1] que representa un <br>valor umbral de la función de evaluación</br> (Eval).",
                "Existen dos propiedades importantes que deben cumplir para la <br>función de evaluación</br>: Propiedad 1.",
                "La <br>función de evaluación</br> debería indicar que el estado del mundo más deseable es aquel en el que se logra el objetivo.",
                "La <br>función de evaluación</br> debe proyectar una acción que cause la finalización de un objetivo o subobjetivo a un valor que sea mayor que TrH (una acción altamente beneficiosa). (∀Ai, G∗ Ai ∈ GAi , α ∈ CAi , w ∈ W, g∗ GAi ∈ gGAi ) Lograr(G∗ Ai , α, w) ∨ Lograr(g∗ GAi , α, w) ⇒ Eval(Ai, α, w) ≥ TrH."
            ],
            "translated_text": "Un modelo de entorno adversarial para agentes racionales limitados en interacciones de suma cero Inon Zuckerman1, Sarit Kraus1, Jeffrey S. Rosenschein2, Gal Kaminka1 1 Departamento de Ciencias de la Computación 2 Escuela de Ingeniería Universidad Bar-Ilan y Ciencias de la Computación Ramat-Gan, Israel Universidad Hebrea, Jerusalén, Israel {zukermi, sarit, galk}@cs.biu.ac.il jeff@cs.huji.ac.il RESUMEN Los entornos multiagentes a menudo no son cooperativos ni colaborativos; en muchos casos, los agentes tienen intereses conflictivos, lo que lleva a interacciones adversariales. Este documento presenta un modelo formal de Entorno Adversarial para agentes racionales acotados que operan en un entorno de suma cero. En tales entornos, los intentos de utilizar métodos de búsqueda basados en utilidad clásica pueden plantear una variedad de dificultades (por ejemplo, modelar implícitamente al oponente como un maximizador de utilidad omnisciente, en lugar de aprovechar un modelo de oponente más matizado y explícito). Definimos un Entorno Adversario al describir los estados mentales de un agente en dicho entorno. Luego presentamos axiomas de comportamiento que están destinados a servir como principios de diseño para construir agentes adversarios. Exploramos la aplicación de nuestro enfoque mediante el análisis de archivos de registro de juegos de Conecta Cuatro completados, y presentamos un análisis empírico de la adecuación de los axiomas. Categorías y Descriptores de Asignaturas I.2.11 [Inteligencia Artificial]: Inteligencia Artificial Distribuida-Agentes Inteligentes, Sistemas Multiagente; I.2.4 [Inteligencia Artificial]: Formalismos y Métodos de Representación del Conocimiento - Lógica Modal Términos Generales Diseño, Teoría 1. Introducción: En las primeras investigaciones en sistemas multiagente (SMA) se consideraban grupos cooperativos de agentes; debido a que los agentes individuales tenían recursos limitados o acceso limitado a la información (por ejemplo, potencia de procesamiento limitada, cobertura sensorial limitada), trabajaban juntos por diseño para resolver problemas que individualmente no podían resolver, o al menos no podían resolver de manera eficiente. Sin embargo, la investigación de MAS pronto comenzó a considerar agentes interactivos con intereses individualizados, como representantes de diferentes humanos u organizaciones con intereses no idénticos. Cuando las interacciones son guiadas por intereses diversos, los participantes pueden tener que superar desacuerdos, interacciones no cooperativas e incluso intentos intencionales de dañarse mutuamente. Cuando ocurren este tipo de interacciones, los entornos requieren un comportamiento apropiado por parte de los agentes situados en ellos. Llamamos a estos entornos Entornos Adversarios, y llamamos a los agentes en conflicto Adversarios. Los modelos de cooperación y trabajo en equipo han sido ampliamente explorados en MAS a través de la axiomatización de estados mentales (por ejemplo, [8, 4, 5]). Sin embargo, ninguna de estas investigaciones abordó los dominios adversarios y sus implicaciones para el comportamiento de los agentes. Nuestro artículo aborda este problema al proporcionar un modelo formal y axiomatizado de estados mentales para un subconjunto de dominios adversariales, específicamente entornos adversariales simples de suma cero. Los encuentros de suma cero simples existen, por supuesto, en varios juegos de dos jugadores (por ejemplo, Ajedrez, Damas), pero también existen en juegos de n jugadores (por ejemplo, Risk, Diplomacia), subastas de un solo bien y en otros lugares. En estos entornos recientes en particular, el uso de una búsqueda adversarial basada en utilidad (como el algoritmo Min-Max) no siempre proporciona una solución adecuada; la función de recompensa podría ser bastante compleja o difícil de cuantificar, y existen limitaciones computacionales naturales en agentes racionales acotados. Además, los métodos de búsqueda tradicionales (como Min-Max) no hacen uso de un modelo del oponente, lo cual ha demostrado ser una adición valiosa a la planificación adversarial [9, 3, 11]. En este artículo, desarrollamos un modelo formal y axiomatizado para agentes racionales limitados que se encuentran en un entorno adversarial de suma cero. El modelo utiliza diferentes operadores de modalidad, y sus principales fundamentos son el modelo SharedPlans [4] para el comportamiento colaborativo. Exploramos las propiedades del entorno y los estados mentales de los agentes para derivar axiomas de comportamiento; estos axiomas de comportamiento constituyen un modelo formal que sirve como especificación y guía de diseño para el diseño de agentes en tales entornos. Luego investigamos el comportamiento de nuestro modelo empíricamente utilizando el juego de mesa Conecta Cuatro. Mostramos que este juego se ajusta a nuestra definición de entorno, y analizamos el comportamiento de los jugadores utilizando un gran conjunto de registros de partidas completadas 550 978-81-904262-7-5 (RPS) c 2007 archivos de IFAAMAS. Además, utilizamos los resultados presentados en [9] para discutir la importancia del modelado del oponente en nuestro dominio adversarial de Conecta Cuatro. El artículo continúa de la siguiente manera. La sección 2 presenta la formalización de los modelos. La sección 3 presenta el análisis empírico y sus resultados. Discutimos el trabajo relacionado en la Sección 4, y concluimos y presentamos las direcciones futuras en la Sección 5. ENTORNOS ADVERSARIOS El modelo de entorno adversario (denominado como AE) está destinado a guiar el diseño de agentes al proporcionar una especificación de las capacidades y actitudes mentales de un agente en un entorno adversario. Nos enfocamos aquí en tipos específicos de entornos adversarios, especificados de la siguiente manera: 1. Interacciones de suma cero: las utilidades positivas y negativas de todos los agentes suman cero; 2. Agentes adversarios simples: todos los agentes en el entorno son agentes adversarios; 3. AEs bilaterales: AEs con exactamente dos agentes; 4. AEs multilaterales: AEs de tres o más agentes. Trabajaremos en instanciaciones tanto bilaterales como multilaterales de entornos de suma cero y simples. En particular, nuestro modelo de entorno adversarial tratará con interacciones que consisten en N agentes (N ≥ 2), donde todos los agentes son adversarios y solo un agente puede tener éxito. Ejemplos de tales entornos van desde juegos de mesa (por ejemplo, Ajedrez, Conecta Cuatro y Diplomacia) hasta ciertos entornos económicos (por ejemplo, subastas de N postores sobre un único bien). 2.1 Resumen del Modelo Nuestro enfoque es formalizar las actitudes mentales y comportamientos de un único agente adversario; consideramos cómo un único agente percibe el AE. La siguiente lista especifica las condiciones y estados mentales de un agente en un AE simple de suma cero: 1. El agente tiene la intención individual de que su propio objetivo se complete; 2. El agente tiene la creencia individual de que él y sus adversarios persiguen objetivos completamente opuestos (definidos a continuación) y que solo puede haber un ganador; 3. El agente tiene la creencia individual de que cada adversario tiene la intención de completar su propio objetivo conflictivo completo; 4. El agente tiene una creencia individual en el perfil (parcial) de sus adversarios. El ítem 3 es necesario, ya que podría ser el caso de que algún agente tenga un objetivo completamente conflictivo y esté considerando adoptar la intención de completarlo, pero aún no está comprometido a lograrlo. Esto podría ocurrir porque el agente aún no ha reflexionado sobre los efectos que adoptar esa intención podría tener en las otras intenciones que actualmente mantiene. En tales casos, es posible que ni siquiera se considere en un entorno adversarial. El ítem 4 establece que el agente debe tener alguna creencia sobre los perfiles de sus adversarios. El perfil representa todo el conocimiento que el agente tiene sobre su adversario: sus debilidades, capacidades estratégicas, objetivos, intenciones, confiabilidad y más. Puede ser dado explícitamente o puede ser aprendido a partir de observaciones de encuentros pasados. 2.2 Definiciones de modelos para estados mentales Utilizamos las definiciones de los operadores modales, predicados y metapredicados de Grosz y Krauss, tal como se definen en su formalización de SharedPlan [4]. Recordamos aquí algunos de los predicados y operadores que se utilizan en esa formalización: Int.To(Ai, α, Tn, Tα, C) representa las intenciones de Ai en el tiempo Tn de realizar una acción α en el tiempo Tα en el contexto de C. Int.Th(Ai, prop, Tn, Tprop, C) representa las intenciones de Ai en el tiempo Tn de que una cierta proposición prop se cumpla en el tiempo Tprop en el contexto de C. Los operadores de intención potencial, Pot.Int.To(...) y Pot.Int.Th(...), se utilizan para representar el estado mental cuando un agente considera adoptar una intención, pero no ha deliberado sobre la interacción de las otras intenciones que tiene. El operador Bel(Ai, f, Tf) representa al agente Ai creyendo en la afirmación expresada en la fórmula f, en el tiempo Tf. MB(A, f, Tf) representa la creencia mutua para un grupo de agentes A. Una instantánea del sistema encuentra que nuestro entorno se encuentra en algún estado e ∈ E de estados variables ambientales, y cada adversario en cualquier LAi ∈ L de posibles estados locales. En cualquier paso de tiempo dado, el sistema estará en algún mundo w del conjunto de todos los mundos posibles w ∈ W, donde w = E×LA1 ×LA2 × ...LAn, y n es el número de adversarios. Por ejemplo, en un juego de póker Texas Holdem, el estado local de un agente podría ser su propio conjunto de cartas (que es desconocido para su adversario) mientras que el entorno consistirá en el bote de apuestas y las cartas comunitarias (que son visibles para ambos jugadores). Una función de utilidad bajo esta formalización se define como un mapeo de un mundo posible w ∈ W a un elemento en , que expresa la deseabilidad del mundo, desde la perspectiva de un único agente. Normalmente normalizamos el rango a [0,1], donde 0 representa el mundo menos deseable posible, y 1 es el mundo más deseable. La implementación de la función de utilidad depende del dominio en cuestión. La siguiente lista especifica nuevos predicados, funciones, variables y constantes utilizadas en conjunto con las definiciones originales para la formalización del entorno adversarial: 1. φ es una acción nula (el agente no hace nada). 2. GAi es el conjunto de objetivos del agente Ai. Cada objetivo es un conjunto de predicados cuya satisfacción hace que el objetivo esté completo (usamos G∗ Ai ∈ GAi para representar un objetivo arbitrario del agente Ai). 3. gAi es el conjunto de subobjetivos del agente Ai. Los subobjetivos son predicados cuya satisfacción representa un hito importante hacia el logro del objetivo completo. gG∗ Ai ⊆ gAi es el conjunto de subobjetivos que son importantes para la finalización del objetivo G∗ Ai (usaremos g∗ G∗ Ai ∈ gG∗ Ai para representar un subobjetivo arbitrario). 4. P Aj Ai es el objeto de perfil que el agente Ai tiene sobre el agente Aj. 5. CA es un conjunto general de acciones para todos los agentes en A que se derivan de las restricciones del entorno. CAi ⊆ CA es el conjunto de acciones posibles del agente Ai. La expresión Do(Ai, α, Tα, w) se cumple cuando Ai realiza la acción α durante el intervalo de tiempo Tα en el mundo w. 7. La afirmación Achieve(G∗ Ai , α, w) es verdadera cuando se logra el objetivo G∗ Ai siguiendo la finalización de la acción α en el mundo w ∈ W, donde α ∈ CAi. El perfil (Ai, PAi Ai) es verdadero cuando el agente Ai tiene un perfil de objeto para el agente Aj. Definición 1. El conflicto total (FulConf) describe una interacción de suma cero donde solo se puede completar un solo objetivo de los objetivos en conflicto. FulConf(G∗ Ai , G∗ Aj ) ⇒ (∃α ∈ CAi , ∀w, β ∈ CAj ) (Achieve(G∗ Ai , α, w) ⇒ ¬Achieve(G∗ Aj , β, w)) ∨ (∃β ∈ CAj , ∀w, α ∈ CAi )(Achieve(G∗ Aj , β, w) ⇒ ¬Achieve(G∗ Ai , α, w)) Definición 2. El Conocimiento Adversarial (AdvKnow) es una función que devuelve un valor que representa la cantidad de The Sixth Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 551 agente de conocimiento Ai tiene sobre el perfil del agente Aj, en el tiempo Tn. Cuanto mayor sea el valor, más conocimiento tiene el agente Ai. AdvKnow: P Aj Ai × Tn → Definición 3. Eval: Esta <br>función de evaluación</br> devuelve un valor de utilidad esperada estimado para un agente en A, después de completar una acción de CA en algún estado del mundo w. Eval: A × CA × w → Definición 4. TrH - (Umbral) es una constante numérica en el rango [0,1] que representa un <br>valor umbral de la función de evaluación</br> (Eval). Una acción que produce una evaluación de utilidad estimada por encima del TrH se considera una acción altamente beneficiosa. El valor Eval es una estimación y no la función de utilidad real, la cual suele ser desconocida. Utilizar el valor real de utilidad para un agente racional fácilmente produciría el mejor resultado para ese agente. Sin embargo, los agentes generalmente no tienen las funciones de utilidad reales, sino más bien una estimación heurística de las mismas. Existen dos propiedades importantes que deben cumplir para la <br>función de evaluación</br>: Propiedad 1. La <br>función de evaluación</br> debería indicar que el estado del mundo más deseable es aquel en el que se logra el objetivo. Por lo tanto, una vez que se ha satisfecho el objetivo, no puede haber ninguna acción futura que pueda colocar al agente en un estado del mundo con un valor de Eval más alto. (∀Ai, G∗ Ai , α, β ∈ CAi , w ∈ W) Lograr(G∗ Ai , α, w) ⇒ Eval(Ai, α, w) ≥ Eval(Ai, β, w) Propiedad 2. La <br>función de evaluación</br> debe proyectar una acción que cause la finalización de un objetivo o subobjetivo a un valor que sea mayor que TrH (una acción altamente beneficiosa). (∀Ai, G∗ Ai ∈ GAi , α ∈ CAi , w ∈ W, g∗ GAi ∈ gGAi ) Lograr(G∗ Ai , α, w) ∨ Lograr(g∗ GAi , α, w) ⇒ Eval(Ai, α, w) ≥ TrH. ",
            "candidates": [],
            "error": [
                [
                    "función de evaluación",
                    "valor umbral de la función de evaluación",
                    "función de evaluación",
                    "función de evaluación",
                    "función de evaluación"
                ]
            ]
        },
        "beneficial action": {
            "translated_key": "",
            "is_in_text": true,
            "original_annotated_sentences": [
                "An Adversarial Environment Model for Bounded Rational Agents in Zero-Sum Interactions Inon Zuckerman1 , Sarit Kraus1 , Jeffrey S. Rosenschein2 , Gal Kaminka1 1 Department of Computer Science 2 The School of Engineering Bar-Ilan University and Computer Science Ramat-Gan, Israel Hebrew University, Jerusalem, Israel {zukermi,sarit,galk}@cs.biu.ac.il jeff@cs.huji.ac.il ABSTRACT Multiagent environments are often not cooperative nor collaborative; in many cases, agents have conflicting interests, leading to adversarial interactions.",
                "This paper presents a formal Adversarial Environment model for bounded rational agents operating in a zero-sum environment.",
                "In such environments, attempts to use classical utility-based search methods can raise a variety of difficulties (e.g., implicitly modeling the opponent as an omniscient utility maximizer, rather than leveraging a more nuanced, explicit opponent model).",
                "We define an Adversarial Environment by describing the mental states of an agent in such an environment.",
                "We then present behavioral axioms that are intended to serve as design principles for building such adversarial agents.",
                "We explore the application of our approach by analyzing log files of completed Connect-Four games, and present an empirical analysis of the axioms appropriateness.",
                "Categories and Subject Descriptors I.2.11 [Artificial Intelligence]: Distributed Artificial Intelligence-Intelligent agents,Multiagent Systems; I.2.4 [Artificial Intelligence]: Knowledge Representation Formalisms and Methods -Modal logic General Terms Design, Theory 1.",
                "INTRODUCTION Early research in multiagent systems (MAS) considered cooperative groups of agents; because individual agents had limited resources, or limited access to information (e.g., limited processing power, limited sensor coverage), they worked together by design to solve problems that individually they could not solve, or at least could not solve as efficiently.",
                "MAS research, however, soon began to consider interacting agents with individuated interests, as representatives of different humans or organizations with non-identical interests.",
                "When interactions are guided by diverse interests, participants may have to overcome disagreements, uncooperative interactions, and even intentional attempts to damage one another.",
                "When these types of interactions occur, environments require appropriate behavior from the agents situated in them.",
                "We call these environments Adversarial Environments, and call the clashing agents Adversaries.",
                "Models of cooperation and teamwork have been extensively explored in MAS through the axiomatization of mental states (e.g., [8, 4, 5]).",
                "However, none of this research dealt with adversarial domains and their implications for agent behavior.",
                "Our paper addresses this issue by providing a formal, axiomatized mental state model for a subset of adversarial domains, namely simple zero-sum adversarial environments.",
                "Simple zero-sum encounters exist of course in various twoplayer games (e.g., Chess, Checkers), but they also exist in n-player games (e.g., Risk, Diplomacy), auctions for a single good, and elsewhere.",
                "In these latter environments especially, using a utility-based adversarial search (such as the Min-Max algorithm) does not always provide an adequate solution; the payoff function might be quite complex or difficult to quantify, and there are natural computational limitations on bounded rational agents.",
                "In addition, traditional search methods (like Min-Max) do not make use of a model of the opponent, which has proven to be a valuable addition to adversarial planning [9, 3, 11].",
                "In this paper, we develop a formal, axiomatized model for bounded rational agents that are situated in a zero-sum adversarial environment.",
                "The model uses different modality operators, and its main foundations are the SharedPlans [4] model for collaborative behavior.",
                "We explore environment properties and the mental states of agents to derive behavioral axioms; these behavioral axioms constitute a formal model that serves as a specification and design guideline for agent design in such settings.",
                "We then investigate the behavior of our model empirically using the Connect-Four board game.",
                "We show that this game conforms to our environment definition, and analyze players behavior using a large set of completed match log 550 978-81-904262-7-5 (RPS) c 2007 IFAAMAS files.",
                "In addition, we use the results presented in [9] to discuss the importance of opponent modeling in our ConnectFour adversarial domain.",
                "The paper proceeds as follows.",
                "Section 2 presents the models formalization.",
                "Section 3 presents the empirical analysis and its results.",
                "We discuss related work in Section 4, and conclude and present future directions in Section 5. 2.",
                "ADVERSARIAL ENVIRONMENTS The adversarial environment model (denoted as AE) is intended to guide the design of agents by providing a specification of the capabilities and mental attitudes of an agent in an adversarial environment.",
                "We focus here on specific types of adversarial environments, specified as follows: 1.",
                "Zero-Sum Interactions: positive and negative utilities of all agents sum to zero; 2.",
                "Simple AEs: all agents in the environment are adversarial agents; 3.",
                "Bilateral AEs: AEs with exactly two agents; 4.",
                "Multilateral AEs: AEs of three or more agents.",
                "We will work on both bilateral and multilateral instantiations of zero-sum and simple environments.",
                "In particular, our adversarial environment model will deal with interactions that consist of N agents (N ≥ 2), where all agents are adversaries, and only one agent can succeed.",
                "Examples of such environments range from board games (e.g., Chess, Connect-Four, and Diplomacy) to certain economic environments (e.g., N-bidder auctions over a single good). 2.1 Model Overview Our approach is to formalize the mental attitudes and behaviors of a single adversarial agent; we consider how a single agent perceives the AE.",
                "The following list specifies the conditions and mental states of an agent in a simple, zero-sum AE: 1.",
                "The agent has an individual intention that its own goal will be completed; 2.",
                "The agent has an individual belief that it and its adversaries are pursuing full conflicting goals (defined below)there can be only one winner; 3.",
                "The agent has an individual belief that each adversary has an intention to complete its own full conflicting goal; 4.",
                "The agent has an individual belief in the (partial) profile of its adversaries.",
                "Item 3 is required, since it might be the case that some agent has a full conflicting goal, and is currently considering adopting the intention to complete it, but is, as of yet, not committed to achieving it.",
                "This might occur because the agent has not yet deliberated about the effects that adopting that intention might have on the other intentions it is currently holding.",
                "In such cases, it might not consider itself to even be in an adversarial environment.",
                "Item 4 states that the agent should hold some belief about the profiles of its adversaries.",
                "The profile represents all the knowledge the agent has about its adversary: its weaknesses, strategic capabilities, goals, intentions, trustworthiness, and more.",
                "It can be given explicitly or can be learned from observations of past encounters. 2.2 Model Definitions for Mental States We use Grosz and Krauss definitions of the modal operators, predicates, and meta-predicates, as defined in their SharedPlan formalization [4].",
                "We recall here some of the predicates and operators that are used in that formalization: Int.To(Ai, α, Tn, Tα, C) represents Ais intentions at time Tn to do an action α at time Tα in the context of C. Int.Th(Ai, prop, Tn, Tprop, C) represents Ais intentions at time Tn that a certain proposition prop holds at time Tprop in the context of C. The potential intention operators, Pot.Int.To(...) and Pot.Int.Th(...), are used to represent the mental state when an agent considers adopting an intention, but has not deliberated about the interaction of the other intentions it holds.",
                "The operator Bel(Ai, f, Tf ) represents agent Ai believing in the statement expressed in formula f, at time Tf .",
                "MB(A, f, Tf ) represents mutual belief for a group of agents A.",
                "A snapshot of the system finds our environment to be in some state e ∈ E of environmental variable states, and each adversary in any LAi ∈ L of possible local states.",
                "At any given time step, the system will be in some world w of the set of all possible worlds w ∈ W, where w = E×LA1 ×LA2 × ...LAn , and n is the number of adversaries.",
                "For example, in a Texas Holdem poker game, an agents local state might be its own set of cards (which is unknown to its adversary) while the environment will consist of the betting pot and the community cards (which are visible to both players).",
                "A utility function under this formalization is defined as a mapping from a possible world w ∈ W to an element in , which expresses the desirability of the world, from a single agent perspective.",
                "We usually normalize the range to [0,1], where 0 represents the least desirable possible world, and 1 is the most desirable world.",
                "The implementation of the utility function is dependent on the domain in question.",
                "The following list specifies new predicates, functions, variables, and constants used in conjunction with the original definitions for the adversarial environment formalization: 1. φ is a null action (the agent does not do anything). 2.",
                "GAi is the set of agent Ais goals.",
                "Each goal is a set of predicates whose satisfaction makes the goal complete (we use G∗ Ai ∈ GAi to represent an arbitrary goal of agent Ai). 3. gAi is the set of agent Ais subgoals.",
                "Subgoals are predicates whose satisfaction represents an important milestone toward achievement of the full goal. gG∗ Ai ⊆ gAi is the set of subgoals that are important to the completion of goal G∗ Ai (we will use g∗ G∗ Ai ∈ gG∗ Ai to represent an arbitrary subgoal). 4.",
                "P Aj Ai is the profile object agent Ai holds about agent Aj. 5.",
                "CA is a general set of actions for all agents in A which are derived from the environments constraints.",
                "CAi ⊆ CA is the set of agent Ais possible actions. 6.",
                "Do(Ai, α, Tα, w) holds when Ai performs action α over time interval Tα in world w. 7.",
                "Achieve(G∗ Ai , α, w) is true when goal G∗ Ai is achieved following the completion of action α in world w ∈ W, where α ∈ CAi . 8.",
                "Profile(Ai, PAi Ai ) is true when agent Ai holds an object profile for agent Aj.",
                "Definition 1.",
                "Full conflict (FulConf ) describes a zerosum interaction where only a single goal of the goals in conflict can be completed.",
                "FulConf(G∗ Ai , G∗ Aj ) ⇒ (∃α ∈ CAi , ∀w, β ∈ CAj ) (Achieve(G∗ Ai , α, w) ⇒ ¬Achieve(G∗ Aj , β, w)) ∨ (∃β ∈ CAj , ∀w, α ∈ CAi )(Achieve(G∗ Aj , β, w) ⇒ ¬Achieve(G∗ Ai , α, w)) Definition 2.",
                "Adversarial Knowledge (AdvKnow) is a function returning a value which represents the amount of The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 551 knowledge agent Ai has on the profile of agent Aj, at time Tn.",
                "The higher the value, the more knowledge agent Ai has.",
                "AdvKnow : P Aj Ai × Tn → Definition 3.",
                "Eval - This evaluation function returns an estimated expected utility value for an agent in A, after completing an action from CA in some world state w. Eval : A × CA × w → Definition 4.",
                "TrH - (Threshold) is a numerical constant in the [0,1] range that represents an evaluation function (Eval) threshold value.",
                "An action that yields an estimated utility evaluation above the TrH is regarded as a highly <br>beneficial action</br>.",
                "The Eval value is an estimation and not the real utility function, which is usually unknown.",
                "Using the real utility value for a rational agent would easily yield the best outcome for that agent.",
                "However, agents usually do not have the real utility functions, but rather a heuristic estimate of it.",
                "There are two important properties that should hold for the evaluation function: Property 1.",
                "The evaluation function should state that the most desirable world state is one in which the goal is achieved.",
                "Therefore, after the goal has been satisfied, there can be no future action that can put the agent in a world state with higher Eval value. (∀Ai, G∗ Ai , α, β ∈ CAi , w ∈ W) Achieve(G∗ Ai , α, w) ⇒ Eval(Ai, α, w) ≥ Eval(Ai, β, w) Property 2.",
                "The evaluation function should project an action that causes a completion of a goal or a subgoal to a value which is greater than TrH (a highly <br>beneficial action</br>). (∀Ai, G∗ Ai ∈ GAi , α ∈ CAi , w ∈ W, g∗ GAi ∈ gGAi ) Achieve(G∗ Ai , α, w) ∨ Achieve(g∗ GAi , α, w) ⇒ Eval(Ai, α, w) ≥ TrH.",
                "Definition 5.",
                "SetAction We define a set action (SetAction) as a set of action operations (either complex or basic actions) from some action sets CAi and CAj which, according to agent Ais belief, are attached together by a temporal and consequential relationship, forming a chain of events (action, and its following consequent action). (∀α1 , . . . , αu ∈ CAi , β1 , . . . , βv ∈ CAj , w ∈ W) SetAction(α1 , . . . , αu , β1 , . . . , βv , w) ⇒ ((Do(Ai, α1 , Tα1 , w) ⇒ Do(Aj, β1 , Tβ1 , w)) ⇒ Do(Ai, α2 , Tα2 , w) ⇒ . . . ⇒ Do(Ai, αu , Tαu , w)) The consequential relation might exist due to various environmental constraints (when one action forces the adversary to respond with a specific action) or due to the agents knowledge about the profile of its adversary.",
                "Property 3.",
                "As the knowledge we have about our adversary increases we will have additional beliefs about its behavior in different situations which in turn creates new set actions.",
                "Formally, if our AdvKnow at time Tn+1 is greater than AdvKnow at time Tn, then every SetAction known at time Tn is also known at time Tn+1.",
                "AdvKnow(P Aj Ai , Tn+1) > AdvKnow(P Aj Ai , Tn) ⇒ (∀α1 , . . . , αu ∈ CAi , β1 , . . . , βv ∈ CAj ) Bel(Aag, SetAction(α1 , . . . , αu , β1 , . . . , βv ), Tn) ⇒ Bel(Aag, SetAction(α1 , . . . , αu , β1 , . . . , βv ), Tn+1) 2.3 The Environment Formulation The following axioms provide the formal definition for a simple, zero-sum Adversarial Environment (AE).",
                "Satisfaction of these axioms means that the agent is situated in such an environment.",
                "It provides specifications for agent Aag to interact with its set of adversaries A with respect to goals G∗ Aag and G∗ A at time TCo at some world state w. AE(Aag, A, G∗ Aag , A1, . . . , Ak, G∗ A1 , . . . , G∗ Ak , Tn, w) 1.",
                "Aag has an Int.Th his goal would be completed: (∃α ∈ CAag , Tα) Int.Th(Aag, Achieve(G∗ Aag , α), Tn, Tα, AE) 2.",
                "Aag believes that it and each of its adversaries Ao are pursuing full conflicting goals: (∀Ao ∈ {A1, . . . , Ak}) Bel(Aag, FulConf(G∗ Aag , G∗ Ao ), Tn) 3.",
                "Aag believes that each of his adversaries in Ao has the Int.Th his conflicting goal G∗ Aoi will be completed: (∀Ao ∈ {A1, . . . , Ak})(∃β ∈ CAo , Tβ) Bel(Aag, Int.Th(Ao, Achieve(G∗ Ao , β), TCo, Tβ, AE), Tn) 4.",
                "Aag has beliefs about the (partial) profiles of its adversaries (∀Ao ∈ {A1, . . . , Ak}) (∃PAo Aag ∈ PAag )Bel(Aag, Profile(Ao, PAo Aag ), Tn) To build an agent that will be able to operate successfully within such an AE, we must specify behavioral guidelines for its interactions.",
                "Using a naive Eval maximization strategy to a certain search depth will not always yield satisfactory results for several reasons: (1) the search horizon problem when searching for a fixed depth; (2) the strong assumption of an optimally rational, unbounded resources adversary; (3) using an estimated evaluation function which will not give optimal results in all world states, and can be exploited [9].",
                "The following axioms specify the behavioral principles that can be used to differentiate between successful and less successful agents in the above Adversarial Environment.",
                "Those axioms should be used as specification principles when designing and implementing agents that should be able to perform well in such Adversarial Environments.",
                "The behavioral axioms represent situations in which the agent will adopt potential intentions to (Pot.Int.To(...)) perform an action, which will typically require some means-end reasoning to select a possible course of action.",
                "This reasoning will lead to the adoption of an Int.To(...) (see [4]).",
                "A1.",
                "Goal Achieving Axiom.",
                "The first axiom is the simplest case; when the agent Aag believes that it is one action (α) away from achieving his conflicting goal G∗ Aag , it should adopt the potential intention to do α and complete its goal. (∀Aag, α ∈ CAag , Tn, Tα, w ∈ W) (Bel(Aag, Do(Aag, α, Tα, w) ⇒ Achieve(G∗ Aag , α, w)) ⇒ Pot.Int.To(Aag, α, Tn, Tα, w) This somewhat trivial behavior is the first and strongest axiom.",
                "In any situation, when the agent is an action away from completing the goal, it should complete the action.",
                "Any fair Eval function would naturally classify α as the maximal value action (property 1).",
                "However, without explicit axiomatization of such behavior there might be situations where the agent will decide on taking another action for various reasons, due to its bounded decision resources.",
                "A2.",
                "Preventive Act Axiom.",
                "Being in an adversarial situation, agent Aag might decide to take actions that will damage one of its adversarys plans to complete its goal, even if those actions do not explicitly advance Aag towards its conflicting goal G∗ Aag .",
                "Such preventive action will take place when agent Aag has a belief about the possibility of its adversary Ao doing an action β that will give it a high 552 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) utility evaluation value (> TrH).",
                "Believing that taking action α will prevent the opponent from doing its β, it will adopt a potential intention to do α. (∀Aag, Ao ∈ A, α ∈ CAag , β ∈ CAo , Tn, Tβ, w ∈ W) (Bel(Aag, Do(Ao, β, Tβ, w) ∧ Eval(Ao, β, w) > TrH, Tn) ∧ Bel(Aag, Do(Aag, α, Tα, w) ⇒ ¬Do(Ao, β, Tβ, w), Tn) ⇒ Pot.Int.To(Aag, α, Tn, Tα, w) This axiom is a basic component of any adversarial environment.",
                "For example, looking at a Chess board game, a player could realize that it is about to be checkmated by its opponent, thus making a preventive move.",
                "Another example is a Connect Four game: when a player has a row of three chips, its opponent must block it, or lose.",
                "A specific instance of A1 occurs when the adversary is one action away from achieving its goal, and immediate preventive action needs to be taken by the agent.",
                "Formally, we have the same beliefs as stated above, with a changed belief that doing action β will cause agent Ao to achieve its goal.",
                "Proposition 1: Prevent or lose case. (∀Aag, Ao ∈ A, α ∈ CAag , β ∈ CAo , G∗ Ao , Tn, Tα, Tβ, w ∈ W) Bel(Aag, Do(Ao, β, Tβ, w) ⇒ Achieve(G∗ Ao , β, w), Tn) ∧ Bel(Aag, Do(Aag, α, Tα, w) ⇒ ¬Do(Ao, β, Tβ, w)) ⇒ Pot.Int.To(Aag, α, Tn, Tα, w) Sketch of proof: Proposition 1 can be easily derived from axiom A1 and the property 2 of the Eval function, which states that any action that causes a completion of a goal is a highly <br>beneficial action</br>.",
                "The preventive act behavior will occur implicitly when the Eval function is equal to the real world utility function.",
                "However, being bounded rational agents and dealing with an estimated evaluation function we need to explicitly axiomatize such behavior, for it will not always occur implicitly from the evaluation function.",
                "A3.",
                "Suboptimal Tactical Move Axiom.",
                "In many scenarios a situation may occur where an agent will decide not to take the current most <br>beneficial action</br> it can take (the action with the maximal utility evaluation value), because it believes that taking another action (with lower utility evaluation value) might yield (depending on the adversarys response) a future possibility for a highly <br>beneficial action</br>.",
                "This will occur most often when the Eval function is inaccurate and differs by a large extent from the Utility function.",
                "Put formally, agent Aag believes in a certain SetAction that will evolve according to its initial action and will yield a high beneficial value (> TrH) solely for it. (∀Aag, Ao ∈ A, Tn, w ∈ W) (∃α1 , . . . , αu ∈ CAi , β1 , . . . , βv ∈ CAj , Tα1 ) Bel(Aag, SetAction(α1 , . . . , αu , β1 , . . . , βv ), Tn) ∧ Bel(Aag, Eval(Ao, βv , w) < TrH < Eval(Aag, αu , w), Tn) ⇒ Pot.Int.To(Aag, α1 , Tn, Tα1 , w) An agent might believe that a chain of events will occur for various reasons due to the inevitable nature of the domain.",
                "For example, in Chess, we often observe the following: a move causes a check position, which in turn limits the opponents moves to avoiding the check, to which the first player might react with another check, and so on.",
                "The agent might also believe in a chain of events based on its knowledge of its adversarys profile, which allows it to foresee the adversarys movements with high accuracy.",
                "A4.",
                "Profile Detection Axiom.",
                "The agent can adjust its adversarys profiles by observations and pattern study (specifically, if there are repeated encounters with the same adversary).",
                "However, instead of waiting for profile information to be revealed, an agent can also initiate actions that will force its adversary to react in a way that will reveal profile knowledge about it.",
                "Formally, the axiom states that if all actions (γ) are not highly beneficial actions (< TrH), the agent can do action α in time Tα if it believes that it will result in a non-highly <br>beneficial action</br> β from its adversary, which in turn teaches it about the adversarys profile, i.e., gives a higher AdvKnow(P Aj Ai , Tβ). (∀Aag, Ao ∈ A, α ∈ CAag , β ∈ CAo , Tn, Tα, Tβ, w ∈ W) Bel(Aag, (∀γ ∈ CAag )Eval(Aag, γ, w) < TrH, Tn) ∧ Bel(Aag, Do(Aag, α, Tα, w) ⇒ Do(Ao, β, Tβ, w), Tn) ∧ Bel(Aag, Eval(Ao, β, w) < TrH) ∧ Bel(Aag, AdvKnow(P Aj Ai , Tβ) > AdvKnow(P Aj Ai , Tn), Tn) ⇒ Pot.Int.To(Aag, α, Tn, Tα, w) For example, going back to the Chess board game scenario, consider starting a game versus an opponent about whom we know nothing, not even if it is a human or a computerized opponent.",
                "We might start playing a strategy that will be suitable versus an average opponent, and adjust our game according to its level of play.",
                "A5.",
                "Alliance Formation Axiom The following behavioral axiom is relevant only in a multilateral instantiation of the adversarial environment (obviously, an alliance cannot be formed in a bilateral, zero-sum encounter).",
                "In different situations during a multilateral interaction, a group of agents might believe that it is in their best interests to form a temporary alliance.",
                "Such an alliance is an agreement that constrains its members behavior, but is believed by its members to enable them to achieve a higher utility value than the one achievable outside of the alliance.",
                "As an example, we can look at the classical Risk board game, where each player has an individual goal of being the sole conquerer of the world, a zero-sum game.",
                "However, in order to achieve this goal, it might be strategically wise to make short-term ceasefire agreements with other players, or to join forces and attack an opponent who is stronger than the rest.",
                "An alliances terms defines the way its members should act.",
                "It is a set of predicates, denoted as Terms, that is agreed upon by the alliance members, and should remain true for the duration of the alliance.",
                "For example, the set Terms in the Risk scenario, could contain the following predicates: 1.",
                "Alliance members will not attack each other on territories X, Y and Z; 2.",
                "Alliance members will contribute C units per turn for attacking adversary Ao; 3.",
                "Members are obligated to stay as part of the alliance until time Tk or until adversarys Ao army is smaller than Q.",
                "The set Terms specifies inter-group constraints on each of the alliance members (∀Aal i ∈ Aal ⊆ A) set of actions Cal i ⊆ C. Definition 6.",
                "Al val - the total evaluation value that agent Ai will achieve while being part of Aal is the sum of Evali (Eval for Ai) of each of Aal j Eval values after taking their own α actions (via the agent(α) predicate): Al val(Ai, Cal , Aal , w) = α∈Cal Evali(Aal j , agent(α), w) Definition 7.",
                "Al TrH - is a number representing an Al val The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 553 threshold; above it, the alliance can be said to be a highly beneficial alliance.",
                "The value of Al TrH will be calculated dynamically according to the progress of the interaction, as can be seen in [7].",
                "After an alliance is formed, its members are now working in their normal adversarial environment, as well as according to the mental states and axioms required for their interactions as part of the alliance.",
                "The following Alliance model (AL) specifies the conditions under which the group Aal can be said to be in an alliance and working with a new and constrained set of actions Cal , at time Tn.",
                "AL(Aal , Cal , w, Tn) 1.",
                "Aal has a MB that all members are part of Aal : MB(Aal , (∀Aal i ∈ Aal )member(Aal i , Aal ), Tn) 2.",
                "Aal has a MB that the group be maintained: MB(Aal , (∀Aal i ∈ Aal )Int.Th (Ai, member(Ai, Aal ), Tn, Tn+1, Co), Tn) 3.",
                "Aal has a MB that being members gives them high utility value: MB(Aal , (∀Aal i ∈ Aal )Al val(Aal i , Cal , Aal , w) ≥ Al TrH, Tn) Members profiles are a crucial part of successful alliances.",
                "We assume that agents that have more accurate profiles of their adversaries will be more successful in such environments.",
                "Such agents will be able to predict when a member is about to breach the alliances contract (item 2 in the above model), and take counter measures (when item 3 will falsify).",
                "The robustness of the alliance is in part a function of its members trustfulness measure, objective position estimation, and other profile properties.",
                "We should note that an agent can simultaneously be part of more than one alliance.",
                "Such a temporary alliance, where the group members do not have a joint goal but act collaboratively for the interest of their own individual goals, is classified as a Treatment Group by modern psychologists [12] (in contrast to a Task Group, where its members have a joint goal).",
                "The Shared Activity model as presented in [5] modeled Treatment Group behavior using the same SharedPlans formalization.",
                "When comparing both definitions of an alliance and a Treatment Group we found an unsurprising resemblance between both models: the environment models definitions are almost identical (see SAs definitions in [5]), and their Selfish-Act and Cooperative Act axioms conform to our adversarial agents behavior.",
                "The main distinction between both models is the integration of a Helpful-behavior act axiom, in the Shared Activity which cannot be part of ours.",
                "This axiom states that an agent will consider taking action that will lower its Eval value (to a certain lower bound), if it believes that a group partner will gain a significant benefit.",
                "Such behavior cannot occur in a pure adversarial environment (as a zero-sum game is), where the alliance members are constantly on watch to manipulate their alliance to their own advantage.",
                "A6.",
                "Evaluation Maximization Axiom.",
                "In a case when all other axioms are inapplicable, we will proceed with the action that maximizes the heuristic value as computed in the Eval function. (∀Aag, Ao ∈ A, α ∈ Cag, Tn, w ∈ W) Bel(Aag, (∀γ ∈ Cag)Eval(Aag, α, w) ≥ Eval(Aag, γ, w), Tn) ⇒ Pot.Int.To(Aag, α, Tn, Tα, w) T1.",
                "Optimality on Eval = Utility The above axiomatic model handles situations where the Utility is unknown and the agents are bounded rational agents.",
                "The following theorem shows that in bilateral interactions, where the agents have the real Utility function (i.e., Eval = Utility) and are rational agents, the axioms provide the same optimal result as classic adversarial search (e.g., Min-Max).",
                "Theorem 1.",
                "Let Ae ag be an unbounded rational AE agent using the Eval heuristic evaluation function, Au ag be the same agent using the true Utility function, and Ao be a sole unbounded utility-based rational adversary.",
                "Given that Eval = Utility: (∀α ∈ CAu ag , α ∈ CAe ag , Tn, w ∈ W) Pot.Int.To(Au ag, α, Tn, Tα, w) → Pot.Int.To(Ae ag, α , Tn, Tα, w) ∧ ((α = α ) ∨ (Utility(Au ag, α, w) = Eval(Ae ag, α , w))) Sketch of proof - Given that Au ag has the real utility function and unbounded resources, it can generate the full game tree and run the optimal MinMax algorithm to choose the highest utility value action, which we denote by, α.",
                "The proof will show that Ae ag, using the AE axioms, will select the same or equal utility α (when there is more than one action with the same max utility) when Eval = Utility. (A1) Goal achieving axiom - suppose there is an α such that its completion will achieve Au ags goal.",
                "It will obtain the highest utility by Min-Max for Au ag.",
                "The Ae ag agent will select α or another action with the same utility value via A1.",
                "If such α does not exist, Ae ag cannot apply this axiom, and proceeds to A2. (A2) Preventive act axiom - (1) Looking at the basic case (see Prop1), if there is a β which leads Ao to achieve its goal, then a preventive action α will yield the highest utility for Au ag.",
                "Au ag will choose it through the utility, while Ae ag will choose it through A2. (2) In the general case, β is a highly <br>beneficial action</br> for Ao, thus yields low utility for Au ag, which will guide it to select an α that will prevent β, while Ae ag will choose it through A2.1 If such β does not exist for Ao, then A2 is not applicable, and Ae ag can proceed to A3. (A3) Suboptimal tactical move axiom - When using a heuristic Eval function, Ae ag has a partial belief in the profile of its adversary (item 4 in AE model), which may lead it to believe in SetActions (Prop1).",
                "In our case, Ae ag is holding a full profile on its optimal adversary and knows that Ao will behave optimally according to the real utility values on the complete search tree, therefore, any belief about suboptimal SetAction cannot exist, yielding this axiom inapplicable.",
                "Ae ag will proceed to A4. (A4) Profile detection axiom - Given that Ae ag has the full profile of Ao, none of Ae ags actions can increase its knowledge.",
                "That axiom will not be applied, and the agent will proceed with A6 (A5 will be disregarded because the interaction is bilateral). (A6) Evaluation maximization axiom - This axiom will select the max Eval for Ae ag.",
                "Given that Eval = Utility, the same α that was selected by Au ag will be selected. 3.",
                "EVALUATION The main purpose of our experimental analysis is to evaluate the models behavior and performance in a real adversarial environment.",
                "This section investigates whether bounded 1 A case where following the completion of β there exists a γ which gives high utility for Agent Au ag, cannot occur because Ao uses the same utility, and γs existence will cause it to classify β as a low utility action. 554 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) rational agents situated in such adversarial environments will be better off applying our suggested behavioral axioms. 3.1 The Domain To explore the use of the above model and its behavioral axioms, we decided to use the Connect-Four game as our adversarial environment.",
                "Connect-Four is a 2-player, zerosum game which is played using a 6x7 matrix-like board.",
                "Each turn, a player drops a disc into one of the 7 columns (the set of 21 discs is usually colored yellow for player 1 and red for player 2; we will use White and Black respectively to avoid confusion).",
                "The winner is the first player to complete a horizontal, vertical, or diagonal set of four discs with its color.",
                "On very rare occasions, the game might end in a tie if all the empty grids are filled, but no player managed to create a 4-disc set.",
                "The Connect-Four game was solved in [1], where it is shown that the first player (playing with the white discs) can force a win by starting in the middle column (column 4) and playing optimally However, the optimal strategy is very complex, and difficult to follow even for complex bounded rational agents, such as human players.",
                "Before we can proceed checking agent behavior, we must first verify that the domain conforms to the adversarial environments definition as given above (which the behavioral axioms are based on).",
                "First, when playing a Connect-Four game, the agent has an intention to win the game (item 1).",
                "Second (item 2), our agent believes that in Connect-Four there can only be one winner (or no winner at all in the rare occurrence of a tie).",
                "In addition, our agent believes that its opponent to the game will try to win (item 3), and we hope it has some partial knowledge (item 4) about its adversary (this knowledge can vary from nothing, through simple facts such as age, to strategies and weaknesses).",
                "Of course, not all Connect-Four encounters are adversarial.",
                "For example, when a parent is playing the game with its child, the following situation might occur: the child, having a strong incentive to win, treats the environment as adversarial (it intends to win, understands that there can only be one winner, and believes that its parent is trying to beat him).",
                "However, the parents point of view might see the environment as an educational one, where its goal is not to win the game, but to cause enjoyment or practice strategic reasoning.",
                "In such an educational environment, a new set of behavioral axioms might be more beneficial to the parents goals than our suggested adversarial behavioral axioms. 3.2 Axiom Analysis After showing that the Connect-Four game is indeed a zero-sum, bilateral adversarial environment, the next step is to look at players behaviors during the game and check whether behaving according to our model does improve performance.",
                "To do so we have collected log files from completed Connect-Four games that were played by human players over the Internet.",
                "Our collected log file data came from Play by eMail (PBeM) sites.",
                "These are web sites that host email games, where each move is taken by an email exchange between the server and the players.",
                "Many such sites archives contain real competitive interactions, and also maintain a ranking system for their members.",
                "Most of the data we used can be found in [6].",
                "As can be learned from [1], Connect-Four has an optimal strategy and a considerable advantage for the player who starts the game (which we call the White player).",
                "We will concentrate in our analysis on the second players moves (to be called Black).",
                "The White player, being the first to act, has the so-called initiative advantage.",
                "Having the advantage and a good strategy will keep the Black player busy reacting to its moves, instead of initiating threats.",
                "A threat is a combination of three discs of the same color, with an empty spot for the fourth winning disk.",
                "An open threat is a threat that can be realized in the opponents next move.",
                "In order for the Black player to win, it must somehow turn the tide, take the advantage and start presenting threats to the White player.",
                "We will explore Black players behavior and their conformance to our axioms.",
                "To do so, we built an application that reads log files and analyzes the Black players moves.",
                "The application contains two main components: (1) a Min-Max algorithm for evaluation of moves; (2) open threats detector for the discovering of open threats.",
                "The Min-Max algorithm will work to a given depth, d and for each move α will output the heuristic value for the next action taken by the player as written in the log file, h(α), alongside the maximum heuristic value, maxh(α), that could be achieved prior to taking the move (obviously, if h(α) = maxh(α), then the player did not do the optimal move heuristically).",
                "The threat detectors job is to notify if some action was taken in order to block an open threat (not blocking an open threat will probably cause the player to lose in the opponents next move).",
                "The heuristic function used by Min-Max to evaluate the players utility is the following function, which is simple to compute, yet provides a reasonable challenge to human opponents: Definition 8.",
                "Let Group be an adjacent set of four squares that are horizontal, vertical, or diagonal.",
                "Groupn b (Groupn w) be a Group with n pieces of the black (white) color and 4−n empty squares. h = ((Group1 b ∗α)+(Group2 b ∗β)+(Group3 b ∗γ)+(Group4 b ∗∞)) − ((Group1 w ∗α)+(Group2 w ∗β)+(Group3 w ∗γ)+(Group4 w ∗∞)) The values of α, β and δ can vary to form any desired linear combination; however, it is important to value them with the α < β < δ ordering in mind (we used 1, 4, and 8 as their respective values).",
                "Groups of 4 discs of the same color means victory, thus discovery of such a group will result in ∞ to ensure an extreme value.",
                "We now use our estimated evaluation function to evaluate the Black players actions during the Connect-Four adversarial interaction.",
                "Each game from the log file was input into the application, which processed and output a reformatted log file containing the h value of the current move, the maxh value that could be achieved, and a notification if an open threat was detected.",
                "A total of 123 games were analyzed (57 with White winning, and 66 with Black winning).",
                "A few additional games were manually ignored in the experiment, due to these problems: a player abandoning the game while the outcome is not final, or a blunt irrational move in the early stages of the game (e.g., not blocking an obvious winning group in the first opening moves).",
                "In addition, a single tie game was also removed.",
                "The simulator was run to a search depth of 3 moves.",
                "We now proceed to analyze the games with respect to each behavioral axiom.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 555 Table 1: Average heuristic difference analysis Black losses Black Won Avg minh -17.62 -12.02 Avg 3 lowest h moves (min3 h) -13.20 -8.70 3.2.1 Affirming the Suboptimal tactical move axiom The following section presents the heuristic evaluations of the Min-Max algorithm for each action, and checks the amount and extent of suboptimal tactical actions and their implications on performance.",
                "Table 1 shows results and insights from the games heuristic analysis, when search depth equals 3 (this search depth was selected for the results to be comparable to [9], see Section 3.2.3).",
                "The tables heuristic data is the difference between the present maximal heuristic value and the heuristic value of the action that was eventually taken by the player (i.e., the closer the number is to 0, the closer the action was to the maximum heuristic action).",
                "The first row presents the difference values of the action that had the maximal difference value among all the Black players actions in a given game, as averaged over all Blacks winning and losing games (see respective columns).",
                "In games in which the Black player loses, its average difference value was -17.62, while in games in which the Black player won, its average was -12.02.",
                "The second row expands the analysis by considering the 3 highest heuristic difference actions, and averaging them.",
                "In that case, we notice an average heuristic difference of 5 points between games which the Black player loses and games in which it wins.",
                "Nevertheless, the importance of those numbers is that they allowed us to take an educated guess on a threshold number of 11.5, as the value of the TrH constant, which differentiates between normal actions and highly beneficial ones.",
                "After finding an approximated TrH constant, we can proceed with an analysis of the importance of suboptimal moves.",
                "To do so we took the subset of games in which the minimum heuristic difference value for Blacks actions was 11.5.",
                "As presented in Table 2, we can see the different min3 h average of the 3 largest ranges and the respective percentage of games won.",
                "The first row shows that the Black player won only 12% of the games in which the average of its 3 highest heuristically difference actions (min3 h) was smaller than the suggested threshold, TrH = 11.5.",
                "The second row shows a surprising result: it seems that when min3 h > −4 the Black player rarely wins.",
                "Intuition would suggest that games in which the action evaluation values were closer to the maximal values will result in more winning games for Black.",
                "However, it seems that in the Connect-Four domain, merely responding with somewhat easily expected actions, without initiating a few surprising and suboptimal moves, does not yield good results.",
                "The last row sums up the main insights from the analysis; most of Blacks wins (83%) came when its min3 h was in the range of -11.5 to -4.",
                "A close inspection of those Black winning games shows the following pattern behind the numbers: after standard opening moves, Black suddenly drops a disc into an isolated column, which seems a waste of a move.",
                "White continues to build its threats, while usually disregarding Blacks last move, which in turn uses the isolated disc as an anchor for a future winning threat.",
                "The results show that it was beneficial for the Black player Table 2: Blacks winnings percentages % of games min3 h < −11.5 12% min3 h > −4 5% −11.5 ≤ min3 h ≤ −4 83% to take suboptimal actions and not give the current highest possible heuristic value, but will not be too harmful for its position (i.e., will not give high beneficial value to its adversary).",
                "As it turned out, learning the threshold is an important aspect of success: taking wildly risky moves (min3 h < −11.5) or trying to avoid them (min3 h > −4) reduces the Black players winning chances by a large margin. 3.2.2 Affirming the Profile Monitoring Axiom In the task of showing the importance of monitoring ones adversaries profiles, our log files could not be used because they did not contain repeated interactions between players, which are needed to infer the players knowledge about their adversaries.",
                "However, the importance of opponent modeling and its use in attaining tactical advantages was already studied in various domains ([3, 9] are good examples).",
                "In a recent paper, Markovitch and Reger [9] explored the notion of learning and exploitation of opponent weakness in competitive interactions.",
                "They apply simple learning strategies by analyzing examples from past interactions in a specific domain.",
                "They also used the Connect-Four adversarial domain, which can now be used to understand the importance of monitoring the adversarys profile.",
                "Following the presentation of their theoretical model, they describe an extensive empirical study and check the agents performance after learning the weakness model with past examples.",
                "One of the domains used as a competitive environment was the same Connect-Four game (Checkers was the second domain).",
                "Their heuristic function was identical to ours with three different variations (H1, H2, and H3) that are distinguished from one another in their linear combination coefficient values.",
                "The search depth for the players was 3 (as in our analysis).",
                "Their extensive experiments check and compare various learning strategies, risk factors, predefined feature sets and usage methods.",
                "The bottom line is that the Connect-Four domain shows an improvement from a 0.556 winning rate before modeling to a 0.69 after modeling (page 22).",
                "Their conclusions, showing improved performance when holding and using the adversarys model, justify the effort to monitor the adversary profile for continuous and repeated interactions.",
                "An additional point that came up in their experiments is the following: after the opponent weakness model has been learned, the authors describe different methods of integrating the opponent weakness model into the agents decision strategy.",
                "Nevertheless, regardless of the specific method they chose to work with, all integration methods might cause the agent to take suboptimal decisions; it might cause the agent to prefer actions that are suboptimal at the present decision junction, but which might cause the opponent to react in accordance with its weakness model (as represented by our agent) which in turn will be beneficial for us in the future.",
                "The agents behavior, as demonstrated in [9] further confirms and strengthens our Suboptimal Tactical Axiom as discussed in the previous section. 556 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 3.2.3 Additional Insights The need for the Goal Achieving, Preventive Act, and Evaluation Maximization axioms are obvious, and need no further verification.",
                "However, even with respect to those axioms, a few interesting insights came up in the log analysis.",
                "The Goal achieving and Preventive Act axioms, though theoretically trivial, seem to provide some challenge to a human player.",
                "In the initial inspection of the logs, we encountered few games2 where a player, for inexplicable reasons, did not block the other from winning or failed to execute its own winning move.",
                "We can blame those faults on the humans lack of attention, or a typing error in its move reply; nevertheless, those errors might occur in bounded rational agents, and the appropriate behavior needs to be axiomatized.",
                "A typical Connect-Four game revolves around generating threats and blocking them.",
                "In our analysis we looked for explicit preventive actions, i.e., moves that block a group of 3 discs, or that remove a future threat (in our limited search horizon).",
                "We found that in 83% of the total games there was at least one preventive action taken by the Black player.",
                "It was also found that Black averaged 2.8 preventive actions per game on the games in which it lost, while averaging 1.5 preventive actions per game when winning.",
                "It seems that Black requires 1 or 2 preventive actions to build its initial taking position, before starting to present threats.",
                "If it did not manage to win, it will usually prevent an extra threat or two before succumbing to White. 4.",
                "RELATED WORK Much research deals with the axiomatization of teamwork and mental states of individuals: some models use knowledge and belief [10], others have models of goals and intentions [8, 4].",
                "However, all these formal theories deal with agent teamwork and cooperation.",
                "As far as we know, our model is the first to provide a formalized model for explicit adversarial environments and agents behavior in it.",
                "The classical Min-Max adversarial search algorithm was the first attempt to integrate the opponent into the search space with a weak assumption of an optimally playing opponent.",
                "Since then, much effort has gone into integrating the opponent model into the decision procedure to predict future behavior.",
                "The M∗ algorithm presented by Carmel and Markovitch [2] showed a method of incorporating opponent models into adversary search, while in [3] they used learning to provide a more accurate opponent model in a 2player repeated game environment, where agents strategies were modeled as finite automata.",
                "Additional Adversarial planning work was done by Willmott et al. [13], which provided an adversarial planning approach to the game of GO.",
                "The research mentioned above dealt with adversarial search and the integration of opponent models into classical utilitybased search methods.",
                "That work shows the importance of opponent modeling and the ability to exploit it to an agents advantage.",
                "However, the basic limitations of those search methods still apply; our model tries to overcome those limitations by presenting a formal model for a new, mental state-based adversarial specification. 5.",
                "CONCLUSIONS We presented an Adversarial Environment model for a 2 These were later removed from the final analysis. bounded rational agent that is situated in an N-player, zerosum environment.",
                "We used the SharedPlans formalization to define the model and the axioms that agents can apply as behavioral guidelines.",
                "The model is meant to be used as a guideline for designing agents that need to operate in such adversarial environments.",
                "We presented empirical results, based on ConnectFour log file analysis, that exemplify the model and the axioms for a bilateral instance of the environment.",
                "The results we presented are a first step towards an expanded model that will cover all types of adversarial environments, for example, environments that are non-zero-sum, and environments that contain natural agents that are not part of the direct conflict.",
                "Those challenges and more will be dealt with in future research. 6.",
                "ACKNOWLEDGMENT This research was supported in part by Israel Science Foundation grants #1211/04 and #898/05. 7.",
                "REFERENCES [1] L. V. Allis.",
                "A knowledge-based approach of Connect-Four - the game is solved: White wins.",
                "Masters thesis, Free University, Amsterdam, The Netherlands, 1988. [2] D. Carmel and S. Markovitch.",
                "Incorporating opponent models into adversary search.",
                "In Proceedings of the Thirteenth National Conference on Artificial Intelligence, pages 120-125, Portland, OR, 1996. [3] D. Carmel and S. Markovitch.",
                "Opponent modeling in multi-agent systems.",
                "In G. Weiß and S. Sen, editors, Adaptation and Learning in Multi-Agent Systems, pages 40-52.",
                "Springer-Verlag, 1996. [4] B. J. Grosz and S. Kraus.",
                "Collaborative plans for complex group action.",
                "Artificial Intelligence, 86(2):269-357, 1996. [5] M. Hadad, G. Kaminka, G. Armon, and S. Kraus.",
                "Supporting collaborative activity.",
                "In Proc. of AAAI-2005, pages 83-88, Pittsburgh, 2005. [6] http://www.gamerz.net/˜pbmserv/. [7] S. Kraus and D. Lehmann.",
                "Designing and building a negotiating automated agent.",
                "Computational Intelligence, 11:132-171, 1995. [8] H. J. Levesque, P. R. Cohen, and J. H. T. Nunes.",
                "On acting together.",
                "In Proc. of AAAI-90, pages 94-99, Boston, MA, 1990. [9] S. Markovitch and R. Reger.",
                "Learning and exploiting relative weaknesses of opponent agents.",
                "Autonomous Agents and Multi-Agent Systems, 10(2):103-130, 2005. [10] Y. M. Ronald Fagin, Joseph Y. Halpern and M. Y. Vardi.",
                "Reasoning about knowledge.",
                "MIT Press, Cambridge, Mass., 1995. [11] P. Thagard.",
                "Adversarial problem solving: Modeling an oponent using explanatory coherence.",
                "Cognitive Science, 16(1):123-149, 1992. [12] R. W. Toseland and R. F. Rivas.",
                "An Introduction to Group Work Practice.",
                "Prentice Hall, Englewood Cliffs, NJ, 2nd edition edition, 1995. [13] S. Willmott, J. Richardson, A. Bundy, and J. Levine.",
                "An adversarial planning approach to Go.",
                "Lecture Notes in Computer Science, 1558:93-112, 1999.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 557"
            ],
            "original_annotated_samples": [
                "An action that yields an estimated utility evaluation above the TrH is regarded as a highly <br>beneficial action</br>.",
                "The evaluation function should project an action that causes a completion of a goal or a subgoal to a value which is greater than TrH (a highly <br>beneficial action</br>). (∀Ai, G∗ Ai ∈ GAi , α ∈ CAi , w ∈ W, g∗ GAi ∈ gGAi ) Achieve(G∗ Ai , α, w) ∨ Achieve(g∗ GAi , α, w) ⇒ Eval(Ai, α, w) ≥ TrH.",
                "Proposition 1: Prevent or lose case. (∀Aag, Ao ∈ A, α ∈ CAag , β ∈ CAo , G∗ Ao , Tn, Tα, Tβ, w ∈ W) Bel(Aag, Do(Ao, β, Tβ, w) ⇒ Achieve(G∗ Ao , β, w), Tn) ∧ Bel(Aag, Do(Aag, α, Tα, w) ⇒ ¬Do(Ao, β, Tβ, w)) ⇒ Pot.Int.To(Aag, α, Tn, Tα, w) Sketch of proof: Proposition 1 can be easily derived from axiom A1 and the property 2 of the Eval function, which states that any action that causes a completion of a goal is a highly <br>beneficial action</br>.",
                "In many scenarios a situation may occur where an agent will decide not to take the current most <br>beneficial action</br> it can take (the action with the maximal utility evaluation value), because it believes that taking another action (with lower utility evaluation value) might yield (depending on the adversarys response) a future possibility for a highly <br>beneficial action</br>.",
                "Formally, the axiom states that if all actions (γ) are not highly beneficial actions (< TrH), the agent can do action α in time Tα if it believes that it will result in a non-highly <br>beneficial action</br> β from its adversary, which in turn teaches it about the adversarys profile, i.e., gives a higher AdvKnow(P Aj Ai , Tβ). (∀Aag, Ao ∈ A, α ∈ CAag , β ∈ CAo , Tn, Tα, Tβ, w ∈ W) Bel(Aag, (∀γ ∈ CAag )Eval(Aag, γ, w) < TrH, Tn) ∧ Bel(Aag, Do(Aag, α, Tα, w) ⇒ Do(Ao, β, Tβ, w), Tn) ∧ Bel(Aag, Eval(Ao, β, w) < TrH) ∧ Bel(Aag, AdvKnow(P Aj Ai , Tβ) > AdvKnow(P Aj Ai , Tn), Tn) ⇒ Pot.Int.To(Aag, α, Tn, Tα, w) For example, going back to the Chess board game scenario, consider starting a game versus an opponent about whom we know nothing, not even if it is a human or a computerized opponent."
            ],
            "translated_annotated_samples": [
                "Una acción que produce una evaluación de utilidad estimada por encima del TrH se considera una <br>acción altamente beneficiosa</br>.",
                "La función de evaluación debe proyectar una acción que cause la finalización de un objetivo o subobjetivo a un valor que sea mayor que TrH (una <br>acción altamente beneficiosa</br>). (∀Ai, G∗ Ai ∈ GAi , α ∈ CAi , w ∈ W, g∗ GAi ∈ gGAi ) Lograr(G∗ Ai , α, w) ∨ Lograr(g∗ GAi , α, w) ⇒ Eval(Ai, α, w) ≥ TrH.",
                "Proposición 1: Caso de prevención o pérdida. (∀Aag, Ao ∈ A, α ∈ CAag , β ∈ CAo , G∗ Ao , Tn, Tα, Tβ, w ∈ W) Bel(Aag, Do(Ao, β, Tβ, w) ⇒ Achieve(G∗ Ao , β, w), Tn) ∧ Bel(Aag, Do(Aag, α, Tα, w) ⇒ ¬Do(Ao, β, Tβ, w)) ⇒ Pot.Int.To(Aag, α, Tn, Tα, w) Esquema de prueba: La Proposición 1 puede derivarse fácilmente del axioma A1 y de la propiedad 2 de la función Eval, que establece que cualquier acción que cause la finalización de un objetivo es una <br>acción altamente beneficiosa</br>.",
                "En muchos escenarios, puede ocurrir una situación en la que un agente decida no tomar la acción más beneficiosa actual que puede tomar (la acción con el valor de evaluación de utilidad máximo), porque cree que tomar otra acción (con un valor de evaluación de utilidad más bajo) podría generar (dependiendo de la respuesta del adversario) una posibilidad futura para una acción altamente beneficiosa.",
                "Formalmente, el axioma establece que si todas las acciones (γ) no son acciones altamente beneficiosas (< TrH), el agente puede realizar la acción α en el tiempo Tα si cree que resultará en una <br>acción no altamente beneficiosa</br> β por parte de su adversario, lo que a su vez le enseña sobre el perfil del adversario, es decir, proporciona un mayor AdvKnow(P Aj Ai , Tβ). (∀Aag, Ao ∈ A, α ∈ CAag , β ∈ CAo , Tn, Tα, Tβ, w ∈ W) Bel(Aag, (∀γ ∈ CAag )Eval(Aag, γ, w) < TrH, Tn) ∧ Bel(Aag, Do(Aag, α, Tα, w) ⇒ Do(Ao, β, Tβ, w), Tn) ∧ Bel(Aag, Eval(Ao, β, w) < TrH) ∧ Bel(Aag, AdvKnow(P Aj Ai , Tβ) > AdvKnow(P Aj Ai , Tn), Tn) ⇒ Pot.Int.To(Aag, α, Tn, Tα, w) Por ejemplo, volviendo al escenario del juego de ajedrez, consideremos comenzar una partida contra un oponente del que no sabemos nada, ni siquiera si es humano o un oponente computarizado."
            ],
            "translated_text": "Un modelo de entorno adversarial para agentes racionales limitados en interacciones de suma cero Inon Zuckerman1, Sarit Kraus1, Jeffrey S. Rosenschein2, Gal Kaminka1 1 Departamento de Ciencias de la Computación 2 Escuela de Ingeniería Universidad Bar-Ilan y Ciencias de la Computación Ramat-Gan, Israel Universidad Hebrea, Jerusalén, Israel {zukermi, sarit, galk}@cs.biu.ac.il jeff@cs.huji.ac.il RESUMEN Los entornos multiagentes a menudo no son cooperativos ni colaborativos; en muchos casos, los agentes tienen intereses conflictivos, lo que lleva a interacciones adversariales. Este documento presenta un modelo formal de Entorno Adversarial para agentes racionales acotados que operan en un entorno de suma cero. En tales entornos, los intentos de utilizar métodos de búsqueda basados en utilidad clásica pueden plantear una variedad de dificultades (por ejemplo, modelar implícitamente al oponente como un maximizador de utilidad omnisciente, en lugar de aprovechar un modelo de oponente más matizado y explícito). Definimos un Entorno Adversario al describir los estados mentales de un agente en dicho entorno. Luego presentamos axiomas de comportamiento que están destinados a servir como principios de diseño para construir agentes adversarios. Exploramos la aplicación de nuestro enfoque mediante el análisis de archivos de registro de juegos de Conecta Cuatro completados, y presentamos un análisis empírico de la adecuación de los axiomas. Categorías y Descriptores de Asignaturas I.2.11 [Inteligencia Artificial]: Inteligencia Artificial Distribuida-Agentes Inteligentes, Sistemas Multiagente; I.2.4 [Inteligencia Artificial]: Formalismos y Métodos de Representación del Conocimiento - Lógica Modal Términos Generales Diseño, Teoría 1. Introducción: En las primeras investigaciones en sistemas multiagente (SMA) se consideraban grupos cooperativos de agentes; debido a que los agentes individuales tenían recursos limitados o acceso limitado a la información (por ejemplo, potencia de procesamiento limitada, cobertura sensorial limitada), trabajaban juntos por diseño para resolver problemas que individualmente no podían resolver, o al menos no podían resolver de manera eficiente. Sin embargo, la investigación de MAS pronto comenzó a considerar agentes interactivos con intereses individualizados, como representantes de diferentes humanos u organizaciones con intereses no idénticos. Cuando las interacciones son guiadas por intereses diversos, los participantes pueden tener que superar desacuerdos, interacciones no cooperativas e incluso intentos intencionales de dañarse mutuamente. Cuando ocurren este tipo de interacciones, los entornos requieren un comportamiento apropiado por parte de los agentes situados en ellos. Llamamos a estos entornos Entornos Adversarios, y llamamos a los agentes en conflicto Adversarios. Los modelos de cooperación y trabajo en equipo han sido ampliamente explorados en MAS a través de la axiomatización de estados mentales (por ejemplo, [8, 4, 5]). Sin embargo, ninguna de estas investigaciones abordó los dominios adversarios y sus implicaciones para el comportamiento de los agentes. Nuestro artículo aborda este problema al proporcionar un modelo formal y axiomatizado de estados mentales para un subconjunto de dominios adversariales, específicamente entornos adversariales simples de suma cero. Los encuentros de suma cero simples existen, por supuesto, en varios juegos de dos jugadores (por ejemplo, Ajedrez, Damas), pero también existen en juegos de n jugadores (por ejemplo, Risk, Diplomacia), subastas de un solo bien y en otros lugares. En estos entornos recientes en particular, el uso de una búsqueda adversarial basada en utilidad (como el algoritmo Min-Max) no siempre proporciona una solución adecuada; la función de recompensa podría ser bastante compleja o difícil de cuantificar, y existen limitaciones computacionales naturales en agentes racionales acotados. Además, los métodos de búsqueda tradicionales (como Min-Max) no hacen uso de un modelo del oponente, lo cual ha demostrado ser una adición valiosa a la planificación adversarial [9, 3, 11]. En este artículo, desarrollamos un modelo formal y axiomatizado para agentes racionales limitados que se encuentran en un entorno adversarial de suma cero. El modelo utiliza diferentes operadores de modalidad, y sus principales fundamentos son el modelo SharedPlans [4] para el comportamiento colaborativo. Exploramos las propiedades del entorno y los estados mentales de los agentes para derivar axiomas de comportamiento; estos axiomas de comportamiento constituyen un modelo formal que sirve como especificación y guía de diseño para el diseño de agentes en tales entornos. Luego investigamos el comportamiento de nuestro modelo empíricamente utilizando el juego de mesa Conecta Cuatro. Mostramos que este juego se ajusta a nuestra definición de entorno, y analizamos el comportamiento de los jugadores utilizando un gran conjunto de registros de partidas completadas 550 978-81-904262-7-5 (RPS) c 2007 archivos de IFAAMAS. Además, utilizamos los resultados presentados en [9] para discutir la importancia del modelado del oponente en nuestro dominio adversarial de Conecta Cuatro. El artículo continúa de la siguiente manera. La sección 2 presenta la formalización de los modelos. La sección 3 presenta el análisis empírico y sus resultados. Discutimos el trabajo relacionado en la Sección 4, y concluimos y presentamos las direcciones futuras en la Sección 5. ENTORNOS ADVERSARIOS El modelo de entorno adversario (denominado como AE) está destinado a guiar el diseño de agentes al proporcionar una especificación de las capacidades y actitudes mentales de un agente en un entorno adversario. Nos enfocamos aquí en tipos específicos de entornos adversarios, especificados de la siguiente manera: 1. Interacciones de suma cero: las utilidades positivas y negativas de todos los agentes suman cero; 2. Agentes adversarios simples: todos los agentes en el entorno son agentes adversarios; 3. AEs bilaterales: AEs con exactamente dos agentes; 4. AEs multilaterales: AEs de tres o más agentes. Trabajaremos en instanciaciones tanto bilaterales como multilaterales de entornos de suma cero y simples. En particular, nuestro modelo de entorno adversarial tratará con interacciones que consisten en N agentes (N ≥ 2), donde todos los agentes son adversarios y solo un agente puede tener éxito. Ejemplos de tales entornos van desde juegos de mesa (por ejemplo, Ajedrez, Conecta Cuatro y Diplomacia) hasta ciertos entornos económicos (por ejemplo, subastas de N postores sobre un único bien). 2.1 Resumen del Modelo Nuestro enfoque es formalizar las actitudes mentales y comportamientos de un único agente adversario; consideramos cómo un único agente percibe el AE. La siguiente lista especifica las condiciones y estados mentales de un agente en un AE simple de suma cero: 1. El agente tiene la intención individual de que su propio objetivo se complete; 2. El agente tiene la creencia individual de que él y sus adversarios persiguen objetivos completamente opuestos (definidos a continuación) y que solo puede haber un ganador; 3. El agente tiene la creencia individual de que cada adversario tiene la intención de completar su propio objetivo conflictivo completo; 4. El agente tiene una creencia individual en el perfil (parcial) de sus adversarios. El ítem 3 es necesario, ya que podría ser el caso de que algún agente tenga un objetivo completamente conflictivo y esté considerando adoptar la intención de completarlo, pero aún no está comprometido a lograrlo. Esto podría ocurrir porque el agente aún no ha reflexionado sobre los efectos que adoptar esa intención podría tener en las otras intenciones que actualmente mantiene. En tales casos, es posible que ni siquiera se considere en un entorno adversarial. El ítem 4 establece que el agente debe tener alguna creencia sobre los perfiles de sus adversarios. El perfil representa todo el conocimiento que el agente tiene sobre su adversario: sus debilidades, capacidades estratégicas, objetivos, intenciones, confiabilidad y más. Puede ser dado explícitamente o puede ser aprendido a partir de observaciones de encuentros pasados. 2.2 Definiciones de modelos para estados mentales Utilizamos las definiciones de los operadores modales, predicados y metapredicados de Grosz y Krauss, tal como se definen en su formalización de SharedPlan [4]. Recordamos aquí algunos de los predicados y operadores que se utilizan en esa formalización: Int.To(Ai, α, Tn, Tα, C) representa las intenciones de Ai en el tiempo Tn de realizar una acción α en el tiempo Tα en el contexto de C. Int.Th(Ai, prop, Tn, Tprop, C) representa las intenciones de Ai en el tiempo Tn de que una cierta proposición prop se cumpla en el tiempo Tprop en el contexto de C. Los operadores de intención potencial, Pot.Int.To(...) y Pot.Int.Th(...), se utilizan para representar el estado mental cuando un agente considera adoptar una intención, pero no ha deliberado sobre la interacción de las otras intenciones que tiene. El operador Bel(Ai, f, Tf) representa al agente Ai creyendo en la afirmación expresada en la fórmula f, en el tiempo Tf. MB(A, f, Tf) representa la creencia mutua para un grupo de agentes A. Una instantánea del sistema encuentra que nuestro entorno se encuentra en algún estado e ∈ E de estados variables ambientales, y cada adversario en cualquier LAi ∈ L de posibles estados locales. En cualquier paso de tiempo dado, el sistema estará en algún mundo w del conjunto de todos los mundos posibles w ∈ W, donde w = E×LA1 ×LA2 × ...LAn, y n es el número de adversarios. Por ejemplo, en un juego de póker Texas Holdem, el estado local de un agente podría ser su propio conjunto de cartas (que es desconocido para su adversario) mientras que el entorno consistirá en el bote de apuestas y las cartas comunitarias (que son visibles para ambos jugadores). Una función de utilidad bajo esta formalización se define como un mapeo de un mundo posible w ∈ W a un elemento en , que expresa la deseabilidad del mundo, desde la perspectiva de un único agente. Normalmente normalizamos el rango a [0,1], donde 0 representa el mundo menos deseable posible, y 1 es el mundo más deseable. La implementación de la función de utilidad depende del dominio en cuestión. La siguiente lista especifica nuevos predicados, funciones, variables y constantes utilizadas en conjunto con las definiciones originales para la formalización del entorno adversarial: 1. φ es una acción nula (el agente no hace nada). 2. GAi es el conjunto de objetivos del agente Ai. Cada objetivo es un conjunto de predicados cuya satisfacción hace que el objetivo esté completo (usamos G∗ Ai ∈ GAi para representar un objetivo arbitrario del agente Ai). 3. gAi es el conjunto de subobjetivos del agente Ai. Los subobjetivos son predicados cuya satisfacción representa un hito importante hacia el logro del objetivo completo. gG∗ Ai ⊆ gAi es el conjunto de subobjetivos que son importantes para la finalización del objetivo G∗ Ai (usaremos g∗ G∗ Ai ∈ gG∗ Ai para representar un subobjetivo arbitrario). 4. P Aj Ai es el objeto de perfil que el agente Ai tiene sobre el agente Aj. 5. CA es un conjunto general de acciones para todos los agentes en A que se derivan de las restricciones del entorno. CAi ⊆ CA es el conjunto de acciones posibles del agente Ai. La expresión Do(Ai, α, Tα, w) se cumple cuando Ai realiza la acción α durante el intervalo de tiempo Tα en el mundo w. 7. La afirmación Achieve(G∗ Ai , α, w) es verdadera cuando se logra el objetivo G∗ Ai siguiendo la finalización de la acción α en el mundo w ∈ W, donde α ∈ CAi. El perfil (Ai, PAi Ai) es verdadero cuando el agente Ai tiene un perfil de objeto para el agente Aj. Definición 1. El conflicto total (FulConf) describe una interacción de suma cero donde solo se puede completar un solo objetivo de los objetivos en conflicto. FulConf(G∗ Ai , G∗ Aj ) ⇒ (∃α ∈ CAi , ∀w, β ∈ CAj ) (Achieve(G∗ Ai , α, w) ⇒ ¬Achieve(G∗ Aj , β, w)) ∨ (∃β ∈ CAj , ∀w, α ∈ CAi )(Achieve(G∗ Aj , β, w) ⇒ ¬Achieve(G∗ Ai , α, w)) Definición 2. El Conocimiento Adversarial (AdvKnow) es una función que devuelve un valor que representa la cantidad de The Sixth Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 551 agente de conocimiento Ai tiene sobre el perfil del agente Aj, en el tiempo Tn. Cuanto mayor sea el valor, más conocimiento tiene el agente Ai. AdvKnow: P Aj Ai × Tn → Definición 3. Eval: Esta función de evaluación devuelve un valor de utilidad esperada estimado para un agente en A, después de completar una acción de CA en algún estado del mundo w. Eval: A × CA × w → Definición 4. TrH - (Umbral) es una constante numérica en el rango [0,1] que representa un valor umbral de la función de evaluación (Eval). Una acción que produce una evaluación de utilidad estimada por encima del TrH se considera una <br>acción altamente beneficiosa</br>. El valor Eval es una estimación y no la función de utilidad real, la cual suele ser desconocida. Utilizar el valor real de utilidad para un agente racional fácilmente produciría el mejor resultado para ese agente. Sin embargo, los agentes generalmente no tienen las funciones de utilidad reales, sino más bien una estimación heurística de las mismas. Existen dos propiedades importantes que deben cumplir para la función de evaluación: Propiedad 1. La función de evaluación debería indicar que el estado del mundo más deseable es aquel en el que se logra el objetivo. Por lo tanto, una vez que se ha satisfecho el objetivo, no puede haber ninguna acción futura que pueda colocar al agente en un estado del mundo con un valor de Eval más alto. (∀Ai, G∗ Ai , α, β ∈ CAi , w ∈ W) Lograr(G∗ Ai , α, w) ⇒ Eval(Ai, α, w) ≥ Eval(Ai, β, w) Propiedad 2. La función de evaluación debe proyectar una acción que cause la finalización de un objetivo o subobjetivo a un valor que sea mayor que TrH (una <br>acción altamente beneficiosa</br>). (∀Ai, G∗ Ai ∈ GAi , α ∈ CAi , w ∈ W, g∗ GAi ∈ gGAi ) Lograr(G∗ Ai , α, w) ∨ Lograr(g∗ GAi , α, w) ⇒ Eval(Ai, α, w) ≥ TrH. Definición 5. La acción de conjunto Definimos una acción de conjunto (SetAction) como un conjunto de operaciones de acción (ya sean acciones complejas o básicas) de algunos conjuntos de acción CAi y CAj que, según la creencia del agente Ai, están unidas por una relación temporal y consecuente, formando una cadena de eventos (acción y su acción consecuente siguiente). (∀α1 , . . . , αu ∈ CAi , β1 , . . . , βv ∈ CAj , w ∈ W) SetAction(α1 , . . . , αu , β1 , . . . , βv , w) ⇒ ((Do(Ai, α1 , Tα1 , w) ⇒ Do(Aj, β1 , Tβ1 , w)) ⇒ Do(Ai, α2 , Tα2 , w) ⇒ . . . ⇒ Do(Ai, αu , Tαu , w)) La relación consecuente podría existir debido a varias restricciones ambientales (cuando una acción obliga al adversario a responder con una acción específica) o debido al conocimiento de los agentes sobre el perfil de su adversario. Propiedad 3. A medida que aumenta el conocimiento que tenemos sobre nuestro adversario, tendremos creencias adicionales sobre su comportamiento en diferentes situaciones, lo que a su vez crea un nuevo conjunto de acciones. Formalmente, si nuestro AdvKnow en el tiempo Tn+1 es mayor que AdvKnow en el tiempo Tn, entonces cada SetAction conocido en el tiempo Tn también es conocido en el tiempo Tn+1. AdvKnow(P Aj Ai , Tn+1) > AdvKnow(P Aj Ai , Tn) ⇒ (∀α1 , . . . , αu ∈ CAi , β1 , . . . , βv ∈ CAj ) Bel(Aag, SetAction(α1 , . . . , αu , β1 , . . . , βv ), Tn) ⇒ Bel(Aag, SetAction(α1 , . . . , αu , β1 , . . . , βv ), Tn+1) 2.3 La formulación del entorno Los siguientes axiomas proporcionan la definición formal para un Entorno Adversarial (AE) simple y de suma cero. La satisfacción de estos axiomas significa que el agente está situado en un entorno así. Proporciona especificaciones para que el agente Aag interactúe con su conjunto de adversarios A con respecto a los objetivos G∗ Aag y G∗ A en el tiempo TCo en algún estado del mundo w. AE(Aag, A, G∗ Aag, A1, . . . , Ak, G∗ A1, . . . , G∗ Ak, Tn, w) 1. Aag tiene un Int.Th su objetivo se completaría: (∃α ∈ CAag , Tα) Int.Th(Aag, Lograr(G∗ Aag , α), Tn, Tα, AE) 2. Aag cree que él y cada uno de sus adversarios Ao están persiguiendo metas completamente conflictivas: (∀Ao ∈ {A1, . . . , Ak}) Bel(Aag, FulConf(G∗ Aag , G∗ Ao ), Tn) 3. Aag cree que cada uno de sus adversarios en Ao tiene la Int.Th su objetivo en conflicto G∗ Aoi se completará: (∀Ao ∈ {A1, . . . , Ak})(∃β ∈ CAo , Tβ) Bel(Aag, Int.Th(Ao, Lograr(G∗ Ao , β), TCo, Tβ, AE), Tn) 4. Para construir un agente que pueda operar con éxito dentro de un entorno de adversarios estratégicos, debemos especificar pautas de comportamiento para sus interacciones. Utilizar una estrategia de maximización de Eval ingenua hasta cierta profundidad de búsqueda no siempre dará resultados satisfactorios por varias razones: (1) el problema del horizonte de búsqueda al buscar a una profundidad fija; (2) la fuerte suposición de un adversario óptimamente racional con recursos ilimitados; (3) utilizar una función de evaluación estimada que no dará resultados óptimos en todos los estados del mundo, y que puede ser explotada [9]. Los siguientes axiomas especifican los principios de comportamiento que se pueden utilizar para diferenciar entre agentes exitosos y menos exitosos en el entorno adversarial mencionado anteriormente. Esos axiomas deben ser utilizados como principios de especificación al diseñar e implementar agentes que deben ser capaces de desempeñarse bien en entornos adversarios. Los axiomas conductuales representan situaciones en las que el agente adoptará intenciones potenciales para (Pot.Int.To(...)) realizar una acción, lo que típicamente requerirá un razonamiento de medios-fines para seleccionar un posible curso de acción. Este razonamiento llevará a la adopción de un Int.To(...) (ver [4]). A1. Axioma para lograr metas. El primer axioma es el caso más simple; cuando el agente Aag cree que está a una acción (α) de lograr su objetivo en conflicto G∗ Aag, debería adoptar la intención potencial de hacer α y completar su objetivo. (∀Aag, α ∈ CAag, Tn, Tα, w ∈ W) (Bel(Aag, Do(Aag, α, Tα, w) ⇒ Achieve(G∗ Aag, α, w)) ⇒ Pot.Int.To(Aag, α, Tn, Tα, w) Este comportamiento algo trivial es el primer y más fuerte axioma. En cualquier situación, cuando el agente está a una acción de completar el objetivo, debería completar la acción. Cualquier función de evaluación justa clasificaría naturalmente a α como la acción de valor máximo (propiedad 1). Sin embargo, sin una axiomatización explícita de dicho comportamiento, podría haber situaciones en las que el agente decida tomar otra acción por diversas razones, debido a sus recursos de decisión limitados. A2. Axioma del Acto Preventivo. En una situación adversarial, el agente Aag podría decidir tomar acciones que dañen uno de los planes de su adversario para completar su objetivo, incluso si esas acciones no avanzan explícitamente a Aag hacia su objetivo en conflicto G∗ Aag. Tal acción preventiva tendrá lugar cuando el agente Aag tenga la creencia sobre la posibilidad de que su adversario Ao realice una acción β que le otorgará un alto 552 The Sixth Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) valor de evaluación de utilidad (> TrH). Creer que tomar la acción α evitará que el oponente realice su β, adoptará una intención potencial de hacer α. (∀Aag, Ao ∈ A, α ∈ CAag , β ∈ CAo , Tn, Tβ, w ∈ W) (Bel(Aag, Do(Ao, β, Tβ, w) ∧ Eval(Ao, β, w) > TrH, Tn) ∧ Bel(Aag, Do(Aag, α, Tα, w) ⇒ ¬Do(Ao, β, Tβ, w), Tn) ⇒ Pot.Int.To(Aag, α, Tn, Tα, w) Este axioma es un componente básico de cualquier entorno adversarial. Por ejemplo, al observar un juego de ajedrez, un jugador podría darse cuenta de que está a punto de ser jaque mateado por su oponente, por lo tanto, realiza una jugada preventiva. Otro ejemplo es el juego de Cuatro en Línea: cuando un jugador tiene una fila de tres fichas, su oponente debe bloquearla o perder. Una instancia específica de A1 ocurre cuando el adversario está a una acción de lograr su objetivo, y la acción preventiva inmediata debe ser tomada por el agente. Formalmente, tenemos las mismas creencias que se mencionaron anteriormente, con la creencia modificada de que realizar la acción β hará que el agente Ao logre su objetivo. Proposición 1: Caso de prevención o pérdida. (∀Aag, Ao ∈ A, α ∈ CAag , β ∈ CAo , G∗ Ao , Tn, Tα, Tβ, w ∈ W) Bel(Aag, Do(Ao, β, Tβ, w) ⇒ Achieve(G∗ Ao , β, w), Tn) ∧ Bel(Aag, Do(Aag, α, Tα, w) ⇒ ¬Do(Ao, β, Tβ, w)) ⇒ Pot.Int.To(Aag, α, Tn, Tα, w) Esquema de prueba: La Proposición 1 puede derivarse fácilmente del axioma A1 y de la propiedad 2 de la función Eval, que establece que cualquier acción que cause la finalización de un objetivo es una <br>acción altamente beneficiosa</br>. El comportamiento del acto preventivo ocurrirá implícitamente cuando la función Eval sea igual a la función de utilidad del mundo real. Sin embargo, al ser agentes racionales limitados y al tratar con una función de evaluación estimada, necesitamos axiomatizar explícitamente dicho comportamiento, ya que no siempre ocurrirá implícitamente a partir de la función de evaluación. A3. Axioma de Movimiento Táctico Subóptimo. En muchos escenarios, puede ocurrir una situación en la que un agente decida no tomar la acción más beneficiosa actual que puede tomar (la acción con el valor de evaluación de utilidad máximo), porque cree que tomar otra acción (con un valor de evaluación de utilidad más bajo) podría generar (dependiendo de la respuesta del adversario) una posibilidad futura para una acción altamente beneficiosa. Esto ocurrirá con mayor frecuencia cuando la función Eval sea inexacta y difiera en gran medida de la función de Utilidad. De manera formal, el agente Aag cree en una cierta SetAction que evolucionará de acuerdo con su acción inicial y producirá un valor beneficioso alto (> TrH) únicamente para él. (∀Aag, Ao ∈ A, Tn, w ∈ W) (∃α1 , . . . , αu ∈ CAi , β1 , . . . , βv ∈ CAj , Tα1 ) Bel(Aag, SetAction(α1 , . . . , αu , β1 , . . . , βv ), Tn) ∧ Bel(Aag, Eval(Ao, βv , w) < TrH < Eval(Aag, αu , w), Tn) ⇒ Pot.Int.To(Aag, α1 , Tn, Tα1 , w) Un agente podría creer que una cadena de eventos ocurrirá por diversas razones debido a la naturaleza inevitable del dominio. Por ejemplo, en el ajedrez, a menudo observamos lo siguiente: una jugada provoca una posición de jaque, lo que a su vez limita los movimientos del oponente para evitar el jaque, a lo que el primer jugador podría reaccionar con otro jaque, y así sucesivamente. El agente también podría creer en una cadena de eventos basada en su conocimiento del perfil de su adversario, lo que le permite prever los movimientos del adversario con alta precisión. A4. Axioma de Detección de Perfiles. El agente puede ajustar los perfiles de sus adversarios mediante observaciones y estudio de patrones (específicamente, si hay encuentros repetidos con el mismo adversario). Sin embargo, en lugar de esperar a que se revele la información del perfil, un agente también puede iniciar acciones que obligarán a su adversario a reaccionar de una manera que revele conocimiento del perfil sobre él. Formalmente, el axioma establece que si todas las acciones (γ) no son acciones altamente beneficiosas (< TrH), el agente puede realizar la acción α en el tiempo Tα si cree que resultará en una <br>acción no altamente beneficiosa</br> β por parte de su adversario, lo que a su vez le enseña sobre el perfil del adversario, es decir, proporciona un mayor AdvKnow(P Aj Ai , Tβ). (∀Aag, Ao ∈ A, α ∈ CAag , β ∈ CAo , Tn, Tα, Tβ, w ∈ W) Bel(Aag, (∀γ ∈ CAag )Eval(Aag, γ, w) < TrH, Tn) ∧ Bel(Aag, Do(Aag, α, Tα, w) ⇒ Do(Ao, β, Tβ, w), Tn) ∧ Bel(Aag, Eval(Ao, β, w) < TrH) ∧ Bel(Aag, AdvKnow(P Aj Ai , Tβ) > AdvKnow(P Aj Ai , Tn), Tn) ⇒ Pot.Int.To(Aag, α, Tn, Tα, w) Por ejemplo, volviendo al escenario del juego de ajedrez, consideremos comenzar una partida contra un oponente del que no sabemos nada, ni siquiera si es humano o un oponente computarizado. ",
            "candidates": [],
            "error": [
                [
                    "acción altamente beneficiosa",
                    "acción altamente beneficiosa",
                    "acción altamente beneficiosa",
                    "acción no altamente beneficiosa"
                ]
            ]
        },
        "connect-four game": {
            "translated_key": "juego de Conecta Cuatro",
            "is_in_text": true,
            "original_annotated_sentences": [
                "An Adversarial Environment Model for Bounded Rational Agents in Zero-Sum Interactions Inon Zuckerman1 , Sarit Kraus1 , Jeffrey S. Rosenschein2 , Gal Kaminka1 1 Department of Computer Science 2 The School of Engineering Bar-Ilan University and Computer Science Ramat-Gan, Israel Hebrew University, Jerusalem, Israel {zukermi,sarit,galk}@cs.biu.ac.il jeff@cs.huji.ac.il ABSTRACT Multiagent environments are often not cooperative nor collaborative; in many cases, agents have conflicting interests, leading to adversarial interactions.",
                "This paper presents a formal Adversarial Environment model for bounded rational agents operating in a zero-sum environment.",
                "In such environments, attempts to use classical utility-based search methods can raise a variety of difficulties (e.g., implicitly modeling the opponent as an omniscient utility maximizer, rather than leveraging a more nuanced, explicit opponent model).",
                "We define an Adversarial Environment by describing the mental states of an agent in such an environment.",
                "We then present behavioral axioms that are intended to serve as design principles for building such adversarial agents.",
                "We explore the application of our approach by analyzing log files of completed Connect-Four games, and present an empirical analysis of the axioms appropriateness.",
                "Categories and Subject Descriptors I.2.11 [Artificial Intelligence]: Distributed Artificial Intelligence-Intelligent agents,Multiagent Systems; I.2.4 [Artificial Intelligence]: Knowledge Representation Formalisms and Methods -Modal logic General Terms Design, Theory 1.",
                "INTRODUCTION Early research in multiagent systems (MAS) considered cooperative groups of agents; because individual agents had limited resources, or limited access to information (e.g., limited processing power, limited sensor coverage), they worked together by design to solve problems that individually they could not solve, or at least could not solve as efficiently.",
                "MAS research, however, soon began to consider interacting agents with individuated interests, as representatives of different humans or organizations with non-identical interests.",
                "When interactions are guided by diverse interests, participants may have to overcome disagreements, uncooperative interactions, and even intentional attempts to damage one another.",
                "When these types of interactions occur, environments require appropriate behavior from the agents situated in them.",
                "We call these environments Adversarial Environments, and call the clashing agents Adversaries.",
                "Models of cooperation and teamwork have been extensively explored in MAS through the axiomatization of mental states (e.g., [8, 4, 5]).",
                "However, none of this research dealt with adversarial domains and their implications for agent behavior.",
                "Our paper addresses this issue by providing a formal, axiomatized mental state model for a subset of adversarial domains, namely simple zero-sum adversarial environments.",
                "Simple zero-sum encounters exist of course in various twoplayer games (e.g., Chess, Checkers), but they also exist in n-player games (e.g., Risk, Diplomacy), auctions for a single good, and elsewhere.",
                "In these latter environments especially, using a utility-based adversarial search (such as the Min-Max algorithm) does not always provide an adequate solution; the payoff function might be quite complex or difficult to quantify, and there are natural computational limitations on bounded rational agents.",
                "In addition, traditional search methods (like Min-Max) do not make use of a model of the opponent, which has proven to be a valuable addition to adversarial planning [9, 3, 11].",
                "In this paper, we develop a formal, axiomatized model for bounded rational agents that are situated in a zero-sum adversarial environment.",
                "The model uses different modality operators, and its main foundations are the SharedPlans [4] model for collaborative behavior.",
                "We explore environment properties and the mental states of agents to derive behavioral axioms; these behavioral axioms constitute a formal model that serves as a specification and design guideline for agent design in such settings.",
                "We then investigate the behavior of our model empirically using the Connect-Four board game.",
                "We show that this game conforms to our environment definition, and analyze players behavior using a large set of completed match log 550 978-81-904262-7-5 (RPS) c 2007 IFAAMAS files.",
                "In addition, we use the results presented in [9] to discuss the importance of opponent modeling in our ConnectFour adversarial domain.",
                "The paper proceeds as follows.",
                "Section 2 presents the models formalization.",
                "Section 3 presents the empirical analysis and its results.",
                "We discuss related work in Section 4, and conclude and present future directions in Section 5. 2.",
                "ADVERSARIAL ENVIRONMENTS The adversarial environment model (denoted as AE) is intended to guide the design of agents by providing a specification of the capabilities and mental attitudes of an agent in an adversarial environment.",
                "We focus here on specific types of adversarial environments, specified as follows: 1.",
                "Zero-Sum Interactions: positive and negative utilities of all agents sum to zero; 2.",
                "Simple AEs: all agents in the environment are adversarial agents; 3.",
                "Bilateral AEs: AEs with exactly two agents; 4.",
                "Multilateral AEs: AEs of three or more agents.",
                "We will work on both bilateral and multilateral instantiations of zero-sum and simple environments.",
                "In particular, our adversarial environment model will deal with interactions that consist of N agents (N ≥ 2), where all agents are adversaries, and only one agent can succeed.",
                "Examples of such environments range from board games (e.g., Chess, Connect-Four, and Diplomacy) to certain economic environments (e.g., N-bidder auctions over a single good). 2.1 Model Overview Our approach is to formalize the mental attitudes and behaviors of a single adversarial agent; we consider how a single agent perceives the AE.",
                "The following list specifies the conditions and mental states of an agent in a simple, zero-sum AE: 1.",
                "The agent has an individual intention that its own goal will be completed; 2.",
                "The agent has an individual belief that it and its adversaries are pursuing full conflicting goals (defined below)there can be only one winner; 3.",
                "The agent has an individual belief that each adversary has an intention to complete its own full conflicting goal; 4.",
                "The agent has an individual belief in the (partial) profile of its adversaries.",
                "Item 3 is required, since it might be the case that some agent has a full conflicting goal, and is currently considering adopting the intention to complete it, but is, as of yet, not committed to achieving it.",
                "This might occur because the agent has not yet deliberated about the effects that adopting that intention might have on the other intentions it is currently holding.",
                "In such cases, it might not consider itself to even be in an adversarial environment.",
                "Item 4 states that the agent should hold some belief about the profiles of its adversaries.",
                "The profile represents all the knowledge the agent has about its adversary: its weaknesses, strategic capabilities, goals, intentions, trustworthiness, and more.",
                "It can be given explicitly or can be learned from observations of past encounters. 2.2 Model Definitions for Mental States We use Grosz and Krauss definitions of the modal operators, predicates, and meta-predicates, as defined in their SharedPlan formalization [4].",
                "We recall here some of the predicates and operators that are used in that formalization: Int.To(Ai, α, Tn, Tα, C) represents Ais intentions at time Tn to do an action α at time Tα in the context of C. Int.Th(Ai, prop, Tn, Tprop, C) represents Ais intentions at time Tn that a certain proposition prop holds at time Tprop in the context of C. The potential intention operators, Pot.Int.To(...) and Pot.Int.Th(...), are used to represent the mental state when an agent considers adopting an intention, but has not deliberated about the interaction of the other intentions it holds.",
                "The operator Bel(Ai, f, Tf ) represents agent Ai believing in the statement expressed in formula f, at time Tf .",
                "MB(A, f, Tf ) represents mutual belief for a group of agents A.",
                "A snapshot of the system finds our environment to be in some state e ∈ E of environmental variable states, and each adversary in any LAi ∈ L of possible local states.",
                "At any given time step, the system will be in some world w of the set of all possible worlds w ∈ W, where w = E×LA1 ×LA2 × ...LAn , and n is the number of adversaries.",
                "For example, in a Texas Holdem poker game, an agents local state might be its own set of cards (which is unknown to its adversary) while the environment will consist of the betting pot and the community cards (which are visible to both players).",
                "A utility function under this formalization is defined as a mapping from a possible world w ∈ W to an element in , which expresses the desirability of the world, from a single agent perspective.",
                "We usually normalize the range to [0,1], where 0 represents the least desirable possible world, and 1 is the most desirable world.",
                "The implementation of the utility function is dependent on the domain in question.",
                "The following list specifies new predicates, functions, variables, and constants used in conjunction with the original definitions for the adversarial environment formalization: 1. φ is a null action (the agent does not do anything). 2.",
                "GAi is the set of agent Ais goals.",
                "Each goal is a set of predicates whose satisfaction makes the goal complete (we use G∗ Ai ∈ GAi to represent an arbitrary goal of agent Ai). 3. gAi is the set of agent Ais subgoals.",
                "Subgoals are predicates whose satisfaction represents an important milestone toward achievement of the full goal. gG∗ Ai ⊆ gAi is the set of subgoals that are important to the completion of goal G∗ Ai (we will use g∗ G∗ Ai ∈ gG∗ Ai to represent an arbitrary subgoal). 4.",
                "P Aj Ai is the profile object agent Ai holds about agent Aj. 5.",
                "CA is a general set of actions for all agents in A which are derived from the environments constraints.",
                "CAi ⊆ CA is the set of agent Ais possible actions. 6.",
                "Do(Ai, α, Tα, w) holds when Ai performs action α over time interval Tα in world w. 7.",
                "Achieve(G∗ Ai , α, w) is true when goal G∗ Ai is achieved following the completion of action α in world w ∈ W, where α ∈ CAi . 8.",
                "Profile(Ai, PAi Ai ) is true when agent Ai holds an object profile for agent Aj.",
                "Definition 1.",
                "Full conflict (FulConf ) describes a zerosum interaction where only a single goal of the goals in conflict can be completed.",
                "FulConf(G∗ Ai , G∗ Aj ) ⇒ (∃α ∈ CAi , ∀w, β ∈ CAj ) (Achieve(G∗ Ai , α, w) ⇒ ¬Achieve(G∗ Aj , β, w)) ∨ (∃β ∈ CAj , ∀w, α ∈ CAi )(Achieve(G∗ Aj , β, w) ⇒ ¬Achieve(G∗ Ai , α, w)) Definition 2.",
                "Adversarial Knowledge (AdvKnow) is a function returning a value which represents the amount of The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 551 knowledge agent Ai has on the profile of agent Aj, at time Tn.",
                "The higher the value, the more knowledge agent Ai has.",
                "AdvKnow : P Aj Ai × Tn → Definition 3.",
                "Eval - This evaluation function returns an estimated expected utility value for an agent in A, after completing an action from CA in some world state w. Eval : A × CA × w → Definition 4.",
                "TrH - (Threshold) is a numerical constant in the [0,1] range that represents an evaluation function (Eval) threshold value.",
                "An action that yields an estimated utility evaluation above the TrH is regarded as a highly beneficial action.",
                "The Eval value is an estimation and not the real utility function, which is usually unknown.",
                "Using the real utility value for a rational agent would easily yield the best outcome for that agent.",
                "However, agents usually do not have the real utility functions, but rather a heuristic estimate of it.",
                "There are two important properties that should hold for the evaluation function: Property 1.",
                "The evaluation function should state that the most desirable world state is one in which the goal is achieved.",
                "Therefore, after the goal has been satisfied, there can be no future action that can put the agent in a world state with higher Eval value. (∀Ai, G∗ Ai , α, β ∈ CAi , w ∈ W) Achieve(G∗ Ai , α, w) ⇒ Eval(Ai, α, w) ≥ Eval(Ai, β, w) Property 2.",
                "The evaluation function should project an action that causes a completion of a goal or a subgoal to a value which is greater than TrH (a highly beneficial action). (∀Ai, G∗ Ai ∈ GAi , α ∈ CAi , w ∈ W, g∗ GAi ∈ gGAi ) Achieve(G∗ Ai , α, w) ∨ Achieve(g∗ GAi , α, w) ⇒ Eval(Ai, α, w) ≥ TrH.",
                "Definition 5.",
                "SetAction We define a set action (SetAction) as a set of action operations (either complex or basic actions) from some action sets CAi and CAj which, according to agent Ais belief, are attached together by a temporal and consequential relationship, forming a chain of events (action, and its following consequent action). (∀α1 , . . . , αu ∈ CAi , β1 , . . . , βv ∈ CAj , w ∈ W) SetAction(α1 , . . . , αu , β1 , . . . , βv , w) ⇒ ((Do(Ai, α1 , Tα1 , w) ⇒ Do(Aj, β1 , Tβ1 , w)) ⇒ Do(Ai, α2 , Tα2 , w) ⇒ . . . ⇒ Do(Ai, αu , Tαu , w)) The consequential relation might exist due to various environmental constraints (when one action forces the adversary to respond with a specific action) or due to the agents knowledge about the profile of its adversary.",
                "Property 3.",
                "As the knowledge we have about our adversary increases we will have additional beliefs about its behavior in different situations which in turn creates new set actions.",
                "Formally, if our AdvKnow at time Tn+1 is greater than AdvKnow at time Tn, then every SetAction known at time Tn is also known at time Tn+1.",
                "AdvKnow(P Aj Ai , Tn+1) > AdvKnow(P Aj Ai , Tn) ⇒ (∀α1 , . . . , αu ∈ CAi , β1 , . . . , βv ∈ CAj ) Bel(Aag, SetAction(α1 , . . . , αu , β1 , . . . , βv ), Tn) ⇒ Bel(Aag, SetAction(α1 , . . . , αu , β1 , . . . , βv ), Tn+1) 2.3 The Environment Formulation The following axioms provide the formal definition for a simple, zero-sum Adversarial Environment (AE).",
                "Satisfaction of these axioms means that the agent is situated in such an environment.",
                "It provides specifications for agent Aag to interact with its set of adversaries A with respect to goals G∗ Aag and G∗ A at time TCo at some world state w. AE(Aag, A, G∗ Aag , A1, . . . , Ak, G∗ A1 , . . . , G∗ Ak , Tn, w) 1.",
                "Aag has an Int.Th his goal would be completed: (∃α ∈ CAag , Tα) Int.Th(Aag, Achieve(G∗ Aag , α), Tn, Tα, AE) 2.",
                "Aag believes that it and each of its adversaries Ao are pursuing full conflicting goals: (∀Ao ∈ {A1, . . . , Ak}) Bel(Aag, FulConf(G∗ Aag , G∗ Ao ), Tn) 3.",
                "Aag believes that each of his adversaries in Ao has the Int.Th his conflicting goal G∗ Aoi will be completed: (∀Ao ∈ {A1, . . . , Ak})(∃β ∈ CAo , Tβ) Bel(Aag, Int.Th(Ao, Achieve(G∗ Ao , β), TCo, Tβ, AE), Tn) 4.",
                "Aag has beliefs about the (partial) profiles of its adversaries (∀Ao ∈ {A1, . . . , Ak}) (∃PAo Aag ∈ PAag )Bel(Aag, Profile(Ao, PAo Aag ), Tn) To build an agent that will be able to operate successfully within such an AE, we must specify behavioral guidelines for its interactions.",
                "Using a naive Eval maximization strategy to a certain search depth will not always yield satisfactory results for several reasons: (1) the search horizon problem when searching for a fixed depth; (2) the strong assumption of an optimally rational, unbounded resources adversary; (3) using an estimated evaluation function which will not give optimal results in all world states, and can be exploited [9].",
                "The following axioms specify the behavioral principles that can be used to differentiate between successful and less successful agents in the above Adversarial Environment.",
                "Those axioms should be used as specification principles when designing and implementing agents that should be able to perform well in such Adversarial Environments.",
                "The behavioral axioms represent situations in which the agent will adopt potential intentions to (Pot.Int.To(...)) perform an action, which will typically require some means-end reasoning to select a possible course of action.",
                "This reasoning will lead to the adoption of an Int.To(...) (see [4]).",
                "A1.",
                "Goal Achieving Axiom.",
                "The first axiom is the simplest case; when the agent Aag believes that it is one action (α) away from achieving his conflicting goal G∗ Aag , it should adopt the potential intention to do α and complete its goal. (∀Aag, α ∈ CAag , Tn, Tα, w ∈ W) (Bel(Aag, Do(Aag, α, Tα, w) ⇒ Achieve(G∗ Aag , α, w)) ⇒ Pot.Int.To(Aag, α, Tn, Tα, w) This somewhat trivial behavior is the first and strongest axiom.",
                "In any situation, when the agent is an action away from completing the goal, it should complete the action.",
                "Any fair Eval function would naturally classify α as the maximal value action (property 1).",
                "However, without explicit axiomatization of such behavior there might be situations where the agent will decide on taking another action for various reasons, due to its bounded decision resources.",
                "A2.",
                "Preventive Act Axiom.",
                "Being in an adversarial situation, agent Aag might decide to take actions that will damage one of its adversarys plans to complete its goal, even if those actions do not explicitly advance Aag towards its conflicting goal G∗ Aag .",
                "Such preventive action will take place when agent Aag has a belief about the possibility of its adversary Ao doing an action β that will give it a high 552 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) utility evaluation value (> TrH).",
                "Believing that taking action α will prevent the opponent from doing its β, it will adopt a potential intention to do α. (∀Aag, Ao ∈ A, α ∈ CAag , β ∈ CAo , Tn, Tβ, w ∈ W) (Bel(Aag, Do(Ao, β, Tβ, w) ∧ Eval(Ao, β, w) > TrH, Tn) ∧ Bel(Aag, Do(Aag, α, Tα, w) ⇒ ¬Do(Ao, β, Tβ, w), Tn) ⇒ Pot.Int.To(Aag, α, Tn, Tα, w) This axiom is a basic component of any adversarial environment.",
                "For example, looking at a Chess board game, a player could realize that it is about to be checkmated by its opponent, thus making a preventive move.",
                "Another example is a Connect Four game: when a player has a row of three chips, its opponent must block it, or lose.",
                "A specific instance of A1 occurs when the adversary is one action away from achieving its goal, and immediate preventive action needs to be taken by the agent.",
                "Formally, we have the same beliefs as stated above, with a changed belief that doing action β will cause agent Ao to achieve its goal.",
                "Proposition 1: Prevent or lose case. (∀Aag, Ao ∈ A, α ∈ CAag , β ∈ CAo , G∗ Ao , Tn, Tα, Tβ, w ∈ W) Bel(Aag, Do(Ao, β, Tβ, w) ⇒ Achieve(G∗ Ao , β, w), Tn) ∧ Bel(Aag, Do(Aag, α, Tα, w) ⇒ ¬Do(Ao, β, Tβ, w)) ⇒ Pot.Int.To(Aag, α, Tn, Tα, w) Sketch of proof: Proposition 1 can be easily derived from axiom A1 and the property 2 of the Eval function, which states that any action that causes a completion of a goal is a highly beneficial action.",
                "The preventive act behavior will occur implicitly when the Eval function is equal to the real world utility function.",
                "However, being bounded rational agents and dealing with an estimated evaluation function we need to explicitly axiomatize such behavior, for it will not always occur implicitly from the evaluation function.",
                "A3.",
                "Suboptimal Tactical Move Axiom.",
                "In many scenarios a situation may occur where an agent will decide not to take the current most beneficial action it can take (the action with the maximal utility evaluation value), because it believes that taking another action (with lower utility evaluation value) might yield (depending on the adversarys response) a future possibility for a highly beneficial action.",
                "This will occur most often when the Eval function is inaccurate and differs by a large extent from the Utility function.",
                "Put formally, agent Aag believes in a certain SetAction that will evolve according to its initial action and will yield a high beneficial value (> TrH) solely for it. (∀Aag, Ao ∈ A, Tn, w ∈ W) (∃α1 , . . . , αu ∈ CAi , β1 , . . . , βv ∈ CAj , Tα1 ) Bel(Aag, SetAction(α1 , . . . , αu , β1 , . . . , βv ), Tn) ∧ Bel(Aag, Eval(Ao, βv , w) < TrH < Eval(Aag, αu , w), Tn) ⇒ Pot.Int.To(Aag, α1 , Tn, Tα1 , w) An agent might believe that a chain of events will occur for various reasons due to the inevitable nature of the domain.",
                "For example, in Chess, we often observe the following: a move causes a check position, which in turn limits the opponents moves to avoiding the check, to which the first player might react with another check, and so on.",
                "The agent might also believe in a chain of events based on its knowledge of its adversarys profile, which allows it to foresee the adversarys movements with high accuracy.",
                "A4.",
                "Profile Detection Axiom.",
                "The agent can adjust its adversarys profiles by observations and pattern study (specifically, if there are repeated encounters with the same adversary).",
                "However, instead of waiting for profile information to be revealed, an agent can also initiate actions that will force its adversary to react in a way that will reveal profile knowledge about it.",
                "Formally, the axiom states that if all actions (γ) are not highly beneficial actions (< TrH), the agent can do action α in time Tα if it believes that it will result in a non-highly beneficial action β from its adversary, which in turn teaches it about the adversarys profile, i.e., gives a higher AdvKnow(P Aj Ai , Tβ). (∀Aag, Ao ∈ A, α ∈ CAag , β ∈ CAo , Tn, Tα, Tβ, w ∈ W) Bel(Aag, (∀γ ∈ CAag )Eval(Aag, γ, w) < TrH, Tn) ∧ Bel(Aag, Do(Aag, α, Tα, w) ⇒ Do(Ao, β, Tβ, w), Tn) ∧ Bel(Aag, Eval(Ao, β, w) < TrH) ∧ Bel(Aag, AdvKnow(P Aj Ai , Tβ) > AdvKnow(P Aj Ai , Tn), Tn) ⇒ Pot.Int.To(Aag, α, Tn, Tα, w) For example, going back to the Chess board game scenario, consider starting a game versus an opponent about whom we know nothing, not even if it is a human or a computerized opponent.",
                "We might start playing a strategy that will be suitable versus an average opponent, and adjust our game according to its level of play.",
                "A5.",
                "Alliance Formation Axiom The following behavioral axiom is relevant only in a multilateral instantiation of the adversarial environment (obviously, an alliance cannot be formed in a bilateral, zero-sum encounter).",
                "In different situations during a multilateral interaction, a group of agents might believe that it is in their best interests to form a temporary alliance.",
                "Such an alliance is an agreement that constrains its members behavior, but is believed by its members to enable them to achieve a higher utility value than the one achievable outside of the alliance.",
                "As an example, we can look at the classical Risk board game, where each player has an individual goal of being the sole conquerer of the world, a zero-sum game.",
                "However, in order to achieve this goal, it might be strategically wise to make short-term ceasefire agreements with other players, or to join forces and attack an opponent who is stronger than the rest.",
                "An alliances terms defines the way its members should act.",
                "It is a set of predicates, denoted as Terms, that is agreed upon by the alliance members, and should remain true for the duration of the alliance.",
                "For example, the set Terms in the Risk scenario, could contain the following predicates: 1.",
                "Alliance members will not attack each other on territories X, Y and Z; 2.",
                "Alliance members will contribute C units per turn for attacking adversary Ao; 3.",
                "Members are obligated to stay as part of the alliance until time Tk or until adversarys Ao army is smaller than Q.",
                "The set Terms specifies inter-group constraints on each of the alliance members (∀Aal i ∈ Aal ⊆ A) set of actions Cal i ⊆ C. Definition 6.",
                "Al val - the total evaluation value that agent Ai will achieve while being part of Aal is the sum of Evali (Eval for Ai) of each of Aal j Eval values after taking their own α actions (via the agent(α) predicate): Al val(Ai, Cal , Aal , w) = α∈Cal Evali(Aal j , agent(α), w) Definition 7.",
                "Al TrH - is a number representing an Al val The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 553 threshold; above it, the alliance can be said to be a highly beneficial alliance.",
                "The value of Al TrH will be calculated dynamically according to the progress of the interaction, as can be seen in [7].",
                "After an alliance is formed, its members are now working in their normal adversarial environment, as well as according to the mental states and axioms required for their interactions as part of the alliance.",
                "The following Alliance model (AL) specifies the conditions under which the group Aal can be said to be in an alliance and working with a new and constrained set of actions Cal , at time Tn.",
                "AL(Aal , Cal , w, Tn) 1.",
                "Aal has a MB that all members are part of Aal : MB(Aal , (∀Aal i ∈ Aal )member(Aal i , Aal ), Tn) 2.",
                "Aal has a MB that the group be maintained: MB(Aal , (∀Aal i ∈ Aal )Int.Th (Ai, member(Ai, Aal ), Tn, Tn+1, Co), Tn) 3.",
                "Aal has a MB that being members gives them high utility value: MB(Aal , (∀Aal i ∈ Aal )Al val(Aal i , Cal , Aal , w) ≥ Al TrH, Tn) Members profiles are a crucial part of successful alliances.",
                "We assume that agents that have more accurate profiles of their adversaries will be more successful in such environments.",
                "Such agents will be able to predict when a member is about to breach the alliances contract (item 2 in the above model), and take counter measures (when item 3 will falsify).",
                "The robustness of the alliance is in part a function of its members trustfulness measure, objective position estimation, and other profile properties.",
                "We should note that an agent can simultaneously be part of more than one alliance.",
                "Such a temporary alliance, where the group members do not have a joint goal but act collaboratively for the interest of their own individual goals, is classified as a Treatment Group by modern psychologists [12] (in contrast to a Task Group, where its members have a joint goal).",
                "The Shared Activity model as presented in [5] modeled Treatment Group behavior using the same SharedPlans formalization.",
                "When comparing both definitions of an alliance and a Treatment Group we found an unsurprising resemblance between both models: the environment models definitions are almost identical (see SAs definitions in [5]), and their Selfish-Act and Cooperative Act axioms conform to our adversarial agents behavior.",
                "The main distinction between both models is the integration of a Helpful-behavior act axiom, in the Shared Activity which cannot be part of ours.",
                "This axiom states that an agent will consider taking action that will lower its Eval value (to a certain lower bound), if it believes that a group partner will gain a significant benefit.",
                "Such behavior cannot occur in a pure adversarial environment (as a zero-sum game is), where the alliance members are constantly on watch to manipulate their alliance to their own advantage.",
                "A6.",
                "Evaluation Maximization Axiom.",
                "In a case when all other axioms are inapplicable, we will proceed with the action that maximizes the heuristic value as computed in the Eval function. (∀Aag, Ao ∈ A, α ∈ Cag, Tn, w ∈ W) Bel(Aag, (∀γ ∈ Cag)Eval(Aag, α, w) ≥ Eval(Aag, γ, w), Tn) ⇒ Pot.Int.To(Aag, α, Tn, Tα, w) T1.",
                "Optimality on Eval = Utility The above axiomatic model handles situations where the Utility is unknown and the agents are bounded rational agents.",
                "The following theorem shows that in bilateral interactions, where the agents have the real Utility function (i.e., Eval = Utility) and are rational agents, the axioms provide the same optimal result as classic adversarial search (e.g., Min-Max).",
                "Theorem 1.",
                "Let Ae ag be an unbounded rational AE agent using the Eval heuristic evaluation function, Au ag be the same agent using the true Utility function, and Ao be a sole unbounded utility-based rational adversary.",
                "Given that Eval = Utility: (∀α ∈ CAu ag , α ∈ CAe ag , Tn, w ∈ W) Pot.Int.To(Au ag, α, Tn, Tα, w) → Pot.Int.To(Ae ag, α , Tn, Tα, w) ∧ ((α = α ) ∨ (Utility(Au ag, α, w) = Eval(Ae ag, α , w))) Sketch of proof - Given that Au ag has the real utility function and unbounded resources, it can generate the full game tree and run the optimal MinMax algorithm to choose the highest utility value action, which we denote by, α.",
                "The proof will show that Ae ag, using the AE axioms, will select the same or equal utility α (when there is more than one action with the same max utility) when Eval = Utility. (A1) Goal achieving axiom - suppose there is an α such that its completion will achieve Au ags goal.",
                "It will obtain the highest utility by Min-Max for Au ag.",
                "The Ae ag agent will select α or another action with the same utility value via A1.",
                "If such α does not exist, Ae ag cannot apply this axiom, and proceeds to A2. (A2) Preventive act axiom - (1) Looking at the basic case (see Prop1), if there is a β which leads Ao to achieve its goal, then a preventive action α will yield the highest utility for Au ag.",
                "Au ag will choose it through the utility, while Ae ag will choose it through A2. (2) In the general case, β is a highly beneficial action for Ao, thus yields low utility for Au ag, which will guide it to select an α that will prevent β, while Ae ag will choose it through A2.1 If such β does not exist for Ao, then A2 is not applicable, and Ae ag can proceed to A3. (A3) Suboptimal tactical move axiom - When using a heuristic Eval function, Ae ag has a partial belief in the profile of its adversary (item 4 in AE model), which may lead it to believe in SetActions (Prop1).",
                "In our case, Ae ag is holding a full profile on its optimal adversary and knows that Ao will behave optimally according to the real utility values on the complete search tree, therefore, any belief about suboptimal SetAction cannot exist, yielding this axiom inapplicable.",
                "Ae ag will proceed to A4. (A4) Profile detection axiom - Given that Ae ag has the full profile of Ao, none of Ae ags actions can increase its knowledge.",
                "That axiom will not be applied, and the agent will proceed with A6 (A5 will be disregarded because the interaction is bilateral). (A6) Evaluation maximization axiom - This axiom will select the max Eval for Ae ag.",
                "Given that Eval = Utility, the same α that was selected by Au ag will be selected. 3.",
                "EVALUATION The main purpose of our experimental analysis is to evaluate the models behavior and performance in a real adversarial environment.",
                "This section investigates whether bounded 1 A case where following the completion of β there exists a γ which gives high utility for Agent Au ag, cannot occur because Ao uses the same utility, and γs existence will cause it to classify β as a low utility action. 554 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) rational agents situated in such adversarial environments will be better off applying our suggested behavioral axioms. 3.1 The Domain To explore the use of the above model and its behavioral axioms, we decided to use the <br>connect-four game</br> as our adversarial environment.",
                "Connect-Four is a 2-player, zerosum game which is played using a 6x7 matrix-like board.",
                "Each turn, a player drops a disc into one of the 7 columns (the set of 21 discs is usually colored yellow for player 1 and red for player 2; we will use White and Black respectively to avoid confusion).",
                "The winner is the first player to complete a horizontal, vertical, or diagonal set of four discs with its color.",
                "On very rare occasions, the game might end in a tie if all the empty grids are filled, but no player managed to create a 4-disc set.",
                "The <br>connect-four game</br> was solved in [1], where it is shown that the first player (playing with the white discs) can force a win by starting in the middle column (column 4) and playing optimally However, the optimal strategy is very complex, and difficult to follow even for complex bounded rational agents, such as human players.",
                "Before we can proceed checking agent behavior, we must first verify that the domain conforms to the adversarial environments definition as given above (which the behavioral axioms are based on).",
                "First, when playing a <br>connect-four game</br>, the agent has an intention to win the game (item 1).",
                "Second (item 2), our agent believes that in Connect-Four there can only be one winner (or no winner at all in the rare occurrence of a tie).",
                "In addition, our agent believes that its opponent to the game will try to win (item 3), and we hope it has some partial knowledge (item 4) about its adversary (this knowledge can vary from nothing, through simple facts such as age, to strategies and weaknesses).",
                "Of course, not all Connect-Four encounters are adversarial.",
                "For example, when a parent is playing the game with its child, the following situation might occur: the child, having a strong incentive to win, treats the environment as adversarial (it intends to win, understands that there can only be one winner, and believes that its parent is trying to beat him).",
                "However, the parents point of view might see the environment as an educational one, where its goal is not to win the game, but to cause enjoyment or practice strategic reasoning.",
                "In such an educational environment, a new set of behavioral axioms might be more beneficial to the parents goals than our suggested adversarial behavioral axioms. 3.2 Axiom Analysis After showing that the <br>connect-four game</br> is indeed a zero-sum, bilateral adversarial environment, the next step is to look at players behaviors during the game and check whether behaving according to our model does improve performance.",
                "To do so we have collected log files from completed Connect-Four games that were played by human players over the Internet.",
                "Our collected log file data came from Play by eMail (PBeM) sites.",
                "These are web sites that host email games, where each move is taken by an email exchange between the server and the players.",
                "Many such sites archives contain real competitive interactions, and also maintain a ranking system for their members.",
                "Most of the data we used can be found in [6].",
                "As can be learned from [1], Connect-Four has an optimal strategy and a considerable advantage for the player who starts the game (which we call the White player).",
                "We will concentrate in our analysis on the second players moves (to be called Black).",
                "The White player, being the first to act, has the so-called initiative advantage.",
                "Having the advantage and a good strategy will keep the Black player busy reacting to its moves, instead of initiating threats.",
                "A threat is a combination of three discs of the same color, with an empty spot for the fourth winning disk.",
                "An open threat is a threat that can be realized in the opponents next move.",
                "In order for the Black player to win, it must somehow turn the tide, take the advantage and start presenting threats to the White player.",
                "We will explore Black players behavior and their conformance to our axioms.",
                "To do so, we built an application that reads log files and analyzes the Black players moves.",
                "The application contains two main components: (1) a Min-Max algorithm for evaluation of moves; (2) open threats detector for the discovering of open threats.",
                "The Min-Max algorithm will work to a given depth, d and for each move α will output the heuristic value for the next action taken by the player as written in the log file, h(α), alongside the maximum heuristic value, maxh(α), that could be achieved prior to taking the move (obviously, if h(α) = maxh(α), then the player did not do the optimal move heuristically).",
                "The threat detectors job is to notify if some action was taken in order to block an open threat (not blocking an open threat will probably cause the player to lose in the opponents next move).",
                "The heuristic function used by Min-Max to evaluate the players utility is the following function, which is simple to compute, yet provides a reasonable challenge to human opponents: Definition 8.",
                "Let Group be an adjacent set of four squares that are horizontal, vertical, or diagonal.",
                "Groupn b (Groupn w) be a Group with n pieces of the black (white) color and 4−n empty squares. h = ((Group1 b ∗α)+(Group2 b ∗β)+(Group3 b ∗γ)+(Group4 b ∗∞)) − ((Group1 w ∗α)+(Group2 w ∗β)+(Group3 w ∗γ)+(Group4 w ∗∞)) The values of α, β and δ can vary to form any desired linear combination; however, it is important to value them with the α < β < δ ordering in mind (we used 1, 4, and 8 as their respective values).",
                "Groups of 4 discs of the same color means victory, thus discovery of such a group will result in ∞ to ensure an extreme value.",
                "We now use our estimated evaluation function to evaluate the Black players actions during the Connect-Four adversarial interaction.",
                "Each game from the log file was input into the application, which processed and output a reformatted log file containing the h value of the current move, the maxh value that could be achieved, and a notification if an open threat was detected.",
                "A total of 123 games were analyzed (57 with White winning, and 66 with Black winning).",
                "A few additional games were manually ignored in the experiment, due to these problems: a player abandoning the game while the outcome is not final, or a blunt irrational move in the early stages of the game (e.g., not blocking an obvious winning group in the first opening moves).",
                "In addition, a single tie game was also removed.",
                "The simulator was run to a search depth of 3 moves.",
                "We now proceed to analyze the games with respect to each behavioral axiom.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 555 Table 1: Average heuristic difference analysis Black losses Black Won Avg minh -17.62 -12.02 Avg 3 lowest h moves (min3 h) -13.20 -8.70 3.2.1 Affirming the Suboptimal tactical move axiom The following section presents the heuristic evaluations of the Min-Max algorithm for each action, and checks the amount and extent of suboptimal tactical actions and their implications on performance.",
                "Table 1 shows results and insights from the games heuristic analysis, when search depth equals 3 (this search depth was selected for the results to be comparable to [9], see Section 3.2.3).",
                "The tables heuristic data is the difference between the present maximal heuristic value and the heuristic value of the action that was eventually taken by the player (i.e., the closer the number is to 0, the closer the action was to the maximum heuristic action).",
                "The first row presents the difference values of the action that had the maximal difference value among all the Black players actions in a given game, as averaged over all Blacks winning and losing games (see respective columns).",
                "In games in which the Black player loses, its average difference value was -17.62, while in games in which the Black player won, its average was -12.02.",
                "The second row expands the analysis by considering the 3 highest heuristic difference actions, and averaging them.",
                "In that case, we notice an average heuristic difference of 5 points between games which the Black player loses and games in which it wins.",
                "Nevertheless, the importance of those numbers is that they allowed us to take an educated guess on a threshold number of 11.5, as the value of the TrH constant, which differentiates between normal actions and highly beneficial ones.",
                "After finding an approximated TrH constant, we can proceed with an analysis of the importance of suboptimal moves.",
                "To do so we took the subset of games in which the minimum heuristic difference value for Blacks actions was 11.5.",
                "As presented in Table 2, we can see the different min3 h average of the 3 largest ranges and the respective percentage of games won.",
                "The first row shows that the Black player won only 12% of the games in which the average of its 3 highest heuristically difference actions (min3 h) was smaller than the suggested threshold, TrH = 11.5.",
                "The second row shows a surprising result: it seems that when min3 h > −4 the Black player rarely wins.",
                "Intuition would suggest that games in which the action evaluation values were closer to the maximal values will result in more winning games for Black.",
                "However, it seems that in the Connect-Four domain, merely responding with somewhat easily expected actions, without initiating a few surprising and suboptimal moves, does not yield good results.",
                "The last row sums up the main insights from the analysis; most of Blacks wins (83%) came when its min3 h was in the range of -11.5 to -4.",
                "A close inspection of those Black winning games shows the following pattern behind the numbers: after standard opening moves, Black suddenly drops a disc into an isolated column, which seems a waste of a move.",
                "White continues to build its threats, while usually disregarding Blacks last move, which in turn uses the isolated disc as an anchor for a future winning threat.",
                "The results show that it was beneficial for the Black player Table 2: Blacks winnings percentages % of games min3 h < −11.5 12% min3 h > −4 5% −11.5 ≤ min3 h ≤ −4 83% to take suboptimal actions and not give the current highest possible heuristic value, but will not be too harmful for its position (i.e., will not give high beneficial value to its adversary).",
                "As it turned out, learning the threshold is an important aspect of success: taking wildly risky moves (min3 h < −11.5) or trying to avoid them (min3 h > −4) reduces the Black players winning chances by a large margin. 3.2.2 Affirming the Profile Monitoring Axiom In the task of showing the importance of monitoring ones adversaries profiles, our log files could not be used because they did not contain repeated interactions between players, which are needed to infer the players knowledge about their adversaries.",
                "However, the importance of opponent modeling and its use in attaining tactical advantages was already studied in various domains ([3, 9] are good examples).",
                "In a recent paper, Markovitch and Reger [9] explored the notion of learning and exploitation of opponent weakness in competitive interactions.",
                "They apply simple learning strategies by analyzing examples from past interactions in a specific domain.",
                "They also used the Connect-Four adversarial domain, which can now be used to understand the importance of monitoring the adversarys profile.",
                "Following the presentation of their theoretical model, they describe an extensive empirical study and check the agents performance after learning the weakness model with past examples.",
                "One of the domains used as a competitive environment was the same <br>connect-four game</br> (Checkers was the second domain).",
                "Their heuristic function was identical to ours with three different variations (H1, H2, and H3) that are distinguished from one another in their linear combination coefficient values.",
                "The search depth for the players was 3 (as in our analysis).",
                "Their extensive experiments check and compare various learning strategies, risk factors, predefined feature sets and usage methods.",
                "The bottom line is that the Connect-Four domain shows an improvement from a 0.556 winning rate before modeling to a 0.69 after modeling (page 22).",
                "Their conclusions, showing improved performance when holding and using the adversarys model, justify the effort to monitor the adversary profile for continuous and repeated interactions.",
                "An additional point that came up in their experiments is the following: after the opponent weakness model has been learned, the authors describe different methods of integrating the opponent weakness model into the agents decision strategy.",
                "Nevertheless, regardless of the specific method they chose to work with, all integration methods might cause the agent to take suboptimal decisions; it might cause the agent to prefer actions that are suboptimal at the present decision junction, but which might cause the opponent to react in accordance with its weakness model (as represented by our agent) which in turn will be beneficial for us in the future.",
                "The agents behavior, as demonstrated in [9] further confirms and strengthens our Suboptimal Tactical Axiom as discussed in the previous section. 556 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 3.2.3 Additional Insights The need for the Goal Achieving, Preventive Act, and Evaluation Maximization axioms are obvious, and need no further verification.",
                "However, even with respect to those axioms, a few interesting insights came up in the log analysis.",
                "The Goal achieving and Preventive Act axioms, though theoretically trivial, seem to provide some challenge to a human player.",
                "In the initial inspection of the logs, we encountered few games2 where a player, for inexplicable reasons, did not block the other from winning or failed to execute its own winning move.",
                "We can blame those faults on the humans lack of attention, or a typing error in its move reply; nevertheless, those errors might occur in bounded rational agents, and the appropriate behavior needs to be axiomatized.",
                "A typical <br>connect-four game</br> revolves around generating threats and blocking them.",
                "In our analysis we looked for explicit preventive actions, i.e., moves that block a group of 3 discs, or that remove a future threat (in our limited search horizon).",
                "We found that in 83% of the total games there was at least one preventive action taken by the Black player.",
                "It was also found that Black averaged 2.8 preventive actions per game on the games in which it lost, while averaging 1.5 preventive actions per game when winning.",
                "It seems that Black requires 1 or 2 preventive actions to build its initial taking position, before starting to present threats.",
                "If it did not manage to win, it will usually prevent an extra threat or two before succumbing to White. 4.",
                "RELATED WORK Much research deals with the axiomatization of teamwork and mental states of individuals: some models use knowledge and belief [10], others have models of goals and intentions [8, 4].",
                "However, all these formal theories deal with agent teamwork and cooperation.",
                "As far as we know, our model is the first to provide a formalized model for explicit adversarial environments and agents behavior in it.",
                "The classical Min-Max adversarial search algorithm was the first attempt to integrate the opponent into the search space with a weak assumption of an optimally playing opponent.",
                "Since then, much effort has gone into integrating the opponent model into the decision procedure to predict future behavior.",
                "The M∗ algorithm presented by Carmel and Markovitch [2] showed a method of incorporating opponent models into adversary search, while in [3] they used learning to provide a more accurate opponent model in a 2player repeated game environment, where agents strategies were modeled as finite automata.",
                "Additional Adversarial planning work was done by Willmott et al. [13], which provided an adversarial planning approach to the game of GO.",
                "The research mentioned above dealt with adversarial search and the integration of opponent models into classical utilitybased search methods.",
                "That work shows the importance of opponent modeling and the ability to exploit it to an agents advantage.",
                "However, the basic limitations of those search methods still apply; our model tries to overcome those limitations by presenting a formal model for a new, mental state-based adversarial specification. 5.",
                "CONCLUSIONS We presented an Adversarial Environment model for a 2 These were later removed from the final analysis. bounded rational agent that is situated in an N-player, zerosum environment.",
                "We used the SharedPlans formalization to define the model and the axioms that agents can apply as behavioral guidelines.",
                "The model is meant to be used as a guideline for designing agents that need to operate in such adversarial environments.",
                "We presented empirical results, based on ConnectFour log file analysis, that exemplify the model and the axioms for a bilateral instance of the environment.",
                "The results we presented are a first step towards an expanded model that will cover all types of adversarial environments, for example, environments that are non-zero-sum, and environments that contain natural agents that are not part of the direct conflict.",
                "Those challenges and more will be dealt with in future research. 6.",
                "ACKNOWLEDGMENT This research was supported in part by Israel Science Foundation grants #1211/04 and #898/05. 7.",
                "REFERENCES [1] L. V. Allis.",
                "A knowledge-based approach of Connect-Four - the game is solved: White wins.",
                "Masters thesis, Free University, Amsterdam, The Netherlands, 1988. [2] D. Carmel and S. Markovitch.",
                "Incorporating opponent models into adversary search.",
                "In Proceedings of the Thirteenth National Conference on Artificial Intelligence, pages 120-125, Portland, OR, 1996. [3] D. Carmel and S. Markovitch.",
                "Opponent modeling in multi-agent systems.",
                "In G. Weiß and S. Sen, editors, Adaptation and Learning in Multi-Agent Systems, pages 40-52.",
                "Springer-Verlag, 1996. [4] B. J. Grosz and S. Kraus.",
                "Collaborative plans for complex group action.",
                "Artificial Intelligence, 86(2):269-357, 1996. [5] M. Hadad, G. Kaminka, G. Armon, and S. Kraus.",
                "Supporting collaborative activity.",
                "In Proc. of AAAI-2005, pages 83-88, Pittsburgh, 2005. [6] http://www.gamerz.net/˜pbmserv/. [7] S. Kraus and D. Lehmann.",
                "Designing and building a negotiating automated agent.",
                "Computational Intelligence, 11:132-171, 1995. [8] H. J. Levesque, P. R. Cohen, and J. H. T. Nunes.",
                "On acting together.",
                "In Proc. of AAAI-90, pages 94-99, Boston, MA, 1990. [9] S. Markovitch and R. Reger.",
                "Learning and exploiting relative weaknesses of opponent agents.",
                "Autonomous Agents and Multi-Agent Systems, 10(2):103-130, 2005. [10] Y. M. Ronald Fagin, Joseph Y. Halpern and M. Y. Vardi.",
                "Reasoning about knowledge.",
                "MIT Press, Cambridge, Mass., 1995. [11] P. Thagard.",
                "Adversarial problem solving: Modeling an oponent using explanatory coherence.",
                "Cognitive Science, 16(1):123-149, 1992. [12] R. W. Toseland and R. F. Rivas.",
                "An Introduction to Group Work Practice.",
                "Prentice Hall, Englewood Cliffs, NJ, 2nd edition edition, 1995. [13] S. Willmott, J. Richardson, A. Bundy, and J. Levine.",
                "An adversarial planning approach to Go.",
                "Lecture Notes in Computer Science, 1558:93-112, 1999.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 557"
            ],
            "original_annotated_samples": [
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) rational agents situated in such adversarial environments will be better off applying our suggested behavioral axioms. 3.1 The Domain To explore the use of the above model and its behavioral axioms, we decided to use the <br>connect-four game</br> as our adversarial environment.",
                "The <br>connect-four game</br> was solved in [1], where it is shown that the first player (playing with the white discs) can force a win by starting in the middle column (column 4) and playing optimally However, the optimal strategy is very complex, and difficult to follow even for complex bounded rational agents, such as human players.",
                "First, when playing a <br>connect-four game</br>, the agent has an intention to win the game (item 1).",
                "In such an educational environment, a new set of behavioral axioms might be more beneficial to the parents goals than our suggested adversarial behavioral axioms. 3.2 Axiom Analysis After showing that the <br>connect-four game</br> is indeed a zero-sum, bilateral adversarial environment, the next step is to look at players behaviors during the game and check whether behaving according to our model does improve performance.",
                "One of the domains used as a competitive environment was the same <br>connect-four game</br> (Checkers was the second domain)."
            ],
            "translated_annotated_samples": [
                "En la Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07), los agentes racionales situados en entornos adversariales se beneficiarán más al aplicar nuestros axiomas de comportamiento sugeridos. 3.1 El Dominio Para explorar el uso del modelo anterior y sus axiomas de comportamiento, decidimos utilizar el <br>juego de Conecta Cuatro</br> como nuestro entorno adversarial.",
                "El <br>juego de Conecta Cuatro</br> fue resuelto en [1], donde se muestra que el primer jugador (jugando con los discos blancos) puede forzar una victoria comenzando en la columna del medio (columna 4) y jugando de manera óptima. Sin embargo, la estrategia óptima es muy compleja y difícil de seguir incluso para agentes racionales limitados complejos, como los jugadores humanos.",
                "Primero, al jugar al <br>juego de Conecta Cuatro</br>, el agente tiene la intención de ganar el juego (ítem 1).",
                "En un entorno educativo como este, un nuevo conjunto de axiomas de comportamiento podría ser más beneficioso para los objetivos de los padres que nuestros axiomas de comportamiento adversarial sugeridos. 3.2 Análisis de Axiomas Después de demostrar que el <br>juego de Conecta Cuatro</br> es de hecho un entorno adversarial bilateral de suma cero, el siguiente paso es observar el comportamiento de los jugadores durante el juego y verificar si comportarse de acuerdo con nuestro modelo mejora el rendimiento.",
                "Uno de los dominios utilizados como entorno competitivo fue el mismo <br>juego de Conecta Cuatro</br> (las Damas fueron el segundo dominio)."
            ],
            "translated_text": "Un modelo de entorno adversarial para agentes racionales limitados en interacciones de suma cero Inon Zuckerman1, Sarit Kraus1, Jeffrey S. Rosenschein2, Gal Kaminka1 1 Departamento de Ciencias de la Computación 2 Escuela de Ingeniería Universidad Bar-Ilan y Ciencias de la Computación Ramat-Gan, Israel Universidad Hebrea, Jerusalén, Israel {zukermi, sarit, galk}@cs.biu.ac.il jeff@cs.huji.ac.il RESUMEN Los entornos multiagentes a menudo no son cooperativos ni colaborativos; en muchos casos, los agentes tienen intereses conflictivos, lo que lleva a interacciones adversariales. Este documento presenta un modelo formal de Entorno Adversarial para agentes racionales acotados que operan en un entorno de suma cero. En tales entornos, los intentos de utilizar métodos de búsqueda basados en utilidad clásica pueden plantear una variedad de dificultades (por ejemplo, modelar implícitamente al oponente como un maximizador de utilidad omnisciente, en lugar de aprovechar un modelo de oponente más matizado y explícito). Definimos un Entorno Adversario al describir los estados mentales de un agente en dicho entorno. Luego presentamos axiomas de comportamiento que están destinados a servir como principios de diseño para construir agentes adversarios. Exploramos la aplicación de nuestro enfoque mediante el análisis de archivos de registro de juegos de Conecta Cuatro completados, y presentamos un análisis empírico de la adecuación de los axiomas. Categorías y Descriptores de Asignaturas I.2.11 [Inteligencia Artificial]: Inteligencia Artificial Distribuida-Agentes Inteligentes, Sistemas Multiagente; I.2.4 [Inteligencia Artificial]: Formalismos y Métodos de Representación del Conocimiento - Lógica Modal Términos Generales Diseño, Teoría 1. Introducción: En las primeras investigaciones en sistemas multiagente (SMA) se consideraban grupos cooperativos de agentes; debido a que los agentes individuales tenían recursos limitados o acceso limitado a la información (por ejemplo, potencia de procesamiento limitada, cobertura sensorial limitada), trabajaban juntos por diseño para resolver problemas que individualmente no podían resolver, o al menos no podían resolver de manera eficiente. Sin embargo, la investigación de MAS pronto comenzó a considerar agentes interactivos con intereses individualizados, como representantes de diferentes humanos u organizaciones con intereses no idénticos. Cuando las interacciones son guiadas por intereses diversos, los participantes pueden tener que superar desacuerdos, interacciones no cooperativas e incluso intentos intencionales de dañarse mutuamente. Cuando ocurren este tipo de interacciones, los entornos requieren un comportamiento apropiado por parte de los agentes situados en ellos. Llamamos a estos entornos Entornos Adversarios, y llamamos a los agentes en conflicto Adversarios. Los modelos de cooperación y trabajo en equipo han sido ampliamente explorados en MAS a través de la axiomatización de estados mentales (por ejemplo, [8, 4, 5]). Sin embargo, ninguna de estas investigaciones abordó los dominios adversarios y sus implicaciones para el comportamiento de los agentes. Nuestro artículo aborda este problema al proporcionar un modelo formal y axiomatizado de estados mentales para un subconjunto de dominios adversariales, específicamente entornos adversariales simples de suma cero. Los encuentros de suma cero simples existen, por supuesto, en varios juegos de dos jugadores (por ejemplo, Ajedrez, Damas), pero también existen en juegos de n jugadores (por ejemplo, Risk, Diplomacia), subastas de un solo bien y en otros lugares. En estos entornos recientes en particular, el uso de una búsqueda adversarial basada en utilidad (como el algoritmo Min-Max) no siempre proporciona una solución adecuada; la función de recompensa podría ser bastante compleja o difícil de cuantificar, y existen limitaciones computacionales naturales en agentes racionales acotados. Además, los métodos de búsqueda tradicionales (como Min-Max) no hacen uso de un modelo del oponente, lo cual ha demostrado ser una adición valiosa a la planificación adversarial [9, 3, 11]. En este artículo, desarrollamos un modelo formal y axiomatizado para agentes racionales limitados que se encuentran en un entorno adversarial de suma cero. El modelo utiliza diferentes operadores de modalidad, y sus principales fundamentos son el modelo SharedPlans [4] para el comportamiento colaborativo. Exploramos las propiedades del entorno y los estados mentales de los agentes para derivar axiomas de comportamiento; estos axiomas de comportamiento constituyen un modelo formal que sirve como especificación y guía de diseño para el diseño de agentes en tales entornos. Luego investigamos el comportamiento de nuestro modelo empíricamente utilizando el juego de mesa Conecta Cuatro. Mostramos que este juego se ajusta a nuestra definición de entorno, y analizamos el comportamiento de los jugadores utilizando un gran conjunto de registros de partidas completadas 550 978-81-904262-7-5 (RPS) c 2007 archivos de IFAAMAS. Además, utilizamos los resultados presentados en [9] para discutir la importancia del modelado del oponente en nuestro dominio adversarial de Conecta Cuatro. El artículo continúa de la siguiente manera. La sección 2 presenta la formalización de los modelos. La sección 3 presenta el análisis empírico y sus resultados. Discutimos el trabajo relacionado en la Sección 4, y concluimos y presentamos las direcciones futuras en la Sección 5. ENTORNOS ADVERSARIOS El modelo de entorno adversario (denominado como AE) está destinado a guiar el diseño de agentes al proporcionar una especificación de las capacidades y actitudes mentales de un agente en un entorno adversario. Nos enfocamos aquí en tipos específicos de entornos adversarios, especificados de la siguiente manera: 1. Interacciones de suma cero: las utilidades positivas y negativas de todos los agentes suman cero; 2. Agentes adversarios simples: todos los agentes en el entorno son agentes adversarios; 3. AEs bilaterales: AEs con exactamente dos agentes; 4. AEs multilaterales: AEs de tres o más agentes. Trabajaremos en instanciaciones tanto bilaterales como multilaterales de entornos de suma cero y simples. En particular, nuestro modelo de entorno adversarial tratará con interacciones que consisten en N agentes (N ≥ 2), donde todos los agentes son adversarios y solo un agente puede tener éxito. Ejemplos de tales entornos van desde juegos de mesa (por ejemplo, Ajedrez, Conecta Cuatro y Diplomacia) hasta ciertos entornos económicos (por ejemplo, subastas de N postores sobre un único bien). 2.1 Resumen del Modelo Nuestro enfoque es formalizar las actitudes mentales y comportamientos de un único agente adversario; consideramos cómo un único agente percibe el AE. La siguiente lista especifica las condiciones y estados mentales de un agente en un AE simple de suma cero: 1. El agente tiene la intención individual de que su propio objetivo se complete; 2. El agente tiene la creencia individual de que él y sus adversarios persiguen objetivos completamente opuestos (definidos a continuación) y que solo puede haber un ganador; 3. El agente tiene la creencia individual de que cada adversario tiene la intención de completar su propio objetivo conflictivo completo; 4. El agente tiene una creencia individual en el perfil (parcial) de sus adversarios. El ítem 3 es necesario, ya que podría ser el caso de que algún agente tenga un objetivo completamente conflictivo y esté considerando adoptar la intención de completarlo, pero aún no está comprometido a lograrlo. Esto podría ocurrir porque el agente aún no ha reflexionado sobre los efectos que adoptar esa intención podría tener en las otras intenciones que actualmente mantiene. En tales casos, es posible que ni siquiera se considere en un entorno adversarial. El ítem 4 establece que el agente debe tener alguna creencia sobre los perfiles de sus adversarios. El perfil representa todo el conocimiento que el agente tiene sobre su adversario: sus debilidades, capacidades estratégicas, objetivos, intenciones, confiabilidad y más. Puede ser dado explícitamente o puede ser aprendido a partir de observaciones de encuentros pasados. 2.2 Definiciones de modelos para estados mentales Utilizamos las definiciones de los operadores modales, predicados y metapredicados de Grosz y Krauss, tal como se definen en su formalización de SharedPlan [4]. Recordamos aquí algunos de los predicados y operadores que se utilizan en esa formalización: Int.To(Ai, α, Tn, Tα, C) representa las intenciones de Ai en el tiempo Tn de realizar una acción α en el tiempo Tα en el contexto de C. Int.Th(Ai, prop, Tn, Tprop, C) representa las intenciones de Ai en el tiempo Tn de que una cierta proposición prop se cumpla en el tiempo Tprop en el contexto de C. Los operadores de intención potencial, Pot.Int.To(...) y Pot.Int.Th(...), se utilizan para representar el estado mental cuando un agente considera adoptar una intención, pero no ha deliberado sobre la interacción de las otras intenciones que tiene. El operador Bel(Ai, f, Tf) representa al agente Ai creyendo en la afirmación expresada en la fórmula f, en el tiempo Tf. MB(A, f, Tf) representa la creencia mutua para un grupo de agentes A. Una instantánea del sistema encuentra que nuestro entorno se encuentra en algún estado e ∈ E de estados variables ambientales, y cada adversario en cualquier LAi ∈ L de posibles estados locales. En cualquier paso de tiempo dado, el sistema estará en algún mundo w del conjunto de todos los mundos posibles w ∈ W, donde w = E×LA1 ×LA2 × ...LAn, y n es el número de adversarios. Por ejemplo, en un juego de póker Texas Holdem, el estado local de un agente podría ser su propio conjunto de cartas (que es desconocido para su adversario) mientras que el entorno consistirá en el bote de apuestas y las cartas comunitarias (que son visibles para ambos jugadores). Una función de utilidad bajo esta formalización se define como un mapeo de un mundo posible w ∈ W a un elemento en , que expresa la deseabilidad del mundo, desde la perspectiva de un único agente. Normalmente normalizamos el rango a [0,1], donde 0 representa el mundo menos deseable posible, y 1 es el mundo más deseable. La implementación de la función de utilidad depende del dominio en cuestión. La siguiente lista especifica nuevos predicados, funciones, variables y constantes utilizadas en conjunto con las definiciones originales para la formalización del entorno adversarial: 1. φ es una acción nula (el agente no hace nada). 2. GAi es el conjunto de objetivos del agente Ai. Cada objetivo es un conjunto de predicados cuya satisfacción hace que el objetivo esté completo (usamos G∗ Ai ∈ GAi para representar un objetivo arbitrario del agente Ai). 3. gAi es el conjunto de subobjetivos del agente Ai. Los subobjetivos son predicados cuya satisfacción representa un hito importante hacia el logro del objetivo completo. gG∗ Ai ⊆ gAi es el conjunto de subobjetivos que son importantes para la finalización del objetivo G∗ Ai (usaremos g∗ G∗ Ai ∈ gG∗ Ai para representar un subobjetivo arbitrario). 4. P Aj Ai es el objeto de perfil que el agente Ai tiene sobre el agente Aj. 5. CA es un conjunto general de acciones para todos los agentes en A que se derivan de las restricciones del entorno. CAi ⊆ CA es el conjunto de acciones posibles del agente Ai. La expresión Do(Ai, α, Tα, w) se cumple cuando Ai realiza la acción α durante el intervalo de tiempo Tα en el mundo w. 7. La afirmación Achieve(G∗ Ai , α, w) es verdadera cuando se logra el objetivo G∗ Ai siguiendo la finalización de la acción α en el mundo w ∈ W, donde α ∈ CAi. El perfil (Ai, PAi Ai) es verdadero cuando el agente Ai tiene un perfil de objeto para el agente Aj. Definición 1. El conflicto total (FulConf) describe una interacción de suma cero donde solo se puede completar un solo objetivo de los objetivos en conflicto. FulConf(G∗ Ai , G∗ Aj ) ⇒ (∃α ∈ CAi , ∀w, β ∈ CAj ) (Achieve(G∗ Ai , α, w) ⇒ ¬Achieve(G∗ Aj , β, w)) ∨ (∃β ∈ CAj , ∀w, α ∈ CAi )(Achieve(G∗ Aj , β, w) ⇒ ¬Achieve(G∗ Ai , α, w)) Definición 2. El Conocimiento Adversarial (AdvKnow) es una función que devuelve un valor que representa la cantidad de The Sixth Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 551 agente de conocimiento Ai tiene sobre el perfil del agente Aj, en el tiempo Tn. Cuanto mayor sea el valor, más conocimiento tiene el agente Ai. AdvKnow: P Aj Ai × Tn → Definición 3. Eval: Esta función de evaluación devuelve un valor de utilidad esperada estimado para un agente en A, después de completar una acción de CA en algún estado del mundo w. Eval: A × CA × w → Definición 4. TrH - (Umbral) es una constante numérica en el rango [0,1] que representa un valor umbral de la función de evaluación (Eval). Una acción que produce una evaluación de utilidad estimada por encima del TrH se considera una acción altamente beneficiosa. El valor Eval es una estimación y no la función de utilidad real, la cual suele ser desconocida. Utilizar el valor real de utilidad para un agente racional fácilmente produciría el mejor resultado para ese agente. Sin embargo, los agentes generalmente no tienen las funciones de utilidad reales, sino más bien una estimación heurística de las mismas. Existen dos propiedades importantes que deben cumplir para la función de evaluación: Propiedad 1. La función de evaluación debería indicar que el estado del mundo más deseable es aquel en el que se logra el objetivo. Por lo tanto, una vez que se ha satisfecho el objetivo, no puede haber ninguna acción futura que pueda colocar al agente en un estado del mundo con un valor de Eval más alto. (∀Ai, G∗ Ai , α, β ∈ CAi , w ∈ W) Lograr(G∗ Ai , α, w) ⇒ Eval(Ai, α, w) ≥ Eval(Ai, β, w) Propiedad 2. La función de evaluación debe proyectar una acción que cause la finalización de un objetivo o subobjetivo a un valor que sea mayor que TrH (una acción altamente beneficiosa). (∀Ai, G∗ Ai ∈ GAi , α ∈ CAi , w ∈ W, g∗ GAi ∈ gGAi ) Lograr(G∗ Ai , α, w) ∨ Lograr(g∗ GAi , α, w) ⇒ Eval(Ai, α, w) ≥ TrH. Definición 5. La acción de conjunto Definimos una acción de conjunto (SetAction) como un conjunto de operaciones de acción (ya sean acciones complejas o básicas) de algunos conjuntos de acción CAi y CAj que, según la creencia del agente Ai, están unidas por una relación temporal y consecuente, formando una cadena de eventos (acción y su acción consecuente siguiente). (∀α1 , . . . , αu ∈ CAi , β1 , . . . , βv ∈ CAj , w ∈ W) SetAction(α1 , . . . , αu , β1 , . . . , βv , w) ⇒ ((Do(Ai, α1 , Tα1 , w) ⇒ Do(Aj, β1 , Tβ1 , w)) ⇒ Do(Ai, α2 , Tα2 , w) ⇒ . . . ⇒ Do(Ai, αu , Tαu , w)) La relación consecuente podría existir debido a varias restricciones ambientales (cuando una acción obliga al adversario a responder con una acción específica) o debido al conocimiento de los agentes sobre el perfil de su adversario. Propiedad 3. A medida que aumenta el conocimiento que tenemos sobre nuestro adversario, tendremos creencias adicionales sobre su comportamiento en diferentes situaciones, lo que a su vez crea un nuevo conjunto de acciones. Formalmente, si nuestro AdvKnow en el tiempo Tn+1 es mayor que AdvKnow en el tiempo Tn, entonces cada SetAction conocido en el tiempo Tn también es conocido en el tiempo Tn+1. AdvKnow(P Aj Ai , Tn+1) > AdvKnow(P Aj Ai , Tn) ⇒ (∀α1 , . . . , αu ∈ CAi , β1 , . . . , βv ∈ CAj ) Bel(Aag, SetAction(α1 , . . . , αu , β1 , . . . , βv ), Tn) ⇒ Bel(Aag, SetAction(α1 , . . . , αu , β1 , . . . , βv ), Tn+1) 2.3 La formulación del entorno Los siguientes axiomas proporcionan la definición formal para un Entorno Adversarial (AE) simple y de suma cero. La satisfacción de estos axiomas significa que el agente está situado en un entorno así. Proporciona especificaciones para que el agente Aag interactúe con su conjunto de adversarios A con respecto a los objetivos G∗ Aag y G∗ A en el tiempo TCo en algún estado del mundo w. AE(Aag, A, G∗ Aag, A1, . . . , Ak, G∗ A1, . . . , G∗ Ak, Tn, w) 1. Aag tiene un Int.Th su objetivo se completaría: (∃α ∈ CAag , Tα) Int.Th(Aag, Lograr(G∗ Aag , α), Tn, Tα, AE) 2. Aag cree que él y cada uno de sus adversarios Ao están persiguiendo metas completamente conflictivas: (∀Ao ∈ {A1, . . . , Ak}) Bel(Aag, FulConf(G∗ Aag , G∗ Ao ), Tn) 3. Aag cree que cada uno de sus adversarios en Ao tiene la Int.Th su objetivo en conflicto G∗ Aoi se completará: (∀Ao ∈ {A1, . . . , Ak})(∃β ∈ CAo , Tβ) Bel(Aag, Int.Th(Ao, Lograr(G∗ Ao , β), TCo, Tβ, AE), Tn) 4. Para construir un agente que pueda operar con éxito dentro de un entorno de adversarios estratégicos, debemos especificar pautas de comportamiento para sus interacciones. Utilizar una estrategia de maximización de Eval ingenua hasta cierta profundidad de búsqueda no siempre dará resultados satisfactorios por varias razones: (1) el problema del horizonte de búsqueda al buscar a una profundidad fija; (2) la fuerte suposición de un adversario óptimamente racional con recursos ilimitados; (3) utilizar una función de evaluación estimada que no dará resultados óptimos en todos los estados del mundo, y que puede ser explotada [9]. Los siguientes axiomas especifican los principios de comportamiento que se pueden utilizar para diferenciar entre agentes exitosos y menos exitosos en el entorno adversarial mencionado anteriormente. Esos axiomas deben ser utilizados como principios de especificación al diseñar e implementar agentes que deben ser capaces de desempeñarse bien en entornos adversarios. Los axiomas conductuales representan situaciones en las que el agente adoptará intenciones potenciales para (Pot.Int.To(...)) realizar una acción, lo que típicamente requerirá un razonamiento de medios-fines para seleccionar un posible curso de acción. Este razonamiento llevará a la adopción de un Int.To(...) (ver [4]). A1. Axioma para lograr metas. El primer axioma es el caso más simple; cuando el agente Aag cree que está a una acción (α) de lograr su objetivo en conflicto G∗ Aag, debería adoptar la intención potencial de hacer α y completar su objetivo. (∀Aag, α ∈ CAag, Tn, Tα, w ∈ W) (Bel(Aag, Do(Aag, α, Tα, w) ⇒ Achieve(G∗ Aag, α, w)) ⇒ Pot.Int.To(Aag, α, Tn, Tα, w) Este comportamiento algo trivial es el primer y más fuerte axioma. En cualquier situación, cuando el agente está a una acción de completar el objetivo, debería completar la acción. Cualquier función de evaluación justa clasificaría naturalmente a α como la acción de valor máximo (propiedad 1). Sin embargo, sin una axiomatización explícita de dicho comportamiento, podría haber situaciones en las que el agente decida tomar otra acción por diversas razones, debido a sus recursos de decisión limitados. A2. Axioma del Acto Preventivo. En una situación adversarial, el agente Aag podría decidir tomar acciones que dañen uno de los planes de su adversario para completar su objetivo, incluso si esas acciones no avanzan explícitamente a Aag hacia su objetivo en conflicto G∗ Aag. Tal acción preventiva tendrá lugar cuando el agente Aag tenga la creencia sobre la posibilidad de que su adversario Ao realice una acción β que le otorgará un alto 552 The Sixth Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) valor de evaluación de utilidad (> TrH). Creer que tomar la acción α evitará que el oponente realice su β, adoptará una intención potencial de hacer α. (∀Aag, Ao ∈ A, α ∈ CAag , β ∈ CAo , Tn, Tβ, w ∈ W) (Bel(Aag, Do(Ao, β, Tβ, w) ∧ Eval(Ao, β, w) > TrH, Tn) ∧ Bel(Aag, Do(Aag, α, Tα, w) ⇒ ¬Do(Ao, β, Tβ, w), Tn) ⇒ Pot.Int.To(Aag, α, Tn, Tα, w) Este axioma es un componente básico de cualquier entorno adversarial. Por ejemplo, al observar un juego de ajedrez, un jugador podría darse cuenta de que está a punto de ser jaque mateado por su oponente, por lo tanto, realiza una jugada preventiva. Otro ejemplo es el juego de Cuatro en Línea: cuando un jugador tiene una fila de tres fichas, su oponente debe bloquearla o perder. Una instancia específica de A1 ocurre cuando el adversario está a una acción de lograr su objetivo, y la acción preventiva inmediata debe ser tomada por el agente. Formalmente, tenemos las mismas creencias que se mencionaron anteriormente, con la creencia modificada de que realizar la acción β hará que el agente Ao logre su objetivo. Proposición 1: Caso de prevención o pérdida. (∀Aag, Ao ∈ A, α ∈ CAag , β ∈ CAo , G∗ Ao , Tn, Tα, Tβ, w ∈ W) Bel(Aag, Do(Ao, β, Tβ, w) ⇒ Achieve(G∗ Ao , β, w), Tn) ∧ Bel(Aag, Do(Aag, α, Tα, w) ⇒ ¬Do(Ao, β, Tβ, w)) ⇒ Pot.Int.To(Aag, α, Tn, Tα, w) Esquema de prueba: La Proposición 1 puede derivarse fácilmente del axioma A1 y de la propiedad 2 de la función Eval, que establece que cualquier acción que cause la finalización de un objetivo es una acción altamente beneficiosa. El comportamiento del acto preventivo ocurrirá implícitamente cuando la función Eval sea igual a la función de utilidad del mundo real. Sin embargo, al ser agentes racionales limitados y al tratar con una función de evaluación estimada, necesitamos axiomatizar explícitamente dicho comportamiento, ya que no siempre ocurrirá implícitamente a partir de la función de evaluación. A3. Axioma de Movimiento Táctico Subóptimo. En muchos escenarios, puede ocurrir una situación en la que un agente decida no tomar la acción más beneficiosa actual que puede tomar (la acción con el valor de evaluación de utilidad máximo), porque cree que tomar otra acción (con un valor de evaluación de utilidad más bajo) podría generar (dependiendo de la respuesta del adversario) una posibilidad futura para una acción altamente beneficiosa. Esto ocurrirá con mayor frecuencia cuando la función Eval sea inexacta y difiera en gran medida de la función de Utilidad. De manera formal, el agente Aag cree en una cierta SetAction que evolucionará de acuerdo con su acción inicial y producirá un valor beneficioso alto (> TrH) únicamente para él. (∀Aag, Ao ∈ A, Tn, w ∈ W) (∃α1 , . . . , αu ∈ CAi , β1 , . . . , βv ∈ CAj , Tα1 ) Bel(Aag, SetAction(α1 , . . . , αu , β1 , . . . , βv ), Tn) ∧ Bel(Aag, Eval(Ao, βv , w) < TrH < Eval(Aag, αu , w), Tn) ⇒ Pot.Int.To(Aag, α1 , Tn, Tα1 , w) Un agente podría creer que una cadena de eventos ocurrirá por diversas razones debido a la naturaleza inevitable del dominio. Por ejemplo, en el ajedrez, a menudo observamos lo siguiente: una jugada provoca una posición de jaque, lo que a su vez limita los movimientos del oponente para evitar el jaque, a lo que el primer jugador podría reaccionar con otro jaque, y así sucesivamente. El agente también podría creer en una cadena de eventos basada en su conocimiento del perfil de su adversario, lo que le permite prever los movimientos del adversario con alta precisión. A4. Axioma de Detección de Perfiles. El agente puede ajustar los perfiles de sus adversarios mediante observaciones y estudio de patrones (específicamente, si hay encuentros repetidos con el mismo adversario). Sin embargo, en lugar de esperar a que se revele la información del perfil, un agente también puede iniciar acciones que obligarán a su adversario a reaccionar de una manera que revele conocimiento del perfil sobre él. Formalmente, el axioma establece que si todas las acciones (γ) no son acciones altamente beneficiosas (< TrH), el agente puede realizar la acción α en el tiempo Tα si cree que resultará en una acción no altamente beneficiosa β por parte de su adversario, lo que a su vez le enseña sobre el perfil del adversario, es decir, proporciona un mayor AdvKnow(P Aj Ai , Tβ). (∀Aag, Ao ∈ A, α ∈ CAag , β ∈ CAo , Tn, Tα, Tβ, w ∈ W) Bel(Aag, (∀γ ∈ CAag )Eval(Aag, γ, w) < TrH, Tn) ∧ Bel(Aag, Do(Aag, α, Tα, w) ⇒ Do(Ao, β, Tβ, w), Tn) ∧ Bel(Aag, Eval(Ao, β, w) < TrH) ∧ Bel(Aag, AdvKnow(P Aj Ai , Tβ) > AdvKnow(P Aj Ai , Tn), Tn) ⇒ Pot.Int.To(Aag, α, Tn, Tα, w) Por ejemplo, volviendo al escenario del juego de ajedrez, consideremos comenzar una partida contra un oponente del que no sabemos nada, ni siquiera si es humano o un oponente computarizado. Podríamos comenzar a jugar una estrategia que sea adecuada frente a un oponente promedio, y ajustar nuestro juego de acuerdo a su nivel de juego. A5. Axioma de Formación de Alianzas El siguiente axioma de comportamiento es relevante únicamente en una instancia multilateral del entorno adversarial (obviamente, una alianza no puede formarse en un encuentro bilateral de suma cero). En diferentes situaciones durante una interacción multilateral, un grupo de agentes podría creer que es de su interés formar una alianza temporal. Una alianza de este tipo es un acuerdo que limita el comportamiento de sus miembros, pero se cree por parte de estos que les permite alcanzar un valor de utilidad mayor al que podrían lograr fuera de la alianza. Como ejemplo, podemos observar el clásico juego de mesa Risk, donde cada jugador tiene como objetivo individual ser el único conquistador del mundo, un juego de suma cero. Sin embargo, para lograr este objetivo, podría ser estratégicamente sabio hacer acuerdos de alto el fuego a corto plazo con otros jugadores, o unirse y atacar a un oponente que sea más fuerte que el resto. Los términos de una alianza definen la forma en que sus miembros deben actuar. Es un conjunto de predicados, denominados Términos, que es acordado por los miembros de la alianza y debe mantenerse verdadero durante la duración de la alianza. Por ejemplo, el conjunto de términos en el escenario de riesgo podría contener los siguientes predicados: 1. Los miembros de la alianza no se atacarán entre sí en los territorios X, Y y Z; 2. Los miembros de la alianza contribuirán C unidades por turno para atacar al adversario Ao; 3. Los miembros están obligados a permanecer como parte de la alianza hasta el tiempo Tk o hasta que el ejército adversario Ao sea más pequeño que Q. El conjunto de términos especifica restricciones intergrupales en cada uno de los miembros de la alianza (∀Aal i ∈ Aal ⊆ A) conjunto de acciones Cal i ⊆ C. Definición 6. El valor total de evaluación que el agente Ai logrará al ser parte de Aal es la suma de Evali (Eval para Ai) de cada uno de los valores de evaluación de Aal j después de tomar sus propias acciones α (a través del predicado del agente(α)): Al val(Ai, Cal, Aal, w) = α∈Cal Evali(Aal j, agente(α), w) Definición 7. Al TrH - es un número que representa un valor de Al en la Sexta Conferencia Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 553 umbral; por encima de él, se puede decir que la alianza es una alianza altamente beneficiosa. El valor de Al TrH se calculará dinámicamente de acuerdo con el progreso de la interacción, como se puede ver en [7]. Después de formarse una alianza, sus miembros ahora están trabajando en su entorno adversarial normal, así como de acuerdo con los estados mentales y axiomas requeridos para sus interacciones como parte de la alianza. El siguiente modelo de Alianza (AL) especifica las condiciones bajo las cuales el grupo Aal puede considerarse en una alianza y trabajando con un nuevo y limitado conjunto de acciones Cal, en el tiempo Tn. AL(Aal , Cal , w, Tn) 1. \n\nAL(Aal , Cal , w, Tn) 1. Aal tiene un MB en el que todos los miembros son parte de Aal: MB(Aal, (∀Aal i ∈ Aal)miembro(Aal i, Aal), Tn) 2. Aal tiene una MB que el grupo se mantenga: MB(Aal , (∀Aal i ∈ Aal )Int.Th (Ai, miembro(Ai, Aal ), Tn, Tn+1, Co), Tn) 3. Aal tiene un beneficio marginal que al ser miembros les otorga un alto valor de utilidad: MB(Aal , (∀Aal i ∈ Aal )Al val(Aal i , Cal , Aal , w) ≥ Al TrH, Tn) Los perfiles de los miembros son una parte crucial de las alianzas exitosas. Suponemos que los agentes que tienen perfiles más precisos de sus adversarios serán más exitosos en tales entornos. Tales agentes podrán predecir cuándo un miembro está a punto de violar el contrato de alianzas (elemento 2 en el modelo anterior) y tomar medidas correctivas (cuando el elemento 3 se falsificará). La robustez de la alianza depende en parte de la medida de confianza de sus miembros, la estimación de su posición objetiva y otras propiedades de su perfil. Debemos tener en cuenta que un agente puede formar parte simultáneamente de más de una alianza. Una alianza temporal, donde los miembros del grupo no tienen un objetivo común pero actúan colaborativamente en interés de sus propias metas individuales, es clasificada como un Grupo de Tratamiento por los psicólogos modernos (en contraste con un Grupo de Tarea, donde sus miembros tienen un objetivo común). El modelo de Actividad Compartida presentado en [5] modeló el comportamiento del Grupo de Tratamiento utilizando la misma formalización de Planes Compartidos. Al comparar ambas definiciones de una alianza y un Grupo de Tratamiento, encontramos una similitud sorprendente entre ambos modelos: las definiciones de los modelos de entorno son casi idénticas (ver las definiciones de SAs en [5]), y sus axiomas de Acto Egoísta y Acto Cooperativo se ajustan al comportamiento de nuestros agentes adversarios. La principal distinción entre ambos modelos es la integración de un axioma de acto de comportamiento útil, en la Actividad Compartida que no puede formar parte de la nuestra. Este axioma establece que un agente considerará tomar una acción que disminuirá su valor de Evaluación (hasta un cierto límite inferior), si cree que un compañero del grupo obtendrá un beneficio significativo. Dicho comportamiento no puede ocurrir en un entorno puramente adversarial (como un juego de suma cero), donde los miembros de la alianza están constantemente alerta para manipular su alianza en su propio beneficio. A6. Axioma de Maximización de la Evaluación. En un caso en el que todos los demás axiomas no sean aplicables, procederemos con la acción que maximice el valor heurístico calculado en la función Eval. (∀Aag, Ao ∈ A, α ∈ Cag, Tn, w ∈ W) Bel(Aag, (∀γ ∈ Cag)Eval(Aag, α, w) ≥ Eval(Aag, γ, w), Tn) ⇒ Pot.Int.To(Aag, α, Tn, Tα, w) T1. Optimalidad en Eval = Utilidad El modelo axiomático anterior aborda situaciones en las que la Utilidad es desconocida y los agentes son agentes racionales limitados. El siguiente teorema muestra que en interacciones bilaterales, donde los agentes tienen la función de Utilidad real (es decir, Eval = Utilidad) y son agentes racionales, los axiomas proporcionan el mismo resultado óptimo que la búsqueda adversarial clásica (por ejemplo, Min-Max). Teorema 1. Que Ae ag sea un agente AE racional ilimitado que utiliza la función de evaluación heurística Eval, Au ag sea el mismo agente utilizando la verdadera función de utilidad y Ao sea un único adversario racional ilimitado basado en utilidad. Dado que Eval = Utilidad: (∀α ∈ CAu ag , α ∈ CAe ag , Tn, w ∈ W) Pot.Int.To(Au ag, α, Tn, Tα, w) → Pot.Int.To(Ae ag, α , Tn, Tα, w) ∧ ((α = α ) ∨ (Utilidad(Au ag, α, w) = Eval(Ae ag, α , w))) Esquema de prueba - Dado que Au ag tiene la función de utilidad real y recursos ilimitados, puede generar el árbol de juego completo y ejecutar el algoritmo MinMax óptimo para elegir la acción de mayor valor de utilidad, que denotamos como α. La prueba mostrará que Ae ag, utilizando los axiomas AE, seleccionará la misma utilidad α o igual (cuando hay más de una acción con la misma utilidad máxima) cuando Eval = Utilidad. (A1) Axioma de logro de objetivo - supongamos que hay un α tal que su cumplimiento logrará el objetivo Au ags. Obtendrá la utilidad más alta mediante Min-Max para Au ag. El agente Ae seleccionará α u otra acción con el mismo valor de utilidad a través de A1. Si tal α no existe, Ae ag no puede aplicar este axioma y procede a A2. (A2) Axioma de la acción preventiva - (1) Observando el caso base (ver Prop1), si hay un β que lleva a Ao a alcanzar su objetivo, entonces una acción preventiva α producirá la mayor utilidad para Au ag. Au ag lo elegirá a través de la utilidad, mientras que Ae ag lo elegirá a través de A2. En el caso general, β es una acción altamente beneficiosa para Ao, lo que resulta en una baja utilidad para Au ag, lo que lo guiará a seleccionar un α que evitará β, mientras que Ae ag lo elegirá a través de A2.1 Si dicho β no existe para Ao, entonces A2 no es aplicable, y Ae ag puede proceder a A3. (A3) Axioma de movimiento táctico subóptimo: al usar una función de evaluación heurística, Ae ag tiene una creencia parcial en el perfil de su adversario (elemento 4 en el modelo AE), lo que puede llevarlo a creer en SetActions (Prop1). En nuestro caso, Ae ag tiene un perfil completo de su adversario óptimo y sabe que Ao se comportará de manera óptima según los valores reales de utilidad en el árbol de búsqueda completo, por lo tanto, cualquier creencia sobre una SetAction subóptima no puede existir, lo que hace que este axioma sea inaplicable. Ae ag procederá a A4. (A4) Axioma de detección de perfil - Dado que Ae ag tiene el perfil completo de Ao, ninguna de las acciones de Ae ag puede aumentar su conocimiento. Ese axioma no se aplicará, y el agente procederá con A6 (A5 será ignorado porque la interacción es bilateral). (A6) Axioma de maximización de la evaluación: este axioma seleccionará la máxima Evaluación para Ae ag. Dado que Eval = Utilidad, se seleccionará el mismo α que fue seleccionado por Au ag. 3. EVALUACIÓN El propósito principal de nuestro análisis experimental es evaluar el comportamiento y rendimiento de los modelos en un entorno adversarial real. Esta sección investiga si un caso acotado 1 en el que, tras la finalización de β, exista un γ que proporcione una alta utilidad para el Agente Au ag, no puede ocurrir porque Ao utiliza la misma utilidad, y la existencia de γ hará que clasifique a β como una acción de baja utilidad. 554 The Sixth Intl. En la Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07), los agentes racionales situados en entornos adversariales se beneficiarán más al aplicar nuestros axiomas de comportamiento sugeridos. 3.1 El Dominio Para explorar el uso del modelo anterior y sus axiomas de comportamiento, decidimos utilizar el <br>juego de Conecta Cuatro</br> como nuestro entorno adversarial. Connect-Four es un juego de 2 jugadores, de suma cero, que se juega utilizando un tablero similar a una matriz de 6x7. En cada turno, un jugador coloca un disco en una de las 7 columnas (el conjunto de 21 discos suele ser de color amarillo para el jugador 1 y rojo para el jugador 2; usaremos Blanco y Negro respectivamente para evitar confusiones). El ganador es el primer jugador en completar un conjunto de cuatro discos de su color en horizontal, vertical o diagonal. En ocasiones muy raras, el juego podría terminar en empate si todos los cuadros vacíos están llenos, pero ningún jugador logró crear un conjunto de 4 discos. El <br>juego de Conecta Cuatro</br> fue resuelto en [1], donde se muestra que el primer jugador (jugando con los discos blancos) puede forzar una victoria comenzando en la columna del medio (columna 4) y jugando de manera óptima. Sin embargo, la estrategia óptima es muy compleja y difícil de seguir incluso para agentes racionales limitados complejos, como los jugadores humanos. Antes de que podamos proceder a verificar el comportamiento del agente, primero debemos verificar que el dominio se ajusta a la definición de entornos adversarios mencionada anteriormente (en la que se basan los axiomas de comportamiento). Primero, al jugar al <br>juego de Conecta Cuatro</br>, el agente tiene la intención de ganar el juego (ítem 1). Segundo (ítem 2), nuestro agente cree que en Conecta Cuatro solo puede haber un ganador (o ningún ganador en la rara ocasión de un empate). Además, nuestro agente cree que su oponente en el juego intentará ganar (ítem 3), y esperamos que tenga algún conocimiento parcial (ítem 4) sobre su adversario (este conocimiento puede variar desde nada, pasando por hechos simples como la edad, hasta estrategias y debilidades). Por supuesto, no todos los encuentros de Conecta Cuatro son adversariales. Por ejemplo, cuando un padre está jugando el juego con su hijo, la siguiente situación podría ocurrir: el niño, con un fuerte incentivo para ganar, trata el entorno como adversarial (intenta ganar, entiende que solo puede haber un ganador y cree que su padre está tratando de vencerlo). Sin embargo, desde el punto de vista de los padres, el entorno podría ser visto como educativo, donde el objetivo no es ganar el juego, sino causar disfrute o practicar el razonamiento estratégico. En un entorno educativo como este, un nuevo conjunto de axiomas de comportamiento podría ser más beneficioso para los objetivos de los padres que nuestros axiomas de comportamiento adversarial sugeridos. 3.2 Análisis de Axiomas Después de demostrar que el <br>juego de Conecta Cuatro</br> es de hecho un entorno adversarial bilateral de suma cero, el siguiente paso es observar el comportamiento de los jugadores durante el juego y verificar si comportarse de acuerdo con nuestro modelo mejora el rendimiento. Para hacerlo, hemos recopilado archivos de registro de juegos de Conecta Cuatro completados que fueron jugados por jugadores humanos a través de Internet. Nuestros datos de archivo de registro recopilados provienen de sitios de Juego por Correo Electrónico (PBeM). Estos son sitios web que alojan juegos por correo electrónico, donde cada movimiento se realiza a través de un intercambio de correos electrónicos entre el servidor y los jugadores. Muchos de estos sitios de archivos contienen interacciones competitivas reales y también mantienen un sistema de clasificación para sus miembros. La mayoría de los datos que utilizamos se pueden encontrar en [6]. Como se puede aprender de [1], Conecta Cuatro tiene una estrategia óptima y una ventaja considerable para el jugador que comienza el juego (a quien llamamos jugador Blanco). Nos concentraremos en nuestro análisis en los movimientos de los segundos jugadores (a los que llamaremos Negras). El jugador Blanco, al ser el primero en actuar, tiene la llamada ventaja de la iniciativa. Tener la ventaja y una buena estrategia mantendrá al jugador Negro ocupado reaccionando a sus movimientos, en lugar de iniciar amenazas. Una amenaza es una combinación de tres discos del mismo color, con un espacio vacío para el cuarto disco ganador. Una amenaza abierta es una amenaza que puede materializarse en el próximo movimiento del oponente. Para que el jugador Negro gane, debe de alguna manera cambiar el rumbo, tomar la ventaja y comenzar a presentar amenazas al jugador Blanco. Exploraremos el comportamiento de los jugadores negros y su conformidad con nuestros axiomas. Para hacerlo, construimos una aplicación que lee archivos de registro y analiza los movimientos de los jugadores Negros. La aplicación contiene dos componentes principales: (1) un algoritmo Min-Max para la evaluación de movimientos; (2) un detector de amenazas abiertas para descubrir amenazas abiertas. El algoritmo Min-Max funcionará hasta una profundidad d dada y, para cada movimiento α, devolverá el valor heurístico para la próxima acción tomada por el jugador según lo escrito en el archivo de registro, h(α), junto con el valor heurístico máximo, maxh(α), que podría lograrse antes de realizar el movimiento (obviamente, si h(α) = maxh(α), entonces el jugador no realizó el movimiento óptimo desde el punto de vista heurístico). El trabajo de los detectores de amenazas es notificar si se tomó alguna acción para bloquear una amenaza abierta (no bloquear una amenaza abierta probablemente hará que el jugador pierda en el próximo movimiento del oponente). La función heurística utilizada por Min-Max para evaluar la utilidad de los jugadores es la siguiente función, que es simple de calcular, pero proporciona un desafío razonable a los oponentes humanos: Definición 8. Sea Grupo un conjunto adyacente de cuatro cuadrados que son horizontales, verticales o diagonales. El Grupo n (Grupo w) es un Grupo con n piezas de color negro (blanco) y 4−n casillas vacías. h = ((Grupo1 negro ∗α)+(Grupo2 negro ∗β)+(Grupo3 negro ∗γ)+(Grupo4 negro ∗∞)) − ((Grupo1 blanco ∗α)+(Grupo2 blanco ∗β)+(Grupo3 blanco ∗γ)+(Grupo4 blanco ∗∞)) Los valores de α, β y δ pueden variar para formar cualquier combinación lineal deseada; sin embargo, es importante asignarles valores teniendo en cuenta el orden α < β < δ (usamos 1, 4 y 8 como sus valores respectivos). Grupos de 4 discos del mismo color significan victoria, por lo tanto, el descubrimiento de dicho grupo resultará en ∞ para asegurar un valor extremo. Ahora utilizamos nuestra función de evaluación estimada para evaluar las acciones de los jugadores Negros durante la interacción adversarial de Conecta Cuatro. Cada juego del archivo de registro fue introducido en la aplicación, la cual procesó y generó un archivo de registro reformateado que contenía el valor h del movimiento actual, el valor maxh que podría ser alcanzado y una notificación si se detectaba una amenaza abierta. Un total de 123 partidas fueron analizadas (57 con victoria de las blancas y 66 con victoria de las negras). Algunos juegos adicionales fueron ignorados manualmente en el experimento, debido a estos problemas: un jugador abandonando el juego mientras el resultado no es definitivo, o una jugada irracional y contundente en las primeras etapas del juego (por ejemplo, no bloquear un grupo obviamente ganador en las primeras jugadas de apertura). Además, también se eliminó un empate. El simulador se ejecutó con una profundidad de búsqueda de 3 movimientos. Ahora procedemos a analizar los juegos con respecto a cada axioma de comportamiento. El Sexto Internacional. La siguiente sección presenta las evaluaciones heurísticas del algoritmo Min-Max para cada acción, y verifica la cantidad y el alcance de las acciones tácticas subóptimas y sus implicaciones en el rendimiento. La Tabla 1 muestra los resultados y las percepciones del análisis heurístico de los juegos, cuando la profundidad de búsqueda es igual a 3 (esta profundidad de búsqueda fue seleccionada para que los resultados sean comparables a [9], ver Sección 3.2.3). La información heurística de las tablas es la diferencia entre el valor heurístico máximo presente y el valor heurístico de la acción que finalmente tomó el jugador (es decir, cuanto más cercano sea el número a 0, más cercana estuvo la acción a la acción heurística máxima). La primera fila presenta los valores de diferencia de la acción que tuvo el valor de diferencia máximo entre todas las acciones de los jugadores Negros en un juego dado, promediado en todos los juegos ganados y perdidos por los Negros (ver columnas respectivas). En los juegos en los que el jugador Negro pierde, su valor promedio de diferencia fue de -17.62, mientras que en los juegos en los que el jugador Negro ganó, su promedio fue de -12.02. La segunda fila amplía el análisis al considerar las 3 acciones con mayor diferencia heurística, y promediarlas. En ese caso, notamos una diferencia heurística promedio de 5 puntos entre los juegos en los que el jugador Negro pierde y los juegos en los que gana. Sin embargo, la importancia de esos números es que nos permitieron hacer una suposición educada sobre un número umbral de 11.5, como el valor de la constante TrH, que diferencia entre acciones normales y altamente beneficiosas. Después de encontrar una constante TrH aproximada, podemos proceder con un análisis de la importancia de los movimientos subóptimos. Para hacerlo, tomamos el subconjunto de juegos en los que el valor mínimo de diferencia heurística para las acciones de las negras era de 11.5. Como se presenta en la Tabla 2, podemos ver el promedio de 3 horas mínimo de los 3 mayores rangos y el porcentaje respectivo de juegos ganados. La primera fila muestra que el jugador Negro ganó solo el 12% de los juegos en los que el promedio de sus 3 acciones con mayor diferencia heurística (min3 h) era menor que el umbral sugerido, TrH = 11.5. La segunda fila muestra un resultado sorprendente: parece que cuando min3 h > −4, el jugador Negro rara vez gana. La intuición sugeriría que los juegos en los que los valores de evaluación de la acción estén más cerca de los valores máximos resultarán en más juegos ganados para las piezas negras. Sin embargo, parece que en el dominio de Conecta Cuatro, simplemente responder con acciones algo fácilmente esperadas, sin iniciar algunos movimientos sorprendentes y subóptimos, no produce buenos resultados. La última fila resume las principales conclusiones del análisis; la mayoría de las victorias de Blacks (83%) se produjeron cuando su min3 h estaba en el rango de -11.5 a -4. Una inspección detallada de esos juegos ganados por las fichas negras muestra el siguiente patrón detrás de los números: después de los movimientos de apertura estándar, las fichas negras de repente colocan una ficha en una columna aislada, lo cual parece ser un desperdicio de un movimiento. El blanco sigue construyendo sus amenazas, generalmente ignorando el último movimiento de las negras, que a su vez utiliza el disco aislado como ancla para una futura amenaza ganadora. Los resultados muestran que fue beneficioso para el jugador Negro en la Tabla 2: Porcentajes de ganancias de los Negros % de juegos min3 h < −11.5 12% min3 h > −4 5% −11.5 ≤ min3 h ≤ −4 83% tomar acciones subóptimas y no dar el valor heurístico más alto posible en ese momento, pero no será demasiado perjudicial para su posición (es decir, no dará un alto valor beneficioso a su adversario). Resultó que aprender el umbral es un aspecto importante del éxito: realizar movimientos extremadamente arriesgados (min3 h < −11.5) o intentar evitarlos (min3 h > −4) reduce significativamente las posibilidades de ganar de los jugadores negros. 3.2.2 Afirmación del Axioma de Monitoreo de Perfiles En la tarea de demostrar la importancia de monitorear los perfiles de los adversarios, nuestros archivos de registro no pudieron ser utilizados porque no contenían interacciones repetidas entre jugadores, que son necesarias para inferir el conocimiento de los jugadores sobre sus adversarios. Sin embargo, la importancia del modelado del oponente y su uso para obtener ventajas tácticas ya ha sido estudiada en varios ámbitos ([3, 9] son buenos ejemplos). En un artículo reciente, Markovitch y Reger [9] exploraron la noción de aprendizaje y explotación de la debilidad del oponente en interacciones competitivas. Aplican estrategias de aprendizaje simples analizando ejemplos de interacciones pasadas en un dominio específico. También utilizaron el dominio adversarial de Conecta Cuatro, que ahora se puede utilizar para comprender la importancia de monitorear el perfil del adversario. Tras la presentación de su modelo teórico, describen un extenso estudio empírico y verifican el rendimiento de los agentes después de aprender el modelo de debilidades con ejemplos pasados. Uno de los dominios utilizados como entorno competitivo fue el mismo <br>juego de Conecta Cuatro</br> (las Damas fueron el segundo dominio). ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "empirical study": {
            "translated_key": "estudio empírico",
            "is_in_text": true,
            "original_annotated_sentences": [
                "An Adversarial Environment Model for Bounded Rational Agents in Zero-Sum Interactions Inon Zuckerman1 , Sarit Kraus1 , Jeffrey S. Rosenschein2 , Gal Kaminka1 1 Department of Computer Science 2 The School of Engineering Bar-Ilan University and Computer Science Ramat-Gan, Israel Hebrew University, Jerusalem, Israel {zukermi,sarit,galk}@cs.biu.ac.il jeff@cs.huji.ac.il ABSTRACT Multiagent environments are often not cooperative nor collaborative; in many cases, agents have conflicting interests, leading to adversarial interactions.",
                "This paper presents a formal Adversarial Environment model for bounded rational agents operating in a zero-sum environment.",
                "In such environments, attempts to use classical utility-based search methods can raise a variety of difficulties (e.g., implicitly modeling the opponent as an omniscient utility maximizer, rather than leveraging a more nuanced, explicit opponent model).",
                "We define an Adversarial Environment by describing the mental states of an agent in such an environment.",
                "We then present behavioral axioms that are intended to serve as design principles for building such adversarial agents.",
                "We explore the application of our approach by analyzing log files of completed Connect-Four games, and present an empirical analysis of the axioms appropriateness.",
                "Categories and Subject Descriptors I.2.11 [Artificial Intelligence]: Distributed Artificial Intelligence-Intelligent agents,Multiagent Systems; I.2.4 [Artificial Intelligence]: Knowledge Representation Formalisms and Methods -Modal logic General Terms Design, Theory 1.",
                "INTRODUCTION Early research in multiagent systems (MAS) considered cooperative groups of agents; because individual agents had limited resources, or limited access to information (e.g., limited processing power, limited sensor coverage), they worked together by design to solve problems that individually they could not solve, or at least could not solve as efficiently.",
                "MAS research, however, soon began to consider interacting agents with individuated interests, as representatives of different humans or organizations with non-identical interests.",
                "When interactions are guided by diverse interests, participants may have to overcome disagreements, uncooperative interactions, and even intentional attempts to damage one another.",
                "When these types of interactions occur, environments require appropriate behavior from the agents situated in them.",
                "We call these environments Adversarial Environments, and call the clashing agents Adversaries.",
                "Models of cooperation and teamwork have been extensively explored in MAS through the axiomatization of mental states (e.g., [8, 4, 5]).",
                "However, none of this research dealt with adversarial domains and their implications for agent behavior.",
                "Our paper addresses this issue by providing a formal, axiomatized mental state model for a subset of adversarial domains, namely simple zero-sum adversarial environments.",
                "Simple zero-sum encounters exist of course in various twoplayer games (e.g., Chess, Checkers), but they also exist in n-player games (e.g., Risk, Diplomacy), auctions for a single good, and elsewhere.",
                "In these latter environments especially, using a utility-based adversarial search (such as the Min-Max algorithm) does not always provide an adequate solution; the payoff function might be quite complex or difficult to quantify, and there are natural computational limitations on bounded rational agents.",
                "In addition, traditional search methods (like Min-Max) do not make use of a model of the opponent, which has proven to be a valuable addition to adversarial planning [9, 3, 11].",
                "In this paper, we develop a formal, axiomatized model for bounded rational agents that are situated in a zero-sum adversarial environment.",
                "The model uses different modality operators, and its main foundations are the SharedPlans [4] model for collaborative behavior.",
                "We explore environment properties and the mental states of agents to derive behavioral axioms; these behavioral axioms constitute a formal model that serves as a specification and design guideline for agent design in such settings.",
                "We then investigate the behavior of our model empirically using the Connect-Four board game.",
                "We show that this game conforms to our environment definition, and analyze players behavior using a large set of completed match log 550 978-81-904262-7-5 (RPS) c 2007 IFAAMAS files.",
                "In addition, we use the results presented in [9] to discuss the importance of opponent modeling in our ConnectFour adversarial domain.",
                "The paper proceeds as follows.",
                "Section 2 presents the models formalization.",
                "Section 3 presents the empirical analysis and its results.",
                "We discuss related work in Section 4, and conclude and present future directions in Section 5. 2.",
                "ADVERSARIAL ENVIRONMENTS The adversarial environment model (denoted as AE) is intended to guide the design of agents by providing a specification of the capabilities and mental attitudes of an agent in an adversarial environment.",
                "We focus here on specific types of adversarial environments, specified as follows: 1.",
                "Zero-Sum Interactions: positive and negative utilities of all agents sum to zero; 2.",
                "Simple AEs: all agents in the environment are adversarial agents; 3.",
                "Bilateral AEs: AEs with exactly two agents; 4.",
                "Multilateral AEs: AEs of three or more agents.",
                "We will work on both bilateral and multilateral instantiations of zero-sum and simple environments.",
                "In particular, our adversarial environment model will deal with interactions that consist of N agents (N ≥ 2), where all agents are adversaries, and only one agent can succeed.",
                "Examples of such environments range from board games (e.g., Chess, Connect-Four, and Diplomacy) to certain economic environments (e.g., N-bidder auctions over a single good). 2.1 Model Overview Our approach is to formalize the mental attitudes and behaviors of a single adversarial agent; we consider how a single agent perceives the AE.",
                "The following list specifies the conditions and mental states of an agent in a simple, zero-sum AE: 1.",
                "The agent has an individual intention that its own goal will be completed; 2.",
                "The agent has an individual belief that it and its adversaries are pursuing full conflicting goals (defined below)there can be only one winner; 3.",
                "The agent has an individual belief that each adversary has an intention to complete its own full conflicting goal; 4.",
                "The agent has an individual belief in the (partial) profile of its adversaries.",
                "Item 3 is required, since it might be the case that some agent has a full conflicting goal, and is currently considering adopting the intention to complete it, but is, as of yet, not committed to achieving it.",
                "This might occur because the agent has not yet deliberated about the effects that adopting that intention might have on the other intentions it is currently holding.",
                "In such cases, it might not consider itself to even be in an adversarial environment.",
                "Item 4 states that the agent should hold some belief about the profiles of its adversaries.",
                "The profile represents all the knowledge the agent has about its adversary: its weaknesses, strategic capabilities, goals, intentions, trustworthiness, and more.",
                "It can be given explicitly or can be learned from observations of past encounters. 2.2 Model Definitions for Mental States We use Grosz and Krauss definitions of the modal operators, predicates, and meta-predicates, as defined in their SharedPlan formalization [4].",
                "We recall here some of the predicates and operators that are used in that formalization: Int.To(Ai, α, Tn, Tα, C) represents Ais intentions at time Tn to do an action α at time Tα in the context of C. Int.Th(Ai, prop, Tn, Tprop, C) represents Ais intentions at time Tn that a certain proposition prop holds at time Tprop in the context of C. The potential intention operators, Pot.Int.To(...) and Pot.Int.Th(...), are used to represent the mental state when an agent considers adopting an intention, but has not deliberated about the interaction of the other intentions it holds.",
                "The operator Bel(Ai, f, Tf ) represents agent Ai believing in the statement expressed in formula f, at time Tf .",
                "MB(A, f, Tf ) represents mutual belief for a group of agents A.",
                "A snapshot of the system finds our environment to be in some state e ∈ E of environmental variable states, and each adversary in any LAi ∈ L of possible local states.",
                "At any given time step, the system will be in some world w of the set of all possible worlds w ∈ W, where w = E×LA1 ×LA2 × ...LAn , and n is the number of adversaries.",
                "For example, in a Texas Holdem poker game, an agents local state might be its own set of cards (which is unknown to its adversary) while the environment will consist of the betting pot and the community cards (which are visible to both players).",
                "A utility function under this formalization is defined as a mapping from a possible world w ∈ W to an element in , which expresses the desirability of the world, from a single agent perspective.",
                "We usually normalize the range to [0,1], where 0 represents the least desirable possible world, and 1 is the most desirable world.",
                "The implementation of the utility function is dependent on the domain in question.",
                "The following list specifies new predicates, functions, variables, and constants used in conjunction with the original definitions for the adversarial environment formalization: 1. φ is a null action (the agent does not do anything). 2.",
                "GAi is the set of agent Ais goals.",
                "Each goal is a set of predicates whose satisfaction makes the goal complete (we use G∗ Ai ∈ GAi to represent an arbitrary goal of agent Ai). 3. gAi is the set of agent Ais subgoals.",
                "Subgoals are predicates whose satisfaction represents an important milestone toward achievement of the full goal. gG∗ Ai ⊆ gAi is the set of subgoals that are important to the completion of goal G∗ Ai (we will use g∗ G∗ Ai ∈ gG∗ Ai to represent an arbitrary subgoal). 4.",
                "P Aj Ai is the profile object agent Ai holds about agent Aj. 5.",
                "CA is a general set of actions for all agents in A which are derived from the environments constraints.",
                "CAi ⊆ CA is the set of agent Ais possible actions. 6.",
                "Do(Ai, α, Tα, w) holds when Ai performs action α over time interval Tα in world w. 7.",
                "Achieve(G∗ Ai , α, w) is true when goal G∗ Ai is achieved following the completion of action α in world w ∈ W, where α ∈ CAi . 8.",
                "Profile(Ai, PAi Ai ) is true when agent Ai holds an object profile for agent Aj.",
                "Definition 1.",
                "Full conflict (FulConf ) describes a zerosum interaction where only a single goal of the goals in conflict can be completed.",
                "FulConf(G∗ Ai , G∗ Aj ) ⇒ (∃α ∈ CAi , ∀w, β ∈ CAj ) (Achieve(G∗ Ai , α, w) ⇒ ¬Achieve(G∗ Aj , β, w)) ∨ (∃β ∈ CAj , ∀w, α ∈ CAi )(Achieve(G∗ Aj , β, w) ⇒ ¬Achieve(G∗ Ai , α, w)) Definition 2.",
                "Adversarial Knowledge (AdvKnow) is a function returning a value which represents the amount of The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 551 knowledge agent Ai has on the profile of agent Aj, at time Tn.",
                "The higher the value, the more knowledge agent Ai has.",
                "AdvKnow : P Aj Ai × Tn → Definition 3.",
                "Eval - This evaluation function returns an estimated expected utility value for an agent in A, after completing an action from CA in some world state w. Eval : A × CA × w → Definition 4.",
                "TrH - (Threshold) is a numerical constant in the [0,1] range that represents an evaluation function (Eval) threshold value.",
                "An action that yields an estimated utility evaluation above the TrH is regarded as a highly beneficial action.",
                "The Eval value is an estimation and not the real utility function, which is usually unknown.",
                "Using the real utility value for a rational agent would easily yield the best outcome for that agent.",
                "However, agents usually do not have the real utility functions, but rather a heuristic estimate of it.",
                "There are two important properties that should hold for the evaluation function: Property 1.",
                "The evaluation function should state that the most desirable world state is one in which the goal is achieved.",
                "Therefore, after the goal has been satisfied, there can be no future action that can put the agent in a world state with higher Eval value. (∀Ai, G∗ Ai , α, β ∈ CAi , w ∈ W) Achieve(G∗ Ai , α, w) ⇒ Eval(Ai, α, w) ≥ Eval(Ai, β, w) Property 2.",
                "The evaluation function should project an action that causes a completion of a goal or a subgoal to a value which is greater than TrH (a highly beneficial action). (∀Ai, G∗ Ai ∈ GAi , α ∈ CAi , w ∈ W, g∗ GAi ∈ gGAi ) Achieve(G∗ Ai , α, w) ∨ Achieve(g∗ GAi , α, w) ⇒ Eval(Ai, α, w) ≥ TrH.",
                "Definition 5.",
                "SetAction We define a set action (SetAction) as a set of action operations (either complex or basic actions) from some action sets CAi and CAj which, according to agent Ais belief, are attached together by a temporal and consequential relationship, forming a chain of events (action, and its following consequent action). (∀α1 , . . . , αu ∈ CAi , β1 , . . . , βv ∈ CAj , w ∈ W) SetAction(α1 , . . . , αu , β1 , . . . , βv , w) ⇒ ((Do(Ai, α1 , Tα1 , w) ⇒ Do(Aj, β1 , Tβ1 , w)) ⇒ Do(Ai, α2 , Tα2 , w) ⇒ . . . ⇒ Do(Ai, αu , Tαu , w)) The consequential relation might exist due to various environmental constraints (when one action forces the adversary to respond with a specific action) or due to the agents knowledge about the profile of its adversary.",
                "Property 3.",
                "As the knowledge we have about our adversary increases we will have additional beliefs about its behavior in different situations which in turn creates new set actions.",
                "Formally, if our AdvKnow at time Tn+1 is greater than AdvKnow at time Tn, then every SetAction known at time Tn is also known at time Tn+1.",
                "AdvKnow(P Aj Ai , Tn+1) > AdvKnow(P Aj Ai , Tn) ⇒ (∀α1 , . . . , αu ∈ CAi , β1 , . . . , βv ∈ CAj ) Bel(Aag, SetAction(α1 , . . . , αu , β1 , . . . , βv ), Tn) ⇒ Bel(Aag, SetAction(α1 , . . . , αu , β1 , . . . , βv ), Tn+1) 2.3 The Environment Formulation The following axioms provide the formal definition for a simple, zero-sum Adversarial Environment (AE).",
                "Satisfaction of these axioms means that the agent is situated in such an environment.",
                "It provides specifications for agent Aag to interact with its set of adversaries A with respect to goals G∗ Aag and G∗ A at time TCo at some world state w. AE(Aag, A, G∗ Aag , A1, . . . , Ak, G∗ A1 , . . . , G∗ Ak , Tn, w) 1.",
                "Aag has an Int.Th his goal would be completed: (∃α ∈ CAag , Tα) Int.Th(Aag, Achieve(G∗ Aag , α), Tn, Tα, AE) 2.",
                "Aag believes that it and each of its adversaries Ao are pursuing full conflicting goals: (∀Ao ∈ {A1, . . . , Ak}) Bel(Aag, FulConf(G∗ Aag , G∗ Ao ), Tn) 3.",
                "Aag believes that each of his adversaries in Ao has the Int.Th his conflicting goal G∗ Aoi will be completed: (∀Ao ∈ {A1, . . . , Ak})(∃β ∈ CAo , Tβ) Bel(Aag, Int.Th(Ao, Achieve(G∗ Ao , β), TCo, Tβ, AE), Tn) 4.",
                "Aag has beliefs about the (partial) profiles of its adversaries (∀Ao ∈ {A1, . . . , Ak}) (∃PAo Aag ∈ PAag )Bel(Aag, Profile(Ao, PAo Aag ), Tn) To build an agent that will be able to operate successfully within such an AE, we must specify behavioral guidelines for its interactions.",
                "Using a naive Eval maximization strategy to a certain search depth will not always yield satisfactory results for several reasons: (1) the search horizon problem when searching for a fixed depth; (2) the strong assumption of an optimally rational, unbounded resources adversary; (3) using an estimated evaluation function which will not give optimal results in all world states, and can be exploited [9].",
                "The following axioms specify the behavioral principles that can be used to differentiate between successful and less successful agents in the above Adversarial Environment.",
                "Those axioms should be used as specification principles when designing and implementing agents that should be able to perform well in such Adversarial Environments.",
                "The behavioral axioms represent situations in which the agent will adopt potential intentions to (Pot.Int.To(...)) perform an action, which will typically require some means-end reasoning to select a possible course of action.",
                "This reasoning will lead to the adoption of an Int.To(...) (see [4]).",
                "A1.",
                "Goal Achieving Axiom.",
                "The first axiom is the simplest case; when the agent Aag believes that it is one action (α) away from achieving his conflicting goal G∗ Aag , it should adopt the potential intention to do α and complete its goal. (∀Aag, α ∈ CAag , Tn, Tα, w ∈ W) (Bel(Aag, Do(Aag, α, Tα, w) ⇒ Achieve(G∗ Aag , α, w)) ⇒ Pot.Int.To(Aag, α, Tn, Tα, w) This somewhat trivial behavior is the first and strongest axiom.",
                "In any situation, when the agent is an action away from completing the goal, it should complete the action.",
                "Any fair Eval function would naturally classify α as the maximal value action (property 1).",
                "However, without explicit axiomatization of such behavior there might be situations where the agent will decide on taking another action for various reasons, due to its bounded decision resources.",
                "A2.",
                "Preventive Act Axiom.",
                "Being in an adversarial situation, agent Aag might decide to take actions that will damage one of its adversarys plans to complete its goal, even if those actions do not explicitly advance Aag towards its conflicting goal G∗ Aag .",
                "Such preventive action will take place when agent Aag has a belief about the possibility of its adversary Ao doing an action β that will give it a high 552 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) utility evaluation value (> TrH).",
                "Believing that taking action α will prevent the opponent from doing its β, it will adopt a potential intention to do α. (∀Aag, Ao ∈ A, α ∈ CAag , β ∈ CAo , Tn, Tβ, w ∈ W) (Bel(Aag, Do(Ao, β, Tβ, w) ∧ Eval(Ao, β, w) > TrH, Tn) ∧ Bel(Aag, Do(Aag, α, Tα, w) ⇒ ¬Do(Ao, β, Tβ, w), Tn) ⇒ Pot.Int.To(Aag, α, Tn, Tα, w) This axiom is a basic component of any adversarial environment.",
                "For example, looking at a Chess board game, a player could realize that it is about to be checkmated by its opponent, thus making a preventive move.",
                "Another example is a Connect Four game: when a player has a row of three chips, its opponent must block it, or lose.",
                "A specific instance of A1 occurs when the adversary is one action away from achieving its goal, and immediate preventive action needs to be taken by the agent.",
                "Formally, we have the same beliefs as stated above, with a changed belief that doing action β will cause agent Ao to achieve its goal.",
                "Proposition 1: Prevent or lose case. (∀Aag, Ao ∈ A, α ∈ CAag , β ∈ CAo , G∗ Ao , Tn, Tα, Tβ, w ∈ W) Bel(Aag, Do(Ao, β, Tβ, w) ⇒ Achieve(G∗ Ao , β, w), Tn) ∧ Bel(Aag, Do(Aag, α, Tα, w) ⇒ ¬Do(Ao, β, Tβ, w)) ⇒ Pot.Int.To(Aag, α, Tn, Tα, w) Sketch of proof: Proposition 1 can be easily derived from axiom A1 and the property 2 of the Eval function, which states that any action that causes a completion of a goal is a highly beneficial action.",
                "The preventive act behavior will occur implicitly when the Eval function is equal to the real world utility function.",
                "However, being bounded rational agents and dealing with an estimated evaluation function we need to explicitly axiomatize such behavior, for it will not always occur implicitly from the evaluation function.",
                "A3.",
                "Suboptimal Tactical Move Axiom.",
                "In many scenarios a situation may occur where an agent will decide not to take the current most beneficial action it can take (the action with the maximal utility evaluation value), because it believes that taking another action (with lower utility evaluation value) might yield (depending on the adversarys response) a future possibility for a highly beneficial action.",
                "This will occur most often when the Eval function is inaccurate and differs by a large extent from the Utility function.",
                "Put formally, agent Aag believes in a certain SetAction that will evolve according to its initial action and will yield a high beneficial value (> TrH) solely for it. (∀Aag, Ao ∈ A, Tn, w ∈ W) (∃α1 , . . . , αu ∈ CAi , β1 , . . . , βv ∈ CAj , Tα1 ) Bel(Aag, SetAction(α1 , . . . , αu , β1 , . . . , βv ), Tn) ∧ Bel(Aag, Eval(Ao, βv , w) < TrH < Eval(Aag, αu , w), Tn) ⇒ Pot.Int.To(Aag, α1 , Tn, Tα1 , w) An agent might believe that a chain of events will occur for various reasons due to the inevitable nature of the domain.",
                "For example, in Chess, we often observe the following: a move causes a check position, which in turn limits the opponents moves to avoiding the check, to which the first player might react with another check, and so on.",
                "The agent might also believe in a chain of events based on its knowledge of its adversarys profile, which allows it to foresee the adversarys movements with high accuracy.",
                "A4.",
                "Profile Detection Axiom.",
                "The agent can adjust its adversarys profiles by observations and pattern study (specifically, if there are repeated encounters with the same adversary).",
                "However, instead of waiting for profile information to be revealed, an agent can also initiate actions that will force its adversary to react in a way that will reveal profile knowledge about it.",
                "Formally, the axiom states that if all actions (γ) are not highly beneficial actions (< TrH), the agent can do action α in time Tα if it believes that it will result in a non-highly beneficial action β from its adversary, which in turn teaches it about the adversarys profile, i.e., gives a higher AdvKnow(P Aj Ai , Tβ). (∀Aag, Ao ∈ A, α ∈ CAag , β ∈ CAo , Tn, Tα, Tβ, w ∈ W) Bel(Aag, (∀γ ∈ CAag )Eval(Aag, γ, w) < TrH, Tn) ∧ Bel(Aag, Do(Aag, α, Tα, w) ⇒ Do(Ao, β, Tβ, w), Tn) ∧ Bel(Aag, Eval(Ao, β, w) < TrH) ∧ Bel(Aag, AdvKnow(P Aj Ai , Tβ) > AdvKnow(P Aj Ai , Tn), Tn) ⇒ Pot.Int.To(Aag, α, Tn, Tα, w) For example, going back to the Chess board game scenario, consider starting a game versus an opponent about whom we know nothing, not even if it is a human or a computerized opponent.",
                "We might start playing a strategy that will be suitable versus an average opponent, and adjust our game according to its level of play.",
                "A5.",
                "Alliance Formation Axiom The following behavioral axiom is relevant only in a multilateral instantiation of the adversarial environment (obviously, an alliance cannot be formed in a bilateral, zero-sum encounter).",
                "In different situations during a multilateral interaction, a group of agents might believe that it is in their best interests to form a temporary alliance.",
                "Such an alliance is an agreement that constrains its members behavior, but is believed by its members to enable them to achieve a higher utility value than the one achievable outside of the alliance.",
                "As an example, we can look at the classical Risk board game, where each player has an individual goal of being the sole conquerer of the world, a zero-sum game.",
                "However, in order to achieve this goal, it might be strategically wise to make short-term ceasefire agreements with other players, or to join forces and attack an opponent who is stronger than the rest.",
                "An alliances terms defines the way its members should act.",
                "It is a set of predicates, denoted as Terms, that is agreed upon by the alliance members, and should remain true for the duration of the alliance.",
                "For example, the set Terms in the Risk scenario, could contain the following predicates: 1.",
                "Alliance members will not attack each other on territories X, Y and Z; 2.",
                "Alliance members will contribute C units per turn for attacking adversary Ao; 3.",
                "Members are obligated to stay as part of the alliance until time Tk or until adversarys Ao army is smaller than Q.",
                "The set Terms specifies inter-group constraints on each of the alliance members (∀Aal i ∈ Aal ⊆ A) set of actions Cal i ⊆ C. Definition 6.",
                "Al val - the total evaluation value that agent Ai will achieve while being part of Aal is the sum of Evali (Eval for Ai) of each of Aal j Eval values after taking their own α actions (via the agent(α) predicate): Al val(Ai, Cal , Aal , w) = α∈Cal Evali(Aal j , agent(α), w) Definition 7.",
                "Al TrH - is a number representing an Al val The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 553 threshold; above it, the alliance can be said to be a highly beneficial alliance.",
                "The value of Al TrH will be calculated dynamically according to the progress of the interaction, as can be seen in [7].",
                "After an alliance is formed, its members are now working in their normal adversarial environment, as well as according to the mental states and axioms required for their interactions as part of the alliance.",
                "The following Alliance model (AL) specifies the conditions under which the group Aal can be said to be in an alliance and working with a new and constrained set of actions Cal , at time Tn.",
                "AL(Aal , Cal , w, Tn) 1.",
                "Aal has a MB that all members are part of Aal : MB(Aal , (∀Aal i ∈ Aal )member(Aal i , Aal ), Tn) 2.",
                "Aal has a MB that the group be maintained: MB(Aal , (∀Aal i ∈ Aal )Int.Th (Ai, member(Ai, Aal ), Tn, Tn+1, Co), Tn) 3.",
                "Aal has a MB that being members gives them high utility value: MB(Aal , (∀Aal i ∈ Aal )Al val(Aal i , Cal , Aal , w) ≥ Al TrH, Tn) Members profiles are a crucial part of successful alliances.",
                "We assume that agents that have more accurate profiles of their adversaries will be more successful in such environments.",
                "Such agents will be able to predict when a member is about to breach the alliances contract (item 2 in the above model), and take counter measures (when item 3 will falsify).",
                "The robustness of the alliance is in part a function of its members trustfulness measure, objective position estimation, and other profile properties.",
                "We should note that an agent can simultaneously be part of more than one alliance.",
                "Such a temporary alliance, where the group members do not have a joint goal but act collaboratively for the interest of their own individual goals, is classified as a Treatment Group by modern psychologists [12] (in contrast to a Task Group, where its members have a joint goal).",
                "The Shared Activity model as presented in [5] modeled Treatment Group behavior using the same SharedPlans formalization.",
                "When comparing both definitions of an alliance and a Treatment Group we found an unsurprising resemblance between both models: the environment models definitions are almost identical (see SAs definitions in [5]), and their Selfish-Act and Cooperative Act axioms conform to our adversarial agents behavior.",
                "The main distinction between both models is the integration of a Helpful-behavior act axiom, in the Shared Activity which cannot be part of ours.",
                "This axiom states that an agent will consider taking action that will lower its Eval value (to a certain lower bound), if it believes that a group partner will gain a significant benefit.",
                "Such behavior cannot occur in a pure adversarial environment (as a zero-sum game is), where the alliance members are constantly on watch to manipulate their alliance to their own advantage.",
                "A6.",
                "Evaluation Maximization Axiom.",
                "In a case when all other axioms are inapplicable, we will proceed with the action that maximizes the heuristic value as computed in the Eval function. (∀Aag, Ao ∈ A, α ∈ Cag, Tn, w ∈ W) Bel(Aag, (∀γ ∈ Cag)Eval(Aag, α, w) ≥ Eval(Aag, γ, w), Tn) ⇒ Pot.Int.To(Aag, α, Tn, Tα, w) T1.",
                "Optimality on Eval = Utility The above axiomatic model handles situations where the Utility is unknown and the agents are bounded rational agents.",
                "The following theorem shows that in bilateral interactions, where the agents have the real Utility function (i.e., Eval = Utility) and are rational agents, the axioms provide the same optimal result as classic adversarial search (e.g., Min-Max).",
                "Theorem 1.",
                "Let Ae ag be an unbounded rational AE agent using the Eval heuristic evaluation function, Au ag be the same agent using the true Utility function, and Ao be a sole unbounded utility-based rational adversary.",
                "Given that Eval = Utility: (∀α ∈ CAu ag , α ∈ CAe ag , Tn, w ∈ W) Pot.Int.To(Au ag, α, Tn, Tα, w) → Pot.Int.To(Ae ag, α , Tn, Tα, w) ∧ ((α = α ) ∨ (Utility(Au ag, α, w) = Eval(Ae ag, α , w))) Sketch of proof - Given that Au ag has the real utility function and unbounded resources, it can generate the full game tree and run the optimal MinMax algorithm to choose the highest utility value action, which we denote by, α.",
                "The proof will show that Ae ag, using the AE axioms, will select the same or equal utility α (when there is more than one action with the same max utility) when Eval = Utility. (A1) Goal achieving axiom - suppose there is an α such that its completion will achieve Au ags goal.",
                "It will obtain the highest utility by Min-Max for Au ag.",
                "The Ae ag agent will select α or another action with the same utility value via A1.",
                "If such α does not exist, Ae ag cannot apply this axiom, and proceeds to A2. (A2) Preventive act axiom - (1) Looking at the basic case (see Prop1), if there is a β which leads Ao to achieve its goal, then a preventive action α will yield the highest utility for Au ag.",
                "Au ag will choose it through the utility, while Ae ag will choose it through A2. (2) In the general case, β is a highly beneficial action for Ao, thus yields low utility for Au ag, which will guide it to select an α that will prevent β, while Ae ag will choose it through A2.1 If such β does not exist for Ao, then A2 is not applicable, and Ae ag can proceed to A3. (A3) Suboptimal tactical move axiom - When using a heuristic Eval function, Ae ag has a partial belief in the profile of its adversary (item 4 in AE model), which may lead it to believe in SetActions (Prop1).",
                "In our case, Ae ag is holding a full profile on its optimal adversary and knows that Ao will behave optimally according to the real utility values on the complete search tree, therefore, any belief about suboptimal SetAction cannot exist, yielding this axiom inapplicable.",
                "Ae ag will proceed to A4. (A4) Profile detection axiom - Given that Ae ag has the full profile of Ao, none of Ae ags actions can increase its knowledge.",
                "That axiom will not be applied, and the agent will proceed with A6 (A5 will be disregarded because the interaction is bilateral). (A6) Evaluation maximization axiom - This axiom will select the max Eval for Ae ag.",
                "Given that Eval = Utility, the same α that was selected by Au ag will be selected. 3.",
                "EVALUATION The main purpose of our experimental analysis is to evaluate the models behavior and performance in a real adversarial environment.",
                "This section investigates whether bounded 1 A case where following the completion of β there exists a γ which gives high utility for Agent Au ag, cannot occur because Ao uses the same utility, and γs existence will cause it to classify β as a low utility action. 554 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) rational agents situated in such adversarial environments will be better off applying our suggested behavioral axioms. 3.1 The Domain To explore the use of the above model and its behavioral axioms, we decided to use the Connect-Four game as our adversarial environment.",
                "Connect-Four is a 2-player, zerosum game which is played using a 6x7 matrix-like board.",
                "Each turn, a player drops a disc into one of the 7 columns (the set of 21 discs is usually colored yellow for player 1 and red for player 2; we will use White and Black respectively to avoid confusion).",
                "The winner is the first player to complete a horizontal, vertical, or diagonal set of four discs with its color.",
                "On very rare occasions, the game might end in a tie if all the empty grids are filled, but no player managed to create a 4-disc set.",
                "The Connect-Four game was solved in [1], where it is shown that the first player (playing with the white discs) can force a win by starting in the middle column (column 4) and playing optimally However, the optimal strategy is very complex, and difficult to follow even for complex bounded rational agents, such as human players.",
                "Before we can proceed checking agent behavior, we must first verify that the domain conforms to the adversarial environments definition as given above (which the behavioral axioms are based on).",
                "First, when playing a Connect-Four game, the agent has an intention to win the game (item 1).",
                "Second (item 2), our agent believes that in Connect-Four there can only be one winner (or no winner at all in the rare occurrence of a tie).",
                "In addition, our agent believes that its opponent to the game will try to win (item 3), and we hope it has some partial knowledge (item 4) about its adversary (this knowledge can vary from nothing, through simple facts such as age, to strategies and weaknesses).",
                "Of course, not all Connect-Four encounters are adversarial.",
                "For example, when a parent is playing the game with its child, the following situation might occur: the child, having a strong incentive to win, treats the environment as adversarial (it intends to win, understands that there can only be one winner, and believes that its parent is trying to beat him).",
                "However, the parents point of view might see the environment as an educational one, where its goal is not to win the game, but to cause enjoyment or practice strategic reasoning.",
                "In such an educational environment, a new set of behavioral axioms might be more beneficial to the parents goals than our suggested adversarial behavioral axioms. 3.2 Axiom Analysis After showing that the Connect-Four game is indeed a zero-sum, bilateral adversarial environment, the next step is to look at players behaviors during the game and check whether behaving according to our model does improve performance.",
                "To do so we have collected log files from completed Connect-Four games that were played by human players over the Internet.",
                "Our collected log file data came from Play by eMail (PBeM) sites.",
                "These are web sites that host email games, where each move is taken by an email exchange between the server and the players.",
                "Many such sites archives contain real competitive interactions, and also maintain a ranking system for their members.",
                "Most of the data we used can be found in [6].",
                "As can be learned from [1], Connect-Four has an optimal strategy and a considerable advantage for the player who starts the game (which we call the White player).",
                "We will concentrate in our analysis on the second players moves (to be called Black).",
                "The White player, being the first to act, has the so-called initiative advantage.",
                "Having the advantage and a good strategy will keep the Black player busy reacting to its moves, instead of initiating threats.",
                "A threat is a combination of three discs of the same color, with an empty spot for the fourth winning disk.",
                "An open threat is a threat that can be realized in the opponents next move.",
                "In order for the Black player to win, it must somehow turn the tide, take the advantage and start presenting threats to the White player.",
                "We will explore Black players behavior and their conformance to our axioms.",
                "To do so, we built an application that reads log files and analyzes the Black players moves.",
                "The application contains two main components: (1) a Min-Max algorithm for evaluation of moves; (2) open threats detector for the discovering of open threats.",
                "The Min-Max algorithm will work to a given depth, d and for each move α will output the heuristic value for the next action taken by the player as written in the log file, h(α), alongside the maximum heuristic value, maxh(α), that could be achieved prior to taking the move (obviously, if h(α) = maxh(α), then the player did not do the optimal move heuristically).",
                "The threat detectors job is to notify if some action was taken in order to block an open threat (not blocking an open threat will probably cause the player to lose in the opponents next move).",
                "The heuristic function used by Min-Max to evaluate the players utility is the following function, which is simple to compute, yet provides a reasonable challenge to human opponents: Definition 8.",
                "Let Group be an adjacent set of four squares that are horizontal, vertical, or diagonal.",
                "Groupn b (Groupn w) be a Group with n pieces of the black (white) color and 4−n empty squares. h = ((Group1 b ∗α)+(Group2 b ∗β)+(Group3 b ∗γ)+(Group4 b ∗∞)) − ((Group1 w ∗α)+(Group2 w ∗β)+(Group3 w ∗γ)+(Group4 w ∗∞)) The values of α, β and δ can vary to form any desired linear combination; however, it is important to value them with the α < β < δ ordering in mind (we used 1, 4, and 8 as their respective values).",
                "Groups of 4 discs of the same color means victory, thus discovery of such a group will result in ∞ to ensure an extreme value.",
                "We now use our estimated evaluation function to evaluate the Black players actions during the Connect-Four adversarial interaction.",
                "Each game from the log file was input into the application, which processed and output a reformatted log file containing the h value of the current move, the maxh value that could be achieved, and a notification if an open threat was detected.",
                "A total of 123 games were analyzed (57 with White winning, and 66 with Black winning).",
                "A few additional games were manually ignored in the experiment, due to these problems: a player abandoning the game while the outcome is not final, or a blunt irrational move in the early stages of the game (e.g., not blocking an obvious winning group in the first opening moves).",
                "In addition, a single tie game was also removed.",
                "The simulator was run to a search depth of 3 moves.",
                "We now proceed to analyze the games with respect to each behavioral axiom.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 555 Table 1: Average heuristic difference analysis Black losses Black Won Avg minh -17.62 -12.02 Avg 3 lowest h moves (min3 h) -13.20 -8.70 3.2.1 Affirming the Suboptimal tactical move axiom The following section presents the heuristic evaluations of the Min-Max algorithm for each action, and checks the amount and extent of suboptimal tactical actions and their implications on performance.",
                "Table 1 shows results and insights from the games heuristic analysis, when search depth equals 3 (this search depth was selected for the results to be comparable to [9], see Section 3.2.3).",
                "The tables heuristic data is the difference between the present maximal heuristic value and the heuristic value of the action that was eventually taken by the player (i.e., the closer the number is to 0, the closer the action was to the maximum heuristic action).",
                "The first row presents the difference values of the action that had the maximal difference value among all the Black players actions in a given game, as averaged over all Blacks winning and losing games (see respective columns).",
                "In games in which the Black player loses, its average difference value was -17.62, while in games in which the Black player won, its average was -12.02.",
                "The second row expands the analysis by considering the 3 highest heuristic difference actions, and averaging them.",
                "In that case, we notice an average heuristic difference of 5 points between games which the Black player loses and games in which it wins.",
                "Nevertheless, the importance of those numbers is that they allowed us to take an educated guess on a threshold number of 11.5, as the value of the TrH constant, which differentiates between normal actions and highly beneficial ones.",
                "After finding an approximated TrH constant, we can proceed with an analysis of the importance of suboptimal moves.",
                "To do so we took the subset of games in which the minimum heuristic difference value for Blacks actions was 11.5.",
                "As presented in Table 2, we can see the different min3 h average of the 3 largest ranges and the respective percentage of games won.",
                "The first row shows that the Black player won only 12% of the games in which the average of its 3 highest heuristically difference actions (min3 h) was smaller than the suggested threshold, TrH = 11.5.",
                "The second row shows a surprising result: it seems that when min3 h > −4 the Black player rarely wins.",
                "Intuition would suggest that games in which the action evaluation values were closer to the maximal values will result in more winning games for Black.",
                "However, it seems that in the Connect-Four domain, merely responding with somewhat easily expected actions, without initiating a few surprising and suboptimal moves, does not yield good results.",
                "The last row sums up the main insights from the analysis; most of Blacks wins (83%) came when its min3 h was in the range of -11.5 to -4.",
                "A close inspection of those Black winning games shows the following pattern behind the numbers: after standard opening moves, Black suddenly drops a disc into an isolated column, which seems a waste of a move.",
                "White continues to build its threats, while usually disregarding Blacks last move, which in turn uses the isolated disc as an anchor for a future winning threat.",
                "The results show that it was beneficial for the Black player Table 2: Blacks winnings percentages % of games min3 h < −11.5 12% min3 h > −4 5% −11.5 ≤ min3 h ≤ −4 83% to take suboptimal actions and not give the current highest possible heuristic value, but will not be too harmful for its position (i.e., will not give high beneficial value to its adversary).",
                "As it turned out, learning the threshold is an important aspect of success: taking wildly risky moves (min3 h < −11.5) or trying to avoid them (min3 h > −4) reduces the Black players winning chances by a large margin. 3.2.2 Affirming the Profile Monitoring Axiom In the task of showing the importance of monitoring ones adversaries profiles, our log files could not be used because they did not contain repeated interactions between players, which are needed to infer the players knowledge about their adversaries.",
                "However, the importance of opponent modeling and its use in attaining tactical advantages was already studied in various domains ([3, 9] are good examples).",
                "In a recent paper, Markovitch and Reger [9] explored the notion of learning and exploitation of opponent weakness in competitive interactions.",
                "They apply simple learning strategies by analyzing examples from past interactions in a specific domain.",
                "They also used the Connect-Four adversarial domain, which can now be used to understand the importance of monitoring the adversarys profile.",
                "Following the presentation of their theoretical model, they describe an extensive <br>empirical study</br> and check the agents performance after learning the weakness model with past examples.",
                "One of the domains used as a competitive environment was the same Connect-Four game (Checkers was the second domain).",
                "Their heuristic function was identical to ours with three different variations (H1, H2, and H3) that are distinguished from one another in their linear combination coefficient values.",
                "The search depth for the players was 3 (as in our analysis).",
                "Their extensive experiments check and compare various learning strategies, risk factors, predefined feature sets and usage methods.",
                "The bottom line is that the Connect-Four domain shows an improvement from a 0.556 winning rate before modeling to a 0.69 after modeling (page 22).",
                "Their conclusions, showing improved performance when holding and using the adversarys model, justify the effort to monitor the adversary profile for continuous and repeated interactions.",
                "An additional point that came up in their experiments is the following: after the opponent weakness model has been learned, the authors describe different methods of integrating the opponent weakness model into the agents decision strategy.",
                "Nevertheless, regardless of the specific method they chose to work with, all integration methods might cause the agent to take suboptimal decisions; it might cause the agent to prefer actions that are suboptimal at the present decision junction, but which might cause the opponent to react in accordance with its weakness model (as represented by our agent) which in turn will be beneficial for us in the future.",
                "The agents behavior, as demonstrated in [9] further confirms and strengthens our Suboptimal Tactical Axiom as discussed in the previous section. 556 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 3.2.3 Additional Insights The need for the Goal Achieving, Preventive Act, and Evaluation Maximization axioms are obvious, and need no further verification.",
                "However, even with respect to those axioms, a few interesting insights came up in the log analysis.",
                "The Goal achieving and Preventive Act axioms, though theoretically trivial, seem to provide some challenge to a human player.",
                "In the initial inspection of the logs, we encountered few games2 where a player, for inexplicable reasons, did not block the other from winning or failed to execute its own winning move.",
                "We can blame those faults on the humans lack of attention, or a typing error in its move reply; nevertheless, those errors might occur in bounded rational agents, and the appropriate behavior needs to be axiomatized.",
                "A typical Connect-Four game revolves around generating threats and blocking them.",
                "In our analysis we looked for explicit preventive actions, i.e., moves that block a group of 3 discs, or that remove a future threat (in our limited search horizon).",
                "We found that in 83% of the total games there was at least one preventive action taken by the Black player.",
                "It was also found that Black averaged 2.8 preventive actions per game on the games in which it lost, while averaging 1.5 preventive actions per game when winning.",
                "It seems that Black requires 1 or 2 preventive actions to build its initial taking position, before starting to present threats.",
                "If it did not manage to win, it will usually prevent an extra threat or two before succumbing to White. 4.",
                "RELATED WORK Much research deals with the axiomatization of teamwork and mental states of individuals: some models use knowledge and belief [10], others have models of goals and intentions [8, 4].",
                "However, all these formal theories deal with agent teamwork and cooperation.",
                "As far as we know, our model is the first to provide a formalized model for explicit adversarial environments and agents behavior in it.",
                "The classical Min-Max adversarial search algorithm was the first attempt to integrate the opponent into the search space with a weak assumption of an optimally playing opponent.",
                "Since then, much effort has gone into integrating the opponent model into the decision procedure to predict future behavior.",
                "The M∗ algorithm presented by Carmel and Markovitch [2] showed a method of incorporating opponent models into adversary search, while in [3] they used learning to provide a more accurate opponent model in a 2player repeated game environment, where agents strategies were modeled as finite automata.",
                "Additional Adversarial planning work was done by Willmott et al. [13], which provided an adversarial planning approach to the game of GO.",
                "The research mentioned above dealt with adversarial search and the integration of opponent models into classical utilitybased search methods.",
                "That work shows the importance of opponent modeling and the ability to exploit it to an agents advantage.",
                "However, the basic limitations of those search methods still apply; our model tries to overcome those limitations by presenting a formal model for a new, mental state-based adversarial specification. 5.",
                "CONCLUSIONS We presented an Adversarial Environment model for a 2 These were later removed from the final analysis. bounded rational agent that is situated in an N-player, zerosum environment.",
                "We used the SharedPlans formalization to define the model and the axioms that agents can apply as behavioral guidelines.",
                "The model is meant to be used as a guideline for designing agents that need to operate in such adversarial environments.",
                "We presented empirical results, based on ConnectFour log file analysis, that exemplify the model and the axioms for a bilateral instance of the environment.",
                "The results we presented are a first step towards an expanded model that will cover all types of adversarial environments, for example, environments that are non-zero-sum, and environments that contain natural agents that are not part of the direct conflict.",
                "Those challenges and more will be dealt with in future research. 6.",
                "ACKNOWLEDGMENT This research was supported in part by Israel Science Foundation grants #1211/04 and #898/05. 7.",
                "REFERENCES [1] L. V. Allis.",
                "A knowledge-based approach of Connect-Four - the game is solved: White wins.",
                "Masters thesis, Free University, Amsterdam, The Netherlands, 1988. [2] D. Carmel and S. Markovitch.",
                "Incorporating opponent models into adversary search.",
                "In Proceedings of the Thirteenth National Conference on Artificial Intelligence, pages 120-125, Portland, OR, 1996. [3] D. Carmel and S. Markovitch.",
                "Opponent modeling in multi-agent systems.",
                "In G. Weiß and S. Sen, editors, Adaptation and Learning in Multi-Agent Systems, pages 40-52.",
                "Springer-Verlag, 1996. [4] B. J. Grosz and S. Kraus.",
                "Collaborative plans for complex group action.",
                "Artificial Intelligence, 86(2):269-357, 1996. [5] M. Hadad, G. Kaminka, G. Armon, and S. Kraus.",
                "Supporting collaborative activity.",
                "In Proc. of AAAI-2005, pages 83-88, Pittsburgh, 2005. [6] http://www.gamerz.net/˜pbmserv/. [7] S. Kraus and D. Lehmann.",
                "Designing and building a negotiating automated agent.",
                "Computational Intelligence, 11:132-171, 1995. [8] H. J. Levesque, P. R. Cohen, and J. H. T. Nunes.",
                "On acting together.",
                "In Proc. of AAAI-90, pages 94-99, Boston, MA, 1990. [9] S. Markovitch and R. Reger.",
                "Learning and exploiting relative weaknesses of opponent agents.",
                "Autonomous Agents and Multi-Agent Systems, 10(2):103-130, 2005. [10] Y. M. Ronald Fagin, Joseph Y. Halpern and M. Y. Vardi.",
                "Reasoning about knowledge.",
                "MIT Press, Cambridge, Mass., 1995. [11] P. Thagard.",
                "Adversarial problem solving: Modeling an oponent using explanatory coherence.",
                "Cognitive Science, 16(1):123-149, 1992. [12] R. W. Toseland and R. F. Rivas.",
                "An Introduction to Group Work Practice.",
                "Prentice Hall, Englewood Cliffs, NJ, 2nd edition edition, 1995. [13] S. Willmott, J. Richardson, A. Bundy, and J. Levine.",
                "An adversarial planning approach to Go.",
                "Lecture Notes in Computer Science, 1558:93-112, 1999.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 557"
            ],
            "original_annotated_samples": [
                "Following the presentation of their theoretical model, they describe an extensive <br>empirical study</br> and check the agents performance after learning the weakness model with past examples."
            ],
            "translated_annotated_samples": [
                "Tras la presentación de su modelo teórico, describen un extenso <br>estudio empírico</br> y verifican el rendimiento de los agentes después de aprender el modelo de debilidades con ejemplos pasados."
            ],
            "translated_text": "Un modelo de entorno adversarial para agentes racionales limitados en interacciones de suma cero Inon Zuckerman1, Sarit Kraus1, Jeffrey S. Rosenschein2, Gal Kaminka1 1 Departamento de Ciencias de la Computación 2 Escuela de Ingeniería Universidad Bar-Ilan y Ciencias de la Computación Ramat-Gan, Israel Universidad Hebrea, Jerusalén, Israel {zukermi, sarit, galk}@cs.biu.ac.il jeff@cs.huji.ac.il RESUMEN Los entornos multiagentes a menudo no son cooperativos ni colaborativos; en muchos casos, los agentes tienen intereses conflictivos, lo que lleva a interacciones adversariales. Este documento presenta un modelo formal de Entorno Adversarial para agentes racionales acotados que operan en un entorno de suma cero. En tales entornos, los intentos de utilizar métodos de búsqueda basados en utilidad clásica pueden plantear una variedad de dificultades (por ejemplo, modelar implícitamente al oponente como un maximizador de utilidad omnisciente, en lugar de aprovechar un modelo de oponente más matizado y explícito). Definimos un Entorno Adversario al describir los estados mentales de un agente en dicho entorno. Luego presentamos axiomas de comportamiento que están destinados a servir como principios de diseño para construir agentes adversarios. Exploramos la aplicación de nuestro enfoque mediante el análisis de archivos de registro de juegos de Conecta Cuatro completados, y presentamos un análisis empírico de la adecuación de los axiomas. Categorías y Descriptores de Asignaturas I.2.11 [Inteligencia Artificial]: Inteligencia Artificial Distribuida-Agentes Inteligentes, Sistemas Multiagente; I.2.4 [Inteligencia Artificial]: Formalismos y Métodos de Representación del Conocimiento - Lógica Modal Términos Generales Diseño, Teoría 1. Introducción: En las primeras investigaciones en sistemas multiagente (SMA) se consideraban grupos cooperativos de agentes; debido a que los agentes individuales tenían recursos limitados o acceso limitado a la información (por ejemplo, potencia de procesamiento limitada, cobertura sensorial limitada), trabajaban juntos por diseño para resolver problemas que individualmente no podían resolver, o al menos no podían resolver de manera eficiente. Sin embargo, la investigación de MAS pronto comenzó a considerar agentes interactivos con intereses individualizados, como representantes de diferentes humanos u organizaciones con intereses no idénticos. Cuando las interacciones son guiadas por intereses diversos, los participantes pueden tener que superar desacuerdos, interacciones no cooperativas e incluso intentos intencionales de dañarse mutuamente. Cuando ocurren este tipo de interacciones, los entornos requieren un comportamiento apropiado por parte de los agentes situados en ellos. Llamamos a estos entornos Entornos Adversarios, y llamamos a los agentes en conflicto Adversarios. Los modelos de cooperación y trabajo en equipo han sido ampliamente explorados en MAS a través de la axiomatización de estados mentales (por ejemplo, [8, 4, 5]). Sin embargo, ninguna de estas investigaciones abordó los dominios adversarios y sus implicaciones para el comportamiento de los agentes. Nuestro artículo aborda este problema al proporcionar un modelo formal y axiomatizado de estados mentales para un subconjunto de dominios adversariales, específicamente entornos adversariales simples de suma cero. Los encuentros de suma cero simples existen, por supuesto, en varios juegos de dos jugadores (por ejemplo, Ajedrez, Damas), pero también existen en juegos de n jugadores (por ejemplo, Risk, Diplomacia), subastas de un solo bien y en otros lugares. En estos entornos recientes en particular, el uso de una búsqueda adversarial basada en utilidad (como el algoritmo Min-Max) no siempre proporciona una solución adecuada; la función de recompensa podría ser bastante compleja o difícil de cuantificar, y existen limitaciones computacionales naturales en agentes racionales acotados. Además, los métodos de búsqueda tradicionales (como Min-Max) no hacen uso de un modelo del oponente, lo cual ha demostrado ser una adición valiosa a la planificación adversarial [9, 3, 11]. En este artículo, desarrollamos un modelo formal y axiomatizado para agentes racionales limitados que se encuentran en un entorno adversarial de suma cero. El modelo utiliza diferentes operadores de modalidad, y sus principales fundamentos son el modelo SharedPlans [4] para el comportamiento colaborativo. Exploramos las propiedades del entorno y los estados mentales de los agentes para derivar axiomas de comportamiento; estos axiomas de comportamiento constituyen un modelo formal que sirve como especificación y guía de diseño para el diseño de agentes en tales entornos. Luego investigamos el comportamiento de nuestro modelo empíricamente utilizando el juego de mesa Conecta Cuatro. Mostramos que este juego se ajusta a nuestra definición de entorno, y analizamos el comportamiento de los jugadores utilizando un gran conjunto de registros de partidas completadas 550 978-81-904262-7-5 (RPS) c 2007 archivos de IFAAMAS. Además, utilizamos los resultados presentados en [9] para discutir la importancia del modelado del oponente en nuestro dominio adversarial de Conecta Cuatro. El artículo continúa de la siguiente manera. La sección 2 presenta la formalización de los modelos. La sección 3 presenta el análisis empírico y sus resultados. Discutimos el trabajo relacionado en la Sección 4, y concluimos y presentamos las direcciones futuras en la Sección 5. ENTORNOS ADVERSARIOS El modelo de entorno adversario (denominado como AE) está destinado a guiar el diseño de agentes al proporcionar una especificación de las capacidades y actitudes mentales de un agente en un entorno adversario. Nos enfocamos aquí en tipos específicos de entornos adversarios, especificados de la siguiente manera: 1. Interacciones de suma cero: las utilidades positivas y negativas de todos los agentes suman cero; 2. Agentes adversarios simples: todos los agentes en el entorno son agentes adversarios; 3. AEs bilaterales: AEs con exactamente dos agentes; 4. AEs multilaterales: AEs de tres o más agentes. Trabajaremos en instanciaciones tanto bilaterales como multilaterales de entornos de suma cero y simples. En particular, nuestro modelo de entorno adversarial tratará con interacciones que consisten en N agentes (N ≥ 2), donde todos los agentes son adversarios y solo un agente puede tener éxito. Ejemplos de tales entornos van desde juegos de mesa (por ejemplo, Ajedrez, Conecta Cuatro y Diplomacia) hasta ciertos entornos económicos (por ejemplo, subastas de N postores sobre un único bien). 2.1 Resumen del Modelo Nuestro enfoque es formalizar las actitudes mentales y comportamientos de un único agente adversario; consideramos cómo un único agente percibe el AE. La siguiente lista especifica las condiciones y estados mentales de un agente en un AE simple de suma cero: 1. El agente tiene la intención individual de que su propio objetivo se complete; 2. El agente tiene la creencia individual de que él y sus adversarios persiguen objetivos completamente opuestos (definidos a continuación) y que solo puede haber un ganador; 3. El agente tiene la creencia individual de que cada adversario tiene la intención de completar su propio objetivo conflictivo completo; 4. El agente tiene una creencia individual en el perfil (parcial) de sus adversarios. El ítem 3 es necesario, ya que podría ser el caso de que algún agente tenga un objetivo completamente conflictivo y esté considerando adoptar la intención de completarlo, pero aún no está comprometido a lograrlo. Esto podría ocurrir porque el agente aún no ha reflexionado sobre los efectos que adoptar esa intención podría tener en las otras intenciones que actualmente mantiene. En tales casos, es posible que ni siquiera se considere en un entorno adversarial. El ítem 4 establece que el agente debe tener alguna creencia sobre los perfiles de sus adversarios. El perfil representa todo el conocimiento que el agente tiene sobre su adversario: sus debilidades, capacidades estratégicas, objetivos, intenciones, confiabilidad y más. Puede ser dado explícitamente o puede ser aprendido a partir de observaciones de encuentros pasados. 2.2 Definiciones de modelos para estados mentales Utilizamos las definiciones de los operadores modales, predicados y metapredicados de Grosz y Krauss, tal como se definen en su formalización de SharedPlan [4]. Recordamos aquí algunos de los predicados y operadores que se utilizan en esa formalización: Int.To(Ai, α, Tn, Tα, C) representa las intenciones de Ai en el tiempo Tn de realizar una acción α en el tiempo Tα en el contexto de C. Int.Th(Ai, prop, Tn, Tprop, C) representa las intenciones de Ai en el tiempo Tn de que una cierta proposición prop se cumpla en el tiempo Tprop en el contexto de C. Los operadores de intención potencial, Pot.Int.To(...) y Pot.Int.Th(...), se utilizan para representar el estado mental cuando un agente considera adoptar una intención, pero no ha deliberado sobre la interacción de las otras intenciones que tiene. El operador Bel(Ai, f, Tf) representa al agente Ai creyendo en la afirmación expresada en la fórmula f, en el tiempo Tf. MB(A, f, Tf) representa la creencia mutua para un grupo de agentes A. Una instantánea del sistema encuentra que nuestro entorno se encuentra en algún estado e ∈ E de estados variables ambientales, y cada adversario en cualquier LAi ∈ L de posibles estados locales. En cualquier paso de tiempo dado, el sistema estará en algún mundo w del conjunto de todos los mundos posibles w ∈ W, donde w = E×LA1 ×LA2 × ...LAn, y n es el número de adversarios. Por ejemplo, en un juego de póker Texas Holdem, el estado local de un agente podría ser su propio conjunto de cartas (que es desconocido para su adversario) mientras que el entorno consistirá en el bote de apuestas y las cartas comunitarias (que son visibles para ambos jugadores). Una función de utilidad bajo esta formalización se define como un mapeo de un mundo posible w ∈ W a un elemento en , que expresa la deseabilidad del mundo, desde la perspectiva de un único agente. Normalmente normalizamos el rango a [0,1], donde 0 representa el mundo menos deseable posible, y 1 es el mundo más deseable. La implementación de la función de utilidad depende del dominio en cuestión. La siguiente lista especifica nuevos predicados, funciones, variables y constantes utilizadas en conjunto con las definiciones originales para la formalización del entorno adversarial: 1. φ es una acción nula (el agente no hace nada). 2. GAi es el conjunto de objetivos del agente Ai. Cada objetivo es un conjunto de predicados cuya satisfacción hace que el objetivo esté completo (usamos G∗ Ai ∈ GAi para representar un objetivo arbitrario del agente Ai). 3. gAi es el conjunto de subobjetivos del agente Ai. Los subobjetivos son predicados cuya satisfacción representa un hito importante hacia el logro del objetivo completo. gG∗ Ai ⊆ gAi es el conjunto de subobjetivos que son importantes para la finalización del objetivo G∗ Ai (usaremos g∗ G∗ Ai ∈ gG∗ Ai para representar un subobjetivo arbitrario). 4. P Aj Ai es el objeto de perfil que el agente Ai tiene sobre el agente Aj. 5. CA es un conjunto general de acciones para todos los agentes en A que se derivan de las restricciones del entorno. CAi ⊆ CA es el conjunto de acciones posibles del agente Ai. La expresión Do(Ai, α, Tα, w) se cumple cuando Ai realiza la acción α durante el intervalo de tiempo Tα en el mundo w. 7. La afirmación Achieve(G∗ Ai , α, w) es verdadera cuando se logra el objetivo G∗ Ai siguiendo la finalización de la acción α en el mundo w ∈ W, donde α ∈ CAi. El perfil (Ai, PAi Ai) es verdadero cuando el agente Ai tiene un perfil de objeto para el agente Aj. Definición 1. El conflicto total (FulConf) describe una interacción de suma cero donde solo se puede completar un solo objetivo de los objetivos en conflicto. FulConf(G∗ Ai , G∗ Aj ) ⇒ (∃α ∈ CAi , ∀w, β ∈ CAj ) (Achieve(G∗ Ai , α, w) ⇒ ¬Achieve(G∗ Aj , β, w)) ∨ (∃β ∈ CAj , ∀w, α ∈ CAi )(Achieve(G∗ Aj , β, w) ⇒ ¬Achieve(G∗ Ai , α, w)) Definición 2. El Conocimiento Adversarial (AdvKnow) es una función que devuelve un valor que representa la cantidad de The Sixth Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 551 agente de conocimiento Ai tiene sobre el perfil del agente Aj, en el tiempo Tn. Cuanto mayor sea el valor, más conocimiento tiene el agente Ai. AdvKnow: P Aj Ai × Tn → Definición 3. Eval: Esta función de evaluación devuelve un valor de utilidad esperada estimado para un agente en A, después de completar una acción de CA en algún estado del mundo w. Eval: A × CA × w → Definición 4. TrH - (Umbral) es una constante numérica en el rango [0,1] que representa un valor umbral de la función de evaluación (Eval). Una acción que produce una evaluación de utilidad estimada por encima del TrH se considera una acción altamente beneficiosa. El valor Eval es una estimación y no la función de utilidad real, la cual suele ser desconocida. Utilizar el valor real de utilidad para un agente racional fácilmente produciría el mejor resultado para ese agente. Sin embargo, los agentes generalmente no tienen las funciones de utilidad reales, sino más bien una estimación heurística de las mismas. Existen dos propiedades importantes que deben cumplir para la función de evaluación: Propiedad 1. La función de evaluación debería indicar que el estado del mundo más deseable es aquel en el que se logra el objetivo. Por lo tanto, una vez que se ha satisfecho el objetivo, no puede haber ninguna acción futura que pueda colocar al agente en un estado del mundo con un valor de Eval más alto. (∀Ai, G∗ Ai , α, β ∈ CAi , w ∈ W) Lograr(G∗ Ai , α, w) ⇒ Eval(Ai, α, w) ≥ Eval(Ai, β, w) Propiedad 2. La función de evaluación debe proyectar una acción que cause la finalización de un objetivo o subobjetivo a un valor que sea mayor que TrH (una acción altamente beneficiosa). (∀Ai, G∗ Ai ∈ GAi , α ∈ CAi , w ∈ W, g∗ GAi ∈ gGAi ) Lograr(G∗ Ai , α, w) ∨ Lograr(g∗ GAi , α, w) ⇒ Eval(Ai, α, w) ≥ TrH. Definición 5. La acción de conjunto Definimos una acción de conjunto (SetAction) como un conjunto de operaciones de acción (ya sean acciones complejas o básicas) de algunos conjuntos de acción CAi y CAj que, según la creencia del agente Ai, están unidas por una relación temporal y consecuente, formando una cadena de eventos (acción y su acción consecuente siguiente). (∀α1 , . . . , αu ∈ CAi , β1 , . . . , βv ∈ CAj , w ∈ W) SetAction(α1 , . . . , αu , β1 , . . . , βv , w) ⇒ ((Do(Ai, α1 , Tα1 , w) ⇒ Do(Aj, β1 , Tβ1 , w)) ⇒ Do(Ai, α2 , Tα2 , w) ⇒ . . . ⇒ Do(Ai, αu , Tαu , w)) La relación consecuente podría existir debido a varias restricciones ambientales (cuando una acción obliga al adversario a responder con una acción específica) o debido al conocimiento de los agentes sobre el perfil de su adversario. Propiedad 3. A medida que aumenta el conocimiento que tenemos sobre nuestro adversario, tendremos creencias adicionales sobre su comportamiento en diferentes situaciones, lo que a su vez crea un nuevo conjunto de acciones. Formalmente, si nuestro AdvKnow en el tiempo Tn+1 es mayor que AdvKnow en el tiempo Tn, entonces cada SetAction conocido en el tiempo Tn también es conocido en el tiempo Tn+1. AdvKnow(P Aj Ai , Tn+1) > AdvKnow(P Aj Ai , Tn) ⇒ (∀α1 , . . . , αu ∈ CAi , β1 , . . . , βv ∈ CAj ) Bel(Aag, SetAction(α1 , . . . , αu , β1 , . . . , βv ), Tn) ⇒ Bel(Aag, SetAction(α1 , . . . , αu , β1 , . . . , βv ), Tn+1) 2.3 La formulación del entorno Los siguientes axiomas proporcionan la definición formal para un Entorno Adversarial (AE) simple y de suma cero. La satisfacción de estos axiomas significa que el agente está situado en un entorno así. Proporciona especificaciones para que el agente Aag interactúe con su conjunto de adversarios A con respecto a los objetivos G∗ Aag y G∗ A en el tiempo TCo en algún estado del mundo w. AE(Aag, A, G∗ Aag, A1, . . . , Ak, G∗ A1, . . . , G∗ Ak, Tn, w) 1. Aag tiene un Int.Th su objetivo se completaría: (∃α ∈ CAag , Tα) Int.Th(Aag, Lograr(G∗ Aag , α), Tn, Tα, AE) 2. Aag cree que él y cada uno de sus adversarios Ao están persiguiendo metas completamente conflictivas: (∀Ao ∈ {A1, . . . , Ak}) Bel(Aag, FulConf(G∗ Aag , G∗ Ao ), Tn) 3. Aag cree que cada uno de sus adversarios en Ao tiene la Int.Th su objetivo en conflicto G∗ Aoi se completará: (∀Ao ∈ {A1, . . . , Ak})(∃β ∈ CAo , Tβ) Bel(Aag, Int.Th(Ao, Lograr(G∗ Ao , β), TCo, Tβ, AE), Tn) 4. Para construir un agente que pueda operar con éxito dentro de un entorno de adversarios estratégicos, debemos especificar pautas de comportamiento para sus interacciones. Utilizar una estrategia de maximización de Eval ingenua hasta cierta profundidad de búsqueda no siempre dará resultados satisfactorios por varias razones: (1) el problema del horizonte de búsqueda al buscar a una profundidad fija; (2) la fuerte suposición de un adversario óptimamente racional con recursos ilimitados; (3) utilizar una función de evaluación estimada que no dará resultados óptimos en todos los estados del mundo, y que puede ser explotada [9]. Los siguientes axiomas especifican los principios de comportamiento que se pueden utilizar para diferenciar entre agentes exitosos y menos exitosos en el entorno adversarial mencionado anteriormente. Esos axiomas deben ser utilizados como principios de especificación al diseñar e implementar agentes que deben ser capaces de desempeñarse bien en entornos adversarios. Los axiomas conductuales representan situaciones en las que el agente adoptará intenciones potenciales para (Pot.Int.To(...)) realizar una acción, lo que típicamente requerirá un razonamiento de medios-fines para seleccionar un posible curso de acción. Este razonamiento llevará a la adopción de un Int.To(...) (ver [4]). A1. Axioma para lograr metas. El primer axioma es el caso más simple; cuando el agente Aag cree que está a una acción (α) de lograr su objetivo en conflicto G∗ Aag, debería adoptar la intención potencial de hacer α y completar su objetivo. (∀Aag, α ∈ CAag, Tn, Tα, w ∈ W) (Bel(Aag, Do(Aag, α, Tα, w) ⇒ Achieve(G∗ Aag, α, w)) ⇒ Pot.Int.To(Aag, α, Tn, Tα, w) Este comportamiento algo trivial es el primer y más fuerte axioma. En cualquier situación, cuando el agente está a una acción de completar el objetivo, debería completar la acción. Cualquier función de evaluación justa clasificaría naturalmente a α como la acción de valor máximo (propiedad 1). Sin embargo, sin una axiomatización explícita de dicho comportamiento, podría haber situaciones en las que el agente decida tomar otra acción por diversas razones, debido a sus recursos de decisión limitados. A2. Axioma del Acto Preventivo. En una situación adversarial, el agente Aag podría decidir tomar acciones que dañen uno de los planes de su adversario para completar su objetivo, incluso si esas acciones no avanzan explícitamente a Aag hacia su objetivo en conflicto G∗ Aag. Tal acción preventiva tendrá lugar cuando el agente Aag tenga la creencia sobre la posibilidad de que su adversario Ao realice una acción β que le otorgará un alto 552 The Sixth Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) valor de evaluación de utilidad (> TrH). Creer que tomar la acción α evitará que el oponente realice su β, adoptará una intención potencial de hacer α. (∀Aag, Ao ∈ A, α ∈ CAag , β ∈ CAo , Tn, Tβ, w ∈ W) (Bel(Aag, Do(Ao, β, Tβ, w) ∧ Eval(Ao, β, w) > TrH, Tn) ∧ Bel(Aag, Do(Aag, α, Tα, w) ⇒ ¬Do(Ao, β, Tβ, w), Tn) ⇒ Pot.Int.To(Aag, α, Tn, Tα, w) Este axioma es un componente básico de cualquier entorno adversarial. Por ejemplo, al observar un juego de ajedrez, un jugador podría darse cuenta de que está a punto de ser jaque mateado por su oponente, por lo tanto, realiza una jugada preventiva. Otro ejemplo es el juego de Cuatro en Línea: cuando un jugador tiene una fila de tres fichas, su oponente debe bloquearla o perder. Una instancia específica de A1 ocurre cuando el adversario está a una acción de lograr su objetivo, y la acción preventiva inmediata debe ser tomada por el agente. Formalmente, tenemos las mismas creencias que se mencionaron anteriormente, con la creencia modificada de que realizar la acción β hará que el agente Ao logre su objetivo. Proposición 1: Caso de prevención o pérdida. (∀Aag, Ao ∈ A, α ∈ CAag , β ∈ CAo , G∗ Ao , Tn, Tα, Tβ, w ∈ W) Bel(Aag, Do(Ao, β, Tβ, w) ⇒ Achieve(G∗ Ao , β, w), Tn) ∧ Bel(Aag, Do(Aag, α, Tα, w) ⇒ ¬Do(Ao, β, Tβ, w)) ⇒ Pot.Int.To(Aag, α, Tn, Tα, w) Esquema de prueba: La Proposición 1 puede derivarse fácilmente del axioma A1 y de la propiedad 2 de la función Eval, que establece que cualquier acción que cause la finalización de un objetivo es una acción altamente beneficiosa. El comportamiento del acto preventivo ocurrirá implícitamente cuando la función Eval sea igual a la función de utilidad del mundo real. Sin embargo, al ser agentes racionales limitados y al tratar con una función de evaluación estimada, necesitamos axiomatizar explícitamente dicho comportamiento, ya que no siempre ocurrirá implícitamente a partir de la función de evaluación. A3. Axioma de Movimiento Táctico Subóptimo. En muchos escenarios, puede ocurrir una situación en la que un agente decida no tomar la acción más beneficiosa actual que puede tomar (la acción con el valor de evaluación de utilidad máximo), porque cree que tomar otra acción (con un valor de evaluación de utilidad más bajo) podría generar (dependiendo de la respuesta del adversario) una posibilidad futura para una acción altamente beneficiosa. Esto ocurrirá con mayor frecuencia cuando la función Eval sea inexacta y difiera en gran medida de la función de Utilidad. De manera formal, el agente Aag cree en una cierta SetAction que evolucionará de acuerdo con su acción inicial y producirá un valor beneficioso alto (> TrH) únicamente para él. (∀Aag, Ao ∈ A, Tn, w ∈ W) (∃α1 , . . . , αu ∈ CAi , β1 , . . . , βv ∈ CAj , Tα1 ) Bel(Aag, SetAction(α1 , . . . , αu , β1 , . . . , βv ), Tn) ∧ Bel(Aag, Eval(Ao, βv , w) < TrH < Eval(Aag, αu , w), Tn) ⇒ Pot.Int.To(Aag, α1 , Tn, Tα1 , w) Un agente podría creer que una cadena de eventos ocurrirá por diversas razones debido a la naturaleza inevitable del dominio. Por ejemplo, en el ajedrez, a menudo observamos lo siguiente: una jugada provoca una posición de jaque, lo que a su vez limita los movimientos del oponente para evitar el jaque, a lo que el primer jugador podría reaccionar con otro jaque, y así sucesivamente. El agente también podría creer en una cadena de eventos basada en su conocimiento del perfil de su adversario, lo que le permite prever los movimientos del adversario con alta precisión. A4. Axioma de Detección de Perfiles. El agente puede ajustar los perfiles de sus adversarios mediante observaciones y estudio de patrones (específicamente, si hay encuentros repetidos con el mismo adversario). Sin embargo, en lugar de esperar a que se revele la información del perfil, un agente también puede iniciar acciones que obligarán a su adversario a reaccionar de una manera que revele conocimiento del perfil sobre él. Formalmente, el axioma establece que si todas las acciones (γ) no son acciones altamente beneficiosas (< TrH), el agente puede realizar la acción α en el tiempo Tα si cree que resultará en una acción no altamente beneficiosa β por parte de su adversario, lo que a su vez le enseña sobre el perfil del adversario, es decir, proporciona un mayor AdvKnow(P Aj Ai , Tβ). (∀Aag, Ao ∈ A, α ∈ CAag , β ∈ CAo , Tn, Tα, Tβ, w ∈ W) Bel(Aag, (∀γ ∈ CAag )Eval(Aag, γ, w) < TrH, Tn) ∧ Bel(Aag, Do(Aag, α, Tα, w) ⇒ Do(Ao, β, Tβ, w), Tn) ∧ Bel(Aag, Eval(Ao, β, w) < TrH) ∧ Bel(Aag, AdvKnow(P Aj Ai , Tβ) > AdvKnow(P Aj Ai , Tn), Tn) ⇒ Pot.Int.To(Aag, α, Tn, Tα, w) Por ejemplo, volviendo al escenario del juego de ajedrez, consideremos comenzar una partida contra un oponente del que no sabemos nada, ni siquiera si es humano o un oponente computarizado. Podríamos comenzar a jugar una estrategia que sea adecuada frente a un oponente promedio, y ajustar nuestro juego de acuerdo a su nivel de juego. A5. Axioma de Formación de Alianzas El siguiente axioma de comportamiento es relevante únicamente en una instancia multilateral del entorno adversarial (obviamente, una alianza no puede formarse en un encuentro bilateral de suma cero). En diferentes situaciones durante una interacción multilateral, un grupo de agentes podría creer que es de su interés formar una alianza temporal. Una alianza de este tipo es un acuerdo que limita el comportamiento de sus miembros, pero se cree por parte de estos que les permite alcanzar un valor de utilidad mayor al que podrían lograr fuera de la alianza. Como ejemplo, podemos observar el clásico juego de mesa Risk, donde cada jugador tiene como objetivo individual ser el único conquistador del mundo, un juego de suma cero. Sin embargo, para lograr este objetivo, podría ser estratégicamente sabio hacer acuerdos de alto el fuego a corto plazo con otros jugadores, o unirse y atacar a un oponente que sea más fuerte que el resto. Los términos de una alianza definen la forma en que sus miembros deben actuar. Es un conjunto de predicados, denominados Términos, que es acordado por los miembros de la alianza y debe mantenerse verdadero durante la duración de la alianza. Por ejemplo, el conjunto de términos en el escenario de riesgo podría contener los siguientes predicados: 1. Los miembros de la alianza no se atacarán entre sí en los territorios X, Y y Z; 2. Los miembros de la alianza contribuirán C unidades por turno para atacar al adversario Ao; 3. Los miembros están obligados a permanecer como parte de la alianza hasta el tiempo Tk o hasta que el ejército adversario Ao sea más pequeño que Q. El conjunto de términos especifica restricciones intergrupales en cada uno de los miembros de la alianza (∀Aal i ∈ Aal ⊆ A) conjunto de acciones Cal i ⊆ C. Definición 6. El valor total de evaluación que el agente Ai logrará al ser parte de Aal es la suma de Evali (Eval para Ai) de cada uno de los valores de evaluación de Aal j después de tomar sus propias acciones α (a través del predicado del agente(α)): Al val(Ai, Cal, Aal, w) = α∈Cal Evali(Aal j, agente(α), w) Definición 7. Al TrH - es un número que representa un valor de Al en la Sexta Conferencia Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 553 umbral; por encima de él, se puede decir que la alianza es una alianza altamente beneficiosa. El valor de Al TrH se calculará dinámicamente de acuerdo con el progreso de la interacción, como se puede ver en [7]. Después de formarse una alianza, sus miembros ahora están trabajando en su entorno adversarial normal, así como de acuerdo con los estados mentales y axiomas requeridos para sus interacciones como parte de la alianza. El siguiente modelo de Alianza (AL) especifica las condiciones bajo las cuales el grupo Aal puede considerarse en una alianza y trabajando con un nuevo y limitado conjunto de acciones Cal, en el tiempo Tn. AL(Aal , Cal , w, Tn) 1. \n\nAL(Aal , Cal , w, Tn) 1. Aal tiene un MB en el que todos los miembros son parte de Aal: MB(Aal, (∀Aal i ∈ Aal)miembro(Aal i, Aal), Tn) 2. Aal tiene una MB que el grupo se mantenga: MB(Aal , (∀Aal i ∈ Aal )Int.Th (Ai, miembro(Ai, Aal ), Tn, Tn+1, Co), Tn) 3. Aal tiene un beneficio marginal que al ser miembros les otorga un alto valor de utilidad: MB(Aal , (∀Aal i ∈ Aal )Al val(Aal i , Cal , Aal , w) ≥ Al TrH, Tn) Los perfiles de los miembros son una parte crucial de las alianzas exitosas. Suponemos que los agentes que tienen perfiles más precisos de sus adversarios serán más exitosos en tales entornos. Tales agentes podrán predecir cuándo un miembro está a punto de violar el contrato de alianzas (elemento 2 en el modelo anterior) y tomar medidas correctivas (cuando el elemento 3 se falsificará). La robustez de la alianza depende en parte de la medida de confianza de sus miembros, la estimación de su posición objetiva y otras propiedades de su perfil. Debemos tener en cuenta que un agente puede formar parte simultáneamente de más de una alianza. Una alianza temporal, donde los miembros del grupo no tienen un objetivo común pero actúan colaborativamente en interés de sus propias metas individuales, es clasificada como un Grupo de Tratamiento por los psicólogos modernos (en contraste con un Grupo de Tarea, donde sus miembros tienen un objetivo común). El modelo de Actividad Compartida presentado en [5] modeló el comportamiento del Grupo de Tratamiento utilizando la misma formalización de Planes Compartidos. Al comparar ambas definiciones de una alianza y un Grupo de Tratamiento, encontramos una similitud sorprendente entre ambos modelos: las definiciones de los modelos de entorno son casi idénticas (ver las definiciones de SAs en [5]), y sus axiomas de Acto Egoísta y Acto Cooperativo se ajustan al comportamiento de nuestros agentes adversarios. La principal distinción entre ambos modelos es la integración de un axioma de acto de comportamiento útil, en la Actividad Compartida que no puede formar parte de la nuestra. Este axioma establece que un agente considerará tomar una acción que disminuirá su valor de Evaluación (hasta un cierto límite inferior), si cree que un compañero del grupo obtendrá un beneficio significativo. Dicho comportamiento no puede ocurrir en un entorno puramente adversarial (como un juego de suma cero), donde los miembros de la alianza están constantemente alerta para manipular su alianza en su propio beneficio. A6. Axioma de Maximización de la Evaluación. En un caso en el que todos los demás axiomas no sean aplicables, procederemos con la acción que maximice el valor heurístico calculado en la función Eval. (∀Aag, Ao ∈ A, α ∈ Cag, Tn, w ∈ W) Bel(Aag, (∀γ ∈ Cag)Eval(Aag, α, w) ≥ Eval(Aag, γ, w), Tn) ⇒ Pot.Int.To(Aag, α, Tn, Tα, w) T1. Optimalidad en Eval = Utilidad El modelo axiomático anterior aborda situaciones en las que la Utilidad es desconocida y los agentes son agentes racionales limitados. El siguiente teorema muestra que en interacciones bilaterales, donde los agentes tienen la función de Utilidad real (es decir, Eval = Utilidad) y son agentes racionales, los axiomas proporcionan el mismo resultado óptimo que la búsqueda adversarial clásica (por ejemplo, Min-Max). Teorema 1. Que Ae ag sea un agente AE racional ilimitado que utiliza la función de evaluación heurística Eval, Au ag sea el mismo agente utilizando la verdadera función de utilidad y Ao sea un único adversario racional ilimitado basado en utilidad. Dado que Eval = Utilidad: (∀α ∈ CAu ag , α ∈ CAe ag , Tn, w ∈ W) Pot.Int.To(Au ag, α, Tn, Tα, w) → Pot.Int.To(Ae ag, α , Tn, Tα, w) ∧ ((α = α ) ∨ (Utilidad(Au ag, α, w) = Eval(Ae ag, α , w))) Esquema de prueba - Dado que Au ag tiene la función de utilidad real y recursos ilimitados, puede generar el árbol de juego completo y ejecutar el algoritmo MinMax óptimo para elegir la acción de mayor valor de utilidad, que denotamos como α. La prueba mostrará que Ae ag, utilizando los axiomas AE, seleccionará la misma utilidad α o igual (cuando hay más de una acción con la misma utilidad máxima) cuando Eval = Utilidad. (A1) Axioma de logro de objetivo - supongamos que hay un α tal que su cumplimiento logrará el objetivo Au ags. Obtendrá la utilidad más alta mediante Min-Max para Au ag. El agente Ae seleccionará α u otra acción con el mismo valor de utilidad a través de A1. Si tal α no existe, Ae ag no puede aplicar este axioma y procede a A2. (A2) Axioma de la acción preventiva - (1) Observando el caso base (ver Prop1), si hay un β que lleva a Ao a alcanzar su objetivo, entonces una acción preventiva α producirá la mayor utilidad para Au ag. Au ag lo elegirá a través de la utilidad, mientras que Ae ag lo elegirá a través de A2. En el caso general, β es una acción altamente beneficiosa para Ao, lo que resulta en una baja utilidad para Au ag, lo que lo guiará a seleccionar un α que evitará β, mientras que Ae ag lo elegirá a través de A2.1 Si dicho β no existe para Ao, entonces A2 no es aplicable, y Ae ag puede proceder a A3. (A3) Axioma de movimiento táctico subóptimo: al usar una función de evaluación heurística, Ae ag tiene una creencia parcial en el perfil de su adversario (elemento 4 en el modelo AE), lo que puede llevarlo a creer en SetActions (Prop1). En nuestro caso, Ae ag tiene un perfil completo de su adversario óptimo y sabe que Ao se comportará de manera óptima según los valores reales de utilidad en el árbol de búsqueda completo, por lo tanto, cualquier creencia sobre una SetAction subóptima no puede existir, lo que hace que este axioma sea inaplicable. Ae ag procederá a A4. (A4) Axioma de detección de perfil - Dado que Ae ag tiene el perfil completo de Ao, ninguna de las acciones de Ae ag puede aumentar su conocimiento. Ese axioma no se aplicará, y el agente procederá con A6 (A5 será ignorado porque la interacción es bilateral). (A6) Axioma de maximización de la evaluación: este axioma seleccionará la máxima Evaluación para Ae ag. Dado que Eval = Utilidad, se seleccionará el mismo α que fue seleccionado por Au ag. 3. EVALUACIÓN El propósito principal de nuestro análisis experimental es evaluar el comportamiento y rendimiento de los modelos en un entorno adversarial real. Esta sección investiga si un caso acotado 1 en el que, tras la finalización de β, exista un γ que proporcione una alta utilidad para el Agente Au ag, no puede ocurrir porque Ao utiliza la misma utilidad, y la existencia de γ hará que clasifique a β como una acción de baja utilidad. 554 The Sixth Intl. En la Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07), los agentes racionales situados en entornos adversariales se beneficiarán más al aplicar nuestros axiomas de comportamiento sugeridos. 3.1 El Dominio Para explorar el uso del modelo anterior y sus axiomas de comportamiento, decidimos utilizar el juego de Conecta Cuatro como nuestro entorno adversarial. Connect-Four es un juego de 2 jugadores, de suma cero, que se juega utilizando un tablero similar a una matriz de 6x7. En cada turno, un jugador coloca un disco en una de las 7 columnas (el conjunto de 21 discos suele ser de color amarillo para el jugador 1 y rojo para el jugador 2; usaremos Blanco y Negro respectivamente para evitar confusiones). El ganador es el primer jugador en completar un conjunto de cuatro discos de su color en horizontal, vertical o diagonal. En ocasiones muy raras, el juego podría terminar en empate si todos los cuadros vacíos están llenos, pero ningún jugador logró crear un conjunto de 4 discos. El juego de Conecta Cuatro fue resuelto en [1], donde se muestra que el primer jugador (jugando con los discos blancos) puede forzar una victoria comenzando en la columna del medio (columna 4) y jugando de manera óptima. Sin embargo, la estrategia óptima es muy compleja y difícil de seguir incluso para agentes racionales limitados complejos, como los jugadores humanos. Antes de que podamos proceder a verificar el comportamiento del agente, primero debemos verificar que el dominio se ajusta a la definición de entornos adversarios mencionada anteriormente (en la que se basan los axiomas de comportamiento). Primero, al jugar al juego de Conecta Cuatro, el agente tiene la intención de ganar el juego (ítem 1). Segundo (ítem 2), nuestro agente cree que en Conecta Cuatro solo puede haber un ganador (o ningún ganador en la rara ocasión de un empate). Además, nuestro agente cree que su oponente en el juego intentará ganar (ítem 3), y esperamos que tenga algún conocimiento parcial (ítem 4) sobre su adversario (este conocimiento puede variar desde nada, pasando por hechos simples como la edad, hasta estrategias y debilidades). Por supuesto, no todos los encuentros de Conecta Cuatro son adversariales. Por ejemplo, cuando un padre está jugando el juego con su hijo, la siguiente situación podría ocurrir: el niño, con un fuerte incentivo para ganar, trata el entorno como adversarial (intenta ganar, entiende que solo puede haber un ganador y cree que su padre está tratando de vencerlo). Sin embargo, desde el punto de vista de los padres, el entorno podría ser visto como educativo, donde el objetivo no es ganar el juego, sino causar disfrute o practicar el razonamiento estratégico. En un entorno educativo como este, un nuevo conjunto de axiomas de comportamiento podría ser más beneficioso para los objetivos de los padres que nuestros axiomas de comportamiento adversarial sugeridos. 3.2 Análisis de Axiomas Después de demostrar que el juego de Conecta Cuatro es de hecho un entorno adversarial bilateral de suma cero, el siguiente paso es observar el comportamiento de los jugadores durante el juego y verificar si comportarse de acuerdo con nuestro modelo mejora el rendimiento. Para hacerlo, hemos recopilado archivos de registro de juegos de Conecta Cuatro completados que fueron jugados por jugadores humanos a través de Internet. Nuestros datos de archivo de registro recopilados provienen de sitios de Juego por Correo Electrónico (PBeM). Estos son sitios web que alojan juegos por correo electrónico, donde cada movimiento se realiza a través de un intercambio de correos electrónicos entre el servidor y los jugadores. Muchos de estos sitios de archivos contienen interacciones competitivas reales y también mantienen un sistema de clasificación para sus miembros. La mayoría de los datos que utilizamos se pueden encontrar en [6]. Como se puede aprender de [1], Conecta Cuatro tiene una estrategia óptima y una ventaja considerable para el jugador que comienza el juego (a quien llamamos jugador Blanco). Nos concentraremos en nuestro análisis en los movimientos de los segundos jugadores (a los que llamaremos Negras). El jugador Blanco, al ser el primero en actuar, tiene la llamada ventaja de la iniciativa. Tener la ventaja y una buena estrategia mantendrá al jugador Negro ocupado reaccionando a sus movimientos, en lugar de iniciar amenazas. Una amenaza es una combinación de tres discos del mismo color, con un espacio vacío para el cuarto disco ganador. Una amenaza abierta es una amenaza que puede materializarse en el próximo movimiento del oponente. Para que el jugador Negro gane, debe de alguna manera cambiar el rumbo, tomar la ventaja y comenzar a presentar amenazas al jugador Blanco. Exploraremos el comportamiento de los jugadores negros y su conformidad con nuestros axiomas. Para hacerlo, construimos una aplicación que lee archivos de registro y analiza los movimientos de los jugadores Negros. La aplicación contiene dos componentes principales: (1) un algoritmo Min-Max para la evaluación de movimientos; (2) un detector de amenazas abiertas para descubrir amenazas abiertas. El algoritmo Min-Max funcionará hasta una profundidad d dada y, para cada movimiento α, devolverá el valor heurístico para la próxima acción tomada por el jugador según lo escrito en el archivo de registro, h(α), junto con el valor heurístico máximo, maxh(α), que podría lograrse antes de realizar el movimiento (obviamente, si h(α) = maxh(α), entonces el jugador no realizó el movimiento óptimo desde el punto de vista heurístico). El trabajo de los detectores de amenazas es notificar si se tomó alguna acción para bloquear una amenaza abierta (no bloquear una amenaza abierta probablemente hará que el jugador pierda en el próximo movimiento del oponente). La función heurística utilizada por Min-Max para evaluar la utilidad de los jugadores es la siguiente función, que es simple de calcular, pero proporciona un desafío razonable a los oponentes humanos: Definición 8. Sea Grupo un conjunto adyacente de cuatro cuadrados que son horizontales, verticales o diagonales. El Grupo n (Grupo w) es un Grupo con n piezas de color negro (blanco) y 4−n casillas vacías. h = ((Grupo1 negro ∗α)+(Grupo2 negro ∗β)+(Grupo3 negro ∗γ)+(Grupo4 negro ∗∞)) − ((Grupo1 blanco ∗α)+(Grupo2 blanco ∗β)+(Grupo3 blanco ∗γ)+(Grupo4 blanco ∗∞)) Los valores de α, β y δ pueden variar para formar cualquier combinación lineal deseada; sin embargo, es importante asignarles valores teniendo en cuenta el orden α < β < δ (usamos 1, 4 y 8 como sus valores respectivos). Grupos de 4 discos del mismo color significan victoria, por lo tanto, el descubrimiento de dicho grupo resultará en ∞ para asegurar un valor extremo. Ahora utilizamos nuestra función de evaluación estimada para evaluar las acciones de los jugadores Negros durante la interacción adversarial de Conecta Cuatro. Cada juego del archivo de registro fue introducido en la aplicación, la cual procesó y generó un archivo de registro reformateado que contenía el valor h del movimiento actual, el valor maxh que podría ser alcanzado y una notificación si se detectaba una amenaza abierta. Un total de 123 partidas fueron analizadas (57 con victoria de las blancas y 66 con victoria de las negras). Algunos juegos adicionales fueron ignorados manualmente en el experimento, debido a estos problemas: un jugador abandonando el juego mientras el resultado no es definitivo, o una jugada irracional y contundente en las primeras etapas del juego (por ejemplo, no bloquear un grupo obviamente ganador en las primeras jugadas de apertura). Además, también se eliminó un empate. El simulador se ejecutó con una profundidad de búsqueda de 3 movimientos. Ahora procedemos a analizar los juegos con respecto a cada axioma de comportamiento. El Sexto Internacional. La siguiente sección presenta las evaluaciones heurísticas del algoritmo Min-Max para cada acción, y verifica la cantidad y el alcance de las acciones tácticas subóptimas y sus implicaciones en el rendimiento. La Tabla 1 muestra los resultados y las percepciones del análisis heurístico de los juegos, cuando la profundidad de búsqueda es igual a 3 (esta profundidad de búsqueda fue seleccionada para que los resultados sean comparables a [9], ver Sección 3.2.3). La información heurística de las tablas es la diferencia entre el valor heurístico máximo presente y el valor heurístico de la acción que finalmente tomó el jugador (es decir, cuanto más cercano sea el número a 0, más cercana estuvo la acción a la acción heurística máxima). La primera fila presenta los valores de diferencia de la acción que tuvo el valor de diferencia máximo entre todas las acciones de los jugadores Negros en un juego dado, promediado en todos los juegos ganados y perdidos por los Negros (ver columnas respectivas). En los juegos en los que el jugador Negro pierde, su valor promedio de diferencia fue de -17.62, mientras que en los juegos en los que el jugador Negro ganó, su promedio fue de -12.02. La segunda fila amplía el análisis al considerar las 3 acciones con mayor diferencia heurística, y promediarlas. En ese caso, notamos una diferencia heurística promedio de 5 puntos entre los juegos en los que el jugador Negro pierde y los juegos en los que gana. Sin embargo, la importancia de esos números es que nos permitieron hacer una suposición educada sobre un número umbral de 11.5, como el valor de la constante TrH, que diferencia entre acciones normales y altamente beneficiosas. Después de encontrar una constante TrH aproximada, podemos proceder con un análisis de la importancia de los movimientos subóptimos. Para hacerlo, tomamos el subconjunto de juegos en los que el valor mínimo de diferencia heurística para las acciones de las negras era de 11.5. Como se presenta en la Tabla 2, podemos ver el promedio de 3 horas mínimo de los 3 mayores rangos y el porcentaje respectivo de juegos ganados. La primera fila muestra que el jugador Negro ganó solo el 12% de los juegos en los que el promedio de sus 3 acciones con mayor diferencia heurística (min3 h) era menor que el umbral sugerido, TrH = 11.5. La segunda fila muestra un resultado sorprendente: parece que cuando min3 h > −4, el jugador Negro rara vez gana. La intuición sugeriría que los juegos en los que los valores de evaluación de la acción estén más cerca de los valores máximos resultarán en más juegos ganados para las piezas negras. Sin embargo, parece que en el dominio de Conecta Cuatro, simplemente responder con acciones algo fácilmente esperadas, sin iniciar algunos movimientos sorprendentes y subóptimos, no produce buenos resultados. La última fila resume las principales conclusiones del análisis; la mayoría de las victorias de Blacks (83%) se produjeron cuando su min3 h estaba en el rango de -11.5 a -4. Una inspección detallada de esos juegos ganados por las fichas negras muestra el siguiente patrón detrás de los números: después de los movimientos de apertura estándar, las fichas negras de repente colocan una ficha en una columna aislada, lo cual parece ser un desperdicio de un movimiento. El blanco sigue construyendo sus amenazas, generalmente ignorando el último movimiento de las negras, que a su vez utiliza el disco aislado como ancla para una futura amenaza ganadora. Los resultados muestran que fue beneficioso para el jugador Negro en la Tabla 2: Porcentajes de ganancias de los Negros % de juegos min3 h < −11.5 12% min3 h > −4 5% −11.5 ≤ min3 h ≤ −4 83% tomar acciones subóptimas y no dar el valor heurístico más alto posible en ese momento, pero no será demasiado perjudicial para su posición (es decir, no dará un alto valor beneficioso a su adversario). Resultó que aprender el umbral es un aspecto importante del éxito: realizar movimientos extremadamente arriesgados (min3 h < −11.5) o intentar evitarlos (min3 h > −4) reduce significativamente las posibilidades de ganar de los jugadores negros. 3.2.2 Afirmación del Axioma de Monitoreo de Perfiles En la tarea de demostrar la importancia de monitorear los perfiles de los adversarios, nuestros archivos de registro no pudieron ser utilizados porque no contenían interacciones repetidas entre jugadores, que son necesarias para inferir el conocimiento de los jugadores sobre sus adversarios. Sin embargo, la importancia del modelado del oponente y su uso para obtener ventajas tácticas ya ha sido estudiada en varios ámbitos ([3, 9] son buenos ejemplos). En un artículo reciente, Markovitch y Reger [9] exploraron la noción de aprendizaje y explotación de la debilidad del oponente en interacciones competitivas. Aplican estrategias de aprendizaje simples analizando ejemplos de interacciones pasadas en un dominio específico. También utilizaron el dominio adversarial de Conecta Cuatro, que ahora se puede utilizar para comprender la importancia de monitorear el perfil del adversario. Tras la presentación de su modelo teórico, describen un extenso <br>estudio empírico</br> y verifican el rendimiento de los agentes después de aprender el modelo de debilidades con ejemplos pasados. Uno de los dominios utilizados como entorno competitivo fue el mismo juego de Conecta Cuatro (las Damas fueron el segundo dominio). Su función heurística era idéntica a la nuestra con tres variaciones diferentes (H1, H2 y H3) que se distinguen entre sí por los valores de sus coeficientes de combinación lineal. La profundidad de búsqueda para los jugadores fue de 3 (como en nuestro análisis). Sus extensos experimentos verifican y comparan diversas estrategias de aprendizaje, factores de riesgo, conjuntos de características predefinidas y métodos de uso. La conclusión es que el dominio de Conecta Cuatro muestra una mejora de una tasa de victoria de 0.556 antes de modelar a 0.69 después de modelar (página 22). Sus conclusiones, que muestran un rendimiento mejorado al sostener y utilizar el modelo del adversario, justifican el esfuerzo de monitorear el perfil del adversario para interacciones continuas y repetidas. Un punto adicional que surgió en sus experimentos es el siguiente: después de que se haya aprendido el modelo de debilidad del oponente, los autores describen diferentes métodos para integrar el modelo de debilidad del oponente en la estrategia de decisión de los agentes. Sin embargo, independientemente del método específico que elijan para trabajar, todos los métodos de integración podrían hacer que el agente tome decisiones subóptimas; podría hacer que el agente prefiera acciones que son subóptimas en el momento actual de la decisión, pero que podrían hacer que el oponente reaccione de acuerdo con su modelo de debilidad (como lo representa nuestro agente), lo cual a su vez será beneficioso para nosotros en el futuro. El comportamiento de los agentes, como se demuestra en [9], confirma y refuerza aún más nuestro Axioma Táctico Subóptimo discutido en la sección anterior. 556 The Sixth Intl. La necesidad de los axiomas de Logro de Objetivos, Acto Preventivo y Maximización de la Evaluación es evidente y no requiere más verificación. Sin embargo, incluso con respecto a esos axiomas, surgieron algunas ideas interesantes en el análisis de registros. Los axiomas de Logro y Prevención de Actos, aunque teóricamente triviales, parecen presentar un desafío para un jugador humano. En la inspección inicial de los registros, nos encontramos con algunos juegos donde un jugador, por razones inexplicables, no bloqueó al otro para que ganara o no logró ejecutar su propia jugada ganadora. Podemos atribuir esas fallas a la falta de atención de los humanos, o a un error de escritura en su respuesta al movimiento; sin embargo, esos errores podrían ocurrir en agentes racionales limitados, y el comportamiento apropiado debe ser axiomatizado. Un juego típico de Cuatro en Línea gira en torno a generar amenazas y bloquearlas. En nuestro análisis buscamos acciones preventivas explícitas, es decir, movimientos que bloqueen un grupo de 3 discos o que eliminen una amenaza futura (en nuestro horizonte de búsqueda limitado). Encontramos que en el 83% de los juegos totales, hubo al menos una acción preventiva tomada por el jugador Negro. También se encontró que Black promedió 2.8 acciones preventivas por juego en los juegos en los que perdió, mientras que promedió 1.5 acciones preventivas por juego al ganar. Parece que Black necesita 1 o 2 acciones preventivas para construir su posición inicial de toma, antes de comenzar a presentar amenazas. Si no logra ganar, generalmente evitará una amenaza adicional o dos antes de sucumbir ante el Blanco. 4. TRABAJO RELACIONADO Mucha investigación se ocupa de la axiomatización del trabajo en equipo y los estados mentales de los individuos: algunos modelos utilizan el conocimiento y la creencia [10], otros tienen modelos de metas e intenciones [8, 4]. Sin embargo, todas estas teorías formales tratan sobre el trabajo en equipo y la cooperación de agentes. Hasta donde sabemos, nuestro modelo es el primero en proporcionar un modelo formalizado para entornos adversarios explícitos y el comportamiento de los agentes en él. El algoritmo de búsqueda adversarial clásico Min-Max fue el primer intento de integrar al oponente en el espacio de búsqueda con la suposición débil de un oponente que juega de manera óptima. Desde entonces, se ha dedicado mucho esfuerzo a integrar el modelo del oponente en el procedimiento de toma de decisiones para predecir el comportamiento futuro. El algoritmo M∗ presentado por Carmel y Markovitch [2] mostró un método para incorporar modelos de oponentes en la búsqueda de adversarios, mientras que en [3] utilizaron el aprendizaje para proporcionar un modelo de oponente más preciso en un entorno de juego repetido de 2 jugadores, donde las estrategias de los agentes se modelaron como autómatas finitos. El trabajo adicional de planificación adversarial fue realizado por Willmott y colaboradores [13], quienes proporcionaron un enfoque de planificación adversarial para el juego de GO. La investigación mencionada anteriormente trató sobre la búsqueda adversarial y la integración de modelos de oponentes en métodos de búsqueda de utilidad clásicos. Ese trabajo muestra la importancia del modelado del oponente y la capacidad de explotarlo en beneficio de un agente. Sin embargo, las limitaciones básicas de esos métodos de búsqueda siguen aplicándose; nuestro modelo intenta superar esas limitaciones presentando un modelo formal para una nueva especificación adversarial basada en estados mentales. 5. CONCLUSIONES Presentamos un modelo de Entorno Adversarial para un agente racional acotado que está situado en un entorno de N jugadores, de suma cero. Estos fueron posteriormente eliminados del análisis final. Utilizamos la formalización de SharedPlans para definir el modelo y los axiomas que los agentes pueden aplicar como pautas de comportamiento. El modelo está destinado a ser utilizado como una guía para diseñar agentes que necesitan operar en entornos adversos. Presentamos resultados empíricos, basados en el análisis del archivo de registro de ConnectFour, que ejemplifican el modelo y los axiomas para una instancia bilateral del entorno. Los resultados que presentamos son un primer paso hacia un modelo ampliado que abarcará todos los tipos de entornos adversarios, por ejemplo, entornos que no son de suma cero y entornos que contienen agentes naturales que no forman parte del conflicto directo. Esos desafíos y más serán abordados en futuras investigaciones. 6. AGRADECIMIENTO Esta investigación fue apoyada en parte por las becas de la Fundación de Ciencia de Israel #1211/04 y #898/05. 7. REFERENCIAS [1] L. V. Allis. Un enfoque basado en el conocimiento de Conecta Cuatro: el juego está resuelto, gana el jugador Blanco. Tesis de maestría, Universidad Libre de Ámsterdam, Países Bajos, 1988. [2] D. Carmel y S. Markovitch. Incorporando modelos de oponentes en la búsqueda de adversarios. En Actas de la Decimotercera Conferencia Nacional de Inteligencia Artificial, páginas 120-125, Portland, Oregón, 1996. [3] D. Carmel y S. Markovitch. Modelado de oponentes en sistemas multiagente. En G. Weiß y S. Sen, editores, Adaptación y Aprendizaje en Sistemas Multiagente, páginas 40-52. Springer-Verlag, 1996. [4] B. J. Grosz y S. Kraus. Planes colaborativos para acciones grupales complejas. Inteligencia Artificial, 86(2):269-357, 1996. [5] M. Hadad, G. Kaminka, G. Armon y S. Kraus. Apoyando la actividad colaborativa. En Proc. de AAAI-2005, páginas 83-88, Pittsburgh, 2005. [6] http://www.gamerz.net/˜pbmserv/. [7] S. Kraus y D. Lehmann. Diseñando y construyendo un agente automatizado de negociación. Inteligencia Computacional, 11:132-171, 1995. [8] H. J. Levesque, P. R. Cohen y J. H. T. Nunes. Actuar juntos. En Proc. de AAAI-90, páginas 94-99, Boston, MA, 1990. [9] S. Markovitch y R. Reger. Aprender y aprovechar las debilidades relativas de los agentes oponentes. Agentes Autónomos y Sistemas Multiagente, 10(2):103-130, 2005. [10] Y. M. Ronald Fagin, Joseph Y. Halpern y M. Y. Vardi. Razonamiento sobre el conocimiento. MIT Press, Cambridge, Mass., 1995. [11] P. Thagard.\nMIT Press, Cambridge, Massachusetts, 1995. [11] P. Thagard. Resolución de problemas adversariales: Modelado de un oponente utilizando coherencia explicativa. Ciencia Cognitiva, 16(1):123-149, 1992. [12] R. W. Toseland y R. F. Rivas. Una introducción a la práctica del trabajo en grupo. Prentice Hall, Englewood Cliffs, NJ, segunda edición, 1995. [13] S. Willmott, J. Richardson, A. Bundy y J. Levine. Un enfoque de planificación adversarial para el juego de Go. Notas de conferencia en Ciencias de la Computación, 1558:93-112, 1999. El Sexto Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 557 ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "axiomatized model": {
            "translated_key": "modelo formal y axiomatizado",
            "is_in_text": true,
            "original_annotated_sentences": [
                "An Adversarial Environment Model for Bounded Rational Agents in Zero-Sum Interactions Inon Zuckerman1 , Sarit Kraus1 , Jeffrey S. Rosenschein2 , Gal Kaminka1 1 Department of Computer Science 2 The School of Engineering Bar-Ilan University and Computer Science Ramat-Gan, Israel Hebrew University, Jerusalem, Israel {zukermi,sarit,galk}@cs.biu.ac.il jeff@cs.huji.ac.il ABSTRACT Multiagent environments are often not cooperative nor collaborative; in many cases, agents have conflicting interests, leading to adversarial interactions.",
                "This paper presents a formal Adversarial Environment model for bounded rational agents operating in a zero-sum environment.",
                "In such environments, attempts to use classical utility-based search methods can raise a variety of difficulties (e.g., implicitly modeling the opponent as an omniscient utility maximizer, rather than leveraging a more nuanced, explicit opponent model).",
                "We define an Adversarial Environment by describing the mental states of an agent in such an environment.",
                "We then present behavioral axioms that are intended to serve as design principles for building such adversarial agents.",
                "We explore the application of our approach by analyzing log files of completed Connect-Four games, and present an empirical analysis of the axioms appropriateness.",
                "Categories and Subject Descriptors I.2.11 [Artificial Intelligence]: Distributed Artificial Intelligence-Intelligent agents,Multiagent Systems; I.2.4 [Artificial Intelligence]: Knowledge Representation Formalisms and Methods -Modal logic General Terms Design, Theory 1.",
                "INTRODUCTION Early research in multiagent systems (MAS) considered cooperative groups of agents; because individual agents had limited resources, or limited access to information (e.g., limited processing power, limited sensor coverage), they worked together by design to solve problems that individually they could not solve, or at least could not solve as efficiently.",
                "MAS research, however, soon began to consider interacting agents with individuated interests, as representatives of different humans or organizations with non-identical interests.",
                "When interactions are guided by diverse interests, participants may have to overcome disagreements, uncooperative interactions, and even intentional attempts to damage one another.",
                "When these types of interactions occur, environments require appropriate behavior from the agents situated in them.",
                "We call these environments Adversarial Environments, and call the clashing agents Adversaries.",
                "Models of cooperation and teamwork have been extensively explored in MAS through the axiomatization of mental states (e.g., [8, 4, 5]).",
                "However, none of this research dealt with adversarial domains and their implications for agent behavior.",
                "Our paper addresses this issue by providing a formal, axiomatized mental state model for a subset of adversarial domains, namely simple zero-sum adversarial environments.",
                "Simple zero-sum encounters exist of course in various twoplayer games (e.g., Chess, Checkers), but they also exist in n-player games (e.g., Risk, Diplomacy), auctions for a single good, and elsewhere.",
                "In these latter environments especially, using a utility-based adversarial search (such as the Min-Max algorithm) does not always provide an adequate solution; the payoff function might be quite complex or difficult to quantify, and there are natural computational limitations on bounded rational agents.",
                "In addition, traditional search methods (like Min-Max) do not make use of a model of the opponent, which has proven to be a valuable addition to adversarial planning [9, 3, 11].",
                "In this paper, we develop a formal, <br>axiomatized model</br> for bounded rational agents that are situated in a zero-sum adversarial environment.",
                "The model uses different modality operators, and its main foundations are the SharedPlans [4] model for collaborative behavior.",
                "We explore environment properties and the mental states of agents to derive behavioral axioms; these behavioral axioms constitute a formal model that serves as a specification and design guideline for agent design in such settings.",
                "We then investigate the behavior of our model empirically using the Connect-Four board game.",
                "We show that this game conforms to our environment definition, and analyze players behavior using a large set of completed match log 550 978-81-904262-7-5 (RPS) c 2007 IFAAMAS files.",
                "In addition, we use the results presented in [9] to discuss the importance of opponent modeling in our ConnectFour adversarial domain.",
                "The paper proceeds as follows.",
                "Section 2 presents the models formalization.",
                "Section 3 presents the empirical analysis and its results.",
                "We discuss related work in Section 4, and conclude and present future directions in Section 5. 2.",
                "ADVERSARIAL ENVIRONMENTS The adversarial environment model (denoted as AE) is intended to guide the design of agents by providing a specification of the capabilities and mental attitudes of an agent in an adversarial environment.",
                "We focus here on specific types of adversarial environments, specified as follows: 1.",
                "Zero-Sum Interactions: positive and negative utilities of all agents sum to zero; 2.",
                "Simple AEs: all agents in the environment are adversarial agents; 3.",
                "Bilateral AEs: AEs with exactly two agents; 4.",
                "Multilateral AEs: AEs of three or more agents.",
                "We will work on both bilateral and multilateral instantiations of zero-sum and simple environments.",
                "In particular, our adversarial environment model will deal with interactions that consist of N agents (N ≥ 2), where all agents are adversaries, and only one agent can succeed.",
                "Examples of such environments range from board games (e.g., Chess, Connect-Four, and Diplomacy) to certain economic environments (e.g., N-bidder auctions over a single good). 2.1 Model Overview Our approach is to formalize the mental attitudes and behaviors of a single adversarial agent; we consider how a single agent perceives the AE.",
                "The following list specifies the conditions and mental states of an agent in a simple, zero-sum AE: 1.",
                "The agent has an individual intention that its own goal will be completed; 2.",
                "The agent has an individual belief that it and its adversaries are pursuing full conflicting goals (defined below)there can be only one winner; 3.",
                "The agent has an individual belief that each adversary has an intention to complete its own full conflicting goal; 4.",
                "The agent has an individual belief in the (partial) profile of its adversaries.",
                "Item 3 is required, since it might be the case that some agent has a full conflicting goal, and is currently considering adopting the intention to complete it, but is, as of yet, not committed to achieving it.",
                "This might occur because the agent has not yet deliberated about the effects that adopting that intention might have on the other intentions it is currently holding.",
                "In such cases, it might not consider itself to even be in an adversarial environment.",
                "Item 4 states that the agent should hold some belief about the profiles of its adversaries.",
                "The profile represents all the knowledge the agent has about its adversary: its weaknesses, strategic capabilities, goals, intentions, trustworthiness, and more.",
                "It can be given explicitly or can be learned from observations of past encounters. 2.2 Model Definitions for Mental States We use Grosz and Krauss definitions of the modal operators, predicates, and meta-predicates, as defined in their SharedPlan formalization [4].",
                "We recall here some of the predicates and operators that are used in that formalization: Int.To(Ai, α, Tn, Tα, C) represents Ais intentions at time Tn to do an action α at time Tα in the context of C. Int.Th(Ai, prop, Tn, Tprop, C) represents Ais intentions at time Tn that a certain proposition prop holds at time Tprop in the context of C. The potential intention operators, Pot.Int.To(...) and Pot.Int.Th(...), are used to represent the mental state when an agent considers adopting an intention, but has not deliberated about the interaction of the other intentions it holds.",
                "The operator Bel(Ai, f, Tf ) represents agent Ai believing in the statement expressed in formula f, at time Tf .",
                "MB(A, f, Tf ) represents mutual belief for a group of agents A.",
                "A snapshot of the system finds our environment to be in some state e ∈ E of environmental variable states, and each adversary in any LAi ∈ L of possible local states.",
                "At any given time step, the system will be in some world w of the set of all possible worlds w ∈ W, where w = E×LA1 ×LA2 × ...LAn , and n is the number of adversaries.",
                "For example, in a Texas Holdem poker game, an agents local state might be its own set of cards (which is unknown to its adversary) while the environment will consist of the betting pot and the community cards (which are visible to both players).",
                "A utility function under this formalization is defined as a mapping from a possible world w ∈ W to an element in , which expresses the desirability of the world, from a single agent perspective.",
                "We usually normalize the range to [0,1], where 0 represents the least desirable possible world, and 1 is the most desirable world.",
                "The implementation of the utility function is dependent on the domain in question.",
                "The following list specifies new predicates, functions, variables, and constants used in conjunction with the original definitions for the adversarial environment formalization: 1. φ is a null action (the agent does not do anything). 2.",
                "GAi is the set of agent Ais goals.",
                "Each goal is a set of predicates whose satisfaction makes the goal complete (we use G∗ Ai ∈ GAi to represent an arbitrary goal of agent Ai). 3. gAi is the set of agent Ais subgoals.",
                "Subgoals are predicates whose satisfaction represents an important milestone toward achievement of the full goal. gG∗ Ai ⊆ gAi is the set of subgoals that are important to the completion of goal G∗ Ai (we will use g∗ G∗ Ai ∈ gG∗ Ai to represent an arbitrary subgoal). 4.",
                "P Aj Ai is the profile object agent Ai holds about agent Aj. 5.",
                "CA is a general set of actions for all agents in A which are derived from the environments constraints.",
                "CAi ⊆ CA is the set of agent Ais possible actions. 6.",
                "Do(Ai, α, Tα, w) holds when Ai performs action α over time interval Tα in world w. 7.",
                "Achieve(G∗ Ai , α, w) is true when goal G∗ Ai is achieved following the completion of action α in world w ∈ W, where α ∈ CAi . 8.",
                "Profile(Ai, PAi Ai ) is true when agent Ai holds an object profile for agent Aj.",
                "Definition 1.",
                "Full conflict (FulConf ) describes a zerosum interaction where only a single goal of the goals in conflict can be completed.",
                "FulConf(G∗ Ai , G∗ Aj ) ⇒ (∃α ∈ CAi , ∀w, β ∈ CAj ) (Achieve(G∗ Ai , α, w) ⇒ ¬Achieve(G∗ Aj , β, w)) ∨ (∃β ∈ CAj , ∀w, α ∈ CAi )(Achieve(G∗ Aj , β, w) ⇒ ¬Achieve(G∗ Ai , α, w)) Definition 2.",
                "Adversarial Knowledge (AdvKnow) is a function returning a value which represents the amount of The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 551 knowledge agent Ai has on the profile of agent Aj, at time Tn.",
                "The higher the value, the more knowledge agent Ai has.",
                "AdvKnow : P Aj Ai × Tn → Definition 3.",
                "Eval - This evaluation function returns an estimated expected utility value for an agent in A, after completing an action from CA in some world state w. Eval : A × CA × w → Definition 4.",
                "TrH - (Threshold) is a numerical constant in the [0,1] range that represents an evaluation function (Eval) threshold value.",
                "An action that yields an estimated utility evaluation above the TrH is regarded as a highly beneficial action.",
                "The Eval value is an estimation and not the real utility function, which is usually unknown.",
                "Using the real utility value for a rational agent would easily yield the best outcome for that agent.",
                "However, agents usually do not have the real utility functions, but rather a heuristic estimate of it.",
                "There are two important properties that should hold for the evaluation function: Property 1.",
                "The evaluation function should state that the most desirable world state is one in which the goal is achieved.",
                "Therefore, after the goal has been satisfied, there can be no future action that can put the agent in a world state with higher Eval value. (∀Ai, G∗ Ai , α, β ∈ CAi , w ∈ W) Achieve(G∗ Ai , α, w) ⇒ Eval(Ai, α, w) ≥ Eval(Ai, β, w) Property 2.",
                "The evaluation function should project an action that causes a completion of a goal or a subgoal to a value which is greater than TrH (a highly beneficial action). (∀Ai, G∗ Ai ∈ GAi , α ∈ CAi , w ∈ W, g∗ GAi ∈ gGAi ) Achieve(G∗ Ai , α, w) ∨ Achieve(g∗ GAi , α, w) ⇒ Eval(Ai, α, w) ≥ TrH.",
                "Definition 5.",
                "SetAction We define a set action (SetAction) as a set of action operations (either complex or basic actions) from some action sets CAi and CAj which, according to agent Ais belief, are attached together by a temporal and consequential relationship, forming a chain of events (action, and its following consequent action). (∀α1 , . . . , αu ∈ CAi , β1 , . . . , βv ∈ CAj , w ∈ W) SetAction(α1 , . . . , αu , β1 , . . . , βv , w) ⇒ ((Do(Ai, α1 , Tα1 , w) ⇒ Do(Aj, β1 , Tβ1 , w)) ⇒ Do(Ai, α2 , Tα2 , w) ⇒ . . . ⇒ Do(Ai, αu , Tαu , w)) The consequential relation might exist due to various environmental constraints (when one action forces the adversary to respond with a specific action) or due to the agents knowledge about the profile of its adversary.",
                "Property 3.",
                "As the knowledge we have about our adversary increases we will have additional beliefs about its behavior in different situations which in turn creates new set actions.",
                "Formally, if our AdvKnow at time Tn+1 is greater than AdvKnow at time Tn, then every SetAction known at time Tn is also known at time Tn+1.",
                "AdvKnow(P Aj Ai , Tn+1) > AdvKnow(P Aj Ai , Tn) ⇒ (∀α1 , . . . , αu ∈ CAi , β1 , . . . , βv ∈ CAj ) Bel(Aag, SetAction(α1 , . . . , αu , β1 , . . . , βv ), Tn) ⇒ Bel(Aag, SetAction(α1 , . . . , αu , β1 , . . . , βv ), Tn+1) 2.3 The Environment Formulation The following axioms provide the formal definition for a simple, zero-sum Adversarial Environment (AE).",
                "Satisfaction of these axioms means that the agent is situated in such an environment.",
                "It provides specifications for agent Aag to interact with its set of adversaries A with respect to goals G∗ Aag and G∗ A at time TCo at some world state w. AE(Aag, A, G∗ Aag , A1, . . . , Ak, G∗ A1 , . . . , G∗ Ak , Tn, w) 1.",
                "Aag has an Int.Th his goal would be completed: (∃α ∈ CAag , Tα) Int.Th(Aag, Achieve(G∗ Aag , α), Tn, Tα, AE) 2.",
                "Aag believes that it and each of its adversaries Ao are pursuing full conflicting goals: (∀Ao ∈ {A1, . . . , Ak}) Bel(Aag, FulConf(G∗ Aag , G∗ Ao ), Tn) 3.",
                "Aag believes that each of his adversaries in Ao has the Int.Th his conflicting goal G∗ Aoi will be completed: (∀Ao ∈ {A1, . . . , Ak})(∃β ∈ CAo , Tβ) Bel(Aag, Int.Th(Ao, Achieve(G∗ Ao , β), TCo, Tβ, AE), Tn) 4.",
                "Aag has beliefs about the (partial) profiles of its adversaries (∀Ao ∈ {A1, . . . , Ak}) (∃PAo Aag ∈ PAag )Bel(Aag, Profile(Ao, PAo Aag ), Tn) To build an agent that will be able to operate successfully within such an AE, we must specify behavioral guidelines for its interactions.",
                "Using a naive Eval maximization strategy to a certain search depth will not always yield satisfactory results for several reasons: (1) the search horizon problem when searching for a fixed depth; (2) the strong assumption of an optimally rational, unbounded resources adversary; (3) using an estimated evaluation function which will not give optimal results in all world states, and can be exploited [9].",
                "The following axioms specify the behavioral principles that can be used to differentiate between successful and less successful agents in the above Adversarial Environment.",
                "Those axioms should be used as specification principles when designing and implementing agents that should be able to perform well in such Adversarial Environments.",
                "The behavioral axioms represent situations in which the agent will adopt potential intentions to (Pot.Int.To(...)) perform an action, which will typically require some means-end reasoning to select a possible course of action.",
                "This reasoning will lead to the adoption of an Int.To(...) (see [4]).",
                "A1.",
                "Goal Achieving Axiom.",
                "The first axiom is the simplest case; when the agent Aag believes that it is one action (α) away from achieving his conflicting goal G∗ Aag , it should adopt the potential intention to do α and complete its goal. (∀Aag, α ∈ CAag , Tn, Tα, w ∈ W) (Bel(Aag, Do(Aag, α, Tα, w) ⇒ Achieve(G∗ Aag , α, w)) ⇒ Pot.Int.To(Aag, α, Tn, Tα, w) This somewhat trivial behavior is the first and strongest axiom.",
                "In any situation, when the agent is an action away from completing the goal, it should complete the action.",
                "Any fair Eval function would naturally classify α as the maximal value action (property 1).",
                "However, without explicit axiomatization of such behavior there might be situations where the agent will decide on taking another action for various reasons, due to its bounded decision resources.",
                "A2.",
                "Preventive Act Axiom.",
                "Being in an adversarial situation, agent Aag might decide to take actions that will damage one of its adversarys plans to complete its goal, even if those actions do not explicitly advance Aag towards its conflicting goal G∗ Aag .",
                "Such preventive action will take place when agent Aag has a belief about the possibility of its adversary Ao doing an action β that will give it a high 552 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) utility evaluation value (> TrH).",
                "Believing that taking action α will prevent the opponent from doing its β, it will adopt a potential intention to do α. (∀Aag, Ao ∈ A, α ∈ CAag , β ∈ CAo , Tn, Tβ, w ∈ W) (Bel(Aag, Do(Ao, β, Tβ, w) ∧ Eval(Ao, β, w) > TrH, Tn) ∧ Bel(Aag, Do(Aag, α, Tα, w) ⇒ ¬Do(Ao, β, Tβ, w), Tn) ⇒ Pot.Int.To(Aag, α, Tn, Tα, w) This axiom is a basic component of any adversarial environment.",
                "For example, looking at a Chess board game, a player could realize that it is about to be checkmated by its opponent, thus making a preventive move.",
                "Another example is a Connect Four game: when a player has a row of three chips, its opponent must block it, or lose.",
                "A specific instance of A1 occurs when the adversary is one action away from achieving its goal, and immediate preventive action needs to be taken by the agent.",
                "Formally, we have the same beliefs as stated above, with a changed belief that doing action β will cause agent Ao to achieve its goal.",
                "Proposition 1: Prevent or lose case. (∀Aag, Ao ∈ A, α ∈ CAag , β ∈ CAo , G∗ Ao , Tn, Tα, Tβ, w ∈ W) Bel(Aag, Do(Ao, β, Tβ, w) ⇒ Achieve(G∗ Ao , β, w), Tn) ∧ Bel(Aag, Do(Aag, α, Tα, w) ⇒ ¬Do(Ao, β, Tβ, w)) ⇒ Pot.Int.To(Aag, α, Tn, Tα, w) Sketch of proof: Proposition 1 can be easily derived from axiom A1 and the property 2 of the Eval function, which states that any action that causes a completion of a goal is a highly beneficial action.",
                "The preventive act behavior will occur implicitly when the Eval function is equal to the real world utility function.",
                "However, being bounded rational agents and dealing with an estimated evaluation function we need to explicitly axiomatize such behavior, for it will not always occur implicitly from the evaluation function.",
                "A3.",
                "Suboptimal Tactical Move Axiom.",
                "In many scenarios a situation may occur where an agent will decide not to take the current most beneficial action it can take (the action with the maximal utility evaluation value), because it believes that taking another action (with lower utility evaluation value) might yield (depending on the adversarys response) a future possibility for a highly beneficial action.",
                "This will occur most often when the Eval function is inaccurate and differs by a large extent from the Utility function.",
                "Put formally, agent Aag believes in a certain SetAction that will evolve according to its initial action and will yield a high beneficial value (> TrH) solely for it. (∀Aag, Ao ∈ A, Tn, w ∈ W) (∃α1 , . . . , αu ∈ CAi , β1 , . . . , βv ∈ CAj , Tα1 ) Bel(Aag, SetAction(α1 , . . . , αu , β1 , . . . , βv ), Tn) ∧ Bel(Aag, Eval(Ao, βv , w) < TrH < Eval(Aag, αu , w), Tn) ⇒ Pot.Int.To(Aag, α1 , Tn, Tα1 , w) An agent might believe that a chain of events will occur for various reasons due to the inevitable nature of the domain.",
                "For example, in Chess, we often observe the following: a move causes a check position, which in turn limits the opponents moves to avoiding the check, to which the first player might react with another check, and so on.",
                "The agent might also believe in a chain of events based on its knowledge of its adversarys profile, which allows it to foresee the adversarys movements with high accuracy.",
                "A4.",
                "Profile Detection Axiom.",
                "The agent can adjust its adversarys profiles by observations and pattern study (specifically, if there are repeated encounters with the same adversary).",
                "However, instead of waiting for profile information to be revealed, an agent can also initiate actions that will force its adversary to react in a way that will reveal profile knowledge about it.",
                "Formally, the axiom states that if all actions (γ) are not highly beneficial actions (< TrH), the agent can do action α in time Tα if it believes that it will result in a non-highly beneficial action β from its adversary, which in turn teaches it about the adversarys profile, i.e., gives a higher AdvKnow(P Aj Ai , Tβ). (∀Aag, Ao ∈ A, α ∈ CAag , β ∈ CAo , Tn, Tα, Tβ, w ∈ W) Bel(Aag, (∀γ ∈ CAag )Eval(Aag, γ, w) < TrH, Tn) ∧ Bel(Aag, Do(Aag, α, Tα, w) ⇒ Do(Ao, β, Tβ, w), Tn) ∧ Bel(Aag, Eval(Ao, β, w) < TrH) ∧ Bel(Aag, AdvKnow(P Aj Ai , Tβ) > AdvKnow(P Aj Ai , Tn), Tn) ⇒ Pot.Int.To(Aag, α, Tn, Tα, w) For example, going back to the Chess board game scenario, consider starting a game versus an opponent about whom we know nothing, not even if it is a human or a computerized opponent.",
                "We might start playing a strategy that will be suitable versus an average opponent, and adjust our game according to its level of play.",
                "A5.",
                "Alliance Formation Axiom The following behavioral axiom is relevant only in a multilateral instantiation of the adversarial environment (obviously, an alliance cannot be formed in a bilateral, zero-sum encounter).",
                "In different situations during a multilateral interaction, a group of agents might believe that it is in their best interests to form a temporary alliance.",
                "Such an alliance is an agreement that constrains its members behavior, but is believed by its members to enable them to achieve a higher utility value than the one achievable outside of the alliance.",
                "As an example, we can look at the classical Risk board game, where each player has an individual goal of being the sole conquerer of the world, a zero-sum game.",
                "However, in order to achieve this goal, it might be strategically wise to make short-term ceasefire agreements with other players, or to join forces and attack an opponent who is stronger than the rest.",
                "An alliances terms defines the way its members should act.",
                "It is a set of predicates, denoted as Terms, that is agreed upon by the alliance members, and should remain true for the duration of the alliance.",
                "For example, the set Terms in the Risk scenario, could contain the following predicates: 1.",
                "Alliance members will not attack each other on territories X, Y and Z; 2.",
                "Alliance members will contribute C units per turn for attacking adversary Ao; 3.",
                "Members are obligated to stay as part of the alliance until time Tk or until adversarys Ao army is smaller than Q.",
                "The set Terms specifies inter-group constraints on each of the alliance members (∀Aal i ∈ Aal ⊆ A) set of actions Cal i ⊆ C. Definition 6.",
                "Al val - the total evaluation value that agent Ai will achieve while being part of Aal is the sum of Evali (Eval for Ai) of each of Aal j Eval values after taking their own α actions (via the agent(α) predicate): Al val(Ai, Cal , Aal , w) = α∈Cal Evali(Aal j , agent(α), w) Definition 7.",
                "Al TrH - is a number representing an Al val The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 553 threshold; above it, the alliance can be said to be a highly beneficial alliance.",
                "The value of Al TrH will be calculated dynamically according to the progress of the interaction, as can be seen in [7].",
                "After an alliance is formed, its members are now working in their normal adversarial environment, as well as according to the mental states and axioms required for their interactions as part of the alliance.",
                "The following Alliance model (AL) specifies the conditions under which the group Aal can be said to be in an alliance and working with a new and constrained set of actions Cal , at time Tn.",
                "AL(Aal , Cal , w, Tn) 1.",
                "Aal has a MB that all members are part of Aal : MB(Aal , (∀Aal i ∈ Aal )member(Aal i , Aal ), Tn) 2.",
                "Aal has a MB that the group be maintained: MB(Aal , (∀Aal i ∈ Aal )Int.Th (Ai, member(Ai, Aal ), Tn, Tn+1, Co), Tn) 3.",
                "Aal has a MB that being members gives them high utility value: MB(Aal , (∀Aal i ∈ Aal )Al val(Aal i , Cal , Aal , w) ≥ Al TrH, Tn) Members profiles are a crucial part of successful alliances.",
                "We assume that agents that have more accurate profiles of their adversaries will be more successful in such environments.",
                "Such agents will be able to predict when a member is about to breach the alliances contract (item 2 in the above model), and take counter measures (when item 3 will falsify).",
                "The robustness of the alliance is in part a function of its members trustfulness measure, objective position estimation, and other profile properties.",
                "We should note that an agent can simultaneously be part of more than one alliance.",
                "Such a temporary alliance, where the group members do not have a joint goal but act collaboratively for the interest of their own individual goals, is classified as a Treatment Group by modern psychologists [12] (in contrast to a Task Group, where its members have a joint goal).",
                "The Shared Activity model as presented in [5] modeled Treatment Group behavior using the same SharedPlans formalization.",
                "When comparing both definitions of an alliance and a Treatment Group we found an unsurprising resemblance between both models: the environment models definitions are almost identical (see SAs definitions in [5]), and their Selfish-Act and Cooperative Act axioms conform to our adversarial agents behavior.",
                "The main distinction between both models is the integration of a Helpful-behavior act axiom, in the Shared Activity which cannot be part of ours.",
                "This axiom states that an agent will consider taking action that will lower its Eval value (to a certain lower bound), if it believes that a group partner will gain a significant benefit.",
                "Such behavior cannot occur in a pure adversarial environment (as a zero-sum game is), where the alliance members are constantly on watch to manipulate their alliance to their own advantage.",
                "A6.",
                "Evaluation Maximization Axiom.",
                "In a case when all other axioms are inapplicable, we will proceed with the action that maximizes the heuristic value as computed in the Eval function. (∀Aag, Ao ∈ A, α ∈ Cag, Tn, w ∈ W) Bel(Aag, (∀γ ∈ Cag)Eval(Aag, α, w) ≥ Eval(Aag, γ, w), Tn) ⇒ Pot.Int.To(Aag, α, Tn, Tα, w) T1.",
                "Optimality on Eval = Utility The above axiomatic model handles situations where the Utility is unknown and the agents are bounded rational agents.",
                "The following theorem shows that in bilateral interactions, where the agents have the real Utility function (i.e., Eval = Utility) and are rational agents, the axioms provide the same optimal result as classic adversarial search (e.g., Min-Max).",
                "Theorem 1.",
                "Let Ae ag be an unbounded rational AE agent using the Eval heuristic evaluation function, Au ag be the same agent using the true Utility function, and Ao be a sole unbounded utility-based rational adversary.",
                "Given that Eval = Utility: (∀α ∈ CAu ag , α ∈ CAe ag , Tn, w ∈ W) Pot.Int.To(Au ag, α, Tn, Tα, w) → Pot.Int.To(Ae ag, α , Tn, Tα, w) ∧ ((α = α ) ∨ (Utility(Au ag, α, w) = Eval(Ae ag, α , w))) Sketch of proof - Given that Au ag has the real utility function and unbounded resources, it can generate the full game tree and run the optimal MinMax algorithm to choose the highest utility value action, which we denote by, α.",
                "The proof will show that Ae ag, using the AE axioms, will select the same or equal utility α (when there is more than one action with the same max utility) when Eval = Utility. (A1) Goal achieving axiom - suppose there is an α such that its completion will achieve Au ags goal.",
                "It will obtain the highest utility by Min-Max for Au ag.",
                "The Ae ag agent will select α or another action with the same utility value via A1.",
                "If such α does not exist, Ae ag cannot apply this axiom, and proceeds to A2. (A2) Preventive act axiom - (1) Looking at the basic case (see Prop1), if there is a β which leads Ao to achieve its goal, then a preventive action α will yield the highest utility for Au ag.",
                "Au ag will choose it through the utility, while Ae ag will choose it through A2. (2) In the general case, β is a highly beneficial action for Ao, thus yields low utility for Au ag, which will guide it to select an α that will prevent β, while Ae ag will choose it through A2.1 If such β does not exist for Ao, then A2 is not applicable, and Ae ag can proceed to A3. (A3) Suboptimal tactical move axiom - When using a heuristic Eval function, Ae ag has a partial belief in the profile of its adversary (item 4 in AE model), which may lead it to believe in SetActions (Prop1).",
                "In our case, Ae ag is holding a full profile on its optimal adversary and knows that Ao will behave optimally according to the real utility values on the complete search tree, therefore, any belief about suboptimal SetAction cannot exist, yielding this axiom inapplicable.",
                "Ae ag will proceed to A4. (A4) Profile detection axiom - Given that Ae ag has the full profile of Ao, none of Ae ags actions can increase its knowledge.",
                "That axiom will not be applied, and the agent will proceed with A6 (A5 will be disregarded because the interaction is bilateral). (A6) Evaluation maximization axiom - This axiom will select the max Eval for Ae ag.",
                "Given that Eval = Utility, the same α that was selected by Au ag will be selected. 3.",
                "EVALUATION The main purpose of our experimental analysis is to evaluate the models behavior and performance in a real adversarial environment.",
                "This section investigates whether bounded 1 A case where following the completion of β there exists a γ which gives high utility for Agent Au ag, cannot occur because Ao uses the same utility, and γs existence will cause it to classify β as a low utility action. 554 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) rational agents situated in such adversarial environments will be better off applying our suggested behavioral axioms. 3.1 The Domain To explore the use of the above model and its behavioral axioms, we decided to use the Connect-Four game as our adversarial environment.",
                "Connect-Four is a 2-player, zerosum game which is played using a 6x7 matrix-like board.",
                "Each turn, a player drops a disc into one of the 7 columns (the set of 21 discs is usually colored yellow for player 1 and red for player 2; we will use White and Black respectively to avoid confusion).",
                "The winner is the first player to complete a horizontal, vertical, or diagonal set of four discs with its color.",
                "On very rare occasions, the game might end in a tie if all the empty grids are filled, but no player managed to create a 4-disc set.",
                "The Connect-Four game was solved in [1], where it is shown that the first player (playing with the white discs) can force a win by starting in the middle column (column 4) and playing optimally However, the optimal strategy is very complex, and difficult to follow even for complex bounded rational agents, such as human players.",
                "Before we can proceed checking agent behavior, we must first verify that the domain conforms to the adversarial environments definition as given above (which the behavioral axioms are based on).",
                "First, when playing a Connect-Four game, the agent has an intention to win the game (item 1).",
                "Second (item 2), our agent believes that in Connect-Four there can only be one winner (or no winner at all in the rare occurrence of a tie).",
                "In addition, our agent believes that its opponent to the game will try to win (item 3), and we hope it has some partial knowledge (item 4) about its adversary (this knowledge can vary from nothing, through simple facts such as age, to strategies and weaknesses).",
                "Of course, not all Connect-Four encounters are adversarial.",
                "For example, when a parent is playing the game with its child, the following situation might occur: the child, having a strong incentive to win, treats the environment as adversarial (it intends to win, understands that there can only be one winner, and believes that its parent is trying to beat him).",
                "However, the parents point of view might see the environment as an educational one, where its goal is not to win the game, but to cause enjoyment or practice strategic reasoning.",
                "In such an educational environment, a new set of behavioral axioms might be more beneficial to the parents goals than our suggested adversarial behavioral axioms. 3.2 Axiom Analysis After showing that the Connect-Four game is indeed a zero-sum, bilateral adversarial environment, the next step is to look at players behaviors during the game and check whether behaving according to our model does improve performance.",
                "To do so we have collected log files from completed Connect-Four games that were played by human players over the Internet.",
                "Our collected log file data came from Play by eMail (PBeM) sites.",
                "These are web sites that host email games, where each move is taken by an email exchange between the server and the players.",
                "Many such sites archives contain real competitive interactions, and also maintain a ranking system for their members.",
                "Most of the data we used can be found in [6].",
                "As can be learned from [1], Connect-Four has an optimal strategy and a considerable advantage for the player who starts the game (which we call the White player).",
                "We will concentrate in our analysis on the second players moves (to be called Black).",
                "The White player, being the first to act, has the so-called initiative advantage.",
                "Having the advantage and a good strategy will keep the Black player busy reacting to its moves, instead of initiating threats.",
                "A threat is a combination of three discs of the same color, with an empty spot for the fourth winning disk.",
                "An open threat is a threat that can be realized in the opponents next move.",
                "In order for the Black player to win, it must somehow turn the tide, take the advantage and start presenting threats to the White player.",
                "We will explore Black players behavior and their conformance to our axioms.",
                "To do so, we built an application that reads log files and analyzes the Black players moves.",
                "The application contains two main components: (1) a Min-Max algorithm for evaluation of moves; (2) open threats detector for the discovering of open threats.",
                "The Min-Max algorithm will work to a given depth, d and for each move α will output the heuristic value for the next action taken by the player as written in the log file, h(α), alongside the maximum heuristic value, maxh(α), that could be achieved prior to taking the move (obviously, if h(α) = maxh(α), then the player did not do the optimal move heuristically).",
                "The threat detectors job is to notify if some action was taken in order to block an open threat (not blocking an open threat will probably cause the player to lose in the opponents next move).",
                "The heuristic function used by Min-Max to evaluate the players utility is the following function, which is simple to compute, yet provides a reasonable challenge to human opponents: Definition 8.",
                "Let Group be an adjacent set of four squares that are horizontal, vertical, or diagonal.",
                "Groupn b (Groupn w) be a Group with n pieces of the black (white) color and 4−n empty squares. h = ((Group1 b ∗α)+(Group2 b ∗β)+(Group3 b ∗γ)+(Group4 b ∗∞)) − ((Group1 w ∗α)+(Group2 w ∗β)+(Group3 w ∗γ)+(Group4 w ∗∞)) The values of α, β and δ can vary to form any desired linear combination; however, it is important to value them with the α < β < δ ordering in mind (we used 1, 4, and 8 as their respective values).",
                "Groups of 4 discs of the same color means victory, thus discovery of such a group will result in ∞ to ensure an extreme value.",
                "We now use our estimated evaluation function to evaluate the Black players actions during the Connect-Four adversarial interaction.",
                "Each game from the log file was input into the application, which processed and output a reformatted log file containing the h value of the current move, the maxh value that could be achieved, and a notification if an open threat was detected.",
                "A total of 123 games were analyzed (57 with White winning, and 66 with Black winning).",
                "A few additional games were manually ignored in the experiment, due to these problems: a player abandoning the game while the outcome is not final, or a blunt irrational move in the early stages of the game (e.g., not blocking an obvious winning group in the first opening moves).",
                "In addition, a single tie game was also removed.",
                "The simulator was run to a search depth of 3 moves.",
                "We now proceed to analyze the games with respect to each behavioral axiom.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 555 Table 1: Average heuristic difference analysis Black losses Black Won Avg minh -17.62 -12.02 Avg 3 lowest h moves (min3 h) -13.20 -8.70 3.2.1 Affirming the Suboptimal tactical move axiom The following section presents the heuristic evaluations of the Min-Max algorithm for each action, and checks the amount and extent of suboptimal tactical actions and their implications on performance.",
                "Table 1 shows results and insights from the games heuristic analysis, when search depth equals 3 (this search depth was selected for the results to be comparable to [9], see Section 3.2.3).",
                "The tables heuristic data is the difference between the present maximal heuristic value and the heuristic value of the action that was eventually taken by the player (i.e., the closer the number is to 0, the closer the action was to the maximum heuristic action).",
                "The first row presents the difference values of the action that had the maximal difference value among all the Black players actions in a given game, as averaged over all Blacks winning and losing games (see respective columns).",
                "In games in which the Black player loses, its average difference value was -17.62, while in games in which the Black player won, its average was -12.02.",
                "The second row expands the analysis by considering the 3 highest heuristic difference actions, and averaging them.",
                "In that case, we notice an average heuristic difference of 5 points between games which the Black player loses and games in which it wins.",
                "Nevertheless, the importance of those numbers is that they allowed us to take an educated guess on a threshold number of 11.5, as the value of the TrH constant, which differentiates between normal actions and highly beneficial ones.",
                "After finding an approximated TrH constant, we can proceed with an analysis of the importance of suboptimal moves.",
                "To do so we took the subset of games in which the minimum heuristic difference value for Blacks actions was 11.5.",
                "As presented in Table 2, we can see the different min3 h average of the 3 largest ranges and the respective percentage of games won.",
                "The first row shows that the Black player won only 12% of the games in which the average of its 3 highest heuristically difference actions (min3 h) was smaller than the suggested threshold, TrH = 11.5.",
                "The second row shows a surprising result: it seems that when min3 h > −4 the Black player rarely wins.",
                "Intuition would suggest that games in which the action evaluation values were closer to the maximal values will result in more winning games for Black.",
                "However, it seems that in the Connect-Four domain, merely responding with somewhat easily expected actions, without initiating a few surprising and suboptimal moves, does not yield good results.",
                "The last row sums up the main insights from the analysis; most of Blacks wins (83%) came when its min3 h was in the range of -11.5 to -4.",
                "A close inspection of those Black winning games shows the following pattern behind the numbers: after standard opening moves, Black suddenly drops a disc into an isolated column, which seems a waste of a move.",
                "White continues to build its threats, while usually disregarding Blacks last move, which in turn uses the isolated disc as an anchor for a future winning threat.",
                "The results show that it was beneficial for the Black player Table 2: Blacks winnings percentages % of games min3 h < −11.5 12% min3 h > −4 5% −11.5 ≤ min3 h ≤ −4 83% to take suboptimal actions and not give the current highest possible heuristic value, but will not be too harmful for its position (i.e., will not give high beneficial value to its adversary).",
                "As it turned out, learning the threshold is an important aspect of success: taking wildly risky moves (min3 h < −11.5) or trying to avoid them (min3 h > −4) reduces the Black players winning chances by a large margin. 3.2.2 Affirming the Profile Monitoring Axiom In the task of showing the importance of monitoring ones adversaries profiles, our log files could not be used because they did not contain repeated interactions between players, which are needed to infer the players knowledge about their adversaries.",
                "However, the importance of opponent modeling and its use in attaining tactical advantages was already studied in various domains ([3, 9] are good examples).",
                "In a recent paper, Markovitch and Reger [9] explored the notion of learning and exploitation of opponent weakness in competitive interactions.",
                "They apply simple learning strategies by analyzing examples from past interactions in a specific domain.",
                "They also used the Connect-Four adversarial domain, which can now be used to understand the importance of monitoring the adversarys profile.",
                "Following the presentation of their theoretical model, they describe an extensive empirical study and check the agents performance after learning the weakness model with past examples.",
                "One of the domains used as a competitive environment was the same Connect-Four game (Checkers was the second domain).",
                "Their heuristic function was identical to ours with three different variations (H1, H2, and H3) that are distinguished from one another in their linear combination coefficient values.",
                "The search depth for the players was 3 (as in our analysis).",
                "Their extensive experiments check and compare various learning strategies, risk factors, predefined feature sets and usage methods.",
                "The bottom line is that the Connect-Four domain shows an improvement from a 0.556 winning rate before modeling to a 0.69 after modeling (page 22).",
                "Their conclusions, showing improved performance when holding and using the adversarys model, justify the effort to monitor the adversary profile for continuous and repeated interactions.",
                "An additional point that came up in their experiments is the following: after the opponent weakness model has been learned, the authors describe different methods of integrating the opponent weakness model into the agents decision strategy.",
                "Nevertheless, regardless of the specific method they chose to work with, all integration methods might cause the agent to take suboptimal decisions; it might cause the agent to prefer actions that are suboptimal at the present decision junction, but which might cause the opponent to react in accordance with its weakness model (as represented by our agent) which in turn will be beneficial for us in the future.",
                "The agents behavior, as demonstrated in [9] further confirms and strengthens our Suboptimal Tactical Axiom as discussed in the previous section. 556 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 3.2.3 Additional Insights The need for the Goal Achieving, Preventive Act, and Evaluation Maximization axioms are obvious, and need no further verification.",
                "However, even with respect to those axioms, a few interesting insights came up in the log analysis.",
                "The Goal achieving and Preventive Act axioms, though theoretically trivial, seem to provide some challenge to a human player.",
                "In the initial inspection of the logs, we encountered few games2 where a player, for inexplicable reasons, did not block the other from winning or failed to execute its own winning move.",
                "We can blame those faults on the humans lack of attention, or a typing error in its move reply; nevertheless, those errors might occur in bounded rational agents, and the appropriate behavior needs to be axiomatized.",
                "A typical Connect-Four game revolves around generating threats and blocking them.",
                "In our analysis we looked for explicit preventive actions, i.e., moves that block a group of 3 discs, or that remove a future threat (in our limited search horizon).",
                "We found that in 83% of the total games there was at least one preventive action taken by the Black player.",
                "It was also found that Black averaged 2.8 preventive actions per game on the games in which it lost, while averaging 1.5 preventive actions per game when winning.",
                "It seems that Black requires 1 or 2 preventive actions to build its initial taking position, before starting to present threats.",
                "If it did not manage to win, it will usually prevent an extra threat or two before succumbing to White. 4.",
                "RELATED WORK Much research deals with the axiomatization of teamwork and mental states of individuals: some models use knowledge and belief [10], others have models of goals and intentions [8, 4].",
                "However, all these formal theories deal with agent teamwork and cooperation.",
                "As far as we know, our model is the first to provide a formalized model for explicit adversarial environments and agents behavior in it.",
                "The classical Min-Max adversarial search algorithm was the first attempt to integrate the opponent into the search space with a weak assumption of an optimally playing opponent.",
                "Since then, much effort has gone into integrating the opponent model into the decision procedure to predict future behavior.",
                "The M∗ algorithm presented by Carmel and Markovitch [2] showed a method of incorporating opponent models into adversary search, while in [3] they used learning to provide a more accurate opponent model in a 2player repeated game environment, where agents strategies were modeled as finite automata.",
                "Additional Adversarial planning work was done by Willmott et al. [13], which provided an adversarial planning approach to the game of GO.",
                "The research mentioned above dealt with adversarial search and the integration of opponent models into classical utilitybased search methods.",
                "That work shows the importance of opponent modeling and the ability to exploit it to an agents advantage.",
                "However, the basic limitations of those search methods still apply; our model tries to overcome those limitations by presenting a formal model for a new, mental state-based adversarial specification. 5.",
                "CONCLUSIONS We presented an Adversarial Environment model for a 2 These were later removed from the final analysis. bounded rational agent that is situated in an N-player, zerosum environment.",
                "We used the SharedPlans formalization to define the model and the axioms that agents can apply as behavioral guidelines.",
                "The model is meant to be used as a guideline for designing agents that need to operate in such adversarial environments.",
                "We presented empirical results, based on ConnectFour log file analysis, that exemplify the model and the axioms for a bilateral instance of the environment.",
                "The results we presented are a first step towards an expanded model that will cover all types of adversarial environments, for example, environments that are non-zero-sum, and environments that contain natural agents that are not part of the direct conflict.",
                "Those challenges and more will be dealt with in future research. 6.",
                "ACKNOWLEDGMENT This research was supported in part by Israel Science Foundation grants #1211/04 and #898/05. 7.",
                "REFERENCES [1] L. V. Allis.",
                "A knowledge-based approach of Connect-Four - the game is solved: White wins.",
                "Masters thesis, Free University, Amsterdam, The Netherlands, 1988. [2] D. Carmel and S. Markovitch.",
                "Incorporating opponent models into adversary search.",
                "In Proceedings of the Thirteenth National Conference on Artificial Intelligence, pages 120-125, Portland, OR, 1996. [3] D. Carmel and S. Markovitch.",
                "Opponent modeling in multi-agent systems.",
                "In G. Weiß and S. Sen, editors, Adaptation and Learning in Multi-Agent Systems, pages 40-52.",
                "Springer-Verlag, 1996. [4] B. J. Grosz and S. Kraus.",
                "Collaborative plans for complex group action.",
                "Artificial Intelligence, 86(2):269-357, 1996. [5] M. Hadad, G. Kaminka, G. Armon, and S. Kraus.",
                "Supporting collaborative activity.",
                "In Proc. of AAAI-2005, pages 83-88, Pittsburgh, 2005. [6] http://www.gamerz.net/˜pbmserv/. [7] S. Kraus and D. Lehmann.",
                "Designing and building a negotiating automated agent.",
                "Computational Intelligence, 11:132-171, 1995. [8] H. J. Levesque, P. R. Cohen, and J. H. T. Nunes.",
                "On acting together.",
                "In Proc. of AAAI-90, pages 94-99, Boston, MA, 1990. [9] S. Markovitch and R. Reger.",
                "Learning and exploiting relative weaknesses of opponent agents.",
                "Autonomous Agents and Multi-Agent Systems, 10(2):103-130, 2005. [10] Y. M. Ronald Fagin, Joseph Y. Halpern and M. Y. Vardi.",
                "Reasoning about knowledge.",
                "MIT Press, Cambridge, Mass., 1995. [11] P. Thagard.",
                "Adversarial problem solving: Modeling an oponent using explanatory coherence.",
                "Cognitive Science, 16(1):123-149, 1992. [12] R. W. Toseland and R. F. Rivas.",
                "An Introduction to Group Work Practice.",
                "Prentice Hall, Englewood Cliffs, NJ, 2nd edition edition, 1995. [13] S. Willmott, J. Richardson, A. Bundy, and J. Levine.",
                "An adversarial planning approach to Go.",
                "Lecture Notes in Computer Science, 1558:93-112, 1999.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 557"
            ],
            "original_annotated_samples": [
                "In this paper, we develop a formal, <br>axiomatized model</br> for bounded rational agents that are situated in a zero-sum adversarial environment."
            ],
            "translated_annotated_samples": [
                "En este artículo, desarrollamos un <br>modelo formal y axiomatizado</br> para agentes racionales limitados que se encuentran en un entorno adversarial de suma cero."
            ],
            "translated_text": "Un modelo de entorno adversarial para agentes racionales limitados en interacciones de suma cero Inon Zuckerman1, Sarit Kraus1, Jeffrey S. Rosenschein2, Gal Kaminka1 1 Departamento de Ciencias de la Computación 2 Escuela de Ingeniería Universidad Bar-Ilan y Ciencias de la Computación Ramat-Gan, Israel Universidad Hebrea, Jerusalén, Israel {zukermi, sarit, galk}@cs.biu.ac.il jeff@cs.huji.ac.il RESUMEN Los entornos multiagentes a menudo no son cooperativos ni colaborativos; en muchos casos, los agentes tienen intereses conflictivos, lo que lleva a interacciones adversariales. Este documento presenta un modelo formal de Entorno Adversarial para agentes racionales acotados que operan en un entorno de suma cero. En tales entornos, los intentos de utilizar métodos de búsqueda basados en utilidad clásica pueden plantear una variedad de dificultades (por ejemplo, modelar implícitamente al oponente como un maximizador de utilidad omnisciente, en lugar de aprovechar un modelo de oponente más matizado y explícito). Definimos un Entorno Adversario al describir los estados mentales de un agente en dicho entorno. Luego presentamos axiomas de comportamiento que están destinados a servir como principios de diseño para construir agentes adversarios. Exploramos la aplicación de nuestro enfoque mediante el análisis de archivos de registro de juegos de Conecta Cuatro completados, y presentamos un análisis empírico de la adecuación de los axiomas. Categorías y Descriptores de Asignaturas I.2.11 [Inteligencia Artificial]: Inteligencia Artificial Distribuida-Agentes Inteligentes, Sistemas Multiagente; I.2.4 [Inteligencia Artificial]: Formalismos y Métodos de Representación del Conocimiento - Lógica Modal Términos Generales Diseño, Teoría 1. Introducción: En las primeras investigaciones en sistemas multiagente (SMA) se consideraban grupos cooperativos de agentes; debido a que los agentes individuales tenían recursos limitados o acceso limitado a la información (por ejemplo, potencia de procesamiento limitada, cobertura sensorial limitada), trabajaban juntos por diseño para resolver problemas que individualmente no podían resolver, o al menos no podían resolver de manera eficiente. Sin embargo, la investigación de MAS pronto comenzó a considerar agentes interactivos con intereses individualizados, como representantes de diferentes humanos u organizaciones con intereses no idénticos. Cuando las interacciones son guiadas por intereses diversos, los participantes pueden tener que superar desacuerdos, interacciones no cooperativas e incluso intentos intencionales de dañarse mutuamente. Cuando ocurren este tipo de interacciones, los entornos requieren un comportamiento apropiado por parte de los agentes situados en ellos. Llamamos a estos entornos Entornos Adversarios, y llamamos a los agentes en conflicto Adversarios. Los modelos de cooperación y trabajo en equipo han sido ampliamente explorados en MAS a través de la axiomatización de estados mentales (por ejemplo, [8, 4, 5]). Sin embargo, ninguna de estas investigaciones abordó los dominios adversarios y sus implicaciones para el comportamiento de los agentes. Nuestro artículo aborda este problema al proporcionar un modelo formal y axiomatizado de estados mentales para un subconjunto de dominios adversariales, específicamente entornos adversariales simples de suma cero. Los encuentros de suma cero simples existen, por supuesto, en varios juegos de dos jugadores (por ejemplo, Ajedrez, Damas), pero también existen en juegos de n jugadores (por ejemplo, Risk, Diplomacia), subastas de un solo bien y en otros lugares. En estos entornos recientes en particular, el uso de una búsqueda adversarial basada en utilidad (como el algoritmo Min-Max) no siempre proporciona una solución adecuada; la función de recompensa podría ser bastante compleja o difícil de cuantificar, y existen limitaciones computacionales naturales en agentes racionales acotados. Además, los métodos de búsqueda tradicionales (como Min-Max) no hacen uso de un modelo del oponente, lo cual ha demostrado ser una adición valiosa a la planificación adversarial [9, 3, 11]. En este artículo, desarrollamos un <br>modelo formal y axiomatizado</br> para agentes racionales limitados que se encuentran en un entorno adversarial de suma cero. El modelo utiliza diferentes operadores de modalidad, y sus principales fundamentos son el modelo SharedPlans [4] para el comportamiento colaborativo. Exploramos las propiedades del entorno y los estados mentales de los agentes para derivar axiomas de comportamiento; estos axiomas de comportamiento constituyen un modelo formal que sirve como especificación y guía de diseño para el diseño de agentes en tales entornos. Luego investigamos el comportamiento de nuestro modelo empíricamente utilizando el juego de mesa Conecta Cuatro. Mostramos que este juego se ajusta a nuestra definición de entorno, y analizamos el comportamiento de los jugadores utilizando un gran conjunto de registros de partidas completadas 550 978-81-904262-7-5 (RPS) c 2007 archivos de IFAAMAS. Además, utilizamos los resultados presentados en [9] para discutir la importancia del modelado del oponente en nuestro dominio adversarial de Conecta Cuatro. El artículo continúa de la siguiente manera. La sección 2 presenta la formalización de los modelos. La sección 3 presenta el análisis empírico y sus resultados. Discutimos el trabajo relacionado en la Sección 4, y concluimos y presentamos las direcciones futuras en la Sección 5. ENTORNOS ADVERSARIOS El modelo de entorno adversario (denominado como AE) está destinado a guiar el diseño de agentes al proporcionar una especificación de las capacidades y actitudes mentales de un agente en un entorno adversario. Nos enfocamos aquí en tipos específicos de entornos adversarios, especificados de la siguiente manera: 1. Interacciones de suma cero: las utilidades positivas y negativas de todos los agentes suman cero; 2. Agentes adversarios simples: todos los agentes en el entorno son agentes adversarios; 3. AEs bilaterales: AEs con exactamente dos agentes; 4. AEs multilaterales: AEs de tres o más agentes. Trabajaremos en instanciaciones tanto bilaterales como multilaterales de entornos de suma cero y simples. En particular, nuestro modelo de entorno adversarial tratará con interacciones que consisten en N agentes (N ≥ 2), donde todos los agentes son adversarios y solo un agente puede tener éxito. Ejemplos de tales entornos van desde juegos de mesa (por ejemplo, Ajedrez, Conecta Cuatro y Diplomacia) hasta ciertos entornos económicos (por ejemplo, subastas de N postores sobre un único bien). 2.1 Resumen del Modelo Nuestro enfoque es formalizar las actitudes mentales y comportamientos de un único agente adversario; consideramos cómo un único agente percibe el AE. La siguiente lista especifica las condiciones y estados mentales de un agente en un AE simple de suma cero: 1. El agente tiene la intención individual de que su propio objetivo se complete; 2. El agente tiene la creencia individual de que él y sus adversarios persiguen objetivos completamente opuestos (definidos a continuación) y que solo puede haber un ganador; 3. El agente tiene la creencia individual de que cada adversario tiene la intención de completar su propio objetivo conflictivo completo; 4. El agente tiene una creencia individual en el perfil (parcial) de sus adversarios. El ítem 3 es necesario, ya que podría ser el caso de que algún agente tenga un objetivo completamente conflictivo y esté considerando adoptar la intención de completarlo, pero aún no está comprometido a lograrlo. Esto podría ocurrir porque el agente aún no ha reflexionado sobre los efectos que adoptar esa intención podría tener en las otras intenciones que actualmente mantiene. En tales casos, es posible que ni siquiera se considere en un entorno adversarial. El ítem 4 establece que el agente debe tener alguna creencia sobre los perfiles de sus adversarios. El perfil representa todo el conocimiento que el agente tiene sobre su adversario: sus debilidades, capacidades estratégicas, objetivos, intenciones, confiabilidad y más. Puede ser dado explícitamente o puede ser aprendido a partir de observaciones de encuentros pasados. 2.2 Definiciones de modelos para estados mentales Utilizamos las definiciones de los operadores modales, predicados y metapredicados de Grosz y Krauss, tal como se definen en su formalización de SharedPlan [4]. Recordamos aquí algunos de los predicados y operadores que se utilizan en esa formalización: Int.To(Ai, α, Tn, Tα, C) representa las intenciones de Ai en el tiempo Tn de realizar una acción α en el tiempo Tα en el contexto de C. Int.Th(Ai, prop, Tn, Tprop, C) representa las intenciones de Ai en el tiempo Tn de que una cierta proposición prop se cumpla en el tiempo Tprop en el contexto de C. Los operadores de intención potencial, Pot.Int.To(...) y Pot.Int.Th(...), se utilizan para representar el estado mental cuando un agente considera adoptar una intención, pero no ha deliberado sobre la interacción de las otras intenciones que tiene. El operador Bel(Ai, f, Tf) representa al agente Ai creyendo en la afirmación expresada en la fórmula f, en el tiempo Tf. MB(A, f, Tf) representa la creencia mutua para un grupo de agentes A. Una instantánea del sistema encuentra que nuestro entorno se encuentra en algún estado e ∈ E de estados variables ambientales, y cada adversario en cualquier LAi ∈ L de posibles estados locales. En cualquier paso de tiempo dado, el sistema estará en algún mundo w del conjunto de todos los mundos posibles w ∈ W, donde w = E×LA1 ×LA2 × ...LAn, y n es el número de adversarios. Por ejemplo, en un juego de póker Texas Holdem, el estado local de un agente podría ser su propio conjunto de cartas (que es desconocido para su adversario) mientras que el entorno consistirá en el bote de apuestas y las cartas comunitarias (que son visibles para ambos jugadores). Una función de utilidad bajo esta formalización se define como un mapeo de un mundo posible w ∈ W a un elemento en , que expresa la deseabilidad del mundo, desde la perspectiva de un único agente. Normalmente normalizamos el rango a [0,1], donde 0 representa el mundo menos deseable posible, y 1 es el mundo más deseable. La implementación de la función de utilidad depende del dominio en cuestión. La siguiente lista especifica nuevos predicados, funciones, variables y constantes utilizadas en conjunto con las definiciones originales para la formalización del entorno adversarial: 1. φ es una acción nula (el agente no hace nada). 2. GAi es el conjunto de objetivos del agente Ai. Cada objetivo es un conjunto de predicados cuya satisfacción hace que el objetivo esté completo (usamos G∗ Ai ∈ GAi para representar un objetivo arbitrario del agente Ai). 3. gAi es el conjunto de subobjetivos del agente Ai. Los subobjetivos son predicados cuya satisfacción representa un hito importante hacia el logro del objetivo completo. gG∗ Ai ⊆ gAi es el conjunto de subobjetivos que son importantes para la finalización del objetivo G∗ Ai (usaremos g∗ G∗ Ai ∈ gG∗ Ai para representar un subobjetivo arbitrario). 4. P Aj Ai es el objeto de perfil que el agente Ai tiene sobre el agente Aj. 5. CA es un conjunto general de acciones para todos los agentes en A que se derivan de las restricciones del entorno. CAi ⊆ CA es el conjunto de acciones posibles del agente Ai. La expresión Do(Ai, α, Tα, w) se cumple cuando Ai realiza la acción α durante el intervalo de tiempo Tα en el mundo w. 7. La afirmación Achieve(G∗ Ai , α, w) es verdadera cuando se logra el objetivo G∗ Ai siguiendo la finalización de la acción α en el mundo w ∈ W, donde α ∈ CAi. El perfil (Ai, PAi Ai) es verdadero cuando el agente Ai tiene un perfil de objeto para el agente Aj. Definición 1. El conflicto total (FulConf) describe una interacción de suma cero donde solo se puede completar un solo objetivo de los objetivos en conflicto. FulConf(G∗ Ai , G∗ Aj ) ⇒ (∃α ∈ CAi , ∀w, β ∈ CAj ) (Achieve(G∗ Ai , α, w) ⇒ ¬Achieve(G∗ Aj , β, w)) ∨ (∃β ∈ CAj , ∀w, α ∈ CAi )(Achieve(G∗ Aj , β, w) ⇒ ¬Achieve(G∗ Ai , α, w)) Definición 2. El Conocimiento Adversarial (AdvKnow) es una función que devuelve un valor que representa la cantidad de The Sixth Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 551 agente de conocimiento Ai tiene sobre el perfil del agente Aj, en el tiempo Tn. Cuanto mayor sea el valor, más conocimiento tiene el agente Ai. AdvKnow: P Aj Ai × Tn → Definición 3. Eval: Esta función de evaluación devuelve un valor de utilidad esperada estimado para un agente en A, después de completar una acción de CA en algún estado del mundo w. Eval: A × CA × w → Definición 4. TrH - (Umbral) es una constante numérica en el rango [0,1] que representa un valor umbral de la función de evaluación (Eval). Una acción que produce una evaluación de utilidad estimada por encima del TrH se considera una acción altamente beneficiosa. El valor Eval es una estimación y no la función de utilidad real, la cual suele ser desconocida. Utilizar el valor real de utilidad para un agente racional fácilmente produciría el mejor resultado para ese agente. Sin embargo, los agentes generalmente no tienen las funciones de utilidad reales, sino más bien una estimación heurística de las mismas. Existen dos propiedades importantes que deben cumplir para la función de evaluación: Propiedad 1. La función de evaluación debería indicar que el estado del mundo más deseable es aquel en el que se logra el objetivo. Por lo tanto, una vez que se ha satisfecho el objetivo, no puede haber ninguna acción futura que pueda colocar al agente en un estado del mundo con un valor de Eval más alto. (∀Ai, G∗ Ai , α, β ∈ CAi , w ∈ W) Lograr(G∗ Ai , α, w) ⇒ Eval(Ai, α, w) ≥ Eval(Ai, β, w) Propiedad 2. La función de evaluación debe proyectar una acción que cause la finalización de un objetivo o subobjetivo a un valor que sea mayor que TrH (una acción altamente beneficiosa). (∀Ai, G∗ Ai ∈ GAi , α ∈ CAi , w ∈ W, g∗ GAi ∈ gGAi ) Lograr(G∗ Ai , α, w) ∨ Lograr(g∗ GAi , α, w) ⇒ Eval(Ai, α, w) ≥ TrH. Definición 5. La acción de conjunto Definimos una acción de conjunto (SetAction) como un conjunto de operaciones de acción (ya sean acciones complejas o básicas) de algunos conjuntos de acción CAi y CAj que, según la creencia del agente Ai, están unidas por una relación temporal y consecuente, formando una cadena de eventos (acción y su acción consecuente siguiente). (∀α1 , . . . , αu ∈ CAi , β1 , . . . , βv ∈ CAj , w ∈ W) SetAction(α1 , . . . , αu , β1 , . . . , βv , w) ⇒ ((Do(Ai, α1 , Tα1 , w) ⇒ Do(Aj, β1 , Tβ1 , w)) ⇒ Do(Ai, α2 , Tα2 , w) ⇒ . . . ⇒ Do(Ai, αu , Tαu , w)) La relación consecuente podría existir debido a varias restricciones ambientales (cuando una acción obliga al adversario a responder con una acción específica) o debido al conocimiento de los agentes sobre el perfil de su adversario. Propiedad 3. A medida que aumenta el conocimiento que tenemos sobre nuestro adversario, tendremos creencias adicionales sobre su comportamiento en diferentes situaciones, lo que a su vez crea un nuevo conjunto de acciones. Formalmente, si nuestro AdvKnow en el tiempo Tn+1 es mayor que AdvKnow en el tiempo Tn, entonces cada SetAction conocido en el tiempo Tn también es conocido en el tiempo Tn+1. AdvKnow(P Aj Ai , Tn+1) > AdvKnow(P Aj Ai , Tn) ⇒ (∀α1 , . . . , αu ∈ CAi , β1 , . . . , βv ∈ CAj ) Bel(Aag, SetAction(α1 , . . . , αu , β1 , . . . , βv ), Tn) ⇒ Bel(Aag, SetAction(α1 , . . . , αu , β1 , . . . , βv ), Tn+1) 2.3 La formulación del entorno Los siguientes axiomas proporcionan la definición formal para un Entorno Adversarial (AE) simple y de suma cero. La satisfacción de estos axiomas significa que el agente está situado en un entorno así. Proporciona especificaciones para que el agente Aag interactúe con su conjunto de adversarios A con respecto a los objetivos G∗ Aag y G∗ A en el tiempo TCo en algún estado del mundo w. AE(Aag, A, G∗ Aag, A1, . . . , Ak, G∗ A1, . . . , G∗ Ak, Tn, w) 1. Aag tiene un Int.Th su objetivo se completaría: (∃α ∈ CAag , Tα) Int.Th(Aag, Lograr(G∗ Aag , α), Tn, Tα, AE) 2. Aag cree que él y cada uno de sus adversarios Ao están persiguiendo metas completamente conflictivas: (∀Ao ∈ {A1, . . . , Ak}) Bel(Aag, FulConf(G∗ Aag , G∗ Ao ), Tn) 3. Aag cree que cada uno de sus adversarios en Ao tiene la Int.Th su objetivo en conflicto G∗ Aoi se completará: (∀Ao ∈ {A1, . . . , Ak})(∃β ∈ CAo , Tβ) Bel(Aag, Int.Th(Ao, Lograr(G∗ Ao , β), TCo, Tβ, AE), Tn) 4. Para construir un agente que pueda operar con éxito dentro de un entorno de adversarios estratégicos, debemos especificar pautas de comportamiento para sus interacciones. Utilizar una estrategia de maximización de Eval ingenua hasta cierta profundidad de búsqueda no siempre dará resultados satisfactorios por varias razones: (1) el problema del horizonte de búsqueda al buscar a una profundidad fija; (2) la fuerte suposición de un adversario óptimamente racional con recursos ilimitados; (3) utilizar una función de evaluación estimada que no dará resultados óptimos en todos los estados del mundo, y que puede ser explotada [9]. Los siguientes axiomas especifican los principios de comportamiento que se pueden utilizar para diferenciar entre agentes exitosos y menos exitosos en el entorno adversarial mencionado anteriormente. Esos axiomas deben ser utilizados como principios de especificación al diseñar e implementar agentes que deben ser capaces de desempeñarse bien en entornos adversarios. Los axiomas conductuales representan situaciones en las que el agente adoptará intenciones potenciales para (Pot.Int.To(...)) realizar una acción, lo que típicamente requerirá un razonamiento de medios-fines para seleccionar un posible curso de acción. Este razonamiento llevará a la adopción de un Int.To(...) (ver [4]). A1. Axioma para lograr metas. El primer axioma es el caso más simple; cuando el agente Aag cree que está a una acción (α) de lograr su objetivo en conflicto G∗ Aag, debería adoptar la intención potencial de hacer α y completar su objetivo. (∀Aag, α ∈ CAag, Tn, Tα, w ∈ W) (Bel(Aag, Do(Aag, α, Tα, w) ⇒ Achieve(G∗ Aag, α, w)) ⇒ Pot.Int.To(Aag, α, Tn, Tα, w) Este comportamiento algo trivial es el primer y más fuerte axioma. En cualquier situación, cuando el agente está a una acción de completar el objetivo, debería completar la acción. Cualquier función de evaluación justa clasificaría naturalmente a α como la acción de valor máximo (propiedad 1). Sin embargo, sin una axiomatización explícita de dicho comportamiento, podría haber situaciones en las que el agente decida tomar otra acción por diversas razones, debido a sus recursos de decisión limitados. A2. Axioma del Acto Preventivo. En una situación adversarial, el agente Aag podría decidir tomar acciones que dañen uno de los planes de su adversario para completar su objetivo, incluso si esas acciones no avanzan explícitamente a Aag hacia su objetivo en conflicto G∗ Aag. Tal acción preventiva tendrá lugar cuando el agente Aag tenga la creencia sobre la posibilidad de que su adversario Ao realice una acción β que le otorgará un alto 552 The Sixth Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) valor de evaluación de utilidad (> TrH). Creer que tomar la acción α evitará que el oponente realice su β, adoptará una intención potencial de hacer α. (∀Aag, Ao ∈ A, α ∈ CAag , β ∈ CAo , Tn, Tβ, w ∈ W) (Bel(Aag, Do(Ao, β, Tβ, w) ∧ Eval(Ao, β, w) > TrH, Tn) ∧ Bel(Aag, Do(Aag, α, Tα, w) ⇒ ¬Do(Ao, β, Tβ, w), Tn) ⇒ Pot.Int.To(Aag, α, Tn, Tα, w) Este axioma es un componente básico de cualquier entorno adversarial. Por ejemplo, al observar un juego de ajedrez, un jugador podría darse cuenta de que está a punto de ser jaque mateado por su oponente, por lo tanto, realiza una jugada preventiva. Otro ejemplo es el juego de Cuatro en Línea: cuando un jugador tiene una fila de tres fichas, su oponente debe bloquearla o perder. Una instancia específica de A1 ocurre cuando el adversario está a una acción de lograr su objetivo, y la acción preventiva inmediata debe ser tomada por el agente. Formalmente, tenemos las mismas creencias que se mencionaron anteriormente, con la creencia modificada de que realizar la acción β hará que el agente Ao logre su objetivo. Proposición 1: Caso de prevención o pérdida. (∀Aag, Ao ∈ A, α ∈ CAag , β ∈ CAo , G∗ Ao , Tn, Tα, Tβ, w ∈ W) Bel(Aag, Do(Ao, β, Tβ, w) ⇒ Achieve(G∗ Ao , β, w), Tn) ∧ Bel(Aag, Do(Aag, α, Tα, w) ⇒ ¬Do(Ao, β, Tβ, w)) ⇒ Pot.Int.To(Aag, α, Tn, Tα, w) Esquema de prueba: La Proposición 1 puede derivarse fácilmente del axioma A1 y de la propiedad 2 de la función Eval, que establece que cualquier acción que cause la finalización de un objetivo es una acción altamente beneficiosa. El comportamiento del acto preventivo ocurrirá implícitamente cuando la función Eval sea igual a la función de utilidad del mundo real. Sin embargo, al ser agentes racionales limitados y al tratar con una función de evaluación estimada, necesitamos axiomatizar explícitamente dicho comportamiento, ya que no siempre ocurrirá implícitamente a partir de la función de evaluación. A3. Axioma de Movimiento Táctico Subóptimo. En muchos escenarios, puede ocurrir una situación en la que un agente decida no tomar la acción más beneficiosa actual que puede tomar (la acción con el valor de evaluación de utilidad máximo), porque cree que tomar otra acción (con un valor de evaluación de utilidad más bajo) podría generar (dependiendo de la respuesta del adversario) una posibilidad futura para una acción altamente beneficiosa. Esto ocurrirá con mayor frecuencia cuando la función Eval sea inexacta y difiera en gran medida de la función de Utilidad. De manera formal, el agente Aag cree en una cierta SetAction que evolucionará de acuerdo con su acción inicial y producirá un valor beneficioso alto (> TrH) únicamente para él. (∀Aag, Ao ∈ A, Tn, w ∈ W) (∃α1 , . . . , αu ∈ CAi , β1 , . . . , βv ∈ CAj , Tα1 ) Bel(Aag, SetAction(α1 , . . . , αu , β1 , . . . , βv ), Tn) ∧ Bel(Aag, Eval(Ao, βv , w) < TrH < Eval(Aag, αu , w), Tn) ⇒ Pot.Int.To(Aag, α1 , Tn, Tα1 , w) Un agente podría creer que una cadena de eventos ocurrirá por diversas razones debido a la naturaleza inevitable del dominio. Por ejemplo, en el ajedrez, a menudo observamos lo siguiente: una jugada provoca una posición de jaque, lo que a su vez limita los movimientos del oponente para evitar el jaque, a lo que el primer jugador podría reaccionar con otro jaque, y así sucesivamente. El agente también podría creer en una cadena de eventos basada en su conocimiento del perfil de su adversario, lo que le permite prever los movimientos del adversario con alta precisión. A4. Axioma de Detección de Perfiles. El agente puede ajustar los perfiles de sus adversarios mediante observaciones y estudio de patrones (específicamente, si hay encuentros repetidos con el mismo adversario). Sin embargo, en lugar de esperar a que se revele la información del perfil, un agente también puede iniciar acciones que obligarán a su adversario a reaccionar de una manera que revele conocimiento del perfil sobre él. Formalmente, el axioma establece que si todas las acciones (γ) no son acciones altamente beneficiosas (< TrH), el agente puede realizar la acción α en el tiempo Tα si cree que resultará en una acción no altamente beneficiosa β por parte de su adversario, lo que a su vez le enseña sobre el perfil del adversario, es decir, proporciona un mayor AdvKnow(P Aj Ai , Tβ). (∀Aag, Ao ∈ A, α ∈ CAag , β ∈ CAo , Tn, Tα, Tβ, w ∈ W) Bel(Aag, (∀γ ∈ CAag )Eval(Aag, γ, w) < TrH, Tn) ∧ Bel(Aag, Do(Aag, α, Tα, w) ⇒ Do(Ao, β, Tβ, w), Tn) ∧ Bel(Aag, Eval(Ao, β, w) < TrH) ∧ Bel(Aag, AdvKnow(P Aj Ai , Tβ) > AdvKnow(P Aj Ai , Tn), Tn) ⇒ Pot.Int.To(Aag, α, Tn, Tα, w) Por ejemplo, volviendo al escenario del juego de ajedrez, consideremos comenzar una partida contra un oponente del que no sabemos nada, ni siquiera si es humano o un oponente computarizado. Podríamos comenzar a jugar una estrategia que sea adecuada frente a un oponente promedio, y ajustar nuestro juego de acuerdo a su nivel de juego. A5. Axioma de Formación de Alianzas El siguiente axioma de comportamiento es relevante únicamente en una instancia multilateral del entorno adversarial (obviamente, una alianza no puede formarse en un encuentro bilateral de suma cero). En diferentes situaciones durante una interacción multilateral, un grupo de agentes podría creer que es de su interés formar una alianza temporal. Una alianza de este tipo es un acuerdo que limita el comportamiento de sus miembros, pero se cree por parte de estos que les permite alcanzar un valor de utilidad mayor al que podrían lograr fuera de la alianza. Como ejemplo, podemos observar el clásico juego de mesa Risk, donde cada jugador tiene como objetivo individual ser el único conquistador del mundo, un juego de suma cero. Sin embargo, para lograr este objetivo, podría ser estratégicamente sabio hacer acuerdos de alto el fuego a corto plazo con otros jugadores, o unirse y atacar a un oponente que sea más fuerte que el resto. Los términos de una alianza definen la forma en que sus miembros deben actuar. Es un conjunto de predicados, denominados Términos, que es acordado por los miembros de la alianza y debe mantenerse verdadero durante la duración de la alianza. Por ejemplo, el conjunto de términos en el escenario de riesgo podría contener los siguientes predicados: 1. Los miembros de la alianza no se atacarán entre sí en los territorios X, Y y Z; 2. Los miembros de la alianza contribuirán C unidades por turno para atacar al adversario Ao; 3. Los miembros están obligados a permanecer como parte de la alianza hasta el tiempo Tk o hasta que el ejército adversario Ao sea más pequeño que Q. El conjunto de términos especifica restricciones intergrupales en cada uno de los miembros de la alianza (∀Aal i ∈ Aal ⊆ A) conjunto de acciones Cal i ⊆ C. Definición 6. El valor total de evaluación que el agente Ai logrará al ser parte de Aal es la suma de Evali (Eval para Ai) de cada uno de los valores de evaluación de Aal j después de tomar sus propias acciones α (a través del predicado del agente(α)): Al val(Ai, Cal, Aal, w) = α∈Cal Evali(Aal j, agente(α), w) Definición 7. Al TrH - es un número que representa un valor de Al en la Sexta Conferencia Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 553 umbral; por encima de él, se puede decir que la alianza es una alianza altamente beneficiosa. El valor de Al TrH se calculará dinámicamente de acuerdo con el progreso de la interacción, como se puede ver en [7]. Después de formarse una alianza, sus miembros ahora están trabajando en su entorno adversarial normal, así como de acuerdo con los estados mentales y axiomas requeridos para sus interacciones como parte de la alianza. El siguiente modelo de Alianza (AL) especifica las condiciones bajo las cuales el grupo Aal puede considerarse en una alianza y trabajando con un nuevo y limitado conjunto de acciones Cal, en el tiempo Tn. AL(Aal , Cal , w, Tn) 1. \n\nAL(Aal , Cal , w, Tn) 1. Aal tiene un MB en el que todos los miembros son parte de Aal: MB(Aal, (∀Aal i ∈ Aal)miembro(Aal i, Aal), Tn) 2. Aal tiene una MB que el grupo se mantenga: MB(Aal , (∀Aal i ∈ Aal )Int.Th (Ai, miembro(Ai, Aal ), Tn, Tn+1, Co), Tn) 3. Aal tiene un beneficio marginal que al ser miembros les otorga un alto valor de utilidad: MB(Aal , (∀Aal i ∈ Aal )Al val(Aal i , Cal , Aal , w) ≥ Al TrH, Tn) Los perfiles de los miembros son una parte crucial de las alianzas exitosas. Suponemos que los agentes que tienen perfiles más precisos de sus adversarios serán más exitosos en tales entornos. Tales agentes podrán predecir cuándo un miembro está a punto de violar el contrato de alianzas (elemento 2 en el modelo anterior) y tomar medidas correctivas (cuando el elemento 3 se falsificará). La robustez de la alianza depende en parte de la medida de confianza de sus miembros, la estimación de su posición objetiva y otras propiedades de su perfil. Debemos tener en cuenta que un agente puede formar parte simultáneamente de más de una alianza. Una alianza temporal, donde los miembros del grupo no tienen un objetivo común pero actúan colaborativamente en interés de sus propias metas individuales, es clasificada como un Grupo de Tratamiento por los psicólogos modernos (en contraste con un Grupo de Tarea, donde sus miembros tienen un objetivo común). El modelo de Actividad Compartida presentado en [5] modeló el comportamiento del Grupo de Tratamiento utilizando la misma formalización de Planes Compartidos. Al comparar ambas definiciones de una alianza y un Grupo de Tratamiento, encontramos una similitud sorprendente entre ambos modelos: las definiciones de los modelos de entorno son casi idénticas (ver las definiciones de SAs en [5]), y sus axiomas de Acto Egoísta y Acto Cooperativo se ajustan al comportamiento de nuestros agentes adversarios. La principal distinción entre ambos modelos es la integración de un axioma de acto de comportamiento útil, en la Actividad Compartida que no puede formar parte de la nuestra. Este axioma establece que un agente considerará tomar una acción que disminuirá su valor de Evaluación (hasta un cierto límite inferior), si cree que un compañero del grupo obtendrá un beneficio significativo. Dicho comportamiento no puede ocurrir en un entorno puramente adversarial (como un juego de suma cero), donde los miembros de la alianza están constantemente alerta para manipular su alianza en su propio beneficio. A6. Axioma de Maximización de la Evaluación. En un caso en el que todos los demás axiomas no sean aplicables, procederemos con la acción que maximice el valor heurístico calculado en la función Eval. (∀Aag, Ao ∈ A, α ∈ Cag, Tn, w ∈ W) Bel(Aag, (∀γ ∈ Cag)Eval(Aag, α, w) ≥ Eval(Aag, γ, w), Tn) ⇒ Pot.Int.To(Aag, α, Tn, Tα, w) T1. Optimalidad en Eval = Utilidad El modelo axiomático anterior aborda situaciones en las que la Utilidad es desconocida y los agentes son agentes racionales limitados. El siguiente teorema muestra que en interacciones bilaterales, donde los agentes tienen la función de Utilidad real (es decir, Eval = Utilidad) y son agentes racionales, los axiomas proporcionan el mismo resultado óptimo que la búsqueda adversarial clásica (por ejemplo, Min-Max). Teorema 1. Que Ae ag sea un agente AE racional ilimitado que utiliza la función de evaluación heurística Eval, Au ag sea el mismo agente utilizando la verdadera función de utilidad y Ao sea un único adversario racional ilimitado basado en utilidad. Dado que Eval = Utilidad: (∀α ∈ CAu ag , α ∈ CAe ag , Tn, w ∈ W) Pot.Int.To(Au ag, α, Tn, Tα, w) → Pot.Int.To(Ae ag, α , Tn, Tα, w) ∧ ((α = α ) ∨ (Utilidad(Au ag, α, w) = Eval(Ae ag, α , w))) Esquema de prueba - Dado que Au ag tiene la función de utilidad real y recursos ilimitados, puede generar el árbol de juego completo y ejecutar el algoritmo MinMax óptimo para elegir la acción de mayor valor de utilidad, que denotamos como α. La prueba mostrará que Ae ag, utilizando los axiomas AE, seleccionará la misma utilidad α o igual (cuando hay más de una acción con la misma utilidad máxima) cuando Eval = Utilidad. (A1) Axioma de logro de objetivo - supongamos que hay un α tal que su cumplimiento logrará el objetivo Au ags. Obtendrá la utilidad más alta mediante Min-Max para Au ag. El agente Ae seleccionará α u otra acción con el mismo valor de utilidad a través de A1. Si tal α no existe, Ae ag no puede aplicar este axioma y procede a A2. (A2) Axioma de la acción preventiva - (1) Observando el caso base (ver Prop1), si hay un β que lleva a Ao a alcanzar su objetivo, entonces una acción preventiva α producirá la mayor utilidad para Au ag. Au ag lo elegirá a través de la utilidad, mientras que Ae ag lo elegirá a través de A2. En el caso general, β es una acción altamente beneficiosa para Ao, lo que resulta en una baja utilidad para Au ag, lo que lo guiará a seleccionar un α que evitará β, mientras que Ae ag lo elegirá a través de A2.1 Si dicho β no existe para Ao, entonces A2 no es aplicable, y Ae ag puede proceder a A3. (A3) Axioma de movimiento táctico subóptimo: al usar una función de evaluación heurística, Ae ag tiene una creencia parcial en el perfil de su adversario (elemento 4 en el modelo AE), lo que puede llevarlo a creer en SetActions (Prop1). En nuestro caso, Ae ag tiene un perfil completo de su adversario óptimo y sabe que Ao se comportará de manera óptima según los valores reales de utilidad en el árbol de búsqueda completo, por lo tanto, cualquier creencia sobre una SetAction subóptima no puede existir, lo que hace que este axioma sea inaplicable. Ae ag procederá a A4. (A4) Axioma de detección de perfil - Dado que Ae ag tiene el perfil completo de Ao, ninguna de las acciones de Ae ag puede aumentar su conocimiento. Ese axioma no se aplicará, y el agente procederá con A6 (A5 será ignorado porque la interacción es bilateral). (A6) Axioma de maximización de la evaluación: este axioma seleccionará la máxima Evaluación para Ae ag. Dado que Eval = Utilidad, se seleccionará el mismo α que fue seleccionado por Au ag. 3. EVALUACIÓN El propósito principal de nuestro análisis experimental es evaluar el comportamiento y rendimiento de los modelos en un entorno adversarial real. Esta sección investiga si un caso acotado 1 en el que, tras la finalización de β, exista un γ que proporcione una alta utilidad para el Agente Au ag, no puede ocurrir porque Ao utiliza la misma utilidad, y la existencia de γ hará que clasifique a β como una acción de baja utilidad. 554 The Sixth Intl. En la Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07), los agentes racionales situados en entornos adversariales se beneficiarán más al aplicar nuestros axiomas de comportamiento sugeridos. 3.1 El Dominio Para explorar el uso del modelo anterior y sus axiomas de comportamiento, decidimos utilizar el juego de Conecta Cuatro como nuestro entorno adversarial. Connect-Four es un juego de 2 jugadores, de suma cero, que se juega utilizando un tablero similar a una matriz de 6x7. En cada turno, un jugador coloca un disco en una de las 7 columnas (el conjunto de 21 discos suele ser de color amarillo para el jugador 1 y rojo para el jugador 2; usaremos Blanco y Negro respectivamente para evitar confusiones). El ganador es el primer jugador en completar un conjunto de cuatro discos de su color en horizontal, vertical o diagonal. En ocasiones muy raras, el juego podría terminar en empate si todos los cuadros vacíos están llenos, pero ningún jugador logró crear un conjunto de 4 discos. El juego de Conecta Cuatro fue resuelto en [1], donde se muestra que el primer jugador (jugando con los discos blancos) puede forzar una victoria comenzando en la columna del medio (columna 4) y jugando de manera óptima. Sin embargo, la estrategia óptima es muy compleja y difícil de seguir incluso para agentes racionales limitados complejos, como los jugadores humanos. Antes de que podamos proceder a verificar el comportamiento del agente, primero debemos verificar que el dominio se ajusta a la definición de entornos adversarios mencionada anteriormente (en la que se basan los axiomas de comportamiento). Primero, al jugar al juego de Conecta Cuatro, el agente tiene la intención de ganar el juego (ítem 1). Segundo (ítem 2), nuestro agente cree que en Conecta Cuatro solo puede haber un ganador (o ningún ganador en la rara ocasión de un empate). Además, nuestro agente cree que su oponente en el juego intentará ganar (ítem 3), y esperamos que tenga algún conocimiento parcial (ítem 4) sobre su adversario (este conocimiento puede variar desde nada, pasando por hechos simples como la edad, hasta estrategias y debilidades). Por supuesto, no todos los encuentros de Conecta Cuatro son adversariales. Por ejemplo, cuando un padre está jugando el juego con su hijo, la siguiente situación podría ocurrir: el niño, con un fuerte incentivo para ganar, trata el entorno como adversarial (intenta ganar, entiende que solo puede haber un ganador y cree que su padre está tratando de vencerlo). Sin embargo, desde el punto de vista de los padres, el entorno podría ser visto como educativo, donde el objetivo no es ganar el juego, sino causar disfrute o practicar el razonamiento estratégico. En un entorno educativo como este, un nuevo conjunto de axiomas de comportamiento podría ser más beneficioso para los objetivos de los padres que nuestros axiomas de comportamiento adversarial sugeridos. 3.2 Análisis de Axiomas Después de demostrar que el juego de Conecta Cuatro es de hecho un entorno adversarial bilateral de suma cero, el siguiente paso es observar el comportamiento de los jugadores durante el juego y verificar si comportarse de acuerdo con nuestro modelo mejora el rendimiento. Para hacerlo, hemos recopilado archivos de registro de juegos de Conecta Cuatro completados que fueron jugados por jugadores humanos a través de Internet. Nuestros datos de archivo de registro recopilados provienen de sitios de Juego por Correo Electrónico (PBeM). Estos son sitios web que alojan juegos por correo electrónico, donde cada movimiento se realiza a través de un intercambio de correos electrónicos entre el servidor y los jugadores. Muchos de estos sitios de archivos contienen interacciones competitivas reales y también mantienen un sistema de clasificación para sus miembros. La mayoría de los datos que utilizamos se pueden encontrar en [6]. Como se puede aprender de [1], Conecta Cuatro tiene una estrategia óptima y una ventaja considerable para el jugador que comienza el juego (a quien llamamos jugador Blanco). Nos concentraremos en nuestro análisis en los movimientos de los segundos jugadores (a los que llamaremos Negras). El jugador Blanco, al ser el primero en actuar, tiene la llamada ventaja de la iniciativa. Tener la ventaja y una buena estrategia mantendrá al jugador Negro ocupado reaccionando a sus movimientos, en lugar de iniciar amenazas. Una amenaza es una combinación de tres discos del mismo color, con un espacio vacío para el cuarto disco ganador. Una amenaza abierta es una amenaza que puede materializarse en el próximo movimiento del oponente. Para que el jugador Negro gane, debe de alguna manera cambiar el rumbo, tomar la ventaja y comenzar a presentar amenazas al jugador Blanco. Exploraremos el comportamiento de los jugadores negros y su conformidad con nuestros axiomas. Para hacerlo, construimos una aplicación que lee archivos de registro y analiza los movimientos de los jugadores Negros. La aplicación contiene dos componentes principales: (1) un algoritmo Min-Max para la evaluación de movimientos; (2) un detector de amenazas abiertas para descubrir amenazas abiertas. El algoritmo Min-Max funcionará hasta una profundidad d dada y, para cada movimiento α, devolverá el valor heurístico para la próxima acción tomada por el jugador según lo escrito en el archivo de registro, h(α), junto con el valor heurístico máximo, maxh(α), que podría lograrse antes de realizar el movimiento (obviamente, si h(α) = maxh(α), entonces el jugador no realizó el movimiento óptimo desde el punto de vista heurístico). El trabajo de los detectores de amenazas es notificar si se tomó alguna acción para bloquear una amenaza abierta (no bloquear una amenaza abierta probablemente hará que el jugador pierda en el próximo movimiento del oponente). La función heurística utilizada por Min-Max para evaluar la utilidad de los jugadores es la siguiente función, que es simple de calcular, pero proporciona un desafío razonable a los oponentes humanos: Definición 8. Sea Grupo un conjunto adyacente de cuatro cuadrados que son horizontales, verticales o diagonales. El Grupo n (Grupo w) es un Grupo con n piezas de color negro (blanco) y 4−n casillas vacías. h = ((Grupo1 negro ∗α)+(Grupo2 negro ∗β)+(Grupo3 negro ∗γ)+(Grupo4 negro ∗∞)) − ((Grupo1 blanco ∗α)+(Grupo2 blanco ∗β)+(Grupo3 blanco ∗γ)+(Grupo4 blanco ∗∞)) Los valores de α, β y δ pueden variar para formar cualquier combinación lineal deseada; sin embargo, es importante asignarles valores teniendo en cuenta el orden α < β < δ (usamos 1, 4 y 8 como sus valores respectivos). Grupos de 4 discos del mismo color significan victoria, por lo tanto, el descubrimiento de dicho grupo resultará en ∞ para asegurar un valor extremo. Ahora utilizamos nuestra función de evaluación estimada para evaluar las acciones de los jugadores Negros durante la interacción adversarial de Conecta Cuatro. Cada juego del archivo de registro fue introducido en la aplicación, la cual procesó y generó un archivo de registro reformateado que contenía el valor h del movimiento actual, el valor maxh que podría ser alcanzado y una notificación si se detectaba una amenaza abierta. Un total de 123 partidas fueron analizadas (57 con victoria de las blancas y 66 con victoria de las negras). Algunos juegos adicionales fueron ignorados manualmente en el experimento, debido a estos problemas: un jugador abandonando el juego mientras el resultado no es definitivo, o una jugada irracional y contundente en las primeras etapas del juego (por ejemplo, no bloquear un grupo obviamente ganador en las primeras jugadas de apertura). Además, también se eliminó un empate. El simulador se ejecutó con una profundidad de búsqueda de 3 movimientos. Ahora procedemos a analizar los juegos con respecto a cada axioma de comportamiento. El Sexto Internacional. La siguiente sección presenta las evaluaciones heurísticas del algoritmo Min-Max para cada acción, y verifica la cantidad y el alcance de las acciones tácticas subóptimas y sus implicaciones en el rendimiento. La Tabla 1 muestra los resultados y las percepciones del análisis heurístico de los juegos, cuando la profundidad de búsqueda es igual a 3 (esta profundidad de búsqueda fue seleccionada para que los resultados sean comparables a [9], ver Sección 3.2.3). La información heurística de las tablas es la diferencia entre el valor heurístico máximo presente y el valor heurístico de la acción que finalmente tomó el jugador (es decir, cuanto más cercano sea el número a 0, más cercana estuvo la acción a la acción heurística máxima). La primera fila presenta los valores de diferencia de la acción que tuvo el valor de diferencia máximo entre todas las acciones de los jugadores Negros en un juego dado, promediado en todos los juegos ganados y perdidos por los Negros (ver columnas respectivas). En los juegos en los que el jugador Negro pierde, su valor promedio de diferencia fue de -17.62, mientras que en los juegos en los que el jugador Negro ganó, su promedio fue de -12.02. La segunda fila amplía el análisis al considerar las 3 acciones con mayor diferencia heurística, y promediarlas. En ese caso, notamos una diferencia heurística promedio de 5 puntos entre los juegos en los que el jugador Negro pierde y los juegos en los que gana. Sin embargo, la importancia de esos números es que nos permitieron hacer una suposición educada sobre un número umbral de 11.5, como el valor de la constante TrH, que diferencia entre acciones normales y altamente beneficiosas. Después de encontrar una constante TrH aproximada, podemos proceder con un análisis de la importancia de los movimientos subóptimos. Para hacerlo, tomamos el subconjunto de juegos en los que el valor mínimo de diferencia heurística para las acciones de las negras era de 11.5. Como se presenta en la Tabla 2, podemos ver el promedio de 3 horas mínimo de los 3 mayores rangos y el porcentaje respectivo de juegos ganados. La primera fila muestra que el jugador Negro ganó solo el 12% de los juegos en los que el promedio de sus 3 acciones con mayor diferencia heurística (min3 h) era menor que el umbral sugerido, TrH = 11.5. La segunda fila muestra un resultado sorprendente: parece que cuando min3 h > −4, el jugador Negro rara vez gana. La intuición sugeriría que los juegos en los que los valores de evaluación de la acción estén más cerca de los valores máximos resultarán en más juegos ganados para las piezas negras. Sin embargo, parece que en el dominio de Conecta Cuatro, simplemente responder con acciones algo fácilmente esperadas, sin iniciar algunos movimientos sorprendentes y subóptimos, no produce buenos resultados. La última fila resume las principales conclusiones del análisis; la mayoría de las victorias de Blacks (83%) se produjeron cuando su min3 h estaba en el rango de -11.5 a -4. Una inspección detallada de esos juegos ganados por las fichas negras muestra el siguiente patrón detrás de los números: después de los movimientos de apertura estándar, las fichas negras de repente colocan una ficha en una columna aislada, lo cual parece ser un desperdicio de un movimiento. El blanco sigue construyendo sus amenazas, generalmente ignorando el último movimiento de las negras, que a su vez utiliza el disco aislado como ancla para una futura amenaza ganadora. Los resultados muestran que fue beneficioso para el jugador Negro en la Tabla 2: Porcentajes de ganancias de los Negros % de juegos min3 h < −11.5 12% min3 h > −4 5% −11.5 ≤ min3 h ≤ −4 83% tomar acciones subóptimas y no dar el valor heurístico más alto posible en ese momento, pero no será demasiado perjudicial para su posición (es decir, no dará un alto valor beneficioso a su adversario). Resultó que aprender el umbral es un aspecto importante del éxito: realizar movimientos extremadamente arriesgados (min3 h < −11.5) o intentar evitarlos (min3 h > −4) reduce significativamente las posibilidades de ganar de los jugadores negros. 3.2.2 Afirmación del Axioma de Monitoreo de Perfiles En la tarea de demostrar la importancia de monitorear los perfiles de los adversarios, nuestros archivos de registro no pudieron ser utilizados porque no contenían interacciones repetidas entre jugadores, que son necesarias para inferir el conocimiento de los jugadores sobre sus adversarios. Sin embargo, la importancia del modelado del oponente y su uso para obtener ventajas tácticas ya ha sido estudiada en varios ámbitos ([3, 9] son buenos ejemplos). En un artículo reciente, Markovitch y Reger [9] exploraron la noción de aprendizaje y explotación de la debilidad del oponente en interacciones competitivas. Aplican estrategias de aprendizaje simples analizando ejemplos de interacciones pasadas en un dominio específico. También utilizaron el dominio adversarial de Conecta Cuatro, que ahora se puede utilizar para comprender la importancia de monitorear el perfil del adversario. Tras la presentación de su modelo teórico, describen un extenso estudio empírico y verifican el rendimiento de los agentes después de aprender el modelo de debilidades con ejemplos pasados. Uno de los dominios utilizados como entorno competitivo fue el mismo juego de Conecta Cuatro (las Damas fueron el segundo dominio). Su función heurística era idéntica a la nuestra con tres variaciones diferentes (H1, H2 y H3) que se distinguen entre sí por los valores de sus coeficientes de combinación lineal. La profundidad de búsqueda para los jugadores fue de 3 (como en nuestro análisis). Sus extensos experimentos verifican y comparan diversas estrategias de aprendizaje, factores de riesgo, conjuntos de características predefinidas y métodos de uso. La conclusión es que el dominio de Conecta Cuatro muestra una mejora de una tasa de victoria de 0.556 antes de modelar a 0.69 después de modelar (página 22). Sus conclusiones, que muestran un rendimiento mejorado al sostener y utilizar el modelo del adversario, justifican el esfuerzo de monitorear el perfil del adversario para interacciones continuas y repetidas. Un punto adicional que surgió en sus experimentos es el siguiente: después de que se haya aprendido el modelo de debilidad del oponente, los autores describen diferentes métodos para integrar el modelo de debilidad del oponente en la estrategia de decisión de los agentes. Sin embargo, independientemente del método específico que elijan para trabajar, todos los métodos de integración podrían hacer que el agente tome decisiones subóptimas; podría hacer que el agente prefiera acciones que son subóptimas en el momento actual de la decisión, pero que podrían hacer que el oponente reaccione de acuerdo con su modelo de debilidad (como lo representa nuestro agente), lo cual a su vez será beneficioso para nosotros en el futuro. El comportamiento de los agentes, como se demuestra en [9], confirma y refuerza aún más nuestro Axioma Táctico Subóptimo discutido en la sección anterior. 556 The Sixth Intl. La necesidad de los axiomas de Logro de Objetivos, Acto Preventivo y Maximización de la Evaluación es evidente y no requiere más verificación. Sin embargo, incluso con respecto a esos axiomas, surgieron algunas ideas interesantes en el análisis de registros. Los axiomas de Logro y Prevención de Actos, aunque teóricamente triviales, parecen presentar un desafío para un jugador humano. En la inspección inicial de los registros, nos encontramos con algunos juegos donde un jugador, por razones inexplicables, no bloqueó al otro para que ganara o no logró ejecutar su propia jugada ganadora. Podemos atribuir esas fallas a la falta de atención de los humanos, o a un error de escritura en su respuesta al movimiento; sin embargo, esos errores podrían ocurrir en agentes racionales limitados, y el comportamiento apropiado debe ser axiomatizado. Un juego típico de Cuatro en Línea gira en torno a generar amenazas y bloquearlas. En nuestro análisis buscamos acciones preventivas explícitas, es decir, movimientos que bloqueen un grupo de 3 discos o que eliminen una amenaza futura (en nuestro horizonte de búsqueda limitado). Encontramos que en el 83% de los juegos totales, hubo al menos una acción preventiva tomada por el jugador Negro. También se encontró que Black promedió 2.8 acciones preventivas por juego en los juegos en los que perdió, mientras que promedió 1.5 acciones preventivas por juego al ganar. Parece que Black necesita 1 o 2 acciones preventivas para construir su posición inicial de toma, antes de comenzar a presentar amenazas. Si no logra ganar, generalmente evitará una amenaza adicional o dos antes de sucumbir ante el Blanco. 4. TRABAJO RELACIONADO Mucha investigación se ocupa de la axiomatización del trabajo en equipo y los estados mentales de los individuos: algunos modelos utilizan el conocimiento y la creencia [10], otros tienen modelos de metas e intenciones [8, 4]. Sin embargo, todas estas teorías formales tratan sobre el trabajo en equipo y la cooperación de agentes. Hasta donde sabemos, nuestro modelo es el primero en proporcionar un modelo formalizado para entornos adversarios explícitos y el comportamiento de los agentes en él. El algoritmo de búsqueda adversarial clásico Min-Max fue el primer intento de integrar al oponente en el espacio de búsqueda con la suposición débil de un oponente que juega de manera óptima. Desde entonces, se ha dedicado mucho esfuerzo a integrar el modelo del oponente en el procedimiento de toma de decisiones para predecir el comportamiento futuro. El algoritmo M∗ presentado por Carmel y Markovitch [2] mostró un método para incorporar modelos de oponentes en la búsqueda de adversarios, mientras que en [3] utilizaron el aprendizaje para proporcionar un modelo de oponente más preciso en un entorno de juego repetido de 2 jugadores, donde las estrategias de los agentes se modelaron como autómatas finitos. El trabajo adicional de planificación adversarial fue realizado por Willmott y colaboradores [13], quienes proporcionaron un enfoque de planificación adversarial para el juego de GO. La investigación mencionada anteriormente trató sobre la búsqueda adversarial y la integración de modelos de oponentes en métodos de búsqueda de utilidad clásicos. Ese trabajo muestra la importancia del modelado del oponente y la capacidad de explotarlo en beneficio de un agente. Sin embargo, las limitaciones básicas de esos métodos de búsqueda siguen aplicándose; nuestro modelo intenta superar esas limitaciones presentando un modelo formal para una nueva especificación adversarial basada en estados mentales. 5. CONCLUSIONES Presentamos un modelo de Entorno Adversarial para un agente racional acotado que está situado en un entorno de N jugadores, de suma cero. Estos fueron posteriormente eliminados del análisis final. Utilizamos la formalización de SharedPlans para definir el modelo y los axiomas que los agentes pueden aplicar como pautas de comportamiento. El modelo está destinado a ser utilizado como una guía para diseñar agentes que necesitan operar en entornos adversos. Presentamos resultados empíricos, basados en el análisis del archivo de registro de ConnectFour, que ejemplifican el modelo y los axiomas para una instancia bilateral del entorno. Los resultados que presentamos son un primer paso hacia un modelo ampliado que abarcará todos los tipos de entornos adversarios, por ejemplo, entornos que no son de suma cero y entornos que contienen agentes naturales que no forman parte del conflicto directo. Esos desafíos y más serán abordados en futuras investigaciones. 6. AGRADECIMIENTO Esta investigación fue apoyada en parte por las becas de la Fundación de Ciencia de Israel #1211/04 y #898/05. 7. REFERENCIAS [1] L. V. Allis. Un enfoque basado en el conocimiento de Conecta Cuatro: el juego está resuelto, gana el jugador Blanco. Tesis de maestría, Universidad Libre de Ámsterdam, Países Bajos, 1988. [2] D. Carmel y S. Markovitch. Incorporando modelos de oponentes en la búsqueda de adversarios. En Actas de la Decimotercera Conferencia Nacional de Inteligencia Artificial, páginas 120-125, Portland, Oregón, 1996. [3] D. Carmel y S. Markovitch. Modelado de oponentes en sistemas multiagente. En G. Weiß y S. Sen, editores, Adaptación y Aprendizaje en Sistemas Multiagente, páginas 40-52. Springer-Verlag, 1996. [4] B. J. Grosz y S. Kraus. Planes colaborativos para acciones grupales complejas. Inteligencia Artificial, 86(2):269-357, 1996. [5] M. Hadad, G. Kaminka, G. Armon y S. Kraus. Apoyando la actividad colaborativa. En Proc. de AAAI-2005, páginas 83-88, Pittsburgh, 2005. [6] http://www.gamerz.net/˜pbmserv/. [7] S. Kraus y D. Lehmann. Diseñando y construyendo un agente automatizado de negociación. Inteligencia Computacional, 11:132-171, 1995. [8] H. J. Levesque, P. R. Cohen y J. H. T. Nunes. Actuar juntos. En Proc. de AAAI-90, páginas 94-99, Boston, MA, 1990. [9] S. Markovitch y R. Reger. Aprender y aprovechar las debilidades relativas de los agentes oponentes. Agentes Autónomos y Sistemas Multiagente, 10(2):103-130, 2005. [10] Y. M. Ronald Fagin, Joseph Y. Halpern y M. Y. Vardi. Razonamiento sobre el conocimiento. MIT Press, Cambridge, Mass., 1995. [11] P. Thagard.\nMIT Press, Cambridge, Massachusetts, 1995. [11] P. Thagard. Resolución de problemas adversariales: Modelado de un oponente utilizando coherencia explicativa. Ciencia Cognitiva, 16(1):123-149, 1992. [12] R. W. Toseland y R. F. Rivas. Una introducción a la práctica del trabajo en grupo. Prentice Hall, Englewood Cliffs, NJ, segunda edición, 1995. [13] S. Willmott, J. Richardson, A. Bundy y J. Levine. Un enfoque de planificación adversarial para el juego de Go. Notas de conferencia en Ciencias de la Computación, 1558:93-112, 1999. El Sexto Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 557 ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "zero-sum encounter": {
            "translated_key": "suma cero",
            "is_in_text": true,
            "original_annotated_sentences": [
                "An Adversarial Environment Model for Bounded Rational Agents in Zero-Sum Interactions Inon Zuckerman1 , Sarit Kraus1 , Jeffrey S. Rosenschein2 , Gal Kaminka1 1 Department of Computer Science 2 The School of Engineering Bar-Ilan University and Computer Science Ramat-Gan, Israel Hebrew University, Jerusalem, Israel {zukermi,sarit,galk}@cs.biu.ac.il jeff@cs.huji.ac.il ABSTRACT Multiagent environments are often not cooperative nor collaborative; in many cases, agents have conflicting interests, leading to adversarial interactions.",
                "This paper presents a formal Adversarial Environment model for bounded rational agents operating in a zero-sum environment.",
                "In such environments, attempts to use classical utility-based search methods can raise a variety of difficulties (e.g., implicitly modeling the opponent as an omniscient utility maximizer, rather than leveraging a more nuanced, explicit opponent model).",
                "We define an Adversarial Environment by describing the mental states of an agent in such an environment.",
                "We then present behavioral axioms that are intended to serve as design principles for building such adversarial agents.",
                "We explore the application of our approach by analyzing log files of completed Connect-Four games, and present an empirical analysis of the axioms appropriateness.",
                "Categories and Subject Descriptors I.2.11 [Artificial Intelligence]: Distributed Artificial Intelligence-Intelligent agents,Multiagent Systems; I.2.4 [Artificial Intelligence]: Knowledge Representation Formalisms and Methods -Modal logic General Terms Design, Theory 1.",
                "INTRODUCTION Early research in multiagent systems (MAS) considered cooperative groups of agents; because individual agents had limited resources, or limited access to information (e.g., limited processing power, limited sensor coverage), they worked together by design to solve problems that individually they could not solve, or at least could not solve as efficiently.",
                "MAS research, however, soon began to consider interacting agents with individuated interests, as representatives of different humans or organizations with non-identical interests.",
                "When interactions are guided by diverse interests, participants may have to overcome disagreements, uncooperative interactions, and even intentional attempts to damage one another.",
                "When these types of interactions occur, environments require appropriate behavior from the agents situated in them.",
                "We call these environments Adversarial Environments, and call the clashing agents Adversaries.",
                "Models of cooperation and teamwork have been extensively explored in MAS through the axiomatization of mental states (e.g., [8, 4, 5]).",
                "However, none of this research dealt with adversarial domains and their implications for agent behavior.",
                "Our paper addresses this issue by providing a formal, axiomatized mental state model for a subset of adversarial domains, namely simple zero-sum adversarial environments.",
                "Simple zero-sum encounters exist of course in various twoplayer games (e.g., Chess, Checkers), but they also exist in n-player games (e.g., Risk, Diplomacy), auctions for a single good, and elsewhere.",
                "In these latter environments especially, using a utility-based adversarial search (such as the Min-Max algorithm) does not always provide an adequate solution; the payoff function might be quite complex or difficult to quantify, and there are natural computational limitations on bounded rational agents.",
                "In addition, traditional search methods (like Min-Max) do not make use of a model of the opponent, which has proven to be a valuable addition to adversarial planning [9, 3, 11].",
                "In this paper, we develop a formal, axiomatized model for bounded rational agents that are situated in a zero-sum adversarial environment.",
                "The model uses different modality operators, and its main foundations are the SharedPlans [4] model for collaborative behavior.",
                "We explore environment properties and the mental states of agents to derive behavioral axioms; these behavioral axioms constitute a formal model that serves as a specification and design guideline for agent design in such settings.",
                "We then investigate the behavior of our model empirically using the Connect-Four board game.",
                "We show that this game conforms to our environment definition, and analyze players behavior using a large set of completed match log 550 978-81-904262-7-5 (RPS) c 2007 IFAAMAS files.",
                "In addition, we use the results presented in [9] to discuss the importance of opponent modeling in our ConnectFour adversarial domain.",
                "The paper proceeds as follows.",
                "Section 2 presents the models formalization.",
                "Section 3 presents the empirical analysis and its results.",
                "We discuss related work in Section 4, and conclude and present future directions in Section 5. 2.",
                "ADVERSARIAL ENVIRONMENTS The adversarial environment model (denoted as AE) is intended to guide the design of agents by providing a specification of the capabilities and mental attitudes of an agent in an adversarial environment.",
                "We focus here on specific types of adversarial environments, specified as follows: 1.",
                "Zero-Sum Interactions: positive and negative utilities of all agents sum to zero; 2.",
                "Simple AEs: all agents in the environment are adversarial agents; 3.",
                "Bilateral AEs: AEs with exactly two agents; 4.",
                "Multilateral AEs: AEs of three or more agents.",
                "We will work on both bilateral and multilateral instantiations of zero-sum and simple environments.",
                "In particular, our adversarial environment model will deal with interactions that consist of N agents (N ≥ 2), where all agents are adversaries, and only one agent can succeed.",
                "Examples of such environments range from board games (e.g., Chess, Connect-Four, and Diplomacy) to certain economic environments (e.g., N-bidder auctions over a single good). 2.1 Model Overview Our approach is to formalize the mental attitudes and behaviors of a single adversarial agent; we consider how a single agent perceives the AE.",
                "The following list specifies the conditions and mental states of an agent in a simple, zero-sum AE: 1.",
                "The agent has an individual intention that its own goal will be completed; 2.",
                "The agent has an individual belief that it and its adversaries are pursuing full conflicting goals (defined below)there can be only one winner; 3.",
                "The agent has an individual belief that each adversary has an intention to complete its own full conflicting goal; 4.",
                "The agent has an individual belief in the (partial) profile of its adversaries.",
                "Item 3 is required, since it might be the case that some agent has a full conflicting goal, and is currently considering adopting the intention to complete it, but is, as of yet, not committed to achieving it.",
                "This might occur because the agent has not yet deliberated about the effects that adopting that intention might have on the other intentions it is currently holding.",
                "In such cases, it might not consider itself to even be in an adversarial environment.",
                "Item 4 states that the agent should hold some belief about the profiles of its adversaries.",
                "The profile represents all the knowledge the agent has about its adversary: its weaknesses, strategic capabilities, goals, intentions, trustworthiness, and more.",
                "It can be given explicitly or can be learned from observations of past encounters. 2.2 Model Definitions for Mental States We use Grosz and Krauss definitions of the modal operators, predicates, and meta-predicates, as defined in their SharedPlan formalization [4].",
                "We recall here some of the predicates and operators that are used in that formalization: Int.To(Ai, α, Tn, Tα, C) represents Ais intentions at time Tn to do an action α at time Tα in the context of C. Int.Th(Ai, prop, Tn, Tprop, C) represents Ais intentions at time Tn that a certain proposition prop holds at time Tprop in the context of C. The potential intention operators, Pot.Int.To(...) and Pot.Int.Th(...), are used to represent the mental state when an agent considers adopting an intention, but has not deliberated about the interaction of the other intentions it holds.",
                "The operator Bel(Ai, f, Tf ) represents agent Ai believing in the statement expressed in formula f, at time Tf .",
                "MB(A, f, Tf ) represents mutual belief for a group of agents A.",
                "A snapshot of the system finds our environment to be in some state e ∈ E of environmental variable states, and each adversary in any LAi ∈ L of possible local states.",
                "At any given time step, the system will be in some world w of the set of all possible worlds w ∈ W, where w = E×LA1 ×LA2 × ...LAn , and n is the number of adversaries.",
                "For example, in a Texas Holdem poker game, an agents local state might be its own set of cards (which is unknown to its adversary) while the environment will consist of the betting pot and the community cards (which are visible to both players).",
                "A utility function under this formalization is defined as a mapping from a possible world w ∈ W to an element in , which expresses the desirability of the world, from a single agent perspective.",
                "We usually normalize the range to [0,1], where 0 represents the least desirable possible world, and 1 is the most desirable world.",
                "The implementation of the utility function is dependent on the domain in question.",
                "The following list specifies new predicates, functions, variables, and constants used in conjunction with the original definitions for the adversarial environment formalization: 1. φ is a null action (the agent does not do anything). 2.",
                "GAi is the set of agent Ais goals.",
                "Each goal is a set of predicates whose satisfaction makes the goal complete (we use G∗ Ai ∈ GAi to represent an arbitrary goal of agent Ai). 3. gAi is the set of agent Ais subgoals.",
                "Subgoals are predicates whose satisfaction represents an important milestone toward achievement of the full goal. gG∗ Ai ⊆ gAi is the set of subgoals that are important to the completion of goal G∗ Ai (we will use g∗ G∗ Ai ∈ gG∗ Ai to represent an arbitrary subgoal). 4.",
                "P Aj Ai is the profile object agent Ai holds about agent Aj. 5.",
                "CA is a general set of actions for all agents in A which are derived from the environments constraints.",
                "CAi ⊆ CA is the set of agent Ais possible actions. 6.",
                "Do(Ai, α, Tα, w) holds when Ai performs action α over time interval Tα in world w. 7.",
                "Achieve(G∗ Ai , α, w) is true when goal G∗ Ai is achieved following the completion of action α in world w ∈ W, where α ∈ CAi . 8.",
                "Profile(Ai, PAi Ai ) is true when agent Ai holds an object profile for agent Aj.",
                "Definition 1.",
                "Full conflict (FulConf ) describes a zerosum interaction where only a single goal of the goals in conflict can be completed.",
                "FulConf(G∗ Ai , G∗ Aj ) ⇒ (∃α ∈ CAi , ∀w, β ∈ CAj ) (Achieve(G∗ Ai , α, w) ⇒ ¬Achieve(G∗ Aj , β, w)) ∨ (∃β ∈ CAj , ∀w, α ∈ CAi )(Achieve(G∗ Aj , β, w) ⇒ ¬Achieve(G∗ Ai , α, w)) Definition 2.",
                "Adversarial Knowledge (AdvKnow) is a function returning a value which represents the amount of The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 551 knowledge agent Ai has on the profile of agent Aj, at time Tn.",
                "The higher the value, the more knowledge agent Ai has.",
                "AdvKnow : P Aj Ai × Tn → Definition 3.",
                "Eval - This evaluation function returns an estimated expected utility value for an agent in A, after completing an action from CA in some world state w. Eval : A × CA × w → Definition 4.",
                "TrH - (Threshold) is a numerical constant in the [0,1] range that represents an evaluation function (Eval) threshold value.",
                "An action that yields an estimated utility evaluation above the TrH is regarded as a highly beneficial action.",
                "The Eval value is an estimation and not the real utility function, which is usually unknown.",
                "Using the real utility value for a rational agent would easily yield the best outcome for that agent.",
                "However, agents usually do not have the real utility functions, but rather a heuristic estimate of it.",
                "There are two important properties that should hold for the evaluation function: Property 1.",
                "The evaluation function should state that the most desirable world state is one in which the goal is achieved.",
                "Therefore, after the goal has been satisfied, there can be no future action that can put the agent in a world state with higher Eval value. (∀Ai, G∗ Ai , α, β ∈ CAi , w ∈ W) Achieve(G∗ Ai , α, w) ⇒ Eval(Ai, α, w) ≥ Eval(Ai, β, w) Property 2.",
                "The evaluation function should project an action that causes a completion of a goal or a subgoal to a value which is greater than TrH (a highly beneficial action). (∀Ai, G∗ Ai ∈ GAi , α ∈ CAi , w ∈ W, g∗ GAi ∈ gGAi ) Achieve(G∗ Ai , α, w) ∨ Achieve(g∗ GAi , α, w) ⇒ Eval(Ai, α, w) ≥ TrH.",
                "Definition 5.",
                "SetAction We define a set action (SetAction) as a set of action operations (either complex or basic actions) from some action sets CAi and CAj which, according to agent Ais belief, are attached together by a temporal and consequential relationship, forming a chain of events (action, and its following consequent action). (∀α1 , . . . , αu ∈ CAi , β1 , . . . , βv ∈ CAj , w ∈ W) SetAction(α1 , . . . , αu , β1 , . . . , βv , w) ⇒ ((Do(Ai, α1 , Tα1 , w) ⇒ Do(Aj, β1 , Tβ1 , w)) ⇒ Do(Ai, α2 , Tα2 , w) ⇒ . . . ⇒ Do(Ai, αu , Tαu , w)) The consequential relation might exist due to various environmental constraints (when one action forces the adversary to respond with a specific action) or due to the agents knowledge about the profile of its adversary.",
                "Property 3.",
                "As the knowledge we have about our adversary increases we will have additional beliefs about its behavior in different situations which in turn creates new set actions.",
                "Formally, if our AdvKnow at time Tn+1 is greater than AdvKnow at time Tn, then every SetAction known at time Tn is also known at time Tn+1.",
                "AdvKnow(P Aj Ai , Tn+1) > AdvKnow(P Aj Ai , Tn) ⇒ (∀α1 , . . . , αu ∈ CAi , β1 , . . . , βv ∈ CAj ) Bel(Aag, SetAction(α1 , . . . , αu , β1 , . . . , βv ), Tn) ⇒ Bel(Aag, SetAction(α1 , . . . , αu , β1 , . . . , βv ), Tn+1) 2.3 The Environment Formulation The following axioms provide the formal definition for a simple, zero-sum Adversarial Environment (AE).",
                "Satisfaction of these axioms means that the agent is situated in such an environment.",
                "It provides specifications for agent Aag to interact with its set of adversaries A with respect to goals G∗ Aag and G∗ A at time TCo at some world state w. AE(Aag, A, G∗ Aag , A1, . . . , Ak, G∗ A1 , . . . , G∗ Ak , Tn, w) 1.",
                "Aag has an Int.Th his goal would be completed: (∃α ∈ CAag , Tα) Int.Th(Aag, Achieve(G∗ Aag , α), Tn, Tα, AE) 2.",
                "Aag believes that it and each of its adversaries Ao are pursuing full conflicting goals: (∀Ao ∈ {A1, . . . , Ak}) Bel(Aag, FulConf(G∗ Aag , G∗ Ao ), Tn) 3.",
                "Aag believes that each of his adversaries in Ao has the Int.Th his conflicting goal G∗ Aoi will be completed: (∀Ao ∈ {A1, . . . , Ak})(∃β ∈ CAo , Tβ) Bel(Aag, Int.Th(Ao, Achieve(G∗ Ao , β), TCo, Tβ, AE), Tn) 4.",
                "Aag has beliefs about the (partial) profiles of its adversaries (∀Ao ∈ {A1, . . . , Ak}) (∃PAo Aag ∈ PAag )Bel(Aag, Profile(Ao, PAo Aag ), Tn) To build an agent that will be able to operate successfully within such an AE, we must specify behavioral guidelines for its interactions.",
                "Using a naive Eval maximization strategy to a certain search depth will not always yield satisfactory results for several reasons: (1) the search horizon problem when searching for a fixed depth; (2) the strong assumption of an optimally rational, unbounded resources adversary; (3) using an estimated evaluation function which will not give optimal results in all world states, and can be exploited [9].",
                "The following axioms specify the behavioral principles that can be used to differentiate between successful and less successful agents in the above Adversarial Environment.",
                "Those axioms should be used as specification principles when designing and implementing agents that should be able to perform well in such Adversarial Environments.",
                "The behavioral axioms represent situations in which the agent will adopt potential intentions to (Pot.Int.To(...)) perform an action, which will typically require some means-end reasoning to select a possible course of action.",
                "This reasoning will lead to the adoption of an Int.To(...) (see [4]).",
                "A1.",
                "Goal Achieving Axiom.",
                "The first axiom is the simplest case; when the agent Aag believes that it is one action (α) away from achieving his conflicting goal G∗ Aag , it should adopt the potential intention to do α and complete its goal. (∀Aag, α ∈ CAag , Tn, Tα, w ∈ W) (Bel(Aag, Do(Aag, α, Tα, w) ⇒ Achieve(G∗ Aag , α, w)) ⇒ Pot.Int.To(Aag, α, Tn, Tα, w) This somewhat trivial behavior is the first and strongest axiom.",
                "In any situation, when the agent is an action away from completing the goal, it should complete the action.",
                "Any fair Eval function would naturally classify α as the maximal value action (property 1).",
                "However, without explicit axiomatization of such behavior there might be situations where the agent will decide on taking another action for various reasons, due to its bounded decision resources.",
                "A2.",
                "Preventive Act Axiom.",
                "Being in an adversarial situation, agent Aag might decide to take actions that will damage one of its adversarys plans to complete its goal, even if those actions do not explicitly advance Aag towards its conflicting goal G∗ Aag .",
                "Such preventive action will take place when agent Aag has a belief about the possibility of its adversary Ao doing an action β that will give it a high 552 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) utility evaluation value (> TrH).",
                "Believing that taking action α will prevent the opponent from doing its β, it will adopt a potential intention to do α. (∀Aag, Ao ∈ A, α ∈ CAag , β ∈ CAo , Tn, Tβ, w ∈ W) (Bel(Aag, Do(Ao, β, Tβ, w) ∧ Eval(Ao, β, w) > TrH, Tn) ∧ Bel(Aag, Do(Aag, α, Tα, w) ⇒ ¬Do(Ao, β, Tβ, w), Tn) ⇒ Pot.Int.To(Aag, α, Tn, Tα, w) This axiom is a basic component of any adversarial environment.",
                "For example, looking at a Chess board game, a player could realize that it is about to be checkmated by its opponent, thus making a preventive move.",
                "Another example is a Connect Four game: when a player has a row of three chips, its opponent must block it, or lose.",
                "A specific instance of A1 occurs when the adversary is one action away from achieving its goal, and immediate preventive action needs to be taken by the agent.",
                "Formally, we have the same beliefs as stated above, with a changed belief that doing action β will cause agent Ao to achieve its goal.",
                "Proposition 1: Prevent or lose case. (∀Aag, Ao ∈ A, α ∈ CAag , β ∈ CAo , G∗ Ao , Tn, Tα, Tβ, w ∈ W) Bel(Aag, Do(Ao, β, Tβ, w) ⇒ Achieve(G∗ Ao , β, w), Tn) ∧ Bel(Aag, Do(Aag, α, Tα, w) ⇒ ¬Do(Ao, β, Tβ, w)) ⇒ Pot.Int.To(Aag, α, Tn, Tα, w) Sketch of proof: Proposition 1 can be easily derived from axiom A1 and the property 2 of the Eval function, which states that any action that causes a completion of a goal is a highly beneficial action.",
                "The preventive act behavior will occur implicitly when the Eval function is equal to the real world utility function.",
                "However, being bounded rational agents and dealing with an estimated evaluation function we need to explicitly axiomatize such behavior, for it will not always occur implicitly from the evaluation function.",
                "A3.",
                "Suboptimal Tactical Move Axiom.",
                "In many scenarios a situation may occur where an agent will decide not to take the current most beneficial action it can take (the action with the maximal utility evaluation value), because it believes that taking another action (with lower utility evaluation value) might yield (depending on the adversarys response) a future possibility for a highly beneficial action.",
                "This will occur most often when the Eval function is inaccurate and differs by a large extent from the Utility function.",
                "Put formally, agent Aag believes in a certain SetAction that will evolve according to its initial action and will yield a high beneficial value (> TrH) solely for it. (∀Aag, Ao ∈ A, Tn, w ∈ W) (∃α1 , . . . , αu ∈ CAi , β1 , . . . , βv ∈ CAj , Tα1 ) Bel(Aag, SetAction(α1 , . . . , αu , β1 , . . . , βv ), Tn) ∧ Bel(Aag, Eval(Ao, βv , w) < TrH < Eval(Aag, αu , w), Tn) ⇒ Pot.Int.To(Aag, α1 , Tn, Tα1 , w) An agent might believe that a chain of events will occur for various reasons due to the inevitable nature of the domain.",
                "For example, in Chess, we often observe the following: a move causes a check position, which in turn limits the opponents moves to avoiding the check, to which the first player might react with another check, and so on.",
                "The agent might also believe in a chain of events based on its knowledge of its adversarys profile, which allows it to foresee the adversarys movements with high accuracy.",
                "A4.",
                "Profile Detection Axiom.",
                "The agent can adjust its adversarys profiles by observations and pattern study (specifically, if there are repeated encounters with the same adversary).",
                "However, instead of waiting for profile information to be revealed, an agent can also initiate actions that will force its adversary to react in a way that will reveal profile knowledge about it.",
                "Formally, the axiom states that if all actions (γ) are not highly beneficial actions (< TrH), the agent can do action α in time Tα if it believes that it will result in a non-highly beneficial action β from its adversary, which in turn teaches it about the adversarys profile, i.e., gives a higher AdvKnow(P Aj Ai , Tβ). (∀Aag, Ao ∈ A, α ∈ CAag , β ∈ CAo , Tn, Tα, Tβ, w ∈ W) Bel(Aag, (∀γ ∈ CAag )Eval(Aag, γ, w) < TrH, Tn) ∧ Bel(Aag, Do(Aag, α, Tα, w) ⇒ Do(Ao, β, Tβ, w), Tn) ∧ Bel(Aag, Eval(Ao, β, w) < TrH) ∧ Bel(Aag, AdvKnow(P Aj Ai , Tβ) > AdvKnow(P Aj Ai , Tn), Tn) ⇒ Pot.Int.To(Aag, α, Tn, Tα, w) For example, going back to the Chess board game scenario, consider starting a game versus an opponent about whom we know nothing, not even if it is a human or a computerized opponent.",
                "We might start playing a strategy that will be suitable versus an average opponent, and adjust our game according to its level of play.",
                "A5.",
                "Alliance Formation Axiom The following behavioral axiom is relevant only in a multilateral instantiation of the adversarial environment (obviously, an alliance cannot be formed in a bilateral, <br>zero-sum encounter</br>).",
                "In different situations during a multilateral interaction, a group of agents might believe that it is in their best interests to form a temporary alliance.",
                "Such an alliance is an agreement that constrains its members behavior, but is believed by its members to enable them to achieve a higher utility value than the one achievable outside of the alliance.",
                "As an example, we can look at the classical Risk board game, where each player has an individual goal of being the sole conquerer of the world, a zero-sum game.",
                "However, in order to achieve this goal, it might be strategically wise to make short-term ceasefire agreements with other players, or to join forces and attack an opponent who is stronger than the rest.",
                "An alliances terms defines the way its members should act.",
                "It is a set of predicates, denoted as Terms, that is agreed upon by the alliance members, and should remain true for the duration of the alliance.",
                "For example, the set Terms in the Risk scenario, could contain the following predicates: 1.",
                "Alliance members will not attack each other on territories X, Y and Z; 2.",
                "Alliance members will contribute C units per turn for attacking adversary Ao; 3.",
                "Members are obligated to stay as part of the alliance until time Tk or until adversarys Ao army is smaller than Q.",
                "The set Terms specifies inter-group constraints on each of the alliance members (∀Aal i ∈ Aal ⊆ A) set of actions Cal i ⊆ C. Definition 6.",
                "Al val - the total evaluation value that agent Ai will achieve while being part of Aal is the sum of Evali (Eval for Ai) of each of Aal j Eval values after taking their own α actions (via the agent(α) predicate): Al val(Ai, Cal , Aal , w) = α∈Cal Evali(Aal j , agent(α), w) Definition 7.",
                "Al TrH - is a number representing an Al val The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 553 threshold; above it, the alliance can be said to be a highly beneficial alliance.",
                "The value of Al TrH will be calculated dynamically according to the progress of the interaction, as can be seen in [7].",
                "After an alliance is formed, its members are now working in their normal adversarial environment, as well as according to the mental states and axioms required for their interactions as part of the alliance.",
                "The following Alliance model (AL) specifies the conditions under which the group Aal can be said to be in an alliance and working with a new and constrained set of actions Cal , at time Tn.",
                "AL(Aal , Cal , w, Tn) 1.",
                "Aal has a MB that all members are part of Aal : MB(Aal , (∀Aal i ∈ Aal )member(Aal i , Aal ), Tn) 2.",
                "Aal has a MB that the group be maintained: MB(Aal , (∀Aal i ∈ Aal )Int.Th (Ai, member(Ai, Aal ), Tn, Tn+1, Co), Tn) 3.",
                "Aal has a MB that being members gives them high utility value: MB(Aal , (∀Aal i ∈ Aal )Al val(Aal i , Cal , Aal , w) ≥ Al TrH, Tn) Members profiles are a crucial part of successful alliances.",
                "We assume that agents that have more accurate profiles of their adversaries will be more successful in such environments.",
                "Such agents will be able to predict when a member is about to breach the alliances contract (item 2 in the above model), and take counter measures (when item 3 will falsify).",
                "The robustness of the alliance is in part a function of its members trustfulness measure, objective position estimation, and other profile properties.",
                "We should note that an agent can simultaneously be part of more than one alliance.",
                "Such a temporary alliance, where the group members do not have a joint goal but act collaboratively for the interest of their own individual goals, is classified as a Treatment Group by modern psychologists [12] (in contrast to a Task Group, where its members have a joint goal).",
                "The Shared Activity model as presented in [5] modeled Treatment Group behavior using the same SharedPlans formalization.",
                "When comparing both definitions of an alliance and a Treatment Group we found an unsurprising resemblance between both models: the environment models definitions are almost identical (see SAs definitions in [5]), and their Selfish-Act and Cooperative Act axioms conform to our adversarial agents behavior.",
                "The main distinction between both models is the integration of a Helpful-behavior act axiom, in the Shared Activity which cannot be part of ours.",
                "This axiom states that an agent will consider taking action that will lower its Eval value (to a certain lower bound), if it believes that a group partner will gain a significant benefit.",
                "Such behavior cannot occur in a pure adversarial environment (as a zero-sum game is), where the alliance members are constantly on watch to manipulate their alliance to their own advantage.",
                "A6.",
                "Evaluation Maximization Axiom.",
                "In a case when all other axioms are inapplicable, we will proceed with the action that maximizes the heuristic value as computed in the Eval function. (∀Aag, Ao ∈ A, α ∈ Cag, Tn, w ∈ W) Bel(Aag, (∀γ ∈ Cag)Eval(Aag, α, w) ≥ Eval(Aag, γ, w), Tn) ⇒ Pot.Int.To(Aag, α, Tn, Tα, w) T1.",
                "Optimality on Eval = Utility The above axiomatic model handles situations where the Utility is unknown and the agents are bounded rational agents.",
                "The following theorem shows that in bilateral interactions, where the agents have the real Utility function (i.e., Eval = Utility) and are rational agents, the axioms provide the same optimal result as classic adversarial search (e.g., Min-Max).",
                "Theorem 1.",
                "Let Ae ag be an unbounded rational AE agent using the Eval heuristic evaluation function, Au ag be the same agent using the true Utility function, and Ao be a sole unbounded utility-based rational adversary.",
                "Given that Eval = Utility: (∀α ∈ CAu ag , α ∈ CAe ag , Tn, w ∈ W) Pot.Int.To(Au ag, α, Tn, Tα, w) → Pot.Int.To(Ae ag, α , Tn, Tα, w) ∧ ((α = α ) ∨ (Utility(Au ag, α, w) = Eval(Ae ag, α , w))) Sketch of proof - Given that Au ag has the real utility function and unbounded resources, it can generate the full game tree and run the optimal MinMax algorithm to choose the highest utility value action, which we denote by, α.",
                "The proof will show that Ae ag, using the AE axioms, will select the same or equal utility α (when there is more than one action with the same max utility) when Eval = Utility. (A1) Goal achieving axiom - suppose there is an α such that its completion will achieve Au ags goal.",
                "It will obtain the highest utility by Min-Max for Au ag.",
                "The Ae ag agent will select α or another action with the same utility value via A1.",
                "If such α does not exist, Ae ag cannot apply this axiom, and proceeds to A2. (A2) Preventive act axiom - (1) Looking at the basic case (see Prop1), if there is a β which leads Ao to achieve its goal, then a preventive action α will yield the highest utility for Au ag.",
                "Au ag will choose it through the utility, while Ae ag will choose it through A2. (2) In the general case, β is a highly beneficial action for Ao, thus yields low utility for Au ag, which will guide it to select an α that will prevent β, while Ae ag will choose it through A2.1 If such β does not exist for Ao, then A2 is not applicable, and Ae ag can proceed to A3. (A3) Suboptimal tactical move axiom - When using a heuristic Eval function, Ae ag has a partial belief in the profile of its adversary (item 4 in AE model), which may lead it to believe in SetActions (Prop1).",
                "In our case, Ae ag is holding a full profile on its optimal adversary and knows that Ao will behave optimally according to the real utility values on the complete search tree, therefore, any belief about suboptimal SetAction cannot exist, yielding this axiom inapplicable.",
                "Ae ag will proceed to A4. (A4) Profile detection axiom - Given that Ae ag has the full profile of Ao, none of Ae ags actions can increase its knowledge.",
                "That axiom will not be applied, and the agent will proceed with A6 (A5 will be disregarded because the interaction is bilateral). (A6) Evaluation maximization axiom - This axiom will select the max Eval for Ae ag.",
                "Given that Eval = Utility, the same α that was selected by Au ag will be selected. 3.",
                "EVALUATION The main purpose of our experimental analysis is to evaluate the models behavior and performance in a real adversarial environment.",
                "This section investigates whether bounded 1 A case where following the completion of β there exists a γ which gives high utility for Agent Au ag, cannot occur because Ao uses the same utility, and γs existence will cause it to classify β as a low utility action. 554 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) rational agents situated in such adversarial environments will be better off applying our suggested behavioral axioms. 3.1 The Domain To explore the use of the above model and its behavioral axioms, we decided to use the Connect-Four game as our adversarial environment.",
                "Connect-Four is a 2-player, zerosum game which is played using a 6x7 matrix-like board.",
                "Each turn, a player drops a disc into one of the 7 columns (the set of 21 discs is usually colored yellow for player 1 and red for player 2; we will use White and Black respectively to avoid confusion).",
                "The winner is the first player to complete a horizontal, vertical, or diagonal set of four discs with its color.",
                "On very rare occasions, the game might end in a tie if all the empty grids are filled, but no player managed to create a 4-disc set.",
                "The Connect-Four game was solved in [1], where it is shown that the first player (playing with the white discs) can force a win by starting in the middle column (column 4) and playing optimally However, the optimal strategy is very complex, and difficult to follow even for complex bounded rational agents, such as human players.",
                "Before we can proceed checking agent behavior, we must first verify that the domain conforms to the adversarial environments definition as given above (which the behavioral axioms are based on).",
                "First, when playing a Connect-Four game, the agent has an intention to win the game (item 1).",
                "Second (item 2), our agent believes that in Connect-Four there can only be one winner (or no winner at all in the rare occurrence of a tie).",
                "In addition, our agent believes that its opponent to the game will try to win (item 3), and we hope it has some partial knowledge (item 4) about its adversary (this knowledge can vary from nothing, through simple facts such as age, to strategies and weaknesses).",
                "Of course, not all Connect-Four encounters are adversarial.",
                "For example, when a parent is playing the game with its child, the following situation might occur: the child, having a strong incentive to win, treats the environment as adversarial (it intends to win, understands that there can only be one winner, and believes that its parent is trying to beat him).",
                "However, the parents point of view might see the environment as an educational one, where its goal is not to win the game, but to cause enjoyment or practice strategic reasoning.",
                "In such an educational environment, a new set of behavioral axioms might be more beneficial to the parents goals than our suggested adversarial behavioral axioms. 3.2 Axiom Analysis After showing that the Connect-Four game is indeed a zero-sum, bilateral adversarial environment, the next step is to look at players behaviors during the game and check whether behaving according to our model does improve performance.",
                "To do so we have collected log files from completed Connect-Four games that were played by human players over the Internet.",
                "Our collected log file data came from Play by eMail (PBeM) sites.",
                "These are web sites that host email games, where each move is taken by an email exchange between the server and the players.",
                "Many such sites archives contain real competitive interactions, and also maintain a ranking system for their members.",
                "Most of the data we used can be found in [6].",
                "As can be learned from [1], Connect-Four has an optimal strategy and a considerable advantage for the player who starts the game (which we call the White player).",
                "We will concentrate in our analysis on the second players moves (to be called Black).",
                "The White player, being the first to act, has the so-called initiative advantage.",
                "Having the advantage and a good strategy will keep the Black player busy reacting to its moves, instead of initiating threats.",
                "A threat is a combination of three discs of the same color, with an empty spot for the fourth winning disk.",
                "An open threat is a threat that can be realized in the opponents next move.",
                "In order for the Black player to win, it must somehow turn the tide, take the advantage and start presenting threats to the White player.",
                "We will explore Black players behavior and their conformance to our axioms.",
                "To do so, we built an application that reads log files and analyzes the Black players moves.",
                "The application contains two main components: (1) a Min-Max algorithm for evaluation of moves; (2) open threats detector for the discovering of open threats.",
                "The Min-Max algorithm will work to a given depth, d and for each move α will output the heuristic value for the next action taken by the player as written in the log file, h(α), alongside the maximum heuristic value, maxh(α), that could be achieved prior to taking the move (obviously, if h(α) = maxh(α), then the player did not do the optimal move heuristically).",
                "The threat detectors job is to notify if some action was taken in order to block an open threat (not blocking an open threat will probably cause the player to lose in the opponents next move).",
                "The heuristic function used by Min-Max to evaluate the players utility is the following function, which is simple to compute, yet provides a reasonable challenge to human opponents: Definition 8.",
                "Let Group be an adjacent set of four squares that are horizontal, vertical, or diagonal.",
                "Groupn b (Groupn w) be a Group with n pieces of the black (white) color and 4−n empty squares. h = ((Group1 b ∗α)+(Group2 b ∗β)+(Group3 b ∗γ)+(Group4 b ∗∞)) − ((Group1 w ∗α)+(Group2 w ∗β)+(Group3 w ∗γ)+(Group4 w ∗∞)) The values of α, β and δ can vary to form any desired linear combination; however, it is important to value them with the α < β < δ ordering in mind (we used 1, 4, and 8 as their respective values).",
                "Groups of 4 discs of the same color means victory, thus discovery of such a group will result in ∞ to ensure an extreme value.",
                "We now use our estimated evaluation function to evaluate the Black players actions during the Connect-Four adversarial interaction.",
                "Each game from the log file was input into the application, which processed and output a reformatted log file containing the h value of the current move, the maxh value that could be achieved, and a notification if an open threat was detected.",
                "A total of 123 games were analyzed (57 with White winning, and 66 with Black winning).",
                "A few additional games were manually ignored in the experiment, due to these problems: a player abandoning the game while the outcome is not final, or a blunt irrational move in the early stages of the game (e.g., not blocking an obvious winning group in the first opening moves).",
                "In addition, a single tie game was also removed.",
                "The simulator was run to a search depth of 3 moves.",
                "We now proceed to analyze the games with respect to each behavioral axiom.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 555 Table 1: Average heuristic difference analysis Black losses Black Won Avg minh -17.62 -12.02 Avg 3 lowest h moves (min3 h) -13.20 -8.70 3.2.1 Affirming the Suboptimal tactical move axiom The following section presents the heuristic evaluations of the Min-Max algorithm for each action, and checks the amount and extent of suboptimal tactical actions and their implications on performance.",
                "Table 1 shows results and insights from the games heuristic analysis, when search depth equals 3 (this search depth was selected for the results to be comparable to [9], see Section 3.2.3).",
                "The tables heuristic data is the difference between the present maximal heuristic value and the heuristic value of the action that was eventually taken by the player (i.e., the closer the number is to 0, the closer the action was to the maximum heuristic action).",
                "The first row presents the difference values of the action that had the maximal difference value among all the Black players actions in a given game, as averaged over all Blacks winning and losing games (see respective columns).",
                "In games in which the Black player loses, its average difference value was -17.62, while in games in which the Black player won, its average was -12.02.",
                "The second row expands the analysis by considering the 3 highest heuristic difference actions, and averaging them.",
                "In that case, we notice an average heuristic difference of 5 points between games which the Black player loses and games in which it wins.",
                "Nevertheless, the importance of those numbers is that they allowed us to take an educated guess on a threshold number of 11.5, as the value of the TrH constant, which differentiates between normal actions and highly beneficial ones.",
                "After finding an approximated TrH constant, we can proceed with an analysis of the importance of suboptimal moves.",
                "To do so we took the subset of games in which the minimum heuristic difference value for Blacks actions was 11.5.",
                "As presented in Table 2, we can see the different min3 h average of the 3 largest ranges and the respective percentage of games won.",
                "The first row shows that the Black player won only 12% of the games in which the average of its 3 highest heuristically difference actions (min3 h) was smaller than the suggested threshold, TrH = 11.5.",
                "The second row shows a surprising result: it seems that when min3 h > −4 the Black player rarely wins.",
                "Intuition would suggest that games in which the action evaluation values were closer to the maximal values will result in more winning games for Black.",
                "However, it seems that in the Connect-Four domain, merely responding with somewhat easily expected actions, without initiating a few surprising and suboptimal moves, does not yield good results.",
                "The last row sums up the main insights from the analysis; most of Blacks wins (83%) came when its min3 h was in the range of -11.5 to -4.",
                "A close inspection of those Black winning games shows the following pattern behind the numbers: after standard opening moves, Black suddenly drops a disc into an isolated column, which seems a waste of a move.",
                "White continues to build its threats, while usually disregarding Blacks last move, which in turn uses the isolated disc as an anchor for a future winning threat.",
                "The results show that it was beneficial for the Black player Table 2: Blacks winnings percentages % of games min3 h < −11.5 12% min3 h > −4 5% −11.5 ≤ min3 h ≤ −4 83% to take suboptimal actions and not give the current highest possible heuristic value, but will not be too harmful for its position (i.e., will not give high beneficial value to its adversary).",
                "As it turned out, learning the threshold is an important aspect of success: taking wildly risky moves (min3 h < −11.5) or trying to avoid them (min3 h > −4) reduces the Black players winning chances by a large margin. 3.2.2 Affirming the Profile Monitoring Axiom In the task of showing the importance of monitoring ones adversaries profiles, our log files could not be used because they did not contain repeated interactions between players, which are needed to infer the players knowledge about their adversaries.",
                "However, the importance of opponent modeling and its use in attaining tactical advantages was already studied in various domains ([3, 9] are good examples).",
                "In a recent paper, Markovitch and Reger [9] explored the notion of learning and exploitation of opponent weakness in competitive interactions.",
                "They apply simple learning strategies by analyzing examples from past interactions in a specific domain.",
                "They also used the Connect-Four adversarial domain, which can now be used to understand the importance of monitoring the adversarys profile.",
                "Following the presentation of their theoretical model, they describe an extensive empirical study and check the agents performance after learning the weakness model with past examples.",
                "One of the domains used as a competitive environment was the same Connect-Four game (Checkers was the second domain).",
                "Their heuristic function was identical to ours with three different variations (H1, H2, and H3) that are distinguished from one another in their linear combination coefficient values.",
                "The search depth for the players was 3 (as in our analysis).",
                "Their extensive experiments check and compare various learning strategies, risk factors, predefined feature sets and usage methods.",
                "The bottom line is that the Connect-Four domain shows an improvement from a 0.556 winning rate before modeling to a 0.69 after modeling (page 22).",
                "Their conclusions, showing improved performance when holding and using the adversarys model, justify the effort to monitor the adversary profile for continuous and repeated interactions.",
                "An additional point that came up in their experiments is the following: after the opponent weakness model has been learned, the authors describe different methods of integrating the opponent weakness model into the agents decision strategy.",
                "Nevertheless, regardless of the specific method they chose to work with, all integration methods might cause the agent to take suboptimal decisions; it might cause the agent to prefer actions that are suboptimal at the present decision junction, but which might cause the opponent to react in accordance with its weakness model (as represented by our agent) which in turn will be beneficial for us in the future.",
                "The agents behavior, as demonstrated in [9] further confirms and strengthens our Suboptimal Tactical Axiom as discussed in the previous section. 556 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 3.2.3 Additional Insights The need for the Goal Achieving, Preventive Act, and Evaluation Maximization axioms are obvious, and need no further verification.",
                "However, even with respect to those axioms, a few interesting insights came up in the log analysis.",
                "The Goal achieving and Preventive Act axioms, though theoretically trivial, seem to provide some challenge to a human player.",
                "In the initial inspection of the logs, we encountered few games2 where a player, for inexplicable reasons, did not block the other from winning or failed to execute its own winning move.",
                "We can blame those faults on the humans lack of attention, or a typing error in its move reply; nevertheless, those errors might occur in bounded rational agents, and the appropriate behavior needs to be axiomatized.",
                "A typical Connect-Four game revolves around generating threats and blocking them.",
                "In our analysis we looked for explicit preventive actions, i.e., moves that block a group of 3 discs, or that remove a future threat (in our limited search horizon).",
                "We found that in 83% of the total games there was at least one preventive action taken by the Black player.",
                "It was also found that Black averaged 2.8 preventive actions per game on the games in which it lost, while averaging 1.5 preventive actions per game when winning.",
                "It seems that Black requires 1 or 2 preventive actions to build its initial taking position, before starting to present threats.",
                "If it did not manage to win, it will usually prevent an extra threat or two before succumbing to White. 4.",
                "RELATED WORK Much research deals with the axiomatization of teamwork and mental states of individuals: some models use knowledge and belief [10], others have models of goals and intentions [8, 4].",
                "However, all these formal theories deal with agent teamwork and cooperation.",
                "As far as we know, our model is the first to provide a formalized model for explicit adversarial environments and agents behavior in it.",
                "The classical Min-Max adversarial search algorithm was the first attempt to integrate the opponent into the search space with a weak assumption of an optimally playing opponent.",
                "Since then, much effort has gone into integrating the opponent model into the decision procedure to predict future behavior.",
                "The M∗ algorithm presented by Carmel and Markovitch [2] showed a method of incorporating opponent models into adversary search, while in [3] they used learning to provide a more accurate opponent model in a 2player repeated game environment, where agents strategies were modeled as finite automata.",
                "Additional Adversarial planning work was done by Willmott et al. [13], which provided an adversarial planning approach to the game of GO.",
                "The research mentioned above dealt with adversarial search and the integration of opponent models into classical utilitybased search methods.",
                "That work shows the importance of opponent modeling and the ability to exploit it to an agents advantage.",
                "However, the basic limitations of those search methods still apply; our model tries to overcome those limitations by presenting a formal model for a new, mental state-based adversarial specification. 5.",
                "CONCLUSIONS We presented an Adversarial Environment model for a 2 These were later removed from the final analysis. bounded rational agent that is situated in an N-player, zerosum environment.",
                "We used the SharedPlans formalization to define the model and the axioms that agents can apply as behavioral guidelines.",
                "The model is meant to be used as a guideline for designing agents that need to operate in such adversarial environments.",
                "We presented empirical results, based on ConnectFour log file analysis, that exemplify the model and the axioms for a bilateral instance of the environment.",
                "The results we presented are a first step towards an expanded model that will cover all types of adversarial environments, for example, environments that are non-zero-sum, and environments that contain natural agents that are not part of the direct conflict.",
                "Those challenges and more will be dealt with in future research. 6.",
                "ACKNOWLEDGMENT This research was supported in part by Israel Science Foundation grants #1211/04 and #898/05. 7.",
                "REFERENCES [1] L. V. Allis.",
                "A knowledge-based approach of Connect-Four - the game is solved: White wins.",
                "Masters thesis, Free University, Amsterdam, The Netherlands, 1988. [2] D. Carmel and S. Markovitch.",
                "Incorporating opponent models into adversary search.",
                "In Proceedings of the Thirteenth National Conference on Artificial Intelligence, pages 120-125, Portland, OR, 1996. [3] D. Carmel and S. Markovitch.",
                "Opponent modeling in multi-agent systems.",
                "In G. Weiß and S. Sen, editors, Adaptation and Learning in Multi-Agent Systems, pages 40-52.",
                "Springer-Verlag, 1996. [4] B. J. Grosz and S. Kraus.",
                "Collaborative plans for complex group action.",
                "Artificial Intelligence, 86(2):269-357, 1996. [5] M. Hadad, G. Kaminka, G. Armon, and S. Kraus.",
                "Supporting collaborative activity.",
                "In Proc. of AAAI-2005, pages 83-88, Pittsburgh, 2005. [6] http://www.gamerz.net/˜pbmserv/. [7] S. Kraus and D. Lehmann.",
                "Designing and building a negotiating automated agent.",
                "Computational Intelligence, 11:132-171, 1995. [8] H. J. Levesque, P. R. Cohen, and J. H. T. Nunes.",
                "On acting together.",
                "In Proc. of AAAI-90, pages 94-99, Boston, MA, 1990. [9] S. Markovitch and R. Reger.",
                "Learning and exploiting relative weaknesses of opponent agents.",
                "Autonomous Agents and Multi-Agent Systems, 10(2):103-130, 2005. [10] Y. M. Ronald Fagin, Joseph Y. Halpern and M. Y. Vardi.",
                "Reasoning about knowledge.",
                "MIT Press, Cambridge, Mass., 1995. [11] P. Thagard.",
                "Adversarial problem solving: Modeling an oponent using explanatory coherence.",
                "Cognitive Science, 16(1):123-149, 1992. [12] R. W. Toseland and R. F. Rivas.",
                "An Introduction to Group Work Practice.",
                "Prentice Hall, Englewood Cliffs, NJ, 2nd edition edition, 1995. [13] S. Willmott, J. Richardson, A. Bundy, and J. Levine.",
                "An adversarial planning approach to Go.",
                "Lecture Notes in Computer Science, 1558:93-112, 1999.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 557"
            ],
            "original_annotated_samples": [
                "Alliance Formation Axiom The following behavioral axiom is relevant only in a multilateral instantiation of the adversarial environment (obviously, an alliance cannot be formed in a bilateral, <br>zero-sum encounter</br>)."
            ],
            "translated_annotated_samples": [
                "Axioma de Formación de Alianzas El siguiente axioma de comportamiento es relevante únicamente en una instancia multilateral del entorno adversarial (obviamente, una alianza no puede formarse en un encuentro bilateral de <br>suma cero</br>)."
            ],
            "translated_text": "Un modelo de entorno adversarial para agentes racionales limitados en interacciones de suma cero Inon Zuckerman1, Sarit Kraus1, Jeffrey S. Rosenschein2, Gal Kaminka1 1 Departamento de Ciencias de la Computación 2 Escuela de Ingeniería Universidad Bar-Ilan y Ciencias de la Computación Ramat-Gan, Israel Universidad Hebrea, Jerusalén, Israel {zukermi, sarit, galk}@cs.biu.ac.il jeff@cs.huji.ac.il RESUMEN Los entornos multiagentes a menudo no son cooperativos ni colaborativos; en muchos casos, los agentes tienen intereses conflictivos, lo que lleva a interacciones adversariales. Este documento presenta un modelo formal de Entorno Adversarial para agentes racionales acotados que operan en un entorno de suma cero. En tales entornos, los intentos de utilizar métodos de búsqueda basados en utilidad clásica pueden plantear una variedad de dificultades (por ejemplo, modelar implícitamente al oponente como un maximizador de utilidad omnisciente, en lugar de aprovechar un modelo de oponente más matizado y explícito). Definimos un Entorno Adversario al describir los estados mentales de un agente en dicho entorno. Luego presentamos axiomas de comportamiento que están destinados a servir como principios de diseño para construir agentes adversarios. Exploramos la aplicación de nuestro enfoque mediante el análisis de archivos de registro de juegos de Conecta Cuatro completados, y presentamos un análisis empírico de la adecuación de los axiomas. Categorías y Descriptores de Asignaturas I.2.11 [Inteligencia Artificial]: Inteligencia Artificial Distribuida-Agentes Inteligentes, Sistemas Multiagente; I.2.4 [Inteligencia Artificial]: Formalismos y Métodos de Representación del Conocimiento - Lógica Modal Términos Generales Diseño, Teoría 1. Introducción: En las primeras investigaciones en sistemas multiagente (SMA) se consideraban grupos cooperativos de agentes; debido a que los agentes individuales tenían recursos limitados o acceso limitado a la información (por ejemplo, potencia de procesamiento limitada, cobertura sensorial limitada), trabajaban juntos por diseño para resolver problemas que individualmente no podían resolver, o al menos no podían resolver de manera eficiente. Sin embargo, la investigación de MAS pronto comenzó a considerar agentes interactivos con intereses individualizados, como representantes de diferentes humanos u organizaciones con intereses no idénticos. Cuando las interacciones son guiadas por intereses diversos, los participantes pueden tener que superar desacuerdos, interacciones no cooperativas e incluso intentos intencionales de dañarse mutuamente. Cuando ocurren este tipo de interacciones, los entornos requieren un comportamiento apropiado por parte de los agentes situados en ellos. Llamamos a estos entornos Entornos Adversarios, y llamamos a los agentes en conflicto Adversarios. Los modelos de cooperación y trabajo en equipo han sido ampliamente explorados en MAS a través de la axiomatización de estados mentales (por ejemplo, [8, 4, 5]). Sin embargo, ninguna de estas investigaciones abordó los dominios adversarios y sus implicaciones para el comportamiento de los agentes. Nuestro artículo aborda este problema al proporcionar un modelo formal y axiomatizado de estados mentales para un subconjunto de dominios adversariales, específicamente entornos adversariales simples de suma cero. Los encuentros de suma cero simples existen, por supuesto, en varios juegos de dos jugadores (por ejemplo, Ajedrez, Damas), pero también existen en juegos de n jugadores (por ejemplo, Risk, Diplomacia), subastas de un solo bien y en otros lugares. En estos entornos recientes en particular, el uso de una búsqueda adversarial basada en utilidad (como el algoritmo Min-Max) no siempre proporciona una solución adecuada; la función de recompensa podría ser bastante compleja o difícil de cuantificar, y existen limitaciones computacionales naturales en agentes racionales acotados. Además, los métodos de búsqueda tradicionales (como Min-Max) no hacen uso de un modelo del oponente, lo cual ha demostrado ser una adición valiosa a la planificación adversarial [9, 3, 11]. En este artículo, desarrollamos un modelo formal y axiomatizado para agentes racionales limitados que se encuentran en un entorno adversarial de suma cero. El modelo utiliza diferentes operadores de modalidad, y sus principales fundamentos son el modelo SharedPlans [4] para el comportamiento colaborativo. Exploramos las propiedades del entorno y los estados mentales de los agentes para derivar axiomas de comportamiento; estos axiomas de comportamiento constituyen un modelo formal que sirve como especificación y guía de diseño para el diseño de agentes en tales entornos. Luego investigamos el comportamiento de nuestro modelo empíricamente utilizando el juego de mesa Conecta Cuatro. Mostramos que este juego se ajusta a nuestra definición de entorno, y analizamos el comportamiento de los jugadores utilizando un gran conjunto de registros de partidas completadas 550 978-81-904262-7-5 (RPS) c 2007 archivos de IFAAMAS. Además, utilizamos los resultados presentados en [9] para discutir la importancia del modelado del oponente en nuestro dominio adversarial de Conecta Cuatro. El artículo continúa de la siguiente manera. La sección 2 presenta la formalización de los modelos. La sección 3 presenta el análisis empírico y sus resultados. Discutimos el trabajo relacionado en la Sección 4, y concluimos y presentamos las direcciones futuras en la Sección 5. ENTORNOS ADVERSARIOS El modelo de entorno adversario (denominado como AE) está destinado a guiar el diseño de agentes al proporcionar una especificación de las capacidades y actitudes mentales de un agente en un entorno adversario. Nos enfocamos aquí en tipos específicos de entornos adversarios, especificados de la siguiente manera: 1. Interacciones de suma cero: las utilidades positivas y negativas de todos los agentes suman cero; 2. Agentes adversarios simples: todos los agentes en el entorno son agentes adversarios; 3. AEs bilaterales: AEs con exactamente dos agentes; 4. AEs multilaterales: AEs de tres o más agentes. Trabajaremos en instanciaciones tanto bilaterales como multilaterales de entornos de suma cero y simples. En particular, nuestro modelo de entorno adversarial tratará con interacciones que consisten en N agentes (N ≥ 2), donde todos los agentes son adversarios y solo un agente puede tener éxito. Ejemplos de tales entornos van desde juegos de mesa (por ejemplo, Ajedrez, Conecta Cuatro y Diplomacia) hasta ciertos entornos económicos (por ejemplo, subastas de N postores sobre un único bien). 2.1 Resumen del Modelo Nuestro enfoque es formalizar las actitudes mentales y comportamientos de un único agente adversario; consideramos cómo un único agente percibe el AE. La siguiente lista especifica las condiciones y estados mentales de un agente en un AE simple de suma cero: 1. El agente tiene la intención individual de que su propio objetivo se complete; 2. El agente tiene la creencia individual de que él y sus adversarios persiguen objetivos completamente opuestos (definidos a continuación) y que solo puede haber un ganador; 3. El agente tiene la creencia individual de que cada adversario tiene la intención de completar su propio objetivo conflictivo completo; 4. El agente tiene una creencia individual en el perfil (parcial) de sus adversarios. El ítem 3 es necesario, ya que podría ser el caso de que algún agente tenga un objetivo completamente conflictivo y esté considerando adoptar la intención de completarlo, pero aún no está comprometido a lograrlo. Esto podría ocurrir porque el agente aún no ha reflexionado sobre los efectos que adoptar esa intención podría tener en las otras intenciones que actualmente mantiene. En tales casos, es posible que ni siquiera se considere en un entorno adversarial. El ítem 4 establece que el agente debe tener alguna creencia sobre los perfiles de sus adversarios. El perfil representa todo el conocimiento que el agente tiene sobre su adversario: sus debilidades, capacidades estratégicas, objetivos, intenciones, confiabilidad y más. Puede ser dado explícitamente o puede ser aprendido a partir de observaciones de encuentros pasados. 2.2 Definiciones de modelos para estados mentales Utilizamos las definiciones de los operadores modales, predicados y metapredicados de Grosz y Krauss, tal como se definen en su formalización de SharedPlan [4]. Recordamos aquí algunos de los predicados y operadores que se utilizan en esa formalización: Int.To(Ai, α, Tn, Tα, C) representa las intenciones de Ai en el tiempo Tn de realizar una acción α en el tiempo Tα en el contexto de C. Int.Th(Ai, prop, Tn, Tprop, C) representa las intenciones de Ai en el tiempo Tn de que una cierta proposición prop se cumpla en el tiempo Tprop en el contexto de C. Los operadores de intención potencial, Pot.Int.To(...) y Pot.Int.Th(...), se utilizan para representar el estado mental cuando un agente considera adoptar una intención, pero no ha deliberado sobre la interacción de las otras intenciones que tiene. El operador Bel(Ai, f, Tf) representa al agente Ai creyendo en la afirmación expresada en la fórmula f, en el tiempo Tf. MB(A, f, Tf) representa la creencia mutua para un grupo de agentes A. Una instantánea del sistema encuentra que nuestro entorno se encuentra en algún estado e ∈ E de estados variables ambientales, y cada adversario en cualquier LAi ∈ L de posibles estados locales. En cualquier paso de tiempo dado, el sistema estará en algún mundo w del conjunto de todos los mundos posibles w ∈ W, donde w = E×LA1 ×LA2 × ...LAn, y n es el número de adversarios. Por ejemplo, en un juego de póker Texas Holdem, el estado local de un agente podría ser su propio conjunto de cartas (que es desconocido para su adversario) mientras que el entorno consistirá en el bote de apuestas y las cartas comunitarias (que son visibles para ambos jugadores). Una función de utilidad bajo esta formalización se define como un mapeo de un mundo posible w ∈ W a un elemento en , que expresa la deseabilidad del mundo, desde la perspectiva de un único agente. Normalmente normalizamos el rango a [0,1], donde 0 representa el mundo menos deseable posible, y 1 es el mundo más deseable. La implementación de la función de utilidad depende del dominio en cuestión. La siguiente lista especifica nuevos predicados, funciones, variables y constantes utilizadas en conjunto con las definiciones originales para la formalización del entorno adversarial: 1. φ es una acción nula (el agente no hace nada). 2. GAi es el conjunto de objetivos del agente Ai. Cada objetivo es un conjunto de predicados cuya satisfacción hace que el objetivo esté completo (usamos G∗ Ai ∈ GAi para representar un objetivo arbitrario del agente Ai). 3. gAi es el conjunto de subobjetivos del agente Ai. Los subobjetivos son predicados cuya satisfacción representa un hito importante hacia el logro del objetivo completo. gG∗ Ai ⊆ gAi es el conjunto de subobjetivos que son importantes para la finalización del objetivo G∗ Ai (usaremos g∗ G∗ Ai ∈ gG∗ Ai para representar un subobjetivo arbitrario). 4. P Aj Ai es el objeto de perfil que el agente Ai tiene sobre el agente Aj. 5. CA es un conjunto general de acciones para todos los agentes en A que se derivan de las restricciones del entorno. CAi ⊆ CA es el conjunto de acciones posibles del agente Ai. La expresión Do(Ai, α, Tα, w) se cumple cuando Ai realiza la acción α durante el intervalo de tiempo Tα en el mundo w. 7. La afirmación Achieve(G∗ Ai , α, w) es verdadera cuando se logra el objetivo G∗ Ai siguiendo la finalización de la acción α en el mundo w ∈ W, donde α ∈ CAi. El perfil (Ai, PAi Ai) es verdadero cuando el agente Ai tiene un perfil de objeto para el agente Aj. Definición 1. El conflicto total (FulConf) describe una interacción de suma cero donde solo se puede completar un solo objetivo de los objetivos en conflicto. FulConf(G∗ Ai , G∗ Aj ) ⇒ (∃α ∈ CAi , ∀w, β ∈ CAj ) (Achieve(G∗ Ai , α, w) ⇒ ¬Achieve(G∗ Aj , β, w)) ∨ (∃β ∈ CAj , ∀w, α ∈ CAi )(Achieve(G∗ Aj , β, w) ⇒ ¬Achieve(G∗ Ai , α, w)) Definición 2. El Conocimiento Adversarial (AdvKnow) es una función que devuelve un valor que representa la cantidad de The Sixth Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 551 agente de conocimiento Ai tiene sobre el perfil del agente Aj, en el tiempo Tn. Cuanto mayor sea el valor, más conocimiento tiene el agente Ai. AdvKnow: P Aj Ai × Tn → Definición 3. Eval: Esta función de evaluación devuelve un valor de utilidad esperada estimado para un agente en A, después de completar una acción de CA en algún estado del mundo w. Eval: A × CA × w → Definición 4. TrH - (Umbral) es una constante numérica en el rango [0,1] que representa un valor umbral de la función de evaluación (Eval). Una acción que produce una evaluación de utilidad estimada por encima del TrH se considera una acción altamente beneficiosa. El valor Eval es una estimación y no la función de utilidad real, la cual suele ser desconocida. Utilizar el valor real de utilidad para un agente racional fácilmente produciría el mejor resultado para ese agente. Sin embargo, los agentes generalmente no tienen las funciones de utilidad reales, sino más bien una estimación heurística de las mismas. Existen dos propiedades importantes que deben cumplir para la función de evaluación: Propiedad 1. La función de evaluación debería indicar que el estado del mundo más deseable es aquel en el que se logra el objetivo. Por lo tanto, una vez que se ha satisfecho el objetivo, no puede haber ninguna acción futura que pueda colocar al agente en un estado del mundo con un valor de Eval más alto. (∀Ai, G∗ Ai , α, β ∈ CAi , w ∈ W) Lograr(G∗ Ai , α, w) ⇒ Eval(Ai, α, w) ≥ Eval(Ai, β, w) Propiedad 2. La función de evaluación debe proyectar una acción que cause la finalización de un objetivo o subobjetivo a un valor que sea mayor que TrH (una acción altamente beneficiosa). (∀Ai, G∗ Ai ∈ GAi , α ∈ CAi , w ∈ W, g∗ GAi ∈ gGAi ) Lograr(G∗ Ai , α, w) ∨ Lograr(g∗ GAi , α, w) ⇒ Eval(Ai, α, w) ≥ TrH. Definición 5. La acción de conjunto Definimos una acción de conjunto (SetAction) como un conjunto de operaciones de acción (ya sean acciones complejas o básicas) de algunos conjuntos de acción CAi y CAj que, según la creencia del agente Ai, están unidas por una relación temporal y consecuente, formando una cadena de eventos (acción y su acción consecuente siguiente). (∀α1 , . . . , αu ∈ CAi , β1 , . . . , βv ∈ CAj , w ∈ W) SetAction(α1 , . . . , αu , β1 , . . . , βv , w) ⇒ ((Do(Ai, α1 , Tα1 , w) ⇒ Do(Aj, β1 , Tβ1 , w)) ⇒ Do(Ai, α2 , Tα2 , w) ⇒ . . . ⇒ Do(Ai, αu , Tαu , w)) La relación consecuente podría existir debido a varias restricciones ambientales (cuando una acción obliga al adversario a responder con una acción específica) o debido al conocimiento de los agentes sobre el perfil de su adversario. Propiedad 3. A medida que aumenta el conocimiento que tenemos sobre nuestro adversario, tendremos creencias adicionales sobre su comportamiento en diferentes situaciones, lo que a su vez crea un nuevo conjunto de acciones. Formalmente, si nuestro AdvKnow en el tiempo Tn+1 es mayor que AdvKnow en el tiempo Tn, entonces cada SetAction conocido en el tiempo Tn también es conocido en el tiempo Tn+1. AdvKnow(P Aj Ai , Tn+1) > AdvKnow(P Aj Ai , Tn) ⇒ (∀α1 , . . . , αu ∈ CAi , β1 , . . . , βv ∈ CAj ) Bel(Aag, SetAction(α1 , . . . , αu , β1 , . . . , βv ), Tn) ⇒ Bel(Aag, SetAction(α1 , . . . , αu , β1 , . . . , βv ), Tn+1) 2.3 La formulación del entorno Los siguientes axiomas proporcionan la definición formal para un Entorno Adversarial (AE) simple y de suma cero. La satisfacción de estos axiomas significa que el agente está situado en un entorno así. Proporciona especificaciones para que el agente Aag interactúe con su conjunto de adversarios A con respecto a los objetivos G∗ Aag y G∗ A en el tiempo TCo en algún estado del mundo w. AE(Aag, A, G∗ Aag, A1, . . . , Ak, G∗ A1, . . . , G∗ Ak, Tn, w) 1. Aag tiene un Int.Th su objetivo se completaría: (∃α ∈ CAag , Tα) Int.Th(Aag, Lograr(G∗ Aag , α), Tn, Tα, AE) 2. Aag cree que él y cada uno de sus adversarios Ao están persiguiendo metas completamente conflictivas: (∀Ao ∈ {A1, . . . , Ak}) Bel(Aag, FulConf(G∗ Aag , G∗ Ao ), Tn) 3. Aag cree que cada uno de sus adversarios en Ao tiene la Int.Th su objetivo en conflicto G∗ Aoi se completará: (∀Ao ∈ {A1, . . . , Ak})(∃β ∈ CAo , Tβ) Bel(Aag, Int.Th(Ao, Lograr(G∗ Ao , β), TCo, Tβ, AE), Tn) 4. Para construir un agente que pueda operar con éxito dentro de un entorno de adversarios estratégicos, debemos especificar pautas de comportamiento para sus interacciones. Utilizar una estrategia de maximización de Eval ingenua hasta cierta profundidad de búsqueda no siempre dará resultados satisfactorios por varias razones: (1) el problema del horizonte de búsqueda al buscar a una profundidad fija; (2) la fuerte suposición de un adversario óptimamente racional con recursos ilimitados; (3) utilizar una función de evaluación estimada que no dará resultados óptimos en todos los estados del mundo, y que puede ser explotada [9]. Los siguientes axiomas especifican los principios de comportamiento que se pueden utilizar para diferenciar entre agentes exitosos y menos exitosos en el entorno adversarial mencionado anteriormente. Esos axiomas deben ser utilizados como principios de especificación al diseñar e implementar agentes que deben ser capaces de desempeñarse bien en entornos adversarios. Los axiomas conductuales representan situaciones en las que el agente adoptará intenciones potenciales para (Pot.Int.To(...)) realizar una acción, lo que típicamente requerirá un razonamiento de medios-fines para seleccionar un posible curso de acción. Este razonamiento llevará a la adopción de un Int.To(...) (ver [4]). A1. Axioma para lograr metas. El primer axioma es el caso más simple; cuando el agente Aag cree que está a una acción (α) de lograr su objetivo en conflicto G∗ Aag, debería adoptar la intención potencial de hacer α y completar su objetivo. (∀Aag, α ∈ CAag, Tn, Tα, w ∈ W) (Bel(Aag, Do(Aag, α, Tα, w) ⇒ Achieve(G∗ Aag, α, w)) ⇒ Pot.Int.To(Aag, α, Tn, Tα, w) Este comportamiento algo trivial es el primer y más fuerte axioma. En cualquier situación, cuando el agente está a una acción de completar el objetivo, debería completar la acción. Cualquier función de evaluación justa clasificaría naturalmente a α como la acción de valor máximo (propiedad 1). Sin embargo, sin una axiomatización explícita de dicho comportamiento, podría haber situaciones en las que el agente decida tomar otra acción por diversas razones, debido a sus recursos de decisión limitados. A2. Axioma del Acto Preventivo. En una situación adversarial, el agente Aag podría decidir tomar acciones que dañen uno de los planes de su adversario para completar su objetivo, incluso si esas acciones no avanzan explícitamente a Aag hacia su objetivo en conflicto G∗ Aag. Tal acción preventiva tendrá lugar cuando el agente Aag tenga la creencia sobre la posibilidad de que su adversario Ao realice una acción β que le otorgará un alto 552 The Sixth Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) valor de evaluación de utilidad (> TrH). Creer que tomar la acción α evitará que el oponente realice su β, adoptará una intención potencial de hacer α. (∀Aag, Ao ∈ A, α ∈ CAag , β ∈ CAo , Tn, Tβ, w ∈ W) (Bel(Aag, Do(Ao, β, Tβ, w) ∧ Eval(Ao, β, w) > TrH, Tn) ∧ Bel(Aag, Do(Aag, α, Tα, w) ⇒ ¬Do(Ao, β, Tβ, w), Tn) ⇒ Pot.Int.To(Aag, α, Tn, Tα, w) Este axioma es un componente básico de cualquier entorno adversarial. Por ejemplo, al observar un juego de ajedrez, un jugador podría darse cuenta de que está a punto de ser jaque mateado por su oponente, por lo tanto, realiza una jugada preventiva. Otro ejemplo es el juego de Cuatro en Línea: cuando un jugador tiene una fila de tres fichas, su oponente debe bloquearla o perder. Una instancia específica de A1 ocurre cuando el adversario está a una acción de lograr su objetivo, y la acción preventiva inmediata debe ser tomada por el agente. Formalmente, tenemos las mismas creencias que se mencionaron anteriormente, con la creencia modificada de que realizar la acción β hará que el agente Ao logre su objetivo. Proposición 1: Caso de prevención o pérdida. (∀Aag, Ao ∈ A, α ∈ CAag , β ∈ CAo , G∗ Ao , Tn, Tα, Tβ, w ∈ W) Bel(Aag, Do(Ao, β, Tβ, w) ⇒ Achieve(G∗ Ao , β, w), Tn) ∧ Bel(Aag, Do(Aag, α, Tα, w) ⇒ ¬Do(Ao, β, Tβ, w)) ⇒ Pot.Int.To(Aag, α, Tn, Tα, w) Esquema de prueba: La Proposición 1 puede derivarse fácilmente del axioma A1 y de la propiedad 2 de la función Eval, que establece que cualquier acción que cause la finalización de un objetivo es una acción altamente beneficiosa. El comportamiento del acto preventivo ocurrirá implícitamente cuando la función Eval sea igual a la función de utilidad del mundo real. Sin embargo, al ser agentes racionales limitados y al tratar con una función de evaluación estimada, necesitamos axiomatizar explícitamente dicho comportamiento, ya que no siempre ocurrirá implícitamente a partir de la función de evaluación. A3. Axioma de Movimiento Táctico Subóptimo. En muchos escenarios, puede ocurrir una situación en la que un agente decida no tomar la acción más beneficiosa actual que puede tomar (la acción con el valor de evaluación de utilidad máximo), porque cree que tomar otra acción (con un valor de evaluación de utilidad más bajo) podría generar (dependiendo de la respuesta del adversario) una posibilidad futura para una acción altamente beneficiosa. Esto ocurrirá con mayor frecuencia cuando la función Eval sea inexacta y difiera en gran medida de la función de Utilidad. De manera formal, el agente Aag cree en una cierta SetAction que evolucionará de acuerdo con su acción inicial y producirá un valor beneficioso alto (> TrH) únicamente para él. (∀Aag, Ao ∈ A, Tn, w ∈ W) (∃α1 , . . . , αu ∈ CAi , β1 , . . . , βv ∈ CAj , Tα1 ) Bel(Aag, SetAction(α1 , . . . , αu , β1 , . . . , βv ), Tn) ∧ Bel(Aag, Eval(Ao, βv , w) < TrH < Eval(Aag, αu , w), Tn) ⇒ Pot.Int.To(Aag, α1 , Tn, Tα1 , w) Un agente podría creer que una cadena de eventos ocurrirá por diversas razones debido a la naturaleza inevitable del dominio. Por ejemplo, en el ajedrez, a menudo observamos lo siguiente: una jugada provoca una posición de jaque, lo que a su vez limita los movimientos del oponente para evitar el jaque, a lo que el primer jugador podría reaccionar con otro jaque, y así sucesivamente. El agente también podría creer en una cadena de eventos basada en su conocimiento del perfil de su adversario, lo que le permite prever los movimientos del adversario con alta precisión. A4. Axioma de Detección de Perfiles. El agente puede ajustar los perfiles de sus adversarios mediante observaciones y estudio de patrones (específicamente, si hay encuentros repetidos con el mismo adversario). Sin embargo, en lugar de esperar a que se revele la información del perfil, un agente también puede iniciar acciones que obligarán a su adversario a reaccionar de una manera que revele conocimiento del perfil sobre él. Formalmente, el axioma establece que si todas las acciones (γ) no son acciones altamente beneficiosas (< TrH), el agente puede realizar la acción α en el tiempo Tα si cree que resultará en una acción no altamente beneficiosa β por parte de su adversario, lo que a su vez le enseña sobre el perfil del adversario, es decir, proporciona un mayor AdvKnow(P Aj Ai , Tβ). (∀Aag, Ao ∈ A, α ∈ CAag , β ∈ CAo , Tn, Tα, Tβ, w ∈ W) Bel(Aag, (∀γ ∈ CAag )Eval(Aag, γ, w) < TrH, Tn) ∧ Bel(Aag, Do(Aag, α, Tα, w) ⇒ Do(Ao, β, Tβ, w), Tn) ∧ Bel(Aag, Eval(Ao, β, w) < TrH) ∧ Bel(Aag, AdvKnow(P Aj Ai , Tβ) > AdvKnow(P Aj Ai , Tn), Tn) ⇒ Pot.Int.To(Aag, α, Tn, Tα, w) Por ejemplo, volviendo al escenario del juego de ajedrez, consideremos comenzar una partida contra un oponente del que no sabemos nada, ni siquiera si es humano o un oponente computarizado. Podríamos comenzar a jugar una estrategia que sea adecuada frente a un oponente promedio, y ajustar nuestro juego de acuerdo a su nivel de juego. A5. Axioma de Formación de Alianzas El siguiente axioma de comportamiento es relevante únicamente en una instancia multilateral del entorno adversarial (obviamente, una alianza no puede formarse en un encuentro bilateral de <br>suma cero</br>). En diferentes situaciones durante una interacción multilateral, un grupo de agentes podría creer que es de su interés formar una alianza temporal. Una alianza de este tipo es un acuerdo que limita el comportamiento de sus miembros, pero se cree por parte de estos que les permite alcanzar un valor de utilidad mayor al que podrían lograr fuera de la alianza. Como ejemplo, podemos observar el clásico juego de mesa Risk, donde cada jugador tiene como objetivo individual ser el único conquistador del mundo, un juego de suma cero. Sin embargo, para lograr este objetivo, podría ser estratégicamente sabio hacer acuerdos de alto el fuego a corto plazo con otros jugadores, o unirse y atacar a un oponente que sea más fuerte que el resto. Los términos de una alianza definen la forma en que sus miembros deben actuar. Es un conjunto de predicados, denominados Términos, que es acordado por los miembros de la alianza y debe mantenerse verdadero durante la duración de la alianza. Por ejemplo, el conjunto de términos en el escenario de riesgo podría contener los siguientes predicados: 1. Los miembros de la alianza no se atacarán entre sí en los territorios X, Y y Z; 2. Los miembros de la alianza contribuirán C unidades por turno para atacar al adversario Ao; 3. Los miembros están obligados a permanecer como parte de la alianza hasta el tiempo Tk o hasta que el ejército adversario Ao sea más pequeño que Q. El conjunto de términos especifica restricciones intergrupales en cada uno de los miembros de la alianza (∀Aal i ∈ Aal ⊆ A) conjunto de acciones Cal i ⊆ C. Definición 6. El valor total de evaluación que el agente Ai logrará al ser parte de Aal es la suma de Evali (Eval para Ai) de cada uno de los valores de evaluación de Aal j después de tomar sus propias acciones α (a través del predicado del agente(α)): Al val(Ai, Cal, Aal, w) = α∈Cal Evali(Aal j, agente(α), w) Definición 7. Al TrH - es un número que representa un valor de Al en la Sexta Conferencia Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 553 umbral; por encima de él, se puede decir que la alianza es una alianza altamente beneficiosa. El valor de Al TrH se calculará dinámicamente de acuerdo con el progreso de la interacción, como se puede ver en [7]. Después de formarse una alianza, sus miembros ahora están trabajando en su entorno adversarial normal, así como de acuerdo con los estados mentales y axiomas requeridos para sus interacciones como parte de la alianza. El siguiente modelo de Alianza (AL) especifica las condiciones bajo las cuales el grupo Aal puede considerarse en una alianza y trabajando con un nuevo y limitado conjunto de acciones Cal, en el tiempo Tn. AL(Aal , Cal , w, Tn) 1. \n\nAL(Aal , Cal , w, Tn) 1. Aal tiene un MB en el que todos los miembros son parte de Aal: MB(Aal, (∀Aal i ∈ Aal)miembro(Aal i, Aal), Tn) 2. Aal tiene una MB que el grupo se mantenga: MB(Aal , (∀Aal i ∈ Aal )Int.Th (Ai, miembro(Ai, Aal ), Tn, Tn+1, Co), Tn) 3. Aal tiene un beneficio marginal que al ser miembros les otorga un alto valor de utilidad: MB(Aal , (∀Aal i ∈ Aal )Al val(Aal i , Cal , Aal , w) ≥ Al TrH, Tn) Los perfiles de los miembros son una parte crucial de las alianzas exitosas. Suponemos que los agentes que tienen perfiles más precisos de sus adversarios serán más exitosos en tales entornos. Tales agentes podrán predecir cuándo un miembro está a punto de violar el contrato de alianzas (elemento 2 en el modelo anterior) y tomar medidas correctivas (cuando el elemento 3 se falsificará). La robustez de la alianza depende en parte de la medida de confianza de sus miembros, la estimación de su posición objetiva y otras propiedades de su perfil. Debemos tener en cuenta que un agente puede formar parte simultáneamente de más de una alianza. Una alianza temporal, donde los miembros del grupo no tienen un objetivo común pero actúan colaborativamente en interés de sus propias metas individuales, es clasificada como un Grupo de Tratamiento por los psicólogos modernos (en contraste con un Grupo de Tarea, donde sus miembros tienen un objetivo común). El modelo de Actividad Compartida presentado en [5] modeló el comportamiento del Grupo de Tratamiento utilizando la misma formalización de Planes Compartidos. Al comparar ambas definiciones de una alianza y un Grupo de Tratamiento, encontramos una similitud sorprendente entre ambos modelos: las definiciones de los modelos de entorno son casi idénticas (ver las definiciones de SAs en [5]), y sus axiomas de Acto Egoísta y Acto Cooperativo se ajustan al comportamiento de nuestros agentes adversarios. La principal distinción entre ambos modelos es la integración de un axioma de acto de comportamiento útil, en la Actividad Compartida que no puede formar parte de la nuestra. Este axioma establece que un agente considerará tomar una acción que disminuirá su valor de Evaluación (hasta un cierto límite inferior), si cree que un compañero del grupo obtendrá un beneficio significativo. Dicho comportamiento no puede ocurrir en un entorno puramente adversarial (como un juego de suma cero), donde los miembros de la alianza están constantemente alerta para manipular su alianza en su propio beneficio. A6. Axioma de Maximización de la Evaluación. En un caso en el que todos los demás axiomas no sean aplicables, procederemos con la acción que maximice el valor heurístico calculado en la función Eval. (∀Aag, Ao ∈ A, α ∈ Cag, Tn, w ∈ W) Bel(Aag, (∀γ ∈ Cag)Eval(Aag, α, w) ≥ Eval(Aag, γ, w), Tn) ⇒ Pot.Int.To(Aag, α, Tn, Tα, w) T1. Optimalidad en Eval = Utilidad El modelo axiomático anterior aborda situaciones en las que la Utilidad es desconocida y los agentes son agentes racionales limitados. El siguiente teorema muestra que en interacciones bilaterales, donde los agentes tienen la función de Utilidad real (es decir, Eval = Utilidad) y son agentes racionales, los axiomas proporcionan el mismo resultado óptimo que la búsqueda adversarial clásica (por ejemplo, Min-Max). Teorema 1. Que Ae ag sea un agente AE racional ilimitado que utiliza la función de evaluación heurística Eval, Au ag sea el mismo agente utilizando la verdadera función de utilidad y Ao sea un único adversario racional ilimitado basado en utilidad. Dado que Eval = Utilidad: (∀α ∈ CAu ag , α ∈ CAe ag , Tn, w ∈ W) Pot.Int.To(Au ag, α, Tn, Tα, w) → Pot.Int.To(Ae ag, α , Tn, Tα, w) ∧ ((α = α ) ∨ (Utilidad(Au ag, α, w) = Eval(Ae ag, α , w))) Esquema de prueba - Dado que Au ag tiene la función de utilidad real y recursos ilimitados, puede generar el árbol de juego completo y ejecutar el algoritmo MinMax óptimo para elegir la acción de mayor valor de utilidad, que denotamos como α. La prueba mostrará que Ae ag, utilizando los axiomas AE, seleccionará la misma utilidad α o igual (cuando hay más de una acción con la misma utilidad máxima) cuando Eval = Utilidad. (A1) Axioma de logro de objetivo - supongamos que hay un α tal que su cumplimiento logrará el objetivo Au ags. Obtendrá la utilidad más alta mediante Min-Max para Au ag. El agente Ae seleccionará α u otra acción con el mismo valor de utilidad a través de A1. Si tal α no existe, Ae ag no puede aplicar este axioma y procede a A2. (A2) Axioma de la acción preventiva - (1) Observando el caso base (ver Prop1), si hay un β que lleva a Ao a alcanzar su objetivo, entonces una acción preventiva α producirá la mayor utilidad para Au ag. Au ag lo elegirá a través de la utilidad, mientras que Ae ag lo elegirá a través de A2. En el caso general, β es una acción altamente beneficiosa para Ao, lo que resulta en una baja utilidad para Au ag, lo que lo guiará a seleccionar un α que evitará β, mientras que Ae ag lo elegirá a través de A2.1 Si dicho β no existe para Ao, entonces A2 no es aplicable, y Ae ag puede proceder a A3. (A3) Axioma de movimiento táctico subóptimo: al usar una función de evaluación heurística, Ae ag tiene una creencia parcial en el perfil de su adversario (elemento 4 en el modelo AE), lo que puede llevarlo a creer en SetActions (Prop1). En nuestro caso, Ae ag tiene un perfil completo de su adversario óptimo y sabe que Ao se comportará de manera óptima según los valores reales de utilidad en el árbol de búsqueda completo, por lo tanto, cualquier creencia sobre una SetAction subóptima no puede existir, lo que hace que este axioma sea inaplicable. Ae ag procederá a A4. (A4) Axioma de detección de perfil - Dado que Ae ag tiene el perfil completo de Ao, ninguna de las acciones de Ae ag puede aumentar su conocimiento. Ese axioma no se aplicará, y el agente procederá con A6 (A5 será ignorado porque la interacción es bilateral). (A6) Axioma de maximización de la evaluación: este axioma seleccionará la máxima Evaluación para Ae ag. Dado que Eval = Utilidad, se seleccionará el mismo α que fue seleccionado por Au ag. 3. EVALUACIÓN El propósito principal de nuestro análisis experimental es evaluar el comportamiento y rendimiento de los modelos en un entorno adversarial real. Esta sección investiga si un caso acotado 1 en el que, tras la finalización de β, exista un γ que proporcione una alta utilidad para el Agente Au ag, no puede ocurrir porque Ao utiliza la misma utilidad, y la existencia de γ hará que clasifique a β como una acción de baja utilidad. 554 The Sixth Intl. En la Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07), los agentes racionales situados en entornos adversariales se beneficiarán más al aplicar nuestros axiomas de comportamiento sugeridos. 3.1 El Dominio Para explorar el uso del modelo anterior y sus axiomas de comportamiento, decidimos utilizar el juego de Conecta Cuatro como nuestro entorno adversarial. Connect-Four es un juego de 2 jugadores, de suma cero, que se juega utilizando un tablero similar a una matriz de 6x7. En cada turno, un jugador coloca un disco en una de las 7 columnas (el conjunto de 21 discos suele ser de color amarillo para el jugador 1 y rojo para el jugador 2; usaremos Blanco y Negro respectivamente para evitar confusiones). El ganador es el primer jugador en completar un conjunto de cuatro discos de su color en horizontal, vertical o diagonal. En ocasiones muy raras, el juego podría terminar en empate si todos los cuadros vacíos están llenos, pero ningún jugador logró crear un conjunto de 4 discos. El juego de Conecta Cuatro fue resuelto en [1], donde se muestra que el primer jugador (jugando con los discos blancos) puede forzar una victoria comenzando en la columna del medio (columna 4) y jugando de manera óptima. Sin embargo, la estrategia óptima es muy compleja y difícil de seguir incluso para agentes racionales limitados complejos, como los jugadores humanos. Antes de que podamos proceder a verificar el comportamiento del agente, primero debemos verificar que el dominio se ajusta a la definición de entornos adversarios mencionada anteriormente (en la que se basan los axiomas de comportamiento). Primero, al jugar al juego de Conecta Cuatro, el agente tiene la intención de ganar el juego (ítem 1). Segundo (ítem 2), nuestro agente cree que en Conecta Cuatro solo puede haber un ganador (o ningún ganador en la rara ocasión de un empate). Además, nuestro agente cree que su oponente en el juego intentará ganar (ítem 3), y esperamos que tenga algún conocimiento parcial (ítem 4) sobre su adversario (este conocimiento puede variar desde nada, pasando por hechos simples como la edad, hasta estrategias y debilidades). Por supuesto, no todos los encuentros de Conecta Cuatro son adversariales. Por ejemplo, cuando un padre está jugando el juego con su hijo, la siguiente situación podría ocurrir: el niño, con un fuerte incentivo para ganar, trata el entorno como adversarial (intenta ganar, entiende que solo puede haber un ganador y cree que su padre está tratando de vencerlo). Sin embargo, desde el punto de vista de los padres, el entorno podría ser visto como educativo, donde el objetivo no es ganar el juego, sino causar disfrute o practicar el razonamiento estratégico. En un entorno educativo como este, un nuevo conjunto de axiomas de comportamiento podría ser más beneficioso para los objetivos de los padres que nuestros axiomas de comportamiento adversarial sugeridos. 3.2 Análisis de Axiomas Después de demostrar que el juego de Conecta Cuatro es de hecho un entorno adversarial bilateral de suma cero, el siguiente paso es observar el comportamiento de los jugadores durante el juego y verificar si comportarse de acuerdo con nuestro modelo mejora el rendimiento. Para hacerlo, hemos recopilado archivos de registro de juegos de Conecta Cuatro completados que fueron jugados por jugadores humanos a través de Internet. Nuestros datos de archivo de registro recopilados provienen de sitios de Juego por Correo Electrónico (PBeM). Estos son sitios web que alojan juegos por correo electrónico, donde cada movimiento se realiza a través de un intercambio de correos electrónicos entre el servidor y los jugadores. Muchos de estos sitios de archivos contienen interacciones competitivas reales y también mantienen un sistema de clasificación para sus miembros. La mayoría de los datos que utilizamos se pueden encontrar en [6]. Como se puede aprender de [1], Conecta Cuatro tiene una estrategia óptima y una ventaja considerable para el jugador que comienza el juego (a quien llamamos jugador Blanco). Nos concentraremos en nuestro análisis en los movimientos de los segundos jugadores (a los que llamaremos Negras). El jugador Blanco, al ser el primero en actuar, tiene la llamada ventaja de la iniciativa. Tener la ventaja y una buena estrategia mantendrá al jugador Negro ocupado reaccionando a sus movimientos, en lugar de iniciar amenazas. Una amenaza es una combinación de tres discos del mismo color, con un espacio vacío para el cuarto disco ganador. Una amenaza abierta es una amenaza que puede materializarse en el próximo movimiento del oponente. Para que el jugador Negro gane, debe de alguna manera cambiar el rumbo, tomar la ventaja y comenzar a presentar amenazas al jugador Blanco. Exploraremos el comportamiento de los jugadores negros y su conformidad con nuestros axiomas. Para hacerlo, construimos una aplicación que lee archivos de registro y analiza los movimientos de los jugadores Negros. La aplicación contiene dos componentes principales: (1) un algoritmo Min-Max para la evaluación de movimientos; (2) un detector de amenazas abiertas para descubrir amenazas abiertas. El algoritmo Min-Max funcionará hasta una profundidad d dada y, para cada movimiento α, devolverá el valor heurístico para la próxima acción tomada por el jugador según lo escrito en el archivo de registro, h(α), junto con el valor heurístico máximo, maxh(α), que podría lograrse antes de realizar el movimiento (obviamente, si h(α) = maxh(α), entonces el jugador no realizó el movimiento óptimo desde el punto de vista heurístico). El trabajo de los detectores de amenazas es notificar si se tomó alguna acción para bloquear una amenaza abierta (no bloquear una amenaza abierta probablemente hará que el jugador pierda en el próximo movimiento del oponente). La función heurística utilizada por Min-Max para evaluar la utilidad de los jugadores es la siguiente función, que es simple de calcular, pero proporciona un desafío razonable a los oponentes humanos: Definición 8. Sea Grupo un conjunto adyacente de cuatro cuadrados que son horizontales, verticales o diagonales. El Grupo n (Grupo w) es un Grupo con n piezas de color negro (blanco) y 4−n casillas vacías. h = ((Grupo1 negro ∗α)+(Grupo2 negro ∗β)+(Grupo3 negro ∗γ)+(Grupo4 negro ∗∞)) − ((Grupo1 blanco ∗α)+(Grupo2 blanco ∗β)+(Grupo3 blanco ∗γ)+(Grupo4 blanco ∗∞)) Los valores de α, β y δ pueden variar para formar cualquier combinación lineal deseada; sin embargo, es importante asignarles valores teniendo en cuenta el orden α < β < δ (usamos 1, 4 y 8 como sus valores respectivos). Grupos de 4 discos del mismo color significan victoria, por lo tanto, el descubrimiento de dicho grupo resultará en ∞ para asegurar un valor extremo. Ahora utilizamos nuestra función de evaluación estimada para evaluar las acciones de los jugadores Negros durante la interacción adversarial de Conecta Cuatro. Cada juego del archivo de registro fue introducido en la aplicación, la cual procesó y generó un archivo de registro reformateado que contenía el valor h del movimiento actual, el valor maxh que podría ser alcanzado y una notificación si se detectaba una amenaza abierta. Un total de 123 partidas fueron analizadas (57 con victoria de las blancas y 66 con victoria de las negras). Algunos juegos adicionales fueron ignorados manualmente en el experimento, debido a estos problemas: un jugador abandonando el juego mientras el resultado no es definitivo, o una jugada irracional y contundente en las primeras etapas del juego (por ejemplo, no bloquear un grupo obviamente ganador en las primeras jugadas de apertura). Además, también se eliminó un empate. El simulador se ejecutó con una profundidad de búsqueda de 3 movimientos. Ahora procedemos a analizar los juegos con respecto a cada axioma de comportamiento. El Sexto Internacional. La siguiente sección presenta las evaluaciones heurísticas del algoritmo Min-Max para cada acción, y verifica la cantidad y el alcance de las acciones tácticas subóptimas y sus implicaciones en el rendimiento. La Tabla 1 muestra los resultados y las percepciones del análisis heurístico de los juegos, cuando la profundidad de búsqueda es igual a 3 (esta profundidad de búsqueda fue seleccionada para que los resultados sean comparables a [9], ver Sección 3.2.3). La información heurística de las tablas es la diferencia entre el valor heurístico máximo presente y el valor heurístico de la acción que finalmente tomó el jugador (es decir, cuanto más cercano sea el número a 0, más cercana estuvo la acción a la acción heurística máxima). La primera fila presenta los valores de diferencia de la acción que tuvo el valor de diferencia máximo entre todas las acciones de los jugadores Negros en un juego dado, promediado en todos los juegos ganados y perdidos por los Negros (ver columnas respectivas). En los juegos en los que el jugador Negro pierde, su valor promedio de diferencia fue de -17.62, mientras que en los juegos en los que el jugador Negro ganó, su promedio fue de -12.02. La segunda fila amplía el análisis al considerar las 3 acciones con mayor diferencia heurística, y promediarlas. En ese caso, notamos una diferencia heurística promedio de 5 puntos entre los juegos en los que el jugador Negro pierde y los juegos en los que gana. Sin embargo, la importancia de esos números es que nos permitieron hacer una suposición educada sobre un número umbral de 11.5, como el valor de la constante TrH, que diferencia entre acciones normales y altamente beneficiosas. Después de encontrar una constante TrH aproximada, podemos proceder con un análisis de la importancia de los movimientos subóptimos. Para hacerlo, tomamos el subconjunto de juegos en los que el valor mínimo de diferencia heurística para las acciones de las negras era de 11.5. Como se presenta en la Tabla 2, podemos ver el promedio de 3 horas mínimo de los 3 mayores rangos y el porcentaje respectivo de juegos ganados. La primera fila muestra que el jugador Negro ganó solo el 12% de los juegos en los que el promedio de sus 3 acciones con mayor diferencia heurística (min3 h) era menor que el umbral sugerido, TrH = 11.5. La segunda fila muestra un resultado sorprendente: parece que cuando min3 h > −4, el jugador Negro rara vez gana. La intuición sugeriría que los juegos en los que los valores de evaluación de la acción estén más cerca de los valores máximos resultarán en más juegos ganados para las piezas negras. Sin embargo, parece que en el dominio de Conecta Cuatro, simplemente responder con acciones algo fácilmente esperadas, sin iniciar algunos movimientos sorprendentes y subóptimos, no produce buenos resultados. La última fila resume las principales conclusiones del análisis; la mayoría de las victorias de Blacks (83%) se produjeron cuando su min3 h estaba en el rango de -11.5 a -4. Una inspección detallada de esos juegos ganados por las fichas negras muestra el siguiente patrón detrás de los números: después de los movimientos de apertura estándar, las fichas negras de repente colocan una ficha en una columna aislada, lo cual parece ser un desperdicio de un movimiento. El blanco sigue construyendo sus amenazas, generalmente ignorando el último movimiento de las negras, que a su vez utiliza el disco aislado como ancla para una futura amenaza ganadora. Los resultados muestran que fue beneficioso para el jugador Negro en la Tabla 2: Porcentajes de ganancias de los Negros % de juegos min3 h < −11.5 12% min3 h > −4 5% −11.5 ≤ min3 h ≤ −4 83% tomar acciones subóptimas y no dar el valor heurístico más alto posible en ese momento, pero no será demasiado perjudicial para su posición (es decir, no dará un alto valor beneficioso a su adversario). Resultó que aprender el umbral es un aspecto importante del éxito: realizar movimientos extremadamente arriesgados (min3 h < −11.5) o intentar evitarlos (min3 h > −4) reduce significativamente las posibilidades de ganar de los jugadores negros. 3.2.2 Afirmación del Axioma de Monitoreo de Perfiles En la tarea de demostrar la importancia de monitorear los perfiles de los adversarios, nuestros archivos de registro no pudieron ser utilizados porque no contenían interacciones repetidas entre jugadores, que son necesarias para inferir el conocimiento de los jugadores sobre sus adversarios. Sin embargo, la importancia del modelado del oponente y su uso para obtener ventajas tácticas ya ha sido estudiada en varios ámbitos ([3, 9] son buenos ejemplos). En un artículo reciente, Markovitch y Reger [9] exploraron la noción de aprendizaje y explotación de la debilidad del oponente en interacciones competitivas. Aplican estrategias de aprendizaje simples analizando ejemplos de interacciones pasadas en un dominio específico. También utilizaron el dominio adversarial de Conecta Cuatro, que ahora se puede utilizar para comprender la importancia de monitorear el perfil del adversario. Tras la presentación de su modelo teórico, describen un extenso estudio empírico y verifican el rendimiento de los agentes después de aprender el modelo de debilidades con ejemplos pasados. Uno de los dominios utilizados como entorno competitivo fue el mismo juego de Conecta Cuatro (las Damas fueron el segundo dominio). Su función heurística era idéntica a la nuestra con tres variaciones diferentes (H1, H2 y H3) que se distinguen entre sí por los valores de sus coeficientes de combinación lineal. La profundidad de búsqueda para los jugadores fue de 3 (como en nuestro análisis). Sus extensos experimentos verifican y comparan diversas estrategias de aprendizaje, factores de riesgo, conjuntos de características predefinidas y métodos de uso. La conclusión es que el dominio de Conecta Cuatro muestra una mejora de una tasa de victoria de 0.556 antes de modelar a 0.69 después de modelar (página 22). Sus conclusiones, que muestran un rendimiento mejorado al sostener y utilizar el modelo del adversario, justifican el esfuerzo de monitorear el perfil del adversario para interacciones continuas y repetidas. Un punto adicional que surgió en sus experimentos es el siguiente: después de que se haya aprendido el modelo de debilidad del oponente, los autores describen diferentes métodos para integrar el modelo de debilidad del oponente en la estrategia de decisión de los agentes. Sin embargo, independientemente del método específico que elijan para trabajar, todos los métodos de integración podrían hacer que el agente tome decisiones subóptimas; podría hacer que el agente prefiera acciones que son subóptimas en el momento actual de la decisión, pero que podrían hacer que el oponente reaccione de acuerdo con su modelo de debilidad (como lo representa nuestro agente), lo cual a su vez será beneficioso para nosotros en el futuro. El comportamiento de los agentes, como se demuestra en [9], confirma y refuerza aún más nuestro Axioma Táctico Subóptimo discutido en la sección anterior. 556 The Sixth Intl. La necesidad de los axiomas de Logro de Objetivos, Acto Preventivo y Maximización de la Evaluación es evidente y no requiere más verificación. Sin embargo, incluso con respecto a esos axiomas, surgieron algunas ideas interesantes en el análisis de registros. Los axiomas de Logro y Prevención de Actos, aunque teóricamente triviales, parecen presentar un desafío para un jugador humano. En la inspección inicial de los registros, nos encontramos con algunos juegos donde un jugador, por razones inexplicables, no bloqueó al otro para que ganara o no logró ejecutar su propia jugada ganadora. Podemos atribuir esas fallas a la falta de atención de los humanos, o a un error de escritura en su respuesta al movimiento; sin embargo, esos errores podrían ocurrir en agentes racionales limitados, y el comportamiento apropiado debe ser axiomatizado. Un juego típico de Cuatro en Línea gira en torno a generar amenazas y bloquearlas. En nuestro análisis buscamos acciones preventivas explícitas, es decir, movimientos que bloqueen un grupo de 3 discos o que eliminen una amenaza futura (en nuestro horizonte de búsqueda limitado). Encontramos que en el 83% de los juegos totales, hubo al menos una acción preventiva tomada por el jugador Negro. También se encontró que Black promedió 2.8 acciones preventivas por juego en los juegos en los que perdió, mientras que promedió 1.5 acciones preventivas por juego al ganar. Parece que Black necesita 1 o 2 acciones preventivas para construir su posición inicial de toma, antes de comenzar a presentar amenazas. Si no logra ganar, generalmente evitará una amenaza adicional o dos antes de sucumbir ante el Blanco. 4. TRABAJO RELACIONADO Mucha investigación se ocupa de la axiomatización del trabajo en equipo y los estados mentales de los individuos: algunos modelos utilizan el conocimiento y la creencia [10], otros tienen modelos de metas e intenciones [8, 4]. Sin embargo, todas estas teorías formales tratan sobre el trabajo en equipo y la cooperación de agentes. Hasta donde sabemos, nuestro modelo es el primero en proporcionar un modelo formalizado para entornos adversarios explícitos y el comportamiento de los agentes en él. El algoritmo de búsqueda adversarial clásico Min-Max fue el primer intento de integrar al oponente en el espacio de búsqueda con la suposición débil de un oponente que juega de manera óptima. Desde entonces, se ha dedicado mucho esfuerzo a integrar el modelo del oponente en el procedimiento de toma de decisiones para predecir el comportamiento futuro. El algoritmo M∗ presentado por Carmel y Markovitch [2] mostró un método para incorporar modelos de oponentes en la búsqueda de adversarios, mientras que en [3] utilizaron el aprendizaje para proporcionar un modelo de oponente más preciso en un entorno de juego repetido de 2 jugadores, donde las estrategias de los agentes se modelaron como autómatas finitos. El trabajo adicional de planificación adversarial fue realizado por Willmott y colaboradores [13], quienes proporcionaron un enfoque de planificación adversarial para el juego de GO. La investigación mencionada anteriormente trató sobre la búsqueda adversarial y la integración de modelos de oponentes en métodos de búsqueda de utilidad clásicos. Ese trabajo muestra la importancia del modelado del oponente y la capacidad de explotarlo en beneficio de un agente. Sin embargo, las limitaciones básicas de esos métodos de búsqueda siguen aplicándose; nuestro modelo intenta superar esas limitaciones presentando un modelo formal para una nueva especificación adversarial basada en estados mentales. 5. CONCLUSIONES Presentamos un modelo de Entorno Adversarial para un agente racional acotado que está situado en un entorno de N jugadores, de suma cero. Estos fueron posteriormente eliminados del análisis final. Utilizamos la formalización de SharedPlans para definir el modelo y los axiomas que los agentes pueden aplicar como pautas de comportamiento. El modelo está destinado a ser utilizado como una guía para diseñar agentes que necesitan operar en entornos adversos. Presentamos resultados empíricos, basados en el análisis del archivo de registro de ConnectFour, que ejemplifican el modelo y los axiomas para una instancia bilateral del entorno. Los resultados que presentamos son un primer paso hacia un modelo ampliado que abarcará todos los tipos de entornos adversarios, por ejemplo, entornos que no son de suma cero y entornos que contienen agentes naturales que no forman parte del conflicto directo. Esos desafíos y más serán abordados en futuras investigaciones. 6. AGRADECIMIENTO Esta investigación fue apoyada en parte por las becas de la Fundación de Ciencia de Israel #1211/04 y #898/05. 7. REFERENCIAS [1] L. V. Allis. Un enfoque basado en el conocimiento de Conecta Cuatro: el juego está resuelto, gana el jugador Blanco. Tesis de maestría, Universidad Libre de Ámsterdam, Países Bajos, 1988. [2] D. Carmel y S. Markovitch. Incorporando modelos de oponentes en la búsqueda de adversarios. En Actas de la Decimotercera Conferencia Nacional de Inteligencia Artificial, páginas 120-125, Portland, Oregón, 1996. [3] D. Carmel y S. Markovitch. Modelado de oponentes en sistemas multiagente. En G. Weiß y S. Sen, editores, Adaptación y Aprendizaje en Sistemas Multiagente, páginas 40-52. Springer-Verlag, 1996. [4] B. J. Grosz y S. Kraus. Planes colaborativos para acciones grupales complejas. Inteligencia Artificial, 86(2):269-357, 1996. [5] M. Hadad, G. Kaminka, G. Armon y S. Kraus. Apoyando la actividad colaborativa. En Proc. de AAAI-2005, páginas 83-88, Pittsburgh, 2005. [6] http://www.gamerz.net/˜pbmserv/. [7] S. Kraus y D. Lehmann. Diseñando y construyendo un agente automatizado de negociación. Inteligencia Computacional, 11:132-171, 1995. [8] H. J. Levesque, P. R. Cohen y J. H. T. Nunes. Actuar juntos. En Proc. de AAAI-90, páginas 94-99, Boston, MA, 1990. [9] S. Markovitch y R. Reger. Aprender y aprovechar las debilidades relativas de los agentes oponentes. Agentes Autónomos y Sistemas Multiagente, 10(2):103-130, 2005. [10] Y. M. Ronald Fagin, Joseph Y. Halpern y M. Y. Vardi. Razonamiento sobre el conocimiento. MIT Press, Cambridge, Mass., 1995. [11] P. Thagard.\nMIT Press, Cambridge, Massachusetts, 1995. [11] P. Thagard. Resolución de problemas adversariales: Modelado de un oponente utilizando coherencia explicativa. Ciencia Cognitiva, 16(1):123-149, 1992. [12] R. W. Toseland y R. F. Rivas. Una introducción a la práctica del trabajo en grupo. Prentice Hall, Englewood Cliffs, NJ, segunda edición, 1995. [13] S. Willmott, J. Richardson, A. Bundy y J. Levine. Un enfoque de planificación adversarial para el juego de Go. Notas de conferencia en Ciencias de la Computación, 1558:93-112, 1999. El Sexto Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 557 ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "treatment group": {
            "translated_key": "Grupo de Tratamiento",
            "is_in_text": true,
            "original_annotated_sentences": [
                "An Adversarial Environment Model for Bounded Rational Agents in Zero-Sum Interactions Inon Zuckerman1 , Sarit Kraus1 , Jeffrey S. Rosenschein2 , Gal Kaminka1 1 Department of Computer Science 2 The School of Engineering Bar-Ilan University and Computer Science Ramat-Gan, Israel Hebrew University, Jerusalem, Israel {zukermi,sarit,galk}@cs.biu.ac.il jeff@cs.huji.ac.il ABSTRACT Multiagent environments are often not cooperative nor collaborative; in many cases, agents have conflicting interests, leading to adversarial interactions.",
                "This paper presents a formal Adversarial Environment model for bounded rational agents operating in a zero-sum environment.",
                "In such environments, attempts to use classical utility-based search methods can raise a variety of difficulties (e.g., implicitly modeling the opponent as an omniscient utility maximizer, rather than leveraging a more nuanced, explicit opponent model).",
                "We define an Adversarial Environment by describing the mental states of an agent in such an environment.",
                "We then present behavioral axioms that are intended to serve as design principles for building such adversarial agents.",
                "We explore the application of our approach by analyzing log files of completed Connect-Four games, and present an empirical analysis of the axioms appropriateness.",
                "Categories and Subject Descriptors I.2.11 [Artificial Intelligence]: Distributed Artificial Intelligence-Intelligent agents,Multiagent Systems; I.2.4 [Artificial Intelligence]: Knowledge Representation Formalisms and Methods -Modal logic General Terms Design, Theory 1.",
                "INTRODUCTION Early research in multiagent systems (MAS) considered cooperative groups of agents; because individual agents had limited resources, or limited access to information (e.g., limited processing power, limited sensor coverage), they worked together by design to solve problems that individually they could not solve, or at least could not solve as efficiently.",
                "MAS research, however, soon began to consider interacting agents with individuated interests, as representatives of different humans or organizations with non-identical interests.",
                "When interactions are guided by diverse interests, participants may have to overcome disagreements, uncooperative interactions, and even intentional attempts to damage one another.",
                "When these types of interactions occur, environments require appropriate behavior from the agents situated in them.",
                "We call these environments Adversarial Environments, and call the clashing agents Adversaries.",
                "Models of cooperation and teamwork have been extensively explored in MAS through the axiomatization of mental states (e.g., [8, 4, 5]).",
                "However, none of this research dealt with adversarial domains and their implications for agent behavior.",
                "Our paper addresses this issue by providing a formal, axiomatized mental state model for a subset of adversarial domains, namely simple zero-sum adversarial environments.",
                "Simple zero-sum encounters exist of course in various twoplayer games (e.g., Chess, Checkers), but they also exist in n-player games (e.g., Risk, Diplomacy), auctions for a single good, and elsewhere.",
                "In these latter environments especially, using a utility-based adversarial search (such as the Min-Max algorithm) does not always provide an adequate solution; the payoff function might be quite complex or difficult to quantify, and there are natural computational limitations on bounded rational agents.",
                "In addition, traditional search methods (like Min-Max) do not make use of a model of the opponent, which has proven to be a valuable addition to adversarial planning [9, 3, 11].",
                "In this paper, we develop a formal, axiomatized model for bounded rational agents that are situated in a zero-sum adversarial environment.",
                "The model uses different modality operators, and its main foundations are the SharedPlans [4] model for collaborative behavior.",
                "We explore environment properties and the mental states of agents to derive behavioral axioms; these behavioral axioms constitute a formal model that serves as a specification and design guideline for agent design in such settings.",
                "We then investigate the behavior of our model empirically using the Connect-Four board game.",
                "We show that this game conforms to our environment definition, and analyze players behavior using a large set of completed match log 550 978-81-904262-7-5 (RPS) c 2007 IFAAMAS files.",
                "In addition, we use the results presented in [9] to discuss the importance of opponent modeling in our ConnectFour adversarial domain.",
                "The paper proceeds as follows.",
                "Section 2 presents the models formalization.",
                "Section 3 presents the empirical analysis and its results.",
                "We discuss related work in Section 4, and conclude and present future directions in Section 5. 2.",
                "ADVERSARIAL ENVIRONMENTS The adversarial environment model (denoted as AE) is intended to guide the design of agents by providing a specification of the capabilities and mental attitudes of an agent in an adversarial environment.",
                "We focus here on specific types of adversarial environments, specified as follows: 1.",
                "Zero-Sum Interactions: positive and negative utilities of all agents sum to zero; 2.",
                "Simple AEs: all agents in the environment are adversarial agents; 3.",
                "Bilateral AEs: AEs with exactly two agents; 4.",
                "Multilateral AEs: AEs of three or more agents.",
                "We will work on both bilateral and multilateral instantiations of zero-sum and simple environments.",
                "In particular, our adversarial environment model will deal with interactions that consist of N agents (N ≥ 2), where all agents are adversaries, and only one agent can succeed.",
                "Examples of such environments range from board games (e.g., Chess, Connect-Four, and Diplomacy) to certain economic environments (e.g., N-bidder auctions over a single good). 2.1 Model Overview Our approach is to formalize the mental attitudes and behaviors of a single adversarial agent; we consider how a single agent perceives the AE.",
                "The following list specifies the conditions and mental states of an agent in a simple, zero-sum AE: 1.",
                "The agent has an individual intention that its own goal will be completed; 2.",
                "The agent has an individual belief that it and its adversaries are pursuing full conflicting goals (defined below)there can be only one winner; 3.",
                "The agent has an individual belief that each adversary has an intention to complete its own full conflicting goal; 4.",
                "The agent has an individual belief in the (partial) profile of its adversaries.",
                "Item 3 is required, since it might be the case that some agent has a full conflicting goal, and is currently considering adopting the intention to complete it, but is, as of yet, not committed to achieving it.",
                "This might occur because the agent has not yet deliberated about the effects that adopting that intention might have on the other intentions it is currently holding.",
                "In such cases, it might not consider itself to even be in an adversarial environment.",
                "Item 4 states that the agent should hold some belief about the profiles of its adversaries.",
                "The profile represents all the knowledge the agent has about its adversary: its weaknesses, strategic capabilities, goals, intentions, trustworthiness, and more.",
                "It can be given explicitly or can be learned from observations of past encounters. 2.2 Model Definitions for Mental States We use Grosz and Krauss definitions of the modal operators, predicates, and meta-predicates, as defined in their SharedPlan formalization [4].",
                "We recall here some of the predicates and operators that are used in that formalization: Int.To(Ai, α, Tn, Tα, C) represents Ais intentions at time Tn to do an action α at time Tα in the context of C. Int.Th(Ai, prop, Tn, Tprop, C) represents Ais intentions at time Tn that a certain proposition prop holds at time Tprop in the context of C. The potential intention operators, Pot.Int.To(...) and Pot.Int.Th(...), are used to represent the mental state when an agent considers adopting an intention, but has not deliberated about the interaction of the other intentions it holds.",
                "The operator Bel(Ai, f, Tf ) represents agent Ai believing in the statement expressed in formula f, at time Tf .",
                "MB(A, f, Tf ) represents mutual belief for a group of agents A.",
                "A snapshot of the system finds our environment to be in some state e ∈ E of environmental variable states, and each adversary in any LAi ∈ L of possible local states.",
                "At any given time step, the system will be in some world w of the set of all possible worlds w ∈ W, where w = E×LA1 ×LA2 × ...LAn , and n is the number of adversaries.",
                "For example, in a Texas Holdem poker game, an agents local state might be its own set of cards (which is unknown to its adversary) while the environment will consist of the betting pot and the community cards (which are visible to both players).",
                "A utility function under this formalization is defined as a mapping from a possible world w ∈ W to an element in , which expresses the desirability of the world, from a single agent perspective.",
                "We usually normalize the range to [0,1], where 0 represents the least desirable possible world, and 1 is the most desirable world.",
                "The implementation of the utility function is dependent on the domain in question.",
                "The following list specifies new predicates, functions, variables, and constants used in conjunction with the original definitions for the adversarial environment formalization: 1. φ is a null action (the agent does not do anything). 2.",
                "GAi is the set of agent Ais goals.",
                "Each goal is a set of predicates whose satisfaction makes the goal complete (we use G∗ Ai ∈ GAi to represent an arbitrary goal of agent Ai). 3. gAi is the set of agent Ais subgoals.",
                "Subgoals are predicates whose satisfaction represents an important milestone toward achievement of the full goal. gG∗ Ai ⊆ gAi is the set of subgoals that are important to the completion of goal G∗ Ai (we will use g∗ G∗ Ai ∈ gG∗ Ai to represent an arbitrary subgoal). 4.",
                "P Aj Ai is the profile object agent Ai holds about agent Aj. 5.",
                "CA is a general set of actions for all agents in A which are derived from the environments constraints.",
                "CAi ⊆ CA is the set of agent Ais possible actions. 6.",
                "Do(Ai, α, Tα, w) holds when Ai performs action α over time interval Tα in world w. 7.",
                "Achieve(G∗ Ai , α, w) is true when goal G∗ Ai is achieved following the completion of action α in world w ∈ W, where α ∈ CAi . 8.",
                "Profile(Ai, PAi Ai ) is true when agent Ai holds an object profile for agent Aj.",
                "Definition 1.",
                "Full conflict (FulConf ) describes a zerosum interaction where only a single goal of the goals in conflict can be completed.",
                "FulConf(G∗ Ai , G∗ Aj ) ⇒ (∃α ∈ CAi , ∀w, β ∈ CAj ) (Achieve(G∗ Ai , α, w) ⇒ ¬Achieve(G∗ Aj , β, w)) ∨ (∃β ∈ CAj , ∀w, α ∈ CAi )(Achieve(G∗ Aj , β, w) ⇒ ¬Achieve(G∗ Ai , α, w)) Definition 2.",
                "Adversarial Knowledge (AdvKnow) is a function returning a value which represents the amount of The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 551 knowledge agent Ai has on the profile of agent Aj, at time Tn.",
                "The higher the value, the more knowledge agent Ai has.",
                "AdvKnow : P Aj Ai × Tn → Definition 3.",
                "Eval - This evaluation function returns an estimated expected utility value for an agent in A, after completing an action from CA in some world state w. Eval : A × CA × w → Definition 4.",
                "TrH - (Threshold) is a numerical constant in the [0,1] range that represents an evaluation function (Eval) threshold value.",
                "An action that yields an estimated utility evaluation above the TrH is regarded as a highly beneficial action.",
                "The Eval value is an estimation and not the real utility function, which is usually unknown.",
                "Using the real utility value for a rational agent would easily yield the best outcome for that agent.",
                "However, agents usually do not have the real utility functions, but rather a heuristic estimate of it.",
                "There are two important properties that should hold for the evaluation function: Property 1.",
                "The evaluation function should state that the most desirable world state is one in which the goal is achieved.",
                "Therefore, after the goal has been satisfied, there can be no future action that can put the agent in a world state with higher Eval value. (∀Ai, G∗ Ai , α, β ∈ CAi , w ∈ W) Achieve(G∗ Ai , α, w) ⇒ Eval(Ai, α, w) ≥ Eval(Ai, β, w) Property 2.",
                "The evaluation function should project an action that causes a completion of a goal or a subgoal to a value which is greater than TrH (a highly beneficial action). (∀Ai, G∗ Ai ∈ GAi , α ∈ CAi , w ∈ W, g∗ GAi ∈ gGAi ) Achieve(G∗ Ai , α, w) ∨ Achieve(g∗ GAi , α, w) ⇒ Eval(Ai, α, w) ≥ TrH.",
                "Definition 5.",
                "SetAction We define a set action (SetAction) as a set of action operations (either complex or basic actions) from some action sets CAi and CAj which, according to agent Ais belief, are attached together by a temporal and consequential relationship, forming a chain of events (action, and its following consequent action). (∀α1 , . . . , αu ∈ CAi , β1 , . . . , βv ∈ CAj , w ∈ W) SetAction(α1 , . . . , αu , β1 , . . . , βv , w) ⇒ ((Do(Ai, α1 , Tα1 , w) ⇒ Do(Aj, β1 , Tβ1 , w)) ⇒ Do(Ai, α2 , Tα2 , w) ⇒ . . . ⇒ Do(Ai, αu , Tαu , w)) The consequential relation might exist due to various environmental constraints (when one action forces the adversary to respond with a specific action) or due to the agents knowledge about the profile of its adversary.",
                "Property 3.",
                "As the knowledge we have about our adversary increases we will have additional beliefs about its behavior in different situations which in turn creates new set actions.",
                "Formally, if our AdvKnow at time Tn+1 is greater than AdvKnow at time Tn, then every SetAction known at time Tn is also known at time Tn+1.",
                "AdvKnow(P Aj Ai , Tn+1) > AdvKnow(P Aj Ai , Tn) ⇒ (∀α1 , . . . , αu ∈ CAi , β1 , . . . , βv ∈ CAj ) Bel(Aag, SetAction(α1 , . . . , αu , β1 , . . . , βv ), Tn) ⇒ Bel(Aag, SetAction(α1 , . . . , αu , β1 , . . . , βv ), Tn+1) 2.3 The Environment Formulation The following axioms provide the formal definition for a simple, zero-sum Adversarial Environment (AE).",
                "Satisfaction of these axioms means that the agent is situated in such an environment.",
                "It provides specifications for agent Aag to interact with its set of adversaries A with respect to goals G∗ Aag and G∗ A at time TCo at some world state w. AE(Aag, A, G∗ Aag , A1, . . . , Ak, G∗ A1 , . . . , G∗ Ak , Tn, w) 1.",
                "Aag has an Int.Th his goal would be completed: (∃α ∈ CAag , Tα) Int.Th(Aag, Achieve(G∗ Aag , α), Tn, Tα, AE) 2.",
                "Aag believes that it and each of its adversaries Ao are pursuing full conflicting goals: (∀Ao ∈ {A1, . . . , Ak}) Bel(Aag, FulConf(G∗ Aag , G∗ Ao ), Tn) 3.",
                "Aag believes that each of his adversaries in Ao has the Int.Th his conflicting goal G∗ Aoi will be completed: (∀Ao ∈ {A1, . . . , Ak})(∃β ∈ CAo , Tβ) Bel(Aag, Int.Th(Ao, Achieve(G∗ Ao , β), TCo, Tβ, AE), Tn) 4.",
                "Aag has beliefs about the (partial) profiles of its adversaries (∀Ao ∈ {A1, . . . , Ak}) (∃PAo Aag ∈ PAag )Bel(Aag, Profile(Ao, PAo Aag ), Tn) To build an agent that will be able to operate successfully within such an AE, we must specify behavioral guidelines for its interactions.",
                "Using a naive Eval maximization strategy to a certain search depth will not always yield satisfactory results for several reasons: (1) the search horizon problem when searching for a fixed depth; (2) the strong assumption of an optimally rational, unbounded resources adversary; (3) using an estimated evaluation function which will not give optimal results in all world states, and can be exploited [9].",
                "The following axioms specify the behavioral principles that can be used to differentiate between successful and less successful agents in the above Adversarial Environment.",
                "Those axioms should be used as specification principles when designing and implementing agents that should be able to perform well in such Adversarial Environments.",
                "The behavioral axioms represent situations in which the agent will adopt potential intentions to (Pot.Int.To(...)) perform an action, which will typically require some means-end reasoning to select a possible course of action.",
                "This reasoning will lead to the adoption of an Int.To(...) (see [4]).",
                "A1.",
                "Goal Achieving Axiom.",
                "The first axiom is the simplest case; when the agent Aag believes that it is one action (α) away from achieving his conflicting goal G∗ Aag , it should adopt the potential intention to do α and complete its goal. (∀Aag, α ∈ CAag , Tn, Tα, w ∈ W) (Bel(Aag, Do(Aag, α, Tα, w) ⇒ Achieve(G∗ Aag , α, w)) ⇒ Pot.Int.To(Aag, α, Tn, Tα, w) This somewhat trivial behavior is the first and strongest axiom.",
                "In any situation, when the agent is an action away from completing the goal, it should complete the action.",
                "Any fair Eval function would naturally classify α as the maximal value action (property 1).",
                "However, without explicit axiomatization of such behavior there might be situations where the agent will decide on taking another action for various reasons, due to its bounded decision resources.",
                "A2.",
                "Preventive Act Axiom.",
                "Being in an adversarial situation, agent Aag might decide to take actions that will damage one of its adversarys plans to complete its goal, even if those actions do not explicitly advance Aag towards its conflicting goal G∗ Aag .",
                "Such preventive action will take place when agent Aag has a belief about the possibility of its adversary Ao doing an action β that will give it a high 552 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) utility evaluation value (> TrH).",
                "Believing that taking action α will prevent the opponent from doing its β, it will adopt a potential intention to do α. (∀Aag, Ao ∈ A, α ∈ CAag , β ∈ CAo , Tn, Tβ, w ∈ W) (Bel(Aag, Do(Ao, β, Tβ, w) ∧ Eval(Ao, β, w) > TrH, Tn) ∧ Bel(Aag, Do(Aag, α, Tα, w) ⇒ ¬Do(Ao, β, Tβ, w), Tn) ⇒ Pot.Int.To(Aag, α, Tn, Tα, w) This axiom is a basic component of any adversarial environment.",
                "For example, looking at a Chess board game, a player could realize that it is about to be checkmated by its opponent, thus making a preventive move.",
                "Another example is a Connect Four game: when a player has a row of three chips, its opponent must block it, or lose.",
                "A specific instance of A1 occurs when the adversary is one action away from achieving its goal, and immediate preventive action needs to be taken by the agent.",
                "Formally, we have the same beliefs as stated above, with a changed belief that doing action β will cause agent Ao to achieve its goal.",
                "Proposition 1: Prevent or lose case. (∀Aag, Ao ∈ A, α ∈ CAag , β ∈ CAo , G∗ Ao , Tn, Tα, Tβ, w ∈ W) Bel(Aag, Do(Ao, β, Tβ, w) ⇒ Achieve(G∗ Ao , β, w), Tn) ∧ Bel(Aag, Do(Aag, α, Tα, w) ⇒ ¬Do(Ao, β, Tβ, w)) ⇒ Pot.Int.To(Aag, α, Tn, Tα, w) Sketch of proof: Proposition 1 can be easily derived from axiom A1 and the property 2 of the Eval function, which states that any action that causes a completion of a goal is a highly beneficial action.",
                "The preventive act behavior will occur implicitly when the Eval function is equal to the real world utility function.",
                "However, being bounded rational agents and dealing with an estimated evaluation function we need to explicitly axiomatize such behavior, for it will not always occur implicitly from the evaluation function.",
                "A3.",
                "Suboptimal Tactical Move Axiom.",
                "In many scenarios a situation may occur where an agent will decide not to take the current most beneficial action it can take (the action with the maximal utility evaluation value), because it believes that taking another action (with lower utility evaluation value) might yield (depending on the adversarys response) a future possibility for a highly beneficial action.",
                "This will occur most often when the Eval function is inaccurate and differs by a large extent from the Utility function.",
                "Put formally, agent Aag believes in a certain SetAction that will evolve according to its initial action and will yield a high beneficial value (> TrH) solely for it. (∀Aag, Ao ∈ A, Tn, w ∈ W) (∃α1 , . . . , αu ∈ CAi , β1 , . . . , βv ∈ CAj , Tα1 ) Bel(Aag, SetAction(α1 , . . . , αu , β1 , . . . , βv ), Tn) ∧ Bel(Aag, Eval(Ao, βv , w) < TrH < Eval(Aag, αu , w), Tn) ⇒ Pot.Int.To(Aag, α1 , Tn, Tα1 , w) An agent might believe that a chain of events will occur for various reasons due to the inevitable nature of the domain.",
                "For example, in Chess, we often observe the following: a move causes a check position, which in turn limits the opponents moves to avoiding the check, to which the first player might react with another check, and so on.",
                "The agent might also believe in a chain of events based on its knowledge of its adversarys profile, which allows it to foresee the adversarys movements with high accuracy.",
                "A4.",
                "Profile Detection Axiom.",
                "The agent can adjust its adversarys profiles by observations and pattern study (specifically, if there are repeated encounters with the same adversary).",
                "However, instead of waiting for profile information to be revealed, an agent can also initiate actions that will force its adversary to react in a way that will reveal profile knowledge about it.",
                "Formally, the axiom states that if all actions (γ) are not highly beneficial actions (< TrH), the agent can do action α in time Tα if it believes that it will result in a non-highly beneficial action β from its adversary, which in turn teaches it about the adversarys profile, i.e., gives a higher AdvKnow(P Aj Ai , Tβ). (∀Aag, Ao ∈ A, α ∈ CAag , β ∈ CAo , Tn, Tα, Tβ, w ∈ W) Bel(Aag, (∀γ ∈ CAag )Eval(Aag, γ, w) < TrH, Tn) ∧ Bel(Aag, Do(Aag, α, Tα, w) ⇒ Do(Ao, β, Tβ, w), Tn) ∧ Bel(Aag, Eval(Ao, β, w) < TrH) ∧ Bel(Aag, AdvKnow(P Aj Ai , Tβ) > AdvKnow(P Aj Ai , Tn), Tn) ⇒ Pot.Int.To(Aag, α, Tn, Tα, w) For example, going back to the Chess board game scenario, consider starting a game versus an opponent about whom we know nothing, not even if it is a human or a computerized opponent.",
                "We might start playing a strategy that will be suitable versus an average opponent, and adjust our game according to its level of play.",
                "A5.",
                "Alliance Formation Axiom The following behavioral axiom is relevant only in a multilateral instantiation of the adversarial environment (obviously, an alliance cannot be formed in a bilateral, zero-sum encounter).",
                "In different situations during a multilateral interaction, a group of agents might believe that it is in their best interests to form a temporary alliance.",
                "Such an alliance is an agreement that constrains its members behavior, but is believed by its members to enable them to achieve a higher utility value than the one achievable outside of the alliance.",
                "As an example, we can look at the classical Risk board game, where each player has an individual goal of being the sole conquerer of the world, a zero-sum game.",
                "However, in order to achieve this goal, it might be strategically wise to make short-term ceasefire agreements with other players, or to join forces and attack an opponent who is stronger than the rest.",
                "An alliances terms defines the way its members should act.",
                "It is a set of predicates, denoted as Terms, that is agreed upon by the alliance members, and should remain true for the duration of the alliance.",
                "For example, the set Terms in the Risk scenario, could contain the following predicates: 1.",
                "Alliance members will not attack each other on territories X, Y and Z; 2.",
                "Alliance members will contribute C units per turn for attacking adversary Ao; 3.",
                "Members are obligated to stay as part of the alliance until time Tk or until adversarys Ao army is smaller than Q.",
                "The set Terms specifies inter-group constraints on each of the alliance members (∀Aal i ∈ Aal ⊆ A) set of actions Cal i ⊆ C. Definition 6.",
                "Al val - the total evaluation value that agent Ai will achieve while being part of Aal is the sum of Evali (Eval for Ai) of each of Aal j Eval values after taking their own α actions (via the agent(α) predicate): Al val(Ai, Cal , Aal , w) = α∈Cal Evali(Aal j , agent(α), w) Definition 7.",
                "Al TrH - is a number representing an Al val The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 553 threshold; above it, the alliance can be said to be a highly beneficial alliance.",
                "The value of Al TrH will be calculated dynamically according to the progress of the interaction, as can be seen in [7].",
                "After an alliance is formed, its members are now working in their normal adversarial environment, as well as according to the mental states and axioms required for their interactions as part of the alliance.",
                "The following Alliance model (AL) specifies the conditions under which the group Aal can be said to be in an alliance and working with a new and constrained set of actions Cal , at time Tn.",
                "AL(Aal , Cal , w, Tn) 1.",
                "Aal has a MB that all members are part of Aal : MB(Aal , (∀Aal i ∈ Aal )member(Aal i , Aal ), Tn) 2.",
                "Aal has a MB that the group be maintained: MB(Aal , (∀Aal i ∈ Aal )Int.Th (Ai, member(Ai, Aal ), Tn, Tn+1, Co), Tn) 3.",
                "Aal has a MB that being members gives them high utility value: MB(Aal , (∀Aal i ∈ Aal )Al val(Aal i , Cal , Aal , w) ≥ Al TrH, Tn) Members profiles are a crucial part of successful alliances.",
                "We assume that agents that have more accurate profiles of their adversaries will be more successful in such environments.",
                "Such agents will be able to predict when a member is about to breach the alliances contract (item 2 in the above model), and take counter measures (when item 3 will falsify).",
                "The robustness of the alliance is in part a function of its members trustfulness measure, objective position estimation, and other profile properties.",
                "We should note that an agent can simultaneously be part of more than one alliance.",
                "Such a temporary alliance, where the group members do not have a joint goal but act collaboratively for the interest of their own individual goals, is classified as a <br>treatment group</br> by modern psychologists [12] (in contrast to a Task Group, where its members have a joint goal).",
                "The Shared Activity model as presented in [5] modeled <br>treatment group</br> behavior using the same SharedPlans formalization.",
                "When comparing both definitions of an alliance and a <br>treatment group</br> we found an unsurprising resemblance between both models: the environment models definitions are almost identical (see SAs definitions in [5]), and their Selfish-Act and Cooperative Act axioms conform to our adversarial agents behavior.",
                "The main distinction between both models is the integration of a Helpful-behavior act axiom, in the Shared Activity which cannot be part of ours.",
                "This axiom states that an agent will consider taking action that will lower its Eval value (to a certain lower bound), if it believes that a group partner will gain a significant benefit.",
                "Such behavior cannot occur in a pure adversarial environment (as a zero-sum game is), where the alliance members are constantly on watch to manipulate their alliance to their own advantage.",
                "A6.",
                "Evaluation Maximization Axiom.",
                "In a case when all other axioms are inapplicable, we will proceed with the action that maximizes the heuristic value as computed in the Eval function. (∀Aag, Ao ∈ A, α ∈ Cag, Tn, w ∈ W) Bel(Aag, (∀γ ∈ Cag)Eval(Aag, α, w) ≥ Eval(Aag, γ, w), Tn) ⇒ Pot.Int.To(Aag, α, Tn, Tα, w) T1.",
                "Optimality on Eval = Utility The above axiomatic model handles situations where the Utility is unknown and the agents are bounded rational agents.",
                "The following theorem shows that in bilateral interactions, where the agents have the real Utility function (i.e., Eval = Utility) and are rational agents, the axioms provide the same optimal result as classic adversarial search (e.g., Min-Max).",
                "Theorem 1.",
                "Let Ae ag be an unbounded rational AE agent using the Eval heuristic evaluation function, Au ag be the same agent using the true Utility function, and Ao be a sole unbounded utility-based rational adversary.",
                "Given that Eval = Utility: (∀α ∈ CAu ag , α ∈ CAe ag , Tn, w ∈ W) Pot.Int.To(Au ag, α, Tn, Tα, w) → Pot.Int.To(Ae ag, α , Tn, Tα, w) ∧ ((α = α ) ∨ (Utility(Au ag, α, w) = Eval(Ae ag, α , w))) Sketch of proof - Given that Au ag has the real utility function and unbounded resources, it can generate the full game tree and run the optimal MinMax algorithm to choose the highest utility value action, which we denote by, α.",
                "The proof will show that Ae ag, using the AE axioms, will select the same or equal utility α (when there is more than one action with the same max utility) when Eval = Utility. (A1) Goal achieving axiom - suppose there is an α such that its completion will achieve Au ags goal.",
                "It will obtain the highest utility by Min-Max for Au ag.",
                "The Ae ag agent will select α or another action with the same utility value via A1.",
                "If such α does not exist, Ae ag cannot apply this axiom, and proceeds to A2. (A2) Preventive act axiom - (1) Looking at the basic case (see Prop1), if there is a β which leads Ao to achieve its goal, then a preventive action α will yield the highest utility for Au ag.",
                "Au ag will choose it through the utility, while Ae ag will choose it through A2. (2) In the general case, β is a highly beneficial action for Ao, thus yields low utility for Au ag, which will guide it to select an α that will prevent β, while Ae ag will choose it through A2.1 If such β does not exist for Ao, then A2 is not applicable, and Ae ag can proceed to A3. (A3) Suboptimal tactical move axiom - When using a heuristic Eval function, Ae ag has a partial belief in the profile of its adversary (item 4 in AE model), which may lead it to believe in SetActions (Prop1).",
                "In our case, Ae ag is holding a full profile on its optimal adversary and knows that Ao will behave optimally according to the real utility values on the complete search tree, therefore, any belief about suboptimal SetAction cannot exist, yielding this axiom inapplicable.",
                "Ae ag will proceed to A4. (A4) Profile detection axiom - Given that Ae ag has the full profile of Ao, none of Ae ags actions can increase its knowledge.",
                "That axiom will not be applied, and the agent will proceed with A6 (A5 will be disregarded because the interaction is bilateral). (A6) Evaluation maximization axiom - This axiom will select the max Eval for Ae ag.",
                "Given that Eval = Utility, the same α that was selected by Au ag will be selected. 3.",
                "EVALUATION The main purpose of our experimental analysis is to evaluate the models behavior and performance in a real adversarial environment.",
                "This section investigates whether bounded 1 A case where following the completion of β there exists a γ which gives high utility for Agent Au ag, cannot occur because Ao uses the same utility, and γs existence will cause it to classify β as a low utility action. 554 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) rational agents situated in such adversarial environments will be better off applying our suggested behavioral axioms. 3.1 The Domain To explore the use of the above model and its behavioral axioms, we decided to use the Connect-Four game as our adversarial environment.",
                "Connect-Four is a 2-player, zerosum game which is played using a 6x7 matrix-like board.",
                "Each turn, a player drops a disc into one of the 7 columns (the set of 21 discs is usually colored yellow for player 1 and red for player 2; we will use White and Black respectively to avoid confusion).",
                "The winner is the first player to complete a horizontal, vertical, or diagonal set of four discs with its color.",
                "On very rare occasions, the game might end in a tie if all the empty grids are filled, but no player managed to create a 4-disc set.",
                "The Connect-Four game was solved in [1], where it is shown that the first player (playing with the white discs) can force a win by starting in the middle column (column 4) and playing optimally However, the optimal strategy is very complex, and difficult to follow even for complex bounded rational agents, such as human players.",
                "Before we can proceed checking agent behavior, we must first verify that the domain conforms to the adversarial environments definition as given above (which the behavioral axioms are based on).",
                "First, when playing a Connect-Four game, the agent has an intention to win the game (item 1).",
                "Second (item 2), our agent believes that in Connect-Four there can only be one winner (or no winner at all in the rare occurrence of a tie).",
                "In addition, our agent believes that its opponent to the game will try to win (item 3), and we hope it has some partial knowledge (item 4) about its adversary (this knowledge can vary from nothing, through simple facts such as age, to strategies and weaknesses).",
                "Of course, not all Connect-Four encounters are adversarial.",
                "For example, when a parent is playing the game with its child, the following situation might occur: the child, having a strong incentive to win, treats the environment as adversarial (it intends to win, understands that there can only be one winner, and believes that its parent is trying to beat him).",
                "However, the parents point of view might see the environment as an educational one, where its goal is not to win the game, but to cause enjoyment or practice strategic reasoning.",
                "In such an educational environment, a new set of behavioral axioms might be more beneficial to the parents goals than our suggested adversarial behavioral axioms. 3.2 Axiom Analysis After showing that the Connect-Four game is indeed a zero-sum, bilateral adversarial environment, the next step is to look at players behaviors during the game and check whether behaving according to our model does improve performance.",
                "To do so we have collected log files from completed Connect-Four games that were played by human players over the Internet.",
                "Our collected log file data came from Play by eMail (PBeM) sites.",
                "These are web sites that host email games, where each move is taken by an email exchange between the server and the players.",
                "Many such sites archives contain real competitive interactions, and also maintain a ranking system for their members.",
                "Most of the data we used can be found in [6].",
                "As can be learned from [1], Connect-Four has an optimal strategy and a considerable advantage for the player who starts the game (which we call the White player).",
                "We will concentrate in our analysis on the second players moves (to be called Black).",
                "The White player, being the first to act, has the so-called initiative advantage.",
                "Having the advantage and a good strategy will keep the Black player busy reacting to its moves, instead of initiating threats.",
                "A threat is a combination of three discs of the same color, with an empty spot for the fourth winning disk.",
                "An open threat is a threat that can be realized in the opponents next move.",
                "In order for the Black player to win, it must somehow turn the tide, take the advantage and start presenting threats to the White player.",
                "We will explore Black players behavior and their conformance to our axioms.",
                "To do so, we built an application that reads log files and analyzes the Black players moves.",
                "The application contains two main components: (1) a Min-Max algorithm for evaluation of moves; (2) open threats detector for the discovering of open threats.",
                "The Min-Max algorithm will work to a given depth, d and for each move α will output the heuristic value for the next action taken by the player as written in the log file, h(α), alongside the maximum heuristic value, maxh(α), that could be achieved prior to taking the move (obviously, if h(α) = maxh(α), then the player did not do the optimal move heuristically).",
                "The threat detectors job is to notify if some action was taken in order to block an open threat (not blocking an open threat will probably cause the player to lose in the opponents next move).",
                "The heuristic function used by Min-Max to evaluate the players utility is the following function, which is simple to compute, yet provides a reasonable challenge to human opponents: Definition 8.",
                "Let Group be an adjacent set of four squares that are horizontal, vertical, or diagonal.",
                "Groupn b (Groupn w) be a Group with n pieces of the black (white) color and 4−n empty squares. h = ((Group1 b ∗α)+(Group2 b ∗β)+(Group3 b ∗γ)+(Group4 b ∗∞)) − ((Group1 w ∗α)+(Group2 w ∗β)+(Group3 w ∗γ)+(Group4 w ∗∞)) The values of α, β and δ can vary to form any desired linear combination; however, it is important to value them with the α < β < δ ordering in mind (we used 1, 4, and 8 as their respective values).",
                "Groups of 4 discs of the same color means victory, thus discovery of such a group will result in ∞ to ensure an extreme value.",
                "We now use our estimated evaluation function to evaluate the Black players actions during the Connect-Four adversarial interaction.",
                "Each game from the log file was input into the application, which processed and output a reformatted log file containing the h value of the current move, the maxh value that could be achieved, and a notification if an open threat was detected.",
                "A total of 123 games were analyzed (57 with White winning, and 66 with Black winning).",
                "A few additional games were manually ignored in the experiment, due to these problems: a player abandoning the game while the outcome is not final, or a blunt irrational move in the early stages of the game (e.g., not blocking an obvious winning group in the first opening moves).",
                "In addition, a single tie game was also removed.",
                "The simulator was run to a search depth of 3 moves.",
                "We now proceed to analyze the games with respect to each behavioral axiom.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 555 Table 1: Average heuristic difference analysis Black losses Black Won Avg minh -17.62 -12.02 Avg 3 lowest h moves (min3 h) -13.20 -8.70 3.2.1 Affirming the Suboptimal tactical move axiom The following section presents the heuristic evaluations of the Min-Max algorithm for each action, and checks the amount and extent of suboptimal tactical actions and their implications on performance.",
                "Table 1 shows results and insights from the games heuristic analysis, when search depth equals 3 (this search depth was selected for the results to be comparable to [9], see Section 3.2.3).",
                "The tables heuristic data is the difference between the present maximal heuristic value and the heuristic value of the action that was eventually taken by the player (i.e., the closer the number is to 0, the closer the action was to the maximum heuristic action).",
                "The first row presents the difference values of the action that had the maximal difference value among all the Black players actions in a given game, as averaged over all Blacks winning and losing games (see respective columns).",
                "In games in which the Black player loses, its average difference value was -17.62, while in games in which the Black player won, its average was -12.02.",
                "The second row expands the analysis by considering the 3 highest heuristic difference actions, and averaging them.",
                "In that case, we notice an average heuristic difference of 5 points between games which the Black player loses and games in which it wins.",
                "Nevertheless, the importance of those numbers is that they allowed us to take an educated guess on a threshold number of 11.5, as the value of the TrH constant, which differentiates between normal actions and highly beneficial ones.",
                "After finding an approximated TrH constant, we can proceed with an analysis of the importance of suboptimal moves.",
                "To do so we took the subset of games in which the minimum heuristic difference value for Blacks actions was 11.5.",
                "As presented in Table 2, we can see the different min3 h average of the 3 largest ranges and the respective percentage of games won.",
                "The first row shows that the Black player won only 12% of the games in which the average of its 3 highest heuristically difference actions (min3 h) was smaller than the suggested threshold, TrH = 11.5.",
                "The second row shows a surprising result: it seems that when min3 h > −4 the Black player rarely wins.",
                "Intuition would suggest that games in which the action evaluation values were closer to the maximal values will result in more winning games for Black.",
                "However, it seems that in the Connect-Four domain, merely responding with somewhat easily expected actions, without initiating a few surprising and suboptimal moves, does not yield good results.",
                "The last row sums up the main insights from the analysis; most of Blacks wins (83%) came when its min3 h was in the range of -11.5 to -4.",
                "A close inspection of those Black winning games shows the following pattern behind the numbers: after standard opening moves, Black suddenly drops a disc into an isolated column, which seems a waste of a move.",
                "White continues to build its threats, while usually disregarding Blacks last move, which in turn uses the isolated disc as an anchor for a future winning threat.",
                "The results show that it was beneficial for the Black player Table 2: Blacks winnings percentages % of games min3 h < −11.5 12% min3 h > −4 5% −11.5 ≤ min3 h ≤ −4 83% to take suboptimal actions and not give the current highest possible heuristic value, but will not be too harmful for its position (i.e., will not give high beneficial value to its adversary).",
                "As it turned out, learning the threshold is an important aspect of success: taking wildly risky moves (min3 h < −11.5) or trying to avoid them (min3 h > −4) reduces the Black players winning chances by a large margin. 3.2.2 Affirming the Profile Monitoring Axiom In the task of showing the importance of monitoring ones adversaries profiles, our log files could not be used because they did not contain repeated interactions between players, which are needed to infer the players knowledge about their adversaries.",
                "However, the importance of opponent modeling and its use in attaining tactical advantages was already studied in various domains ([3, 9] are good examples).",
                "In a recent paper, Markovitch and Reger [9] explored the notion of learning and exploitation of opponent weakness in competitive interactions.",
                "They apply simple learning strategies by analyzing examples from past interactions in a specific domain.",
                "They also used the Connect-Four adversarial domain, which can now be used to understand the importance of monitoring the adversarys profile.",
                "Following the presentation of their theoretical model, they describe an extensive empirical study and check the agents performance after learning the weakness model with past examples.",
                "One of the domains used as a competitive environment was the same Connect-Four game (Checkers was the second domain).",
                "Their heuristic function was identical to ours with three different variations (H1, H2, and H3) that are distinguished from one another in their linear combination coefficient values.",
                "The search depth for the players was 3 (as in our analysis).",
                "Their extensive experiments check and compare various learning strategies, risk factors, predefined feature sets and usage methods.",
                "The bottom line is that the Connect-Four domain shows an improvement from a 0.556 winning rate before modeling to a 0.69 after modeling (page 22).",
                "Their conclusions, showing improved performance when holding and using the adversarys model, justify the effort to monitor the adversary profile for continuous and repeated interactions.",
                "An additional point that came up in their experiments is the following: after the opponent weakness model has been learned, the authors describe different methods of integrating the opponent weakness model into the agents decision strategy.",
                "Nevertheless, regardless of the specific method they chose to work with, all integration methods might cause the agent to take suboptimal decisions; it might cause the agent to prefer actions that are suboptimal at the present decision junction, but which might cause the opponent to react in accordance with its weakness model (as represented by our agent) which in turn will be beneficial for us in the future.",
                "The agents behavior, as demonstrated in [9] further confirms and strengthens our Suboptimal Tactical Axiom as discussed in the previous section. 556 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 3.2.3 Additional Insights The need for the Goal Achieving, Preventive Act, and Evaluation Maximization axioms are obvious, and need no further verification.",
                "However, even with respect to those axioms, a few interesting insights came up in the log analysis.",
                "The Goal achieving and Preventive Act axioms, though theoretically trivial, seem to provide some challenge to a human player.",
                "In the initial inspection of the logs, we encountered few games2 where a player, for inexplicable reasons, did not block the other from winning or failed to execute its own winning move.",
                "We can blame those faults on the humans lack of attention, or a typing error in its move reply; nevertheless, those errors might occur in bounded rational agents, and the appropriate behavior needs to be axiomatized.",
                "A typical Connect-Four game revolves around generating threats and blocking them.",
                "In our analysis we looked for explicit preventive actions, i.e., moves that block a group of 3 discs, or that remove a future threat (in our limited search horizon).",
                "We found that in 83% of the total games there was at least one preventive action taken by the Black player.",
                "It was also found that Black averaged 2.8 preventive actions per game on the games in which it lost, while averaging 1.5 preventive actions per game when winning.",
                "It seems that Black requires 1 or 2 preventive actions to build its initial taking position, before starting to present threats.",
                "If it did not manage to win, it will usually prevent an extra threat or two before succumbing to White. 4.",
                "RELATED WORK Much research deals with the axiomatization of teamwork and mental states of individuals: some models use knowledge and belief [10], others have models of goals and intentions [8, 4].",
                "However, all these formal theories deal with agent teamwork and cooperation.",
                "As far as we know, our model is the first to provide a formalized model for explicit adversarial environments and agents behavior in it.",
                "The classical Min-Max adversarial search algorithm was the first attempt to integrate the opponent into the search space with a weak assumption of an optimally playing opponent.",
                "Since then, much effort has gone into integrating the opponent model into the decision procedure to predict future behavior.",
                "The M∗ algorithm presented by Carmel and Markovitch [2] showed a method of incorporating opponent models into adversary search, while in [3] they used learning to provide a more accurate opponent model in a 2player repeated game environment, where agents strategies were modeled as finite automata.",
                "Additional Adversarial planning work was done by Willmott et al. [13], which provided an adversarial planning approach to the game of GO.",
                "The research mentioned above dealt with adversarial search and the integration of opponent models into classical utilitybased search methods.",
                "That work shows the importance of opponent modeling and the ability to exploit it to an agents advantage.",
                "However, the basic limitations of those search methods still apply; our model tries to overcome those limitations by presenting a formal model for a new, mental state-based adversarial specification. 5.",
                "CONCLUSIONS We presented an Adversarial Environment model for a 2 These were later removed from the final analysis. bounded rational agent that is situated in an N-player, zerosum environment.",
                "We used the SharedPlans formalization to define the model and the axioms that agents can apply as behavioral guidelines.",
                "The model is meant to be used as a guideline for designing agents that need to operate in such adversarial environments.",
                "We presented empirical results, based on ConnectFour log file analysis, that exemplify the model and the axioms for a bilateral instance of the environment.",
                "The results we presented are a first step towards an expanded model that will cover all types of adversarial environments, for example, environments that are non-zero-sum, and environments that contain natural agents that are not part of the direct conflict.",
                "Those challenges and more will be dealt with in future research. 6.",
                "ACKNOWLEDGMENT This research was supported in part by Israel Science Foundation grants #1211/04 and #898/05. 7.",
                "REFERENCES [1] L. V. Allis.",
                "A knowledge-based approach of Connect-Four - the game is solved: White wins.",
                "Masters thesis, Free University, Amsterdam, The Netherlands, 1988. [2] D. Carmel and S. Markovitch.",
                "Incorporating opponent models into adversary search.",
                "In Proceedings of the Thirteenth National Conference on Artificial Intelligence, pages 120-125, Portland, OR, 1996. [3] D. Carmel and S. Markovitch.",
                "Opponent modeling in multi-agent systems.",
                "In G. Weiß and S. Sen, editors, Adaptation and Learning in Multi-Agent Systems, pages 40-52.",
                "Springer-Verlag, 1996. [4] B. J. Grosz and S. Kraus.",
                "Collaborative plans for complex group action.",
                "Artificial Intelligence, 86(2):269-357, 1996. [5] M. Hadad, G. Kaminka, G. Armon, and S. Kraus.",
                "Supporting collaborative activity.",
                "In Proc. of AAAI-2005, pages 83-88, Pittsburgh, 2005. [6] http://www.gamerz.net/˜pbmserv/. [7] S. Kraus and D. Lehmann.",
                "Designing and building a negotiating automated agent.",
                "Computational Intelligence, 11:132-171, 1995. [8] H. J. Levesque, P. R. Cohen, and J. H. T. Nunes.",
                "On acting together.",
                "In Proc. of AAAI-90, pages 94-99, Boston, MA, 1990. [9] S. Markovitch and R. Reger.",
                "Learning and exploiting relative weaknesses of opponent agents.",
                "Autonomous Agents and Multi-Agent Systems, 10(2):103-130, 2005. [10] Y. M. Ronald Fagin, Joseph Y. Halpern and M. Y. Vardi.",
                "Reasoning about knowledge.",
                "MIT Press, Cambridge, Mass., 1995. [11] P. Thagard.",
                "Adversarial problem solving: Modeling an oponent using explanatory coherence.",
                "Cognitive Science, 16(1):123-149, 1992. [12] R. W. Toseland and R. F. Rivas.",
                "An Introduction to Group Work Practice.",
                "Prentice Hall, Englewood Cliffs, NJ, 2nd edition edition, 1995. [13] S. Willmott, J. Richardson, A. Bundy, and J. Levine.",
                "An adversarial planning approach to Go.",
                "Lecture Notes in Computer Science, 1558:93-112, 1999.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 557"
            ],
            "original_annotated_samples": [
                "Such a temporary alliance, where the group members do not have a joint goal but act collaboratively for the interest of their own individual goals, is classified as a <br>treatment group</br> by modern psychologists [12] (in contrast to a Task Group, where its members have a joint goal).",
                "The Shared Activity model as presented in [5] modeled <br>treatment group</br> behavior using the same SharedPlans formalization.",
                "When comparing both definitions of an alliance and a <br>treatment group</br> we found an unsurprising resemblance between both models: the environment models definitions are almost identical (see SAs definitions in [5]), and their Selfish-Act and Cooperative Act axioms conform to our adversarial agents behavior."
            ],
            "translated_annotated_samples": [
                "Una alianza temporal, donde los miembros del grupo no tienen un objetivo común pero actúan colaborativamente en interés de sus propias metas individuales, es clasificada como un <br>Grupo de Tratamiento</br> por los psicólogos modernos (en contraste con un Grupo de Tarea, donde sus miembros tienen un objetivo común).",
                "El modelo de Actividad Compartida presentado en [5] modeló el comportamiento del Grupo de Tratamiento utilizando la misma formalización de Planes Compartidos.",
                "Al comparar ambas definiciones de una alianza y un <br>Grupo de Tratamiento</br>, encontramos una similitud sorprendente entre ambos modelos: las definiciones de los modelos de entorno son casi idénticas (ver las definiciones de SAs en [5]), y sus axiomas de Acto Egoísta y Acto Cooperativo se ajustan al comportamiento de nuestros agentes adversarios."
            ],
            "translated_text": "Un modelo de entorno adversarial para agentes racionales limitados en interacciones de suma cero Inon Zuckerman1, Sarit Kraus1, Jeffrey S. Rosenschein2, Gal Kaminka1 1 Departamento de Ciencias de la Computación 2 Escuela de Ingeniería Universidad Bar-Ilan y Ciencias de la Computación Ramat-Gan, Israel Universidad Hebrea, Jerusalén, Israel {zukermi, sarit, galk}@cs.biu.ac.il jeff@cs.huji.ac.il RESUMEN Los entornos multiagentes a menudo no son cooperativos ni colaborativos; en muchos casos, los agentes tienen intereses conflictivos, lo que lleva a interacciones adversariales. Este documento presenta un modelo formal de Entorno Adversarial para agentes racionales acotados que operan en un entorno de suma cero. En tales entornos, los intentos de utilizar métodos de búsqueda basados en utilidad clásica pueden plantear una variedad de dificultades (por ejemplo, modelar implícitamente al oponente como un maximizador de utilidad omnisciente, en lugar de aprovechar un modelo de oponente más matizado y explícito). Definimos un Entorno Adversario al describir los estados mentales de un agente en dicho entorno. Luego presentamos axiomas de comportamiento que están destinados a servir como principios de diseño para construir agentes adversarios. Exploramos la aplicación de nuestro enfoque mediante el análisis de archivos de registro de juegos de Conecta Cuatro completados, y presentamos un análisis empírico de la adecuación de los axiomas. Categorías y Descriptores de Asignaturas I.2.11 [Inteligencia Artificial]: Inteligencia Artificial Distribuida-Agentes Inteligentes, Sistemas Multiagente; I.2.4 [Inteligencia Artificial]: Formalismos y Métodos de Representación del Conocimiento - Lógica Modal Términos Generales Diseño, Teoría 1. Introducción: En las primeras investigaciones en sistemas multiagente (SMA) se consideraban grupos cooperativos de agentes; debido a que los agentes individuales tenían recursos limitados o acceso limitado a la información (por ejemplo, potencia de procesamiento limitada, cobertura sensorial limitada), trabajaban juntos por diseño para resolver problemas que individualmente no podían resolver, o al menos no podían resolver de manera eficiente. Sin embargo, la investigación de MAS pronto comenzó a considerar agentes interactivos con intereses individualizados, como representantes de diferentes humanos u organizaciones con intereses no idénticos. Cuando las interacciones son guiadas por intereses diversos, los participantes pueden tener que superar desacuerdos, interacciones no cooperativas e incluso intentos intencionales de dañarse mutuamente. Cuando ocurren este tipo de interacciones, los entornos requieren un comportamiento apropiado por parte de los agentes situados en ellos. Llamamos a estos entornos Entornos Adversarios, y llamamos a los agentes en conflicto Adversarios. Los modelos de cooperación y trabajo en equipo han sido ampliamente explorados en MAS a través de la axiomatización de estados mentales (por ejemplo, [8, 4, 5]). Sin embargo, ninguna de estas investigaciones abordó los dominios adversarios y sus implicaciones para el comportamiento de los agentes. Nuestro artículo aborda este problema al proporcionar un modelo formal y axiomatizado de estados mentales para un subconjunto de dominios adversariales, específicamente entornos adversariales simples de suma cero. Los encuentros de suma cero simples existen, por supuesto, en varios juegos de dos jugadores (por ejemplo, Ajedrez, Damas), pero también existen en juegos de n jugadores (por ejemplo, Risk, Diplomacia), subastas de un solo bien y en otros lugares. En estos entornos recientes en particular, el uso de una búsqueda adversarial basada en utilidad (como el algoritmo Min-Max) no siempre proporciona una solución adecuada; la función de recompensa podría ser bastante compleja o difícil de cuantificar, y existen limitaciones computacionales naturales en agentes racionales acotados. Además, los métodos de búsqueda tradicionales (como Min-Max) no hacen uso de un modelo del oponente, lo cual ha demostrado ser una adición valiosa a la planificación adversarial [9, 3, 11]. En este artículo, desarrollamos un modelo formal y axiomatizado para agentes racionales limitados que se encuentran en un entorno adversarial de suma cero. El modelo utiliza diferentes operadores de modalidad, y sus principales fundamentos son el modelo SharedPlans [4] para el comportamiento colaborativo. Exploramos las propiedades del entorno y los estados mentales de los agentes para derivar axiomas de comportamiento; estos axiomas de comportamiento constituyen un modelo formal que sirve como especificación y guía de diseño para el diseño de agentes en tales entornos. Luego investigamos el comportamiento de nuestro modelo empíricamente utilizando el juego de mesa Conecta Cuatro. Mostramos que este juego se ajusta a nuestra definición de entorno, y analizamos el comportamiento de los jugadores utilizando un gran conjunto de registros de partidas completadas 550 978-81-904262-7-5 (RPS) c 2007 archivos de IFAAMAS. Además, utilizamos los resultados presentados en [9] para discutir la importancia del modelado del oponente en nuestro dominio adversarial de Conecta Cuatro. El artículo continúa de la siguiente manera. La sección 2 presenta la formalización de los modelos. La sección 3 presenta el análisis empírico y sus resultados. Discutimos el trabajo relacionado en la Sección 4, y concluimos y presentamos las direcciones futuras en la Sección 5. ENTORNOS ADVERSARIOS El modelo de entorno adversario (denominado como AE) está destinado a guiar el diseño de agentes al proporcionar una especificación de las capacidades y actitudes mentales de un agente en un entorno adversario. Nos enfocamos aquí en tipos específicos de entornos adversarios, especificados de la siguiente manera: 1. Interacciones de suma cero: las utilidades positivas y negativas de todos los agentes suman cero; 2. Agentes adversarios simples: todos los agentes en el entorno son agentes adversarios; 3. AEs bilaterales: AEs con exactamente dos agentes; 4. AEs multilaterales: AEs de tres o más agentes. Trabajaremos en instanciaciones tanto bilaterales como multilaterales de entornos de suma cero y simples. En particular, nuestro modelo de entorno adversarial tratará con interacciones que consisten en N agentes (N ≥ 2), donde todos los agentes son adversarios y solo un agente puede tener éxito. Ejemplos de tales entornos van desde juegos de mesa (por ejemplo, Ajedrez, Conecta Cuatro y Diplomacia) hasta ciertos entornos económicos (por ejemplo, subastas de N postores sobre un único bien). 2.1 Resumen del Modelo Nuestro enfoque es formalizar las actitudes mentales y comportamientos de un único agente adversario; consideramos cómo un único agente percibe el AE. La siguiente lista especifica las condiciones y estados mentales de un agente en un AE simple de suma cero: 1. El agente tiene la intención individual de que su propio objetivo se complete; 2. El agente tiene la creencia individual de que él y sus adversarios persiguen objetivos completamente opuestos (definidos a continuación) y que solo puede haber un ganador; 3. El agente tiene la creencia individual de que cada adversario tiene la intención de completar su propio objetivo conflictivo completo; 4. El agente tiene una creencia individual en el perfil (parcial) de sus adversarios. El ítem 3 es necesario, ya que podría ser el caso de que algún agente tenga un objetivo completamente conflictivo y esté considerando adoptar la intención de completarlo, pero aún no está comprometido a lograrlo. Esto podría ocurrir porque el agente aún no ha reflexionado sobre los efectos que adoptar esa intención podría tener en las otras intenciones que actualmente mantiene. En tales casos, es posible que ni siquiera se considere en un entorno adversarial. El ítem 4 establece que el agente debe tener alguna creencia sobre los perfiles de sus adversarios. El perfil representa todo el conocimiento que el agente tiene sobre su adversario: sus debilidades, capacidades estratégicas, objetivos, intenciones, confiabilidad y más. Puede ser dado explícitamente o puede ser aprendido a partir de observaciones de encuentros pasados. 2.2 Definiciones de modelos para estados mentales Utilizamos las definiciones de los operadores modales, predicados y metapredicados de Grosz y Krauss, tal como se definen en su formalización de SharedPlan [4]. Recordamos aquí algunos de los predicados y operadores que se utilizan en esa formalización: Int.To(Ai, α, Tn, Tα, C) representa las intenciones de Ai en el tiempo Tn de realizar una acción α en el tiempo Tα en el contexto de C. Int.Th(Ai, prop, Tn, Tprop, C) representa las intenciones de Ai en el tiempo Tn de que una cierta proposición prop se cumpla en el tiempo Tprop en el contexto de C. Los operadores de intención potencial, Pot.Int.To(...) y Pot.Int.Th(...), se utilizan para representar el estado mental cuando un agente considera adoptar una intención, pero no ha deliberado sobre la interacción de las otras intenciones que tiene. El operador Bel(Ai, f, Tf) representa al agente Ai creyendo en la afirmación expresada en la fórmula f, en el tiempo Tf. MB(A, f, Tf) representa la creencia mutua para un grupo de agentes A. Una instantánea del sistema encuentra que nuestro entorno se encuentra en algún estado e ∈ E de estados variables ambientales, y cada adversario en cualquier LAi ∈ L de posibles estados locales. En cualquier paso de tiempo dado, el sistema estará en algún mundo w del conjunto de todos los mundos posibles w ∈ W, donde w = E×LA1 ×LA2 × ...LAn, y n es el número de adversarios. Por ejemplo, en un juego de póker Texas Holdem, el estado local de un agente podría ser su propio conjunto de cartas (que es desconocido para su adversario) mientras que el entorno consistirá en el bote de apuestas y las cartas comunitarias (que son visibles para ambos jugadores). Una función de utilidad bajo esta formalización se define como un mapeo de un mundo posible w ∈ W a un elemento en , que expresa la deseabilidad del mundo, desde la perspectiva de un único agente. Normalmente normalizamos el rango a [0,1], donde 0 representa el mundo menos deseable posible, y 1 es el mundo más deseable. La implementación de la función de utilidad depende del dominio en cuestión. La siguiente lista especifica nuevos predicados, funciones, variables y constantes utilizadas en conjunto con las definiciones originales para la formalización del entorno adversarial: 1. φ es una acción nula (el agente no hace nada). 2. GAi es el conjunto de objetivos del agente Ai. Cada objetivo es un conjunto de predicados cuya satisfacción hace que el objetivo esté completo (usamos G∗ Ai ∈ GAi para representar un objetivo arbitrario del agente Ai). 3. gAi es el conjunto de subobjetivos del agente Ai. Los subobjetivos son predicados cuya satisfacción representa un hito importante hacia el logro del objetivo completo. gG∗ Ai ⊆ gAi es el conjunto de subobjetivos que son importantes para la finalización del objetivo G∗ Ai (usaremos g∗ G∗ Ai ∈ gG∗ Ai para representar un subobjetivo arbitrario). 4. P Aj Ai es el objeto de perfil que el agente Ai tiene sobre el agente Aj. 5. CA es un conjunto general de acciones para todos los agentes en A que se derivan de las restricciones del entorno. CAi ⊆ CA es el conjunto de acciones posibles del agente Ai. La expresión Do(Ai, α, Tα, w) se cumple cuando Ai realiza la acción α durante el intervalo de tiempo Tα en el mundo w. 7. La afirmación Achieve(G∗ Ai , α, w) es verdadera cuando se logra el objetivo G∗ Ai siguiendo la finalización de la acción α en el mundo w ∈ W, donde α ∈ CAi. El perfil (Ai, PAi Ai) es verdadero cuando el agente Ai tiene un perfil de objeto para el agente Aj. Definición 1. El conflicto total (FulConf) describe una interacción de suma cero donde solo se puede completar un solo objetivo de los objetivos en conflicto. FulConf(G∗ Ai , G∗ Aj ) ⇒ (∃α ∈ CAi , ∀w, β ∈ CAj ) (Achieve(G∗ Ai , α, w) ⇒ ¬Achieve(G∗ Aj , β, w)) ∨ (∃β ∈ CAj , ∀w, α ∈ CAi )(Achieve(G∗ Aj , β, w) ⇒ ¬Achieve(G∗ Ai , α, w)) Definición 2. El Conocimiento Adversarial (AdvKnow) es una función que devuelve un valor que representa la cantidad de The Sixth Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 551 agente de conocimiento Ai tiene sobre el perfil del agente Aj, en el tiempo Tn. Cuanto mayor sea el valor, más conocimiento tiene el agente Ai. AdvKnow: P Aj Ai × Tn → Definición 3. Eval: Esta función de evaluación devuelve un valor de utilidad esperada estimado para un agente en A, después de completar una acción de CA en algún estado del mundo w. Eval: A × CA × w → Definición 4. TrH - (Umbral) es una constante numérica en el rango [0,1] que representa un valor umbral de la función de evaluación (Eval). Una acción que produce una evaluación de utilidad estimada por encima del TrH se considera una acción altamente beneficiosa. El valor Eval es una estimación y no la función de utilidad real, la cual suele ser desconocida. Utilizar el valor real de utilidad para un agente racional fácilmente produciría el mejor resultado para ese agente. Sin embargo, los agentes generalmente no tienen las funciones de utilidad reales, sino más bien una estimación heurística de las mismas. Existen dos propiedades importantes que deben cumplir para la función de evaluación: Propiedad 1. La función de evaluación debería indicar que el estado del mundo más deseable es aquel en el que se logra el objetivo. Por lo tanto, una vez que se ha satisfecho el objetivo, no puede haber ninguna acción futura que pueda colocar al agente en un estado del mundo con un valor de Eval más alto. (∀Ai, G∗ Ai , α, β ∈ CAi , w ∈ W) Lograr(G∗ Ai , α, w) ⇒ Eval(Ai, α, w) ≥ Eval(Ai, β, w) Propiedad 2. La función de evaluación debe proyectar una acción que cause la finalización de un objetivo o subobjetivo a un valor que sea mayor que TrH (una acción altamente beneficiosa). (∀Ai, G∗ Ai ∈ GAi , α ∈ CAi , w ∈ W, g∗ GAi ∈ gGAi ) Lograr(G∗ Ai , α, w) ∨ Lograr(g∗ GAi , α, w) ⇒ Eval(Ai, α, w) ≥ TrH. Definición 5. La acción de conjunto Definimos una acción de conjunto (SetAction) como un conjunto de operaciones de acción (ya sean acciones complejas o básicas) de algunos conjuntos de acción CAi y CAj que, según la creencia del agente Ai, están unidas por una relación temporal y consecuente, formando una cadena de eventos (acción y su acción consecuente siguiente). (∀α1 , . . . , αu ∈ CAi , β1 , . . . , βv ∈ CAj , w ∈ W) SetAction(α1 , . . . , αu , β1 , . . . , βv , w) ⇒ ((Do(Ai, α1 , Tα1 , w) ⇒ Do(Aj, β1 , Tβ1 , w)) ⇒ Do(Ai, α2 , Tα2 , w) ⇒ . . . ⇒ Do(Ai, αu , Tαu , w)) La relación consecuente podría existir debido a varias restricciones ambientales (cuando una acción obliga al adversario a responder con una acción específica) o debido al conocimiento de los agentes sobre el perfil de su adversario. Propiedad 3. A medida que aumenta el conocimiento que tenemos sobre nuestro adversario, tendremos creencias adicionales sobre su comportamiento en diferentes situaciones, lo que a su vez crea un nuevo conjunto de acciones. Formalmente, si nuestro AdvKnow en el tiempo Tn+1 es mayor que AdvKnow en el tiempo Tn, entonces cada SetAction conocido en el tiempo Tn también es conocido en el tiempo Tn+1. AdvKnow(P Aj Ai , Tn+1) > AdvKnow(P Aj Ai , Tn) ⇒ (∀α1 , . . . , αu ∈ CAi , β1 , . . . , βv ∈ CAj ) Bel(Aag, SetAction(α1 , . . . , αu , β1 , . . . , βv ), Tn) ⇒ Bel(Aag, SetAction(α1 , . . . , αu , β1 , . . . , βv ), Tn+1) 2.3 La formulación del entorno Los siguientes axiomas proporcionan la definición formal para un Entorno Adversarial (AE) simple y de suma cero. La satisfacción de estos axiomas significa que el agente está situado en un entorno así. Proporciona especificaciones para que el agente Aag interactúe con su conjunto de adversarios A con respecto a los objetivos G∗ Aag y G∗ A en el tiempo TCo en algún estado del mundo w. AE(Aag, A, G∗ Aag, A1, . . . , Ak, G∗ A1, . . . , G∗ Ak, Tn, w) 1. Aag tiene un Int.Th su objetivo se completaría: (∃α ∈ CAag , Tα) Int.Th(Aag, Lograr(G∗ Aag , α), Tn, Tα, AE) 2. Aag cree que él y cada uno de sus adversarios Ao están persiguiendo metas completamente conflictivas: (∀Ao ∈ {A1, . . . , Ak}) Bel(Aag, FulConf(G∗ Aag , G∗ Ao ), Tn) 3. Aag cree que cada uno de sus adversarios en Ao tiene la Int.Th su objetivo en conflicto G∗ Aoi se completará: (∀Ao ∈ {A1, . . . , Ak})(∃β ∈ CAo , Tβ) Bel(Aag, Int.Th(Ao, Lograr(G∗ Ao , β), TCo, Tβ, AE), Tn) 4. Para construir un agente que pueda operar con éxito dentro de un entorno de adversarios estratégicos, debemos especificar pautas de comportamiento para sus interacciones. Utilizar una estrategia de maximización de Eval ingenua hasta cierta profundidad de búsqueda no siempre dará resultados satisfactorios por varias razones: (1) el problema del horizonte de búsqueda al buscar a una profundidad fija; (2) la fuerte suposición de un adversario óptimamente racional con recursos ilimitados; (3) utilizar una función de evaluación estimada que no dará resultados óptimos en todos los estados del mundo, y que puede ser explotada [9]. Los siguientes axiomas especifican los principios de comportamiento que se pueden utilizar para diferenciar entre agentes exitosos y menos exitosos en el entorno adversarial mencionado anteriormente. Esos axiomas deben ser utilizados como principios de especificación al diseñar e implementar agentes que deben ser capaces de desempeñarse bien en entornos adversarios. Los axiomas conductuales representan situaciones en las que el agente adoptará intenciones potenciales para (Pot.Int.To(...)) realizar una acción, lo que típicamente requerirá un razonamiento de medios-fines para seleccionar un posible curso de acción. Este razonamiento llevará a la adopción de un Int.To(...) (ver [4]). A1. Axioma para lograr metas. El primer axioma es el caso más simple; cuando el agente Aag cree que está a una acción (α) de lograr su objetivo en conflicto G∗ Aag, debería adoptar la intención potencial de hacer α y completar su objetivo. (∀Aag, α ∈ CAag, Tn, Tα, w ∈ W) (Bel(Aag, Do(Aag, α, Tα, w) ⇒ Achieve(G∗ Aag, α, w)) ⇒ Pot.Int.To(Aag, α, Tn, Tα, w) Este comportamiento algo trivial es el primer y más fuerte axioma. En cualquier situación, cuando el agente está a una acción de completar el objetivo, debería completar la acción. Cualquier función de evaluación justa clasificaría naturalmente a α como la acción de valor máximo (propiedad 1). Sin embargo, sin una axiomatización explícita de dicho comportamiento, podría haber situaciones en las que el agente decida tomar otra acción por diversas razones, debido a sus recursos de decisión limitados. A2. Axioma del Acto Preventivo. En una situación adversarial, el agente Aag podría decidir tomar acciones que dañen uno de los planes de su adversario para completar su objetivo, incluso si esas acciones no avanzan explícitamente a Aag hacia su objetivo en conflicto G∗ Aag. Tal acción preventiva tendrá lugar cuando el agente Aag tenga la creencia sobre la posibilidad de que su adversario Ao realice una acción β que le otorgará un alto 552 The Sixth Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) valor de evaluación de utilidad (> TrH). Creer que tomar la acción α evitará que el oponente realice su β, adoptará una intención potencial de hacer α. (∀Aag, Ao ∈ A, α ∈ CAag , β ∈ CAo , Tn, Tβ, w ∈ W) (Bel(Aag, Do(Ao, β, Tβ, w) ∧ Eval(Ao, β, w) > TrH, Tn) ∧ Bel(Aag, Do(Aag, α, Tα, w) ⇒ ¬Do(Ao, β, Tβ, w), Tn) ⇒ Pot.Int.To(Aag, α, Tn, Tα, w) Este axioma es un componente básico de cualquier entorno adversarial. Por ejemplo, al observar un juego de ajedrez, un jugador podría darse cuenta de que está a punto de ser jaque mateado por su oponente, por lo tanto, realiza una jugada preventiva. Otro ejemplo es el juego de Cuatro en Línea: cuando un jugador tiene una fila de tres fichas, su oponente debe bloquearla o perder. Una instancia específica de A1 ocurre cuando el adversario está a una acción de lograr su objetivo, y la acción preventiva inmediata debe ser tomada por el agente. Formalmente, tenemos las mismas creencias que se mencionaron anteriormente, con la creencia modificada de que realizar la acción β hará que el agente Ao logre su objetivo. Proposición 1: Caso de prevención o pérdida. (∀Aag, Ao ∈ A, α ∈ CAag , β ∈ CAo , G∗ Ao , Tn, Tα, Tβ, w ∈ W) Bel(Aag, Do(Ao, β, Tβ, w) ⇒ Achieve(G∗ Ao , β, w), Tn) ∧ Bel(Aag, Do(Aag, α, Tα, w) ⇒ ¬Do(Ao, β, Tβ, w)) ⇒ Pot.Int.To(Aag, α, Tn, Tα, w) Esquema de prueba: La Proposición 1 puede derivarse fácilmente del axioma A1 y de la propiedad 2 de la función Eval, que establece que cualquier acción que cause la finalización de un objetivo es una acción altamente beneficiosa. El comportamiento del acto preventivo ocurrirá implícitamente cuando la función Eval sea igual a la función de utilidad del mundo real. Sin embargo, al ser agentes racionales limitados y al tratar con una función de evaluación estimada, necesitamos axiomatizar explícitamente dicho comportamiento, ya que no siempre ocurrirá implícitamente a partir de la función de evaluación. A3. Axioma de Movimiento Táctico Subóptimo. En muchos escenarios, puede ocurrir una situación en la que un agente decida no tomar la acción más beneficiosa actual que puede tomar (la acción con el valor de evaluación de utilidad máximo), porque cree que tomar otra acción (con un valor de evaluación de utilidad más bajo) podría generar (dependiendo de la respuesta del adversario) una posibilidad futura para una acción altamente beneficiosa. Esto ocurrirá con mayor frecuencia cuando la función Eval sea inexacta y difiera en gran medida de la función de Utilidad. De manera formal, el agente Aag cree en una cierta SetAction que evolucionará de acuerdo con su acción inicial y producirá un valor beneficioso alto (> TrH) únicamente para él. (∀Aag, Ao ∈ A, Tn, w ∈ W) (∃α1 , . . . , αu ∈ CAi , β1 , . . . , βv ∈ CAj , Tα1 ) Bel(Aag, SetAction(α1 , . . . , αu , β1 , . . . , βv ), Tn) ∧ Bel(Aag, Eval(Ao, βv , w) < TrH < Eval(Aag, αu , w), Tn) ⇒ Pot.Int.To(Aag, α1 , Tn, Tα1 , w) Un agente podría creer que una cadena de eventos ocurrirá por diversas razones debido a la naturaleza inevitable del dominio. Por ejemplo, en el ajedrez, a menudo observamos lo siguiente: una jugada provoca una posición de jaque, lo que a su vez limita los movimientos del oponente para evitar el jaque, a lo que el primer jugador podría reaccionar con otro jaque, y así sucesivamente. El agente también podría creer en una cadena de eventos basada en su conocimiento del perfil de su adversario, lo que le permite prever los movimientos del adversario con alta precisión. A4. Axioma de Detección de Perfiles. El agente puede ajustar los perfiles de sus adversarios mediante observaciones y estudio de patrones (específicamente, si hay encuentros repetidos con el mismo adversario). Sin embargo, en lugar de esperar a que se revele la información del perfil, un agente también puede iniciar acciones que obligarán a su adversario a reaccionar de una manera que revele conocimiento del perfil sobre él. Formalmente, el axioma establece que si todas las acciones (γ) no son acciones altamente beneficiosas (< TrH), el agente puede realizar la acción α en el tiempo Tα si cree que resultará en una acción no altamente beneficiosa β por parte de su adversario, lo que a su vez le enseña sobre el perfil del adversario, es decir, proporciona un mayor AdvKnow(P Aj Ai , Tβ). (∀Aag, Ao ∈ A, α ∈ CAag , β ∈ CAo , Tn, Tα, Tβ, w ∈ W) Bel(Aag, (∀γ ∈ CAag )Eval(Aag, γ, w) < TrH, Tn) ∧ Bel(Aag, Do(Aag, α, Tα, w) ⇒ Do(Ao, β, Tβ, w), Tn) ∧ Bel(Aag, Eval(Ao, β, w) < TrH) ∧ Bel(Aag, AdvKnow(P Aj Ai , Tβ) > AdvKnow(P Aj Ai , Tn), Tn) ⇒ Pot.Int.To(Aag, α, Tn, Tα, w) Por ejemplo, volviendo al escenario del juego de ajedrez, consideremos comenzar una partida contra un oponente del que no sabemos nada, ni siquiera si es humano o un oponente computarizado. Podríamos comenzar a jugar una estrategia que sea adecuada frente a un oponente promedio, y ajustar nuestro juego de acuerdo a su nivel de juego. A5. Axioma de Formación de Alianzas El siguiente axioma de comportamiento es relevante únicamente en una instancia multilateral del entorno adversarial (obviamente, una alianza no puede formarse en un encuentro bilateral de suma cero). En diferentes situaciones durante una interacción multilateral, un grupo de agentes podría creer que es de su interés formar una alianza temporal. Una alianza de este tipo es un acuerdo que limita el comportamiento de sus miembros, pero se cree por parte de estos que les permite alcanzar un valor de utilidad mayor al que podrían lograr fuera de la alianza. Como ejemplo, podemos observar el clásico juego de mesa Risk, donde cada jugador tiene como objetivo individual ser el único conquistador del mundo, un juego de suma cero. Sin embargo, para lograr este objetivo, podría ser estratégicamente sabio hacer acuerdos de alto el fuego a corto plazo con otros jugadores, o unirse y atacar a un oponente que sea más fuerte que el resto. Los términos de una alianza definen la forma en que sus miembros deben actuar. Es un conjunto de predicados, denominados Términos, que es acordado por los miembros de la alianza y debe mantenerse verdadero durante la duración de la alianza. Por ejemplo, el conjunto de términos en el escenario de riesgo podría contener los siguientes predicados: 1. Los miembros de la alianza no se atacarán entre sí en los territorios X, Y y Z; 2. Los miembros de la alianza contribuirán C unidades por turno para atacar al adversario Ao; 3. Los miembros están obligados a permanecer como parte de la alianza hasta el tiempo Tk o hasta que el ejército adversario Ao sea más pequeño que Q. El conjunto de términos especifica restricciones intergrupales en cada uno de los miembros de la alianza (∀Aal i ∈ Aal ⊆ A) conjunto de acciones Cal i ⊆ C. Definición 6. El valor total de evaluación que el agente Ai logrará al ser parte de Aal es la suma de Evali (Eval para Ai) de cada uno de los valores de evaluación de Aal j después de tomar sus propias acciones α (a través del predicado del agente(α)): Al val(Ai, Cal, Aal, w) = α∈Cal Evali(Aal j, agente(α), w) Definición 7. Al TrH - es un número que representa un valor de Al en la Sexta Conferencia Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 553 umbral; por encima de él, se puede decir que la alianza es una alianza altamente beneficiosa. El valor de Al TrH se calculará dinámicamente de acuerdo con el progreso de la interacción, como se puede ver en [7]. Después de formarse una alianza, sus miembros ahora están trabajando en su entorno adversarial normal, así como de acuerdo con los estados mentales y axiomas requeridos para sus interacciones como parte de la alianza. El siguiente modelo de Alianza (AL) especifica las condiciones bajo las cuales el grupo Aal puede considerarse en una alianza y trabajando con un nuevo y limitado conjunto de acciones Cal, en el tiempo Tn. AL(Aal , Cal , w, Tn) 1. \n\nAL(Aal , Cal , w, Tn) 1. Aal tiene un MB en el que todos los miembros son parte de Aal: MB(Aal, (∀Aal i ∈ Aal)miembro(Aal i, Aal), Tn) 2. Aal tiene una MB que el grupo se mantenga: MB(Aal , (∀Aal i ∈ Aal )Int.Th (Ai, miembro(Ai, Aal ), Tn, Tn+1, Co), Tn) 3. Aal tiene un beneficio marginal que al ser miembros les otorga un alto valor de utilidad: MB(Aal , (∀Aal i ∈ Aal )Al val(Aal i , Cal , Aal , w) ≥ Al TrH, Tn) Los perfiles de los miembros son una parte crucial de las alianzas exitosas. Suponemos que los agentes que tienen perfiles más precisos de sus adversarios serán más exitosos en tales entornos. Tales agentes podrán predecir cuándo un miembro está a punto de violar el contrato de alianzas (elemento 2 en el modelo anterior) y tomar medidas correctivas (cuando el elemento 3 se falsificará). La robustez de la alianza depende en parte de la medida de confianza de sus miembros, la estimación de su posición objetiva y otras propiedades de su perfil. Debemos tener en cuenta que un agente puede formar parte simultáneamente de más de una alianza. Una alianza temporal, donde los miembros del grupo no tienen un objetivo común pero actúan colaborativamente en interés de sus propias metas individuales, es clasificada como un <br>Grupo de Tratamiento</br> por los psicólogos modernos (en contraste con un Grupo de Tarea, donde sus miembros tienen un objetivo común). El modelo de Actividad Compartida presentado en [5] modeló el comportamiento del Grupo de Tratamiento utilizando la misma formalización de Planes Compartidos. Al comparar ambas definiciones de una alianza y un <br>Grupo de Tratamiento</br>, encontramos una similitud sorprendente entre ambos modelos: las definiciones de los modelos de entorno son casi idénticas (ver las definiciones de SAs en [5]), y sus axiomas de Acto Egoísta y Acto Cooperativo se ajustan al comportamiento de nuestros agentes adversarios. La principal distinción entre ambos modelos es la integración de un axioma de acto de comportamiento útil, en la Actividad Compartida que no puede formar parte de la nuestra. Este axioma establece que un agente considerará tomar una acción que disminuirá su valor de Evaluación (hasta un cierto límite inferior), si cree que un compañero del grupo obtendrá un beneficio significativo. Dicho comportamiento no puede ocurrir en un entorno puramente adversarial (como un juego de suma cero), donde los miembros de la alianza están constantemente alerta para manipular su alianza en su propio beneficio. A6. Axioma de Maximización de la Evaluación. En un caso en el que todos los demás axiomas no sean aplicables, procederemos con la acción que maximice el valor heurístico calculado en la función Eval. (∀Aag, Ao ∈ A, α ∈ Cag, Tn, w ∈ W) Bel(Aag, (∀γ ∈ Cag)Eval(Aag, α, w) ≥ Eval(Aag, γ, w), Tn) ⇒ Pot.Int.To(Aag, α, Tn, Tα, w) T1. Optimalidad en Eval = Utilidad El modelo axiomático anterior aborda situaciones en las que la Utilidad es desconocida y los agentes son agentes racionales limitados. El siguiente teorema muestra que en interacciones bilaterales, donde los agentes tienen la función de Utilidad real (es decir, Eval = Utilidad) y son agentes racionales, los axiomas proporcionan el mismo resultado óptimo que la búsqueda adversarial clásica (por ejemplo, Min-Max). Teorema 1. Que Ae ag sea un agente AE racional ilimitado que utiliza la función de evaluación heurística Eval, Au ag sea el mismo agente utilizando la verdadera función de utilidad y Ao sea un único adversario racional ilimitado basado en utilidad. Dado que Eval = Utilidad: (∀α ∈ CAu ag , α ∈ CAe ag , Tn, w ∈ W) Pot.Int.To(Au ag, α, Tn, Tα, w) → Pot.Int.To(Ae ag, α , Tn, Tα, w) ∧ ((α = α ) ∨ (Utilidad(Au ag, α, w) = Eval(Ae ag, α , w))) Esquema de prueba - Dado que Au ag tiene la función de utilidad real y recursos ilimitados, puede generar el árbol de juego completo y ejecutar el algoritmo MinMax óptimo para elegir la acción de mayor valor de utilidad, que denotamos como α. La prueba mostrará que Ae ag, utilizando los axiomas AE, seleccionará la misma utilidad α o igual (cuando hay más de una acción con la misma utilidad máxima) cuando Eval = Utilidad. (A1) Axioma de logro de objetivo - supongamos que hay un α tal que su cumplimiento logrará el objetivo Au ags. Obtendrá la utilidad más alta mediante Min-Max para Au ag. El agente Ae seleccionará α u otra acción con el mismo valor de utilidad a través de A1. Si tal α no existe, Ae ag no puede aplicar este axioma y procede a A2. (A2) Axioma de la acción preventiva - (1) Observando el caso base (ver Prop1), si hay un β que lleva a Ao a alcanzar su objetivo, entonces una acción preventiva α producirá la mayor utilidad para Au ag. Au ag lo elegirá a través de la utilidad, mientras que Ae ag lo elegirá a través de A2. En el caso general, β es una acción altamente beneficiosa para Ao, lo que resulta en una baja utilidad para Au ag, lo que lo guiará a seleccionar un α que evitará β, mientras que Ae ag lo elegirá a través de A2.1 Si dicho β no existe para Ao, entonces A2 no es aplicable, y Ae ag puede proceder a A3. (A3) Axioma de movimiento táctico subóptimo: al usar una función de evaluación heurística, Ae ag tiene una creencia parcial en el perfil de su adversario (elemento 4 en el modelo AE), lo que puede llevarlo a creer en SetActions (Prop1). En nuestro caso, Ae ag tiene un perfil completo de su adversario óptimo y sabe que Ao se comportará de manera óptima según los valores reales de utilidad en el árbol de búsqueda completo, por lo tanto, cualquier creencia sobre una SetAction subóptima no puede existir, lo que hace que este axioma sea inaplicable. Ae ag procederá a A4. (A4) Axioma de detección de perfil - Dado que Ae ag tiene el perfil completo de Ao, ninguna de las acciones de Ae ag puede aumentar su conocimiento. Ese axioma no se aplicará, y el agente procederá con A6 (A5 será ignorado porque la interacción es bilateral). (A6) Axioma de maximización de la evaluación: este axioma seleccionará la máxima Evaluación para Ae ag. Dado que Eval = Utilidad, se seleccionará el mismo α que fue seleccionado por Au ag. 3. EVALUACIÓN El propósito principal de nuestro análisis experimental es evaluar el comportamiento y rendimiento de los modelos en un entorno adversarial real. Esta sección investiga si un caso acotado 1 en el que, tras la finalización de β, exista un γ que proporcione una alta utilidad para el Agente Au ag, no puede ocurrir porque Ao utiliza la misma utilidad, y la existencia de γ hará que clasifique a β como una acción de baja utilidad. 554 The Sixth Intl. En la Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07), los agentes racionales situados en entornos adversariales se beneficiarán más al aplicar nuestros axiomas de comportamiento sugeridos. 3.1 El Dominio Para explorar el uso del modelo anterior y sus axiomas de comportamiento, decidimos utilizar el juego de Conecta Cuatro como nuestro entorno adversarial. Connect-Four es un juego de 2 jugadores, de suma cero, que se juega utilizando un tablero similar a una matriz de 6x7. En cada turno, un jugador coloca un disco en una de las 7 columnas (el conjunto de 21 discos suele ser de color amarillo para el jugador 1 y rojo para el jugador 2; usaremos Blanco y Negro respectivamente para evitar confusiones). El ganador es el primer jugador en completar un conjunto de cuatro discos de su color en horizontal, vertical o diagonal. En ocasiones muy raras, el juego podría terminar en empate si todos los cuadros vacíos están llenos, pero ningún jugador logró crear un conjunto de 4 discos. El juego de Conecta Cuatro fue resuelto en [1], donde se muestra que el primer jugador (jugando con los discos blancos) puede forzar una victoria comenzando en la columna del medio (columna 4) y jugando de manera óptima. Sin embargo, la estrategia óptima es muy compleja y difícil de seguir incluso para agentes racionales limitados complejos, como los jugadores humanos. Antes de que podamos proceder a verificar el comportamiento del agente, primero debemos verificar que el dominio se ajusta a la definición de entornos adversarios mencionada anteriormente (en la que se basan los axiomas de comportamiento). Primero, al jugar al juego de Conecta Cuatro, el agente tiene la intención de ganar el juego (ítem 1). Segundo (ítem 2), nuestro agente cree que en Conecta Cuatro solo puede haber un ganador (o ningún ganador en la rara ocasión de un empate). Además, nuestro agente cree que su oponente en el juego intentará ganar (ítem 3), y esperamos que tenga algún conocimiento parcial (ítem 4) sobre su adversario (este conocimiento puede variar desde nada, pasando por hechos simples como la edad, hasta estrategias y debilidades). Por supuesto, no todos los encuentros de Conecta Cuatro son adversariales. Por ejemplo, cuando un padre está jugando el juego con su hijo, la siguiente situación podría ocurrir: el niño, con un fuerte incentivo para ganar, trata el entorno como adversarial (intenta ganar, entiende que solo puede haber un ganador y cree que su padre está tratando de vencerlo). Sin embargo, desde el punto de vista de los padres, el entorno podría ser visto como educativo, donde el objetivo no es ganar el juego, sino causar disfrute o practicar el razonamiento estratégico. En un entorno educativo como este, un nuevo conjunto de axiomas de comportamiento podría ser más beneficioso para los objetivos de los padres que nuestros axiomas de comportamiento adversarial sugeridos. 3.2 Análisis de Axiomas Después de demostrar que el juego de Conecta Cuatro es de hecho un entorno adversarial bilateral de suma cero, el siguiente paso es observar el comportamiento de los jugadores durante el juego y verificar si comportarse de acuerdo con nuestro modelo mejora el rendimiento. Para hacerlo, hemos recopilado archivos de registro de juegos de Conecta Cuatro completados que fueron jugados por jugadores humanos a través de Internet. Nuestros datos de archivo de registro recopilados provienen de sitios de Juego por Correo Electrónico (PBeM). Estos son sitios web que alojan juegos por correo electrónico, donde cada movimiento se realiza a través de un intercambio de correos electrónicos entre el servidor y los jugadores. Muchos de estos sitios de archivos contienen interacciones competitivas reales y también mantienen un sistema de clasificación para sus miembros. La mayoría de los datos que utilizamos se pueden encontrar en [6]. Como se puede aprender de [1], Conecta Cuatro tiene una estrategia óptima y una ventaja considerable para el jugador que comienza el juego (a quien llamamos jugador Blanco). Nos concentraremos en nuestro análisis en los movimientos de los segundos jugadores (a los que llamaremos Negras). El jugador Blanco, al ser el primero en actuar, tiene la llamada ventaja de la iniciativa. Tener la ventaja y una buena estrategia mantendrá al jugador Negro ocupado reaccionando a sus movimientos, en lugar de iniciar amenazas. Una amenaza es una combinación de tres discos del mismo color, con un espacio vacío para el cuarto disco ganador. Una amenaza abierta es una amenaza que puede materializarse en el próximo movimiento del oponente. Para que el jugador Negro gane, debe de alguna manera cambiar el rumbo, tomar la ventaja y comenzar a presentar amenazas al jugador Blanco. Exploraremos el comportamiento de los jugadores negros y su conformidad con nuestros axiomas. Para hacerlo, construimos una aplicación que lee archivos de registro y analiza los movimientos de los jugadores Negros. La aplicación contiene dos componentes principales: (1) un algoritmo Min-Max para la evaluación de movimientos; (2) un detector de amenazas abiertas para descubrir amenazas abiertas. El algoritmo Min-Max funcionará hasta una profundidad d dada y, para cada movimiento α, devolverá el valor heurístico para la próxima acción tomada por el jugador según lo escrito en el archivo de registro, h(α), junto con el valor heurístico máximo, maxh(α), que podría lograrse antes de realizar el movimiento (obviamente, si h(α) = maxh(α), entonces el jugador no realizó el movimiento óptimo desde el punto de vista heurístico). El trabajo de los detectores de amenazas es notificar si se tomó alguna acción para bloquear una amenaza abierta (no bloquear una amenaza abierta probablemente hará que el jugador pierda en el próximo movimiento del oponente). La función heurística utilizada por Min-Max para evaluar la utilidad de los jugadores es la siguiente función, que es simple de calcular, pero proporciona un desafío razonable a los oponentes humanos: Definición 8. Sea Grupo un conjunto adyacente de cuatro cuadrados que son horizontales, verticales o diagonales. El Grupo n (Grupo w) es un Grupo con n piezas de color negro (blanco) y 4−n casillas vacías. h = ((Grupo1 negro ∗α)+(Grupo2 negro ∗β)+(Grupo3 negro ∗γ)+(Grupo4 negro ∗∞)) − ((Grupo1 blanco ∗α)+(Grupo2 blanco ∗β)+(Grupo3 blanco ∗γ)+(Grupo4 blanco ∗∞)) Los valores de α, β y δ pueden variar para formar cualquier combinación lineal deseada; sin embargo, es importante asignarles valores teniendo en cuenta el orden α < β < δ (usamos 1, 4 y 8 como sus valores respectivos). Grupos de 4 discos del mismo color significan victoria, por lo tanto, el descubrimiento de dicho grupo resultará en ∞ para asegurar un valor extremo. Ahora utilizamos nuestra función de evaluación estimada para evaluar las acciones de los jugadores Negros durante la interacción adversarial de Conecta Cuatro. Cada juego del archivo de registro fue introducido en la aplicación, la cual procesó y generó un archivo de registro reformateado que contenía el valor h del movimiento actual, el valor maxh que podría ser alcanzado y una notificación si se detectaba una amenaza abierta. Un total de 123 partidas fueron analizadas (57 con victoria de las blancas y 66 con victoria de las negras). Algunos juegos adicionales fueron ignorados manualmente en el experimento, debido a estos problemas: un jugador abandonando el juego mientras el resultado no es definitivo, o una jugada irracional y contundente en las primeras etapas del juego (por ejemplo, no bloquear un grupo obviamente ganador en las primeras jugadas de apertura). Además, también se eliminó un empate. El simulador se ejecutó con una profundidad de búsqueda de 3 movimientos. Ahora procedemos a analizar los juegos con respecto a cada axioma de comportamiento. El Sexto Internacional. La siguiente sección presenta las evaluaciones heurísticas del algoritmo Min-Max para cada acción, y verifica la cantidad y el alcance de las acciones tácticas subóptimas y sus implicaciones en el rendimiento. La Tabla 1 muestra los resultados y las percepciones del análisis heurístico de los juegos, cuando la profundidad de búsqueda es igual a 3 (esta profundidad de búsqueda fue seleccionada para que los resultados sean comparables a [9], ver Sección 3.2.3). La información heurística de las tablas es la diferencia entre el valor heurístico máximo presente y el valor heurístico de la acción que finalmente tomó el jugador (es decir, cuanto más cercano sea el número a 0, más cercana estuvo la acción a la acción heurística máxima). La primera fila presenta los valores de diferencia de la acción que tuvo el valor de diferencia máximo entre todas las acciones de los jugadores Negros en un juego dado, promediado en todos los juegos ganados y perdidos por los Negros (ver columnas respectivas). En los juegos en los que el jugador Negro pierde, su valor promedio de diferencia fue de -17.62, mientras que en los juegos en los que el jugador Negro ganó, su promedio fue de -12.02. La segunda fila amplía el análisis al considerar las 3 acciones con mayor diferencia heurística, y promediarlas. En ese caso, notamos una diferencia heurística promedio de 5 puntos entre los juegos en los que el jugador Negro pierde y los juegos en los que gana. Sin embargo, la importancia de esos números es que nos permitieron hacer una suposición educada sobre un número umbral de 11.5, como el valor de la constante TrH, que diferencia entre acciones normales y altamente beneficiosas. Después de encontrar una constante TrH aproximada, podemos proceder con un análisis de la importancia de los movimientos subóptimos. Para hacerlo, tomamos el subconjunto de juegos en los que el valor mínimo de diferencia heurística para las acciones de las negras era de 11.5. Como se presenta en la Tabla 2, podemos ver el promedio de 3 horas mínimo de los 3 mayores rangos y el porcentaje respectivo de juegos ganados. La primera fila muestra que el jugador Negro ganó solo el 12% de los juegos en los que el promedio de sus 3 acciones con mayor diferencia heurística (min3 h) era menor que el umbral sugerido, TrH = 11.5. La segunda fila muestra un resultado sorprendente: parece que cuando min3 h > −4, el jugador Negro rara vez gana. La intuición sugeriría que los juegos en los que los valores de evaluación de la acción estén más cerca de los valores máximos resultarán en más juegos ganados para las piezas negras. Sin embargo, parece que en el dominio de Conecta Cuatro, simplemente responder con acciones algo fácilmente esperadas, sin iniciar algunos movimientos sorprendentes y subóptimos, no produce buenos resultados. La última fila resume las principales conclusiones del análisis; la mayoría de las victorias de Blacks (83%) se produjeron cuando su min3 h estaba en el rango de -11.5 a -4. Una inspección detallada de esos juegos ganados por las fichas negras muestra el siguiente patrón detrás de los números: después de los movimientos de apertura estándar, las fichas negras de repente colocan una ficha en una columna aislada, lo cual parece ser un desperdicio de un movimiento. El blanco sigue construyendo sus amenazas, generalmente ignorando el último movimiento de las negras, que a su vez utiliza el disco aislado como ancla para una futura amenaza ganadora. Los resultados muestran que fue beneficioso para el jugador Negro en la Tabla 2: Porcentajes de ganancias de los Negros % de juegos min3 h < −11.5 12% min3 h > −4 5% −11.5 ≤ min3 h ≤ −4 83% tomar acciones subóptimas y no dar el valor heurístico más alto posible en ese momento, pero no será demasiado perjudicial para su posición (es decir, no dará un alto valor beneficioso a su adversario). Resultó que aprender el umbral es un aspecto importante del éxito: realizar movimientos extremadamente arriesgados (min3 h < −11.5) o intentar evitarlos (min3 h > −4) reduce significativamente las posibilidades de ganar de los jugadores negros. 3.2.2 Afirmación del Axioma de Monitoreo de Perfiles En la tarea de demostrar la importancia de monitorear los perfiles de los adversarios, nuestros archivos de registro no pudieron ser utilizados porque no contenían interacciones repetidas entre jugadores, que son necesarias para inferir el conocimiento de los jugadores sobre sus adversarios. Sin embargo, la importancia del modelado del oponente y su uso para obtener ventajas tácticas ya ha sido estudiada en varios ámbitos ([3, 9] son buenos ejemplos). En un artículo reciente, Markovitch y Reger [9] exploraron la noción de aprendizaje y explotación de la debilidad del oponente en interacciones competitivas. Aplican estrategias de aprendizaje simples analizando ejemplos de interacciones pasadas en un dominio específico. También utilizaron el dominio adversarial de Conecta Cuatro, que ahora se puede utilizar para comprender la importancia de monitorear el perfil del adversario. Tras la presentación de su modelo teórico, describen un extenso estudio empírico y verifican el rendimiento de los agentes después de aprender el modelo de debilidades con ejemplos pasados. Uno de los dominios utilizados como entorno competitivo fue el mismo juego de Conecta Cuatro (las Damas fueron el segundo dominio). Su función heurística era idéntica a la nuestra con tres variaciones diferentes (H1, H2 y H3) que se distinguen entre sí por los valores de sus coeficientes de combinación lineal. La profundidad de búsqueda para los jugadores fue de 3 (como en nuestro análisis). Sus extensos experimentos verifican y comparan diversas estrategias de aprendizaje, factores de riesgo, conjuntos de características predefinidas y métodos de uso. La conclusión es que el dominio de Conecta Cuatro muestra una mejora de una tasa de victoria de 0.556 antes de modelar a 0.69 después de modelar (página 22). Sus conclusiones, que muestran un rendimiento mejorado al sostener y utilizar el modelo del adversario, justifican el esfuerzo de monitorear el perfil del adversario para interacciones continuas y repetidas. Un punto adicional que surgió en sus experimentos es el siguiente: después de que se haya aprendido el modelo de debilidad del oponente, los autores describen diferentes métodos para integrar el modelo de debilidad del oponente en la estrategia de decisión de los agentes. Sin embargo, independientemente del método específico que elijan para trabajar, todos los métodos de integración podrían hacer que el agente tome decisiones subóptimas; podría hacer que el agente prefiera acciones que son subóptimas en el momento actual de la decisión, pero que podrían hacer que el oponente reaccione de acuerdo con su modelo de debilidad (como lo representa nuestro agente), lo cual a su vez será beneficioso para nosotros en el futuro. El comportamiento de los agentes, como se demuestra en [9], confirma y refuerza aún más nuestro Axioma Táctico Subóptimo discutido en la sección anterior. 556 The Sixth Intl. La necesidad de los axiomas de Logro de Objetivos, Acto Preventivo y Maximización de la Evaluación es evidente y no requiere más verificación. Sin embargo, incluso con respecto a esos axiomas, surgieron algunas ideas interesantes en el análisis de registros. Los axiomas de Logro y Prevención de Actos, aunque teóricamente triviales, parecen presentar un desafío para un jugador humano. En la inspección inicial de los registros, nos encontramos con algunos juegos donde un jugador, por razones inexplicables, no bloqueó al otro para que ganara o no logró ejecutar su propia jugada ganadora. Podemos atribuir esas fallas a la falta de atención de los humanos, o a un error de escritura en su respuesta al movimiento; sin embargo, esos errores podrían ocurrir en agentes racionales limitados, y el comportamiento apropiado debe ser axiomatizado. Un juego típico de Cuatro en Línea gira en torno a generar amenazas y bloquearlas. En nuestro análisis buscamos acciones preventivas explícitas, es decir, movimientos que bloqueen un grupo de 3 discos o que eliminen una amenaza futura (en nuestro horizonte de búsqueda limitado). Encontramos que en el 83% de los juegos totales, hubo al menos una acción preventiva tomada por el jugador Negro. También se encontró que Black promedió 2.8 acciones preventivas por juego en los juegos en los que perdió, mientras que promedió 1.5 acciones preventivas por juego al ganar. Parece que Black necesita 1 o 2 acciones preventivas para construir su posición inicial de toma, antes de comenzar a presentar amenazas. Si no logra ganar, generalmente evitará una amenaza adicional o dos antes de sucumbir ante el Blanco. 4. TRABAJO RELACIONADO Mucha investigación se ocupa de la axiomatización del trabajo en equipo y los estados mentales de los individuos: algunos modelos utilizan el conocimiento y la creencia [10], otros tienen modelos de metas e intenciones [8, 4]. Sin embargo, todas estas teorías formales tratan sobre el trabajo en equipo y la cooperación de agentes. Hasta donde sabemos, nuestro modelo es el primero en proporcionar un modelo formalizado para entornos adversarios explícitos y el comportamiento de los agentes en él. El algoritmo de búsqueda adversarial clásico Min-Max fue el primer intento de integrar al oponente en el espacio de búsqueda con la suposición débil de un oponente que juega de manera óptima. Desde entonces, se ha dedicado mucho esfuerzo a integrar el modelo del oponente en el procedimiento de toma de decisiones para predecir el comportamiento futuro. El algoritmo M∗ presentado por Carmel y Markovitch [2] mostró un método para incorporar modelos de oponentes en la búsqueda de adversarios, mientras que en [3] utilizaron el aprendizaje para proporcionar un modelo de oponente más preciso en un entorno de juego repetido de 2 jugadores, donde las estrategias de los agentes se modelaron como autómatas finitos. El trabajo adicional de planificación adversarial fue realizado por Willmott y colaboradores [13], quienes proporcionaron un enfoque de planificación adversarial para el juego de GO. La investigación mencionada anteriormente trató sobre la búsqueda adversarial y la integración de modelos de oponentes en métodos de búsqueda de utilidad clásicos. Ese trabajo muestra la importancia del modelado del oponente y la capacidad de explotarlo en beneficio de un agente. Sin embargo, las limitaciones básicas de esos métodos de búsqueda siguen aplicándose; nuestro modelo intenta superar esas limitaciones presentando un modelo formal para una nueva especificación adversarial basada en estados mentales. 5. CONCLUSIONES Presentamos un modelo de Entorno Adversarial para un agente racional acotado que está situado en un entorno de N jugadores, de suma cero. Estos fueron posteriormente eliminados del análisis final. Utilizamos la formalización de SharedPlans para definir el modelo y los axiomas que los agentes pueden aplicar como pautas de comportamiento. El modelo está destinado a ser utilizado como una guía para diseñar agentes que necesitan operar en entornos adversos. Presentamos resultados empíricos, basados en el análisis del archivo de registro de ConnectFour, que ejemplifican el modelo y los axiomas para una instancia bilateral del entorno. Los resultados que presentamos son un primer paso hacia un modelo ampliado que abarcará todos los tipos de entornos adversarios, por ejemplo, entornos que no son de suma cero y entornos que contienen agentes naturales que no forman parte del conflicto directo. Esos desafíos y más serán abordados en futuras investigaciones. 6. AGRADECIMIENTO Esta investigación fue apoyada en parte por las becas de la Fundación de Ciencia de Israel #1211/04 y #898/05. 7. REFERENCIAS [1] L. V. Allis. Un enfoque basado en el conocimiento de Conecta Cuatro: el juego está resuelto, gana el jugador Blanco. Tesis de maestría, Universidad Libre de Ámsterdam, Países Bajos, 1988. [2] D. Carmel y S. Markovitch. Incorporando modelos de oponentes en la búsqueda de adversarios. En Actas de la Decimotercera Conferencia Nacional de Inteligencia Artificial, páginas 120-125, Portland, Oregón, 1996. [3] D. Carmel y S. Markovitch. Modelado de oponentes en sistemas multiagente. En G. Weiß y S. Sen, editores, Adaptación y Aprendizaje en Sistemas Multiagente, páginas 40-52. Springer-Verlag, 1996. [4] B. J. Grosz y S. Kraus. Planes colaborativos para acciones grupales complejas. Inteligencia Artificial, 86(2):269-357, 1996. [5] M. Hadad, G. Kaminka, G. Armon y S. Kraus. Apoyando la actividad colaborativa. En Proc. de AAAI-2005, páginas 83-88, Pittsburgh, 2005. [6] http://www.gamerz.net/˜pbmserv/. [7] S. Kraus y D. Lehmann. Diseñando y construyendo un agente automatizado de negociación. Inteligencia Computacional, 11:132-171, 1995. [8] H. J. Levesque, P. R. Cohen y J. H. T. Nunes. Actuar juntos. En Proc. de AAAI-90, páginas 94-99, Boston, MA, 1990. [9] S. Markovitch y R. Reger. Aprender y aprovechar las debilidades relativas de los agentes oponentes. Agentes Autónomos y Sistemas Multiagente, 10(2):103-130, 2005. [10] Y. M. Ronald Fagin, Joseph Y. Halpern y M. Y. Vardi. Razonamiento sobre el conocimiento. MIT Press, Cambridge, Mass., 1995. [11] P. Thagard.\nMIT Press, Cambridge, Massachusetts, 1995. [11] P. Thagard. Resolución de problemas adversariales: Modelado de un oponente utilizando coherencia explicativa. Ciencia Cognitiva, 16(1):123-149, 1992. [12] R. W. Toseland y R. F. Rivas. Una introducción a la práctica del trabajo en grupo. Prentice Hall, Englewood Cliffs, NJ, segunda edición, 1995. [13] S. Willmott, J. Richardson, A. Bundy y J. Levine. Un enfoque de planificación adversarial para el juego de Go. Notas de conferencia en Ciencias de la Computación, 1558:93-112, 1999. El Sexto Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 557 ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "eval value": {
            "translated_key": "",
            "is_in_text": true,
            "original_annotated_sentences": [
                "An Adversarial Environment Model for Bounded Rational Agents in Zero-Sum Interactions Inon Zuckerman1 , Sarit Kraus1 , Jeffrey S. Rosenschein2 , Gal Kaminka1 1 Department of Computer Science 2 The School of Engineering Bar-Ilan University and Computer Science Ramat-Gan, Israel Hebrew University, Jerusalem, Israel {zukermi,sarit,galk}@cs.biu.ac.il jeff@cs.huji.ac.il ABSTRACT Multiagent environments are often not cooperative nor collaborative; in many cases, agents have conflicting interests, leading to adversarial interactions.",
                "This paper presents a formal Adversarial Environment model for bounded rational agents operating in a zero-sum environment.",
                "In such environments, attempts to use classical utility-based search methods can raise a variety of difficulties (e.g., implicitly modeling the opponent as an omniscient utility maximizer, rather than leveraging a more nuanced, explicit opponent model).",
                "We define an Adversarial Environment by describing the mental states of an agent in such an environment.",
                "We then present behavioral axioms that are intended to serve as design principles for building such adversarial agents.",
                "We explore the application of our approach by analyzing log files of completed Connect-Four games, and present an empirical analysis of the axioms appropriateness.",
                "Categories and Subject Descriptors I.2.11 [Artificial Intelligence]: Distributed Artificial Intelligence-Intelligent agents,Multiagent Systems; I.2.4 [Artificial Intelligence]: Knowledge Representation Formalisms and Methods -Modal logic General Terms Design, Theory 1.",
                "INTRODUCTION Early research in multiagent systems (MAS) considered cooperative groups of agents; because individual agents had limited resources, or limited access to information (e.g., limited processing power, limited sensor coverage), they worked together by design to solve problems that individually they could not solve, or at least could not solve as efficiently.",
                "MAS research, however, soon began to consider interacting agents with individuated interests, as representatives of different humans or organizations with non-identical interests.",
                "When interactions are guided by diverse interests, participants may have to overcome disagreements, uncooperative interactions, and even intentional attempts to damage one another.",
                "When these types of interactions occur, environments require appropriate behavior from the agents situated in them.",
                "We call these environments Adversarial Environments, and call the clashing agents Adversaries.",
                "Models of cooperation and teamwork have been extensively explored in MAS through the axiomatization of mental states (e.g., [8, 4, 5]).",
                "However, none of this research dealt with adversarial domains and their implications for agent behavior.",
                "Our paper addresses this issue by providing a formal, axiomatized mental state model for a subset of adversarial domains, namely simple zero-sum adversarial environments.",
                "Simple zero-sum encounters exist of course in various twoplayer games (e.g., Chess, Checkers), but they also exist in n-player games (e.g., Risk, Diplomacy), auctions for a single good, and elsewhere.",
                "In these latter environments especially, using a utility-based adversarial search (such as the Min-Max algorithm) does not always provide an adequate solution; the payoff function might be quite complex or difficult to quantify, and there are natural computational limitations on bounded rational agents.",
                "In addition, traditional search methods (like Min-Max) do not make use of a model of the opponent, which has proven to be a valuable addition to adversarial planning [9, 3, 11].",
                "In this paper, we develop a formal, axiomatized model for bounded rational agents that are situated in a zero-sum adversarial environment.",
                "The model uses different modality operators, and its main foundations are the SharedPlans [4] model for collaborative behavior.",
                "We explore environment properties and the mental states of agents to derive behavioral axioms; these behavioral axioms constitute a formal model that serves as a specification and design guideline for agent design in such settings.",
                "We then investigate the behavior of our model empirically using the Connect-Four board game.",
                "We show that this game conforms to our environment definition, and analyze players behavior using a large set of completed match log 550 978-81-904262-7-5 (RPS) c 2007 IFAAMAS files.",
                "In addition, we use the results presented in [9] to discuss the importance of opponent modeling in our ConnectFour adversarial domain.",
                "The paper proceeds as follows.",
                "Section 2 presents the models formalization.",
                "Section 3 presents the empirical analysis and its results.",
                "We discuss related work in Section 4, and conclude and present future directions in Section 5. 2.",
                "ADVERSARIAL ENVIRONMENTS The adversarial environment model (denoted as AE) is intended to guide the design of agents by providing a specification of the capabilities and mental attitudes of an agent in an adversarial environment.",
                "We focus here on specific types of adversarial environments, specified as follows: 1.",
                "Zero-Sum Interactions: positive and negative utilities of all agents sum to zero; 2.",
                "Simple AEs: all agents in the environment are adversarial agents; 3.",
                "Bilateral AEs: AEs with exactly two agents; 4.",
                "Multilateral AEs: AEs of three or more agents.",
                "We will work on both bilateral and multilateral instantiations of zero-sum and simple environments.",
                "In particular, our adversarial environment model will deal with interactions that consist of N agents (N ≥ 2), where all agents are adversaries, and only one agent can succeed.",
                "Examples of such environments range from board games (e.g., Chess, Connect-Four, and Diplomacy) to certain economic environments (e.g., N-bidder auctions over a single good). 2.1 Model Overview Our approach is to formalize the mental attitudes and behaviors of a single adversarial agent; we consider how a single agent perceives the AE.",
                "The following list specifies the conditions and mental states of an agent in a simple, zero-sum AE: 1.",
                "The agent has an individual intention that its own goal will be completed; 2.",
                "The agent has an individual belief that it and its adversaries are pursuing full conflicting goals (defined below)there can be only one winner; 3.",
                "The agent has an individual belief that each adversary has an intention to complete its own full conflicting goal; 4.",
                "The agent has an individual belief in the (partial) profile of its adversaries.",
                "Item 3 is required, since it might be the case that some agent has a full conflicting goal, and is currently considering adopting the intention to complete it, but is, as of yet, not committed to achieving it.",
                "This might occur because the agent has not yet deliberated about the effects that adopting that intention might have on the other intentions it is currently holding.",
                "In such cases, it might not consider itself to even be in an adversarial environment.",
                "Item 4 states that the agent should hold some belief about the profiles of its adversaries.",
                "The profile represents all the knowledge the agent has about its adversary: its weaknesses, strategic capabilities, goals, intentions, trustworthiness, and more.",
                "It can be given explicitly or can be learned from observations of past encounters. 2.2 Model Definitions for Mental States We use Grosz and Krauss definitions of the modal operators, predicates, and meta-predicates, as defined in their SharedPlan formalization [4].",
                "We recall here some of the predicates and operators that are used in that formalization: Int.To(Ai, α, Tn, Tα, C) represents Ais intentions at time Tn to do an action α at time Tα in the context of C. Int.Th(Ai, prop, Tn, Tprop, C) represents Ais intentions at time Tn that a certain proposition prop holds at time Tprop in the context of C. The potential intention operators, Pot.Int.To(...) and Pot.Int.Th(...), are used to represent the mental state when an agent considers adopting an intention, but has not deliberated about the interaction of the other intentions it holds.",
                "The operator Bel(Ai, f, Tf ) represents agent Ai believing in the statement expressed in formula f, at time Tf .",
                "MB(A, f, Tf ) represents mutual belief for a group of agents A.",
                "A snapshot of the system finds our environment to be in some state e ∈ E of environmental variable states, and each adversary in any LAi ∈ L of possible local states.",
                "At any given time step, the system will be in some world w of the set of all possible worlds w ∈ W, where w = E×LA1 ×LA2 × ...LAn , and n is the number of adversaries.",
                "For example, in a Texas Holdem poker game, an agents local state might be its own set of cards (which is unknown to its adversary) while the environment will consist of the betting pot and the community cards (which are visible to both players).",
                "A utility function under this formalization is defined as a mapping from a possible world w ∈ W to an element in , which expresses the desirability of the world, from a single agent perspective.",
                "We usually normalize the range to [0,1], where 0 represents the least desirable possible world, and 1 is the most desirable world.",
                "The implementation of the utility function is dependent on the domain in question.",
                "The following list specifies new predicates, functions, variables, and constants used in conjunction with the original definitions for the adversarial environment formalization: 1. φ is a null action (the agent does not do anything). 2.",
                "GAi is the set of agent Ais goals.",
                "Each goal is a set of predicates whose satisfaction makes the goal complete (we use G∗ Ai ∈ GAi to represent an arbitrary goal of agent Ai). 3. gAi is the set of agent Ais subgoals.",
                "Subgoals are predicates whose satisfaction represents an important milestone toward achievement of the full goal. gG∗ Ai ⊆ gAi is the set of subgoals that are important to the completion of goal G∗ Ai (we will use g∗ G∗ Ai ∈ gG∗ Ai to represent an arbitrary subgoal). 4.",
                "P Aj Ai is the profile object agent Ai holds about agent Aj. 5.",
                "CA is a general set of actions for all agents in A which are derived from the environments constraints.",
                "CAi ⊆ CA is the set of agent Ais possible actions. 6.",
                "Do(Ai, α, Tα, w) holds when Ai performs action α over time interval Tα in world w. 7.",
                "Achieve(G∗ Ai , α, w) is true when goal G∗ Ai is achieved following the completion of action α in world w ∈ W, where α ∈ CAi . 8.",
                "Profile(Ai, PAi Ai ) is true when agent Ai holds an object profile for agent Aj.",
                "Definition 1.",
                "Full conflict (FulConf ) describes a zerosum interaction where only a single goal of the goals in conflict can be completed.",
                "FulConf(G∗ Ai , G∗ Aj ) ⇒ (∃α ∈ CAi , ∀w, β ∈ CAj ) (Achieve(G∗ Ai , α, w) ⇒ ¬Achieve(G∗ Aj , β, w)) ∨ (∃β ∈ CAj , ∀w, α ∈ CAi )(Achieve(G∗ Aj , β, w) ⇒ ¬Achieve(G∗ Ai , α, w)) Definition 2.",
                "Adversarial Knowledge (AdvKnow) is a function returning a value which represents the amount of The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 551 knowledge agent Ai has on the profile of agent Aj, at time Tn.",
                "The higher the value, the more knowledge agent Ai has.",
                "AdvKnow : P Aj Ai × Tn → Definition 3.",
                "Eval - This evaluation function returns an estimated expected utility value for an agent in A, after completing an action from CA in some world state w. Eval : A × CA × w → Definition 4.",
                "TrH - (Threshold) is a numerical constant in the [0,1] range that represents an evaluation function (Eval) threshold value.",
                "An action that yields an estimated utility evaluation above the TrH is regarded as a highly beneficial action.",
                "The <br>eval value</br> is an estimation and not the real utility function, which is usually unknown.",
                "Using the real utility value for a rational agent would easily yield the best outcome for that agent.",
                "However, agents usually do not have the real utility functions, but rather a heuristic estimate of it.",
                "There are two important properties that should hold for the evaluation function: Property 1.",
                "The evaluation function should state that the most desirable world state is one in which the goal is achieved.",
                "Therefore, after the goal has been satisfied, there can be no future action that can put the agent in a world state with higher <br>eval value</br>. (∀Ai, G∗ Ai , α, β ∈ CAi , w ∈ W) Achieve(G∗ Ai , α, w) ⇒ Eval(Ai, α, w) ≥ Eval(Ai, β, w) Property 2.",
                "The evaluation function should project an action that causes a completion of a goal or a subgoal to a value which is greater than TrH (a highly beneficial action). (∀Ai, G∗ Ai ∈ GAi , α ∈ CAi , w ∈ W, g∗ GAi ∈ gGAi ) Achieve(G∗ Ai , α, w) ∨ Achieve(g∗ GAi , α, w) ⇒ Eval(Ai, α, w) ≥ TrH.",
                "Definition 5.",
                "SetAction We define a set action (SetAction) as a set of action operations (either complex or basic actions) from some action sets CAi and CAj which, according to agent Ais belief, are attached together by a temporal and consequential relationship, forming a chain of events (action, and its following consequent action). (∀α1 , . . . , αu ∈ CAi , β1 , . . . , βv ∈ CAj , w ∈ W) SetAction(α1 , . . . , αu , β1 , . . . , βv , w) ⇒ ((Do(Ai, α1 , Tα1 , w) ⇒ Do(Aj, β1 , Tβ1 , w)) ⇒ Do(Ai, α2 , Tα2 , w) ⇒ . . . ⇒ Do(Ai, αu , Tαu , w)) The consequential relation might exist due to various environmental constraints (when one action forces the adversary to respond with a specific action) or due to the agents knowledge about the profile of its adversary.",
                "Property 3.",
                "As the knowledge we have about our adversary increases we will have additional beliefs about its behavior in different situations which in turn creates new set actions.",
                "Formally, if our AdvKnow at time Tn+1 is greater than AdvKnow at time Tn, then every SetAction known at time Tn is also known at time Tn+1.",
                "AdvKnow(P Aj Ai , Tn+1) > AdvKnow(P Aj Ai , Tn) ⇒ (∀α1 , . . . , αu ∈ CAi , β1 , . . . , βv ∈ CAj ) Bel(Aag, SetAction(α1 , . . . , αu , β1 , . . . , βv ), Tn) ⇒ Bel(Aag, SetAction(α1 , . . . , αu , β1 , . . . , βv ), Tn+1) 2.3 The Environment Formulation The following axioms provide the formal definition for a simple, zero-sum Adversarial Environment (AE).",
                "Satisfaction of these axioms means that the agent is situated in such an environment.",
                "It provides specifications for agent Aag to interact with its set of adversaries A with respect to goals G∗ Aag and G∗ A at time TCo at some world state w. AE(Aag, A, G∗ Aag , A1, . . . , Ak, G∗ A1 , . . . , G∗ Ak , Tn, w) 1.",
                "Aag has an Int.Th his goal would be completed: (∃α ∈ CAag , Tα) Int.Th(Aag, Achieve(G∗ Aag , α), Tn, Tα, AE) 2.",
                "Aag believes that it and each of its adversaries Ao are pursuing full conflicting goals: (∀Ao ∈ {A1, . . . , Ak}) Bel(Aag, FulConf(G∗ Aag , G∗ Ao ), Tn) 3.",
                "Aag believes that each of his adversaries in Ao has the Int.Th his conflicting goal G∗ Aoi will be completed: (∀Ao ∈ {A1, . . . , Ak})(∃β ∈ CAo , Tβ) Bel(Aag, Int.Th(Ao, Achieve(G∗ Ao , β), TCo, Tβ, AE), Tn) 4.",
                "Aag has beliefs about the (partial) profiles of its adversaries (∀Ao ∈ {A1, . . . , Ak}) (∃PAo Aag ∈ PAag )Bel(Aag, Profile(Ao, PAo Aag ), Tn) To build an agent that will be able to operate successfully within such an AE, we must specify behavioral guidelines for its interactions.",
                "Using a naive Eval maximization strategy to a certain search depth will not always yield satisfactory results for several reasons: (1) the search horizon problem when searching for a fixed depth; (2) the strong assumption of an optimally rational, unbounded resources adversary; (3) using an estimated evaluation function which will not give optimal results in all world states, and can be exploited [9].",
                "The following axioms specify the behavioral principles that can be used to differentiate between successful and less successful agents in the above Adversarial Environment.",
                "Those axioms should be used as specification principles when designing and implementing agents that should be able to perform well in such Adversarial Environments.",
                "The behavioral axioms represent situations in which the agent will adopt potential intentions to (Pot.Int.To(...)) perform an action, which will typically require some means-end reasoning to select a possible course of action.",
                "This reasoning will lead to the adoption of an Int.To(...) (see [4]).",
                "A1.",
                "Goal Achieving Axiom.",
                "The first axiom is the simplest case; when the agent Aag believes that it is one action (α) away from achieving his conflicting goal G∗ Aag , it should adopt the potential intention to do α and complete its goal. (∀Aag, α ∈ CAag , Tn, Tα, w ∈ W) (Bel(Aag, Do(Aag, α, Tα, w) ⇒ Achieve(G∗ Aag , α, w)) ⇒ Pot.Int.To(Aag, α, Tn, Tα, w) This somewhat trivial behavior is the first and strongest axiom.",
                "In any situation, when the agent is an action away from completing the goal, it should complete the action.",
                "Any fair Eval function would naturally classify α as the maximal value action (property 1).",
                "However, without explicit axiomatization of such behavior there might be situations where the agent will decide on taking another action for various reasons, due to its bounded decision resources.",
                "A2.",
                "Preventive Act Axiom.",
                "Being in an adversarial situation, agent Aag might decide to take actions that will damage one of its adversarys plans to complete its goal, even if those actions do not explicitly advance Aag towards its conflicting goal G∗ Aag .",
                "Such preventive action will take place when agent Aag has a belief about the possibility of its adversary Ao doing an action β that will give it a high 552 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) utility evaluation value (> TrH).",
                "Believing that taking action α will prevent the opponent from doing its β, it will adopt a potential intention to do α. (∀Aag, Ao ∈ A, α ∈ CAag , β ∈ CAo , Tn, Tβ, w ∈ W) (Bel(Aag, Do(Ao, β, Tβ, w) ∧ Eval(Ao, β, w) > TrH, Tn) ∧ Bel(Aag, Do(Aag, α, Tα, w) ⇒ ¬Do(Ao, β, Tβ, w), Tn) ⇒ Pot.Int.To(Aag, α, Tn, Tα, w) This axiom is a basic component of any adversarial environment.",
                "For example, looking at a Chess board game, a player could realize that it is about to be checkmated by its opponent, thus making a preventive move.",
                "Another example is a Connect Four game: when a player has a row of three chips, its opponent must block it, or lose.",
                "A specific instance of A1 occurs when the adversary is one action away from achieving its goal, and immediate preventive action needs to be taken by the agent.",
                "Formally, we have the same beliefs as stated above, with a changed belief that doing action β will cause agent Ao to achieve its goal.",
                "Proposition 1: Prevent or lose case. (∀Aag, Ao ∈ A, α ∈ CAag , β ∈ CAo , G∗ Ao , Tn, Tα, Tβ, w ∈ W) Bel(Aag, Do(Ao, β, Tβ, w) ⇒ Achieve(G∗ Ao , β, w), Tn) ∧ Bel(Aag, Do(Aag, α, Tα, w) ⇒ ¬Do(Ao, β, Tβ, w)) ⇒ Pot.Int.To(Aag, α, Tn, Tα, w) Sketch of proof: Proposition 1 can be easily derived from axiom A1 and the property 2 of the Eval function, which states that any action that causes a completion of a goal is a highly beneficial action.",
                "The preventive act behavior will occur implicitly when the Eval function is equal to the real world utility function.",
                "However, being bounded rational agents and dealing with an estimated evaluation function we need to explicitly axiomatize such behavior, for it will not always occur implicitly from the evaluation function.",
                "A3.",
                "Suboptimal Tactical Move Axiom.",
                "In many scenarios a situation may occur where an agent will decide not to take the current most beneficial action it can take (the action with the maximal utility evaluation value), because it believes that taking another action (with lower utility evaluation value) might yield (depending on the adversarys response) a future possibility for a highly beneficial action.",
                "This will occur most often when the Eval function is inaccurate and differs by a large extent from the Utility function.",
                "Put formally, agent Aag believes in a certain SetAction that will evolve according to its initial action and will yield a high beneficial value (> TrH) solely for it. (∀Aag, Ao ∈ A, Tn, w ∈ W) (∃α1 , . . . , αu ∈ CAi , β1 , . . . , βv ∈ CAj , Tα1 ) Bel(Aag, SetAction(α1 , . . . , αu , β1 , . . . , βv ), Tn) ∧ Bel(Aag, Eval(Ao, βv , w) < TrH < Eval(Aag, αu , w), Tn) ⇒ Pot.Int.To(Aag, α1 , Tn, Tα1 , w) An agent might believe that a chain of events will occur for various reasons due to the inevitable nature of the domain.",
                "For example, in Chess, we often observe the following: a move causes a check position, which in turn limits the opponents moves to avoiding the check, to which the first player might react with another check, and so on.",
                "The agent might also believe in a chain of events based on its knowledge of its adversarys profile, which allows it to foresee the adversarys movements with high accuracy.",
                "A4.",
                "Profile Detection Axiom.",
                "The agent can adjust its adversarys profiles by observations and pattern study (specifically, if there are repeated encounters with the same adversary).",
                "However, instead of waiting for profile information to be revealed, an agent can also initiate actions that will force its adversary to react in a way that will reveal profile knowledge about it.",
                "Formally, the axiom states that if all actions (γ) are not highly beneficial actions (< TrH), the agent can do action α in time Tα if it believes that it will result in a non-highly beneficial action β from its adversary, which in turn teaches it about the adversarys profile, i.e., gives a higher AdvKnow(P Aj Ai , Tβ). (∀Aag, Ao ∈ A, α ∈ CAag , β ∈ CAo , Tn, Tα, Tβ, w ∈ W) Bel(Aag, (∀γ ∈ CAag )Eval(Aag, γ, w) < TrH, Tn) ∧ Bel(Aag, Do(Aag, α, Tα, w) ⇒ Do(Ao, β, Tβ, w), Tn) ∧ Bel(Aag, Eval(Ao, β, w) < TrH) ∧ Bel(Aag, AdvKnow(P Aj Ai , Tβ) > AdvKnow(P Aj Ai , Tn), Tn) ⇒ Pot.Int.To(Aag, α, Tn, Tα, w) For example, going back to the Chess board game scenario, consider starting a game versus an opponent about whom we know nothing, not even if it is a human or a computerized opponent.",
                "We might start playing a strategy that will be suitable versus an average opponent, and adjust our game according to its level of play.",
                "A5.",
                "Alliance Formation Axiom The following behavioral axiom is relevant only in a multilateral instantiation of the adversarial environment (obviously, an alliance cannot be formed in a bilateral, zero-sum encounter).",
                "In different situations during a multilateral interaction, a group of agents might believe that it is in their best interests to form a temporary alliance.",
                "Such an alliance is an agreement that constrains its members behavior, but is believed by its members to enable them to achieve a higher utility value than the one achievable outside of the alliance.",
                "As an example, we can look at the classical Risk board game, where each player has an individual goal of being the sole conquerer of the world, a zero-sum game.",
                "However, in order to achieve this goal, it might be strategically wise to make short-term ceasefire agreements with other players, or to join forces and attack an opponent who is stronger than the rest.",
                "An alliances terms defines the way its members should act.",
                "It is a set of predicates, denoted as Terms, that is agreed upon by the alliance members, and should remain true for the duration of the alliance.",
                "For example, the set Terms in the Risk scenario, could contain the following predicates: 1.",
                "Alliance members will not attack each other on territories X, Y and Z; 2.",
                "Alliance members will contribute C units per turn for attacking adversary Ao; 3.",
                "Members are obligated to stay as part of the alliance until time Tk or until adversarys Ao army is smaller than Q.",
                "The set Terms specifies inter-group constraints on each of the alliance members (∀Aal i ∈ Aal ⊆ A) set of actions Cal i ⊆ C. Definition 6.",
                "Al val - the total evaluation value that agent Ai will achieve while being part of Aal is the sum of Evali (Eval for Ai) of each of Aal j Eval values after taking their own α actions (via the agent(α) predicate): Al val(Ai, Cal , Aal , w) = α∈Cal Evali(Aal j , agent(α), w) Definition 7.",
                "Al TrH - is a number representing an Al val The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 553 threshold; above it, the alliance can be said to be a highly beneficial alliance.",
                "The value of Al TrH will be calculated dynamically according to the progress of the interaction, as can be seen in [7].",
                "After an alliance is formed, its members are now working in their normal adversarial environment, as well as according to the mental states and axioms required for their interactions as part of the alliance.",
                "The following Alliance model (AL) specifies the conditions under which the group Aal can be said to be in an alliance and working with a new and constrained set of actions Cal , at time Tn.",
                "AL(Aal , Cal , w, Tn) 1.",
                "Aal has a MB that all members are part of Aal : MB(Aal , (∀Aal i ∈ Aal )member(Aal i , Aal ), Tn) 2.",
                "Aal has a MB that the group be maintained: MB(Aal , (∀Aal i ∈ Aal )Int.Th (Ai, member(Ai, Aal ), Tn, Tn+1, Co), Tn) 3.",
                "Aal has a MB that being members gives them high utility value: MB(Aal , (∀Aal i ∈ Aal )Al val(Aal i , Cal , Aal , w) ≥ Al TrH, Tn) Members profiles are a crucial part of successful alliances.",
                "We assume that agents that have more accurate profiles of their adversaries will be more successful in such environments.",
                "Such agents will be able to predict when a member is about to breach the alliances contract (item 2 in the above model), and take counter measures (when item 3 will falsify).",
                "The robustness of the alliance is in part a function of its members trustfulness measure, objective position estimation, and other profile properties.",
                "We should note that an agent can simultaneously be part of more than one alliance.",
                "Such a temporary alliance, where the group members do not have a joint goal but act collaboratively for the interest of their own individual goals, is classified as a Treatment Group by modern psychologists [12] (in contrast to a Task Group, where its members have a joint goal).",
                "The Shared Activity model as presented in [5] modeled Treatment Group behavior using the same SharedPlans formalization.",
                "When comparing both definitions of an alliance and a Treatment Group we found an unsurprising resemblance between both models: the environment models definitions are almost identical (see SAs definitions in [5]), and their Selfish-Act and Cooperative Act axioms conform to our adversarial agents behavior.",
                "The main distinction between both models is the integration of a Helpful-behavior act axiom, in the Shared Activity which cannot be part of ours.",
                "This axiom states that an agent will consider taking action that will lower its <br>eval value</br> (to a certain lower bound), if it believes that a group partner will gain a significant benefit.",
                "Such behavior cannot occur in a pure adversarial environment (as a zero-sum game is), where the alliance members are constantly on watch to manipulate their alliance to their own advantage.",
                "A6.",
                "Evaluation Maximization Axiom.",
                "In a case when all other axioms are inapplicable, we will proceed with the action that maximizes the heuristic value as computed in the Eval function. (∀Aag, Ao ∈ A, α ∈ Cag, Tn, w ∈ W) Bel(Aag, (∀γ ∈ Cag)Eval(Aag, α, w) ≥ Eval(Aag, γ, w), Tn) ⇒ Pot.Int.To(Aag, α, Tn, Tα, w) T1.",
                "Optimality on Eval = Utility The above axiomatic model handles situations where the Utility is unknown and the agents are bounded rational agents.",
                "The following theorem shows that in bilateral interactions, where the agents have the real Utility function (i.e., Eval = Utility) and are rational agents, the axioms provide the same optimal result as classic adversarial search (e.g., Min-Max).",
                "Theorem 1.",
                "Let Ae ag be an unbounded rational AE agent using the Eval heuristic evaluation function, Au ag be the same agent using the true Utility function, and Ao be a sole unbounded utility-based rational adversary.",
                "Given that Eval = Utility: (∀α ∈ CAu ag , α ∈ CAe ag , Tn, w ∈ W) Pot.Int.To(Au ag, α, Tn, Tα, w) → Pot.Int.To(Ae ag, α , Tn, Tα, w) ∧ ((α = α ) ∨ (Utility(Au ag, α, w) = Eval(Ae ag, α , w))) Sketch of proof - Given that Au ag has the real utility function and unbounded resources, it can generate the full game tree and run the optimal MinMax algorithm to choose the highest utility value action, which we denote by, α.",
                "The proof will show that Ae ag, using the AE axioms, will select the same or equal utility α (when there is more than one action with the same max utility) when Eval = Utility. (A1) Goal achieving axiom - suppose there is an α such that its completion will achieve Au ags goal.",
                "It will obtain the highest utility by Min-Max for Au ag.",
                "The Ae ag agent will select α or another action with the same utility value via A1.",
                "If such α does not exist, Ae ag cannot apply this axiom, and proceeds to A2. (A2) Preventive act axiom - (1) Looking at the basic case (see Prop1), if there is a β which leads Ao to achieve its goal, then a preventive action α will yield the highest utility for Au ag.",
                "Au ag will choose it through the utility, while Ae ag will choose it through A2. (2) In the general case, β is a highly beneficial action for Ao, thus yields low utility for Au ag, which will guide it to select an α that will prevent β, while Ae ag will choose it through A2.1 If such β does not exist for Ao, then A2 is not applicable, and Ae ag can proceed to A3. (A3) Suboptimal tactical move axiom - When using a heuristic Eval function, Ae ag has a partial belief in the profile of its adversary (item 4 in AE model), which may lead it to believe in SetActions (Prop1).",
                "In our case, Ae ag is holding a full profile on its optimal adversary and knows that Ao will behave optimally according to the real utility values on the complete search tree, therefore, any belief about suboptimal SetAction cannot exist, yielding this axiom inapplicable.",
                "Ae ag will proceed to A4. (A4) Profile detection axiom - Given that Ae ag has the full profile of Ao, none of Ae ags actions can increase its knowledge.",
                "That axiom will not be applied, and the agent will proceed with A6 (A5 will be disregarded because the interaction is bilateral). (A6) Evaluation maximization axiom - This axiom will select the max Eval for Ae ag.",
                "Given that Eval = Utility, the same α that was selected by Au ag will be selected. 3.",
                "EVALUATION The main purpose of our experimental analysis is to evaluate the models behavior and performance in a real adversarial environment.",
                "This section investigates whether bounded 1 A case where following the completion of β there exists a γ which gives high utility for Agent Au ag, cannot occur because Ao uses the same utility, and γs existence will cause it to classify β as a low utility action. 554 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) rational agents situated in such adversarial environments will be better off applying our suggested behavioral axioms. 3.1 The Domain To explore the use of the above model and its behavioral axioms, we decided to use the Connect-Four game as our adversarial environment.",
                "Connect-Four is a 2-player, zerosum game which is played using a 6x7 matrix-like board.",
                "Each turn, a player drops a disc into one of the 7 columns (the set of 21 discs is usually colored yellow for player 1 and red for player 2; we will use White and Black respectively to avoid confusion).",
                "The winner is the first player to complete a horizontal, vertical, or diagonal set of four discs with its color.",
                "On very rare occasions, the game might end in a tie if all the empty grids are filled, but no player managed to create a 4-disc set.",
                "The Connect-Four game was solved in [1], where it is shown that the first player (playing with the white discs) can force a win by starting in the middle column (column 4) and playing optimally However, the optimal strategy is very complex, and difficult to follow even for complex bounded rational agents, such as human players.",
                "Before we can proceed checking agent behavior, we must first verify that the domain conforms to the adversarial environments definition as given above (which the behavioral axioms are based on).",
                "First, when playing a Connect-Four game, the agent has an intention to win the game (item 1).",
                "Second (item 2), our agent believes that in Connect-Four there can only be one winner (or no winner at all in the rare occurrence of a tie).",
                "In addition, our agent believes that its opponent to the game will try to win (item 3), and we hope it has some partial knowledge (item 4) about its adversary (this knowledge can vary from nothing, through simple facts such as age, to strategies and weaknesses).",
                "Of course, not all Connect-Four encounters are adversarial.",
                "For example, when a parent is playing the game with its child, the following situation might occur: the child, having a strong incentive to win, treats the environment as adversarial (it intends to win, understands that there can only be one winner, and believes that its parent is trying to beat him).",
                "However, the parents point of view might see the environment as an educational one, where its goal is not to win the game, but to cause enjoyment or practice strategic reasoning.",
                "In such an educational environment, a new set of behavioral axioms might be more beneficial to the parents goals than our suggested adversarial behavioral axioms. 3.2 Axiom Analysis After showing that the Connect-Four game is indeed a zero-sum, bilateral adversarial environment, the next step is to look at players behaviors during the game and check whether behaving according to our model does improve performance.",
                "To do so we have collected log files from completed Connect-Four games that were played by human players over the Internet.",
                "Our collected log file data came from Play by eMail (PBeM) sites.",
                "These are web sites that host email games, where each move is taken by an email exchange between the server and the players.",
                "Many such sites archives contain real competitive interactions, and also maintain a ranking system for their members.",
                "Most of the data we used can be found in [6].",
                "As can be learned from [1], Connect-Four has an optimal strategy and a considerable advantage for the player who starts the game (which we call the White player).",
                "We will concentrate in our analysis on the second players moves (to be called Black).",
                "The White player, being the first to act, has the so-called initiative advantage.",
                "Having the advantage and a good strategy will keep the Black player busy reacting to its moves, instead of initiating threats.",
                "A threat is a combination of three discs of the same color, with an empty spot for the fourth winning disk.",
                "An open threat is a threat that can be realized in the opponents next move.",
                "In order for the Black player to win, it must somehow turn the tide, take the advantage and start presenting threats to the White player.",
                "We will explore Black players behavior and their conformance to our axioms.",
                "To do so, we built an application that reads log files and analyzes the Black players moves.",
                "The application contains two main components: (1) a Min-Max algorithm for evaluation of moves; (2) open threats detector for the discovering of open threats.",
                "The Min-Max algorithm will work to a given depth, d and for each move α will output the heuristic value for the next action taken by the player as written in the log file, h(α), alongside the maximum heuristic value, maxh(α), that could be achieved prior to taking the move (obviously, if h(α) = maxh(α), then the player did not do the optimal move heuristically).",
                "The threat detectors job is to notify if some action was taken in order to block an open threat (not blocking an open threat will probably cause the player to lose in the opponents next move).",
                "The heuristic function used by Min-Max to evaluate the players utility is the following function, which is simple to compute, yet provides a reasonable challenge to human opponents: Definition 8.",
                "Let Group be an adjacent set of four squares that are horizontal, vertical, or diagonal.",
                "Groupn b (Groupn w) be a Group with n pieces of the black (white) color and 4−n empty squares. h = ((Group1 b ∗α)+(Group2 b ∗β)+(Group3 b ∗γ)+(Group4 b ∗∞)) − ((Group1 w ∗α)+(Group2 w ∗β)+(Group3 w ∗γ)+(Group4 w ∗∞)) The values of α, β and δ can vary to form any desired linear combination; however, it is important to value them with the α < β < δ ordering in mind (we used 1, 4, and 8 as their respective values).",
                "Groups of 4 discs of the same color means victory, thus discovery of such a group will result in ∞ to ensure an extreme value.",
                "We now use our estimated evaluation function to evaluate the Black players actions during the Connect-Four adversarial interaction.",
                "Each game from the log file was input into the application, which processed and output a reformatted log file containing the h value of the current move, the maxh value that could be achieved, and a notification if an open threat was detected.",
                "A total of 123 games were analyzed (57 with White winning, and 66 with Black winning).",
                "A few additional games were manually ignored in the experiment, due to these problems: a player abandoning the game while the outcome is not final, or a blunt irrational move in the early stages of the game (e.g., not blocking an obvious winning group in the first opening moves).",
                "In addition, a single tie game was also removed.",
                "The simulator was run to a search depth of 3 moves.",
                "We now proceed to analyze the games with respect to each behavioral axiom.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 555 Table 1: Average heuristic difference analysis Black losses Black Won Avg minh -17.62 -12.02 Avg 3 lowest h moves (min3 h) -13.20 -8.70 3.2.1 Affirming the Suboptimal tactical move axiom The following section presents the heuristic evaluations of the Min-Max algorithm for each action, and checks the amount and extent of suboptimal tactical actions and their implications on performance.",
                "Table 1 shows results and insights from the games heuristic analysis, when search depth equals 3 (this search depth was selected for the results to be comparable to [9], see Section 3.2.3).",
                "The tables heuristic data is the difference between the present maximal heuristic value and the heuristic value of the action that was eventually taken by the player (i.e., the closer the number is to 0, the closer the action was to the maximum heuristic action).",
                "The first row presents the difference values of the action that had the maximal difference value among all the Black players actions in a given game, as averaged over all Blacks winning and losing games (see respective columns).",
                "In games in which the Black player loses, its average difference value was -17.62, while in games in which the Black player won, its average was -12.02.",
                "The second row expands the analysis by considering the 3 highest heuristic difference actions, and averaging them.",
                "In that case, we notice an average heuristic difference of 5 points between games which the Black player loses and games in which it wins.",
                "Nevertheless, the importance of those numbers is that they allowed us to take an educated guess on a threshold number of 11.5, as the value of the TrH constant, which differentiates between normal actions and highly beneficial ones.",
                "After finding an approximated TrH constant, we can proceed with an analysis of the importance of suboptimal moves.",
                "To do so we took the subset of games in which the minimum heuristic difference value for Blacks actions was 11.5.",
                "As presented in Table 2, we can see the different min3 h average of the 3 largest ranges and the respective percentage of games won.",
                "The first row shows that the Black player won only 12% of the games in which the average of its 3 highest heuristically difference actions (min3 h) was smaller than the suggested threshold, TrH = 11.5.",
                "The second row shows a surprising result: it seems that when min3 h > −4 the Black player rarely wins.",
                "Intuition would suggest that games in which the action evaluation values were closer to the maximal values will result in more winning games for Black.",
                "However, it seems that in the Connect-Four domain, merely responding with somewhat easily expected actions, without initiating a few surprising and suboptimal moves, does not yield good results.",
                "The last row sums up the main insights from the analysis; most of Blacks wins (83%) came when its min3 h was in the range of -11.5 to -4.",
                "A close inspection of those Black winning games shows the following pattern behind the numbers: after standard opening moves, Black suddenly drops a disc into an isolated column, which seems a waste of a move.",
                "White continues to build its threats, while usually disregarding Blacks last move, which in turn uses the isolated disc as an anchor for a future winning threat.",
                "The results show that it was beneficial for the Black player Table 2: Blacks winnings percentages % of games min3 h < −11.5 12% min3 h > −4 5% −11.5 ≤ min3 h ≤ −4 83% to take suboptimal actions and not give the current highest possible heuristic value, but will not be too harmful for its position (i.e., will not give high beneficial value to its adversary).",
                "As it turned out, learning the threshold is an important aspect of success: taking wildly risky moves (min3 h < −11.5) or trying to avoid them (min3 h > −4) reduces the Black players winning chances by a large margin. 3.2.2 Affirming the Profile Monitoring Axiom In the task of showing the importance of monitoring ones adversaries profiles, our log files could not be used because they did not contain repeated interactions between players, which are needed to infer the players knowledge about their adversaries.",
                "However, the importance of opponent modeling and its use in attaining tactical advantages was already studied in various domains ([3, 9] are good examples).",
                "In a recent paper, Markovitch and Reger [9] explored the notion of learning and exploitation of opponent weakness in competitive interactions.",
                "They apply simple learning strategies by analyzing examples from past interactions in a specific domain.",
                "They also used the Connect-Four adversarial domain, which can now be used to understand the importance of monitoring the adversarys profile.",
                "Following the presentation of their theoretical model, they describe an extensive empirical study and check the agents performance after learning the weakness model with past examples.",
                "One of the domains used as a competitive environment was the same Connect-Four game (Checkers was the second domain).",
                "Their heuristic function was identical to ours with three different variations (H1, H2, and H3) that are distinguished from one another in their linear combination coefficient values.",
                "The search depth for the players was 3 (as in our analysis).",
                "Their extensive experiments check and compare various learning strategies, risk factors, predefined feature sets and usage methods.",
                "The bottom line is that the Connect-Four domain shows an improvement from a 0.556 winning rate before modeling to a 0.69 after modeling (page 22).",
                "Their conclusions, showing improved performance when holding and using the adversarys model, justify the effort to monitor the adversary profile for continuous and repeated interactions.",
                "An additional point that came up in their experiments is the following: after the opponent weakness model has been learned, the authors describe different methods of integrating the opponent weakness model into the agents decision strategy.",
                "Nevertheless, regardless of the specific method they chose to work with, all integration methods might cause the agent to take suboptimal decisions; it might cause the agent to prefer actions that are suboptimal at the present decision junction, but which might cause the opponent to react in accordance with its weakness model (as represented by our agent) which in turn will be beneficial for us in the future.",
                "The agents behavior, as demonstrated in [9] further confirms and strengthens our Suboptimal Tactical Axiom as discussed in the previous section. 556 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 3.2.3 Additional Insights The need for the Goal Achieving, Preventive Act, and Evaluation Maximization axioms are obvious, and need no further verification.",
                "However, even with respect to those axioms, a few interesting insights came up in the log analysis.",
                "The Goal achieving and Preventive Act axioms, though theoretically trivial, seem to provide some challenge to a human player.",
                "In the initial inspection of the logs, we encountered few games2 where a player, for inexplicable reasons, did not block the other from winning or failed to execute its own winning move.",
                "We can blame those faults on the humans lack of attention, or a typing error in its move reply; nevertheless, those errors might occur in bounded rational agents, and the appropriate behavior needs to be axiomatized.",
                "A typical Connect-Four game revolves around generating threats and blocking them.",
                "In our analysis we looked for explicit preventive actions, i.e., moves that block a group of 3 discs, or that remove a future threat (in our limited search horizon).",
                "We found that in 83% of the total games there was at least one preventive action taken by the Black player.",
                "It was also found that Black averaged 2.8 preventive actions per game on the games in which it lost, while averaging 1.5 preventive actions per game when winning.",
                "It seems that Black requires 1 or 2 preventive actions to build its initial taking position, before starting to present threats.",
                "If it did not manage to win, it will usually prevent an extra threat or two before succumbing to White. 4.",
                "RELATED WORK Much research deals with the axiomatization of teamwork and mental states of individuals: some models use knowledge and belief [10], others have models of goals and intentions [8, 4].",
                "However, all these formal theories deal with agent teamwork and cooperation.",
                "As far as we know, our model is the first to provide a formalized model for explicit adversarial environments and agents behavior in it.",
                "The classical Min-Max adversarial search algorithm was the first attempt to integrate the opponent into the search space with a weak assumption of an optimally playing opponent.",
                "Since then, much effort has gone into integrating the opponent model into the decision procedure to predict future behavior.",
                "The M∗ algorithm presented by Carmel and Markovitch [2] showed a method of incorporating opponent models into adversary search, while in [3] they used learning to provide a more accurate opponent model in a 2player repeated game environment, where agents strategies were modeled as finite automata.",
                "Additional Adversarial planning work was done by Willmott et al. [13], which provided an adversarial planning approach to the game of GO.",
                "The research mentioned above dealt with adversarial search and the integration of opponent models into classical utilitybased search methods.",
                "That work shows the importance of opponent modeling and the ability to exploit it to an agents advantage.",
                "However, the basic limitations of those search methods still apply; our model tries to overcome those limitations by presenting a formal model for a new, mental state-based adversarial specification. 5.",
                "CONCLUSIONS We presented an Adversarial Environment model for a 2 These were later removed from the final analysis. bounded rational agent that is situated in an N-player, zerosum environment.",
                "We used the SharedPlans formalization to define the model and the axioms that agents can apply as behavioral guidelines.",
                "The model is meant to be used as a guideline for designing agents that need to operate in such adversarial environments.",
                "We presented empirical results, based on ConnectFour log file analysis, that exemplify the model and the axioms for a bilateral instance of the environment.",
                "The results we presented are a first step towards an expanded model that will cover all types of adversarial environments, for example, environments that are non-zero-sum, and environments that contain natural agents that are not part of the direct conflict.",
                "Those challenges and more will be dealt with in future research. 6.",
                "ACKNOWLEDGMENT This research was supported in part by Israel Science Foundation grants #1211/04 and #898/05. 7.",
                "REFERENCES [1] L. V. Allis.",
                "A knowledge-based approach of Connect-Four - the game is solved: White wins.",
                "Masters thesis, Free University, Amsterdam, The Netherlands, 1988. [2] D. Carmel and S. Markovitch.",
                "Incorporating opponent models into adversary search.",
                "In Proceedings of the Thirteenth National Conference on Artificial Intelligence, pages 120-125, Portland, OR, 1996. [3] D. Carmel and S. Markovitch.",
                "Opponent modeling in multi-agent systems.",
                "In G. Weiß and S. Sen, editors, Adaptation and Learning in Multi-Agent Systems, pages 40-52.",
                "Springer-Verlag, 1996. [4] B. J. Grosz and S. Kraus.",
                "Collaborative plans for complex group action.",
                "Artificial Intelligence, 86(2):269-357, 1996. [5] M. Hadad, G. Kaminka, G. Armon, and S. Kraus.",
                "Supporting collaborative activity.",
                "In Proc. of AAAI-2005, pages 83-88, Pittsburgh, 2005. [6] http://www.gamerz.net/˜pbmserv/. [7] S. Kraus and D. Lehmann.",
                "Designing and building a negotiating automated agent.",
                "Computational Intelligence, 11:132-171, 1995. [8] H. J. Levesque, P. R. Cohen, and J. H. T. Nunes.",
                "On acting together.",
                "In Proc. of AAAI-90, pages 94-99, Boston, MA, 1990. [9] S. Markovitch and R. Reger.",
                "Learning and exploiting relative weaknesses of opponent agents.",
                "Autonomous Agents and Multi-Agent Systems, 10(2):103-130, 2005. [10] Y. M. Ronald Fagin, Joseph Y. Halpern and M. Y. Vardi.",
                "Reasoning about knowledge.",
                "MIT Press, Cambridge, Mass., 1995. [11] P. Thagard.",
                "Adversarial problem solving: Modeling an oponent using explanatory coherence.",
                "Cognitive Science, 16(1):123-149, 1992. [12] R. W. Toseland and R. F. Rivas.",
                "An Introduction to Group Work Practice.",
                "Prentice Hall, Englewood Cliffs, NJ, 2nd edition edition, 1995. [13] S. Willmott, J. Richardson, A. Bundy, and J. Levine.",
                "An adversarial planning approach to Go.",
                "Lecture Notes in Computer Science, 1558:93-112, 1999.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 557"
            ],
            "original_annotated_samples": [
                "The <br>eval value</br> is an estimation and not the real utility function, which is usually unknown.",
                "Therefore, after the goal has been satisfied, there can be no future action that can put the agent in a world state with higher <br>eval value</br>. (∀Ai, G∗ Ai , α, β ∈ CAi , w ∈ W) Achieve(G∗ Ai , α, w) ⇒ Eval(Ai, α, w) ≥ Eval(Ai, β, w) Property 2.",
                "This axiom states that an agent will consider taking action that will lower its <br>eval value</br> (to a certain lower bound), if it believes that a group partner will gain a significant benefit."
            ],
            "translated_annotated_samples": [
                "El <br>valor Eval</br> es una estimación y no la función de utilidad real, la cual suele ser desconocida.",
                "Por lo tanto, una vez que se ha satisfecho el objetivo, no puede haber ninguna acción futura que pueda colocar al agente en un estado del mundo con un <br>valor de Eval</br> más alto. (∀Ai, G∗ Ai , α, β ∈ CAi , w ∈ W) Lograr(G∗ Ai , α, w) ⇒ Eval(Ai, α, w) ≥ Eval(Ai, β, w) Propiedad 2.",
                "Este axioma establece que un agente considerará tomar una acción que disminuirá su <br>valor de Evaluación</br> (hasta un cierto límite inferior), si cree que un compañero del grupo obtendrá un beneficio significativo."
            ],
            "translated_text": "Un modelo de entorno adversarial para agentes racionales limitados en interacciones de suma cero Inon Zuckerman1, Sarit Kraus1, Jeffrey S. Rosenschein2, Gal Kaminka1 1 Departamento de Ciencias de la Computación 2 Escuela de Ingeniería Universidad Bar-Ilan y Ciencias de la Computación Ramat-Gan, Israel Universidad Hebrea, Jerusalén, Israel {zukermi, sarit, galk}@cs.biu.ac.il jeff@cs.huji.ac.il RESUMEN Los entornos multiagentes a menudo no son cooperativos ni colaborativos; en muchos casos, los agentes tienen intereses conflictivos, lo que lleva a interacciones adversariales. Este documento presenta un modelo formal de Entorno Adversarial para agentes racionales acotados que operan en un entorno de suma cero. En tales entornos, los intentos de utilizar métodos de búsqueda basados en utilidad clásica pueden plantear una variedad de dificultades (por ejemplo, modelar implícitamente al oponente como un maximizador de utilidad omnisciente, en lugar de aprovechar un modelo de oponente más matizado y explícito). Definimos un Entorno Adversario al describir los estados mentales de un agente en dicho entorno. Luego presentamos axiomas de comportamiento que están destinados a servir como principios de diseño para construir agentes adversarios. Exploramos la aplicación de nuestro enfoque mediante el análisis de archivos de registro de juegos de Conecta Cuatro completados, y presentamos un análisis empírico de la adecuación de los axiomas. Categorías y Descriptores de Asignaturas I.2.11 [Inteligencia Artificial]: Inteligencia Artificial Distribuida-Agentes Inteligentes, Sistemas Multiagente; I.2.4 [Inteligencia Artificial]: Formalismos y Métodos de Representación del Conocimiento - Lógica Modal Términos Generales Diseño, Teoría 1. Introducción: En las primeras investigaciones en sistemas multiagente (SMA) se consideraban grupos cooperativos de agentes; debido a que los agentes individuales tenían recursos limitados o acceso limitado a la información (por ejemplo, potencia de procesamiento limitada, cobertura sensorial limitada), trabajaban juntos por diseño para resolver problemas que individualmente no podían resolver, o al menos no podían resolver de manera eficiente. Sin embargo, la investigación de MAS pronto comenzó a considerar agentes interactivos con intereses individualizados, como representantes de diferentes humanos u organizaciones con intereses no idénticos. Cuando las interacciones son guiadas por intereses diversos, los participantes pueden tener que superar desacuerdos, interacciones no cooperativas e incluso intentos intencionales de dañarse mutuamente. Cuando ocurren este tipo de interacciones, los entornos requieren un comportamiento apropiado por parte de los agentes situados en ellos. Llamamos a estos entornos Entornos Adversarios, y llamamos a los agentes en conflicto Adversarios. Los modelos de cooperación y trabajo en equipo han sido ampliamente explorados en MAS a través de la axiomatización de estados mentales (por ejemplo, [8, 4, 5]). Sin embargo, ninguna de estas investigaciones abordó los dominios adversarios y sus implicaciones para el comportamiento de los agentes. Nuestro artículo aborda este problema al proporcionar un modelo formal y axiomatizado de estados mentales para un subconjunto de dominios adversariales, específicamente entornos adversariales simples de suma cero. Los encuentros de suma cero simples existen, por supuesto, en varios juegos de dos jugadores (por ejemplo, Ajedrez, Damas), pero también existen en juegos de n jugadores (por ejemplo, Risk, Diplomacia), subastas de un solo bien y en otros lugares. En estos entornos recientes en particular, el uso de una búsqueda adversarial basada en utilidad (como el algoritmo Min-Max) no siempre proporciona una solución adecuada; la función de recompensa podría ser bastante compleja o difícil de cuantificar, y existen limitaciones computacionales naturales en agentes racionales acotados. Además, los métodos de búsqueda tradicionales (como Min-Max) no hacen uso de un modelo del oponente, lo cual ha demostrado ser una adición valiosa a la planificación adversarial [9, 3, 11]. En este artículo, desarrollamos un modelo formal y axiomatizado para agentes racionales limitados que se encuentran en un entorno adversarial de suma cero. El modelo utiliza diferentes operadores de modalidad, y sus principales fundamentos son el modelo SharedPlans [4] para el comportamiento colaborativo. Exploramos las propiedades del entorno y los estados mentales de los agentes para derivar axiomas de comportamiento; estos axiomas de comportamiento constituyen un modelo formal que sirve como especificación y guía de diseño para el diseño de agentes en tales entornos. Luego investigamos el comportamiento de nuestro modelo empíricamente utilizando el juego de mesa Conecta Cuatro. Mostramos que este juego se ajusta a nuestra definición de entorno, y analizamos el comportamiento de los jugadores utilizando un gran conjunto de registros de partidas completadas 550 978-81-904262-7-5 (RPS) c 2007 archivos de IFAAMAS. Además, utilizamos los resultados presentados en [9] para discutir la importancia del modelado del oponente en nuestro dominio adversarial de Conecta Cuatro. El artículo continúa de la siguiente manera. La sección 2 presenta la formalización de los modelos. La sección 3 presenta el análisis empírico y sus resultados. Discutimos el trabajo relacionado en la Sección 4, y concluimos y presentamos las direcciones futuras en la Sección 5. ENTORNOS ADVERSARIOS El modelo de entorno adversario (denominado como AE) está destinado a guiar el diseño de agentes al proporcionar una especificación de las capacidades y actitudes mentales de un agente en un entorno adversario. Nos enfocamos aquí en tipos específicos de entornos adversarios, especificados de la siguiente manera: 1. Interacciones de suma cero: las utilidades positivas y negativas de todos los agentes suman cero; 2. Agentes adversarios simples: todos los agentes en el entorno son agentes adversarios; 3. AEs bilaterales: AEs con exactamente dos agentes; 4. AEs multilaterales: AEs de tres o más agentes. Trabajaremos en instanciaciones tanto bilaterales como multilaterales de entornos de suma cero y simples. En particular, nuestro modelo de entorno adversarial tratará con interacciones que consisten en N agentes (N ≥ 2), donde todos los agentes son adversarios y solo un agente puede tener éxito. Ejemplos de tales entornos van desde juegos de mesa (por ejemplo, Ajedrez, Conecta Cuatro y Diplomacia) hasta ciertos entornos económicos (por ejemplo, subastas de N postores sobre un único bien). 2.1 Resumen del Modelo Nuestro enfoque es formalizar las actitudes mentales y comportamientos de un único agente adversario; consideramos cómo un único agente percibe el AE. La siguiente lista especifica las condiciones y estados mentales de un agente en un AE simple de suma cero: 1. El agente tiene la intención individual de que su propio objetivo se complete; 2. El agente tiene la creencia individual de que él y sus adversarios persiguen objetivos completamente opuestos (definidos a continuación) y que solo puede haber un ganador; 3. El agente tiene la creencia individual de que cada adversario tiene la intención de completar su propio objetivo conflictivo completo; 4. El agente tiene una creencia individual en el perfil (parcial) de sus adversarios. El ítem 3 es necesario, ya que podría ser el caso de que algún agente tenga un objetivo completamente conflictivo y esté considerando adoptar la intención de completarlo, pero aún no está comprometido a lograrlo. Esto podría ocurrir porque el agente aún no ha reflexionado sobre los efectos que adoptar esa intención podría tener en las otras intenciones que actualmente mantiene. En tales casos, es posible que ni siquiera se considere en un entorno adversarial. El ítem 4 establece que el agente debe tener alguna creencia sobre los perfiles de sus adversarios. El perfil representa todo el conocimiento que el agente tiene sobre su adversario: sus debilidades, capacidades estratégicas, objetivos, intenciones, confiabilidad y más. Puede ser dado explícitamente o puede ser aprendido a partir de observaciones de encuentros pasados. 2.2 Definiciones de modelos para estados mentales Utilizamos las definiciones de los operadores modales, predicados y metapredicados de Grosz y Krauss, tal como se definen en su formalización de SharedPlan [4]. Recordamos aquí algunos de los predicados y operadores que se utilizan en esa formalización: Int.To(Ai, α, Tn, Tα, C) representa las intenciones de Ai en el tiempo Tn de realizar una acción α en el tiempo Tα en el contexto de C. Int.Th(Ai, prop, Tn, Tprop, C) representa las intenciones de Ai en el tiempo Tn de que una cierta proposición prop se cumpla en el tiempo Tprop en el contexto de C. Los operadores de intención potencial, Pot.Int.To(...) y Pot.Int.Th(...), se utilizan para representar el estado mental cuando un agente considera adoptar una intención, pero no ha deliberado sobre la interacción de las otras intenciones que tiene. El operador Bel(Ai, f, Tf) representa al agente Ai creyendo en la afirmación expresada en la fórmula f, en el tiempo Tf. MB(A, f, Tf) representa la creencia mutua para un grupo de agentes A. Una instantánea del sistema encuentra que nuestro entorno se encuentra en algún estado e ∈ E de estados variables ambientales, y cada adversario en cualquier LAi ∈ L de posibles estados locales. En cualquier paso de tiempo dado, el sistema estará en algún mundo w del conjunto de todos los mundos posibles w ∈ W, donde w = E×LA1 ×LA2 × ...LAn, y n es el número de adversarios. Por ejemplo, en un juego de póker Texas Holdem, el estado local de un agente podría ser su propio conjunto de cartas (que es desconocido para su adversario) mientras que el entorno consistirá en el bote de apuestas y las cartas comunitarias (que son visibles para ambos jugadores). Una función de utilidad bajo esta formalización se define como un mapeo de un mundo posible w ∈ W a un elemento en , que expresa la deseabilidad del mundo, desde la perspectiva de un único agente. Normalmente normalizamos el rango a [0,1], donde 0 representa el mundo menos deseable posible, y 1 es el mundo más deseable. La implementación de la función de utilidad depende del dominio en cuestión. La siguiente lista especifica nuevos predicados, funciones, variables y constantes utilizadas en conjunto con las definiciones originales para la formalización del entorno adversarial: 1. φ es una acción nula (el agente no hace nada). 2. GAi es el conjunto de objetivos del agente Ai. Cada objetivo es un conjunto de predicados cuya satisfacción hace que el objetivo esté completo (usamos G∗ Ai ∈ GAi para representar un objetivo arbitrario del agente Ai). 3. gAi es el conjunto de subobjetivos del agente Ai. Los subobjetivos son predicados cuya satisfacción representa un hito importante hacia el logro del objetivo completo. gG∗ Ai ⊆ gAi es el conjunto de subobjetivos que son importantes para la finalización del objetivo G∗ Ai (usaremos g∗ G∗ Ai ∈ gG∗ Ai para representar un subobjetivo arbitrario). 4. P Aj Ai es el objeto de perfil que el agente Ai tiene sobre el agente Aj. 5. CA es un conjunto general de acciones para todos los agentes en A que se derivan de las restricciones del entorno. CAi ⊆ CA es el conjunto de acciones posibles del agente Ai. La expresión Do(Ai, α, Tα, w) se cumple cuando Ai realiza la acción α durante el intervalo de tiempo Tα en el mundo w. 7. La afirmación Achieve(G∗ Ai , α, w) es verdadera cuando se logra el objetivo G∗ Ai siguiendo la finalización de la acción α en el mundo w ∈ W, donde α ∈ CAi. El perfil (Ai, PAi Ai) es verdadero cuando el agente Ai tiene un perfil de objeto para el agente Aj. Definición 1. El conflicto total (FulConf) describe una interacción de suma cero donde solo se puede completar un solo objetivo de los objetivos en conflicto. FulConf(G∗ Ai , G∗ Aj ) ⇒ (∃α ∈ CAi , ∀w, β ∈ CAj ) (Achieve(G∗ Ai , α, w) ⇒ ¬Achieve(G∗ Aj , β, w)) ∨ (∃β ∈ CAj , ∀w, α ∈ CAi )(Achieve(G∗ Aj , β, w) ⇒ ¬Achieve(G∗ Ai , α, w)) Definición 2. El Conocimiento Adversarial (AdvKnow) es una función que devuelve un valor que representa la cantidad de The Sixth Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 551 agente de conocimiento Ai tiene sobre el perfil del agente Aj, en el tiempo Tn. Cuanto mayor sea el valor, más conocimiento tiene el agente Ai. AdvKnow: P Aj Ai × Tn → Definición 3. Eval: Esta función de evaluación devuelve un valor de utilidad esperada estimado para un agente en A, después de completar una acción de CA en algún estado del mundo w. Eval: A × CA × w → Definición 4. TrH - (Umbral) es una constante numérica en el rango [0,1] que representa un valor umbral de la función de evaluación (Eval). Una acción que produce una evaluación de utilidad estimada por encima del TrH se considera una acción altamente beneficiosa. El <br>valor Eval</br> es una estimación y no la función de utilidad real, la cual suele ser desconocida. Utilizar el valor real de utilidad para un agente racional fácilmente produciría el mejor resultado para ese agente. Sin embargo, los agentes generalmente no tienen las funciones de utilidad reales, sino más bien una estimación heurística de las mismas. Existen dos propiedades importantes que deben cumplir para la función de evaluación: Propiedad 1. La función de evaluación debería indicar que el estado del mundo más deseable es aquel en el que se logra el objetivo. Por lo tanto, una vez que se ha satisfecho el objetivo, no puede haber ninguna acción futura que pueda colocar al agente en un estado del mundo con un <br>valor de Eval</br> más alto. (∀Ai, G∗ Ai , α, β ∈ CAi , w ∈ W) Lograr(G∗ Ai , α, w) ⇒ Eval(Ai, α, w) ≥ Eval(Ai, β, w) Propiedad 2. La función de evaluación debe proyectar una acción que cause la finalización de un objetivo o subobjetivo a un valor que sea mayor que TrH (una acción altamente beneficiosa). (∀Ai, G∗ Ai ∈ GAi , α ∈ CAi , w ∈ W, g∗ GAi ∈ gGAi ) Lograr(G∗ Ai , α, w) ∨ Lograr(g∗ GAi , α, w) ⇒ Eval(Ai, α, w) ≥ TrH. Definición 5. La acción de conjunto Definimos una acción de conjunto (SetAction) como un conjunto de operaciones de acción (ya sean acciones complejas o básicas) de algunos conjuntos de acción CAi y CAj que, según la creencia del agente Ai, están unidas por una relación temporal y consecuente, formando una cadena de eventos (acción y su acción consecuente siguiente). (∀α1 , . . . , αu ∈ CAi , β1 , . . . , βv ∈ CAj , w ∈ W) SetAction(α1 , . . . , αu , β1 , . . . , βv , w) ⇒ ((Do(Ai, α1 , Tα1 , w) ⇒ Do(Aj, β1 , Tβ1 , w)) ⇒ Do(Ai, α2 , Tα2 , w) ⇒ . . . ⇒ Do(Ai, αu , Tαu , w)) La relación consecuente podría existir debido a varias restricciones ambientales (cuando una acción obliga al adversario a responder con una acción específica) o debido al conocimiento de los agentes sobre el perfil de su adversario. Propiedad 3. A medida que aumenta el conocimiento que tenemos sobre nuestro adversario, tendremos creencias adicionales sobre su comportamiento en diferentes situaciones, lo que a su vez crea un nuevo conjunto de acciones. Formalmente, si nuestro AdvKnow en el tiempo Tn+1 es mayor que AdvKnow en el tiempo Tn, entonces cada SetAction conocido en el tiempo Tn también es conocido en el tiempo Tn+1. AdvKnow(P Aj Ai , Tn+1) > AdvKnow(P Aj Ai , Tn) ⇒ (∀α1 , . . . , αu ∈ CAi , β1 , . . . , βv ∈ CAj ) Bel(Aag, SetAction(α1 , . . . , αu , β1 , . . . , βv ), Tn) ⇒ Bel(Aag, SetAction(α1 , . . . , αu , β1 , . . . , βv ), Tn+1) 2.3 La formulación del entorno Los siguientes axiomas proporcionan la definición formal para un Entorno Adversarial (AE) simple y de suma cero. La satisfacción de estos axiomas significa que el agente está situado en un entorno así. Proporciona especificaciones para que el agente Aag interactúe con su conjunto de adversarios A con respecto a los objetivos G∗ Aag y G∗ A en el tiempo TCo en algún estado del mundo w. AE(Aag, A, G∗ Aag, A1, . . . , Ak, G∗ A1, . . . , G∗ Ak, Tn, w) 1. Aag tiene un Int.Th su objetivo se completaría: (∃α ∈ CAag , Tα) Int.Th(Aag, Lograr(G∗ Aag , α), Tn, Tα, AE) 2. Aag cree que él y cada uno de sus adversarios Ao están persiguiendo metas completamente conflictivas: (∀Ao ∈ {A1, . . . , Ak}) Bel(Aag, FulConf(G∗ Aag , G∗ Ao ), Tn) 3. Aag cree que cada uno de sus adversarios en Ao tiene la Int.Th su objetivo en conflicto G∗ Aoi se completará: (∀Ao ∈ {A1, . . . , Ak})(∃β ∈ CAo , Tβ) Bel(Aag, Int.Th(Ao, Lograr(G∗ Ao , β), TCo, Tβ, AE), Tn) 4. Para construir un agente que pueda operar con éxito dentro de un entorno de adversarios estratégicos, debemos especificar pautas de comportamiento para sus interacciones. Utilizar una estrategia de maximización de Eval ingenua hasta cierta profundidad de búsqueda no siempre dará resultados satisfactorios por varias razones: (1) el problema del horizonte de búsqueda al buscar a una profundidad fija; (2) la fuerte suposición de un adversario óptimamente racional con recursos ilimitados; (3) utilizar una función de evaluación estimada que no dará resultados óptimos en todos los estados del mundo, y que puede ser explotada [9]. Los siguientes axiomas especifican los principios de comportamiento que se pueden utilizar para diferenciar entre agentes exitosos y menos exitosos en el entorno adversarial mencionado anteriormente. Esos axiomas deben ser utilizados como principios de especificación al diseñar e implementar agentes que deben ser capaces de desempeñarse bien en entornos adversarios. Los axiomas conductuales representan situaciones en las que el agente adoptará intenciones potenciales para (Pot.Int.To(...)) realizar una acción, lo que típicamente requerirá un razonamiento de medios-fines para seleccionar un posible curso de acción. Este razonamiento llevará a la adopción de un Int.To(...) (ver [4]). A1. Axioma para lograr metas. El primer axioma es el caso más simple; cuando el agente Aag cree que está a una acción (α) de lograr su objetivo en conflicto G∗ Aag, debería adoptar la intención potencial de hacer α y completar su objetivo. (∀Aag, α ∈ CAag, Tn, Tα, w ∈ W) (Bel(Aag, Do(Aag, α, Tα, w) ⇒ Achieve(G∗ Aag, α, w)) ⇒ Pot.Int.To(Aag, α, Tn, Tα, w) Este comportamiento algo trivial es el primer y más fuerte axioma. En cualquier situación, cuando el agente está a una acción de completar el objetivo, debería completar la acción. Cualquier función de evaluación justa clasificaría naturalmente a α como la acción de valor máximo (propiedad 1). Sin embargo, sin una axiomatización explícita de dicho comportamiento, podría haber situaciones en las que el agente decida tomar otra acción por diversas razones, debido a sus recursos de decisión limitados. A2. Axioma del Acto Preventivo. En una situación adversarial, el agente Aag podría decidir tomar acciones que dañen uno de los planes de su adversario para completar su objetivo, incluso si esas acciones no avanzan explícitamente a Aag hacia su objetivo en conflicto G∗ Aag. Tal acción preventiva tendrá lugar cuando el agente Aag tenga la creencia sobre la posibilidad de que su adversario Ao realice una acción β que le otorgará un alto 552 The Sixth Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) valor de evaluación de utilidad (> TrH). Creer que tomar la acción α evitará que el oponente realice su β, adoptará una intención potencial de hacer α. (∀Aag, Ao ∈ A, α ∈ CAag , β ∈ CAo , Tn, Tβ, w ∈ W) (Bel(Aag, Do(Ao, β, Tβ, w) ∧ Eval(Ao, β, w) > TrH, Tn) ∧ Bel(Aag, Do(Aag, α, Tα, w) ⇒ ¬Do(Ao, β, Tβ, w), Tn) ⇒ Pot.Int.To(Aag, α, Tn, Tα, w) Este axioma es un componente básico de cualquier entorno adversarial. Por ejemplo, al observar un juego de ajedrez, un jugador podría darse cuenta de que está a punto de ser jaque mateado por su oponente, por lo tanto, realiza una jugada preventiva. Otro ejemplo es el juego de Cuatro en Línea: cuando un jugador tiene una fila de tres fichas, su oponente debe bloquearla o perder. Una instancia específica de A1 ocurre cuando el adversario está a una acción de lograr su objetivo, y la acción preventiva inmediata debe ser tomada por el agente. Formalmente, tenemos las mismas creencias que se mencionaron anteriormente, con la creencia modificada de que realizar la acción β hará que el agente Ao logre su objetivo. Proposición 1: Caso de prevención o pérdida. (∀Aag, Ao ∈ A, α ∈ CAag , β ∈ CAo , G∗ Ao , Tn, Tα, Tβ, w ∈ W) Bel(Aag, Do(Ao, β, Tβ, w) ⇒ Achieve(G∗ Ao , β, w), Tn) ∧ Bel(Aag, Do(Aag, α, Tα, w) ⇒ ¬Do(Ao, β, Tβ, w)) ⇒ Pot.Int.To(Aag, α, Tn, Tα, w) Esquema de prueba: La Proposición 1 puede derivarse fácilmente del axioma A1 y de la propiedad 2 de la función Eval, que establece que cualquier acción que cause la finalización de un objetivo es una acción altamente beneficiosa. El comportamiento del acto preventivo ocurrirá implícitamente cuando la función Eval sea igual a la función de utilidad del mundo real. Sin embargo, al ser agentes racionales limitados y al tratar con una función de evaluación estimada, necesitamos axiomatizar explícitamente dicho comportamiento, ya que no siempre ocurrirá implícitamente a partir de la función de evaluación. A3. Axioma de Movimiento Táctico Subóptimo. En muchos escenarios, puede ocurrir una situación en la que un agente decida no tomar la acción más beneficiosa actual que puede tomar (la acción con el valor de evaluación de utilidad máximo), porque cree que tomar otra acción (con un valor de evaluación de utilidad más bajo) podría generar (dependiendo de la respuesta del adversario) una posibilidad futura para una acción altamente beneficiosa. Esto ocurrirá con mayor frecuencia cuando la función Eval sea inexacta y difiera en gran medida de la función de Utilidad. De manera formal, el agente Aag cree en una cierta SetAction que evolucionará de acuerdo con su acción inicial y producirá un valor beneficioso alto (> TrH) únicamente para él. (∀Aag, Ao ∈ A, Tn, w ∈ W) (∃α1 , . . . , αu ∈ CAi , β1 , . . . , βv ∈ CAj , Tα1 ) Bel(Aag, SetAction(α1 , . . . , αu , β1 , . . . , βv ), Tn) ∧ Bel(Aag, Eval(Ao, βv , w) < TrH < Eval(Aag, αu , w), Tn) ⇒ Pot.Int.To(Aag, α1 , Tn, Tα1 , w) Un agente podría creer que una cadena de eventos ocurrirá por diversas razones debido a la naturaleza inevitable del dominio. Por ejemplo, en el ajedrez, a menudo observamos lo siguiente: una jugada provoca una posición de jaque, lo que a su vez limita los movimientos del oponente para evitar el jaque, a lo que el primer jugador podría reaccionar con otro jaque, y así sucesivamente. El agente también podría creer en una cadena de eventos basada en su conocimiento del perfil de su adversario, lo que le permite prever los movimientos del adversario con alta precisión. A4. Axioma de Detección de Perfiles. El agente puede ajustar los perfiles de sus adversarios mediante observaciones y estudio de patrones (específicamente, si hay encuentros repetidos con el mismo adversario). Sin embargo, en lugar de esperar a que se revele la información del perfil, un agente también puede iniciar acciones que obligarán a su adversario a reaccionar de una manera que revele conocimiento del perfil sobre él. Formalmente, el axioma establece que si todas las acciones (γ) no son acciones altamente beneficiosas (< TrH), el agente puede realizar la acción α en el tiempo Tα si cree que resultará en una acción no altamente beneficiosa β por parte de su adversario, lo que a su vez le enseña sobre el perfil del adversario, es decir, proporciona un mayor AdvKnow(P Aj Ai , Tβ). (∀Aag, Ao ∈ A, α ∈ CAag , β ∈ CAo , Tn, Tα, Tβ, w ∈ W) Bel(Aag, (∀γ ∈ CAag )Eval(Aag, γ, w) < TrH, Tn) ∧ Bel(Aag, Do(Aag, α, Tα, w) ⇒ Do(Ao, β, Tβ, w), Tn) ∧ Bel(Aag, Eval(Ao, β, w) < TrH) ∧ Bel(Aag, AdvKnow(P Aj Ai , Tβ) > AdvKnow(P Aj Ai , Tn), Tn) ⇒ Pot.Int.To(Aag, α, Tn, Tα, w) Por ejemplo, volviendo al escenario del juego de ajedrez, consideremos comenzar una partida contra un oponente del que no sabemos nada, ni siquiera si es humano o un oponente computarizado. Podríamos comenzar a jugar una estrategia que sea adecuada frente a un oponente promedio, y ajustar nuestro juego de acuerdo a su nivel de juego. A5. Axioma de Formación de Alianzas El siguiente axioma de comportamiento es relevante únicamente en una instancia multilateral del entorno adversarial (obviamente, una alianza no puede formarse en un encuentro bilateral de suma cero). En diferentes situaciones durante una interacción multilateral, un grupo de agentes podría creer que es de su interés formar una alianza temporal. Una alianza de este tipo es un acuerdo que limita el comportamiento de sus miembros, pero se cree por parte de estos que les permite alcanzar un valor de utilidad mayor al que podrían lograr fuera de la alianza. Como ejemplo, podemos observar el clásico juego de mesa Risk, donde cada jugador tiene como objetivo individual ser el único conquistador del mundo, un juego de suma cero. Sin embargo, para lograr este objetivo, podría ser estratégicamente sabio hacer acuerdos de alto el fuego a corto plazo con otros jugadores, o unirse y atacar a un oponente que sea más fuerte que el resto. Los términos de una alianza definen la forma en que sus miembros deben actuar. Es un conjunto de predicados, denominados Términos, que es acordado por los miembros de la alianza y debe mantenerse verdadero durante la duración de la alianza. Por ejemplo, el conjunto de términos en el escenario de riesgo podría contener los siguientes predicados: 1. Los miembros de la alianza no se atacarán entre sí en los territorios X, Y y Z; 2. Los miembros de la alianza contribuirán C unidades por turno para atacar al adversario Ao; 3. Los miembros están obligados a permanecer como parte de la alianza hasta el tiempo Tk o hasta que el ejército adversario Ao sea más pequeño que Q. El conjunto de términos especifica restricciones intergrupales en cada uno de los miembros de la alianza (∀Aal i ∈ Aal ⊆ A) conjunto de acciones Cal i ⊆ C. Definición 6. El valor total de evaluación que el agente Ai logrará al ser parte de Aal es la suma de Evali (Eval para Ai) de cada uno de los valores de evaluación de Aal j después de tomar sus propias acciones α (a través del predicado del agente(α)): Al val(Ai, Cal, Aal, w) = α∈Cal Evali(Aal j, agente(α), w) Definición 7. Al TrH - es un número que representa un valor de Al en la Sexta Conferencia Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 553 umbral; por encima de él, se puede decir que la alianza es una alianza altamente beneficiosa. El valor de Al TrH se calculará dinámicamente de acuerdo con el progreso de la interacción, como se puede ver en [7]. Después de formarse una alianza, sus miembros ahora están trabajando en su entorno adversarial normal, así como de acuerdo con los estados mentales y axiomas requeridos para sus interacciones como parte de la alianza. El siguiente modelo de Alianza (AL) especifica las condiciones bajo las cuales el grupo Aal puede considerarse en una alianza y trabajando con un nuevo y limitado conjunto de acciones Cal, en el tiempo Tn. AL(Aal , Cal , w, Tn) 1. \n\nAL(Aal , Cal , w, Tn) 1. Aal tiene un MB en el que todos los miembros son parte de Aal: MB(Aal, (∀Aal i ∈ Aal)miembro(Aal i, Aal), Tn) 2. Aal tiene una MB que el grupo se mantenga: MB(Aal , (∀Aal i ∈ Aal )Int.Th (Ai, miembro(Ai, Aal ), Tn, Tn+1, Co), Tn) 3. Aal tiene un beneficio marginal que al ser miembros les otorga un alto valor de utilidad: MB(Aal , (∀Aal i ∈ Aal )Al val(Aal i , Cal , Aal , w) ≥ Al TrH, Tn) Los perfiles de los miembros son una parte crucial de las alianzas exitosas. Suponemos que los agentes que tienen perfiles más precisos de sus adversarios serán más exitosos en tales entornos. Tales agentes podrán predecir cuándo un miembro está a punto de violar el contrato de alianzas (elemento 2 en el modelo anterior) y tomar medidas correctivas (cuando el elemento 3 se falsificará). La robustez de la alianza depende en parte de la medida de confianza de sus miembros, la estimación de su posición objetiva y otras propiedades de su perfil. Debemos tener en cuenta que un agente puede formar parte simultáneamente de más de una alianza. Una alianza temporal, donde los miembros del grupo no tienen un objetivo común pero actúan colaborativamente en interés de sus propias metas individuales, es clasificada como un Grupo de Tratamiento por los psicólogos modernos (en contraste con un Grupo de Tarea, donde sus miembros tienen un objetivo común). El modelo de Actividad Compartida presentado en [5] modeló el comportamiento del Grupo de Tratamiento utilizando la misma formalización de Planes Compartidos. Al comparar ambas definiciones de una alianza y un Grupo de Tratamiento, encontramos una similitud sorprendente entre ambos modelos: las definiciones de los modelos de entorno son casi idénticas (ver las definiciones de SAs en [5]), y sus axiomas de Acto Egoísta y Acto Cooperativo se ajustan al comportamiento de nuestros agentes adversarios. La principal distinción entre ambos modelos es la integración de un axioma de acto de comportamiento útil, en la Actividad Compartida que no puede formar parte de la nuestra. Este axioma establece que un agente considerará tomar una acción que disminuirá su <br>valor de Evaluación</br> (hasta un cierto límite inferior), si cree que un compañero del grupo obtendrá un beneficio significativo. Dicho comportamiento no puede ocurrir en un entorno puramente adversarial (como un juego de suma cero), donde los miembros de la alianza están constantemente alerta para manipular su alianza en su propio beneficio. A6. Axioma de Maximización de la Evaluación. En un caso en el que todos los demás axiomas no sean aplicables, procederemos con la acción que maximice el valor heurístico calculado en la función Eval. (∀Aag, Ao ∈ A, α ∈ Cag, Tn, w ∈ W) Bel(Aag, (∀γ ∈ Cag)Eval(Aag, α, w) ≥ Eval(Aag, γ, w), Tn) ⇒ Pot.Int.To(Aag, α, Tn, Tα, w) T1. Optimalidad en Eval = Utilidad El modelo axiomático anterior aborda situaciones en las que la Utilidad es desconocida y los agentes son agentes racionales limitados. El siguiente teorema muestra que en interacciones bilaterales, donde los agentes tienen la función de Utilidad real (es decir, Eval = Utilidad) y son agentes racionales, los axiomas proporcionan el mismo resultado óptimo que la búsqueda adversarial clásica (por ejemplo, Min-Max). Teorema 1. Que Ae ag sea un agente AE racional ilimitado que utiliza la función de evaluación heurística Eval, Au ag sea el mismo agente utilizando la verdadera función de utilidad y Ao sea un único adversario racional ilimitado basado en utilidad. Dado que Eval = Utilidad: (∀α ∈ CAu ag , α ∈ CAe ag , Tn, w ∈ W) Pot.Int.To(Au ag, α, Tn, Tα, w) → Pot.Int.To(Ae ag, α , Tn, Tα, w) ∧ ((α = α ) ∨ (Utilidad(Au ag, α, w) = Eval(Ae ag, α , w))) Esquema de prueba - Dado que Au ag tiene la función de utilidad real y recursos ilimitados, puede generar el árbol de juego completo y ejecutar el algoritmo MinMax óptimo para elegir la acción de mayor valor de utilidad, que denotamos como α. La prueba mostrará que Ae ag, utilizando los axiomas AE, seleccionará la misma utilidad α o igual (cuando hay más de una acción con la misma utilidad máxima) cuando Eval = Utilidad. (A1) Axioma de logro de objetivo - supongamos que hay un α tal que su cumplimiento logrará el objetivo Au ags. Obtendrá la utilidad más alta mediante Min-Max para Au ag. El agente Ae seleccionará α u otra acción con el mismo valor de utilidad a través de A1. Si tal α no existe, Ae ag no puede aplicar este axioma y procede a A2. (A2) Axioma de la acción preventiva - (1) Observando el caso base (ver Prop1), si hay un β que lleva a Ao a alcanzar su objetivo, entonces una acción preventiva α producirá la mayor utilidad para Au ag. Au ag lo elegirá a través de la utilidad, mientras que Ae ag lo elegirá a través de A2. En el caso general, β es una acción altamente beneficiosa para Ao, lo que resulta en una baja utilidad para Au ag, lo que lo guiará a seleccionar un α que evitará β, mientras que Ae ag lo elegirá a través de A2.1 Si dicho β no existe para Ao, entonces A2 no es aplicable, y Ae ag puede proceder a A3. (A3) Axioma de movimiento táctico subóptimo: al usar una función de evaluación heurística, Ae ag tiene una creencia parcial en el perfil de su adversario (elemento 4 en el modelo AE), lo que puede llevarlo a creer en SetActions (Prop1). En nuestro caso, Ae ag tiene un perfil completo de su adversario óptimo y sabe que Ao se comportará de manera óptima según los valores reales de utilidad en el árbol de búsqueda completo, por lo tanto, cualquier creencia sobre una SetAction subóptima no puede existir, lo que hace que este axioma sea inaplicable. Ae ag procederá a A4. (A4) Axioma de detección de perfil - Dado que Ae ag tiene el perfil completo de Ao, ninguna de las acciones de Ae ag puede aumentar su conocimiento. Ese axioma no se aplicará, y el agente procederá con A6 (A5 será ignorado porque la interacción es bilateral). (A6) Axioma de maximización de la evaluación: este axioma seleccionará la máxima Evaluación para Ae ag. Dado que Eval = Utilidad, se seleccionará el mismo α que fue seleccionado por Au ag. 3. EVALUACIÓN El propósito principal de nuestro análisis experimental es evaluar el comportamiento y rendimiento de los modelos en un entorno adversarial real. Esta sección investiga si un caso acotado 1 en el que, tras la finalización de β, exista un γ que proporcione una alta utilidad para el Agente Au ag, no puede ocurrir porque Ao utiliza la misma utilidad, y la existencia de γ hará que clasifique a β como una acción de baja utilidad. 554 The Sixth Intl. En la Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07), los agentes racionales situados en entornos adversariales se beneficiarán más al aplicar nuestros axiomas de comportamiento sugeridos. 3.1 El Dominio Para explorar el uso del modelo anterior y sus axiomas de comportamiento, decidimos utilizar el juego de Conecta Cuatro como nuestro entorno adversarial. Connect-Four es un juego de 2 jugadores, de suma cero, que se juega utilizando un tablero similar a una matriz de 6x7. En cada turno, un jugador coloca un disco en una de las 7 columnas (el conjunto de 21 discos suele ser de color amarillo para el jugador 1 y rojo para el jugador 2; usaremos Blanco y Negro respectivamente para evitar confusiones). El ganador es el primer jugador en completar un conjunto de cuatro discos de su color en horizontal, vertical o diagonal. En ocasiones muy raras, el juego podría terminar en empate si todos los cuadros vacíos están llenos, pero ningún jugador logró crear un conjunto de 4 discos. El juego de Conecta Cuatro fue resuelto en [1], donde se muestra que el primer jugador (jugando con los discos blancos) puede forzar una victoria comenzando en la columna del medio (columna 4) y jugando de manera óptima. Sin embargo, la estrategia óptima es muy compleja y difícil de seguir incluso para agentes racionales limitados complejos, como los jugadores humanos. Antes de que podamos proceder a verificar el comportamiento del agente, primero debemos verificar que el dominio se ajusta a la definición de entornos adversarios mencionada anteriormente (en la que se basan los axiomas de comportamiento). Primero, al jugar al juego de Conecta Cuatro, el agente tiene la intención de ganar el juego (ítem 1). Segundo (ítem 2), nuestro agente cree que en Conecta Cuatro solo puede haber un ganador (o ningún ganador en la rara ocasión de un empate). Además, nuestro agente cree que su oponente en el juego intentará ganar (ítem 3), y esperamos que tenga algún conocimiento parcial (ítem 4) sobre su adversario (este conocimiento puede variar desde nada, pasando por hechos simples como la edad, hasta estrategias y debilidades). Por supuesto, no todos los encuentros de Conecta Cuatro son adversariales. Por ejemplo, cuando un padre está jugando el juego con su hijo, la siguiente situación podría ocurrir: el niño, con un fuerte incentivo para ganar, trata el entorno como adversarial (intenta ganar, entiende que solo puede haber un ganador y cree que su padre está tratando de vencerlo). Sin embargo, desde el punto de vista de los padres, el entorno podría ser visto como educativo, donde el objetivo no es ganar el juego, sino causar disfrute o practicar el razonamiento estratégico. En un entorno educativo como este, un nuevo conjunto de axiomas de comportamiento podría ser más beneficioso para los objetivos de los padres que nuestros axiomas de comportamiento adversarial sugeridos. 3.2 Análisis de Axiomas Después de demostrar que el juego de Conecta Cuatro es de hecho un entorno adversarial bilateral de suma cero, el siguiente paso es observar el comportamiento de los jugadores durante el juego y verificar si comportarse de acuerdo con nuestro modelo mejora el rendimiento. Para hacerlo, hemos recopilado archivos de registro de juegos de Conecta Cuatro completados que fueron jugados por jugadores humanos a través de Internet. Nuestros datos de archivo de registro recopilados provienen de sitios de Juego por Correo Electrónico (PBeM). Estos son sitios web que alojan juegos por correo electrónico, donde cada movimiento se realiza a través de un intercambio de correos electrónicos entre el servidor y los jugadores. Muchos de estos sitios de archivos contienen interacciones competitivas reales y también mantienen un sistema de clasificación para sus miembros. La mayoría de los datos que utilizamos se pueden encontrar en [6]. Como se puede aprender de [1], Conecta Cuatro tiene una estrategia óptima y una ventaja considerable para el jugador que comienza el juego (a quien llamamos jugador Blanco). Nos concentraremos en nuestro análisis en los movimientos de los segundos jugadores (a los que llamaremos Negras). El jugador Blanco, al ser el primero en actuar, tiene la llamada ventaja de la iniciativa. Tener la ventaja y una buena estrategia mantendrá al jugador Negro ocupado reaccionando a sus movimientos, en lugar de iniciar amenazas. Una amenaza es una combinación de tres discos del mismo color, con un espacio vacío para el cuarto disco ganador. Una amenaza abierta es una amenaza que puede materializarse en el próximo movimiento del oponente. Para que el jugador Negro gane, debe de alguna manera cambiar el rumbo, tomar la ventaja y comenzar a presentar amenazas al jugador Blanco. Exploraremos el comportamiento de los jugadores negros y su conformidad con nuestros axiomas. Para hacerlo, construimos una aplicación que lee archivos de registro y analiza los movimientos de los jugadores Negros. La aplicación contiene dos componentes principales: (1) un algoritmo Min-Max para la evaluación de movimientos; (2) un detector de amenazas abiertas para descubrir amenazas abiertas. El algoritmo Min-Max funcionará hasta una profundidad d dada y, para cada movimiento α, devolverá el valor heurístico para la próxima acción tomada por el jugador según lo escrito en el archivo de registro, h(α), junto con el valor heurístico máximo, maxh(α), que podría lograrse antes de realizar el movimiento (obviamente, si h(α) = maxh(α), entonces el jugador no realizó el movimiento óptimo desde el punto de vista heurístico). El trabajo de los detectores de amenazas es notificar si se tomó alguna acción para bloquear una amenaza abierta (no bloquear una amenaza abierta probablemente hará que el jugador pierda en el próximo movimiento del oponente). La función heurística utilizada por Min-Max para evaluar la utilidad de los jugadores es la siguiente función, que es simple de calcular, pero proporciona un desafío razonable a los oponentes humanos: Definición 8. Sea Grupo un conjunto adyacente de cuatro cuadrados que son horizontales, verticales o diagonales. El Grupo n (Grupo w) es un Grupo con n piezas de color negro (blanco) y 4−n casillas vacías. h = ((Grupo1 negro ∗α)+(Grupo2 negro ∗β)+(Grupo3 negro ∗γ)+(Grupo4 negro ∗∞)) − ((Grupo1 blanco ∗α)+(Grupo2 blanco ∗β)+(Grupo3 blanco ∗γ)+(Grupo4 blanco ∗∞)) Los valores de α, β y δ pueden variar para formar cualquier combinación lineal deseada; sin embargo, es importante asignarles valores teniendo en cuenta el orden α < β < δ (usamos 1, 4 y 8 como sus valores respectivos). Grupos de 4 discos del mismo color significan victoria, por lo tanto, el descubrimiento de dicho grupo resultará en ∞ para asegurar un valor extremo. Ahora utilizamos nuestra función de evaluación estimada para evaluar las acciones de los jugadores Negros durante la interacción adversarial de Conecta Cuatro. Cada juego del archivo de registro fue introducido en la aplicación, la cual procesó y generó un archivo de registro reformateado que contenía el valor h del movimiento actual, el valor maxh que podría ser alcanzado y una notificación si se detectaba una amenaza abierta. Un total de 123 partidas fueron analizadas (57 con victoria de las blancas y 66 con victoria de las negras). Algunos juegos adicionales fueron ignorados manualmente en el experimento, debido a estos problemas: un jugador abandonando el juego mientras el resultado no es definitivo, o una jugada irracional y contundente en las primeras etapas del juego (por ejemplo, no bloquear un grupo obviamente ganador en las primeras jugadas de apertura). Además, también se eliminó un empate. El simulador se ejecutó con una profundidad de búsqueda de 3 movimientos. Ahora procedemos a analizar los juegos con respecto a cada axioma de comportamiento. El Sexto Internacional. La siguiente sección presenta las evaluaciones heurísticas del algoritmo Min-Max para cada acción, y verifica la cantidad y el alcance de las acciones tácticas subóptimas y sus implicaciones en el rendimiento. La Tabla 1 muestra los resultados y las percepciones del análisis heurístico de los juegos, cuando la profundidad de búsqueda es igual a 3 (esta profundidad de búsqueda fue seleccionada para que los resultados sean comparables a [9], ver Sección 3.2.3). La información heurística de las tablas es la diferencia entre el valor heurístico máximo presente y el valor heurístico de la acción que finalmente tomó el jugador (es decir, cuanto más cercano sea el número a 0, más cercana estuvo la acción a la acción heurística máxima). La primera fila presenta los valores de diferencia de la acción que tuvo el valor de diferencia máximo entre todas las acciones de los jugadores Negros en un juego dado, promediado en todos los juegos ganados y perdidos por los Negros (ver columnas respectivas). En los juegos en los que el jugador Negro pierde, su valor promedio de diferencia fue de -17.62, mientras que en los juegos en los que el jugador Negro ganó, su promedio fue de -12.02. La segunda fila amplía el análisis al considerar las 3 acciones con mayor diferencia heurística, y promediarlas. En ese caso, notamos una diferencia heurística promedio de 5 puntos entre los juegos en los que el jugador Negro pierde y los juegos en los que gana. Sin embargo, la importancia de esos números es que nos permitieron hacer una suposición educada sobre un número umbral de 11.5, como el valor de la constante TrH, que diferencia entre acciones normales y altamente beneficiosas. Después de encontrar una constante TrH aproximada, podemos proceder con un análisis de la importancia de los movimientos subóptimos. Para hacerlo, tomamos el subconjunto de juegos en los que el valor mínimo de diferencia heurística para las acciones de las negras era de 11.5. Como se presenta en la Tabla 2, podemos ver el promedio de 3 horas mínimo de los 3 mayores rangos y el porcentaje respectivo de juegos ganados. La primera fila muestra que el jugador Negro ganó solo el 12% de los juegos en los que el promedio de sus 3 acciones con mayor diferencia heurística (min3 h) era menor que el umbral sugerido, TrH = 11.5. La segunda fila muestra un resultado sorprendente: parece que cuando min3 h > −4, el jugador Negro rara vez gana. La intuición sugeriría que los juegos en los que los valores de evaluación de la acción estén más cerca de los valores máximos resultarán en más juegos ganados para las piezas negras. Sin embargo, parece que en el dominio de Conecta Cuatro, simplemente responder con acciones algo fácilmente esperadas, sin iniciar algunos movimientos sorprendentes y subóptimos, no produce buenos resultados. La última fila resume las principales conclusiones del análisis; la mayoría de las victorias de Blacks (83%) se produjeron cuando su min3 h estaba en el rango de -11.5 a -4. Una inspección detallada de esos juegos ganados por las fichas negras muestra el siguiente patrón detrás de los números: después de los movimientos de apertura estándar, las fichas negras de repente colocan una ficha en una columna aislada, lo cual parece ser un desperdicio de un movimiento. El blanco sigue construyendo sus amenazas, generalmente ignorando el último movimiento de las negras, que a su vez utiliza el disco aislado como ancla para una futura amenaza ganadora. Los resultados muestran que fue beneficioso para el jugador Negro en la Tabla 2: Porcentajes de ganancias de los Negros % de juegos min3 h < −11.5 12% min3 h > −4 5% −11.5 ≤ min3 h ≤ −4 83% tomar acciones subóptimas y no dar el valor heurístico más alto posible en ese momento, pero no será demasiado perjudicial para su posición (es decir, no dará un alto valor beneficioso a su adversario). Resultó que aprender el umbral es un aspecto importante del éxito: realizar movimientos extremadamente arriesgados (min3 h < −11.5) o intentar evitarlos (min3 h > −4) reduce significativamente las posibilidades de ganar de los jugadores negros. 3.2.2 Afirmación del Axioma de Monitoreo de Perfiles En la tarea de demostrar la importancia de monitorear los perfiles de los adversarios, nuestros archivos de registro no pudieron ser utilizados porque no contenían interacciones repetidas entre jugadores, que son necesarias para inferir el conocimiento de los jugadores sobre sus adversarios. Sin embargo, la importancia del modelado del oponente y su uso para obtener ventajas tácticas ya ha sido estudiada en varios ámbitos ([3, 9] son buenos ejemplos). En un artículo reciente, Markovitch y Reger [9] exploraron la noción de aprendizaje y explotación de la debilidad del oponente en interacciones competitivas. Aplican estrategias de aprendizaje simples analizando ejemplos de interacciones pasadas en un dominio específico. También utilizaron el dominio adversarial de Conecta Cuatro, que ahora se puede utilizar para comprender la importancia de monitorear el perfil del adversario. Tras la presentación de su modelo teórico, describen un extenso estudio empírico y verifican el rendimiento de los agentes después de aprender el modelo de debilidades con ejemplos pasados. Uno de los dominios utilizados como entorno competitivo fue el mismo juego de Conecta Cuatro (las Damas fueron el segundo dominio). Su función heurística era idéntica a la nuestra con tres variaciones diferentes (H1, H2 y H3) que se distinguen entre sí por los valores de sus coeficientes de combinación lineal. La profundidad de búsqueda para los jugadores fue de 3 (como en nuestro análisis). Sus extensos experimentos verifican y comparan diversas estrategias de aprendizaje, factores de riesgo, conjuntos de características predefinidas y métodos de uso. La conclusión es que el dominio de Conecta Cuatro muestra una mejora de una tasa de victoria de 0.556 antes de modelar a 0.69 después de modelar (página 22). Sus conclusiones, que muestran un rendimiento mejorado al sostener y utilizar el modelo del adversario, justifican el esfuerzo de monitorear el perfil del adversario para interacciones continuas y repetidas. Un punto adicional que surgió en sus experimentos es el siguiente: después de que se haya aprendido el modelo de debilidad del oponente, los autores describen diferentes métodos para integrar el modelo de debilidad del oponente en la estrategia de decisión de los agentes. Sin embargo, independientemente del método específico que elijan para trabajar, todos los métodos de integración podrían hacer que el agente tome decisiones subóptimas; podría hacer que el agente prefiera acciones que son subóptimas en el momento actual de la decisión, pero que podrían hacer que el oponente reaccione de acuerdo con su modelo de debilidad (como lo representa nuestro agente), lo cual a su vez será beneficioso para nosotros en el futuro. El comportamiento de los agentes, como se demuestra en [9], confirma y refuerza aún más nuestro Axioma Táctico Subóptimo discutido en la sección anterior. 556 The Sixth Intl. La necesidad de los axiomas de Logro de Objetivos, Acto Preventivo y Maximización de la Evaluación es evidente y no requiere más verificación. Sin embargo, incluso con respecto a esos axiomas, surgieron algunas ideas interesantes en el análisis de registros. Los axiomas de Logro y Prevención de Actos, aunque teóricamente triviales, parecen presentar un desafío para un jugador humano. En la inspección inicial de los registros, nos encontramos con algunos juegos donde un jugador, por razones inexplicables, no bloqueó al otro para que ganara o no logró ejecutar su propia jugada ganadora. Podemos atribuir esas fallas a la falta de atención de los humanos, o a un error de escritura en su respuesta al movimiento; sin embargo, esos errores podrían ocurrir en agentes racionales limitados, y el comportamiento apropiado debe ser axiomatizado. Un juego típico de Cuatro en Línea gira en torno a generar amenazas y bloquearlas. En nuestro análisis buscamos acciones preventivas explícitas, es decir, movimientos que bloqueen un grupo de 3 discos o que eliminen una amenaza futura (en nuestro horizonte de búsqueda limitado). Encontramos que en el 83% de los juegos totales, hubo al menos una acción preventiva tomada por el jugador Negro. También se encontró que Black promedió 2.8 acciones preventivas por juego en los juegos en los que perdió, mientras que promedió 1.5 acciones preventivas por juego al ganar. Parece que Black necesita 1 o 2 acciones preventivas para construir su posición inicial de toma, antes de comenzar a presentar amenazas. Si no logra ganar, generalmente evitará una amenaza adicional o dos antes de sucumbir ante el Blanco. 4. TRABAJO RELACIONADO Mucha investigación se ocupa de la axiomatización del trabajo en equipo y los estados mentales de los individuos: algunos modelos utilizan el conocimiento y la creencia [10], otros tienen modelos de metas e intenciones [8, 4]. Sin embargo, todas estas teorías formales tratan sobre el trabajo en equipo y la cooperación de agentes. Hasta donde sabemos, nuestro modelo es el primero en proporcionar un modelo formalizado para entornos adversarios explícitos y el comportamiento de los agentes en él. El algoritmo de búsqueda adversarial clásico Min-Max fue el primer intento de integrar al oponente en el espacio de búsqueda con la suposición débil de un oponente que juega de manera óptima. Desde entonces, se ha dedicado mucho esfuerzo a integrar el modelo del oponente en el procedimiento de toma de decisiones para predecir el comportamiento futuro. El algoritmo M∗ presentado por Carmel y Markovitch [2] mostró un método para incorporar modelos de oponentes en la búsqueda de adversarios, mientras que en [3] utilizaron el aprendizaje para proporcionar un modelo de oponente más preciso en un entorno de juego repetido de 2 jugadores, donde las estrategias de los agentes se modelaron como autómatas finitos. El trabajo adicional de planificación adversarial fue realizado por Willmott y colaboradores [13], quienes proporcionaron un enfoque de planificación adversarial para el juego de GO. La investigación mencionada anteriormente trató sobre la búsqueda adversarial y la integración de modelos de oponentes en métodos de búsqueda de utilidad clásicos. Ese trabajo muestra la importancia del modelado del oponente y la capacidad de explotarlo en beneficio de un agente. Sin embargo, las limitaciones básicas de esos métodos de búsqueda siguen aplicándose; nuestro modelo intenta superar esas limitaciones presentando un modelo formal para una nueva especificación adversarial basada en estados mentales. 5. CONCLUSIONES Presentamos un modelo de Entorno Adversarial para un agente racional acotado que está situado en un entorno de N jugadores, de suma cero. Estos fueron posteriormente eliminados del análisis final. Utilizamos la formalización de SharedPlans para definir el modelo y los axiomas que los agentes pueden aplicar como pautas de comportamiento. El modelo está destinado a ser utilizado como una guía para diseñar agentes que necesitan operar en entornos adversos. Presentamos resultados empíricos, basados en el análisis del archivo de registro de ConnectFour, que ejemplifican el modelo y los axiomas para una instancia bilateral del entorno. Los resultados que presentamos son un primer paso hacia un modelo ampliado que abarcará todos los tipos de entornos adversarios, por ejemplo, entornos que no son de suma cero y entornos que contienen agentes naturales que no forman parte del conflicto directo. Esos desafíos y más serán abordados en futuras investigaciones. 6. AGRADECIMIENTO Esta investigación fue apoyada en parte por las becas de la Fundación de Ciencia de Israel #1211/04 y #898/05. 7. REFERENCIAS [1] L. V. Allis. Un enfoque basado en el conocimiento de Conecta Cuatro: el juego está resuelto, gana el jugador Blanco. Tesis de maestría, Universidad Libre de Ámsterdam, Países Bajos, 1988. [2] D. Carmel y S. Markovitch. Incorporando modelos de oponentes en la búsqueda de adversarios. En Actas de la Decimotercera Conferencia Nacional de Inteligencia Artificial, páginas 120-125, Portland, Oregón, 1996. [3] D. Carmel y S. Markovitch. Modelado de oponentes en sistemas multiagente. En G. Weiß y S. Sen, editores, Adaptación y Aprendizaje en Sistemas Multiagente, páginas 40-52. Springer-Verlag, 1996. [4] B. J. Grosz y S. Kraus. Planes colaborativos para acciones grupales complejas. Inteligencia Artificial, 86(2):269-357, 1996. [5] M. Hadad, G. Kaminka, G. Armon y S. Kraus. Apoyando la actividad colaborativa. En Proc. de AAAI-2005, páginas 83-88, Pittsburgh, 2005. [6] http://www.gamerz.net/˜pbmserv/. [7] S. Kraus y D. Lehmann. Diseñando y construyendo un agente automatizado de negociación. Inteligencia Computacional, 11:132-171, 1995. [8] H. J. Levesque, P. R. Cohen y J. H. T. Nunes. Actuar juntos. En Proc. de AAAI-90, páginas 94-99, Boston, MA, 1990. [9] S. Markovitch y R. Reger. Aprender y aprovechar las debilidades relativas de los agentes oponentes. Agentes Autónomos y Sistemas Multiagente, 10(2):103-130, 2005. [10] Y. M. Ronald Fagin, Joseph Y. Halpern y M. Y. Vardi. Razonamiento sobre el conocimiento. MIT Press, Cambridge, Mass., 1995. [11] P. Thagard.\nMIT Press, Cambridge, Massachusetts, 1995. [11] P. Thagard. Resolución de problemas adversariales: Modelado de un oponente utilizando coherencia explicativa. Ciencia Cognitiva, 16(1):123-149, 1992. [12] R. W. Toseland y R. F. Rivas. Una introducción a la práctica del trabajo en grupo. Prentice Hall, Englewood Cliffs, NJ, segunda edición, 1995. [13] S. Willmott, J. Richardson, A. Bundy y J. Levine. Un enfoque de planificación adversarial para el juego de Go. Notas de conferencia en Ciencias de la Computación, 1558:93-112, 1999. El Sexto Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 557 ",
            "candidates": [],
            "error": [
                [
                    "valor Eval",
                    "valor de Eval",
                    "valor de Evaluación"
                ]
            ]
        },
        "interaction": {
            "translated_key": "interacción",
            "is_in_text": true,
            "original_annotated_sentences": [
                "An Adversarial Environment Model for Bounded Rational Agents in Zero-Sum Interactions Inon Zuckerman1 , Sarit Kraus1 , Jeffrey S. Rosenschein2 , Gal Kaminka1 1 Department of Computer Science 2 The School of Engineering Bar-Ilan University and Computer Science Ramat-Gan, Israel Hebrew University, Jerusalem, Israel {zukermi,sarit,galk}@cs.biu.ac.il jeff@cs.huji.ac.il ABSTRACT Multiagent environments are often not cooperative nor collaborative; in many cases, agents have conflicting interests, leading to adversarial interactions.",
                "This paper presents a formal Adversarial Environment model for bounded rational agents operating in a zero-sum environment.",
                "In such environments, attempts to use classical utility-based search methods can raise a variety of difficulties (e.g., implicitly modeling the opponent as an omniscient utility maximizer, rather than leveraging a more nuanced, explicit opponent model).",
                "We define an Adversarial Environment by describing the mental states of an agent in such an environment.",
                "We then present behavioral axioms that are intended to serve as design principles for building such adversarial agents.",
                "We explore the application of our approach by analyzing log files of completed Connect-Four games, and present an empirical analysis of the axioms appropriateness.",
                "Categories and Subject Descriptors I.2.11 [Artificial Intelligence]: Distributed Artificial Intelligence-Intelligent agents,Multiagent Systems; I.2.4 [Artificial Intelligence]: Knowledge Representation Formalisms and Methods -Modal logic General Terms Design, Theory 1.",
                "INTRODUCTION Early research in multiagent systems (MAS) considered cooperative groups of agents; because individual agents had limited resources, or limited access to information (e.g., limited processing power, limited sensor coverage), they worked together by design to solve problems that individually they could not solve, or at least could not solve as efficiently.",
                "MAS research, however, soon began to consider interacting agents with individuated interests, as representatives of different humans or organizations with non-identical interests.",
                "When interactions are guided by diverse interests, participants may have to overcome disagreements, uncooperative interactions, and even intentional attempts to damage one another.",
                "When these types of interactions occur, environments require appropriate behavior from the agents situated in them.",
                "We call these environments Adversarial Environments, and call the clashing agents Adversaries.",
                "Models of cooperation and teamwork have been extensively explored in MAS through the axiomatization of mental states (e.g., [8, 4, 5]).",
                "However, none of this research dealt with adversarial domains and their implications for agent behavior.",
                "Our paper addresses this issue by providing a formal, axiomatized mental state model for a subset of adversarial domains, namely simple zero-sum adversarial environments.",
                "Simple zero-sum encounters exist of course in various twoplayer games (e.g., Chess, Checkers), but they also exist in n-player games (e.g., Risk, Diplomacy), auctions for a single good, and elsewhere.",
                "In these latter environments especially, using a utility-based adversarial search (such as the Min-Max algorithm) does not always provide an adequate solution; the payoff function might be quite complex or difficult to quantify, and there are natural computational limitations on bounded rational agents.",
                "In addition, traditional search methods (like Min-Max) do not make use of a model of the opponent, which has proven to be a valuable addition to adversarial planning [9, 3, 11].",
                "In this paper, we develop a formal, axiomatized model for bounded rational agents that are situated in a zero-sum adversarial environment.",
                "The model uses different modality operators, and its main foundations are the SharedPlans [4] model for collaborative behavior.",
                "We explore environment properties and the mental states of agents to derive behavioral axioms; these behavioral axioms constitute a formal model that serves as a specification and design guideline for agent design in such settings.",
                "We then investigate the behavior of our model empirically using the Connect-Four board game.",
                "We show that this game conforms to our environment definition, and analyze players behavior using a large set of completed match log 550 978-81-904262-7-5 (RPS) c 2007 IFAAMAS files.",
                "In addition, we use the results presented in [9] to discuss the importance of opponent modeling in our ConnectFour adversarial domain.",
                "The paper proceeds as follows.",
                "Section 2 presents the models formalization.",
                "Section 3 presents the empirical analysis and its results.",
                "We discuss related work in Section 4, and conclude and present future directions in Section 5. 2.",
                "ADVERSARIAL ENVIRONMENTS The adversarial environment model (denoted as AE) is intended to guide the design of agents by providing a specification of the capabilities and mental attitudes of an agent in an adversarial environment.",
                "We focus here on specific types of adversarial environments, specified as follows: 1.",
                "Zero-Sum Interactions: positive and negative utilities of all agents sum to zero; 2.",
                "Simple AEs: all agents in the environment are adversarial agents; 3.",
                "Bilateral AEs: AEs with exactly two agents; 4.",
                "Multilateral AEs: AEs of three or more agents.",
                "We will work on both bilateral and multilateral instantiations of zero-sum and simple environments.",
                "In particular, our adversarial environment model will deal with interactions that consist of N agents (N ≥ 2), where all agents are adversaries, and only one agent can succeed.",
                "Examples of such environments range from board games (e.g., Chess, Connect-Four, and Diplomacy) to certain economic environments (e.g., N-bidder auctions over a single good). 2.1 Model Overview Our approach is to formalize the mental attitudes and behaviors of a single adversarial agent; we consider how a single agent perceives the AE.",
                "The following list specifies the conditions and mental states of an agent in a simple, zero-sum AE: 1.",
                "The agent has an individual intention that its own goal will be completed; 2.",
                "The agent has an individual belief that it and its adversaries are pursuing full conflicting goals (defined below)there can be only one winner; 3.",
                "The agent has an individual belief that each adversary has an intention to complete its own full conflicting goal; 4.",
                "The agent has an individual belief in the (partial) profile of its adversaries.",
                "Item 3 is required, since it might be the case that some agent has a full conflicting goal, and is currently considering adopting the intention to complete it, but is, as of yet, not committed to achieving it.",
                "This might occur because the agent has not yet deliberated about the effects that adopting that intention might have on the other intentions it is currently holding.",
                "In such cases, it might not consider itself to even be in an adversarial environment.",
                "Item 4 states that the agent should hold some belief about the profiles of its adversaries.",
                "The profile represents all the knowledge the agent has about its adversary: its weaknesses, strategic capabilities, goals, intentions, trustworthiness, and more.",
                "It can be given explicitly or can be learned from observations of past encounters. 2.2 Model Definitions for Mental States We use Grosz and Krauss definitions of the modal operators, predicates, and meta-predicates, as defined in their SharedPlan formalization [4].",
                "We recall here some of the predicates and operators that are used in that formalization: Int.To(Ai, α, Tn, Tα, C) represents Ais intentions at time Tn to do an action α at time Tα in the context of C. Int.Th(Ai, prop, Tn, Tprop, C) represents Ais intentions at time Tn that a certain proposition prop holds at time Tprop in the context of C. The potential intention operators, Pot.Int.To(...) and Pot.Int.Th(...), are used to represent the mental state when an agent considers adopting an intention, but has not deliberated about the <br>interaction</br> of the other intentions it holds.",
                "The operator Bel(Ai, f, Tf ) represents agent Ai believing in the statement expressed in formula f, at time Tf .",
                "MB(A, f, Tf ) represents mutual belief for a group of agents A.",
                "A snapshot of the system finds our environment to be in some state e ∈ E of environmental variable states, and each adversary in any LAi ∈ L of possible local states.",
                "At any given time step, the system will be in some world w of the set of all possible worlds w ∈ W, where w = E×LA1 ×LA2 × ...LAn , and n is the number of adversaries.",
                "For example, in a Texas Holdem poker game, an agents local state might be its own set of cards (which is unknown to its adversary) while the environment will consist of the betting pot and the community cards (which are visible to both players).",
                "A utility function under this formalization is defined as a mapping from a possible world w ∈ W to an element in , which expresses the desirability of the world, from a single agent perspective.",
                "We usually normalize the range to [0,1], where 0 represents the least desirable possible world, and 1 is the most desirable world.",
                "The implementation of the utility function is dependent on the domain in question.",
                "The following list specifies new predicates, functions, variables, and constants used in conjunction with the original definitions for the adversarial environment formalization: 1. φ is a null action (the agent does not do anything). 2.",
                "GAi is the set of agent Ais goals.",
                "Each goal is a set of predicates whose satisfaction makes the goal complete (we use G∗ Ai ∈ GAi to represent an arbitrary goal of agent Ai). 3. gAi is the set of agent Ais subgoals.",
                "Subgoals are predicates whose satisfaction represents an important milestone toward achievement of the full goal. gG∗ Ai ⊆ gAi is the set of subgoals that are important to the completion of goal G∗ Ai (we will use g∗ G∗ Ai ∈ gG∗ Ai to represent an arbitrary subgoal). 4.",
                "P Aj Ai is the profile object agent Ai holds about agent Aj. 5.",
                "CA is a general set of actions for all agents in A which are derived from the environments constraints.",
                "CAi ⊆ CA is the set of agent Ais possible actions. 6.",
                "Do(Ai, α, Tα, w) holds when Ai performs action α over time interval Tα in world w. 7.",
                "Achieve(G∗ Ai , α, w) is true when goal G∗ Ai is achieved following the completion of action α in world w ∈ W, where α ∈ CAi . 8.",
                "Profile(Ai, PAi Ai ) is true when agent Ai holds an object profile for agent Aj.",
                "Definition 1.",
                "Full conflict (FulConf ) describes a zerosum <br>interaction</br> where only a single goal of the goals in conflict can be completed.",
                "FulConf(G∗ Ai , G∗ Aj ) ⇒ (∃α ∈ CAi , ∀w, β ∈ CAj ) (Achieve(G∗ Ai , α, w) ⇒ ¬Achieve(G∗ Aj , β, w)) ∨ (∃β ∈ CAj , ∀w, α ∈ CAi )(Achieve(G∗ Aj , β, w) ⇒ ¬Achieve(G∗ Ai , α, w)) Definition 2.",
                "Adversarial Knowledge (AdvKnow) is a function returning a value which represents the amount of The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 551 knowledge agent Ai has on the profile of agent Aj, at time Tn.",
                "The higher the value, the more knowledge agent Ai has.",
                "AdvKnow : P Aj Ai × Tn → Definition 3.",
                "Eval - This evaluation function returns an estimated expected utility value for an agent in A, after completing an action from CA in some world state w. Eval : A × CA × w → Definition 4.",
                "TrH - (Threshold) is a numerical constant in the [0,1] range that represents an evaluation function (Eval) threshold value.",
                "An action that yields an estimated utility evaluation above the TrH is regarded as a highly beneficial action.",
                "The Eval value is an estimation and not the real utility function, which is usually unknown.",
                "Using the real utility value for a rational agent would easily yield the best outcome for that agent.",
                "However, agents usually do not have the real utility functions, but rather a heuristic estimate of it.",
                "There are two important properties that should hold for the evaluation function: Property 1.",
                "The evaluation function should state that the most desirable world state is one in which the goal is achieved.",
                "Therefore, after the goal has been satisfied, there can be no future action that can put the agent in a world state with higher Eval value. (∀Ai, G∗ Ai , α, β ∈ CAi , w ∈ W) Achieve(G∗ Ai , α, w) ⇒ Eval(Ai, α, w) ≥ Eval(Ai, β, w) Property 2.",
                "The evaluation function should project an action that causes a completion of a goal or a subgoal to a value which is greater than TrH (a highly beneficial action). (∀Ai, G∗ Ai ∈ GAi , α ∈ CAi , w ∈ W, g∗ GAi ∈ gGAi ) Achieve(G∗ Ai , α, w) ∨ Achieve(g∗ GAi , α, w) ⇒ Eval(Ai, α, w) ≥ TrH.",
                "Definition 5.",
                "SetAction We define a set action (SetAction) as a set of action operations (either complex or basic actions) from some action sets CAi and CAj which, according to agent Ais belief, are attached together by a temporal and consequential relationship, forming a chain of events (action, and its following consequent action). (∀α1 , . . . , αu ∈ CAi , β1 , . . . , βv ∈ CAj , w ∈ W) SetAction(α1 , . . . , αu , β1 , . . . , βv , w) ⇒ ((Do(Ai, α1 , Tα1 , w) ⇒ Do(Aj, β1 , Tβ1 , w)) ⇒ Do(Ai, α2 , Tα2 , w) ⇒ . . . ⇒ Do(Ai, αu , Tαu , w)) The consequential relation might exist due to various environmental constraints (when one action forces the adversary to respond with a specific action) or due to the agents knowledge about the profile of its adversary.",
                "Property 3.",
                "As the knowledge we have about our adversary increases we will have additional beliefs about its behavior in different situations which in turn creates new set actions.",
                "Formally, if our AdvKnow at time Tn+1 is greater than AdvKnow at time Tn, then every SetAction known at time Tn is also known at time Tn+1.",
                "AdvKnow(P Aj Ai , Tn+1) > AdvKnow(P Aj Ai , Tn) ⇒ (∀α1 , . . . , αu ∈ CAi , β1 , . . . , βv ∈ CAj ) Bel(Aag, SetAction(α1 , . . . , αu , β1 , . . . , βv ), Tn) ⇒ Bel(Aag, SetAction(α1 , . . . , αu , β1 , . . . , βv ), Tn+1) 2.3 The Environment Formulation The following axioms provide the formal definition for a simple, zero-sum Adversarial Environment (AE).",
                "Satisfaction of these axioms means that the agent is situated in such an environment.",
                "It provides specifications for agent Aag to interact with its set of adversaries A with respect to goals G∗ Aag and G∗ A at time TCo at some world state w. AE(Aag, A, G∗ Aag , A1, . . . , Ak, G∗ A1 , . . . , G∗ Ak , Tn, w) 1.",
                "Aag has an Int.Th his goal would be completed: (∃α ∈ CAag , Tα) Int.Th(Aag, Achieve(G∗ Aag , α), Tn, Tα, AE) 2.",
                "Aag believes that it and each of its adversaries Ao are pursuing full conflicting goals: (∀Ao ∈ {A1, . . . , Ak}) Bel(Aag, FulConf(G∗ Aag , G∗ Ao ), Tn) 3.",
                "Aag believes that each of his adversaries in Ao has the Int.Th his conflicting goal G∗ Aoi will be completed: (∀Ao ∈ {A1, . . . , Ak})(∃β ∈ CAo , Tβ) Bel(Aag, Int.Th(Ao, Achieve(G∗ Ao , β), TCo, Tβ, AE), Tn) 4.",
                "Aag has beliefs about the (partial) profiles of its adversaries (∀Ao ∈ {A1, . . . , Ak}) (∃PAo Aag ∈ PAag )Bel(Aag, Profile(Ao, PAo Aag ), Tn) To build an agent that will be able to operate successfully within such an AE, we must specify behavioral guidelines for its interactions.",
                "Using a naive Eval maximization strategy to a certain search depth will not always yield satisfactory results for several reasons: (1) the search horizon problem when searching for a fixed depth; (2) the strong assumption of an optimally rational, unbounded resources adversary; (3) using an estimated evaluation function which will not give optimal results in all world states, and can be exploited [9].",
                "The following axioms specify the behavioral principles that can be used to differentiate between successful and less successful agents in the above Adversarial Environment.",
                "Those axioms should be used as specification principles when designing and implementing agents that should be able to perform well in such Adversarial Environments.",
                "The behavioral axioms represent situations in which the agent will adopt potential intentions to (Pot.Int.To(...)) perform an action, which will typically require some means-end reasoning to select a possible course of action.",
                "This reasoning will lead to the adoption of an Int.To(...) (see [4]).",
                "A1.",
                "Goal Achieving Axiom.",
                "The first axiom is the simplest case; when the agent Aag believes that it is one action (α) away from achieving his conflicting goal G∗ Aag , it should adopt the potential intention to do α and complete its goal. (∀Aag, α ∈ CAag , Tn, Tα, w ∈ W) (Bel(Aag, Do(Aag, α, Tα, w) ⇒ Achieve(G∗ Aag , α, w)) ⇒ Pot.Int.To(Aag, α, Tn, Tα, w) This somewhat trivial behavior is the first and strongest axiom.",
                "In any situation, when the agent is an action away from completing the goal, it should complete the action.",
                "Any fair Eval function would naturally classify α as the maximal value action (property 1).",
                "However, without explicit axiomatization of such behavior there might be situations where the agent will decide on taking another action for various reasons, due to its bounded decision resources.",
                "A2.",
                "Preventive Act Axiom.",
                "Being in an adversarial situation, agent Aag might decide to take actions that will damage one of its adversarys plans to complete its goal, even if those actions do not explicitly advance Aag towards its conflicting goal G∗ Aag .",
                "Such preventive action will take place when agent Aag has a belief about the possibility of its adversary Ao doing an action β that will give it a high 552 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) utility evaluation value (> TrH).",
                "Believing that taking action α will prevent the opponent from doing its β, it will adopt a potential intention to do α. (∀Aag, Ao ∈ A, α ∈ CAag , β ∈ CAo , Tn, Tβ, w ∈ W) (Bel(Aag, Do(Ao, β, Tβ, w) ∧ Eval(Ao, β, w) > TrH, Tn) ∧ Bel(Aag, Do(Aag, α, Tα, w) ⇒ ¬Do(Ao, β, Tβ, w), Tn) ⇒ Pot.Int.To(Aag, α, Tn, Tα, w) This axiom is a basic component of any adversarial environment.",
                "For example, looking at a Chess board game, a player could realize that it is about to be checkmated by its opponent, thus making a preventive move.",
                "Another example is a Connect Four game: when a player has a row of three chips, its opponent must block it, or lose.",
                "A specific instance of A1 occurs when the adversary is one action away from achieving its goal, and immediate preventive action needs to be taken by the agent.",
                "Formally, we have the same beliefs as stated above, with a changed belief that doing action β will cause agent Ao to achieve its goal.",
                "Proposition 1: Prevent or lose case. (∀Aag, Ao ∈ A, α ∈ CAag , β ∈ CAo , G∗ Ao , Tn, Tα, Tβ, w ∈ W) Bel(Aag, Do(Ao, β, Tβ, w) ⇒ Achieve(G∗ Ao , β, w), Tn) ∧ Bel(Aag, Do(Aag, α, Tα, w) ⇒ ¬Do(Ao, β, Tβ, w)) ⇒ Pot.Int.To(Aag, α, Tn, Tα, w) Sketch of proof: Proposition 1 can be easily derived from axiom A1 and the property 2 of the Eval function, which states that any action that causes a completion of a goal is a highly beneficial action.",
                "The preventive act behavior will occur implicitly when the Eval function is equal to the real world utility function.",
                "However, being bounded rational agents and dealing with an estimated evaluation function we need to explicitly axiomatize such behavior, for it will not always occur implicitly from the evaluation function.",
                "A3.",
                "Suboptimal Tactical Move Axiom.",
                "In many scenarios a situation may occur where an agent will decide not to take the current most beneficial action it can take (the action with the maximal utility evaluation value), because it believes that taking another action (with lower utility evaluation value) might yield (depending on the adversarys response) a future possibility for a highly beneficial action.",
                "This will occur most often when the Eval function is inaccurate and differs by a large extent from the Utility function.",
                "Put formally, agent Aag believes in a certain SetAction that will evolve according to its initial action and will yield a high beneficial value (> TrH) solely for it. (∀Aag, Ao ∈ A, Tn, w ∈ W) (∃α1 , . . . , αu ∈ CAi , β1 , . . . , βv ∈ CAj , Tα1 ) Bel(Aag, SetAction(α1 , . . . , αu , β1 , . . . , βv ), Tn) ∧ Bel(Aag, Eval(Ao, βv , w) < TrH < Eval(Aag, αu , w), Tn) ⇒ Pot.Int.To(Aag, α1 , Tn, Tα1 , w) An agent might believe that a chain of events will occur for various reasons due to the inevitable nature of the domain.",
                "For example, in Chess, we often observe the following: a move causes a check position, which in turn limits the opponents moves to avoiding the check, to which the first player might react with another check, and so on.",
                "The agent might also believe in a chain of events based on its knowledge of its adversarys profile, which allows it to foresee the adversarys movements with high accuracy.",
                "A4.",
                "Profile Detection Axiom.",
                "The agent can adjust its adversarys profiles by observations and pattern study (specifically, if there are repeated encounters with the same adversary).",
                "However, instead of waiting for profile information to be revealed, an agent can also initiate actions that will force its adversary to react in a way that will reveal profile knowledge about it.",
                "Formally, the axiom states that if all actions (γ) are not highly beneficial actions (< TrH), the agent can do action α in time Tα if it believes that it will result in a non-highly beneficial action β from its adversary, which in turn teaches it about the adversarys profile, i.e., gives a higher AdvKnow(P Aj Ai , Tβ). (∀Aag, Ao ∈ A, α ∈ CAag , β ∈ CAo , Tn, Tα, Tβ, w ∈ W) Bel(Aag, (∀γ ∈ CAag )Eval(Aag, γ, w) < TrH, Tn) ∧ Bel(Aag, Do(Aag, α, Tα, w) ⇒ Do(Ao, β, Tβ, w), Tn) ∧ Bel(Aag, Eval(Ao, β, w) < TrH) ∧ Bel(Aag, AdvKnow(P Aj Ai , Tβ) > AdvKnow(P Aj Ai , Tn), Tn) ⇒ Pot.Int.To(Aag, α, Tn, Tα, w) For example, going back to the Chess board game scenario, consider starting a game versus an opponent about whom we know nothing, not even if it is a human or a computerized opponent.",
                "We might start playing a strategy that will be suitable versus an average opponent, and adjust our game according to its level of play.",
                "A5.",
                "Alliance Formation Axiom The following behavioral axiom is relevant only in a multilateral instantiation of the adversarial environment (obviously, an alliance cannot be formed in a bilateral, zero-sum encounter).",
                "In different situations during a multilateral <br>interaction</br>, a group of agents might believe that it is in their best interests to form a temporary alliance.",
                "Such an alliance is an agreement that constrains its members behavior, but is believed by its members to enable them to achieve a higher utility value than the one achievable outside of the alliance.",
                "As an example, we can look at the classical Risk board game, where each player has an individual goal of being the sole conquerer of the world, a zero-sum game.",
                "However, in order to achieve this goal, it might be strategically wise to make short-term ceasefire agreements with other players, or to join forces and attack an opponent who is stronger than the rest.",
                "An alliances terms defines the way its members should act.",
                "It is a set of predicates, denoted as Terms, that is agreed upon by the alliance members, and should remain true for the duration of the alliance.",
                "For example, the set Terms in the Risk scenario, could contain the following predicates: 1.",
                "Alliance members will not attack each other on territories X, Y and Z; 2.",
                "Alliance members will contribute C units per turn for attacking adversary Ao; 3.",
                "Members are obligated to stay as part of the alliance until time Tk or until adversarys Ao army is smaller than Q.",
                "The set Terms specifies inter-group constraints on each of the alliance members (∀Aal i ∈ Aal ⊆ A) set of actions Cal i ⊆ C. Definition 6.",
                "Al val - the total evaluation value that agent Ai will achieve while being part of Aal is the sum of Evali (Eval for Ai) of each of Aal j Eval values after taking their own α actions (via the agent(α) predicate): Al val(Ai, Cal , Aal , w) = α∈Cal Evali(Aal j , agent(α), w) Definition 7.",
                "Al TrH - is a number representing an Al val The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 553 threshold; above it, the alliance can be said to be a highly beneficial alliance.",
                "The value of Al TrH will be calculated dynamically according to the progress of the <br>interaction</br>, as can be seen in [7].",
                "After an alliance is formed, its members are now working in their normal adversarial environment, as well as according to the mental states and axioms required for their interactions as part of the alliance.",
                "The following Alliance model (AL) specifies the conditions under which the group Aal can be said to be in an alliance and working with a new and constrained set of actions Cal , at time Tn.",
                "AL(Aal , Cal , w, Tn) 1.",
                "Aal has a MB that all members are part of Aal : MB(Aal , (∀Aal i ∈ Aal )member(Aal i , Aal ), Tn) 2.",
                "Aal has a MB that the group be maintained: MB(Aal , (∀Aal i ∈ Aal )Int.Th (Ai, member(Ai, Aal ), Tn, Tn+1, Co), Tn) 3.",
                "Aal has a MB that being members gives them high utility value: MB(Aal , (∀Aal i ∈ Aal )Al val(Aal i , Cal , Aal , w) ≥ Al TrH, Tn) Members profiles are a crucial part of successful alliances.",
                "We assume that agents that have more accurate profiles of their adversaries will be more successful in such environments.",
                "Such agents will be able to predict when a member is about to breach the alliances contract (item 2 in the above model), and take counter measures (when item 3 will falsify).",
                "The robustness of the alliance is in part a function of its members trustfulness measure, objective position estimation, and other profile properties.",
                "We should note that an agent can simultaneously be part of more than one alliance.",
                "Such a temporary alliance, where the group members do not have a joint goal but act collaboratively for the interest of their own individual goals, is classified as a Treatment Group by modern psychologists [12] (in contrast to a Task Group, where its members have a joint goal).",
                "The Shared Activity model as presented in [5] modeled Treatment Group behavior using the same SharedPlans formalization.",
                "When comparing both definitions of an alliance and a Treatment Group we found an unsurprising resemblance between both models: the environment models definitions are almost identical (see SAs definitions in [5]), and their Selfish-Act and Cooperative Act axioms conform to our adversarial agents behavior.",
                "The main distinction between both models is the integration of a Helpful-behavior act axiom, in the Shared Activity which cannot be part of ours.",
                "This axiom states that an agent will consider taking action that will lower its Eval value (to a certain lower bound), if it believes that a group partner will gain a significant benefit.",
                "Such behavior cannot occur in a pure adversarial environment (as a zero-sum game is), where the alliance members are constantly on watch to manipulate their alliance to their own advantage.",
                "A6.",
                "Evaluation Maximization Axiom.",
                "In a case when all other axioms are inapplicable, we will proceed with the action that maximizes the heuristic value as computed in the Eval function. (∀Aag, Ao ∈ A, α ∈ Cag, Tn, w ∈ W) Bel(Aag, (∀γ ∈ Cag)Eval(Aag, α, w) ≥ Eval(Aag, γ, w), Tn) ⇒ Pot.Int.To(Aag, α, Tn, Tα, w) T1.",
                "Optimality on Eval = Utility The above axiomatic model handles situations where the Utility is unknown and the agents are bounded rational agents.",
                "The following theorem shows that in bilateral interactions, where the agents have the real Utility function (i.e., Eval = Utility) and are rational agents, the axioms provide the same optimal result as classic adversarial search (e.g., Min-Max).",
                "Theorem 1.",
                "Let Ae ag be an unbounded rational AE agent using the Eval heuristic evaluation function, Au ag be the same agent using the true Utility function, and Ao be a sole unbounded utility-based rational adversary.",
                "Given that Eval = Utility: (∀α ∈ CAu ag , α ∈ CAe ag , Tn, w ∈ W) Pot.Int.To(Au ag, α, Tn, Tα, w) → Pot.Int.To(Ae ag, α , Tn, Tα, w) ∧ ((α = α ) ∨ (Utility(Au ag, α, w) = Eval(Ae ag, α , w))) Sketch of proof - Given that Au ag has the real utility function and unbounded resources, it can generate the full game tree and run the optimal MinMax algorithm to choose the highest utility value action, which we denote by, α.",
                "The proof will show that Ae ag, using the AE axioms, will select the same or equal utility α (when there is more than one action with the same max utility) when Eval = Utility. (A1) Goal achieving axiom - suppose there is an α such that its completion will achieve Au ags goal.",
                "It will obtain the highest utility by Min-Max for Au ag.",
                "The Ae ag agent will select α or another action with the same utility value via A1.",
                "If such α does not exist, Ae ag cannot apply this axiom, and proceeds to A2. (A2) Preventive act axiom - (1) Looking at the basic case (see Prop1), if there is a β which leads Ao to achieve its goal, then a preventive action α will yield the highest utility for Au ag.",
                "Au ag will choose it through the utility, while Ae ag will choose it through A2. (2) In the general case, β is a highly beneficial action for Ao, thus yields low utility for Au ag, which will guide it to select an α that will prevent β, while Ae ag will choose it through A2.1 If such β does not exist for Ao, then A2 is not applicable, and Ae ag can proceed to A3. (A3) Suboptimal tactical move axiom - When using a heuristic Eval function, Ae ag has a partial belief in the profile of its adversary (item 4 in AE model), which may lead it to believe in SetActions (Prop1).",
                "In our case, Ae ag is holding a full profile on its optimal adversary and knows that Ao will behave optimally according to the real utility values on the complete search tree, therefore, any belief about suboptimal SetAction cannot exist, yielding this axiom inapplicable.",
                "Ae ag will proceed to A4. (A4) Profile detection axiom - Given that Ae ag has the full profile of Ao, none of Ae ags actions can increase its knowledge.",
                "That axiom will not be applied, and the agent will proceed with A6 (A5 will be disregarded because the <br>interaction</br> is bilateral). (A6) Evaluation maximization axiom - This axiom will select the max Eval for Ae ag.",
                "Given that Eval = Utility, the same α that was selected by Au ag will be selected. 3.",
                "EVALUATION The main purpose of our experimental analysis is to evaluate the models behavior and performance in a real adversarial environment.",
                "This section investigates whether bounded 1 A case where following the completion of β there exists a γ which gives high utility for Agent Au ag, cannot occur because Ao uses the same utility, and γs existence will cause it to classify β as a low utility action. 554 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) rational agents situated in such adversarial environments will be better off applying our suggested behavioral axioms. 3.1 The Domain To explore the use of the above model and its behavioral axioms, we decided to use the Connect-Four game as our adversarial environment.",
                "Connect-Four is a 2-player, zerosum game which is played using a 6x7 matrix-like board.",
                "Each turn, a player drops a disc into one of the 7 columns (the set of 21 discs is usually colored yellow for player 1 and red for player 2; we will use White and Black respectively to avoid confusion).",
                "The winner is the first player to complete a horizontal, vertical, or diagonal set of four discs with its color.",
                "On very rare occasions, the game might end in a tie if all the empty grids are filled, but no player managed to create a 4-disc set.",
                "The Connect-Four game was solved in [1], where it is shown that the first player (playing with the white discs) can force a win by starting in the middle column (column 4) and playing optimally However, the optimal strategy is very complex, and difficult to follow even for complex bounded rational agents, such as human players.",
                "Before we can proceed checking agent behavior, we must first verify that the domain conforms to the adversarial environments definition as given above (which the behavioral axioms are based on).",
                "First, when playing a Connect-Four game, the agent has an intention to win the game (item 1).",
                "Second (item 2), our agent believes that in Connect-Four there can only be one winner (or no winner at all in the rare occurrence of a tie).",
                "In addition, our agent believes that its opponent to the game will try to win (item 3), and we hope it has some partial knowledge (item 4) about its adversary (this knowledge can vary from nothing, through simple facts such as age, to strategies and weaknesses).",
                "Of course, not all Connect-Four encounters are adversarial.",
                "For example, when a parent is playing the game with its child, the following situation might occur: the child, having a strong incentive to win, treats the environment as adversarial (it intends to win, understands that there can only be one winner, and believes that its parent is trying to beat him).",
                "However, the parents point of view might see the environment as an educational one, where its goal is not to win the game, but to cause enjoyment or practice strategic reasoning.",
                "In such an educational environment, a new set of behavioral axioms might be more beneficial to the parents goals than our suggested adversarial behavioral axioms. 3.2 Axiom Analysis After showing that the Connect-Four game is indeed a zero-sum, bilateral adversarial environment, the next step is to look at players behaviors during the game and check whether behaving according to our model does improve performance.",
                "To do so we have collected log files from completed Connect-Four games that were played by human players over the Internet.",
                "Our collected log file data came from Play by eMail (PBeM) sites.",
                "These are web sites that host email games, where each move is taken by an email exchange between the server and the players.",
                "Many such sites archives contain real competitive interactions, and also maintain a ranking system for their members.",
                "Most of the data we used can be found in [6].",
                "As can be learned from [1], Connect-Four has an optimal strategy and a considerable advantage for the player who starts the game (which we call the White player).",
                "We will concentrate in our analysis on the second players moves (to be called Black).",
                "The White player, being the first to act, has the so-called initiative advantage.",
                "Having the advantage and a good strategy will keep the Black player busy reacting to its moves, instead of initiating threats.",
                "A threat is a combination of three discs of the same color, with an empty spot for the fourth winning disk.",
                "An open threat is a threat that can be realized in the opponents next move.",
                "In order for the Black player to win, it must somehow turn the tide, take the advantage and start presenting threats to the White player.",
                "We will explore Black players behavior and their conformance to our axioms.",
                "To do so, we built an application that reads log files and analyzes the Black players moves.",
                "The application contains two main components: (1) a Min-Max algorithm for evaluation of moves; (2) open threats detector for the discovering of open threats.",
                "The Min-Max algorithm will work to a given depth, d and for each move α will output the heuristic value for the next action taken by the player as written in the log file, h(α), alongside the maximum heuristic value, maxh(α), that could be achieved prior to taking the move (obviously, if h(α) = maxh(α), then the player did not do the optimal move heuristically).",
                "The threat detectors job is to notify if some action was taken in order to block an open threat (not blocking an open threat will probably cause the player to lose in the opponents next move).",
                "The heuristic function used by Min-Max to evaluate the players utility is the following function, which is simple to compute, yet provides a reasonable challenge to human opponents: Definition 8.",
                "Let Group be an adjacent set of four squares that are horizontal, vertical, or diagonal.",
                "Groupn b (Groupn w) be a Group with n pieces of the black (white) color and 4−n empty squares. h = ((Group1 b ∗α)+(Group2 b ∗β)+(Group3 b ∗γ)+(Group4 b ∗∞)) − ((Group1 w ∗α)+(Group2 w ∗β)+(Group3 w ∗γ)+(Group4 w ∗∞)) The values of α, β and δ can vary to form any desired linear combination; however, it is important to value them with the α < β < δ ordering in mind (we used 1, 4, and 8 as their respective values).",
                "Groups of 4 discs of the same color means victory, thus discovery of such a group will result in ∞ to ensure an extreme value.",
                "We now use our estimated evaluation function to evaluate the Black players actions during the Connect-Four adversarial <br>interaction</br>.",
                "Each game from the log file was input into the application, which processed and output a reformatted log file containing the h value of the current move, the maxh value that could be achieved, and a notification if an open threat was detected.",
                "A total of 123 games were analyzed (57 with White winning, and 66 with Black winning).",
                "A few additional games were manually ignored in the experiment, due to these problems: a player abandoning the game while the outcome is not final, or a blunt irrational move in the early stages of the game (e.g., not blocking an obvious winning group in the first opening moves).",
                "In addition, a single tie game was also removed.",
                "The simulator was run to a search depth of 3 moves.",
                "We now proceed to analyze the games with respect to each behavioral axiom.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 555 Table 1: Average heuristic difference analysis Black losses Black Won Avg minh -17.62 -12.02 Avg 3 lowest h moves (min3 h) -13.20 -8.70 3.2.1 Affirming the Suboptimal tactical move axiom The following section presents the heuristic evaluations of the Min-Max algorithm for each action, and checks the amount and extent of suboptimal tactical actions and their implications on performance.",
                "Table 1 shows results and insights from the games heuristic analysis, when search depth equals 3 (this search depth was selected for the results to be comparable to [9], see Section 3.2.3).",
                "The tables heuristic data is the difference between the present maximal heuristic value and the heuristic value of the action that was eventually taken by the player (i.e., the closer the number is to 0, the closer the action was to the maximum heuristic action).",
                "The first row presents the difference values of the action that had the maximal difference value among all the Black players actions in a given game, as averaged over all Blacks winning and losing games (see respective columns).",
                "In games in which the Black player loses, its average difference value was -17.62, while in games in which the Black player won, its average was -12.02.",
                "The second row expands the analysis by considering the 3 highest heuristic difference actions, and averaging them.",
                "In that case, we notice an average heuristic difference of 5 points between games which the Black player loses and games in which it wins.",
                "Nevertheless, the importance of those numbers is that they allowed us to take an educated guess on a threshold number of 11.5, as the value of the TrH constant, which differentiates between normal actions and highly beneficial ones.",
                "After finding an approximated TrH constant, we can proceed with an analysis of the importance of suboptimal moves.",
                "To do so we took the subset of games in which the minimum heuristic difference value for Blacks actions was 11.5.",
                "As presented in Table 2, we can see the different min3 h average of the 3 largest ranges and the respective percentage of games won.",
                "The first row shows that the Black player won only 12% of the games in which the average of its 3 highest heuristically difference actions (min3 h) was smaller than the suggested threshold, TrH = 11.5.",
                "The second row shows a surprising result: it seems that when min3 h > −4 the Black player rarely wins.",
                "Intuition would suggest that games in which the action evaluation values were closer to the maximal values will result in more winning games for Black.",
                "However, it seems that in the Connect-Four domain, merely responding with somewhat easily expected actions, without initiating a few surprising and suboptimal moves, does not yield good results.",
                "The last row sums up the main insights from the analysis; most of Blacks wins (83%) came when its min3 h was in the range of -11.5 to -4.",
                "A close inspection of those Black winning games shows the following pattern behind the numbers: after standard opening moves, Black suddenly drops a disc into an isolated column, which seems a waste of a move.",
                "White continues to build its threats, while usually disregarding Blacks last move, which in turn uses the isolated disc as an anchor for a future winning threat.",
                "The results show that it was beneficial for the Black player Table 2: Blacks winnings percentages % of games min3 h < −11.5 12% min3 h > −4 5% −11.5 ≤ min3 h ≤ −4 83% to take suboptimal actions and not give the current highest possible heuristic value, but will not be too harmful for its position (i.e., will not give high beneficial value to its adversary).",
                "As it turned out, learning the threshold is an important aspect of success: taking wildly risky moves (min3 h < −11.5) or trying to avoid them (min3 h > −4) reduces the Black players winning chances by a large margin. 3.2.2 Affirming the Profile Monitoring Axiom In the task of showing the importance of monitoring ones adversaries profiles, our log files could not be used because they did not contain repeated interactions between players, which are needed to infer the players knowledge about their adversaries.",
                "However, the importance of opponent modeling and its use in attaining tactical advantages was already studied in various domains ([3, 9] are good examples).",
                "In a recent paper, Markovitch and Reger [9] explored the notion of learning and exploitation of opponent weakness in competitive interactions.",
                "They apply simple learning strategies by analyzing examples from past interactions in a specific domain.",
                "They also used the Connect-Four adversarial domain, which can now be used to understand the importance of monitoring the adversarys profile.",
                "Following the presentation of their theoretical model, they describe an extensive empirical study and check the agents performance after learning the weakness model with past examples.",
                "One of the domains used as a competitive environment was the same Connect-Four game (Checkers was the second domain).",
                "Their heuristic function was identical to ours with three different variations (H1, H2, and H3) that are distinguished from one another in their linear combination coefficient values.",
                "The search depth for the players was 3 (as in our analysis).",
                "Their extensive experiments check and compare various learning strategies, risk factors, predefined feature sets and usage methods.",
                "The bottom line is that the Connect-Four domain shows an improvement from a 0.556 winning rate before modeling to a 0.69 after modeling (page 22).",
                "Their conclusions, showing improved performance when holding and using the adversarys model, justify the effort to monitor the adversary profile for continuous and repeated interactions.",
                "An additional point that came up in their experiments is the following: after the opponent weakness model has been learned, the authors describe different methods of integrating the opponent weakness model into the agents decision strategy.",
                "Nevertheless, regardless of the specific method they chose to work with, all integration methods might cause the agent to take suboptimal decisions; it might cause the agent to prefer actions that are suboptimal at the present decision junction, but which might cause the opponent to react in accordance with its weakness model (as represented by our agent) which in turn will be beneficial for us in the future.",
                "The agents behavior, as demonstrated in [9] further confirms and strengthens our Suboptimal Tactical Axiom as discussed in the previous section. 556 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 3.2.3 Additional Insights The need for the Goal Achieving, Preventive Act, and Evaluation Maximization axioms are obvious, and need no further verification.",
                "However, even with respect to those axioms, a few interesting insights came up in the log analysis.",
                "The Goal achieving and Preventive Act axioms, though theoretically trivial, seem to provide some challenge to a human player.",
                "In the initial inspection of the logs, we encountered few games2 where a player, for inexplicable reasons, did not block the other from winning or failed to execute its own winning move.",
                "We can blame those faults on the humans lack of attention, or a typing error in its move reply; nevertheless, those errors might occur in bounded rational agents, and the appropriate behavior needs to be axiomatized.",
                "A typical Connect-Four game revolves around generating threats and blocking them.",
                "In our analysis we looked for explicit preventive actions, i.e., moves that block a group of 3 discs, or that remove a future threat (in our limited search horizon).",
                "We found that in 83% of the total games there was at least one preventive action taken by the Black player.",
                "It was also found that Black averaged 2.8 preventive actions per game on the games in which it lost, while averaging 1.5 preventive actions per game when winning.",
                "It seems that Black requires 1 or 2 preventive actions to build its initial taking position, before starting to present threats.",
                "If it did not manage to win, it will usually prevent an extra threat or two before succumbing to White. 4.",
                "RELATED WORK Much research deals with the axiomatization of teamwork and mental states of individuals: some models use knowledge and belief [10], others have models of goals and intentions [8, 4].",
                "However, all these formal theories deal with agent teamwork and cooperation.",
                "As far as we know, our model is the first to provide a formalized model for explicit adversarial environments and agents behavior in it.",
                "The classical Min-Max adversarial search algorithm was the first attempt to integrate the opponent into the search space with a weak assumption of an optimally playing opponent.",
                "Since then, much effort has gone into integrating the opponent model into the decision procedure to predict future behavior.",
                "The M∗ algorithm presented by Carmel and Markovitch [2] showed a method of incorporating opponent models into adversary search, while in [3] they used learning to provide a more accurate opponent model in a 2player repeated game environment, where agents strategies were modeled as finite automata.",
                "Additional Adversarial planning work was done by Willmott et al. [13], which provided an adversarial planning approach to the game of GO.",
                "The research mentioned above dealt with adversarial search and the integration of opponent models into classical utilitybased search methods.",
                "That work shows the importance of opponent modeling and the ability to exploit it to an agents advantage.",
                "However, the basic limitations of those search methods still apply; our model tries to overcome those limitations by presenting a formal model for a new, mental state-based adversarial specification. 5.",
                "CONCLUSIONS We presented an Adversarial Environment model for a 2 These were later removed from the final analysis. bounded rational agent that is situated in an N-player, zerosum environment.",
                "We used the SharedPlans formalization to define the model and the axioms that agents can apply as behavioral guidelines.",
                "The model is meant to be used as a guideline for designing agents that need to operate in such adversarial environments.",
                "We presented empirical results, based on ConnectFour log file analysis, that exemplify the model and the axioms for a bilateral instance of the environment.",
                "The results we presented are a first step towards an expanded model that will cover all types of adversarial environments, for example, environments that are non-zero-sum, and environments that contain natural agents that are not part of the direct conflict.",
                "Those challenges and more will be dealt with in future research. 6.",
                "ACKNOWLEDGMENT This research was supported in part by Israel Science Foundation grants #1211/04 and #898/05. 7.",
                "REFERENCES [1] L. V. Allis.",
                "A knowledge-based approach of Connect-Four - the game is solved: White wins.",
                "Masters thesis, Free University, Amsterdam, The Netherlands, 1988. [2] D. Carmel and S. Markovitch.",
                "Incorporating opponent models into adversary search.",
                "In Proceedings of the Thirteenth National Conference on Artificial Intelligence, pages 120-125, Portland, OR, 1996. [3] D. Carmel and S. Markovitch.",
                "Opponent modeling in multi-agent systems.",
                "In G. Weiß and S. Sen, editors, Adaptation and Learning in Multi-Agent Systems, pages 40-52.",
                "Springer-Verlag, 1996. [4] B. J. Grosz and S. Kraus.",
                "Collaborative plans for complex group action.",
                "Artificial Intelligence, 86(2):269-357, 1996. [5] M. Hadad, G. Kaminka, G. Armon, and S. Kraus.",
                "Supporting collaborative activity.",
                "In Proc. of AAAI-2005, pages 83-88, Pittsburgh, 2005. [6] http://www.gamerz.net/˜pbmserv/. [7] S. Kraus and D. Lehmann.",
                "Designing and building a negotiating automated agent.",
                "Computational Intelligence, 11:132-171, 1995. [8] H. J. Levesque, P. R. Cohen, and J. H. T. Nunes.",
                "On acting together.",
                "In Proc. of AAAI-90, pages 94-99, Boston, MA, 1990. [9] S. Markovitch and R. Reger.",
                "Learning and exploiting relative weaknesses of opponent agents.",
                "Autonomous Agents and Multi-Agent Systems, 10(2):103-130, 2005. [10] Y. M. Ronald Fagin, Joseph Y. Halpern and M. Y. Vardi.",
                "Reasoning about knowledge.",
                "MIT Press, Cambridge, Mass., 1995. [11] P. Thagard.",
                "Adversarial problem solving: Modeling an oponent using explanatory coherence.",
                "Cognitive Science, 16(1):123-149, 1992. [12] R. W. Toseland and R. F. Rivas.",
                "An Introduction to Group Work Practice.",
                "Prentice Hall, Englewood Cliffs, NJ, 2nd edition edition, 1995. [13] S. Willmott, J. Richardson, A. Bundy, and J. Levine.",
                "An adversarial planning approach to Go.",
                "Lecture Notes in Computer Science, 1558:93-112, 1999.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 557"
            ],
            "original_annotated_samples": [
                "We recall here some of the predicates and operators that are used in that formalization: Int.To(Ai, α, Tn, Tα, C) represents Ais intentions at time Tn to do an action α at time Tα in the context of C. Int.Th(Ai, prop, Tn, Tprop, C) represents Ais intentions at time Tn that a certain proposition prop holds at time Tprop in the context of C. The potential intention operators, Pot.Int.To(...) and Pot.Int.Th(...), are used to represent the mental state when an agent considers adopting an intention, but has not deliberated about the <br>interaction</br> of the other intentions it holds.",
                "Full conflict (FulConf ) describes a zerosum <br>interaction</br> where only a single goal of the goals in conflict can be completed.",
                "In different situations during a multilateral <br>interaction</br>, a group of agents might believe that it is in their best interests to form a temporary alliance.",
                "The value of Al TrH will be calculated dynamically according to the progress of the <br>interaction</br>, as can be seen in [7].",
                "That axiom will not be applied, and the agent will proceed with A6 (A5 will be disregarded because the <br>interaction</br> is bilateral). (A6) Evaluation maximization axiom - This axiom will select the max Eval for Ae ag."
            ],
            "translated_annotated_samples": [
                "Recordamos aquí algunos de los predicados y operadores que se utilizan en esa formalización: Int.To(Ai, α, Tn, Tα, C) representa las intenciones de Ai en el tiempo Tn de realizar una acción α en el tiempo Tα en el contexto de C. Int.Th(Ai, prop, Tn, Tprop, C) representa las intenciones de Ai en el tiempo Tn de que una cierta proposición prop se cumpla en el tiempo Tprop en el contexto de C. Los operadores de intención potencial, Pot.Int.To(...) y Pot.Int.Th(...), se utilizan para representar el estado mental cuando un agente considera adoptar una intención, pero no ha deliberado sobre la <br>interacción</br> de las otras intenciones que tiene.",
                "El conflicto total (FulConf) describe una <br>interacción</br> de suma cero donde solo se puede completar un solo objetivo de los objetivos en conflicto.",
                "En diferentes situaciones durante una <br>interacción</br> multilateral, un grupo de agentes podría creer que es de su interés formar una alianza temporal.",
                "El valor de Al TrH se calculará dinámicamente de acuerdo con el progreso de la <br>interacción</br>, como se puede ver en [7].",
                "Ese axioma no se aplicará, y el agente procederá con A6 (A5 será ignorado porque la <br>interacción</br> es bilateral). (A6) Axioma de maximización de la evaluación: este axioma seleccionará la máxima Evaluación para Ae ag."
            ],
            "translated_text": "Un modelo de entorno adversarial para agentes racionales limitados en interacciones de suma cero Inon Zuckerman1, Sarit Kraus1, Jeffrey S. Rosenschein2, Gal Kaminka1 1 Departamento de Ciencias de la Computación 2 Escuela de Ingeniería Universidad Bar-Ilan y Ciencias de la Computación Ramat-Gan, Israel Universidad Hebrea, Jerusalén, Israel {zukermi, sarit, galk}@cs.biu.ac.il jeff@cs.huji.ac.il RESUMEN Los entornos multiagentes a menudo no son cooperativos ni colaborativos; en muchos casos, los agentes tienen intereses conflictivos, lo que lleva a interacciones adversariales. Este documento presenta un modelo formal de Entorno Adversarial para agentes racionales acotados que operan en un entorno de suma cero. En tales entornos, los intentos de utilizar métodos de búsqueda basados en utilidad clásica pueden plantear una variedad de dificultades (por ejemplo, modelar implícitamente al oponente como un maximizador de utilidad omnisciente, en lugar de aprovechar un modelo de oponente más matizado y explícito). Definimos un Entorno Adversario al describir los estados mentales de un agente en dicho entorno. Luego presentamos axiomas de comportamiento que están destinados a servir como principios de diseño para construir agentes adversarios. Exploramos la aplicación de nuestro enfoque mediante el análisis de archivos de registro de juegos de Conecta Cuatro completados, y presentamos un análisis empírico de la adecuación de los axiomas. Categorías y Descriptores de Asignaturas I.2.11 [Inteligencia Artificial]: Inteligencia Artificial Distribuida-Agentes Inteligentes, Sistemas Multiagente; I.2.4 [Inteligencia Artificial]: Formalismos y Métodos de Representación del Conocimiento - Lógica Modal Términos Generales Diseño, Teoría 1. Introducción: En las primeras investigaciones en sistemas multiagente (SMA) se consideraban grupos cooperativos de agentes; debido a que los agentes individuales tenían recursos limitados o acceso limitado a la información (por ejemplo, potencia de procesamiento limitada, cobertura sensorial limitada), trabajaban juntos por diseño para resolver problemas que individualmente no podían resolver, o al menos no podían resolver de manera eficiente. Sin embargo, la investigación de MAS pronto comenzó a considerar agentes interactivos con intereses individualizados, como representantes de diferentes humanos u organizaciones con intereses no idénticos. Cuando las interacciones son guiadas por intereses diversos, los participantes pueden tener que superar desacuerdos, interacciones no cooperativas e incluso intentos intencionales de dañarse mutuamente. Cuando ocurren este tipo de interacciones, los entornos requieren un comportamiento apropiado por parte de los agentes situados en ellos. Llamamos a estos entornos Entornos Adversarios, y llamamos a los agentes en conflicto Adversarios. Los modelos de cooperación y trabajo en equipo han sido ampliamente explorados en MAS a través de la axiomatización de estados mentales (por ejemplo, [8, 4, 5]). Sin embargo, ninguna de estas investigaciones abordó los dominios adversarios y sus implicaciones para el comportamiento de los agentes. Nuestro artículo aborda este problema al proporcionar un modelo formal y axiomatizado de estados mentales para un subconjunto de dominios adversariales, específicamente entornos adversariales simples de suma cero. Los encuentros de suma cero simples existen, por supuesto, en varios juegos de dos jugadores (por ejemplo, Ajedrez, Damas), pero también existen en juegos de n jugadores (por ejemplo, Risk, Diplomacia), subastas de un solo bien y en otros lugares. En estos entornos recientes en particular, el uso de una búsqueda adversarial basada en utilidad (como el algoritmo Min-Max) no siempre proporciona una solución adecuada; la función de recompensa podría ser bastante compleja o difícil de cuantificar, y existen limitaciones computacionales naturales en agentes racionales acotados. Además, los métodos de búsqueda tradicionales (como Min-Max) no hacen uso de un modelo del oponente, lo cual ha demostrado ser una adición valiosa a la planificación adversarial [9, 3, 11]. En este artículo, desarrollamos un modelo formal y axiomatizado para agentes racionales limitados que se encuentran en un entorno adversarial de suma cero. El modelo utiliza diferentes operadores de modalidad, y sus principales fundamentos son el modelo SharedPlans [4] para el comportamiento colaborativo. Exploramos las propiedades del entorno y los estados mentales de los agentes para derivar axiomas de comportamiento; estos axiomas de comportamiento constituyen un modelo formal que sirve como especificación y guía de diseño para el diseño de agentes en tales entornos. Luego investigamos el comportamiento de nuestro modelo empíricamente utilizando el juego de mesa Conecta Cuatro. Mostramos que este juego se ajusta a nuestra definición de entorno, y analizamos el comportamiento de los jugadores utilizando un gran conjunto de registros de partidas completadas 550 978-81-904262-7-5 (RPS) c 2007 archivos de IFAAMAS. Además, utilizamos los resultados presentados en [9] para discutir la importancia del modelado del oponente en nuestro dominio adversarial de Conecta Cuatro. El artículo continúa de la siguiente manera. La sección 2 presenta la formalización de los modelos. La sección 3 presenta el análisis empírico y sus resultados. Discutimos el trabajo relacionado en la Sección 4, y concluimos y presentamos las direcciones futuras en la Sección 5. ENTORNOS ADVERSARIOS El modelo de entorno adversario (denominado como AE) está destinado a guiar el diseño de agentes al proporcionar una especificación de las capacidades y actitudes mentales de un agente en un entorno adversario. Nos enfocamos aquí en tipos específicos de entornos adversarios, especificados de la siguiente manera: 1. Interacciones de suma cero: las utilidades positivas y negativas de todos los agentes suman cero; 2. Agentes adversarios simples: todos los agentes en el entorno son agentes adversarios; 3. AEs bilaterales: AEs con exactamente dos agentes; 4. AEs multilaterales: AEs de tres o más agentes. Trabajaremos en instanciaciones tanto bilaterales como multilaterales de entornos de suma cero y simples. En particular, nuestro modelo de entorno adversarial tratará con interacciones que consisten en N agentes (N ≥ 2), donde todos los agentes son adversarios y solo un agente puede tener éxito. Ejemplos de tales entornos van desde juegos de mesa (por ejemplo, Ajedrez, Conecta Cuatro y Diplomacia) hasta ciertos entornos económicos (por ejemplo, subastas de N postores sobre un único bien). 2.1 Resumen del Modelo Nuestro enfoque es formalizar las actitudes mentales y comportamientos de un único agente adversario; consideramos cómo un único agente percibe el AE. La siguiente lista especifica las condiciones y estados mentales de un agente en un AE simple de suma cero: 1. El agente tiene la intención individual de que su propio objetivo se complete; 2. El agente tiene la creencia individual de que él y sus adversarios persiguen objetivos completamente opuestos (definidos a continuación) y que solo puede haber un ganador; 3. El agente tiene la creencia individual de que cada adversario tiene la intención de completar su propio objetivo conflictivo completo; 4. El agente tiene una creencia individual en el perfil (parcial) de sus adversarios. El ítem 3 es necesario, ya que podría ser el caso de que algún agente tenga un objetivo completamente conflictivo y esté considerando adoptar la intención de completarlo, pero aún no está comprometido a lograrlo. Esto podría ocurrir porque el agente aún no ha reflexionado sobre los efectos que adoptar esa intención podría tener en las otras intenciones que actualmente mantiene. En tales casos, es posible que ni siquiera se considere en un entorno adversarial. El ítem 4 establece que el agente debe tener alguna creencia sobre los perfiles de sus adversarios. El perfil representa todo el conocimiento que el agente tiene sobre su adversario: sus debilidades, capacidades estratégicas, objetivos, intenciones, confiabilidad y más. Puede ser dado explícitamente o puede ser aprendido a partir de observaciones de encuentros pasados. 2.2 Definiciones de modelos para estados mentales Utilizamos las definiciones de los operadores modales, predicados y metapredicados de Grosz y Krauss, tal como se definen en su formalización de SharedPlan [4]. Recordamos aquí algunos de los predicados y operadores que se utilizan en esa formalización: Int.To(Ai, α, Tn, Tα, C) representa las intenciones de Ai en el tiempo Tn de realizar una acción α en el tiempo Tα en el contexto de C. Int.Th(Ai, prop, Tn, Tprop, C) representa las intenciones de Ai en el tiempo Tn de que una cierta proposición prop se cumpla en el tiempo Tprop en el contexto de C. Los operadores de intención potencial, Pot.Int.To(...) y Pot.Int.Th(...), se utilizan para representar el estado mental cuando un agente considera adoptar una intención, pero no ha deliberado sobre la <br>interacción</br> de las otras intenciones que tiene. El operador Bel(Ai, f, Tf) representa al agente Ai creyendo en la afirmación expresada en la fórmula f, en el tiempo Tf. MB(A, f, Tf) representa la creencia mutua para un grupo de agentes A. Una instantánea del sistema encuentra que nuestro entorno se encuentra en algún estado e ∈ E de estados variables ambientales, y cada adversario en cualquier LAi ∈ L de posibles estados locales. En cualquier paso de tiempo dado, el sistema estará en algún mundo w del conjunto de todos los mundos posibles w ∈ W, donde w = E×LA1 ×LA2 × ...LAn, y n es el número de adversarios. Por ejemplo, en un juego de póker Texas Holdem, el estado local de un agente podría ser su propio conjunto de cartas (que es desconocido para su adversario) mientras que el entorno consistirá en el bote de apuestas y las cartas comunitarias (que son visibles para ambos jugadores). Una función de utilidad bajo esta formalización se define como un mapeo de un mundo posible w ∈ W a un elemento en , que expresa la deseabilidad del mundo, desde la perspectiva de un único agente. Normalmente normalizamos el rango a [0,1], donde 0 representa el mundo menos deseable posible, y 1 es el mundo más deseable. La implementación de la función de utilidad depende del dominio en cuestión. La siguiente lista especifica nuevos predicados, funciones, variables y constantes utilizadas en conjunto con las definiciones originales para la formalización del entorno adversarial: 1. φ es una acción nula (el agente no hace nada). 2. GAi es el conjunto de objetivos del agente Ai. Cada objetivo es un conjunto de predicados cuya satisfacción hace que el objetivo esté completo (usamos G∗ Ai ∈ GAi para representar un objetivo arbitrario del agente Ai). 3. gAi es el conjunto de subobjetivos del agente Ai. Los subobjetivos son predicados cuya satisfacción representa un hito importante hacia el logro del objetivo completo. gG∗ Ai ⊆ gAi es el conjunto de subobjetivos que son importantes para la finalización del objetivo G∗ Ai (usaremos g∗ G∗ Ai ∈ gG∗ Ai para representar un subobjetivo arbitrario). 4. P Aj Ai es el objeto de perfil que el agente Ai tiene sobre el agente Aj. 5. CA es un conjunto general de acciones para todos los agentes en A que se derivan de las restricciones del entorno. CAi ⊆ CA es el conjunto de acciones posibles del agente Ai. La expresión Do(Ai, α, Tα, w) se cumple cuando Ai realiza la acción α durante el intervalo de tiempo Tα en el mundo w. 7. La afirmación Achieve(G∗ Ai , α, w) es verdadera cuando se logra el objetivo G∗ Ai siguiendo la finalización de la acción α en el mundo w ∈ W, donde α ∈ CAi. El perfil (Ai, PAi Ai) es verdadero cuando el agente Ai tiene un perfil de objeto para el agente Aj. Definición 1. El conflicto total (FulConf) describe una <br>interacción</br> de suma cero donde solo se puede completar un solo objetivo de los objetivos en conflicto. FulConf(G∗ Ai , G∗ Aj ) ⇒ (∃α ∈ CAi , ∀w, β ∈ CAj ) (Achieve(G∗ Ai , α, w) ⇒ ¬Achieve(G∗ Aj , β, w)) ∨ (∃β ∈ CAj , ∀w, α ∈ CAi )(Achieve(G∗ Aj , β, w) ⇒ ¬Achieve(G∗ Ai , α, w)) Definición 2. El Conocimiento Adversarial (AdvKnow) es una función que devuelve un valor que representa la cantidad de The Sixth Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 551 agente de conocimiento Ai tiene sobre el perfil del agente Aj, en el tiempo Tn. Cuanto mayor sea el valor, más conocimiento tiene el agente Ai. AdvKnow: P Aj Ai × Tn → Definición 3. Eval: Esta función de evaluación devuelve un valor de utilidad esperada estimado para un agente en A, después de completar una acción de CA en algún estado del mundo w. Eval: A × CA × w → Definición 4. TrH - (Umbral) es una constante numérica en el rango [0,1] que representa un valor umbral de la función de evaluación (Eval). Una acción que produce una evaluación de utilidad estimada por encima del TrH se considera una acción altamente beneficiosa. El valor Eval es una estimación y no la función de utilidad real, la cual suele ser desconocida. Utilizar el valor real de utilidad para un agente racional fácilmente produciría el mejor resultado para ese agente. Sin embargo, los agentes generalmente no tienen las funciones de utilidad reales, sino más bien una estimación heurística de las mismas. Existen dos propiedades importantes que deben cumplir para la función de evaluación: Propiedad 1. La función de evaluación debería indicar que el estado del mundo más deseable es aquel en el que se logra el objetivo. Por lo tanto, una vez que se ha satisfecho el objetivo, no puede haber ninguna acción futura que pueda colocar al agente en un estado del mundo con un valor de Eval más alto. (∀Ai, G∗ Ai , α, β ∈ CAi , w ∈ W) Lograr(G∗ Ai , α, w) ⇒ Eval(Ai, α, w) ≥ Eval(Ai, β, w) Propiedad 2. La función de evaluación debe proyectar una acción que cause la finalización de un objetivo o subobjetivo a un valor que sea mayor que TrH (una acción altamente beneficiosa). (∀Ai, G∗ Ai ∈ GAi , α ∈ CAi , w ∈ W, g∗ GAi ∈ gGAi ) Lograr(G∗ Ai , α, w) ∨ Lograr(g∗ GAi , α, w) ⇒ Eval(Ai, α, w) ≥ TrH. Definición 5. La acción de conjunto Definimos una acción de conjunto (SetAction) como un conjunto de operaciones de acción (ya sean acciones complejas o básicas) de algunos conjuntos de acción CAi y CAj que, según la creencia del agente Ai, están unidas por una relación temporal y consecuente, formando una cadena de eventos (acción y su acción consecuente siguiente). (∀α1 , . . . , αu ∈ CAi , β1 , . . . , βv ∈ CAj , w ∈ W) SetAction(α1 , . . . , αu , β1 , . . . , βv , w) ⇒ ((Do(Ai, α1 , Tα1 , w) ⇒ Do(Aj, β1 , Tβ1 , w)) ⇒ Do(Ai, α2 , Tα2 , w) ⇒ . . . ⇒ Do(Ai, αu , Tαu , w)) La relación consecuente podría existir debido a varias restricciones ambientales (cuando una acción obliga al adversario a responder con una acción específica) o debido al conocimiento de los agentes sobre el perfil de su adversario. Propiedad 3. A medida que aumenta el conocimiento que tenemos sobre nuestro adversario, tendremos creencias adicionales sobre su comportamiento en diferentes situaciones, lo que a su vez crea un nuevo conjunto de acciones. Formalmente, si nuestro AdvKnow en el tiempo Tn+1 es mayor que AdvKnow en el tiempo Tn, entonces cada SetAction conocido en el tiempo Tn también es conocido en el tiempo Tn+1. AdvKnow(P Aj Ai , Tn+1) > AdvKnow(P Aj Ai , Tn) ⇒ (∀α1 , . . . , αu ∈ CAi , β1 , . . . , βv ∈ CAj ) Bel(Aag, SetAction(α1 , . . . , αu , β1 , . . . , βv ), Tn) ⇒ Bel(Aag, SetAction(α1 , . . . , αu , β1 , . . . , βv ), Tn+1) 2.3 La formulación del entorno Los siguientes axiomas proporcionan la definición formal para un Entorno Adversarial (AE) simple y de suma cero. La satisfacción de estos axiomas significa que el agente está situado en un entorno así. Proporciona especificaciones para que el agente Aag interactúe con su conjunto de adversarios A con respecto a los objetivos G∗ Aag y G∗ A en el tiempo TCo en algún estado del mundo w. AE(Aag, A, G∗ Aag, A1, . . . , Ak, G∗ A1, . . . , G∗ Ak, Tn, w) 1. Aag tiene un Int.Th su objetivo se completaría: (∃α ∈ CAag , Tα) Int.Th(Aag, Lograr(G∗ Aag , α), Tn, Tα, AE) 2. Aag cree que él y cada uno de sus adversarios Ao están persiguiendo metas completamente conflictivas: (∀Ao ∈ {A1, . . . , Ak}) Bel(Aag, FulConf(G∗ Aag , G∗ Ao ), Tn) 3. Aag cree que cada uno de sus adversarios en Ao tiene la Int.Th su objetivo en conflicto G∗ Aoi se completará: (∀Ao ∈ {A1, . . . , Ak})(∃β ∈ CAo , Tβ) Bel(Aag, Int.Th(Ao, Lograr(G∗ Ao , β), TCo, Tβ, AE), Tn) 4. Para construir un agente que pueda operar con éxito dentro de un entorno de adversarios estratégicos, debemos especificar pautas de comportamiento para sus interacciones. Utilizar una estrategia de maximización de Eval ingenua hasta cierta profundidad de búsqueda no siempre dará resultados satisfactorios por varias razones: (1) el problema del horizonte de búsqueda al buscar a una profundidad fija; (2) la fuerte suposición de un adversario óptimamente racional con recursos ilimitados; (3) utilizar una función de evaluación estimada que no dará resultados óptimos en todos los estados del mundo, y que puede ser explotada [9]. Los siguientes axiomas especifican los principios de comportamiento que se pueden utilizar para diferenciar entre agentes exitosos y menos exitosos en el entorno adversarial mencionado anteriormente. Esos axiomas deben ser utilizados como principios de especificación al diseñar e implementar agentes que deben ser capaces de desempeñarse bien en entornos adversarios. Los axiomas conductuales representan situaciones en las que el agente adoptará intenciones potenciales para (Pot.Int.To(...)) realizar una acción, lo que típicamente requerirá un razonamiento de medios-fines para seleccionar un posible curso de acción. Este razonamiento llevará a la adopción de un Int.To(...) (ver [4]). A1. Axioma para lograr metas. El primer axioma es el caso más simple; cuando el agente Aag cree que está a una acción (α) de lograr su objetivo en conflicto G∗ Aag, debería adoptar la intención potencial de hacer α y completar su objetivo. (∀Aag, α ∈ CAag, Tn, Tα, w ∈ W) (Bel(Aag, Do(Aag, α, Tα, w) ⇒ Achieve(G∗ Aag, α, w)) ⇒ Pot.Int.To(Aag, α, Tn, Tα, w) Este comportamiento algo trivial es el primer y más fuerte axioma. En cualquier situación, cuando el agente está a una acción de completar el objetivo, debería completar la acción. Cualquier función de evaluación justa clasificaría naturalmente a α como la acción de valor máximo (propiedad 1). Sin embargo, sin una axiomatización explícita de dicho comportamiento, podría haber situaciones en las que el agente decida tomar otra acción por diversas razones, debido a sus recursos de decisión limitados. A2. Axioma del Acto Preventivo. En una situación adversarial, el agente Aag podría decidir tomar acciones que dañen uno de los planes de su adversario para completar su objetivo, incluso si esas acciones no avanzan explícitamente a Aag hacia su objetivo en conflicto G∗ Aag. Tal acción preventiva tendrá lugar cuando el agente Aag tenga la creencia sobre la posibilidad de que su adversario Ao realice una acción β que le otorgará un alto 552 The Sixth Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) valor de evaluación de utilidad (> TrH). Creer que tomar la acción α evitará que el oponente realice su β, adoptará una intención potencial de hacer α. (∀Aag, Ao ∈ A, α ∈ CAag , β ∈ CAo , Tn, Tβ, w ∈ W) (Bel(Aag, Do(Ao, β, Tβ, w) ∧ Eval(Ao, β, w) > TrH, Tn) ∧ Bel(Aag, Do(Aag, α, Tα, w) ⇒ ¬Do(Ao, β, Tβ, w), Tn) ⇒ Pot.Int.To(Aag, α, Tn, Tα, w) Este axioma es un componente básico de cualquier entorno adversarial. Por ejemplo, al observar un juego de ajedrez, un jugador podría darse cuenta de que está a punto de ser jaque mateado por su oponente, por lo tanto, realiza una jugada preventiva. Otro ejemplo es el juego de Cuatro en Línea: cuando un jugador tiene una fila de tres fichas, su oponente debe bloquearla o perder. Una instancia específica de A1 ocurre cuando el adversario está a una acción de lograr su objetivo, y la acción preventiva inmediata debe ser tomada por el agente. Formalmente, tenemos las mismas creencias que se mencionaron anteriormente, con la creencia modificada de que realizar la acción β hará que el agente Ao logre su objetivo. Proposición 1: Caso de prevención o pérdida. (∀Aag, Ao ∈ A, α ∈ CAag , β ∈ CAo , G∗ Ao , Tn, Tα, Tβ, w ∈ W) Bel(Aag, Do(Ao, β, Tβ, w) ⇒ Achieve(G∗ Ao , β, w), Tn) ∧ Bel(Aag, Do(Aag, α, Tα, w) ⇒ ¬Do(Ao, β, Tβ, w)) ⇒ Pot.Int.To(Aag, α, Tn, Tα, w) Esquema de prueba: La Proposición 1 puede derivarse fácilmente del axioma A1 y de la propiedad 2 de la función Eval, que establece que cualquier acción que cause la finalización de un objetivo es una acción altamente beneficiosa. El comportamiento del acto preventivo ocurrirá implícitamente cuando la función Eval sea igual a la función de utilidad del mundo real. Sin embargo, al ser agentes racionales limitados y al tratar con una función de evaluación estimada, necesitamos axiomatizar explícitamente dicho comportamiento, ya que no siempre ocurrirá implícitamente a partir de la función de evaluación. A3. Axioma de Movimiento Táctico Subóptimo. En muchos escenarios, puede ocurrir una situación en la que un agente decida no tomar la acción más beneficiosa actual que puede tomar (la acción con el valor de evaluación de utilidad máximo), porque cree que tomar otra acción (con un valor de evaluación de utilidad más bajo) podría generar (dependiendo de la respuesta del adversario) una posibilidad futura para una acción altamente beneficiosa. Esto ocurrirá con mayor frecuencia cuando la función Eval sea inexacta y difiera en gran medida de la función de Utilidad. De manera formal, el agente Aag cree en una cierta SetAction que evolucionará de acuerdo con su acción inicial y producirá un valor beneficioso alto (> TrH) únicamente para él. (∀Aag, Ao ∈ A, Tn, w ∈ W) (∃α1 , . . . , αu ∈ CAi , β1 , . . . , βv ∈ CAj , Tα1 ) Bel(Aag, SetAction(α1 , . . . , αu , β1 , . . . , βv ), Tn) ∧ Bel(Aag, Eval(Ao, βv , w) < TrH < Eval(Aag, αu , w), Tn) ⇒ Pot.Int.To(Aag, α1 , Tn, Tα1 , w) Un agente podría creer que una cadena de eventos ocurrirá por diversas razones debido a la naturaleza inevitable del dominio. Por ejemplo, en el ajedrez, a menudo observamos lo siguiente: una jugada provoca una posición de jaque, lo que a su vez limita los movimientos del oponente para evitar el jaque, a lo que el primer jugador podría reaccionar con otro jaque, y así sucesivamente. El agente también podría creer en una cadena de eventos basada en su conocimiento del perfil de su adversario, lo que le permite prever los movimientos del adversario con alta precisión. A4. Axioma de Detección de Perfiles. El agente puede ajustar los perfiles de sus adversarios mediante observaciones y estudio de patrones (específicamente, si hay encuentros repetidos con el mismo adversario). Sin embargo, en lugar de esperar a que se revele la información del perfil, un agente también puede iniciar acciones que obligarán a su adversario a reaccionar de una manera que revele conocimiento del perfil sobre él. Formalmente, el axioma establece que si todas las acciones (γ) no son acciones altamente beneficiosas (< TrH), el agente puede realizar la acción α en el tiempo Tα si cree que resultará en una acción no altamente beneficiosa β por parte de su adversario, lo que a su vez le enseña sobre el perfil del adversario, es decir, proporciona un mayor AdvKnow(P Aj Ai , Tβ). (∀Aag, Ao ∈ A, α ∈ CAag , β ∈ CAo , Tn, Tα, Tβ, w ∈ W) Bel(Aag, (∀γ ∈ CAag )Eval(Aag, γ, w) < TrH, Tn) ∧ Bel(Aag, Do(Aag, α, Tα, w) ⇒ Do(Ao, β, Tβ, w), Tn) ∧ Bel(Aag, Eval(Ao, β, w) < TrH) ∧ Bel(Aag, AdvKnow(P Aj Ai , Tβ) > AdvKnow(P Aj Ai , Tn), Tn) ⇒ Pot.Int.To(Aag, α, Tn, Tα, w) Por ejemplo, volviendo al escenario del juego de ajedrez, consideremos comenzar una partida contra un oponente del que no sabemos nada, ni siquiera si es humano o un oponente computarizado. Podríamos comenzar a jugar una estrategia que sea adecuada frente a un oponente promedio, y ajustar nuestro juego de acuerdo a su nivel de juego. A5. Axioma de Formación de Alianzas El siguiente axioma de comportamiento es relevante únicamente en una instancia multilateral del entorno adversarial (obviamente, una alianza no puede formarse en un encuentro bilateral de suma cero). En diferentes situaciones durante una <br>interacción</br> multilateral, un grupo de agentes podría creer que es de su interés formar una alianza temporal. Una alianza de este tipo es un acuerdo que limita el comportamiento de sus miembros, pero se cree por parte de estos que les permite alcanzar un valor de utilidad mayor al que podrían lograr fuera de la alianza. Como ejemplo, podemos observar el clásico juego de mesa Risk, donde cada jugador tiene como objetivo individual ser el único conquistador del mundo, un juego de suma cero. Sin embargo, para lograr este objetivo, podría ser estratégicamente sabio hacer acuerdos de alto el fuego a corto plazo con otros jugadores, o unirse y atacar a un oponente que sea más fuerte que el resto. Los términos de una alianza definen la forma en que sus miembros deben actuar. Es un conjunto de predicados, denominados Términos, que es acordado por los miembros de la alianza y debe mantenerse verdadero durante la duración de la alianza. Por ejemplo, el conjunto de términos en el escenario de riesgo podría contener los siguientes predicados: 1. Los miembros de la alianza no se atacarán entre sí en los territorios X, Y y Z; 2. Los miembros de la alianza contribuirán C unidades por turno para atacar al adversario Ao; 3. Los miembros están obligados a permanecer como parte de la alianza hasta el tiempo Tk o hasta que el ejército adversario Ao sea más pequeño que Q. El conjunto de términos especifica restricciones intergrupales en cada uno de los miembros de la alianza (∀Aal i ∈ Aal ⊆ A) conjunto de acciones Cal i ⊆ C. Definición 6. El valor total de evaluación que el agente Ai logrará al ser parte de Aal es la suma de Evali (Eval para Ai) de cada uno de los valores de evaluación de Aal j después de tomar sus propias acciones α (a través del predicado del agente(α)): Al val(Ai, Cal, Aal, w) = α∈Cal Evali(Aal j, agente(α), w) Definición 7. Al TrH - es un número que representa un valor de Al en la Sexta Conferencia Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 553 umbral; por encima de él, se puede decir que la alianza es una alianza altamente beneficiosa. El valor de Al TrH se calculará dinámicamente de acuerdo con el progreso de la <br>interacción</br>, como se puede ver en [7]. Después de formarse una alianza, sus miembros ahora están trabajando en su entorno adversarial normal, así como de acuerdo con los estados mentales y axiomas requeridos para sus interacciones como parte de la alianza. El siguiente modelo de Alianza (AL) especifica las condiciones bajo las cuales el grupo Aal puede considerarse en una alianza y trabajando con un nuevo y limitado conjunto de acciones Cal, en el tiempo Tn. AL(Aal , Cal , w, Tn) 1. \n\nAL(Aal , Cal , w, Tn) 1. Aal tiene un MB en el que todos los miembros son parte de Aal: MB(Aal, (∀Aal i ∈ Aal)miembro(Aal i, Aal), Tn) 2. Aal tiene una MB que el grupo se mantenga: MB(Aal , (∀Aal i ∈ Aal )Int.Th (Ai, miembro(Ai, Aal ), Tn, Tn+1, Co), Tn) 3. Aal tiene un beneficio marginal que al ser miembros les otorga un alto valor de utilidad: MB(Aal , (∀Aal i ∈ Aal )Al val(Aal i , Cal , Aal , w) ≥ Al TrH, Tn) Los perfiles de los miembros son una parte crucial de las alianzas exitosas. Suponemos que los agentes que tienen perfiles más precisos de sus adversarios serán más exitosos en tales entornos. Tales agentes podrán predecir cuándo un miembro está a punto de violar el contrato de alianzas (elemento 2 en el modelo anterior) y tomar medidas correctivas (cuando el elemento 3 se falsificará). La robustez de la alianza depende en parte de la medida de confianza de sus miembros, la estimación de su posición objetiva y otras propiedades de su perfil. Debemos tener en cuenta que un agente puede formar parte simultáneamente de más de una alianza. Una alianza temporal, donde los miembros del grupo no tienen un objetivo común pero actúan colaborativamente en interés de sus propias metas individuales, es clasificada como un Grupo de Tratamiento por los psicólogos modernos (en contraste con un Grupo de Tarea, donde sus miembros tienen un objetivo común). El modelo de Actividad Compartida presentado en [5] modeló el comportamiento del Grupo de Tratamiento utilizando la misma formalización de Planes Compartidos. Al comparar ambas definiciones de una alianza y un Grupo de Tratamiento, encontramos una similitud sorprendente entre ambos modelos: las definiciones de los modelos de entorno son casi idénticas (ver las definiciones de SAs en [5]), y sus axiomas de Acto Egoísta y Acto Cooperativo se ajustan al comportamiento de nuestros agentes adversarios. La principal distinción entre ambos modelos es la integración de un axioma de acto de comportamiento útil, en la Actividad Compartida que no puede formar parte de la nuestra. Este axioma establece que un agente considerará tomar una acción que disminuirá su valor de Evaluación (hasta un cierto límite inferior), si cree que un compañero del grupo obtendrá un beneficio significativo. Dicho comportamiento no puede ocurrir en un entorno puramente adversarial (como un juego de suma cero), donde los miembros de la alianza están constantemente alerta para manipular su alianza en su propio beneficio. A6. Axioma de Maximización de la Evaluación. En un caso en el que todos los demás axiomas no sean aplicables, procederemos con la acción que maximice el valor heurístico calculado en la función Eval. (∀Aag, Ao ∈ A, α ∈ Cag, Tn, w ∈ W) Bel(Aag, (∀γ ∈ Cag)Eval(Aag, α, w) ≥ Eval(Aag, γ, w), Tn) ⇒ Pot.Int.To(Aag, α, Tn, Tα, w) T1. Optimalidad en Eval = Utilidad El modelo axiomático anterior aborda situaciones en las que la Utilidad es desconocida y los agentes son agentes racionales limitados. El siguiente teorema muestra que en interacciones bilaterales, donde los agentes tienen la función de Utilidad real (es decir, Eval = Utilidad) y son agentes racionales, los axiomas proporcionan el mismo resultado óptimo que la búsqueda adversarial clásica (por ejemplo, Min-Max). Teorema 1. Que Ae ag sea un agente AE racional ilimitado que utiliza la función de evaluación heurística Eval, Au ag sea el mismo agente utilizando la verdadera función de utilidad y Ao sea un único adversario racional ilimitado basado en utilidad. Dado que Eval = Utilidad: (∀α ∈ CAu ag , α ∈ CAe ag , Tn, w ∈ W) Pot.Int.To(Au ag, α, Tn, Tα, w) → Pot.Int.To(Ae ag, α , Tn, Tα, w) ∧ ((α = α ) ∨ (Utilidad(Au ag, α, w) = Eval(Ae ag, α , w))) Esquema de prueba - Dado que Au ag tiene la función de utilidad real y recursos ilimitados, puede generar el árbol de juego completo y ejecutar el algoritmo MinMax óptimo para elegir la acción de mayor valor de utilidad, que denotamos como α. La prueba mostrará que Ae ag, utilizando los axiomas AE, seleccionará la misma utilidad α o igual (cuando hay más de una acción con la misma utilidad máxima) cuando Eval = Utilidad. (A1) Axioma de logro de objetivo - supongamos que hay un α tal que su cumplimiento logrará el objetivo Au ags. Obtendrá la utilidad más alta mediante Min-Max para Au ag. El agente Ae seleccionará α u otra acción con el mismo valor de utilidad a través de A1. Si tal α no existe, Ae ag no puede aplicar este axioma y procede a A2. (A2) Axioma de la acción preventiva - (1) Observando el caso base (ver Prop1), si hay un β que lleva a Ao a alcanzar su objetivo, entonces una acción preventiva α producirá la mayor utilidad para Au ag. Au ag lo elegirá a través de la utilidad, mientras que Ae ag lo elegirá a través de A2. En el caso general, β es una acción altamente beneficiosa para Ao, lo que resulta en una baja utilidad para Au ag, lo que lo guiará a seleccionar un α que evitará β, mientras que Ae ag lo elegirá a través de A2.1 Si dicho β no existe para Ao, entonces A2 no es aplicable, y Ae ag puede proceder a A3. (A3) Axioma de movimiento táctico subóptimo: al usar una función de evaluación heurística, Ae ag tiene una creencia parcial en el perfil de su adversario (elemento 4 en el modelo AE), lo que puede llevarlo a creer en SetActions (Prop1). En nuestro caso, Ae ag tiene un perfil completo de su adversario óptimo y sabe que Ao se comportará de manera óptima según los valores reales de utilidad en el árbol de búsqueda completo, por lo tanto, cualquier creencia sobre una SetAction subóptima no puede existir, lo que hace que este axioma sea inaplicable. Ae ag procederá a A4. (A4) Axioma de detección de perfil - Dado que Ae ag tiene el perfil completo de Ao, ninguna de las acciones de Ae ag puede aumentar su conocimiento. Ese axioma no se aplicará, y el agente procederá con A6 (A5 será ignorado porque la <br>interacción</br> es bilateral). (A6) Axioma de maximización de la evaluación: este axioma seleccionará la máxima Evaluación para Ae ag. ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "agent": {
            "translated_key": "agente",
            "is_in_text": true,
            "original_annotated_sentences": [
                "An Adversarial Environment Model for Bounded Rational Agents in Zero-Sum Interactions Inon Zuckerman1 , Sarit Kraus1 , Jeffrey S. Rosenschein2 , Gal Kaminka1 1 Department of Computer Science 2 The School of Engineering Bar-Ilan University and Computer Science Ramat-Gan, Israel Hebrew University, Jerusalem, Israel {zukermi,sarit,galk}@cs.biu.ac.il jeff@cs.huji.ac.il ABSTRACT Multiagent environments are often not cooperative nor collaborative; in many cases, agents have conflicting interests, leading to adversarial interactions.",
                "This paper presents a formal Adversarial Environment model for bounded rational agents operating in a zero-sum environment.",
                "In such environments, attempts to use classical utility-based search methods can raise a variety of difficulties (e.g., implicitly modeling the opponent as an omniscient utility maximizer, rather than leveraging a more nuanced, explicit opponent model).",
                "We define an Adversarial Environment by describing the mental states of an <br>agent</br> in such an environment.",
                "We then present behavioral axioms that are intended to serve as design principles for building such adversarial agents.",
                "We explore the application of our approach by analyzing log files of completed Connect-Four games, and present an empirical analysis of the axioms appropriateness.",
                "Categories and Subject Descriptors I.2.11 [Artificial Intelligence]: Distributed Artificial Intelligence-Intelligent agents,Multiagent Systems; I.2.4 [Artificial Intelligence]: Knowledge Representation Formalisms and Methods -Modal logic General Terms Design, Theory 1.",
                "INTRODUCTION Early research in multiagent systems (MAS) considered cooperative groups of agents; because individual agents had limited resources, or limited access to information (e.g., limited processing power, limited sensor coverage), they worked together by design to solve problems that individually they could not solve, or at least could not solve as efficiently.",
                "MAS research, however, soon began to consider interacting agents with individuated interests, as representatives of different humans or organizations with non-identical interests.",
                "When interactions are guided by diverse interests, participants may have to overcome disagreements, uncooperative interactions, and even intentional attempts to damage one another.",
                "When these types of interactions occur, environments require appropriate behavior from the agents situated in them.",
                "We call these environments Adversarial Environments, and call the clashing agents Adversaries.",
                "Models of cooperation and teamwork have been extensively explored in MAS through the axiomatization of mental states (e.g., [8, 4, 5]).",
                "However, none of this research dealt with adversarial domains and their implications for <br>agent</br> behavior.",
                "Our paper addresses this issue by providing a formal, axiomatized mental state model for a subset of adversarial domains, namely simple zero-sum adversarial environments.",
                "Simple zero-sum encounters exist of course in various twoplayer games (e.g., Chess, Checkers), but they also exist in n-player games (e.g., Risk, Diplomacy), auctions for a single good, and elsewhere.",
                "In these latter environments especially, using a utility-based adversarial search (such as the Min-Max algorithm) does not always provide an adequate solution; the payoff function might be quite complex or difficult to quantify, and there are natural computational limitations on bounded rational agents.",
                "In addition, traditional search methods (like Min-Max) do not make use of a model of the opponent, which has proven to be a valuable addition to adversarial planning [9, 3, 11].",
                "In this paper, we develop a formal, axiomatized model for bounded rational agents that are situated in a zero-sum adversarial environment.",
                "The model uses different modality operators, and its main foundations are the SharedPlans [4] model for collaborative behavior.",
                "We explore environment properties and the mental states of agents to derive behavioral axioms; these behavioral axioms constitute a formal model that serves as a specification and design guideline for <br>agent</br> design in such settings.",
                "We then investigate the behavior of our model empirically using the Connect-Four board game.",
                "We show that this game conforms to our environment definition, and analyze players behavior using a large set of completed match log 550 978-81-904262-7-5 (RPS) c 2007 IFAAMAS files.",
                "In addition, we use the results presented in [9] to discuss the importance of opponent modeling in our ConnectFour adversarial domain.",
                "The paper proceeds as follows.",
                "Section 2 presents the models formalization.",
                "Section 3 presents the empirical analysis and its results.",
                "We discuss related work in Section 4, and conclude and present future directions in Section 5. 2.",
                "ADVERSARIAL ENVIRONMENTS The adversarial environment model (denoted as AE) is intended to guide the design of agents by providing a specification of the capabilities and mental attitudes of an <br>agent</br> in an adversarial environment.",
                "We focus here on specific types of adversarial environments, specified as follows: 1.",
                "Zero-Sum Interactions: positive and negative utilities of all agents sum to zero; 2.",
                "Simple AEs: all agents in the environment are adversarial agents; 3.",
                "Bilateral AEs: AEs with exactly two agents; 4.",
                "Multilateral AEs: AEs of three or more agents.",
                "We will work on both bilateral and multilateral instantiations of zero-sum and simple environments.",
                "In particular, our adversarial environment model will deal with interactions that consist of N agents (N ≥ 2), where all agents are adversaries, and only one <br>agent</br> can succeed.",
                "Examples of such environments range from board games (e.g., Chess, Connect-Four, and Diplomacy) to certain economic environments (e.g., N-bidder auctions over a single good). 2.1 Model Overview Our approach is to formalize the mental attitudes and behaviors of a single adversarial <br>agent</br>; we consider how a single <br>agent</br> perceives the AE.",
                "The following list specifies the conditions and mental states of an <br>agent</br> in a simple, zero-sum AE: 1.",
                "The <br>agent</br> has an individual intention that its own goal will be completed; 2.",
                "The <br>agent</br> has an individual belief that it and its adversaries are pursuing full conflicting goals (defined below)there can be only one winner; 3.",
                "The <br>agent</br> has an individual belief that each adversary has an intention to complete its own full conflicting goal; 4.",
                "The <br>agent</br> has an individual belief in the (partial) profile of its adversaries.",
                "Item 3 is required, since it might be the case that some <br>agent</br> has a full conflicting goal, and is currently considering adopting the intention to complete it, but is, as of yet, not committed to achieving it.",
                "This might occur because the <br>agent</br> has not yet deliberated about the effects that adopting that intention might have on the other intentions it is currently holding.",
                "In such cases, it might not consider itself to even be in an adversarial environment.",
                "Item 4 states that the <br>agent</br> should hold some belief about the profiles of its adversaries.",
                "The profile represents all the knowledge the <br>agent</br> has about its adversary: its weaknesses, strategic capabilities, goals, intentions, trustworthiness, and more.",
                "It can be given explicitly or can be learned from observations of past encounters. 2.2 Model Definitions for Mental States We use Grosz and Krauss definitions of the modal operators, predicates, and meta-predicates, as defined in their SharedPlan formalization [4].",
                "We recall here some of the predicates and operators that are used in that formalization: Int.To(Ai, α, Tn, Tα, C) represents Ais intentions at time Tn to do an action α at time Tα in the context of C. Int.Th(Ai, prop, Tn, Tprop, C) represents Ais intentions at time Tn that a certain proposition prop holds at time Tprop in the context of C. The potential intention operators, Pot.Int.To(...) and Pot.Int.Th(...), are used to represent the mental state when an <br>agent</br> considers adopting an intention, but has not deliberated about the interaction of the other intentions it holds.",
                "The operator Bel(Ai, f, Tf ) represents <br>agent</br> Ai believing in the statement expressed in formula f, at time Tf .",
                "MB(A, f, Tf ) represents mutual belief for a group of agents A.",
                "A snapshot of the system finds our environment to be in some state e ∈ E of environmental variable states, and each adversary in any LAi ∈ L of possible local states.",
                "At any given time step, the system will be in some world w of the set of all possible worlds w ∈ W, where w = E×LA1 ×LA2 × ...LAn , and n is the number of adversaries.",
                "For example, in a Texas Holdem poker game, an agents local state might be its own set of cards (which is unknown to its adversary) while the environment will consist of the betting pot and the community cards (which are visible to both players).",
                "A utility function under this formalization is defined as a mapping from a possible world w ∈ W to an element in , which expresses the desirability of the world, from a single <br>agent</br> perspective.",
                "We usually normalize the range to [0,1], where 0 represents the least desirable possible world, and 1 is the most desirable world.",
                "The implementation of the utility function is dependent on the domain in question.",
                "The following list specifies new predicates, functions, variables, and constants used in conjunction with the original definitions for the adversarial environment formalization: 1. φ is a null action (the <br>agent</br> does not do anything). 2.",
                "GAi is the set of <br>agent</br> Ais goals.",
                "Each goal is a set of predicates whose satisfaction makes the goal complete (we use G∗ Ai ∈ GAi to represent an arbitrary goal of <br>agent</br> Ai). 3. gAi is the set of <br>agent</br> Ais subgoals.",
                "Subgoals are predicates whose satisfaction represents an important milestone toward achievement of the full goal. gG∗ Ai ⊆ gAi is the set of subgoals that are important to the completion of goal G∗ Ai (we will use g∗ G∗ Ai ∈ gG∗ Ai to represent an arbitrary subgoal). 4.",
                "P Aj Ai is the profile object <br>agent</br> Ai holds about <br>agent</br> Aj. 5.",
                "CA is a general set of actions for all agents in A which are derived from the environments constraints.",
                "CAi ⊆ CA is the set of <br>agent</br> Ais possible actions. 6.",
                "Do(Ai, α, Tα, w) holds when Ai performs action α over time interval Tα in world w. 7.",
                "Achieve(G∗ Ai , α, w) is true when goal G∗ Ai is achieved following the completion of action α in world w ∈ W, where α ∈ CAi . 8.",
                "Profile(Ai, PAi Ai ) is true when <br>agent</br> Ai holds an object profile for <br>agent</br> Aj.",
                "Definition 1.",
                "Full conflict (FulConf ) describes a zerosum interaction where only a single goal of the goals in conflict can be completed.",
                "FulConf(G∗ Ai , G∗ Aj ) ⇒ (∃α ∈ CAi , ∀w, β ∈ CAj ) (Achieve(G∗ Ai , α, w) ⇒ ¬Achieve(G∗ Aj , β, w)) ∨ (∃β ∈ CAj , ∀w, α ∈ CAi )(Achieve(G∗ Aj , β, w) ⇒ ¬Achieve(G∗ Ai , α, w)) Definition 2.",
                "Adversarial Knowledge (AdvKnow) is a function returning a value which represents the amount of The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-<br>agent</br> Systems (AAMAS 07) 551 knowledge <br>agent</br> Ai has on the profile of agent Aj, at time Tn.",
                "The higher the value, the more knowledge <br>agent</br> Ai has.",
                "AdvKnow : P Aj Ai × Tn → Definition 3.",
                "Eval - This evaluation function returns an estimated expected utility value for an <br>agent</br> in A, after completing an action from CA in some world state w. Eval : A × CA × w → Definition 4.",
                "TrH - (Threshold) is a numerical constant in the [0,1] range that represents an evaluation function (Eval) threshold value.",
                "An action that yields an estimated utility evaluation above the TrH is regarded as a highly beneficial action.",
                "The Eval value is an estimation and not the real utility function, which is usually unknown.",
                "Using the real utility value for a rational <br>agent</br> would easily yield the best outcome for that <br>agent</br>.",
                "However, agents usually do not have the real utility functions, but rather a heuristic estimate of it.",
                "There are two important properties that should hold for the evaluation function: Property 1.",
                "The evaluation function should state that the most desirable world state is one in which the goal is achieved.",
                "Therefore, after the goal has been satisfied, there can be no future action that can put the <br>agent</br> in a world state with higher Eval value. (∀Ai, G∗ Ai , α, β ∈ CAi , w ∈ W) Achieve(G∗ Ai , α, w) ⇒ Eval(Ai, α, w) ≥ Eval(Ai, β, w) Property 2.",
                "The evaluation function should project an action that causes a completion of a goal or a subgoal to a value which is greater than TrH (a highly beneficial action). (∀Ai, G∗ Ai ∈ GAi , α ∈ CAi , w ∈ W, g∗ GAi ∈ gGAi ) Achieve(G∗ Ai , α, w) ∨ Achieve(g∗ GAi , α, w) ⇒ Eval(Ai, α, w) ≥ TrH.",
                "Definition 5.",
                "SetAction We define a set action (SetAction) as a set of action operations (either complex or basic actions) from some action sets CAi and CAj which, according to <br>agent</br> Ais belief, are attached together by a temporal and consequential relationship, forming a chain of events (action, and its following consequent action). (∀α1 , . . . , αu ∈ CAi , β1 , . . . , βv ∈ CAj , w ∈ W) SetAction(α1 , . . . , αu , β1 , . . . , βv , w) ⇒ ((Do(Ai, α1 , Tα1 , w) ⇒ Do(Aj, β1 , Tβ1 , w)) ⇒ Do(Ai, α2 , Tα2 , w) ⇒ . . . ⇒ Do(Ai, αu , Tαu , w)) The consequential relation might exist due to various environmental constraints (when one action forces the adversary to respond with a specific action) or due to the agents knowledge about the profile of its adversary.",
                "Property 3.",
                "As the knowledge we have about our adversary increases we will have additional beliefs about its behavior in different situations which in turn creates new set actions.",
                "Formally, if our AdvKnow at time Tn+1 is greater than AdvKnow at time Tn, then every SetAction known at time Tn is also known at time Tn+1.",
                "AdvKnow(P Aj Ai , Tn+1) > AdvKnow(P Aj Ai , Tn) ⇒ (∀α1 , . . . , αu ∈ CAi , β1 , . . . , βv ∈ CAj ) Bel(Aag, SetAction(α1 , . . . , αu , β1 , . . . , βv ), Tn) ⇒ Bel(Aag, SetAction(α1 , . . . , αu , β1 , . . . , βv ), Tn+1) 2.3 The Environment Formulation The following axioms provide the formal definition for a simple, zero-sum Adversarial Environment (AE).",
                "Satisfaction of these axioms means that the <br>agent</br> is situated in such an environment.",
                "It provides specifications for <br>agent</br> Aag to interact with its set of adversaries A with respect to goals G∗ Aag and G∗ A at time TCo at some world state w. AE(Aag, A, G∗ Aag , A1, . . . , Ak, G∗ A1 , . . . , G∗ Ak , Tn, w) 1.",
                "Aag has an Int.Th his goal would be completed: (∃α ∈ CAag , Tα) Int.Th(Aag, Achieve(G∗ Aag , α), Tn, Tα, AE) 2.",
                "Aag believes that it and each of its adversaries Ao are pursuing full conflicting goals: (∀Ao ∈ {A1, . . . , Ak}) Bel(Aag, FulConf(G∗ Aag , G∗ Ao ), Tn) 3.",
                "Aag believes that each of his adversaries in Ao has the Int.Th his conflicting goal G∗ Aoi will be completed: (∀Ao ∈ {A1, . . . , Ak})(∃β ∈ CAo , Tβ) Bel(Aag, Int.Th(Ao, Achieve(G∗ Ao , β), TCo, Tβ, AE), Tn) 4.",
                "Aag has beliefs about the (partial) profiles of its adversaries (∀Ao ∈ {A1, . . . , Ak}) (∃PAo Aag ∈ PAag )Bel(Aag, Profile(Ao, PAo Aag ), Tn) To build an <br>agent</br> that will be able to operate successfully within such an AE, we must specify behavioral guidelines for its interactions.",
                "Using a naive Eval maximization strategy to a certain search depth will not always yield satisfactory results for several reasons: (1) the search horizon problem when searching for a fixed depth; (2) the strong assumption of an optimally rational, unbounded resources adversary; (3) using an estimated evaluation function which will not give optimal results in all world states, and can be exploited [9].",
                "The following axioms specify the behavioral principles that can be used to differentiate between successful and less successful agents in the above Adversarial Environment.",
                "Those axioms should be used as specification principles when designing and implementing agents that should be able to perform well in such Adversarial Environments.",
                "The behavioral axioms represent situations in which the <br>agent</br> will adopt potential intentions to (Pot.Int.To(...)) perform an action, which will typically require some means-end reasoning to select a possible course of action.",
                "This reasoning will lead to the adoption of an Int.To(...) (see [4]).",
                "A1.",
                "Goal Achieving Axiom.",
                "The first axiom is the simplest case; when the <br>agent</br> Aag believes that it is one action (α) away from achieving his conflicting goal G∗ Aag , it should adopt the potential intention to do α and complete its goal. (∀Aag, α ∈ CAag , Tn, Tα, w ∈ W) (Bel(Aag, Do(Aag, α, Tα, w) ⇒ Achieve(G∗ Aag , α, w)) ⇒ Pot.Int.To(Aag, α, Tn, Tα, w) This somewhat trivial behavior is the first and strongest axiom.",
                "In any situation, when the <br>agent</br> is an action away from completing the goal, it should complete the action.",
                "Any fair Eval function would naturally classify α as the maximal value action (property 1).",
                "However, without explicit axiomatization of such behavior there might be situations where the <br>agent</br> will decide on taking another action for various reasons, due to its bounded decision resources.",
                "A2.",
                "Preventive Act Axiom.",
                "Being in an adversarial situation, <br>agent</br> Aag might decide to take actions that will damage one of its adversarys plans to complete its goal, even if those actions do not explicitly advance Aag towards its conflicting goal G∗ Aag .",
                "Such preventive action will take place when <br>agent</br> Aag has a belief about the possibility of its adversary Ao doing an action β that will give it a high 552 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-<br>agent</br> Systems (AAMAS 07) utility evaluation value (> TrH).",
                "Believing that taking action α will prevent the opponent from doing its β, it will adopt a potential intention to do α. (∀Aag, Ao ∈ A, α ∈ CAag , β ∈ CAo , Tn, Tβ, w ∈ W) (Bel(Aag, Do(Ao, β, Tβ, w) ∧ Eval(Ao, β, w) > TrH, Tn) ∧ Bel(Aag, Do(Aag, α, Tα, w) ⇒ ¬Do(Ao, β, Tβ, w), Tn) ⇒ Pot.Int.To(Aag, α, Tn, Tα, w) This axiom is a basic component of any adversarial environment.",
                "For example, looking at a Chess board game, a player could realize that it is about to be checkmated by its opponent, thus making a preventive move.",
                "Another example is a Connect Four game: when a player has a row of three chips, its opponent must block it, or lose.",
                "A specific instance of A1 occurs when the adversary is one action away from achieving its goal, and immediate preventive action needs to be taken by the <br>agent</br>.",
                "Formally, we have the same beliefs as stated above, with a changed belief that doing action β will cause <br>agent</br> Ao to achieve its goal.",
                "Proposition 1: Prevent or lose case. (∀Aag, Ao ∈ A, α ∈ CAag , β ∈ CAo , G∗ Ao , Tn, Tα, Tβ, w ∈ W) Bel(Aag, Do(Ao, β, Tβ, w) ⇒ Achieve(G∗ Ao , β, w), Tn) ∧ Bel(Aag, Do(Aag, α, Tα, w) ⇒ ¬Do(Ao, β, Tβ, w)) ⇒ Pot.Int.To(Aag, α, Tn, Tα, w) Sketch of proof: Proposition 1 can be easily derived from axiom A1 and the property 2 of the Eval function, which states that any action that causes a completion of a goal is a highly beneficial action.",
                "The preventive act behavior will occur implicitly when the Eval function is equal to the real world utility function.",
                "However, being bounded rational agents and dealing with an estimated evaluation function we need to explicitly axiomatize such behavior, for it will not always occur implicitly from the evaluation function.",
                "A3.",
                "Suboptimal Tactical Move Axiom.",
                "In many scenarios a situation may occur where an <br>agent</br> will decide not to take the current most beneficial action it can take (the action with the maximal utility evaluation value), because it believes that taking another action (with lower utility evaluation value) might yield (depending on the adversarys response) a future possibility for a highly beneficial action.",
                "This will occur most often when the Eval function is inaccurate and differs by a large extent from the Utility function.",
                "Put formally, <br>agent</br> Aag believes in a certain SetAction that will evolve according to its initial action and will yield a high beneficial value (> TrH) solely for it. (∀Aag, Ao ∈ A, Tn, w ∈ W) (∃α1 , . . . , αu ∈ CAi , β1 , . . . , βv ∈ CAj , Tα1 ) Bel(Aag, SetAction(α1 , . . . , αu , β1 , . . . , βv ), Tn) ∧ Bel(Aag, Eval(Ao, βv , w) < TrH < Eval(Aag, αu , w), Tn) ⇒ Pot.Int.To(Aag, α1 , Tn, Tα1 , w) An <br>agent</br> might believe that a chain of events will occur for various reasons due to the inevitable nature of the domain.",
                "For example, in Chess, we often observe the following: a move causes a check position, which in turn limits the opponents moves to avoiding the check, to which the first player might react with another check, and so on.",
                "The <br>agent</br> might also believe in a chain of events based on its knowledge of its adversarys profile, which allows it to foresee the adversarys movements with high accuracy.",
                "A4.",
                "Profile Detection Axiom.",
                "The <br>agent</br> can adjust its adversarys profiles by observations and pattern study (specifically, if there are repeated encounters with the same adversary).",
                "However, instead of waiting for profile information to be revealed, an <br>agent</br> can also initiate actions that will force its adversary to react in a way that will reveal profile knowledge about it.",
                "Formally, the axiom states that if all actions (γ) are not highly beneficial actions (< TrH), the <br>agent</br> can do action α in time Tα if it believes that it will result in a non-highly beneficial action β from its adversary, which in turn teaches it about the adversarys profile, i.e., gives a higher AdvKnow(P Aj Ai , Tβ). (∀Aag, Ao ∈ A, α ∈ CAag , β ∈ CAo , Tn, Tα, Tβ, w ∈ W) Bel(Aag, (∀γ ∈ CAag )Eval(Aag, γ, w) < TrH, Tn) ∧ Bel(Aag, Do(Aag, α, Tα, w) ⇒ Do(Ao, β, Tβ, w), Tn) ∧ Bel(Aag, Eval(Ao, β, w) < TrH) ∧ Bel(Aag, AdvKnow(P Aj Ai , Tβ) > AdvKnow(P Aj Ai , Tn), Tn) ⇒ Pot.Int.To(Aag, α, Tn, Tα, w) For example, going back to the Chess board game scenario, consider starting a game versus an opponent about whom we know nothing, not even if it is a human or a computerized opponent.",
                "We might start playing a strategy that will be suitable versus an average opponent, and adjust our game according to its level of play.",
                "A5.",
                "Alliance Formation Axiom The following behavioral axiom is relevant only in a multilateral instantiation of the adversarial environment (obviously, an alliance cannot be formed in a bilateral, zero-sum encounter).",
                "In different situations during a multilateral interaction, a group of agents might believe that it is in their best interests to form a temporary alliance.",
                "Such an alliance is an agreement that constrains its members behavior, but is believed by its members to enable them to achieve a higher utility value than the one achievable outside of the alliance.",
                "As an example, we can look at the classical Risk board game, where each player has an individual goal of being the sole conquerer of the world, a zero-sum game.",
                "However, in order to achieve this goal, it might be strategically wise to make short-term ceasefire agreements with other players, or to join forces and attack an opponent who is stronger than the rest.",
                "An alliances terms defines the way its members should act.",
                "It is a set of predicates, denoted as Terms, that is agreed upon by the alliance members, and should remain true for the duration of the alliance.",
                "For example, the set Terms in the Risk scenario, could contain the following predicates: 1.",
                "Alliance members will not attack each other on territories X, Y and Z; 2.",
                "Alliance members will contribute C units per turn for attacking adversary Ao; 3.",
                "Members are obligated to stay as part of the alliance until time Tk or until adversarys Ao army is smaller than Q.",
                "The set Terms specifies inter-group constraints on each of the alliance members (∀Aal i ∈ Aal ⊆ A) set of actions Cal i ⊆ C. Definition 6.",
                "Al val - the total evaluation value that <br>agent</br> Ai will achieve while being part of Aal is the sum of Evali (Eval for Ai) of each of Aal j Eval values after taking their own α actions (via the <br>agent</br>(α) predicate): Al val(Ai, Cal , Aal , w) = α∈Cal Evali(Aal j , agent(α), w) Definition 7.",
                "Al TrH - is a number representing an Al val The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-<br>agent</br> Systems (AAMAS 07) 553 threshold; above it, the alliance can be said to be a highly beneficial alliance.",
                "The value of Al TrH will be calculated dynamically according to the progress of the interaction, as can be seen in [7].",
                "After an alliance is formed, its members are now working in their normal adversarial environment, as well as according to the mental states and axioms required for their interactions as part of the alliance.",
                "The following Alliance model (AL) specifies the conditions under which the group Aal can be said to be in an alliance and working with a new and constrained set of actions Cal , at time Tn.",
                "AL(Aal , Cal , w, Tn) 1.",
                "Aal has a MB that all members are part of Aal : MB(Aal , (∀Aal i ∈ Aal )member(Aal i , Aal ), Tn) 2.",
                "Aal has a MB that the group be maintained: MB(Aal , (∀Aal i ∈ Aal )Int.Th (Ai, member(Ai, Aal ), Tn, Tn+1, Co), Tn) 3.",
                "Aal has a MB that being members gives them high utility value: MB(Aal , (∀Aal i ∈ Aal )Al val(Aal i , Cal , Aal , w) ≥ Al TrH, Tn) Members profiles are a crucial part of successful alliances.",
                "We assume that agents that have more accurate profiles of their adversaries will be more successful in such environments.",
                "Such agents will be able to predict when a member is about to breach the alliances contract (item 2 in the above model), and take counter measures (when item 3 will falsify).",
                "The robustness of the alliance is in part a function of its members trustfulness measure, objective position estimation, and other profile properties.",
                "We should note that an <br>agent</br> can simultaneously be part of more than one alliance.",
                "Such a temporary alliance, where the group members do not have a joint goal but act collaboratively for the interest of their own individual goals, is classified as a Treatment Group by modern psychologists [12] (in contrast to a Task Group, where its members have a joint goal).",
                "The Shared Activity model as presented in [5] modeled Treatment Group behavior using the same SharedPlans formalization.",
                "When comparing both definitions of an alliance and a Treatment Group we found an unsurprising resemblance between both models: the environment models definitions are almost identical (see SAs definitions in [5]), and their Selfish-Act and Cooperative Act axioms conform to our adversarial agents behavior.",
                "The main distinction between both models is the integration of a Helpful-behavior act axiom, in the Shared Activity which cannot be part of ours.",
                "This axiom states that an <br>agent</br> will consider taking action that will lower its Eval value (to a certain lower bound), if it believes that a group partner will gain a significant benefit.",
                "Such behavior cannot occur in a pure adversarial environment (as a zero-sum game is), where the alliance members are constantly on watch to manipulate their alliance to their own advantage.",
                "A6.",
                "Evaluation Maximization Axiom.",
                "In a case when all other axioms are inapplicable, we will proceed with the action that maximizes the heuristic value as computed in the Eval function. (∀Aag, Ao ∈ A, α ∈ Cag, Tn, w ∈ W) Bel(Aag, (∀γ ∈ Cag)Eval(Aag, α, w) ≥ Eval(Aag, γ, w), Tn) ⇒ Pot.Int.To(Aag, α, Tn, Tα, w) T1.",
                "Optimality on Eval = Utility The above axiomatic model handles situations where the Utility is unknown and the agents are bounded rational agents.",
                "The following theorem shows that in bilateral interactions, where the agents have the real Utility function (i.e., Eval = Utility) and are rational agents, the axioms provide the same optimal result as classic adversarial search (e.g., Min-Max).",
                "Theorem 1.",
                "Let Ae ag be an unbounded rational AE <br>agent</br> using the Eval heuristic evaluation function, Au ag be the same <br>agent</br> using the true Utility function, and Ao be a sole unbounded utility-based rational adversary.",
                "Given that Eval = Utility: (∀α ∈ CAu ag , α ∈ CAe ag , Tn, w ∈ W) Pot.Int.To(Au ag, α, Tn, Tα, w) → Pot.Int.To(Ae ag, α , Tn, Tα, w) ∧ ((α = α ) ∨ (Utility(Au ag, α, w) = Eval(Ae ag, α , w))) Sketch of proof - Given that Au ag has the real utility function and unbounded resources, it can generate the full game tree and run the optimal MinMax algorithm to choose the highest utility value action, which we denote by, α.",
                "The proof will show that Ae ag, using the AE axioms, will select the same or equal utility α (when there is more than one action with the same max utility) when Eval = Utility. (A1) Goal achieving axiom - suppose there is an α such that its completion will achieve Au ags goal.",
                "It will obtain the highest utility by Min-Max for Au ag.",
                "The Ae ag <br>agent</br> will select α or another action with the same utility value via A1.",
                "If such α does not exist, Ae ag cannot apply this axiom, and proceeds to A2. (A2) Preventive act axiom - (1) Looking at the basic case (see Prop1), if there is a β which leads Ao to achieve its goal, then a preventive action α will yield the highest utility for Au ag.",
                "Au ag will choose it through the utility, while Ae ag will choose it through A2. (2) In the general case, β is a highly beneficial action for Ao, thus yields low utility for Au ag, which will guide it to select an α that will prevent β, while Ae ag will choose it through A2.1 If such β does not exist for Ao, then A2 is not applicable, and Ae ag can proceed to A3. (A3) Suboptimal tactical move axiom - When using a heuristic Eval function, Ae ag has a partial belief in the profile of its adversary (item 4 in AE model), which may lead it to believe in SetActions (Prop1).",
                "In our case, Ae ag is holding a full profile on its optimal adversary and knows that Ao will behave optimally according to the real utility values on the complete search tree, therefore, any belief about suboptimal SetAction cannot exist, yielding this axiom inapplicable.",
                "Ae ag will proceed to A4. (A4) Profile detection axiom - Given that Ae ag has the full profile of Ao, none of Ae ags actions can increase its knowledge.",
                "That axiom will not be applied, and the <br>agent</br> will proceed with A6 (A5 will be disregarded because the interaction is bilateral). (A6) Evaluation maximization axiom - This axiom will select the max Eval for Ae ag.",
                "Given that Eval = Utility, the same α that was selected by Au ag will be selected. 3.",
                "EVALUATION The main purpose of our experimental analysis is to evaluate the models behavior and performance in a real adversarial environment.",
                "This section investigates whether bounded 1 A case where following the completion of β there exists a γ which gives high utility for <br>agent</br> Au ag, cannot occur because Ao uses the same utility, and γs existence will cause it to classify β as a low utility action. 554 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-<br>agent</br> Systems (AAMAS 07) rational agents situated in such adversarial environments will be better off applying our suggested behavioral axioms. 3.1 The Domain To explore the use of the above model and its behavioral axioms, we decided to use the Connect-Four game as our adversarial environment.",
                "Connect-Four is a 2-player, zerosum game which is played using a 6x7 matrix-like board.",
                "Each turn, a player drops a disc into one of the 7 columns (the set of 21 discs is usually colored yellow for player 1 and red for player 2; we will use White and Black respectively to avoid confusion).",
                "The winner is the first player to complete a horizontal, vertical, or diagonal set of four discs with its color.",
                "On very rare occasions, the game might end in a tie if all the empty grids are filled, but no player managed to create a 4-disc set.",
                "The Connect-Four game was solved in [1], where it is shown that the first player (playing with the white discs) can force a win by starting in the middle column (column 4) and playing optimally However, the optimal strategy is very complex, and difficult to follow even for complex bounded rational agents, such as human players.",
                "Before we can proceed checking <br>agent</br> behavior, we must first verify that the domain conforms to the adversarial environments definition as given above (which the behavioral axioms are based on).",
                "First, when playing a Connect-Four game, the <br>agent</br> has an intention to win the game (item 1).",
                "Second (item 2), our <br>agent</br> believes that in Connect-Four there can only be one winner (or no winner at all in the rare occurrence of a tie).",
                "In addition, our <br>agent</br> believes that its opponent to the game will try to win (item 3), and we hope it has some partial knowledge (item 4) about its adversary (this knowledge can vary from nothing, through simple facts such as age, to strategies and weaknesses).",
                "Of course, not all Connect-Four encounters are adversarial.",
                "For example, when a parent is playing the game with its child, the following situation might occur: the child, having a strong incentive to win, treats the environment as adversarial (it intends to win, understands that there can only be one winner, and believes that its parent is trying to beat him).",
                "However, the parents point of view might see the environment as an educational one, where its goal is not to win the game, but to cause enjoyment or practice strategic reasoning.",
                "In such an educational environment, a new set of behavioral axioms might be more beneficial to the parents goals than our suggested adversarial behavioral axioms. 3.2 Axiom Analysis After showing that the Connect-Four game is indeed a zero-sum, bilateral adversarial environment, the next step is to look at players behaviors during the game and check whether behaving according to our model does improve performance.",
                "To do so we have collected log files from completed Connect-Four games that were played by human players over the Internet.",
                "Our collected log file data came from Play by eMail (PBeM) sites.",
                "These are web sites that host email games, where each move is taken by an email exchange between the server and the players.",
                "Many such sites archives contain real competitive interactions, and also maintain a ranking system for their members.",
                "Most of the data we used can be found in [6].",
                "As can be learned from [1], Connect-Four has an optimal strategy and a considerable advantage for the player who starts the game (which we call the White player).",
                "We will concentrate in our analysis on the second players moves (to be called Black).",
                "The White player, being the first to act, has the so-called initiative advantage.",
                "Having the advantage and a good strategy will keep the Black player busy reacting to its moves, instead of initiating threats.",
                "A threat is a combination of three discs of the same color, with an empty spot for the fourth winning disk.",
                "An open threat is a threat that can be realized in the opponents next move.",
                "In order for the Black player to win, it must somehow turn the tide, take the advantage and start presenting threats to the White player.",
                "We will explore Black players behavior and their conformance to our axioms.",
                "To do so, we built an application that reads log files and analyzes the Black players moves.",
                "The application contains two main components: (1) a Min-Max algorithm for evaluation of moves; (2) open threats detector for the discovering of open threats.",
                "The Min-Max algorithm will work to a given depth, d and for each move α will output the heuristic value for the next action taken by the player as written in the log file, h(α), alongside the maximum heuristic value, maxh(α), that could be achieved prior to taking the move (obviously, if h(α) = maxh(α), then the player did not do the optimal move heuristically).",
                "The threat detectors job is to notify if some action was taken in order to block an open threat (not blocking an open threat will probably cause the player to lose in the opponents next move).",
                "The heuristic function used by Min-Max to evaluate the players utility is the following function, which is simple to compute, yet provides a reasonable challenge to human opponents: Definition 8.",
                "Let Group be an adjacent set of four squares that are horizontal, vertical, or diagonal.",
                "Groupn b (Groupn w) be a Group with n pieces of the black (white) color and 4−n empty squares. h = ((Group1 b ∗α)+(Group2 b ∗β)+(Group3 b ∗γ)+(Group4 b ∗∞)) − ((Group1 w ∗α)+(Group2 w ∗β)+(Group3 w ∗γ)+(Group4 w ∗∞)) The values of α, β and δ can vary to form any desired linear combination; however, it is important to value them with the α < β < δ ordering in mind (we used 1, 4, and 8 as their respective values).",
                "Groups of 4 discs of the same color means victory, thus discovery of such a group will result in ∞ to ensure an extreme value.",
                "We now use our estimated evaluation function to evaluate the Black players actions during the Connect-Four adversarial interaction.",
                "Each game from the log file was input into the application, which processed and output a reformatted log file containing the h value of the current move, the maxh value that could be achieved, and a notification if an open threat was detected.",
                "A total of 123 games were analyzed (57 with White winning, and 66 with Black winning).",
                "A few additional games were manually ignored in the experiment, due to these problems: a player abandoning the game while the outcome is not final, or a blunt irrational move in the early stages of the game (e.g., not blocking an obvious winning group in the first opening moves).",
                "In addition, a single tie game was also removed.",
                "The simulator was run to a search depth of 3 moves.",
                "We now proceed to analyze the games with respect to each behavioral axiom.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-<br>agent</br> Systems (AAMAS 07) 555 Table 1: Average heuristic difference analysis Black losses Black Won Avg minh -17.62 -12.02 Avg 3 lowest h moves (min3 h) -13.20 -8.70 3.2.1 Affirming the Suboptimal tactical move axiom The following section presents the heuristic evaluations of the Min-Max algorithm for each action, and checks the amount and extent of suboptimal tactical actions and their implications on performance.",
                "Table 1 shows results and insights from the games heuristic analysis, when search depth equals 3 (this search depth was selected for the results to be comparable to [9], see Section 3.2.3).",
                "The tables heuristic data is the difference between the present maximal heuristic value and the heuristic value of the action that was eventually taken by the player (i.e., the closer the number is to 0, the closer the action was to the maximum heuristic action).",
                "The first row presents the difference values of the action that had the maximal difference value among all the Black players actions in a given game, as averaged over all Blacks winning and losing games (see respective columns).",
                "In games in which the Black player loses, its average difference value was -17.62, while in games in which the Black player won, its average was -12.02.",
                "The second row expands the analysis by considering the 3 highest heuristic difference actions, and averaging them.",
                "In that case, we notice an average heuristic difference of 5 points between games which the Black player loses and games in which it wins.",
                "Nevertheless, the importance of those numbers is that they allowed us to take an educated guess on a threshold number of 11.5, as the value of the TrH constant, which differentiates between normal actions and highly beneficial ones.",
                "After finding an approximated TrH constant, we can proceed with an analysis of the importance of suboptimal moves.",
                "To do so we took the subset of games in which the minimum heuristic difference value for Blacks actions was 11.5.",
                "As presented in Table 2, we can see the different min3 h average of the 3 largest ranges and the respective percentage of games won.",
                "The first row shows that the Black player won only 12% of the games in which the average of its 3 highest heuristically difference actions (min3 h) was smaller than the suggested threshold, TrH = 11.5.",
                "The second row shows a surprising result: it seems that when min3 h > −4 the Black player rarely wins.",
                "Intuition would suggest that games in which the action evaluation values were closer to the maximal values will result in more winning games for Black.",
                "However, it seems that in the Connect-Four domain, merely responding with somewhat easily expected actions, without initiating a few surprising and suboptimal moves, does not yield good results.",
                "The last row sums up the main insights from the analysis; most of Blacks wins (83%) came when its min3 h was in the range of -11.5 to -4.",
                "A close inspection of those Black winning games shows the following pattern behind the numbers: after standard opening moves, Black suddenly drops a disc into an isolated column, which seems a waste of a move.",
                "White continues to build its threats, while usually disregarding Blacks last move, which in turn uses the isolated disc as an anchor for a future winning threat.",
                "The results show that it was beneficial for the Black player Table 2: Blacks winnings percentages % of games min3 h < −11.5 12% min3 h > −4 5% −11.5 ≤ min3 h ≤ −4 83% to take suboptimal actions and not give the current highest possible heuristic value, but will not be too harmful for its position (i.e., will not give high beneficial value to its adversary).",
                "As it turned out, learning the threshold is an important aspect of success: taking wildly risky moves (min3 h < −11.5) or trying to avoid them (min3 h > −4) reduces the Black players winning chances by a large margin. 3.2.2 Affirming the Profile Monitoring Axiom In the task of showing the importance of monitoring ones adversaries profiles, our log files could not be used because they did not contain repeated interactions between players, which are needed to infer the players knowledge about their adversaries.",
                "However, the importance of opponent modeling and its use in attaining tactical advantages was already studied in various domains ([3, 9] are good examples).",
                "In a recent paper, Markovitch and Reger [9] explored the notion of learning and exploitation of opponent weakness in competitive interactions.",
                "They apply simple learning strategies by analyzing examples from past interactions in a specific domain.",
                "They also used the Connect-Four adversarial domain, which can now be used to understand the importance of monitoring the adversarys profile.",
                "Following the presentation of their theoretical model, they describe an extensive empirical study and check the agents performance after learning the weakness model with past examples.",
                "One of the domains used as a competitive environment was the same Connect-Four game (Checkers was the second domain).",
                "Their heuristic function was identical to ours with three different variations (H1, H2, and H3) that are distinguished from one another in their linear combination coefficient values.",
                "The search depth for the players was 3 (as in our analysis).",
                "Their extensive experiments check and compare various learning strategies, risk factors, predefined feature sets and usage methods.",
                "The bottom line is that the Connect-Four domain shows an improvement from a 0.556 winning rate before modeling to a 0.69 after modeling (page 22).",
                "Their conclusions, showing improved performance when holding and using the adversarys model, justify the effort to monitor the adversary profile for continuous and repeated interactions.",
                "An additional point that came up in their experiments is the following: after the opponent weakness model has been learned, the authors describe different methods of integrating the opponent weakness model into the agents decision strategy.",
                "Nevertheless, regardless of the specific method they chose to work with, all integration methods might cause the <br>agent</br> to take suboptimal decisions; it might cause the <br>agent</br> to prefer actions that are suboptimal at the present decision junction, but which might cause the opponent to react in accordance with its weakness model (as represented by our agent) which in turn will be beneficial for us in the future.",
                "The agents behavior, as demonstrated in [9] further confirms and strengthens our Suboptimal Tactical Axiom as discussed in the previous section. 556 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-<br>agent</br> Systems (AAMAS 07) 3.2.3 Additional Insights The need for the Goal Achieving, Preventive Act, and Evaluation Maximization axioms are obvious, and need no further verification.",
                "However, even with respect to those axioms, a few interesting insights came up in the log analysis.",
                "The Goal achieving and Preventive Act axioms, though theoretically trivial, seem to provide some challenge to a human player.",
                "In the initial inspection of the logs, we encountered few games2 where a player, for inexplicable reasons, did not block the other from winning or failed to execute its own winning move.",
                "We can blame those faults on the humans lack of attention, or a typing error in its move reply; nevertheless, those errors might occur in bounded rational agents, and the appropriate behavior needs to be axiomatized.",
                "A typical Connect-Four game revolves around generating threats and blocking them.",
                "In our analysis we looked for explicit preventive actions, i.e., moves that block a group of 3 discs, or that remove a future threat (in our limited search horizon).",
                "We found that in 83% of the total games there was at least one preventive action taken by the Black player.",
                "It was also found that Black averaged 2.8 preventive actions per game on the games in which it lost, while averaging 1.5 preventive actions per game when winning.",
                "It seems that Black requires 1 or 2 preventive actions to build its initial taking position, before starting to present threats.",
                "If it did not manage to win, it will usually prevent an extra threat or two before succumbing to White. 4.",
                "RELATED WORK Much research deals with the axiomatization of teamwork and mental states of individuals: some models use knowledge and belief [10], others have models of goals and intentions [8, 4].",
                "However, all these formal theories deal with <br>agent</br> teamwork and cooperation.",
                "As far as we know, our model is the first to provide a formalized model for explicit adversarial environments and agents behavior in it.",
                "The classical Min-Max adversarial search algorithm was the first attempt to integrate the opponent into the search space with a weak assumption of an optimally playing opponent.",
                "Since then, much effort has gone into integrating the opponent model into the decision procedure to predict future behavior.",
                "The M∗ algorithm presented by Carmel and Markovitch [2] showed a method of incorporating opponent models into adversary search, while in [3] they used learning to provide a more accurate opponent model in a 2player repeated game environment, where agents strategies were modeled as finite automata.",
                "Additional Adversarial planning work was done by Willmott et al. [13], which provided an adversarial planning approach to the game of GO.",
                "The research mentioned above dealt with adversarial search and the integration of opponent models into classical utilitybased search methods.",
                "That work shows the importance of opponent modeling and the ability to exploit it to an agents advantage.",
                "However, the basic limitations of those search methods still apply; our model tries to overcome those limitations by presenting a formal model for a new, mental state-based adversarial specification. 5.",
                "CONCLUSIONS We presented an Adversarial Environment model for a 2 These were later removed from the final analysis. bounded rational <br>agent</br> that is situated in an N-player, zerosum environment.",
                "We used the SharedPlans formalization to define the model and the axioms that agents can apply as behavioral guidelines.",
                "The model is meant to be used as a guideline for designing agents that need to operate in such adversarial environments.",
                "We presented empirical results, based on ConnectFour log file analysis, that exemplify the model and the axioms for a bilateral instance of the environment.",
                "The results we presented are a first step towards an expanded model that will cover all types of adversarial environments, for example, environments that are non-zero-sum, and environments that contain natural agents that are not part of the direct conflict.",
                "Those challenges and more will be dealt with in future research. 6.",
                "ACKNOWLEDGMENT This research was supported in part by Israel Science Foundation grants #1211/04 and #898/05. 7.",
                "REFERENCES [1] L. V. Allis.",
                "A knowledge-based approach of Connect-Four - the game is solved: White wins.",
                "Masters thesis, Free University, Amsterdam, The Netherlands, 1988. [2] D. Carmel and S. Markovitch.",
                "Incorporating opponent models into adversary search.",
                "In Proceedings of the Thirteenth National Conference on Artificial Intelligence, pages 120-125, Portland, OR, 1996. [3] D. Carmel and S. Markovitch.",
                "Opponent modeling in multi-<br>agent</br> systems.",
                "In G. Weiß and S. Sen, editors, Adaptation and Learning in Multi-<br>agent</br> Systems, pages 40-52.",
                "Springer-Verlag, 1996. [4] B. J. Grosz and S. Kraus.",
                "Collaborative plans for complex group action.",
                "Artificial Intelligence, 86(2):269-357, 1996. [5] M. Hadad, G. Kaminka, G. Armon, and S. Kraus.",
                "Supporting collaborative activity.",
                "In Proc. of AAAI-2005, pages 83-88, Pittsburgh, 2005. [6] http://www.gamerz.net/˜pbmserv/. [7] S. Kraus and D. Lehmann.",
                "Designing and building a negotiating automated <br>agent</br>.",
                "Computational Intelligence, 11:132-171, 1995. [8] H. J. Levesque, P. R. Cohen, and J. H. T. Nunes.",
                "On acting together.",
                "In Proc. of AAAI-90, pages 94-99, Boston, MA, 1990. [9] S. Markovitch and R. Reger.",
                "Learning and exploiting relative weaknesses of opponent agents.",
                "Autonomous Agents and Multi-<br>agent</br> Systems, 10(2):103-130, 2005. [10] Y. M. Ronald Fagin, Joseph Y. Halpern and M. Y. Vardi.",
                "Reasoning about knowledge.",
                "MIT Press, Cambridge, Mass., 1995. [11] P. Thagard.",
                "Adversarial problem solving: Modeling an oponent using explanatory coherence.",
                "Cognitive Science, 16(1):123-149, 1992. [12] R. W. Toseland and R. F. Rivas.",
                "An Introduction to Group Work Practice.",
                "Prentice Hall, Englewood Cliffs, NJ, 2nd edition edition, 1995. [13] S. Willmott, J. Richardson, A. Bundy, and J. Levine.",
                "An adversarial planning approach to Go.",
                "Lecture Notes in Computer Science, 1558:93-112, 1999.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-<br>agent</br> Systems (AAMAS 07) 557"
            ],
            "original_annotated_samples": [
                "We define an Adversarial Environment by describing the mental states of an <br>agent</br> in such an environment.",
                "However, none of this research dealt with adversarial domains and their implications for <br>agent</br> behavior.",
                "We explore environment properties and the mental states of agents to derive behavioral axioms; these behavioral axioms constitute a formal model that serves as a specification and design guideline for <br>agent</br> design in such settings.",
                "ADVERSARIAL ENVIRONMENTS The adversarial environment model (denoted as AE) is intended to guide the design of agents by providing a specification of the capabilities and mental attitudes of an <br>agent</br> in an adversarial environment.",
                "In particular, our adversarial environment model will deal with interactions that consist of N agents (N ≥ 2), where all agents are adversaries, and only one <br>agent</br> can succeed."
            ],
            "translated_annotated_samples": [
                "Definimos un Entorno Adversario al describir los estados mentales de un <br>agente</br> en dicho entorno.",
                "Sin embargo, ninguna de estas investigaciones abordó los dominios adversarios y sus implicaciones para el comportamiento de los <br>agente</br>s.",
                "Exploramos las propiedades del entorno y los estados mentales de los <br>agente</br>s para derivar axiomas de comportamiento; estos axiomas de comportamiento constituyen un modelo formal que sirve como especificación y guía de diseño para el diseño de <br>agente</br>s en tales entornos.",
                "ENTORNOS ADVERSARIOS El modelo de entorno adversario (denominado como AE) está destinado a guiar el diseño de <br>agente</br>s al proporcionar una especificación de las capacidades y actitudes mentales de un <br>agente</br> en un entorno adversario.",
                "En particular, nuestro modelo de entorno adversarial tratará con interacciones que consisten en N <br>agente</br>s (N ≥ 2), donde todos los <br>agente</br>s son adversarios y solo un agente puede tener éxito."
            ],
            "translated_text": "Un modelo de entorno adversarial para agentes racionales limitados en interacciones de suma cero Inon Zuckerman1, Sarit Kraus1, Jeffrey S. Rosenschein2, Gal Kaminka1 1 Departamento de Ciencias de la Computación 2 Escuela de Ingeniería Universidad Bar-Ilan y Ciencias de la Computación Ramat-Gan, Israel Universidad Hebrea, Jerusalén, Israel {zukermi, sarit, galk}@cs.biu.ac.il jeff@cs.huji.ac.il RESUMEN Los entornos multiagentes a menudo no son cooperativos ni colaborativos; en muchos casos, los agentes tienen intereses conflictivos, lo que lleva a interacciones adversariales. Este documento presenta un modelo formal de Entorno Adversarial para agentes racionales acotados que operan en un entorno de suma cero. En tales entornos, los intentos de utilizar métodos de búsqueda basados en utilidad clásica pueden plantear una variedad de dificultades (por ejemplo, modelar implícitamente al oponente como un maximizador de utilidad omnisciente, en lugar de aprovechar un modelo de oponente más matizado y explícito). Definimos un Entorno Adversario al describir los estados mentales de un <br>agente</br> en dicho entorno. Luego presentamos axiomas de comportamiento que están destinados a servir como principios de diseño para construir agentes adversarios. Exploramos la aplicación de nuestro enfoque mediante el análisis de archivos de registro de juegos de Conecta Cuatro completados, y presentamos un análisis empírico de la adecuación de los axiomas. Categorías y Descriptores de Asignaturas I.2.11 [Inteligencia Artificial]: Inteligencia Artificial Distribuida-Agentes Inteligentes, Sistemas Multiagente; I.2.4 [Inteligencia Artificial]: Formalismos y Métodos de Representación del Conocimiento - Lógica Modal Términos Generales Diseño, Teoría 1. Introducción: En las primeras investigaciones en sistemas multiagente (SMA) se consideraban grupos cooperativos de agentes; debido a que los agentes individuales tenían recursos limitados o acceso limitado a la información (por ejemplo, potencia de procesamiento limitada, cobertura sensorial limitada), trabajaban juntos por diseño para resolver problemas que individualmente no podían resolver, o al menos no podían resolver de manera eficiente. Sin embargo, la investigación de MAS pronto comenzó a considerar agentes interactivos con intereses individualizados, como representantes de diferentes humanos u organizaciones con intereses no idénticos. Cuando las interacciones son guiadas por intereses diversos, los participantes pueden tener que superar desacuerdos, interacciones no cooperativas e incluso intentos intencionales de dañarse mutuamente. Cuando ocurren este tipo de interacciones, los entornos requieren un comportamiento apropiado por parte de los agentes situados en ellos. Llamamos a estos entornos Entornos Adversarios, y llamamos a los agentes en conflicto Adversarios. Los modelos de cooperación y trabajo en equipo han sido ampliamente explorados en MAS a través de la axiomatización de estados mentales (por ejemplo, [8, 4, 5]). Sin embargo, ninguna de estas investigaciones abordó los dominios adversarios y sus implicaciones para el comportamiento de los <br>agente</br>s. Nuestro artículo aborda este problema al proporcionar un modelo formal y axiomatizado de estados mentales para un subconjunto de dominios adversariales, específicamente entornos adversariales simples de suma cero. Los encuentros de suma cero simples existen, por supuesto, en varios juegos de dos jugadores (por ejemplo, Ajedrez, Damas), pero también existen en juegos de n jugadores (por ejemplo, Risk, Diplomacia), subastas de un solo bien y en otros lugares. En estos entornos recientes en particular, el uso de una búsqueda adversarial basada en utilidad (como el algoritmo Min-Max) no siempre proporciona una solución adecuada; la función de recompensa podría ser bastante compleja o difícil de cuantificar, y existen limitaciones computacionales naturales en agentes racionales acotados. Además, los métodos de búsqueda tradicionales (como Min-Max) no hacen uso de un modelo del oponente, lo cual ha demostrado ser una adición valiosa a la planificación adversarial [9, 3, 11]. En este artículo, desarrollamos un modelo formal y axiomatizado para agentes racionales limitados que se encuentran en un entorno adversarial de suma cero. El modelo utiliza diferentes operadores de modalidad, y sus principales fundamentos son el modelo SharedPlans [4] para el comportamiento colaborativo. Exploramos las propiedades del entorno y los estados mentales de los <br>agente</br>s para derivar axiomas de comportamiento; estos axiomas de comportamiento constituyen un modelo formal que sirve como especificación y guía de diseño para el diseño de <br>agente</br>s en tales entornos. Luego investigamos el comportamiento de nuestro modelo empíricamente utilizando el juego de mesa Conecta Cuatro. Mostramos que este juego se ajusta a nuestra definición de entorno, y analizamos el comportamiento de los jugadores utilizando un gran conjunto de registros de partidas completadas 550 978-81-904262-7-5 (RPS) c 2007 archivos de IFAAMAS. Además, utilizamos los resultados presentados en [9] para discutir la importancia del modelado del oponente en nuestro dominio adversarial de Conecta Cuatro. El artículo continúa de la siguiente manera. La sección 2 presenta la formalización de los modelos. La sección 3 presenta el análisis empírico y sus resultados. Discutimos el trabajo relacionado en la Sección 4, y concluimos y presentamos las direcciones futuras en la Sección 5. ENTORNOS ADVERSARIOS El modelo de entorno adversario (denominado como AE) está destinado a guiar el diseño de <br>agente</br>s al proporcionar una especificación de las capacidades y actitudes mentales de un <br>agente</br> en un entorno adversario. Nos enfocamos aquí en tipos específicos de entornos adversarios, especificados de la siguiente manera: 1. Interacciones de suma cero: las utilidades positivas y negativas de todos los agentes suman cero; 2. Agentes adversarios simples: todos los agentes en el entorno son agentes adversarios; 3. AEs bilaterales: AEs con exactamente dos agentes; 4. AEs multilaterales: AEs de tres o más agentes. Trabajaremos en instanciaciones tanto bilaterales como multilaterales de entornos de suma cero y simples. En particular, nuestro modelo de entorno adversarial tratará con interacciones que consisten en N <br>agente</br>s (N ≥ 2), donde todos los <br>agente</br>s son adversarios y solo un agente puede tener éxito. ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "multiagent system": {
            "translated_key": "sistemas multiagente",
            "is_in_text": true,
            "original_annotated_sentences": [
                "An Adversarial Environment Model for Bounded Rational Agents in Zero-Sum Interactions Inon Zuckerman1 , Sarit Kraus1 , Jeffrey S. Rosenschein2 , Gal Kaminka1 1 Department of Computer Science 2 The School of Engineering Bar-Ilan University and Computer Science Ramat-Gan, Israel Hebrew University, Jerusalem, Israel {zukermi,sarit,galk}@cs.biu.ac.il jeff@cs.huji.ac.il ABSTRACT Multiagent environments are often not cooperative nor collaborative; in many cases, agents have conflicting interests, leading to adversarial interactions.",
                "This paper presents a formal Adversarial Environment model for bounded rational agents operating in a zero-sum environment.",
                "In such environments, attempts to use classical utility-based search methods can raise a variety of difficulties (e.g., implicitly modeling the opponent as an omniscient utility maximizer, rather than leveraging a more nuanced, explicit opponent model).",
                "We define an Adversarial Environment by describing the mental states of an agent in such an environment.",
                "We then present behavioral axioms that are intended to serve as design principles for building such adversarial agents.",
                "We explore the application of our approach by analyzing log files of completed Connect-Four games, and present an empirical analysis of the axioms appropriateness.",
                "Categories and Subject Descriptors I.2.11 [Artificial Intelligence]: Distributed Artificial Intelligence-Intelligent agents,Multiagent Systems; I.2.4 [Artificial Intelligence]: Knowledge Representation Formalisms and Methods -Modal logic General Terms Design, Theory 1.",
                "INTRODUCTION Early research in <br>multiagent system</br>s (MAS) considered cooperative groups of agents; because individual agents had limited resources, or limited access to information (e.g., limited processing power, limited sensor coverage), they worked together by design to solve problems that individually they could not solve, or at least could not solve as efficiently.",
                "MAS research, however, soon began to consider interacting agents with individuated interests, as representatives of different humans or organizations with non-identical interests.",
                "When interactions are guided by diverse interests, participants may have to overcome disagreements, uncooperative interactions, and even intentional attempts to damage one another.",
                "When these types of interactions occur, environments require appropriate behavior from the agents situated in them.",
                "We call these environments Adversarial Environments, and call the clashing agents Adversaries.",
                "Models of cooperation and teamwork have been extensively explored in MAS through the axiomatization of mental states (e.g., [8, 4, 5]).",
                "However, none of this research dealt with adversarial domains and their implications for agent behavior.",
                "Our paper addresses this issue by providing a formal, axiomatized mental state model for a subset of adversarial domains, namely simple zero-sum adversarial environments.",
                "Simple zero-sum encounters exist of course in various twoplayer games (e.g., Chess, Checkers), but they also exist in n-player games (e.g., Risk, Diplomacy), auctions for a single good, and elsewhere.",
                "In these latter environments especially, using a utility-based adversarial search (such as the Min-Max algorithm) does not always provide an adequate solution; the payoff function might be quite complex or difficult to quantify, and there are natural computational limitations on bounded rational agents.",
                "In addition, traditional search methods (like Min-Max) do not make use of a model of the opponent, which has proven to be a valuable addition to adversarial planning [9, 3, 11].",
                "In this paper, we develop a formal, axiomatized model for bounded rational agents that are situated in a zero-sum adversarial environment.",
                "The model uses different modality operators, and its main foundations are the SharedPlans [4] model for collaborative behavior.",
                "We explore environment properties and the mental states of agents to derive behavioral axioms; these behavioral axioms constitute a formal model that serves as a specification and design guideline for agent design in such settings.",
                "We then investigate the behavior of our model empirically using the Connect-Four board game.",
                "We show that this game conforms to our environment definition, and analyze players behavior using a large set of completed match log 550 978-81-904262-7-5 (RPS) c 2007 IFAAMAS files.",
                "In addition, we use the results presented in [9] to discuss the importance of opponent modeling in our ConnectFour adversarial domain.",
                "The paper proceeds as follows.",
                "Section 2 presents the models formalization.",
                "Section 3 presents the empirical analysis and its results.",
                "We discuss related work in Section 4, and conclude and present future directions in Section 5. 2.",
                "ADVERSARIAL ENVIRONMENTS The adversarial environment model (denoted as AE) is intended to guide the design of agents by providing a specification of the capabilities and mental attitudes of an agent in an adversarial environment.",
                "We focus here on specific types of adversarial environments, specified as follows: 1.",
                "Zero-Sum Interactions: positive and negative utilities of all agents sum to zero; 2.",
                "Simple AEs: all agents in the environment are adversarial agents; 3.",
                "Bilateral AEs: AEs with exactly two agents; 4.",
                "Multilateral AEs: AEs of three or more agents.",
                "We will work on both bilateral and multilateral instantiations of zero-sum and simple environments.",
                "In particular, our adversarial environment model will deal with interactions that consist of N agents (N ≥ 2), where all agents are adversaries, and only one agent can succeed.",
                "Examples of such environments range from board games (e.g., Chess, Connect-Four, and Diplomacy) to certain economic environments (e.g., N-bidder auctions over a single good). 2.1 Model Overview Our approach is to formalize the mental attitudes and behaviors of a single adversarial agent; we consider how a single agent perceives the AE.",
                "The following list specifies the conditions and mental states of an agent in a simple, zero-sum AE: 1.",
                "The agent has an individual intention that its own goal will be completed; 2.",
                "The agent has an individual belief that it and its adversaries are pursuing full conflicting goals (defined below)there can be only one winner; 3.",
                "The agent has an individual belief that each adversary has an intention to complete its own full conflicting goal; 4.",
                "The agent has an individual belief in the (partial) profile of its adversaries.",
                "Item 3 is required, since it might be the case that some agent has a full conflicting goal, and is currently considering adopting the intention to complete it, but is, as of yet, not committed to achieving it.",
                "This might occur because the agent has not yet deliberated about the effects that adopting that intention might have on the other intentions it is currently holding.",
                "In such cases, it might not consider itself to even be in an adversarial environment.",
                "Item 4 states that the agent should hold some belief about the profiles of its adversaries.",
                "The profile represents all the knowledge the agent has about its adversary: its weaknesses, strategic capabilities, goals, intentions, trustworthiness, and more.",
                "It can be given explicitly or can be learned from observations of past encounters. 2.2 Model Definitions for Mental States We use Grosz and Krauss definitions of the modal operators, predicates, and meta-predicates, as defined in their SharedPlan formalization [4].",
                "We recall here some of the predicates and operators that are used in that formalization: Int.To(Ai, α, Tn, Tα, C) represents Ais intentions at time Tn to do an action α at time Tα in the context of C. Int.Th(Ai, prop, Tn, Tprop, C) represents Ais intentions at time Tn that a certain proposition prop holds at time Tprop in the context of C. The potential intention operators, Pot.Int.To(...) and Pot.Int.Th(...), are used to represent the mental state when an agent considers adopting an intention, but has not deliberated about the interaction of the other intentions it holds.",
                "The operator Bel(Ai, f, Tf ) represents agent Ai believing in the statement expressed in formula f, at time Tf .",
                "MB(A, f, Tf ) represents mutual belief for a group of agents A.",
                "A snapshot of the system finds our environment to be in some state e ∈ E of environmental variable states, and each adversary in any LAi ∈ L of possible local states.",
                "At any given time step, the system will be in some world w of the set of all possible worlds w ∈ W, where w = E×LA1 ×LA2 × ...LAn , and n is the number of adversaries.",
                "For example, in a Texas Holdem poker game, an agents local state might be its own set of cards (which is unknown to its adversary) while the environment will consist of the betting pot and the community cards (which are visible to both players).",
                "A utility function under this formalization is defined as a mapping from a possible world w ∈ W to an element in , which expresses the desirability of the world, from a single agent perspective.",
                "We usually normalize the range to [0,1], where 0 represents the least desirable possible world, and 1 is the most desirable world.",
                "The implementation of the utility function is dependent on the domain in question.",
                "The following list specifies new predicates, functions, variables, and constants used in conjunction with the original definitions for the adversarial environment formalization: 1. φ is a null action (the agent does not do anything). 2.",
                "GAi is the set of agent Ais goals.",
                "Each goal is a set of predicates whose satisfaction makes the goal complete (we use G∗ Ai ∈ GAi to represent an arbitrary goal of agent Ai). 3. gAi is the set of agent Ais subgoals.",
                "Subgoals are predicates whose satisfaction represents an important milestone toward achievement of the full goal. gG∗ Ai ⊆ gAi is the set of subgoals that are important to the completion of goal G∗ Ai (we will use g∗ G∗ Ai ∈ gG∗ Ai to represent an arbitrary subgoal). 4.",
                "P Aj Ai is the profile object agent Ai holds about agent Aj. 5.",
                "CA is a general set of actions for all agents in A which are derived from the environments constraints.",
                "CAi ⊆ CA is the set of agent Ais possible actions. 6.",
                "Do(Ai, α, Tα, w) holds when Ai performs action α over time interval Tα in world w. 7.",
                "Achieve(G∗ Ai , α, w) is true when goal G∗ Ai is achieved following the completion of action α in world w ∈ W, where α ∈ CAi . 8.",
                "Profile(Ai, PAi Ai ) is true when agent Ai holds an object profile for agent Aj.",
                "Definition 1.",
                "Full conflict (FulConf ) describes a zerosum interaction where only a single goal of the goals in conflict can be completed.",
                "FulConf(G∗ Ai , G∗ Aj ) ⇒ (∃α ∈ CAi , ∀w, β ∈ CAj ) (Achieve(G∗ Ai , α, w) ⇒ ¬Achieve(G∗ Aj , β, w)) ∨ (∃β ∈ CAj , ∀w, α ∈ CAi )(Achieve(G∗ Aj , β, w) ⇒ ¬Achieve(G∗ Ai , α, w)) Definition 2.",
                "Adversarial Knowledge (AdvKnow) is a function returning a value which represents the amount of The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 551 knowledge agent Ai has on the profile of agent Aj, at time Tn.",
                "The higher the value, the more knowledge agent Ai has.",
                "AdvKnow : P Aj Ai × Tn → Definition 3.",
                "Eval - This evaluation function returns an estimated expected utility value for an agent in A, after completing an action from CA in some world state w. Eval : A × CA × w → Definition 4.",
                "TrH - (Threshold) is a numerical constant in the [0,1] range that represents an evaluation function (Eval) threshold value.",
                "An action that yields an estimated utility evaluation above the TrH is regarded as a highly beneficial action.",
                "The Eval value is an estimation and not the real utility function, which is usually unknown.",
                "Using the real utility value for a rational agent would easily yield the best outcome for that agent.",
                "However, agents usually do not have the real utility functions, but rather a heuristic estimate of it.",
                "There are two important properties that should hold for the evaluation function: Property 1.",
                "The evaluation function should state that the most desirable world state is one in which the goal is achieved.",
                "Therefore, after the goal has been satisfied, there can be no future action that can put the agent in a world state with higher Eval value. (∀Ai, G∗ Ai , α, β ∈ CAi , w ∈ W) Achieve(G∗ Ai , α, w) ⇒ Eval(Ai, α, w) ≥ Eval(Ai, β, w) Property 2.",
                "The evaluation function should project an action that causes a completion of a goal or a subgoal to a value which is greater than TrH (a highly beneficial action). (∀Ai, G∗ Ai ∈ GAi , α ∈ CAi , w ∈ W, g∗ GAi ∈ gGAi ) Achieve(G∗ Ai , α, w) ∨ Achieve(g∗ GAi , α, w) ⇒ Eval(Ai, α, w) ≥ TrH.",
                "Definition 5.",
                "SetAction We define a set action (SetAction) as a set of action operations (either complex or basic actions) from some action sets CAi and CAj which, according to agent Ais belief, are attached together by a temporal and consequential relationship, forming a chain of events (action, and its following consequent action). (∀α1 , . . . , αu ∈ CAi , β1 , . . . , βv ∈ CAj , w ∈ W) SetAction(α1 , . . . , αu , β1 , . . . , βv , w) ⇒ ((Do(Ai, α1 , Tα1 , w) ⇒ Do(Aj, β1 , Tβ1 , w)) ⇒ Do(Ai, α2 , Tα2 , w) ⇒ . . . ⇒ Do(Ai, αu , Tαu , w)) The consequential relation might exist due to various environmental constraints (when one action forces the adversary to respond with a specific action) or due to the agents knowledge about the profile of its adversary.",
                "Property 3.",
                "As the knowledge we have about our adversary increases we will have additional beliefs about its behavior in different situations which in turn creates new set actions.",
                "Formally, if our AdvKnow at time Tn+1 is greater than AdvKnow at time Tn, then every SetAction known at time Tn is also known at time Tn+1.",
                "AdvKnow(P Aj Ai , Tn+1) > AdvKnow(P Aj Ai , Tn) ⇒ (∀α1 , . . . , αu ∈ CAi , β1 , . . . , βv ∈ CAj ) Bel(Aag, SetAction(α1 , . . . , αu , β1 , . . . , βv ), Tn) ⇒ Bel(Aag, SetAction(α1 , . . . , αu , β1 , . . . , βv ), Tn+1) 2.3 The Environment Formulation The following axioms provide the formal definition for a simple, zero-sum Adversarial Environment (AE).",
                "Satisfaction of these axioms means that the agent is situated in such an environment.",
                "It provides specifications for agent Aag to interact with its set of adversaries A with respect to goals G∗ Aag and G∗ A at time TCo at some world state w. AE(Aag, A, G∗ Aag , A1, . . . , Ak, G∗ A1 , . . . , G∗ Ak , Tn, w) 1.",
                "Aag has an Int.Th his goal would be completed: (∃α ∈ CAag , Tα) Int.Th(Aag, Achieve(G∗ Aag , α), Tn, Tα, AE) 2.",
                "Aag believes that it and each of its adversaries Ao are pursuing full conflicting goals: (∀Ao ∈ {A1, . . . , Ak}) Bel(Aag, FulConf(G∗ Aag , G∗ Ao ), Tn) 3.",
                "Aag believes that each of his adversaries in Ao has the Int.Th his conflicting goal G∗ Aoi will be completed: (∀Ao ∈ {A1, . . . , Ak})(∃β ∈ CAo , Tβ) Bel(Aag, Int.Th(Ao, Achieve(G∗ Ao , β), TCo, Tβ, AE), Tn) 4.",
                "Aag has beliefs about the (partial) profiles of its adversaries (∀Ao ∈ {A1, . . . , Ak}) (∃PAo Aag ∈ PAag )Bel(Aag, Profile(Ao, PAo Aag ), Tn) To build an agent that will be able to operate successfully within such an AE, we must specify behavioral guidelines for its interactions.",
                "Using a naive Eval maximization strategy to a certain search depth will not always yield satisfactory results for several reasons: (1) the search horizon problem when searching for a fixed depth; (2) the strong assumption of an optimally rational, unbounded resources adversary; (3) using an estimated evaluation function which will not give optimal results in all world states, and can be exploited [9].",
                "The following axioms specify the behavioral principles that can be used to differentiate between successful and less successful agents in the above Adversarial Environment.",
                "Those axioms should be used as specification principles when designing and implementing agents that should be able to perform well in such Adversarial Environments.",
                "The behavioral axioms represent situations in which the agent will adopt potential intentions to (Pot.Int.To(...)) perform an action, which will typically require some means-end reasoning to select a possible course of action.",
                "This reasoning will lead to the adoption of an Int.To(...) (see [4]).",
                "A1.",
                "Goal Achieving Axiom.",
                "The first axiom is the simplest case; when the agent Aag believes that it is one action (α) away from achieving his conflicting goal G∗ Aag , it should adopt the potential intention to do α and complete its goal. (∀Aag, α ∈ CAag , Tn, Tα, w ∈ W) (Bel(Aag, Do(Aag, α, Tα, w) ⇒ Achieve(G∗ Aag , α, w)) ⇒ Pot.Int.To(Aag, α, Tn, Tα, w) This somewhat trivial behavior is the first and strongest axiom.",
                "In any situation, when the agent is an action away from completing the goal, it should complete the action.",
                "Any fair Eval function would naturally classify α as the maximal value action (property 1).",
                "However, without explicit axiomatization of such behavior there might be situations where the agent will decide on taking another action for various reasons, due to its bounded decision resources.",
                "A2.",
                "Preventive Act Axiom.",
                "Being in an adversarial situation, agent Aag might decide to take actions that will damage one of its adversarys plans to complete its goal, even if those actions do not explicitly advance Aag towards its conflicting goal G∗ Aag .",
                "Such preventive action will take place when agent Aag has a belief about the possibility of its adversary Ao doing an action β that will give it a high 552 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) utility evaluation value (> TrH).",
                "Believing that taking action α will prevent the opponent from doing its β, it will adopt a potential intention to do α. (∀Aag, Ao ∈ A, α ∈ CAag , β ∈ CAo , Tn, Tβ, w ∈ W) (Bel(Aag, Do(Ao, β, Tβ, w) ∧ Eval(Ao, β, w) > TrH, Tn) ∧ Bel(Aag, Do(Aag, α, Tα, w) ⇒ ¬Do(Ao, β, Tβ, w), Tn) ⇒ Pot.Int.To(Aag, α, Tn, Tα, w) This axiom is a basic component of any adversarial environment.",
                "For example, looking at a Chess board game, a player could realize that it is about to be checkmated by its opponent, thus making a preventive move.",
                "Another example is a Connect Four game: when a player has a row of three chips, its opponent must block it, or lose.",
                "A specific instance of A1 occurs when the adversary is one action away from achieving its goal, and immediate preventive action needs to be taken by the agent.",
                "Formally, we have the same beliefs as stated above, with a changed belief that doing action β will cause agent Ao to achieve its goal.",
                "Proposition 1: Prevent or lose case. (∀Aag, Ao ∈ A, α ∈ CAag , β ∈ CAo , G∗ Ao , Tn, Tα, Tβ, w ∈ W) Bel(Aag, Do(Ao, β, Tβ, w) ⇒ Achieve(G∗ Ao , β, w), Tn) ∧ Bel(Aag, Do(Aag, α, Tα, w) ⇒ ¬Do(Ao, β, Tβ, w)) ⇒ Pot.Int.To(Aag, α, Tn, Tα, w) Sketch of proof: Proposition 1 can be easily derived from axiom A1 and the property 2 of the Eval function, which states that any action that causes a completion of a goal is a highly beneficial action.",
                "The preventive act behavior will occur implicitly when the Eval function is equal to the real world utility function.",
                "However, being bounded rational agents and dealing with an estimated evaluation function we need to explicitly axiomatize such behavior, for it will not always occur implicitly from the evaluation function.",
                "A3.",
                "Suboptimal Tactical Move Axiom.",
                "In many scenarios a situation may occur where an agent will decide not to take the current most beneficial action it can take (the action with the maximal utility evaluation value), because it believes that taking another action (with lower utility evaluation value) might yield (depending on the adversarys response) a future possibility for a highly beneficial action.",
                "This will occur most often when the Eval function is inaccurate and differs by a large extent from the Utility function.",
                "Put formally, agent Aag believes in a certain SetAction that will evolve according to its initial action and will yield a high beneficial value (> TrH) solely for it. (∀Aag, Ao ∈ A, Tn, w ∈ W) (∃α1 , . . . , αu ∈ CAi , β1 , . . . , βv ∈ CAj , Tα1 ) Bel(Aag, SetAction(α1 , . . . , αu , β1 , . . . , βv ), Tn) ∧ Bel(Aag, Eval(Ao, βv , w) < TrH < Eval(Aag, αu , w), Tn) ⇒ Pot.Int.To(Aag, α1 , Tn, Tα1 , w) An agent might believe that a chain of events will occur for various reasons due to the inevitable nature of the domain.",
                "For example, in Chess, we often observe the following: a move causes a check position, which in turn limits the opponents moves to avoiding the check, to which the first player might react with another check, and so on.",
                "The agent might also believe in a chain of events based on its knowledge of its adversarys profile, which allows it to foresee the adversarys movements with high accuracy.",
                "A4.",
                "Profile Detection Axiom.",
                "The agent can adjust its adversarys profiles by observations and pattern study (specifically, if there are repeated encounters with the same adversary).",
                "However, instead of waiting for profile information to be revealed, an agent can also initiate actions that will force its adversary to react in a way that will reveal profile knowledge about it.",
                "Formally, the axiom states that if all actions (γ) are not highly beneficial actions (< TrH), the agent can do action α in time Tα if it believes that it will result in a non-highly beneficial action β from its adversary, which in turn teaches it about the adversarys profile, i.e., gives a higher AdvKnow(P Aj Ai , Tβ). (∀Aag, Ao ∈ A, α ∈ CAag , β ∈ CAo , Tn, Tα, Tβ, w ∈ W) Bel(Aag, (∀γ ∈ CAag )Eval(Aag, γ, w) < TrH, Tn) ∧ Bel(Aag, Do(Aag, α, Tα, w) ⇒ Do(Ao, β, Tβ, w), Tn) ∧ Bel(Aag, Eval(Ao, β, w) < TrH) ∧ Bel(Aag, AdvKnow(P Aj Ai , Tβ) > AdvKnow(P Aj Ai , Tn), Tn) ⇒ Pot.Int.To(Aag, α, Tn, Tα, w) For example, going back to the Chess board game scenario, consider starting a game versus an opponent about whom we know nothing, not even if it is a human or a computerized opponent.",
                "We might start playing a strategy that will be suitable versus an average opponent, and adjust our game according to its level of play.",
                "A5.",
                "Alliance Formation Axiom The following behavioral axiom is relevant only in a multilateral instantiation of the adversarial environment (obviously, an alliance cannot be formed in a bilateral, zero-sum encounter).",
                "In different situations during a multilateral interaction, a group of agents might believe that it is in their best interests to form a temporary alliance.",
                "Such an alliance is an agreement that constrains its members behavior, but is believed by its members to enable them to achieve a higher utility value than the one achievable outside of the alliance.",
                "As an example, we can look at the classical Risk board game, where each player has an individual goal of being the sole conquerer of the world, a zero-sum game.",
                "However, in order to achieve this goal, it might be strategically wise to make short-term ceasefire agreements with other players, or to join forces and attack an opponent who is stronger than the rest.",
                "An alliances terms defines the way its members should act.",
                "It is a set of predicates, denoted as Terms, that is agreed upon by the alliance members, and should remain true for the duration of the alliance.",
                "For example, the set Terms in the Risk scenario, could contain the following predicates: 1.",
                "Alliance members will not attack each other on territories X, Y and Z; 2.",
                "Alliance members will contribute C units per turn for attacking adversary Ao; 3.",
                "Members are obligated to stay as part of the alliance until time Tk or until adversarys Ao army is smaller than Q.",
                "The set Terms specifies inter-group constraints on each of the alliance members (∀Aal i ∈ Aal ⊆ A) set of actions Cal i ⊆ C. Definition 6.",
                "Al val - the total evaluation value that agent Ai will achieve while being part of Aal is the sum of Evali (Eval for Ai) of each of Aal j Eval values after taking their own α actions (via the agent(α) predicate): Al val(Ai, Cal , Aal , w) = α∈Cal Evali(Aal j , agent(α), w) Definition 7.",
                "Al TrH - is a number representing an Al val The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 553 threshold; above it, the alliance can be said to be a highly beneficial alliance.",
                "The value of Al TrH will be calculated dynamically according to the progress of the interaction, as can be seen in [7].",
                "After an alliance is formed, its members are now working in their normal adversarial environment, as well as according to the mental states and axioms required for their interactions as part of the alliance.",
                "The following Alliance model (AL) specifies the conditions under which the group Aal can be said to be in an alliance and working with a new and constrained set of actions Cal , at time Tn.",
                "AL(Aal , Cal , w, Tn) 1.",
                "Aal has a MB that all members are part of Aal : MB(Aal , (∀Aal i ∈ Aal )member(Aal i , Aal ), Tn) 2.",
                "Aal has a MB that the group be maintained: MB(Aal , (∀Aal i ∈ Aal )Int.Th (Ai, member(Ai, Aal ), Tn, Tn+1, Co), Tn) 3.",
                "Aal has a MB that being members gives them high utility value: MB(Aal , (∀Aal i ∈ Aal )Al val(Aal i , Cal , Aal , w) ≥ Al TrH, Tn) Members profiles are a crucial part of successful alliances.",
                "We assume that agents that have more accurate profiles of their adversaries will be more successful in such environments.",
                "Such agents will be able to predict when a member is about to breach the alliances contract (item 2 in the above model), and take counter measures (when item 3 will falsify).",
                "The robustness of the alliance is in part a function of its members trustfulness measure, objective position estimation, and other profile properties.",
                "We should note that an agent can simultaneously be part of more than one alliance.",
                "Such a temporary alliance, where the group members do not have a joint goal but act collaboratively for the interest of their own individual goals, is classified as a Treatment Group by modern psychologists [12] (in contrast to a Task Group, where its members have a joint goal).",
                "The Shared Activity model as presented in [5] modeled Treatment Group behavior using the same SharedPlans formalization.",
                "When comparing both definitions of an alliance and a Treatment Group we found an unsurprising resemblance between both models: the environment models definitions are almost identical (see SAs definitions in [5]), and their Selfish-Act and Cooperative Act axioms conform to our adversarial agents behavior.",
                "The main distinction between both models is the integration of a Helpful-behavior act axiom, in the Shared Activity which cannot be part of ours.",
                "This axiom states that an agent will consider taking action that will lower its Eval value (to a certain lower bound), if it believes that a group partner will gain a significant benefit.",
                "Such behavior cannot occur in a pure adversarial environment (as a zero-sum game is), where the alliance members are constantly on watch to manipulate their alliance to their own advantage.",
                "A6.",
                "Evaluation Maximization Axiom.",
                "In a case when all other axioms are inapplicable, we will proceed with the action that maximizes the heuristic value as computed in the Eval function. (∀Aag, Ao ∈ A, α ∈ Cag, Tn, w ∈ W) Bel(Aag, (∀γ ∈ Cag)Eval(Aag, α, w) ≥ Eval(Aag, γ, w), Tn) ⇒ Pot.Int.To(Aag, α, Tn, Tα, w) T1.",
                "Optimality on Eval = Utility The above axiomatic model handles situations where the Utility is unknown and the agents are bounded rational agents.",
                "The following theorem shows that in bilateral interactions, where the agents have the real Utility function (i.e., Eval = Utility) and are rational agents, the axioms provide the same optimal result as classic adversarial search (e.g., Min-Max).",
                "Theorem 1.",
                "Let Ae ag be an unbounded rational AE agent using the Eval heuristic evaluation function, Au ag be the same agent using the true Utility function, and Ao be a sole unbounded utility-based rational adversary.",
                "Given that Eval = Utility: (∀α ∈ CAu ag , α ∈ CAe ag , Tn, w ∈ W) Pot.Int.To(Au ag, α, Tn, Tα, w) → Pot.Int.To(Ae ag, α , Tn, Tα, w) ∧ ((α = α ) ∨ (Utility(Au ag, α, w) = Eval(Ae ag, α , w))) Sketch of proof - Given that Au ag has the real utility function and unbounded resources, it can generate the full game tree and run the optimal MinMax algorithm to choose the highest utility value action, which we denote by, α.",
                "The proof will show that Ae ag, using the AE axioms, will select the same or equal utility α (when there is more than one action with the same max utility) when Eval = Utility. (A1) Goal achieving axiom - suppose there is an α such that its completion will achieve Au ags goal.",
                "It will obtain the highest utility by Min-Max for Au ag.",
                "The Ae ag agent will select α or another action with the same utility value via A1.",
                "If such α does not exist, Ae ag cannot apply this axiom, and proceeds to A2. (A2) Preventive act axiom - (1) Looking at the basic case (see Prop1), if there is a β which leads Ao to achieve its goal, then a preventive action α will yield the highest utility for Au ag.",
                "Au ag will choose it through the utility, while Ae ag will choose it through A2. (2) In the general case, β is a highly beneficial action for Ao, thus yields low utility for Au ag, which will guide it to select an α that will prevent β, while Ae ag will choose it through A2.1 If such β does not exist for Ao, then A2 is not applicable, and Ae ag can proceed to A3. (A3) Suboptimal tactical move axiom - When using a heuristic Eval function, Ae ag has a partial belief in the profile of its adversary (item 4 in AE model), which may lead it to believe in SetActions (Prop1).",
                "In our case, Ae ag is holding a full profile on its optimal adversary and knows that Ao will behave optimally according to the real utility values on the complete search tree, therefore, any belief about suboptimal SetAction cannot exist, yielding this axiom inapplicable.",
                "Ae ag will proceed to A4. (A4) Profile detection axiom - Given that Ae ag has the full profile of Ao, none of Ae ags actions can increase its knowledge.",
                "That axiom will not be applied, and the agent will proceed with A6 (A5 will be disregarded because the interaction is bilateral). (A6) Evaluation maximization axiom - This axiom will select the max Eval for Ae ag.",
                "Given that Eval = Utility, the same α that was selected by Au ag will be selected. 3.",
                "EVALUATION The main purpose of our experimental analysis is to evaluate the models behavior and performance in a real adversarial environment.",
                "This section investigates whether bounded 1 A case where following the completion of β there exists a γ which gives high utility for Agent Au ag, cannot occur because Ao uses the same utility, and γs existence will cause it to classify β as a low utility action. 554 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) rational agents situated in such adversarial environments will be better off applying our suggested behavioral axioms. 3.1 The Domain To explore the use of the above model and its behavioral axioms, we decided to use the Connect-Four game as our adversarial environment.",
                "Connect-Four is a 2-player, zerosum game which is played using a 6x7 matrix-like board.",
                "Each turn, a player drops a disc into one of the 7 columns (the set of 21 discs is usually colored yellow for player 1 and red for player 2; we will use White and Black respectively to avoid confusion).",
                "The winner is the first player to complete a horizontal, vertical, or diagonal set of four discs with its color.",
                "On very rare occasions, the game might end in a tie if all the empty grids are filled, but no player managed to create a 4-disc set.",
                "The Connect-Four game was solved in [1], where it is shown that the first player (playing with the white discs) can force a win by starting in the middle column (column 4) and playing optimally However, the optimal strategy is very complex, and difficult to follow even for complex bounded rational agents, such as human players.",
                "Before we can proceed checking agent behavior, we must first verify that the domain conforms to the adversarial environments definition as given above (which the behavioral axioms are based on).",
                "First, when playing a Connect-Four game, the agent has an intention to win the game (item 1).",
                "Second (item 2), our agent believes that in Connect-Four there can only be one winner (or no winner at all in the rare occurrence of a tie).",
                "In addition, our agent believes that its opponent to the game will try to win (item 3), and we hope it has some partial knowledge (item 4) about its adversary (this knowledge can vary from nothing, through simple facts such as age, to strategies and weaknesses).",
                "Of course, not all Connect-Four encounters are adversarial.",
                "For example, when a parent is playing the game with its child, the following situation might occur: the child, having a strong incentive to win, treats the environment as adversarial (it intends to win, understands that there can only be one winner, and believes that its parent is trying to beat him).",
                "However, the parents point of view might see the environment as an educational one, where its goal is not to win the game, but to cause enjoyment or practice strategic reasoning.",
                "In such an educational environment, a new set of behavioral axioms might be more beneficial to the parents goals than our suggested adversarial behavioral axioms. 3.2 Axiom Analysis After showing that the Connect-Four game is indeed a zero-sum, bilateral adversarial environment, the next step is to look at players behaviors during the game and check whether behaving according to our model does improve performance.",
                "To do so we have collected log files from completed Connect-Four games that were played by human players over the Internet.",
                "Our collected log file data came from Play by eMail (PBeM) sites.",
                "These are web sites that host email games, where each move is taken by an email exchange between the server and the players.",
                "Many such sites archives contain real competitive interactions, and also maintain a ranking system for their members.",
                "Most of the data we used can be found in [6].",
                "As can be learned from [1], Connect-Four has an optimal strategy and a considerable advantage for the player who starts the game (which we call the White player).",
                "We will concentrate in our analysis on the second players moves (to be called Black).",
                "The White player, being the first to act, has the so-called initiative advantage.",
                "Having the advantage and a good strategy will keep the Black player busy reacting to its moves, instead of initiating threats.",
                "A threat is a combination of three discs of the same color, with an empty spot for the fourth winning disk.",
                "An open threat is a threat that can be realized in the opponents next move.",
                "In order for the Black player to win, it must somehow turn the tide, take the advantage and start presenting threats to the White player.",
                "We will explore Black players behavior and their conformance to our axioms.",
                "To do so, we built an application that reads log files and analyzes the Black players moves.",
                "The application contains two main components: (1) a Min-Max algorithm for evaluation of moves; (2) open threats detector for the discovering of open threats.",
                "The Min-Max algorithm will work to a given depth, d and for each move α will output the heuristic value for the next action taken by the player as written in the log file, h(α), alongside the maximum heuristic value, maxh(α), that could be achieved prior to taking the move (obviously, if h(α) = maxh(α), then the player did not do the optimal move heuristically).",
                "The threat detectors job is to notify if some action was taken in order to block an open threat (not blocking an open threat will probably cause the player to lose in the opponents next move).",
                "The heuristic function used by Min-Max to evaluate the players utility is the following function, which is simple to compute, yet provides a reasonable challenge to human opponents: Definition 8.",
                "Let Group be an adjacent set of four squares that are horizontal, vertical, or diagonal.",
                "Groupn b (Groupn w) be a Group with n pieces of the black (white) color and 4−n empty squares. h = ((Group1 b ∗α)+(Group2 b ∗β)+(Group3 b ∗γ)+(Group4 b ∗∞)) − ((Group1 w ∗α)+(Group2 w ∗β)+(Group3 w ∗γ)+(Group4 w ∗∞)) The values of α, β and δ can vary to form any desired linear combination; however, it is important to value them with the α < β < δ ordering in mind (we used 1, 4, and 8 as their respective values).",
                "Groups of 4 discs of the same color means victory, thus discovery of such a group will result in ∞ to ensure an extreme value.",
                "We now use our estimated evaluation function to evaluate the Black players actions during the Connect-Four adversarial interaction.",
                "Each game from the log file was input into the application, which processed and output a reformatted log file containing the h value of the current move, the maxh value that could be achieved, and a notification if an open threat was detected.",
                "A total of 123 games were analyzed (57 with White winning, and 66 with Black winning).",
                "A few additional games were manually ignored in the experiment, due to these problems: a player abandoning the game while the outcome is not final, or a blunt irrational move in the early stages of the game (e.g., not blocking an obvious winning group in the first opening moves).",
                "In addition, a single tie game was also removed.",
                "The simulator was run to a search depth of 3 moves.",
                "We now proceed to analyze the games with respect to each behavioral axiom.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 555 Table 1: Average heuristic difference analysis Black losses Black Won Avg minh -17.62 -12.02 Avg 3 lowest h moves (min3 h) -13.20 -8.70 3.2.1 Affirming the Suboptimal tactical move axiom The following section presents the heuristic evaluations of the Min-Max algorithm for each action, and checks the amount and extent of suboptimal tactical actions and their implications on performance.",
                "Table 1 shows results and insights from the games heuristic analysis, when search depth equals 3 (this search depth was selected for the results to be comparable to [9], see Section 3.2.3).",
                "The tables heuristic data is the difference between the present maximal heuristic value and the heuristic value of the action that was eventually taken by the player (i.e., the closer the number is to 0, the closer the action was to the maximum heuristic action).",
                "The first row presents the difference values of the action that had the maximal difference value among all the Black players actions in a given game, as averaged over all Blacks winning and losing games (see respective columns).",
                "In games in which the Black player loses, its average difference value was -17.62, while in games in which the Black player won, its average was -12.02.",
                "The second row expands the analysis by considering the 3 highest heuristic difference actions, and averaging them.",
                "In that case, we notice an average heuristic difference of 5 points between games which the Black player loses and games in which it wins.",
                "Nevertheless, the importance of those numbers is that they allowed us to take an educated guess on a threshold number of 11.5, as the value of the TrH constant, which differentiates between normal actions and highly beneficial ones.",
                "After finding an approximated TrH constant, we can proceed with an analysis of the importance of suboptimal moves.",
                "To do so we took the subset of games in which the minimum heuristic difference value for Blacks actions was 11.5.",
                "As presented in Table 2, we can see the different min3 h average of the 3 largest ranges and the respective percentage of games won.",
                "The first row shows that the Black player won only 12% of the games in which the average of its 3 highest heuristically difference actions (min3 h) was smaller than the suggested threshold, TrH = 11.5.",
                "The second row shows a surprising result: it seems that when min3 h > −4 the Black player rarely wins.",
                "Intuition would suggest that games in which the action evaluation values were closer to the maximal values will result in more winning games for Black.",
                "However, it seems that in the Connect-Four domain, merely responding with somewhat easily expected actions, without initiating a few surprising and suboptimal moves, does not yield good results.",
                "The last row sums up the main insights from the analysis; most of Blacks wins (83%) came when its min3 h was in the range of -11.5 to -4.",
                "A close inspection of those Black winning games shows the following pattern behind the numbers: after standard opening moves, Black suddenly drops a disc into an isolated column, which seems a waste of a move.",
                "White continues to build its threats, while usually disregarding Blacks last move, which in turn uses the isolated disc as an anchor for a future winning threat.",
                "The results show that it was beneficial for the Black player Table 2: Blacks winnings percentages % of games min3 h < −11.5 12% min3 h > −4 5% −11.5 ≤ min3 h ≤ −4 83% to take suboptimal actions and not give the current highest possible heuristic value, but will not be too harmful for its position (i.e., will not give high beneficial value to its adversary).",
                "As it turned out, learning the threshold is an important aspect of success: taking wildly risky moves (min3 h < −11.5) or trying to avoid them (min3 h > −4) reduces the Black players winning chances by a large margin. 3.2.2 Affirming the Profile Monitoring Axiom In the task of showing the importance of monitoring ones adversaries profiles, our log files could not be used because they did not contain repeated interactions between players, which are needed to infer the players knowledge about their adversaries.",
                "However, the importance of opponent modeling and its use in attaining tactical advantages was already studied in various domains ([3, 9] are good examples).",
                "In a recent paper, Markovitch and Reger [9] explored the notion of learning and exploitation of opponent weakness in competitive interactions.",
                "They apply simple learning strategies by analyzing examples from past interactions in a specific domain.",
                "They also used the Connect-Four adversarial domain, which can now be used to understand the importance of monitoring the adversarys profile.",
                "Following the presentation of their theoretical model, they describe an extensive empirical study and check the agents performance after learning the weakness model with past examples.",
                "One of the domains used as a competitive environment was the same Connect-Four game (Checkers was the second domain).",
                "Their heuristic function was identical to ours with three different variations (H1, H2, and H3) that are distinguished from one another in their linear combination coefficient values.",
                "The search depth for the players was 3 (as in our analysis).",
                "Their extensive experiments check and compare various learning strategies, risk factors, predefined feature sets and usage methods.",
                "The bottom line is that the Connect-Four domain shows an improvement from a 0.556 winning rate before modeling to a 0.69 after modeling (page 22).",
                "Their conclusions, showing improved performance when holding and using the adversarys model, justify the effort to monitor the adversary profile for continuous and repeated interactions.",
                "An additional point that came up in their experiments is the following: after the opponent weakness model has been learned, the authors describe different methods of integrating the opponent weakness model into the agents decision strategy.",
                "Nevertheless, regardless of the specific method they chose to work with, all integration methods might cause the agent to take suboptimal decisions; it might cause the agent to prefer actions that are suboptimal at the present decision junction, but which might cause the opponent to react in accordance with its weakness model (as represented by our agent) which in turn will be beneficial for us in the future.",
                "The agents behavior, as demonstrated in [9] further confirms and strengthens our Suboptimal Tactical Axiom as discussed in the previous section. 556 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 3.2.3 Additional Insights The need for the Goal Achieving, Preventive Act, and Evaluation Maximization axioms are obvious, and need no further verification.",
                "However, even with respect to those axioms, a few interesting insights came up in the log analysis.",
                "The Goal achieving and Preventive Act axioms, though theoretically trivial, seem to provide some challenge to a human player.",
                "In the initial inspection of the logs, we encountered few games2 where a player, for inexplicable reasons, did not block the other from winning or failed to execute its own winning move.",
                "We can blame those faults on the humans lack of attention, or a typing error in its move reply; nevertheless, those errors might occur in bounded rational agents, and the appropriate behavior needs to be axiomatized.",
                "A typical Connect-Four game revolves around generating threats and blocking them.",
                "In our analysis we looked for explicit preventive actions, i.e., moves that block a group of 3 discs, or that remove a future threat (in our limited search horizon).",
                "We found that in 83% of the total games there was at least one preventive action taken by the Black player.",
                "It was also found that Black averaged 2.8 preventive actions per game on the games in which it lost, while averaging 1.5 preventive actions per game when winning.",
                "It seems that Black requires 1 or 2 preventive actions to build its initial taking position, before starting to present threats.",
                "If it did not manage to win, it will usually prevent an extra threat or two before succumbing to White. 4.",
                "RELATED WORK Much research deals with the axiomatization of teamwork and mental states of individuals: some models use knowledge and belief [10], others have models of goals and intentions [8, 4].",
                "However, all these formal theories deal with agent teamwork and cooperation.",
                "As far as we know, our model is the first to provide a formalized model for explicit adversarial environments and agents behavior in it.",
                "The classical Min-Max adversarial search algorithm was the first attempt to integrate the opponent into the search space with a weak assumption of an optimally playing opponent.",
                "Since then, much effort has gone into integrating the opponent model into the decision procedure to predict future behavior.",
                "The M∗ algorithm presented by Carmel and Markovitch [2] showed a method of incorporating opponent models into adversary search, while in [3] they used learning to provide a more accurate opponent model in a 2player repeated game environment, where agents strategies were modeled as finite automata.",
                "Additional Adversarial planning work was done by Willmott et al. [13], which provided an adversarial planning approach to the game of GO.",
                "The research mentioned above dealt with adversarial search and the integration of opponent models into classical utilitybased search methods.",
                "That work shows the importance of opponent modeling and the ability to exploit it to an agents advantage.",
                "However, the basic limitations of those search methods still apply; our model tries to overcome those limitations by presenting a formal model for a new, mental state-based adversarial specification. 5.",
                "CONCLUSIONS We presented an Adversarial Environment model for a 2 These were later removed from the final analysis. bounded rational agent that is situated in an N-player, zerosum environment.",
                "We used the SharedPlans formalization to define the model and the axioms that agents can apply as behavioral guidelines.",
                "The model is meant to be used as a guideline for designing agents that need to operate in such adversarial environments.",
                "We presented empirical results, based on ConnectFour log file analysis, that exemplify the model and the axioms for a bilateral instance of the environment.",
                "The results we presented are a first step towards an expanded model that will cover all types of adversarial environments, for example, environments that are non-zero-sum, and environments that contain natural agents that are not part of the direct conflict.",
                "Those challenges and more will be dealt with in future research. 6.",
                "ACKNOWLEDGMENT This research was supported in part by Israel Science Foundation grants #1211/04 and #898/05. 7.",
                "REFERENCES [1] L. V. Allis.",
                "A knowledge-based approach of Connect-Four - the game is solved: White wins.",
                "Masters thesis, Free University, Amsterdam, The Netherlands, 1988. [2] D. Carmel and S. Markovitch.",
                "Incorporating opponent models into adversary search.",
                "In Proceedings of the Thirteenth National Conference on Artificial Intelligence, pages 120-125, Portland, OR, 1996. [3] D. Carmel and S. Markovitch.",
                "Opponent modeling in multi-agent systems.",
                "In G. Weiß and S. Sen, editors, Adaptation and Learning in Multi-Agent Systems, pages 40-52.",
                "Springer-Verlag, 1996. [4] B. J. Grosz and S. Kraus.",
                "Collaborative plans for complex group action.",
                "Artificial Intelligence, 86(2):269-357, 1996. [5] M. Hadad, G. Kaminka, G. Armon, and S. Kraus.",
                "Supporting collaborative activity.",
                "In Proc. of AAAI-2005, pages 83-88, Pittsburgh, 2005. [6] http://www.gamerz.net/˜pbmserv/. [7] S. Kraus and D. Lehmann.",
                "Designing and building a negotiating automated agent.",
                "Computational Intelligence, 11:132-171, 1995. [8] H. J. Levesque, P. R. Cohen, and J. H. T. Nunes.",
                "On acting together.",
                "In Proc. of AAAI-90, pages 94-99, Boston, MA, 1990. [9] S. Markovitch and R. Reger.",
                "Learning and exploiting relative weaknesses of opponent agents.",
                "Autonomous Agents and Multi-Agent Systems, 10(2):103-130, 2005. [10] Y. M. Ronald Fagin, Joseph Y. Halpern and M. Y. Vardi.",
                "Reasoning about knowledge.",
                "MIT Press, Cambridge, Mass., 1995. [11] P. Thagard.",
                "Adversarial problem solving: Modeling an oponent using explanatory coherence.",
                "Cognitive Science, 16(1):123-149, 1992. [12] R. W. Toseland and R. F. Rivas.",
                "An Introduction to Group Work Practice.",
                "Prentice Hall, Englewood Cliffs, NJ, 2nd edition edition, 1995. [13] S. Willmott, J. Richardson, A. Bundy, and J. Levine.",
                "An adversarial planning approach to Go.",
                "Lecture Notes in Computer Science, 1558:93-112, 1999.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 557"
            ],
            "original_annotated_samples": [
                "INTRODUCTION Early research in <br>multiagent system</br>s (MAS) considered cooperative groups of agents; because individual agents had limited resources, or limited access to information (e.g., limited processing power, limited sensor coverage), they worked together by design to solve problems that individually they could not solve, or at least could not solve as efficiently."
            ],
            "translated_annotated_samples": [
                "Introducción: En las primeras investigaciones en <br>sistemas multiagente</br> (SMA) se consideraban grupos cooperativos de agentes; debido a que los agentes individuales tenían recursos limitados o acceso limitado a la información (por ejemplo, potencia de procesamiento limitada, cobertura sensorial limitada), trabajaban juntos por diseño para resolver problemas que individualmente no podían resolver, o al menos no podían resolver de manera eficiente."
            ],
            "translated_text": "Un modelo de entorno adversarial para agentes racionales limitados en interacciones de suma cero Inon Zuckerman1, Sarit Kraus1, Jeffrey S. Rosenschein2, Gal Kaminka1 1 Departamento de Ciencias de la Computación 2 Escuela de Ingeniería Universidad Bar-Ilan y Ciencias de la Computación Ramat-Gan, Israel Universidad Hebrea, Jerusalén, Israel {zukermi, sarit, galk}@cs.biu.ac.il jeff@cs.huji.ac.il RESUMEN Los entornos multiagentes a menudo no son cooperativos ni colaborativos; en muchos casos, los agentes tienen intereses conflictivos, lo que lleva a interacciones adversariales. Este documento presenta un modelo formal de Entorno Adversarial para agentes racionales acotados que operan en un entorno de suma cero. En tales entornos, los intentos de utilizar métodos de búsqueda basados en utilidad clásica pueden plantear una variedad de dificultades (por ejemplo, modelar implícitamente al oponente como un maximizador de utilidad omnisciente, en lugar de aprovechar un modelo de oponente más matizado y explícito). Definimos un Entorno Adversario al describir los estados mentales de un agente en dicho entorno. Luego presentamos axiomas de comportamiento que están destinados a servir como principios de diseño para construir agentes adversarios. Exploramos la aplicación de nuestro enfoque mediante el análisis de archivos de registro de juegos de Conecta Cuatro completados, y presentamos un análisis empírico de la adecuación de los axiomas. Categorías y Descriptores de Asignaturas I.2.11 [Inteligencia Artificial]: Inteligencia Artificial Distribuida-Agentes Inteligentes, Sistemas Multiagente; I.2.4 [Inteligencia Artificial]: Formalismos y Métodos de Representación del Conocimiento - Lógica Modal Términos Generales Diseño, Teoría 1. Introducción: En las primeras investigaciones en <br>sistemas multiagente</br> (SMA) se consideraban grupos cooperativos de agentes; debido a que los agentes individuales tenían recursos limitados o acceso limitado a la información (por ejemplo, potencia de procesamiento limitada, cobertura sensorial limitada), trabajaban juntos por diseño para resolver problemas que individualmente no podían resolver, o al menos no podían resolver de manera eficiente. Sin embargo, la investigación de MAS pronto comenzó a considerar agentes interactivos con intereses individualizados, como representantes de diferentes humanos u organizaciones con intereses no idénticos. Cuando las interacciones son guiadas por intereses diversos, los participantes pueden tener que superar desacuerdos, interacciones no cooperativas e incluso intentos intencionales de dañarse mutuamente. Cuando ocurren este tipo de interacciones, los entornos requieren un comportamiento apropiado por parte de los agentes situados en ellos. Llamamos a estos entornos Entornos Adversarios, y llamamos a los agentes en conflicto Adversarios. Los modelos de cooperación y trabajo en equipo han sido ampliamente explorados en MAS a través de la axiomatización de estados mentales (por ejemplo, [8, 4, 5]). Sin embargo, ninguna de estas investigaciones abordó los dominios adversarios y sus implicaciones para el comportamiento de los agentes. Nuestro artículo aborda este problema al proporcionar un modelo formal y axiomatizado de estados mentales para un subconjunto de dominios adversariales, específicamente entornos adversariales simples de suma cero. Los encuentros de suma cero simples existen, por supuesto, en varios juegos de dos jugadores (por ejemplo, Ajedrez, Damas), pero también existen en juegos de n jugadores (por ejemplo, Risk, Diplomacia), subastas de un solo bien y en otros lugares. En estos entornos recientes en particular, el uso de una búsqueda adversarial basada en utilidad (como el algoritmo Min-Max) no siempre proporciona una solución adecuada; la función de recompensa podría ser bastante compleja o difícil de cuantificar, y existen limitaciones computacionales naturales en agentes racionales acotados. Además, los métodos de búsqueda tradicionales (como Min-Max) no hacen uso de un modelo del oponente, lo cual ha demostrado ser una adición valiosa a la planificación adversarial [9, 3, 11]. En este artículo, desarrollamos un modelo formal y axiomatizado para agentes racionales limitados que se encuentran en un entorno adversarial de suma cero. El modelo utiliza diferentes operadores de modalidad, y sus principales fundamentos son el modelo SharedPlans [4] para el comportamiento colaborativo. Exploramos las propiedades del entorno y los estados mentales de los agentes para derivar axiomas de comportamiento; estos axiomas de comportamiento constituyen un modelo formal que sirve como especificación y guía de diseño para el diseño de agentes en tales entornos. Luego investigamos el comportamiento de nuestro modelo empíricamente utilizando el juego de mesa Conecta Cuatro. Mostramos que este juego se ajusta a nuestra definición de entorno, y analizamos el comportamiento de los jugadores utilizando un gran conjunto de registros de partidas completadas 550 978-81-904262-7-5 (RPS) c 2007 archivos de IFAAMAS. Además, utilizamos los resultados presentados en [9] para discutir la importancia del modelado del oponente en nuestro dominio adversarial de Conecta Cuatro. El artículo continúa de la siguiente manera. La sección 2 presenta la formalización de los modelos. La sección 3 presenta el análisis empírico y sus resultados. Discutimos el trabajo relacionado en la Sección 4, y concluimos y presentamos las direcciones futuras en la Sección 5. ENTORNOS ADVERSARIOS El modelo de entorno adversario (denominado como AE) está destinado a guiar el diseño de agentes al proporcionar una especificación de las capacidades y actitudes mentales de un agente en un entorno adversario. Nos enfocamos aquí en tipos específicos de entornos adversarios, especificados de la siguiente manera: 1. Interacciones de suma cero: las utilidades positivas y negativas de todos los agentes suman cero; 2. Agentes adversarios simples: todos los agentes en el entorno son agentes adversarios; 3. AEs bilaterales: AEs con exactamente dos agentes; 4. AEs multilaterales: AEs de tres o más agentes. Trabajaremos en instanciaciones tanto bilaterales como multilaterales de entornos de suma cero y simples. En particular, nuestro modelo de entorno adversarial tratará con interacciones que consisten en N agentes (N ≥ 2), donde todos los agentes son adversarios y solo un agente puede tener éxito. Ejemplos de tales entornos van desde juegos de mesa (por ejemplo, Ajedrez, Conecta Cuatro y Diplomacia) hasta ciertos entornos económicos (por ejemplo, subastas de N postores sobre un único bien). 2.1 Resumen del Modelo Nuestro enfoque es formalizar las actitudes mentales y comportamientos de un único agente adversario; consideramos cómo un único agente percibe el AE. La siguiente lista especifica las condiciones y estados mentales de un agente en un AE simple de suma cero: 1. El agente tiene la intención individual de que su propio objetivo se complete; 2. El agente tiene la creencia individual de que él y sus adversarios persiguen objetivos completamente opuestos (definidos a continuación) y que solo puede haber un ganador; 3. El agente tiene la creencia individual de que cada adversario tiene la intención de completar su propio objetivo conflictivo completo; 4. El agente tiene una creencia individual en el perfil (parcial) de sus adversarios. El ítem 3 es necesario, ya que podría ser el caso de que algún agente tenga un objetivo completamente conflictivo y esté considerando adoptar la intención de completarlo, pero aún no está comprometido a lograrlo. Esto podría ocurrir porque el agente aún no ha reflexionado sobre los efectos que adoptar esa intención podría tener en las otras intenciones que actualmente mantiene. En tales casos, es posible que ni siquiera se considere en un entorno adversarial. El ítem 4 establece que el agente debe tener alguna creencia sobre los perfiles de sus adversarios. El perfil representa todo el conocimiento que el agente tiene sobre su adversario: sus debilidades, capacidades estratégicas, objetivos, intenciones, confiabilidad y más. Puede ser dado explícitamente o puede ser aprendido a partir de observaciones de encuentros pasados. 2.2 Definiciones de modelos para estados mentales Utilizamos las definiciones de los operadores modales, predicados y metapredicados de Grosz y Krauss, tal como se definen en su formalización de SharedPlan [4]. Recordamos aquí algunos de los predicados y operadores que se utilizan en esa formalización: Int.To(Ai, α, Tn, Tα, C) representa las intenciones de Ai en el tiempo Tn de realizar una acción α en el tiempo Tα en el contexto de C. Int.Th(Ai, prop, Tn, Tprop, C) representa las intenciones de Ai en el tiempo Tn de que una cierta proposición prop se cumpla en el tiempo Tprop en el contexto de C. Los operadores de intención potencial, Pot.Int.To(...) y Pot.Int.Th(...), se utilizan para representar el estado mental cuando un agente considera adoptar una intención, pero no ha deliberado sobre la interacción de las otras intenciones que tiene. El operador Bel(Ai, f, Tf) representa al agente Ai creyendo en la afirmación expresada en la fórmula f, en el tiempo Tf. MB(A, f, Tf) representa la creencia mutua para un grupo de agentes A. Una instantánea del sistema encuentra que nuestro entorno se encuentra en algún estado e ∈ E de estados variables ambientales, y cada adversario en cualquier LAi ∈ L de posibles estados locales. En cualquier paso de tiempo dado, el sistema estará en algún mundo w del conjunto de todos los mundos posibles w ∈ W, donde w = E×LA1 ×LA2 × ...LAn, y n es el número de adversarios. Por ejemplo, en un juego de póker Texas Holdem, el estado local de un agente podría ser su propio conjunto de cartas (que es desconocido para su adversario) mientras que el entorno consistirá en el bote de apuestas y las cartas comunitarias (que son visibles para ambos jugadores). Una función de utilidad bajo esta formalización se define como un mapeo de un mundo posible w ∈ W a un elemento en , que expresa la deseabilidad del mundo, desde la perspectiva de un único agente. Normalmente normalizamos el rango a [0,1], donde 0 representa el mundo menos deseable posible, y 1 es el mundo más deseable. La implementación de la función de utilidad depende del dominio en cuestión. La siguiente lista especifica nuevos predicados, funciones, variables y constantes utilizadas en conjunto con las definiciones originales para la formalización del entorno adversarial: 1. φ es una acción nula (el agente no hace nada). 2. GAi es el conjunto de objetivos del agente Ai. Cada objetivo es un conjunto de predicados cuya satisfacción hace que el objetivo esté completo (usamos G∗ Ai ∈ GAi para representar un objetivo arbitrario del agente Ai). 3. gAi es el conjunto de subobjetivos del agente Ai. Los subobjetivos son predicados cuya satisfacción representa un hito importante hacia el logro del objetivo completo. gG∗ Ai ⊆ gAi es el conjunto de subobjetivos que son importantes para la finalización del objetivo G∗ Ai (usaremos g∗ G∗ Ai ∈ gG∗ Ai para representar un subobjetivo arbitrario). 4. P Aj Ai es el objeto de perfil que el agente Ai tiene sobre el agente Aj. 5. CA es un conjunto general de acciones para todos los agentes en A que se derivan de las restricciones del entorno. CAi ⊆ CA es el conjunto de acciones posibles del agente Ai. La expresión Do(Ai, α, Tα, w) se cumple cuando Ai realiza la acción α durante el intervalo de tiempo Tα en el mundo w. 7. La afirmación Achieve(G∗ Ai , α, w) es verdadera cuando se logra el objetivo G∗ Ai siguiendo la finalización de la acción α en el mundo w ∈ W, donde α ∈ CAi. El perfil (Ai, PAi Ai) es verdadero cuando el agente Ai tiene un perfil de objeto para el agente Aj. Definición 1. El conflicto total (FulConf) describe una interacción de suma cero donde solo se puede completar un solo objetivo de los objetivos en conflicto. FulConf(G∗ Ai , G∗ Aj ) ⇒ (∃α ∈ CAi , ∀w, β ∈ CAj ) (Achieve(G∗ Ai , α, w) ⇒ ¬Achieve(G∗ Aj , β, w)) ∨ (∃β ∈ CAj , ∀w, α ∈ CAi )(Achieve(G∗ Aj , β, w) ⇒ ¬Achieve(G∗ Ai , α, w)) Definición 2. El Conocimiento Adversarial (AdvKnow) es una función que devuelve un valor que representa la cantidad de The Sixth Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 551 agente de conocimiento Ai tiene sobre el perfil del agente Aj, en el tiempo Tn. Cuanto mayor sea el valor, más conocimiento tiene el agente Ai. AdvKnow: P Aj Ai × Tn → Definición 3. Eval: Esta función de evaluación devuelve un valor de utilidad esperada estimado para un agente en A, después de completar una acción de CA en algún estado del mundo w. Eval: A × CA × w → Definición 4. TrH - (Umbral) es una constante numérica en el rango [0,1] que representa un valor umbral de la función de evaluación (Eval). Una acción que produce una evaluación de utilidad estimada por encima del TrH se considera una acción altamente beneficiosa. El valor Eval es una estimación y no la función de utilidad real, la cual suele ser desconocida. Utilizar el valor real de utilidad para un agente racional fácilmente produciría el mejor resultado para ese agente. Sin embargo, los agentes generalmente no tienen las funciones de utilidad reales, sino más bien una estimación heurística de las mismas. Existen dos propiedades importantes que deben cumplir para la función de evaluación: Propiedad 1. La función de evaluación debería indicar que el estado del mundo más deseable es aquel en el que se logra el objetivo. Por lo tanto, una vez que se ha satisfecho el objetivo, no puede haber ninguna acción futura que pueda colocar al agente en un estado del mundo con un valor de Eval más alto. (∀Ai, G∗ Ai , α, β ∈ CAi , w ∈ W) Lograr(G∗ Ai , α, w) ⇒ Eval(Ai, α, w) ≥ Eval(Ai, β, w) Propiedad 2. La función de evaluación debe proyectar una acción que cause la finalización de un objetivo o subobjetivo a un valor que sea mayor que TrH (una acción altamente beneficiosa). (∀Ai, G∗ Ai ∈ GAi , α ∈ CAi , w ∈ W, g∗ GAi ∈ gGAi ) Lograr(G∗ Ai , α, w) ∨ Lograr(g∗ GAi , α, w) ⇒ Eval(Ai, α, w) ≥ TrH. Definición 5. La acción de conjunto Definimos una acción de conjunto (SetAction) como un conjunto de operaciones de acción (ya sean acciones complejas o básicas) de algunos conjuntos de acción CAi y CAj que, según la creencia del agente Ai, están unidas por una relación temporal y consecuente, formando una cadena de eventos (acción y su acción consecuente siguiente). (∀α1 , . . . , αu ∈ CAi , β1 , . . . , βv ∈ CAj , w ∈ W) SetAction(α1 , . . . , αu , β1 , . . . , βv , w) ⇒ ((Do(Ai, α1 , Tα1 , w) ⇒ Do(Aj, β1 , Tβ1 , w)) ⇒ Do(Ai, α2 , Tα2 , w) ⇒ . . . ⇒ Do(Ai, αu , Tαu , w)) La relación consecuente podría existir debido a varias restricciones ambientales (cuando una acción obliga al adversario a responder con una acción específica) o debido al conocimiento de los agentes sobre el perfil de su adversario. Propiedad 3. A medida que aumenta el conocimiento que tenemos sobre nuestro adversario, tendremos creencias adicionales sobre su comportamiento en diferentes situaciones, lo que a su vez crea un nuevo conjunto de acciones. Formalmente, si nuestro AdvKnow en el tiempo Tn+1 es mayor que AdvKnow en el tiempo Tn, entonces cada SetAction conocido en el tiempo Tn también es conocido en el tiempo Tn+1. AdvKnow(P Aj Ai , Tn+1) > AdvKnow(P Aj Ai , Tn) ⇒ (∀α1 , . . . , αu ∈ CAi , β1 , . . . , βv ∈ CAj ) Bel(Aag, SetAction(α1 , . . . , αu , β1 , . . . , βv ), Tn) ⇒ Bel(Aag, SetAction(α1 , . . . , αu , β1 , . . . , βv ), Tn+1) 2.3 La formulación del entorno Los siguientes axiomas proporcionan la definición formal para un Entorno Adversarial (AE) simple y de suma cero. La satisfacción de estos axiomas significa que el agente está situado en un entorno así. Proporciona especificaciones para que el agente Aag interactúe con su conjunto de adversarios A con respecto a los objetivos G∗ Aag y G∗ A en el tiempo TCo en algún estado del mundo w. AE(Aag, A, G∗ Aag, A1, . . . , Ak, G∗ A1, . . . , G∗ Ak, Tn, w) 1. Aag tiene un Int.Th su objetivo se completaría: (∃α ∈ CAag , Tα) Int.Th(Aag, Lograr(G∗ Aag , α), Tn, Tα, AE) 2. Aag cree que él y cada uno de sus adversarios Ao están persiguiendo metas completamente conflictivas: (∀Ao ∈ {A1, . . . , Ak}) Bel(Aag, FulConf(G∗ Aag , G∗ Ao ), Tn) 3. Aag cree que cada uno de sus adversarios en Ao tiene la Int.Th su objetivo en conflicto G∗ Aoi se completará: (∀Ao ∈ {A1, . . . , Ak})(∃β ∈ CAo , Tβ) Bel(Aag, Int.Th(Ao, Lograr(G∗ Ao , β), TCo, Tβ, AE), Tn) 4. Para construir un agente que pueda operar con éxito dentro de un entorno de adversarios estratégicos, debemos especificar pautas de comportamiento para sus interacciones. Utilizar una estrategia de maximización de Eval ingenua hasta cierta profundidad de búsqueda no siempre dará resultados satisfactorios por varias razones: (1) el problema del horizonte de búsqueda al buscar a una profundidad fija; (2) la fuerte suposición de un adversario óptimamente racional con recursos ilimitados; (3) utilizar una función de evaluación estimada que no dará resultados óptimos en todos los estados del mundo, y que puede ser explotada [9]. Los siguientes axiomas especifican los principios de comportamiento que se pueden utilizar para diferenciar entre agentes exitosos y menos exitosos en el entorno adversarial mencionado anteriormente. Esos axiomas deben ser utilizados como principios de especificación al diseñar e implementar agentes que deben ser capaces de desempeñarse bien en entornos adversarios. Los axiomas conductuales representan situaciones en las que el agente adoptará intenciones potenciales para (Pot.Int.To(...)) realizar una acción, lo que típicamente requerirá un razonamiento de medios-fines para seleccionar un posible curso de acción. Este razonamiento llevará a la adopción de un Int.To(...) (ver [4]). A1. Axioma para lograr metas. El primer axioma es el caso más simple; cuando el agente Aag cree que está a una acción (α) de lograr su objetivo en conflicto G∗ Aag, debería adoptar la intención potencial de hacer α y completar su objetivo. (∀Aag, α ∈ CAag, Tn, Tα, w ∈ W) (Bel(Aag, Do(Aag, α, Tα, w) ⇒ Achieve(G∗ Aag, α, w)) ⇒ Pot.Int.To(Aag, α, Tn, Tα, w) Este comportamiento algo trivial es el primer y más fuerte axioma. En cualquier situación, cuando el agente está a una acción de completar el objetivo, debería completar la acción. Cualquier función de evaluación justa clasificaría naturalmente a α como la acción de valor máximo (propiedad 1). Sin embargo, sin una axiomatización explícita de dicho comportamiento, podría haber situaciones en las que el agente decida tomar otra acción por diversas razones, debido a sus recursos de decisión limitados. A2. Axioma del Acto Preventivo. En una situación adversarial, el agente Aag podría decidir tomar acciones que dañen uno de los planes de su adversario para completar su objetivo, incluso si esas acciones no avanzan explícitamente a Aag hacia su objetivo en conflicto G∗ Aag. Tal acción preventiva tendrá lugar cuando el agente Aag tenga la creencia sobre la posibilidad de que su adversario Ao realice una acción β que le otorgará un alto 552 The Sixth Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) valor de evaluación de utilidad (> TrH). Creer que tomar la acción α evitará que el oponente realice su β, adoptará una intención potencial de hacer α. (∀Aag, Ao ∈ A, α ∈ CAag , β ∈ CAo , Tn, Tβ, w ∈ W) (Bel(Aag, Do(Ao, β, Tβ, w) ∧ Eval(Ao, β, w) > TrH, Tn) ∧ Bel(Aag, Do(Aag, α, Tα, w) ⇒ ¬Do(Ao, β, Tβ, w), Tn) ⇒ Pot.Int.To(Aag, α, Tn, Tα, w) Este axioma es un componente básico de cualquier entorno adversarial. Por ejemplo, al observar un juego de ajedrez, un jugador podría darse cuenta de que está a punto de ser jaque mateado por su oponente, por lo tanto, realiza una jugada preventiva. Otro ejemplo es el juego de Cuatro en Línea: cuando un jugador tiene una fila de tres fichas, su oponente debe bloquearla o perder. Una instancia específica de A1 ocurre cuando el adversario está a una acción de lograr su objetivo, y la acción preventiva inmediata debe ser tomada por el agente. Formalmente, tenemos las mismas creencias que se mencionaron anteriormente, con la creencia modificada de que realizar la acción β hará que el agente Ao logre su objetivo. Proposición 1: Caso de prevención o pérdida. (∀Aag, Ao ∈ A, α ∈ CAag , β ∈ CAo , G∗ Ao , Tn, Tα, Tβ, w ∈ W) Bel(Aag, Do(Ao, β, Tβ, w) ⇒ Achieve(G∗ Ao , β, w), Tn) ∧ Bel(Aag, Do(Aag, α, Tα, w) ⇒ ¬Do(Ao, β, Tβ, w)) ⇒ Pot.Int.To(Aag, α, Tn, Tα, w) Esquema de prueba: La Proposición 1 puede derivarse fácilmente del axioma A1 y de la propiedad 2 de la función Eval, que establece que cualquier acción que cause la finalización de un objetivo es una acción altamente beneficiosa. El comportamiento del acto preventivo ocurrirá implícitamente cuando la función Eval sea igual a la función de utilidad del mundo real. Sin embargo, al ser agentes racionales limitados y al tratar con una función de evaluación estimada, necesitamos axiomatizar explícitamente dicho comportamiento, ya que no siempre ocurrirá implícitamente a partir de la función de evaluación. A3. Axioma de Movimiento Táctico Subóptimo. En muchos escenarios, puede ocurrir una situación en la que un agente decida no tomar la acción más beneficiosa actual que puede tomar (la acción con el valor de evaluación de utilidad máximo), porque cree que tomar otra acción (con un valor de evaluación de utilidad más bajo) podría generar (dependiendo de la respuesta del adversario) una posibilidad futura para una acción altamente beneficiosa. Esto ocurrirá con mayor frecuencia cuando la función Eval sea inexacta y difiera en gran medida de la función de Utilidad. De manera formal, el agente Aag cree en una cierta SetAction que evolucionará de acuerdo con su acción inicial y producirá un valor beneficioso alto (> TrH) únicamente para él. (∀Aag, Ao ∈ A, Tn, w ∈ W) (∃α1 , . . . , αu ∈ CAi , β1 , . . . , βv ∈ CAj , Tα1 ) Bel(Aag, SetAction(α1 , . . . , αu , β1 , . . . , βv ), Tn) ∧ Bel(Aag, Eval(Ao, βv , w) < TrH < Eval(Aag, αu , w), Tn) ⇒ Pot.Int.To(Aag, α1 , Tn, Tα1 , w) Un agente podría creer que una cadena de eventos ocurrirá por diversas razones debido a la naturaleza inevitable del dominio. Por ejemplo, en el ajedrez, a menudo observamos lo siguiente: una jugada provoca una posición de jaque, lo que a su vez limita los movimientos del oponente para evitar el jaque, a lo que el primer jugador podría reaccionar con otro jaque, y así sucesivamente. El agente también podría creer en una cadena de eventos basada en su conocimiento del perfil de su adversario, lo que le permite prever los movimientos del adversario con alta precisión. A4. Axioma de Detección de Perfiles. El agente puede ajustar los perfiles de sus adversarios mediante observaciones y estudio de patrones (específicamente, si hay encuentros repetidos con el mismo adversario). Sin embargo, en lugar de esperar a que se revele la información del perfil, un agente también puede iniciar acciones que obligarán a su adversario a reaccionar de una manera que revele conocimiento del perfil sobre él. Formalmente, el axioma establece que si todas las acciones (γ) no son acciones altamente beneficiosas (< TrH), el agente puede realizar la acción α en el tiempo Tα si cree que resultará en una acción no altamente beneficiosa β por parte de su adversario, lo que a su vez le enseña sobre el perfil del adversario, es decir, proporciona un mayor AdvKnow(P Aj Ai , Tβ). (∀Aag, Ao ∈ A, α ∈ CAag , β ∈ CAo , Tn, Tα, Tβ, w ∈ W) Bel(Aag, (∀γ ∈ CAag )Eval(Aag, γ, w) < TrH, Tn) ∧ Bel(Aag, Do(Aag, α, Tα, w) ⇒ Do(Ao, β, Tβ, w), Tn) ∧ Bel(Aag, Eval(Ao, β, w) < TrH) ∧ Bel(Aag, AdvKnow(P Aj Ai , Tβ) > AdvKnow(P Aj Ai , Tn), Tn) ⇒ Pot.Int.To(Aag, α, Tn, Tα, w) Por ejemplo, volviendo al escenario del juego de ajedrez, consideremos comenzar una partida contra un oponente del que no sabemos nada, ni siquiera si es humano o un oponente computarizado. Podríamos comenzar a jugar una estrategia que sea adecuada frente a un oponente promedio, y ajustar nuestro juego de acuerdo a su nivel de juego. A5. Axioma de Formación de Alianzas El siguiente axioma de comportamiento es relevante únicamente en una instancia multilateral del entorno adversarial (obviamente, una alianza no puede formarse en un encuentro bilateral de suma cero). En diferentes situaciones durante una interacción multilateral, un grupo de agentes podría creer que es de su interés formar una alianza temporal. Una alianza de este tipo es un acuerdo que limita el comportamiento de sus miembros, pero se cree por parte de estos que les permite alcanzar un valor de utilidad mayor al que podrían lograr fuera de la alianza. Como ejemplo, podemos observar el clásico juego de mesa Risk, donde cada jugador tiene como objetivo individual ser el único conquistador del mundo, un juego de suma cero. Sin embargo, para lograr este objetivo, podría ser estratégicamente sabio hacer acuerdos de alto el fuego a corto plazo con otros jugadores, o unirse y atacar a un oponente que sea más fuerte que el resto. Los términos de una alianza definen la forma en que sus miembros deben actuar. Es un conjunto de predicados, denominados Términos, que es acordado por los miembros de la alianza y debe mantenerse verdadero durante la duración de la alianza. Por ejemplo, el conjunto de términos en el escenario de riesgo podría contener los siguientes predicados: 1. Los miembros de la alianza no se atacarán entre sí en los territorios X, Y y Z; 2. Los miembros de la alianza contribuirán C unidades por turno para atacar al adversario Ao; 3. Los miembros están obligados a permanecer como parte de la alianza hasta el tiempo Tk o hasta que el ejército adversario Ao sea más pequeño que Q. El conjunto de términos especifica restricciones intergrupales en cada uno de los miembros de la alianza (∀Aal i ∈ Aal ⊆ A) conjunto de acciones Cal i ⊆ C. Definición 6. El valor total de evaluación que el agente Ai logrará al ser parte de Aal es la suma de Evali (Eval para Ai) de cada uno de los valores de evaluación de Aal j después de tomar sus propias acciones α (a través del predicado del agente(α)): Al val(Ai, Cal, Aal, w) = α∈Cal Evali(Aal j, agente(α), w) Definición 7. Al TrH - es un número que representa un valor de Al en la Sexta Conferencia Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 553 umbral; por encima de él, se puede decir que la alianza es una alianza altamente beneficiosa. El valor de Al TrH se calculará dinámicamente de acuerdo con el progreso de la interacción, como se puede ver en [7]. Después de formarse una alianza, sus miembros ahora están trabajando en su entorno adversarial normal, así como de acuerdo con los estados mentales y axiomas requeridos para sus interacciones como parte de la alianza. El siguiente modelo de Alianza (AL) especifica las condiciones bajo las cuales el grupo Aal puede considerarse en una alianza y trabajando con un nuevo y limitado conjunto de acciones Cal, en el tiempo Tn. AL(Aal , Cal , w, Tn) 1. \n\nAL(Aal , Cal , w, Tn) 1. Aal tiene un MB en el que todos los miembros son parte de Aal: MB(Aal, (∀Aal i ∈ Aal)miembro(Aal i, Aal), Tn) 2. Aal tiene una MB que el grupo se mantenga: MB(Aal , (∀Aal i ∈ Aal )Int.Th (Ai, miembro(Ai, Aal ), Tn, Tn+1, Co), Tn) 3. Aal tiene un beneficio marginal que al ser miembros les otorga un alto valor de utilidad: MB(Aal , (∀Aal i ∈ Aal )Al val(Aal i , Cal , Aal , w) ≥ Al TrH, Tn) Los perfiles de los miembros son una parte crucial de las alianzas exitosas. Suponemos que los agentes que tienen perfiles más precisos de sus adversarios serán más exitosos en tales entornos. Tales agentes podrán predecir cuándo un miembro está a punto de violar el contrato de alianzas (elemento 2 en el modelo anterior) y tomar medidas correctivas (cuando el elemento 3 se falsificará). La robustez de la alianza depende en parte de la medida de confianza de sus miembros, la estimación de su posición objetiva y otras propiedades de su perfil. Debemos tener en cuenta que un agente puede formar parte simultáneamente de más de una alianza. Una alianza temporal, donde los miembros del grupo no tienen un objetivo común pero actúan colaborativamente en interés de sus propias metas individuales, es clasificada como un Grupo de Tratamiento por los psicólogos modernos (en contraste con un Grupo de Tarea, donde sus miembros tienen un objetivo común). El modelo de Actividad Compartida presentado en [5] modeló el comportamiento del Grupo de Tratamiento utilizando la misma formalización de Planes Compartidos. Al comparar ambas definiciones de una alianza y un Grupo de Tratamiento, encontramos una similitud sorprendente entre ambos modelos: las definiciones de los modelos de entorno son casi idénticas (ver las definiciones de SAs en [5]), y sus axiomas de Acto Egoísta y Acto Cooperativo se ajustan al comportamiento de nuestros agentes adversarios. La principal distinción entre ambos modelos es la integración de un axioma de acto de comportamiento útil, en la Actividad Compartida que no puede formar parte de la nuestra. Este axioma establece que un agente considerará tomar una acción que disminuirá su valor de Evaluación (hasta un cierto límite inferior), si cree que un compañero del grupo obtendrá un beneficio significativo. Dicho comportamiento no puede ocurrir en un entorno puramente adversarial (como un juego de suma cero), donde los miembros de la alianza están constantemente alerta para manipular su alianza en su propio beneficio. A6. Axioma de Maximización de la Evaluación. En un caso en el que todos los demás axiomas no sean aplicables, procederemos con la acción que maximice el valor heurístico calculado en la función Eval. (∀Aag, Ao ∈ A, α ∈ Cag, Tn, w ∈ W) Bel(Aag, (∀γ ∈ Cag)Eval(Aag, α, w) ≥ Eval(Aag, γ, w), Tn) ⇒ Pot.Int.To(Aag, α, Tn, Tα, w) T1. Optimalidad en Eval = Utilidad El modelo axiomático anterior aborda situaciones en las que la Utilidad es desconocida y los agentes son agentes racionales limitados. El siguiente teorema muestra que en interacciones bilaterales, donde los agentes tienen la función de Utilidad real (es decir, Eval = Utilidad) y son agentes racionales, los axiomas proporcionan el mismo resultado óptimo que la búsqueda adversarial clásica (por ejemplo, Min-Max). Teorema 1. Que Ae ag sea un agente AE racional ilimitado que utiliza la función de evaluación heurística Eval, Au ag sea el mismo agente utilizando la verdadera función de utilidad y Ao sea un único adversario racional ilimitado basado en utilidad. Dado que Eval = Utilidad: (∀α ∈ CAu ag , α ∈ CAe ag , Tn, w ∈ W) Pot.Int.To(Au ag, α, Tn, Tα, w) → Pot.Int.To(Ae ag, α , Tn, Tα, w) ∧ ((α = α ) ∨ (Utilidad(Au ag, α, w) = Eval(Ae ag, α , w))) Esquema de prueba - Dado que Au ag tiene la función de utilidad real y recursos ilimitados, puede generar el árbol de juego completo y ejecutar el algoritmo MinMax óptimo para elegir la acción de mayor valor de utilidad, que denotamos como α. La prueba mostrará que Ae ag, utilizando los axiomas AE, seleccionará la misma utilidad α o igual (cuando hay más de una acción con la misma utilidad máxima) cuando Eval = Utilidad. (A1) Axioma de logro de objetivo - supongamos que hay un α tal que su cumplimiento logrará el objetivo Au ags. Obtendrá la utilidad más alta mediante Min-Max para Au ag. El agente Ae seleccionará α u otra acción con el mismo valor de utilidad a través de A1. Si tal α no existe, Ae ag no puede aplicar este axioma y procede a A2. (A2) Axioma de la acción preventiva - (1) Observando el caso base (ver Prop1), si hay un β que lleva a Ao a alcanzar su objetivo, entonces una acción preventiva α producirá la mayor utilidad para Au ag. Au ag lo elegirá a través de la utilidad, mientras que Ae ag lo elegirá a través de A2. En el caso general, β es una acción altamente beneficiosa para Ao, lo que resulta en una baja utilidad para Au ag, lo que lo guiará a seleccionar un α que evitará β, mientras que Ae ag lo elegirá a través de A2.1 Si dicho β no existe para Ao, entonces A2 no es aplicable, y Ae ag puede proceder a A3. (A3) Axioma de movimiento táctico subóptimo: al usar una función de evaluación heurística, Ae ag tiene una creencia parcial en el perfil de su adversario (elemento 4 en el modelo AE), lo que puede llevarlo a creer en SetActions (Prop1). En nuestro caso, Ae ag tiene un perfil completo de su adversario óptimo y sabe que Ao se comportará de manera óptima según los valores reales de utilidad en el árbol de búsqueda completo, por lo tanto, cualquier creencia sobre una SetAction subóptima no puede existir, lo que hace que este axioma sea inaplicable. Ae ag procederá a A4. (A4) Axioma de detección de perfil - Dado que Ae ag tiene el perfil completo de Ao, ninguna de las acciones de Ae ag puede aumentar su conocimiento. Ese axioma no se aplicará, y el agente procederá con A6 (A5 será ignorado porque la interacción es bilateral). (A6) Axioma de maximización de la evaluación: este axioma seleccionará la máxima Evaluación para Ae ag. Dado que Eval = Utilidad, se seleccionará el mismo α que fue seleccionado por Au ag. 3. EVALUACIÓN El propósito principal de nuestro análisis experimental es evaluar el comportamiento y rendimiento de los modelos en un entorno adversarial real. Esta sección investiga si un caso acotado 1 en el que, tras la finalización de β, exista un γ que proporcione una alta utilidad para el Agente Au ag, no puede ocurrir porque Ao utiliza la misma utilidad, y la existencia de γ hará que clasifique a β como una acción de baja utilidad. 554 The Sixth Intl. En la Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07), los agentes racionales situados en entornos adversariales se beneficiarán más al aplicar nuestros axiomas de comportamiento sugeridos. 3.1 El Dominio Para explorar el uso del modelo anterior y sus axiomas de comportamiento, decidimos utilizar el juego de Conecta Cuatro como nuestro entorno adversarial. Connect-Four es un juego de 2 jugadores, de suma cero, que se juega utilizando un tablero similar a una matriz de 6x7. En cada turno, un jugador coloca un disco en una de las 7 columnas (el conjunto de 21 discos suele ser de color amarillo para el jugador 1 y rojo para el jugador 2; usaremos Blanco y Negro respectivamente para evitar confusiones). El ganador es el primer jugador en completar un conjunto de cuatro discos de su color en horizontal, vertical o diagonal. En ocasiones muy raras, el juego podría terminar en empate si todos los cuadros vacíos están llenos, pero ningún jugador logró crear un conjunto de 4 discos. El juego de Conecta Cuatro fue resuelto en [1], donde se muestra que el primer jugador (jugando con los discos blancos) puede forzar una victoria comenzando en la columna del medio (columna 4) y jugando de manera óptima. Sin embargo, la estrategia óptima es muy compleja y difícil de seguir incluso para agentes racionales limitados complejos, como los jugadores humanos. Antes de que podamos proceder a verificar el comportamiento del agente, primero debemos verificar que el dominio se ajusta a la definición de entornos adversarios mencionada anteriormente (en la que se basan los axiomas de comportamiento). Primero, al jugar al juego de Conecta Cuatro, el agente tiene la intención de ganar el juego (ítem 1). Segundo (ítem 2), nuestro agente cree que en Conecta Cuatro solo puede haber un ganador (o ningún ganador en la rara ocasión de un empate). Además, nuestro agente cree que su oponente en el juego intentará ganar (ítem 3), y esperamos que tenga algún conocimiento parcial (ítem 4) sobre su adversario (este conocimiento puede variar desde nada, pasando por hechos simples como la edad, hasta estrategias y debilidades). Por supuesto, no todos los encuentros de Conecta Cuatro son adversariales. Por ejemplo, cuando un padre está jugando el juego con su hijo, la siguiente situación podría ocurrir: el niño, con un fuerte incentivo para ganar, trata el entorno como adversarial (intenta ganar, entiende que solo puede haber un ganador y cree que su padre está tratando de vencerlo). Sin embargo, desde el punto de vista de los padres, el entorno podría ser visto como educativo, donde el objetivo no es ganar el juego, sino causar disfrute o practicar el razonamiento estratégico. En un entorno educativo como este, un nuevo conjunto de axiomas de comportamiento podría ser más beneficioso para los objetivos de los padres que nuestros axiomas de comportamiento adversarial sugeridos. 3.2 Análisis de Axiomas Después de demostrar que el juego de Conecta Cuatro es de hecho un entorno adversarial bilateral de suma cero, el siguiente paso es observar el comportamiento de los jugadores durante el juego y verificar si comportarse de acuerdo con nuestro modelo mejora el rendimiento. Para hacerlo, hemos recopilado archivos de registro de juegos de Conecta Cuatro completados que fueron jugados por jugadores humanos a través de Internet. Nuestros datos de archivo de registro recopilados provienen de sitios de Juego por Correo Electrónico (PBeM). Estos son sitios web que alojan juegos por correo electrónico, donde cada movimiento se realiza a través de un intercambio de correos electrónicos entre el servidor y los jugadores. Muchos de estos sitios de archivos contienen interacciones competitivas reales y también mantienen un sistema de clasificación para sus miembros. La mayoría de los datos que utilizamos se pueden encontrar en [6]. Como se puede aprender de [1], Conecta Cuatro tiene una estrategia óptima y una ventaja considerable para el jugador que comienza el juego (a quien llamamos jugador Blanco). Nos concentraremos en nuestro análisis en los movimientos de los segundos jugadores (a los que llamaremos Negras). El jugador Blanco, al ser el primero en actuar, tiene la llamada ventaja de la iniciativa. Tener la ventaja y una buena estrategia mantendrá al jugador Negro ocupado reaccionando a sus movimientos, en lugar de iniciar amenazas. Una amenaza es una combinación de tres discos del mismo color, con un espacio vacío para el cuarto disco ganador. Una amenaza abierta es una amenaza que puede materializarse en el próximo movimiento del oponente. Para que el jugador Negro gane, debe de alguna manera cambiar el rumbo, tomar la ventaja y comenzar a presentar amenazas al jugador Blanco. Exploraremos el comportamiento de los jugadores negros y su conformidad con nuestros axiomas. Para hacerlo, construimos una aplicación que lee archivos de registro y analiza los movimientos de los jugadores Negros. La aplicación contiene dos componentes principales: (1) un algoritmo Min-Max para la evaluación de movimientos; (2) un detector de amenazas abiertas para descubrir amenazas abiertas. El algoritmo Min-Max funcionará hasta una profundidad d dada y, para cada movimiento α, devolverá el valor heurístico para la próxima acción tomada por el jugador según lo escrito en el archivo de registro, h(α), junto con el valor heurístico máximo, maxh(α), que podría lograrse antes de realizar el movimiento (obviamente, si h(α) = maxh(α), entonces el jugador no realizó el movimiento óptimo desde el punto de vista heurístico). El trabajo de los detectores de amenazas es notificar si se tomó alguna acción para bloquear una amenaza abierta (no bloquear una amenaza abierta probablemente hará que el jugador pierda en el próximo movimiento del oponente). La función heurística utilizada por Min-Max para evaluar la utilidad de los jugadores es la siguiente función, que es simple de calcular, pero proporciona un desafío razonable a los oponentes humanos: Definición 8. Sea Grupo un conjunto adyacente de cuatro cuadrados que son horizontales, verticales o diagonales. El Grupo n (Grupo w) es un Grupo con n piezas de color negro (blanco) y 4−n casillas vacías. h = ((Grupo1 negro ∗α)+(Grupo2 negro ∗β)+(Grupo3 negro ∗γ)+(Grupo4 negro ∗∞)) − ((Grupo1 blanco ∗α)+(Grupo2 blanco ∗β)+(Grupo3 blanco ∗γ)+(Grupo4 blanco ∗∞)) Los valores de α, β y δ pueden variar para formar cualquier combinación lineal deseada; sin embargo, es importante asignarles valores teniendo en cuenta el orden α < β < δ (usamos 1, 4 y 8 como sus valores respectivos). Grupos de 4 discos del mismo color significan victoria, por lo tanto, el descubrimiento de dicho grupo resultará en ∞ para asegurar un valor extremo. Ahora utilizamos nuestra función de evaluación estimada para evaluar las acciones de los jugadores Negros durante la interacción adversarial de Conecta Cuatro. Cada juego del archivo de registro fue introducido en la aplicación, la cual procesó y generó un archivo de registro reformateado que contenía el valor h del movimiento actual, el valor maxh que podría ser alcanzado y una notificación si se detectaba una amenaza abierta. Un total de 123 partidas fueron analizadas (57 con victoria de las blancas y 66 con victoria de las negras). Algunos juegos adicionales fueron ignorados manualmente en el experimento, debido a estos problemas: un jugador abandonando el juego mientras el resultado no es definitivo, o una jugada irracional y contundente en las primeras etapas del juego (por ejemplo, no bloquear un grupo obviamente ganador en las primeras jugadas de apertura). Además, también se eliminó un empate. El simulador se ejecutó con una profundidad de búsqueda de 3 movimientos. Ahora procedemos a analizar los juegos con respecto a cada axioma de comportamiento. El Sexto Internacional. La siguiente sección presenta las evaluaciones heurísticas del algoritmo Min-Max para cada acción, y verifica la cantidad y el alcance de las acciones tácticas subóptimas y sus implicaciones en el rendimiento. La Tabla 1 muestra los resultados y las percepciones del análisis heurístico de los juegos, cuando la profundidad de búsqueda es igual a 3 (esta profundidad de búsqueda fue seleccionada para que los resultados sean comparables a [9], ver Sección 3.2.3). La información heurística de las tablas es la diferencia entre el valor heurístico máximo presente y el valor heurístico de la acción que finalmente tomó el jugador (es decir, cuanto más cercano sea el número a 0, más cercana estuvo la acción a la acción heurística máxima). La primera fila presenta los valores de diferencia de la acción que tuvo el valor de diferencia máximo entre todas las acciones de los jugadores Negros en un juego dado, promediado en todos los juegos ganados y perdidos por los Negros (ver columnas respectivas). En los juegos en los que el jugador Negro pierde, su valor promedio de diferencia fue de -17.62, mientras que en los juegos en los que el jugador Negro ganó, su promedio fue de -12.02. La segunda fila amplía el análisis al considerar las 3 acciones con mayor diferencia heurística, y promediarlas. En ese caso, notamos una diferencia heurística promedio de 5 puntos entre los juegos en los que el jugador Negro pierde y los juegos en los que gana. Sin embargo, la importancia de esos números es que nos permitieron hacer una suposición educada sobre un número umbral de 11.5, como el valor de la constante TrH, que diferencia entre acciones normales y altamente beneficiosas. Después de encontrar una constante TrH aproximada, podemos proceder con un análisis de la importancia de los movimientos subóptimos. Para hacerlo, tomamos el subconjunto de juegos en los que el valor mínimo de diferencia heurística para las acciones de las negras era de 11.5. Como se presenta en la Tabla 2, podemos ver el promedio de 3 horas mínimo de los 3 mayores rangos y el porcentaje respectivo de juegos ganados. La primera fila muestra que el jugador Negro ganó solo el 12% de los juegos en los que el promedio de sus 3 acciones con mayor diferencia heurística (min3 h) era menor que el umbral sugerido, TrH = 11.5. La segunda fila muestra un resultado sorprendente: parece que cuando min3 h > −4, el jugador Negro rara vez gana. La intuición sugeriría que los juegos en los que los valores de evaluación de la acción estén más cerca de los valores máximos resultarán en más juegos ganados para las piezas negras. Sin embargo, parece que en el dominio de Conecta Cuatro, simplemente responder con acciones algo fácilmente esperadas, sin iniciar algunos movimientos sorprendentes y subóptimos, no produce buenos resultados. La última fila resume las principales conclusiones del análisis; la mayoría de las victorias de Blacks (83%) se produjeron cuando su min3 h estaba en el rango de -11.5 a -4. Una inspección detallada de esos juegos ganados por las fichas negras muestra el siguiente patrón detrás de los números: después de los movimientos de apertura estándar, las fichas negras de repente colocan una ficha en una columna aislada, lo cual parece ser un desperdicio de un movimiento. El blanco sigue construyendo sus amenazas, generalmente ignorando el último movimiento de las negras, que a su vez utiliza el disco aislado como ancla para una futura amenaza ganadora. Los resultados muestran que fue beneficioso para el jugador Negro en la Tabla 2: Porcentajes de ganancias de los Negros % de juegos min3 h < −11.5 12% min3 h > −4 5% −11.5 ≤ min3 h ≤ −4 83% tomar acciones subóptimas y no dar el valor heurístico más alto posible en ese momento, pero no será demasiado perjudicial para su posición (es decir, no dará un alto valor beneficioso a su adversario). Resultó que aprender el umbral es un aspecto importante del éxito: realizar movimientos extremadamente arriesgados (min3 h < −11.5) o intentar evitarlos (min3 h > −4) reduce significativamente las posibilidades de ganar de los jugadores negros. 3.2.2 Afirmación del Axioma de Monitoreo de Perfiles En la tarea de demostrar la importancia de monitorear los perfiles de los adversarios, nuestros archivos de registro no pudieron ser utilizados porque no contenían interacciones repetidas entre jugadores, que son necesarias para inferir el conocimiento de los jugadores sobre sus adversarios. Sin embargo, la importancia del modelado del oponente y su uso para obtener ventajas tácticas ya ha sido estudiada en varios ámbitos ([3, 9] son buenos ejemplos). En un artículo reciente, Markovitch y Reger [9] exploraron la noción de aprendizaje y explotación de la debilidad del oponente en interacciones competitivas. Aplican estrategias de aprendizaje simples analizando ejemplos de interacciones pasadas en un dominio específico. También utilizaron el dominio adversarial de Conecta Cuatro, que ahora se puede utilizar para comprender la importancia de monitorear el perfil del adversario. Tras la presentación de su modelo teórico, describen un extenso estudio empírico y verifican el rendimiento de los agentes después de aprender el modelo de debilidades con ejemplos pasados. Uno de los dominios utilizados como entorno competitivo fue el mismo juego de Conecta Cuatro (las Damas fueron el segundo dominio). Su función heurística era idéntica a la nuestra con tres variaciones diferentes (H1, H2 y H3) que se distinguen entre sí por los valores de sus coeficientes de combinación lineal. La profundidad de búsqueda para los jugadores fue de 3 (como en nuestro análisis). Sus extensos experimentos verifican y comparan diversas estrategias de aprendizaje, factores de riesgo, conjuntos de características predefinidas y métodos de uso. La conclusión es que el dominio de Conecta Cuatro muestra una mejora de una tasa de victoria de 0.556 antes de modelar a 0.69 después de modelar (página 22). Sus conclusiones, que muestran un rendimiento mejorado al sostener y utilizar el modelo del adversario, justifican el esfuerzo de monitorear el perfil del adversario para interacciones continuas y repetidas. Un punto adicional que surgió en sus experimentos es el siguiente: después de que se haya aprendido el modelo de debilidad del oponente, los autores describen diferentes métodos para integrar el modelo de debilidad del oponente en la estrategia de decisión de los agentes. Sin embargo, independientemente del método específico que elijan para trabajar, todos los métodos de integración podrían hacer que el agente tome decisiones subóptimas; podría hacer que el agente prefiera acciones que son subóptimas en el momento actual de la decisión, pero que podrían hacer que el oponente reaccione de acuerdo con su modelo de debilidad (como lo representa nuestro agente), lo cual a su vez será beneficioso para nosotros en el futuro. El comportamiento de los agentes, como se demuestra en [9], confirma y refuerza aún más nuestro Axioma Táctico Subóptimo discutido en la sección anterior. 556 The Sixth Intl. La necesidad de los axiomas de Logro de Objetivos, Acto Preventivo y Maximización de la Evaluación es evidente y no requiere más verificación. Sin embargo, incluso con respecto a esos axiomas, surgieron algunas ideas interesantes en el análisis de registros. Los axiomas de Logro y Prevención de Actos, aunque teóricamente triviales, parecen presentar un desafío para un jugador humano. En la inspección inicial de los registros, nos encontramos con algunos juegos donde un jugador, por razones inexplicables, no bloqueó al otro para que ganara o no logró ejecutar su propia jugada ganadora. Podemos atribuir esas fallas a la falta de atención de los humanos, o a un error de escritura en su respuesta al movimiento; sin embargo, esos errores podrían ocurrir en agentes racionales limitados, y el comportamiento apropiado debe ser axiomatizado. Un juego típico de Cuatro en Línea gira en torno a generar amenazas y bloquearlas. En nuestro análisis buscamos acciones preventivas explícitas, es decir, movimientos que bloqueen un grupo de 3 discos o que eliminen una amenaza futura (en nuestro horizonte de búsqueda limitado). Encontramos que en el 83% de los juegos totales, hubo al menos una acción preventiva tomada por el jugador Negro. También se encontró que Black promedió 2.8 acciones preventivas por juego en los juegos en los que perdió, mientras que promedió 1.5 acciones preventivas por juego al ganar. Parece que Black necesita 1 o 2 acciones preventivas para construir su posición inicial de toma, antes de comenzar a presentar amenazas. Si no logra ganar, generalmente evitará una amenaza adicional o dos antes de sucumbir ante el Blanco. 4. TRABAJO RELACIONADO Mucha investigación se ocupa de la axiomatización del trabajo en equipo y los estados mentales de los individuos: algunos modelos utilizan el conocimiento y la creencia [10], otros tienen modelos de metas e intenciones [8, 4]. Sin embargo, todas estas teorías formales tratan sobre el trabajo en equipo y la cooperación de agentes. Hasta donde sabemos, nuestro modelo es el primero en proporcionar un modelo formalizado para entornos adversarios explícitos y el comportamiento de los agentes en él. El algoritmo de búsqueda adversarial clásico Min-Max fue el primer intento de integrar al oponente en el espacio de búsqueda con la suposición débil de un oponente que juega de manera óptima. Desde entonces, se ha dedicado mucho esfuerzo a integrar el modelo del oponente en el procedimiento de toma de decisiones para predecir el comportamiento futuro. El algoritmo M∗ presentado por Carmel y Markovitch [2] mostró un método para incorporar modelos de oponentes en la búsqueda de adversarios, mientras que en [3] utilizaron el aprendizaje para proporcionar un modelo de oponente más preciso en un entorno de juego repetido de 2 jugadores, donde las estrategias de los agentes se modelaron como autómatas finitos. El trabajo adicional de planificación adversarial fue realizado por Willmott y colaboradores [13], quienes proporcionaron un enfoque de planificación adversarial para el juego de GO. La investigación mencionada anteriormente trató sobre la búsqueda adversarial y la integración de modelos de oponentes en métodos de búsqueda de utilidad clásicos. Ese trabajo muestra la importancia del modelado del oponente y la capacidad de explotarlo en beneficio de un agente. Sin embargo, las limitaciones básicas de esos métodos de búsqueda siguen aplicándose; nuestro modelo intenta superar esas limitaciones presentando un modelo formal para una nueva especificación adversarial basada en estados mentales. 5. CONCLUSIONES Presentamos un modelo de Entorno Adversarial para un agente racional acotado que está situado en un entorno de N jugadores, de suma cero. Estos fueron posteriormente eliminados del análisis final. Utilizamos la formalización de SharedPlans para definir el modelo y los axiomas que los agentes pueden aplicar como pautas de comportamiento. El modelo está destinado a ser utilizado como una guía para diseñar agentes que necesitan operar en entornos adversos. Presentamos resultados empíricos, basados en el análisis del archivo de registro de ConnectFour, que ejemplifican el modelo y los axiomas para una instancia bilateral del entorno. Los resultados que presentamos son un primer paso hacia un modelo ampliado que abarcará todos los tipos de entornos adversarios, por ejemplo, entornos que no son de suma cero y entornos que contienen agentes naturales que no forman parte del conflicto directo. Esos desafíos y más serán abordados en futuras investigaciones. 6. AGRADECIMIENTO Esta investigación fue apoyada en parte por las becas de la Fundación de Ciencia de Israel #1211/04 y #898/05. 7. REFERENCIAS [1] L. V. Allis. Un enfoque basado en el conocimiento de Conecta Cuatro: el juego está resuelto, gana el jugador Blanco. Tesis de maestría, Universidad Libre de Ámsterdam, Países Bajos, 1988. [2] D. Carmel y S. Markovitch. Incorporando modelos de oponentes en la búsqueda de adversarios. En Actas de la Decimotercera Conferencia Nacional de Inteligencia Artificial, páginas 120-125, Portland, Oregón, 1996. [3] D. Carmel y S. Markovitch. Modelado de oponentes en sistemas multiagente. En G. Weiß y S. Sen, editores, Adaptación y Aprendizaje en Sistemas Multiagente, páginas 40-52. Springer-Verlag, 1996. [4] B. J. Grosz y S. Kraus. Planes colaborativos para acciones grupales complejas. Inteligencia Artificial, 86(2):269-357, 1996. [5] M. Hadad, G. Kaminka, G. Armon y S. Kraus. Apoyando la actividad colaborativa. En Proc. de AAAI-2005, páginas 83-88, Pittsburgh, 2005. [6] http://www.gamerz.net/˜pbmserv/. [7] S. Kraus y D. Lehmann. Diseñando y construyendo un agente automatizado de negociación. Inteligencia Computacional, 11:132-171, 1995. [8] H. J. Levesque, P. R. Cohen y J. H. T. Nunes. Actuar juntos. En Proc. de AAAI-90, páginas 94-99, Boston, MA, 1990. [9] S. Markovitch y R. Reger. Aprender y aprovechar las debilidades relativas de los agentes oponentes. Agentes Autónomos y Sistemas Multiagente, 10(2):103-130, 2005. [10] Y. M. Ronald Fagin, Joseph Y. Halpern y M. Y. Vardi. Razonamiento sobre el conocimiento. MIT Press, Cambridge, Mass., 1995. [11] P. Thagard.\nMIT Press, Cambridge, Massachusetts, 1995. [11] P. Thagard. Resolución de problemas adversariales: Modelado de un oponente utilizando coherencia explicativa. Ciencia Cognitiva, 16(1):123-149, 1992. [12] R. W. Toseland y R. F. Rivas. Una introducción a la práctica del trabajo en grupo. Prentice Hall, Englewood Cliffs, NJ, segunda edición, 1995. [13] S. Willmott, J. Richardson, A. Bundy y J. Levine. Un enfoque de planificación adversarial para el juego de Go. Notas de conferencia en Ciencias de la Computación, 1558:93-112, 1999. El Sexto Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 557 ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "modal logic": {
            "translated_key": "Lógica Modal",
            "is_in_text": true,
            "original_annotated_sentences": [
                "An Adversarial Environment Model for Bounded Rational Agents in Zero-Sum Interactions Inon Zuckerman1 , Sarit Kraus1 , Jeffrey S. Rosenschein2 , Gal Kaminka1 1 Department of Computer Science 2 The School of Engineering Bar-Ilan University and Computer Science Ramat-Gan, Israel Hebrew University, Jerusalem, Israel {zukermi,sarit,galk}@cs.biu.ac.il jeff@cs.huji.ac.il ABSTRACT Multiagent environments are often not cooperative nor collaborative; in many cases, agents have conflicting interests, leading to adversarial interactions.",
                "This paper presents a formal Adversarial Environment model for bounded rational agents operating in a zero-sum environment.",
                "In such environments, attempts to use classical utility-based search methods can raise a variety of difficulties (e.g., implicitly modeling the opponent as an omniscient utility maximizer, rather than leveraging a more nuanced, explicit opponent model).",
                "We define an Adversarial Environment by describing the mental states of an agent in such an environment.",
                "We then present behavioral axioms that are intended to serve as design principles for building such adversarial agents.",
                "We explore the application of our approach by analyzing log files of completed Connect-Four games, and present an empirical analysis of the axioms appropriateness.",
                "Categories and Subject Descriptors I.2.11 [Artificial Intelligence]: Distributed Artificial Intelligence-Intelligent agents,Multiagent Systems; I.2.4 [Artificial Intelligence]: Knowledge Representation Formalisms and Methods -<br>modal logic</br> General Terms Design, Theory 1.",
                "INTRODUCTION Early research in multiagent systems (MAS) considered cooperative groups of agents; because individual agents had limited resources, or limited access to information (e.g., limited processing power, limited sensor coverage), they worked together by design to solve problems that individually they could not solve, or at least could not solve as efficiently.",
                "MAS research, however, soon began to consider interacting agents with individuated interests, as representatives of different humans or organizations with non-identical interests.",
                "When interactions are guided by diverse interests, participants may have to overcome disagreements, uncooperative interactions, and even intentional attempts to damage one another.",
                "When these types of interactions occur, environments require appropriate behavior from the agents situated in them.",
                "We call these environments Adversarial Environments, and call the clashing agents Adversaries.",
                "Models of cooperation and teamwork have been extensively explored in MAS through the axiomatization of mental states (e.g., [8, 4, 5]).",
                "However, none of this research dealt with adversarial domains and their implications for agent behavior.",
                "Our paper addresses this issue by providing a formal, axiomatized mental state model for a subset of adversarial domains, namely simple zero-sum adversarial environments.",
                "Simple zero-sum encounters exist of course in various twoplayer games (e.g., Chess, Checkers), but they also exist in n-player games (e.g., Risk, Diplomacy), auctions for a single good, and elsewhere.",
                "In these latter environments especially, using a utility-based adversarial search (such as the Min-Max algorithm) does not always provide an adequate solution; the payoff function might be quite complex or difficult to quantify, and there are natural computational limitations on bounded rational agents.",
                "In addition, traditional search methods (like Min-Max) do not make use of a model of the opponent, which has proven to be a valuable addition to adversarial planning [9, 3, 11].",
                "In this paper, we develop a formal, axiomatized model for bounded rational agents that are situated in a zero-sum adversarial environment.",
                "The model uses different modality operators, and its main foundations are the SharedPlans [4] model for collaborative behavior.",
                "We explore environment properties and the mental states of agents to derive behavioral axioms; these behavioral axioms constitute a formal model that serves as a specification and design guideline for agent design in such settings.",
                "We then investigate the behavior of our model empirically using the Connect-Four board game.",
                "We show that this game conforms to our environment definition, and analyze players behavior using a large set of completed match log 550 978-81-904262-7-5 (RPS) c 2007 IFAAMAS files.",
                "In addition, we use the results presented in [9] to discuss the importance of opponent modeling in our ConnectFour adversarial domain.",
                "The paper proceeds as follows.",
                "Section 2 presents the models formalization.",
                "Section 3 presents the empirical analysis and its results.",
                "We discuss related work in Section 4, and conclude and present future directions in Section 5. 2.",
                "ADVERSARIAL ENVIRONMENTS The adversarial environment model (denoted as AE) is intended to guide the design of agents by providing a specification of the capabilities and mental attitudes of an agent in an adversarial environment.",
                "We focus here on specific types of adversarial environments, specified as follows: 1.",
                "Zero-Sum Interactions: positive and negative utilities of all agents sum to zero; 2.",
                "Simple AEs: all agents in the environment are adversarial agents; 3.",
                "Bilateral AEs: AEs with exactly two agents; 4.",
                "Multilateral AEs: AEs of three or more agents.",
                "We will work on both bilateral and multilateral instantiations of zero-sum and simple environments.",
                "In particular, our adversarial environment model will deal with interactions that consist of N agents (N ≥ 2), where all agents are adversaries, and only one agent can succeed.",
                "Examples of such environments range from board games (e.g., Chess, Connect-Four, and Diplomacy) to certain economic environments (e.g., N-bidder auctions over a single good). 2.1 Model Overview Our approach is to formalize the mental attitudes and behaviors of a single adversarial agent; we consider how a single agent perceives the AE.",
                "The following list specifies the conditions and mental states of an agent in a simple, zero-sum AE: 1.",
                "The agent has an individual intention that its own goal will be completed; 2.",
                "The agent has an individual belief that it and its adversaries are pursuing full conflicting goals (defined below)there can be only one winner; 3.",
                "The agent has an individual belief that each adversary has an intention to complete its own full conflicting goal; 4.",
                "The agent has an individual belief in the (partial) profile of its adversaries.",
                "Item 3 is required, since it might be the case that some agent has a full conflicting goal, and is currently considering adopting the intention to complete it, but is, as of yet, not committed to achieving it.",
                "This might occur because the agent has not yet deliberated about the effects that adopting that intention might have on the other intentions it is currently holding.",
                "In such cases, it might not consider itself to even be in an adversarial environment.",
                "Item 4 states that the agent should hold some belief about the profiles of its adversaries.",
                "The profile represents all the knowledge the agent has about its adversary: its weaknesses, strategic capabilities, goals, intentions, trustworthiness, and more.",
                "It can be given explicitly or can be learned from observations of past encounters. 2.2 Model Definitions for Mental States We use Grosz and Krauss definitions of the modal operators, predicates, and meta-predicates, as defined in their SharedPlan formalization [4].",
                "We recall here some of the predicates and operators that are used in that formalization: Int.To(Ai, α, Tn, Tα, C) represents Ais intentions at time Tn to do an action α at time Tα in the context of C. Int.Th(Ai, prop, Tn, Tprop, C) represents Ais intentions at time Tn that a certain proposition prop holds at time Tprop in the context of C. The potential intention operators, Pot.Int.To(...) and Pot.Int.Th(...), are used to represent the mental state when an agent considers adopting an intention, but has not deliberated about the interaction of the other intentions it holds.",
                "The operator Bel(Ai, f, Tf ) represents agent Ai believing in the statement expressed in formula f, at time Tf .",
                "MB(A, f, Tf ) represents mutual belief for a group of agents A.",
                "A snapshot of the system finds our environment to be in some state e ∈ E of environmental variable states, and each adversary in any LAi ∈ L of possible local states.",
                "At any given time step, the system will be in some world w of the set of all possible worlds w ∈ W, where w = E×LA1 ×LA2 × ...LAn , and n is the number of adversaries.",
                "For example, in a Texas Holdem poker game, an agents local state might be its own set of cards (which is unknown to its adversary) while the environment will consist of the betting pot and the community cards (which are visible to both players).",
                "A utility function under this formalization is defined as a mapping from a possible world w ∈ W to an element in , which expresses the desirability of the world, from a single agent perspective.",
                "We usually normalize the range to [0,1], where 0 represents the least desirable possible world, and 1 is the most desirable world.",
                "The implementation of the utility function is dependent on the domain in question.",
                "The following list specifies new predicates, functions, variables, and constants used in conjunction with the original definitions for the adversarial environment formalization: 1. φ is a null action (the agent does not do anything). 2.",
                "GAi is the set of agent Ais goals.",
                "Each goal is a set of predicates whose satisfaction makes the goal complete (we use G∗ Ai ∈ GAi to represent an arbitrary goal of agent Ai). 3. gAi is the set of agent Ais subgoals.",
                "Subgoals are predicates whose satisfaction represents an important milestone toward achievement of the full goal. gG∗ Ai ⊆ gAi is the set of subgoals that are important to the completion of goal G∗ Ai (we will use g∗ G∗ Ai ∈ gG∗ Ai to represent an arbitrary subgoal). 4.",
                "P Aj Ai is the profile object agent Ai holds about agent Aj. 5.",
                "CA is a general set of actions for all agents in A which are derived from the environments constraints.",
                "CAi ⊆ CA is the set of agent Ais possible actions. 6.",
                "Do(Ai, α, Tα, w) holds when Ai performs action α over time interval Tα in world w. 7.",
                "Achieve(G∗ Ai , α, w) is true when goal G∗ Ai is achieved following the completion of action α in world w ∈ W, where α ∈ CAi . 8.",
                "Profile(Ai, PAi Ai ) is true when agent Ai holds an object profile for agent Aj.",
                "Definition 1.",
                "Full conflict (FulConf ) describes a zerosum interaction where only a single goal of the goals in conflict can be completed.",
                "FulConf(G∗ Ai , G∗ Aj ) ⇒ (∃α ∈ CAi , ∀w, β ∈ CAj ) (Achieve(G∗ Ai , α, w) ⇒ ¬Achieve(G∗ Aj , β, w)) ∨ (∃β ∈ CAj , ∀w, α ∈ CAi )(Achieve(G∗ Aj , β, w) ⇒ ¬Achieve(G∗ Ai , α, w)) Definition 2.",
                "Adversarial Knowledge (AdvKnow) is a function returning a value which represents the amount of The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 551 knowledge agent Ai has on the profile of agent Aj, at time Tn.",
                "The higher the value, the more knowledge agent Ai has.",
                "AdvKnow : P Aj Ai × Tn → Definition 3.",
                "Eval - This evaluation function returns an estimated expected utility value for an agent in A, after completing an action from CA in some world state w. Eval : A × CA × w → Definition 4.",
                "TrH - (Threshold) is a numerical constant in the [0,1] range that represents an evaluation function (Eval) threshold value.",
                "An action that yields an estimated utility evaluation above the TrH is regarded as a highly beneficial action.",
                "The Eval value is an estimation and not the real utility function, which is usually unknown.",
                "Using the real utility value for a rational agent would easily yield the best outcome for that agent.",
                "However, agents usually do not have the real utility functions, but rather a heuristic estimate of it.",
                "There are two important properties that should hold for the evaluation function: Property 1.",
                "The evaluation function should state that the most desirable world state is one in which the goal is achieved.",
                "Therefore, after the goal has been satisfied, there can be no future action that can put the agent in a world state with higher Eval value. (∀Ai, G∗ Ai , α, β ∈ CAi , w ∈ W) Achieve(G∗ Ai , α, w) ⇒ Eval(Ai, α, w) ≥ Eval(Ai, β, w) Property 2.",
                "The evaluation function should project an action that causes a completion of a goal or a subgoal to a value which is greater than TrH (a highly beneficial action). (∀Ai, G∗ Ai ∈ GAi , α ∈ CAi , w ∈ W, g∗ GAi ∈ gGAi ) Achieve(G∗ Ai , α, w) ∨ Achieve(g∗ GAi , α, w) ⇒ Eval(Ai, α, w) ≥ TrH.",
                "Definition 5.",
                "SetAction We define a set action (SetAction) as a set of action operations (either complex or basic actions) from some action sets CAi and CAj which, according to agent Ais belief, are attached together by a temporal and consequential relationship, forming a chain of events (action, and its following consequent action). (∀α1 , . . . , αu ∈ CAi , β1 , . . . , βv ∈ CAj , w ∈ W) SetAction(α1 , . . . , αu , β1 , . . . , βv , w) ⇒ ((Do(Ai, α1 , Tα1 , w) ⇒ Do(Aj, β1 , Tβ1 , w)) ⇒ Do(Ai, α2 , Tα2 , w) ⇒ . . . ⇒ Do(Ai, αu , Tαu , w)) The consequential relation might exist due to various environmental constraints (when one action forces the adversary to respond with a specific action) or due to the agents knowledge about the profile of its adversary.",
                "Property 3.",
                "As the knowledge we have about our adversary increases we will have additional beliefs about its behavior in different situations which in turn creates new set actions.",
                "Formally, if our AdvKnow at time Tn+1 is greater than AdvKnow at time Tn, then every SetAction known at time Tn is also known at time Tn+1.",
                "AdvKnow(P Aj Ai , Tn+1) > AdvKnow(P Aj Ai , Tn) ⇒ (∀α1 , . . . , αu ∈ CAi , β1 , . . . , βv ∈ CAj ) Bel(Aag, SetAction(α1 , . . . , αu , β1 , . . . , βv ), Tn) ⇒ Bel(Aag, SetAction(α1 , . . . , αu , β1 , . . . , βv ), Tn+1) 2.3 The Environment Formulation The following axioms provide the formal definition for a simple, zero-sum Adversarial Environment (AE).",
                "Satisfaction of these axioms means that the agent is situated in such an environment.",
                "It provides specifications for agent Aag to interact with its set of adversaries A with respect to goals G∗ Aag and G∗ A at time TCo at some world state w. AE(Aag, A, G∗ Aag , A1, . . . , Ak, G∗ A1 , . . . , G∗ Ak , Tn, w) 1.",
                "Aag has an Int.Th his goal would be completed: (∃α ∈ CAag , Tα) Int.Th(Aag, Achieve(G∗ Aag , α), Tn, Tα, AE) 2.",
                "Aag believes that it and each of its adversaries Ao are pursuing full conflicting goals: (∀Ao ∈ {A1, . . . , Ak}) Bel(Aag, FulConf(G∗ Aag , G∗ Ao ), Tn) 3.",
                "Aag believes that each of his adversaries in Ao has the Int.Th his conflicting goal G∗ Aoi will be completed: (∀Ao ∈ {A1, . . . , Ak})(∃β ∈ CAo , Tβ) Bel(Aag, Int.Th(Ao, Achieve(G∗ Ao , β), TCo, Tβ, AE), Tn) 4.",
                "Aag has beliefs about the (partial) profiles of its adversaries (∀Ao ∈ {A1, . . . , Ak}) (∃PAo Aag ∈ PAag )Bel(Aag, Profile(Ao, PAo Aag ), Tn) To build an agent that will be able to operate successfully within such an AE, we must specify behavioral guidelines for its interactions.",
                "Using a naive Eval maximization strategy to a certain search depth will not always yield satisfactory results for several reasons: (1) the search horizon problem when searching for a fixed depth; (2) the strong assumption of an optimally rational, unbounded resources adversary; (3) using an estimated evaluation function which will not give optimal results in all world states, and can be exploited [9].",
                "The following axioms specify the behavioral principles that can be used to differentiate between successful and less successful agents in the above Adversarial Environment.",
                "Those axioms should be used as specification principles when designing and implementing agents that should be able to perform well in such Adversarial Environments.",
                "The behavioral axioms represent situations in which the agent will adopt potential intentions to (Pot.Int.To(...)) perform an action, which will typically require some means-end reasoning to select a possible course of action.",
                "This reasoning will lead to the adoption of an Int.To(...) (see [4]).",
                "A1.",
                "Goal Achieving Axiom.",
                "The first axiom is the simplest case; when the agent Aag believes that it is one action (α) away from achieving his conflicting goal G∗ Aag , it should adopt the potential intention to do α and complete its goal. (∀Aag, α ∈ CAag , Tn, Tα, w ∈ W) (Bel(Aag, Do(Aag, α, Tα, w) ⇒ Achieve(G∗ Aag , α, w)) ⇒ Pot.Int.To(Aag, α, Tn, Tα, w) This somewhat trivial behavior is the first and strongest axiom.",
                "In any situation, when the agent is an action away from completing the goal, it should complete the action.",
                "Any fair Eval function would naturally classify α as the maximal value action (property 1).",
                "However, without explicit axiomatization of such behavior there might be situations where the agent will decide on taking another action for various reasons, due to its bounded decision resources.",
                "A2.",
                "Preventive Act Axiom.",
                "Being in an adversarial situation, agent Aag might decide to take actions that will damage one of its adversarys plans to complete its goal, even if those actions do not explicitly advance Aag towards its conflicting goal G∗ Aag .",
                "Such preventive action will take place when agent Aag has a belief about the possibility of its adversary Ao doing an action β that will give it a high 552 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) utility evaluation value (> TrH).",
                "Believing that taking action α will prevent the opponent from doing its β, it will adopt a potential intention to do α. (∀Aag, Ao ∈ A, α ∈ CAag , β ∈ CAo , Tn, Tβ, w ∈ W) (Bel(Aag, Do(Ao, β, Tβ, w) ∧ Eval(Ao, β, w) > TrH, Tn) ∧ Bel(Aag, Do(Aag, α, Tα, w) ⇒ ¬Do(Ao, β, Tβ, w), Tn) ⇒ Pot.Int.To(Aag, α, Tn, Tα, w) This axiom is a basic component of any adversarial environment.",
                "For example, looking at a Chess board game, a player could realize that it is about to be checkmated by its opponent, thus making a preventive move.",
                "Another example is a Connect Four game: when a player has a row of three chips, its opponent must block it, or lose.",
                "A specific instance of A1 occurs when the adversary is one action away from achieving its goal, and immediate preventive action needs to be taken by the agent.",
                "Formally, we have the same beliefs as stated above, with a changed belief that doing action β will cause agent Ao to achieve its goal.",
                "Proposition 1: Prevent or lose case. (∀Aag, Ao ∈ A, α ∈ CAag , β ∈ CAo , G∗ Ao , Tn, Tα, Tβ, w ∈ W) Bel(Aag, Do(Ao, β, Tβ, w) ⇒ Achieve(G∗ Ao , β, w), Tn) ∧ Bel(Aag, Do(Aag, α, Tα, w) ⇒ ¬Do(Ao, β, Tβ, w)) ⇒ Pot.Int.To(Aag, α, Tn, Tα, w) Sketch of proof: Proposition 1 can be easily derived from axiom A1 and the property 2 of the Eval function, which states that any action that causes a completion of a goal is a highly beneficial action.",
                "The preventive act behavior will occur implicitly when the Eval function is equal to the real world utility function.",
                "However, being bounded rational agents and dealing with an estimated evaluation function we need to explicitly axiomatize such behavior, for it will not always occur implicitly from the evaluation function.",
                "A3.",
                "Suboptimal Tactical Move Axiom.",
                "In many scenarios a situation may occur where an agent will decide not to take the current most beneficial action it can take (the action with the maximal utility evaluation value), because it believes that taking another action (with lower utility evaluation value) might yield (depending on the adversarys response) a future possibility for a highly beneficial action.",
                "This will occur most often when the Eval function is inaccurate and differs by a large extent from the Utility function.",
                "Put formally, agent Aag believes in a certain SetAction that will evolve according to its initial action and will yield a high beneficial value (> TrH) solely for it. (∀Aag, Ao ∈ A, Tn, w ∈ W) (∃α1 , . . . , αu ∈ CAi , β1 , . . . , βv ∈ CAj , Tα1 ) Bel(Aag, SetAction(α1 , . . . , αu , β1 , . . . , βv ), Tn) ∧ Bel(Aag, Eval(Ao, βv , w) < TrH < Eval(Aag, αu , w), Tn) ⇒ Pot.Int.To(Aag, α1 , Tn, Tα1 , w) An agent might believe that a chain of events will occur for various reasons due to the inevitable nature of the domain.",
                "For example, in Chess, we often observe the following: a move causes a check position, which in turn limits the opponents moves to avoiding the check, to which the first player might react with another check, and so on.",
                "The agent might also believe in a chain of events based on its knowledge of its adversarys profile, which allows it to foresee the adversarys movements with high accuracy.",
                "A4.",
                "Profile Detection Axiom.",
                "The agent can adjust its adversarys profiles by observations and pattern study (specifically, if there are repeated encounters with the same adversary).",
                "However, instead of waiting for profile information to be revealed, an agent can also initiate actions that will force its adversary to react in a way that will reveal profile knowledge about it.",
                "Formally, the axiom states that if all actions (γ) are not highly beneficial actions (< TrH), the agent can do action α in time Tα if it believes that it will result in a non-highly beneficial action β from its adversary, which in turn teaches it about the adversarys profile, i.e., gives a higher AdvKnow(P Aj Ai , Tβ). (∀Aag, Ao ∈ A, α ∈ CAag , β ∈ CAo , Tn, Tα, Tβ, w ∈ W) Bel(Aag, (∀γ ∈ CAag )Eval(Aag, γ, w) < TrH, Tn) ∧ Bel(Aag, Do(Aag, α, Tα, w) ⇒ Do(Ao, β, Tβ, w), Tn) ∧ Bel(Aag, Eval(Ao, β, w) < TrH) ∧ Bel(Aag, AdvKnow(P Aj Ai , Tβ) > AdvKnow(P Aj Ai , Tn), Tn) ⇒ Pot.Int.To(Aag, α, Tn, Tα, w) For example, going back to the Chess board game scenario, consider starting a game versus an opponent about whom we know nothing, not even if it is a human or a computerized opponent.",
                "We might start playing a strategy that will be suitable versus an average opponent, and adjust our game according to its level of play.",
                "A5.",
                "Alliance Formation Axiom The following behavioral axiom is relevant only in a multilateral instantiation of the adversarial environment (obviously, an alliance cannot be formed in a bilateral, zero-sum encounter).",
                "In different situations during a multilateral interaction, a group of agents might believe that it is in their best interests to form a temporary alliance.",
                "Such an alliance is an agreement that constrains its members behavior, but is believed by its members to enable them to achieve a higher utility value than the one achievable outside of the alliance.",
                "As an example, we can look at the classical Risk board game, where each player has an individual goal of being the sole conquerer of the world, a zero-sum game.",
                "However, in order to achieve this goal, it might be strategically wise to make short-term ceasefire agreements with other players, or to join forces and attack an opponent who is stronger than the rest.",
                "An alliances terms defines the way its members should act.",
                "It is a set of predicates, denoted as Terms, that is agreed upon by the alliance members, and should remain true for the duration of the alliance.",
                "For example, the set Terms in the Risk scenario, could contain the following predicates: 1.",
                "Alliance members will not attack each other on territories X, Y and Z; 2.",
                "Alliance members will contribute C units per turn for attacking adversary Ao; 3.",
                "Members are obligated to stay as part of the alliance until time Tk or until adversarys Ao army is smaller than Q.",
                "The set Terms specifies inter-group constraints on each of the alliance members (∀Aal i ∈ Aal ⊆ A) set of actions Cal i ⊆ C. Definition 6.",
                "Al val - the total evaluation value that agent Ai will achieve while being part of Aal is the sum of Evali (Eval for Ai) of each of Aal j Eval values after taking their own α actions (via the agent(α) predicate): Al val(Ai, Cal , Aal , w) = α∈Cal Evali(Aal j , agent(α), w) Definition 7.",
                "Al TrH - is a number representing an Al val The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 553 threshold; above it, the alliance can be said to be a highly beneficial alliance.",
                "The value of Al TrH will be calculated dynamically according to the progress of the interaction, as can be seen in [7].",
                "After an alliance is formed, its members are now working in their normal adversarial environment, as well as according to the mental states and axioms required for their interactions as part of the alliance.",
                "The following Alliance model (AL) specifies the conditions under which the group Aal can be said to be in an alliance and working with a new and constrained set of actions Cal , at time Tn.",
                "AL(Aal , Cal , w, Tn) 1.",
                "Aal has a MB that all members are part of Aal : MB(Aal , (∀Aal i ∈ Aal )member(Aal i , Aal ), Tn) 2.",
                "Aal has a MB that the group be maintained: MB(Aal , (∀Aal i ∈ Aal )Int.Th (Ai, member(Ai, Aal ), Tn, Tn+1, Co), Tn) 3.",
                "Aal has a MB that being members gives them high utility value: MB(Aal , (∀Aal i ∈ Aal )Al val(Aal i , Cal , Aal , w) ≥ Al TrH, Tn) Members profiles are a crucial part of successful alliances.",
                "We assume that agents that have more accurate profiles of their adversaries will be more successful in such environments.",
                "Such agents will be able to predict when a member is about to breach the alliances contract (item 2 in the above model), and take counter measures (when item 3 will falsify).",
                "The robustness of the alliance is in part a function of its members trustfulness measure, objective position estimation, and other profile properties.",
                "We should note that an agent can simultaneously be part of more than one alliance.",
                "Such a temporary alliance, where the group members do not have a joint goal but act collaboratively for the interest of their own individual goals, is classified as a Treatment Group by modern psychologists [12] (in contrast to a Task Group, where its members have a joint goal).",
                "The Shared Activity model as presented in [5] modeled Treatment Group behavior using the same SharedPlans formalization.",
                "When comparing both definitions of an alliance and a Treatment Group we found an unsurprising resemblance between both models: the environment models definitions are almost identical (see SAs definitions in [5]), and their Selfish-Act and Cooperative Act axioms conform to our adversarial agents behavior.",
                "The main distinction between both models is the integration of a Helpful-behavior act axiom, in the Shared Activity which cannot be part of ours.",
                "This axiom states that an agent will consider taking action that will lower its Eval value (to a certain lower bound), if it believes that a group partner will gain a significant benefit.",
                "Such behavior cannot occur in a pure adversarial environment (as a zero-sum game is), where the alliance members are constantly on watch to manipulate their alliance to their own advantage.",
                "A6.",
                "Evaluation Maximization Axiom.",
                "In a case when all other axioms are inapplicable, we will proceed with the action that maximizes the heuristic value as computed in the Eval function. (∀Aag, Ao ∈ A, α ∈ Cag, Tn, w ∈ W) Bel(Aag, (∀γ ∈ Cag)Eval(Aag, α, w) ≥ Eval(Aag, γ, w), Tn) ⇒ Pot.Int.To(Aag, α, Tn, Tα, w) T1.",
                "Optimality on Eval = Utility The above axiomatic model handles situations where the Utility is unknown and the agents are bounded rational agents.",
                "The following theorem shows that in bilateral interactions, where the agents have the real Utility function (i.e., Eval = Utility) and are rational agents, the axioms provide the same optimal result as classic adversarial search (e.g., Min-Max).",
                "Theorem 1.",
                "Let Ae ag be an unbounded rational AE agent using the Eval heuristic evaluation function, Au ag be the same agent using the true Utility function, and Ao be a sole unbounded utility-based rational adversary.",
                "Given that Eval = Utility: (∀α ∈ CAu ag , α ∈ CAe ag , Tn, w ∈ W) Pot.Int.To(Au ag, α, Tn, Tα, w) → Pot.Int.To(Ae ag, α , Tn, Tα, w) ∧ ((α = α ) ∨ (Utility(Au ag, α, w) = Eval(Ae ag, α , w))) Sketch of proof - Given that Au ag has the real utility function and unbounded resources, it can generate the full game tree and run the optimal MinMax algorithm to choose the highest utility value action, which we denote by, α.",
                "The proof will show that Ae ag, using the AE axioms, will select the same or equal utility α (when there is more than one action with the same max utility) when Eval = Utility. (A1) Goal achieving axiom - suppose there is an α such that its completion will achieve Au ags goal.",
                "It will obtain the highest utility by Min-Max for Au ag.",
                "The Ae ag agent will select α or another action with the same utility value via A1.",
                "If such α does not exist, Ae ag cannot apply this axiom, and proceeds to A2. (A2) Preventive act axiom - (1) Looking at the basic case (see Prop1), if there is a β which leads Ao to achieve its goal, then a preventive action α will yield the highest utility for Au ag.",
                "Au ag will choose it through the utility, while Ae ag will choose it through A2. (2) In the general case, β is a highly beneficial action for Ao, thus yields low utility for Au ag, which will guide it to select an α that will prevent β, while Ae ag will choose it through A2.1 If such β does not exist for Ao, then A2 is not applicable, and Ae ag can proceed to A3. (A3) Suboptimal tactical move axiom - When using a heuristic Eval function, Ae ag has a partial belief in the profile of its adversary (item 4 in AE model), which may lead it to believe in SetActions (Prop1).",
                "In our case, Ae ag is holding a full profile on its optimal adversary and knows that Ao will behave optimally according to the real utility values on the complete search tree, therefore, any belief about suboptimal SetAction cannot exist, yielding this axiom inapplicable.",
                "Ae ag will proceed to A4. (A4) Profile detection axiom - Given that Ae ag has the full profile of Ao, none of Ae ags actions can increase its knowledge.",
                "That axiom will not be applied, and the agent will proceed with A6 (A5 will be disregarded because the interaction is bilateral). (A6) Evaluation maximization axiom - This axiom will select the max Eval for Ae ag.",
                "Given that Eval = Utility, the same α that was selected by Au ag will be selected. 3.",
                "EVALUATION The main purpose of our experimental analysis is to evaluate the models behavior and performance in a real adversarial environment.",
                "This section investigates whether bounded 1 A case where following the completion of β there exists a γ which gives high utility for Agent Au ag, cannot occur because Ao uses the same utility, and γs existence will cause it to classify β as a low utility action. 554 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) rational agents situated in such adversarial environments will be better off applying our suggested behavioral axioms. 3.1 The Domain To explore the use of the above model and its behavioral axioms, we decided to use the Connect-Four game as our adversarial environment.",
                "Connect-Four is a 2-player, zerosum game which is played using a 6x7 matrix-like board.",
                "Each turn, a player drops a disc into one of the 7 columns (the set of 21 discs is usually colored yellow for player 1 and red for player 2; we will use White and Black respectively to avoid confusion).",
                "The winner is the first player to complete a horizontal, vertical, or diagonal set of four discs with its color.",
                "On very rare occasions, the game might end in a tie if all the empty grids are filled, but no player managed to create a 4-disc set.",
                "The Connect-Four game was solved in [1], where it is shown that the first player (playing with the white discs) can force a win by starting in the middle column (column 4) and playing optimally However, the optimal strategy is very complex, and difficult to follow even for complex bounded rational agents, such as human players.",
                "Before we can proceed checking agent behavior, we must first verify that the domain conforms to the adversarial environments definition as given above (which the behavioral axioms are based on).",
                "First, when playing a Connect-Four game, the agent has an intention to win the game (item 1).",
                "Second (item 2), our agent believes that in Connect-Four there can only be one winner (or no winner at all in the rare occurrence of a tie).",
                "In addition, our agent believes that its opponent to the game will try to win (item 3), and we hope it has some partial knowledge (item 4) about its adversary (this knowledge can vary from nothing, through simple facts such as age, to strategies and weaknesses).",
                "Of course, not all Connect-Four encounters are adversarial.",
                "For example, when a parent is playing the game with its child, the following situation might occur: the child, having a strong incentive to win, treats the environment as adversarial (it intends to win, understands that there can only be one winner, and believes that its parent is trying to beat him).",
                "However, the parents point of view might see the environment as an educational one, where its goal is not to win the game, but to cause enjoyment or practice strategic reasoning.",
                "In such an educational environment, a new set of behavioral axioms might be more beneficial to the parents goals than our suggested adversarial behavioral axioms. 3.2 Axiom Analysis After showing that the Connect-Four game is indeed a zero-sum, bilateral adversarial environment, the next step is to look at players behaviors during the game and check whether behaving according to our model does improve performance.",
                "To do so we have collected log files from completed Connect-Four games that were played by human players over the Internet.",
                "Our collected log file data came from Play by eMail (PBeM) sites.",
                "These are web sites that host email games, where each move is taken by an email exchange between the server and the players.",
                "Many such sites archives contain real competitive interactions, and also maintain a ranking system for their members.",
                "Most of the data we used can be found in [6].",
                "As can be learned from [1], Connect-Four has an optimal strategy and a considerable advantage for the player who starts the game (which we call the White player).",
                "We will concentrate in our analysis on the second players moves (to be called Black).",
                "The White player, being the first to act, has the so-called initiative advantage.",
                "Having the advantage and a good strategy will keep the Black player busy reacting to its moves, instead of initiating threats.",
                "A threat is a combination of three discs of the same color, with an empty spot for the fourth winning disk.",
                "An open threat is a threat that can be realized in the opponents next move.",
                "In order for the Black player to win, it must somehow turn the tide, take the advantage and start presenting threats to the White player.",
                "We will explore Black players behavior and their conformance to our axioms.",
                "To do so, we built an application that reads log files and analyzes the Black players moves.",
                "The application contains two main components: (1) a Min-Max algorithm for evaluation of moves; (2) open threats detector for the discovering of open threats.",
                "The Min-Max algorithm will work to a given depth, d and for each move α will output the heuristic value for the next action taken by the player as written in the log file, h(α), alongside the maximum heuristic value, maxh(α), that could be achieved prior to taking the move (obviously, if h(α) = maxh(α), then the player did not do the optimal move heuristically).",
                "The threat detectors job is to notify if some action was taken in order to block an open threat (not blocking an open threat will probably cause the player to lose in the opponents next move).",
                "The heuristic function used by Min-Max to evaluate the players utility is the following function, which is simple to compute, yet provides a reasonable challenge to human opponents: Definition 8.",
                "Let Group be an adjacent set of four squares that are horizontal, vertical, or diagonal.",
                "Groupn b (Groupn w) be a Group with n pieces of the black (white) color and 4−n empty squares. h = ((Group1 b ∗α)+(Group2 b ∗β)+(Group3 b ∗γ)+(Group4 b ∗∞)) − ((Group1 w ∗α)+(Group2 w ∗β)+(Group3 w ∗γ)+(Group4 w ∗∞)) The values of α, β and δ can vary to form any desired linear combination; however, it is important to value them with the α < β < δ ordering in mind (we used 1, 4, and 8 as their respective values).",
                "Groups of 4 discs of the same color means victory, thus discovery of such a group will result in ∞ to ensure an extreme value.",
                "We now use our estimated evaluation function to evaluate the Black players actions during the Connect-Four adversarial interaction.",
                "Each game from the log file was input into the application, which processed and output a reformatted log file containing the h value of the current move, the maxh value that could be achieved, and a notification if an open threat was detected.",
                "A total of 123 games were analyzed (57 with White winning, and 66 with Black winning).",
                "A few additional games were manually ignored in the experiment, due to these problems: a player abandoning the game while the outcome is not final, or a blunt irrational move in the early stages of the game (e.g., not blocking an obvious winning group in the first opening moves).",
                "In addition, a single tie game was also removed.",
                "The simulator was run to a search depth of 3 moves.",
                "We now proceed to analyze the games with respect to each behavioral axiom.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 555 Table 1: Average heuristic difference analysis Black losses Black Won Avg minh -17.62 -12.02 Avg 3 lowest h moves (min3 h) -13.20 -8.70 3.2.1 Affirming the Suboptimal tactical move axiom The following section presents the heuristic evaluations of the Min-Max algorithm for each action, and checks the amount and extent of suboptimal tactical actions and their implications on performance.",
                "Table 1 shows results and insights from the games heuristic analysis, when search depth equals 3 (this search depth was selected for the results to be comparable to [9], see Section 3.2.3).",
                "The tables heuristic data is the difference between the present maximal heuristic value and the heuristic value of the action that was eventually taken by the player (i.e., the closer the number is to 0, the closer the action was to the maximum heuristic action).",
                "The first row presents the difference values of the action that had the maximal difference value among all the Black players actions in a given game, as averaged over all Blacks winning and losing games (see respective columns).",
                "In games in which the Black player loses, its average difference value was -17.62, while in games in which the Black player won, its average was -12.02.",
                "The second row expands the analysis by considering the 3 highest heuristic difference actions, and averaging them.",
                "In that case, we notice an average heuristic difference of 5 points between games which the Black player loses and games in which it wins.",
                "Nevertheless, the importance of those numbers is that they allowed us to take an educated guess on a threshold number of 11.5, as the value of the TrH constant, which differentiates between normal actions and highly beneficial ones.",
                "After finding an approximated TrH constant, we can proceed with an analysis of the importance of suboptimal moves.",
                "To do so we took the subset of games in which the minimum heuristic difference value for Blacks actions was 11.5.",
                "As presented in Table 2, we can see the different min3 h average of the 3 largest ranges and the respective percentage of games won.",
                "The first row shows that the Black player won only 12% of the games in which the average of its 3 highest heuristically difference actions (min3 h) was smaller than the suggested threshold, TrH = 11.5.",
                "The second row shows a surprising result: it seems that when min3 h > −4 the Black player rarely wins.",
                "Intuition would suggest that games in which the action evaluation values were closer to the maximal values will result in more winning games for Black.",
                "However, it seems that in the Connect-Four domain, merely responding with somewhat easily expected actions, without initiating a few surprising and suboptimal moves, does not yield good results.",
                "The last row sums up the main insights from the analysis; most of Blacks wins (83%) came when its min3 h was in the range of -11.5 to -4.",
                "A close inspection of those Black winning games shows the following pattern behind the numbers: after standard opening moves, Black suddenly drops a disc into an isolated column, which seems a waste of a move.",
                "White continues to build its threats, while usually disregarding Blacks last move, which in turn uses the isolated disc as an anchor for a future winning threat.",
                "The results show that it was beneficial for the Black player Table 2: Blacks winnings percentages % of games min3 h < −11.5 12% min3 h > −4 5% −11.5 ≤ min3 h ≤ −4 83% to take suboptimal actions and not give the current highest possible heuristic value, but will not be too harmful for its position (i.e., will not give high beneficial value to its adversary).",
                "As it turned out, learning the threshold is an important aspect of success: taking wildly risky moves (min3 h < −11.5) or trying to avoid them (min3 h > −4) reduces the Black players winning chances by a large margin. 3.2.2 Affirming the Profile Monitoring Axiom In the task of showing the importance of monitoring ones adversaries profiles, our log files could not be used because they did not contain repeated interactions between players, which are needed to infer the players knowledge about their adversaries.",
                "However, the importance of opponent modeling and its use in attaining tactical advantages was already studied in various domains ([3, 9] are good examples).",
                "In a recent paper, Markovitch and Reger [9] explored the notion of learning and exploitation of opponent weakness in competitive interactions.",
                "They apply simple learning strategies by analyzing examples from past interactions in a specific domain.",
                "They also used the Connect-Four adversarial domain, which can now be used to understand the importance of monitoring the adversarys profile.",
                "Following the presentation of their theoretical model, they describe an extensive empirical study and check the agents performance after learning the weakness model with past examples.",
                "One of the domains used as a competitive environment was the same Connect-Four game (Checkers was the second domain).",
                "Their heuristic function was identical to ours with three different variations (H1, H2, and H3) that are distinguished from one another in their linear combination coefficient values.",
                "The search depth for the players was 3 (as in our analysis).",
                "Their extensive experiments check and compare various learning strategies, risk factors, predefined feature sets and usage methods.",
                "The bottom line is that the Connect-Four domain shows an improvement from a 0.556 winning rate before modeling to a 0.69 after modeling (page 22).",
                "Their conclusions, showing improved performance when holding and using the adversarys model, justify the effort to monitor the adversary profile for continuous and repeated interactions.",
                "An additional point that came up in their experiments is the following: after the opponent weakness model has been learned, the authors describe different methods of integrating the opponent weakness model into the agents decision strategy.",
                "Nevertheless, regardless of the specific method they chose to work with, all integration methods might cause the agent to take suboptimal decisions; it might cause the agent to prefer actions that are suboptimal at the present decision junction, but which might cause the opponent to react in accordance with its weakness model (as represented by our agent) which in turn will be beneficial for us in the future.",
                "The agents behavior, as demonstrated in [9] further confirms and strengthens our Suboptimal Tactical Axiom as discussed in the previous section. 556 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 3.2.3 Additional Insights The need for the Goal Achieving, Preventive Act, and Evaluation Maximization axioms are obvious, and need no further verification.",
                "However, even with respect to those axioms, a few interesting insights came up in the log analysis.",
                "The Goal achieving and Preventive Act axioms, though theoretically trivial, seem to provide some challenge to a human player.",
                "In the initial inspection of the logs, we encountered few games2 where a player, for inexplicable reasons, did not block the other from winning or failed to execute its own winning move.",
                "We can blame those faults on the humans lack of attention, or a typing error in its move reply; nevertheless, those errors might occur in bounded rational agents, and the appropriate behavior needs to be axiomatized.",
                "A typical Connect-Four game revolves around generating threats and blocking them.",
                "In our analysis we looked for explicit preventive actions, i.e., moves that block a group of 3 discs, or that remove a future threat (in our limited search horizon).",
                "We found that in 83% of the total games there was at least one preventive action taken by the Black player.",
                "It was also found that Black averaged 2.8 preventive actions per game on the games in which it lost, while averaging 1.5 preventive actions per game when winning.",
                "It seems that Black requires 1 or 2 preventive actions to build its initial taking position, before starting to present threats.",
                "If it did not manage to win, it will usually prevent an extra threat or two before succumbing to White. 4.",
                "RELATED WORK Much research deals with the axiomatization of teamwork and mental states of individuals: some models use knowledge and belief [10], others have models of goals and intentions [8, 4].",
                "However, all these formal theories deal with agent teamwork and cooperation.",
                "As far as we know, our model is the first to provide a formalized model for explicit adversarial environments and agents behavior in it.",
                "The classical Min-Max adversarial search algorithm was the first attempt to integrate the opponent into the search space with a weak assumption of an optimally playing opponent.",
                "Since then, much effort has gone into integrating the opponent model into the decision procedure to predict future behavior.",
                "The M∗ algorithm presented by Carmel and Markovitch [2] showed a method of incorporating opponent models into adversary search, while in [3] they used learning to provide a more accurate opponent model in a 2player repeated game environment, where agents strategies were modeled as finite automata.",
                "Additional Adversarial planning work was done by Willmott et al. [13], which provided an adversarial planning approach to the game of GO.",
                "The research mentioned above dealt with adversarial search and the integration of opponent models into classical utilitybased search methods.",
                "That work shows the importance of opponent modeling and the ability to exploit it to an agents advantage.",
                "However, the basic limitations of those search methods still apply; our model tries to overcome those limitations by presenting a formal model for a new, mental state-based adversarial specification. 5.",
                "CONCLUSIONS We presented an Adversarial Environment model for a 2 These were later removed from the final analysis. bounded rational agent that is situated in an N-player, zerosum environment.",
                "We used the SharedPlans formalization to define the model and the axioms that agents can apply as behavioral guidelines.",
                "The model is meant to be used as a guideline for designing agents that need to operate in such adversarial environments.",
                "We presented empirical results, based on ConnectFour log file analysis, that exemplify the model and the axioms for a bilateral instance of the environment.",
                "The results we presented are a first step towards an expanded model that will cover all types of adversarial environments, for example, environments that are non-zero-sum, and environments that contain natural agents that are not part of the direct conflict.",
                "Those challenges and more will be dealt with in future research. 6.",
                "ACKNOWLEDGMENT This research was supported in part by Israel Science Foundation grants #1211/04 and #898/05. 7.",
                "REFERENCES [1] L. V. Allis.",
                "A knowledge-based approach of Connect-Four - the game is solved: White wins.",
                "Masters thesis, Free University, Amsterdam, The Netherlands, 1988. [2] D. Carmel and S. Markovitch.",
                "Incorporating opponent models into adversary search.",
                "In Proceedings of the Thirteenth National Conference on Artificial Intelligence, pages 120-125, Portland, OR, 1996. [3] D. Carmel and S. Markovitch.",
                "Opponent modeling in multi-agent systems.",
                "In G. Weiß and S. Sen, editors, Adaptation and Learning in Multi-Agent Systems, pages 40-52.",
                "Springer-Verlag, 1996. [4] B. J. Grosz and S. Kraus.",
                "Collaborative plans for complex group action.",
                "Artificial Intelligence, 86(2):269-357, 1996. [5] M. Hadad, G. Kaminka, G. Armon, and S. Kraus.",
                "Supporting collaborative activity.",
                "In Proc. of AAAI-2005, pages 83-88, Pittsburgh, 2005. [6] http://www.gamerz.net/˜pbmserv/. [7] S. Kraus and D. Lehmann.",
                "Designing and building a negotiating automated agent.",
                "Computational Intelligence, 11:132-171, 1995. [8] H. J. Levesque, P. R. Cohen, and J. H. T. Nunes.",
                "On acting together.",
                "In Proc. of AAAI-90, pages 94-99, Boston, MA, 1990. [9] S. Markovitch and R. Reger.",
                "Learning and exploiting relative weaknesses of opponent agents.",
                "Autonomous Agents and Multi-Agent Systems, 10(2):103-130, 2005. [10] Y. M. Ronald Fagin, Joseph Y. Halpern and M. Y. Vardi.",
                "Reasoning about knowledge.",
                "MIT Press, Cambridge, Mass., 1995. [11] P. Thagard.",
                "Adversarial problem solving: Modeling an oponent using explanatory coherence.",
                "Cognitive Science, 16(1):123-149, 1992. [12] R. W. Toseland and R. F. Rivas.",
                "An Introduction to Group Work Practice.",
                "Prentice Hall, Englewood Cliffs, NJ, 2nd edition edition, 1995. [13] S. Willmott, J. Richardson, A. Bundy, and J. Levine.",
                "An adversarial planning approach to Go.",
                "Lecture Notes in Computer Science, 1558:93-112, 1999.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 557"
            ],
            "original_annotated_samples": [
                "Categories and Subject Descriptors I.2.11 [Artificial Intelligence]: Distributed Artificial Intelligence-Intelligent agents,Multiagent Systems; I.2.4 [Artificial Intelligence]: Knowledge Representation Formalisms and Methods -<br>modal logic</br> General Terms Design, Theory 1."
            ],
            "translated_annotated_samples": [
                "Categorías y Descriptores de Asignaturas I.2.11 [Inteligencia Artificial]: Inteligencia Artificial Distribuida-Agentes Inteligentes, Sistemas Multiagente; I.2.4 [Inteligencia Artificial]: Formalismos y Métodos de Representación del Conocimiento - <br>Lógica Modal</br> Términos Generales Diseño, Teoría 1."
            ],
            "translated_text": "Un modelo de entorno adversarial para agentes racionales limitados en interacciones de suma cero Inon Zuckerman1, Sarit Kraus1, Jeffrey S. Rosenschein2, Gal Kaminka1 1 Departamento de Ciencias de la Computación 2 Escuela de Ingeniería Universidad Bar-Ilan y Ciencias de la Computación Ramat-Gan, Israel Universidad Hebrea, Jerusalén, Israel {zukermi, sarit, galk}@cs.biu.ac.il jeff@cs.huji.ac.il RESUMEN Los entornos multiagentes a menudo no son cooperativos ni colaborativos; en muchos casos, los agentes tienen intereses conflictivos, lo que lleva a interacciones adversariales. Este documento presenta un modelo formal de Entorno Adversarial para agentes racionales acotados que operan en un entorno de suma cero. En tales entornos, los intentos de utilizar métodos de búsqueda basados en utilidad clásica pueden plantear una variedad de dificultades (por ejemplo, modelar implícitamente al oponente como un maximizador de utilidad omnisciente, en lugar de aprovechar un modelo de oponente más matizado y explícito). Definimos un Entorno Adversario al describir los estados mentales de un agente en dicho entorno. Luego presentamos axiomas de comportamiento que están destinados a servir como principios de diseño para construir agentes adversarios. Exploramos la aplicación de nuestro enfoque mediante el análisis de archivos de registro de juegos de Conecta Cuatro completados, y presentamos un análisis empírico de la adecuación de los axiomas. Categorías y Descriptores de Asignaturas I.2.11 [Inteligencia Artificial]: Inteligencia Artificial Distribuida-Agentes Inteligentes, Sistemas Multiagente; I.2.4 [Inteligencia Artificial]: Formalismos y Métodos de Representación del Conocimiento - <br>Lógica Modal</br> Términos Generales Diseño, Teoría 1. Introducción: En las primeras investigaciones en sistemas multiagente (SMA) se consideraban grupos cooperativos de agentes; debido a que los agentes individuales tenían recursos limitados o acceso limitado a la información (por ejemplo, potencia de procesamiento limitada, cobertura sensorial limitada), trabajaban juntos por diseño para resolver problemas que individualmente no podían resolver, o al menos no podían resolver de manera eficiente. Sin embargo, la investigación de MAS pronto comenzó a considerar agentes interactivos con intereses individualizados, como representantes de diferentes humanos u organizaciones con intereses no idénticos. Cuando las interacciones son guiadas por intereses diversos, los participantes pueden tener que superar desacuerdos, interacciones no cooperativas e incluso intentos intencionales de dañarse mutuamente. Cuando ocurren este tipo de interacciones, los entornos requieren un comportamiento apropiado por parte de los agentes situados en ellos. Llamamos a estos entornos Entornos Adversarios, y llamamos a los agentes en conflicto Adversarios. Los modelos de cooperación y trabajo en equipo han sido ampliamente explorados en MAS a través de la axiomatización de estados mentales (por ejemplo, [8, 4, 5]). Sin embargo, ninguna de estas investigaciones abordó los dominios adversarios y sus implicaciones para el comportamiento de los agentes. Nuestro artículo aborda este problema al proporcionar un modelo formal y axiomatizado de estados mentales para un subconjunto de dominios adversariales, específicamente entornos adversariales simples de suma cero. Los encuentros de suma cero simples existen, por supuesto, en varios juegos de dos jugadores (por ejemplo, Ajedrez, Damas), pero también existen en juegos de n jugadores (por ejemplo, Risk, Diplomacia), subastas de un solo bien y en otros lugares. En estos entornos recientes en particular, el uso de una búsqueda adversarial basada en utilidad (como el algoritmo Min-Max) no siempre proporciona una solución adecuada; la función de recompensa podría ser bastante compleja o difícil de cuantificar, y existen limitaciones computacionales naturales en agentes racionales acotados. Además, los métodos de búsqueda tradicionales (como Min-Max) no hacen uso de un modelo del oponente, lo cual ha demostrado ser una adición valiosa a la planificación adversarial [9, 3, 11]. En este artículo, desarrollamos un modelo formal y axiomatizado para agentes racionales limitados que se encuentran en un entorno adversarial de suma cero. El modelo utiliza diferentes operadores de modalidad, y sus principales fundamentos son el modelo SharedPlans [4] para el comportamiento colaborativo. Exploramos las propiedades del entorno y los estados mentales de los agentes para derivar axiomas de comportamiento; estos axiomas de comportamiento constituyen un modelo formal que sirve como especificación y guía de diseño para el diseño de agentes en tales entornos. Luego investigamos el comportamiento de nuestro modelo empíricamente utilizando el juego de mesa Conecta Cuatro. Mostramos que este juego se ajusta a nuestra definición de entorno, y analizamos el comportamiento de los jugadores utilizando un gran conjunto de registros de partidas completadas 550 978-81-904262-7-5 (RPS) c 2007 archivos de IFAAMAS. Además, utilizamos los resultados presentados en [9] para discutir la importancia del modelado del oponente en nuestro dominio adversarial de Conecta Cuatro. El artículo continúa de la siguiente manera. La sección 2 presenta la formalización de los modelos. La sección 3 presenta el análisis empírico y sus resultados. Discutimos el trabajo relacionado en la Sección 4, y concluimos y presentamos las direcciones futuras en la Sección 5. ENTORNOS ADVERSARIOS El modelo de entorno adversario (denominado como AE) está destinado a guiar el diseño de agentes al proporcionar una especificación de las capacidades y actitudes mentales de un agente en un entorno adversario. Nos enfocamos aquí en tipos específicos de entornos adversarios, especificados de la siguiente manera: 1. Interacciones de suma cero: las utilidades positivas y negativas de todos los agentes suman cero; 2. Agentes adversarios simples: todos los agentes en el entorno son agentes adversarios; 3. AEs bilaterales: AEs con exactamente dos agentes; 4. AEs multilaterales: AEs de tres o más agentes. Trabajaremos en instanciaciones tanto bilaterales como multilaterales de entornos de suma cero y simples. En particular, nuestro modelo de entorno adversarial tratará con interacciones que consisten en N agentes (N ≥ 2), donde todos los agentes son adversarios y solo un agente puede tener éxito. Ejemplos de tales entornos van desde juegos de mesa (por ejemplo, Ajedrez, Conecta Cuatro y Diplomacia) hasta ciertos entornos económicos (por ejemplo, subastas de N postores sobre un único bien). 2.1 Resumen del Modelo Nuestro enfoque es formalizar las actitudes mentales y comportamientos de un único agente adversario; consideramos cómo un único agente percibe el AE. La siguiente lista especifica las condiciones y estados mentales de un agente en un AE simple de suma cero: 1. El agente tiene la intención individual de que su propio objetivo se complete; 2. El agente tiene la creencia individual de que él y sus adversarios persiguen objetivos completamente opuestos (definidos a continuación) y que solo puede haber un ganador; 3. El agente tiene la creencia individual de que cada adversario tiene la intención de completar su propio objetivo conflictivo completo; 4. El agente tiene una creencia individual en el perfil (parcial) de sus adversarios. El ítem 3 es necesario, ya que podría ser el caso de que algún agente tenga un objetivo completamente conflictivo y esté considerando adoptar la intención de completarlo, pero aún no está comprometido a lograrlo. Esto podría ocurrir porque el agente aún no ha reflexionado sobre los efectos que adoptar esa intención podría tener en las otras intenciones que actualmente mantiene. En tales casos, es posible que ni siquiera se considere en un entorno adversarial. El ítem 4 establece que el agente debe tener alguna creencia sobre los perfiles de sus adversarios. El perfil representa todo el conocimiento que el agente tiene sobre su adversario: sus debilidades, capacidades estratégicas, objetivos, intenciones, confiabilidad y más. Puede ser dado explícitamente o puede ser aprendido a partir de observaciones de encuentros pasados. 2.2 Definiciones de modelos para estados mentales Utilizamos las definiciones de los operadores modales, predicados y metapredicados de Grosz y Krauss, tal como se definen en su formalización de SharedPlan [4]. Recordamos aquí algunos de los predicados y operadores que se utilizan en esa formalización: Int.To(Ai, α, Tn, Tα, C) representa las intenciones de Ai en el tiempo Tn de realizar una acción α en el tiempo Tα en el contexto de C. Int.Th(Ai, prop, Tn, Tprop, C) representa las intenciones de Ai en el tiempo Tn de que una cierta proposición prop se cumpla en el tiempo Tprop en el contexto de C. Los operadores de intención potencial, Pot.Int.To(...) y Pot.Int.Th(...), se utilizan para representar el estado mental cuando un agente considera adoptar una intención, pero no ha deliberado sobre la interacción de las otras intenciones que tiene. El operador Bel(Ai, f, Tf) representa al agente Ai creyendo en la afirmación expresada en la fórmula f, en el tiempo Tf. MB(A, f, Tf) representa la creencia mutua para un grupo de agentes A. Una instantánea del sistema encuentra que nuestro entorno se encuentra en algún estado e ∈ E de estados variables ambientales, y cada adversario en cualquier LAi ∈ L de posibles estados locales. En cualquier paso de tiempo dado, el sistema estará en algún mundo w del conjunto de todos los mundos posibles w ∈ W, donde w = E×LA1 ×LA2 × ...LAn, y n es el número de adversarios. Por ejemplo, en un juego de póker Texas Holdem, el estado local de un agente podría ser su propio conjunto de cartas (que es desconocido para su adversario) mientras que el entorno consistirá en el bote de apuestas y las cartas comunitarias (que son visibles para ambos jugadores). Una función de utilidad bajo esta formalización se define como un mapeo de un mundo posible w ∈ W a un elemento en , que expresa la deseabilidad del mundo, desde la perspectiva de un único agente. Normalmente normalizamos el rango a [0,1], donde 0 representa el mundo menos deseable posible, y 1 es el mundo más deseable. La implementación de la función de utilidad depende del dominio en cuestión. La siguiente lista especifica nuevos predicados, funciones, variables y constantes utilizadas en conjunto con las definiciones originales para la formalización del entorno adversarial: 1. φ es una acción nula (el agente no hace nada). 2. GAi es el conjunto de objetivos del agente Ai. Cada objetivo es un conjunto de predicados cuya satisfacción hace que el objetivo esté completo (usamos G∗ Ai ∈ GAi para representar un objetivo arbitrario del agente Ai). 3. gAi es el conjunto de subobjetivos del agente Ai. Los subobjetivos son predicados cuya satisfacción representa un hito importante hacia el logro del objetivo completo. gG∗ Ai ⊆ gAi es el conjunto de subobjetivos que son importantes para la finalización del objetivo G∗ Ai (usaremos g∗ G∗ Ai ∈ gG∗ Ai para representar un subobjetivo arbitrario). 4. P Aj Ai es el objeto de perfil que el agente Ai tiene sobre el agente Aj. 5. CA es un conjunto general de acciones para todos los agentes en A que se derivan de las restricciones del entorno. CAi ⊆ CA es el conjunto de acciones posibles del agente Ai. La expresión Do(Ai, α, Tα, w) se cumple cuando Ai realiza la acción α durante el intervalo de tiempo Tα en el mundo w. 7. La afirmación Achieve(G∗ Ai , α, w) es verdadera cuando se logra el objetivo G∗ Ai siguiendo la finalización de la acción α en el mundo w ∈ W, donde α ∈ CAi. El perfil (Ai, PAi Ai) es verdadero cuando el agente Ai tiene un perfil de objeto para el agente Aj. Definición 1. El conflicto total (FulConf) describe una interacción de suma cero donde solo se puede completar un solo objetivo de los objetivos en conflicto. FulConf(G∗ Ai , G∗ Aj ) ⇒ (∃α ∈ CAi , ∀w, β ∈ CAj ) (Achieve(G∗ Ai , α, w) ⇒ ¬Achieve(G∗ Aj , β, w)) ∨ (∃β ∈ CAj , ∀w, α ∈ CAi )(Achieve(G∗ Aj , β, w) ⇒ ¬Achieve(G∗ Ai , α, w)) Definición 2. El Conocimiento Adversarial (AdvKnow) es una función que devuelve un valor que representa la cantidad de The Sixth Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 551 agente de conocimiento Ai tiene sobre el perfil del agente Aj, en el tiempo Tn. Cuanto mayor sea el valor, más conocimiento tiene el agente Ai. AdvKnow: P Aj Ai × Tn → Definición 3. Eval: Esta función de evaluación devuelve un valor de utilidad esperada estimado para un agente en A, después de completar una acción de CA en algún estado del mundo w. Eval: A × CA × w → Definición 4. TrH - (Umbral) es una constante numérica en el rango [0,1] que representa un valor umbral de la función de evaluación (Eval). Una acción que produce una evaluación de utilidad estimada por encima del TrH se considera una acción altamente beneficiosa. El valor Eval es una estimación y no la función de utilidad real, la cual suele ser desconocida. Utilizar el valor real de utilidad para un agente racional fácilmente produciría el mejor resultado para ese agente. Sin embargo, los agentes generalmente no tienen las funciones de utilidad reales, sino más bien una estimación heurística de las mismas. Existen dos propiedades importantes que deben cumplir para la función de evaluación: Propiedad 1. La función de evaluación debería indicar que el estado del mundo más deseable es aquel en el que se logra el objetivo. Por lo tanto, una vez que se ha satisfecho el objetivo, no puede haber ninguna acción futura que pueda colocar al agente en un estado del mundo con un valor de Eval más alto. (∀Ai, G∗ Ai , α, β ∈ CAi , w ∈ W) Lograr(G∗ Ai , α, w) ⇒ Eval(Ai, α, w) ≥ Eval(Ai, β, w) Propiedad 2. La función de evaluación debe proyectar una acción que cause la finalización de un objetivo o subobjetivo a un valor que sea mayor que TrH (una acción altamente beneficiosa). (∀Ai, G∗ Ai ∈ GAi , α ∈ CAi , w ∈ W, g∗ GAi ∈ gGAi ) Lograr(G∗ Ai , α, w) ∨ Lograr(g∗ GAi , α, w) ⇒ Eval(Ai, α, w) ≥ TrH. Definición 5. La acción de conjunto Definimos una acción de conjunto (SetAction) como un conjunto de operaciones de acción (ya sean acciones complejas o básicas) de algunos conjuntos de acción CAi y CAj que, según la creencia del agente Ai, están unidas por una relación temporal y consecuente, formando una cadena de eventos (acción y su acción consecuente siguiente). (∀α1 , . . . , αu ∈ CAi , β1 , . . . , βv ∈ CAj , w ∈ W) SetAction(α1 , . . . , αu , β1 , . . . , βv , w) ⇒ ((Do(Ai, α1 , Tα1 , w) ⇒ Do(Aj, β1 , Tβ1 , w)) ⇒ Do(Ai, α2 , Tα2 , w) ⇒ . . . ⇒ Do(Ai, αu , Tαu , w)) La relación consecuente podría existir debido a varias restricciones ambientales (cuando una acción obliga al adversario a responder con una acción específica) o debido al conocimiento de los agentes sobre el perfil de su adversario. Propiedad 3. A medida que aumenta el conocimiento que tenemos sobre nuestro adversario, tendremos creencias adicionales sobre su comportamiento en diferentes situaciones, lo que a su vez crea un nuevo conjunto de acciones. Formalmente, si nuestro AdvKnow en el tiempo Tn+1 es mayor que AdvKnow en el tiempo Tn, entonces cada SetAction conocido en el tiempo Tn también es conocido en el tiempo Tn+1. AdvKnow(P Aj Ai , Tn+1) > AdvKnow(P Aj Ai , Tn) ⇒ (∀α1 , . . . , αu ∈ CAi , β1 , . . . , βv ∈ CAj ) Bel(Aag, SetAction(α1 , . . . , αu , β1 , . . . , βv ), Tn) ⇒ Bel(Aag, SetAction(α1 , . . . , αu , β1 , . . . , βv ), Tn+1) 2.3 La formulación del entorno Los siguientes axiomas proporcionan la definición formal para un Entorno Adversarial (AE) simple y de suma cero. La satisfacción de estos axiomas significa que el agente está situado en un entorno así. Proporciona especificaciones para que el agente Aag interactúe con su conjunto de adversarios A con respecto a los objetivos G∗ Aag y G∗ A en el tiempo TCo en algún estado del mundo w. AE(Aag, A, G∗ Aag, A1, . . . , Ak, G∗ A1, . . . , G∗ Ak, Tn, w) 1. Aag tiene un Int.Th su objetivo se completaría: (∃α ∈ CAag , Tα) Int.Th(Aag, Lograr(G∗ Aag , α), Tn, Tα, AE) 2. Aag cree que él y cada uno de sus adversarios Ao están persiguiendo metas completamente conflictivas: (∀Ao ∈ {A1, . . . , Ak}) Bel(Aag, FulConf(G∗ Aag , G∗ Ao ), Tn) 3. Aag cree que cada uno de sus adversarios en Ao tiene la Int.Th su objetivo en conflicto G∗ Aoi se completará: (∀Ao ∈ {A1, . . . , Ak})(∃β ∈ CAo , Tβ) Bel(Aag, Int.Th(Ao, Lograr(G∗ Ao , β), TCo, Tβ, AE), Tn) 4. Para construir un agente que pueda operar con éxito dentro de un entorno de adversarios estratégicos, debemos especificar pautas de comportamiento para sus interacciones. Utilizar una estrategia de maximización de Eval ingenua hasta cierta profundidad de búsqueda no siempre dará resultados satisfactorios por varias razones: (1) el problema del horizonte de búsqueda al buscar a una profundidad fija; (2) la fuerte suposición de un adversario óptimamente racional con recursos ilimitados; (3) utilizar una función de evaluación estimada que no dará resultados óptimos en todos los estados del mundo, y que puede ser explotada [9]. Los siguientes axiomas especifican los principios de comportamiento que se pueden utilizar para diferenciar entre agentes exitosos y menos exitosos en el entorno adversarial mencionado anteriormente. Esos axiomas deben ser utilizados como principios de especificación al diseñar e implementar agentes que deben ser capaces de desempeñarse bien en entornos adversarios. Los axiomas conductuales representan situaciones en las que el agente adoptará intenciones potenciales para (Pot.Int.To(...)) realizar una acción, lo que típicamente requerirá un razonamiento de medios-fines para seleccionar un posible curso de acción. Este razonamiento llevará a la adopción de un Int.To(...) (ver [4]). A1. Axioma para lograr metas. El primer axioma es el caso más simple; cuando el agente Aag cree que está a una acción (α) de lograr su objetivo en conflicto G∗ Aag, debería adoptar la intención potencial de hacer α y completar su objetivo. (∀Aag, α ∈ CAag, Tn, Tα, w ∈ W) (Bel(Aag, Do(Aag, α, Tα, w) ⇒ Achieve(G∗ Aag, α, w)) ⇒ Pot.Int.To(Aag, α, Tn, Tα, w) Este comportamiento algo trivial es el primer y más fuerte axioma. En cualquier situación, cuando el agente está a una acción de completar el objetivo, debería completar la acción. Cualquier función de evaluación justa clasificaría naturalmente a α como la acción de valor máximo (propiedad 1). Sin embargo, sin una axiomatización explícita de dicho comportamiento, podría haber situaciones en las que el agente decida tomar otra acción por diversas razones, debido a sus recursos de decisión limitados. A2. Axioma del Acto Preventivo. En una situación adversarial, el agente Aag podría decidir tomar acciones que dañen uno de los planes de su adversario para completar su objetivo, incluso si esas acciones no avanzan explícitamente a Aag hacia su objetivo en conflicto G∗ Aag. Tal acción preventiva tendrá lugar cuando el agente Aag tenga la creencia sobre la posibilidad de que su adversario Ao realice una acción β que le otorgará un alto 552 The Sixth Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) valor de evaluación de utilidad (> TrH). Creer que tomar la acción α evitará que el oponente realice su β, adoptará una intención potencial de hacer α. (∀Aag, Ao ∈ A, α ∈ CAag , β ∈ CAo , Tn, Tβ, w ∈ W) (Bel(Aag, Do(Ao, β, Tβ, w) ∧ Eval(Ao, β, w) > TrH, Tn) ∧ Bel(Aag, Do(Aag, α, Tα, w) ⇒ ¬Do(Ao, β, Tβ, w), Tn) ⇒ Pot.Int.To(Aag, α, Tn, Tα, w) Este axioma es un componente básico de cualquier entorno adversarial. Por ejemplo, al observar un juego de ajedrez, un jugador podría darse cuenta de que está a punto de ser jaque mateado por su oponente, por lo tanto, realiza una jugada preventiva. Otro ejemplo es el juego de Cuatro en Línea: cuando un jugador tiene una fila de tres fichas, su oponente debe bloquearla o perder. Una instancia específica de A1 ocurre cuando el adversario está a una acción de lograr su objetivo, y la acción preventiva inmediata debe ser tomada por el agente. Formalmente, tenemos las mismas creencias que se mencionaron anteriormente, con la creencia modificada de que realizar la acción β hará que el agente Ao logre su objetivo. Proposición 1: Caso de prevención o pérdida. (∀Aag, Ao ∈ A, α ∈ CAag , β ∈ CAo , G∗ Ao , Tn, Tα, Tβ, w ∈ W) Bel(Aag, Do(Ao, β, Tβ, w) ⇒ Achieve(G∗ Ao , β, w), Tn) ∧ Bel(Aag, Do(Aag, α, Tα, w) ⇒ ¬Do(Ao, β, Tβ, w)) ⇒ Pot.Int.To(Aag, α, Tn, Tα, w) Esquema de prueba: La Proposición 1 puede derivarse fácilmente del axioma A1 y de la propiedad 2 de la función Eval, que establece que cualquier acción que cause la finalización de un objetivo es una acción altamente beneficiosa. El comportamiento del acto preventivo ocurrirá implícitamente cuando la función Eval sea igual a la función de utilidad del mundo real. Sin embargo, al ser agentes racionales limitados y al tratar con una función de evaluación estimada, necesitamos axiomatizar explícitamente dicho comportamiento, ya que no siempre ocurrirá implícitamente a partir de la función de evaluación. A3. Axioma de Movimiento Táctico Subóptimo. En muchos escenarios, puede ocurrir una situación en la que un agente decida no tomar la acción más beneficiosa actual que puede tomar (la acción con el valor de evaluación de utilidad máximo), porque cree que tomar otra acción (con un valor de evaluación de utilidad más bajo) podría generar (dependiendo de la respuesta del adversario) una posibilidad futura para una acción altamente beneficiosa. Esto ocurrirá con mayor frecuencia cuando la función Eval sea inexacta y difiera en gran medida de la función de Utilidad. De manera formal, el agente Aag cree en una cierta SetAction que evolucionará de acuerdo con su acción inicial y producirá un valor beneficioso alto (> TrH) únicamente para él. (∀Aag, Ao ∈ A, Tn, w ∈ W) (∃α1 , . . . , αu ∈ CAi , β1 , . . . , βv ∈ CAj , Tα1 ) Bel(Aag, SetAction(α1 , . . . , αu , β1 , . . . , βv ), Tn) ∧ Bel(Aag, Eval(Ao, βv , w) < TrH < Eval(Aag, αu , w), Tn) ⇒ Pot.Int.To(Aag, α1 , Tn, Tα1 , w) Un agente podría creer que una cadena de eventos ocurrirá por diversas razones debido a la naturaleza inevitable del dominio. Por ejemplo, en el ajedrez, a menudo observamos lo siguiente: una jugada provoca una posición de jaque, lo que a su vez limita los movimientos del oponente para evitar el jaque, a lo que el primer jugador podría reaccionar con otro jaque, y así sucesivamente. El agente también podría creer en una cadena de eventos basada en su conocimiento del perfil de su adversario, lo que le permite prever los movimientos del adversario con alta precisión. A4. Axioma de Detección de Perfiles. El agente puede ajustar los perfiles de sus adversarios mediante observaciones y estudio de patrones (específicamente, si hay encuentros repetidos con el mismo adversario). Sin embargo, en lugar de esperar a que se revele la información del perfil, un agente también puede iniciar acciones que obligarán a su adversario a reaccionar de una manera que revele conocimiento del perfil sobre él. Formalmente, el axioma establece que si todas las acciones (γ) no son acciones altamente beneficiosas (< TrH), el agente puede realizar la acción α en el tiempo Tα si cree que resultará en una acción no altamente beneficiosa β por parte de su adversario, lo que a su vez le enseña sobre el perfil del adversario, es decir, proporciona un mayor AdvKnow(P Aj Ai , Tβ). (∀Aag, Ao ∈ A, α ∈ CAag , β ∈ CAo , Tn, Tα, Tβ, w ∈ W) Bel(Aag, (∀γ ∈ CAag )Eval(Aag, γ, w) < TrH, Tn) ∧ Bel(Aag, Do(Aag, α, Tα, w) ⇒ Do(Ao, β, Tβ, w), Tn) ∧ Bel(Aag, Eval(Ao, β, w) < TrH) ∧ Bel(Aag, AdvKnow(P Aj Ai , Tβ) > AdvKnow(P Aj Ai , Tn), Tn) ⇒ Pot.Int.To(Aag, α, Tn, Tα, w) Por ejemplo, volviendo al escenario del juego de ajedrez, consideremos comenzar una partida contra un oponente del que no sabemos nada, ni siquiera si es humano o un oponente computarizado. Podríamos comenzar a jugar una estrategia que sea adecuada frente a un oponente promedio, y ajustar nuestro juego de acuerdo a su nivel de juego. A5. Axioma de Formación de Alianzas El siguiente axioma de comportamiento es relevante únicamente en una instancia multilateral del entorno adversarial (obviamente, una alianza no puede formarse en un encuentro bilateral de suma cero). En diferentes situaciones durante una interacción multilateral, un grupo de agentes podría creer que es de su interés formar una alianza temporal. Una alianza de este tipo es un acuerdo que limita el comportamiento de sus miembros, pero se cree por parte de estos que les permite alcanzar un valor de utilidad mayor al que podrían lograr fuera de la alianza. Como ejemplo, podemos observar el clásico juego de mesa Risk, donde cada jugador tiene como objetivo individual ser el único conquistador del mundo, un juego de suma cero. Sin embargo, para lograr este objetivo, podría ser estratégicamente sabio hacer acuerdos de alto el fuego a corto plazo con otros jugadores, o unirse y atacar a un oponente que sea más fuerte que el resto. Los términos de una alianza definen la forma en que sus miembros deben actuar. Es un conjunto de predicados, denominados Términos, que es acordado por los miembros de la alianza y debe mantenerse verdadero durante la duración de la alianza. Por ejemplo, el conjunto de términos en el escenario de riesgo podría contener los siguientes predicados: 1. Los miembros de la alianza no se atacarán entre sí en los territorios X, Y y Z; 2. Los miembros de la alianza contribuirán C unidades por turno para atacar al adversario Ao; 3. Los miembros están obligados a permanecer como parte de la alianza hasta el tiempo Tk o hasta que el ejército adversario Ao sea más pequeño que Q. El conjunto de términos especifica restricciones intergrupales en cada uno de los miembros de la alianza (∀Aal i ∈ Aal ⊆ A) conjunto de acciones Cal i ⊆ C. Definición 6. El valor total de evaluación que el agente Ai logrará al ser parte de Aal es la suma de Evali (Eval para Ai) de cada uno de los valores de evaluación de Aal j después de tomar sus propias acciones α (a través del predicado del agente(α)): Al val(Ai, Cal, Aal, w) = α∈Cal Evali(Aal j, agente(α), w) Definición 7. Al TrH - es un número que representa un valor de Al en la Sexta Conferencia Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 553 umbral; por encima de él, se puede decir que la alianza es una alianza altamente beneficiosa. El valor de Al TrH se calculará dinámicamente de acuerdo con el progreso de la interacción, como se puede ver en [7]. Después de formarse una alianza, sus miembros ahora están trabajando en su entorno adversarial normal, así como de acuerdo con los estados mentales y axiomas requeridos para sus interacciones como parte de la alianza. El siguiente modelo de Alianza (AL) especifica las condiciones bajo las cuales el grupo Aal puede considerarse en una alianza y trabajando con un nuevo y limitado conjunto de acciones Cal, en el tiempo Tn. AL(Aal , Cal , w, Tn) 1. \n\nAL(Aal , Cal , w, Tn) 1. Aal tiene un MB en el que todos los miembros son parte de Aal: MB(Aal, (∀Aal i ∈ Aal)miembro(Aal i, Aal), Tn) 2. Aal tiene una MB que el grupo se mantenga: MB(Aal , (∀Aal i ∈ Aal )Int.Th (Ai, miembro(Ai, Aal ), Tn, Tn+1, Co), Tn) 3. Aal tiene un beneficio marginal que al ser miembros les otorga un alto valor de utilidad: MB(Aal , (∀Aal i ∈ Aal )Al val(Aal i , Cal , Aal , w) ≥ Al TrH, Tn) Los perfiles de los miembros son una parte crucial de las alianzas exitosas. Suponemos que los agentes que tienen perfiles más precisos de sus adversarios serán más exitosos en tales entornos. Tales agentes podrán predecir cuándo un miembro está a punto de violar el contrato de alianzas (elemento 2 en el modelo anterior) y tomar medidas correctivas (cuando el elemento 3 se falsificará). La robustez de la alianza depende en parte de la medida de confianza de sus miembros, la estimación de su posición objetiva y otras propiedades de su perfil. Debemos tener en cuenta que un agente puede formar parte simultáneamente de más de una alianza. Una alianza temporal, donde los miembros del grupo no tienen un objetivo común pero actúan colaborativamente en interés de sus propias metas individuales, es clasificada como un Grupo de Tratamiento por los psicólogos modernos (en contraste con un Grupo de Tarea, donde sus miembros tienen un objetivo común). El modelo de Actividad Compartida presentado en [5] modeló el comportamiento del Grupo de Tratamiento utilizando la misma formalización de Planes Compartidos. Al comparar ambas definiciones de una alianza y un Grupo de Tratamiento, encontramos una similitud sorprendente entre ambos modelos: las definiciones de los modelos de entorno son casi idénticas (ver las definiciones de SAs en [5]), y sus axiomas de Acto Egoísta y Acto Cooperativo se ajustan al comportamiento de nuestros agentes adversarios. La principal distinción entre ambos modelos es la integración de un axioma de acto de comportamiento útil, en la Actividad Compartida que no puede formar parte de la nuestra. Este axioma establece que un agente considerará tomar una acción que disminuirá su valor de Evaluación (hasta un cierto límite inferior), si cree que un compañero del grupo obtendrá un beneficio significativo. Dicho comportamiento no puede ocurrir en un entorno puramente adversarial (como un juego de suma cero), donde los miembros de la alianza están constantemente alerta para manipular su alianza en su propio beneficio. A6. Axioma de Maximización de la Evaluación. En un caso en el que todos los demás axiomas no sean aplicables, procederemos con la acción que maximice el valor heurístico calculado en la función Eval. (∀Aag, Ao ∈ A, α ∈ Cag, Tn, w ∈ W) Bel(Aag, (∀γ ∈ Cag)Eval(Aag, α, w) ≥ Eval(Aag, γ, w), Tn) ⇒ Pot.Int.To(Aag, α, Tn, Tα, w) T1. Optimalidad en Eval = Utilidad El modelo axiomático anterior aborda situaciones en las que la Utilidad es desconocida y los agentes son agentes racionales limitados. El siguiente teorema muestra que en interacciones bilaterales, donde los agentes tienen la función de Utilidad real (es decir, Eval = Utilidad) y son agentes racionales, los axiomas proporcionan el mismo resultado óptimo que la búsqueda adversarial clásica (por ejemplo, Min-Max). Teorema 1. Que Ae ag sea un agente AE racional ilimitado que utiliza la función de evaluación heurística Eval, Au ag sea el mismo agente utilizando la verdadera función de utilidad y Ao sea un único adversario racional ilimitado basado en utilidad. Dado que Eval = Utilidad: (∀α ∈ CAu ag , α ∈ CAe ag , Tn, w ∈ W) Pot.Int.To(Au ag, α, Tn, Tα, w) → Pot.Int.To(Ae ag, α , Tn, Tα, w) ∧ ((α = α ) ∨ (Utilidad(Au ag, α, w) = Eval(Ae ag, α , w))) Esquema de prueba - Dado que Au ag tiene la función de utilidad real y recursos ilimitados, puede generar el árbol de juego completo y ejecutar el algoritmo MinMax óptimo para elegir la acción de mayor valor de utilidad, que denotamos como α. La prueba mostrará que Ae ag, utilizando los axiomas AE, seleccionará la misma utilidad α o igual (cuando hay más de una acción con la misma utilidad máxima) cuando Eval = Utilidad. (A1) Axioma de logro de objetivo - supongamos que hay un α tal que su cumplimiento logrará el objetivo Au ags. Obtendrá la utilidad más alta mediante Min-Max para Au ag. El agente Ae seleccionará α u otra acción con el mismo valor de utilidad a través de A1. Si tal α no existe, Ae ag no puede aplicar este axioma y procede a A2. (A2) Axioma de la acción preventiva - (1) Observando el caso base (ver Prop1), si hay un β que lleva a Ao a alcanzar su objetivo, entonces una acción preventiva α producirá la mayor utilidad para Au ag. Au ag lo elegirá a través de la utilidad, mientras que Ae ag lo elegirá a través de A2. En el caso general, β es una acción altamente beneficiosa para Ao, lo que resulta en una baja utilidad para Au ag, lo que lo guiará a seleccionar un α que evitará β, mientras que Ae ag lo elegirá a través de A2.1 Si dicho β no existe para Ao, entonces A2 no es aplicable, y Ae ag puede proceder a A3. (A3) Axioma de movimiento táctico subóptimo: al usar una función de evaluación heurística, Ae ag tiene una creencia parcial en el perfil de su adversario (elemento 4 en el modelo AE), lo que puede llevarlo a creer en SetActions (Prop1). En nuestro caso, Ae ag tiene un perfil completo de su adversario óptimo y sabe que Ao se comportará de manera óptima según los valores reales de utilidad en el árbol de búsqueda completo, por lo tanto, cualquier creencia sobre una SetAction subóptima no puede existir, lo que hace que este axioma sea inaplicable. Ae ag procederá a A4. (A4) Axioma de detección de perfil - Dado que Ae ag tiene el perfil completo de Ao, ninguna de las acciones de Ae ag puede aumentar su conocimiento. Ese axioma no se aplicará, y el agente procederá con A6 (A5 será ignorado porque la interacción es bilateral). (A6) Axioma de maximización de la evaluación: este axioma seleccionará la máxima Evaluación para Ae ag. Dado que Eval = Utilidad, se seleccionará el mismo α que fue seleccionado por Au ag. 3. EVALUACIÓN El propósito principal de nuestro análisis experimental es evaluar el comportamiento y rendimiento de los modelos en un entorno adversarial real. Esta sección investiga si un caso acotado 1 en el que, tras la finalización de β, exista un γ que proporcione una alta utilidad para el Agente Au ag, no puede ocurrir porque Ao utiliza la misma utilidad, y la existencia de γ hará que clasifique a β como una acción de baja utilidad. 554 The Sixth Intl. En la Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07), los agentes racionales situados en entornos adversariales se beneficiarán más al aplicar nuestros axiomas de comportamiento sugeridos. 3.1 El Dominio Para explorar el uso del modelo anterior y sus axiomas de comportamiento, decidimos utilizar el juego de Conecta Cuatro como nuestro entorno adversarial. Connect-Four es un juego de 2 jugadores, de suma cero, que se juega utilizando un tablero similar a una matriz de 6x7. En cada turno, un jugador coloca un disco en una de las 7 columnas (el conjunto de 21 discos suele ser de color amarillo para el jugador 1 y rojo para el jugador 2; usaremos Blanco y Negro respectivamente para evitar confusiones). El ganador es el primer jugador en completar un conjunto de cuatro discos de su color en horizontal, vertical o diagonal. En ocasiones muy raras, el juego podría terminar en empate si todos los cuadros vacíos están llenos, pero ningún jugador logró crear un conjunto de 4 discos. El juego de Conecta Cuatro fue resuelto en [1], donde se muestra que el primer jugador (jugando con los discos blancos) puede forzar una victoria comenzando en la columna del medio (columna 4) y jugando de manera óptima. Sin embargo, la estrategia óptima es muy compleja y difícil de seguir incluso para agentes racionales limitados complejos, como los jugadores humanos. Antes de que podamos proceder a verificar el comportamiento del agente, primero debemos verificar que el dominio se ajusta a la definición de entornos adversarios mencionada anteriormente (en la que se basan los axiomas de comportamiento). Primero, al jugar al juego de Conecta Cuatro, el agente tiene la intención de ganar el juego (ítem 1). Segundo (ítem 2), nuestro agente cree que en Conecta Cuatro solo puede haber un ganador (o ningún ganador en la rara ocasión de un empate). Además, nuestro agente cree que su oponente en el juego intentará ganar (ítem 3), y esperamos que tenga algún conocimiento parcial (ítem 4) sobre su adversario (este conocimiento puede variar desde nada, pasando por hechos simples como la edad, hasta estrategias y debilidades). Por supuesto, no todos los encuentros de Conecta Cuatro son adversariales. Por ejemplo, cuando un padre está jugando el juego con su hijo, la siguiente situación podría ocurrir: el niño, con un fuerte incentivo para ganar, trata el entorno como adversarial (intenta ganar, entiende que solo puede haber un ganador y cree que su padre está tratando de vencerlo). Sin embargo, desde el punto de vista de los padres, el entorno podría ser visto como educativo, donde el objetivo no es ganar el juego, sino causar disfrute o practicar el razonamiento estratégico. En un entorno educativo como este, un nuevo conjunto de axiomas de comportamiento podría ser más beneficioso para los objetivos de los padres que nuestros axiomas de comportamiento adversarial sugeridos. 3.2 Análisis de Axiomas Después de demostrar que el juego de Conecta Cuatro es de hecho un entorno adversarial bilateral de suma cero, el siguiente paso es observar el comportamiento de los jugadores durante el juego y verificar si comportarse de acuerdo con nuestro modelo mejora el rendimiento. Para hacerlo, hemos recopilado archivos de registro de juegos de Conecta Cuatro completados que fueron jugados por jugadores humanos a través de Internet. Nuestros datos de archivo de registro recopilados provienen de sitios de Juego por Correo Electrónico (PBeM). Estos son sitios web que alojan juegos por correo electrónico, donde cada movimiento se realiza a través de un intercambio de correos electrónicos entre el servidor y los jugadores. Muchos de estos sitios de archivos contienen interacciones competitivas reales y también mantienen un sistema de clasificación para sus miembros. La mayoría de los datos que utilizamos se pueden encontrar en [6]. Como se puede aprender de [1], Conecta Cuatro tiene una estrategia óptima y una ventaja considerable para el jugador que comienza el juego (a quien llamamos jugador Blanco). Nos concentraremos en nuestro análisis en los movimientos de los segundos jugadores (a los que llamaremos Negras). El jugador Blanco, al ser el primero en actuar, tiene la llamada ventaja de la iniciativa. Tener la ventaja y una buena estrategia mantendrá al jugador Negro ocupado reaccionando a sus movimientos, en lugar de iniciar amenazas. Una amenaza es una combinación de tres discos del mismo color, con un espacio vacío para el cuarto disco ganador. Una amenaza abierta es una amenaza que puede materializarse en el próximo movimiento del oponente. Para que el jugador Negro gane, debe de alguna manera cambiar el rumbo, tomar la ventaja y comenzar a presentar amenazas al jugador Blanco. Exploraremos el comportamiento de los jugadores negros y su conformidad con nuestros axiomas. Para hacerlo, construimos una aplicación que lee archivos de registro y analiza los movimientos de los jugadores Negros. La aplicación contiene dos componentes principales: (1) un algoritmo Min-Max para la evaluación de movimientos; (2) un detector de amenazas abiertas para descubrir amenazas abiertas. El algoritmo Min-Max funcionará hasta una profundidad d dada y, para cada movimiento α, devolverá el valor heurístico para la próxima acción tomada por el jugador según lo escrito en el archivo de registro, h(α), junto con el valor heurístico máximo, maxh(α), que podría lograrse antes de realizar el movimiento (obviamente, si h(α) = maxh(α), entonces el jugador no realizó el movimiento óptimo desde el punto de vista heurístico). El trabajo de los detectores de amenazas es notificar si se tomó alguna acción para bloquear una amenaza abierta (no bloquear una amenaza abierta probablemente hará que el jugador pierda en el próximo movimiento del oponente). La función heurística utilizada por Min-Max para evaluar la utilidad de los jugadores es la siguiente función, que es simple de calcular, pero proporciona un desafío razonable a los oponentes humanos: Definición 8. Sea Grupo un conjunto adyacente de cuatro cuadrados que son horizontales, verticales o diagonales. El Grupo n (Grupo w) es un Grupo con n piezas de color negro (blanco) y 4−n casillas vacías. h = ((Grupo1 negro ∗α)+(Grupo2 negro ∗β)+(Grupo3 negro ∗γ)+(Grupo4 negro ∗∞)) − ((Grupo1 blanco ∗α)+(Grupo2 blanco ∗β)+(Grupo3 blanco ∗γ)+(Grupo4 blanco ∗∞)) Los valores de α, β y δ pueden variar para formar cualquier combinación lineal deseada; sin embargo, es importante asignarles valores teniendo en cuenta el orden α < β < δ (usamos 1, 4 y 8 como sus valores respectivos). Grupos de 4 discos del mismo color significan victoria, por lo tanto, el descubrimiento de dicho grupo resultará en ∞ para asegurar un valor extremo. Ahora utilizamos nuestra función de evaluación estimada para evaluar las acciones de los jugadores Negros durante la interacción adversarial de Conecta Cuatro. Cada juego del archivo de registro fue introducido en la aplicación, la cual procesó y generó un archivo de registro reformateado que contenía el valor h del movimiento actual, el valor maxh que podría ser alcanzado y una notificación si se detectaba una amenaza abierta. Un total de 123 partidas fueron analizadas (57 con victoria de las blancas y 66 con victoria de las negras). Algunos juegos adicionales fueron ignorados manualmente en el experimento, debido a estos problemas: un jugador abandonando el juego mientras el resultado no es definitivo, o una jugada irracional y contundente en las primeras etapas del juego (por ejemplo, no bloquear un grupo obviamente ganador en las primeras jugadas de apertura). Además, también se eliminó un empate. El simulador se ejecutó con una profundidad de búsqueda de 3 movimientos. Ahora procedemos a analizar los juegos con respecto a cada axioma de comportamiento. El Sexto Internacional. La siguiente sección presenta las evaluaciones heurísticas del algoritmo Min-Max para cada acción, y verifica la cantidad y el alcance de las acciones tácticas subóptimas y sus implicaciones en el rendimiento. La Tabla 1 muestra los resultados y las percepciones del análisis heurístico de los juegos, cuando la profundidad de búsqueda es igual a 3 (esta profundidad de búsqueda fue seleccionada para que los resultados sean comparables a [9], ver Sección 3.2.3). La información heurística de las tablas es la diferencia entre el valor heurístico máximo presente y el valor heurístico de la acción que finalmente tomó el jugador (es decir, cuanto más cercano sea el número a 0, más cercana estuvo la acción a la acción heurística máxima). La primera fila presenta los valores de diferencia de la acción que tuvo el valor de diferencia máximo entre todas las acciones de los jugadores Negros en un juego dado, promediado en todos los juegos ganados y perdidos por los Negros (ver columnas respectivas). En los juegos en los que el jugador Negro pierde, su valor promedio de diferencia fue de -17.62, mientras que en los juegos en los que el jugador Negro ganó, su promedio fue de -12.02. La segunda fila amplía el análisis al considerar las 3 acciones con mayor diferencia heurística, y promediarlas. En ese caso, notamos una diferencia heurística promedio de 5 puntos entre los juegos en los que el jugador Negro pierde y los juegos en los que gana. Sin embargo, la importancia de esos números es que nos permitieron hacer una suposición educada sobre un número umbral de 11.5, como el valor de la constante TrH, que diferencia entre acciones normales y altamente beneficiosas. Después de encontrar una constante TrH aproximada, podemos proceder con un análisis de la importancia de los movimientos subóptimos. Para hacerlo, tomamos el subconjunto de juegos en los que el valor mínimo de diferencia heurística para las acciones de las negras era de 11.5. Como se presenta en la Tabla 2, podemos ver el promedio de 3 horas mínimo de los 3 mayores rangos y el porcentaje respectivo de juegos ganados. La primera fila muestra que el jugador Negro ganó solo el 12% de los juegos en los que el promedio de sus 3 acciones con mayor diferencia heurística (min3 h) era menor que el umbral sugerido, TrH = 11.5. La segunda fila muestra un resultado sorprendente: parece que cuando min3 h > −4, el jugador Negro rara vez gana. La intuición sugeriría que los juegos en los que los valores de evaluación de la acción estén más cerca de los valores máximos resultarán en más juegos ganados para las piezas negras. Sin embargo, parece que en el dominio de Conecta Cuatro, simplemente responder con acciones algo fácilmente esperadas, sin iniciar algunos movimientos sorprendentes y subóptimos, no produce buenos resultados. La última fila resume las principales conclusiones del análisis; la mayoría de las victorias de Blacks (83%) se produjeron cuando su min3 h estaba en el rango de -11.5 a -4. Una inspección detallada de esos juegos ganados por las fichas negras muestra el siguiente patrón detrás de los números: después de los movimientos de apertura estándar, las fichas negras de repente colocan una ficha en una columna aislada, lo cual parece ser un desperdicio de un movimiento. El blanco sigue construyendo sus amenazas, generalmente ignorando el último movimiento de las negras, que a su vez utiliza el disco aislado como ancla para una futura amenaza ganadora. Los resultados muestran que fue beneficioso para el jugador Negro en la Tabla 2: Porcentajes de ganancias de los Negros % de juegos min3 h < −11.5 12% min3 h > −4 5% −11.5 ≤ min3 h ≤ −4 83% tomar acciones subóptimas y no dar el valor heurístico más alto posible en ese momento, pero no será demasiado perjudicial para su posición (es decir, no dará un alto valor beneficioso a su adversario). Resultó que aprender el umbral es un aspecto importante del éxito: realizar movimientos extremadamente arriesgados (min3 h < −11.5) o intentar evitarlos (min3 h > −4) reduce significativamente las posibilidades de ganar de los jugadores negros. 3.2.2 Afirmación del Axioma de Monitoreo de Perfiles En la tarea de demostrar la importancia de monitorear los perfiles de los adversarios, nuestros archivos de registro no pudieron ser utilizados porque no contenían interacciones repetidas entre jugadores, que son necesarias para inferir el conocimiento de los jugadores sobre sus adversarios. Sin embargo, la importancia del modelado del oponente y su uso para obtener ventajas tácticas ya ha sido estudiada en varios ámbitos ([3, 9] son buenos ejemplos). En un artículo reciente, Markovitch y Reger [9] exploraron la noción de aprendizaje y explotación de la debilidad del oponente en interacciones competitivas. Aplican estrategias de aprendizaje simples analizando ejemplos de interacciones pasadas en un dominio específico. También utilizaron el dominio adversarial de Conecta Cuatro, que ahora se puede utilizar para comprender la importancia de monitorear el perfil del adversario. Tras la presentación de su modelo teórico, describen un extenso estudio empírico y verifican el rendimiento de los agentes después de aprender el modelo de debilidades con ejemplos pasados. Uno de los dominios utilizados como entorno competitivo fue el mismo juego de Conecta Cuatro (las Damas fueron el segundo dominio). Su función heurística era idéntica a la nuestra con tres variaciones diferentes (H1, H2 y H3) que se distinguen entre sí por los valores de sus coeficientes de combinación lineal. La profundidad de búsqueda para los jugadores fue de 3 (como en nuestro análisis). Sus extensos experimentos verifican y comparan diversas estrategias de aprendizaje, factores de riesgo, conjuntos de características predefinidas y métodos de uso. La conclusión es que el dominio de Conecta Cuatro muestra una mejora de una tasa de victoria de 0.556 antes de modelar a 0.69 después de modelar (página 22). Sus conclusiones, que muestran un rendimiento mejorado al sostener y utilizar el modelo del adversario, justifican el esfuerzo de monitorear el perfil del adversario para interacciones continuas y repetidas. Un punto adicional que surgió en sus experimentos es el siguiente: después de que se haya aprendido el modelo de debilidad del oponente, los autores describen diferentes métodos para integrar el modelo de debilidad del oponente en la estrategia de decisión de los agentes. Sin embargo, independientemente del método específico que elijan para trabajar, todos los métodos de integración podrían hacer que el agente tome decisiones subóptimas; podría hacer que el agente prefiera acciones que son subóptimas en el momento actual de la decisión, pero que podrían hacer que el oponente reaccione de acuerdo con su modelo de debilidad (como lo representa nuestro agente), lo cual a su vez será beneficioso para nosotros en el futuro. El comportamiento de los agentes, como se demuestra en [9], confirma y refuerza aún más nuestro Axioma Táctico Subóptimo discutido en la sección anterior. 556 The Sixth Intl. La necesidad de los axiomas de Logro de Objetivos, Acto Preventivo y Maximización de la Evaluación es evidente y no requiere más verificación. Sin embargo, incluso con respecto a esos axiomas, surgieron algunas ideas interesantes en el análisis de registros. Los axiomas de Logro y Prevención de Actos, aunque teóricamente triviales, parecen presentar un desafío para un jugador humano. En la inspección inicial de los registros, nos encontramos con algunos juegos donde un jugador, por razones inexplicables, no bloqueó al otro para que ganara o no logró ejecutar su propia jugada ganadora. Podemos atribuir esas fallas a la falta de atención de los humanos, o a un error de escritura en su respuesta al movimiento; sin embargo, esos errores podrían ocurrir en agentes racionales limitados, y el comportamiento apropiado debe ser axiomatizado. Un juego típico de Cuatro en Línea gira en torno a generar amenazas y bloquearlas. En nuestro análisis buscamos acciones preventivas explícitas, es decir, movimientos que bloqueen un grupo de 3 discos o que eliminen una amenaza futura (en nuestro horizonte de búsqueda limitado). Encontramos que en el 83% de los juegos totales, hubo al menos una acción preventiva tomada por el jugador Negro. También se encontró que Black promedió 2.8 acciones preventivas por juego en los juegos en los que perdió, mientras que promedió 1.5 acciones preventivas por juego al ganar. Parece que Black necesita 1 o 2 acciones preventivas para construir su posición inicial de toma, antes de comenzar a presentar amenazas. Si no logra ganar, generalmente evitará una amenaza adicional o dos antes de sucumbir ante el Blanco. 4. TRABAJO RELACIONADO Mucha investigación se ocupa de la axiomatización del trabajo en equipo y los estados mentales de los individuos: algunos modelos utilizan el conocimiento y la creencia [10], otros tienen modelos de metas e intenciones [8, 4]. Sin embargo, todas estas teorías formales tratan sobre el trabajo en equipo y la cooperación de agentes. Hasta donde sabemos, nuestro modelo es el primero en proporcionar un modelo formalizado para entornos adversarios explícitos y el comportamiento de los agentes en él. El algoritmo de búsqueda adversarial clásico Min-Max fue el primer intento de integrar al oponente en el espacio de búsqueda con la suposición débil de un oponente que juega de manera óptima. Desde entonces, se ha dedicado mucho esfuerzo a integrar el modelo del oponente en el procedimiento de toma de decisiones para predecir el comportamiento futuro. El algoritmo M∗ presentado por Carmel y Markovitch [2] mostró un método para incorporar modelos de oponentes en la búsqueda de adversarios, mientras que en [3] utilizaron el aprendizaje para proporcionar un modelo de oponente más preciso en un entorno de juego repetido de 2 jugadores, donde las estrategias de los agentes se modelaron como autómatas finitos. El trabajo adicional de planificación adversarial fue realizado por Willmott y colaboradores [13], quienes proporcionaron un enfoque de planificación adversarial para el juego de GO. La investigación mencionada anteriormente trató sobre la búsqueda adversarial y la integración de modelos de oponentes en métodos de búsqueda de utilidad clásicos. Ese trabajo muestra la importancia del modelado del oponente y la capacidad de explotarlo en beneficio de un agente. Sin embargo, las limitaciones básicas de esos métodos de búsqueda siguen aplicándose; nuestro modelo intenta superar esas limitaciones presentando un modelo formal para una nueva especificación adversarial basada en estados mentales. 5. CONCLUSIONES Presentamos un modelo de Entorno Adversarial para un agente racional acotado que está situado en un entorno de N jugadores, de suma cero. Estos fueron posteriormente eliminados del análisis final. Utilizamos la formalización de SharedPlans para definir el modelo y los axiomas que los agentes pueden aplicar como pautas de comportamiento. El modelo está destinado a ser utilizado como una guía para diseñar agentes que necesitan operar en entornos adversos. Presentamos resultados empíricos, basados en el análisis del archivo de registro de ConnectFour, que ejemplifican el modelo y los axiomas para una instancia bilateral del entorno. Los resultados que presentamos son un primer paso hacia un modelo ampliado que abarcará todos los tipos de entornos adversarios, por ejemplo, entornos que no son de suma cero y entornos que contienen agentes naturales que no forman parte del conflicto directo. Esos desafíos y más serán abordados en futuras investigaciones. 6. AGRADECIMIENTO Esta investigación fue apoyada en parte por las becas de la Fundación de Ciencia de Israel #1211/04 y #898/05. 7. REFERENCIAS [1] L. V. Allis. Un enfoque basado en el conocimiento de Conecta Cuatro: el juego está resuelto, gana el jugador Blanco. Tesis de maestría, Universidad Libre de Ámsterdam, Países Bajos, 1988. [2] D. Carmel y S. Markovitch. Incorporando modelos de oponentes en la búsqueda de adversarios. En Actas de la Decimotercera Conferencia Nacional de Inteligencia Artificial, páginas 120-125, Portland, Oregón, 1996. [3] D. Carmel y S. Markovitch. Modelado de oponentes en sistemas multiagente. En G. Weiß y S. Sen, editores, Adaptación y Aprendizaje en Sistemas Multiagente, páginas 40-52. Springer-Verlag, 1996. [4] B. J. Grosz y S. Kraus. Planes colaborativos para acciones grupales complejas. Inteligencia Artificial, 86(2):269-357, 1996. [5] M. Hadad, G. Kaminka, G. Armon y S. Kraus. Apoyando la actividad colaborativa. En Proc. de AAAI-2005, páginas 83-88, Pittsburgh, 2005. [6] http://www.gamerz.net/˜pbmserv/. [7] S. Kraus y D. Lehmann. Diseñando y construyendo un agente automatizado de negociación. Inteligencia Computacional, 11:132-171, 1995. [8] H. J. Levesque, P. R. Cohen y J. H. T. Nunes. Actuar juntos. En Proc. de AAAI-90, páginas 94-99, Boston, MA, 1990. [9] S. Markovitch y R. Reger. Aprender y aprovechar las debilidades relativas de los agentes oponentes. Agentes Autónomos y Sistemas Multiagente, 10(2):103-130, 2005. [10] Y. M. Ronald Fagin, Joseph Y. Halpern y M. Y. Vardi. Razonamiento sobre el conocimiento. MIT Press, Cambridge, Mass., 1995. [11] P. Thagard.\nMIT Press, Cambridge, Massachusetts, 1995. [11] P. Thagard. Resolución de problemas adversariales: Modelado de un oponente utilizando coherencia explicativa. Ciencia Cognitiva, 16(1):123-149, 1992. [12] R. W. Toseland y R. F. Rivas. Una introducción a la práctica del trabajo en grupo. Prentice Hall, Englewood Cliffs, NJ, segunda edición, 1995. [13] S. Willmott, J. Richardson, A. Bundy y J. Levine. Un enfoque de planificación adversarial para el juego de Go. Notas de conferencia en Ciencias de la Computación, 1558:93-112, 1999. El Sexto Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 557 ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        }
    }
}