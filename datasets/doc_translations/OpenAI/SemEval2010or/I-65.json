{
    "id": "I-65",
    "original_text": "Graphical Models for Online Solutions to Interactive POMDPs Prashant Doshi Dept. of Computer Science University of Georgia Athens, GA 30602, USA pdoshi@cs.uga.edu Yifeng Zeng Dept. of Computer Science Aalborg University DK-9220 Aalborg, Denmark yfzeng@cs.aau.edu Qiongyu Chen Dept. of Computer Science National Univ. of Singapore 117543, Singapore chenqy@comp.nus.edu.sg ABSTRACT We develop a new graphical representation for interactive partially observable Markov decision processes (I-POMDPs) that is significantly more transparent and semantically clear than the previous representation. These graphical models called interactive dynamic influence diagrams (I-DIDs) seek to explicitly model the structure that is often present in real-world problems by decomposing the situation into chance and decision variables, and the dependencies between the variables. I-DIDs generalize DIDs, which may be viewed as graphical representations of POMDPs, to multiagent settings in the same way that I-POMDPs generalize POMDPs. I-DIDs may be used to compute the policy of an agent online as the agent acts and observes in a setting that is populated by other interacting agents. Using several examples, we show how I-DIDs may be applied and demonstrate their usefulness. Categories and Subject Descriptors I.2.11 [Distributed Artificial Intelligence]: Multiagent Systems General Terms Theory 1. INTRODUCTION Interactive partially observable Markov decision processes (IPOMDPs) [9] provide a framework for sequential decision-making in partially observable multiagent environments. They generalize POMDPs [13] to multiagent settings by including the other agents computable models in the state space along with the states of the physical environment. The models encompass all information influencing the agents behaviors, including their preferences, capabilities, and beliefs, and are thus analogous to types in Bayesian games [11]. I-POMDPs adopt a subjective approach to understanding strategic behavior, rooted in a decision-theoretic framework that takes a decision-makers perspective in the interaction. In [15], Polich and Gmytrasiewicz introduced interactive dynamic influence diagrams (I-DIDs) as the computational representations of I-POMDPs. I-DIDs generalize DIDs [12], which may be viewed as computational counterparts of POMDPs, to multiagents settings in the same way that I-POMDPs generalize POMDPs. I-DIDs contribute to a growing line of work [19] that includes multi-agent influence diagrams (MAIDs) [14], and more recently, networks of influence diagrams (NIDs) [8]. These formalisms seek to explicitly model the structure that is often present in real-world problems by decomposing the situation into chance and decision variables, and the dependencies between the variables. MAIDs provide an alternative to normal and extensive game forms using a graphical formalism to represent games of imperfect information with a decision node for each agents actions and chance nodes capturing the agents private information. MAIDs objectively analyze the game, efficiently computing the Nash equilibrium profile by exploiting the independence structure. NIDs extend MAIDs to include agents uncertainty over the game being played and over models of the other agents. Each model is a MAID and the network of MAIDs is collapsed, bottom up, into a single MAID for computing the equilibrium of the game keeping in mind the different models of each agent. Graphical formalisms such as MAIDs and NIDs open up a promising area of research that aims to represent multiagent interactions more transparently. However, MAIDs provide an analysis of the game from an external viewpoint and the applicability of both is limited to static single play games. Matters are more complex when we consider interactions that are extended over time, where predictions about others future actions must be made using models that change as the agents act and observe. I-DIDs address this gap by allowing the representation of other agents models as the values of a special model node. Both, other agents models and the original agents beliefs over these models are updated over time using special-purpose implementations. In this paper, we improve on the previous preliminary representation of the I-DID shown in [15] by using the insight that the static I-ID is a type of NID. Thus, we may utilize NID-specific language constructs such as multiplexers to represent the model node, and subsequently the I-ID, more transparently. Furthermore, we clarify the semantics of the special purpose policy link introduced in the representation of I-DID by [15], and show that it could be replaced by traditional dependency links. In the previous representation of the I-DID, the update of the agents belief over the models of others as the agents act and receive observations was denoted using a special link called the model update link that connected the model nodes over time. We explicate the semantics of this link by showing how it can be implemented using the traditional dependency links between the chance nodes that constitute the model nodes. The net result is a representation of I-DID that is significantly more transparent, semantically clear, and capable of being implemented using the standard algorithms for solving DIDs. We show how IDIDs may be used to model an agents uncertainty over others models, that may themselves be I-DIDs. Solution to the I-DID is a policy that prescribes what the agent should do over time, given its beliefs over the physical state and others models. Analogous to DIDs, I-DIDs may be used to compute the policy of an agent online as the agent acts and observes in a setting that is populated by other interacting agents. 2. BACKGROUND: FINITELY NESTED IPOMDPS Interactive POMDPs generalize POMDPs to multiagent settings by including other agents models as part of the state space [9]. Since other agents may also reason about others, the interactive state space is strategically nested; it contains beliefs about other agents models and their beliefs about others. For simplicity of presentation we consider an agent, i, that is interacting with one other agent, j. A finitely nested I-POMDP of agent i with a strategy level l is defined as the tuple: I-POMDPi,l = ISi,l, A, Ti, Ωi, Oi, Ri where: • ISi,l denotes a set of interactive states defined as, ISi,l = S × Mj,l−1, where Mj,l−1 = {Θj,l−1 ∪ SMj}, for l ≥ 1, and ISi,0 = S, where S is the set of states of the physical environment. Θj,l−1 is the set of computable intentional models of agent j: θj,l−1 = bj,l−1, ˆθj where the frame, ˆθj = A, Ωj, Tj, Oj, Rj, OCj . Here, j is Bayes rational and OCj is js optimality criterion. SMj is the set of subintentional models of j. Simple examples of subintentional models include a no-information model [10] and a fictitious play model [6], both of which are history independent. We give a recursive bottom-up construction of the interactive state space below. ISi,0 = S, Θj,0 = { bj,0, ˆθj | bj,0 ∈ Δ(ISj,0)} ISi,1 = S × {Θj,0 ∪ SMj}, Θj,1 = { bj,1, ˆθj | bj,1 ∈ Δ(ISj,1)} . . . . . . ISi,l = S × {Θj,l−1 ∪ SMj}, Θj,l = { bj,l, ˆθj | bj,l ∈ Δ(ISj,l)} Similar formulations of nested spaces have appeared in [1, 3]. • A = Ai × Aj is the set of joint actions of all agents in the environment; • Ti : S ×A×S → [0, 1], describes the effect of the joint actions on the physical states of the environment; • Ωi is the set of observations of agent i; • Oi : S × A × Ωi → [0, 1] gives the likelihood of the observations given the physical state and joint action; • Ri : ISi × A → R describes agent is preferences over its interactive states. Usually only the physical states will matter. Agent is policy is the mapping, Ω∗ i → Δ(Ai), where Ω∗ i is the set of all observation histories of agent i. Since belief over the interactive states forms a sufficient statistic [9], the policy can also be represented as a mapping from the set of all beliefs of agent i to a distribution over its actions, Δ(ISi) → Δ(Ai). 2.1 Belief Update Analogous to POMDPs, an agent within the I-POMDP framework updates its belief as it acts and observes. However, there are two differences that complicate the belief update in multiagent settings when compared to single agent ones. First, since the state of the physical environment depends on the actions of both agents, is prediction of how the physical state changes has to be made based on its prediction of js actions. Second, changes in js models have to be included in is belief update. Specifically, if j is intentional then an update of js beliefs due to its action and observation has to be included. In other words, i has to update its belief based on its prediction of what j would observe and how j would update its belief. If js model is subintentional, then js probable observations are appended to the observation history contained in the model. Formally, we have: Pr(ist |at−1 i , bt−1 i,l ) = β ISt−1:mt−1 j =θt j bt−1 i,l (ist−1 ) × at−1 j Pr(at−1 j |θt−1 j,l−1)Oi(st , at−1 i , at−1 j , ot i) ×Ti(st−1 , at−1 i , at−1 j , st ) ot j Oj(st , at−1 i , at−1 j , ot j) ×τ(SEθt j (bt−1 j,l−1, at−1 j , ot j) − bt j,l−1) (1) where β is the normalizing constant, τ is 1 if its argument is 0 otherwise it is 0, Pr(at−1 j |θt−1 j,l−1) is the probability that at−1 j is Bayes rational for the agent described by model θt−1 j,l−1, and SE(·) is an abbreviation for the belief update. For a version of the belief update when js model is subintentional, see [9]. If agent j is also modeled as an I-POMDP, then is belief update invokes js belief update (via the term SEθt j ( bt−1 j,l−1 , at−1 j , ot j)), which in turn could invoke is belief update and so on. This recursion in belief nesting bottoms out at the 0th level. At this level, the belief update of the agent reduces to a POMDP belief update. 1 For illustrations of the belief update, additional details on I-POMDPs, and how they compare with other multiagent frameworks, see [9]. 2.2 Value Iteration Each belief state in a finitely nested I-POMDP has an associated value reflecting the maximum payoff the agent can expect in this belief state: Un ( bi,l, θi ) = max ai∈Ai is∈ISi,l ERi(is, ai)bi,l(is)+ γ oi∈Ωi Pr(oi|ai, bi,l)Un−1 ( SEθi (bi,l, ai, oi), θi ) (2) where, ERi(is, ai) = aj Ri(is, ai, aj)Pr(aj|mj,l−1) (since is = (s, mj,l−1)). Eq. 2 is a basis for value iteration in I-POMDPs. Agent is optimal action, a∗ i , for the case of finite horizon with discounting, is an element of the set of optimal actions for the belief state, OPT(θi), defined as: OPT( bi,l, θi ) = argmax ai∈Ai is∈ISi,l ERi(is, ai)bi,l(is) +γ oi∈Ωi Pr(oi|ai, bi,l)Un ( SEθi (bi,l, ai, oi), θi ) (3) 3. INTERACTIVEINFLUENCEDIAGRAMS A naive extension of influence diagrams (IDs) to settings populated by multiple agents is possible by treating other agents as automatons, represented using chance nodes. However, this approach assumes that the agents actions are controlled using a probability distribution that does not change over time. Interactive influence diagrams (I-IDs) adopt a more sophisticated approach by generalizing IDs to make them applicable to settings shared with other agents who may act and observe, and update their beliefs. 3.1 Syntax In addition to the usual chance, decision, and utility nodes, IIDs include a new type of node called the model node. We show a general level l I-ID in Fig. 1(a), where the model node (Mj,l−1) is denoted using a hexagon. We note that the probability distribution over the chance node, S, and the model node together represents agent is belief over its interactive states. In addition to the model 1 The 0th level model is a POMDP: Other agents actions are treated as exogenous events and folded into the T, O, and R functions. The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 815 Figure 1: (a) A generic level l I-ID for agent i situated with one other agent j. The hexagon is the model node (Mj,l−1) whose structure we show in (b). Members of the model node are I-IDs themselves (m1 j,l−1, m2 j,l−1; diagrams not shown here for simplicity) whose decision nodes are mapped to the corresponding chance nodes (A1 j , A2 j ). Depending on the value of the node, Mod[Mj], the distribution of each of the chance nodes is assigned to the node Aj. (c) The transformed I-ID with the model node replaced by the chance nodes and the relationships between them. node, I-IDs differ from IDs by having a dashed link (called the policy link in [15]) between the model node and a chance node, Aj, that represents the distribution over the other agents actions given its model. In the absence of other agents, the model node and the chance node, Aj, vanish and I-IDs collapse into traditional IDs. The model node contains the alternative computational models ascribed by i to the other agent from the set, Θj,l−1 ∪ SMj, where Θj,l−1 and SMj were defined previously in Section 2. Thus, a model in the model node may itself be an I-ID or ID, and the recursion terminates when a model is an ID or subintentional. Because the model node contains the alternative models of the other agent as its values, its representation is not trivial. In particular, some of the models within the node are I-IDs that when solved generate the agents optimal policy in their decision nodes. Each decision node is mapped to the corresponding chance node, say A1 j , in the following way: if OPT is the set of optimal actions obtained by solving the I-ID (or ID), then Pr(aj ∈ A1 j ) = 1 |OP T | if aj ∈ OPT, 0 otherwise. Borrowing insights from previous work [8], we observe that the model node and the dashed policy link that connects it to the chance node, Aj, could be represented as shown in Fig. 1(b). The decision node of each level l − 1 I-ID is transformed into a chance node, as we mentioned previously, so that the actions with the largest value in the decision node are assigned uniform probabilities in the chance node while the rest are assigned zero probability. The different chance nodes (A1 j , A2 j ), one for each model, and additionally, the chance node labeled Mod[Mj] form the parents of the chance node, Aj. Thus, there are as many action nodes (A1 j , A2 j ) in Mj,l−1 as the number of models in the support of agent is beliefs. The conditional probability table of the chance node, Aj, is a multiplexer that assumes the distribution of each of the action nodes (A1 j , A2 j ) depending on the value of Mod[Mj]. The values of Mod[Mj] denote the different models of j. In other words, when Mod[Mj] has the value m1 j,l−1, the chance node Aj assumes the distribution of the node A1 j , and Aj assumes the distribution of A2 j when Mod[Mj] has the value m2 j,l−1. The distribution over the node, Mod[Mj], is the agent is belief over the models of j given a physical state. For more agents, we will have as many model nodes as there are agents. Notice that Fig. 1(b) clarifies the semantics of the policy link, and shows how it can be represented using the traditional dependency links. In Fig. 1(c), we show the transformed I-ID when the model node is replaced by the chance nodes and relationships between them. In contrast to the representation in [15], there are no special-purpose policy links, rather the I-ID is composed of only those types of nodes that are found in traditional IDs and dependency relationships between the nodes. This allows I-IDs to be represented and implemented using conventional application tools that target IDs. Note that we may view the level l I-ID as a NID. Specifically, each of the level l − 1 models within the model node are blocks in the NID (see Fig. 2). If the level l = 1, each block is a traditional ID, otherwise if l > 1, each block within the NID may itself be a NID. Note that within the I-IDs (or IDs) at each level, there is only a single decision node. Thus, our NID does not contain any MAIDs. Figure 2: A level l I-ID represented as a NID. The probabilities assigned to the blocks of the NID are is beliefs over js models conditioned on a physical state. 3.2 Solution The solution of an I-ID proceeds in a bottom-up manner, and is implemented recursively. We start by solving the level 0 models, which, if intentional, are traditional IDs. Their solutions provide probability distributions over the other agents actions, which are entered in the corresponding chance nodes found in the model node of the level 1 I-ID. The mapping from the level 0 models decision nodes to the chance nodes is carried out so that actions with the largest value in the decision node are assigned uniform probabilities in the chance node while the rest are assigned zero probability. Given the distributions over the actions within the different chance nodes (one for each model of the other agent), the level 1 I-ID is transformed as shown in Fig. 1(c). During the transformation, the conditional probability table (CPT) of the node, Aj, is populated such that the node assumes the distribution of each of the chance nodes depending on the value of the node, Mod[Mj]. As we mentioned previously, the values of the node Mod[Mj] denote the different models of the other agent, and its distribution is the agent is belief over the models of j conditioned on the physical state. The transformed level 1 I-ID is a traditional ID that may be solved us816 The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) (a) (b) Figure 3: (a) A generic two time-slice level l I-DID for agent i in a setting with one other agent j. Notice the dotted model update link that denotes the update of the models of j and the distribution over the models over time. (b) The semantics of the model update link. ing the standard expected utility maximization method [18]. This procedure is carried out up to the level l I-ID whose solution gives the non-empty set of optimal actions that the agent should perform given its belief. Notice that analogous to IDs, I-IDs are suitable for online decision-making when the agents current belief is known. 4. INTERACTIVE DYNAMIC INFLUENCE DIAGRAMS Interactive dynamic influence diagrams (I-DIDs) extend I-IDs (and NIDs) to allow sequential decision-making over several time steps. Just as DIDs are structured graphical representations of POMDPs, I-DIDs are the graphical online analogs for finitely nested I-POMDPs. I-DIDs may be used to optimize over a finite look-ahead given initial beliefs while interacting with other, possibly similar, agents. 4.1 Syntax We depict a general two time-slice I-DID in Fig. 3(a). In addition to the model nodes and the dashed policy link, what differentiates an I-DID from a DID is the model update link shown as a dotted arrow in Fig. 3(a). We explained the semantics of the model node and the policy link in the previous section; we describe the model updates next. The update of the model node over time involves two steps: First, given the models at time t, we identify the updated set of models that reside in the model node at time t + 1. Recall from Section 2 that an agents intentional model includes its belief. Because the agents act and receive observations, their models are updated to reflect their changed beliefs. Since the set of optimal actions for a model could include all the actions, and the agent may receive any one of |Ωj| possible observations, the updated set at time step t + 1 will have at most |Mt j,l−1||Aj||Ωj| models. Here, |Mt j,l−1| is the number of models at time step t, |Aj| and |Ωj| are the largest spaces of actions and observations respectively, among all the models. Second, we compute the new distribution over the updated models given the original distribution and the probability of the agent performing the action and receiving the observation that led to the updated model. These steps are a part of agent is belief update formalized using Eq. 1. In Fig. 3(b), we show how the dotted model update link is implemented in the I-DID. If each of the two level l − 1 models ascribed to j at time step t results in one action, and j could make one of two possible observations, then the model node at time step t + 1 contains four updated models (mt+1,1 j,l−1 ,mt+1,2 j,l−1 , mt+1,3 j,l−1 , and mt+1,4 j,l−1 ). These models differ in their initial beliefs, each of which is the result of j updating its beliefs due to its action and a possible observation. The decision nodes in each of the I-DIDs or DIDs that represent the lower level models are mapped to the corresponding Figure 4: Transformed I-DID with the model nodes and model update link replaced with the chance nodes and the relationships (in bold). chance nodes, as mentioned previously. Next, we describe how the distribution over the updated set of models (the distribution over the chance node Mod[Mt+1 j ] in Mt+1 j,l−1) is computed. The probability that js updated model is, say mt+1,1 j,l−1 , depends on the probability of j performing the action and receiving the observation that led to this model, and the prior distribution over the models at time step t. Because the chance node At j assumes the distribution of each of the action nodes based on the value of Mod[Mt j ], the probability of the action is given by this chance node. In order to obtain the probability of js possible observation, we introduce the chance node Oj, which depending on the value of Mod[Mt j ] assumes the distribution of the observation node in the lower level model denoted by Mod[Mt j ]. Because the probability of js observations depends on the physical state and the joint actions of both agents, the node Oj is linked with St+1 , At j, and At i. 2 Analogous to At j, the conditional probability table of Oj is also a multiplexer modulated by Mod[Mt j ]. Finally, the distribution over the prior models at time t is obtained from the chance node, Mod[Mt j ] in Mt j,l−1. Consequently, the chance nodes, Mod[Mt j ], At j, and Oj, form the parents of Mod[Mt+1 j ] in Mt+1 j,l−1. Notice that the model update link may be replaced by the dependency links between the chance nodes that constitute the model nodes in the two time slices. In Fig. 4 we show the two time-slice I-DID with the model nodes replaced by the chance nodes and the relationships between them. Chance nodes and dependency links that not in bold are standard, usually found in DIDs. Expansion of the I-DID over more time steps requires the repetition of the two steps of updating the set of models that form the 2 Note that Oj represents js observation at time t + 1. The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 817 values of the model node and adding the relationships between the chance nodes, as many times as there are model update links. We note that the possible set of models of the other agent j grows exponentially with the number of time steps. For example, after T steps, there may be at most |Mt=1 j,l−1|(|Aj||Ωj|)T −1 candidate models residing in the model node. 4.2 Solution Analogous to I-IDs, the solution to a level l I-DID for agent i expanded over T time steps may be carried out recursively. For the purpose of illustration, let l=1 and T=2. The solution method uses the standard look-ahead technique, projecting the agents action and observation sequences forward from the current belief state [17], and finding the possible beliefs that i could have in the next time step. Because agent i has a belief over js models as well, the lookahead includes finding out the possible models that j could have in the future. Consequently, each of js subintentional or level 0 models (represented using a standard DID) in the first time step must be solved to obtain its optimal set of actions. These actions are combined with the set of possible observations that j could make in that model, resulting in an updated set of candidate models (that include the updated beliefs) that could describe the behavior of j. Beliefs over this updated set of candidate models are calculated using the standard inference methods using the dependency relationships between the model nodes as shown in Fig. 3(b). We note the recursive nature of this solution: in solving agent is level 1 I-DID, js level 0 DIDs must be solved. If the nesting of models is deeper, all models at all levels starting from 0 are solved in a bottom-up manner. We briefly outline the recursive algorithm for solving agent is Algorithm for solving I-DID Input : level l ≥ 1 I-ID or level 0 ID, T Expansion Phase 1. For t from 1 to T − 1 do 2. If l ≥ 1 then Populate Mt+1 j,l−1 3. For each mt j in Range(Mt j,l−1) do 4. Recursively call algorithm with the l − 1 I-ID (or ID) that represents mt j and the horizon, T − t + 1 5. Map the decision node of the solved I-ID (or ID), OPT(mt j), to a chance node Aj 6. For each aj in OPT(mt j) do 7. For each oj in Oj (part of mt j) do 8. Update js belief, bt+1 j ← SE(bt j, aj, oj) 9. mt+1 j ← New I-ID (or ID) with bt+1 j as the initial belief 10. Range(Mt+1 j,l−1) ∪ ← {mt+1 j } 11. Add the model node, Mt+1 j,l−1, and the dependency links between Mt j,l−1 and Mt+1 j,l−1 (shown in Fig. 3(b)) 12. Add the chance, decision, and utility nodes for t + 1 time slice and the dependency links between them 13. Establish the CPTs for each chance node and utility node Look-Ahead Phase 14. Apply the standard look-ahead and backup method to solve the expanded I-DID Figure 5: Algorithm for solving a level l ≥ 0 I-DID. level l I-DID expanded over T time steps with one other agent j in Fig. 5. We adopt a two-phase approach: Given an I-ID of level l (described previously in Section 3) with all lower level models also represented as I-IDs or IDs (if level 0), the first step is to expand the level l I-ID over T time steps adding the dependency links and the conditional probability tables for each node. We particularly focus on establishing and populating the model nodes (lines 3-11). Note that Range(·) returns the values (lower level models) of the random variable given as input (model node). In the second phase, we use a standard look-ahead technique projecting the action and observation sequences over T time steps in the future, and backing up the utility values of the reachable beliefs. Similar to I-IDs, the I-DIDs reduce to DIDs in the absence of other agents. As we mentioned previously, the 0-th level models are the traditional DIDs. Their solutions provide probability distributions over actions of the agent modeled at that level to I-DIDs at level 1. Given probability distributions over other agents actions the level 1 IDIDs can themselves be solved as DIDs, and provide probability distributions to yet higher level models. Assume that the number of models considered at each level is bound by a number, M. Solving an I-DID of level l in then equivalent to solving O(Ml ) DIDs. 5. EXAMPLE APPLICATIONS To illustrate the usefulness of I-DIDs, we apply them to three problem domains. We describe, in particular, the formulation of the I-DID and the optimal prescriptions obtained on solving it. 5.1 Followership-Leadership in the Multiagent Tiger Problem We begin our illustrations of using I-IDs and I-DIDs with a slightly modified version of the multiagent tiger problem discussed in [9]. The problem has two agents, each of which can open the right door (OR), the left door (OL) or listen (L). In addition to hearing growls (from the left (GL) or from the right (GR)) when they listen, the agents also hear creaks (from the left (CL), from the right (CR), or no creaks (S)), which noisily indicate the other agents opening one of the doors. When any door is opened, the tiger persists in its original location with a probability of 95%. Agent i hears growls with a reliability of 65% and creaks with a reliability of 95%. Agent j, on the other hand, hears growls with a reliability of 95%. Thus, the setting is such that agent i hears agent j opening doors more reliably than the tigers growls. This suggests that i could use js actions as an indication of the location of the tiger, as we discuss below. Each agents preferences are as in the single agent game discussed in [13]. The transition, observation, and reward functions are shown in [16]. A good indicator of the usefulness of normative methods for decision-making like I-DIDs is the emergence of realistic social behaviors in their prescriptions. In settings of the persistent multiagent tiger problem that reflect real world situations, we demonstrate followership between the agents and, as shown in [15], deception among agents who believe that they are in a follower-leader type of relationship. In particular, we analyze the situational and epistemological conditions sufficient for their emergence. The followership behavior, for example, results from the agent knowing its own weaknesses, assessing the strengths, preferences, and possible behaviors of the other, and realizing that its best for it to follow the others actions in order to maximize its payoffs. Let us consider a particular setting of the tiger problem in which agent i believes that js preferences are aligned with its own - both of them just want to get the gold - and js hearing is more reliable in comparison to itself. As an example, suppose that j, on listening can discern the tigers location 95% of the times compared to is 65% accuracy. Additionally, agent i does not have any initial information about the tigers location. In other words, is single-level nested belief, bi,1, assigns 0.5 to each of the two locations of the tiger. In addition, i considers two models of j, which differ in js flat level 0 initial beliefs. This is represented in the level 1 I-ID shown in Fig. 6(a). According to one model, j assigns a probability of 0.9 that the tiger is behind the left door, while the other 818 The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Figure 6: (a) Level 1 I-ID of agent i, (b) two level 0 IDs of agent j whose decision nodes are mapped to the chance nodes, A1 j , A2 j , in (a). model assigns 0.1 to that location (see Fig. 6(b)). Agent i is undecided on these two models of j. If we vary is hearing ability, and solve the corresponding level 1 I-ID expanded over three time steps, we obtain the normative behavioral policies shown in Fig 7 that exhibit followership behavior. If is probability of correctly hearing the growls is 0.65, then as shown in the policy in Fig. 7(a), i begins to conditionally follow js actions: i opens the same door that j opened previously iff is own assessment of the tigers location confirms js pick. If i loses the ability to correctly interpret the growls completely, it blindly follows j and opens the same door that j opened previously (Fig. 7(b)). Figure 7: Emergence of (a) conditional followership, and (b) blind followership in the tiger problem. Behaviors of interest are in bold. * is a wildcard, and denotes any one of the observations. We observed that a single level of belief nesting - beliefs about the others models - was sufficient for followership to emerge in the tiger problem. However, the epistemological requirements for the emergence of leadership are more complex. For an agent, say j, to emerge as a leader, followership must first emerge in the other agent i. As we mentioned previously, if i is certain that its preferences are identical to those of j, and believes that j has a better sense of hearing, i will follow js actions over time. Agent j emerges as a leader if it believes that i will follow it, which implies that js belief must be nested two levels deep to enable it to recognize its leadership role. Realizing that i will follow presents j with an opportunity to influence is actions in the benefit of the collective good or its self-interest alone. For example, in the tiger problem, let us consider a setting in which if both i and j open the correct door, then each gets a payoff of 20 that is double the original. If j alone selects the correct door, it gets the payoff of 10. On the other hand, if both agents pick the wrong door, their penalties are cut in half. In this setting, it is in both js best interest as well as the collective betterment for j to use its expertise in selecting the correct door, and thus be a good leader. However, consider a slightly different problem in which j gains from is loss and is penalized if i gains. Specifically, let is payoff be subtracted from js, indicating that j is antagonistic toward i - if j picks the correct door and i the wrong one, then is loss of 100 becomes js gain. Agent j believes that i incorrectly thinks that js preferences are those that promote the collective good and that it starts off by believing with 99% confidence where the tiger is. Because i believes that its preferences are similar to those of j, and that j starts by believing almost surely that one of the two is the correct location (two level 0 models of j), i will start by following js actions. We show is normative policy on solving its singly-nested I-DID over three time steps in Fig. 8(a). The policy demonstrates that i will blindly follow js actions. Since the tiger persists in its original location with a probability of 0.95, i will select the same door again. If j begins the game with a 99% probability that the tiger is on the right, solving js I-DID nested two levels deep, results in the policy shown in Fig. 8(b). Even though j is almost certain that OL is the correct action, it will start by selecting OR, followed by OL. Agent js intention is to deceive i who, it believes, will follow js actions, so as to gain $110 in the second time step, which is more than what j would gain if it were to be honest. Figure 8: Emergence of deception between agents in the tiger problem. Behaviors of interest are in bold. * denotes as before. (a) Agent is policy demonstrating that it will blindly follow js actions. (b) Even though j is almost certain that the tiger is on the right, it will start by selecting OR, followed by OL, in order to deceive i. 5.2 Altruism and Reciprocity in the Public Good Problem The public good (PG) problem [7], consists of a group of M agents, each of whom must either contribute some resource to a public pot or keep it for themselves. Since resources contributed to the public pot are shared among all the agents, they are less valuable to the agent when in the public pot. However, if all agents choose to contribute their resources, then the payoff to each agent is more than if no one contributes. Since an agent gets its share of the public pot irrespective of whether it has contributed or not, the dominating action is for each agent to not contribute, and instead free ride on others contributions. However, behaviors of human players in empirical simulations of the PG problem differ from the normative predictions. The experiments reveal that many players initially contribute a large amount to the public pot, and continue to contribute when the PG problem is played repeatedly, though in decreasing amounts [4]. Many of these experiments [5] report that a small core group of players persistently contributes to the public pot even when all others are defecting. These experiments also reveal that players who persistently contribute have altruistic or reciprocal preferences matching expected cooperation of others. For simplicity, we assume that the game is played between M = 2 agents, i and j. Let each agent be initially endowed with XT amount of resources. While the classical PG game formulation permits each agent to contribute any quantity of resources (≤ XT ) to the public pot, we simplify the action space by allowing two possible actions. Each agent may choose to either contribute (C) a fixed amount of the resources, or not contribute. The latter action is deThe Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 819 noted as defect (D). We assume that the actions are not observable to others. The value of resources in the public pot is discounted by ci for each agent i, where ci is the marginal private return. We assume that ci < 1 so that the agent does not benefit enough that it contributes to the public pot for private gain. Simultaneously, ciM > 1, making collective contribution pareto optimal. i/j C D C 2ciXT , 2cjXT ciXT − cp, XT + cjXT − P D XT + ciXT − P, cjXT − cp XT , XT Table 1: The one-shot PG game with punishment. In order to encourage contributions, the contributing agents punish free riders but incur a small cost for administering the punishment. Let P be the punishment meted out to the defecting agent and cp the non-zero cost of punishing for the contributing agent. For simplicity, we assume that the cost of punishing is same for both the agents. The one-shot PG game with punishment is shown in Table. 1. Let ci = cj, cp > 0, and if P > XT − ciXT , then defection is no longer a dominating action. If P < XT − ciXT , then defection is the dominating action for both. If P = XT − ciXT , then the game is not dominance-solvable. Figure 9: (a) Level 1 I-ID of agent i, (b) level 0 IDs of agent j with decision nodes mapped to the chance nodes, A1 j and A2 j , in (a). We formulate a sequential version of the PG problem with punishment from the perspective of agent i. Though in the repeated PG game, the quantity in the public pot is revealed to all the agents after each round of actions, we assume in our formulation that it is hidden from the agents. Each agent may contribute a fixed amount, xc, or defect. An agent on performing an action receives an observation of plenty (PY) or meager (MR) symbolizing the state of the public pot. Notice that the observations are also indirectly indicative of agent js actions because the state of the public pot is influenced by them. The amount of resources in agent is private pot, is perfectly observable to i. The payoffs are analogous to Table. 1. Borrowing from the empirical investigations of the PG problem [5], we construct level 0 IDs for j that model altruistic and non-altruistic types (Fig. 9(b)). Specifically, our altruistic agent has a high marginal private return (cj is close to 1) and does not punish others who defect. Let xc = 1 and the level 0 agent be punished half the times it defects. With one action remaining, both types of agents choose to contribute to avoid being punished. With two actions to go, the altruistic type chooses to contribute, while the other defects. This is because cj for the altruistic type is close to 1, thus the expected punishment, 0.5P > (1 − cj), which the altruistic type avoids. Because cj for the non-altruistic type is less, it prefers not to contribute. With three steps to go, the altruistic agent contributes to avoid punishment (0.5P > 2(1 − cj)), and the non-altruistic type defects. For greater than three steps, while the altruistic agent continues to contribute to the public pot depending on how close its marginal private return is to 1, the non-altruistic type prescribes defection. We analyzed the decisions of an altruistic agent i modeled using a level 1 I-DID expanded over 3 time steps. i ascribes the two level 0 models, mentioned previously, to j (see Fig. 9). If i believes with a probability 1 that j is altruistic, i chooses to contribute for each of the three steps. This behavior persists when i is unaware of whether j is altruistic (Fig. 10(a)), and when i assigns a high probability to j being the non-altruistic type. However, when i believes with a probability 1 that j is non-altruistic and will thus surely defect, i chooses to defect to avoid being punished and because its marginal private return is less than 1. These results demonstrate that the behavior of our altruistic type resembles that found experimentally. The non-altruistic level 1 agent chooses to defect regardless of how likely it believes the other agent to be altruistic. We analyzed the behavior of a reciprocal agent type that matches expected cooperation or defection. The reciprocal types marginal private return is similar to that of the non-altruistic type, however, it obtains a greater payoff when its action is similar to that of the other. We consider the case when the reciprocal agent i is unsure of whether j is altruistic and believes that the public pot is likely to be half full. For this prior belief, i chooses to defect. On receiving an observation of plenty, i decides to contribute, while an observation of meager makes it defect (Fig. 10(b)). This is because an observation of plenty signals that the pot is likely to be greater than half full, which results from js action to contribute. Thus, among the two models ascribed to j, its type is likely to be altruistic making it likely that j will contribute again in the next time step. Agent i therefore chooses to contribute to reciprocate js action. An analogous reasoning leads i to defect when it observes a meager pot. With one action to go, i believing that j contributes, will choose to contribute too to avoid punishment regardless of its observations. Figure 10: (a) An altruistic level 1 agent always contributes. (b) A reciprocal agent i starts off by defecting followed by choosing to contribute or defect based on its observation of plenty (indicating that j is likely altruistic) or meager (j is non-altruistic). 5.3 Strategies in Two-Player Poker Poker is a popular zero sum card game that has received much attention among the AI research community as a testbed [2]. Poker is played among M ≥ 2 players in which each player receives a hand of cards from a deck. While several flavors of Poker with varying complexity exist, we consider a simple version in which each player has three plys during which the player may either exchange a card (E), keep the existing hand (K), fold (F) and withdraw from the game, or call (C), requiring all players to show their hands. To keep matters simple, let M = 2, and each player receive a hand consisting of a single card drawn from the same suit. Thus, during a showdown, the player who has the numerically larger card (2 is the lowest, ace is the highest) wins the pot. During an exchange of cards, the discarded card is placed either in the L pile, indicating to the other agent that it was a low numbered card less than 8, or in the 820 The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) H pile, indicating that the card had a rank greater than or equal to 8. Notice that, for example, if a lower numbered card is discarded, the probability of receiving a low card in exchange is now reduced. We show the level 1 I-ID for the simplified two-player Poker in Fig. 11. We considered two models (personality types) of agent j. The conservative type believes that it is likely that its opponent has a high numbered card in its hand. On the other hand, the aggressive agent j believes with a high probability that its opponent has a lower numbered card. Thus, the two types differ in their beliefs over their opponents hand. In both these level 0 models, the opponent is assumed to perform its actions following a fixed, uniform distribution. With three actions to go, regardless of its hand (unless it is an ace), the aggressive agent chooses to exchange its card, with the intent of improving on its current hand. This is because it believes the other to have a low card, which improves its chances of getting a high card during the exchange. The conservative agent chooses to keep its card, no matter its hand because its chances of getting a high card are slim as it believes that its opponent has one. Figure 11: (a) Level 1 I-ID of agent i. The observation reveals information about js hand of the previous time step, (b) level 0 IDs of agent j whose decision nodes are mapped to the chance nodes, A1 j , A2 j , in (a). The policy of a level 1 agent i who believes that each card except its own has an equal likelihood of being in js hand (neutral personality type) and j could be either an aggressive or conservative type, is shown in Fig. 12. is own hand contains the card numbered 8. The agent starts by keeping its card. On seeing that j did not exchange a card (N), i believes with probability 1 that j is conservative and hence will keep its cards. i responds by either keeping its card or exchanging it because j is equally likely to have a lower or higher card. If i observes that j discarded its card into the L or H pile, i believes that j is aggressive. On observing L, i realizes that j had a low card, and is likely to have a high card after its exchange. Because the probability of receiving a low card is high now, i chooses to keep its card. On observing H, believing that the probability of receiving a high numbered card is high, i chooses to exchange its card. In the final step, i chooses to call regardless of its observation history because its belief that j has a higher card is not sufficiently high to conclude that its better to fold and relinquish the payoff. This is partly due to the fact that an observation of, say, L resets the agent is previous time step beliefs over js hand to the low numbered cards only. 6. DISCUSSION We showed how DIDs may be extended to I-DIDs that enable online sequential decision-making in uncertain multiagent settings. Our graphical representation of I-DIDs improves on the previous Figure 12: A level 1 agent is three step policy in the Poker problem. i starts by believing that j is equally likely to be aggressive or conservative and could have any card in its hand with equal probability. work significantly by being more transparent, semantically clear, and capable of being solved using standard algorithms that target DIDs. I-DIDs extend NIDs to allow sequential decision-making over multiple time steps in the presence of other interacting agents. I-DIDs may be seen as concise graphical representations for IPOMDPs providing a way to exploit problem structure and carry out online decision-making as the agent acts and observes given its prior beliefs. We are currently investigating ways to solve I-DIDs approximately with provable bounds on the solution quality. Acknowledgment: We thank Piotr Gmytrasiewicz for some useful discussions related to this work. The first author would like to acknowledge the support of a UGARF grant. 7. REFERENCES [1] R. J. Aumann. Interactive epistemology i: Knowledge. International Journal of Game Theory, 28:263-300, 1999. [2] D. Billings, A. Davidson, J. Schaeffer, and D. Szafron. The challenge of poker. AIJ, 2001. [3] A. Brandenburger and E. Dekel. Hierarchies of beliefs and common knowledge. Journal of Economic Theory, 59:189-198, 1993. [4] C. Camerer. Behavioral Game Theory: Experiments in Strategic Interaction. Princeton University Press, 2003. [5] E. Fehr and S. Gachter. Cooperation and punishment in public goods experiments. American Economic Review, 90(4):980-994, 2000. [6] D. Fudenberg and D. K. Levine. The Theory of Learning in Games. MIT Press, 1998. [7] D. Fudenberg and J. Tirole. Game Theory. MIT Press, 1991. [8] Y. Gal and A. Pfeffer. A language for modeling agents decision-making processes in games. In AAMAS, 2003. [9] P. Gmytrasiewicz and P. Doshi. A framework for sequential planning in multiagent settings. JAIR, 24:49-79, 2005. [10] P. Gmytrasiewicz and E. Durfee. Rational coordination in multi-agent environments. JAAMAS, 3(4):319-350, 2000. [11] J. C. Harsanyi. Games with incomplete information played by bayesian players. Management Science, 14(3):159-182, 1967. [12] R. A. Howard and J. E. Matheson. Influence diagrams. In R. A. Howard and J. E. Matheson, editors, The Principles and Applications of Decision Analysis. Strategic Decisions Group, Menlo Park, CA 94025, 1984. [13] L. Kaelbling, M. Littman, and A. Cassandra. Planning and acting in partially observable stochastic domains. Artificial Intelligence Journal, 2, 1998. [14] D. Koller and B. Milch. Multi-agent influence diagrams for representing and solving games. In IJCAI, pages 1027-1034, 2001. [15] K. Polich and P. Gmytrasiewicz. Interactive dynamic influence diagrams. In GTDT Workshop, AAMAS, 2006. [16] B. Rathnas., P. Doshi, and P. J. Gmytrasiewicz. Exact solutions to interactive pomdps using behavioral equivalence. In Autonomous Agents and Multi-Agent Systems Conference (AAMAS), 2006. [17] S. Russell and P. Norvig. Artificial Intelligence: A Modern Approach (Second Edition). Prentice Hall, 2003. [18] R. D. Shachter. Evaluating influence diagrams. Operations Research, 34(6):871-882, 1986. [19] D. Suryadi and P. Gmytrasiewicz. Learning models of other agents using influence diagrams. In UM, 1999. The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 821",
    "original_translation": "Modelos gráficos para soluciones en línea a POMDP interactivos de Prashant Doshi Dept. Departamento de Ciencias de la Computación Universidad de Georgia Athens, GA 30602, EE. UU. pdoshi@cs.uga.edu Yifeng Zeng Dept. de Ciencias de la Computación Universidad de Aalborg DK-9220 Aalborg, Dinamarca yfzeng@cs.aau.edu Qiongyu Chen Dept. de Ciencias de la Computación Universidad Nacional de Singapur 117543, Singapur chenqy@comp.nus.edu.sg RESUMEN Desarrollamos una nueva representación gráfica para procesos de decisión de Markov parcialmente observables interactivos (I-POMDPs) que es significativamente más transparente y semánticamente clara que la representación anterior. Estos modelos gráficos llamados diagramas de influencia dinámica interactiva (I-DIDs) buscan modelar explícitamente la estructura que a menudo está presente en problemas del mundo real al descomponer la situación en variables de probabilidad y decisión, y las dependencias entre las variables. Los I-DIDs generalizan los DIDs, los cuales pueden ser vistos como representaciones gráficas de POMDPs, a entornos multiagentes de la misma manera que los I-POMDPs generalizan los POMDPs. Los I-DIDs pueden ser utilizados para calcular la política de un agente en línea mientras el agente actúa y observa en un entorno poblado por otros agentes que interactúan entre sí. Usando varios ejemplos, mostramos cómo se pueden aplicar los I-DIDs y demostramos su utilidad. Categorías y Descriptores de Asignaturas I.2.11 [Inteligencia Artificial Distribuida]: Sistemas Multiagentes Términos Generales Teoría 1. Las Procesos de Decisión de Markov Interactivos Parcialmente Observables (IPOMDPs) proporcionan un marco para la toma de decisiones secuenciales en entornos multiagentes parcialmente observables. Generalizan los POMDPs [13] a entornos multiagentes al incluir los modelos computables de los otros agentes en el espacio de estados junto con los estados del entorno físico. Los modelos abarcan toda la información que influye en los comportamientos de los agentes, incluyendo sus preferencias, capacidades y creencias, y por lo tanto son análogos a los tipos en los juegos bayesianos [11]. I-POMDPs adoptan un enfoque subjetivo para comprender el comportamiento estratégico, arraigado en un marco de teoría de decisiones que toma la perspectiva de los tomadores de decisiones en la interacción. En [15], Polich y Gmytrasiewicz introdujeron diagramas de influencia dinámica interactivos (I-DIDs) como las representaciones computacionales de I-POMDPs. Los I-DIDs generalizan los DIDs [12], los cuales pueden ser vistos como contrapartes computacionales de POMDPs, a entornos de múltiples agentes de la misma manera que los I-POMDPs generalizan los POMDPs. Los I-DIDs contribuyen a una creciente línea de trabajo [19] que incluye diagramas de influencia multiagente (MAIDs) [14], y más recientemente, redes de diagramas de influencia (NIDs) [8]. Estos formalismos buscan modelar explícitamente la estructura que a menudo está presente en problemas del mundo real al descomponer la situación en variables de azar y decisiones, y las dependencias entre las variables. Los MAIDs proporcionan una alternativa a las formas normales y extensivas de juego utilizando un formalismo gráfico para representar juegos de información imperfecta con un nodo de decisión para las acciones de cada agente y nodos de azar que capturan la información privada de los agentes. Los MAIDs analizan objetivamente el juego, calculando eficientemente el perfil de equilibrio de Nash al explotar la estructura de independencia. NIDs extienden los MAIDs para incluir la incertidumbre de los agentes sobre el juego que se está jugando y sobre los modelos de los otros agentes. Cada modelo es un MAID y la red de MAIDs se colapsa, de abajo hacia arriba, en un solo MAID para calcular el equilibrio del juego teniendo en cuenta los diferentes modelos de cada agente. Los formalismos gráficos como MAIDs y NIDs abren una área de investigación prometedora que tiene como objetivo representar las interacciones multiagente de manera más transparente. Sin embargo, los MAIDs proporcionan un análisis del juego desde un punto de vista externo y la aplicabilidad de ambos está limitada a juegos estáticos de un solo jugador. Los asuntos son más complejos cuando consideramos interacciones que se extienden en el tiempo, donde las predicciones sobre las acciones futuras de otros deben hacerse utilizando modelos que cambian a medida que los agentes actúan y observan. Los I-DIDs abordan esta brecha al permitir la representación de modelos de otros agentes como los valores de un nodo de modelo especial. Tanto los modelos de otros agentes como las creencias originales de los agentes sobre estos modelos se actualizan con el tiempo utilizando implementaciones especializadas. En este documento, mejoramos la representación preliminar previa del I-DID mostrada en [15] utilizando la idea de que el I-ID estático es un tipo de NID. Por lo tanto, podemos utilizar construcciones de lenguaje específicas de NID, como multiplexores, para representar de manera más transparente el nodo del modelo y, posteriormente, el I-ID. Además, aclaramos la semántica del enlace de política de propósito especial introducido en la representación de I-DID por [15], y mostramos que podría ser reemplazado por enlaces de dependencia tradicionales. En la representación previa del I-DID, la actualización de la creencia de los agentes sobre los modelos de los demás a medida que los agentes actúan y reciben observaciones se denotaba utilizando un enlace especial llamado el enlace de actualización del modelo que conectaba los nodos del modelo a lo largo del tiempo. Explicamos la semántica de este enlace mostrando cómo puede ser implementado utilizando los enlaces de dependencia tradicionales entre los nodos de probabilidad que constituyen los nodos del modelo. El resultado final es una representación de I-DID que es significativamente más transparente, semánticamente clara y capaz de ser implementada utilizando los algoritmos estándar para resolver DIDs. Mostramos cómo los IDIDs pueden ser utilizados para modelar la incertidumbre de un agente sobre los modelos de otros, que a su vez pueden ser I-DIDs. La solución al I-DID es una política que prescribe lo que el agente debe hacer con el tiempo, dadas sus creencias sobre el estado físico y otros modelos. Análogos a los DIDs, los I-DIDs pueden ser utilizados para calcular la política de un agente en línea mientras el agente actúa y observa en un entorno poblado por otros agentes interactuantes. 2. ANTECEDENTES: Los IPOMDPS ANIDADOS FINITAMENTE generalizan los POMDPs a entornos multiagentes al incluir los modelos de otros agentes como parte del espacio de estados [9]. Dado que otros agentes también pueden razonar sobre otros, el espacio de estado interactivo está estratégicamente anidado; contiene creencias sobre los modelos de otros agentes y sus creencias sobre los demás. Para simplificar la presentación, consideramos un agente, i, que está interactuando con otro agente, j. Un I-POMDP finitamente anidado del agente i con un nivel de estrategia l se define como la tupla: I-POMDPi,l = ISi,l, A, Ti, Ωi, Oi, Ri donde: • ISi,l denota un conjunto de estados interactivos definido como, ISi,l = S × Mj,l−1, donde Mj,l−1 = {Θj,l−1 ∪ SMj}, para l ≥ 1, e ISi,0 = S, donde S es el conjunto de estados del entorno físico. Θj,l−1 es el conjunto de modelos intencionales computables del agente j: θj,l−1 = bj,l−1, ˆθj donde el marco, ˆθj = A, Ωj, Tj, Oj, Rj, OCj. Aquí, j es Bayesiano racional y OCj es el criterio de optimalidad de j. SMj es el conjunto de modelos subintencionales de j. Ejemplos simples de modelos subintencionales incluyen un modelo sin información [10] y un modelo de juego ficticio [6], ambos de los cuales son independientes de la historia. Damos una construcción recursiva de abajo hacia arriba del espacio de estados interactivo a continuación. Sí,0 = S, Θj,0 = { bj,0, ˆθj | bj,0 ∈ Δ(ISj,0)} Sí,1 = S × {Θj,0 ∪ SMj}, Θj,1 = { bj,1, ˆθj | bj,1 ∈ Δ(ISj,1)} . . . . . . Sí, l = S × {Θj,l−1 ∪ SMj}, Θj,l = { bj,l, ˆθj | bj,l ∈ Δ(ISj,l)} Formulaciones similares de espacios anidados han aparecido en [1, 3]. • A = Ai × Aj es el conjunto de acciones conjuntas de todos los agentes en el entorno; • Ti : S ×A×S → [0, 1], describe el efecto de las acciones conjuntas en los estados físicos del entorno; • Ωi es el conjunto de observaciones del agente i; • Oi : S × A × Ωi → [0, 1] da la probabilidad de las observaciones dadas el estado físico y la acción conjunta; • Ri : ISi × A → R describe las preferencias del agente sobre sus estados interactivos. Normalmente solo importarán los estados físicos. La política del agente es el mapeo, Ω∗ i → Δ(Ai), donde Ω∗ i es el conjunto de todos los historiales de observación del agente i. Dado que la creencia sobre los estados interactivos forma una estadística suficiente [9], la política también puede ser representada como un mapeo del conjunto de todas las creencias del agente i a una distribución sobre sus acciones, Δ(ISi) → Δ(Ai). 2.1 Actualización de Creencias Análogo a los POMDPs, un agente dentro del marco I-POMDP actualiza su creencia a medida que actúa y observa. Sin embargo, hay dos diferencias que complican la actualización de creencias en entornos multiagentes en comparación con los entornos de un solo agente. Primero, dado que el estado del entorno físico depende de las acciones de ambos agentes, la predicción de cómo cambia el estado físico debe hacerse basándose en la predicción de sus acciones. Segundo, los cambios en los modelos de js deben ser incluidos en la actualización de creencias. Específicamente, si j es intencional, entonces se debe incluir una actualización de las creencias de j debido a su acción y observación. En otras palabras, i tiene que actualizar su creencia basándose en su predicción de lo que j observaría y cómo j actualizaría su creencia. Si el modelo de js es subintencional, entonces las observaciones probables de js se añaden al historial de observaciones contenido en el modelo. Formalmente, tenemos: Pr(ist |at−1 i , bt−1 i,l ) = β ISt−1:mt−1 j =θt j bt−1 i,l (ist−1 ) × at−1 j Pr(at−1 j |θt−1 j,l−1)Oi(st , at−1 i , at−1 j , ot i) ×Ti(st−1 , at−1 i , at−1 j , st ) ot j Oj(st , at−1 i , at−1 j , ot j) ×τ(SEθt j (bt−1 j,l−1, at−1 j , ot j) − bt j,l−1) (1) donde β es la constante de normalización, τ es 1 si su argumento es 0, de lo contrario es 0, Pr(at−1 j |θt−1 j,l−1) es la probabilidad de que at−1 j sea Bayes racional para el agente descrito por el modelo θt−1 j,l−1, y SE(·) es una abreviatura para la actualización de creencias. Para una versión de la actualización de creencias cuando el modelo js es subintencional, consulte [9]. Si el agente j también se modela como un I-POMDP, entonces la actualización de creencias invoca la actualización de creencias de j (a través del término SEθt j ( bt−1 j,l−1 , at−1 j , ot j)), lo que a su vez podría invocar la actualización de creencias de is y así sucesivamente. Esta recursión en la anidación de creencias llega al nivel 0. En este nivel, la actualización de creencias del agente se reduce a una actualización de creencias de un POMDP. Para ilustraciones de la actualización de creencias, detalles adicionales sobre I-POMDPs y cómo se comparan con otros marcos multiagentes, consulte [9]. Iteración de Valor Cada estado de creencia en un I-POMDP finitamente anidado tiene un valor asociado que refleja la máxima ganancia que el agente puede esperar en este estado de creencia: Un ( bi,l, θi ) = max ai∈Ai is∈ISi,l ERi(is, ai)bi,l(is)+ γ oi∈Ωi Pr(oi|ai, bi,l)Un−1 ( SEθi (bi,l, ai, oi), θi ) (2) donde, ERi(is, ai) = aj Ri(is, ai, aj)Pr(aj|mj,l−1) (ya que is = (s, mj,l−1)). La ecuación 2 es una base para la iteración de valor en I-POMDPs. La acción óptima del agente, a∗ i, para el caso de horizonte finito con descuento, es un elemento del conjunto de acciones óptimas para el estado de creencia, OPT(θi), definido como: OPT( bi,l, θi ) = argmax ai∈Ai is∈ISi,l ERi(is, ai)bi,l(is) +γ oi∈Ωi Pr(oi|ai, bi,l)Un ( SEθi (bi,l, ai, oi), θi ) (3) 3. Diagramas de influencia interactivos. Una extensión ingenua de los diagramas de influencia (IDs) a entornos poblados por múltiples agentes es posible tratando a los otros agentes como autómatas, representados mediante nodos de probabilidad. Sin embargo, este enfoque asume que las acciones de los agentes están controladas utilizando una distribución de probabilidad que no cambia con el tiempo. Los diagramas de influencia interactivos (I-IDs) adoptan un enfoque más sofisticado al generalizar los IDs para hacerlos aplicables a entornos compartidos con otros agentes que pueden actuar, observar y actualizar sus creencias. 3.1 Sintaxis Además de los nodos habituales de probabilidad, decisión y utilidad, los IIDs incluyen un nuevo tipo de nodo llamado nodo de modelo. Mostramos un nivel general l I-ID en la Fig. 1(a), donde el nodo del modelo (Mj,l−1) está representado con un hexágono. Observamos que la distribución de probabilidad sobre el nodo de probabilidad, S, y el nodo del modelo juntos representan la creencia del agente sobre sus estados interactivos. Además del modelo 1, el modelo de nivel 0 es un POMDP: las acciones de otros agentes se tratan como eventos exógenos y se incorporan en las funciones T, O y R. El Sexto Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) Figura 1: (a) Un nivel genérico l I-ID para el agente i situado con otro agente j. El hexágono es el nodo modelo (Mj,l−1) cuya estructura mostramos en (b). Los miembros del nodo modelo son ellos mismos I-ID (m1 j,l−1, m2 j,l−1; los diagramas no se muestran aquí por simplicidad) cuyos nodos de decisión se asignan a los nodos de probabilidad correspondientes (A1 j , A2 j). Dependiendo del valor del nodo, Mod[Mj], la distribución de cada uno de los nodos de probabilidad se asigna al nodo Aj. (c) El I-ID transformado con el nodo del modelo reemplazado por los nodos de probabilidad y las relaciones entre ellos. Los I-IDs difieren de los IDs al tener un enlace discontinuo (llamado enlace de política en [15]) entre el nodo del modelo y un nodo de probabilidad, Aj, que representa la distribución sobre las acciones de los otros agentes dada su modelo. En ausencia de otros agentes, el nodo del modelo y el nodo de probabilidad, Aj, desaparecen y los I-ID se convierten en ID tradicionales. El nodo del modelo contiene los modelos computacionales alternativos atribuidos por i al otro agente del conjunto, Θj,l−1 ∪ SMj, donde Θj,l−1 y SMj fueron definidos previamente en la Sección 2. Por lo tanto, un modelo en el nodo del modelo puede ser en sí mismo un I-ID o ID, y la recursión termina cuando un modelo es un ID o subintencional. Dado que el nodo del modelo contiene los modelos alternativos del otro agente como sus valores, su representación no es trivial. En particular, algunos de los modelos dentro del nodo son I-IDs que, al resolverse, generan la política óptima de los agentes en sus nodos de decisión. Cada nodo de decisión se asigna al nodo de probabilidad correspondiente, digamos A1 j, de la siguiente manera: si OPT es el conjunto de acciones óptimas obtenidas al resolver el I-ID (o ID), entonces Pr(aj ∈ A1 j) = 1 |OPT| si aj ∈ OPT, 0 en caso contrario. Tomando prestados los conocimientos de trabajos anteriores [8], observamos que el nodo del modelo y el enlace de política discontinua que lo conecta con el nodo de probabilidad, Aj, podrían representarse como se muestra en la Fig. 1(b). El nodo de decisión de cada nivel l − 1 I-ID se transforma en un nodo de probabilidad, como mencionamos anteriormente, de modo que las acciones con el mayor valor en el nodo de decisión se les asignan probabilidades uniformes en el nodo de probabilidad, mientras que al resto se les asigna probabilidad cero. Los diferentes nodos de probabilidad (A1 j , A2 j ), uno para cada modelo, y adicionalmente, el nodo de probabilidad etiquetado Mod[Mj] forman los padres del nodo de probabilidad, Aj. Por lo tanto, hay tantos nodos de acción (A1 j , A2 j ) en Mj,l−1 como el número de modelos en el soporte de las creencias del agente. La tabla de probabilidad condicional del nodo de probabilidad, Aj, es un multiplexor que asume la distribución de cada uno de los nodos de acción (A1 j , A2 j ) dependiendo del valor de Mod[Mj]. Los valores de Mod[Mj] denotan los diferentes modelos de j. En otras palabras, cuando Mod[Mj] tiene el valor m1 j,l−1, el nodo de probabilidad Aj asume la distribución del nodo A1 j, y Aj asume la distribución de A2 j cuando Mod[Mj] tiene el valor m2 j,l−1. La distribución sobre el nodo, Mod[Mj], es la creencia del agente sobre los modelos de j dado un estado físico. Para más agentes, tendremos tantos nodos de modelo como agentes haya. Observa que la Fig. 1(b) aclara la semántica del enlace de política y muestra cómo puede ser representado utilizando los enlaces de dependencia tradicionales. En la Fig. 1(c), mostramos el I-ID transformado cuando el nodo del modelo es reemplazado por los nodos de probabilidad y las relaciones entre ellos. A diferencia de la representación en [15], no hay enlaces de políticas especiales, sino que el I-ID está compuesto únicamente por aquellos tipos de nodos que se encuentran en IDs tradicionales y relaciones de dependencia entre los nodos. Esto permite que los I-IDs sean representados e implementados utilizando herramientas de aplicación convencionales que apuntan a los IDs. Ten en cuenta que podemos ver el nivel l I-ID como un NID. Específicamente, cada uno de los modelos del nivel l − 1 dentro del nodo del modelo son bloques en el NID (ver Fig. 2). Si el nivel l = 1, cada bloque es un ID tradicional, de lo contrario, si l > 1, cada bloque dentro del NID puede ser un NID en sí mismo. Ten en cuenta que dentro de los I-IDs (o IDs) en cada nivel, solo hay un nodo de decisión único. Por lo tanto, nuestro NID no contiene ningún MAID. Figura 2: Un nivel l I-ID representado como un NID. Las probabilidades asignadas a los bloques del NID son creencias sobre modelos js condicionados a un estado físico. 3.2 Solución La solución de un I-ID procede de manera ascendente y se implementa de forma recursiva. Comenzamos resolviendo los modelos de nivel 0, que, si son intencionales, son identificadores tradicionales. Sus soluciones proporcionan distribuciones de probabilidad sobre las acciones de los otros agentes, las cuales se ingresan en los nodos de probabilidad correspondientes encontrados en el nodo del modelo del nivel 1 I-ID. El mapeo de los nodos de decisión de los modelos de nivel 0 a los nodos de probabilidad se realiza de manera que las acciones con el mayor valor en el nodo de decisión se les asignan probabilidades uniformes en el nodo de probabilidad, mientras que el resto se les asigna probabilidad cero. Dadas las distribuciones sobre las acciones dentro de los diferentes nodos de probabilidad (uno para cada modelo del otro agente), el I-ID de nivel 1 se transforma como se muestra en la Fig. 1(c). Durante la transformación, la tabla de probabilidad condicional (CPT) del nodo, Aj, se llena de tal manera que el nodo asume la distribución de cada uno de los nodos de probabilidad dependiendo del valor del nodo, Mod[Mj]. Como mencionamos anteriormente, los valores del nodo Mod[Mj] denotan los diferentes modelos del otro agente, y su distribución es la creencia del agente sobre los modelos de j condicionados al estado físico. El nivel 1 I-ID transformado es un ID tradicional que puede ser resuelto como us816 The Sixth Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) (a) (b) Figura 3: (a) Un I-DID genérico de dos niveles de tiempo para el agente i en un entorno con otro agente j. Observa el enlace de actualización del modelo punteado que denota la actualización de los modelos de j y la distribución de los modelos a lo largo del tiempo. (b) La semántica del enlace de actualización del modelo utilizando el método estándar de maximización de utilidad esperada [18]. Este procedimiento se lleva a cabo hasta el nivel l I-ID cuya solución proporciona el conjunto no vacío de acciones óptimas que el agente debe realizar dada su creencia. Ten en cuenta que, al igual que los IDs, los I-IDs son adecuados para la toma de decisiones en línea cuando se conoce la creencia actual de los agentes. 4. Diagramas de influencia dinámica interactivos (I-DIDs) Los diagramas de influencia dinámica interactivos (I-DIDs) extienden los I-IDs (y NIDs) para permitir la toma de decisiones secuencial a lo largo de varios pasos de tiempo. Así como los DIDs son representaciones gráficas estructuradas de POMDPs, los I-DIDs son los análogos gráficos en línea para I-POMDPs finitamente anidados. Los I-DIDs pueden ser utilizados para optimizar sobre una mirada anticipada finita dadas las creencias iniciales mientras se interactúa con otros agentes, posiblemente similares. 4.1 Sintaxis Representamos un I-DID de dos cortes temporales en la Fig. 3(a). Además de los nodos del modelo y el enlace de política discontinuo, lo que diferencia a un I-DID de un DID es el enlace de actualización del modelo mostrado como una flecha punteada en la Fig. 3(a). Explicamos la semántica del nodo del modelo y el enlace de la política en la sección anterior; a continuación, describimos las actualizaciones del modelo. La actualización del nodo del modelo con el tiempo implica dos pasos: primero, dados los modelos en el tiempo t, identificamos el conjunto actualizado de modelos que residen en el nodo del modelo en el tiempo t + 1. Recuerda de la Sección 2 que el modelo intencional de un agente incluye sus creencias. Debido a que los agentes actúan y reciben observaciones, sus modelos se actualizan para reflejar sus creencias cambiadas. Dado que el conjunto de acciones óptimas para un modelo podría incluir todas las acciones, y el agente podría recibir cualquiera de las |Ωj| posibles observaciones, el conjunto actualizado en el paso de tiempo t + 1 tendrá como máximo |Mt j,l−1||Aj||Ωj| modelos. Aquí, |Mt j,l−1| es el número de modelos en el paso de tiempo t, |Aj| y |Ωj| son los espacios más grandes de acciones y observaciones respectivamente, entre todos los modelos. Segundo, calculamos la nueva distribución sobre los modelos actualizados dados la distribución original y la probabilidad de que el agente realice la acción y reciba la observación que llevó al modelo actualizado. Estos pasos son parte de la actualización de creencias del agente formalizada utilizando la Ecuación 1. En la Fig. 3(b), mostramos cómo se implementa el enlace de actualización del modelo punteado en el I-DID. Si cada uno de los dos modelos de nivel l − 1 atribuidos a j en el paso de tiempo t resulta en una acción, y j podría hacer una de dos posibles observaciones, entonces el nodo del modelo en el paso de tiempo t + 1 contiene cuatro modelos actualizados (mt+1,1 j,l−1 ,mt+1,2 j,l−1 , mt+1,3 j,l−1 , y mt+1,4 j,l−1 ). Estos modelos difieren en sus creencias iniciales, cada una de las cuales es el resultado de j actualizar sus creencias debido a su acción y una posible observación. Los nodos de decisión en cada uno de los I-DIDs o DIDs que representan los modelos de nivel inferior se asignan a la Figura 4 correspondiente: I-DID transformado con los nodos del modelo y el enlace de actualización del modelo reemplazados por los nodos de probabilidad y las relaciones (en negrita) de los nodos de probabilidad, como se mencionó anteriormente. A continuación, describimos cómo se calcula la distribución sobre el conjunto actualizado de modelos (la distribución sobre el nodo de probabilidad Mod[Mt+1 j ] en Mt+1 j,l−1). La probabilidad de que el modelo actualizado js sea, digamos mt+1,1 j,l−1, depende de la probabilidad de que j realice la acción y reciba la observación que condujo a este modelo, y de la distribución previa sobre los modelos en el paso de tiempo t. Dado que el nodo de probabilidad At j asume la distribución de cada uno de los nodos de acción basados en el valor de Mod[Mt j], la probabilidad de la acción está dada por este nodo de probabilidad. Para obtener la probabilidad de una observación posible js, introducimos el nodo de probabilidad Oj, que dependiendo del valor de Mod[Mt j], asume la distribución del nodo de observación en el modelo de nivel inferior denominado Mod[Mt j]. Dado que la probabilidad de las observaciones js depende del estado físico y de las acciones conjuntas de ambos agentes, el nodo Oj está vinculado con St+1, At j y At i. De manera análoga a At j, la tabla de probabilidad condicional de Oj también es un multiplexor modulado por Mod[Mt j]. Finalmente, la distribución de los modelos previos en el tiempo t se obtiene a partir del nodo de probabilidad, Mod[Mt j ] en Mt j,l−1. Por consiguiente, los nodos de probabilidad, Mod[Mt j], At j y Oj, forman los padres de Mod[Mt+1 j] en Mt+1 j,l−1. Ten en cuenta que el enlace de actualización del modelo puede ser reemplazado por los enlaces de dependencia entre los nodos de probabilidad que constituyen los nodos del modelo en las dos etapas temporales. En la Fig. 4 mostramos los dos cortes temporales I-DID con los nodos del modelo reemplazados por los nodos de probabilidad y las relaciones entre ellos. Los nodos de probabilidad y los enlaces de dependencia que no están en negrita son estándar, generalmente encontrados en DIDs. La expansión del I-DID a lo largo de más pasos de tiempo requiere la repetición de los dos pasos de actualizar el conjunto de modelos que forman el 2. Nota que Oj representa la observación j en el tiempo t + 1. El Sexto Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 817 valores del nodo del modelo y añadiendo las relaciones entre los nodos de probabilidad, tantas veces como enlaces de actualización del modelo haya. Observamos que el conjunto posible de modelos del otro agente j crece exponencialmente con el número de pasos de tiempo. Por ejemplo, después de T pasos, puede haber como máximo |Mt=1 j,l−1|(|Aj||Ωj|)T −1 modelos candidatos residiendo en el nodo del modelo. 4.2 Solución Análoga a los I-ID, la solución a un I-DID de nivel l para el agente i expandido a lo largo de T pasos de tiempo puede llevarse a cabo de forma recursiva. Con el propósito de ilustración, dejemos l=1 y T=2. El método de solución utiliza la técnica estándar de anticipación, proyectando las secuencias de acciones y observaciones de los agentes hacia adelante desde el estado de creencia actual [17], y encontrando las posibles creencias que podría tener en el siguiente paso temporal. Dado que el agente i también tiene una creencia sobre los modelos de j, la búsqueda anticipada incluye descubrir los posibles modelos que j podría tener en el futuro. Por consiguiente, cada uno de los modelos subintencionales o de nivel 0 de js (representados utilizando un DID estándar) en el primer paso de tiempo debe resolverse para obtener su conjunto óptimo de acciones. Estas acciones se combinan con el conjunto de observaciones posibles que j podría hacer en ese modelo, lo que resulta en un conjunto actualizado de modelos candidatos (que incluyen las creencias actualizadas) que podrían describir el comportamiento de j. Las creencias sobre este conjunto actualizado de modelos candidatos se calculan utilizando los métodos estándar de inferencia que utilizan las relaciones de dependencia entre los nodos del modelo, como se muestra en la Figura 3(b). Observamos la naturaleza recursiva de esta solución: al resolver el agente de nivel 1 I-DID, los DIDs de nivel 0 deben ser resueltos. Si el anidamiento de los modelos es más profundo, todos los modelos en todos los niveles, comenzando desde el nivel 0, se resuelven de manera ascendente. Breve descripción del algoritmo recursivo para resolver el agente es el Algoritmo para resolver I-DID. Entrada: nivel l ≥ 1 I-ID o nivel 0 ID, T Fase de Expansión 1. Para t desde 1 hasta T − 1, hacer 2. Si l ≥ 1, entonces llenar Mt+1 j,l−1 3. Para cada mt j en el rango (Mt j,l−1) hacer 4. Llame recursivamente al algoritmo con el l - 1 I-ID (o ID) que representa mt j y el horizonte, T - t + 1 5. Mapea el nodo de decisión del I-ID (o ID) resuelto, OPT(mt j), a un nodo de probabilidad Aj 6. Para cada aj en OPT(mt j) hacer 7. Para cada oj en Oj (parte de mt j) hacer 8. Actualizar la creencia js, bt+1 j ← SE(bt j, aj, oj) 9. mt+1 j ← Nuevo I-ID (o ID) con bt+1 j como la creencia inicial 10. Rango(Mt+1 j,l−1) ∪ ← {mt+1 j } 11. Añadir el nodo del modelo, Mt+1 j,l−1, y los enlaces de dependencia entre Mt j,l−1 y Mt+1 j,l−1 (mostrados en la Fig. 3(b)) 12. Agregue los nodos de probabilidad, decisión y utilidad para la franja de tiempo t + 1 y los enlaces de dependencia entre ellos 13. Establezca las CPTs para cada nodo de probabilidad y nodo de utilidad de la Fase de Mirada Adelante 14. Aplica el método estándar de anticipación y respaldo para resolver la Figura 5 expandida de I-DID: Algoritmo para resolver un nivel l ≥ 0 I-DID. Nivel l I-DID expandido durante T pasos de tiempo con otro agente j en la Figura 5. Adoptamos un enfoque de dos fases: Dado un I-ID de nivel l (descrito previamente en la Sección 3) con todos los modelos de niveles inferiores representados también como I-IDs o IDs (si es nivel 0), el primer paso es expandir el I-ID de nivel l a lo largo de T pasos de tiempo añadiendo los enlaces de dependencia y las tablas de probabilidad condicional para cada nodo. Nos enfocamos especialmente en establecer y poblar los nodos del modelo (líneas 3-11). Ten en cuenta que Range(·) devuelve los valores (modelos de nivel inferior) de la variable aleatoria dada como entrada (nodo del modelo). En la segunda fase, utilizamos una técnica estándar de anticipación proyectando las secuencias de acción y observación durante T pasos de tiempo en el futuro, y respaldando los valores de utilidad de las creencias alcanzables. Similar a los I-IDs, los I-DIDs se reducen a DIDs en ausencia de otros agentes. Como mencionamos anteriormente, los modelos de nivel 0 son los DIDs tradicionales. Sus soluciones proporcionan distribuciones de probabilidad sobre las acciones del agente modelado en ese nivel a los I-DIDs en el nivel 1. Dadas las distribuciones de probabilidad sobre las acciones de otros agentes, los IDIDs de nivel 1 pueden resolverse como DIDs y proporcionar distribuciones de probabilidad a modelos de niveles superiores. Suponga que el número de modelos considerados en cada nivel está limitado por un número, M. Resolver un I-DID de nivel l es equivalente a resolver O(Ml) DIDs. Para ilustrar la utilidad de los I-DIDs, los aplicamos a tres dominios de problemas. Describimos, en particular, la formulación del I-DID y las prescripciones óptimas obtenidas al resolverlo. 5.1 Seguimiento-Liderazgo en el Problema del Tigre Multiagente Comenzamos nuestras ilustraciones del uso de I-IDs e I-DIDs con una versión ligeramente modificada del problema del tigre multiagente discutido en [9]. El problema tiene dos agentes, cada uno de los cuales puede abrir la puerta derecha (OR), la puerta izquierda (OL) o escuchar (L). Además de escuchar gruñidos (desde la izquierda (GL) o desde la derecha (GR)) cuando escuchan, los agentes también escuchan chirridos (desde la izquierda (CL), desde la derecha (CR) o sin chirridos (S)), que indican ruidosamente que los otros agentes están abriendo una de las puertas. Cuando se abre cualquier puerta, el tigre persiste en su ubicación original con una probabilidad del 95%. El agente i escucha gruñidos con una fiabilidad del 65% y crujidos con una fiabilidad del 95%. El agente J, por otro lado, escucha gruñidos con una fiabilidad del 95%. Por lo tanto, la configuración es tal que el agente i escucha la apertura de puertas del agente j de manera más confiable que los rugidos de los tigres. Esto sugiere que podría usar acciones de JavaScript como indicación de la ubicación del tigre, como discutimos a continuación. Las preferencias de cada agente son como en el juego de un solo agente discutido en [13]. Las funciones de transición, observación y recompensa se muestran en [16]. Un buen indicador de la utilidad de los métodos normativos para la toma de decisiones como los I-DIDs es la aparición de comportamientos sociales realistas en sus prescripciones. En entornos del persistente problema del tigre multiagente que reflejan situaciones del mundo real, demostramos la seguidismo entre los agentes y, como se muestra en [15], el engaño entre agentes que creen que están en una relación de tipo seguidor-líder. En particular, analizamos las condiciones situacionales y epistemológicas suficientes para su surgimiento. El comportamiento de seguidores, por ejemplo, resulta del agente conociendo sus propias debilidades, evaluando las fortalezas, preferencias y posibles comportamientos del otro, y dándose cuenta de que es mejor para él seguir las acciones de los demás para maximizar sus ganancias. Consideremos una configuración particular del problema del tigre en la que el agente i cree que las preferencias de j están alineadas con las suyas: ambos solo quieren obtener el oro, y la audición de j es más confiable en comparación consigo mismo. Como ejemplo, supongamos que j, al escuchar, puede discernir la ubicación del tigre el 95% de las veces en comparación con su precisión del 65%. Además, el agente i no tiene ninguna información inicial sobre la ubicación de los tigres. En otras palabras, la creencia anidada de un solo nivel, bi,1, asigna 0.5 a cada una de las dos ubicaciones del tigre. Además, considera dos modelos de j, que difieren en los niveles iniciales de creencias planas de j. Esto se representa en el nivel 1 I-ID mostrado en la Fig. 6(a). Según un modelo, j asigna una probabilidad de 0.9 a que el tigre esté detrás de la puerta izquierda, mientras que el otro 818 The Sixth Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) Figura 6: (a) Nivel 1 I-ID del agente i, (b) dos IDs de nivel 0 del agente j cuyos nodos de decisión están mapeados a los nodos de probabilidad, A1 j , A2 j , en (a). El modelo asigna 0.1 a esa ubicación (ver Fig. 6(b)). El agente i está indeciso sobre estos dos modelos de j. Si variamos la capacidad auditiva, y resolvemos el nivel 1 correspondiente I-ID expandido en tres pasos de tiempo, obtenemos las políticas de comportamiento normativas mostradas en la Figura 7 que exhiben comportamiento de seguidores. Si la probabilidad de escuchar correctamente los gruñidos es de 0.65, entonces, como se muestra en la política en la Fig. 7(a), i comienza a seguir condicionalmente las acciones de j: i abre la misma puerta que j abrió anteriormente si su evaluación propia de la ubicación de los tigres confirma la elección de j. Si i pierde la capacidad de interpretar correctamente los gruñidos por completo, sigue ciegamente a j y abre la misma puerta que j abrió anteriormente (Fig. 7(b)). Figura 7: Emergencia de (a) seguidores condicionales, y (b) seguidores ciegos en el problema del tigre. Los comportamientos de interés están en negrita. * es un comodín y representa cualquiera de las observaciones. Observamos que un solo nivel de anidación de creencias, creencias sobre los modelos de los demás, fue suficiente para que surgiera el seguidismo en el problema del tigre. Sin embargo, los requisitos epistemológicos para el surgimiento del liderazgo son más complejos. Para que un agente, digamos j, surja como líder, primero debe surgir el seguidismo en el otro agente i. Como mencionamos anteriormente, si i está seguro de que sus preferencias son idénticas a las de j, y cree que j tiene un mejor sentido del oído, i seguirá las acciones de j con el tiempo. El agente j emerge como líder si cree que lo seguiré, lo que implica que la creencia de j debe estar anidada dos niveles de profundidad para permitirle reconocer su papel de liderazgo. Darse cuenta de que lo seguiré presenta a J una oportunidad para influir en sus acciones en beneficio del bien colectivo o de su interés propio solamente. Por ejemplo, en el problema del tigre, consideremos una situación en la que si tanto i como j abren la puerta correcta, entonces cada uno recibe un pago de 20 que es el doble del original. Si solo j selecciona la puerta correcta, obtiene la recompensa de 10. Por otro lado, si ambos agentes eligen la puerta incorrecta, sus penalizaciones se reducen a la mitad. En este entorno, es tanto en el mejor interés de j como en el beneficio colectivo que utilice su experiencia para seleccionar la puerta correcta, y así ser un buen líder. Sin embargo, considera un problema ligeramente diferente en el que j se beneficia de la pérdida de i y es penalizado si i se beneficia. Específicamente, dejemos que la ganancia se reste de js, indicando que j es antagonista hacia i: si j elige la puerta correcta y i la incorrecta, entonces la pérdida de 100 se convierte en la ganancia de js. El agente J cree que yo incorrectamente pienso que las preferencias de J son aquellas que promueven el bien colectivo y que comienza creyendo con un 99% de confianza dónde está el tigre. Dado que creo que mis preferencias son similares a las de j, y que j comienza creyendo casi con certeza que una de las dos ubicaciones es la correcta (dos modelos de nivel 0 de j), comenzaré siguiendo las acciones de j. Mostramos la política normativa para resolver su I-DID anidado individualmente durante tres pasos de tiempo en la Fig. 8(a). La política demuestra que seguiré ciegamente las acciones de js. Dado que el tigre persiste en su ubicación original con una probabilidad del 0.95, seleccionaré nuevamente la misma puerta. Si j comienza el juego con una probabilidad del 99% de que el tigre esté a la derecha, resolver su I-DID anidado dos niveles profundamente, resulta en la política mostrada en la Fig. 8(b). Aunque j está casi seguro de que OL es la acción correcta, comenzará seleccionando OR, seguido de OL. La intención del agente js es engañar a i, a quien cree que seguirá las acciones de js, para así obtener $110 en el segundo paso de tiempo, lo cual es más de lo que j ganaría si fuera honesto. Figura 8: Emergencia del engaño entre agentes en el problema del tigre. Los comportamientos de interés están en negrita. * denota como antes. (a) El agente es una política que demuestra que seguirá ciegamente las acciones de js. (b) Aunque j está casi seguro de que el tigre está a la derecha, comenzará seleccionando OR, seguido de OL, para engañar a i. 5.2 Altruismo y reciprocidad en el problema del bien público El problema del bien público (PG) [7], consiste en un grupo de M agentes, cada uno de los cuales debe contribuir con algún recurso a una olla pública o guardarlo para sí mismos. Dado que los recursos aportados al fondo público se comparten entre todos los agentes, son menos valiosos para el agente cuando están en el fondo público. Sin embargo, si todos los agentes eligen contribuir con sus recursos, entonces la recompensa para cada agente es mayor que si nadie contribuye. Dado que un agente recibe su parte del bote público independientemente de si ha contribuido o no, la acción dominante es que cada agente no contribuya y en su lugar se aproveche de las contribuciones de los demás. Sin embargo, los comportamientos de los jugadores humanos en simulaciones empíricas del problema de PG difieren de las predicciones normativas. Los experimentos revelan que muchos jugadores inicialmente contribuyen con una gran cantidad al bote público, y continúan contribuyendo cuando se juega el problema del PG repetidamente, aunque en cantidades decrecientes [4]. Muchos de estos experimentos [5] informan que un pequeño grupo central de jugadores contribuye persistentemente al bote público incluso cuando todos los demás están desertando. Estos experimentos también revelan que los jugadores que contribuyen persistentemente tienen preferencias altruistas o recíprocas que coinciden con la cooperación esperada de los demás. Para simplificar, asumimos que el juego se juega entre M = 2 agentes, i y j. Que cada agente esté inicialmente dotado con una cantidad de recursos XT. Mientras que la formulación clásica del juego PG permite que cada agente contribuya con cualquier cantidad de recursos (≤ XT) al bote público, simplificamos el espacio de acciones al permitir dos acciones posibles. Cada agente puede elegir contribuir (C) una cantidad fija de recursos, o no contribuir. La última acción es deThe Sixth Intl. La Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 819 fue señalada como defectuosa (D). Suponemos que las acciones no son observables por otros. El valor de los recursos en la bolsa pública se descuenta por ci para cada agente i, donde ci es el retorno privado marginal. Suponemos que ci < 1 para que el agente no se beneficie lo suficiente como para contribuir al bote público en beneficio propio. Simultáneamente, ciM > 1, lo que hace que la contribución colectiva sea óptima de Pareto. i/j C D C 2ciXT , 2cjXT ciXT − cp, XT + cjXT − P D XT + ciXT − P, cjXT − cp XT , XT Tabla 1: El juego PG de un solo disparo con castigo. Para fomentar las contribuciones, los agentes contribuyentes castigan a los que no colaboran pero incurren en un pequeño costo por administrar el castigo. Sea P la sanción impuesta al agente que se desvía y cp el costo no nulo de castigar al agente contribuyente. Para simplificar, asumimos que el costo de castigar es el mismo para ambos agentes. El juego PG de un solo disparo con castigo se muestra en la Tabla 1. Si ci = cj, cp > 0, y si P > XT − ciXT, entonces la deserción ya no es una acción dominante. Si P < XT − ciXT, entonces la deserción es la acción dominante para ambos. Si P = XT − ciXT, entonces el juego no es resoluble por dominancia. Figura 9: (a) Nivel 1 I-ID del agente i, (b) IDs de nivel 0 del agente j con nodos de decisión mapeados a los nodos de probabilidad, A1 j y A2 j, en (a). Formulamos una versión secuencial del problema PG con castigo desde la perspectiva del agente i. Aunque en el juego de PG repetido, la cantidad en la olla pública se revela a todos los agentes después de cada ronda de acciones, asumimos en nuestra formulación que está oculta para los agentes. Cada agente puede contribuir con una cantidad fija, xc, o desertar. Un agente al realizar una acción recibe una observación de abundante (PY) o escasa (MR) simbolizando el estado de la olla pública. Ten en cuenta que las observaciones también son indirectamente indicativas de las acciones del agente js porque el estado del bote público es influenciado por ellas. La cantidad de recursos en el agente es una olla privada, es perfectamente observable para i. Los pagos son análogos a la Tabla 1. Tomando prestado de las investigaciones empíricas del problema PG [5], construimos IDs de nivel 0 para j que modelan tipos altruistas y no altruistas (Fig. 9(b)). Específicamente, nuestro agente altruista tiene un alto retorno privado marginal (cj está cerca de 1) y no castiga a los demás que incumplen. Que xc = 1 y el agente de nivel 0 sea castigado la mitad de las veces que se desvía. Con una acción restante, ambos tipos de agentes eligen contribuir para evitar ser castigados. Con dos acciones por delante, el tipo altruista elige contribuir, mientras que el otro defecta. Esto se debe a que cj para el tipo altruista es cercano a 1, por lo tanto, el castigo esperado, 0.5P > (1 − cj), que el tipo altruista evita. Debido a que cj para el tipo no altruista es menor, prefiere no contribuir. Con tres pasos por delante, el agente altruista contribuye para evitar el castigo (0.5P > 2(1 − cj)), mientras que el tipo no altruista se desentiende. Para más de tres pasos, mientras el agente altruista continúa contribuyendo al bote público dependiendo de qué tan cercano esté su retorno privado marginal a 1, el tipo no altruista prescribe la deserción. Analizamos las decisiones de un agente altruista i modelado usando un nivel 1 I-DID expandido en 3 pasos de tiempo. i atribuye los dos modelos de nivel 0, mencionados anteriormente, a j (ver Fig. 9). Si creo con una probabilidad de 1 que j es altruista, elijo contribuir en cada uno de los tres pasos. Este comportamiento persiste cuando i no está al tanto de si j es altruista (Fig. 10(a)), y cuando i asigna una alta probabilidad a que j sea del tipo no altruista. Sin embargo, cuando i cree con una probabilidad de 1 que j no es altruista y por lo tanto seguramente va a traicionar, i elige traicionar para evitar ser castigado y porque su beneficio privado marginal es menor que 1. Estos resultados demuestran que el comportamiento de nuestro tipo altruista se asemeja al encontrado experimentalmente. El agente de nivel 1 no altruista elige desertar independientemente de lo altruista que crea que sea el otro agente. Analizamos el comportamiento de un tipo de agente recíproco que se ajusta a la cooperación o defección esperada. El retorno privado marginal del tipo recíproco es similar al del tipo no altruista, sin embargo, obtiene una mayor recompensa cuando su acción es similar a la del otro. Consideramos el caso en el que el agente recíproco i no está seguro de si j es altruista y cree que es probable que el bote público esté medio lleno. Para esta creencia previa, yo elijo defectar. Al recibir una observación de abundancia, decide contribuir, mientras que una observación de escasez lo hace defectar (Fig. 10(b)). Esto se debe a que una observación de abundancia indica que es probable que la olla esté más llena que a la mitad, lo cual resulta de la acción de js para contribuir. Por lo tanto, entre los dos modelos atribuidos a j, es probable que su tipo sea altruista, lo que hace probable que j contribuya nuevamente en el próximo paso de tiempo. El agente i, por lo tanto, elige contribuir para corresponder la acción de js. Un razonamiento análogo lleva a uno a defectar cuando observa una olla escasa. Con una acción por hacer, creo que si j contribuye, también elegiré contribuir para evitar castigo independientemente de sus observaciones. Figura 10: (a) Un agente altruista de nivel 1 siempre contribuye. (b) Un agente recíproco i comienza defraudando y luego elige contribuir o defraudar basándose en su observación de abundancia (indicando que j es probablemente altruista) o escasez (j no es altruista). 5.3 Estrategias en el póker de dos jugadores El póker es un popular juego de cartas de suma cero que ha recibido mucha atención en la comunidad de investigación de IA como un banco de pruebas [2]. El póker se juega entre M ≥ 2 jugadores, en el que cada jugador recibe una mano de cartas de una baraja. Si bien existen varias variantes de póker con diferentes niveles de complejidad, consideramos una versión simple en la que cada jugador tiene tres turnos durante los cuales el jugador puede intercambiar una carta (E), mantener la mano existente (K), retirarse (F) y abandonar el juego, o apostar (C), lo que requiere que todos los jugadores muestren sus manos. Para mantener las cosas simples, dejemos que M = 2, y que cada jugador reciba una mano compuesta por una sola carta sacada del mismo palo. Por lo tanto, durante un enfrentamiento, el jugador que tenga la carta numéricamente más alta (el 2 es el más bajo, el as es el más alto) gana el bote. Durante un intercambio de cartas, la carta descartada se coloca ya sea en la pila L, indicando al otro agente que era una carta de número bajo menor que 8, o en la 820 The Sixth Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) H, indicando que la carta tenía un rango mayor o igual a 8. Ten en cuenta que, por ejemplo, si se descarta una carta de número bajo, la probabilidad de recibir una carta baja a cambio se reduce. Mostramos el nivel 1 I-ID para el Poker simplificado de dos jugadores en la Fig. 11. Consideramos dos modelos (tipos de personalidad) del agente j. El tipo conservador cree que es probable que su oponente tenga una carta de alto número en su mano. Por otro lado, el agente agresivo j cree con alta probabilidad que su oponente tiene una carta de número más bajo. Por lo tanto, los dos tipos difieren en sus creencias sobre la mano de sus oponentes. En ambos estos modelos de nivel 0, se asume que el oponente realiza sus acciones siguiendo una distribución fija y uniforme. Con tres acciones restantes, independientemente de su mano (a menos que sea un as), el agente agresivo elige intercambiar su carta, con la intención de mejorar su mano actual. Esto se debe a que cree que el otro tiene una carta baja, lo que mejora sus posibilidades de obtener una carta alta durante el intercambio. El agente conservador elige quedarse con su carta, sin importar su mano, porque considera que sus posibilidades de obtener una carta alta son escasas, ya que cree que su oponente tiene una. Figura 11: (a) Nivel 1 I-ID del agente i. La observación revela información sobre la mano de js del paso de tiempo anterior, (b) IDs de nivel 0 del agente j cuyos nodos de decisión están mapeados a los nodos de probabilidad, A1 j , A2 j , en (a). La política de un agente de nivel 1 i que cree que cada carta, excepto la suya, tiene la misma probabilidad de estar en su mano (tipo de personalidad neutral) y j podría ser de tipo agresivo o conservador, se muestra en la Figura 12. Su mano contiene la carta numerada 8. El agente comienza por mantener su carta. Al ver que j no intercambió una carta (N), i cree con probabilidad 1 que j es conservador y, por lo tanto, mantendrá sus cartas. i responde manteniendo su carta o intercambiándola porque j tiene igual probabilidad de tener una carta más baja o más alta. Si observo que j descartó su carta en la pila L o H, creo que j es agresivo. Al observar a L, i se da cuenta de que j tenía una carta baja, y es probable que tenga una carta alta después de su intercambio. Dado que la probabilidad de recibir una carta baja es alta en este momento, i decide quedarse con su carta. Al observar la carta H, creyendo que la probabilidad de recibir una carta de número alto es alta, i decide intercambiar su carta. En el paso final, i decide llamar independientemente de su historial de observaciones porque su creencia de que j tiene una carta más alta no es lo suficientemente alta como para concluir que es mejor retirarse y renunciar al pago. Esto se debe en parte al hecho de que una observación de, digamos, L, restablece las creencias del agente en el paso de tiempo anterior sobre las cartas de números bajos solamente. 6. DISCUSIÓN Mostramos cómo los DIDs pueden ser extendidos a los I-DIDs que permiten la toma de decisiones secuenciales en línea en entornos multiagentes inciertos. Nuestra representación gráfica de I-DIDs mejora la Figura 12 anterior: Un agente de nivel 1 es una política de tres pasos en el problema de Poker. i comienza creyendo que j tiene la misma probabilidad de ser agresivo o conservador y podría tener cualquier carta en su mano con igual probabilidad. Trabaja significativamente al ser más transparente, semánticamente claro y capaz de ser resuelto utilizando algoritmos estándar que apuntan a los DIDs. Los I-DIDs extienden los NIDs para permitir la toma de decisiones secuenciales a lo largo de múltiples pasos de tiempo en presencia de otros agentes que interactúan. Los I-DIDs pueden ser vistos como representaciones gráficas concisas para IPOMDPs que proporcionan una forma de explotar la estructura del problema y llevar a cabo la toma de decisiones en línea a medida que el agente actúa y observa dadas sus creencias previas. Actualmente estamos investigando formas de resolver I-DIDs aproximadamente con límites demostrables en la calidad de la solución. Agradecimiento: Agradecemos a Piotr Gmytrasiewicz por algunas discusiones útiles relacionadas con este trabajo. El primer autor desea agradecer el apoyo de una beca UGARF. 7. REFERENCIAS [1] R. J. Aumann. Epistemología interactiva i: Conocimiento. Revista Internacional de Teoría de Juegos, 28:263-300, 1999. [2] D. Billings, A. Davidson, J. Schaeffer y D. Szafron. El desafío del póker. AIJ, 2001. [3] A. Brandenburger y E. Dekel. Jerarquías de creencias y conocimiento común. Revista de Teoría Económica, 59:189-198, 1993. [4] C. Camerer. Teoría de juegos conductual: Experimentos en interacción estratégica. Princeton University Press, 2003. [5] E. Fehr y S. Gachter. Cooperación y castigo en experimentos de bienes públicos. Revista Económica Americana, 90(4):980-994, 2000. [6] D. Fudenberg y D. K. Levine. La Teoría del Aprendizaje en los Juegos. MIT Press, 1998. [7] D. Fudenberg y J. Tirole. Teoría de juegos. MIT Press, 1991. [8] Y. Gal y A. Pfeffer. Un lenguaje para modelar los procesos de toma de decisiones de agentes en juegos. En AAMAS, 2003. [9] P. Gmytrasiewicz y P. Doshi. Un marco para la planificación secuencial en entornos multiagentes. JAIR, 24:49-79, 2005. [10] P. Gmytrasiewicz y E. Durfee. Coordinación racional en entornos multiagente. JAAMAS, 3(4):319-350, 2000. [11] J. C. Harsanyi. Juegos con información incompleta jugados por jugadores bayesianos. Ciencia de la Gestión, 14(3):159-182, 1967. [12] R. A. Howard y J. E. Matheson. Diagramas de influencia. En R. A. Howard y J. E. Matheson, editores, Los Principios y Aplicaciones del Análisis de Decisiones. Grupo de Decisiones Estratégicas, Menlo Park, CA 94025, 1984. [13] L. Kaelbling, M. Littman y A. Cassandra. Planificación y actuación en dominios estocásticos parcialmente observables. Revista de Inteligencia Artificial, 2, 1998. [14] D. Koller y B. Milch. Diagramas de influencia multiagente para representar y resolver juegos. En IJCAI, páginas 1027-1034, 2001. [15] K. Polich y P. Gmytrasiewicz. Diagramas de influencia interactivos y dinámicos. En el taller GTDT, AAMAS, 2006. [16] B. Rathnas, P. Doshi y P. J. Gmytrasiewicz. Soluciones exactas para POMDP interactivos utilizando equivalencia conductual. En la Conferencia de Agentes Autónomos y Sistemas Multiagente (AAMAS), 2006. [17] S. Russell y P. Norvig. Inteligencia Artificial: Un Enfoque Moderno (Segunda Edición). Prentice Hall, 2003. [18] R. D. Shachter. \n\nPrentice Hall, 2003. [18] R. D. Shachter. Evaluando diagramas de influencia. Investigación de Operaciones, 34(6):871-882, 1986. [19] D. Suryadi y P. Gmytrasiewicz. Aprendiendo modelos de otros agentes utilizando diagramas de influencia. En UM, 1999. El Sexto Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 821",
    "original_sentences": [
        "Graphical Models for Online Solutions to Interactive POMDPs Prashant Doshi Dept.",
        "of Computer Science University of Georgia Athens, GA 30602, USA pdoshi@cs.uga.edu Yifeng Zeng Dept. of Computer Science Aalborg University DK-9220 Aalborg, Denmark yfzeng@cs.aau.edu Qiongyu Chen Dept. of Computer Science National Univ. of Singapore 117543, Singapore chenqy@comp.nus.edu.sg ABSTRACT We develop a new graphical representation for interactive partially observable Markov decision processes (I-POMDPs) that is significantly more transparent and semantically clear than the previous representation.",
        "These graphical models called interactive dynamic influence diagrams (I-DIDs) seek to explicitly model the structure that is often present in real-world problems by decomposing the situation into chance and decision variables, and the dependencies between the variables.",
        "I-DIDs generalize DIDs, which may be viewed as graphical representations of POMDPs, to multiagent settings in the same way that I-POMDPs generalize POMDPs.",
        "I-DIDs may be used to compute the policy of an agent online as the agent acts and observes in a setting that is populated by other interacting agents.",
        "Using several examples, we show how I-DIDs may be applied and demonstrate their usefulness.",
        "Categories and Subject Descriptors I.2.11 [Distributed Artificial Intelligence]: Multiagent Systems General Terms Theory 1.",
        "INTRODUCTION Interactive partially observable Markov decision processes (IPOMDPs) [9] provide a framework for sequential decision-making in partially observable multiagent environments.",
        "They generalize POMDPs [13] to multiagent settings by including the other agents computable models in the state space along with the states of the physical environment.",
        "The models encompass all information influencing the agents behaviors, including their preferences, capabilities, and beliefs, and are thus analogous to types in Bayesian games [11].",
        "I-POMDPs adopt a subjective approach to understanding strategic behavior, rooted in a decision-theoretic framework that takes a decision-makers perspective in the interaction.",
        "In [15], Polich and Gmytrasiewicz introduced interactive dynamic influence diagrams (I-DIDs) as the computational representations of I-POMDPs.",
        "I-DIDs generalize DIDs [12], which may be viewed as computational counterparts of POMDPs, to multiagents settings in the same way that I-POMDPs generalize POMDPs.",
        "I-DIDs contribute to a growing line of work [19] that includes multi-agent influence diagrams (MAIDs) [14], and more recently, networks of influence diagrams (NIDs) [8].",
        "These formalisms seek to explicitly model the structure that is often present in real-world problems by decomposing the situation into chance and decision variables, and the dependencies between the variables.",
        "MAIDs provide an alternative to normal and extensive game forms using a graphical formalism to represent games of imperfect information with a decision node for each agents actions and chance nodes capturing the agents private information.",
        "MAIDs objectively analyze the game, efficiently computing the Nash equilibrium profile by exploiting the independence structure.",
        "NIDs extend MAIDs to include agents uncertainty over the game being played and over models of the other agents.",
        "Each model is a MAID and the network of MAIDs is collapsed, bottom up, into a single MAID for computing the equilibrium of the game keeping in mind the different models of each agent.",
        "Graphical formalisms such as MAIDs and NIDs open up a promising area of research that aims to represent multiagent interactions more transparently.",
        "However, MAIDs provide an analysis of the game from an external viewpoint and the applicability of both is limited to static single play games.",
        "Matters are more complex when we consider interactions that are extended over time, where predictions about others future actions must be made using models that change as the agents act and observe.",
        "I-DIDs address this gap by allowing the representation of other agents models as the values of a special model node.",
        "Both, other agents models and the original agents beliefs over these models are updated over time using special-purpose implementations.",
        "In this paper, we improve on the previous preliminary representation of the I-DID shown in [15] by using the insight that the static I-ID is a type of NID.",
        "Thus, we may utilize NID-specific language constructs such as multiplexers to represent the model node, and subsequently the I-ID, more transparently.",
        "Furthermore, we clarify the semantics of the special purpose policy link introduced in the representation of I-DID by [15], and show that it could be replaced by traditional dependency links.",
        "In the previous representation of the I-DID, the update of the agents belief over the models of others as the agents act and receive observations was denoted using a special link called the model update link that connected the model nodes over time.",
        "We explicate the semantics of this link by showing how it can be implemented using the traditional dependency links between the chance nodes that constitute the model nodes.",
        "The net result is a representation of I-DID that is significantly more transparent, semantically clear, and capable of being implemented using the standard algorithms for solving DIDs.",
        "We show how IDIDs may be used to model an agents uncertainty over others models, that may themselves be I-DIDs.",
        "Solution to the I-DID is a policy that prescribes what the agent should do over time, given its beliefs over the physical state and others models.",
        "Analogous to DIDs, I-DIDs may be used to compute the policy of an agent online as the agent acts and observes in a setting that is populated by other interacting agents. 2.",
        "BACKGROUND: FINITELY NESTED IPOMDPS Interactive POMDPs generalize POMDPs to multiagent settings by including other agents models as part of the state space [9].",
        "Since other agents may also reason about others, the interactive state space is strategically nested; it contains beliefs about other agents models and their beliefs about others.",
        "For simplicity of presentation we consider an agent, i, that is interacting with one other agent, j.",
        "A finitely nested I-POMDP of agent i with a strategy level l is defined as the tuple: I-POMDPi,l = ISi,l, A, Ti, Ωi, Oi, Ri where: • ISi,l denotes a set of interactive states defined as, ISi,l = S × Mj,l−1, where Mj,l−1 = {Θj,l−1 ∪ SMj}, for l ≥ 1, and ISi,0 = S, where S is the set of states of the physical environment.",
        "Θj,l−1 is the set of computable intentional models of agent j: θj,l−1 = bj,l−1, ˆθj where the frame, ˆθj = A, Ωj, Tj, Oj, Rj, OCj .",
        "Here, j is Bayes rational and OCj is js optimality criterion.",
        "SMj is the set of subintentional models of j.",
        "Simple examples of subintentional models include a no-information model [10] and a fictitious play model [6], both of which are history independent.",
        "We give a recursive bottom-up construction of the interactive state space below.",
        "ISi,0 = S, Θj,0 = { bj,0, ˆθj | bj,0 ∈ Δ(ISj,0)} ISi,1 = S × {Θj,0 ∪ SMj}, Θj,1 = { bj,1, ˆθj | bj,1 ∈ Δ(ISj,1)} . . . . . .",
        "ISi,l = S × {Θj,l−1 ∪ SMj}, Θj,l = { bj,l, ˆθj | bj,l ∈ Δ(ISj,l)} Similar formulations of nested spaces have appeared in [1, 3]. • A = Ai × Aj is the set of joint actions of all agents in the environment; • Ti : S ×A×S → [0, 1], describes the effect of the joint actions on the physical states of the environment; • Ωi is the set of observations of agent i; • Oi : S × A × Ωi → [0, 1] gives the likelihood of the observations given the physical state and joint action; • Ri : ISi × A → R describes agent is preferences over its interactive states.",
        "Usually only the physical states will matter.",
        "Agent is policy is the mapping, Ω∗ i → Δ(Ai), where Ω∗ i is the set of all observation histories of agent i.",
        "Since belief over the interactive states forms a sufficient statistic [9], the policy can also be represented as a mapping from the set of all beliefs of agent i to a distribution over its actions, Δ(ISi) → Δ(Ai). 2.1 Belief Update Analogous to POMDPs, an agent within the I-POMDP framework updates its belief as it acts and observes.",
        "However, there are two differences that complicate the belief update in multiagent settings when compared to single agent ones.",
        "First, since the state of the physical environment depends on the actions of both agents, is prediction of how the physical state changes has to be made based on its prediction of js actions.",
        "Second, changes in js models have to be included in is belief update.",
        "Specifically, if j is intentional then an update of js beliefs due to its action and observation has to be included.",
        "In other words, i has to update its belief based on its prediction of what j would observe and how j would update its belief.",
        "If js model is subintentional, then js probable observations are appended to the observation history contained in the model.",
        "Formally, we have: Pr(ist |at−1 i , bt−1 i,l ) = β ISt−1:mt−1 j =θt j bt−1 i,l (ist−1 ) × at−1 j Pr(at−1 j |θt−1 j,l−1)Oi(st , at−1 i , at−1 j , ot i) ×Ti(st−1 , at−1 i , at−1 j , st ) ot j Oj(st , at−1 i , at−1 j , ot j) ×τ(SEθt j (bt−1 j,l−1, at−1 j , ot j) − bt j,l−1) (1) where β is the normalizing constant, τ is 1 if its argument is 0 otherwise it is 0, Pr(at−1 j |θt−1 j,l−1) is the probability that at−1 j is Bayes rational for the agent described by model θt−1 j,l−1, and SE(·) is an abbreviation for the belief update.",
        "For a version of the belief update when js model is subintentional, see [9].",
        "If agent j is also modeled as an I-POMDP, then is belief update invokes js belief update (via the term SEθt j ( bt−1 j,l−1 , at−1 j , ot j)), which in turn could invoke is belief update and so on.",
        "This recursion in belief nesting bottoms out at the 0th level.",
        "At this level, the belief update of the agent reduces to a POMDP belief update. 1 For illustrations of the belief update, additional details on I-POMDPs, and how they compare with other multiagent frameworks, see [9]. 2.2 Value Iteration Each belief state in a finitely nested I-POMDP has an associated value reflecting the maximum payoff the agent can expect in this belief state: Un ( bi,l, θi ) = max ai∈Ai is∈ISi,l ERi(is, ai)bi,l(is)+ γ oi∈Ωi Pr(oi|ai, bi,l)Un−1 ( SEθi (bi,l, ai, oi), θi ) (2) where, ERi(is, ai) = aj Ri(is, ai, aj)Pr(aj|mj,l−1) (since is = (s, mj,l−1)).",
        "Eq. 2 is a basis for value iteration in I-POMDPs.",
        "Agent is optimal action, a∗ i , for the case of finite horizon with discounting, is an element of the set of optimal actions for the belief state, OPT(θi), defined as: OPT( bi,l, θi ) = argmax ai∈Ai is∈ISi,l ERi(is, ai)bi,l(is) +γ oi∈Ωi Pr(oi|ai, bi,l)Un ( SEθi (bi,l, ai, oi), θi ) (3) 3.",
        "INTERACTIVEINFLUENCEDIAGRAMS A naive extension of influence diagrams (IDs) to settings populated by multiple agents is possible by treating other agents as automatons, represented using chance nodes.",
        "However, this approach assumes that the agents actions are controlled using a probability distribution that does not change over time.",
        "Interactive influence diagrams (I-IDs) adopt a more sophisticated approach by generalizing IDs to make them applicable to settings shared with other agents who may act and observe, and update their beliefs. 3.1 Syntax In addition to the usual chance, decision, and utility nodes, IIDs include a new type of node called the model node.",
        "We show a general level l I-ID in Fig. 1(a), where the model node (Mj,l−1) is denoted using a hexagon.",
        "We note that the probability distribution over the chance node, S, and the model node together represents agent is belief over its interactive states.",
        "In addition to the model 1 The 0th level model is a POMDP: Other agents actions are treated as exogenous events and folded into the T, O, and R functions.",
        "The Sixth Intl.",
        "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 815 Figure 1: (a) A generic level l I-ID for agent i situated with one other agent j.",
        "The hexagon is the model node (Mj,l−1) whose structure we show in (b).",
        "Members of the model node are I-IDs themselves (m1 j,l−1, m2 j,l−1; diagrams not shown here for simplicity) whose decision nodes are mapped to the corresponding chance nodes (A1 j , A2 j ).",
        "Depending on the value of the node, Mod[Mj], the distribution of each of the chance nodes is assigned to the node Aj. (c) The transformed I-ID with the model node replaced by the chance nodes and the relationships between them. node, I-IDs differ from IDs by having a dashed link (called the policy link in [15]) between the model node and a chance node, Aj, that represents the distribution over the other agents actions given its model.",
        "In the absence of other agents, the model node and the chance node, Aj, vanish and I-IDs collapse into traditional IDs.",
        "The model node contains the alternative computational models ascribed by i to the other agent from the set, Θj,l−1 ∪ SMj, where Θj,l−1 and SMj were defined previously in Section 2.",
        "Thus, a model in the model node may itself be an I-ID or ID, and the recursion terminates when a model is an ID or subintentional.",
        "Because the model node contains the alternative models of the other agent as its values, its representation is not trivial.",
        "In particular, some of the models within the node are I-IDs that when solved generate the agents optimal policy in their decision nodes.",
        "Each decision node is mapped to the corresponding chance node, say A1 j , in the following way: if OPT is the set of optimal actions obtained by solving the I-ID (or ID), then Pr(aj ∈ A1 j ) = 1 |OP T | if aj ∈ OPT, 0 otherwise.",
        "Borrowing insights from previous work [8], we observe that the model node and the dashed policy link that connects it to the chance node, Aj, could be represented as shown in Fig. 1(b).",
        "The decision node of each level l − 1 I-ID is transformed into a chance node, as we mentioned previously, so that the actions with the largest value in the decision node are assigned uniform probabilities in the chance node while the rest are assigned zero probability.",
        "The different chance nodes (A1 j , A2 j ), one for each model, and additionally, the chance node labeled Mod[Mj] form the parents of the chance node, Aj.",
        "Thus, there are as many action nodes (A1 j , A2 j ) in Mj,l−1 as the number of models in the support of agent is beliefs.",
        "The conditional probability table of the chance node, Aj, is a multiplexer that assumes the distribution of each of the action nodes (A1 j , A2 j ) depending on the value of Mod[Mj].",
        "The values of Mod[Mj] denote the different models of j.",
        "In other words, when Mod[Mj] has the value m1 j,l−1, the chance node Aj assumes the distribution of the node A1 j , and Aj assumes the distribution of A2 j when Mod[Mj] has the value m2 j,l−1.",
        "The distribution over the node, Mod[Mj], is the agent is belief over the models of j given a physical state.",
        "For more agents, we will have as many model nodes as there are agents.",
        "Notice that Fig. 1(b) clarifies the semantics of the policy link, and shows how it can be represented using the traditional dependency links.",
        "In Fig. 1(c), we show the transformed I-ID when the model node is replaced by the chance nodes and relationships between them.",
        "In contrast to the representation in [15], there are no special-purpose policy links, rather the I-ID is composed of only those types of nodes that are found in traditional IDs and dependency relationships between the nodes.",
        "This allows I-IDs to be represented and implemented using conventional application tools that target IDs.",
        "Note that we may view the level l I-ID as a NID.",
        "Specifically, each of the level l − 1 models within the model node are blocks in the NID (see Fig. 2).",
        "If the level l = 1, each block is a traditional ID, otherwise if l > 1, each block within the NID may itself be a NID.",
        "Note that within the I-IDs (or IDs) at each level, there is only a single decision node.",
        "Thus, our NID does not contain any MAIDs.",
        "Figure 2: A level l I-ID represented as a NID.",
        "The probabilities assigned to the blocks of the NID are is beliefs over js models conditioned on a physical state. 3.2 Solution The solution of an I-ID proceeds in a bottom-up manner, and is implemented recursively.",
        "We start by solving the level 0 models, which, if intentional, are traditional IDs.",
        "Their solutions provide probability distributions over the other agents actions, which are entered in the corresponding chance nodes found in the model node of the level 1 I-ID.",
        "The mapping from the level 0 models decision nodes to the chance nodes is carried out so that actions with the largest value in the decision node are assigned uniform probabilities in the chance node while the rest are assigned zero probability.",
        "Given the distributions over the actions within the different chance nodes (one for each model of the other agent), the level 1 I-ID is transformed as shown in Fig. 1(c).",
        "During the transformation, the conditional probability table (CPT) of the node, Aj, is populated such that the node assumes the distribution of each of the chance nodes depending on the value of the node, Mod[Mj].",
        "As we mentioned previously, the values of the node Mod[Mj] denote the different models of the other agent, and its distribution is the agent is belief over the models of j conditioned on the physical state.",
        "The transformed level 1 I-ID is a traditional ID that may be solved us816 The Sixth Intl.",
        "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) (a) (b) Figure 3: (a) A generic two time-slice level l I-DID for agent i in a setting with one other agent j.",
        "Notice the dotted model update link that denotes the update of the models of j and the distribution over the models over time. (b) The semantics of the model update link. ing the standard expected utility maximization method [18].",
        "This procedure is carried out up to the level l I-ID whose solution gives the non-empty set of optimal actions that the agent should perform given its belief.",
        "Notice that analogous to IDs, I-IDs are suitable for online decision-making when the agents current belief is known. 4.",
        "INTERACTIVE DYNAMIC INFLUENCE DIAGRAMS Interactive dynamic influence diagrams (I-DIDs) extend I-IDs (and NIDs) to allow sequential decision-making over several time steps.",
        "Just as DIDs are structured graphical representations of POMDPs, I-DIDs are the graphical online analogs for finitely nested I-POMDPs.",
        "I-DIDs may be used to optimize over a finite look-ahead given initial beliefs while interacting with other, possibly similar, agents. 4.1 Syntax We depict a general two time-slice I-DID in Fig. 3(a).",
        "In addition to the model nodes and the dashed policy link, what differentiates an I-DID from a DID is the model update link shown as a dotted arrow in Fig. 3(a).",
        "We explained the semantics of the model node and the policy link in the previous section; we describe the model updates next.",
        "The update of the model node over time involves two steps: First, given the models at time t, we identify the updated set of models that reside in the model node at time t + 1.",
        "Recall from Section 2 that an agents intentional model includes its belief.",
        "Because the agents act and receive observations, their models are updated to reflect their changed beliefs.",
        "Since the set of optimal actions for a model could include all the actions, and the agent may receive any one of |Ωj| possible observations, the updated set at time step t + 1 will have at most |Mt j,l−1||Aj||Ωj| models.",
        "Here, |Mt j,l−1| is the number of models at time step t, |Aj| and |Ωj| are the largest spaces of actions and observations respectively, among all the models.",
        "Second, we compute the new distribution over the updated models given the original distribution and the probability of the agent performing the action and receiving the observation that led to the updated model.",
        "These steps are a part of agent is belief update formalized using Eq. 1.",
        "In Fig. 3(b), we show how the dotted model update link is implemented in the I-DID.",
        "If each of the two level l − 1 models ascribed to j at time step t results in one action, and j could make one of two possible observations, then the model node at time step t + 1 contains four updated models (mt+1,1 j,l−1 ,mt+1,2 j,l−1 , mt+1,3 j,l−1 , and mt+1,4 j,l−1 ).",
        "These models differ in their initial beliefs, each of which is the result of j updating its beliefs due to its action and a possible observation.",
        "The decision nodes in each of the I-DIDs or DIDs that represent the lower level models are mapped to the corresponding Figure 4: Transformed I-DID with the model nodes and model update link replaced with the chance nodes and the relationships (in bold). chance nodes, as mentioned previously.",
        "Next, we describe how the distribution over the updated set of models (the distribution over the chance node Mod[Mt+1 j ] in Mt+1 j,l−1) is computed.",
        "The probability that js updated model is, say mt+1,1 j,l−1 , depends on the probability of j performing the action and receiving the observation that led to this model, and the prior distribution over the models at time step t. Because the chance node At j assumes the distribution of each of the action nodes based on the value of Mod[Mt j ], the probability of the action is given by this chance node.",
        "In order to obtain the probability of js possible observation, we introduce the chance node Oj, which depending on the value of Mod[Mt j ] assumes the distribution of the observation node in the lower level model denoted by Mod[Mt j ].",
        "Because the probability of js observations depends on the physical state and the joint actions of both agents, the node Oj is linked with St+1 , At j, and At i. 2 Analogous to At j, the conditional probability table of Oj is also a multiplexer modulated by Mod[Mt j ].",
        "Finally, the distribution over the prior models at time t is obtained from the chance node, Mod[Mt j ] in Mt j,l−1.",
        "Consequently, the chance nodes, Mod[Mt j ], At j, and Oj, form the parents of Mod[Mt+1 j ] in Mt+1 j,l−1.",
        "Notice that the model update link may be replaced by the dependency links between the chance nodes that constitute the model nodes in the two time slices.",
        "In Fig. 4 we show the two time-slice I-DID with the model nodes replaced by the chance nodes and the relationships between them.",
        "Chance nodes and dependency links that not in bold are standard, usually found in DIDs.",
        "Expansion of the I-DID over more time steps requires the repetition of the two steps of updating the set of models that form the 2 Note that Oj represents js observation at time t + 1.",
        "The Sixth Intl.",
        "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 817 values of the model node and adding the relationships between the chance nodes, as many times as there are model update links.",
        "We note that the possible set of models of the other agent j grows exponentially with the number of time steps.",
        "For example, after T steps, there may be at most |Mt=1 j,l−1|(|Aj||Ωj|)T −1 candidate models residing in the model node. 4.2 Solution Analogous to I-IDs, the solution to a level l I-DID for agent i expanded over T time steps may be carried out recursively.",
        "For the purpose of illustration, let l=1 and T=2.",
        "The solution method uses the standard look-ahead technique, projecting the agents action and observation sequences forward from the current belief state [17], and finding the possible beliefs that i could have in the next time step.",
        "Because agent i has a belief over js models as well, the lookahead includes finding out the possible models that j could have in the future.",
        "Consequently, each of js subintentional or level 0 models (represented using a standard DID) in the first time step must be solved to obtain its optimal set of actions.",
        "These actions are combined with the set of possible observations that j could make in that model, resulting in an updated set of candidate models (that include the updated beliefs) that could describe the behavior of j. Beliefs over this updated set of candidate models are calculated using the standard inference methods using the dependency relationships between the model nodes as shown in Fig. 3(b).",
        "We note the recursive nature of this solution: in solving agent is level 1 I-DID, js level 0 DIDs must be solved.",
        "If the nesting of models is deeper, all models at all levels starting from 0 are solved in a bottom-up manner.",
        "We briefly outline the recursive algorithm for solving agent is Algorithm for solving I-DID Input : level l ≥ 1 I-ID or level 0 ID, T Expansion Phase 1.",
        "For t from 1 to T − 1 do 2.",
        "If l ≥ 1 then Populate Mt+1 j,l−1 3.",
        "For each mt j in Range(Mt j,l−1) do 4.",
        "Recursively call algorithm with the l − 1 I-ID (or ID) that represents mt j and the horizon, T − t + 1 5.",
        "Map the decision node of the solved I-ID (or ID), OPT(mt j), to a chance node Aj 6.",
        "For each aj in OPT(mt j) do 7.",
        "For each oj in Oj (part of mt j) do 8.",
        "Update js belief, bt+1 j ← SE(bt j, aj, oj) 9. mt+1 j ← New I-ID (or ID) with bt+1 j as the initial belief 10.",
        "Range(Mt+1 j,l−1) ∪ ← {mt+1 j } 11.",
        "Add the model node, Mt+1 j,l−1, and the dependency links between Mt j,l−1 and Mt+1 j,l−1 (shown in Fig. 3(b)) 12.",
        "Add the chance, decision, and utility nodes for t + 1 time slice and the dependency links between them 13.",
        "Establish the CPTs for each chance node and utility node Look-Ahead Phase 14.",
        "Apply the standard look-ahead and backup method to solve the expanded I-DID Figure 5: Algorithm for solving a level l ≥ 0 I-DID. level l I-DID expanded over T time steps with one other agent j in Fig. 5.",
        "We adopt a two-phase approach: Given an I-ID of level l (described previously in Section 3) with all lower level models also represented as I-IDs or IDs (if level 0), the first step is to expand the level l I-ID over T time steps adding the dependency links and the conditional probability tables for each node.",
        "We particularly focus on establishing and populating the model nodes (lines 3-11).",
        "Note that Range(·) returns the values (lower level models) of the random variable given as input (model node).",
        "In the second phase, we use a standard look-ahead technique projecting the action and observation sequences over T time steps in the future, and backing up the utility values of the reachable beliefs.",
        "Similar to I-IDs, the I-DIDs reduce to DIDs in the absence of other agents.",
        "As we mentioned previously, the 0-th level models are the traditional DIDs.",
        "Their solutions provide probability distributions over actions of the agent modeled at that level to I-DIDs at level 1.",
        "Given probability distributions over other agents actions the level 1 IDIDs can themselves be solved as DIDs, and provide probability distributions to yet higher level models.",
        "Assume that the number of models considered at each level is bound by a number, M. Solving an I-DID of level l in then equivalent to solving O(Ml ) DIDs. 5.",
        "EXAMPLE APPLICATIONS To illustrate the usefulness of I-DIDs, we apply them to three problem domains.",
        "We describe, in particular, the formulation of the I-DID and the optimal prescriptions obtained on solving it. 5.1 Followership-Leadership in the Multiagent Tiger Problem We begin our illustrations of using I-IDs and I-DIDs with a slightly modified version of the multiagent tiger problem discussed in [9].",
        "The problem has two agents, each of which can open the right door (OR), the left door (OL) or listen (L).",
        "In addition to hearing growls (from the left (GL) or from the right (GR)) when they listen, the agents also hear creaks (from the left (CL), from the right (CR), or no creaks (S)), which noisily indicate the other agents opening one of the doors.",
        "When any door is opened, the tiger persists in its original location with a probability of 95%.",
        "Agent i hears growls with a reliability of 65% and creaks with a reliability of 95%.",
        "Agent j, on the other hand, hears growls with a reliability of 95%.",
        "Thus, the setting is such that agent i hears agent j opening doors more reliably than the tigers growls.",
        "This suggests that i could use js actions as an indication of the location of the tiger, as we discuss below.",
        "Each agents preferences are as in the single agent game discussed in [13].",
        "The transition, observation, and reward functions are shown in [16].",
        "A good indicator of the usefulness of normative methods for decision-making like I-DIDs is the emergence of realistic social behaviors in their prescriptions.",
        "In settings of the persistent multiagent tiger problem that reflect real world situations, we demonstrate followership between the agents and, as shown in [15], deception among agents who believe that they are in a follower-leader type of relationship.",
        "In particular, we analyze the situational and epistemological conditions sufficient for their emergence.",
        "The followership behavior, for example, results from the agent knowing its own weaknesses, assessing the strengths, preferences, and possible behaviors of the other, and realizing that its best for it to follow the others actions in order to maximize its payoffs.",
        "Let us consider a particular setting of the tiger problem in which agent i believes that js preferences are aligned with its own - both of them just want to get the gold - and js hearing is more reliable in comparison to itself.",
        "As an example, suppose that j, on listening can discern the tigers location 95% of the times compared to is 65% accuracy.",
        "Additionally, agent i does not have any initial information about the tigers location.",
        "In other words, is single-level nested belief, bi,1, assigns 0.5 to each of the two locations of the tiger.",
        "In addition, i considers two models of j, which differ in js flat level 0 initial beliefs.",
        "This is represented in the level 1 I-ID shown in Fig. 6(a).",
        "According to one model, j assigns a probability of 0.9 that the tiger is behind the left door, while the other 818 The Sixth Intl.",
        "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Figure 6: (a) Level 1 I-ID of agent i, (b) two level 0 IDs of agent j whose decision nodes are mapped to the chance nodes, A1 j , A2 j , in (a). model assigns 0.1 to that location (see Fig. 6(b)).",
        "Agent i is undecided on these two models of j.",
        "If we vary is hearing ability, and solve the corresponding level 1 I-ID expanded over three time steps, we obtain the normative behavioral policies shown in Fig 7 that exhibit followership behavior.",
        "If is probability of correctly hearing the growls is 0.65, then as shown in the policy in Fig. 7(a), i begins to conditionally follow js actions: i opens the same door that j opened previously iff is own assessment of the tigers location confirms js pick.",
        "If i loses the ability to correctly interpret the growls completely, it blindly follows j and opens the same door that j opened previously (Fig. 7(b)).",
        "Figure 7: Emergence of (a) conditional followership, and (b) blind followership in the tiger problem.",
        "Behaviors of interest are in bold. * is a wildcard, and denotes any one of the observations.",
        "We observed that a single level of belief nesting - beliefs about the others models - was sufficient for followership to emerge in the tiger problem.",
        "However, the epistemological requirements for the emergence of leadership are more complex.",
        "For an agent, say j, to emerge as a leader, followership must first emerge in the other agent i.",
        "As we mentioned previously, if i is certain that its preferences are identical to those of j, and believes that j has a better sense of hearing, i will follow js actions over time.",
        "Agent j emerges as a leader if it believes that i will follow it, which implies that js belief must be nested two levels deep to enable it to recognize its leadership role.",
        "Realizing that i will follow presents j with an opportunity to influence is actions in the benefit of the collective good or its self-interest alone.",
        "For example, in the tiger problem, let us consider a setting in which if both i and j open the correct door, then each gets a payoff of 20 that is double the original.",
        "If j alone selects the correct door, it gets the payoff of 10.",
        "On the other hand, if both agents pick the wrong door, their penalties are cut in half.",
        "In this setting, it is in both js best interest as well as the collective betterment for j to use its expertise in selecting the correct door, and thus be a good leader.",
        "However, consider a slightly different problem in which j gains from is loss and is penalized if i gains.",
        "Specifically, let is payoff be subtracted from js, indicating that j is antagonistic toward i - if j picks the correct door and i the wrong one, then is loss of 100 becomes js gain.",
        "Agent j believes that i incorrectly thinks that js preferences are those that promote the collective good and that it starts off by believing with 99% confidence where the tiger is.",
        "Because i believes that its preferences are similar to those of j, and that j starts by believing almost surely that one of the two is the correct location (two level 0 models of j), i will start by following js actions.",
        "We show is normative policy on solving its singly-nested I-DID over three time steps in Fig. 8(a).",
        "The policy demonstrates that i will blindly follow js actions.",
        "Since the tiger persists in its original location with a probability of 0.95, i will select the same door again.",
        "If j begins the game with a 99% probability that the tiger is on the right, solving js I-DID nested two levels deep, results in the policy shown in Fig. 8(b).",
        "Even though j is almost certain that OL is the correct action, it will start by selecting OR, followed by OL.",
        "Agent js intention is to deceive i who, it believes, will follow js actions, so as to gain $110 in the second time step, which is more than what j would gain if it were to be honest.",
        "Figure 8: Emergence of deception between agents in the tiger problem.",
        "Behaviors of interest are in bold. * denotes as before. (a) Agent is policy demonstrating that it will blindly follow js actions. (b) Even though j is almost certain that the tiger is on the right, it will start by selecting OR, followed by OL, in order to deceive i. 5.2 Altruism and Reciprocity in the Public Good Problem The public good (PG) problem [7], consists of a group of M agents, each of whom must either contribute some resource to a public pot or keep it for themselves.",
        "Since resources contributed to the public pot are shared among all the agents, they are less valuable to the agent when in the public pot.",
        "However, if all agents choose to contribute their resources, then the payoff to each agent is more than if no one contributes.",
        "Since an agent gets its share of the public pot irrespective of whether it has contributed or not, the dominating action is for each agent to not contribute, and instead free ride on others contributions.",
        "However, behaviors of human players in empirical simulations of the PG problem differ from the normative predictions.",
        "The experiments reveal that many players initially contribute a large amount to the public pot, and continue to contribute when the PG problem is played repeatedly, though in decreasing amounts [4].",
        "Many of these experiments [5] report that a small core group of players persistently contributes to the public pot even when all others are defecting.",
        "These experiments also reveal that players who persistently contribute have altruistic or reciprocal preferences matching expected cooperation of others.",
        "For simplicity, we assume that the game is played between M = 2 agents, i and j.",
        "Let each agent be initially endowed with XT amount of resources.",
        "While the classical PG game formulation permits each agent to contribute any quantity of resources (≤ XT ) to the public pot, we simplify the action space by allowing two possible actions.",
        "Each agent may choose to either contribute (C) a fixed amount of the resources, or not contribute.",
        "The latter action is deThe Sixth Intl.",
        "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 819 noted as defect (D).",
        "We assume that the actions are not observable to others.",
        "The value of resources in the public pot is discounted by ci for each agent i, where ci is the marginal private return.",
        "We assume that ci < 1 so that the agent does not benefit enough that it contributes to the public pot for private gain.",
        "Simultaneously, ciM > 1, making collective contribution pareto optimal. i/j C D C 2ciXT , 2cjXT ciXT − cp, XT + cjXT − P D XT + ciXT − P, cjXT − cp XT , XT Table 1: The one-shot PG game with punishment.",
        "In order to encourage contributions, the contributing agents punish free riders but incur a small cost for administering the punishment.",
        "Let P be the punishment meted out to the defecting agent and cp the non-zero cost of punishing for the contributing agent.",
        "For simplicity, we assume that the cost of punishing is same for both the agents.",
        "The one-shot PG game with punishment is shown in Table. 1.",
        "Let ci = cj, cp > 0, and if P > XT − ciXT , then defection is no longer a dominating action.",
        "If P < XT − ciXT , then defection is the dominating action for both.",
        "If P = XT − ciXT , then the game is not dominance-solvable.",
        "Figure 9: (a) Level 1 I-ID of agent i, (b) level 0 IDs of agent j with decision nodes mapped to the chance nodes, A1 j and A2 j , in (a).",
        "We formulate a sequential version of the PG problem with punishment from the perspective of agent i.",
        "Though in the repeated PG game, the quantity in the public pot is revealed to all the agents after each round of actions, we assume in our formulation that it is hidden from the agents.",
        "Each agent may contribute a fixed amount, xc, or defect.",
        "An agent on performing an action receives an observation of plenty (PY) or meager (MR) symbolizing the state of the public pot.",
        "Notice that the observations are also indirectly indicative of agent js actions because the state of the public pot is influenced by them.",
        "The amount of resources in agent is private pot, is perfectly observable to i.",
        "The payoffs are analogous to Table. 1.",
        "Borrowing from the empirical investigations of the PG problem [5], we construct level 0 IDs for j that model altruistic and non-altruistic types (Fig. 9(b)).",
        "Specifically, our altruistic agent has a high marginal private return (cj is close to 1) and does not punish others who defect.",
        "Let xc = 1 and the level 0 agent be punished half the times it defects.",
        "With one action remaining, both types of agents choose to contribute to avoid being punished.",
        "With two actions to go, the altruistic type chooses to contribute, while the other defects.",
        "This is because cj for the altruistic type is close to 1, thus the expected punishment, 0.5P > (1 − cj), which the altruistic type avoids.",
        "Because cj for the non-altruistic type is less, it prefers not to contribute.",
        "With three steps to go, the altruistic agent contributes to avoid punishment (0.5P > 2(1 − cj)), and the non-altruistic type defects.",
        "For greater than three steps, while the altruistic agent continues to contribute to the public pot depending on how close its marginal private return is to 1, the non-altruistic type prescribes defection.",
        "We analyzed the decisions of an altruistic agent i modeled using a level 1 I-DID expanded over 3 time steps. i ascribes the two level 0 models, mentioned previously, to j (see Fig. 9).",
        "If i believes with a probability 1 that j is altruistic, i chooses to contribute for each of the three steps.",
        "This behavior persists when i is unaware of whether j is altruistic (Fig. 10(a)), and when i assigns a high probability to j being the non-altruistic type.",
        "However, when i believes with a probability 1 that j is non-altruistic and will thus surely defect, i chooses to defect to avoid being punished and because its marginal private return is less than 1.",
        "These results demonstrate that the behavior of our altruistic type resembles that found experimentally.",
        "The non-altruistic level 1 agent chooses to defect regardless of how likely it believes the other agent to be altruistic.",
        "We analyzed the behavior of a reciprocal agent type that matches expected cooperation or defection.",
        "The reciprocal types marginal private return is similar to that of the non-altruistic type, however, it obtains a greater payoff when its action is similar to that of the other.",
        "We consider the case when the reciprocal agent i is unsure of whether j is altruistic and believes that the public pot is likely to be half full.",
        "For this prior belief, i chooses to defect.",
        "On receiving an observation of plenty, i decides to contribute, while an observation of meager makes it defect (Fig. 10(b)).",
        "This is because an observation of plenty signals that the pot is likely to be greater than half full, which results from js action to contribute.",
        "Thus, among the two models ascribed to j, its type is likely to be altruistic making it likely that j will contribute again in the next time step.",
        "Agent i therefore chooses to contribute to reciprocate js action.",
        "An analogous reasoning leads i to defect when it observes a meager pot.",
        "With one action to go, i believing that j contributes, will choose to contribute too to avoid punishment regardless of its observations.",
        "Figure 10: (a) An altruistic level 1 agent always contributes. (b) A reciprocal agent i starts off by defecting followed by choosing to contribute or defect based on its observation of plenty (indicating that j is likely altruistic) or meager (j is non-altruistic). 5.3 Strategies in Two-Player Poker Poker is a popular zero sum card game that has received much attention among the AI research community as a testbed [2].",
        "Poker is played among M ≥ 2 players in which each player receives a hand of cards from a deck.",
        "While several flavors of Poker with varying complexity exist, we consider a simple version in which each player has three plys during which the player may either exchange a card (E), keep the existing hand (K), fold (F) and withdraw from the game, or call (C), requiring all players to show their hands.",
        "To keep matters simple, let M = 2, and each player receive a hand consisting of a single card drawn from the same suit.",
        "Thus, during a showdown, the player who has the numerically larger card (2 is the lowest, ace is the highest) wins the pot.",
        "During an exchange of cards, the discarded card is placed either in the L pile, indicating to the other agent that it was a low numbered card less than 8, or in the 820 The Sixth Intl.",
        "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) H pile, indicating that the card had a rank greater than or equal to 8.",
        "Notice that, for example, if a lower numbered card is discarded, the probability of receiving a low card in exchange is now reduced.",
        "We show the level 1 I-ID for the simplified two-player Poker in Fig. 11.",
        "We considered two models (personality types) of agent j.",
        "The conservative type believes that it is likely that its opponent has a high numbered card in its hand.",
        "On the other hand, the aggressive agent j believes with a high probability that its opponent has a lower numbered card.",
        "Thus, the two types differ in their beliefs over their opponents hand.",
        "In both these level 0 models, the opponent is assumed to perform its actions following a fixed, uniform distribution.",
        "With three actions to go, regardless of its hand (unless it is an ace), the aggressive agent chooses to exchange its card, with the intent of improving on its current hand.",
        "This is because it believes the other to have a low card, which improves its chances of getting a high card during the exchange.",
        "The conservative agent chooses to keep its card, no matter its hand because its chances of getting a high card are slim as it believes that its opponent has one.",
        "Figure 11: (a) Level 1 I-ID of agent i.",
        "The observation reveals information about js hand of the previous time step, (b) level 0 IDs of agent j whose decision nodes are mapped to the chance nodes, A1 j , A2 j , in (a).",
        "The policy of a level 1 agent i who believes that each card except its own has an equal likelihood of being in js hand (neutral personality type) and j could be either an aggressive or conservative type, is shown in Fig. 12. is own hand contains the card numbered 8.",
        "The agent starts by keeping its card.",
        "On seeing that j did not exchange a card (N), i believes with probability 1 that j is conservative and hence will keep its cards. i responds by either keeping its card or exchanging it because j is equally likely to have a lower or higher card.",
        "If i observes that j discarded its card into the L or H pile, i believes that j is aggressive.",
        "On observing L, i realizes that j had a low card, and is likely to have a high card after its exchange.",
        "Because the probability of receiving a low card is high now, i chooses to keep its card.",
        "On observing H, believing that the probability of receiving a high numbered card is high, i chooses to exchange its card.",
        "In the final step, i chooses to call regardless of its observation history because its belief that j has a higher card is not sufficiently high to conclude that its better to fold and relinquish the payoff.",
        "This is partly due to the fact that an observation of, say, L resets the agent is previous time step beliefs over js hand to the low numbered cards only. 6.",
        "DISCUSSION We showed how DIDs may be extended to I-DIDs that enable online sequential decision-making in uncertain multiagent settings.",
        "Our graphical representation of I-DIDs improves on the previous Figure 12: A level 1 agent is three step policy in the Poker problem. i starts by believing that j is equally likely to be aggressive or conservative and could have any card in its hand with equal probability. work significantly by being more transparent, semantically clear, and capable of being solved using standard algorithms that target DIDs.",
        "I-DIDs extend NIDs to allow sequential decision-making over multiple time steps in the presence of other interacting agents.",
        "I-DIDs may be seen as concise graphical representations for IPOMDPs providing a way to exploit problem structure and carry out online decision-making as the agent acts and observes given its prior beliefs.",
        "We are currently investigating ways to solve I-DIDs approximately with provable bounds on the solution quality.",
        "Acknowledgment: We thank Piotr Gmytrasiewicz for some useful discussions related to this work.",
        "The first author would like to acknowledge the support of a UGARF grant. 7.",
        "REFERENCES [1] R. J. Aumann.",
        "Interactive epistemology i: Knowledge.",
        "International Journal of Game Theory, 28:263-300, 1999. [2] D. Billings, A. Davidson, J. Schaeffer, and D. Szafron.",
        "The challenge of poker.",
        "AIJ, 2001. [3] A. Brandenburger and E. Dekel.",
        "Hierarchies of beliefs and common knowledge.",
        "Journal of Economic Theory, 59:189-198, 1993. [4] C. Camerer.",
        "Behavioral Game Theory: Experiments in Strategic Interaction.",
        "Princeton University Press, 2003. [5] E. Fehr and S. Gachter.",
        "Cooperation and punishment in public goods experiments.",
        "American Economic Review, 90(4):980-994, 2000. [6] D. Fudenberg and D. K. Levine.",
        "The Theory of Learning in Games.",
        "MIT Press, 1998. [7] D. Fudenberg and J. Tirole.",
        "Game Theory.",
        "MIT Press, 1991. [8] Y. Gal and A. Pfeffer.",
        "A language for modeling agents decision-making processes in games.",
        "In AAMAS, 2003. [9] P. Gmytrasiewicz and P. Doshi.",
        "A framework for sequential planning in multiagent settings.",
        "JAIR, 24:49-79, 2005. [10] P. Gmytrasiewicz and E. Durfee.",
        "Rational coordination in multi-agent environments.",
        "JAAMAS, 3(4):319-350, 2000. [11] J. C. Harsanyi.",
        "Games with incomplete information played by bayesian players.",
        "Management Science, 14(3):159-182, 1967. [12] R. A. Howard and J. E. Matheson.",
        "Influence diagrams.",
        "In R. A. Howard and J. E. Matheson, editors, The Principles and Applications of Decision Analysis.",
        "Strategic Decisions Group, Menlo Park, CA 94025, 1984. [13] L. Kaelbling, M. Littman, and A. Cassandra.",
        "Planning and acting in partially observable stochastic domains.",
        "Artificial Intelligence Journal, 2, 1998. [14] D. Koller and B. Milch.",
        "Multi-agent influence diagrams for representing and solving games.",
        "In IJCAI, pages 1027-1034, 2001. [15] K. Polich and P. Gmytrasiewicz.",
        "Interactive dynamic influence diagrams.",
        "In GTDT Workshop, AAMAS, 2006. [16] B.",
        "Rathnas., P. Doshi, and P. J. Gmytrasiewicz.",
        "Exact solutions to interactive pomdps using behavioral equivalence.",
        "In Autonomous Agents and Multi-Agent Systems Conference (AAMAS), 2006. [17] S. Russell and P. Norvig.",
        "Artificial Intelligence: A Modern Approach (Second Edition).",
        "Prentice Hall, 2003. [18] R. D. Shachter.",
        "Evaluating influence diagrams.",
        "Operations Research, 34(6):871-882, 1986. [19] D. Suryadi and P. Gmytrasiewicz.",
        "Learning models of other agents using influence diagrams.",
        "In UM, 1999.",
        "The Sixth Intl.",
        "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 821"
    ],
    "translated_text_sentences": [
        "Modelos gráficos para soluciones en línea a POMDP interactivos de Prashant Doshi Dept.",
        "Departamento de Ciencias de la Computación Universidad de Georgia Athens, GA 30602, EE. UU. pdoshi@cs.uga.edu Yifeng Zeng Dept. de Ciencias de la Computación Universidad de Aalborg DK-9220 Aalborg, Dinamarca yfzeng@cs.aau.edu Qiongyu Chen Dept. de Ciencias de la Computación Universidad Nacional de Singapur 117543, Singapur chenqy@comp.nus.edu.sg RESUMEN Desarrollamos una nueva representación gráfica para procesos de decisión de Markov parcialmente observables interactivos (I-POMDPs) que es significativamente más transparente y semánticamente clara que la representación anterior.",
        "Estos modelos gráficos llamados diagramas de influencia dinámica interactiva (I-DIDs) buscan modelar explícitamente la estructura que a menudo está presente en problemas del mundo real al descomponer la situación en variables de probabilidad y decisión, y las dependencias entre las variables.",
        "Los I-DIDs generalizan los DIDs, los cuales pueden ser vistos como representaciones gráficas de POMDPs, a entornos multiagentes de la misma manera que los I-POMDPs generalizan los POMDPs.",
        "Los I-DIDs pueden ser utilizados para calcular la política de un agente en línea mientras el agente actúa y observa en un entorno poblado por otros agentes que interactúan entre sí.",
        "Usando varios ejemplos, mostramos cómo se pueden aplicar los I-DIDs y demostramos su utilidad.",
        "Categorías y Descriptores de Asignaturas I.2.11 [Inteligencia Artificial Distribuida]: Sistemas Multiagentes Términos Generales Teoría 1.",
        "Las Procesos de Decisión de Markov Interactivos Parcialmente Observables (IPOMDPs) proporcionan un marco para la toma de decisiones secuenciales en entornos multiagentes parcialmente observables.",
        "Generalizan los POMDPs [13] a entornos multiagentes al incluir los modelos computables de los otros agentes en el espacio de estados junto con los estados del entorno físico.",
        "Los modelos abarcan toda la información que influye en los comportamientos de los agentes, incluyendo sus preferencias, capacidades y creencias, y por lo tanto son análogos a los tipos en los juegos bayesianos [11].",
        "I-POMDPs adoptan un enfoque subjetivo para comprender el comportamiento estratégico, arraigado en un marco de teoría de decisiones que toma la perspectiva de los tomadores de decisiones en la interacción.",
        "En [15], Polich y Gmytrasiewicz introdujeron diagramas de influencia dinámica interactivos (I-DIDs) como las representaciones computacionales de I-POMDPs.",
        "Los I-DIDs generalizan los DIDs [12], los cuales pueden ser vistos como contrapartes computacionales de POMDPs, a entornos de múltiples agentes de la misma manera que los I-POMDPs generalizan los POMDPs.",
        "Los I-DIDs contribuyen a una creciente línea de trabajo [19] que incluye diagramas de influencia multiagente (MAIDs) [14], y más recientemente, redes de diagramas de influencia (NIDs) [8].",
        "Estos formalismos buscan modelar explícitamente la estructura que a menudo está presente en problemas del mundo real al descomponer la situación en variables de azar y decisiones, y las dependencias entre las variables.",
        "Los MAIDs proporcionan una alternativa a las formas normales y extensivas de juego utilizando un formalismo gráfico para representar juegos de información imperfecta con un nodo de decisión para las acciones de cada agente y nodos de azar que capturan la información privada de los agentes.",
        "Los MAIDs analizan objetivamente el juego, calculando eficientemente el perfil de equilibrio de Nash al explotar la estructura de independencia.",
        "NIDs extienden los MAIDs para incluir la incertidumbre de los agentes sobre el juego que se está jugando y sobre los modelos de los otros agentes.",
        "Cada modelo es un MAID y la red de MAIDs se colapsa, de abajo hacia arriba, en un solo MAID para calcular el equilibrio del juego teniendo en cuenta los diferentes modelos de cada agente.",
        "Los formalismos gráficos como MAIDs y NIDs abren una área de investigación prometedora que tiene como objetivo representar las interacciones multiagente de manera más transparente.",
        "Sin embargo, los MAIDs proporcionan un análisis del juego desde un punto de vista externo y la aplicabilidad de ambos está limitada a juegos estáticos de un solo jugador.",
        "Los asuntos son más complejos cuando consideramos interacciones que se extienden en el tiempo, donde las predicciones sobre las acciones futuras de otros deben hacerse utilizando modelos que cambian a medida que los agentes actúan y observan.",
        "Los I-DIDs abordan esta brecha al permitir la representación de modelos de otros agentes como los valores de un nodo de modelo especial.",
        "Tanto los modelos de otros agentes como las creencias originales de los agentes sobre estos modelos se actualizan con el tiempo utilizando implementaciones especializadas.",
        "En este documento, mejoramos la representación preliminar previa del I-DID mostrada en [15] utilizando la idea de que el I-ID estático es un tipo de NID.",
        "Por lo tanto, podemos utilizar construcciones de lenguaje específicas de NID, como multiplexores, para representar de manera más transparente el nodo del modelo y, posteriormente, el I-ID.",
        "Además, aclaramos la semántica del enlace de política de propósito especial introducido en la representación de I-DID por [15], y mostramos que podría ser reemplazado por enlaces de dependencia tradicionales.",
        "En la representación previa del I-DID, la actualización de la creencia de los agentes sobre los modelos de los demás a medida que los agentes actúan y reciben observaciones se denotaba utilizando un enlace especial llamado el enlace de actualización del modelo que conectaba los nodos del modelo a lo largo del tiempo.",
        "Explicamos la semántica de este enlace mostrando cómo puede ser implementado utilizando los enlaces de dependencia tradicionales entre los nodos de probabilidad que constituyen los nodos del modelo.",
        "El resultado final es una representación de I-DID que es significativamente más transparente, semánticamente clara y capaz de ser implementada utilizando los algoritmos estándar para resolver DIDs.",
        "Mostramos cómo los IDIDs pueden ser utilizados para modelar la incertidumbre de un agente sobre los modelos de otros, que a su vez pueden ser I-DIDs.",
        "La solución al I-DID es una política que prescribe lo que el agente debe hacer con el tiempo, dadas sus creencias sobre el estado físico y otros modelos.",
        "Análogos a los DIDs, los I-DIDs pueden ser utilizados para calcular la política de un agente en línea mientras el agente actúa y observa en un entorno poblado por otros agentes interactuantes. 2.",
        "ANTECEDENTES: Los IPOMDPS ANIDADOS FINITAMENTE generalizan los POMDPs a entornos multiagentes al incluir los modelos de otros agentes como parte del espacio de estados [9].",
        "Dado que otros agentes también pueden razonar sobre otros, el espacio de estado interactivo está estratégicamente anidado; contiene creencias sobre los modelos de otros agentes y sus creencias sobre los demás.",
        "Para simplificar la presentación, consideramos un agente, i, que está interactuando con otro agente, j.",
        "Un I-POMDP finitamente anidado del agente i con un nivel de estrategia l se define como la tupla: I-POMDPi,l = ISi,l, A, Ti, Ωi, Oi, Ri donde: • ISi,l denota un conjunto de estados interactivos definido como, ISi,l = S × Mj,l−1, donde Mj,l−1 = {Θj,l−1 ∪ SMj}, para l ≥ 1, e ISi,0 = S, donde S es el conjunto de estados del entorno físico.",
        "Θj,l−1 es el conjunto de modelos intencionales computables del agente j: θj,l−1 = bj,l−1, ˆθj donde el marco, ˆθj = A, Ωj, Tj, Oj, Rj, OCj.",
        "Aquí, j es Bayesiano racional y OCj es el criterio de optimalidad de j.",
        "SMj es el conjunto de modelos subintencionales de j.",
        "Ejemplos simples de modelos subintencionales incluyen un modelo sin información [10] y un modelo de juego ficticio [6], ambos de los cuales son independientes de la historia.",
        "Damos una construcción recursiva de abajo hacia arriba del espacio de estados interactivo a continuación.",
        "Sí,0 = S, Θj,0 = { bj,0, ˆθj | bj,0 ∈ Δ(ISj,0)} Sí,1 = S × {Θj,0 ∪ SMj}, Θj,1 = { bj,1, ˆθj | bj,1 ∈ Δ(ISj,1)} . . . . . .",
        "Sí, l = S × {Θj,l−1 ∪ SMj}, Θj,l = { bj,l, ˆθj | bj,l ∈ Δ(ISj,l)} Formulaciones similares de espacios anidados han aparecido en [1, 3]. • A = Ai × Aj es el conjunto de acciones conjuntas de todos los agentes en el entorno; • Ti : S ×A×S → [0, 1], describe el efecto de las acciones conjuntas en los estados físicos del entorno; • Ωi es el conjunto de observaciones del agente i; • Oi : S × A × Ωi → [0, 1] da la probabilidad de las observaciones dadas el estado físico y la acción conjunta; • Ri : ISi × A → R describe las preferencias del agente sobre sus estados interactivos.",
        "Normalmente solo importarán los estados físicos.",
        "La política del agente es el mapeo, Ω∗ i → Δ(Ai), donde Ω∗ i es el conjunto de todos los historiales de observación del agente i.",
        "Dado que la creencia sobre los estados interactivos forma una estadística suficiente [9], la política también puede ser representada como un mapeo del conjunto de todas las creencias del agente i a una distribución sobre sus acciones, Δ(ISi) → Δ(Ai). 2.1 Actualización de Creencias Análogo a los POMDPs, un agente dentro del marco I-POMDP actualiza su creencia a medida que actúa y observa.",
        "Sin embargo, hay dos diferencias que complican la actualización de creencias en entornos multiagentes en comparación con los entornos de un solo agente.",
        "Primero, dado que el estado del entorno físico depende de las acciones de ambos agentes, la predicción de cómo cambia el estado físico debe hacerse basándose en la predicción de sus acciones.",
        "Segundo, los cambios en los modelos de js deben ser incluidos en la actualización de creencias.",
        "Específicamente, si j es intencional, entonces se debe incluir una actualización de las creencias de j debido a su acción y observación.",
        "En otras palabras, i tiene que actualizar su creencia basándose en su predicción de lo que j observaría y cómo j actualizaría su creencia.",
        "Si el modelo de js es subintencional, entonces las observaciones probables de js se añaden al historial de observaciones contenido en el modelo.",
        "Formalmente, tenemos: Pr(ist |at−1 i , bt−1 i,l ) = β ISt−1:mt−1 j =θt j bt−1 i,l (ist−1 ) × at−1 j Pr(at−1 j |θt−1 j,l−1)Oi(st , at−1 i , at−1 j , ot i) ×Ti(st−1 , at−1 i , at−1 j , st ) ot j Oj(st , at−1 i , at−1 j , ot j) ×τ(SEθt j (bt−1 j,l−1, at−1 j , ot j) − bt j,l−1) (1) donde β es la constante de normalización, τ es 1 si su argumento es 0, de lo contrario es 0, Pr(at−1 j |θt−1 j,l−1) es la probabilidad de que at−1 j sea Bayes racional para el agente descrito por el modelo θt−1 j,l−1, y SE(·) es una abreviatura para la actualización de creencias.",
        "Para una versión de la actualización de creencias cuando el modelo js es subintencional, consulte [9].",
        "Si el agente j también se modela como un I-POMDP, entonces la actualización de creencias invoca la actualización de creencias de j (a través del término SEθt j ( bt−1 j,l−1 , at−1 j , ot j)), lo que a su vez podría invocar la actualización de creencias de is y así sucesivamente.",
        "Esta recursión en la anidación de creencias llega al nivel 0.",
        "En este nivel, la actualización de creencias del agente se reduce a una actualización de creencias de un POMDP. Para ilustraciones de la actualización de creencias, detalles adicionales sobre I-POMDPs y cómo se comparan con otros marcos multiagentes, consulte [9]. Iteración de Valor Cada estado de creencia en un I-POMDP finitamente anidado tiene un valor asociado que refleja la máxima ganancia que el agente puede esperar en este estado de creencia: Un ( bi,l, θi ) = max ai∈Ai is∈ISi,l ERi(is, ai)bi,l(is)+ γ oi∈Ωi Pr(oi|ai, bi,l)Un−1 ( SEθi (bi,l, ai, oi), θi ) (2) donde, ERi(is, ai) = aj Ri(is, ai, aj)Pr(aj|mj,l−1) (ya que is = (s, mj,l−1)).",
        "La ecuación 2 es una base para la iteración de valor en I-POMDPs.",
        "La acción óptima del agente, a∗ i, para el caso de horizonte finito con descuento, es un elemento del conjunto de acciones óptimas para el estado de creencia, OPT(θi), definido como: OPT( bi,l, θi ) = argmax ai∈Ai is∈ISi,l ERi(is, ai)bi,l(is) +γ oi∈Ωi Pr(oi|ai, bi,l)Un ( SEθi (bi,l, ai, oi), θi ) (3) 3.",
        "Diagramas de influencia interactivos. Una extensión ingenua de los diagramas de influencia (IDs) a entornos poblados por múltiples agentes es posible tratando a los otros agentes como autómatas, representados mediante nodos de probabilidad.",
        "Sin embargo, este enfoque asume que las acciones de los agentes están controladas utilizando una distribución de probabilidad que no cambia con el tiempo.",
        "Los diagramas de influencia interactivos (I-IDs) adoptan un enfoque más sofisticado al generalizar los IDs para hacerlos aplicables a entornos compartidos con otros agentes que pueden actuar, observar y actualizar sus creencias. 3.1 Sintaxis Además de los nodos habituales de probabilidad, decisión y utilidad, los IIDs incluyen un nuevo tipo de nodo llamado nodo de modelo.",
        "Mostramos un nivel general l I-ID en la Fig. 1(a), donde el nodo del modelo (Mj,l−1) está representado con un hexágono.",
        "Observamos que la distribución de probabilidad sobre el nodo de probabilidad, S, y el nodo del modelo juntos representan la creencia del agente sobre sus estados interactivos.",
        "Además del modelo 1, el modelo de nivel 0 es un POMDP: las acciones de otros agentes se tratan como eventos exógenos y se incorporan en las funciones T, O y R.",
        "El Sexto Internacional.",
        "Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) Figura 1: (a) Un nivel genérico l I-ID para el agente i situado con otro agente j.",
        "El hexágono es el nodo modelo (Mj,l−1) cuya estructura mostramos en (b).",
        "Los miembros del nodo modelo son ellos mismos I-ID (m1 j,l−1, m2 j,l−1; los diagramas no se muestran aquí por simplicidad) cuyos nodos de decisión se asignan a los nodos de probabilidad correspondientes (A1 j , A2 j).",
        "Dependiendo del valor del nodo, Mod[Mj], la distribución de cada uno de los nodos de probabilidad se asigna al nodo Aj. (c) El I-ID transformado con el nodo del modelo reemplazado por los nodos de probabilidad y las relaciones entre ellos. Los I-IDs difieren de los IDs al tener un enlace discontinuo (llamado enlace de política en [15]) entre el nodo del modelo y un nodo de probabilidad, Aj, que representa la distribución sobre las acciones de los otros agentes dada su modelo.",
        "En ausencia de otros agentes, el nodo del modelo y el nodo de probabilidad, Aj, desaparecen y los I-ID se convierten en ID tradicionales.",
        "El nodo del modelo contiene los modelos computacionales alternativos atribuidos por i al otro agente del conjunto, Θj,l−1 ∪ SMj, donde Θj,l−1 y SMj fueron definidos previamente en la Sección 2.",
        "Por lo tanto, un modelo en el nodo del modelo puede ser en sí mismo un I-ID o ID, y la recursión termina cuando un modelo es un ID o subintencional.",
        "Dado que el nodo del modelo contiene los modelos alternativos del otro agente como sus valores, su representación no es trivial.",
        "En particular, algunos de los modelos dentro del nodo son I-IDs que, al resolverse, generan la política óptima de los agentes en sus nodos de decisión.",
        "Cada nodo de decisión se asigna al nodo de probabilidad correspondiente, digamos A1 j, de la siguiente manera: si OPT es el conjunto de acciones óptimas obtenidas al resolver el I-ID (o ID), entonces Pr(aj ∈ A1 j) = 1 |OPT| si aj ∈ OPT, 0 en caso contrario.",
        "Tomando prestados los conocimientos de trabajos anteriores [8], observamos que el nodo del modelo y el enlace de política discontinua que lo conecta con el nodo de probabilidad, Aj, podrían representarse como se muestra en la Fig. 1(b).",
        "El nodo de decisión de cada nivel l − 1 I-ID se transforma en un nodo de probabilidad, como mencionamos anteriormente, de modo que las acciones con el mayor valor en el nodo de decisión se les asignan probabilidades uniformes en el nodo de probabilidad, mientras que al resto se les asigna probabilidad cero.",
        "Los diferentes nodos de probabilidad (A1 j , A2 j ), uno para cada modelo, y adicionalmente, el nodo de probabilidad etiquetado Mod[Mj] forman los padres del nodo de probabilidad, Aj.",
        "Por lo tanto, hay tantos nodos de acción (A1 j , A2 j ) en Mj,l−1 como el número de modelos en el soporte de las creencias del agente.",
        "La tabla de probabilidad condicional del nodo de probabilidad, Aj, es un multiplexor que asume la distribución de cada uno de los nodos de acción (A1 j , A2 j ) dependiendo del valor de Mod[Mj].",
        "Los valores de Mod[Mj] denotan los diferentes modelos de j.",
        "En otras palabras, cuando Mod[Mj] tiene el valor m1 j,l−1, el nodo de probabilidad Aj asume la distribución del nodo A1 j, y Aj asume la distribución de A2 j cuando Mod[Mj] tiene el valor m2 j,l−1.",
        "La distribución sobre el nodo, Mod[Mj], es la creencia del agente sobre los modelos de j dado un estado físico.",
        "Para más agentes, tendremos tantos nodos de modelo como agentes haya.",
        "Observa que la Fig. 1(b) aclara la semántica del enlace de política y muestra cómo puede ser representado utilizando los enlaces de dependencia tradicionales.",
        "En la Fig. 1(c), mostramos el I-ID transformado cuando el nodo del modelo es reemplazado por los nodos de probabilidad y las relaciones entre ellos.",
        "A diferencia de la representación en [15], no hay enlaces de políticas especiales, sino que el I-ID está compuesto únicamente por aquellos tipos de nodos que se encuentran en IDs tradicionales y relaciones de dependencia entre los nodos.",
        "Esto permite que los I-IDs sean representados e implementados utilizando herramientas de aplicación convencionales que apuntan a los IDs.",
        "Ten en cuenta que podemos ver el nivel l I-ID como un NID.",
        "Específicamente, cada uno de los modelos del nivel l − 1 dentro del nodo del modelo son bloques en el NID (ver Fig. 2).",
        "Si el nivel l = 1, cada bloque es un ID tradicional, de lo contrario, si l > 1, cada bloque dentro del NID puede ser un NID en sí mismo.",
        "Ten en cuenta que dentro de los I-IDs (o IDs) en cada nivel, solo hay un nodo de decisión único.",
        "Por lo tanto, nuestro NID no contiene ningún MAID.",
        "Figura 2: Un nivel l I-ID representado como un NID.",
        "Las probabilidades asignadas a los bloques del NID son creencias sobre modelos js condicionados a un estado físico. 3.2 Solución La solución de un I-ID procede de manera ascendente y se implementa de forma recursiva.",
        "Comenzamos resolviendo los modelos de nivel 0, que, si son intencionales, son identificadores tradicionales.",
        "Sus soluciones proporcionan distribuciones de probabilidad sobre las acciones de los otros agentes, las cuales se ingresan en los nodos de probabilidad correspondientes encontrados en el nodo del modelo del nivel 1 I-ID.",
        "El mapeo de los nodos de decisión de los modelos de nivel 0 a los nodos de probabilidad se realiza de manera que las acciones con el mayor valor en el nodo de decisión se les asignan probabilidades uniformes en el nodo de probabilidad, mientras que el resto se les asigna probabilidad cero.",
        "Dadas las distribuciones sobre las acciones dentro de los diferentes nodos de probabilidad (uno para cada modelo del otro agente), el I-ID de nivel 1 se transforma como se muestra en la Fig. 1(c).",
        "Durante la transformación, la tabla de probabilidad condicional (CPT) del nodo, Aj, se llena de tal manera que el nodo asume la distribución de cada uno de los nodos de probabilidad dependiendo del valor del nodo, Mod[Mj].",
        "Como mencionamos anteriormente, los valores del nodo Mod[Mj] denotan los diferentes modelos del otro agente, y su distribución es la creencia del agente sobre los modelos de j condicionados al estado físico.",
        "El nivel 1 I-ID transformado es un ID tradicional que puede ser resuelto como us816 The Sixth Intl.",
        "Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) (a) (b) Figura 3: (a) Un I-DID genérico de dos niveles de tiempo para el agente i en un entorno con otro agente j.",
        "Observa el enlace de actualización del modelo punteado que denota la actualización de los modelos de j y la distribución de los modelos a lo largo del tiempo. (b) La semántica del enlace de actualización del modelo utilizando el método estándar de maximización de utilidad esperada [18].",
        "Este procedimiento se lleva a cabo hasta el nivel l I-ID cuya solución proporciona el conjunto no vacío de acciones óptimas que el agente debe realizar dada su creencia.",
        "Ten en cuenta que, al igual que los IDs, los I-IDs son adecuados para la toma de decisiones en línea cuando se conoce la creencia actual de los agentes. 4.",
        "Diagramas de influencia dinámica interactivos (I-DIDs) Los diagramas de influencia dinámica interactivos (I-DIDs) extienden los I-IDs (y NIDs) para permitir la toma de decisiones secuencial a lo largo de varios pasos de tiempo.",
        "Así como los DIDs son representaciones gráficas estructuradas de POMDPs, los I-DIDs son los análogos gráficos en línea para I-POMDPs finitamente anidados.",
        "Los I-DIDs pueden ser utilizados para optimizar sobre una mirada anticipada finita dadas las creencias iniciales mientras se interactúa con otros agentes, posiblemente similares. 4.1 Sintaxis Representamos un I-DID de dos cortes temporales en la Fig. 3(a).",
        "Además de los nodos del modelo y el enlace de política discontinuo, lo que diferencia a un I-DID de un DID es el enlace de actualización del modelo mostrado como una flecha punteada en la Fig. 3(a).",
        "Explicamos la semántica del nodo del modelo y el enlace de la política en la sección anterior; a continuación, describimos las actualizaciones del modelo.",
        "La actualización del nodo del modelo con el tiempo implica dos pasos: primero, dados los modelos en el tiempo t, identificamos el conjunto actualizado de modelos que residen en el nodo del modelo en el tiempo t + 1.",
        "Recuerda de la Sección 2 que el modelo intencional de un agente incluye sus creencias.",
        "Debido a que los agentes actúan y reciben observaciones, sus modelos se actualizan para reflejar sus creencias cambiadas.",
        "Dado que el conjunto de acciones óptimas para un modelo podría incluir todas las acciones, y el agente podría recibir cualquiera de las |Ωj| posibles observaciones, el conjunto actualizado en el paso de tiempo t + 1 tendrá como máximo |Mt j,l−1||Aj||Ωj| modelos.",
        "Aquí, |Mt j,l−1| es el número de modelos en el paso de tiempo t, |Aj| y |Ωj| son los espacios más grandes de acciones y observaciones respectivamente, entre todos los modelos.",
        "Segundo, calculamos la nueva distribución sobre los modelos actualizados dados la distribución original y la probabilidad de que el agente realice la acción y reciba la observación que llevó al modelo actualizado.",
        "Estos pasos son parte de la actualización de creencias del agente formalizada utilizando la Ecuación 1.",
        "En la Fig. 3(b), mostramos cómo se implementa el enlace de actualización del modelo punteado en el I-DID.",
        "Si cada uno de los dos modelos de nivel l − 1 atribuidos a j en el paso de tiempo t resulta en una acción, y j podría hacer una de dos posibles observaciones, entonces el nodo del modelo en el paso de tiempo t + 1 contiene cuatro modelos actualizados (mt+1,1 j,l−1 ,mt+1,2 j,l−1 , mt+1,3 j,l−1 , y mt+1,4 j,l−1 ).",
        "Estos modelos difieren en sus creencias iniciales, cada una de las cuales es el resultado de j actualizar sus creencias debido a su acción y una posible observación.",
        "Los nodos de decisión en cada uno de los I-DIDs o DIDs que representan los modelos de nivel inferior se asignan a la Figura 4 correspondiente: I-DID transformado con los nodos del modelo y el enlace de actualización del modelo reemplazados por los nodos de probabilidad y las relaciones (en negrita) de los nodos de probabilidad, como se mencionó anteriormente.",
        "A continuación, describimos cómo se calcula la distribución sobre el conjunto actualizado de modelos (la distribución sobre el nodo de probabilidad Mod[Mt+1 j ] en Mt+1 j,l−1).",
        "La probabilidad de que el modelo actualizado js sea, digamos mt+1,1 j,l−1, depende de la probabilidad de que j realice la acción y reciba la observación que condujo a este modelo, y de la distribución previa sobre los modelos en el paso de tiempo t. Dado que el nodo de probabilidad At j asume la distribución de cada uno de los nodos de acción basados en el valor de Mod[Mt j], la probabilidad de la acción está dada por este nodo de probabilidad.",
        "Para obtener la probabilidad de una observación posible js, introducimos el nodo de probabilidad Oj, que dependiendo del valor de Mod[Mt j], asume la distribución del nodo de observación en el modelo de nivel inferior denominado Mod[Mt j].",
        "Dado que la probabilidad de las observaciones js depende del estado físico y de las acciones conjuntas de ambos agentes, el nodo Oj está vinculado con St+1, At j y At i. De manera análoga a At j, la tabla de probabilidad condicional de Oj también es un multiplexor modulado por Mod[Mt j].",
        "Finalmente, la distribución de los modelos previos en el tiempo t se obtiene a partir del nodo de probabilidad, Mod[Mt j ] en Mt j,l−1.",
        "Por consiguiente, los nodos de probabilidad, Mod[Mt j], At j y Oj, forman los padres de Mod[Mt+1 j] en Mt+1 j,l−1.",
        "Ten en cuenta que el enlace de actualización del modelo puede ser reemplazado por los enlaces de dependencia entre los nodos de probabilidad que constituyen los nodos del modelo en las dos etapas temporales.",
        "En la Fig. 4 mostramos los dos cortes temporales I-DID con los nodos del modelo reemplazados por los nodos de probabilidad y las relaciones entre ellos.",
        "Los nodos de probabilidad y los enlaces de dependencia que no están en negrita son estándar, generalmente encontrados en DIDs.",
        "La expansión del I-DID a lo largo de más pasos de tiempo requiere la repetición de los dos pasos de actualizar el conjunto de modelos que forman el 2. Nota que Oj representa la observación j en el tiempo t + 1.",
        "El Sexto Internacional.",
        "Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 817 valores del nodo del modelo y añadiendo las relaciones entre los nodos de probabilidad, tantas veces como enlaces de actualización del modelo haya.",
        "Observamos que el conjunto posible de modelos del otro agente j crece exponencialmente con el número de pasos de tiempo.",
        "Por ejemplo, después de T pasos, puede haber como máximo |Mt=1 j,l−1|(|Aj||Ωj|)T −1 modelos candidatos residiendo en el nodo del modelo. 4.2 Solución Análoga a los I-ID, la solución a un I-DID de nivel l para el agente i expandido a lo largo de T pasos de tiempo puede llevarse a cabo de forma recursiva.",
        "Con el propósito de ilustración, dejemos l=1 y T=2.",
        "El método de solución utiliza la técnica estándar de anticipación, proyectando las secuencias de acciones y observaciones de los agentes hacia adelante desde el estado de creencia actual [17], y encontrando las posibles creencias que podría tener en el siguiente paso temporal.",
        "Dado que el agente i también tiene una creencia sobre los modelos de j, la búsqueda anticipada incluye descubrir los posibles modelos que j podría tener en el futuro.",
        "Por consiguiente, cada uno de los modelos subintencionales o de nivel 0 de js (representados utilizando un DID estándar) en el primer paso de tiempo debe resolverse para obtener su conjunto óptimo de acciones.",
        "Estas acciones se combinan con el conjunto de observaciones posibles que j podría hacer en ese modelo, lo que resulta en un conjunto actualizado de modelos candidatos (que incluyen las creencias actualizadas) que podrían describir el comportamiento de j. Las creencias sobre este conjunto actualizado de modelos candidatos se calculan utilizando los métodos estándar de inferencia que utilizan las relaciones de dependencia entre los nodos del modelo, como se muestra en la Figura 3(b).",
        "Observamos la naturaleza recursiva de esta solución: al resolver el agente de nivel 1 I-DID, los DIDs de nivel 0 deben ser resueltos.",
        "Si el anidamiento de los modelos es más profundo, todos los modelos en todos los niveles, comenzando desde el nivel 0, se resuelven de manera ascendente.",
        "Breve descripción del algoritmo recursivo para resolver el agente es el Algoritmo para resolver I-DID. Entrada: nivel l ≥ 1 I-ID o nivel 0 ID, T Fase de Expansión 1.",
        "Para t desde 1 hasta T − 1, hacer 2.",
        "Si l ≥ 1, entonces llenar Mt+1 j,l−1 3.",
        "Para cada mt j en el rango (Mt j,l−1) hacer 4.",
        "Llame recursivamente al algoritmo con el l - 1 I-ID (o ID) que representa mt j y el horizonte, T - t + 1 5.",
        "Mapea el nodo de decisión del I-ID (o ID) resuelto, OPT(mt j), a un nodo de probabilidad Aj 6.",
        "Para cada aj en OPT(mt j) hacer 7.",
        "Para cada oj en Oj (parte de mt j) hacer 8.",
        "Actualizar la creencia js, bt+1 j ← SE(bt j, aj, oj) 9. mt+1 j ← Nuevo I-ID (o ID) con bt+1 j como la creencia inicial 10.",
        "Rango(Mt+1 j,l−1) ∪ ← {mt+1 j } 11.",
        "Añadir el nodo del modelo, Mt+1 j,l−1, y los enlaces de dependencia entre Mt j,l−1 y Mt+1 j,l−1 (mostrados en la Fig. 3(b)) 12.",
        "Agregue los nodos de probabilidad, decisión y utilidad para la franja de tiempo t + 1 y los enlaces de dependencia entre ellos 13.",
        "Establezca las CPTs para cada nodo de probabilidad y nodo de utilidad de la Fase de Mirada Adelante 14.",
        "Aplica el método estándar de anticipación y respaldo para resolver la Figura 5 expandida de I-DID: Algoritmo para resolver un nivel l ≥ 0 I-DID. Nivel l I-DID expandido durante T pasos de tiempo con otro agente j en la Figura 5.",
        "Adoptamos un enfoque de dos fases: Dado un I-ID de nivel l (descrito previamente en la Sección 3) con todos los modelos de niveles inferiores representados también como I-IDs o IDs (si es nivel 0), el primer paso es expandir el I-ID de nivel l a lo largo de T pasos de tiempo añadiendo los enlaces de dependencia y las tablas de probabilidad condicional para cada nodo.",
        "Nos enfocamos especialmente en establecer y poblar los nodos del modelo (líneas 3-11).",
        "Ten en cuenta que Range(·) devuelve los valores (modelos de nivel inferior) de la variable aleatoria dada como entrada (nodo del modelo).",
        "En la segunda fase, utilizamos una técnica estándar de anticipación proyectando las secuencias de acción y observación durante T pasos de tiempo en el futuro, y respaldando los valores de utilidad de las creencias alcanzables.",
        "Similar a los I-IDs, los I-DIDs se reducen a DIDs en ausencia de otros agentes.",
        "Como mencionamos anteriormente, los modelos de nivel 0 son los DIDs tradicionales.",
        "Sus soluciones proporcionan distribuciones de probabilidad sobre las acciones del agente modelado en ese nivel a los I-DIDs en el nivel 1.",
        "Dadas las distribuciones de probabilidad sobre las acciones de otros agentes, los IDIDs de nivel 1 pueden resolverse como DIDs y proporcionar distribuciones de probabilidad a modelos de niveles superiores.",
        "Suponga que el número de modelos considerados en cada nivel está limitado por un número, M. Resolver un I-DID de nivel l es equivalente a resolver O(Ml) DIDs.",
        "Para ilustrar la utilidad de los I-DIDs, los aplicamos a tres dominios de problemas.",
        "Describimos, en particular, la formulación del I-DID y las prescripciones óptimas obtenidas al resolverlo. 5.1 Seguimiento-Liderazgo en el Problema del Tigre Multiagente Comenzamos nuestras ilustraciones del uso de I-IDs e I-DIDs con una versión ligeramente modificada del problema del tigre multiagente discutido en [9].",
        "El problema tiene dos agentes, cada uno de los cuales puede abrir la puerta derecha (OR), la puerta izquierda (OL) o escuchar (L).",
        "Además de escuchar gruñidos (desde la izquierda (GL) o desde la derecha (GR)) cuando escuchan, los agentes también escuchan chirridos (desde la izquierda (CL), desde la derecha (CR) o sin chirridos (S)), que indican ruidosamente que los otros agentes están abriendo una de las puertas.",
        "Cuando se abre cualquier puerta, el tigre persiste en su ubicación original con una probabilidad del 95%.",
        "El agente i escucha gruñidos con una fiabilidad del 65% y crujidos con una fiabilidad del 95%.",
        "El agente J, por otro lado, escucha gruñidos con una fiabilidad del 95%.",
        "Por lo tanto, la configuración es tal que el agente i escucha la apertura de puertas del agente j de manera más confiable que los rugidos de los tigres.",
        "Esto sugiere que podría usar acciones de JavaScript como indicación de la ubicación del tigre, como discutimos a continuación.",
        "Las preferencias de cada agente son como en el juego de un solo agente discutido en [13].",
        "Las funciones de transición, observación y recompensa se muestran en [16].",
        "Un buen indicador de la utilidad de los métodos normativos para la toma de decisiones como los I-DIDs es la aparición de comportamientos sociales realistas en sus prescripciones.",
        "En entornos del persistente problema del tigre multiagente que reflejan situaciones del mundo real, demostramos la seguidismo entre los agentes y, como se muestra en [15], el engaño entre agentes que creen que están en una relación de tipo seguidor-líder.",
        "En particular, analizamos las condiciones situacionales y epistemológicas suficientes para su surgimiento.",
        "El comportamiento de seguidores, por ejemplo, resulta del agente conociendo sus propias debilidades, evaluando las fortalezas, preferencias y posibles comportamientos del otro, y dándose cuenta de que es mejor para él seguir las acciones de los demás para maximizar sus ganancias.",
        "Consideremos una configuración particular del problema del tigre en la que el agente i cree que las preferencias de j están alineadas con las suyas: ambos solo quieren obtener el oro, y la audición de j es más confiable en comparación consigo mismo.",
        "Como ejemplo, supongamos que j, al escuchar, puede discernir la ubicación del tigre el 95% de las veces en comparación con su precisión del 65%.",
        "Además, el agente i no tiene ninguna información inicial sobre la ubicación de los tigres.",
        "En otras palabras, la creencia anidada de un solo nivel, bi,1, asigna 0.5 a cada una de las dos ubicaciones del tigre.",
        "Además, considera dos modelos de j, que difieren en los niveles iniciales de creencias planas de j.",
        "Esto se representa en el nivel 1 I-ID mostrado en la Fig. 6(a).",
        "Según un modelo, j asigna una probabilidad de 0.9 a que el tigre esté detrás de la puerta izquierda, mientras que el otro 818 The Sixth Intl.",
        "Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) Figura 6: (a) Nivel 1 I-ID del agente i, (b) dos IDs de nivel 0 del agente j cuyos nodos de decisión están mapeados a los nodos de probabilidad, A1 j , A2 j , en (a). El modelo asigna 0.1 a esa ubicación (ver Fig. 6(b)).",
        "El agente i está indeciso sobre estos dos modelos de j.",
        "Si variamos la capacidad auditiva, y resolvemos el nivel 1 correspondiente I-ID expandido en tres pasos de tiempo, obtenemos las políticas de comportamiento normativas mostradas en la Figura 7 que exhiben comportamiento de seguidores.",
        "Si la probabilidad de escuchar correctamente los gruñidos es de 0.65, entonces, como se muestra en la política en la Fig. 7(a), i comienza a seguir condicionalmente las acciones de j: i abre la misma puerta que j abrió anteriormente si su evaluación propia de la ubicación de los tigres confirma la elección de j.",
        "Si i pierde la capacidad de interpretar correctamente los gruñidos por completo, sigue ciegamente a j y abre la misma puerta que j abrió anteriormente (Fig. 7(b)).",
        "Figura 7: Emergencia de (a) seguidores condicionales, y (b) seguidores ciegos en el problema del tigre.",
        "Los comportamientos de interés están en negrita. * es un comodín y representa cualquiera de las observaciones.",
        "Observamos que un solo nivel de anidación de creencias, creencias sobre los modelos de los demás, fue suficiente para que surgiera el seguidismo en el problema del tigre.",
        "Sin embargo, los requisitos epistemológicos para el surgimiento del liderazgo son más complejos.",
        "Para que un agente, digamos j, surja como líder, primero debe surgir el seguidismo en el otro agente i.",
        "Como mencionamos anteriormente, si i está seguro de que sus preferencias son idénticas a las de j, y cree que j tiene un mejor sentido del oído, i seguirá las acciones de j con el tiempo.",
        "El agente j emerge como líder si cree que lo seguiré, lo que implica que la creencia de j debe estar anidada dos niveles de profundidad para permitirle reconocer su papel de liderazgo.",
        "Darse cuenta de que lo seguiré presenta a J una oportunidad para influir en sus acciones en beneficio del bien colectivo o de su interés propio solamente.",
        "Por ejemplo, en el problema del tigre, consideremos una situación en la que si tanto i como j abren la puerta correcta, entonces cada uno recibe un pago de 20 que es el doble del original.",
        "Si solo j selecciona la puerta correcta, obtiene la recompensa de 10.",
        "Por otro lado, si ambos agentes eligen la puerta incorrecta, sus penalizaciones se reducen a la mitad.",
        "En este entorno, es tanto en el mejor interés de j como en el beneficio colectivo que utilice su experiencia para seleccionar la puerta correcta, y así ser un buen líder.",
        "Sin embargo, considera un problema ligeramente diferente en el que j se beneficia de la pérdida de i y es penalizado si i se beneficia.",
        "Específicamente, dejemos que la ganancia se reste de js, indicando que j es antagonista hacia i: si j elige la puerta correcta y i la incorrecta, entonces la pérdida de 100 se convierte en la ganancia de js.",
        "El agente J cree que yo incorrectamente pienso que las preferencias de J son aquellas que promueven el bien colectivo y que comienza creyendo con un 99% de confianza dónde está el tigre.",
        "Dado que creo que mis preferencias son similares a las de j, y que j comienza creyendo casi con certeza que una de las dos ubicaciones es la correcta (dos modelos de nivel 0 de j), comenzaré siguiendo las acciones de j.",
        "Mostramos la política normativa para resolver su I-DID anidado individualmente durante tres pasos de tiempo en la Fig. 8(a).",
        "La política demuestra que seguiré ciegamente las acciones de js.",
        "Dado que el tigre persiste en su ubicación original con una probabilidad del 0.95, seleccionaré nuevamente la misma puerta.",
        "Si j comienza el juego con una probabilidad del 99% de que el tigre esté a la derecha, resolver su I-DID anidado dos niveles profundamente, resulta en la política mostrada en la Fig. 8(b).",
        "Aunque j está casi seguro de que OL es la acción correcta, comenzará seleccionando OR, seguido de OL.",
        "La intención del agente js es engañar a i, a quien cree que seguirá las acciones de js, para así obtener $110 en el segundo paso de tiempo, lo cual es más de lo que j ganaría si fuera honesto.",
        "Figura 8: Emergencia del engaño entre agentes en el problema del tigre.",
        "Los comportamientos de interés están en negrita. * denota como antes. (a) El agente es una política que demuestra que seguirá ciegamente las acciones de js. (b) Aunque j está casi seguro de que el tigre está a la derecha, comenzará seleccionando OR, seguido de OL, para engañar a i. 5.2 Altruismo y reciprocidad en el problema del bien público El problema del bien público (PG) [7], consiste en un grupo de M agentes, cada uno de los cuales debe contribuir con algún recurso a una olla pública o guardarlo para sí mismos.",
        "Dado que los recursos aportados al fondo público se comparten entre todos los agentes, son menos valiosos para el agente cuando están en el fondo público.",
        "Sin embargo, si todos los agentes eligen contribuir con sus recursos, entonces la recompensa para cada agente es mayor que si nadie contribuye.",
        "Dado que un agente recibe su parte del bote público independientemente de si ha contribuido o no, la acción dominante es que cada agente no contribuya y en su lugar se aproveche de las contribuciones de los demás.",
        "Sin embargo, los comportamientos de los jugadores humanos en simulaciones empíricas del problema de PG difieren de las predicciones normativas.",
        "Los experimentos revelan que muchos jugadores inicialmente contribuyen con una gran cantidad al bote público, y continúan contribuyendo cuando se juega el problema del PG repetidamente, aunque en cantidades decrecientes [4].",
        "Muchos de estos experimentos [5] informan que un pequeño grupo central de jugadores contribuye persistentemente al bote público incluso cuando todos los demás están desertando.",
        "Estos experimentos también revelan que los jugadores que contribuyen persistentemente tienen preferencias altruistas o recíprocas que coinciden con la cooperación esperada de los demás.",
        "Para simplificar, asumimos que el juego se juega entre M = 2 agentes, i y j.",
        "Que cada agente esté inicialmente dotado con una cantidad de recursos XT.",
        "Mientras que la formulación clásica del juego PG permite que cada agente contribuya con cualquier cantidad de recursos (≤ XT) al bote público, simplificamos el espacio de acciones al permitir dos acciones posibles.",
        "Cada agente puede elegir contribuir (C) una cantidad fija de recursos, o no contribuir.",
        "La última acción es deThe Sixth Intl.",
        "La Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 819 fue señalada como defectuosa (D).",
        "Suponemos que las acciones no son observables por otros.",
        "El valor de los recursos en la bolsa pública se descuenta por ci para cada agente i, donde ci es el retorno privado marginal.",
        "Suponemos que ci < 1 para que el agente no se beneficie lo suficiente como para contribuir al bote público en beneficio propio.",
        "Simultáneamente, ciM > 1, lo que hace que la contribución colectiva sea óptima de Pareto. i/j C D C 2ciXT , 2cjXT ciXT − cp, XT + cjXT − P D XT + ciXT − P, cjXT − cp XT , XT Tabla 1: El juego PG de un solo disparo con castigo.",
        "Para fomentar las contribuciones, los agentes contribuyentes castigan a los que no colaboran pero incurren en un pequeño costo por administrar el castigo.",
        "Sea P la sanción impuesta al agente que se desvía y cp el costo no nulo de castigar al agente contribuyente.",
        "Para simplificar, asumimos que el costo de castigar es el mismo para ambos agentes.",
        "El juego PG de un solo disparo con castigo se muestra en la Tabla 1.",
        "Si ci = cj, cp > 0, y si P > XT − ciXT, entonces la deserción ya no es una acción dominante.",
        "Si P < XT − ciXT, entonces la deserción es la acción dominante para ambos.",
        "Si P = XT − ciXT, entonces el juego no es resoluble por dominancia.",
        "Figura 9: (a) Nivel 1 I-ID del agente i, (b) IDs de nivel 0 del agente j con nodos de decisión mapeados a los nodos de probabilidad, A1 j y A2 j, en (a).",
        "Formulamos una versión secuencial del problema PG con castigo desde la perspectiva del agente i.",
        "Aunque en el juego de PG repetido, la cantidad en la olla pública se revela a todos los agentes después de cada ronda de acciones, asumimos en nuestra formulación que está oculta para los agentes.",
        "Cada agente puede contribuir con una cantidad fija, xc, o desertar.",
        "Un agente al realizar una acción recibe una observación de abundante (PY) o escasa (MR) simbolizando el estado de la olla pública.",
        "Ten en cuenta que las observaciones también son indirectamente indicativas de las acciones del agente js porque el estado del bote público es influenciado por ellas.",
        "La cantidad de recursos en el agente es una olla privada, es perfectamente observable para i.",
        "Los pagos son análogos a la Tabla 1.",
        "Tomando prestado de las investigaciones empíricas del problema PG [5], construimos IDs de nivel 0 para j que modelan tipos altruistas y no altruistas (Fig. 9(b)).",
        "Específicamente, nuestro agente altruista tiene un alto retorno privado marginal (cj está cerca de 1) y no castiga a los demás que incumplen.",
        "Que xc = 1 y el agente de nivel 0 sea castigado la mitad de las veces que se desvía.",
        "Con una acción restante, ambos tipos de agentes eligen contribuir para evitar ser castigados.",
        "Con dos acciones por delante, el tipo altruista elige contribuir, mientras que el otro defecta.",
        "Esto se debe a que cj para el tipo altruista es cercano a 1, por lo tanto, el castigo esperado, 0.5P > (1 − cj), que el tipo altruista evita.",
        "Debido a que cj para el tipo no altruista es menor, prefiere no contribuir.",
        "Con tres pasos por delante, el agente altruista contribuye para evitar el castigo (0.5P > 2(1 − cj)), mientras que el tipo no altruista se desentiende.",
        "Para más de tres pasos, mientras el agente altruista continúa contribuyendo al bote público dependiendo de qué tan cercano esté su retorno privado marginal a 1, el tipo no altruista prescribe la deserción.",
        "Analizamos las decisiones de un agente altruista i modelado usando un nivel 1 I-DID expandido en 3 pasos de tiempo. i atribuye los dos modelos de nivel 0, mencionados anteriormente, a j (ver Fig. 9).",
        "Si creo con una probabilidad de 1 que j es altruista, elijo contribuir en cada uno de los tres pasos.",
        "Este comportamiento persiste cuando i no está al tanto de si j es altruista (Fig. 10(a)), y cuando i asigna una alta probabilidad a que j sea del tipo no altruista.",
        "Sin embargo, cuando i cree con una probabilidad de 1 que j no es altruista y por lo tanto seguramente va a traicionar, i elige traicionar para evitar ser castigado y porque su beneficio privado marginal es menor que 1.",
        "Estos resultados demuestran que el comportamiento de nuestro tipo altruista se asemeja al encontrado experimentalmente.",
        "El agente de nivel 1 no altruista elige desertar independientemente de lo altruista que crea que sea el otro agente.",
        "Analizamos el comportamiento de un tipo de agente recíproco que se ajusta a la cooperación o defección esperada.",
        "El retorno privado marginal del tipo recíproco es similar al del tipo no altruista, sin embargo, obtiene una mayor recompensa cuando su acción es similar a la del otro.",
        "Consideramos el caso en el que el agente recíproco i no está seguro de si j es altruista y cree que es probable que el bote público esté medio lleno.",
        "Para esta creencia previa, yo elijo defectar.",
        "Al recibir una observación de abundancia, decide contribuir, mientras que una observación de escasez lo hace defectar (Fig. 10(b)).",
        "Esto se debe a que una observación de abundancia indica que es probable que la olla esté más llena que a la mitad, lo cual resulta de la acción de js para contribuir.",
        "Por lo tanto, entre los dos modelos atribuidos a j, es probable que su tipo sea altruista, lo que hace probable que j contribuya nuevamente en el próximo paso de tiempo.",
        "El agente i, por lo tanto, elige contribuir para corresponder la acción de js.",
        "Un razonamiento análogo lleva a uno a defectar cuando observa una olla escasa.",
        "Con una acción por hacer, creo que si j contribuye, también elegiré contribuir para evitar castigo independientemente de sus observaciones.",
        "Figura 10: (a) Un agente altruista de nivel 1 siempre contribuye. (b) Un agente recíproco i comienza defraudando y luego elige contribuir o defraudar basándose en su observación de abundancia (indicando que j es probablemente altruista) o escasez (j no es altruista). 5.3 Estrategias en el póker de dos jugadores El póker es un popular juego de cartas de suma cero que ha recibido mucha atención en la comunidad de investigación de IA como un banco de pruebas [2].",
        "El póker se juega entre M ≥ 2 jugadores, en el que cada jugador recibe una mano de cartas de una baraja.",
        "Si bien existen varias variantes de póker con diferentes niveles de complejidad, consideramos una versión simple en la que cada jugador tiene tres turnos durante los cuales el jugador puede intercambiar una carta (E), mantener la mano existente (K), retirarse (F) y abandonar el juego, o apostar (C), lo que requiere que todos los jugadores muestren sus manos.",
        "Para mantener las cosas simples, dejemos que M = 2, y que cada jugador reciba una mano compuesta por una sola carta sacada del mismo palo.",
        "Por lo tanto, durante un enfrentamiento, el jugador que tenga la carta numéricamente más alta (el 2 es el más bajo, el as es el más alto) gana el bote.",
        "Durante un intercambio de cartas, la carta descartada se coloca ya sea en la pila L, indicando al otro agente que era una carta de número bajo menor que 8, o en la 820 The Sixth Intl.",
        "Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) H, indicando que la carta tenía un rango mayor o igual a 8.",
        "Ten en cuenta que, por ejemplo, si se descarta una carta de número bajo, la probabilidad de recibir una carta baja a cambio se reduce.",
        "Mostramos el nivel 1 I-ID para el Poker simplificado de dos jugadores en la Fig. 11.",
        "Consideramos dos modelos (tipos de personalidad) del agente j.",
        "El tipo conservador cree que es probable que su oponente tenga una carta de alto número en su mano.",
        "Por otro lado, el agente agresivo j cree con alta probabilidad que su oponente tiene una carta de número más bajo.",
        "Por lo tanto, los dos tipos difieren en sus creencias sobre la mano de sus oponentes.",
        "En ambos estos modelos de nivel 0, se asume que el oponente realiza sus acciones siguiendo una distribución fija y uniforme.",
        "Con tres acciones restantes, independientemente de su mano (a menos que sea un as), el agente agresivo elige intercambiar su carta, con la intención de mejorar su mano actual.",
        "Esto se debe a que cree que el otro tiene una carta baja, lo que mejora sus posibilidades de obtener una carta alta durante el intercambio.",
        "El agente conservador elige quedarse con su carta, sin importar su mano, porque considera que sus posibilidades de obtener una carta alta son escasas, ya que cree que su oponente tiene una.",
        "Figura 11: (a) Nivel 1 I-ID del agente i.",
        "La observación revela información sobre la mano de js del paso de tiempo anterior, (b) IDs de nivel 0 del agente j cuyos nodos de decisión están mapeados a los nodos de probabilidad, A1 j , A2 j , en (a).",
        "La política de un agente de nivel 1 i que cree que cada carta, excepto la suya, tiene la misma probabilidad de estar en su mano (tipo de personalidad neutral) y j podría ser de tipo agresivo o conservador, se muestra en la Figura 12. Su mano contiene la carta numerada 8.",
        "El agente comienza por mantener su carta.",
        "Al ver que j no intercambió una carta (N), i cree con probabilidad 1 que j es conservador y, por lo tanto, mantendrá sus cartas. i responde manteniendo su carta o intercambiándola porque j tiene igual probabilidad de tener una carta más baja o más alta.",
        "Si observo que j descartó su carta en la pila L o H, creo que j es agresivo.",
        "Al observar a L, i se da cuenta de que j tenía una carta baja, y es probable que tenga una carta alta después de su intercambio.",
        "Dado que la probabilidad de recibir una carta baja es alta en este momento, i decide quedarse con su carta.",
        "Al observar la carta H, creyendo que la probabilidad de recibir una carta de número alto es alta, i decide intercambiar su carta.",
        "En el paso final, i decide llamar independientemente de su historial de observaciones porque su creencia de que j tiene una carta más alta no es lo suficientemente alta como para concluir que es mejor retirarse y renunciar al pago.",
        "Esto se debe en parte al hecho de que una observación de, digamos, L, restablece las creencias del agente en el paso de tiempo anterior sobre las cartas de números bajos solamente. 6.",
        "DISCUSIÓN Mostramos cómo los DIDs pueden ser extendidos a los I-DIDs que permiten la toma de decisiones secuenciales en línea en entornos multiagentes inciertos.",
        "Nuestra representación gráfica de I-DIDs mejora la Figura 12 anterior: Un agente de nivel 1 es una política de tres pasos en el problema de Poker. i comienza creyendo que j tiene la misma probabilidad de ser agresivo o conservador y podría tener cualquier carta en su mano con igual probabilidad. Trabaja significativamente al ser más transparente, semánticamente claro y capaz de ser resuelto utilizando algoritmos estándar que apuntan a los DIDs.",
        "Los I-DIDs extienden los NIDs para permitir la toma de decisiones secuenciales a lo largo de múltiples pasos de tiempo en presencia de otros agentes que interactúan.",
        "Los I-DIDs pueden ser vistos como representaciones gráficas concisas para IPOMDPs que proporcionan una forma de explotar la estructura del problema y llevar a cabo la toma de decisiones en línea a medida que el agente actúa y observa dadas sus creencias previas.",
        "Actualmente estamos investigando formas de resolver I-DIDs aproximadamente con límites demostrables en la calidad de la solución.",
        "Agradecimiento: Agradecemos a Piotr Gmytrasiewicz por algunas discusiones útiles relacionadas con este trabajo.",
        "El primer autor desea agradecer el apoyo de una beca UGARF. 7.",
        "REFERENCIAS [1] R. J. Aumann.",
        "Epistemología interactiva i: Conocimiento.",
        "Revista Internacional de Teoría de Juegos, 28:263-300, 1999. [2] D. Billings, A. Davidson, J. Schaeffer y D. Szafron.",
        "El desafío del póker.",
        "AIJ, 2001. [3] A. Brandenburger y E. Dekel.",
        "Jerarquías de creencias y conocimiento común.",
        "Revista de Teoría Económica, 59:189-198, 1993. [4] C. Camerer.",
        "Teoría de juegos conductual: Experimentos en interacción estratégica.",
        "Princeton University Press, 2003. [5] E. Fehr y S. Gachter.",
        "Cooperación y castigo en experimentos de bienes públicos.",
        "Revista Económica Americana, 90(4):980-994, 2000. [6] D. Fudenberg y D. K. Levine.",
        "La Teoría del Aprendizaje en los Juegos.",
        "MIT Press, 1998. [7] D. Fudenberg y J. Tirole.",
        "Teoría de juegos.",
        "MIT Press, 1991. [8] Y. Gal y A. Pfeffer.",
        "Un lenguaje para modelar los procesos de toma de decisiones de agentes en juegos.",
        "En AAMAS, 2003. [9] P. Gmytrasiewicz y P. Doshi.",
        "Un marco para la planificación secuencial en entornos multiagentes.",
        "JAIR, 24:49-79, 2005. [10] P. Gmytrasiewicz y E. Durfee.",
        "Coordinación racional en entornos multiagente.",
        "JAAMAS, 3(4):319-350, 2000. [11] J. C. Harsanyi.",
        "Juegos con información incompleta jugados por jugadores bayesianos.",
        "Ciencia de la Gestión, 14(3):159-182, 1967. [12] R. A. Howard y J. E. Matheson.",
        "Diagramas de influencia.",
        "En R. A. Howard y J. E. Matheson, editores, Los Principios y Aplicaciones del Análisis de Decisiones.",
        "Grupo de Decisiones Estratégicas, Menlo Park, CA 94025, 1984. [13] L. Kaelbling, M. Littman y A. Cassandra.",
        "Planificación y actuación en dominios estocásticos parcialmente observables.",
        "Revista de Inteligencia Artificial, 2, 1998. [14] D. Koller y B. Milch.",
        "Diagramas de influencia multiagente para representar y resolver juegos.",
        "En IJCAI, páginas 1027-1034, 2001. [15] K. Polich y P. Gmytrasiewicz.",
        "Diagramas de influencia interactivos y dinámicos.",
        "En el taller GTDT, AAMAS, 2006. [16] B.",
        "Rathnas, P. Doshi y P. J. Gmytrasiewicz.",
        "Soluciones exactas para POMDP interactivos utilizando equivalencia conductual.",
        "En la Conferencia de Agentes Autónomos y Sistemas Multiagente (AAMAS), 2006. [17] S. Russell y P. Norvig.",
        "Inteligencia Artificial: Un Enfoque Moderno (Segunda Edición).",
        "Prentice Hall, 2003. [18] R. D. Shachter. \n\nPrentice Hall, 2003. [18] R. D. Shachter.",
        "Evaluando diagramas de influencia.",
        "Investigación de Operaciones, 34(6):871-882, 1986. [19] D. Suryadi y P. Gmytrasiewicz.",
        "Aprendiendo modelos de otros agentes utilizando diagramas de influencia.",
        "En UM, 1999.",
        "El Sexto Internacional.",
        "Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 821"
    ],
    "error_count": 7,
    "keys": {
        "graphical model": {
            "translated_key": "modelos gráficos",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Graphical Models for Online Solutions to Interactive POMDPs Prashant Doshi Dept.",
                "of Computer Science University of Georgia Athens, GA 30602, USA pdoshi@cs.uga.edu Yifeng Zeng Dept. of Computer Science Aalborg University DK-9220 Aalborg, Denmark yfzeng@cs.aau.edu Qiongyu Chen Dept. of Computer Science National Univ. of Singapore 117543, Singapore chenqy@comp.nus.edu.sg ABSTRACT We develop a new graphical representation for interactive partially observable Markov decision processes (I-POMDPs) that is significantly more transparent and semantically clear than the previous representation.",
                "These <br>graphical model</br>s called interactive dynamic influence diagrams (I-DIDs) seek to explicitly model the structure that is often present in real-world problems by decomposing the situation into chance and decision variables, and the dependencies between the variables.",
                "I-DIDs generalize DIDs, which may be viewed as graphical representations of POMDPs, to multiagent settings in the same way that I-POMDPs generalize POMDPs.",
                "I-DIDs may be used to compute the policy of an agent online as the agent acts and observes in a setting that is populated by other interacting agents.",
                "Using several examples, we show how I-DIDs may be applied and demonstrate their usefulness.",
                "Categories and Subject Descriptors I.2.11 [Distributed Artificial Intelligence]: Multiagent Systems General Terms Theory 1.",
                "INTRODUCTION Interactive partially observable Markov decision processes (IPOMDPs) [9] provide a framework for sequential decision-making in partially observable multiagent environments.",
                "They generalize POMDPs [13] to multiagent settings by including the other agents computable models in the state space along with the states of the physical environment.",
                "The models encompass all information influencing the agents behaviors, including their preferences, capabilities, and beliefs, and are thus analogous to types in Bayesian games [11].",
                "I-POMDPs adopt a subjective approach to understanding strategic behavior, rooted in a decision-theoretic framework that takes a decision-makers perspective in the interaction.",
                "In [15], Polich and Gmytrasiewicz introduced interactive dynamic influence diagrams (I-DIDs) as the computational representations of I-POMDPs.",
                "I-DIDs generalize DIDs [12], which may be viewed as computational counterparts of POMDPs, to multiagents settings in the same way that I-POMDPs generalize POMDPs.",
                "I-DIDs contribute to a growing line of work [19] that includes multi-agent influence diagrams (MAIDs) [14], and more recently, networks of influence diagrams (NIDs) [8].",
                "These formalisms seek to explicitly model the structure that is often present in real-world problems by decomposing the situation into chance and decision variables, and the dependencies between the variables.",
                "MAIDs provide an alternative to normal and extensive game forms using a graphical formalism to represent games of imperfect information with a decision node for each agents actions and chance nodes capturing the agents private information.",
                "MAIDs objectively analyze the game, efficiently computing the Nash equilibrium profile by exploiting the independence structure.",
                "NIDs extend MAIDs to include agents uncertainty over the game being played and over models of the other agents.",
                "Each model is a MAID and the network of MAIDs is collapsed, bottom up, into a single MAID for computing the equilibrium of the game keeping in mind the different models of each agent.",
                "Graphical formalisms such as MAIDs and NIDs open up a promising area of research that aims to represent multiagent interactions more transparently.",
                "However, MAIDs provide an analysis of the game from an external viewpoint and the applicability of both is limited to static single play games.",
                "Matters are more complex when we consider interactions that are extended over time, where predictions about others future actions must be made using models that change as the agents act and observe.",
                "I-DIDs address this gap by allowing the representation of other agents models as the values of a special model node.",
                "Both, other agents models and the original agents beliefs over these models are updated over time using special-purpose implementations.",
                "In this paper, we improve on the previous preliminary representation of the I-DID shown in [15] by using the insight that the static I-ID is a type of NID.",
                "Thus, we may utilize NID-specific language constructs such as multiplexers to represent the model node, and subsequently the I-ID, more transparently.",
                "Furthermore, we clarify the semantics of the special purpose policy link introduced in the representation of I-DID by [15], and show that it could be replaced by traditional dependency links.",
                "In the previous representation of the I-DID, the update of the agents belief over the models of others as the agents act and receive observations was denoted using a special link called the model update link that connected the model nodes over time.",
                "We explicate the semantics of this link by showing how it can be implemented using the traditional dependency links between the chance nodes that constitute the model nodes.",
                "The net result is a representation of I-DID that is significantly more transparent, semantically clear, and capable of being implemented using the standard algorithms for solving DIDs.",
                "We show how IDIDs may be used to model an agents uncertainty over others models, that may themselves be I-DIDs.",
                "Solution to the I-DID is a policy that prescribes what the agent should do over time, given its beliefs over the physical state and others models.",
                "Analogous to DIDs, I-DIDs may be used to compute the policy of an agent online as the agent acts and observes in a setting that is populated by other interacting agents. 2.",
                "BACKGROUND: FINITELY NESTED IPOMDPS Interactive POMDPs generalize POMDPs to multiagent settings by including other agents models as part of the state space [9].",
                "Since other agents may also reason about others, the interactive state space is strategically nested; it contains beliefs about other agents models and their beliefs about others.",
                "For simplicity of presentation we consider an agent, i, that is interacting with one other agent, j.",
                "A finitely nested I-POMDP of agent i with a strategy level l is defined as the tuple: I-POMDPi,l = ISi,l, A, Ti, Ωi, Oi, Ri where: • ISi,l denotes a set of interactive states defined as, ISi,l = S × Mj,l−1, where Mj,l−1 = {Θj,l−1 ∪ SMj}, for l ≥ 1, and ISi,0 = S, where S is the set of states of the physical environment.",
                "Θj,l−1 is the set of computable intentional models of agent j: θj,l−1 = bj,l−1, ˆθj where the frame, ˆθj = A, Ωj, Tj, Oj, Rj, OCj .",
                "Here, j is Bayes rational and OCj is js optimality criterion.",
                "SMj is the set of subintentional models of j.",
                "Simple examples of subintentional models include a no-information model [10] and a fictitious play model [6], both of which are history independent.",
                "We give a recursive bottom-up construction of the interactive state space below.",
                "ISi,0 = S, Θj,0 = { bj,0, ˆθj | bj,0 ∈ Δ(ISj,0)} ISi,1 = S × {Θj,0 ∪ SMj}, Θj,1 = { bj,1, ˆθj | bj,1 ∈ Δ(ISj,1)} . . . . . .",
                "ISi,l = S × {Θj,l−1 ∪ SMj}, Θj,l = { bj,l, ˆθj | bj,l ∈ Δ(ISj,l)} Similar formulations of nested spaces have appeared in [1, 3]. • A = Ai × Aj is the set of joint actions of all agents in the environment; • Ti : S ×A×S → [0, 1], describes the effect of the joint actions on the physical states of the environment; • Ωi is the set of observations of agent i; • Oi : S × A × Ωi → [0, 1] gives the likelihood of the observations given the physical state and joint action; • Ri : ISi × A → R describes agent is preferences over its interactive states.",
                "Usually only the physical states will matter.",
                "Agent is policy is the mapping, Ω∗ i → Δ(Ai), where Ω∗ i is the set of all observation histories of agent i.",
                "Since belief over the interactive states forms a sufficient statistic [9], the policy can also be represented as a mapping from the set of all beliefs of agent i to a distribution over its actions, Δ(ISi) → Δ(Ai). 2.1 Belief Update Analogous to POMDPs, an agent within the I-POMDP framework updates its belief as it acts and observes.",
                "However, there are two differences that complicate the belief update in multiagent settings when compared to single agent ones.",
                "First, since the state of the physical environment depends on the actions of both agents, is prediction of how the physical state changes has to be made based on its prediction of js actions.",
                "Second, changes in js models have to be included in is belief update.",
                "Specifically, if j is intentional then an update of js beliefs due to its action and observation has to be included.",
                "In other words, i has to update its belief based on its prediction of what j would observe and how j would update its belief.",
                "If js model is subintentional, then js probable observations are appended to the observation history contained in the model.",
                "Formally, we have: Pr(ist |at−1 i , bt−1 i,l ) = β ISt−1:mt−1 j =θt j bt−1 i,l (ist−1 ) × at−1 j Pr(at−1 j |θt−1 j,l−1)Oi(st , at−1 i , at−1 j , ot i) ×Ti(st−1 , at−1 i , at−1 j , st ) ot j Oj(st , at−1 i , at−1 j , ot j) ×τ(SEθt j (bt−1 j,l−1, at−1 j , ot j) − bt j,l−1) (1) where β is the normalizing constant, τ is 1 if its argument is 0 otherwise it is 0, Pr(at−1 j |θt−1 j,l−1) is the probability that at−1 j is Bayes rational for the agent described by model θt−1 j,l−1, and SE(·) is an abbreviation for the belief update.",
                "For a version of the belief update when js model is subintentional, see [9].",
                "If agent j is also modeled as an I-POMDP, then is belief update invokes js belief update (via the term SEθt j ( bt−1 j,l−1 , at−1 j , ot j)), which in turn could invoke is belief update and so on.",
                "This recursion in belief nesting bottoms out at the 0th level.",
                "At this level, the belief update of the agent reduces to a POMDP belief update. 1 For illustrations of the belief update, additional details on I-POMDPs, and how they compare with other multiagent frameworks, see [9]. 2.2 Value Iteration Each belief state in a finitely nested I-POMDP has an associated value reflecting the maximum payoff the agent can expect in this belief state: Un ( bi,l, θi ) = max ai∈Ai is∈ISi,l ERi(is, ai)bi,l(is)+ γ oi∈Ωi Pr(oi|ai, bi,l)Un−1 ( SEθi (bi,l, ai, oi), θi ) (2) where, ERi(is, ai) = aj Ri(is, ai, aj)Pr(aj|mj,l−1) (since is = (s, mj,l−1)).",
                "Eq. 2 is a basis for value iteration in I-POMDPs.",
                "Agent is optimal action, a∗ i , for the case of finite horizon with discounting, is an element of the set of optimal actions for the belief state, OPT(θi), defined as: OPT( bi,l, θi ) = argmax ai∈Ai is∈ISi,l ERi(is, ai)bi,l(is) +γ oi∈Ωi Pr(oi|ai, bi,l)Un ( SEθi (bi,l, ai, oi), θi ) (3) 3.",
                "INTERACTIVEINFLUENCEDIAGRAMS A naive extension of influence diagrams (IDs) to settings populated by multiple agents is possible by treating other agents as automatons, represented using chance nodes.",
                "However, this approach assumes that the agents actions are controlled using a probability distribution that does not change over time.",
                "Interactive influence diagrams (I-IDs) adopt a more sophisticated approach by generalizing IDs to make them applicable to settings shared with other agents who may act and observe, and update their beliefs. 3.1 Syntax In addition to the usual chance, decision, and utility nodes, IIDs include a new type of node called the model node.",
                "We show a general level l I-ID in Fig. 1(a), where the model node (Mj,l−1) is denoted using a hexagon.",
                "We note that the probability distribution over the chance node, S, and the model node together represents agent is belief over its interactive states.",
                "In addition to the model 1 The 0th level model is a POMDP: Other agents actions are treated as exogenous events and folded into the T, O, and R functions.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 815 Figure 1: (a) A generic level l I-ID for agent i situated with one other agent j.",
                "The hexagon is the model node (Mj,l−1) whose structure we show in (b).",
                "Members of the model node are I-IDs themselves (m1 j,l−1, m2 j,l−1; diagrams not shown here for simplicity) whose decision nodes are mapped to the corresponding chance nodes (A1 j , A2 j ).",
                "Depending on the value of the node, Mod[Mj], the distribution of each of the chance nodes is assigned to the node Aj. (c) The transformed I-ID with the model node replaced by the chance nodes and the relationships between them. node, I-IDs differ from IDs by having a dashed link (called the policy link in [15]) between the model node and a chance node, Aj, that represents the distribution over the other agents actions given its model.",
                "In the absence of other agents, the model node and the chance node, Aj, vanish and I-IDs collapse into traditional IDs.",
                "The model node contains the alternative computational models ascribed by i to the other agent from the set, Θj,l−1 ∪ SMj, where Θj,l−1 and SMj were defined previously in Section 2.",
                "Thus, a model in the model node may itself be an I-ID or ID, and the recursion terminates when a model is an ID or subintentional.",
                "Because the model node contains the alternative models of the other agent as its values, its representation is not trivial.",
                "In particular, some of the models within the node are I-IDs that when solved generate the agents optimal policy in their decision nodes.",
                "Each decision node is mapped to the corresponding chance node, say A1 j , in the following way: if OPT is the set of optimal actions obtained by solving the I-ID (or ID), then Pr(aj ∈ A1 j ) = 1 |OP T | if aj ∈ OPT, 0 otherwise.",
                "Borrowing insights from previous work [8], we observe that the model node and the dashed policy link that connects it to the chance node, Aj, could be represented as shown in Fig. 1(b).",
                "The decision node of each level l − 1 I-ID is transformed into a chance node, as we mentioned previously, so that the actions with the largest value in the decision node are assigned uniform probabilities in the chance node while the rest are assigned zero probability.",
                "The different chance nodes (A1 j , A2 j ), one for each model, and additionally, the chance node labeled Mod[Mj] form the parents of the chance node, Aj.",
                "Thus, there are as many action nodes (A1 j , A2 j ) in Mj,l−1 as the number of models in the support of agent is beliefs.",
                "The conditional probability table of the chance node, Aj, is a multiplexer that assumes the distribution of each of the action nodes (A1 j , A2 j ) depending on the value of Mod[Mj].",
                "The values of Mod[Mj] denote the different models of j.",
                "In other words, when Mod[Mj] has the value m1 j,l−1, the chance node Aj assumes the distribution of the node A1 j , and Aj assumes the distribution of A2 j when Mod[Mj] has the value m2 j,l−1.",
                "The distribution over the node, Mod[Mj], is the agent is belief over the models of j given a physical state.",
                "For more agents, we will have as many model nodes as there are agents.",
                "Notice that Fig. 1(b) clarifies the semantics of the policy link, and shows how it can be represented using the traditional dependency links.",
                "In Fig. 1(c), we show the transformed I-ID when the model node is replaced by the chance nodes and relationships between them.",
                "In contrast to the representation in [15], there are no special-purpose policy links, rather the I-ID is composed of only those types of nodes that are found in traditional IDs and dependency relationships between the nodes.",
                "This allows I-IDs to be represented and implemented using conventional application tools that target IDs.",
                "Note that we may view the level l I-ID as a NID.",
                "Specifically, each of the level l − 1 models within the model node are blocks in the NID (see Fig. 2).",
                "If the level l = 1, each block is a traditional ID, otherwise if l > 1, each block within the NID may itself be a NID.",
                "Note that within the I-IDs (or IDs) at each level, there is only a single decision node.",
                "Thus, our NID does not contain any MAIDs.",
                "Figure 2: A level l I-ID represented as a NID.",
                "The probabilities assigned to the blocks of the NID are is beliefs over js models conditioned on a physical state. 3.2 Solution The solution of an I-ID proceeds in a bottom-up manner, and is implemented recursively.",
                "We start by solving the level 0 models, which, if intentional, are traditional IDs.",
                "Their solutions provide probability distributions over the other agents actions, which are entered in the corresponding chance nodes found in the model node of the level 1 I-ID.",
                "The mapping from the level 0 models decision nodes to the chance nodes is carried out so that actions with the largest value in the decision node are assigned uniform probabilities in the chance node while the rest are assigned zero probability.",
                "Given the distributions over the actions within the different chance nodes (one for each model of the other agent), the level 1 I-ID is transformed as shown in Fig. 1(c).",
                "During the transformation, the conditional probability table (CPT) of the node, Aj, is populated such that the node assumes the distribution of each of the chance nodes depending on the value of the node, Mod[Mj].",
                "As we mentioned previously, the values of the node Mod[Mj] denote the different models of the other agent, and its distribution is the agent is belief over the models of j conditioned on the physical state.",
                "The transformed level 1 I-ID is a traditional ID that may be solved us816 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) (a) (b) Figure 3: (a) A generic two time-slice level l I-DID for agent i in a setting with one other agent j.",
                "Notice the dotted model update link that denotes the update of the models of j and the distribution over the models over time. (b) The semantics of the model update link. ing the standard expected utility maximization method [18].",
                "This procedure is carried out up to the level l I-ID whose solution gives the non-empty set of optimal actions that the agent should perform given its belief.",
                "Notice that analogous to IDs, I-IDs are suitable for online decision-making when the agents current belief is known. 4.",
                "INTERACTIVE DYNAMIC INFLUENCE DIAGRAMS Interactive dynamic influence diagrams (I-DIDs) extend I-IDs (and NIDs) to allow sequential decision-making over several time steps.",
                "Just as DIDs are structured graphical representations of POMDPs, I-DIDs are the graphical online analogs for finitely nested I-POMDPs.",
                "I-DIDs may be used to optimize over a finite look-ahead given initial beliefs while interacting with other, possibly similar, agents. 4.1 Syntax We depict a general two time-slice I-DID in Fig. 3(a).",
                "In addition to the model nodes and the dashed policy link, what differentiates an I-DID from a DID is the model update link shown as a dotted arrow in Fig. 3(a).",
                "We explained the semantics of the model node and the policy link in the previous section; we describe the model updates next.",
                "The update of the model node over time involves two steps: First, given the models at time t, we identify the updated set of models that reside in the model node at time t + 1.",
                "Recall from Section 2 that an agents intentional model includes its belief.",
                "Because the agents act and receive observations, their models are updated to reflect their changed beliefs.",
                "Since the set of optimal actions for a model could include all the actions, and the agent may receive any one of |Ωj| possible observations, the updated set at time step t + 1 will have at most |Mt j,l−1||Aj||Ωj| models.",
                "Here, |Mt j,l−1| is the number of models at time step t, |Aj| and |Ωj| are the largest spaces of actions and observations respectively, among all the models.",
                "Second, we compute the new distribution over the updated models given the original distribution and the probability of the agent performing the action and receiving the observation that led to the updated model.",
                "These steps are a part of agent is belief update formalized using Eq. 1.",
                "In Fig. 3(b), we show how the dotted model update link is implemented in the I-DID.",
                "If each of the two level l − 1 models ascribed to j at time step t results in one action, and j could make one of two possible observations, then the model node at time step t + 1 contains four updated models (mt+1,1 j,l−1 ,mt+1,2 j,l−1 , mt+1,3 j,l−1 , and mt+1,4 j,l−1 ).",
                "These models differ in their initial beliefs, each of which is the result of j updating its beliefs due to its action and a possible observation.",
                "The decision nodes in each of the I-DIDs or DIDs that represent the lower level models are mapped to the corresponding Figure 4: Transformed I-DID with the model nodes and model update link replaced with the chance nodes and the relationships (in bold). chance nodes, as mentioned previously.",
                "Next, we describe how the distribution over the updated set of models (the distribution over the chance node Mod[Mt+1 j ] in Mt+1 j,l−1) is computed.",
                "The probability that js updated model is, say mt+1,1 j,l−1 , depends on the probability of j performing the action and receiving the observation that led to this model, and the prior distribution over the models at time step t. Because the chance node At j assumes the distribution of each of the action nodes based on the value of Mod[Mt j ], the probability of the action is given by this chance node.",
                "In order to obtain the probability of js possible observation, we introduce the chance node Oj, which depending on the value of Mod[Mt j ] assumes the distribution of the observation node in the lower level model denoted by Mod[Mt j ].",
                "Because the probability of js observations depends on the physical state and the joint actions of both agents, the node Oj is linked with St+1 , At j, and At i. 2 Analogous to At j, the conditional probability table of Oj is also a multiplexer modulated by Mod[Mt j ].",
                "Finally, the distribution over the prior models at time t is obtained from the chance node, Mod[Mt j ] in Mt j,l−1.",
                "Consequently, the chance nodes, Mod[Mt j ], At j, and Oj, form the parents of Mod[Mt+1 j ] in Mt+1 j,l−1.",
                "Notice that the model update link may be replaced by the dependency links between the chance nodes that constitute the model nodes in the two time slices.",
                "In Fig. 4 we show the two time-slice I-DID with the model nodes replaced by the chance nodes and the relationships between them.",
                "Chance nodes and dependency links that not in bold are standard, usually found in DIDs.",
                "Expansion of the I-DID over more time steps requires the repetition of the two steps of updating the set of models that form the 2 Note that Oj represents js observation at time t + 1.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 817 values of the model node and adding the relationships between the chance nodes, as many times as there are model update links.",
                "We note that the possible set of models of the other agent j grows exponentially with the number of time steps.",
                "For example, after T steps, there may be at most |Mt=1 j,l−1|(|Aj||Ωj|)T −1 candidate models residing in the model node. 4.2 Solution Analogous to I-IDs, the solution to a level l I-DID for agent i expanded over T time steps may be carried out recursively.",
                "For the purpose of illustration, let l=1 and T=2.",
                "The solution method uses the standard look-ahead technique, projecting the agents action and observation sequences forward from the current belief state [17], and finding the possible beliefs that i could have in the next time step.",
                "Because agent i has a belief over js models as well, the lookahead includes finding out the possible models that j could have in the future.",
                "Consequently, each of js subintentional or level 0 models (represented using a standard DID) in the first time step must be solved to obtain its optimal set of actions.",
                "These actions are combined with the set of possible observations that j could make in that model, resulting in an updated set of candidate models (that include the updated beliefs) that could describe the behavior of j. Beliefs over this updated set of candidate models are calculated using the standard inference methods using the dependency relationships between the model nodes as shown in Fig. 3(b).",
                "We note the recursive nature of this solution: in solving agent is level 1 I-DID, js level 0 DIDs must be solved.",
                "If the nesting of models is deeper, all models at all levels starting from 0 are solved in a bottom-up manner.",
                "We briefly outline the recursive algorithm for solving agent is Algorithm for solving I-DID Input : level l ≥ 1 I-ID or level 0 ID, T Expansion Phase 1.",
                "For t from 1 to T − 1 do 2.",
                "If l ≥ 1 then Populate Mt+1 j,l−1 3.",
                "For each mt j in Range(Mt j,l−1) do 4.",
                "Recursively call algorithm with the l − 1 I-ID (or ID) that represents mt j and the horizon, T − t + 1 5.",
                "Map the decision node of the solved I-ID (or ID), OPT(mt j), to a chance node Aj 6.",
                "For each aj in OPT(mt j) do 7.",
                "For each oj in Oj (part of mt j) do 8.",
                "Update js belief, bt+1 j ← SE(bt j, aj, oj) 9. mt+1 j ← New I-ID (or ID) with bt+1 j as the initial belief 10.",
                "Range(Mt+1 j,l−1) ∪ ← {mt+1 j } 11.",
                "Add the model node, Mt+1 j,l−1, and the dependency links between Mt j,l−1 and Mt+1 j,l−1 (shown in Fig. 3(b)) 12.",
                "Add the chance, decision, and utility nodes for t + 1 time slice and the dependency links between them 13.",
                "Establish the CPTs for each chance node and utility node Look-Ahead Phase 14.",
                "Apply the standard look-ahead and backup method to solve the expanded I-DID Figure 5: Algorithm for solving a level l ≥ 0 I-DID. level l I-DID expanded over T time steps with one other agent j in Fig. 5.",
                "We adopt a two-phase approach: Given an I-ID of level l (described previously in Section 3) with all lower level models also represented as I-IDs or IDs (if level 0), the first step is to expand the level l I-ID over T time steps adding the dependency links and the conditional probability tables for each node.",
                "We particularly focus on establishing and populating the model nodes (lines 3-11).",
                "Note that Range(·) returns the values (lower level models) of the random variable given as input (model node).",
                "In the second phase, we use a standard look-ahead technique projecting the action and observation sequences over T time steps in the future, and backing up the utility values of the reachable beliefs.",
                "Similar to I-IDs, the I-DIDs reduce to DIDs in the absence of other agents.",
                "As we mentioned previously, the 0-th level models are the traditional DIDs.",
                "Their solutions provide probability distributions over actions of the agent modeled at that level to I-DIDs at level 1.",
                "Given probability distributions over other agents actions the level 1 IDIDs can themselves be solved as DIDs, and provide probability distributions to yet higher level models.",
                "Assume that the number of models considered at each level is bound by a number, M. Solving an I-DID of level l in then equivalent to solving O(Ml ) DIDs. 5.",
                "EXAMPLE APPLICATIONS To illustrate the usefulness of I-DIDs, we apply them to three problem domains.",
                "We describe, in particular, the formulation of the I-DID and the optimal prescriptions obtained on solving it. 5.1 Followership-Leadership in the Multiagent Tiger Problem We begin our illustrations of using I-IDs and I-DIDs with a slightly modified version of the multiagent tiger problem discussed in [9].",
                "The problem has two agents, each of which can open the right door (OR), the left door (OL) or listen (L).",
                "In addition to hearing growls (from the left (GL) or from the right (GR)) when they listen, the agents also hear creaks (from the left (CL), from the right (CR), or no creaks (S)), which noisily indicate the other agents opening one of the doors.",
                "When any door is opened, the tiger persists in its original location with a probability of 95%.",
                "Agent i hears growls with a reliability of 65% and creaks with a reliability of 95%.",
                "Agent j, on the other hand, hears growls with a reliability of 95%.",
                "Thus, the setting is such that agent i hears agent j opening doors more reliably than the tigers growls.",
                "This suggests that i could use js actions as an indication of the location of the tiger, as we discuss below.",
                "Each agents preferences are as in the single agent game discussed in [13].",
                "The transition, observation, and reward functions are shown in [16].",
                "A good indicator of the usefulness of normative methods for decision-making like I-DIDs is the emergence of realistic social behaviors in their prescriptions.",
                "In settings of the persistent multiagent tiger problem that reflect real world situations, we demonstrate followership between the agents and, as shown in [15], deception among agents who believe that they are in a follower-leader type of relationship.",
                "In particular, we analyze the situational and epistemological conditions sufficient for their emergence.",
                "The followership behavior, for example, results from the agent knowing its own weaknesses, assessing the strengths, preferences, and possible behaviors of the other, and realizing that its best for it to follow the others actions in order to maximize its payoffs.",
                "Let us consider a particular setting of the tiger problem in which agent i believes that js preferences are aligned with its own - both of them just want to get the gold - and js hearing is more reliable in comparison to itself.",
                "As an example, suppose that j, on listening can discern the tigers location 95% of the times compared to is 65% accuracy.",
                "Additionally, agent i does not have any initial information about the tigers location.",
                "In other words, is single-level nested belief, bi,1, assigns 0.5 to each of the two locations of the tiger.",
                "In addition, i considers two models of j, which differ in js flat level 0 initial beliefs.",
                "This is represented in the level 1 I-ID shown in Fig. 6(a).",
                "According to one model, j assigns a probability of 0.9 that the tiger is behind the left door, while the other 818 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Figure 6: (a) Level 1 I-ID of agent i, (b) two level 0 IDs of agent j whose decision nodes are mapped to the chance nodes, A1 j , A2 j , in (a). model assigns 0.1 to that location (see Fig. 6(b)).",
                "Agent i is undecided on these two models of j.",
                "If we vary is hearing ability, and solve the corresponding level 1 I-ID expanded over three time steps, we obtain the normative behavioral policies shown in Fig 7 that exhibit followership behavior.",
                "If is probability of correctly hearing the growls is 0.65, then as shown in the policy in Fig. 7(a), i begins to conditionally follow js actions: i opens the same door that j opened previously iff is own assessment of the tigers location confirms js pick.",
                "If i loses the ability to correctly interpret the growls completely, it blindly follows j and opens the same door that j opened previously (Fig. 7(b)).",
                "Figure 7: Emergence of (a) conditional followership, and (b) blind followership in the tiger problem.",
                "Behaviors of interest are in bold. * is a wildcard, and denotes any one of the observations.",
                "We observed that a single level of belief nesting - beliefs about the others models - was sufficient for followership to emerge in the tiger problem.",
                "However, the epistemological requirements for the emergence of leadership are more complex.",
                "For an agent, say j, to emerge as a leader, followership must first emerge in the other agent i.",
                "As we mentioned previously, if i is certain that its preferences are identical to those of j, and believes that j has a better sense of hearing, i will follow js actions over time.",
                "Agent j emerges as a leader if it believes that i will follow it, which implies that js belief must be nested two levels deep to enable it to recognize its leadership role.",
                "Realizing that i will follow presents j with an opportunity to influence is actions in the benefit of the collective good or its self-interest alone.",
                "For example, in the tiger problem, let us consider a setting in which if both i and j open the correct door, then each gets a payoff of 20 that is double the original.",
                "If j alone selects the correct door, it gets the payoff of 10.",
                "On the other hand, if both agents pick the wrong door, their penalties are cut in half.",
                "In this setting, it is in both js best interest as well as the collective betterment for j to use its expertise in selecting the correct door, and thus be a good leader.",
                "However, consider a slightly different problem in which j gains from is loss and is penalized if i gains.",
                "Specifically, let is payoff be subtracted from js, indicating that j is antagonistic toward i - if j picks the correct door and i the wrong one, then is loss of 100 becomes js gain.",
                "Agent j believes that i incorrectly thinks that js preferences are those that promote the collective good and that it starts off by believing with 99% confidence where the tiger is.",
                "Because i believes that its preferences are similar to those of j, and that j starts by believing almost surely that one of the two is the correct location (two level 0 models of j), i will start by following js actions.",
                "We show is normative policy on solving its singly-nested I-DID over three time steps in Fig. 8(a).",
                "The policy demonstrates that i will blindly follow js actions.",
                "Since the tiger persists in its original location with a probability of 0.95, i will select the same door again.",
                "If j begins the game with a 99% probability that the tiger is on the right, solving js I-DID nested two levels deep, results in the policy shown in Fig. 8(b).",
                "Even though j is almost certain that OL is the correct action, it will start by selecting OR, followed by OL.",
                "Agent js intention is to deceive i who, it believes, will follow js actions, so as to gain $110 in the second time step, which is more than what j would gain if it were to be honest.",
                "Figure 8: Emergence of deception between agents in the tiger problem.",
                "Behaviors of interest are in bold. * denotes as before. (a) Agent is policy demonstrating that it will blindly follow js actions. (b) Even though j is almost certain that the tiger is on the right, it will start by selecting OR, followed by OL, in order to deceive i. 5.2 Altruism and Reciprocity in the Public Good Problem The public good (PG) problem [7], consists of a group of M agents, each of whom must either contribute some resource to a public pot or keep it for themselves.",
                "Since resources contributed to the public pot are shared among all the agents, they are less valuable to the agent when in the public pot.",
                "However, if all agents choose to contribute their resources, then the payoff to each agent is more than if no one contributes.",
                "Since an agent gets its share of the public pot irrespective of whether it has contributed or not, the dominating action is for each agent to not contribute, and instead free ride on others contributions.",
                "However, behaviors of human players in empirical simulations of the PG problem differ from the normative predictions.",
                "The experiments reveal that many players initially contribute a large amount to the public pot, and continue to contribute when the PG problem is played repeatedly, though in decreasing amounts [4].",
                "Many of these experiments [5] report that a small core group of players persistently contributes to the public pot even when all others are defecting.",
                "These experiments also reveal that players who persistently contribute have altruistic or reciprocal preferences matching expected cooperation of others.",
                "For simplicity, we assume that the game is played between M = 2 agents, i and j.",
                "Let each agent be initially endowed with XT amount of resources.",
                "While the classical PG game formulation permits each agent to contribute any quantity of resources (≤ XT ) to the public pot, we simplify the action space by allowing two possible actions.",
                "Each agent may choose to either contribute (C) a fixed amount of the resources, or not contribute.",
                "The latter action is deThe Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 819 noted as defect (D).",
                "We assume that the actions are not observable to others.",
                "The value of resources in the public pot is discounted by ci for each agent i, where ci is the marginal private return.",
                "We assume that ci < 1 so that the agent does not benefit enough that it contributes to the public pot for private gain.",
                "Simultaneously, ciM > 1, making collective contribution pareto optimal. i/j C D C 2ciXT , 2cjXT ciXT − cp, XT + cjXT − P D XT + ciXT − P, cjXT − cp XT , XT Table 1: The one-shot PG game with punishment.",
                "In order to encourage contributions, the contributing agents punish free riders but incur a small cost for administering the punishment.",
                "Let P be the punishment meted out to the defecting agent and cp the non-zero cost of punishing for the contributing agent.",
                "For simplicity, we assume that the cost of punishing is same for both the agents.",
                "The one-shot PG game with punishment is shown in Table. 1.",
                "Let ci = cj, cp > 0, and if P > XT − ciXT , then defection is no longer a dominating action.",
                "If P < XT − ciXT , then defection is the dominating action for both.",
                "If P = XT − ciXT , then the game is not dominance-solvable.",
                "Figure 9: (a) Level 1 I-ID of agent i, (b) level 0 IDs of agent j with decision nodes mapped to the chance nodes, A1 j and A2 j , in (a).",
                "We formulate a sequential version of the PG problem with punishment from the perspective of agent i.",
                "Though in the repeated PG game, the quantity in the public pot is revealed to all the agents after each round of actions, we assume in our formulation that it is hidden from the agents.",
                "Each agent may contribute a fixed amount, xc, or defect.",
                "An agent on performing an action receives an observation of plenty (PY) or meager (MR) symbolizing the state of the public pot.",
                "Notice that the observations are also indirectly indicative of agent js actions because the state of the public pot is influenced by them.",
                "The amount of resources in agent is private pot, is perfectly observable to i.",
                "The payoffs are analogous to Table. 1.",
                "Borrowing from the empirical investigations of the PG problem [5], we construct level 0 IDs for j that model altruistic and non-altruistic types (Fig. 9(b)).",
                "Specifically, our altruistic agent has a high marginal private return (cj is close to 1) and does not punish others who defect.",
                "Let xc = 1 and the level 0 agent be punished half the times it defects.",
                "With one action remaining, both types of agents choose to contribute to avoid being punished.",
                "With two actions to go, the altruistic type chooses to contribute, while the other defects.",
                "This is because cj for the altruistic type is close to 1, thus the expected punishment, 0.5P > (1 − cj), which the altruistic type avoids.",
                "Because cj for the non-altruistic type is less, it prefers not to contribute.",
                "With three steps to go, the altruistic agent contributes to avoid punishment (0.5P > 2(1 − cj)), and the non-altruistic type defects.",
                "For greater than three steps, while the altruistic agent continues to contribute to the public pot depending on how close its marginal private return is to 1, the non-altruistic type prescribes defection.",
                "We analyzed the decisions of an altruistic agent i modeled using a level 1 I-DID expanded over 3 time steps. i ascribes the two level 0 models, mentioned previously, to j (see Fig. 9).",
                "If i believes with a probability 1 that j is altruistic, i chooses to contribute for each of the three steps.",
                "This behavior persists when i is unaware of whether j is altruistic (Fig. 10(a)), and when i assigns a high probability to j being the non-altruistic type.",
                "However, when i believes with a probability 1 that j is non-altruistic and will thus surely defect, i chooses to defect to avoid being punished and because its marginal private return is less than 1.",
                "These results demonstrate that the behavior of our altruistic type resembles that found experimentally.",
                "The non-altruistic level 1 agent chooses to defect regardless of how likely it believes the other agent to be altruistic.",
                "We analyzed the behavior of a reciprocal agent type that matches expected cooperation or defection.",
                "The reciprocal types marginal private return is similar to that of the non-altruistic type, however, it obtains a greater payoff when its action is similar to that of the other.",
                "We consider the case when the reciprocal agent i is unsure of whether j is altruistic and believes that the public pot is likely to be half full.",
                "For this prior belief, i chooses to defect.",
                "On receiving an observation of plenty, i decides to contribute, while an observation of meager makes it defect (Fig. 10(b)).",
                "This is because an observation of plenty signals that the pot is likely to be greater than half full, which results from js action to contribute.",
                "Thus, among the two models ascribed to j, its type is likely to be altruistic making it likely that j will contribute again in the next time step.",
                "Agent i therefore chooses to contribute to reciprocate js action.",
                "An analogous reasoning leads i to defect when it observes a meager pot.",
                "With one action to go, i believing that j contributes, will choose to contribute too to avoid punishment regardless of its observations.",
                "Figure 10: (a) An altruistic level 1 agent always contributes. (b) A reciprocal agent i starts off by defecting followed by choosing to contribute or defect based on its observation of plenty (indicating that j is likely altruistic) or meager (j is non-altruistic). 5.3 Strategies in Two-Player Poker Poker is a popular zero sum card game that has received much attention among the AI research community as a testbed [2].",
                "Poker is played among M ≥ 2 players in which each player receives a hand of cards from a deck.",
                "While several flavors of Poker with varying complexity exist, we consider a simple version in which each player has three plys during which the player may either exchange a card (E), keep the existing hand (K), fold (F) and withdraw from the game, or call (C), requiring all players to show their hands.",
                "To keep matters simple, let M = 2, and each player receive a hand consisting of a single card drawn from the same suit.",
                "Thus, during a showdown, the player who has the numerically larger card (2 is the lowest, ace is the highest) wins the pot.",
                "During an exchange of cards, the discarded card is placed either in the L pile, indicating to the other agent that it was a low numbered card less than 8, or in the 820 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) H pile, indicating that the card had a rank greater than or equal to 8.",
                "Notice that, for example, if a lower numbered card is discarded, the probability of receiving a low card in exchange is now reduced.",
                "We show the level 1 I-ID for the simplified two-player Poker in Fig. 11.",
                "We considered two models (personality types) of agent j.",
                "The conservative type believes that it is likely that its opponent has a high numbered card in its hand.",
                "On the other hand, the aggressive agent j believes with a high probability that its opponent has a lower numbered card.",
                "Thus, the two types differ in their beliefs over their opponents hand.",
                "In both these level 0 models, the opponent is assumed to perform its actions following a fixed, uniform distribution.",
                "With three actions to go, regardless of its hand (unless it is an ace), the aggressive agent chooses to exchange its card, with the intent of improving on its current hand.",
                "This is because it believes the other to have a low card, which improves its chances of getting a high card during the exchange.",
                "The conservative agent chooses to keep its card, no matter its hand because its chances of getting a high card are slim as it believes that its opponent has one.",
                "Figure 11: (a) Level 1 I-ID of agent i.",
                "The observation reveals information about js hand of the previous time step, (b) level 0 IDs of agent j whose decision nodes are mapped to the chance nodes, A1 j , A2 j , in (a).",
                "The policy of a level 1 agent i who believes that each card except its own has an equal likelihood of being in js hand (neutral personality type) and j could be either an aggressive or conservative type, is shown in Fig. 12. is own hand contains the card numbered 8.",
                "The agent starts by keeping its card.",
                "On seeing that j did not exchange a card (N), i believes with probability 1 that j is conservative and hence will keep its cards. i responds by either keeping its card or exchanging it because j is equally likely to have a lower or higher card.",
                "If i observes that j discarded its card into the L or H pile, i believes that j is aggressive.",
                "On observing L, i realizes that j had a low card, and is likely to have a high card after its exchange.",
                "Because the probability of receiving a low card is high now, i chooses to keep its card.",
                "On observing H, believing that the probability of receiving a high numbered card is high, i chooses to exchange its card.",
                "In the final step, i chooses to call regardless of its observation history because its belief that j has a higher card is not sufficiently high to conclude that its better to fold and relinquish the payoff.",
                "This is partly due to the fact that an observation of, say, L resets the agent is previous time step beliefs over js hand to the low numbered cards only. 6.",
                "DISCUSSION We showed how DIDs may be extended to I-DIDs that enable online sequential decision-making in uncertain multiagent settings.",
                "Our graphical representation of I-DIDs improves on the previous Figure 12: A level 1 agent is three step policy in the Poker problem. i starts by believing that j is equally likely to be aggressive or conservative and could have any card in its hand with equal probability. work significantly by being more transparent, semantically clear, and capable of being solved using standard algorithms that target DIDs.",
                "I-DIDs extend NIDs to allow sequential decision-making over multiple time steps in the presence of other interacting agents.",
                "I-DIDs may be seen as concise graphical representations for IPOMDPs providing a way to exploit problem structure and carry out online decision-making as the agent acts and observes given its prior beliefs.",
                "We are currently investigating ways to solve I-DIDs approximately with provable bounds on the solution quality.",
                "Acknowledgment: We thank Piotr Gmytrasiewicz for some useful discussions related to this work.",
                "The first author would like to acknowledge the support of a UGARF grant. 7.",
                "REFERENCES [1] R. J. Aumann.",
                "Interactive epistemology i: Knowledge.",
                "International Journal of Game Theory, 28:263-300, 1999. [2] D. Billings, A. Davidson, J. Schaeffer, and D. Szafron.",
                "The challenge of poker.",
                "AIJ, 2001. [3] A. Brandenburger and E. Dekel.",
                "Hierarchies of beliefs and common knowledge.",
                "Journal of Economic Theory, 59:189-198, 1993. [4] C. Camerer.",
                "Behavioral Game Theory: Experiments in Strategic Interaction.",
                "Princeton University Press, 2003. [5] E. Fehr and S. Gachter.",
                "Cooperation and punishment in public goods experiments.",
                "American Economic Review, 90(4):980-994, 2000. [6] D. Fudenberg and D. K. Levine.",
                "The Theory of Learning in Games.",
                "MIT Press, 1998. [7] D. Fudenberg and J. Tirole.",
                "Game Theory.",
                "MIT Press, 1991. [8] Y. Gal and A. Pfeffer.",
                "A language for modeling agents decision-making processes in games.",
                "In AAMAS, 2003. [9] P. Gmytrasiewicz and P. Doshi.",
                "A framework for sequential planning in multiagent settings.",
                "JAIR, 24:49-79, 2005. [10] P. Gmytrasiewicz and E. Durfee.",
                "Rational coordination in multi-agent environments.",
                "JAAMAS, 3(4):319-350, 2000. [11] J. C. Harsanyi.",
                "Games with incomplete information played by bayesian players.",
                "Management Science, 14(3):159-182, 1967. [12] R. A. Howard and J. E. Matheson.",
                "Influence diagrams.",
                "In R. A. Howard and J. E. Matheson, editors, The Principles and Applications of Decision Analysis.",
                "Strategic Decisions Group, Menlo Park, CA 94025, 1984. [13] L. Kaelbling, M. Littman, and A. Cassandra.",
                "Planning and acting in partially observable stochastic domains.",
                "Artificial Intelligence Journal, 2, 1998. [14] D. Koller and B. Milch.",
                "Multi-agent influence diagrams for representing and solving games.",
                "In IJCAI, pages 1027-1034, 2001. [15] K. Polich and P. Gmytrasiewicz.",
                "Interactive dynamic influence diagrams.",
                "In GTDT Workshop, AAMAS, 2006. [16] B.",
                "Rathnas., P. Doshi, and P. J. Gmytrasiewicz.",
                "Exact solutions to interactive pomdps using behavioral equivalence.",
                "In Autonomous Agents and Multi-Agent Systems Conference (AAMAS), 2006. [17] S. Russell and P. Norvig.",
                "Artificial Intelligence: A Modern Approach (Second Edition).",
                "Prentice Hall, 2003. [18] R. D. Shachter.",
                "Evaluating influence diagrams.",
                "Operations Research, 34(6):871-882, 1986. [19] D. Suryadi and P. Gmytrasiewicz.",
                "Learning models of other agents using influence diagrams.",
                "In UM, 1999.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 821"
            ],
            "original_annotated_samples": [
                "These <br>graphical model</br>s called interactive dynamic influence diagrams (I-DIDs) seek to explicitly model the structure that is often present in real-world problems by decomposing the situation into chance and decision variables, and the dependencies between the variables."
            ],
            "translated_annotated_samples": [
                "Estos <br>modelos gráficos</br> llamados diagramas de influencia dinámica interactiva (I-DIDs) buscan modelar explícitamente la estructura que a menudo está presente en problemas del mundo real al descomponer la situación en variables de probabilidad y decisión, y las dependencias entre las variables."
            ],
            "translated_text": "Modelos gráficos para soluciones en línea a POMDP interactivos de Prashant Doshi Dept. Departamento de Ciencias de la Computación Universidad de Georgia Athens, GA 30602, EE. UU. pdoshi@cs.uga.edu Yifeng Zeng Dept. de Ciencias de la Computación Universidad de Aalborg DK-9220 Aalborg, Dinamarca yfzeng@cs.aau.edu Qiongyu Chen Dept. de Ciencias de la Computación Universidad Nacional de Singapur 117543, Singapur chenqy@comp.nus.edu.sg RESUMEN Desarrollamos una nueva representación gráfica para procesos de decisión de Markov parcialmente observables interactivos (I-POMDPs) que es significativamente más transparente y semánticamente clara que la representación anterior. Estos <br>modelos gráficos</br> llamados diagramas de influencia dinámica interactiva (I-DIDs) buscan modelar explícitamente la estructura que a menudo está presente en problemas del mundo real al descomponer la situación en variables de probabilidad y decisión, y las dependencias entre las variables. Los I-DIDs generalizan los DIDs, los cuales pueden ser vistos como representaciones gráficas de POMDPs, a entornos multiagentes de la misma manera que los I-POMDPs generalizan los POMDPs. Los I-DIDs pueden ser utilizados para calcular la política de un agente en línea mientras el agente actúa y observa en un entorno poblado por otros agentes que interactúan entre sí. Usando varios ejemplos, mostramos cómo se pueden aplicar los I-DIDs y demostramos su utilidad. Categorías y Descriptores de Asignaturas I.2.11 [Inteligencia Artificial Distribuida]: Sistemas Multiagentes Términos Generales Teoría 1. Las Procesos de Decisión de Markov Interactivos Parcialmente Observables (IPOMDPs) proporcionan un marco para la toma de decisiones secuenciales en entornos multiagentes parcialmente observables. Generalizan los POMDPs [13] a entornos multiagentes al incluir los modelos computables de los otros agentes en el espacio de estados junto con los estados del entorno físico. Los modelos abarcan toda la información que influye en los comportamientos de los agentes, incluyendo sus preferencias, capacidades y creencias, y por lo tanto son análogos a los tipos en los juegos bayesianos [11]. I-POMDPs adoptan un enfoque subjetivo para comprender el comportamiento estratégico, arraigado en un marco de teoría de decisiones que toma la perspectiva de los tomadores de decisiones en la interacción. En [15], Polich y Gmytrasiewicz introdujeron diagramas de influencia dinámica interactivos (I-DIDs) como las representaciones computacionales de I-POMDPs. Los I-DIDs generalizan los DIDs [12], los cuales pueden ser vistos como contrapartes computacionales de POMDPs, a entornos de múltiples agentes de la misma manera que los I-POMDPs generalizan los POMDPs. Los I-DIDs contribuyen a una creciente línea de trabajo [19] que incluye diagramas de influencia multiagente (MAIDs) [14], y más recientemente, redes de diagramas de influencia (NIDs) [8]. Estos formalismos buscan modelar explícitamente la estructura que a menudo está presente en problemas del mundo real al descomponer la situación en variables de azar y decisiones, y las dependencias entre las variables. Los MAIDs proporcionan una alternativa a las formas normales y extensivas de juego utilizando un formalismo gráfico para representar juegos de información imperfecta con un nodo de decisión para las acciones de cada agente y nodos de azar que capturan la información privada de los agentes. Los MAIDs analizan objetivamente el juego, calculando eficientemente el perfil de equilibrio de Nash al explotar la estructura de independencia. NIDs extienden los MAIDs para incluir la incertidumbre de los agentes sobre el juego que se está jugando y sobre los modelos de los otros agentes. Cada modelo es un MAID y la red de MAIDs se colapsa, de abajo hacia arriba, en un solo MAID para calcular el equilibrio del juego teniendo en cuenta los diferentes modelos de cada agente. Los formalismos gráficos como MAIDs y NIDs abren una área de investigación prometedora que tiene como objetivo representar las interacciones multiagente de manera más transparente. Sin embargo, los MAIDs proporcionan un análisis del juego desde un punto de vista externo y la aplicabilidad de ambos está limitada a juegos estáticos de un solo jugador. Los asuntos son más complejos cuando consideramos interacciones que se extienden en el tiempo, donde las predicciones sobre las acciones futuras de otros deben hacerse utilizando modelos que cambian a medida que los agentes actúan y observan. Los I-DIDs abordan esta brecha al permitir la representación de modelos de otros agentes como los valores de un nodo de modelo especial. Tanto los modelos de otros agentes como las creencias originales de los agentes sobre estos modelos se actualizan con el tiempo utilizando implementaciones especializadas. En este documento, mejoramos la representación preliminar previa del I-DID mostrada en [15] utilizando la idea de que el I-ID estático es un tipo de NID. Por lo tanto, podemos utilizar construcciones de lenguaje específicas de NID, como multiplexores, para representar de manera más transparente el nodo del modelo y, posteriormente, el I-ID. Además, aclaramos la semántica del enlace de política de propósito especial introducido en la representación de I-DID por [15], y mostramos que podría ser reemplazado por enlaces de dependencia tradicionales. En la representación previa del I-DID, la actualización de la creencia de los agentes sobre los modelos de los demás a medida que los agentes actúan y reciben observaciones se denotaba utilizando un enlace especial llamado el enlace de actualización del modelo que conectaba los nodos del modelo a lo largo del tiempo. Explicamos la semántica de este enlace mostrando cómo puede ser implementado utilizando los enlaces de dependencia tradicionales entre los nodos de probabilidad que constituyen los nodos del modelo. El resultado final es una representación de I-DID que es significativamente más transparente, semánticamente clara y capaz de ser implementada utilizando los algoritmos estándar para resolver DIDs. Mostramos cómo los IDIDs pueden ser utilizados para modelar la incertidumbre de un agente sobre los modelos de otros, que a su vez pueden ser I-DIDs. La solución al I-DID es una política que prescribe lo que el agente debe hacer con el tiempo, dadas sus creencias sobre el estado físico y otros modelos. Análogos a los DIDs, los I-DIDs pueden ser utilizados para calcular la política de un agente en línea mientras el agente actúa y observa en un entorno poblado por otros agentes interactuantes. 2. ANTECEDENTES: Los IPOMDPS ANIDADOS FINITAMENTE generalizan los POMDPs a entornos multiagentes al incluir los modelos de otros agentes como parte del espacio de estados [9]. Dado que otros agentes también pueden razonar sobre otros, el espacio de estado interactivo está estratégicamente anidado; contiene creencias sobre los modelos de otros agentes y sus creencias sobre los demás. Para simplificar la presentación, consideramos un agente, i, que está interactuando con otro agente, j. Un I-POMDP finitamente anidado del agente i con un nivel de estrategia l se define como la tupla: I-POMDPi,l = ISi,l, A, Ti, Ωi, Oi, Ri donde: • ISi,l denota un conjunto de estados interactivos definido como, ISi,l = S × Mj,l−1, donde Mj,l−1 = {Θj,l−1 ∪ SMj}, para l ≥ 1, e ISi,0 = S, donde S es el conjunto de estados del entorno físico. Θj,l−1 es el conjunto de modelos intencionales computables del agente j: θj,l−1 = bj,l−1, ˆθj donde el marco, ˆθj = A, Ωj, Tj, Oj, Rj, OCj. Aquí, j es Bayesiano racional y OCj es el criterio de optimalidad de j. SMj es el conjunto de modelos subintencionales de j. Ejemplos simples de modelos subintencionales incluyen un modelo sin información [10] y un modelo de juego ficticio [6], ambos de los cuales son independientes de la historia. Damos una construcción recursiva de abajo hacia arriba del espacio de estados interactivo a continuación. Sí,0 = S, Θj,0 = { bj,0, ˆθj | bj,0 ∈ Δ(ISj,0)} Sí,1 = S × {Θj,0 ∪ SMj}, Θj,1 = { bj,1, ˆθj | bj,1 ∈ Δ(ISj,1)} . . . . . . Sí, l = S × {Θj,l−1 ∪ SMj}, Θj,l = { bj,l, ˆθj | bj,l ∈ Δ(ISj,l)} Formulaciones similares de espacios anidados han aparecido en [1, 3]. • A = Ai × Aj es el conjunto de acciones conjuntas de todos los agentes en el entorno; • Ti : S ×A×S → [0, 1], describe el efecto de las acciones conjuntas en los estados físicos del entorno; • Ωi es el conjunto de observaciones del agente i; • Oi : S × A × Ωi → [0, 1] da la probabilidad de las observaciones dadas el estado físico y la acción conjunta; • Ri : ISi × A → R describe las preferencias del agente sobre sus estados interactivos. Normalmente solo importarán los estados físicos. La política del agente es el mapeo, Ω∗ i → Δ(Ai), donde Ω∗ i es el conjunto de todos los historiales de observación del agente i. Dado que la creencia sobre los estados interactivos forma una estadística suficiente [9], la política también puede ser representada como un mapeo del conjunto de todas las creencias del agente i a una distribución sobre sus acciones, Δ(ISi) → Δ(Ai). 2.1 Actualización de Creencias Análogo a los POMDPs, un agente dentro del marco I-POMDP actualiza su creencia a medida que actúa y observa. Sin embargo, hay dos diferencias que complican la actualización de creencias en entornos multiagentes en comparación con los entornos de un solo agente. Primero, dado que el estado del entorno físico depende de las acciones de ambos agentes, la predicción de cómo cambia el estado físico debe hacerse basándose en la predicción de sus acciones. Segundo, los cambios en los modelos de js deben ser incluidos en la actualización de creencias. Específicamente, si j es intencional, entonces se debe incluir una actualización de las creencias de j debido a su acción y observación. En otras palabras, i tiene que actualizar su creencia basándose en su predicción de lo que j observaría y cómo j actualizaría su creencia. Si el modelo de js es subintencional, entonces las observaciones probables de js se añaden al historial de observaciones contenido en el modelo. Formalmente, tenemos: Pr(ist |at−1 i , bt−1 i,l ) = β ISt−1:mt−1 j =θt j bt−1 i,l (ist−1 ) × at−1 j Pr(at−1 j |θt−1 j,l−1)Oi(st , at−1 i , at−1 j , ot i) ×Ti(st−1 , at−1 i , at−1 j , st ) ot j Oj(st , at−1 i , at−1 j , ot j) ×τ(SEθt j (bt−1 j,l−1, at−1 j , ot j) − bt j,l−1) (1) donde β es la constante de normalización, τ es 1 si su argumento es 0, de lo contrario es 0, Pr(at−1 j |θt−1 j,l−1) es la probabilidad de que at−1 j sea Bayes racional para el agente descrito por el modelo θt−1 j,l−1, y SE(·) es una abreviatura para la actualización de creencias. Para una versión de la actualización de creencias cuando el modelo js es subintencional, consulte [9]. Si el agente j también se modela como un I-POMDP, entonces la actualización de creencias invoca la actualización de creencias de j (a través del término SEθt j ( bt−1 j,l−1 , at−1 j , ot j)), lo que a su vez podría invocar la actualización de creencias de is y así sucesivamente. Esta recursión en la anidación de creencias llega al nivel 0. En este nivel, la actualización de creencias del agente se reduce a una actualización de creencias de un POMDP. Para ilustraciones de la actualización de creencias, detalles adicionales sobre I-POMDPs y cómo se comparan con otros marcos multiagentes, consulte [9]. Iteración de Valor Cada estado de creencia en un I-POMDP finitamente anidado tiene un valor asociado que refleja la máxima ganancia que el agente puede esperar en este estado de creencia: Un ( bi,l, θi ) = max ai∈Ai is∈ISi,l ERi(is, ai)bi,l(is)+ γ oi∈Ωi Pr(oi|ai, bi,l)Un−1 ( SEθi (bi,l, ai, oi), θi ) (2) donde, ERi(is, ai) = aj Ri(is, ai, aj)Pr(aj|mj,l−1) (ya que is = (s, mj,l−1)). La ecuación 2 es una base para la iteración de valor en I-POMDPs. La acción óptima del agente, a∗ i, para el caso de horizonte finito con descuento, es un elemento del conjunto de acciones óptimas para el estado de creencia, OPT(θi), definido como: OPT( bi,l, θi ) = argmax ai∈Ai is∈ISi,l ERi(is, ai)bi,l(is) +γ oi∈Ωi Pr(oi|ai, bi,l)Un ( SEθi (bi,l, ai, oi), θi ) (3) 3. Diagramas de influencia interactivos. Una extensión ingenua de los diagramas de influencia (IDs) a entornos poblados por múltiples agentes es posible tratando a los otros agentes como autómatas, representados mediante nodos de probabilidad. Sin embargo, este enfoque asume que las acciones de los agentes están controladas utilizando una distribución de probabilidad que no cambia con el tiempo. Los diagramas de influencia interactivos (I-IDs) adoptan un enfoque más sofisticado al generalizar los IDs para hacerlos aplicables a entornos compartidos con otros agentes que pueden actuar, observar y actualizar sus creencias. 3.1 Sintaxis Además de los nodos habituales de probabilidad, decisión y utilidad, los IIDs incluyen un nuevo tipo de nodo llamado nodo de modelo. Mostramos un nivel general l I-ID en la Fig. 1(a), donde el nodo del modelo (Mj,l−1) está representado con un hexágono. Observamos que la distribución de probabilidad sobre el nodo de probabilidad, S, y el nodo del modelo juntos representan la creencia del agente sobre sus estados interactivos. Además del modelo 1, el modelo de nivel 0 es un POMDP: las acciones de otros agentes se tratan como eventos exógenos y se incorporan en las funciones T, O y R. El Sexto Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) Figura 1: (a) Un nivel genérico l I-ID para el agente i situado con otro agente j. El hexágono es el nodo modelo (Mj,l−1) cuya estructura mostramos en (b). Los miembros del nodo modelo son ellos mismos I-ID (m1 j,l−1, m2 j,l−1; los diagramas no se muestran aquí por simplicidad) cuyos nodos de decisión se asignan a los nodos de probabilidad correspondientes (A1 j , A2 j). Dependiendo del valor del nodo, Mod[Mj], la distribución de cada uno de los nodos de probabilidad se asigna al nodo Aj. (c) El I-ID transformado con el nodo del modelo reemplazado por los nodos de probabilidad y las relaciones entre ellos. Los I-IDs difieren de los IDs al tener un enlace discontinuo (llamado enlace de política en [15]) entre el nodo del modelo y un nodo de probabilidad, Aj, que representa la distribución sobre las acciones de los otros agentes dada su modelo. En ausencia de otros agentes, el nodo del modelo y el nodo de probabilidad, Aj, desaparecen y los I-ID se convierten en ID tradicionales. El nodo del modelo contiene los modelos computacionales alternativos atribuidos por i al otro agente del conjunto, Θj,l−1 ∪ SMj, donde Θj,l−1 y SMj fueron definidos previamente en la Sección 2. Por lo tanto, un modelo en el nodo del modelo puede ser en sí mismo un I-ID o ID, y la recursión termina cuando un modelo es un ID o subintencional. Dado que el nodo del modelo contiene los modelos alternativos del otro agente como sus valores, su representación no es trivial. En particular, algunos de los modelos dentro del nodo son I-IDs que, al resolverse, generan la política óptima de los agentes en sus nodos de decisión. Cada nodo de decisión se asigna al nodo de probabilidad correspondiente, digamos A1 j, de la siguiente manera: si OPT es el conjunto de acciones óptimas obtenidas al resolver el I-ID (o ID), entonces Pr(aj ∈ A1 j) = 1 |OPT| si aj ∈ OPT, 0 en caso contrario. Tomando prestados los conocimientos de trabajos anteriores [8], observamos que el nodo del modelo y el enlace de política discontinua que lo conecta con el nodo de probabilidad, Aj, podrían representarse como se muestra en la Fig. 1(b). El nodo de decisión de cada nivel l − 1 I-ID se transforma en un nodo de probabilidad, como mencionamos anteriormente, de modo que las acciones con el mayor valor en el nodo de decisión se les asignan probabilidades uniformes en el nodo de probabilidad, mientras que al resto se les asigna probabilidad cero. Los diferentes nodos de probabilidad (A1 j , A2 j ), uno para cada modelo, y adicionalmente, el nodo de probabilidad etiquetado Mod[Mj] forman los padres del nodo de probabilidad, Aj. Por lo tanto, hay tantos nodos de acción (A1 j , A2 j ) en Mj,l−1 como el número de modelos en el soporte de las creencias del agente. La tabla de probabilidad condicional del nodo de probabilidad, Aj, es un multiplexor que asume la distribución de cada uno de los nodos de acción (A1 j , A2 j ) dependiendo del valor de Mod[Mj]. Los valores de Mod[Mj] denotan los diferentes modelos de j. En otras palabras, cuando Mod[Mj] tiene el valor m1 j,l−1, el nodo de probabilidad Aj asume la distribución del nodo A1 j, y Aj asume la distribución de A2 j cuando Mod[Mj] tiene el valor m2 j,l−1. La distribución sobre el nodo, Mod[Mj], es la creencia del agente sobre los modelos de j dado un estado físico. Para más agentes, tendremos tantos nodos de modelo como agentes haya. Observa que la Fig. 1(b) aclara la semántica del enlace de política y muestra cómo puede ser representado utilizando los enlaces de dependencia tradicionales. En la Fig. 1(c), mostramos el I-ID transformado cuando el nodo del modelo es reemplazado por los nodos de probabilidad y las relaciones entre ellos. A diferencia de la representación en [15], no hay enlaces de políticas especiales, sino que el I-ID está compuesto únicamente por aquellos tipos de nodos que se encuentran en IDs tradicionales y relaciones de dependencia entre los nodos. Esto permite que los I-IDs sean representados e implementados utilizando herramientas de aplicación convencionales que apuntan a los IDs. Ten en cuenta que podemos ver el nivel l I-ID como un NID. Específicamente, cada uno de los modelos del nivel l − 1 dentro del nodo del modelo son bloques en el NID (ver Fig. 2). Si el nivel l = 1, cada bloque es un ID tradicional, de lo contrario, si l > 1, cada bloque dentro del NID puede ser un NID en sí mismo. Ten en cuenta que dentro de los I-IDs (o IDs) en cada nivel, solo hay un nodo de decisión único. Por lo tanto, nuestro NID no contiene ningún MAID. Figura 2: Un nivel l I-ID representado como un NID. Las probabilidades asignadas a los bloques del NID son creencias sobre modelos js condicionados a un estado físico. 3.2 Solución La solución de un I-ID procede de manera ascendente y se implementa de forma recursiva. Comenzamos resolviendo los modelos de nivel 0, que, si son intencionales, son identificadores tradicionales. Sus soluciones proporcionan distribuciones de probabilidad sobre las acciones de los otros agentes, las cuales se ingresan en los nodos de probabilidad correspondientes encontrados en el nodo del modelo del nivel 1 I-ID. El mapeo de los nodos de decisión de los modelos de nivel 0 a los nodos de probabilidad se realiza de manera que las acciones con el mayor valor en el nodo de decisión se les asignan probabilidades uniformes en el nodo de probabilidad, mientras que el resto se les asigna probabilidad cero. Dadas las distribuciones sobre las acciones dentro de los diferentes nodos de probabilidad (uno para cada modelo del otro agente), el I-ID de nivel 1 se transforma como se muestra en la Fig. 1(c). Durante la transformación, la tabla de probabilidad condicional (CPT) del nodo, Aj, se llena de tal manera que el nodo asume la distribución de cada uno de los nodos de probabilidad dependiendo del valor del nodo, Mod[Mj]. Como mencionamos anteriormente, los valores del nodo Mod[Mj] denotan los diferentes modelos del otro agente, y su distribución es la creencia del agente sobre los modelos de j condicionados al estado físico. El nivel 1 I-ID transformado es un ID tradicional que puede ser resuelto como us816 The Sixth Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) (a) (b) Figura 3: (a) Un I-DID genérico de dos niveles de tiempo para el agente i en un entorno con otro agente j. Observa el enlace de actualización del modelo punteado que denota la actualización de los modelos de j y la distribución de los modelos a lo largo del tiempo. (b) La semántica del enlace de actualización del modelo utilizando el método estándar de maximización de utilidad esperada [18]. Este procedimiento se lleva a cabo hasta el nivel l I-ID cuya solución proporciona el conjunto no vacío de acciones óptimas que el agente debe realizar dada su creencia. Ten en cuenta que, al igual que los IDs, los I-IDs son adecuados para la toma de decisiones en línea cuando se conoce la creencia actual de los agentes. 4. Diagramas de influencia dinámica interactivos (I-DIDs) Los diagramas de influencia dinámica interactivos (I-DIDs) extienden los I-IDs (y NIDs) para permitir la toma de decisiones secuencial a lo largo de varios pasos de tiempo. Así como los DIDs son representaciones gráficas estructuradas de POMDPs, los I-DIDs son los análogos gráficos en línea para I-POMDPs finitamente anidados. Los I-DIDs pueden ser utilizados para optimizar sobre una mirada anticipada finita dadas las creencias iniciales mientras se interactúa con otros agentes, posiblemente similares. 4.1 Sintaxis Representamos un I-DID de dos cortes temporales en la Fig. 3(a). Además de los nodos del modelo y el enlace de política discontinuo, lo que diferencia a un I-DID de un DID es el enlace de actualización del modelo mostrado como una flecha punteada en la Fig. 3(a). Explicamos la semántica del nodo del modelo y el enlace de la política en la sección anterior; a continuación, describimos las actualizaciones del modelo. La actualización del nodo del modelo con el tiempo implica dos pasos: primero, dados los modelos en el tiempo t, identificamos el conjunto actualizado de modelos que residen en el nodo del modelo en el tiempo t + 1. Recuerda de la Sección 2 que el modelo intencional de un agente incluye sus creencias. Debido a que los agentes actúan y reciben observaciones, sus modelos se actualizan para reflejar sus creencias cambiadas. Dado que el conjunto de acciones óptimas para un modelo podría incluir todas las acciones, y el agente podría recibir cualquiera de las |Ωj| posibles observaciones, el conjunto actualizado en el paso de tiempo t + 1 tendrá como máximo |Mt j,l−1||Aj||Ωj| modelos. Aquí, |Mt j,l−1| es el número de modelos en el paso de tiempo t, |Aj| y |Ωj| son los espacios más grandes de acciones y observaciones respectivamente, entre todos los modelos. Segundo, calculamos la nueva distribución sobre los modelos actualizados dados la distribución original y la probabilidad de que el agente realice la acción y reciba la observación que llevó al modelo actualizado. Estos pasos son parte de la actualización de creencias del agente formalizada utilizando la Ecuación 1. En la Fig. 3(b), mostramos cómo se implementa el enlace de actualización del modelo punteado en el I-DID. Si cada uno de los dos modelos de nivel l − 1 atribuidos a j en el paso de tiempo t resulta en una acción, y j podría hacer una de dos posibles observaciones, entonces el nodo del modelo en el paso de tiempo t + 1 contiene cuatro modelos actualizados (mt+1,1 j,l−1 ,mt+1,2 j,l−1 , mt+1,3 j,l−1 , y mt+1,4 j,l−1 ). Estos modelos difieren en sus creencias iniciales, cada una de las cuales es el resultado de j actualizar sus creencias debido a su acción y una posible observación. Los nodos de decisión en cada uno de los I-DIDs o DIDs que representan los modelos de nivel inferior se asignan a la Figura 4 correspondiente: I-DID transformado con los nodos del modelo y el enlace de actualización del modelo reemplazados por los nodos de probabilidad y las relaciones (en negrita) de los nodos de probabilidad, como se mencionó anteriormente. A continuación, describimos cómo se calcula la distribución sobre el conjunto actualizado de modelos (la distribución sobre el nodo de probabilidad Mod[Mt+1 j ] en Mt+1 j,l−1). La probabilidad de que el modelo actualizado js sea, digamos mt+1,1 j,l−1, depende de la probabilidad de que j realice la acción y reciba la observación que condujo a este modelo, y de la distribución previa sobre los modelos en el paso de tiempo t. Dado que el nodo de probabilidad At j asume la distribución de cada uno de los nodos de acción basados en el valor de Mod[Mt j], la probabilidad de la acción está dada por este nodo de probabilidad. Para obtener la probabilidad de una observación posible js, introducimos el nodo de probabilidad Oj, que dependiendo del valor de Mod[Mt j], asume la distribución del nodo de observación en el modelo de nivel inferior denominado Mod[Mt j]. Dado que la probabilidad de las observaciones js depende del estado físico y de las acciones conjuntas de ambos agentes, el nodo Oj está vinculado con St+1, At j y At i. De manera análoga a At j, la tabla de probabilidad condicional de Oj también es un multiplexor modulado por Mod[Mt j]. Finalmente, la distribución de los modelos previos en el tiempo t se obtiene a partir del nodo de probabilidad, Mod[Mt j ] en Mt j,l−1. Por consiguiente, los nodos de probabilidad, Mod[Mt j], At j y Oj, forman los padres de Mod[Mt+1 j] en Mt+1 j,l−1. Ten en cuenta que el enlace de actualización del modelo puede ser reemplazado por los enlaces de dependencia entre los nodos de probabilidad que constituyen los nodos del modelo en las dos etapas temporales. En la Fig. 4 mostramos los dos cortes temporales I-DID con los nodos del modelo reemplazados por los nodos de probabilidad y las relaciones entre ellos. Los nodos de probabilidad y los enlaces de dependencia que no están en negrita son estándar, generalmente encontrados en DIDs. La expansión del I-DID a lo largo de más pasos de tiempo requiere la repetición de los dos pasos de actualizar el conjunto de modelos que forman el 2. Nota que Oj representa la observación j en el tiempo t + 1. El Sexto Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 817 valores del nodo del modelo y añadiendo las relaciones entre los nodos de probabilidad, tantas veces como enlaces de actualización del modelo haya. Observamos que el conjunto posible de modelos del otro agente j crece exponencialmente con el número de pasos de tiempo. Por ejemplo, después de T pasos, puede haber como máximo |Mt=1 j,l−1|(|Aj||Ωj|)T −1 modelos candidatos residiendo en el nodo del modelo. 4.2 Solución Análoga a los I-ID, la solución a un I-DID de nivel l para el agente i expandido a lo largo de T pasos de tiempo puede llevarse a cabo de forma recursiva. Con el propósito de ilustración, dejemos l=1 y T=2. El método de solución utiliza la técnica estándar de anticipación, proyectando las secuencias de acciones y observaciones de los agentes hacia adelante desde el estado de creencia actual [17], y encontrando las posibles creencias que podría tener en el siguiente paso temporal. Dado que el agente i también tiene una creencia sobre los modelos de j, la búsqueda anticipada incluye descubrir los posibles modelos que j podría tener en el futuro. Por consiguiente, cada uno de los modelos subintencionales o de nivel 0 de js (representados utilizando un DID estándar) en el primer paso de tiempo debe resolverse para obtener su conjunto óptimo de acciones. Estas acciones se combinan con el conjunto de observaciones posibles que j podría hacer en ese modelo, lo que resulta en un conjunto actualizado de modelos candidatos (que incluyen las creencias actualizadas) que podrían describir el comportamiento de j. Las creencias sobre este conjunto actualizado de modelos candidatos se calculan utilizando los métodos estándar de inferencia que utilizan las relaciones de dependencia entre los nodos del modelo, como se muestra en la Figura 3(b). Observamos la naturaleza recursiva de esta solución: al resolver el agente de nivel 1 I-DID, los DIDs de nivel 0 deben ser resueltos. Si el anidamiento de los modelos es más profundo, todos los modelos en todos los niveles, comenzando desde el nivel 0, se resuelven de manera ascendente. Breve descripción del algoritmo recursivo para resolver el agente es el Algoritmo para resolver I-DID. Entrada: nivel l ≥ 1 I-ID o nivel 0 ID, T Fase de Expansión 1. Para t desde 1 hasta T − 1, hacer 2. Si l ≥ 1, entonces llenar Mt+1 j,l−1 3. Para cada mt j en el rango (Mt j,l−1) hacer 4. Llame recursivamente al algoritmo con el l - 1 I-ID (o ID) que representa mt j y el horizonte, T - t + 1 5. Mapea el nodo de decisión del I-ID (o ID) resuelto, OPT(mt j), a un nodo de probabilidad Aj 6. Para cada aj en OPT(mt j) hacer 7. Para cada oj en Oj (parte de mt j) hacer 8. Actualizar la creencia js, bt+1 j ← SE(bt j, aj, oj) 9. mt+1 j ← Nuevo I-ID (o ID) con bt+1 j como la creencia inicial 10. Rango(Mt+1 j,l−1) ∪ ← {mt+1 j } 11. Añadir el nodo del modelo, Mt+1 j,l−1, y los enlaces de dependencia entre Mt j,l−1 y Mt+1 j,l−1 (mostrados en la Fig. 3(b)) 12. Agregue los nodos de probabilidad, decisión y utilidad para la franja de tiempo t + 1 y los enlaces de dependencia entre ellos 13. Establezca las CPTs para cada nodo de probabilidad y nodo de utilidad de la Fase de Mirada Adelante 14. Aplica el método estándar de anticipación y respaldo para resolver la Figura 5 expandida de I-DID: Algoritmo para resolver un nivel l ≥ 0 I-DID. Nivel l I-DID expandido durante T pasos de tiempo con otro agente j en la Figura 5. Adoptamos un enfoque de dos fases: Dado un I-ID de nivel l (descrito previamente en la Sección 3) con todos los modelos de niveles inferiores representados también como I-IDs o IDs (si es nivel 0), el primer paso es expandir el I-ID de nivel l a lo largo de T pasos de tiempo añadiendo los enlaces de dependencia y las tablas de probabilidad condicional para cada nodo. Nos enfocamos especialmente en establecer y poblar los nodos del modelo (líneas 3-11). Ten en cuenta que Range(·) devuelve los valores (modelos de nivel inferior) de la variable aleatoria dada como entrada (nodo del modelo). En la segunda fase, utilizamos una técnica estándar de anticipación proyectando las secuencias de acción y observación durante T pasos de tiempo en el futuro, y respaldando los valores de utilidad de las creencias alcanzables. Similar a los I-IDs, los I-DIDs se reducen a DIDs en ausencia de otros agentes. Como mencionamos anteriormente, los modelos de nivel 0 son los DIDs tradicionales. Sus soluciones proporcionan distribuciones de probabilidad sobre las acciones del agente modelado en ese nivel a los I-DIDs en el nivel 1. Dadas las distribuciones de probabilidad sobre las acciones de otros agentes, los IDIDs de nivel 1 pueden resolverse como DIDs y proporcionar distribuciones de probabilidad a modelos de niveles superiores. Suponga que el número de modelos considerados en cada nivel está limitado por un número, M. Resolver un I-DID de nivel l es equivalente a resolver O(Ml) DIDs. Para ilustrar la utilidad de los I-DIDs, los aplicamos a tres dominios de problemas. Describimos, en particular, la formulación del I-DID y las prescripciones óptimas obtenidas al resolverlo. 5.1 Seguimiento-Liderazgo en el Problema del Tigre Multiagente Comenzamos nuestras ilustraciones del uso de I-IDs e I-DIDs con una versión ligeramente modificada del problema del tigre multiagente discutido en [9]. El problema tiene dos agentes, cada uno de los cuales puede abrir la puerta derecha (OR), la puerta izquierda (OL) o escuchar (L). Además de escuchar gruñidos (desde la izquierda (GL) o desde la derecha (GR)) cuando escuchan, los agentes también escuchan chirridos (desde la izquierda (CL), desde la derecha (CR) o sin chirridos (S)), que indican ruidosamente que los otros agentes están abriendo una de las puertas. Cuando se abre cualquier puerta, el tigre persiste en su ubicación original con una probabilidad del 95%. El agente i escucha gruñidos con una fiabilidad del 65% y crujidos con una fiabilidad del 95%. El agente J, por otro lado, escucha gruñidos con una fiabilidad del 95%. Por lo tanto, la configuración es tal que el agente i escucha la apertura de puertas del agente j de manera más confiable que los rugidos de los tigres. Esto sugiere que podría usar acciones de JavaScript como indicación de la ubicación del tigre, como discutimos a continuación. Las preferencias de cada agente son como en el juego de un solo agente discutido en [13]. Las funciones de transición, observación y recompensa se muestran en [16]. Un buen indicador de la utilidad de los métodos normativos para la toma de decisiones como los I-DIDs es la aparición de comportamientos sociales realistas en sus prescripciones. En entornos del persistente problema del tigre multiagente que reflejan situaciones del mundo real, demostramos la seguidismo entre los agentes y, como se muestra en [15], el engaño entre agentes que creen que están en una relación de tipo seguidor-líder. En particular, analizamos las condiciones situacionales y epistemológicas suficientes para su surgimiento. El comportamiento de seguidores, por ejemplo, resulta del agente conociendo sus propias debilidades, evaluando las fortalezas, preferencias y posibles comportamientos del otro, y dándose cuenta de que es mejor para él seguir las acciones de los demás para maximizar sus ganancias. Consideremos una configuración particular del problema del tigre en la que el agente i cree que las preferencias de j están alineadas con las suyas: ambos solo quieren obtener el oro, y la audición de j es más confiable en comparación consigo mismo. Como ejemplo, supongamos que j, al escuchar, puede discernir la ubicación del tigre el 95% de las veces en comparación con su precisión del 65%. Además, el agente i no tiene ninguna información inicial sobre la ubicación de los tigres. En otras palabras, la creencia anidada de un solo nivel, bi,1, asigna 0.5 a cada una de las dos ubicaciones del tigre. Además, considera dos modelos de j, que difieren en los niveles iniciales de creencias planas de j. Esto se representa en el nivel 1 I-ID mostrado en la Fig. 6(a). Según un modelo, j asigna una probabilidad de 0.9 a que el tigre esté detrás de la puerta izquierda, mientras que el otro 818 The Sixth Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) Figura 6: (a) Nivel 1 I-ID del agente i, (b) dos IDs de nivel 0 del agente j cuyos nodos de decisión están mapeados a los nodos de probabilidad, A1 j , A2 j , en (a). El modelo asigna 0.1 a esa ubicación (ver Fig. 6(b)). El agente i está indeciso sobre estos dos modelos de j. Si variamos la capacidad auditiva, y resolvemos el nivel 1 correspondiente I-ID expandido en tres pasos de tiempo, obtenemos las políticas de comportamiento normativas mostradas en la Figura 7 que exhiben comportamiento de seguidores. Si la probabilidad de escuchar correctamente los gruñidos es de 0.65, entonces, como se muestra en la política en la Fig. 7(a), i comienza a seguir condicionalmente las acciones de j: i abre la misma puerta que j abrió anteriormente si su evaluación propia de la ubicación de los tigres confirma la elección de j. Si i pierde la capacidad de interpretar correctamente los gruñidos por completo, sigue ciegamente a j y abre la misma puerta que j abrió anteriormente (Fig. 7(b)). Figura 7: Emergencia de (a) seguidores condicionales, y (b) seguidores ciegos en el problema del tigre. Los comportamientos de interés están en negrita. * es un comodín y representa cualquiera de las observaciones. Observamos que un solo nivel de anidación de creencias, creencias sobre los modelos de los demás, fue suficiente para que surgiera el seguidismo en el problema del tigre. Sin embargo, los requisitos epistemológicos para el surgimiento del liderazgo son más complejos. Para que un agente, digamos j, surja como líder, primero debe surgir el seguidismo en el otro agente i. Como mencionamos anteriormente, si i está seguro de que sus preferencias son idénticas a las de j, y cree que j tiene un mejor sentido del oído, i seguirá las acciones de j con el tiempo. El agente j emerge como líder si cree que lo seguiré, lo que implica que la creencia de j debe estar anidada dos niveles de profundidad para permitirle reconocer su papel de liderazgo. Darse cuenta de que lo seguiré presenta a J una oportunidad para influir en sus acciones en beneficio del bien colectivo o de su interés propio solamente. Por ejemplo, en el problema del tigre, consideremos una situación en la que si tanto i como j abren la puerta correcta, entonces cada uno recibe un pago de 20 que es el doble del original. Si solo j selecciona la puerta correcta, obtiene la recompensa de 10. Por otro lado, si ambos agentes eligen la puerta incorrecta, sus penalizaciones se reducen a la mitad. En este entorno, es tanto en el mejor interés de j como en el beneficio colectivo que utilice su experiencia para seleccionar la puerta correcta, y así ser un buen líder. Sin embargo, considera un problema ligeramente diferente en el que j se beneficia de la pérdida de i y es penalizado si i se beneficia. Específicamente, dejemos que la ganancia se reste de js, indicando que j es antagonista hacia i: si j elige la puerta correcta y i la incorrecta, entonces la pérdida de 100 se convierte en la ganancia de js. El agente J cree que yo incorrectamente pienso que las preferencias de J son aquellas que promueven el bien colectivo y que comienza creyendo con un 99% de confianza dónde está el tigre. Dado que creo que mis preferencias son similares a las de j, y que j comienza creyendo casi con certeza que una de las dos ubicaciones es la correcta (dos modelos de nivel 0 de j), comenzaré siguiendo las acciones de j. Mostramos la política normativa para resolver su I-DID anidado individualmente durante tres pasos de tiempo en la Fig. 8(a). La política demuestra que seguiré ciegamente las acciones de js. Dado que el tigre persiste en su ubicación original con una probabilidad del 0.95, seleccionaré nuevamente la misma puerta. Si j comienza el juego con una probabilidad del 99% de que el tigre esté a la derecha, resolver su I-DID anidado dos niveles profundamente, resulta en la política mostrada en la Fig. 8(b). Aunque j está casi seguro de que OL es la acción correcta, comenzará seleccionando OR, seguido de OL. La intención del agente js es engañar a i, a quien cree que seguirá las acciones de js, para así obtener $110 en el segundo paso de tiempo, lo cual es más de lo que j ganaría si fuera honesto. Figura 8: Emergencia del engaño entre agentes en el problema del tigre. Los comportamientos de interés están en negrita. * denota como antes. (a) El agente es una política que demuestra que seguirá ciegamente las acciones de js. (b) Aunque j está casi seguro de que el tigre está a la derecha, comenzará seleccionando OR, seguido de OL, para engañar a i. 5.2 Altruismo y reciprocidad en el problema del bien público El problema del bien público (PG) [7], consiste en un grupo de M agentes, cada uno de los cuales debe contribuir con algún recurso a una olla pública o guardarlo para sí mismos. Dado que los recursos aportados al fondo público se comparten entre todos los agentes, son menos valiosos para el agente cuando están en el fondo público. Sin embargo, si todos los agentes eligen contribuir con sus recursos, entonces la recompensa para cada agente es mayor que si nadie contribuye. Dado que un agente recibe su parte del bote público independientemente de si ha contribuido o no, la acción dominante es que cada agente no contribuya y en su lugar se aproveche de las contribuciones de los demás. Sin embargo, los comportamientos de los jugadores humanos en simulaciones empíricas del problema de PG difieren de las predicciones normativas. Los experimentos revelan que muchos jugadores inicialmente contribuyen con una gran cantidad al bote público, y continúan contribuyendo cuando se juega el problema del PG repetidamente, aunque en cantidades decrecientes [4]. Muchos de estos experimentos [5] informan que un pequeño grupo central de jugadores contribuye persistentemente al bote público incluso cuando todos los demás están desertando. Estos experimentos también revelan que los jugadores que contribuyen persistentemente tienen preferencias altruistas o recíprocas que coinciden con la cooperación esperada de los demás. Para simplificar, asumimos que el juego se juega entre M = 2 agentes, i y j. Que cada agente esté inicialmente dotado con una cantidad de recursos XT. Mientras que la formulación clásica del juego PG permite que cada agente contribuya con cualquier cantidad de recursos (≤ XT) al bote público, simplificamos el espacio de acciones al permitir dos acciones posibles. Cada agente puede elegir contribuir (C) una cantidad fija de recursos, o no contribuir. La última acción es deThe Sixth Intl. La Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 819 fue señalada como defectuosa (D). Suponemos que las acciones no son observables por otros. El valor de los recursos en la bolsa pública se descuenta por ci para cada agente i, donde ci es el retorno privado marginal. Suponemos que ci < 1 para que el agente no se beneficie lo suficiente como para contribuir al bote público en beneficio propio. Simultáneamente, ciM > 1, lo que hace que la contribución colectiva sea óptima de Pareto. i/j C D C 2ciXT , 2cjXT ciXT − cp, XT + cjXT − P D XT + ciXT − P, cjXT − cp XT , XT Tabla 1: El juego PG de un solo disparo con castigo. Para fomentar las contribuciones, los agentes contribuyentes castigan a los que no colaboran pero incurren en un pequeño costo por administrar el castigo. Sea P la sanción impuesta al agente que se desvía y cp el costo no nulo de castigar al agente contribuyente. Para simplificar, asumimos que el costo de castigar es el mismo para ambos agentes. El juego PG de un solo disparo con castigo se muestra en la Tabla 1. Si ci = cj, cp > 0, y si P > XT − ciXT, entonces la deserción ya no es una acción dominante. Si P < XT − ciXT, entonces la deserción es la acción dominante para ambos. Si P = XT − ciXT, entonces el juego no es resoluble por dominancia. Figura 9: (a) Nivel 1 I-ID del agente i, (b) IDs de nivel 0 del agente j con nodos de decisión mapeados a los nodos de probabilidad, A1 j y A2 j, en (a). Formulamos una versión secuencial del problema PG con castigo desde la perspectiva del agente i. Aunque en el juego de PG repetido, la cantidad en la olla pública se revela a todos los agentes después de cada ronda de acciones, asumimos en nuestra formulación que está oculta para los agentes. Cada agente puede contribuir con una cantidad fija, xc, o desertar. Un agente al realizar una acción recibe una observación de abundante (PY) o escasa (MR) simbolizando el estado de la olla pública. Ten en cuenta que las observaciones también son indirectamente indicativas de las acciones del agente js porque el estado del bote público es influenciado por ellas. La cantidad de recursos en el agente es una olla privada, es perfectamente observable para i. Los pagos son análogos a la Tabla 1. Tomando prestado de las investigaciones empíricas del problema PG [5], construimos IDs de nivel 0 para j que modelan tipos altruistas y no altruistas (Fig. 9(b)). Específicamente, nuestro agente altruista tiene un alto retorno privado marginal (cj está cerca de 1) y no castiga a los demás que incumplen. Que xc = 1 y el agente de nivel 0 sea castigado la mitad de las veces que se desvía. Con una acción restante, ambos tipos de agentes eligen contribuir para evitar ser castigados. Con dos acciones por delante, el tipo altruista elige contribuir, mientras que el otro defecta. Esto se debe a que cj para el tipo altruista es cercano a 1, por lo tanto, el castigo esperado, 0.5P > (1 − cj), que el tipo altruista evita. Debido a que cj para el tipo no altruista es menor, prefiere no contribuir. Con tres pasos por delante, el agente altruista contribuye para evitar el castigo (0.5P > 2(1 − cj)), mientras que el tipo no altruista se desentiende. Para más de tres pasos, mientras el agente altruista continúa contribuyendo al bote público dependiendo de qué tan cercano esté su retorno privado marginal a 1, el tipo no altruista prescribe la deserción. Analizamos las decisiones de un agente altruista i modelado usando un nivel 1 I-DID expandido en 3 pasos de tiempo. i atribuye los dos modelos de nivel 0, mencionados anteriormente, a j (ver Fig. 9). Si creo con una probabilidad de 1 que j es altruista, elijo contribuir en cada uno de los tres pasos. Este comportamiento persiste cuando i no está al tanto de si j es altruista (Fig. 10(a)), y cuando i asigna una alta probabilidad a que j sea del tipo no altruista. Sin embargo, cuando i cree con una probabilidad de 1 que j no es altruista y por lo tanto seguramente va a traicionar, i elige traicionar para evitar ser castigado y porque su beneficio privado marginal es menor que 1. Estos resultados demuestran que el comportamiento de nuestro tipo altruista se asemeja al encontrado experimentalmente. El agente de nivel 1 no altruista elige desertar independientemente de lo altruista que crea que sea el otro agente. Analizamos el comportamiento de un tipo de agente recíproco que se ajusta a la cooperación o defección esperada. El retorno privado marginal del tipo recíproco es similar al del tipo no altruista, sin embargo, obtiene una mayor recompensa cuando su acción es similar a la del otro. Consideramos el caso en el que el agente recíproco i no está seguro de si j es altruista y cree que es probable que el bote público esté medio lleno. Para esta creencia previa, yo elijo defectar. Al recibir una observación de abundancia, decide contribuir, mientras que una observación de escasez lo hace defectar (Fig. 10(b)). Esto se debe a que una observación de abundancia indica que es probable que la olla esté más llena que a la mitad, lo cual resulta de la acción de js para contribuir. Por lo tanto, entre los dos modelos atribuidos a j, es probable que su tipo sea altruista, lo que hace probable que j contribuya nuevamente en el próximo paso de tiempo. El agente i, por lo tanto, elige contribuir para corresponder la acción de js. Un razonamiento análogo lleva a uno a defectar cuando observa una olla escasa. Con una acción por hacer, creo que si j contribuye, también elegiré contribuir para evitar castigo independientemente de sus observaciones. Figura 10: (a) Un agente altruista de nivel 1 siempre contribuye. (b) Un agente recíproco i comienza defraudando y luego elige contribuir o defraudar basándose en su observación de abundancia (indicando que j es probablemente altruista) o escasez (j no es altruista). 5.3 Estrategias en el póker de dos jugadores El póker es un popular juego de cartas de suma cero que ha recibido mucha atención en la comunidad de investigación de IA como un banco de pruebas [2]. El póker se juega entre M ≥ 2 jugadores, en el que cada jugador recibe una mano de cartas de una baraja. Si bien existen varias variantes de póker con diferentes niveles de complejidad, consideramos una versión simple en la que cada jugador tiene tres turnos durante los cuales el jugador puede intercambiar una carta (E), mantener la mano existente (K), retirarse (F) y abandonar el juego, o apostar (C), lo que requiere que todos los jugadores muestren sus manos. Para mantener las cosas simples, dejemos que M = 2, y que cada jugador reciba una mano compuesta por una sola carta sacada del mismo palo. Por lo tanto, durante un enfrentamiento, el jugador que tenga la carta numéricamente más alta (el 2 es el más bajo, el as es el más alto) gana el bote. Durante un intercambio de cartas, la carta descartada se coloca ya sea en la pila L, indicando al otro agente que era una carta de número bajo menor que 8, o en la 820 The Sixth Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) H, indicando que la carta tenía un rango mayor o igual a 8. Ten en cuenta que, por ejemplo, si se descarta una carta de número bajo, la probabilidad de recibir una carta baja a cambio se reduce. Mostramos el nivel 1 I-ID para el Poker simplificado de dos jugadores en la Fig. 11. Consideramos dos modelos (tipos de personalidad) del agente j. El tipo conservador cree que es probable que su oponente tenga una carta de alto número en su mano. Por otro lado, el agente agresivo j cree con alta probabilidad que su oponente tiene una carta de número más bajo. Por lo tanto, los dos tipos difieren en sus creencias sobre la mano de sus oponentes. En ambos estos modelos de nivel 0, se asume que el oponente realiza sus acciones siguiendo una distribución fija y uniforme. Con tres acciones restantes, independientemente de su mano (a menos que sea un as), el agente agresivo elige intercambiar su carta, con la intención de mejorar su mano actual. Esto se debe a que cree que el otro tiene una carta baja, lo que mejora sus posibilidades de obtener una carta alta durante el intercambio. El agente conservador elige quedarse con su carta, sin importar su mano, porque considera que sus posibilidades de obtener una carta alta son escasas, ya que cree que su oponente tiene una. Figura 11: (a) Nivel 1 I-ID del agente i. La observación revela información sobre la mano de js del paso de tiempo anterior, (b) IDs de nivel 0 del agente j cuyos nodos de decisión están mapeados a los nodos de probabilidad, A1 j , A2 j , en (a). La política de un agente de nivel 1 i que cree que cada carta, excepto la suya, tiene la misma probabilidad de estar en su mano (tipo de personalidad neutral) y j podría ser de tipo agresivo o conservador, se muestra en la Figura 12. Su mano contiene la carta numerada 8. El agente comienza por mantener su carta. Al ver que j no intercambió una carta (N), i cree con probabilidad 1 que j es conservador y, por lo tanto, mantendrá sus cartas. i responde manteniendo su carta o intercambiándola porque j tiene igual probabilidad de tener una carta más baja o más alta. Si observo que j descartó su carta en la pila L o H, creo que j es agresivo. Al observar a L, i se da cuenta de que j tenía una carta baja, y es probable que tenga una carta alta después de su intercambio. Dado que la probabilidad de recibir una carta baja es alta en este momento, i decide quedarse con su carta. Al observar la carta H, creyendo que la probabilidad de recibir una carta de número alto es alta, i decide intercambiar su carta. En el paso final, i decide llamar independientemente de su historial de observaciones porque su creencia de que j tiene una carta más alta no es lo suficientemente alta como para concluir que es mejor retirarse y renunciar al pago. Esto se debe en parte al hecho de que una observación de, digamos, L, restablece las creencias del agente en el paso de tiempo anterior sobre las cartas de números bajos solamente. 6. DISCUSIÓN Mostramos cómo los DIDs pueden ser extendidos a los I-DIDs que permiten la toma de decisiones secuenciales en línea en entornos multiagentes inciertos. Nuestra representación gráfica de I-DIDs mejora la Figura 12 anterior: Un agente de nivel 1 es una política de tres pasos en el problema de Poker. i comienza creyendo que j tiene la misma probabilidad de ser agresivo o conservador y podría tener cualquier carta en su mano con igual probabilidad. Trabaja significativamente al ser más transparente, semánticamente claro y capaz de ser resuelto utilizando algoritmos estándar que apuntan a los DIDs. Los I-DIDs extienden los NIDs para permitir la toma de decisiones secuenciales a lo largo de múltiples pasos de tiempo en presencia de otros agentes que interactúan. Los I-DIDs pueden ser vistos como representaciones gráficas concisas para IPOMDPs que proporcionan una forma de explotar la estructura del problema y llevar a cabo la toma de decisiones en línea a medida que el agente actúa y observa dadas sus creencias previas. Actualmente estamos investigando formas de resolver I-DIDs aproximadamente con límites demostrables en la calidad de la solución. Agradecimiento: Agradecemos a Piotr Gmytrasiewicz por algunas discusiones útiles relacionadas con este trabajo. El primer autor desea agradecer el apoyo de una beca UGARF. 7. REFERENCIAS [1] R. J. Aumann. Epistemología interactiva i: Conocimiento. Revista Internacional de Teoría de Juegos, 28:263-300, 1999. [2] D. Billings, A. Davidson, J. Schaeffer y D. Szafron. El desafío del póker. AIJ, 2001. [3] A. Brandenburger y E. Dekel. Jerarquías de creencias y conocimiento común. Revista de Teoría Económica, 59:189-198, 1993. [4] C. Camerer. Teoría de juegos conductual: Experimentos en interacción estratégica. Princeton University Press, 2003. [5] E. Fehr y S. Gachter. Cooperación y castigo en experimentos de bienes públicos. Revista Económica Americana, 90(4):980-994, 2000. [6] D. Fudenberg y D. K. Levine. La Teoría del Aprendizaje en los Juegos. MIT Press, 1998. [7] D. Fudenberg y J. Tirole. Teoría de juegos. MIT Press, 1991. [8] Y. Gal y A. Pfeffer. Un lenguaje para modelar los procesos de toma de decisiones de agentes en juegos. En AAMAS, 2003. [9] P. Gmytrasiewicz y P. Doshi. Un marco para la planificación secuencial en entornos multiagentes. JAIR, 24:49-79, 2005. [10] P. Gmytrasiewicz y E. Durfee. Coordinación racional en entornos multiagente. JAAMAS, 3(4):319-350, 2000. [11] J. C. Harsanyi. Juegos con información incompleta jugados por jugadores bayesianos. Ciencia de la Gestión, 14(3):159-182, 1967. [12] R. A. Howard y J. E. Matheson. Diagramas de influencia. En R. A. Howard y J. E. Matheson, editores, Los Principios y Aplicaciones del Análisis de Decisiones. Grupo de Decisiones Estratégicas, Menlo Park, CA 94025, 1984. [13] L. Kaelbling, M. Littman y A. Cassandra. Planificación y actuación en dominios estocásticos parcialmente observables. Revista de Inteligencia Artificial, 2, 1998. [14] D. Koller y B. Milch. Diagramas de influencia multiagente para representar y resolver juegos. En IJCAI, páginas 1027-1034, 2001. [15] K. Polich y P. Gmytrasiewicz. Diagramas de influencia interactivos y dinámicos. En el taller GTDT, AAMAS, 2006. [16] B. Rathnas, P. Doshi y P. J. Gmytrasiewicz. Soluciones exactas para POMDP interactivos utilizando equivalencia conductual. En la Conferencia de Agentes Autónomos y Sistemas Multiagente (AAMAS), 2006. [17] S. Russell y P. Norvig. Inteligencia Artificial: Un Enfoque Moderno (Segunda Edición). Prentice Hall, 2003. [18] R. D. Shachter. \n\nPrentice Hall, 2003. [18] R. D. Shachter. Evaluando diagramas de influencia. Investigación de Operaciones, 34(6):871-882, 1986. [19] D. Suryadi y P. Gmytrasiewicz. Aprendiendo modelos de otros agentes utilizando diagramas de influencia. En UM, 1999. El Sexto Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 821 ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "agent online": {
            "translated_key": "agente en línea",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Graphical Models for Online Solutions to Interactive POMDPs Prashant Doshi Dept.",
                "of Computer Science University of Georgia Athens, GA 30602, USA pdoshi@cs.uga.edu Yifeng Zeng Dept. of Computer Science Aalborg University DK-9220 Aalborg, Denmark yfzeng@cs.aau.edu Qiongyu Chen Dept. of Computer Science National Univ. of Singapore 117543, Singapore chenqy@comp.nus.edu.sg ABSTRACT We develop a new graphical representation for interactive partially observable Markov decision processes (I-POMDPs) that is significantly more transparent and semantically clear than the previous representation.",
                "These graphical models called interactive dynamic influence diagrams (I-DIDs) seek to explicitly model the structure that is often present in real-world problems by decomposing the situation into chance and decision variables, and the dependencies between the variables.",
                "I-DIDs generalize DIDs, which may be viewed as graphical representations of POMDPs, to multiagent settings in the same way that I-POMDPs generalize POMDPs.",
                "I-DIDs may be used to compute the policy of an <br>agent online</br> as the agent acts and observes in a setting that is populated by other interacting agents.",
                "Using several examples, we show how I-DIDs may be applied and demonstrate their usefulness.",
                "Categories and Subject Descriptors I.2.11 [Distributed Artificial Intelligence]: Multiagent Systems General Terms Theory 1.",
                "INTRODUCTION Interactive partially observable Markov decision processes (IPOMDPs) [9] provide a framework for sequential decision-making in partially observable multiagent environments.",
                "They generalize POMDPs [13] to multiagent settings by including the other agents computable models in the state space along with the states of the physical environment.",
                "The models encompass all information influencing the agents behaviors, including their preferences, capabilities, and beliefs, and are thus analogous to types in Bayesian games [11].",
                "I-POMDPs adopt a subjective approach to understanding strategic behavior, rooted in a decision-theoretic framework that takes a decision-makers perspective in the interaction.",
                "In [15], Polich and Gmytrasiewicz introduced interactive dynamic influence diagrams (I-DIDs) as the computational representations of I-POMDPs.",
                "I-DIDs generalize DIDs [12], which may be viewed as computational counterparts of POMDPs, to multiagents settings in the same way that I-POMDPs generalize POMDPs.",
                "I-DIDs contribute to a growing line of work [19] that includes multi-agent influence diagrams (MAIDs) [14], and more recently, networks of influence diagrams (NIDs) [8].",
                "These formalisms seek to explicitly model the structure that is often present in real-world problems by decomposing the situation into chance and decision variables, and the dependencies between the variables.",
                "MAIDs provide an alternative to normal and extensive game forms using a graphical formalism to represent games of imperfect information with a decision node for each agents actions and chance nodes capturing the agents private information.",
                "MAIDs objectively analyze the game, efficiently computing the Nash equilibrium profile by exploiting the independence structure.",
                "NIDs extend MAIDs to include agents uncertainty over the game being played and over models of the other agents.",
                "Each model is a MAID and the network of MAIDs is collapsed, bottom up, into a single MAID for computing the equilibrium of the game keeping in mind the different models of each agent.",
                "Graphical formalisms such as MAIDs and NIDs open up a promising area of research that aims to represent multiagent interactions more transparently.",
                "However, MAIDs provide an analysis of the game from an external viewpoint and the applicability of both is limited to static single play games.",
                "Matters are more complex when we consider interactions that are extended over time, where predictions about others future actions must be made using models that change as the agents act and observe.",
                "I-DIDs address this gap by allowing the representation of other agents models as the values of a special model node.",
                "Both, other agents models and the original agents beliefs over these models are updated over time using special-purpose implementations.",
                "In this paper, we improve on the previous preliminary representation of the I-DID shown in [15] by using the insight that the static I-ID is a type of NID.",
                "Thus, we may utilize NID-specific language constructs such as multiplexers to represent the model node, and subsequently the I-ID, more transparently.",
                "Furthermore, we clarify the semantics of the special purpose policy link introduced in the representation of I-DID by [15], and show that it could be replaced by traditional dependency links.",
                "In the previous representation of the I-DID, the update of the agents belief over the models of others as the agents act and receive observations was denoted using a special link called the model update link that connected the model nodes over time.",
                "We explicate the semantics of this link by showing how it can be implemented using the traditional dependency links between the chance nodes that constitute the model nodes.",
                "The net result is a representation of I-DID that is significantly more transparent, semantically clear, and capable of being implemented using the standard algorithms for solving DIDs.",
                "We show how IDIDs may be used to model an agents uncertainty over others models, that may themselves be I-DIDs.",
                "Solution to the I-DID is a policy that prescribes what the agent should do over time, given its beliefs over the physical state and others models.",
                "Analogous to DIDs, I-DIDs may be used to compute the policy of an <br>agent online</br> as the agent acts and observes in a setting that is populated by other interacting agents. 2.",
                "BACKGROUND: FINITELY NESTED IPOMDPS Interactive POMDPs generalize POMDPs to multiagent settings by including other agents models as part of the state space [9].",
                "Since other agents may also reason about others, the interactive state space is strategically nested; it contains beliefs about other agents models and their beliefs about others.",
                "For simplicity of presentation we consider an agent, i, that is interacting with one other agent, j.",
                "A finitely nested I-POMDP of agent i with a strategy level l is defined as the tuple: I-POMDPi,l = ISi,l, A, Ti, Ωi, Oi, Ri where: • ISi,l denotes a set of interactive states defined as, ISi,l = S × Mj,l−1, where Mj,l−1 = {Θj,l−1 ∪ SMj}, for l ≥ 1, and ISi,0 = S, where S is the set of states of the physical environment.",
                "Θj,l−1 is the set of computable intentional models of agent j: θj,l−1 = bj,l−1, ˆθj where the frame, ˆθj = A, Ωj, Tj, Oj, Rj, OCj .",
                "Here, j is Bayes rational and OCj is js optimality criterion.",
                "SMj is the set of subintentional models of j.",
                "Simple examples of subintentional models include a no-information model [10] and a fictitious play model [6], both of which are history independent.",
                "We give a recursive bottom-up construction of the interactive state space below.",
                "ISi,0 = S, Θj,0 = { bj,0, ˆθj | bj,0 ∈ Δ(ISj,0)} ISi,1 = S × {Θj,0 ∪ SMj}, Θj,1 = { bj,1, ˆθj | bj,1 ∈ Δ(ISj,1)} . . . . . .",
                "ISi,l = S × {Θj,l−1 ∪ SMj}, Θj,l = { bj,l, ˆθj | bj,l ∈ Δ(ISj,l)} Similar formulations of nested spaces have appeared in [1, 3]. • A = Ai × Aj is the set of joint actions of all agents in the environment; • Ti : S ×A×S → [0, 1], describes the effect of the joint actions on the physical states of the environment; • Ωi is the set of observations of agent i; • Oi : S × A × Ωi → [0, 1] gives the likelihood of the observations given the physical state and joint action; • Ri : ISi × A → R describes agent is preferences over its interactive states.",
                "Usually only the physical states will matter.",
                "Agent is policy is the mapping, Ω∗ i → Δ(Ai), where Ω∗ i is the set of all observation histories of agent i.",
                "Since belief over the interactive states forms a sufficient statistic [9], the policy can also be represented as a mapping from the set of all beliefs of agent i to a distribution over its actions, Δ(ISi) → Δ(Ai). 2.1 Belief Update Analogous to POMDPs, an agent within the I-POMDP framework updates its belief as it acts and observes.",
                "However, there are two differences that complicate the belief update in multiagent settings when compared to single agent ones.",
                "First, since the state of the physical environment depends on the actions of both agents, is prediction of how the physical state changes has to be made based on its prediction of js actions.",
                "Second, changes in js models have to be included in is belief update.",
                "Specifically, if j is intentional then an update of js beliefs due to its action and observation has to be included.",
                "In other words, i has to update its belief based on its prediction of what j would observe and how j would update its belief.",
                "If js model is subintentional, then js probable observations are appended to the observation history contained in the model.",
                "Formally, we have: Pr(ist |at−1 i , bt−1 i,l ) = β ISt−1:mt−1 j =θt j bt−1 i,l (ist−1 ) × at−1 j Pr(at−1 j |θt−1 j,l−1)Oi(st , at−1 i , at−1 j , ot i) ×Ti(st−1 , at−1 i , at−1 j , st ) ot j Oj(st , at−1 i , at−1 j , ot j) ×τ(SEθt j (bt−1 j,l−1, at−1 j , ot j) − bt j,l−1) (1) where β is the normalizing constant, τ is 1 if its argument is 0 otherwise it is 0, Pr(at−1 j |θt−1 j,l−1) is the probability that at−1 j is Bayes rational for the agent described by model θt−1 j,l−1, and SE(·) is an abbreviation for the belief update.",
                "For a version of the belief update when js model is subintentional, see [9].",
                "If agent j is also modeled as an I-POMDP, then is belief update invokes js belief update (via the term SEθt j ( bt−1 j,l−1 , at−1 j , ot j)), which in turn could invoke is belief update and so on.",
                "This recursion in belief nesting bottoms out at the 0th level.",
                "At this level, the belief update of the agent reduces to a POMDP belief update. 1 For illustrations of the belief update, additional details on I-POMDPs, and how they compare with other multiagent frameworks, see [9]. 2.2 Value Iteration Each belief state in a finitely nested I-POMDP has an associated value reflecting the maximum payoff the agent can expect in this belief state: Un ( bi,l, θi ) = max ai∈Ai is∈ISi,l ERi(is, ai)bi,l(is)+ γ oi∈Ωi Pr(oi|ai, bi,l)Un−1 ( SEθi (bi,l, ai, oi), θi ) (2) where, ERi(is, ai) = aj Ri(is, ai, aj)Pr(aj|mj,l−1) (since is = (s, mj,l−1)).",
                "Eq. 2 is a basis for value iteration in I-POMDPs.",
                "Agent is optimal action, a∗ i , for the case of finite horizon with discounting, is an element of the set of optimal actions for the belief state, OPT(θi), defined as: OPT( bi,l, θi ) = argmax ai∈Ai is∈ISi,l ERi(is, ai)bi,l(is) +γ oi∈Ωi Pr(oi|ai, bi,l)Un ( SEθi (bi,l, ai, oi), θi ) (3) 3.",
                "INTERACTIVEINFLUENCEDIAGRAMS A naive extension of influence diagrams (IDs) to settings populated by multiple agents is possible by treating other agents as automatons, represented using chance nodes.",
                "However, this approach assumes that the agents actions are controlled using a probability distribution that does not change over time.",
                "Interactive influence diagrams (I-IDs) adopt a more sophisticated approach by generalizing IDs to make them applicable to settings shared with other agents who may act and observe, and update their beliefs. 3.1 Syntax In addition to the usual chance, decision, and utility nodes, IIDs include a new type of node called the model node.",
                "We show a general level l I-ID in Fig. 1(a), where the model node (Mj,l−1) is denoted using a hexagon.",
                "We note that the probability distribution over the chance node, S, and the model node together represents agent is belief over its interactive states.",
                "In addition to the model 1 The 0th level model is a POMDP: Other agents actions are treated as exogenous events and folded into the T, O, and R functions.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 815 Figure 1: (a) A generic level l I-ID for agent i situated with one other agent j.",
                "The hexagon is the model node (Mj,l−1) whose structure we show in (b).",
                "Members of the model node are I-IDs themselves (m1 j,l−1, m2 j,l−1; diagrams not shown here for simplicity) whose decision nodes are mapped to the corresponding chance nodes (A1 j , A2 j ).",
                "Depending on the value of the node, Mod[Mj], the distribution of each of the chance nodes is assigned to the node Aj. (c) The transformed I-ID with the model node replaced by the chance nodes and the relationships between them. node, I-IDs differ from IDs by having a dashed link (called the policy link in [15]) between the model node and a chance node, Aj, that represents the distribution over the other agents actions given its model.",
                "In the absence of other agents, the model node and the chance node, Aj, vanish and I-IDs collapse into traditional IDs.",
                "The model node contains the alternative computational models ascribed by i to the other agent from the set, Θj,l−1 ∪ SMj, where Θj,l−1 and SMj were defined previously in Section 2.",
                "Thus, a model in the model node may itself be an I-ID or ID, and the recursion terminates when a model is an ID or subintentional.",
                "Because the model node contains the alternative models of the other agent as its values, its representation is not trivial.",
                "In particular, some of the models within the node are I-IDs that when solved generate the agents optimal policy in their decision nodes.",
                "Each decision node is mapped to the corresponding chance node, say A1 j , in the following way: if OPT is the set of optimal actions obtained by solving the I-ID (or ID), then Pr(aj ∈ A1 j ) = 1 |OP T | if aj ∈ OPT, 0 otherwise.",
                "Borrowing insights from previous work [8], we observe that the model node and the dashed policy link that connects it to the chance node, Aj, could be represented as shown in Fig. 1(b).",
                "The decision node of each level l − 1 I-ID is transformed into a chance node, as we mentioned previously, so that the actions with the largest value in the decision node are assigned uniform probabilities in the chance node while the rest are assigned zero probability.",
                "The different chance nodes (A1 j , A2 j ), one for each model, and additionally, the chance node labeled Mod[Mj] form the parents of the chance node, Aj.",
                "Thus, there are as many action nodes (A1 j , A2 j ) in Mj,l−1 as the number of models in the support of agent is beliefs.",
                "The conditional probability table of the chance node, Aj, is a multiplexer that assumes the distribution of each of the action nodes (A1 j , A2 j ) depending on the value of Mod[Mj].",
                "The values of Mod[Mj] denote the different models of j.",
                "In other words, when Mod[Mj] has the value m1 j,l−1, the chance node Aj assumes the distribution of the node A1 j , and Aj assumes the distribution of A2 j when Mod[Mj] has the value m2 j,l−1.",
                "The distribution over the node, Mod[Mj], is the agent is belief over the models of j given a physical state.",
                "For more agents, we will have as many model nodes as there are agents.",
                "Notice that Fig. 1(b) clarifies the semantics of the policy link, and shows how it can be represented using the traditional dependency links.",
                "In Fig. 1(c), we show the transformed I-ID when the model node is replaced by the chance nodes and relationships between them.",
                "In contrast to the representation in [15], there are no special-purpose policy links, rather the I-ID is composed of only those types of nodes that are found in traditional IDs and dependency relationships between the nodes.",
                "This allows I-IDs to be represented and implemented using conventional application tools that target IDs.",
                "Note that we may view the level l I-ID as a NID.",
                "Specifically, each of the level l − 1 models within the model node are blocks in the NID (see Fig. 2).",
                "If the level l = 1, each block is a traditional ID, otherwise if l > 1, each block within the NID may itself be a NID.",
                "Note that within the I-IDs (or IDs) at each level, there is only a single decision node.",
                "Thus, our NID does not contain any MAIDs.",
                "Figure 2: A level l I-ID represented as a NID.",
                "The probabilities assigned to the blocks of the NID are is beliefs over js models conditioned on a physical state. 3.2 Solution The solution of an I-ID proceeds in a bottom-up manner, and is implemented recursively.",
                "We start by solving the level 0 models, which, if intentional, are traditional IDs.",
                "Their solutions provide probability distributions over the other agents actions, which are entered in the corresponding chance nodes found in the model node of the level 1 I-ID.",
                "The mapping from the level 0 models decision nodes to the chance nodes is carried out so that actions with the largest value in the decision node are assigned uniform probabilities in the chance node while the rest are assigned zero probability.",
                "Given the distributions over the actions within the different chance nodes (one for each model of the other agent), the level 1 I-ID is transformed as shown in Fig. 1(c).",
                "During the transformation, the conditional probability table (CPT) of the node, Aj, is populated such that the node assumes the distribution of each of the chance nodes depending on the value of the node, Mod[Mj].",
                "As we mentioned previously, the values of the node Mod[Mj] denote the different models of the other agent, and its distribution is the agent is belief over the models of j conditioned on the physical state.",
                "The transformed level 1 I-ID is a traditional ID that may be solved us816 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) (a) (b) Figure 3: (a) A generic two time-slice level l I-DID for agent i in a setting with one other agent j.",
                "Notice the dotted model update link that denotes the update of the models of j and the distribution over the models over time. (b) The semantics of the model update link. ing the standard expected utility maximization method [18].",
                "This procedure is carried out up to the level l I-ID whose solution gives the non-empty set of optimal actions that the agent should perform given its belief.",
                "Notice that analogous to IDs, I-IDs are suitable for online decision-making when the agents current belief is known. 4.",
                "INTERACTIVE DYNAMIC INFLUENCE DIAGRAMS Interactive dynamic influence diagrams (I-DIDs) extend I-IDs (and NIDs) to allow sequential decision-making over several time steps.",
                "Just as DIDs are structured graphical representations of POMDPs, I-DIDs are the graphical online analogs for finitely nested I-POMDPs.",
                "I-DIDs may be used to optimize over a finite look-ahead given initial beliefs while interacting with other, possibly similar, agents. 4.1 Syntax We depict a general two time-slice I-DID in Fig. 3(a).",
                "In addition to the model nodes and the dashed policy link, what differentiates an I-DID from a DID is the model update link shown as a dotted arrow in Fig. 3(a).",
                "We explained the semantics of the model node and the policy link in the previous section; we describe the model updates next.",
                "The update of the model node over time involves two steps: First, given the models at time t, we identify the updated set of models that reside in the model node at time t + 1.",
                "Recall from Section 2 that an agents intentional model includes its belief.",
                "Because the agents act and receive observations, their models are updated to reflect their changed beliefs.",
                "Since the set of optimal actions for a model could include all the actions, and the agent may receive any one of |Ωj| possible observations, the updated set at time step t + 1 will have at most |Mt j,l−1||Aj||Ωj| models.",
                "Here, |Mt j,l−1| is the number of models at time step t, |Aj| and |Ωj| are the largest spaces of actions and observations respectively, among all the models.",
                "Second, we compute the new distribution over the updated models given the original distribution and the probability of the agent performing the action and receiving the observation that led to the updated model.",
                "These steps are a part of agent is belief update formalized using Eq. 1.",
                "In Fig. 3(b), we show how the dotted model update link is implemented in the I-DID.",
                "If each of the two level l − 1 models ascribed to j at time step t results in one action, and j could make one of two possible observations, then the model node at time step t + 1 contains four updated models (mt+1,1 j,l−1 ,mt+1,2 j,l−1 , mt+1,3 j,l−1 , and mt+1,4 j,l−1 ).",
                "These models differ in their initial beliefs, each of which is the result of j updating its beliefs due to its action and a possible observation.",
                "The decision nodes in each of the I-DIDs or DIDs that represent the lower level models are mapped to the corresponding Figure 4: Transformed I-DID with the model nodes and model update link replaced with the chance nodes and the relationships (in bold). chance nodes, as mentioned previously.",
                "Next, we describe how the distribution over the updated set of models (the distribution over the chance node Mod[Mt+1 j ] in Mt+1 j,l−1) is computed.",
                "The probability that js updated model is, say mt+1,1 j,l−1 , depends on the probability of j performing the action and receiving the observation that led to this model, and the prior distribution over the models at time step t. Because the chance node At j assumes the distribution of each of the action nodes based on the value of Mod[Mt j ], the probability of the action is given by this chance node.",
                "In order to obtain the probability of js possible observation, we introduce the chance node Oj, which depending on the value of Mod[Mt j ] assumes the distribution of the observation node in the lower level model denoted by Mod[Mt j ].",
                "Because the probability of js observations depends on the physical state and the joint actions of both agents, the node Oj is linked with St+1 , At j, and At i. 2 Analogous to At j, the conditional probability table of Oj is also a multiplexer modulated by Mod[Mt j ].",
                "Finally, the distribution over the prior models at time t is obtained from the chance node, Mod[Mt j ] in Mt j,l−1.",
                "Consequently, the chance nodes, Mod[Mt j ], At j, and Oj, form the parents of Mod[Mt+1 j ] in Mt+1 j,l−1.",
                "Notice that the model update link may be replaced by the dependency links between the chance nodes that constitute the model nodes in the two time slices.",
                "In Fig. 4 we show the two time-slice I-DID with the model nodes replaced by the chance nodes and the relationships between them.",
                "Chance nodes and dependency links that not in bold are standard, usually found in DIDs.",
                "Expansion of the I-DID over more time steps requires the repetition of the two steps of updating the set of models that form the 2 Note that Oj represents js observation at time t + 1.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 817 values of the model node and adding the relationships between the chance nodes, as many times as there are model update links.",
                "We note that the possible set of models of the other agent j grows exponentially with the number of time steps.",
                "For example, after T steps, there may be at most |Mt=1 j,l−1|(|Aj||Ωj|)T −1 candidate models residing in the model node. 4.2 Solution Analogous to I-IDs, the solution to a level l I-DID for agent i expanded over T time steps may be carried out recursively.",
                "For the purpose of illustration, let l=1 and T=2.",
                "The solution method uses the standard look-ahead technique, projecting the agents action and observation sequences forward from the current belief state [17], and finding the possible beliefs that i could have in the next time step.",
                "Because agent i has a belief over js models as well, the lookahead includes finding out the possible models that j could have in the future.",
                "Consequently, each of js subintentional or level 0 models (represented using a standard DID) in the first time step must be solved to obtain its optimal set of actions.",
                "These actions are combined with the set of possible observations that j could make in that model, resulting in an updated set of candidate models (that include the updated beliefs) that could describe the behavior of j. Beliefs over this updated set of candidate models are calculated using the standard inference methods using the dependency relationships between the model nodes as shown in Fig. 3(b).",
                "We note the recursive nature of this solution: in solving agent is level 1 I-DID, js level 0 DIDs must be solved.",
                "If the nesting of models is deeper, all models at all levels starting from 0 are solved in a bottom-up manner.",
                "We briefly outline the recursive algorithm for solving agent is Algorithm for solving I-DID Input : level l ≥ 1 I-ID or level 0 ID, T Expansion Phase 1.",
                "For t from 1 to T − 1 do 2.",
                "If l ≥ 1 then Populate Mt+1 j,l−1 3.",
                "For each mt j in Range(Mt j,l−1) do 4.",
                "Recursively call algorithm with the l − 1 I-ID (or ID) that represents mt j and the horizon, T − t + 1 5.",
                "Map the decision node of the solved I-ID (or ID), OPT(mt j), to a chance node Aj 6.",
                "For each aj in OPT(mt j) do 7.",
                "For each oj in Oj (part of mt j) do 8.",
                "Update js belief, bt+1 j ← SE(bt j, aj, oj) 9. mt+1 j ← New I-ID (or ID) with bt+1 j as the initial belief 10.",
                "Range(Mt+1 j,l−1) ∪ ← {mt+1 j } 11.",
                "Add the model node, Mt+1 j,l−1, and the dependency links between Mt j,l−1 and Mt+1 j,l−1 (shown in Fig. 3(b)) 12.",
                "Add the chance, decision, and utility nodes for t + 1 time slice and the dependency links between them 13.",
                "Establish the CPTs for each chance node and utility node Look-Ahead Phase 14.",
                "Apply the standard look-ahead and backup method to solve the expanded I-DID Figure 5: Algorithm for solving a level l ≥ 0 I-DID. level l I-DID expanded over T time steps with one other agent j in Fig. 5.",
                "We adopt a two-phase approach: Given an I-ID of level l (described previously in Section 3) with all lower level models also represented as I-IDs or IDs (if level 0), the first step is to expand the level l I-ID over T time steps adding the dependency links and the conditional probability tables for each node.",
                "We particularly focus on establishing and populating the model nodes (lines 3-11).",
                "Note that Range(·) returns the values (lower level models) of the random variable given as input (model node).",
                "In the second phase, we use a standard look-ahead technique projecting the action and observation sequences over T time steps in the future, and backing up the utility values of the reachable beliefs.",
                "Similar to I-IDs, the I-DIDs reduce to DIDs in the absence of other agents.",
                "As we mentioned previously, the 0-th level models are the traditional DIDs.",
                "Their solutions provide probability distributions over actions of the agent modeled at that level to I-DIDs at level 1.",
                "Given probability distributions over other agents actions the level 1 IDIDs can themselves be solved as DIDs, and provide probability distributions to yet higher level models.",
                "Assume that the number of models considered at each level is bound by a number, M. Solving an I-DID of level l in then equivalent to solving O(Ml ) DIDs. 5.",
                "EXAMPLE APPLICATIONS To illustrate the usefulness of I-DIDs, we apply them to three problem domains.",
                "We describe, in particular, the formulation of the I-DID and the optimal prescriptions obtained on solving it. 5.1 Followership-Leadership in the Multiagent Tiger Problem We begin our illustrations of using I-IDs and I-DIDs with a slightly modified version of the multiagent tiger problem discussed in [9].",
                "The problem has two agents, each of which can open the right door (OR), the left door (OL) or listen (L).",
                "In addition to hearing growls (from the left (GL) or from the right (GR)) when they listen, the agents also hear creaks (from the left (CL), from the right (CR), or no creaks (S)), which noisily indicate the other agents opening one of the doors.",
                "When any door is opened, the tiger persists in its original location with a probability of 95%.",
                "Agent i hears growls with a reliability of 65% and creaks with a reliability of 95%.",
                "Agent j, on the other hand, hears growls with a reliability of 95%.",
                "Thus, the setting is such that agent i hears agent j opening doors more reliably than the tigers growls.",
                "This suggests that i could use js actions as an indication of the location of the tiger, as we discuss below.",
                "Each agents preferences are as in the single agent game discussed in [13].",
                "The transition, observation, and reward functions are shown in [16].",
                "A good indicator of the usefulness of normative methods for decision-making like I-DIDs is the emergence of realistic social behaviors in their prescriptions.",
                "In settings of the persistent multiagent tiger problem that reflect real world situations, we demonstrate followership between the agents and, as shown in [15], deception among agents who believe that they are in a follower-leader type of relationship.",
                "In particular, we analyze the situational and epistemological conditions sufficient for their emergence.",
                "The followership behavior, for example, results from the agent knowing its own weaknesses, assessing the strengths, preferences, and possible behaviors of the other, and realizing that its best for it to follow the others actions in order to maximize its payoffs.",
                "Let us consider a particular setting of the tiger problem in which agent i believes that js preferences are aligned with its own - both of them just want to get the gold - and js hearing is more reliable in comparison to itself.",
                "As an example, suppose that j, on listening can discern the tigers location 95% of the times compared to is 65% accuracy.",
                "Additionally, agent i does not have any initial information about the tigers location.",
                "In other words, is single-level nested belief, bi,1, assigns 0.5 to each of the two locations of the tiger.",
                "In addition, i considers two models of j, which differ in js flat level 0 initial beliefs.",
                "This is represented in the level 1 I-ID shown in Fig. 6(a).",
                "According to one model, j assigns a probability of 0.9 that the tiger is behind the left door, while the other 818 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Figure 6: (a) Level 1 I-ID of agent i, (b) two level 0 IDs of agent j whose decision nodes are mapped to the chance nodes, A1 j , A2 j , in (a). model assigns 0.1 to that location (see Fig. 6(b)).",
                "Agent i is undecided on these two models of j.",
                "If we vary is hearing ability, and solve the corresponding level 1 I-ID expanded over three time steps, we obtain the normative behavioral policies shown in Fig 7 that exhibit followership behavior.",
                "If is probability of correctly hearing the growls is 0.65, then as shown in the policy in Fig. 7(a), i begins to conditionally follow js actions: i opens the same door that j opened previously iff is own assessment of the tigers location confirms js pick.",
                "If i loses the ability to correctly interpret the growls completely, it blindly follows j and opens the same door that j opened previously (Fig. 7(b)).",
                "Figure 7: Emergence of (a) conditional followership, and (b) blind followership in the tiger problem.",
                "Behaviors of interest are in bold. * is a wildcard, and denotes any one of the observations.",
                "We observed that a single level of belief nesting - beliefs about the others models - was sufficient for followership to emerge in the tiger problem.",
                "However, the epistemological requirements for the emergence of leadership are more complex.",
                "For an agent, say j, to emerge as a leader, followership must first emerge in the other agent i.",
                "As we mentioned previously, if i is certain that its preferences are identical to those of j, and believes that j has a better sense of hearing, i will follow js actions over time.",
                "Agent j emerges as a leader if it believes that i will follow it, which implies that js belief must be nested two levels deep to enable it to recognize its leadership role.",
                "Realizing that i will follow presents j with an opportunity to influence is actions in the benefit of the collective good or its self-interest alone.",
                "For example, in the tiger problem, let us consider a setting in which if both i and j open the correct door, then each gets a payoff of 20 that is double the original.",
                "If j alone selects the correct door, it gets the payoff of 10.",
                "On the other hand, if both agents pick the wrong door, their penalties are cut in half.",
                "In this setting, it is in both js best interest as well as the collective betterment for j to use its expertise in selecting the correct door, and thus be a good leader.",
                "However, consider a slightly different problem in which j gains from is loss and is penalized if i gains.",
                "Specifically, let is payoff be subtracted from js, indicating that j is antagonistic toward i - if j picks the correct door and i the wrong one, then is loss of 100 becomes js gain.",
                "Agent j believes that i incorrectly thinks that js preferences are those that promote the collective good and that it starts off by believing with 99% confidence where the tiger is.",
                "Because i believes that its preferences are similar to those of j, and that j starts by believing almost surely that one of the two is the correct location (two level 0 models of j), i will start by following js actions.",
                "We show is normative policy on solving its singly-nested I-DID over three time steps in Fig. 8(a).",
                "The policy demonstrates that i will blindly follow js actions.",
                "Since the tiger persists in its original location with a probability of 0.95, i will select the same door again.",
                "If j begins the game with a 99% probability that the tiger is on the right, solving js I-DID nested two levels deep, results in the policy shown in Fig. 8(b).",
                "Even though j is almost certain that OL is the correct action, it will start by selecting OR, followed by OL.",
                "Agent js intention is to deceive i who, it believes, will follow js actions, so as to gain $110 in the second time step, which is more than what j would gain if it were to be honest.",
                "Figure 8: Emergence of deception between agents in the tiger problem.",
                "Behaviors of interest are in bold. * denotes as before. (a) Agent is policy demonstrating that it will blindly follow js actions. (b) Even though j is almost certain that the tiger is on the right, it will start by selecting OR, followed by OL, in order to deceive i. 5.2 Altruism and Reciprocity in the Public Good Problem The public good (PG) problem [7], consists of a group of M agents, each of whom must either contribute some resource to a public pot or keep it for themselves.",
                "Since resources contributed to the public pot are shared among all the agents, they are less valuable to the agent when in the public pot.",
                "However, if all agents choose to contribute their resources, then the payoff to each agent is more than if no one contributes.",
                "Since an agent gets its share of the public pot irrespective of whether it has contributed or not, the dominating action is for each agent to not contribute, and instead free ride on others contributions.",
                "However, behaviors of human players in empirical simulations of the PG problem differ from the normative predictions.",
                "The experiments reveal that many players initially contribute a large amount to the public pot, and continue to contribute when the PG problem is played repeatedly, though in decreasing amounts [4].",
                "Many of these experiments [5] report that a small core group of players persistently contributes to the public pot even when all others are defecting.",
                "These experiments also reveal that players who persistently contribute have altruistic or reciprocal preferences matching expected cooperation of others.",
                "For simplicity, we assume that the game is played between M = 2 agents, i and j.",
                "Let each agent be initially endowed with XT amount of resources.",
                "While the classical PG game formulation permits each agent to contribute any quantity of resources (≤ XT ) to the public pot, we simplify the action space by allowing two possible actions.",
                "Each agent may choose to either contribute (C) a fixed amount of the resources, or not contribute.",
                "The latter action is deThe Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 819 noted as defect (D).",
                "We assume that the actions are not observable to others.",
                "The value of resources in the public pot is discounted by ci for each agent i, where ci is the marginal private return.",
                "We assume that ci < 1 so that the agent does not benefit enough that it contributes to the public pot for private gain.",
                "Simultaneously, ciM > 1, making collective contribution pareto optimal. i/j C D C 2ciXT , 2cjXT ciXT − cp, XT + cjXT − P D XT + ciXT − P, cjXT − cp XT , XT Table 1: The one-shot PG game with punishment.",
                "In order to encourage contributions, the contributing agents punish free riders but incur a small cost for administering the punishment.",
                "Let P be the punishment meted out to the defecting agent and cp the non-zero cost of punishing for the contributing agent.",
                "For simplicity, we assume that the cost of punishing is same for both the agents.",
                "The one-shot PG game with punishment is shown in Table. 1.",
                "Let ci = cj, cp > 0, and if P > XT − ciXT , then defection is no longer a dominating action.",
                "If P < XT − ciXT , then defection is the dominating action for both.",
                "If P = XT − ciXT , then the game is not dominance-solvable.",
                "Figure 9: (a) Level 1 I-ID of agent i, (b) level 0 IDs of agent j with decision nodes mapped to the chance nodes, A1 j and A2 j , in (a).",
                "We formulate a sequential version of the PG problem with punishment from the perspective of agent i.",
                "Though in the repeated PG game, the quantity in the public pot is revealed to all the agents after each round of actions, we assume in our formulation that it is hidden from the agents.",
                "Each agent may contribute a fixed amount, xc, or defect.",
                "An agent on performing an action receives an observation of plenty (PY) or meager (MR) symbolizing the state of the public pot.",
                "Notice that the observations are also indirectly indicative of agent js actions because the state of the public pot is influenced by them.",
                "The amount of resources in agent is private pot, is perfectly observable to i.",
                "The payoffs are analogous to Table. 1.",
                "Borrowing from the empirical investigations of the PG problem [5], we construct level 0 IDs for j that model altruistic and non-altruistic types (Fig. 9(b)).",
                "Specifically, our altruistic agent has a high marginal private return (cj is close to 1) and does not punish others who defect.",
                "Let xc = 1 and the level 0 agent be punished half the times it defects.",
                "With one action remaining, both types of agents choose to contribute to avoid being punished.",
                "With two actions to go, the altruistic type chooses to contribute, while the other defects.",
                "This is because cj for the altruistic type is close to 1, thus the expected punishment, 0.5P > (1 − cj), which the altruistic type avoids.",
                "Because cj for the non-altruistic type is less, it prefers not to contribute.",
                "With three steps to go, the altruistic agent contributes to avoid punishment (0.5P > 2(1 − cj)), and the non-altruistic type defects.",
                "For greater than three steps, while the altruistic agent continues to contribute to the public pot depending on how close its marginal private return is to 1, the non-altruistic type prescribes defection.",
                "We analyzed the decisions of an altruistic agent i modeled using a level 1 I-DID expanded over 3 time steps. i ascribes the two level 0 models, mentioned previously, to j (see Fig. 9).",
                "If i believes with a probability 1 that j is altruistic, i chooses to contribute for each of the three steps.",
                "This behavior persists when i is unaware of whether j is altruistic (Fig. 10(a)), and when i assigns a high probability to j being the non-altruistic type.",
                "However, when i believes with a probability 1 that j is non-altruistic and will thus surely defect, i chooses to defect to avoid being punished and because its marginal private return is less than 1.",
                "These results demonstrate that the behavior of our altruistic type resembles that found experimentally.",
                "The non-altruistic level 1 agent chooses to defect regardless of how likely it believes the other agent to be altruistic.",
                "We analyzed the behavior of a reciprocal agent type that matches expected cooperation or defection.",
                "The reciprocal types marginal private return is similar to that of the non-altruistic type, however, it obtains a greater payoff when its action is similar to that of the other.",
                "We consider the case when the reciprocal agent i is unsure of whether j is altruistic and believes that the public pot is likely to be half full.",
                "For this prior belief, i chooses to defect.",
                "On receiving an observation of plenty, i decides to contribute, while an observation of meager makes it defect (Fig. 10(b)).",
                "This is because an observation of plenty signals that the pot is likely to be greater than half full, which results from js action to contribute.",
                "Thus, among the two models ascribed to j, its type is likely to be altruistic making it likely that j will contribute again in the next time step.",
                "Agent i therefore chooses to contribute to reciprocate js action.",
                "An analogous reasoning leads i to defect when it observes a meager pot.",
                "With one action to go, i believing that j contributes, will choose to contribute too to avoid punishment regardless of its observations.",
                "Figure 10: (a) An altruistic level 1 agent always contributes. (b) A reciprocal agent i starts off by defecting followed by choosing to contribute or defect based on its observation of plenty (indicating that j is likely altruistic) or meager (j is non-altruistic). 5.3 Strategies in Two-Player Poker Poker is a popular zero sum card game that has received much attention among the AI research community as a testbed [2].",
                "Poker is played among M ≥ 2 players in which each player receives a hand of cards from a deck.",
                "While several flavors of Poker with varying complexity exist, we consider a simple version in which each player has three plys during which the player may either exchange a card (E), keep the existing hand (K), fold (F) and withdraw from the game, or call (C), requiring all players to show their hands.",
                "To keep matters simple, let M = 2, and each player receive a hand consisting of a single card drawn from the same suit.",
                "Thus, during a showdown, the player who has the numerically larger card (2 is the lowest, ace is the highest) wins the pot.",
                "During an exchange of cards, the discarded card is placed either in the L pile, indicating to the other agent that it was a low numbered card less than 8, or in the 820 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) H pile, indicating that the card had a rank greater than or equal to 8.",
                "Notice that, for example, if a lower numbered card is discarded, the probability of receiving a low card in exchange is now reduced.",
                "We show the level 1 I-ID for the simplified two-player Poker in Fig. 11.",
                "We considered two models (personality types) of agent j.",
                "The conservative type believes that it is likely that its opponent has a high numbered card in its hand.",
                "On the other hand, the aggressive agent j believes with a high probability that its opponent has a lower numbered card.",
                "Thus, the two types differ in their beliefs over their opponents hand.",
                "In both these level 0 models, the opponent is assumed to perform its actions following a fixed, uniform distribution.",
                "With three actions to go, regardless of its hand (unless it is an ace), the aggressive agent chooses to exchange its card, with the intent of improving on its current hand.",
                "This is because it believes the other to have a low card, which improves its chances of getting a high card during the exchange.",
                "The conservative agent chooses to keep its card, no matter its hand because its chances of getting a high card are slim as it believes that its opponent has one.",
                "Figure 11: (a) Level 1 I-ID of agent i.",
                "The observation reveals information about js hand of the previous time step, (b) level 0 IDs of agent j whose decision nodes are mapped to the chance nodes, A1 j , A2 j , in (a).",
                "The policy of a level 1 agent i who believes that each card except its own has an equal likelihood of being in js hand (neutral personality type) and j could be either an aggressive or conservative type, is shown in Fig. 12. is own hand contains the card numbered 8.",
                "The agent starts by keeping its card.",
                "On seeing that j did not exchange a card (N), i believes with probability 1 that j is conservative and hence will keep its cards. i responds by either keeping its card or exchanging it because j is equally likely to have a lower or higher card.",
                "If i observes that j discarded its card into the L or H pile, i believes that j is aggressive.",
                "On observing L, i realizes that j had a low card, and is likely to have a high card after its exchange.",
                "Because the probability of receiving a low card is high now, i chooses to keep its card.",
                "On observing H, believing that the probability of receiving a high numbered card is high, i chooses to exchange its card.",
                "In the final step, i chooses to call regardless of its observation history because its belief that j has a higher card is not sufficiently high to conclude that its better to fold and relinquish the payoff.",
                "This is partly due to the fact that an observation of, say, L resets the agent is previous time step beliefs over js hand to the low numbered cards only. 6.",
                "DISCUSSION We showed how DIDs may be extended to I-DIDs that enable online sequential decision-making in uncertain multiagent settings.",
                "Our graphical representation of I-DIDs improves on the previous Figure 12: A level 1 agent is three step policy in the Poker problem. i starts by believing that j is equally likely to be aggressive or conservative and could have any card in its hand with equal probability. work significantly by being more transparent, semantically clear, and capable of being solved using standard algorithms that target DIDs.",
                "I-DIDs extend NIDs to allow sequential decision-making over multiple time steps in the presence of other interacting agents.",
                "I-DIDs may be seen as concise graphical representations for IPOMDPs providing a way to exploit problem structure and carry out online decision-making as the agent acts and observes given its prior beliefs.",
                "We are currently investigating ways to solve I-DIDs approximately with provable bounds on the solution quality.",
                "Acknowledgment: We thank Piotr Gmytrasiewicz for some useful discussions related to this work.",
                "The first author would like to acknowledge the support of a UGARF grant. 7.",
                "REFERENCES [1] R. J. Aumann.",
                "Interactive epistemology i: Knowledge.",
                "International Journal of Game Theory, 28:263-300, 1999. [2] D. Billings, A. Davidson, J. Schaeffer, and D. Szafron.",
                "The challenge of poker.",
                "AIJ, 2001. [3] A. Brandenburger and E. Dekel.",
                "Hierarchies of beliefs and common knowledge.",
                "Journal of Economic Theory, 59:189-198, 1993. [4] C. Camerer.",
                "Behavioral Game Theory: Experiments in Strategic Interaction.",
                "Princeton University Press, 2003. [5] E. Fehr and S. Gachter.",
                "Cooperation and punishment in public goods experiments.",
                "American Economic Review, 90(4):980-994, 2000. [6] D. Fudenberg and D. K. Levine.",
                "The Theory of Learning in Games.",
                "MIT Press, 1998. [7] D. Fudenberg and J. Tirole.",
                "Game Theory.",
                "MIT Press, 1991. [8] Y. Gal and A. Pfeffer.",
                "A language for modeling agents decision-making processes in games.",
                "In AAMAS, 2003. [9] P. Gmytrasiewicz and P. Doshi.",
                "A framework for sequential planning in multiagent settings.",
                "JAIR, 24:49-79, 2005. [10] P. Gmytrasiewicz and E. Durfee.",
                "Rational coordination in multi-agent environments.",
                "JAAMAS, 3(4):319-350, 2000. [11] J. C. Harsanyi.",
                "Games with incomplete information played by bayesian players.",
                "Management Science, 14(3):159-182, 1967. [12] R. A. Howard and J. E. Matheson.",
                "Influence diagrams.",
                "In R. A. Howard and J. E. Matheson, editors, The Principles and Applications of Decision Analysis.",
                "Strategic Decisions Group, Menlo Park, CA 94025, 1984. [13] L. Kaelbling, M. Littman, and A. Cassandra.",
                "Planning and acting in partially observable stochastic domains.",
                "Artificial Intelligence Journal, 2, 1998. [14] D. Koller and B. Milch.",
                "Multi-agent influence diagrams for representing and solving games.",
                "In IJCAI, pages 1027-1034, 2001. [15] K. Polich and P. Gmytrasiewicz.",
                "Interactive dynamic influence diagrams.",
                "In GTDT Workshop, AAMAS, 2006. [16] B.",
                "Rathnas., P. Doshi, and P. J. Gmytrasiewicz.",
                "Exact solutions to interactive pomdps using behavioral equivalence.",
                "In Autonomous Agents and Multi-Agent Systems Conference (AAMAS), 2006. [17] S. Russell and P. Norvig.",
                "Artificial Intelligence: A Modern Approach (Second Edition).",
                "Prentice Hall, 2003. [18] R. D. Shachter.",
                "Evaluating influence diagrams.",
                "Operations Research, 34(6):871-882, 1986. [19] D. Suryadi and P. Gmytrasiewicz.",
                "Learning models of other agents using influence diagrams.",
                "In UM, 1999.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 821"
            ],
            "original_annotated_samples": [
                "I-DIDs may be used to compute the policy of an <br>agent online</br> as the agent acts and observes in a setting that is populated by other interacting agents.",
                "Analogous to DIDs, I-DIDs may be used to compute the policy of an <br>agent online</br> as the agent acts and observes in a setting that is populated by other interacting agents. 2."
            ],
            "translated_annotated_samples": [
                "Los I-DIDs pueden ser utilizados para calcular la política de un <br>agente en línea</br> mientras el agente actúa y observa en un entorno poblado por otros agentes que interactúan entre sí.",
                "Análogos a los DIDs, los I-DIDs pueden ser utilizados para calcular la política de un <br>agente en línea</br> mientras el agente actúa y observa en un entorno poblado por otros agentes interactuantes. 2."
            ],
            "translated_text": "Modelos gráficos para soluciones en línea a POMDP interactivos de Prashant Doshi Dept. Departamento de Ciencias de la Computación Universidad de Georgia Athens, GA 30602, EE. UU. pdoshi@cs.uga.edu Yifeng Zeng Dept. de Ciencias de la Computación Universidad de Aalborg DK-9220 Aalborg, Dinamarca yfzeng@cs.aau.edu Qiongyu Chen Dept. de Ciencias de la Computación Universidad Nacional de Singapur 117543, Singapur chenqy@comp.nus.edu.sg RESUMEN Desarrollamos una nueva representación gráfica para procesos de decisión de Markov parcialmente observables interactivos (I-POMDPs) que es significativamente más transparente y semánticamente clara que la representación anterior. Estos modelos gráficos llamados diagramas de influencia dinámica interactiva (I-DIDs) buscan modelar explícitamente la estructura que a menudo está presente en problemas del mundo real al descomponer la situación en variables de probabilidad y decisión, y las dependencias entre las variables. Los I-DIDs generalizan los DIDs, los cuales pueden ser vistos como representaciones gráficas de POMDPs, a entornos multiagentes de la misma manera que los I-POMDPs generalizan los POMDPs. Los I-DIDs pueden ser utilizados para calcular la política de un <br>agente en línea</br> mientras el agente actúa y observa en un entorno poblado por otros agentes que interactúan entre sí. Usando varios ejemplos, mostramos cómo se pueden aplicar los I-DIDs y demostramos su utilidad. Categorías y Descriptores de Asignaturas I.2.11 [Inteligencia Artificial Distribuida]: Sistemas Multiagentes Términos Generales Teoría 1. Las Procesos de Decisión de Markov Interactivos Parcialmente Observables (IPOMDPs) proporcionan un marco para la toma de decisiones secuenciales en entornos multiagentes parcialmente observables. Generalizan los POMDPs [13] a entornos multiagentes al incluir los modelos computables de los otros agentes en el espacio de estados junto con los estados del entorno físico. Los modelos abarcan toda la información que influye en los comportamientos de los agentes, incluyendo sus preferencias, capacidades y creencias, y por lo tanto son análogos a los tipos en los juegos bayesianos [11]. I-POMDPs adoptan un enfoque subjetivo para comprender el comportamiento estratégico, arraigado en un marco de teoría de decisiones que toma la perspectiva de los tomadores de decisiones en la interacción. En [15], Polich y Gmytrasiewicz introdujeron diagramas de influencia dinámica interactivos (I-DIDs) como las representaciones computacionales de I-POMDPs. Los I-DIDs generalizan los DIDs [12], los cuales pueden ser vistos como contrapartes computacionales de POMDPs, a entornos de múltiples agentes de la misma manera que los I-POMDPs generalizan los POMDPs. Los I-DIDs contribuyen a una creciente línea de trabajo [19] que incluye diagramas de influencia multiagente (MAIDs) [14], y más recientemente, redes de diagramas de influencia (NIDs) [8]. Estos formalismos buscan modelar explícitamente la estructura que a menudo está presente en problemas del mundo real al descomponer la situación en variables de azar y decisiones, y las dependencias entre las variables. Los MAIDs proporcionan una alternativa a las formas normales y extensivas de juego utilizando un formalismo gráfico para representar juegos de información imperfecta con un nodo de decisión para las acciones de cada agente y nodos de azar que capturan la información privada de los agentes. Los MAIDs analizan objetivamente el juego, calculando eficientemente el perfil de equilibrio de Nash al explotar la estructura de independencia. NIDs extienden los MAIDs para incluir la incertidumbre de los agentes sobre el juego que se está jugando y sobre los modelos de los otros agentes. Cada modelo es un MAID y la red de MAIDs se colapsa, de abajo hacia arriba, en un solo MAID para calcular el equilibrio del juego teniendo en cuenta los diferentes modelos de cada agente. Los formalismos gráficos como MAIDs y NIDs abren una área de investigación prometedora que tiene como objetivo representar las interacciones multiagente de manera más transparente. Sin embargo, los MAIDs proporcionan un análisis del juego desde un punto de vista externo y la aplicabilidad de ambos está limitada a juegos estáticos de un solo jugador. Los asuntos son más complejos cuando consideramos interacciones que se extienden en el tiempo, donde las predicciones sobre las acciones futuras de otros deben hacerse utilizando modelos que cambian a medida que los agentes actúan y observan. Los I-DIDs abordan esta brecha al permitir la representación de modelos de otros agentes como los valores de un nodo de modelo especial. Tanto los modelos de otros agentes como las creencias originales de los agentes sobre estos modelos se actualizan con el tiempo utilizando implementaciones especializadas. En este documento, mejoramos la representación preliminar previa del I-DID mostrada en [15] utilizando la idea de que el I-ID estático es un tipo de NID. Por lo tanto, podemos utilizar construcciones de lenguaje específicas de NID, como multiplexores, para representar de manera más transparente el nodo del modelo y, posteriormente, el I-ID. Además, aclaramos la semántica del enlace de política de propósito especial introducido en la representación de I-DID por [15], y mostramos que podría ser reemplazado por enlaces de dependencia tradicionales. En la representación previa del I-DID, la actualización de la creencia de los agentes sobre los modelos de los demás a medida que los agentes actúan y reciben observaciones se denotaba utilizando un enlace especial llamado el enlace de actualización del modelo que conectaba los nodos del modelo a lo largo del tiempo. Explicamos la semántica de este enlace mostrando cómo puede ser implementado utilizando los enlaces de dependencia tradicionales entre los nodos de probabilidad que constituyen los nodos del modelo. El resultado final es una representación de I-DID que es significativamente más transparente, semánticamente clara y capaz de ser implementada utilizando los algoritmos estándar para resolver DIDs. Mostramos cómo los IDIDs pueden ser utilizados para modelar la incertidumbre de un agente sobre los modelos de otros, que a su vez pueden ser I-DIDs. La solución al I-DID es una política que prescribe lo que el agente debe hacer con el tiempo, dadas sus creencias sobre el estado físico y otros modelos. Análogos a los DIDs, los I-DIDs pueden ser utilizados para calcular la política de un <br>agente en línea</br> mientras el agente actúa y observa en un entorno poblado por otros agentes interactuantes. 2. ANTECEDENTES: Los IPOMDPS ANIDADOS FINITAMENTE generalizan los POMDPs a entornos multiagentes al incluir los modelos de otros agentes como parte del espacio de estados [9]. Dado que otros agentes también pueden razonar sobre otros, el espacio de estado interactivo está estratégicamente anidado; contiene creencias sobre los modelos de otros agentes y sus creencias sobre los demás. Para simplificar la presentación, consideramos un agente, i, que está interactuando con otro agente, j. Un I-POMDP finitamente anidado del agente i con un nivel de estrategia l se define como la tupla: I-POMDPi,l = ISi,l, A, Ti, Ωi, Oi, Ri donde: • ISi,l denota un conjunto de estados interactivos definido como, ISi,l = S × Mj,l−1, donde Mj,l−1 = {Θj,l−1 ∪ SMj}, para l ≥ 1, e ISi,0 = S, donde S es el conjunto de estados del entorno físico. Θj,l−1 es el conjunto de modelos intencionales computables del agente j: θj,l−1 = bj,l−1, ˆθj donde el marco, ˆθj = A, Ωj, Tj, Oj, Rj, OCj. Aquí, j es Bayesiano racional y OCj es el criterio de optimalidad de j. SMj es el conjunto de modelos subintencionales de j. Ejemplos simples de modelos subintencionales incluyen un modelo sin información [10] y un modelo de juego ficticio [6], ambos de los cuales son independientes de la historia. Damos una construcción recursiva de abajo hacia arriba del espacio de estados interactivo a continuación. Sí,0 = S, Θj,0 = { bj,0, ˆθj | bj,0 ∈ Δ(ISj,0)} Sí,1 = S × {Θj,0 ∪ SMj}, Θj,1 = { bj,1, ˆθj | bj,1 ∈ Δ(ISj,1)} . . . . . . Sí, l = S × {Θj,l−1 ∪ SMj}, Θj,l = { bj,l, ˆθj | bj,l ∈ Δ(ISj,l)} Formulaciones similares de espacios anidados han aparecido en [1, 3]. • A = Ai × Aj es el conjunto de acciones conjuntas de todos los agentes en el entorno; • Ti : S ×A×S → [0, 1], describe el efecto de las acciones conjuntas en los estados físicos del entorno; • Ωi es el conjunto de observaciones del agente i; • Oi : S × A × Ωi → [0, 1] da la probabilidad de las observaciones dadas el estado físico y la acción conjunta; • Ri : ISi × A → R describe las preferencias del agente sobre sus estados interactivos. Normalmente solo importarán los estados físicos. La política del agente es el mapeo, Ω∗ i → Δ(Ai), donde Ω∗ i es el conjunto de todos los historiales de observación del agente i. Dado que la creencia sobre los estados interactivos forma una estadística suficiente [9], la política también puede ser representada como un mapeo del conjunto de todas las creencias del agente i a una distribución sobre sus acciones, Δ(ISi) → Δ(Ai). 2.1 Actualización de Creencias Análogo a los POMDPs, un agente dentro del marco I-POMDP actualiza su creencia a medida que actúa y observa. Sin embargo, hay dos diferencias que complican la actualización de creencias en entornos multiagentes en comparación con los entornos de un solo agente. Primero, dado que el estado del entorno físico depende de las acciones de ambos agentes, la predicción de cómo cambia el estado físico debe hacerse basándose en la predicción de sus acciones. Segundo, los cambios en los modelos de js deben ser incluidos en la actualización de creencias. Específicamente, si j es intencional, entonces se debe incluir una actualización de las creencias de j debido a su acción y observación. En otras palabras, i tiene que actualizar su creencia basándose en su predicción de lo que j observaría y cómo j actualizaría su creencia. Si el modelo de js es subintencional, entonces las observaciones probables de js se añaden al historial de observaciones contenido en el modelo. Formalmente, tenemos: Pr(ist |at−1 i , bt−1 i,l ) = β ISt−1:mt−1 j =θt j bt−1 i,l (ist−1 ) × at−1 j Pr(at−1 j |θt−1 j,l−1)Oi(st , at−1 i , at−1 j , ot i) ×Ti(st−1 , at−1 i , at−1 j , st ) ot j Oj(st , at−1 i , at−1 j , ot j) ×τ(SEθt j (bt−1 j,l−1, at−1 j , ot j) − bt j,l−1) (1) donde β es la constante de normalización, τ es 1 si su argumento es 0, de lo contrario es 0, Pr(at−1 j |θt−1 j,l−1) es la probabilidad de que at−1 j sea Bayes racional para el agente descrito por el modelo θt−1 j,l−1, y SE(·) es una abreviatura para la actualización de creencias. Para una versión de la actualización de creencias cuando el modelo js es subintencional, consulte [9]. Si el agente j también se modela como un I-POMDP, entonces la actualización de creencias invoca la actualización de creencias de j (a través del término SEθt j ( bt−1 j,l−1 , at−1 j , ot j)), lo que a su vez podría invocar la actualización de creencias de is y así sucesivamente. Esta recursión en la anidación de creencias llega al nivel 0. En este nivel, la actualización de creencias del agente se reduce a una actualización de creencias de un POMDP. Para ilustraciones de la actualización de creencias, detalles adicionales sobre I-POMDPs y cómo se comparan con otros marcos multiagentes, consulte [9]. Iteración de Valor Cada estado de creencia en un I-POMDP finitamente anidado tiene un valor asociado que refleja la máxima ganancia que el agente puede esperar en este estado de creencia: Un ( bi,l, θi ) = max ai∈Ai is∈ISi,l ERi(is, ai)bi,l(is)+ γ oi∈Ωi Pr(oi|ai, bi,l)Un−1 ( SEθi (bi,l, ai, oi), θi ) (2) donde, ERi(is, ai) = aj Ri(is, ai, aj)Pr(aj|mj,l−1) (ya que is = (s, mj,l−1)). La ecuación 2 es una base para la iteración de valor en I-POMDPs. La acción óptima del agente, a∗ i, para el caso de horizonte finito con descuento, es un elemento del conjunto de acciones óptimas para el estado de creencia, OPT(θi), definido como: OPT( bi,l, θi ) = argmax ai∈Ai is∈ISi,l ERi(is, ai)bi,l(is) +γ oi∈Ωi Pr(oi|ai, bi,l)Un ( SEθi (bi,l, ai, oi), θi ) (3) 3. Diagramas de influencia interactivos. Una extensión ingenua de los diagramas de influencia (IDs) a entornos poblados por múltiples agentes es posible tratando a los otros agentes como autómatas, representados mediante nodos de probabilidad. Sin embargo, este enfoque asume que las acciones de los agentes están controladas utilizando una distribución de probabilidad que no cambia con el tiempo. Los diagramas de influencia interactivos (I-IDs) adoptan un enfoque más sofisticado al generalizar los IDs para hacerlos aplicables a entornos compartidos con otros agentes que pueden actuar, observar y actualizar sus creencias. 3.1 Sintaxis Además de los nodos habituales de probabilidad, decisión y utilidad, los IIDs incluyen un nuevo tipo de nodo llamado nodo de modelo. Mostramos un nivel general l I-ID en la Fig. 1(a), donde el nodo del modelo (Mj,l−1) está representado con un hexágono. Observamos que la distribución de probabilidad sobre el nodo de probabilidad, S, y el nodo del modelo juntos representan la creencia del agente sobre sus estados interactivos. Además del modelo 1, el modelo de nivel 0 es un POMDP: las acciones de otros agentes se tratan como eventos exógenos y se incorporan en las funciones T, O y R. El Sexto Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) Figura 1: (a) Un nivel genérico l I-ID para el agente i situado con otro agente j. El hexágono es el nodo modelo (Mj,l−1) cuya estructura mostramos en (b). Los miembros del nodo modelo son ellos mismos I-ID (m1 j,l−1, m2 j,l−1; los diagramas no se muestran aquí por simplicidad) cuyos nodos de decisión se asignan a los nodos de probabilidad correspondientes (A1 j , A2 j). Dependiendo del valor del nodo, Mod[Mj], la distribución de cada uno de los nodos de probabilidad se asigna al nodo Aj. (c) El I-ID transformado con el nodo del modelo reemplazado por los nodos de probabilidad y las relaciones entre ellos. Los I-IDs difieren de los IDs al tener un enlace discontinuo (llamado enlace de política en [15]) entre el nodo del modelo y un nodo de probabilidad, Aj, que representa la distribución sobre las acciones de los otros agentes dada su modelo. En ausencia de otros agentes, el nodo del modelo y el nodo de probabilidad, Aj, desaparecen y los I-ID se convierten en ID tradicionales. El nodo del modelo contiene los modelos computacionales alternativos atribuidos por i al otro agente del conjunto, Θj,l−1 ∪ SMj, donde Θj,l−1 y SMj fueron definidos previamente en la Sección 2. Por lo tanto, un modelo en el nodo del modelo puede ser en sí mismo un I-ID o ID, y la recursión termina cuando un modelo es un ID o subintencional. Dado que el nodo del modelo contiene los modelos alternativos del otro agente como sus valores, su representación no es trivial. En particular, algunos de los modelos dentro del nodo son I-IDs que, al resolverse, generan la política óptima de los agentes en sus nodos de decisión. Cada nodo de decisión se asigna al nodo de probabilidad correspondiente, digamos A1 j, de la siguiente manera: si OPT es el conjunto de acciones óptimas obtenidas al resolver el I-ID (o ID), entonces Pr(aj ∈ A1 j) = 1 |OPT| si aj ∈ OPT, 0 en caso contrario. Tomando prestados los conocimientos de trabajos anteriores [8], observamos que el nodo del modelo y el enlace de política discontinua que lo conecta con el nodo de probabilidad, Aj, podrían representarse como se muestra en la Fig. 1(b). El nodo de decisión de cada nivel l − 1 I-ID se transforma en un nodo de probabilidad, como mencionamos anteriormente, de modo que las acciones con el mayor valor en el nodo de decisión se les asignan probabilidades uniformes en el nodo de probabilidad, mientras que al resto se les asigna probabilidad cero. Los diferentes nodos de probabilidad (A1 j , A2 j ), uno para cada modelo, y adicionalmente, el nodo de probabilidad etiquetado Mod[Mj] forman los padres del nodo de probabilidad, Aj. Por lo tanto, hay tantos nodos de acción (A1 j , A2 j ) en Mj,l−1 como el número de modelos en el soporte de las creencias del agente. La tabla de probabilidad condicional del nodo de probabilidad, Aj, es un multiplexor que asume la distribución de cada uno de los nodos de acción (A1 j , A2 j ) dependiendo del valor de Mod[Mj]. Los valores de Mod[Mj] denotan los diferentes modelos de j. En otras palabras, cuando Mod[Mj] tiene el valor m1 j,l−1, el nodo de probabilidad Aj asume la distribución del nodo A1 j, y Aj asume la distribución de A2 j cuando Mod[Mj] tiene el valor m2 j,l−1. La distribución sobre el nodo, Mod[Mj], es la creencia del agente sobre los modelos de j dado un estado físico. Para más agentes, tendremos tantos nodos de modelo como agentes haya. Observa que la Fig. 1(b) aclara la semántica del enlace de política y muestra cómo puede ser representado utilizando los enlaces de dependencia tradicionales. En la Fig. 1(c), mostramos el I-ID transformado cuando el nodo del modelo es reemplazado por los nodos de probabilidad y las relaciones entre ellos. A diferencia de la representación en [15], no hay enlaces de políticas especiales, sino que el I-ID está compuesto únicamente por aquellos tipos de nodos que se encuentran en IDs tradicionales y relaciones de dependencia entre los nodos. Esto permite que los I-IDs sean representados e implementados utilizando herramientas de aplicación convencionales que apuntan a los IDs. Ten en cuenta que podemos ver el nivel l I-ID como un NID. Específicamente, cada uno de los modelos del nivel l − 1 dentro del nodo del modelo son bloques en el NID (ver Fig. 2). Si el nivel l = 1, cada bloque es un ID tradicional, de lo contrario, si l > 1, cada bloque dentro del NID puede ser un NID en sí mismo. Ten en cuenta que dentro de los I-IDs (o IDs) en cada nivel, solo hay un nodo de decisión único. Por lo tanto, nuestro NID no contiene ningún MAID. Figura 2: Un nivel l I-ID representado como un NID. Las probabilidades asignadas a los bloques del NID son creencias sobre modelos js condicionados a un estado físico. 3.2 Solución La solución de un I-ID procede de manera ascendente y se implementa de forma recursiva. Comenzamos resolviendo los modelos de nivel 0, que, si son intencionales, son identificadores tradicionales. Sus soluciones proporcionan distribuciones de probabilidad sobre las acciones de los otros agentes, las cuales se ingresan en los nodos de probabilidad correspondientes encontrados en el nodo del modelo del nivel 1 I-ID. El mapeo de los nodos de decisión de los modelos de nivel 0 a los nodos de probabilidad se realiza de manera que las acciones con el mayor valor en el nodo de decisión se les asignan probabilidades uniformes en el nodo de probabilidad, mientras que el resto se les asigna probabilidad cero. Dadas las distribuciones sobre las acciones dentro de los diferentes nodos de probabilidad (uno para cada modelo del otro agente), el I-ID de nivel 1 se transforma como se muestra en la Fig. 1(c). Durante la transformación, la tabla de probabilidad condicional (CPT) del nodo, Aj, se llena de tal manera que el nodo asume la distribución de cada uno de los nodos de probabilidad dependiendo del valor del nodo, Mod[Mj]. Como mencionamos anteriormente, los valores del nodo Mod[Mj] denotan los diferentes modelos del otro agente, y su distribución es la creencia del agente sobre los modelos de j condicionados al estado físico. El nivel 1 I-ID transformado es un ID tradicional que puede ser resuelto como us816 The Sixth Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) (a) (b) Figura 3: (a) Un I-DID genérico de dos niveles de tiempo para el agente i en un entorno con otro agente j. Observa el enlace de actualización del modelo punteado que denota la actualización de los modelos de j y la distribución de los modelos a lo largo del tiempo. (b) La semántica del enlace de actualización del modelo utilizando el método estándar de maximización de utilidad esperada [18]. Este procedimiento se lleva a cabo hasta el nivel l I-ID cuya solución proporciona el conjunto no vacío de acciones óptimas que el agente debe realizar dada su creencia. Ten en cuenta que, al igual que los IDs, los I-IDs son adecuados para la toma de decisiones en línea cuando se conoce la creencia actual de los agentes. 4. Diagramas de influencia dinámica interactivos (I-DIDs) Los diagramas de influencia dinámica interactivos (I-DIDs) extienden los I-IDs (y NIDs) para permitir la toma de decisiones secuencial a lo largo de varios pasos de tiempo. Así como los DIDs son representaciones gráficas estructuradas de POMDPs, los I-DIDs son los análogos gráficos en línea para I-POMDPs finitamente anidados. Los I-DIDs pueden ser utilizados para optimizar sobre una mirada anticipada finita dadas las creencias iniciales mientras se interactúa con otros agentes, posiblemente similares. 4.1 Sintaxis Representamos un I-DID de dos cortes temporales en la Fig. 3(a). Además de los nodos del modelo y el enlace de política discontinuo, lo que diferencia a un I-DID de un DID es el enlace de actualización del modelo mostrado como una flecha punteada en la Fig. 3(a). Explicamos la semántica del nodo del modelo y el enlace de la política en la sección anterior; a continuación, describimos las actualizaciones del modelo. La actualización del nodo del modelo con el tiempo implica dos pasos: primero, dados los modelos en el tiempo t, identificamos el conjunto actualizado de modelos que residen en el nodo del modelo en el tiempo t + 1. Recuerda de la Sección 2 que el modelo intencional de un agente incluye sus creencias. Debido a que los agentes actúan y reciben observaciones, sus modelos se actualizan para reflejar sus creencias cambiadas. Dado que el conjunto de acciones óptimas para un modelo podría incluir todas las acciones, y el agente podría recibir cualquiera de las |Ωj| posibles observaciones, el conjunto actualizado en el paso de tiempo t + 1 tendrá como máximo |Mt j,l−1||Aj||Ωj| modelos. Aquí, |Mt j,l−1| es el número de modelos en el paso de tiempo t, |Aj| y |Ωj| son los espacios más grandes de acciones y observaciones respectivamente, entre todos los modelos. Segundo, calculamos la nueva distribución sobre los modelos actualizados dados la distribución original y la probabilidad de que el agente realice la acción y reciba la observación que llevó al modelo actualizado. Estos pasos son parte de la actualización de creencias del agente formalizada utilizando la Ecuación 1. En la Fig. 3(b), mostramos cómo se implementa el enlace de actualización del modelo punteado en el I-DID. Si cada uno de los dos modelos de nivel l − 1 atribuidos a j en el paso de tiempo t resulta en una acción, y j podría hacer una de dos posibles observaciones, entonces el nodo del modelo en el paso de tiempo t + 1 contiene cuatro modelos actualizados (mt+1,1 j,l−1 ,mt+1,2 j,l−1 , mt+1,3 j,l−1 , y mt+1,4 j,l−1 ). Estos modelos difieren en sus creencias iniciales, cada una de las cuales es el resultado de j actualizar sus creencias debido a su acción y una posible observación. Los nodos de decisión en cada uno de los I-DIDs o DIDs que representan los modelos de nivel inferior se asignan a la Figura 4 correspondiente: I-DID transformado con los nodos del modelo y el enlace de actualización del modelo reemplazados por los nodos de probabilidad y las relaciones (en negrita) de los nodos de probabilidad, como se mencionó anteriormente. A continuación, describimos cómo se calcula la distribución sobre el conjunto actualizado de modelos (la distribución sobre el nodo de probabilidad Mod[Mt+1 j ] en Mt+1 j,l−1). La probabilidad de que el modelo actualizado js sea, digamos mt+1,1 j,l−1, depende de la probabilidad de que j realice la acción y reciba la observación que condujo a este modelo, y de la distribución previa sobre los modelos en el paso de tiempo t. Dado que el nodo de probabilidad At j asume la distribución de cada uno de los nodos de acción basados en el valor de Mod[Mt j], la probabilidad de la acción está dada por este nodo de probabilidad. Para obtener la probabilidad de una observación posible js, introducimos el nodo de probabilidad Oj, que dependiendo del valor de Mod[Mt j], asume la distribución del nodo de observación en el modelo de nivel inferior denominado Mod[Mt j]. Dado que la probabilidad de las observaciones js depende del estado físico y de las acciones conjuntas de ambos agentes, el nodo Oj está vinculado con St+1, At j y At i. De manera análoga a At j, la tabla de probabilidad condicional de Oj también es un multiplexor modulado por Mod[Mt j]. Finalmente, la distribución de los modelos previos en el tiempo t se obtiene a partir del nodo de probabilidad, Mod[Mt j ] en Mt j,l−1. Por consiguiente, los nodos de probabilidad, Mod[Mt j], At j y Oj, forman los padres de Mod[Mt+1 j] en Mt+1 j,l−1. Ten en cuenta que el enlace de actualización del modelo puede ser reemplazado por los enlaces de dependencia entre los nodos de probabilidad que constituyen los nodos del modelo en las dos etapas temporales. En la Fig. 4 mostramos los dos cortes temporales I-DID con los nodos del modelo reemplazados por los nodos de probabilidad y las relaciones entre ellos. Los nodos de probabilidad y los enlaces de dependencia que no están en negrita son estándar, generalmente encontrados en DIDs. La expansión del I-DID a lo largo de más pasos de tiempo requiere la repetición de los dos pasos de actualizar el conjunto de modelos que forman el 2. Nota que Oj representa la observación j en el tiempo t + 1. El Sexto Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 817 valores del nodo del modelo y añadiendo las relaciones entre los nodos de probabilidad, tantas veces como enlaces de actualización del modelo haya. Observamos que el conjunto posible de modelos del otro agente j crece exponencialmente con el número de pasos de tiempo. Por ejemplo, después de T pasos, puede haber como máximo |Mt=1 j,l−1|(|Aj||Ωj|)T −1 modelos candidatos residiendo en el nodo del modelo. 4.2 Solución Análoga a los I-ID, la solución a un I-DID de nivel l para el agente i expandido a lo largo de T pasos de tiempo puede llevarse a cabo de forma recursiva. Con el propósito de ilustración, dejemos l=1 y T=2. El método de solución utiliza la técnica estándar de anticipación, proyectando las secuencias de acciones y observaciones de los agentes hacia adelante desde el estado de creencia actual [17], y encontrando las posibles creencias que podría tener en el siguiente paso temporal. Dado que el agente i también tiene una creencia sobre los modelos de j, la búsqueda anticipada incluye descubrir los posibles modelos que j podría tener en el futuro. Por consiguiente, cada uno de los modelos subintencionales o de nivel 0 de js (representados utilizando un DID estándar) en el primer paso de tiempo debe resolverse para obtener su conjunto óptimo de acciones. Estas acciones se combinan con el conjunto de observaciones posibles que j podría hacer en ese modelo, lo que resulta en un conjunto actualizado de modelos candidatos (que incluyen las creencias actualizadas) que podrían describir el comportamiento de j. Las creencias sobre este conjunto actualizado de modelos candidatos se calculan utilizando los métodos estándar de inferencia que utilizan las relaciones de dependencia entre los nodos del modelo, como se muestra en la Figura 3(b). Observamos la naturaleza recursiva de esta solución: al resolver el agente de nivel 1 I-DID, los DIDs de nivel 0 deben ser resueltos. Si el anidamiento de los modelos es más profundo, todos los modelos en todos los niveles, comenzando desde el nivel 0, se resuelven de manera ascendente. Breve descripción del algoritmo recursivo para resolver el agente es el Algoritmo para resolver I-DID. Entrada: nivel l ≥ 1 I-ID o nivel 0 ID, T Fase de Expansión 1. Para t desde 1 hasta T − 1, hacer 2. Si l ≥ 1, entonces llenar Mt+1 j,l−1 3. Para cada mt j en el rango (Mt j,l−1) hacer 4. Llame recursivamente al algoritmo con el l - 1 I-ID (o ID) que representa mt j y el horizonte, T - t + 1 5. Mapea el nodo de decisión del I-ID (o ID) resuelto, OPT(mt j), a un nodo de probabilidad Aj 6. Para cada aj en OPT(mt j) hacer 7. Para cada oj en Oj (parte de mt j) hacer 8. Actualizar la creencia js, bt+1 j ← SE(bt j, aj, oj) 9. mt+1 j ← Nuevo I-ID (o ID) con bt+1 j como la creencia inicial 10. Rango(Mt+1 j,l−1) ∪ ← {mt+1 j } 11. Añadir el nodo del modelo, Mt+1 j,l−1, y los enlaces de dependencia entre Mt j,l−1 y Mt+1 j,l−1 (mostrados en la Fig. 3(b)) 12. Agregue los nodos de probabilidad, decisión y utilidad para la franja de tiempo t + 1 y los enlaces de dependencia entre ellos 13. Establezca las CPTs para cada nodo de probabilidad y nodo de utilidad de la Fase de Mirada Adelante 14. Aplica el método estándar de anticipación y respaldo para resolver la Figura 5 expandida de I-DID: Algoritmo para resolver un nivel l ≥ 0 I-DID. Nivel l I-DID expandido durante T pasos de tiempo con otro agente j en la Figura 5. Adoptamos un enfoque de dos fases: Dado un I-ID de nivel l (descrito previamente en la Sección 3) con todos los modelos de niveles inferiores representados también como I-IDs o IDs (si es nivel 0), el primer paso es expandir el I-ID de nivel l a lo largo de T pasos de tiempo añadiendo los enlaces de dependencia y las tablas de probabilidad condicional para cada nodo. Nos enfocamos especialmente en establecer y poblar los nodos del modelo (líneas 3-11). Ten en cuenta que Range(·) devuelve los valores (modelos de nivel inferior) de la variable aleatoria dada como entrada (nodo del modelo). En la segunda fase, utilizamos una técnica estándar de anticipación proyectando las secuencias de acción y observación durante T pasos de tiempo en el futuro, y respaldando los valores de utilidad de las creencias alcanzables. Similar a los I-IDs, los I-DIDs se reducen a DIDs en ausencia de otros agentes. Como mencionamos anteriormente, los modelos de nivel 0 son los DIDs tradicionales. Sus soluciones proporcionan distribuciones de probabilidad sobre las acciones del agente modelado en ese nivel a los I-DIDs en el nivel 1. Dadas las distribuciones de probabilidad sobre las acciones de otros agentes, los IDIDs de nivel 1 pueden resolverse como DIDs y proporcionar distribuciones de probabilidad a modelos de niveles superiores. Suponga que el número de modelos considerados en cada nivel está limitado por un número, M. Resolver un I-DID de nivel l es equivalente a resolver O(Ml) DIDs. Para ilustrar la utilidad de los I-DIDs, los aplicamos a tres dominios de problemas. Describimos, en particular, la formulación del I-DID y las prescripciones óptimas obtenidas al resolverlo. 5.1 Seguimiento-Liderazgo en el Problema del Tigre Multiagente Comenzamos nuestras ilustraciones del uso de I-IDs e I-DIDs con una versión ligeramente modificada del problema del tigre multiagente discutido en [9]. El problema tiene dos agentes, cada uno de los cuales puede abrir la puerta derecha (OR), la puerta izquierda (OL) o escuchar (L). Además de escuchar gruñidos (desde la izquierda (GL) o desde la derecha (GR)) cuando escuchan, los agentes también escuchan chirridos (desde la izquierda (CL), desde la derecha (CR) o sin chirridos (S)), que indican ruidosamente que los otros agentes están abriendo una de las puertas. Cuando se abre cualquier puerta, el tigre persiste en su ubicación original con una probabilidad del 95%. El agente i escucha gruñidos con una fiabilidad del 65% y crujidos con una fiabilidad del 95%. El agente J, por otro lado, escucha gruñidos con una fiabilidad del 95%. Por lo tanto, la configuración es tal que el agente i escucha la apertura de puertas del agente j de manera más confiable que los rugidos de los tigres. Esto sugiere que podría usar acciones de JavaScript como indicación de la ubicación del tigre, como discutimos a continuación. Las preferencias de cada agente son como en el juego de un solo agente discutido en [13]. Las funciones de transición, observación y recompensa se muestran en [16]. Un buen indicador de la utilidad de los métodos normativos para la toma de decisiones como los I-DIDs es la aparición de comportamientos sociales realistas en sus prescripciones. En entornos del persistente problema del tigre multiagente que reflejan situaciones del mundo real, demostramos la seguidismo entre los agentes y, como se muestra en [15], el engaño entre agentes que creen que están en una relación de tipo seguidor-líder. En particular, analizamos las condiciones situacionales y epistemológicas suficientes para su surgimiento. El comportamiento de seguidores, por ejemplo, resulta del agente conociendo sus propias debilidades, evaluando las fortalezas, preferencias y posibles comportamientos del otro, y dándose cuenta de que es mejor para él seguir las acciones de los demás para maximizar sus ganancias. Consideremos una configuración particular del problema del tigre en la que el agente i cree que las preferencias de j están alineadas con las suyas: ambos solo quieren obtener el oro, y la audición de j es más confiable en comparación consigo mismo. Como ejemplo, supongamos que j, al escuchar, puede discernir la ubicación del tigre el 95% de las veces en comparación con su precisión del 65%. Además, el agente i no tiene ninguna información inicial sobre la ubicación de los tigres. En otras palabras, la creencia anidada de un solo nivel, bi,1, asigna 0.5 a cada una de las dos ubicaciones del tigre. Además, considera dos modelos de j, que difieren en los niveles iniciales de creencias planas de j. Esto se representa en el nivel 1 I-ID mostrado en la Fig. 6(a). Según un modelo, j asigna una probabilidad de 0.9 a que el tigre esté detrás de la puerta izquierda, mientras que el otro 818 The Sixth Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) Figura 6: (a) Nivel 1 I-ID del agente i, (b) dos IDs de nivel 0 del agente j cuyos nodos de decisión están mapeados a los nodos de probabilidad, A1 j , A2 j , en (a). El modelo asigna 0.1 a esa ubicación (ver Fig. 6(b)). El agente i está indeciso sobre estos dos modelos de j. Si variamos la capacidad auditiva, y resolvemos el nivel 1 correspondiente I-ID expandido en tres pasos de tiempo, obtenemos las políticas de comportamiento normativas mostradas en la Figura 7 que exhiben comportamiento de seguidores. Si la probabilidad de escuchar correctamente los gruñidos es de 0.65, entonces, como se muestra en la política en la Fig. 7(a), i comienza a seguir condicionalmente las acciones de j: i abre la misma puerta que j abrió anteriormente si su evaluación propia de la ubicación de los tigres confirma la elección de j. Si i pierde la capacidad de interpretar correctamente los gruñidos por completo, sigue ciegamente a j y abre la misma puerta que j abrió anteriormente (Fig. 7(b)). Figura 7: Emergencia de (a) seguidores condicionales, y (b) seguidores ciegos en el problema del tigre. Los comportamientos de interés están en negrita. * es un comodín y representa cualquiera de las observaciones. Observamos que un solo nivel de anidación de creencias, creencias sobre los modelos de los demás, fue suficiente para que surgiera el seguidismo en el problema del tigre. Sin embargo, los requisitos epistemológicos para el surgimiento del liderazgo son más complejos. Para que un agente, digamos j, surja como líder, primero debe surgir el seguidismo en el otro agente i. Como mencionamos anteriormente, si i está seguro de que sus preferencias son idénticas a las de j, y cree que j tiene un mejor sentido del oído, i seguirá las acciones de j con el tiempo. El agente j emerge como líder si cree que lo seguiré, lo que implica que la creencia de j debe estar anidada dos niveles de profundidad para permitirle reconocer su papel de liderazgo. Darse cuenta de que lo seguiré presenta a J una oportunidad para influir en sus acciones en beneficio del bien colectivo o de su interés propio solamente. Por ejemplo, en el problema del tigre, consideremos una situación en la que si tanto i como j abren la puerta correcta, entonces cada uno recibe un pago de 20 que es el doble del original. Si solo j selecciona la puerta correcta, obtiene la recompensa de 10. Por otro lado, si ambos agentes eligen la puerta incorrecta, sus penalizaciones se reducen a la mitad. En este entorno, es tanto en el mejor interés de j como en el beneficio colectivo que utilice su experiencia para seleccionar la puerta correcta, y así ser un buen líder. Sin embargo, considera un problema ligeramente diferente en el que j se beneficia de la pérdida de i y es penalizado si i se beneficia. Específicamente, dejemos que la ganancia se reste de js, indicando que j es antagonista hacia i: si j elige la puerta correcta y i la incorrecta, entonces la pérdida de 100 se convierte en la ganancia de js. El agente J cree que yo incorrectamente pienso que las preferencias de J son aquellas que promueven el bien colectivo y que comienza creyendo con un 99% de confianza dónde está el tigre. Dado que creo que mis preferencias son similares a las de j, y que j comienza creyendo casi con certeza que una de las dos ubicaciones es la correcta (dos modelos de nivel 0 de j), comenzaré siguiendo las acciones de j. Mostramos la política normativa para resolver su I-DID anidado individualmente durante tres pasos de tiempo en la Fig. 8(a). La política demuestra que seguiré ciegamente las acciones de js. Dado que el tigre persiste en su ubicación original con una probabilidad del 0.95, seleccionaré nuevamente la misma puerta. Si j comienza el juego con una probabilidad del 99% de que el tigre esté a la derecha, resolver su I-DID anidado dos niveles profundamente, resulta en la política mostrada en la Fig. 8(b). Aunque j está casi seguro de que OL es la acción correcta, comenzará seleccionando OR, seguido de OL. La intención del agente js es engañar a i, a quien cree que seguirá las acciones de js, para así obtener $110 en el segundo paso de tiempo, lo cual es más de lo que j ganaría si fuera honesto. Figura 8: Emergencia del engaño entre agentes en el problema del tigre. Los comportamientos de interés están en negrita. * denota como antes. (a) El agente es una política que demuestra que seguirá ciegamente las acciones de js. (b) Aunque j está casi seguro de que el tigre está a la derecha, comenzará seleccionando OR, seguido de OL, para engañar a i. 5.2 Altruismo y reciprocidad en el problema del bien público El problema del bien público (PG) [7], consiste en un grupo de M agentes, cada uno de los cuales debe contribuir con algún recurso a una olla pública o guardarlo para sí mismos. Dado que los recursos aportados al fondo público se comparten entre todos los agentes, son menos valiosos para el agente cuando están en el fondo público. Sin embargo, si todos los agentes eligen contribuir con sus recursos, entonces la recompensa para cada agente es mayor que si nadie contribuye. Dado que un agente recibe su parte del bote público independientemente de si ha contribuido o no, la acción dominante es que cada agente no contribuya y en su lugar se aproveche de las contribuciones de los demás. Sin embargo, los comportamientos de los jugadores humanos en simulaciones empíricas del problema de PG difieren de las predicciones normativas. Los experimentos revelan que muchos jugadores inicialmente contribuyen con una gran cantidad al bote público, y continúan contribuyendo cuando se juega el problema del PG repetidamente, aunque en cantidades decrecientes [4]. Muchos de estos experimentos [5] informan que un pequeño grupo central de jugadores contribuye persistentemente al bote público incluso cuando todos los demás están desertando. Estos experimentos también revelan que los jugadores que contribuyen persistentemente tienen preferencias altruistas o recíprocas que coinciden con la cooperación esperada de los demás. Para simplificar, asumimos que el juego se juega entre M = 2 agentes, i y j. Que cada agente esté inicialmente dotado con una cantidad de recursos XT. Mientras que la formulación clásica del juego PG permite que cada agente contribuya con cualquier cantidad de recursos (≤ XT) al bote público, simplificamos el espacio de acciones al permitir dos acciones posibles. Cada agente puede elegir contribuir (C) una cantidad fija de recursos, o no contribuir. La última acción es deThe Sixth Intl. La Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 819 fue señalada como defectuosa (D). Suponemos que las acciones no son observables por otros. El valor de los recursos en la bolsa pública se descuenta por ci para cada agente i, donde ci es el retorno privado marginal. Suponemos que ci < 1 para que el agente no se beneficie lo suficiente como para contribuir al bote público en beneficio propio. Simultáneamente, ciM > 1, lo que hace que la contribución colectiva sea óptima de Pareto. i/j C D C 2ciXT , 2cjXT ciXT − cp, XT + cjXT − P D XT + ciXT − P, cjXT − cp XT , XT Tabla 1: El juego PG de un solo disparo con castigo. Para fomentar las contribuciones, los agentes contribuyentes castigan a los que no colaboran pero incurren en un pequeño costo por administrar el castigo. Sea P la sanción impuesta al agente que se desvía y cp el costo no nulo de castigar al agente contribuyente. Para simplificar, asumimos que el costo de castigar es el mismo para ambos agentes. El juego PG de un solo disparo con castigo se muestra en la Tabla 1. Si ci = cj, cp > 0, y si P > XT − ciXT, entonces la deserción ya no es una acción dominante. Si P < XT − ciXT, entonces la deserción es la acción dominante para ambos. Si P = XT − ciXT, entonces el juego no es resoluble por dominancia. Figura 9: (a) Nivel 1 I-ID del agente i, (b) IDs de nivel 0 del agente j con nodos de decisión mapeados a los nodos de probabilidad, A1 j y A2 j, en (a). Formulamos una versión secuencial del problema PG con castigo desde la perspectiva del agente i. Aunque en el juego de PG repetido, la cantidad en la olla pública se revela a todos los agentes después de cada ronda de acciones, asumimos en nuestra formulación que está oculta para los agentes. Cada agente puede contribuir con una cantidad fija, xc, o desertar. Un agente al realizar una acción recibe una observación de abundante (PY) o escasa (MR) simbolizando el estado de la olla pública. Ten en cuenta que las observaciones también son indirectamente indicativas de las acciones del agente js porque el estado del bote público es influenciado por ellas. La cantidad de recursos en el agente es una olla privada, es perfectamente observable para i. Los pagos son análogos a la Tabla 1. Tomando prestado de las investigaciones empíricas del problema PG [5], construimos IDs de nivel 0 para j que modelan tipos altruistas y no altruistas (Fig. 9(b)). Específicamente, nuestro agente altruista tiene un alto retorno privado marginal (cj está cerca de 1) y no castiga a los demás que incumplen. Que xc = 1 y el agente de nivel 0 sea castigado la mitad de las veces que se desvía. Con una acción restante, ambos tipos de agentes eligen contribuir para evitar ser castigados. Con dos acciones por delante, el tipo altruista elige contribuir, mientras que el otro defecta. Esto se debe a que cj para el tipo altruista es cercano a 1, por lo tanto, el castigo esperado, 0.5P > (1 − cj), que el tipo altruista evita. Debido a que cj para el tipo no altruista es menor, prefiere no contribuir. Con tres pasos por delante, el agente altruista contribuye para evitar el castigo (0.5P > 2(1 − cj)), mientras que el tipo no altruista se desentiende. Para más de tres pasos, mientras el agente altruista continúa contribuyendo al bote público dependiendo de qué tan cercano esté su retorno privado marginal a 1, el tipo no altruista prescribe la deserción. Analizamos las decisiones de un agente altruista i modelado usando un nivel 1 I-DID expandido en 3 pasos de tiempo. i atribuye los dos modelos de nivel 0, mencionados anteriormente, a j (ver Fig. 9). Si creo con una probabilidad de 1 que j es altruista, elijo contribuir en cada uno de los tres pasos. Este comportamiento persiste cuando i no está al tanto de si j es altruista (Fig. 10(a)), y cuando i asigna una alta probabilidad a que j sea del tipo no altruista. Sin embargo, cuando i cree con una probabilidad de 1 que j no es altruista y por lo tanto seguramente va a traicionar, i elige traicionar para evitar ser castigado y porque su beneficio privado marginal es menor que 1. Estos resultados demuestran que el comportamiento de nuestro tipo altruista se asemeja al encontrado experimentalmente. El agente de nivel 1 no altruista elige desertar independientemente de lo altruista que crea que sea el otro agente. Analizamos el comportamiento de un tipo de agente recíproco que se ajusta a la cooperación o defección esperada. El retorno privado marginal del tipo recíproco es similar al del tipo no altruista, sin embargo, obtiene una mayor recompensa cuando su acción es similar a la del otro. Consideramos el caso en el que el agente recíproco i no está seguro de si j es altruista y cree que es probable que el bote público esté medio lleno. Para esta creencia previa, yo elijo defectar. Al recibir una observación de abundancia, decide contribuir, mientras que una observación de escasez lo hace defectar (Fig. 10(b)). Esto se debe a que una observación de abundancia indica que es probable que la olla esté más llena que a la mitad, lo cual resulta de la acción de js para contribuir. Por lo tanto, entre los dos modelos atribuidos a j, es probable que su tipo sea altruista, lo que hace probable que j contribuya nuevamente en el próximo paso de tiempo. El agente i, por lo tanto, elige contribuir para corresponder la acción de js. Un razonamiento análogo lleva a uno a defectar cuando observa una olla escasa. Con una acción por hacer, creo que si j contribuye, también elegiré contribuir para evitar castigo independientemente de sus observaciones. Figura 10: (a) Un agente altruista de nivel 1 siempre contribuye. (b) Un agente recíproco i comienza defraudando y luego elige contribuir o defraudar basándose en su observación de abundancia (indicando que j es probablemente altruista) o escasez (j no es altruista). 5.3 Estrategias en el póker de dos jugadores El póker es un popular juego de cartas de suma cero que ha recibido mucha atención en la comunidad de investigación de IA como un banco de pruebas [2]. El póker se juega entre M ≥ 2 jugadores, en el que cada jugador recibe una mano de cartas de una baraja. Si bien existen varias variantes de póker con diferentes niveles de complejidad, consideramos una versión simple en la que cada jugador tiene tres turnos durante los cuales el jugador puede intercambiar una carta (E), mantener la mano existente (K), retirarse (F) y abandonar el juego, o apostar (C), lo que requiere que todos los jugadores muestren sus manos. Para mantener las cosas simples, dejemos que M = 2, y que cada jugador reciba una mano compuesta por una sola carta sacada del mismo palo. Por lo tanto, durante un enfrentamiento, el jugador que tenga la carta numéricamente más alta (el 2 es el más bajo, el as es el más alto) gana el bote. Durante un intercambio de cartas, la carta descartada se coloca ya sea en la pila L, indicando al otro agente que era una carta de número bajo menor que 8, o en la 820 The Sixth Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) H, indicando que la carta tenía un rango mayor o igual a 8. Ten en cuenta que, por ejemplo, si se descarta una carta de número bajo, la probabilidad de recibir una carta baja a cambio se reduce. Mostramos el nivel 1 I-ID para el Poker simplificado de dos jugadores en la Fig. 11. Consideramos dos modelos (tipos de personalidad) del agente j. El tipo conservador cree que es probable que su oponente tenga una carta de alto número en su mano. Por otro lado, el agente agresivo j cree con alta probabilidad que su oponente tiene una carta de número más bajo. Por lo tanto, los dos tipos difieren en sus creencias sobre la mano de sus oponentes. En ambos estos modelos de nivel 0, se asume que el oponente realiza sus acciones siguiendo una distribución fija y uniforme. Con tres acciones restantes, independientemente de su mano (a menos que sea un as), el agente agresivo elige intercambiar su carta, con la intención de mejorar su mano actual. Esto se debe a que cree que el otro tiene una carta baja, lo que mejora sus posibilidades de obtener una carta alta durante el intercambio. El agente conservador elige quedarse con su carta, sin importar su mano, porque considera que sus posibilidades de obtener una carta alta son escasas, ya que cree que su oponente tiene una. Figura 11: (a) Nivel 1 I-ID del agente i. La observación revela información sobre la mano de js del paso de tiempo anterior, (b) IDs de nivel 0 del agente j cuyos nodos de decisión están mapeados a los nodos de probabilidad, A1 j , A2 j , en (a). La política de un agente de nivel 1 i que cree que cada carta, excepto la suya, tiene la misma probabilidad de estar en su mano (tipo de personalidad neutral) y j podría ser de tipo agresivo o conservador, se muestra en la Figura 12. Su mano contiene la carta numerada 8. El agente comienza por mantener su carta. Al ver que j no intercambió una carta (N), i cree con probabilidad 1 que j es conservador y, por lo tanto, mantendrá sus cartas. i responde manteniendo su carta o intercambiándola porque j tiene igual probabilidad de tener una carta más baja o más alta. Si observo que j descartó su carta en la pila L o H, creo que j es agresivo. Al observar a L, i se da cuenta de que j tenía una carta baja, y es probable que tenga una carta alta después de su intercambio. Dado que la probabilidad de recibir una carta baja es alta en este momento, i decide quedarse con su carta. Al observar la carta H, creyendo que la probabilidad de recibir una carta de número alto es alta, i decide intercambiar su carta. En el paso final, i decide llamar independientemente de su historial de observaciones porque su creencia de que j tiene una carta más alta no es lo suficientemente alta como para concluir que es mejor retirarse y renunciar al pago. Esto se debe en parte al hecho de que una observación de, digamos, L, restablece las creencias del agente en el paso de tiempo anterior sobre las cartas de números bajos solamente. 6. DISCUSIÓN Mostramos cómo los DIDs pueden ser extendidos a los I-DIDs que permiten la toma de decisiones secuenciales en línea en entornos multiagentes inciertos. Nuestra representación gráfica de I-DIDs mejora la Figura 12 anterior: Un agente de nivel 1 es una política de tres pasos en el problema de Poker. i comienza creyendo que j tiene la misma probabilidad de ser agresivo o conservador y podría tener cualquier carta en su mano con igual probabilidad. Trabaja significativamente al ser más transparente, semánticamente claro y capaz de ser resuelto utilizando algoritmos estándar que apuntan a los DIDs. Los I-DIDs extienden los NIDs para permitir la toma de decisiones secuenciales a lo largo de múltiples pasos de tiempo en presencia de otros agentes que interactúan. Los I-DIDs pueden ser vistos como representaciones gráficas concisas para IPOMDPs que proporcionan una forma de explotar la estructura del problema y llevar a cabo la toma de decisiones en línea a medida que el agente actúa y observa dadas sus creencias previas. Actualmente estamos investigando formas de resolver I-DIDs aproximadamente con límites demostrables en la calidad de la solución. Agradecimiento: Agradecemos a Piotr Gmytrasiewicz por algunas discusiones útiles relacionadas con este trabajo. El primer autor desea agradecer el apoyo de una beca UGARF. 7. REFERENCIAS [1] R. J. Aumann. Epistemología interactiva i: Conocimiento. Revista Internacional de Teoría de Juegos, 28:263-300, 1999. [2] D. Billings, A. Davidson, J. Schaeffer y D. Szafron. El desafío del póker. AIJ, 2001. [3] A. Brandenburger y E. Dekel. Jerarquías de creencias y conocimiento común. Revista de Teoría Económica, 59:189-198, 1993. [4] C. Camerer. Teoría de juegos conductual: Experimentos en interacción estratégica. Princeton University Press, 2003. [5] E. Fehr y S. Gachter. Cooperación y castigo en experimentos de bienes públicos. Revista Económica Americana, 90(4):980-994, 2000. [6] D. Fudenberg y D. K. Levine. La Teoría del Aprendizaje en los Juegos. MIT Press, 1998. [7] D. Fudenberg y J. Tirole. Teoría de juegos. MIT Press, 1991. [8] Y. Gal y A. Pfeffer. Un lenguaje para modelar los procesos de toma de decisiones de agentes en juegos. En AAMAS, 2003. [9] P. Gmytrasiewicz y P. Doshi. Un marco para la planificación secuencial en entornos multiagentes. JAIR, 24:49-79, 2005. [10] P. Gmytrasiewicz y E. Durfee. Coordinación racional en entornos multiagente. JAAMAS, 3(4):319-350, 2000. [11] J. C. Harsanyi. Juegos con información incompleta jugados por jugadores bayesianos. Ciencia de la Gestión, 14(3):159-182, 1967. [12] R. A. Howard y J. E. Matheson. Diagramas de influencia. En R. A. Howard y J. E. Matheson, editores, Los Principios y Aplicaciones del Análisis de Decisiones. Grupo de Decisiones Estratégicas, Menlo Park, CA 94025, 1984. [13] L. Kaelbling, M. Littman y A. Cassandra. Planificación y actuación en dominios estocásticos parcialmente observables. Revista de Inteligencia Artificial, 2, 1998. [14] D. Koller y B. Milch. Diagramas de influencia multiagente para representar y resolver juegos. En IJCAI, páginas 1027-1034, 2001. [15] K. Polich y P. Gmytrasiewicz. Diagramas de influencia interactivos y dinámicos. En el taller GTDT, AAMAS, 2006. [16] B. Rathnas, P. Doshi y P. J. Gmytrasiewicz. Soluciones exactas para POMDP interactivos utilizando equivalencia conductual. En la Conferencia de Agentes Autónomos y Sistemas Multiagente (AAMAS), 2006. [17] S. Russell y P. Norvig. Inteligencia Artificial: Un Enfoque Moderno (Segunda Edición). Prentice Hall, 2003. [18] R. D. Shachter. \n\nPrentice Hall, 2003. [18] R. D. Shachter. Evaluando diagramas de influencia. Investigación de Operaciones, 34(6):871-882, 1986. [19] D. Suryadi y P. Gmytrasiewicz. Aprendiendo modelos de otros agentes utilizando diagramas de influencia. En UM, 1999. El Sexto Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 821 ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "interactive partially observable markov decision process": {
            "translated_key": "proceso de decisión de Markov parcialmente observable interactivo",
            "is_in_text": false,
            "original_annotated_sentences": [
                "Graphical Models for Online Solutions to Interactive POMDPs Prashant Doshi Dept.",
                "of Computer Science University of Georgia Athens, GA 30602, USA pdoshi@cs.uga.edu Yifeng Zeng Dept. of Computer Science Aalborg University DK-9220 Aalborg, Denmark yfzeng@cs.aau.edu Qiongyu Chen Dept. of Computer Science National Univ. of Singapore 117543, Singapore chenqy@comp.nus.edu.sg ABSTRACT We develop a new graphical representation for interactive partially observable Markov decision processes (I-POMDPs) that is significantly more transparent and semantically clear than the previous representation.",
                "These graphical models called interactive dynamic influence diagrams (I-DIDs) seek to explicitly model the structure that is often present in real-world problems by decomposing the situation into chance and decision variables, and the dependencies between the variables.",
                "I-DIDs generalize DIDs, which may be viewed as graphical representations of POMDPs, to multiagent settings in the same way that I-POMDPs generalize POMDPs.",
                "I-DIDs may be used to compute the policy of an agent online as the agent acts and observes in a setting that is populated by other interacting agents.",
                "Using several examples, we show how I-DIDs may be applied and demonstrate their usefulness.",
                "Categories and Subject Descriptors I.2.11 [Distributed Artificial Intelligence]: Multiagent Systems General Terms Theory 1.",
                "INTRODUCTION Interactive partially observable Markov decision processes (IPOMDPs) [9] provide a framework for sequential decision-making in partially observable multiagent environments.",
                "They generalize POMDPs [13] to multiagent settings by including the other agents computable models in the state space along with the states of the physical environment.",
                "The models encompass all information influencing the agents behaviors, including their preferences, capabilities, and beliefs, and are thus analogous to types in Bayesian games [11].",
                "I-POMDPs adopt a subjective approach to understanding strategic behavior, rooted in a decision-theoretic framework that takes a decision-makers perspective in the interaction.",
                "In [15], Polich and Gmytrasiewicz introduced interactive dynamic influence diagrams (I-DIDs) as the computational representations of I-POMDPs.",
                "I-DIDs generalize DIDs [12], which may be viewed as computational counterparts of POMDPs, to multiagents settings in the same way that I-POMDPs generalize POMDPs.",
                "I-DIDs contribute to a growing line of work [19] that includes multi-agent influence diagrams (MAIDs) [14], and more recently, networks of influence diagrams (NIDs) [8].",
                "These formalisms seek to explicitly model the structure that is often present in real-world problems by decomposing the situation into chance and decision variables, and the dependencies between the variables.",
                "MAIDs provide an alternative to normal and extensive game forms using a graphical formalism to represent games of imperfect information with a decision node for each agents actions and chance nodes capturing the agents private information.",
                "MAIDs objectively analyze the game, efficiently computing the Nash equilibrium profile by exploiting the independence structure.",
                "NIDs extend MAIDs to include agents uncertainty over the game being played and over models of the other agents.",
                "Each model is a MAID and the network of MAIDs is collapsed, bottom up, into a single MAID for computing the equilibrium of the game keeping in mind the different models of each agent.",
                "Graphical formalisms such as MAIDs and NIDs open up a promising area of research that aims to represent multiagent interactions more transparently.",
                "However, MAIDs provide an analysis of the game from an external viewpoint and the applicability of both is limited to static single play games.",
                "Matters are more complex when we consider interactions that are extended over time, where predictions about others future actions must be made using models that change as the agents act and observe.",
                "I-DIDs address this gap by allowing the representation of other agents models as the values of a special model node.",
                "Both, other agents models and the original agents beliefs over these models are updated over time using special-purpose implementations.",
                "In this paper, we improve on the previous preliminary representation of the I-DID shown in [15] by using the insight that the static I-ID is a type of NID.",
                "Thus, we may utilize NID-specific language constructs such as multiplexers to represent the model node, and subsequently the I-ID, more transparently.",
                "Furthermore, we clarify the semantics of the special purpose policy link introduced in the representation of I-DID by [15], and show that it could be replaced by traditional dependency links.",
                "In the previous representation of the I-DID, the update of the agents belief over the models of others as the agents act and receive observations was denoted using a special link called the model update link that connected the model nodes over time.",
                "We explicate the semantics of this link by showing how it can be implemented using the traditional dependency links between the chance nodes that constitute the model nodes.",
                "The net result is a representation of I-DID that is significantly more transparent, semantically clear, and capable of being implemented using the standard algorithms for solving DIDs.",
                "We show how IDIDs may be used to model an agents uncertainty over others models, that may themselves be I-DIDs.",
                "Solution to the I-DID is a policy that prescribes what the agent should do over time, given its beliefs over the physical state and others models.",
                "Analogous to DIDs, I-DIDs may be used to compute the policy of an agent online as the agent acts and observes in a setting that is populated by other interacting agents. 2.",
                "BACKGROUND: FINITELY NESTED IPOMDPS Interactive POMDPs generalize POMDPs to multiagent settings by including other agents models as part of the state space [9].",
                "Since other agents may also reason about others, the interactive state space is strategically nested; it contains beliefs about other agents models and their beliefs about others.",
                "For simplicity of presentation we consider an agent, i, that is interacting with one other agent, j.",
                "A finitely nested I-POMDP of agent i with a strategy level l is defined as the tuple: I-POMDPi,l = ISi,l, A, Ti, Ωi, Oi, Ri where: • ISi,l denotes a set of interactive states defined as, ISi,l = S × Mj,l−1, where Mj,l−1 = {Θj,l−1 ∪ SMj}, for l ≥ 1, and ISi,0 = S, where S is the set of states of the physical environment.",
                "Θj,l−1 is the set of computable intentional models of agent j: θj,l−1 = bj,l−1, ˆθj where the frame, ˆθj = A, Ωj, Tj, Oj, Rj, OCj .",
                "Here, j is Bayes rational and OCj is js optimality criterion.",
                "SMj is the set of subintentional models of j.",
                "Simple examples of subintentional models include a no-information model [10] and a fictitious play model [6], both of which are history independent.",
                "We give a recursive bottom-up construction of the interactive state space below.",
                "ISi,0 = S, Θj,0 = { bj,0, ˆθj | bj,0 ∈ Δ(ISj,0)} ISi,1 = S × {Θj,0 ∪ SMj}, Θj,1 = { bj,1, ˆθj | bj,1 ∈ Δ(ISj,1)} . . . . . .",
                "ISi,l = S × {Θj,l−1 ∪ SMj}, Θj,l = { bj,l, ˆθj | bj,l ∈ Δ(ISj,l)} Similar formulations of nested spaces have appeared in [1, 3]. • A = Ai × Aj is the set of joint actions of all agents in the environment; • Ti : S ×A×S → [0, 1], describes the effect of the joint actions on the physical states of the environment; • Ωi is the set of observations of agent i; • Oi : S × A × Ωi → [0, 1] gives the likelihood of the observations given the physical state and joint action; • Ri : ISi × A → R describes agent is preferences over its interactive states.",
                "Usually only the physical states will matter.",
                "Agent is policy is the mapping, Ω∗ i → Δ(Ai), where Ω∗ i is the set of all observation histories of agent i.",
                "Since belief over the interactive states forms a sufficient statistic [9], the policy can also be represented as a mapping from the set of all beliefs of agent i to a distribution over its actions, Δ(ISi) → Δ(Ai). 2.1 Belief Update Analogous to POMDPs, an agent within the I-POMDP framework updates its belief as it acts and observes.",
                "However, there are two differences that complicate the belief update in multiagent settings when compared to single agent ones.",
                "First, since the state of the physical environment depends on the actions of both agents, is prediction of how the physical state changes has to be made based on its prediction of js actions.",
                "Second, changes in js models have to be included in is belief update.",
                "Specifically, if j is intentional then an update of js beliefs due to its action and observation has to be included.",
                "In other words, i has to update its belief based on its prediction of what j would observe and how j would update its belief.",
                "If js model is subintentional, then js probable observations are appended to the observation history contained in the model.",
                "Formally, we have: Pr(ist |at−1 i , bt−1 i,l ) = β ISt−1:mt−1 j =θt j bt−1 i,l (ist−1 ) × at−1 j Pr(at−1 j |θt−1 j,l−1)Oi(st , at−1 i , at−1 j , ot i) ×Ti(st−1 , at−1 i , at−1 j , st ) ot j Oj(st , at−1 i , at−1 j , ot j) ×τ(SEθt j (bt−1 j,l−1, at−1 j , ot j) − bt j,l−1) (1) where β is the normalizing constant, τ is 1 if its argument is 0 otherwise it is 0, Pr(at−1 j |θt−1 j,l−1) is the probability that at−1 j is Bayes rational for the agent described by model θt−1 j,l−1, and SE(·) is an abbreviation for the belief update.",
                "For a version of the belief update when js model is subintentional, see [9].",
                "If agent j is also modeled as an I-POMDP, then is belief update invokes js belief update (via the term SEθt j ( bt−1 j,l−1 , at−1 j , ot j)), which in turn could invoke is belief update and so on.",
                "This recursion in belief nesting bottoms out at the 0th level.",
                "At this level, the belief update of the agent reduces to a POMDP belief update. 1 For illustrations of the belief update, additional details on I-POMDPs, and how they compare with other multiagent frameworks, see [9]. 2.2 Value Iteration Each belief state in a finitely nested I-POMDP has an associated value reflecting the maximum payoff the agent can expect in this belief state: Un ( bi,l, θi ) = max ai∈Ai is∈ISi,l ERi(is, ai)bi,l(is)+ γ oi∈Ωi Pr(oi|ai, bi,l)Un−1 ( SEθi (bi,l, ai, oi), θi ) (2) where, ERi(is, ai) = aj Ri(is, ai, aj)Pr(aj|mj,l−1) (since is = (s, mj,l−1)).",
                "Eq. 2 is a basis for value iteration in I-POMDPs.",
                "Agent is optimal action, a∗ i , for the case of finite horizon with discounting, is an element of the set of optimal actions for the belief state, OPT(θi), defined as: OPT( bi,l, θi ) = argmax ai∈Ai is∈ISi,l ERi(is, ai)bi,l(is) +γ oi∈Ωi Pr(oi|ai, bi,l)Un ( SEθi (bi,l, ai, oi), θi ) (3) 3.",
                "INTERACTIVEINFLUENCEDIAGRAMS A naive extension of influence diagrams (IDs) to settings populated by multiple agents is possible by treating other agents as automatons, represented using chance nodes.",
                "However, this approach assumes that the agents actions are controlled using a probability distribution that does not change over time.",
                "Interactive influence diagrams (I-IDs) adopt a more sophisticated approach by generalizing IDs to make them applicable to settings shared with other agents who may act and observe, and update their beliefs. 3.1 Syntax In addition to the usual chance, decision, and utility nodes, IIDs include a new type of node called the model node.",
                "We show a general level l I-ID in Fig. 1(a), where the model node (Mj,l−1) is denoted using a hexagon.",
                "We note that the probability distribution over the chance node, S, and the model node together represents agent is belief over its interactive states.",
                "In addition to the model 1 The 0th level model is a POMDP: Other agents actions are treated as exogenous events and folded into the T, O, and R functions.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 815 Figure 1: (a) A generic level l I-ID for agent i situated with one other agent j.",
                "The hexagon is the model node (Mj,l−1) whose structure we show in (b).",
                "Members of the model node are I-IDs themselves (m1 j,l−1, m2 j,l−1; diagrams not shown here for simplicity) whose decision nodes are mapped to the corresponding chance nodes (A1 j , A2 j ).",
                "Depending on the value of the node, Mod[Mj], the distribution of each of the chance nodes is assigned to the node Aj. (c) The transformed I-ID with the model node replaced by the chance nodes and the relationships between them. node, I-IDs differ from IDs by having a dashed link (called the policy link in [15]) between the model node and a chance node, Aj, that represents the distribution over the other agents actions given its model.",
                "In the absence of other agents, the model node and the chance node, Aj, vanish and I-IDs collapse into traditional IDs.",
                "The model node contains the alternative computational models ascribed by i to the other agent from the set, Θj,l−1 ∪ SMj, where Θj,l−1 and SMj were defined previously in Section 2.",
                "Thus, a model in the model node may itself be an I-ID or ID, and the recursion terminates when a model is an ID or subintentional.",
                "Because the model node contains the alternative models of the other agent as its values, its representation is not trivial.",
                "In particular, some of the models within the node are I-IDs that when solved generate the agents optimal policy in their decision nodes.",
                "Each decision node is mapped to the corresponding chance node, say A1 j , in the following way: if OPT is the set of optimal actions obtained by solving the I-ID (or ID), then Pr(aj ∈ A1 j ) = 1 |OP T | if aj ∈ OPT, 0 otherwise.",
                "Borrowing insights from previous work [8], we observe that the model node and the dashed policy link that connects it to the chance node, Aj, could be represented as shown in Fig. 1(b).",
                "The decision node of each level l − 1 I-ID is transformed into a chance node, as we mentioned previously, so that the actions with the largest value in the decision node are assigned uniform probabilities in the chance node while the rest are assigned zero probability.",
                "The different chance nodes (A1 j , A2 j ), one for each model, and additionally, the chance node labeled Mod[Mj] form the parents of the chance node, Aj.",
                "Thus, there are as many action nodes (A1 j , A2 j ) in Mj,l−1 as the number of models in the support of agent is beliefs.",
                "The conditional probability table of the chance node, Aj, is a multiplexer that assumes the distribution of each of the action nodes (A1 j , A2 j ) depending on the value of Mod[Mj].",
                "The values of Mod[Mj] denote the different models of j.",
                "In other words, when Mod[Mj] has the value m1 j,l−1, the chance node Aj assumes the distribution of the node A1 j , and Aj assumes the distribution of A2 j when Mod[Mj] has the value m2 j,l−1.",
                "The distribution over the node, Mod[Mj], is the agent is belief over the models of j given a physical state.",
                "For more agents, we will have as many model nodes as there are agents.",
                "Notice that Fig. 1(b) clarifies the semantics of the policy link, and shows how it can be represented using the traditional dependency links.",
                "In Fig. 1(c), we show the transformed I-ID when the model node is replaced by the chance nodes and relationships between them.",
                "In contrast to the representation in [15], there are no special-purpose policy links, rather the I-ID is composed of only those types of nodes that are found in traditional IDs and dependency relationships between the nodes.",
                "This allows I-IDs to be represented and implemented using conventional application tools that target IDs.",
                "Note that we may view the level l I-ID as a NID.",
                "Specifically, each of the level l − 1 models within the model node are blocks in the NID (see Fig. 2).",
                "If the level l = 1, each block is a traditional ID, otherwise if l > 1, each block within the NID may itself be a NID.",
                "Note that within the I-IDs (or IDs) at each level, there is only a single decision node.",
                "Thus, our NID does not contain any MAIDs.",
                "Figure 2: A level l I-ID represented as a NID.",
                "The probabilities assigned to the blocks of the NID are is beliefs over js models conditioned on a physical state. 3.2 Solution The solution of an I-ID proceeds in a bottom-up manner, and is implemented recursively.",
                "We start by solving the level 0 models, which, if intentional, are traditional IDs.",
                "Their solutions provide probability distributions over the other agents actions, which are entered in the corresponding chance nodes found in the model node of the level 1 I-ID.",
                "The mapping from the level 0 models decision nodes to the chance nodes is carried out so that actions with the largest value in the decision node are assigned uniform probabilities in the chance node while the rest are assigned zero probability.",
                "Given the distributions over the actions within the different chance nodes (one for each model of the other agent), the level 1 I-ID is transformed as shown in Fig. 1(c).",
                "During the transformation, the conditional probability table (CPT) of the node, Aj, is populated such that the node assumes the distribution of each of the chance nodes depending on the value of the node, Mod[Mj].",
                "As we mentioned previously, the values of the node Mod[Mj] denote the different models of the other agent, and its distribution is the agent is belief over the models of j conditioned on the physical state.",
                "The transformed level 1 I-ID is a traditional ID that may be solved us816 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) (a) (b) Figure 3: (a) A generic two time-slice level l I-DID for agent i in a setting with one other agent j.",
                "Notice the dotted model update link that denotes the update of the models of j and the distribution over the models over time. (b) The semantics of the model update link. ing the standard expected utility maximization method [18].",
                "This procedure is carried out up to the level l I-ID whose solution gives the non-empty set of optimal actions that the agent should perform given its belief.",
                "Notice that analogous to IDs, I-IDs are suitable for online decision-making when the agents current belief is known. 4.",
                "INTERACTIVE DYNAMIC INFLUENCE DIAGRAMS Interactive dynamic influence diagrams (I-DIDs) extend I-IDs (and NIDs) to allow sequential decision-making over several time steps.",
                "Just as DIDs are structured graphical representations of POMDPs, I-DIDs are the graphical online analogs for finitely nested I-POMDPs.",
                "I-DIDs may be used to optimize over a finite look-ahead given initial beliefs while interacting with other, possibly similar, agents. 4.1 Syntax We depict a general two time-slice I-DID in Fig. 3(a).",
                "In addition to the model nodes and the dashed policy link, what differentiates an I-DID from a DID is the model update link shown as a dotted arrow in Fig. 3(a).",
                "We explained the semantics of the model node and the policy link in the previous section; we describe the model updates next.",
                "The update of the model node over time involves two steps: First, given the models at time t, we identify the updated set of models that reside in the model node at time t + 1.",
                "Recall from Section 2 that an agents intentional model includes its belief.",
                "Because the agents act and receive observations, their models are updated to reflect their changed beliefs.",
                "Since the set of optimal actions for a model could include all the actions, and the agent may receive any one of |Ωj| possible observations, the updated set at time step t + 1 will have at most |Mt j,l−1||Aj||Ωj| models.",
                "Here, |Mt j,l−1| is the number of models at time step t, |Aj| and |Ωj| are the largest spaces of actions and observations respectively, among all the models.",
                "Second, we compute the new distribution over the updated models given the original distribution and the probability of the agent performing the action and receiving the observation that led to the updated model.",
                "These steps are a part of agent is belief update formalized using Eq. 1.",
                "In Fig. 3(b), we show how the dotted model update link is implemented in the I-DID.",
                "If each of the two level l − 1 models ascribed to j at time step t results in one action, and j could make one of two possible observations, then the model node at time step t + 1 contains four updated models (mt+1,1 j,l−1 ,mt+1,2 j,l−1 , mt+1,3 j,l−1 , and mt+1,4 j,l−1 ).",
                "These models differ in their initial beliefs, each of which is the result of j updating its beliefs due to its action and a possible observation.",
                "The decision nodes in each of the I-DIDs or DIDs that represent the lower level models are mapped to the corresponding Figure 4: Transformed I-DID with the model nodes and model update link replaced with the chance nodes and the relationships (in bold). chance nodes, as mentioned previously.",
                "Next, we describe how the distribution over the updated set of models (the distribution over the chance node Mod[Mt+1 j ] in Mt+1 j,l−1) is computed.",
                "The probability that js updated model is, say mt+1,1 j,l−1 , depends on the probability of j performing the action and receiving the observation that led to this model, and the prior distribution over the models at time step t. Because the chance node At j assumes the distribution of each of the action nodes based on the value of Mod[Mt j ], the probability of the action is given by this chance node.",
                "In order to obtain the probability of js possible observation, we introduce the chance node Oj, which depending on the value of Mod[Mt j ] assumes the distribution of the observation node in the lower level model denoted by Mod[Mt j ].",
                "Because the probability of js observations depends on the physical state and the joint actions of both agents, the node Oj is linked with St+1 , At j, and At i. 2 Analogous to At j, the conditional probability table of Oj is also a multiplexer modulated by Mod[Mt j ].",
                "Finally, the distribution over the prior models at time t is obtained from the chance node, Mod[Mt j ] in Mt j,l−1.",
                "Consequently, the chance nodes, Mod[Mt j ], At j, and Oj, form the parents of Mod[Mt+1 j ] in Mt+1 j,l−1.",
                "Notice that the model update link may be replaced by the dependency links between the chance nodes that constitute the model nodes in the two time slices.",
                "In Fig. 4 we show the two time-slice I-DID with the model nodes replaced by the chance nodes and the relationships between them.",
                "Chance nodes and dependency links that not in bold are standard, usually found in DIDs.",
                "Expansion of the I-DID over more time steps requires the repetition of the two steps of updating the set of models that form the 2 Note that Oj represents js observation at time t + 1.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 817 values of the model node and adding the relationships between the chance nodes, as many times as there are model update links.",
                "We note that the possible set of models of the other agent j grows exponentially with the number of time steps.",
                "For example, after T steps, there may be at most |Mt=1 j,l−1|(|Aj||Ωj|)T −1 candidate models residing in the model node. 4.2 Solution Analogous to I-IDs, the solution to a level l I-DID for agent i expanded over T time steps may be carried out recursively.",
                "For the purpose of illustration, let l=1 and T=2.",
                "The solution method uses the standard look-ahead technique, projecting the agents action and observation sequences forward from the current belief state [17], and finding the possible beliefs that i could have in the next time step.",
                "Because agent i has a belief over js models as well, the lookahead includes finding out the possible models that j could have in the future.",
                "Consequently, each of js subintentional or level 0 models (represented using a standard DID) in the first time step must be solved to obtain its optimal set of actions.",
                "These actions are combined with the set of possible observations that j could make in that model, resulting in an updated set of candidate models (that include the updated beliefs) that could describe the behavior of j. Beliefs over this updated set of candidate models are calculated using the standard inference methods using the dependency relationships between the model nodes as shown in Fig. 3(b).",
                "We note the recursive nature of this solution: in solving agent is level 1 I-DID, js level 0 DIDs must be solved.",
                "If the nesting of models is deeper, all models at all levels starting from 0 are solved in a bottom-up manner.",
                "We briefly outline the recursive algorithm for solving agent is Algorithm for solving I-DID Input : level l ≥ 1 I-ID or level 0 ID, T Expansion Phase 1.",
                "For t from 1 to T − 1 do 2.",
                "If l ≥ 1 then Populate Mt+1 j,l−1 3.",
                "For each mt j in Range(Mt j,l−1) do 4.",
                "Recursively call algorithm with the l − 1 I-ID (or ID) that represents mt j and the horizon, T − t + 1 5.",
                "Map the decision node of the solved I-ID (or ID), OPT(mt j), to a chance node Aj 6.",
                "For each aj in OPT(mt j) do 7.",
                "For each oj in Oj (part of mt j) do 8.",
                "Update js belief, bt+1 j ← SE(bt j, aj, oj) 9. mt+1 j ← New I-ID (or ID) with bt+1 j as the initial belief 10.",
                "Range(Mt+1 j,l−1) ∪ ← {mt+1 j } 11.",
                "Add the model node, Mt+1 j,l−1, and the dependency links between Mt j,l−1 and Mt+1 j,l−1 (shown in Fig. 3(b)) 12.",
                "Add the chance, decision, and utility nodes for t + 1 time slice and the dependency links between them 13.",
                "Establish the CPTs for each chance node and utility node Look-Ahead Phase 14.",
                "Apply the standard look-ahead and backup method to solve the expanded I-DID Figure 5: Algorithm for solving a level l ≥ 0 I-DID. level l I-DID expanded over T time steps with one other agent j in Fig. 5.",
                "We adopt a two-phase approach: Given an I-ID of level l (described previously in Section 3) with all lower level models also represented as I-IDs or IDs (if level 0), the first step is to expand the level l I-ID over T time steps adding the dependency links and the conditional probability tables for each node.",
                "We particularly focus on establishing and populating the model nodes (lines 3-11).",
                "Note that Range(·) returns the values (lower level models) of the random variable given as input (model node).",
                "In the second phase, we use a standard look-ahead technique projecting the action and observation sequences over T time steps in the future, and backing up the utility values of the reachable beliefs.",
                "Similar to I-IDs, the I-DIDs reduce to DIDs in the absence of other agents.",
                "As we mentioned previously, the 0-th level models are the traditional DIDs.",
                "Their solutions provide probability distributions over actions of the agent modeled at that level to I-DIDs at level 1.",
                "Given probability distributions over other agents actions the level 1 IDIDs can themselves be solved as DIDs, and provide probability distributions to yet higher level models.",
                "Assume that the number of models considered at each level is bound by a number, M. Solving an I-DID of level l in then equivalent to solving O(Ml ) DIDs. 5.",
                "EXAMPLE APPLICATIONS To illustrate the usefulness of I-DIDs, we apply them to three problem domains.",
                "We describe, in particular, the formulation of the I-DID and the optimal prescriptions obtained on solving it. 5.1 Followership-Leadership in the Multiagent Tiger Problem We begin our illustrations of using I-IDs and I-DIDs with a slightly modified version of the multiagent tiger problem discussed in [9].",
                "The problem has two agents, each of which can open the right door (OR), the left door (OL) or listen (L).",
                "In addition to hearing growls (from the left (GL) or from the right (GR)) when they listen, the agents also hear creaks (from the left (CL), from the right (CR), or no creaks (S)), which noisily indicate the other agents opening one of the doors.",
                "When any door is opened, the tiger persists in its original location with a probability of 95%.",
                "Agent i hears growls with a reliability of 65% and creaks with a reliability of 95%.",
                "Agent j, on the other hand, hears growls with a reliability of 95%.",
                "Thus, the setting is such that agent i hears agent j opening doors more reliably than the tigers growls.",
                "This suggests that i could use js actions as an indication of the location of the tiger, as we discuss below.",
                "Each agents preferences are as in the single agent game discussed in [13].",
                "The transition, observation, and reward functions are shown in [16].",
                "A good indicator of the usefulness of normative methods for decision-making like I-DIDs is the emergence of realistic social behaviors in their prescriptions.",
                "In settings of the persistent multiagent tiger problem that reflect real world situations, we demonstrate followership between the agents and, as shown in [15], deception among agents who believe that they are in a follower-leader type of relationship.",
                "In particular, we analyze the situational and epistemological conditions sufficient for their emergence.",
                "The followership behavior, for example, results from the agent knowing its own weaknesses, assessing the strengths, preferences, and possible behaviors of the other, and realizing that its best for it to follow the others actions in order to maximize its payoffs.",
                "Let us consider a particular setting of the tiger problem in which agent i believes that js preferences are aligned with its own - both of them just want to get the gold - and js hearing is more reliable in comparison to itself.",
                "As an example, suppose that j, on listening can discern the tigers location 95% of the times compared to is 65% accuracy.",
                "Additionally, agent i does not have any initial information about the tigers location.",
                "In other words, is single-level nested belief, bi,1, assigns 0.5 to each of the two locations of the tiger.",
                "In addition, i considers two models of j, which differ in js flat level 0 initial beliefs.",
                "This is represented in the level 1 I-ID shown in Fig. 6(a).",
                "According to one model, j assigns a probability of 0.9 that the tiger is behind the left door, while the other 818 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Figure 6: (a) Level 1 I-ID of agent i, (b) two level 0 IDs of agent j whose decision nodes are mapped to the chance nodes, A1 j , A2 j , in (a). model assigns 0.1 to that location (see Fig. 6(b)).",
                "Agent i is undecided on these two models of j.",
                "If we vary is hearing ability, and solve the corresponding level 1 I-ID expanded over three time steps, we obtain the normative behavioral policies shown in Fig 7 that exhibit followership behavior.",
                "If is probability of correctly hearing the growls is 0.65, then as shown in the policy in Fig. 7(a), i begins to conditionally follow js actions: i opens the same door that j opened previously iff is own assessment of the tigers location confirms js pick.",
                "If i loses the ability to correctly interpret the growls completely, it blindly follows j and opens the same door that j opened previously (Fig. 7(b)).",
                "Figure 7: Emergence of (a) conditional followership, and (b) blind followership in the tiger problem.",
                "Behaviors of interest are in bold. * is a wildcard, and denotes any one of the observations.",
                "We observed that a single level of belief nesting - beliefs about the others models - was sufficient for followership to emerge in the tiger problem.",
                "However, the epistemological requirements for the emergence of leadership are more complex.",
                "For an agent, say j, to emerge as a leader, followership must first emerge in the other agent i.",
                "As we mentioned previously, if i is certain that its preferences are identical to those of j, and believes that j has a better sense of hearing, i will follow js actions over time.",
                "Agent j emerges as a leader if it believes that i will follow it, which implies that js belief must be nested two levels deep to enable it to recognize its leadership role.",
                "Realizing that i will follow presents j with an opportunity to influence is actions in the benefit of the collective good or its self-interest alone.",
                "For example, in the tiger problem, let us consider a setting in which if both i and j open the correct door, then each gets a payoff of 20 that is double the original.",
                "If j alone selects the correct door, it gets the payoff of 10.",
                "On the other hand, if both agents pick the wrong door, their penalties are cut in half.",
                "In this setting, it is in both js best interest as well as the collective betterment for j to use its expertise in selecting the correct door, and thus be a good leader.",
                "However, consider a slightly different problem in which j gains from is loss and is penalized if i gains.",
                "Specifically, let is payoff be subtracted from js, indicating that j is antagonistic toward i - if j picks the correct door and i the wrong one, then is loss of 100 becomes js gain.",
                "Agent j believes that i incorrectly thinks that js preferences are those that promote the collective good and that it starts off by believing with 99% confidence where the tiger is.",
                "Because i believes that its preferences are similar to those of j, and that j starts by believing almost surely that one of the two is the correct location (two level 0 models of j), i will start by following js actions.",
                "We show is normative policy on solving its singly-nested I-DID over three time steps in Fig. 8(a).",
                "The policy demonstrates that i will blindly follow js actions.",
                "Since the tiger persists in its original location with a probability of 0.95, i will select the same door again.",
                "If j begins the game with a 99% probability that the tiger is on the right, solving js I-DID nested two levels deep, results in the policy shown in Fig. 8(b).",
                "Even though j is almost certain that OL is the correct action, it will start by selecting OR, followed by OL.",
                "Agent js intention is to deceive i who, it believes, will follow js actions, so as to gain $110 in the second time step, which is more than what j would gain if it were to be honest.",
                "Figure 8: Emergence of deception between agents in the tiger problem.",
                "Behaviors of interest are in bold. * denotes as before. (a) Agent is policy demonstrating that it will blindly follow js actions. (b) Even though j is almost certain that the tiger is on the right, it will start by selecting OR, followed by OL, in order to deceive i. 5.2 Altruism and Reciprocity in the Public Good Problem The public good (PG) problem [7], consists of a group of M agents, each of whom must either contribute some resource to a public pot or keep it for themselves.",
                "Since resources contributed to the public pot are shared among all the agents, they are less valuable to the agent when in the public pot.",
                "However, if all agents choose to contribute their resources, then the payoff to each agent is more than if no one contributes.",
                "Since an agent gets its share of the public pot irrespective of whether it has contributed or not, the dominating action is for each agent to not contribute, and instead free ride on others contributions.",
                "However, behaviors of human players in empirical simulations of the PG problem differ from the normative predictions.",
                "The experiments reveal that many players initially contribute a large amount to the public pot, and continue to contribute when the PG problem is played repeatedly, though in decreasing amounts [4].",
                "Many of these experiments [5] report that a small core group of players persistently contributes to the public pot even when all others are defecting.",
                "These experiments also reveal that players who persistently contribute have altruistic or reciprocal preferences matching expected cooperation of others.",
                "For simplicity, we assume that the game is played between M = 2 agents, i and j.",
                "Let each agent be initially endowed with XT amount of resources.",
                "While the classical PG game formulation permits each agent to contribute any quantity of resources (≤ XT ) to the public pot, we simplify the action space by allowing two possible actions.",
                "Each agent may choose to either contribute (C) a fixed amount of the resources, or not contribute.",
                "The latter action is deThe Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 819 noted as defect (D).",
                "We assume that the actions are not observable to others.",
                "The value of resources in the public pot is discounted by ci for each agent i, where ci is the marginal private return.",
                "We assume that ci < 1 so that the agent does not benefit enough that it contributes to the public pot for private gain.",
                "Simultaneously, ciM > 1, making collective contribution pareto optimal. i/j C D C 2ciXT , 2cjXT ciXT − cp, XT + cjXT − P D XT + ciXT − P, cjXT − cp XT , XT Table 1: The one-shot PG game with punishment.",
                "In order to encourage contributions, the contributing agents punish free riders but incur a small cost for administering the punishment.",
                "Let P be the punishment meted out to the defecting agent and cp the non-zero cost of punishing for the contributing agent.",
                "For simplicity, we assume that the cost of punishing is same for both the agents.",
                "The one-shot PG game with punishment is shown in Table. 1.",
                "Let ci = cj, cp > 0, and if P > XT − ciXT , then defection is no longer a dominating action.",
                "If P < XT − ciXT , then defection is the dominating action for both.",
                "If P = XT − ciXT , then the game is not dominance-solvable.",
                "Figure 9: (a) Level 1 I-ID of agent i, (b) level 0 IDs of agent j with decision nodes mapped to the chance nodes, A1 j and A2 j , in (a).",
                "We formulate a sequential version of the PG problem with punishment from the perspective of agent i.",
                "Though in the repeated PG game, the quantity in the public pot is revealed to all the agents after each round of actions, we assume in our formulation that it is hidden from the agents.",
                "Each agent may contribute a fixed amount, xc, or defect.",
                "An agent on performing an action receives an observation of plenty (PY) or meager (MR) symbolizing the state of the public pot.",
                "Notice that the observations are also indirectly indicative of agent js actions because the state of the public pot is influenced by them.",
                "The amount of resources in agent is private pot, is perfectly observable to i.",
                "The payoffs are analogous to Table. 1.",
                "Borrowing from the empirical investigations of the PG problem [5], we construct level 0 IDs for j that model altruistic and non-altruistic types (Fig. 9(b)).",
                "Specifically, our altruistic agent has a high marginal private return (cj is close to 1) and does not punish others who defect.",
                "Let xc = 1 and the level 0 agent be punished half the times it defects.",
                "With one action remaining, both types of agents choose to contribute to avoid being punished.",
                "With two actions to go, the altruistic type chooses to contribute, while the other defects.",
                "This is because cj for the altruistic type is close to 1, thus the expected punishment, 0.5P > (1 − cj), which the altruistic type avoids.",
                "Because cj for the non-altruistic type is less, it prefers not to contribute.",
                "With three steps to go, the altruistic agent contributes to avoid punishment (0.5P > 2(1 − cj)), and the non-altruistic type defects.",
                "For greater than three steps, while the altruistic agent continues to contribute to the public pot depending on how close its marginal private return is to 1, the non-altruistic type prescribes defection.",
                "We analyzed the decisions of an altruistic agent i modeled using a level 1 I-DID expanded over 3 time steps. i ascribes the two level 0 models, mentioned previously, to j (see Fig. 9).",
                "If i believes with a probability 1 that j is altruistic, i chooses to contribute for each of the three steps.",
                "This behavior persists when i is unaware of whether j is altruistic (Fig. 10(a)), and when i assigns a high probability to j being the non-altruistic type.",
                "However, when i believes with a probability 1 that j is non-altruistic and will thus surely defect, i chooses to defect to avoid being punished and because its marginal private return is less than 1.",
                "These results demonstrate that the behavior of our altruistic type resembles that found experimentally.",
                "The non-altruistic level 1 agent chooses to defect regardless of how likely it believes the other agent to be altruistic.",
                "We analyzed the behavior of a reciprocal agent type that matches expected cooperation or defection.",
                "The reciprocal types marginal private return is similar to that of the non-altruistic type, however, it obtains a greater payoff when its action is similar to that of the other.",
                "We consider the case when the reciprocal agent i is unsure of whether j is altruistic and believes that the public pot is likely to be half full.",
                "For this prior belief, i chooses to defect.",
                "On receiving an observation of plenty, i decides to contribute, while an observation of meager makes it defect (Fig. 10(b)).",
                "This is because an observation of plenty signals that the pot is likely to be greater than half full, which results from js action to contribute.",
                "Thus, among the two models ascribed to j, its type is likely to be altruistic making it likely that j will contribute again in the next time step.",
                "Agent i therefore chooses to contribute to reciprocate js action.",
                "An analogous reasoning leads i to defect when it observes a meager pot.",
                "With one action to go, i believing that j contributes, will choose to contribute too to avoid punishment regardless of its observations.",
                "Figure 10: (a) An altruistic level 1 agent always contributes. (b) A reciprocal agent i starts off by defecting followed by choosing to contribute or defect based on its observation of plenty (indicating that j is likely altruistic) or meager (j is non-altruistic). 5.3 Strategies in Two-Player Poker Poker is a popular zero sum card game that has received much attention among the AI research community as a testbed [2].",
                "Poker is played among M ≥ 2 players in which each player receives a hand of cards from a deck.",
                "While several flavors of Poker with varying complexity exist, we consider a simple version in which each player has three plys during which the player may either exchange a card (E), keep the existing hand (K), fold (F) and withdraw from the game, or call (C), requiring all players to show their hands.",
                "To keep matters simple, let M = 2, and each player receive a hand consisting of a single card drawn from the same suit.",
                "Thus, during a showdown, the player who has the numerically larger card (2 is the lowest, ace is the highest) wins the pot.",
                "During an exchange of cards, the discarded card is placed either in the L pile, indicating to the other agent that it was a low numbered card less than 8, or in the 820 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) H pile, indicating that the card had a rank greater than or equal to 8.",
                "Notice that, for example, if a lower numbered card is discarded, the probability of receiving a low card in exchange is now reduced.",
                "We show the level 1 I-ID for the simplified two-player Poker in Fig. 11.",
                "We considered two models (personality types) of agent j.",
                "The conservative type believes that it is likely that its opponent has a high numbered card in its hand.",
                "On the other hand, the aggressive agent j believes with a high probability that its opponent has a lower numbered card.",
                "Thus, the two types differ in their beliefs over their opponents hand.",
                "In both these level 0 models, the opponent is assumed to perform its actions following a fixed, uniform distribution.",
                "With three actions to go, regardless of its hand (unless it is an ace), the aggressive agent chooses to exchange its card, with the intent of improving on its current hand.",
                "This is because it believes the other to have a low card, which improves its chances of getting a high card during the exchange.",
                "The conservative agent chooses to keep its card, no matter its hand because its chances of getting a high card are slim as it believes that its opponent has one.",
                "Figure 11: (a) Level 1 I-ID of agent i.",
                "The observation reveals information about js hand of the previous time step, (b) level 0 IDs of agent j whose decision nodes are mapped to the chance nodes, A1 j , A2 j , in (a).",
                "The policy of a level 1 agent i who believes that each card except its own has an equal likelihood of being in js hand (neutral personality type) and j could be either an aggressive or conservative type, is shown in Fig. 12. is own hand contains the card numbered 8.",
                "The agent starts by keeping its card.",
                "On seeing that j did not exchange a card (N), i believes with probability 1 that j is conservative and hence will keep its cards. i responds by either keeping its card or exchanging it because j is equally likely to have a lower or higher card.",
                "If i observes that j discarded its card into the L or H pile, i believes that j is aggressive.",
                "On observing L, i realizes that j had a low card, and is likely to have a high card after its exchange.",
                "Because the probability of receiving a low card is high now, i chooses to keep its card.",
                "On observing H, believing that the probability of receiving a high numbered card is high, i chooses to exchange its card.",
                "In the final step, i chooses to call regardless of its observation history because its belief that j has a higher card is not sufficiently high to conclude that its better to fold and relinquish the payoff.",
                "This is partly due to the fact that an observation of, say, L resets the agent is previous time step beliefs over js hand to the low numbered cards only. 6.",
                "DISCUSSION We showed how DIDs may be extended to I-DIDs that enable online sequential decision-making in uncertain multiagent settings.",
                "Our graphical representation of I-DIDs improves on the previous Figure 12: A level 1 agent is three step policy in the Poker problem. i starts by believing that j is equally likely to be aggressive or conservative and could have any card in its hand with equal probability. work significantly by being more transparent, semantically clear, and capable of being solved using standard algorithms that target DIDs.",
                "I-DIDs extend NIDs to allow sequential decision-making over multiple time steps in the presence of other interacting agents.",
                "I-DIDs may be seen as concise graphical representations for IPOMDPs providing a way to exploit problem structure and carry out online decision-making as the agent acts and observes given its prior beliefs.",
                "We are currently investigating ways to solve I-DIDs approximately with provable bounds on the solution quality.",
                "Acknowledgment: We thank Piotr Gmytrasiewicz for some useful discussions related to this work.",
                "The first author would like to acknowledge the support of a UGARF grant. 7.",
                "REFERENCES [1] R. J. Aumann.",
                "Interactive epistemology i: Knowledge.",
                "International Journal of Game Theory, 28:263-300, 1999. [2] D. Billings, A. Davidson, J. Schaeffer, and D. Szafron.",
                "The challenge of poker.",
                "AIJ, 2001. [3] A. Brandenburger and E. Dekel.",
                "Hierarchies of beliefs and common knowledge.",
                "Journal of Economic Theory, 59:189-198, 1993. [4] C. Camerer.",
                "Behavioral Game Theory: Experiments in Strategic Interaction.",
                "Princeton University Press, 2003. [5] E. Fehr and S. Gachter.",
                "Cooperation and punishment in public goods experiments.",
                "American Economic Review, 90(4):980-994, 2000. [6] D. Fudenberg and D. K. Levine.",
                "The Theory of Learning in Games.",
                "MIT Press, 1998. [7] D. Fudenberg and J. Tirole.",
                "Game Theory.",
                "MIT Press, 1991. [8] Y. Gal and A. Pfeffer.",
                "A language for modeling agents decision-making processes in games.",
                "In AAMAS, 2003. [9] P. Gmytrasiewicz and P. Doshi.",
                "A framework for sequential planning in multiagent settings.",
                "JAIR, 24:49-79, 2005. [10] P. Gmytrasiewicz and E. Durfee.",
                "Rational coordination in multi-agent environments.",
                "JAAMAS, 3(4):319-350, 2000. [11] J. C. Harsanyi.",
                "Games with incomplete information played by bayesian players.",
                "Management Science, 14(3):159-182, 1967. [12] R. A. Howard and J. E. Matheson.",
                "Influence diagrams.",
                "In R. A. Howard and J. E. Matheson, editors, The Principles and Applications of Decision Analysis.",
                "Strategic Decisions Group, Menlo Park, CA 94025, 1984. [13] L. Kaelbling, M. Littman, and A. Cassandra.",
                "Planning and acting in partially observable stochastic domains.",
                "Artificial Intelligence Journal, 2, 1998. [14] D. Koller and B. Milch.",
                "Multi-agent influence diagrams for representing and solving games.",
                "In IJCAI, pages 1027-1034, 2001. [15] K. Polich and P. Gmytrasiewicz.",
                "Interactive dynamic influence diagrams.",
                "In GTDT Workshop, AAMAS, 2006. [16] B.",
                "Rathnas., P. Doshi, and P. J. Gmytrasiewicz.",
                "Exact solutions to interactive pomdps using behavioral equivalence.",
                "In Autonomous Agents and Multi-Agent Systems Conference (AAMAS), 2006. [17] S. Russell and P. Norvig.",
                "Artificial Intelligence: A Modern Approach (Second Edition).",
                "Prentice Hall, 2003. [18] R. D. Shachter.",
                "Evaluating influence diagrams.",
                "Operations Research, 34(6):871-882, 1986. [19] D. Suryadi and P. Gmytrasiewicz.",
                "Learning models of other agents using influence diagrams.",
                "In UM, 1999.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 821"
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [],
            "translated_text": "",
            "candidates": [],
            "error": [
                []
            ]
        },
        "interactive dynamic influence diagram": {
            "translated_key": "",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Graphical Models for Online Solutions to Interactive POMDPs Prashant Doshi Dept.",
                "of Computer Science University of Georgia Athens, GA 30602, USA pdoshi@cs.uga.edu Yifeng Zeng Dept. of Computer Science Aalborg University DK-9220 Aalborg, Denmark yfzeng@cs.aau.edu Qiongyu Chen Dept. of Computer Science National Univ. of Singapore 117543, Singapore chenqy@comp.nus.edu.sg ABSTRACT We develop a new graphical representation for interactive partially observable Markov decision processes (I-POMDPs) that is significantly more transparent and semantically clear than the previous representation.",
                "These graphical models called <br>interactive dynamic influence diagram</br>s (I-DIDs) seek to explicitly model the structure that is often present in real-world problems by decomposing the situation into chance and decision variables, and the dependencies between the variables.",
                "I-DIDs generalize DIDs, which may be viewed as graphical representations of POMDPs, to multiagent settings in the same way that I-POMDPs generalize POMDPs.",
                "I-DIDs may be used to compute the policy of an agent online as the agent acts and observes in a setting that is populated by other interacting agents.",
                "Using several examples, we show how I-DIDs may be applied and demonstrate their usefulness.",
                "Categories and Subject Descriptors I.2.11 [Distributed Artificial Intelligence]: Multiagent Systems General Terms Theory 1.",
                "INTRODUCTION Interactive partially observable Markov decision processes (IPOMDPs) [9] provide a framework for sequential decision-making in partially observable multiagent environments.",
                "They generalize POMDPs [13] to multiagent settings by including the other agents computable models in the state space along with the states of the physical environment.",
                "The models encompass all information influencing the agents behaviors, including their preferences, capabilities, and beliefs, and are thus analogous to types in Bayesian games [11].",
                "I-POMDPs adopt a subjective approach to understanding strategic behavior, rooted in a decision-theoretic framework that takes a decision-makers perspective in the interaction.",
                "In [15], Polich and Gmytrasiewicz introduced <br>interactive dynamic influence diagram</br>s (I-DIDs) as the computational representations of I-POMDPs.",
                "I-DIDs generalize DIDs [12], which may be viewed as computational counterparts of POMDPs, to multiagents settings in the same way that I-POMDPs generalize POMDPs.",
                "I-DIDs contribute to a growing line of work [19] that includes multi-agent influence diagrams (MAIDs) [14], and more recently, networks of influence diagrams (NIDs) [8].",
                "These formalisms seek to explicitly model the structure that is often present in real-world problems by decomposing the situation into chance and decision variables, and the dependencies between the variables.",
                "MAIDs provide an alternative to normal and extensive game forms using a graphical formalism to represent games of imperfect information with a decision node for each agents actions and chance nodes capturing the agents private information.",
                "MAIDs objectively analyze the game, efficiently computing the Nash equilibrium profile by exploiting the independence structure.",
                "NIDs extend MAIDs to include agents uncertainty over the game being played and over models of the other agents.",
                "Each model is a MAID and the network of MAIDs is collapsed, bottom up, into a single MAID for computing the equilibrium of the game keeping in mind the different models of each agent.",
                "Graphical formalisms such as MAIDs and NIDs open up a promising area of research that aims to represent multiagent interactions more transparently.",
                "However, MAIDs provide an analysis of the game from an external viewpoint and the applicability of both is limited to static single play games.",
                "Matters are more complex when we consider interactions that are extended over time, where predictions about others future actions must be made using models that change as the agents act and observe.",
                "I-DIDs address this gap by allowing the representation of other agents models as the values of a special model node.",
                "Both, other agents models and the original agents beliefs over these models are updated over time using special-purpose implementations.",
                "In this paper, we improve on the previous preliminary representation of the I-DID shown in [15] by using the insight that the static I-ID is a type of NID.",
                "Thus, we may utilize NID-specific language constructs such as multiplexers to represent the model node, and subsequently the I-ID, more transparently.",
                "Furthermore, we clarify the semantics of the special purpose policy link introduced in the representation of I-DID by [15], and show that it could be replaced by traditional dependency links.",
                "In the previous representation of the I-DID, the update of the agents belief over the models of others as the agents act and receive observations was denoted using a special link called the model update link that connected the model nodes over time.",
                "We explicate the semantics of this link by showing how it can be implemented using the traditional dependency links between the chance nodes that constitute the model nodes.",
                "The net result is a representation of I-DID that is significantly more transparent, semantically clear, and capable of being implemented using the standard algorithms for solving DIDs.",
                "We show how IDIDs may be used to model an agents uncertainty over others models, that may themselves be I-DIDs.",
                "Solution to the I-DID is a policy that prescribes what the agent should do over time, given its beliefs over the physical state and others models.",
                "Analogous to DIDs, I-DIDs may be used to compute the policy of an agent online as the agent acts and observes in a setting that is populated by other interacting agents. 2.",
                "BACKGROUND: FINITELY NESTED IPOMDPS Interactive POMDPs generalize POMDPs to multiagent settings by including other agents models as part of the state space [9].",
                "Since other agents may also reason about others, the interactive state space is strategically nested; it contains beliefs about other agents models and their beliefs about others.",
                "For simplicity of presentation we consider an agent, i, that is interacting with one other agent, j.",
                "A finitely nested I-POMDP of agent i with a strategy level l is defined as the tuple: I-POMDPi,l = ISi,l, A, Ti, Ωi, Oi, Ri where: • ISi,l denotes a set of interactive states defined as, ISi,l = S × Mj,l−1, where Mj,l−1 = {Θj,l−1 ∪ SMj}, for l ≥ 1, and ISi,0 = S, where S is the set of states of the physical environment.",
                "Θj,l−1 is the set of computable intentional models of agent j: θj,l−1 = bj,l−1, ˆθj where the frame, ˆθj = A, Ωj, Tj, Oj, Rj, OCj .",
                "Here, j is Bayes rational and OCj is js optimality criterion.",
                "SMj is the set of subintentional models of j.",
                "Simple examples of subintentional models include a no-information model [10] and a fictitious play model [6], both of which are history independent.",
                "We give a recursive bottom-up construction of the interactive state space below.",
                "ISi,0 = S, Θj,0 = { bj,0, ˆθj | bj,0 ∈ Δ(ISj,0)} ISi,1 = S × {Θj,0 ∪ SMj}, Θj,1 = { bj,1, ˆθj | bj,1 ∈ Δ(ISj,1)} . . . . . .",
                "ISi,l = S × {Θj,l−1 ∪ SMj}, Θj,l = { bj,l, ˆθj | bj,l ∈ Δ(ISj,l)} Similar formulations of nested spaces have appeared in [1, 3]. • A = Ai × Aj is the set of joint actions of all agents in the environment; • Ti : S ×A×S → [0, 1], describes the effect of the joint actions on the physical states of the environment; • Ωi is the set of observations of agent i; • Oi : S × A × Ωi → [0, 1] gives the likelihood of the observations given the physical state and joint action; • Ri : ISi × A → R describes agent is preferences over its interactive states.",
                "Usually only the physical states will matter.",
                "Agent is policy is the mapping, Ω∗ i → Δ(Ai), where Ω∗ i is the set of all observation histories of agent i.",
                "Since belief over the interactive states forms a sufficient statistic [9], the policy can also be represented as a mapping from the set of all beliefs of agent i to a distribution over its actions, Δ(ISi) → Δ(Ai). 2.1 Belief Update Analogous to POMDPs, an agent within the I-POMDP framework updates its belief as it acts and observes.",
                "However, there are two differences that complicate the belief update in multiagent settings when compared to single agent ones.",
                "First, since the state of the physical environment depends on the actions of both agents, is prediction of how the physical state changes has to be made based on its prediction of js actions.",
                "Second, changes in js models have to be included in is belief update.",
                "Specifically, if j is intentional then an update of js beliefs due to its action and observation has to be included.",
                "In other words, i has to update its belief based on its prediction of what j would observe and how j would update its belief.",
                "If js model is subintentional, then js probable observations are appended to the observation history contained in the model.",
                "Formally, we have: Pr(ist |at−1 i , bt−1 i,l ) = β ISt−1:mt−1 j =θt j bt−1 i,l (ist−1 ) × at−1 j Pr(at−1 j |θt−1 j,l−1)Oi(st , at−1 i , at−1 j , ot i) ×Ti(st−1 , at−1 i , at−1 j , st ) ot j Oj(st , at−1 i , at−1 j , ot j) ×τ(SEθt j (bt−1 j,l−1, at−1 j , ot j) − bt j,l−1) (1) where β is the normalizing constant, τ is 1 if its argument is 0 otherwise it is 0, Pr(at−1 j |θt−1 j,l−1) is the probability that at−1 j is Bayes rational for the agent described by model θt−1 j,l−1, and SE(·) is an abbreviation for the belief update.",
                "For a version of the belief update when js model is subintentional, see [9].",
                "If agent j is also modeled as an I-POMDP, then is belief update invokes js belief update (via the term SEθt j ( bt−1 j,l−1 , at−1 j , ot j)), which in turn could invoke is belief update and so on.",
                "This recursion in belief nesting bottoms out at the 0th level.",
                "At this level, the belief update of the agent reduces to a POMDP belief update. 1 For illustrations of the belief update, additional details on I-POMDPs, and how they compare with other multiagent frameworks, see [9]. 2.2 Value Iteration Each belief state in a finitely nested I-POMDP has an associated value reflecting the maximum payoff the agent can expect in this belief state: Un ( bi,l, θi ) = max ai∈Ai is∈ISi,l ERi(is, ai)bi,l(is)+ γ oi∈Ωi Pr(oi|ai, bi,l)Un−1 ( SEθi (bi,l, ai, oi), θi ) (2) where, ERi(is, ai) = aj Ri(is, ai, aj)Pr(aj|mj,l−1) (since is = (s, mj,l−1)).",
                "Eq. 2 is a basis for value iteration in I-POMDPs.",
                "Agent is optimal action, a∗ i , for the case of finite horizon with discounting, is an element of the set of optimal actions for the belief state, OPT(θi), defined as: OPT( bi,l, θi ) = argmax ai∈Ai is∈ISi,l ERi(is, ai)bi,l(is) +γ oi∈Ωi Pr(oi|ai, bi,l)Un ( SEθi (bi,l, ai, oi), θi ) (3) 3.",
                "INTERACTIVEINFLUENCEDIAGRAMS A naive extension of influence diagrams (IDs) to settings populated by multiple agents is possible by treating other agents as automatons, represented using chance nodes.",
                "However, this approach assumes that the agents actions are controlled using a probability distribution that does not change over time.",
                "Interactive influence diagrams (I-IDs) adopt a more sophisticated approach by generalizing IDs to make them applicable to settings shared with other agents who may act and observe, and update their beliefs. 3.1 Syntax In addition to the usual chance, decision, and utility nodes, IIDs include a new type of node called the model node.",
                "We show a general level l I-ID in Fig. 1(a), where the model node (Mj,l−1) is denoted using a hexagon.",
                "We note that the probability distribution over the chance node, S, and the model node together represents agent is belief over its interactive states.",
                "In addition to the model 1 The 0th level model is a POMDP: Other agents actions are treated as exogenous events and folded into the T, O, and R functions.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 815 Figure 1: (a) A generic level l I-ID for agent i situated with one other agent j.",
                "The hexagon is the model node (Mj,l−1) whose structure we show in (b).",
                "Members of the model node are I-IDs themselves (m1 j,l−1, m2 j,l−1; diagrams not shown here for simplicity) whose decision nodes are mapped to the corresponding chance nodes (A1 j , A2 j ).",
                "Depending on the value of the node, Mod[Mj], the distribution of each of the chance nodes is assigned to the node Aj. (c) The transformed I-ID with the model node replaced by the chance nodes and the relationships between them. node, I-IDs differ from IDs by having a dashed link (called the policy link in [15]) between the model node and a chance node, Aj, that represents the distribution over the other agents actions given its model.",
                "In the absence of other agents, the model node and the chance node, Aj, vanish and I-IDs collapse into traditional IDs.",
                "The model node contains the alternative computational models ascribed by i to the other agent from the set, Θj,l−1 ∪ SMj, where Θj,l−1 and SMj were defined previously in Section 2.",
                "Thus, a model in the model node may itself be an I-ID or ID, and the recursion terminates when a model is an ID or subintentional.",
                "Because the model node contains the alternative models of the other agent as its values, its representation is not trivial.",
                "In particular, some of the models within the node are I-IDs that when solved generate the agents optimal policy in their decision nodes.",
                "Each decision node is mapped to the corresponding chance node, say A1 j , in the following way: if OPT is the set of optimal actions obtained by solving the I-ID (or ID), then Pr(aj ∈ A1 j ) = 1 |OP T | if aj ∈ OPT, 0 otherwise.",
                "Borrowing insights from previous work [8], we observe that the model node and the dashed policy link that connects it to the chance node, Aj, could be represented as shown in Fig. 1(b).",
                "The decision node of each level l − 1 I-ID is transformed into a chance node, as we mentioned previously, so that the actions with the largest value in the decision node are assigned uniform probabilities in the chance node while the rest are assigned zero probability.",
                "The different chance nodes (A1 j , A2 j ), one for each model, and additionally, the chance node labeled Mod[Mj] form the parents of the chance node, Aj.",
                "Thus, there are as many action nodes (A1 j , A2 j ) in Mj,l−1 as the number of models in the support of agent is beliefs.",
                "The conditional probability table of the chance node, Aj, is a multiplexer that assumes the distribution of each of the action nodes (A1 j , A2 j ) depending on the value of Mod[Mj].",
                "The values of Mod[Mj] denote the different models of j.",
                "In other words, when Mod[Mj] has the value m1 j,l−1, the chance node Aj assumes the distribution of the node A1 j , and Aj assumes the distribution of A2 j when Mod[Mj] has the value m2 j,l−1.",
                "The distribution over the node, Mod[Mj], is the agent is belief over the models of j given a physical state.",
                "For more agents, we will have as many model nodes as there are agents.",
                "Notice that Fig. 1(b) clarifies the semantics of the policy link, and shows how it can be represented using the traditional dependency links.",
                "In Fig. 1(c), we show the transformed I-ID when the model node is replaced by the chance nodes and relationships between them.",
                "In contrast to the representation in [15], there are no special-purpose policy links, rather the I-ID is composed of only those types of nodes that are found in traditional IDs and dependency relationships between the nodes.",
                "This allows I-IDs to be represented and implemented using conventional application tools that target IDs.",
                "Note that we may view the level l I-ID as a NID.",
                "Specifically, each of the level l − 1 models within the model node are blocks in the NID (see Fig. 2).",
                "If the level l = 1, each block is a traditional ID, otherwise if l > 1, each block within the NID may itself be a NID.",
                "Note that within the I-IDs (or IDs) at each level, there is only a single decision node.",
                "Thus, our NID does not contain any MAIDs.",
                "Figure 2: A level l I-ID represented as a NID.",
                "The probabilities assigned to the blocks of the NID are is beliefs over js models conditioned on a physical state. 3.2 Solution The solution of an I-ID proceeds in a bottom-up manner, and is implemented recursively.",
                "We start by solving the level 0 models, which, if intentional, are traditional IDs.",
                "Their solutions provide probability distributions over the other agents actions, which are entered in the corresponding chance nodes found in the model node of the level 1 I-ID.",
                "The mapping from the level 0 models decision nodes to the chance nodes is carried out so that actions with the largest value in the decision node are assigned uniform probabilities in the chance node while the rest are assigned zero probability.",
                "Given the distributions over the actions within the different chance nodes (one for each model of the other agent), the level 1 I-ID is transformed as shown in Fig. 1(c).",
                "During the transformation, the conditional probability table (CPT) of the node, Aj, is populated such that the node assumes the distribution of each of the chance nodes depending on the value of the node, Mod[Mj].",
                "As we mentioned previously, the values of the node Mod[Mj] denote the different models of the other agent, and its distribution is the agent is belief over the models of j conditioned on the physical state.",
                "The transformed level 1 I-ID is a traditional ID that may be solved us816 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) (a) (b) Figure 3: (a) A generic two time-slice level l I-DID for agent i in a setting with one other agent j.",
                "Notice the dotted model update link that denotes the update of the models of j and the distribution over the models over time. (b) The semantics of the model update link. ing the standard expected utility maximization method [18].",
                "This procedure is carried out up to the level l I-ID whose solution gives the non-empty set of optimal actions that the agent should perform given its belief.",
                "Notice that analogous to IDs, I-IDs are suitable for online decision-making when the agents current belief is known. 4.",
                "INTERACTIVE DYNAMIC INFLUENCE DIAGRAMS Interactive dynamic influence diagrams (I-DIDs) extend I-IDs (and NIDs) to allow sequential decision-making over several time steps.",
                "Just as DIDs are structured graphical representations of POMDPs, I-DIDs are the graphical online analogs for finitely nested I-POMDPs.",
                "I-DIDs may be used to optimize over a finite look-ahead given initial beliefs while interacting with other, possibly similar, agents. 4.1 Syntax We depict a general two time-slice I-DID in Fig. 3(a).",
                "In addition to the model nodes and the dashed policy link, what differentiates an I-DID from a DID is the model update link shown as a dotted arrow in Fig. 3(a).",
                "We explained the semantics of the model node and the policy link in the previous section; we describe the model updates next.",
                "The update of the model node over time involves two steps: First, given the models at time t, we identify the updated set of models that reside in the model node at time t + 1.",
                "Recall from Section 2 that an agents intentional model includes its belief.",
                "Because the agents act and receive observations, their models are updated to reflect their changed beliefs.",
                "Since the set of optimal actions for a model could include all the actions, and the agent may receive any one of |Ωj| possible observations, the updated set at time step t + 1 will have at most |Mt j,l−1||Aj||Ωj| models.",
                "Here, |Mt j,l−1| is the number of models at time step t, |Aj| and |Ωj| are the largest spaces of actions and observations respectively, among all the models.",
                "Second, we compute the new distribution over the updated models given the original distribution and the probability of the agent performing the action and receiving the observation that led to the updated model.",
                "These steps are a part of agent is belief update formalized using Eq. 1.",
                "In Fig. 3(b), we show how the dotted model update link is implemented in the I-DID.",
                "If each of the two level l − 1 models ascribed to j at time step t results in one action, and j could make one of two possible observations, then the model node at time step t + 1 contains four updated models (mt+1,1 j,l−1 ,mt+1,2 j,l−1 , mt+1,3 j,l−1 , and mt+1,4 j,l−1 ).",
                "These models differ in their initial beliefs, each of which is the result of j updating its beliefs due to its action and a possible observation.",
                "The decision nodes in each of the I-DIDs or DIDs that represent the lower level models are mapped to the corresponding Figure 4: Transformed I-DID with the model nodes and model update link replaced with the chance nodes and the relationships (in bold). chance nodes, as mentioned previously.",
                "Next, we describe how the distribution over the updated set of models (the distribution over the chance node Mod[Mt+1 j ] in Mt+1 j,l−1) is computed.",
                "The probability that js updated model is, say mt+1,1 j,l−1 , depends on the probability of j performing the action and receiving the observation that led to this model, and the prior distribution over the models at time step t. Because the chance node At j assumes the distribution of each of the action nodes based on the value of Mod[Mt j ], the probability of the action is given by this chance node.",
                "In order to obtain the probability of js possible observation, we introduce the chance node Oj, which depending on the value of Mod[Mt j ] assumes the distribution of the observation node in the lower level model denoted by Mod[Mt j ].",
                "Because the probability of js observations depends on the physical state and the joint actions of both agents, the node Oj is linked with St+1 , At j, and At i. 2 Analogous to At j, the conditional probability table of Oj is also a multiplexer modulated by Mod[Mt j ].",
                "Finally, the distribution over the prior models at time t is obtained from the chance node, Mod[Mt j ] in Mt j,l−1.",
                "Consequently, the chance nodes, Mod[Mt j ], At j, and Oj, form the parents of Mod[Mt+1 j ] in Mt+1 j,l−1.",
                "Notice that the model update link may be replaced by the dependency links between the chance nodes that constitute the model nodes in the two time slices.",
                "In Fig. 4 we show the two time-slice I-DID with the model nodes replaced by the chance nodes and the relationships between them.",
                "Chance nodes and dependency links that not in bold are standard, usually found in DIDs.",
                "Expansion of the I-DID over more time steps requires the repetition of the two steps of updating the set of models that form the 2 Note that Oj represents js observation at time t + 1.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 817 values of the model node and adding the relationships between the chance nodes, as many times as there are model update links.",
                "We note that the possible set of models of the other agent j grows exponentially with the number of time steps.",
                "For example, after T steps, there may be at most |Mt=1 j,l−1|(|Aj||Ωj|)T −1 candidate models residing in the model node. 4.2 Solution Analogous to I-IDs, the solution to a level l I-DID for agent i expanded over T time steps may be carried out recursively.",
                "For the purpose of illustration, let l=1 and T=2.",
                "The solution method uses the standard look-ahead technique, projecting the agents action and observation sequences forward from the current belief state [17], and finding the possible beliefs that i could have in the next time step.",
                "Because agent i has a belief over js models as well, the lookahead includes finding out the possible models that j could have in the future.",
                "Consequently, each of js subintentional or level 0 models (represented using a standard DID) in the first time step must be solved to obtain its optimal set of actions.",
                "These actions are combined with the set of possible observations that j could make in that model, resulting in an updated set of candidate models (that include the updated beliefs) that could describe the behavior of j. Beliefs over this updated set of candidate models are calculated using the standard inference methods using the dependency relationships between the model nodes as shown in Fig. 3(b).",
                "We note the recursive nature of this solution: in solving agent is level 1 I-DID, js level 0 DIDs must be solved.",
                "If the nesting of models is deeper, all models at all levels starting from 0 are solved in a bottom-up manner.",
                "We briefly outline the recursive algorithm for solving agent is Algorithm for solving I-DID Input : level l ≥ 1 I-ID or level 0 ID, T Expansion Phase 1.",
                "For t from 1 to T − 1 do 2.",
                "If l ≥ 1 then Populate Mt+1 j,l−1 3.",
                "For each mt j in Range(Mt j,l−1) do 4.",
                "Recursively call algorithm with the l − 1 I-ID (or ID) that represents mt j and the horizon, T − t + 1 5.",
                "Map the decision node of the solved I-ID (or ID), OPT(mt j), to a chance node Aj 6.",
                "For each aj in OPT(mt j) do 7.",
                "For each oj in Oj (part of mt j) do 8.",
                "Update js belief, bt+1 j ← SE(bt j, aj, oj) 9. mt+1 j ← New I-ID (or ID) with bt+1 j as the initial belief 10.",
                "Range(Mt+1 j,l−1) ∪ ← {mt+1 j } 11.",
                "Add the model node, Mt+1 j,l−1, and the dependency links between Mt j,l−1 and Mt+1 j,l−1 (shown in Fig. 3(b)) 12.",
                "Add the chance, decision, and utility nodes for t + 1 time slice and the dependency links between them 13.",
                "Establish the CPTs for each chance node and utility node Look-Ahead Phase 14.",
                "Apply the standard look-ahead and backup method to solve the expanded I-DID Figure 5: Algorithm for solving a level l ≥ 0 I-DID. level l I-DID expanded over T time steps with one other agent j in Fig. 5.",
                "We adopt a two-phase approach: Given an I-ID of level l (described previously in Section 3) with all lower level models also represented as I-IDs or IDs (if level 0), the first step is to expand the level l I-ID over T time steps adding the dependency links and the conditional probability tables for each node.",
                "We particularly focus on establishing and populating the model nodes (lines 3-11).",
                "Note that Range(·) returns the values (lower level models) of the random variable given as input (model node).",
                "In the second phase, we use a standard look-ahead technique projecting the action and observation sequences over T time steps in the future, and backing up the utility values of the reachable beliefs.",
                "Similar to I-IDs, the I-DIDs reduce to DIDs in the absence of other agents.",
                "As we mentioned previously, the 0-th level models are the traditional DIDs.",
                "Their solutions provide probability distributions over actions of the agent modeled at that level to I-DIDs at level 1.",
                "Given probability distributions over other agents actions the level 1 IDIDs can themselves be solved as DIDs, and provide probability distributions to yet higher level models.",
                "Assume that the number of models considered at each level is bound by a number, M. Solving an I-DID of level l in then equivalent to solving O(Ml ) DIDs. 5.",
                "EXAMPLE APPLICATIONS To illustrate the usefulness of I-DIDs, we apply them to three problem domains.",
                "We describe, in particular, the formulation of the I-DID and the optimal prescriptions obtained on solving it. 5.1 Followership-Leadership in the Multiagent Tiger Problem We begin our illustrations of using I-IDs and I-DIDs with a slightly modified version of the multiagent tiger problem discussed in [9].",
                "The problem has two agents, each of which can open the right door (OR), the left door (OL) or listen (L).",
                "In addition to hearing growls (from the left (GL) or from the right (GR)) when they listen, the agents also hear creaks (from the left (CL), from the right (CR), or no creaks (S)), which noisily indicate the other agents opening one of the doors.",
                "When any door is opened, the tiger persists in its original location with a probability of 95%.",
                "Agent i hears growls with a reliability of 65% and creaks with a reliability of 95%.",
                "Agent j, on the other hand, hears growls with a reliability of 95%.",
                "Thus, the setting is such that agent i hears agent j opening doors more reliably than the tigers growls.",
                "This suggests that i could use js actions as an indication of the location of the tiger, as we discuss below.",
                "Each agents preferences are as in the single agent game discussed in [13].",
                "The transition, observation, and reward functions are shown in [16].",
                "A good indicator of the usefulness of normative methods for decision-making like I-DIDs is the emergence of realistic social behaviors in their prescriptions.",
                "In settings of the persistent multiagent tiger problem that reflect real world situations, we demonstrate followership between the agents and, as shown in [15], deception among agents who believe that they are in a follower-leader type of relationship.",
                "In particular, we analyze the situational and epistemological conditions sufficient for their emergence.",
                "The followership behavior, for example, results from the agent knowing its own weaknesses, assessing the strengths, preferences, and possible behaviors of the other, and realizing that its best for it to follow the others actions in order to maximize its payoffs.",
                "Let us consider a particular setting of the tiger problem in which agent i believes that js preferences are aligned with its own - both of them just want to get the gold - and js hearing is more reliable in comparison to itself.",
                "As an example, suppose that j, on listening can discern the tigers location 95% of the times compared to is 65% accuracy.",
                "Additionally, agent i does not have any initial information about the tigers location.",
                "In other words, is single-level nested belief, bi,1, assigns 0.5 to each of the two locations of the tiger.",
                "In addition, i considers two models of j, which differ in js flat level 0 initial beliefs.",
                "This is represented in the level 1 I-ID shown in Fig. 6(a).",
                "According to one model, j assigns a probability of 0.9 that the tiger is behind the left door, while the other 818 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Figure 6: (a) Level 1 I-ID of agent i, (b) two level 0 IDs of agent j whose decision nodes are mapped to the chance nodes, A1 j , A2 j , in (a). model assigns 0.1 to that location (see Fig. 6(b)).",
                "Agent i is undecided on these two models of j.",
                "If we vary is hearing ability, and solve the corresponding level 1 I-ID expanded over three time steps, we obtain the normative behavioral policies shown in Fig 7 that exhibit followership behavior.",
                "If is probability of correctly hearing the growls is 0.65, then as shown in the policy in Fig. 7(a), i begins to conditionally follow js actions: i opens the same door that j opened previously iff is own assessment of the tigers location confirms js pick.",
                "If i loses the ability to correctly interpret the growls completely, it blindly follows j and opens the same door that j opened previously (Fig. 7(b)).",
                "Figure 7: Emergence of (a) conditional followership, and (b) blind followership in the tiger problem.",
                "Behaviors of interest are in bold. * is a wildcard, and denotes any one of the observations.",
                "We observed that a single level of belief nesting - beliefs about the others models - was sufficient for followership to emerge in the tiger problem.",
                "However, the epistemological requirements for the emergence of leadership are more complex.",
                "For an agent, say j, to emerge as a leader, followership must first emerge in the other agent i.",
                "As we mentioned previously, if i is certain that its preferences are identical to those of j, and believes that j has a better sense of hearing, i will follow js actions over time.",
                "Agent j emerges as a leader if it believes that i will follow it, which implies that js belief must be nested two levels deep to enable it to recognize its leadership role.",
                "Realizing that i will follow presents j with an opportunity to influence is actions in the benefit of the collective good or its self-interest alone.",
                "For example, in the tiger problem, let us consider a setting in which if both i and j open the correct door, then each gets a payoff of 20 that is double the original.",
                "If j alone selects the correct door, it gets the payoff of 10.",
                "On the other hand, if both agents pick the wrong door, their penalties are cut in half.",
                "In this setting, it is in both js best interest as well as the collective betterment for j to use its expertise in selecting the correct door, and thus be a good leader.",
                "However, consider a slightly different problem in which j gains from is loss and is penalized if i gains.",
                "Specifically, let is payoff be subtracted from js, indicating that j is antagonistic toward i - if j picks the correct door and i the wrong one, then is loss of 100 becomes js gain.",
                "Agent j believes that i incorrectly thinks that js preferences are those that promote the collective good and that it starts off by believing with 99% confidence where the tiger is.",
                "Because i believes that its preferences are similar to those of j, and that j starts by believing almost surely that one of the two is the correct location (two level 0 models of j), i will start by following js actions.",
                "We show is normative policy on solving its singly-nested I-DID over three time steps in Fig. 8(a).",
                "The policy demonstrates that i will blindly follow js actions.",
                "Since the tiger persists in its original location with a probability of 0.95, i will select the same door again.",
                "If j begins the game with a 99% probability that the tiger is on the right, solving js I-DID nested two levels deep, results in the policy shown in Fig. 8(b).",
                "Even though j is almost certain that OL is the correct action, it will start by selecting OR, followed by OL.",
                "Agent js intention is to deceive i who, it believes, will follow js actions, so as to gain $110 in the second time step, which is more than what j would gain if it were to be honest.",
                "Figure 8: Emergence of deception between agents in the tiger problem.",
                "Behaviors of interest are in bold. * denotes as before. (a) Agent is policy demonstrating that it will blindly follow js actions. (b) Even though j is almost certain that the tiger is on the right, it will start by selecting OR, followed by OL, in order to deceive i. 5.2 Altruism and Reciprocity in the Public Good Problem The public good (PG) problem [7], consists of a group of M agents, each of whom must either contribute some resource to a public pot or keep it for themselves.",
                "Since resources contributed to the public pot are shared among all the agents, they are less valuable to the agent when in the public pot.",
                "However, if all agents choose to contribute their resources, then the payoff to each agent is more than if no one contributes.",
                "Since an agent gets its share of the public pot irrespective of whether it has contributed or not, the dominating action is for each agent to not contribute, and instead free ride on others contributions.",
                "However, behaviors of human players in empirical simulations of the PG problem differ from the normative predictions.",
                "The experiments reveal that many players initially contribute a large amount to the public pot, and continue to contribute when the PG problem is played repeatedly, though in decreasing amounts [4].",
                "Many of these experiments [5] report that a small core group of players persistently contributes to the public pot even when all others are defecting.",
                "These experiments also reveal that players who persistently contribute have altruistic or reciprocal preferences matching expected cooperation of others.",
                "For simplicity, we assume that the game is played between M = 2 agents, i and j.",
                "Let each agent be initially endowed with XT amount of resources.",
                "While the classical PG game formulation permits each agent to contribute any quantity of resources (≤ XT ) to the public pot, we simplify the action space by allowing two possible actions.",
                "Each agent may choose to either contribute (C) a fixed amount of the resources, or not contribute.",
                "The latter action is deThe Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 819 noted as defect (D).",
                "We assume that the actions are not observable to others.",
                "The value of resources in the public pot is discounted by ci for each agent i, where ci is the marginal private return.",
                "We assume that ci < 1 so that the agent does not benefit enough that it contributes to the public pot for private gain.",
                "Simultaneously, ciM > 1, making collective contribution pareto optimal. i/j C D C 2ciXT , 2cjXT ciXT − cp, XT + cjXT − P D XT + ciXT − P, cjXT − cp XT , XT Table 1: The one-shot PG game with punishment.",
                "In order to encourage contributions, the contributing agents punish free riders but incur a small cost for administering the punishment.",
                "Let P be the punishment meted out to the defecting agent and cp the non-zero cost of punishing for the contributing agent.",
                "For simplicity, we assume that the cost of punishing is same for both the agents.",
                "The one-shot PG game with punishment is shown in Table. 1.",
                "Let ci = cj, cp > 0, and if P > XT − ciXT , then defection is no longer a dominating action.",
                "If P < XT − ciXT , then defection is the dominating action for both.",
                "If P = XT − ciXT , then the game is not dominance-solvable.",
                "Figure 9: (a) Level 1 I-ID of agent i, (b) level 0 IDs of agent j with decision nodes mapped to the chance nodes, A1 j and A2 j , in (a).",
                "We formulate a sequential version of the PG problem with punishment from the perspective of agent i.",
                "Though in the repeated PG game, the quantity in the public pot is revealed to all the agents after each round of actions, we assume in our formulation that it is hidden from the agents.",
                "Each agent may contribute a fixed amount, xc, or defect.",
                "An agent on performing an action receives an observation of plenty (PY) or meager (MR) symbolizing the state of the public pot.",
                "Notice that the observations are also indirectly indicative of agent js actions because the state of the public pot is influenced by them.",
                "The amount of resources in agent is private pot, is perfectly observable to i.",
                "The payoffs are analogous to Table. 1.",
                "Borrowing from the empirical investigations of the PG problem [5], we construct level 0 IDs for j that model altruistic and non-altruistic types (Fig. 9(b)).",
                "Specifically, our altruistic agent has a high marginal private return (cj is close to 1) and does not punish others who defect.",
                "Let xc = 1 and the level 0 agent be punished half the times it defects.",
                "With one action remaining, both types of agents choose to contribute to avoid being punished.",
                "With two actions to go, the altruistic type chooses to contribute, while the other defects.",
                "This is because cj for the altruistic type is close to 1, thus the expected punishment, 0.5P > (1 − cj), which the altruistic type avoids.",
                "Because cj for the non-altruistic type is less, it prefers not to contribute.",
                "With three steps to go, the altruistic agent contributes to avoid punishment (0.5P > 2(1 − cj)), and the non-altruistic type defects.",
                "For greater than three steps, while the altruistic agent continues to contribute to the public pot depending on how close its marginal private return is to 1, the non-altruistic type prescribes defection.",
                "We analyzed the decisions of an altruistic agent i modeled using a level 1 I-DID expanded over 3 time steps. i ascribes the two level 0 models, mentioned previously, to j (see Fig. 9).",
                "If i believes with a probability 1 that j is altruistic, i chooses to contribute for each of the three steps.",
                "This behavior persists when i is unaware of whether j is altruistic (Fig. 10(a)), and when i assigns a high probability to j being the non-altruistic type.",
                "However, when i believes with a probability 1 that j is non-altruistic and will thus surely defect, i chooses to defect to avoid being punished and because its marginal private return is less than 1.",
                "These results demonstrate that the behavior of our altruistic type resembles that found experimentally.",
                "The non-altruistic level 1 agent chooses to defect regardless of how likely it believes the other agent to be altruistic.",
                "We analyzed the behavior of a reciprocal agent type that matches expected cooperation or defection.",
                "The reciprocal types marginal private return is similar to that of the non-altruistic type, however, it obtains a greater payoff when its action is similar to that of the other.",
                "We consider the case when the reciprocal agent i is unsure of whether j is altruistic and believes that the public pot is likely to be half full.",
                "For this prior belief, i chooses to defect.",
                "On receiving an observation of plenty, i decides to contribute, while an observation of meager makes it defect (Fig. 10(b)).",
                "This is because an observation of plenty signals that the pot is likely to be greater than half full, which results from js action to contribute.",
                "Thus, among the two models ascribed to j, its type is likely to be altruistic making it likely that j will contribute again in the next time step.",
                "Agent i therefore chooses to contribute to reciprocate js action.",
                "An analogous reasoning leads i to defect when it observes a meager pot.",
                "With one action to go, i believing that j contributes, will choose to contribute too to avoid punishment regardless of its observations.",
                "Figure 10: (a) An altruistic level 1 agent always contributes. (b) A reciprocal agent i starts off by defecting followed by choosing to contribute or defect based on its observation of plenty (indicating that j is likely altruistic) or meager (j is non-altruistic). 5.3 Strategies in Two-Player Poker Poker is a popular zero sum card game that has received much attention among the AI research community as a testbed [2].",
                "Poker is played among M ≥ 2 players in which each player receives a hand of cards from a deck.",
                "While several flavors of Poker with varying complexity exist, we consider a simple version in which each player has three plys during which the player may either exchange a card (E), keep the existing hand (K), fold (F) and withdraw from the game, or call (C), requiring all players to show their hands.",
                "To keep matters simple, let M = 2, and each player receive a hand consisting of a single card drawn from the same suit.",
                "Thus, during a showdown, the player who has the numerically larger card (2 is the lowest, ace is the highest) wins the pot.",
                "During an exchange of cards, the discarded card is placed either in the L pile, indicating to the other agent that it was a low numbered card less than 8, or in the 820 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) H pile, indicating that the card had a rank greater than or equal to 8.",
                "Notice that, for example, if a lower numbered card is discarded, the probability of receiving a low card in exchange is now reduced.",
                "We show the level 1 I-ID for the simplified two-player Poker in Fig. 11.",
                "We considered two models (personality types) of agent j.",
                "The conservative type believes that it is likely that its opponent has a high numbered card in its hand.",
                "On the other hand, the aggressive agent j believes with a high probability that its opponent has a lower numbered card.",
                "Thus, the two types differ in their beliefs over their opponents hand.",
                "In both these level 0 models, the opponent is assumed to perform its actions following a fixed, uniform distribution.",
                "With three actions to go, regardless of its hand (unless it is an ace), the aggressive agent chooses to exchange its card, with the intent of improving on its current hand.",
                "This is because it believes the other to have a low card, which improves its chances of getting a high card during the exchange.",
                "The conservative agent chooses to keep its card, no matter its hand because its chances of getting a high card are slim as it believes that its opponent has one.",
                "Figure 11: (a) Level 1 I-ID of agent i.",
                "The observation reveals information about js hand of the previous time step, (b) level 0 IDs of agent j whose decision nodes are mapped to the chance nodes, A1 j , A2 j , in (a).",
                "The policy of a level 1 agent i who believes that each card except its own has an equal likelihood of being in js hand (neutral personality type) and j could be either an aggressive or conservative type, is shown in Fig. 12. is own hand contains the card numbered 8.",
                "The agent starts by keeping its card.",
                "On seeing that j did not exchange a card (N), i believes with probability 1 that j is conservative and hence will keep its cards. i responds by either keeping its card or exchanging it because j is equally likely to have a lower or higher card.",
                "If i observes that j discarded its card into the L or H pile, i believes that j is aggressive.",
                "On observing L, i realizes that j had a low card, and is likely to have a high card after its exchange.",
                "Because the probability of receiving a low card is high now, i chooses to keep its card.",
                "On observing H, believing that the probability of receiving a high numbered card is high, i chooses to exchange its card.",
                "In the final step, i chooses to call regardless of its observation history because its belief that j has a higher card is not sufficiently high to conclude that its better to fold and relinquish the payoff.",
                "This is partly due to the fact that an observation of, say, L resets the agent is previous time step beliefs over js hand to the low numbered cards only. 6.",
                "DISCUSSION We showed how DIDs may be extended to I-DIDs that enable online sequential decision-making in uncertain multiagent settings.",
                "Our graphical representation of I-DIDs improves on the previous Figure 12: A level 1 agent is three step policy in the Poker problem. i starts by believing that j is equally likely to be aggressive or conservative and could have any card in its hand with equal probability. work significantly by being more transparent, semantically clear, and capable of being solved using standard algorithms that target DIDs.",
                "I-DIDs extend NIDs to allow sequential decision-making over multiple time steps in the presence of other interacting agents.",
                "I-DIDs may be seen as concise graphical representations for IPOMDPs providing a way to exploit problem structure and carry out online decision-making as the agent acts and observes given its prior beliefs.",
                "We are currently investigating ways to solve I-DIDs approximately with provable bounds on the solution quality.",
                "Acknowledgment: We thank Piotr Gmytrasiewicz for some useful discussions related to this work.",
                "The first author would like to acknowledge the support of a UGARF grant. 7.",
                "REFERENCES [1] R. J. Aumann.",
                "Interactive epistemology i: Knowledge.",
                "International Journal of Game Theory, 28:263-300, 1999. [2] D. Billings, A. Davidson, J. Schaeffer, and D. Szafron.",
                "The challenge of poker.",
                "AIJ, 2001. [3] A. Brandenburger and E. Dekel.",
                "Hierarchies of beliefs and common knowledge.",
                "Journal of Economic Theory, 59:189-198, 1993. [4] C. Camerer.",
                "Behavioral Game Theory: Experiments in Strategic Interaction.",
                "Princeton University Press, 2003. [5] E. Fehr and S. Gachter.",
                "Cooperation and punishment in public goods experiments.",
                "American Economic Review, 90(4):980-994, 2000. [6] D. Fudenberg and D. K. Levine.",
                "The Theory of Learning in Games.",
                "MIT Press, 1998. [7] D. Fudenberg and J. Tirole.",
                "Game Theory.",
                "MIT Press, 1991. [8] Y. Gal and A. Pfeffer.",
                "A language for modeling agents decision-making processes in games.",
                "In AAMAS, 2003. [9] P. Gmytrasiewicz and P. Doshi.",
                "A framework for sequential planning in multiagent settings.",
                "JAIR, 24:49-79, 2005. [10] P. Gmytrasiewicz and E. Durfee.",
                "Rational coordination in multi-agent environments.",
                "JAAMAS, 3(4):319-350, 2000. [11] J. C. Harsanyi.",
                "Games with incomplete information played by bayesian players.",
                "Management Science, 14(3):159-182, 1967. [12] R. A. Howard and J. E. Matheson.",
                "Influence diagrams.",
                "In R. A. Howard and J. E. Matheson, editors, The Principles and Applications of Decision Analysis.",
                "Strategic Decisions Group, Menlo Park, CA 94025, 1984. [13] L. Kaelbling, M. Littman, and A. Cassandra.",
                "Planning and acting in partially observable stochastic domains.",
                "Artificial Intelligence Journal, 2, 1998. [14] D. Koller and B. Milch.",
                "Multi-agent influence diagrams for representing and solving games.",
                "In IJCAI, pages 1027-1034, 2001. [15] K. Polich and P. Gmytrasiewicz.",
                "Interactive dynamic influence diagrams.",
                "In GTDT Workshop, AAMAS, 2006. [16] B.",
                "Rathnas., P. Doshi, and P. J. Gmytrasiewicz.",
                "Exact solutions to interactive pomdps using behavioral equivalence.",
                "In Autonomous Agents and Multi-Agent Systems Conference (AAMAS), 2006. [17] S. Russell and P. Norvig.",
                "Artificial Intelligence: A Modern Approach (Second Edition).",
                "Prentice Hall, 2003. [18] R. D. Shachter.",
                "Evaluating influence diagrams.",
                "Operations Research, 34(6):871-882, 1986. [19] D. Suryadi and P. Gmytrasiewicz.",
                "Learning models of other agents using influence diagrams.",
                "In UM, 1999.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 821"
            ],
            "original_annotated_samples": [
                "These graphical models called <br>interactive dynamic influence diagram</br>s (I-DIDs) seek to explicitly model the structure that is often present in real-world problems by decomposing the situation into chance and decision variables, and the dependencies between the variables.",
                "In [15], Polich and Gmytrasiewicz introduced <br>interactive dynamic influence diagram</br>s (I-DIDs) as the computational representations of I-POMDPs."
            ],
            "translated_annotated_samples": [
                "Estos modelos gráficos llamados <br>diagramas de influencia dinámica interactiva</br> (I-DIDs) buscan modelar explícitamente la estructura que a menudo está presente en problemas del mundo real al descomponer la situación en variables de probabilidad y decisión, y las dependencias entre las variables.",
                "En [15], Polich y Gmytrasiewicz introdujeron <br>diagramas de influencia dinámica interactivos</br> (I-DIDs) como las representaciones computacionales de I-POMDPs."
            ],
            "translated_text": "Modelos gráficos para soluciones en línea a POMDP interactivos de Prashant Doshi Dept. Departamento de Ciencias de la Computación Universidad de Georgia Athens, GA 30602, EE. UU. pdoshi@cs.uga.edu Yifeng Zeng Dept. de Ciencias de la Computación Universidad de Aalborg DK-9220 Aalborg, Dinamarca yfzeng@cs.aau.edu Qiongyu Chen Dept. de Ciencias de la Computación Universidad Nacional de Singapur 117543, Singapur chenqy@comp.nus.edu.sg RESUMEN Desarrollamos una nueva representación gráfica para procesos de decisión de Markov parcialmente observables interactivos (I-POMDPs) que es significativamente más transparente y semánticamente clara que la representación anterior. Estos modelos gráficos llamados <br>diagramas de influencia dinámica interactiva</br> (I-DIDs) buscan modelar explícitamente la estructura que a menudo está presente en problemas del mundo real al descomponer la situación en variables de probabilidad y decisión, y las dependencias entre las variables. Los I-DIDs generalizan los DIDs, los cuales pueden ser vistos como representaciones gráficas de POMDPs, a entornos multiagentes de la misma manera que los I-POMDPs generalizan los POMDPs. Los I-DIDs pueden ser utilizados para calcular la política de un agente en línea mientras el agente actúa y observa en un entorno poblado por otros agentes que interactúan entre sí. Usando varios ejemplos, mostramos cómo se pueden aplicar los I-DIDs y demostramos su utilidad. Categorías y Descriptores de Asignaturas I.2.11 [Inteligencia Artificial Distribuida]: Sistemas Multiagentes Términos Generales Teoría 1. Las Procesos de Decisión de Markov Interactivos Parcialmente Observables (IPOMDPs) proporcionan un marco para la toma de decisiones secuenciales en entornos multiagentes parcialmente observables. Generalizan los POMDPs [13] a entornos multiagentes al incluir los modelos computables de los otros agentes en el espacio de estados junto con los estados del entorno físico. Los modelos abarcan toda la información que influye en los comportamientos de los agentes, incluyendo sus preferencias, capacidades y creencias, y por lo tanto son análogos a los tipos en los juegos bayesianos [11]. I-POMDPs adoptan un enfoque subjetivo para comprender el comportamiento estratégico, arraigado en un marco de teoría de decisiones que toma la perspectiva de los tomadores de decisiones en la interacción. En [15], Polich y Gmytrasiewicz introdujeron <br>diagramas de influencia dinámica interactivos</br> (I-DIDs) como las representaciones computacionales de I-POMDPs. Los I-DIDs generalizan los DIDs [12], los cuales pueden ser vistos como contrapartes computacionales de POMDPs, a entornos de múltiples agentes de la misma manera que los I-POMDPs generalizan los POMDPs. Los I-DIDs contribuyen a una creciente línea de trabajo [19] que incluye diagramas de influencia multiagente (MAIDs) [14], y más recientemente, redes de diagramas de influencia (NIDs) [8]. Estos formalismos buscan modelar explícitamente la estructura que a menudo está presente en problemas del mundo real al descomponer la situación en variables de azar y decisiones, y las dependencias entre las variables. Los MAIDs proporcionan una alternativa a las formas normales y extensivas de juego utilizando un formalismo gráfico para representar juegos de información imperfecta con un nodo de decisión para las acciones de cada agente y nodos de azar que capturan la información privada de los agentes. Los MAIDs analizan objetivamente el juego, calculando eficientemente el perfil de equilibrio de Nash al explotar la estructura de independencia. NIDs extienden los MAIDs para incluir la incertidumbre de los agentes sobre el juego que se está jugando y sobre los modelos de los otros agentes. Cada modelo es un MAID y la red de MAIDs se colapsa, de abajo hacia arriba, en un solo MAID para calcular el equilibrio del juego teniendo en cuenta los diferentes modelos de cada agente. Los formalismos gráficos como MAIDs y NIDs abren una área de investigación prometedora que tiene como objetivo representar las interacciones multiagente de manera más transparente. Sin embargo, los MAIDs proporcionan un análisis del juego desde un punto de vista externo y la aplicabilidad de ambos está limitada a juegos estáticos de un solo jugador. Los asuntos son más complejos cuando consideramos interacciones que se extienden en el tiempo, donde las predicciones sobre las acciones futuras de otros deben hacerse utilizando modelos que cambian a medida que los agentes actúan y observan. Los I-DIDs abordan esta brecha al permitir la representación de modelos de otros agentes como los valores de un nodo de modelo especial. Tanto los modelos de otros agentes como las creencias originales de los agentes sobre estos modelos se actualizan con el tiempo utilizando implementaciones especializadas. En este documento, mejoramos la representación preliminar previa del I-DID mostrada en [15] utilizando la idea de que el I-ID estático es un tipo de NID. Por lo tanto, podemos utilizar construcciones de lenguaje específicas de NID, como multiplexores, para representar de manera más transparente el nodo del modelo y, posteriormente, el I-ID. Además, aclaramos la semántica del enlace de política de propósito especial introducido en la representación de I-DID por [15], y mostramos que podría ser reemplazado por enlaces de dependencia tradicionales. En la representación previa del I-DID, la actualización de la creencia de los agentes sobre los modelos de los demás a medida que los agentes actúan y reciben observaciones se denotaba utilizando un enlace especial llamado el enlace de actualización del modelo que conectaba los nodos del modelo a lo largo del tiempo. Explicamos la semántica de este enlace mostrando cómo puede ser implementado utilizando los enlaces de dependencia tradicionales entre los nodos de probabilidad que constituyen los nodos del modelo. El resultado final es una representación de I-DID que es significativamente más transparente, semánticamente clara y capaz de ser implementada utilizando los algoritmos estándar para resolver DIDs. Mostramos cómo los IDIDs pueden ser utilizados para modelar la incertidumbre de un agente sobre los modelos de otros, que a su vez pueden ser I-DIDs. La solución al I-DID es una política que prescribe lo que el agente debe hacer con el tiempo, dadas sus creencias sobre el estado físico y otros modelos. Análogos a los DIDs, los I-DIDs pueden ser utilizados para calcular la política de un agente en línea mientras el agente actúa y observa en un entorno poblado por otros agentes interactuantes. 2. ANTECEDENTES: Los IPOMDPS ANIDADOS FINITAMENTE generalizan los POMDPs a entornos multiagentes al incluir los modelos de otros agentes como parte del espacio de estados [9]. Dado que otros agentes también pueden razonar sobre otros, el espacio de estado interactivo está estratégicamente anidado; contiene creencias sobre los modelos de otros agentes y sus creencias sobre los demás. Para simplificar la presentación, consideramos un agente, i, que está interactuando con otro agente, j. Un I-POMDP finitamente anidado del agente i con un nivel de estrategia l se define como la tupla: I-POMDPi,l = ISi,l, A, Ti, Ωi, Oi, Ri donde: • ISi,l denota un conjunto de estados interactivos definido como, ISi,l = S × Mj,l−1, donde Mj,l−1 = {Θj,l−1 ∪ SMj}, para l ≥ 1, e ISi,0 = S, donde S es el conjunto de estados del entorno físico. Θj,l−1 es el conjunto de modelos intencionales computables del agente j: θj,l−1 = bj,l−1, ˆθj donde el marco, ˆθj = A, Ωj, Tj, Oj, Rj, OCj. Aquí, j es Bayesiano racional y OCj es el criterio de optimalidad de j. SMj es el conjunto de modelos subintencionales de j. Ejemplos simples de modelos subintencionales incluyen un modelo sin información [10] y un modelo de juego ficticio [6], ambos de los cuales son independientes de la historia. Damos una construcción recursiva de abajo hacia arriba del espacio de estados interactivo a continuación. Sí,0 = S, Θj,0 = { bj,0, ˆθj | bj,0 ∈ Δ(ISj,0)} Sí,1 = S × {Θj,0 ∪ SMj}, Θj,1 = { bj,1, ˆθj | bj,1 ∈ Δ(ISj,1)} . . . . . . Sí, l = S × {Θj,l−1 ∪ SMj}, Θj,l = { bj,l, ˆθj | bj,l ∈ Δ(ISj,l)} Formulaciones similares de espacios anidados han aparecido en [1, 3]. • A = Ai × Aj es el conjunto de acciones conjuntas de todos los agentes en el entorno; • Ti : S ×A×S → [0, 1], describe el efecto de las acciones conjuntas en los estados físicos del entorno; • Ωi es el conjunto de observaciones del agente i; • Oi : S × A × Ωi → [0, 1] da la probabilidad de las observaciones dadas el estado físico y la acción conjunta; • Ri : ISi × A → R describe las preferencias del agente sobre sus estados interactivos. Normalmente solo importarán los estados físicos. La política del agente es el mapeo, Ω∗ i → Δ(Ai), donde Ω∗ i es el conjunto de todos los historiales de observación del agente i. Dado que la creencia sobre los estados interactivos forma una estadística suficiente [9], la política también puede ser representada como un mapeo del conjunto de todas las creencias del agente i a una distribución sobre sus acciones, Δ(ISi) → Δ(Ai). 2.1 Actualización de Creencias Análogo a los POMDPs, un agente dentro del marco I-POMDP actualiza su creencia a medida que actúa y observa. Sin embargo, hay dos diferencias que complican la actualización de creencias en entornos multiagentes en comparación con los entornos de un solo agente. Primero, dado que el estado del entorno físico depende de las acciones de ambos agentes, la predicción de cómo cambia el estado físico debe hacerse basándose en la predicción de sus acciones. Segundo, los cambios en los modelos de js deben ser incluidos en la actualización de creencias. Específicamente, si j es intencional, entonces se debe incluir una actualización de las creencias de j debido a su acción y observación. En otras palabras, i tiene que actualizar su creencia basándose en su predicción de lo que j observaría y cómo j actualizaría su creencia. Si el modelo de js es subintencional, entonces las observaciones probables de js se añaden al historial de observaciones contenido en el modelo. Formalmente, tenemos: Pr(ist |at−1 i , bt−1 i,l ) = β ISt−1:mt−1 j =θt j bt−1 i,l (ist−1 ) × at−1 j Pr(at−1 j |θt−1 j,l−1)Oi(st , at−1 i , at−1 j , ot i) ×Ti(st−1 , at−1 i , at−1 j , st ) ot j Oj(st , at−1 i , at−1 j , ot j) ×τ(SEθt j (bt−1 j,l−1, at−1 j , ot j) − bt j,l−1) (1) donde β es la constante de normalización, τ es 1 si su argumento es 0, de lo contrario es 0, Pr(at−1 j |θt−1 j,l−1) es la probabilidad de que at−1 j sea Bayes racional para el agente descrito por el modelo θt−1 j,l−1, y SE(·) es una abreviatura para la actualización de creencias. Para una versión de la actualización de creencias cuando el modelo js es subintencional, consulte [9]. Si el agente j también se modela como un I-POMDP, entonces la actualización de creencias invoca la actualización de creencias de j (a través del término SEθt j ( bt−1 j,l−1 , at−1 j , ot j)), lo que a su vez podría invocar la actualización de creencias de is y así sucesivamente. Esta recursión en la anidación de creencias llega al nivel 0. En este nivel, la actualización de creencias del agente se reduce a una actualización de creencias de un POMDP. Para ilustraciones de la actualización de creencias, detalles adicionales sobre I-POMDPs y cómo se comparan con otros marcos multiagentes, consulte [9]. Iteración de Valor Cada estado de creencia en un I-POMDP finitamente anidado tiene un valor asociado que refleja la máxima ganancia que el agente puede esperar en este estado de creencia: Un ( bi,l, θi ) = max ai∈Ai is∈ISi,l ERi(is, ai)bi,l(is)+ γ oi∈Ωi Pr(oi|ai, bi,l)Un−1 ( SEθi (bi,l, ai, oi), θi ) (2) donde, ERi(is, ai) = aj Ri(is, ai, aj)Pr(aj|mj,l−1) (ya que is = (s, mj,l−1)). La ecuación 2 es una base para la iteración de valor en I-POMDPs. La acción óptima del agente, a∗ i, para el caso de horizonte finito con descuento, es un elemento del conjunto de acciones óptimas para el estado de creencia, OPT(θi), definido como: OPT( bi,l, θi ) = argmax ai∈Ai is∈ISi,l ERi(is, ai)bi,l(is) +γ oi∈Ωi Pr(oi|ai, bi,l)Un ( SEθi (bi,l, ai, oi), θi ) (3) 3. Diagramas de influencia interactivos. Una extensión ingenua de los diagramas de influencia (IDs) a entornos poblados por múltiples agentes es posible tratando a los otros agentes como autómatas, representados mediante nodos de probabilidad. Sin embargo, este enfoque asume que las acciones de los agentes están controladas utilizando una distribución de probabilidad que no cambia con el tiempo. Los diagramas de influencia interactivos (I-IDs) adoptan un enfoque más sofisticado al generalizar los IDs para hacerlos aplicables a entornos compartidos con otros agentes que pueden actuar, observar y actualizar sus creencias. 3.1 Sintaxis Además de los nodos habituales de probabilidad, decisión y utilidad, los IIDs incluyen un nuevo tipo de nodo llamado nodo de modelo. Mostramos un nivel general l I-ID en la Fig. 1(a), donde el nodo del modelo (Mj,l−1) está representado con un hexágono. Observamos que la distribución de probabilidad sobre el nodo de probabilidad, S, y el nodo del modelo juntos representan la creencia del agente sobre sus estados interactivos. Además del modelo 1, el modelo de nivel 0 es un POMDP: las acciones de otros agentes se tratan como eventos exógenos y se incorporan en las funciones T, O y R. El Sexto Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) Figura 1: (a) Un nivel genérico l I-ID para el agente i situado con otro agente j. El hexágono es el nodo modelo (Mj,l−1) cuya estructura mostramos en (b). Los miembros del nodo modelo son ellos mismos I-ID (m1 j,l−1, m2 j,l−1; los diagramas no se muestran aquí por simplicidad) cuyos nodos de decisión se asignan a los nodos de probabilidad correspondientes (A1 j , A2 j). Dependiendo del valor del nodo, Mod[Mj], la distribución de cada uno de los nodos de probabilidad se asigna al nodo Aj. (c) El I-ID transformado con el nodo del modelo reemplazado por los nodos de probabilidad y las relaciones entre ellos. Los I-IDs difieren de los IDs al tener un enlace discontinuo (llamado enlace de política en [15]) entre el nodo del modelo y un nodo de probabilidad, Aj, que representa la distribución sobre las acciones de los otros agentes dada su modelo. En ausencia de otros agentes, el nodo del modelo y el nodo de probabilidad, Aj, desaparecen y los I-ID se convierten en ID tradicionales. El nodo del modelo contiene los modelos computacionales alternativos atribuidos por i al otro agente del conjunto, Θj,l−1 ∪ SMj, donde Θj,l−1 y SMj fueron definidos previamente en la Sección 2. Por lo tanto, un modelo en el nodo del modelo puede ser en sí mismo un I-ID o ID, y la recursión termina cuando un modelo es un ID o subintencional. Dado que el nodo del modelo contiene los modelos alternativos del otro agente como sus valores, su representación no es trivial. En particular, algunos de los modelos dentro del nodo son I-IDs que, al resolverse, generan la política óptima de los agentes en sus nodos de decisión. Cada nodo de decisión se asigna al nodo de probabilidad correspondiente, digamos A1 j, de la siguiente manera: si OPT es el conjunto de acciones óptimas obtenidas al resolver el I-ID (o ID), entonces Pr(aj ∈ A1 j) = 1 |OPT| si aj ∈ OPT, 0 en caso contrario. Tomando prestados los conocimientos de trabajos anteriores [8], observamos que el nodo del modelo y el enlace de política discontinua que lo conecta con el nodo de probabilidad, Aj, podrían representarse como se muestra en la Fig. 1(b). El nodo de decisión de cada nivel l − 1 I-ID se transforma en un nodo de probabilidad, como mencionamos anteriormente, de modo que las acciones con el mayor valor en el nodo de decisión se les asignan probabilidades uniformes en el nodo de probabilidad, mientras que al resto se les asigna probabilidad cero. Los diferentes nodos de probabilidad (A1 j , A2 j ), uno para cada modelo, y adicionalmente, el nodo de probabilidad etiquetado Mod[Mj] forman los padres del nodo de probabilidad, Aj. Por lo tanto, hay tantos nodos de acción (A1 j , A2 j ) en Mj,l−1 como el número de modelos en el soporte de las creencias del agente. La tabla de probabilidad condicional del nodo de probabilidad, Aj, es un multiplexor que asume la distribución de cada uno de los nodos de acción (A1 j , A2 j ) dependiendo del valor de Mod[Mj]. Los valores de Mod[Mj] denotan los diferentes modelos de j. En otras palabras, cuando Mod[Mj] tiene el valor m1 j,l−1, el nodo de probabilidad Aj asume la distribución del nodo A1 j, y Aj asume la distribución de A2 j cuando Mod[Mj] tiene el valor m2 j,l−1. La distribución sobre el nodo, Mod[Mj], es la creencia del agente sobre los modelos de j dado un estado físico. Para más agentes, tendremos tantos nodos de modelo como agentes haya. Observa que la Fig. 1(b) aclara la semántica del enlace de política y muestra cómo puede ser representado utilizando los enlaces de dependencia tradicionales. En la Fig. 1(c), mostramos el I-ID transformado cuando el nodo del modelo es reemplazado por los nodos de probabilidad y las relaciones entre ellos. A diferencia de la representación en [15], no hay enlaces de políticas especiales, sino que el I-ID está compuesto únicamente por aquellos tipos de nodos que se encuentran en IDs tradicionales y relaciones de dependencia entre los nodos. Esto permite que los I-IDs sean representados e implementados utilizando herramientas de aplicación convencionales que apuntan a los IDs. Ten en cuenta que podemos ver el nivel l I-ID como un NID. Específicamente, cada uno de los modelos del nivel l − 1 dentro del nodo del modelo son bloques en el NID (ver Fig. 2). Si el nivel l = 1, cada bloque es un ID tradicional, de lo contrario, si l > 1, cada bloque dentro del NID puede ser un NID en sí mismo. Ten en cuenta que dentro de los I-IDs (o IDs) en cada nivel, solo hay un nodo de decisión único. Por lo tanto, nuestro NID no contiene ningún MAID. Figura 2: Un nivel l I-ID representado como un NID. Las probabilidades asignadas a los bloques del NID son creencias sobre modelos js condicionados a un estado físico. 3.2 Solución La solución de un I-ID procede de manera ascendente y se implementa de forma recursiva. Comenzamos resolviendo los modelos de nivel 0, que, si son intencionales, son identificadores tradicionales. Sus soluciones proporcionan distribuciones de probabilidad sobre las acciones de los otros agentes, las cuales se ingresan en los nodos de probabilidad correspondientes encontrados en el nodo del modelo del nivel 1 I-ID. El mapeo de los nodos de decisión de los modelos de nivel 0 a los nodos de probabilidad se realiza de manera que las acciones con el mayor valor en el nodo de decisión se les asignan probabilidades uniformes en el nodo de probabilidad, mientras que el resto se les asigna probabilidad cero. Dadas las distribuciones sobre las acciones dentro de los diferentes nodos de probabilidad (uno para cada modelo del otro agente), el I-ID de nivel 1 se transforma como se muestra en la Fig. 1(c). Durante la transformación, la tabla de probabilidad condicional (CPT) del nodo, Aj, se llena de tal manera que el nodo asume la distribución de cada uno de los nodos de probabilidad dependiendo del valor del nodo, Mod[Mj]. Como mencionamos anteriormente, los valores del nodo Mod[Mj] denotan los diferentes modelos del otro agente, y su distribución es la creencia del agente sobre los modelos de j condicionados al estado físico. El nivel 1 I-ID transformado es un ID tradicional que puede ser resuelto como us816 The Sixth Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) (a) (b) Figura 3: (a) Un I-DID genérico de dos niveles de tiempo para el agente i en un entorno con otro agente j. Observa el enlace de actualización del modelo punteado que denota la actualización de los modelos de j y la distribución de los modelos a lo largo del tiempo. (b) La semántica del enlace de actualización del modelo utilizando el método estándar de maximización de utilidad esperada [18]. Este procedimiento se lleva a cabo hasta el nivel l I-ID cuya solución proporciona el conjunto no vacío de acciones óptimas que el agente debe realizar dada su creencia. Ten en cuenta que, al igual que los IDs, los I-IDs son adecuados para la toma de decisiones en línea cuando se conoce la creencia actual de los agentes. 4. Diagramas de influencia dinámica interactivos (I-DIDs) Los diagramas de influencia dinámica interactivos (I-DIDs) extienden los I-IDs (y NIDs) para permitir la toma de decisiones secuencial a lo largo de varios pasos de tiempo. Así como los DIDs son representaciones gráficas estructuradas de POMDPs, los I-DIDs son los análogos gráficos en línea para I-POMDPs finitamente anidados. Los I-DIDs pueden ser utilizados para optimizar sobre una mirada anticipada finita dadas las creencias iniciales mientras se interactúa con otros agentes, posiblemente similares. 4.1 Sintaxis Representamos un I-DID de dos cortes temporales en la Fig. 3(a). Además de los nodos del modelo y el enlace de política discontinuo, lo que diferencia a un I-DID de un DID es el enlace de actualización del modelo mostrado como una flecha punteada en la Fig. 3(a). Explicamos la semántica del nodo del modelo y el enlace de la política en la sección anterior; a continuación, describimos las actualizaciones del modelo. La actualización del nodo del modelo con el tiempo implica dos pasos: primero, dados los modelos en el tiempo t, identificamos el conjunto actualizado de modelos que residen en el nodo del modelo en el tiempo t + 1. Recuerda de la Sección 2 que el modelo intencional de un agente incluye sus creencias. Debido a que los agentes actúan y reciben observaciones, sus modelos se actualizan para reflejar sus creencias cambiadas. Dado que el conjunto de acciones óptimas para un modelo podría incluir todas las acciones, y el agente podría recibir cualquiera de las |Ωj| posibles observaciones, el conjunto actualizado en el paso de tiempo t + 1 tendrá como máximo |Mt j,l−1||Aj||Ωj| modelos. Aquí, |Mt j,l−1| es el número de modelos en el paso de tiempo t, |Aj| y |Ωj| son los espacios más grandes de acciones y observaciones respectivamente, entre todos los modelos. Segundo, calculamos la nueva distribución sobre los modelos actualizados dados la distribución original y la probabilidad de que el agente realice la acción y reciba la observación que llevó al modelo actualizado. Estos pasos son parte de la actualización de creencias del agente formalizada utilizando la Ecuación 1. En la Fig. 3(b), mostramos cómo se implementa el enlace de actualización del modelo punteado en el I-DID. Si cada uno de los dos modelos de nivel l − 1 atribuidos a j en el paso de tiempo t resulta en una acción, y j podría hacer una de dos posibles observaciones, entonces el nodo del modelo en el paso de tiempo t + 1 contiene cuatro modelos actualizados (mt+1,1 j,l−1 ,mt+1,2 j,l−1 , mt+1,3 j,l−1 , y mt+1,4 j,l−1 ). Estos modelos difieren en sus creencias iniciales, cada una de las cuales es el resultado de j actualizar sus creencias debido a su acción y una posible observación. Los nodos de decisión en cada uno de los I-DIDs o DIDs que representan los modelos de nivel inferior se asignan a la Figura 4 correspondiente: I-DID transformado con los nodos del modelo y el enlace de actualización del modelo reemplazados por los nodos de probabilidad y las relaciones (en negrita) de los nodos de probabilidad, como se mencionó anteriormente. A continuación, describimos cómo se calcula la distribución sobre el conjunto actualizado de modelos (la distribución sobre el nodo de probabilidad Mod[Mt+1 j ] en Mt+1 j,l−1). La probabilidad de que el modelo actualizado js sea, digamos mt+1,1 j,l−1, depende de la probabilidad de que j realice la acción y reciba la observación que condujo a este modelo, y de la distribución previa sobre los modelos en el paso de tiempo t. Dado que el nodo de probabilidad At j asume la distribución de cada uno de los nodos de acción basados en el valor de Mod[Mt j], la probabilidad de la acción está dada por este nodo de probabilidad. Para obtener la probabilidad de una observación posible js, introducimos el nodo de probabilidad Oj, que dependiendo del valor de Mod[Mt j], asume la distribución del nodo de observación en el modelo de nivel inferior denominado Mod[Mt j]. Dado que la probabilidad de las observaciones js depende del estado físico y de las acciones conjuntas de ambos agentes, el nodo Oj está vinculado con St+1, At j y At i. De manera análoga a At j, la tabla de probabilidad condicional de Oj también es un multiplexor modulado por Mod[Mt j]. Finalmente, la distribución de los modelos previos en el tiempo t se obtiene a partir del nodo de probabilidad, Mod[Mt j ] en Mt j,l−1. Por consiguiente, los nodos de probabilidad, Mod[Mt j], At j y Oj, forman los padres de Mod[Mt+1 j] en Mt+1 j,l−1. Ten en cuenta que el enlace de actualización del modelo puede ser reemplazado por los enlaces de dependencia entre los nodos de probabilidad que constituyen los nodos del modelo en las dos etapas temporales. En la Fig. 4 mostramos los dos cortes temporales I-DID con los nodos del modelo reemplazados por los nodos de probabilidad y las relaciones entre ellos. Los nodos de probabilidad y los enlaces de dependencia que no están en negrita son estándar, generalmente encontrados en DIDs. La expansión del I-DID a lo largo de más pasos de tiempo requiere la repetición de los dos pasos de actualizar el conjunto de modelos que forman el 2. Nota que Oj representa la observación j en el tiempo t + 1. El Sexto Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 817 valores del nodo del modelo y añadiendo las relaciones entre los nodos de probabilidad, tantas veces como enlaces de actualización del modelo haya. Observamos que el conjunto posible de modelos del otro agente j crece exponencialmente con el número de pasos de tiempo. Por ejemplo, después de T pasos, puede haber como máximo |Mt=1 j,l−1|(|Aj||Ωj|)T −1 modelos candidatos residiendo en el nodo del modelo. 4.2 Solución Análoga a los I-ID, la solución a un I-DID de nivel l para el agente i expandido a lo largo de T pasos de tiempo puede llevarse a cabo de forma recursiva. Con el propósito de ilustración, dejemos l=1 y T=2. El método de solución utiliza la técnica estándar de anticipación, proyectando las secuencias de acciones y observaciones de los agentes hacia adelante desde el estado de creencia actual [17], y encontrando las posibles creencias que podría tener en el siguiente paso temporal. Dado que el agente i también tiene una creencia sobre los modelos de j, la búsqueda anticipada incluye descubrir los posibles modelos que j podría tener en el futuro. Por consiguiente, cada uno de los modelos subintencionales o de nivel 0 de js (representados utilizando un DID estándar) en el primer paso de tiempo debe resolverse para obtener su conjunto óptimo de acciones. Estas acciones se combinan con el conjunto de observaciones posibles que j podría hacer en ese modelo, lo que resulta en un conjunto actualizado de modelos candidatos (que incluyen las creencias actualizadas) que podrían describir el comportamiento de j. Las creencias sobre este conjunto actualizado de modelos candidatos se calculan utilizando los métodos estándar de inferencia que utilizan las relaciones de dependencia entre los nodos del modelo, como se muestra en la Figura 3(b). Observamos la naturaleza recursiva de esta solución: al resolver el agente de nivel 1 I-DID, los DIDs de nivel 0 deben ser resueltos. Si el anidamiento de los modelos es más profundo, todos los modelos en todos los niveles, comenzando desde el nivel 0, se resuelven de manera ascendente. Breve descripción del algoritmo recursivo para resolver el agente es el Algoritmo para resolver I-DID. Entrada: nivel l ≥ 1 I-ID o nivel 0 ID, T Fase de Expansión 1. Para t desde 1 hasta T − 1, hacer 2. Si l ≥ 1, entonces llenar Mt+1 j,l−1 3. Para cada mt j en el rango (Mt j,l−1) hacer 4. Llame recursivamente al algoritmo con el l - 1 I-ID (o ID) que representa mt j y el horizonte, T - t + 1 5. Mapea el nodo de decisión del I-ID (o ID) resuelto, OPT(mt j), a un nodo de probabilidad Aj 6. Para cada aj en OPT(mt j) hacer 7. Para cada oj en Oj (parte de mt j) hacer 8. Actualizar la creencia js, bt+1 j ← SE(bt j, aj, oj) 9. mt+1 j ← Nuevo I-ID (o ID) con bt+1 j como la creencia inicial 10. Rango(Mt+1 j,l−1) ∪ ← {mt+1 j } 11. Añadir el nodo del modelo, Mt+1 j,l−1, y los enlaces de dependencia entre Mt j,l−1 y Mt+1 j,l−1 (mostrados en la Fig. 3(b)) 12. Agregue los nodos de probabilidad, decisión y utilidad para la franja de tiempo t + 1 y los enlaces de dependencia entre ellos 13. Establezca las CPTs para cada nodo de probabilidad y nodo de utilidad de la Fase de Mirada Adelante 14. Aplica el método estándar de anticipación y respaldo para resolver la Figura 5 expandida de I-DID: Algoritmo para resolver un nivel l ≥ 0 I-DID. Nivel l I-DID expandido durante T pasos de tiempo con otro agente j en la Figura 5. Adoptamos un enfoque de dos fases: Dado un I-ID de nivel l (descrito previamente en la Sección 3) con todos los modelos de niveles inferiores representados también como I-IDs o IDs (si es nivel 0), el primer paso es expandir el I-ID de nivel l a lo largo de T pasos de tiempo añadiendo los enlaces de dependencia y las tablas de probabilidad condicional para cada nodo. Nos enfocamos especialmente en establecer y poblar los nodos del modelo (líneas 3-11). Ten en cuenta que Range(·) devuelve los valores (modelos de nivel inferior) de la variable aleatoria dada como entrada (nodo del modelo). En la segunda fase, utilizamos una técnica estándar de anticipación proyectando las secuencias de acción y observación durante T pasos de tiempo en el futuro, y respaldando los valores de utilidad de las creencias alcanzables. Similar a los I-IDs, los I-DIDs se reducen a DIDs en ausencia de otros agentes. Como mencionamos anteriormente, los modelos de nivel 0 son los DIDs tradicionales. Sus soluciones proporcionan distribuciones de probabilidad sobre las acciones del agente modelado en ese nivel a los I-DIDs en el nivel 1. Dadas las distribuciones de probabilidad sobre las acciones de otros agentes, los IDIDs de nivel 1 pueden resolverse como DIDs y proporcionar distribuciones de probabilidad a modelos de niveles superiores. Suponga que el número de modelos considerados en cada nivel está limitado por un número, M. Resolver un I-DID de nivel l es equivalente a resolver O(Ml) DIDs. Para ilustrar la utilidad de los I-DIDs, los aplicamos a tres dominios de problemas. Describimos, en particular, la formulación del I-DID y las prescripciones óptimas obtenidas al resolverlo. 5.1 Seguimiento-Liderazgo en el Problema del Tigre Multiagente Comenzamos nuestras ilustraciones del uso de I-IDs e I-DIDs con una versión ligeramente modificada del problema del tigre multiagente discutido en [9]. El problema tiene dos agentes, cada uno de los cuales puede abrir la puerta derecha (OR), la puerta izquierda (OL) o escuchar (L). Además de escuchar gruñidos (desde la izquierda (GL) o desde la derecha (GR)) cuando escuchan, los agentes también escuchan chirridos (desde la izquierda (CL), desde la derecha (CR) o sin chirridos (S)), que indican ruidosamente que los otros agentes están abriendo una de las puertas. Cuando se abre cualquier puerta, el tigre persiste en su ubicación original con una probabilidad del 95%. El agente i escucha gruñidos con una fiabilidad del 65% y crujidos con una fiabilidad del 95%. El agente J, por otro lado, escucha gruñidos con una fiabilidad del 95%. Por lo tanto, la configuración es tal que el agente i escucha la apertura de puertas del agente j de manera más confiable que los rugidos de los tigres. Esto sugiere que podría usar acciones de JavaScript como indicación de la ubicación del tigre, como discutimos a continuación. Las preferencias de cada agente son como en el juego de un solo agente discutido en [13]. Las funciones de transición, observación y recompensa se muestran en [16]. Un buen indicador de la utilidad de los métodos normativos para la toma de decisiones como los I-DIDs es la aparición de comportamientos sociales realistas en sus prescripciones. En entornos del persistente problema del tigre multiagente que reflejan situaciones del mundo real, demostramos la seguidismo entre los agentes y, como se muestra en [15], el engaño entre agentes que creen que están en una relación de tipo seguidor-líder. En particular, analizamos las condiciones situacionales y epistemológicas suficientes para su surgimiento. El comportamiento de seguidores, por ejemplo, resulta del agente conociendo sus propias debilidades, evaluando las fortalezas, preferencias y posibles comportamientos del otro, y dándose cuenta de que es mejor para él seguir las acciones de los demás para maximizar sus ganancias. Consideremos una configuración particular del problema del tigre en la que el agente i cree que las preferencias de j están alineadas con las suyas: ambos solo quieren obtener el oro, y la audición de j es más confiable en comparación consigo mismo. Como ejemplo, supongamos que j, al escuchar, puede discernir la ubicación del tigre el 95% de las veces en comparación con su precisión del 65%. Además, el agente i no tiene ninguna información inicial sobre la ubicación de los tigres. En otras palabras, la creencia anidada de un solo nivel, bi,1, asigna 0.5 a cada una de las dos ubicaciones del tigre. Además, considera dos modelos de j, que difieren en los niveles iniciales de creencias planas de j. Esto se representa en el nivel 1 I-ID mostrado en la Fig. 6(a). Según un modelo, j asigna una probabilidad de 0.9 a que el tigre esté detrás de la puerta izquierda, mientras que el otro 818 The Sixth Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) Figura 6: (a) Nivel 1 I-ID del agente i, (b) dos IDs de nivel 0 del agente j cuyos nodos de decisión están mapeados a los nodos de probabilidad, A1 j , A2 j , en (a). El modelo asigna 0.1 a esa ubicación (ver Fig. 6(b)). El agente i está indeciso sobre estos dos modelos de j. Si variamos la capacidad auditiva, y resolvemos el nivel 1 correspondiente I-ID expandido en tres pasos de tiempo, obtenemos las políticas de comportamiento normativas mostradas en la Figura 7 que exhiben comportamiento de seguidores. Si la probabilidad de escuchar correctamente los gruñidos es de 0.65, entonces, como se muestra en la política en la Fig. 7(a), i comienza a seguir condicionalmente las acciones de j: i abre la misma puerta que j abrió anteriormente si su evaluación propia de la ubicación de los tigres confirma la elección de j. Si i pierde la capacidad de interpretar correctamente los gruñidos por completo, sigue ciegamente a j y abre la misma puerta que j abrió anteriormente (Fig. 7(b)). Figura 7: Emergencia de (a) seguidores condicionales, y (b) seguidores ciegos en el problema del tigre. Los comportamientos de interés están en negrita. * es un comodín y representa cualquiera de las observaciones. Observamos que un solo nivel de anidación de creencias, creencias sobre los modelos de los demás, fue suficiente para que surgiera el seguidismo en el problema del tigre. Sin embargo, los requisitos epistemológicos para el surgimiento del liderazgo son más complejos. Para que un agente, digamos j, surja como líder, primero debe surgir el seguidismo en el otro agente i. Como mencionamos anteriormente, si i está seguro de que sus preferencias son idénticas a las de j, y cree que j tiene un mejor sentido del oído, i seguirá las acciones de j con el tiempo. El agente j emerge como líder si cree que lo seguiré, lo que implica que la creencia de j debe estar anidada dos niveles de profundidad para permitirle reconocer su papel de liderazgo. Darse cuenta de que lo seguiré presenta a J una oportunidad para influir en sus acciones en beneficio del bien colectivo o de su interés propio solamente. Por ejemplo, en el problema del tigre, consideremos una situación en la que si tanto i como j abren la puerta correcta, entonces cada uno recibe un pago de 20 que es el doble del original. Si solo j selecciona la puerta correcta, obtiene la recompensa de 10. Por otro lado, si ambos agentes eligen la puerta incorrecta, sus penalizaciones se reducen a la mitad. En este entorno, es tanto en el mejor interés de j como en el beneficio colectivo que utilice su experiencia para seleccionar la puerta correcta, y así ser un buen líder. Sin embargo, considera un problema ligeramente diferente en el que j se beneficia de la pérdida de i y es penalizado si i se beneficia. Específicamente, dejemos que la ganancia se reste de js, indicando que j es antagonista hacia i: si j elige la puerta correcta y i la incorrecta, entonces la pérdida de 100 se convierte en la ganancia de js. El agente J cree que yo incorrectamente pienso que las preferencias de J son aquellas que promueven el bien colectivo y que comienza creyendo con un 99% de confianza dónde está el tigre. Dado que creo que mis preferencias son similares a las de j, y que j comienza creyendo casi con certeza que una de las dos ubicaciones es la correcta (dos modelos de nivel 0 de j), comenzaré siguiendo las acciones de j. Mostramos la política normativa para resolver su I-DID anidado individualmente durante tres pasos de tiempo en la Fig. 8(a). La política demuestra que seguiré ciegamente las acciones de js. Dado que el tigre persiste en su ubicación original con una probabilidad del 0.95, seleccionaré nuevamente la misma puerta. Si j comienza el juego con una probabilidad del 99% de que el tigre esté a la derecha, resolver su I-DID anidado dos niveles profundamente, resulta en la política mostrada en la Fig. 8(b). Aunque j está casi seguro de que OL es la acción correcta, comenzará seleccionando OR, seguido de OL. La intención del agente js es engañar a i, a quien cree que seguirá las acciones de js, para así obtener $110 en el segundo paso de tiempo, lo cual es más de lo que j ganaría si fuera honesto. Figura 8: Emergencia del engaño entre agentes en el problema del tigre. Los comportamientos de interés están en negrita. * denota como antes. (a) El agente es una política que demuestra que seguirá ciegamente las acciones de js. (b) Aunque j está casi seguro de que el tigre está a la derecha, comenzará seleccionando OR, seguido de OL, para engañar a i. 5.2 Altruismo y reciprocidad en el problema del bien público El problema del bien público (PG) [7], consiste en un grupo de M agentes, cada uno de los cuales debe contribuir con algún recurso a una olla pública o guardarlo para sí mismos. Dado que los recursos aportados al fondo público se comparten entre todos los agentes, son menos valiosos para el agente cuando están en el fondo público. Sin embargo, si todos los agentes eligen contribuir con sus recursos, entonces la recompensa para cada agente es mayor que si nadie contribuye. Dado que un agente recibe su parte del bote público independientemente de si ha contribuido o no, la acción dominante es que cada agente no contribuya y en su lugar se aproveche de las contribuciones de los demás. Sin embargo, los comportamientos de los jugadores humanos en simulaciones empíricas del problema de PG difieren de las predicciones normativas. Los experimentos revelan que muchos jugadores inicialmente contribuyen con una gran cantidad al bote público, y continúan contribuyendo cuando se juega el problema del PG repetidamente, aunque en cantidades decrecientes [4]. Muchos de estos experimentos [5] informan que un pequeño grupo central de jugadores contribuye persistentemente al bote público incluso cuando todos los demás están desertando. Estos experimentos también revelan que los jugadores que contribuyen persistentemente tienen preferencias altruistas o recíprocas que coinciden con la cooperación esperada de los demás. Para simplificar, asumimos que el juego se juega entre M = 2 agentes, i y j. Que cada agente esté inicialmente dotado con una cantidad de recursos XT. Mientras que la formulación clásica del juego PG permite que cada agente contribuya con cualquier cantidad de recursos (≤ XT) al bote público, simplificamos el espacio de acciones al permitir dos acciones posibles. Cada agente puede elegir contribuir (C) una cantidad fija de recursos, o no contribuir. La última acción es deThe Sixth Intl. La Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 819 fue señalada como defectuosa (D). Suponemos que las acciones no son observables por otros. El valor de los recursos en la bolsa pública se descuenta por ci para cada agente i, donde ci es el retorno privado marginal. Suponemos que ci < 1 para que el agente no se beneficie lo suficiente como para contribuir al bote público en beneficio propio. Simultáneamente, ciM > 1, lo que hace que la contribución colectiva sea óptima de Pareto. i/j C D C 2ciXT , 2cjXT ciXT − cp, XT + cjXT − P D XT + ciXT − P, cjXT − cp XT , XT Tabla 1: El juego PG de un solo disparo con castigo. Para fomentar las contribuciones, los agentes contribuyentes castigan a los que no colaboran pero incurren en un pequeño costo por administrar el castigo. Sea P la sanción impuesta al agente que se desvía y cp el costo no nulo de castigar al agente contribuyente. Para simplificar, asumimos que el costo de castigar es el mismo para ambos agentes. El juego PG de un solo disparo con castigo se muestra en la Tabla 1. Si ci = cj, cp > 0, y si P > XT − ciXT, entonces la deserción ya no es una acción dominante. Si P < XT − ciXT, entonces la deserción es la acción dominante para ambos. Si P = XT − ciXT, entonces el juego no es resoluble por dominancia. Figura 9: (a) Nivel 1 I-ID del agente i, (b) IDs de nivel 0 del agente j con nodos de decisión mapeados a los nodos de probabilidad, A1 j y A2 j, en (a). Formulamos una versión secuencial del problema PG con castigo desde la perspectiva del agente i. Aunque en el juego de PG repetido, la cantidad en la olla pública se revela a todos los agentes después de cada ronda de acciones, asumimos en nuestra formulación que está oculta para los agentes. Cada agente puede contribuir con una cantidad fija, xc, o desertar. Un agente al realizar una acción recibe una observación de abundante (PY) o escasa (MR) simbolizando el estado de la olla pública. Ten en cuenta que las observaciones también son indirectamente indicativas de las acciones del agente js porque el estado del bote público es influenciado por ellas. La cantidad de recursos en el agente es una olla privada, es perfectamente observable para i. Los pagos son análogos a la Tabla 1. Tomando prestado de las investigaciones empíricas del problema PG [5], construimos IDs de nivel 0 para j que modelan tipos altruistas y no altruistas (Fig. 9(b)). Específicamente, nuestro agente altruista tiene un alto retorno privado marginal (cj está cerca de 1) y no castiga a los demás que incumplen. Que xc = 1 y el agente de nivel 0 sea castigado la mitad de las veces que se desvía. Con una acción restante, ambos tipos de agentes eligen contribuir para evitar ser castigados. Con dos acciones por delante, el tipo altruista elige contribuir, mientras que el otro defecta. Esto se debe a que cj para el tipo altruista es cercano a 1, por lo tanto, el castigo esperado, 0.5P > (1 − cj), que el tipo altruista evita. Debido a que cj para el tipo no altruista es menor, prefiere no contribuir. Con tres pasos por delante, el agente altruista contribuye para evitar el castigo (0.5P > 2(1 − cj)), mientras que el tipo no altruista se desentiende. Para más de tres pasos, mientras el agente altruista continúa contribuyendo al bote público dependiendo de qué tan cercano esté su retorno privado marginal a 1, el tipo no altruista prescribe la deserción. Analizamos las decisiones de un agente altruista i modelado usando un nivel 1 I-DID expandido en 3 pasos de tiempo. i atribuye los dos modelos de nivel 0, mencionados anteriormente, a j (ver Fig. 9). Si creo con una probabilidad de 1 que j es altruista, elijo contribuir en cada uno de los tres pasos. Este comportamiento persiste cuando i no está al tanto de si j es altruista (Fig. 10(a)), y cuando i asigna una alta probabilidad a que j sea del tipo no altruista. Sin embargo, cuando i cree con una probabilidad de 1 que j no es altruista y por lo tanto seguramente va a traicionar, i elige traicionar para evitar ser castigado y porque su beneficio privado marginal es menor que 1. Estos resultados demuestran que el comportamiento de nuestro tipo altruista se asemeja al encontrado experimentalmente. El agente de nivel 1 no altruista elige desertar independientemente de lo altruista que crea que sea el otro agente. Analizamos el comportamiento de un tipo de agente recíproco que se ajusta a la cooperación o defección esperada. El retorno privado marginal del tipo recíproco es similar al del tipo no altruista, sin embargo, obtiene una mayor recompensa cuando su acción es similar a la del otro. Consideramos el caso en el que el agente recíproco i no está seguro de si j es altruista y cree que es probable que el bote público esté medio lleno. Para esta creencia previa, yo elijo defectar. Al recibir una observación de abundancia, decide contribuir, mientras que una observación de escasez lo hace defectar (Fig. 10(b)). Esto se debe a que una observación de abundancia indica que es probable que la olla esté más llena que a la mitad, lo cual resulta de la acción de js para contribuir. Por lo tanto, entre los dos modelos atribuidos a j, es probable que su tipo sea altruista, lo que hace probable que j contribuya nuevamente en el próximo paso de tiempo. El agente i, por lo tanto, elige contribuir para corresponder la acción de js. Un razonamiento análogo lleva a uno a defectar cuando observa una olla escasa. Con una acción por hacer, creo que si j contribuye, también elegiré contribuir para evitar castigo independientemente de sus observaciones. Figura 10: (a) Un agente altruista de nivel 1 siempre contribuye. (b) Un agente recíproco i comienza defraudando y luego elige contribuir o defraudar basándose en su observación de abundancia (indicando que j es probablemente altruista) o escasez (j no es altruista). 5.3 Estrategias en el póker de dos jugadores El póker es un popular juego de cartas de suma cero que ha recibido mucha atención en la comunidad de investigación de IA como un banco de pruebas [2]. El póker se juega entre M ≥ 2 jugadores, en el que cada jugador recibe una mano de cartas de una baraja. Si bien existen varias variantes de póker con diferentes niveles de complejidad, consideramos una versión simple en la que cada jugador tiene tres turnos durante los cuales el jugador puede intercambiar una carta (E), mantener la mano existente (K), retirarse (F) y abandonar el juego, o apostar (C), lo que requiere que todos los jugadores muestren sus manos. Para mantener las cosas simples, dejemos que M = 2, y que cada jugador reciba una mano compuesta por una sola carta sacada del mismo palo. Por lo tanto, durante un enfrentamiento, el jugador que tenga la carta numéricamente más alta (el 2 es el más bajo, el as es el más alto) gana el bote. Durante un intercambio de cartas, la carta descartada se coloca ya sea en la pila L, indicando al otro agente que era una carta de número bajo menor que 8, o en la 820 The Sixth Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) H, indicando que la carta tenía un rango mayor o igual a 8. Ten en cuenta que, por ejemplo, si se descarta una carta de número bajo, la probabilidad de recibir una carta baja a cambio se reduce. Mostramos el nivel 1 I-ID para el Poker simplificado de dos jugadores en la Fig. 11. Consideramos dos modelos (tipos de personalidad) del agente j. El tipo conservador cree que es probable que su oponente tenga una carta de alto número en su mano. Por otro lado, el agente agresivo j cree con alta probabilidad que su oponente tiene una carta de número más bajo. Por lo tanto, los dos tipos difieren en sus creencias sobre la mano de sus oponentes. En ambos estos modelos de nivel 0, se asume que el oponente realiza sus acciones siguiendo una distribución fija y uniforme. Con tres acciones restantes, independientemente de su mano (a menos que sea un as), el agente agresivo elige intercambiar su carta, con la intención de mejorar su mano actual. Esto se debe a que cree que el otro tiene una carta baja, lo que mejora sus posibilidades de obtener una carta alta durante el intercambio. El agente conservador elige quedarse con su carta, sin importar su mano, porque considera que sus posibilidades de obtener una carta alta son escasas, ya que cree que su oponente tiene una. Figura 11: (a) Nivel 1 I-ID del agente i. La observación revela información sobre la mano de js del paso de tiempo anterior, (b) IDs de nivel 0 del agente j cuyos nodos de decisión están mapeados a los nodos de probabilidad, A1 j , A2 j , en (a). La política de un agente de nivel 1 i que cree que cada carta, excepto la suya, tiene la misma probabilidad de estar en su mano (tipo de personalidad neutral) y j podría ser de tipo agresivo o conservador, se muestra en la Figura 12. Su mano contiene la carta numerada 8. El agente comienza por mantener su carta. Al ver que j no intercambió una carta (N), i cree con probabilidad 1 que j es conservador y, por lo tanto, mantendrá sus cartas. i responde manteniendo su carta o intercambiándola porque j tiene igual probabilidad de tener una carta más baja o más alta. Si observo que j descartó su carta en la pila L o H, creo que j es agresivo. Al observar a L, i se da cuenta de que j tenía una carta baja, y es probable que tenga una carta alta después de su intercambio. Dado que la probabilidad de recibir una carta baja es alta en este momento, i decide quedarse con su carta. Al observar la carta H, creyendo que la probabilidad de recibir una carta de número alto es alta, i decide intercambiar su carta. En el paso final, i decide llamar independientemente de su historial de observaciones porque su creencia de que j tiene una carta más alta no es lo suficientemente alta como para concluir que es mejor retirarse y renunciar al pago. Esto se debe en parte al hecho de que una observación de, digamos, L, restablece las creencias del agente en el paso de tiempo anterior sobre las cartas de números bajos solamente. 6. DISCUSIÓN Mostramos cómo los DIDs pueden ser extendidos a los I-DIDs que permiten la toma de decisiones secuenciales en línea en entornos multiagentes inciertos. Nuestra representación gráfica de I-DIDs mejora la Figura 12 anterior: Un agente de nivel 1 es una política de tres pasos en el problema de Poker. i comienza creyendo que j tiene la misma probabilidad de ser agresivo o conservador y podría tener cualquier carta en su mano con igual probabilidad. Trabaja significativamente al ser más transparente, semánticamente claro y capaz de ser resuelto utilizando algoritmos estándar que apuntan a los DIDs. Los I-DIDs extienden los NIDs para permitir la toma de decisiones secuenciales a lo largo de múltiples pasos de tiempo en presencia de otros agentes que interactúan. Los I-DIDs pueden ser vistos como representaciones gráficas concisas para IPOMDPs que proporcionan una forma de explotar la estructura del problema y llevar a cabo la toma de decisiones en línea a medida que el agente actúa y observa dadas sus creencias previas. Actualmente estamos investigando formas de resolver I-DIDs aproximadamente con límites demostrables en la calidad de la solución. Agradecimiento: Agradecemos a Piotr Gmytrasiewicz por algunas discusiones útiles relacionadas con este trabajo. El primer autor desea agradecer el apoyo de una beca UGARF. 7. REFERENCIAS [1] R. J. Aumann. Epistemología interactiva i: Conocimiento. Revista Internacional de Teoría de Juegos, 28:263-300, 1999. [2] D. Billings, A. Davidson, J. Schaeffer y D. Szafron. El desafío del póker. AIJ, 2001. [3] A. Brandenburger y E. Dekel. Jerarquías de creencias y conocimiento común. Revista de Teoría Económica, 59:189-198, 1993. [4] C. Camerer. Teoría de juegos conductual: Experimentos en interacción estratégica. Princeton University Press, 2003. [5] E. Fehr y S. Gachter. Cooperación y castigo en experimentos de bienes públicos. Revista Económica Americana, 90(4):980-994, 2000. [6] D. Fudenberg y D. K. Levine. La Teoría del Aprendizaje en los Juegos. MIT Press, 1998. [7] D. Fudenberg y J. Tirole. Teoría de juegos. MIT Press, 1991. [8] Y. Gal y A. Pfeffer. Un lenguaje para modelar los procesos de toma de decisiones de agentes en juegos. En AAMAS, 2003. [9] P. Gmytrasiewicz y P. Doshi. Un marco para la planificación secuencial en entornos multiagentes. JAIR, 24:49-79, 2005. [10] P. Gmytrasiewicz y E. Durfee. Coordinación racional en entornos multiagente. JAAMAS, 3(4):319-350, 2000. [11] J. C. Harsanyi. Juegos con información incompleta jugados por jugadores bayesianos. Ciencia de la Gestión, 14(3):159-182, 1967. [12] R. A. Howard y J. E. Matheson. Diagramas de influencia. En R. A. Howard y J. E. Matheson, editores, Los Principios y Aplicaciones del Análisis de Decisiones. Grupo de Decisiones Estratégicas, Menlo Park, CA 94025, 1984. [13] L. Kaelbling, M. Littman y A. Cassandra. Planificación y actuación en dominios estocásticos parcialmente observables. Revista de Inteligencia Artificial, 2, 1998. [14] D. Koller y B. Milch. Diagramas de influencia multiagente para representar y resolver juegos. En IJCAI, páginas 1027-1034, 2001. [15] K. Polich y P. Gmytrasiewicz. Diagramas de influencia interactivos y dinámicos. En el taller GTDT, AAMAS, 2006. [16] B. Rathnas, P. Doshi y P. J. Gmytrasiewicz. Soluciones exactas para POMDP interactivos utilizando equivalencia conductual. En la Conferencia de Agentes Autónomos y Sistemas Multiagente (AAMAS), 2006. [17] S. Russell y P. Norvig. Inteligencia Artificial: Un Enfoque Moderno (Segunda Edición). Prentice Hall, 2003. [18] R. D. Shachter. \n\nPrentice Hall, 2003. [18] R. D. Shachter. Evaluando diagramas de influencia. Investigación de Operaciones, 34(6):871-882, 1986. [19] D. Suryadi y P. Gmytrasiewicz. Aprendiendo modelos de otros agentes utilizando diagramas de influencia. En UM, 1999. El Sexto Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 821 ",
            "candidates": [],
            "error": [
                [
                    "diagramas de influencia dinámica interactiva",
                    "diagramas de influencia dinámica interactivos"
                ]
            ]
        },
        "sequential decision-making": {
            "translated_key": "",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Graphical Models for Online Solutions to Interactive POMDPs Prashant Doshi Dept.",
                "of Computer Science University of Georgia Athens, GA 30602, USA pdoshi@cs.uga.edu Yifeng Zeng Dept. of Computer Science Aalborg University DK-9220 Aalborg, Denmark yfzeng@cs.aau.edu Qiongyu Chen Dept. of Computer Science National Univ. of Singapore 117543, Singapore chenqy@comp.nus.edu.sg ABSTRACT We develop a new graphical representation for interactive partially observable Markov decision processes (I-POMDPs) that is significantly more transparent and semantically clear than the previous representation.",
                "These graphical models called interactive dynamic influence diagrams (I-DIDs) seek to explicitly model the structure that is often present in real-world problems by decomposing the situation into chance and decision variables, and the dependencies between the variables.",
                "I-DIDs generalize DIDs, which may be viewed as graphical representations of POMDPs, to multiagent settings in the same way that I-POMDPs generalize POMDPs.",
                "I-DIDs may be used to compute the policy of an agent online as the agent acts and observes in a setting that is populated by other interacting agents.",
                "Using several examples, we show how I-DIDs may be applied and demonstrate their usefulness.",
                "Categories and Subject Descriptors I.2.11 [Distributed Artificial Intelligence]: Multiagent Systems General Terms Theory 1.",
                "INTRODUCTION Interactive partially observable Markov decision processes (IPOMDPs) [9] provide a framework for <br>sequential decision-making</br> in partially observable multiagent environments.",
                "They generalize POMDPs [13] to multiagent settings by including the other agents computable models in the state space along with the states of the physical environment.",
                "The models encompass all information influencing the agents behaviors, including their preferences, capabilities, and beliefs, and are thus analogous to types in Bayesian games [11].",
                "I-POMDPs adopt a subjective approach to understanding strategic behavior, rooted in a decision-theoretic framework that takes a decision-makers perspective in the interaction.",
                "In [15], Polich and Gmytrasiewicz introduced interactive dynamic influence diagrams (I-DIDs) as the computational representations of I-POMDPs.",
                "I-DIDs generalize DIDs [12], which may be viewed as computational counterparts of POMDPs, to multiagents settings in the same way that I-POMDPs generalize POMDPs.",
                "I-DIDs contribute to a growing line of work [19] that includes multi-agent influence diagrams (MAIDs) [14], and more recently, networks of influence diagrams (NIDs) [8].",
                "These formalisms seek to explicitly model the structure that is often present in real-world problems by decomposing the situation into chance and decision variables, and the dependencies between the variables.",
                "MAIDs provide an alternative to normal and extensive game forms using a graphical formalism to represent games of imperfect information with a decision node for each agents actions and chance nodes capturing the agents private information.",
                "MAIDs objectively analyze the game, efficiently computing the Nash equilibrium profile by exploiting the independence structure.",
                "NIDs extend MAIDs to include agents uncertainty over the game being played and over models of the other agents.",
                "Each model is a MAID and the network of MAIDs is collapsed, bottom up, into a single MAID for computing the equilibrium of the game keeping in mind the different models of each agent.",
                "Graphical formalisms such as MAIDs and NIDs open up a promising area of research that aims to represent multiagent interactions more transparently.",
                "However, MAIDs provide an analysis of the game from an external viewpoint and the applicability of both is limited to static single play games.",
                "Matters are more complex when we consider interactions that are extended over time, where predictions about others future actions must be made using models that change as the agents act and observe.",
                "I-DIDs address this gap by allowing the representation of other agents models as the values of a special model node.",
                "Both, other agents models and the original agents beliefs over these models are updated over time using special-purpose implementations.",
                "In this paper, we improve on the previous preliminary representation of the I-DID shown in [15] by using the insight that the static I-ID is a type of NID.",
                "Thus, we may utilize NID-specific language constructs such as multiplexers to represent the model node, and subsequently the I-ID, more transparently.",
                "Furthermore, we clarify the semantics of the special purpose policy link introduced in the representation of I-DID by [15], and show that it could be replaced by traditional dependency links.",
                "In the previous representation of the I-DID, the update of the agents belief over the models of others as the agents act and receive observations was denoted using a special link called the model update link that connected the model nodes over time.",
                "We explicate the semantics of this link by showing how it can be implemented using the traditional dependency links between the chance nodes that constitute the model nodes.",
                "The net result is a representation of I-DID that is significantly more transparent, semantically clear, and capable of being implemented using the standard algorithms for solving DIDs.",
                "We show how IDIDs may be used to model an agents uncertainty over others models, that may themselves be I-DIDs.",
                "Solution to the I-DID is a policy that prescribes what the agent should do over time, given its beliefs over the physical state and others models.",
                "Analogous to DIDs, I-DIDs may be used to compute the policy of an agent online as the agent acts and observes in a setting that is populated by other interacting agents. 2.",
                "BACKGROUND: FINITELY NESTED IPOMDPS Interactive POMDPs generalize POMDPs to multiagent settings by including other agents models as part of the state space [9].",
                "Since other agents may also reason about others, the interactive state space is strategically nested; it contains beliefs about other agents models and their beliefs about others.",
                "For simplicity of presentation we consider an agent, i, that is interacting with one other agent, j.",
                "A finitely nested I-POMDP of agent i with a strategy level l is defined as the tuple: I-POMDPi,l = ISi,l, A, Ti, Ωi, Oi, Ri where: • ISi,l denotes a set of interactive states defined as, ISi,l = S × Mj,l−1, where Mj,l−1 = {Θj,l−1 ∪ SMj}, for l ≥ 1, and ISi,0 = S, where S is the set of states of the physical environment.",
                "Θj,l−1 is the set of computable intentional models of agent j: θj,l−1 = bj,l−1, ˆθj where the frame, ˆθj = A, Ωj, Tj, Oj, Rj, OCj .",
                "Here, j is Bayes rational and OCj is js optimality criterion.",
                "SMj is the set of subintentional models of j.",
                "Simple examples of subintentional models include a no-information model [10] and a fictitious play model [6], both of which are history independent.",
                "We give a recursive bottom-up construction of the interactive state space below.",
                "ISi,0 = S, Θj,0 = { bj,0, ˆθj | bj,0 ∈ Δ(ISj,0)} ISi,1 = S × {Θj,0 ∪ SMj}, Θj,1 = { bj,1, ˆθj | bj,1 ∈ Δ(ISj,1)} . . . . . .",
                "ISi,l = S × {Θj,l−1 ∪ SMj}, Θj,l = { bj,l, ˆθj | bj,l ∈ Δ(ISj,l)} Similar formulations of nested spaces have appeared in [1, 3]. • A = Ai × Aj is the set of joint actions of all agents in the environment; • Ti : S ×A×S → [0, 1], describes the effect of the joint actions on the physical states of the environment; • Ωi is the set of observations of agent i; • Oi : S × A × Ωi → [0, 1] gives the likelihood of the observations given the physical state and joint action; • Ri : ISi × A → R describes agent is preferences over its interactive states.",
                "Usually only the physical states will matter.",
                "Agent is policy is the mapping, Ω∗ i → Δ(Ai), where Ω∗ i is the set of all observation histories of agent i.",
                "Since belief over the interactive states forms a sufficient statistic [9], the policy can also be represented as a mapping from the set of all beliefs of agent i to a distribution over its actions, Δ(ISi) → Δ(Ai). 2.1 Belief Update Analogous to POMDPs, an agent within the I-POMDP framework updates its belief as it acts and observes.",
                "However, there are two differences that complicate the belief update in multiagent settings when compared to single agent ones.",
                "First, since the state of the physical environment depends on the actions of both agents, is prediction of how the physical state changes has to be made based on its prediction of js actions.",
                "Second, changes in js models have to be included in is belief update.",
                "Specifically, if j is intentional then an update of js beliefs due to its action and observation has to be included.",
                "In other words, i has to update its belief based on its prediction of what j would observe and how j would update its belief.",
                "If js model is subintentional, then js probable observations are appended to the observation history contained in the model.",
                "Formally, we have: Pr(ist |at−1 i , bt−1 i,l ) = β ISt−1:mt−1 j =θt j bt−1 i,l (ist−1 ) × at−1 j Pr(at−1 j |θt−1 j,l−1)Oi(st , at−1 i , at−1 j , ot i) ×Ti(st−1 , at−1 i , at−1 j , st ) ot j Oj(st , at−1 i , at−1 j , ot j) ×τ(SEθt j (bt−1 j,l−1, at−1 j , ot j) − bt j,l−1) (1) where β is the normalizing constant, τ is 1 if its argument is 0 otherwise it is 0, Pr(at−1 j |θt−1 j,l−1) is the probability that at−1 j is Bayes rational for the agent described by model θt−1 j,l−1, and SE(·) is an abbreviation for the belief update.",
                "For a version of the belief update when js model is subintentional, see [9].",
                "If agent j is also modeled as an I-POMDP, then is belief update invokes js belief update (via the term SEθt j ( bt−1 j,l−1 , at−1 j , ot j)), which in turn could invoke is belief update and so on.",
                "This recursion in belief nesting bottoms out at the 0th level.",
                "At this level, the belief update of the agent reduces to a POMDP belief update. 1 For illustrations of the belief update, additional details on I-POMDPs, and how they compare with other multiagent frameworks, see [9]. 2.2 Value Iteration Each belief state in a finitely nested I-POMDP has an associated value reflecting the maximum payoff the agent can expect in this belief state: Un ( bi,l, θi ) = max ai∈Ai is∈ISi,l ERi(is, ai)bi,l(is)+ γ oi∈Ωi Pr(oi|ai, bi,l)Un−1 ( SEθi (bi,l, ai, oi), θi ) (2) where, ERi(is, ai) = aj Ri(is, ai, aj)Pr(aj|mj,l−1) (since is = (s, mj,l−1)).",
                "Eq. 2 is a basis for value iteration in I-POMDPs.",
                "Agent is optimal action, a∗ i , for the case of finite horizon with discounting, is an element of the set of optimal actions for the belief state, OPT(θi), defined as: OPT( bi,l, θi ) = argmax ai∈Ai is∈ISi,l ERi(is, ai)bi,l(is) +γ oi∈Ωi Pr(oi|ai, bi,l)Un ( SEθi (bi,l, ai, oi), θi ) (3) 3.",
                "INTERACTIVEINFLUENCEDIAGRAMS A naive extension of influence diagrams (IDs) to settings populated by multiple agents is possible by treating other agents as automatons, represented using chance nodes.",
                "However, this approach assumes that the agents actions are controlled using a probability distribution that does not change over time.",
                "Interactive influence diagrams (I-IDs) adopt a more sophisticated approach by generalizing IDs to make them applicable to settings shared with other agents who may act and observe, and update their beliefs. 3.1 Syntax In addition to the usual chance, decision, and utility nodes, IIDs include a new type of node called the model node.",
                "We show a general level l I-ID in Fig. 1(a), where the model node (Mj,l−1) is denoted using a hexagon.",
                "We note that the probability distribution over the chance node, S, and the model node together represents agent is belief over its interactive states.",
                "In addition to the model 1 The 0th level model is a POMDP: Other agents actions are treated as exogenous events and folded into the T, O, and R functions.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 815 Figure 1: (a) A generic level l I-ID for agent i situated with one other agent j.",
                "The hexagon is the model node (Mj,l−1) whose structure we show in (b).",
                "Members of the model node are I-IDs themselves (m1 j,l−1, m2 j,l−1; diagrams not shown here for simplicity) whose decision nodes are mapped to the corresponding chance nodes (A1 j , A2 j ).",
                "Depending on the value of the node, Mod[Mj], the distribution of each of the chance nodes is assigned to the node Aj. (c) The transformed I-ID with the model node replaced by the chance nodes and the relationships between them. node, I-IDs differ from IDs by having a dashed link (called the policy link in [15]) between the model node and a chance node, Aj, that represents the distribution over the other agents actions given its model.",
                "In the absence of other agents, the model node and the chance node, Aj, vanish and I-IDs collapse into traditional IDs.",
                "The model node contains the alternative computational models ascribed by i to the other agent from the set, Θj,l−1 ∪ SMj, where Θj,l−1 and SMj were defined previously in Section 2.",
                "Thus, a model in the model node may itself be an I-ID or ID, and the recursion terminates when a model is an ID or subintentional.",
                "Because the model node contains the alternative models of the other agent as its values, its representation is not trivial.",
                "In particular, some of the models within the node are I-IDs that when solved generate the agents optimal policy in their decision nodes.",
                "Each decision node is mapped to the corresponding chance node, say A1 j , in the following way: if OPT is the set of optimal actions obtained by solving the I-ID (or ID), then Pr(aj ∈ A1 j ) = 1 |OP T | if aj ∈ OPT, 0 otherwise.",
                "Borrowing insights from previous work [8], we observe that the model node and the dashed policy link that connects it to the chance node, Aj, could be represented as shown in Fig. 1(b).",
                "The decision node of each level l − 1 I-ID is transformed into a chance node, as we mentioned previously, so that the actions with the largest value in the decision node are assigned uniform probabilities in the chance node while the rest are assigned zero probability.",
                "The different chance nodes (A1 j , A2 j ), one for each model, and additionally, the chance node labeled Mod[Mj] form the parents of the chance node, Aj.",
                "Thus, there are as many action nodes (A1 j , A2 j ) in Mj,l−1 as the number of models in the support of agent is beliefs.",
                "The conditional probability table of the chance node, Aj, is a multiplexer that assumes the distribution of each of the action nodes (A1 j , A2 j ) depending on the value of Mod[Mj].",
                "The values of Mod[Mj] denote the different models of j.",
                "In other words, when Mod[Mj] has the value m1 j,l−1, the chance node Aj assumes the distribution of the node A1 j , and Aj assumes the distribution of A2 j when Mod[Mj] has the value m2 j,l−1.",
                "The distribution over the node, Mod[Mj], is the agent is belief over the models of j given a physical state.",
                "For more agents, we will have as many model nodes as there are agents.",
                "Notice that Fig. 1(b) clarifies the semantics of the policy link, and shows how it can be represented using the traditional dependency links.",
                "In Fig. 1(c), we show the transformed I-ID when the model node is replaced by the chance nodes and relationships between them.",
                "In contrast to the representation in [15], there are no special-purpose policy links, rather the I-ID is composed of only those types of nodes that are found in traditional IDs and dependency relationships between the nodes.",
                "This allows I-IDs to be represented and implemented using conventional application tools that target IDs.",
                "Note that we may view the level l I-ID as a NID.",
                "Specifically, each of the level l − 1 models within the model node are blocks in the NID (see Fig. 2).",
                "If the level l = 1, each block is a traditional ID, otherwise if l > 1, each block within the NID may itself be a NID.",
                "Note that within the I-IDs (or IDs) at each level, there is only a single decision node.",
                "Thus, our NID does not contain any MAIDs.",
                "Figure 2: A level l I-ID represented as a NID.",
                "The probabilities assigned to the blocks of the NID are is beliefs over js models conditioned on a physical state. 3.2 Solution The solution of an I-ID proceeds in a bottom-up manner, and is implemented recursively.",
                "We start by solving the level 0 models, which, if intentional, are traditional IDs.",
                "Their solutions provide probability distributions over the other agents actions, which are entered in the corresponding chance nodes found in the model node of the level 1 I-ID.",
                "The mapping from the level 0 models decision nodes to the chance nodes is carried out so that actions with the largest value in the decision node are assigned uniform probabilities in the chance node while the rest are assigned zero probability.",
                "Given the distributions over the actions within the different chance nodes (one for each model of the other agent), the level 1 I-ID is transformed as shown in Fig. 1(c).",
                "During the transformation, the conditional probability table (CPT) of the node, Aj, is populated such that the node assumes the distribution of each of the chance nodes depending on the value of the node, Mod[Mj].",
                "As we mentioned previously, the values of the node Mod[Mj] denote the different models of the other agent, and its distribution is the agent is belief over the models of j conditioned on the physical state.",
                "The transformed level 1 I-ID is a traditional ID that may be solved us816 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) (a) (b) Figure 3: (a) A generic two time-slice level l I-DID for agent i in a setting with one other agent j.",
                "Notice the dotted model update link that denotes the update of the models of j and the distribution over the models over time. (b) The semantics of the model update link. ing the standard expected utility maximization method [18].",
                "This procedure is carried out up to the level l I-ID whose solution gives the non-empty set of optimal actions that the agent should perform given its belief.",
                "Notice that analogous to IDs, I-IDs are suitable for online decision-making when the agents current belief is known. 4.",
                "INTERACTIVE DYNAMIC INFLUENCE DIAGRAMS Interactive dynamic influence diagrams (I-DIDs) extend I-IDs (and NIDs) to allow <br>sequential decision-making</br> over several time steps.",
                "Just as DIDs are structured graphical representations of POMDPs, I-DIDs are the graphical online analogs for finitely nested I-POMDPs.",
                "I-DIDs may be used to optimize over a finite look-ahead given initial beliefs while interacting with other, possibly similar, agents. 4.1 Syntax We depict a general two time-slice I-DID in Fig. 3(a).",
                "In addition to the model nodes and the dashed policy link, what differentiates an I-DID from a DID is the model update link shown as a dotted arrow in Fig. 3(a).",
                "We explained the semantics of the model node and the policy link in the previous section; we describe the model updates next.",
                "The update of the model node over time involves two steps: First, given the models at time t, we identify the updated set of models that reside in the model node at time t + 1.",
                "Recall from Section 2 that an agents intentional model includes its belief.",
                "Because the agents act and receive observations, their models are updated to reflect their changed beliefs.",
                "Since the set of optimal actions for a model could include all the actions, and the agent may receive any one of |Ωj| possible observations, the updated set at time step t + 1 will have at most |Mt j,l−1||Aj||Ωj| models.",
                "Here, |Mt j,l−1| is the number of models at time step t, |Aj| and |Ωj| are the largest spaces of actions and observations respectively, among all the models.",
                "Second, we compute the new distribution over the updated models given the original distribution and the probability of the agent performing the action and receiving the observation that led to the updated model.",
                "These steps are a part of agent is belief update formalized using Eq. 1.",
                "In Fig. 3(b), we show how the dotted model update link is implemented in the I-DID.",
                "If each of the two level l − 1 models ascribed to j at time step t results in one action, and j could make one of two possible observations, then the model node at time step t + 1 contains four updated models (mt+1,1 j,l−1 ,mt+1,2 j,l−1 , mt+1,3 j,l−1 , and mt+1,4 j,l−1 ).",
                "These models differ in their initial beliefs, each of which is the result of j updating its beliefs due to its action and a possible observation.",
                "The decision nodes in each of the I-DIDs or DIDs that represent the lower level models are mapped to the corresponding Figure 4: Transformed I-DID with the model nodes and model update link replaced with the chance nodes and the relationships (in bold). chance nodes, as mentioned previously.",
                "Next, we describe how the distribution over the updated set of models (the distribution over the chance node Mod[Mt+1 j ] in Mt+1 j,l−1) is computed.",
                "The probability that js updated model is, say mt+1,1 j,l−1 , depends on the probability of j performing the action and receiving the observation that led to this model, and the prior distribution over the models at time step t. Because the chance node At j assumes the distribution of each of the action nodes based on the value of Mod[Mt j ], the probability of the action is given by this chance node.",
                "In order to obtain the probability of js possible observation, we introduce the chance node Oj, which depending on the value of Mod[Mt j ] assumes the distribution of the observation node in the lower level model denoted by Mod[Mt j ].",
                "Because the probability of js observations depends on the physical state and the joint actions of both agents, the node Oj is linked with St+1 , At j, and At i. 2 Analogous to At j, the conditional probability table of Oj is also a multiplexer modulated by Mod[Mt j ].",
                "Finally, the distribution over the prior models at time t is obtained from the chance node, Mod[Mt j ] in Mt j,l−1.",
                "Consequently, the chance nodes, Mod[Mt j ], At j, and Oj, form the parents of Mod[Mt+1 j ] in Mt+1 j,l−1.",
                "Notice that the model update link may be replaced by the dependency links between the chance nodes that constitute the model nodes in the two time slices.",
                "In Fig. 4 we show the two time-slice I-DID with the model nodes replaced by the chance nodes and the relationships between them.",
                "Chance nodes and dependency links that not in bold are standard, usually found in DIDs.",
                "Expansion of the I-DID over more time steps requires the repetition of the two steps of updating the set of models that form the 2 Note that Oj represents js observation at time t + 1.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 817 values of the model node and adding the relationships between the chance nodes, as many times as there are model update links.",
                "We note that the possible set of models of the other agent j grows exponentially with the number of time steps.",
                "For example, after T steps, there may be at most |Mt=1 j,l−1|(|Aj||Ωj|)T −1 candidate models residing in the model node. 4.2 Solution Analogous to I-IDs, the solution to a level l I-DID for agent i expanded over T time steps may be carried out recursively.",
                "For the purpose of illustration, let l=1 and T=2.",
                "The solution method uses the standard look-ahead technique, projecting the agents action and observation sequences forward from the current belief state [17], and finding the possible beliefs that i could have in the next time step.",
                "Because agent i has a belief over js models as well, the lookahead includes finding out the possible models that j could have in the future.",
                "Consequently, each of js subintentional or level 0 models (represented using a standard DID) in the first time step must be solved to obtain its optimal set of actions.",
                "These actions are combined with the set of possible observations that j could make in that model, resulting in an updated set of candidate models (that include the updated beliefs) that could describe the behavior of j. Beliefs over this updated set of candidate models are calculated using the standard inference methods using the dependency relationships between the model nodes as shown in Fig. 3(b).",
                "We note the recursive nature of this solution: in solving agent is level 1 I-DID, js level 0 DIDs must be solved.",
                "If the nesting of models is deeper, all models at all levels starting from 0 are solved in a bottom-up manner.",
                "We briefly outline the recursive algorithm for solving agent is Algorithm for solving I-DID Input : level l ≥ 1 I-ID or level 0 ID, T Expansion Phase 1.",
                "For t from 1 to T − 1 do 2.",
                "If l ≥ 1 then Populate Mt+1 j,l−1 3.",
                "For each mt j in Range(Mt j,l−1) do 4.",
                "Recursively call algorithm with the l − 1 I-ID (or ID) that represents mt j and the horizon, T − t + 1 5.",
                "Map the decision node of the solved I-ID (or ID), OPT(mt j), to a chance node Aj 6.",
                "For each aj in OPT(mt j) do 7.",
                "For each oj in Oj (part of mt j) do 8.",
                "Update js belief, bt+1 j ← SE(bt j, aj, oj) 9. mt+1 j ← New I-ID (or ID) with bt+1 j as the initial belief 10.",
                "Range(Mt+1 j,l−1) ∪ ← {mt+1 j } 11.",
                "Add the model node, Mt+1 j,l−1, and the dependency links between Mt j,l−1 and Mt+1 j,l−1 (shown in Fig. 3(b)) 12.",
                "Add the chance, decision, and utility nodes for t + 1 time slice and the dependency links between them 13.",
                "Establish the CPTs for each chance node and utility node Look-Ahead Phase 14.",
                "Apply the standard look-ahead and backup method to solve the expanded I-DID Figure 5: Algorithm for solving a level l ≥ 0 I-DID. level l I-DID expanded over T time steps with one other agent j in Fig. 5.",
                "We adopt a two-phase approach: Given an I-ID of level l (described previously in Section 3) with all lower level models also represented as I-IDs or IDs (if level 0), the first step is to expand the level l I-ID over T time steps adding the dependency links and the conditional probability tables for each node.",
                "We particularly focus on establishing and populating the model nodes (lines 3-11).",
                "Note that Range(·) returns the values (lower level models) of the random variable given as input (model node).",
                "In the second phase, we use a standard look-ahead technique projecting the action and observation sequences over T time steps in the future, and backing up the utility values of the reachable beliefs.",
                "Similar to I-IDs, the I-DIDs reduce to DIDs in the absence of other agents.",
                "As we mentioned previously, the 0-th level models are the traditional DIDs.",
                "Their solutions provide probability distributions over actions of the agent modeled at that level to I-DIDs at level 1.",
                "Given probability distributions over other agents actions the level 1 IDIDs can themselves be solved as DIDs, and provide probability distributions to yet higher level models.",
                "Assume that the number of models considered at each level is bound by a number, M. Solving an I-DID of level l in then equivalent to solving O(Ml ) DIDs. 5.",
                "EXAMPLE APPLICATIONS To illustrate the usefulness of I-DIDs, we apply them to three problem domains.",
                "We describe, in particular, the formulation of the I-DID and the optimal prescriptions obtained on solving it. 5.1 Followership-Leadership in the Multiagent Tiger Problem We begin our illustrations of using I-IDs and I-DIDs with a slightly modified version of the multiagent tiger problem discussed in [9].",
                "The problem has two agents, each of which can open the right door (OR), the left door (OL) or listen (L).",
                "In addition to hearing growls (from the left (GL) or from the right (GR)) when they listen, the agents also hear creaks (from the left (CL), from the right (CR), or no creaks (S)), which noisily indicate the other agents opening one of the doors.",
                "When any door is opened, the tiger persists in its original location with a probability of 95%.",
                "Agent i hears growls with a reliability of 65% and creaks with a reliability of 95%.",
                "Agent j, on the other hand, hears growls with a reliability of 95%.",
                "Thus, the setting is such that agent i hears agent j opening doors more reliably than the tigers growls.",
                "This suggests that i could use js actions as an indication of the location of the tiger, as we discuss below.",
                "Each agents preferences are as in the single agent game discussed in [13].",
                "The transition, observation, and reward functions are shown in [16].",
                "A good indicator of the usefulness of normative methods for decision-making like I-DIDs is the emergence of realistic social behaviors in their prescriptions.",
                "In settings of the persistent multiagent tiger problem that reflect real world situations, we demonstrate followership between the agents and, as shown in [15], deception among agents who believe that they are in a follower-leader type of relationship.",
                "In particular, we analyze the situational and epistemological conditions sufficient for their emergence.",
                "The followership behavior, for example, results from the agent knowing its own weaknesses, assessing the strengths, preferences, and possible behaviors of the other, and realizing that its best for it to follow the others actions in order to maximize its payoffs.",
                "Let us consider a particular setting of the tiger problem in which agent i believes that js preferences are aligned with its own - both of them just want to get the gold - and js hearing is more reliable in comparison to itself.",
                "As an example, suppose that j, on listening can discern the tigers location 95% of the times compared to is 65% accuracy.",
                "Additionally, agent i does not have any initial information about the tigers location.",
                "In other words, is single-level nested belief, bi,1, assigns 0.5 to each of the two locations of the tiger.",
                "In addition, i considers two models of j, which differ in js flat level 0 initial beliefs.",
                "This is represented in the level 1 I-ID shown in Fig. 6(a).",
                "According to one model, j assigns a probability of 0.9 that the tiger is behind the left door, while the other 818 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Figure 6: (a) Level 1 I-ID of agent i, (b) two level 0 IDs of agent j whose decision nodes are mapped to the chance nodes, A1 j , A2 j , in (a). model assigns 0.1 to that location (see Fig. 6(b)).",
                "Agent i is undecided on these two models of j.",
                "If we vary is hearing ability, and solve the corresponding level 1 I-ID expanded over three time steps, we obtain the normative behavioral policies shown in Fig 7 that exhibit followership behavior.",
                "If is probability of correctly hearing the growls is 0.65, then as shown in the policy in Fig. 7(a), i begins to conditionally follow js actions: i opens the same door that j opened previously iff is own assessment of the tigers location confirms js pick.",
                "If i loses the ability to correctly interpret the growls completely, it blindly follows j and opens the same door that j opened previously (Fig. 7(b)).",
                "Figure 7: Emergence of (a) conditional followership, and (b) blind followership in the tiger problem.",
                "Behaviors of interest are in bold. * is a wildcard, and denotes any one of the observations.",
                "We observed that a single level of belief nesting - beliefs about the others models - was sufficient for followership to emerge in the tiger problem.",
                "However, the epistemological requirements for the emergence of leadership are more complex.",
                "For an agent, say j, to emerge as a leader, followership must first emerge in the other agent i.",
                "As we mentioned previously, if i is certain that its preferences are identical to those of j, and believes that j has a better sense of hearing, i will follow js actions over time.",
                "Agent j emerges as a leader if it believes that i will follow it, which implies that js belief must be nested two levels deep to enable it to recognize its leadership role.",
                "Realizing that i will follow presents j with an opportunity to influence is actions in the benefit of the collective good or its self-interest alone.",
                "For example, in the tiger problem, let us consider a setting in which if both i and j open the correct door, then each gets a payoff of 20 that is double the original.",
                "If j alone selects the correct door, it gets the payoff of 10.",
                "On the other hand, if both agents pick the wrong door, their penalties are cut in half.",
                "In this setting, it is in both js best interest as well as the collective betterment for j to use its expertise in selecting the correct door, and thus be a good leader.",
                "However, consider a slightly different problem in which j gains from is loss and is penalized if i gains.",
                "Specifically, let is payoff be subtracted from js, indicating that j is antagonistic toward i - if j picks the correct door and i the wrong one, then is loss of 100 becomes js gain.",
                "Agent j believes that i incorrectly thinks that js preferences are those that promote the collective good and that it starts off by believing with 99% confidence where the tiger is.",
                "Because i believes that its preferences are similar to those of j, and that j starts by believing almost surely that one of the two is the correct location (two level 0 models of j), i will start by following js actions.",
                "We show is normative policy on solving its singly-nested I-DID over three time steps in Fig. 8(a).",
                "The policy demonstrates that i will blindly follow js actions.",
                "Since the tiger persists in its original location with a probability of 0.95, i will select the same door again.",
                "If j begins the game with a 99% probability that the tiger is on the right, solving js I-DID nested two levels deep, results in the policy shown in Fig. 8(b).",
                "Even though j is almost certain that OL is the correct action, it will start by selecting OR, followed by OL.",
                "Agent js intention is to deceive i who, it believes, will follow js actions, so as to gain $110 in the second time step, which is more than what j would gain if it were to be honest.",
                "Figure 8: Emergence of deception between agents in the tiger problem.",
                "Behaviors of interest are in bold. * denotes as before. (a) Agent is policy demonstrating that it will blindly follow js actions. (b) Even though j is almost certain that the tiger is on the right, it will start by selecting OR, followed by OL, in order to deceive i. 5.2 Altruism and Reciprocity in the Public Good Problem The public good (PG) problem [7], consists of a group of M agents, each of whom must either contribute some resource to a public pot or keep it for themselves.",
                "Since resources contributed to the public pot are shared among all the agents, they are less valuable to the agent when in the public pot.",
                "However, if all agents choose to contribute their resources, then the payoff to each agent is more than if no one contributes.",
                "Since an agent gets its share of the public pot irrespective of whether it has contributed or not, the dominating action is for each agent to not contribute, and instead free ride on others contributions.",
                "However, behaviors of human players in empirical simulations of the PG problem differ from the normative predictions.",
                "The experiments reveal that many players initially contribute a large amount to the public pot, and continue to contribute when the PG problem is played repeatedly, though in decreasing amounts [4].",
                "Many of these experiments [5] report that a small core group of players persistently contributes to the public pot even when all others are defecting.",
                "These experiments also reveal that players who persistently contribute have altruistic or reciprocal preferences matching expected cooperation of others.",
                "For simplicity, we assume that the game is played between M = 2 agents, i and j.",
                "Let each agent be initially endowed with XT amount of resources.",
                "While the classical PG game formulation permits each agent to contribute any quantity of resources (≤ XT ) to the public pot, we simplify the action space by allowing two possible actions.",
                "Each agent may choose to either contribute (C) a fixed amount of the resources, or not contribute.",
                "The latter action is deThe Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 819 noted as defect (D).",
                "We assume that the actions are not observable to others.",
                "The value of resources in the public pot is discounted by ci for each agent i, where ci is the marginal private return.",
                "We assume that ci < 1 so that the agent does not benefit enough that it contributes to the public pot for private gain.",
                "Simultaneously, ciM > 1, making collective contribution pareto optimal. i/j C D C 2ciXT , 2cjXT ciXT − cp, XT + cjXT − P D XT + ciXT − P, cjXT − cp XT , XT Table 1: The one-shot PG game with punishment.",
                "In order to encourage contributions, the contributing agents punish free riders but incur a small cost for administering the punishment.",
                "Let P be the punishment meted out to the defecting agent and cp the non-zero cost of punishing for the contributing agent.",
                "For simplicity, we assume that the cost of punishing is same for both the agents.",
                "The one-shot PG game with punishment is shown in Table. 1.",
                "Let ci = cj, cp > 0, and if P > XT − ciXT , then defection is no longer a dominating action.",
                "If P < XT − ciXT , then defection is the dominating action for both.",
                "If P = XT − ciXT , then the game is not dominance-solvable.",
                "Figure 9: (a) Level 1 I-ID of agent i, (b) level 0 IDs of agent j with decision nodes mapped to the chance nodes, A1 j and A2 j , in (a).",
                "We formulate a sequential version of the PG problem with punishment from the perspective of agent i.",
                "Though in the repeated PG game, the quantity in the public pot is revealed to all the agents after each round of actions, we assume in our formulation that it is hidden from the agents.",
                "Each agent may contribute a fixed amount, xc, or defect.",
                "An agent on performing an action receives an observation of plenty (PY) or meager (MR) symbolizing the state of the public pot.",
                "Notice that the observations are also indirectly indicative of agent js actions because the state of the public pot is influenced by them.",
                "The amount of resources in agent is private pot, is perfectly observable to i.",
                "The payoffs are analogous to Table. 1.",
                "Borrowing from the empirical investigations of the PG problem [5], we construct level 0 IDs for j that model altruistic and non-altruistic types (Fig. 9(b)).",
                "Specifically, our altruistic agent has a high marginal private return (cj is close to 1) and does not punish others who defect.",
                "Let xc = 1 and the level 0 agent be punished half the times it defects.",
                "With one action remaining, both types of agents choose to contribute to avoid being punished.",
                "With two actions to go, the altruistic type chooses to contribute, while the other defects.",
                "This is because cj for the altruistic type is close to 1, thus the expected punishment, 0.5P > (1 − cj), which the altruistic type avoids.",
                "Because cj for the non-altruistic type is less, it prefers not to contribute.",
                "With three steps to go, the altruistic agent contributes to avoid punishment (0.5P > 2(1 − cj)), and the non-altruistic type defects.",
                "For greater than three steps, while the altruistic agent continues to contribute to the public pot depending on how close its marginal private return is to 1, the non-altruistic type prescribes defection.",
                "We analyzed the decisions of an altruistic agent i modeled using a level 1 I-DID expanded over 3 time steps. i ascribes the two level 0 models, mentioned previously, to j (see Fig. 9).",
                "If i believes with a probability 1 that j is altruistic, i chooses to contribute for each of the three steps.",
                "This behavior persists when i is unaware of whether j is altruistic (Fig. 10(a)), and when i assigns a high probability to j being the non-altruistic type.",
                "However, when i believes with a probability 1 that j is non-altruistic and will thus surely defect, i chooses to defect to avoid being punished and because its marginal private return is less than 1.",
                "These results demonstrate that the behavior of our altruistic type resembles that found experimentally.",
                "The non-altruistic level 1 agent chooses to defect regardless of how likely it believes the other agent to be altruistic.",
                "We analyzed the behavior of a reciprocal agent type that matches expected cooperation or defection.",
                "The reciprocal types marginal private return is similar to that of the non-altruistic type, however, it obtains a greater payoff when its action is similar to that of the other.",
                "We consider the case when the reciprocal agent i is unsure of whether j is altruistic and believes that the public pot is likely to be half full.",
                "For this prior belief, i chooses to defect.",
                "On receiving an observation of plenty, i decides to contribute, while an observation of meager makes it defect (Fig. 10(b)).",
                "This is because an observation of plenty signals that the pot is likely to be greater than half full, which results from js action to contribute.",
                "Thus, among the two models ascribed to j, its type is likely to be altruistic making it likely that j will contribute again in the next time step.",
                "Agent i therefore chooses to contribute to reciprocate js action.",
                "An analogous reasoning leads i to defect when it observes a meager pot.",
                "With one action to go, i believing that j contributes, will choose to contribute too to avoid punishment regardless of its observations.",
                "Figure 10: (a) An altruistic level 1 agent always contributes. (b) A reciprocal agent i starts off by defecting followed by choosing to contribute or defect based on its observation of plenty (indicating that j is likely altruistic) or meager (j is non-altruistic). 5.3 Strategies in Two-Player Poker Poker is a popular zero sum card game that has received much attention among the AI research community as a testbed [2].",
                "Poker is played among M ≥ 2 players in which each player receives a hand of cards from a deck.",
                "While several flavors of Poker with varying complexity exist, we consider a simple version in which each player has three plys during which the player may either exchange a card (E), keep the existing hand (K), fold (F) and withdraw from the game, or call (C), requiring all players to show their hands.",
                "To keep matters simple, let M = 2, and each player receive a hand consisting of a single card drawn from the same suit.",
                "Thus, during a showdown, the player who has the numerically larger card (2 is the lowest, ace is the highest) wins the pot.",
                "During an exchange of cards, the discarded card is placed either in the L pile, indicating to the other agent that it was a low numbered card less than 8, or in the 820 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) H pile, indicating that the card had a rank greater than or equal to 8.",
                "Notice that, for example, if a lower numbered card is discarded, the probability of receiving a low card in exchange is now reduced.",
                "We show the level 1 I-ID for the simplified two-player Poker in Fig. 11.",
                "We considered two models (personality types) of agent j.",
                "The conservative type believes that it is likely that its opponent has a high numbered card in its hand.",
                "On the other hand, the aggressive agent j believes with a high probability that its opponent has a lower numbered card.",
                "Thus, the two types differ in their beliefs over their opponents hand.",
                "In both these level 0 models, the opponent is assumed to perform its actions following a fixed, uniform distribution.",
                "With three actions to go, regardless of its hand (unless it is an ace), the aggressive agent chooses to exchange its card, with the intent of improving on its current hand.",
                "This is because it believes the other to have a low card, which improves its chances of getting a high card during the exchange.",
                "The conservative agent chooses to keep its card, no matter its hand because its chances of getting a high card are slim as it believes that its opponent has one.",
                "Figure 11: (a) Level 1 I-ID of agent i.",
                "The observation reveals information about js hand of the previous time step, (b) level 0 IDs of agent j whose decision nodes are mapped to the chance nodes, A1 j , A2 j , in (a).",
                "The policy of a level 1 agent i who believes that each card except its own has an equal likelihood of being in js hand (neutral personality type) and j could be either an aggressive or conservative type, is shown in Fig. 12. is own hand contains the card numbered 8.",
                "The agent starts by keeping its card.",
                "On seeing that j did not exchange a card (N), i believes with probability 1 that j is conservative and hence will keep its cards. i responds by either keeping its card or exchanging it because j is equally likely to have a lower or higher card.",
                "If i observes that j discarded its card into the L or H pile, i believes that j is aggressive.",
                "On observing L, i realizes that j had a low card, and is likely to have a high card after its exchange.",
                "Because the probability of receiving a low card is high now, i chooses to keep its card.",
                "On observing H, believing that the probability of receiving a high numbered card is high, i chooses to exchange its card.",
                "In the final step, i chooses to call regardless of its observation history because its belief that j has a higher card is not sufficiently high to conclude that its better to fold and relinquish the payoff.",
                "This is partly due to the fact that an observation of, say, L resets the agent is previous time step beliefs over js hand to the low numbered cards only. 6.",
                "DISCUSSION We showed how DIDs may be extended to I-DIDs that enable online <br>sequential decision-making</br> in uncertain multiagent settings.",
                "Our graphical representation of I-DIDs improves on the previous Figure 12: A level 1 agent is three step policy in the Poker problem. i starts by believing that j is equally likely to be aggressive or conservative and could have any card in its hand with equal probability. work significantly by being more transparent, semantically clear, and capable of being solved using standard algorithms that target DIDs.",
                "I-DIDs extend NIDs to allow <br>sequential decision-making</br> over multiple time steps in the presence of other interacting agents.",
                "I-DIDs may be seen as concise graphical representations for IPOMDPs providing a way to exploit problem structure and carry out online decision-making as the agent acts and observes given its prior beliefs.",
                "We are currently investigating ways to solve I-DIDs approximately with provable bounds on the solution quality.",
                "Acknowledgment: We thank Piotr Gmytrasiewicz for some useful discussions related to this work.",
                "The first author would like to acknowledge the support of a UGARF grant. 7.",
                "REFERENCES [1] R. J. Aumann.",
                "Interactive epistemology i: Knowledge.",
                "International Journal of Game Theory, 28:263-300, 1999. [2] D. Billings, A. Davidson, J. Schaeffer, and D. Szafron.",
                "The challenge of poker.",
                "AIJ, 2001. [3] A. Brandenburger and E. Dekel.",
                "Hierarchies of beliefs and common knowledge.",
                "Journal of Economic Theory, 59:189-198, 1993. [4] C. Camerer.",
                "Behavioral Game Theory: Experiments in Strategic Interaction.",
                "Princeton University Press, 2003. [5] E. Fehr and S. Gachter.",
                "Cooperation and punishment in public goods experiments.",
                "American Economic Review, 90(4):980-994, 2000. [6] D. Fudenberg and D. K. Levine.",
                "The Theory of Learning in Games.",
                "MIT Press, 1998. [7] D. Fudenberg and J. Tirole.",
                "Game Theory.",
                "MIT Press, 1991. [8] Y. Gal and A. Pfeffer.",
                "A language for modeling agents decision-making processes in games.",
                "In AAMAS, 2003. [9] P. Gmytrasiewicz and P. Doshi.",
                "A framework for sequential planning in multiagent settings.",
                "JAIR, 24:49-79, 2005. [10] P. Gmytrasiewicz and E. Durfee.",
                "Rational coordination in multi-agent environments.",
                "JAAMAS, 3(4):319-350, 2000. [11] J. C. Harsanyi.",
                "Games with incomplete information played by bayesian players.",
                "Management Science, 14(3):159-182, 1967. [12] R. A. Howard and J. E. Matheson.",
                "Influence diagrams.",
                "In R. A. Howard and J. E. Matheson, editors, The Principles and Applications of Decision Analysis.",
                "Strategic Decisions Group, Menlo Park, CA 94025, 1984. [13] L. Kaelbling, M. Littman, and A. Cassandra.",
                "Planning and acting in partially observable stochastic domains.",
                "Artificial Intelligence Journal, 2, 1998. [14] D. Koller and B. Milch.",
                "Multi-agent influence diagrams for representing and solving games.",
                "In IJCAI, pages 1027-1034, 2001. [15] K. Polich and P. Gmytrasiewicz.",
                "Interactive dynamic influence diagrams.",
                "In GTDT Workshop, AAMAS, 2006. [16] B.",
                "Rathnas., P. Doshi, and P. J. Gmytrasiewicz.",
                "Exact solutions to interactive pomdps using behavioral equivalence.",
                "In Autonomous Agents and Multi-Agent Systems Conference (AAMAS), 2006. [17] S. Russell and P. Norvig.",
                "Artificial Intelligence: A Modern Approach (Second Edition).",
                "Prentice Hall, 2003. [18] R. D. Shachter.",
                "Evaluating influence diagrams.",
                "Operations Research, 34(6):871-882, 1986. [19] D. Suryadi and P. Gmytrasiewicz.",
                "Learning models of other agents using influence diagrams.",
                "In UM, 1999.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 821"
            ],
            "original_annotated_samples": [
                "INTRODUCTION Interactive partially observable Markov decision processes (IPOMDPs) [9] provide a framework for <br>sequential decision-making</br> in partially observable multiagent environments.",
                "INTERACTIVE DYNAMIC INFLUENCE DIAGRAMS Interactive dynamic influence diagrams (I-DIDs) extend I-IDs (and NIDs) to allow <br>sequential decision-making</br> over several time steps.",
                "DISCUSSION We showed how DIDs may be extended to I-DIDs that enable online <br>sequential decision-making</br> in uncertain multiagent settings.",
                "I-DIDs extend NIDs to allow <br>sequential decision-making</br> over multiple time steps in the presence of other interacting agents."
            ],
            "translated_annotated_samples": [
                "Las Procesos de Decisión de Markov Interactivos Parcialmente Observables (IPOMDPs) proporcionan un marco para la <br>toma de decisiones secuenciales</br> en entornos multiagentes parcialmente observables.",
                "Diagramas de influencia dinámica interactivos (I-DIDs) Los diagramas de influencia dinámica interactivos (I-DIDs) extienden los I-IDs (y NIDs) para permitir la <br>toma de decisiones secuencial</br> a lo largo de varios pasos de tiempo.",
                "DISCUSIÓN Mostramos cómo los DIDs pueden ser extendidos a los I-DIDs que permiten la <br>toma de decisiones secuenciales</br> en línea en entornos multiagentes inciertos.",
                "Los I-DIDs extienden los NIDs para permitir la <br>toma de decisiones secuenciales</br> a lo largo de múltiples pasos de tiempo en presencia de otros agentes que interactúan."
            ],
            "translated_text": "Modelos gráficos para soluciones en línea a POMDP interactivos de Prashant Doshi Dept. Departamento de Ciencias de la Computación Universidad de Georgia Athens, GA 30602, EE. UU. pdoshi@cs.uga.edu Yifeng Zeng Dept. de Ciencias de la Computación Universidad de Aalborg DK-9220 Aalborg, Dinamarca yfzeng@cs.aau.edu Qiongyu Chen Dept. de Ciencias de la Computación Universidad Nacional de Singapur 117543, Singapur chenqy@comp.nus.edu.sg RESUMEN Desarrollamos una nueva representación gráfica para procesos de decisión de Markov parcialmente observables interactivos (I-POMDPs) que es significativamente más transparente y semánticamente clara que la representación anterior. Estos modelos gráficos llamados diagramas de influencia dinámica interactiva (I-DIDs) buscan modelar explícitamente la estructura que a menudo está presente en problemas del mundo real al descomponer la situación en variables de probabilidad y decisión, y las dependencias entre las variables. Los I-DIDs generalizan los DIDs, los cuales pueden ser vistos como representaciones gráficas de POMDPs, a entornos multiagentes de la misma manera que los I-POMDPs generalizan los POMDPs. Los I-DIDs pueden ser utilizados para calcular la política de un agente en línea mientras el agente actúa y observa en un entorno poblado por otros agentes que interactúan entre sí. Usando varios ejemplos, mostramos cómo se pueden aplicar los I-DIDs y demostramos su utilidad. Categorías y Descriptores de Asignaturas I.2.11 [Inteligencia Artificial Distribuida]: Sistemas Multiagentes Términos Generales Teoría 1. Las Procesos de Decisión de Markov Interactivos Parcialmente Observables (IPOMDPs) proporcionan un marco para la <br>toma de decisiones secuenciales</br> en entornos multiagentes parcialmente observables. Generalizan los POMDPs [13] a entornos multiagentes al incluir los modelos computables de los otros agentes en el espacio de estados junto con los estados del entorno físico. Los modelos abarcan toda la información que influye en los comportamientos de los agentes, incluyendo sus preferencias, capacidades y creencias, y por lo tanto son análogos a los tipos en los juegos bayesianos [11]. I-POMDPs adoptan un enfoque subjetivo para comprender el comportamiento estratégico, arraigado en un marco de teoría de decisiones que toma la perspectiva de los tomadores de decisiones en la interacción. En [15], Polich y Gmytrasiewicz introdujeron diagramas de influencia dinámica interactivos (I-DIDs) como las representaciones computacionales de I-POMDPs. Los I-DIDs generalizan los DIDs [12], los cuales pueden ser vistos como contrapartes computacionales de POMDPs, a entornos de múltiples agentes de la misma manera que los I-POMDPs generalizan los POMDPs. Los I-DIDs contribuyen a una creciente línea de trabajo [19] que incluye diagramas de influencia multiagente (MAIDs) [14], y más recientemente, redes de diagramas de influencia (NIDs) [8]. Estos formalismos buscan modelar explícitamente la estructura que a menudo está presente en problemas del mundo real al descomponer la situación en variables de azar y decisiones, y las dependencias entre las variables. Los MAIDs proporcionan una alternativa a las formas normales y extensivas de juego utilizando un formalismo gráfico para representar juegos de información imperfecta con un nodo de decisión para las acciones de cada agente y nodos de azar que capturan la información privada de los agentes. Los MAIDs analizan objetivamente el juego, calculando eficientemente el perfil de equilibrio de Nash al explotar la estructura de independencia. NIDs extienden los MAIDs para incluir la incertidumbre de los agentes sobre el juego que se está jugando y sobre los modelos de los otros agentes. Cada modelo es un MAID y la red de MAIDs se colapsa, de abajo hacia arriba, en un solo MAID para calcular el equilibrio del juego teniendo en cuenta los diferentes modelos de cada agente. Los formalismos gráficos como MAIDs y NIDs abren una área de investigación prometedora que tiene como objetivo representar las interacciones multiagente de manera más transparente. Sin embargo, los MAIDs proporcionan un análisis del juego desde un punto de vista externo y la aplicabilidad de ambos está limitada a juegos estáticos de un solo jugador. Los asuntos son más complejos cuando consideramos interacciones que se extienden en el tiempo, donde las predicciones sobre las acciones futuras de otros deben hacerse utilizando modelos que cambian a medida que los agentes actúan y observan. Los I-DIDs abordan esta brecha al permitir la representación de modelos de otros agentes como los valores de un nodo de modelo especial. Tanto los modelos de otros agentes como las creencias originales de los agentes sobre estos modelos se actualizan con el tiempo utilizando implementaciones especializadas. En este documento, mejoramos la representación preliminar previa del I-DID mostrada en [15] utilizando la idea de que el I-ID estático es un tipo de NID. Por lo tanto, podemos utilizar construcciones de lenguaje específicas de NID, como multiplexores, para representar de manera más transparente el nodo del modelo y, posteriormente, el I-ID. Además, aclaramos la semántica del enlace de política de propósito especial introducido en la representación de I-DID por [15], y mostramos que podría ser reemplazado por enlaces de dependencia tradicionales. En la representación previa del I-DID, la actualización de la creencia de los agentes sobre los modelos de los demás a medida que los agentes actúan y reciben observaciones se denotaba utilizando un enlace especial llamado el enlace de actualización del modelo que conectaba los nodos del modelo a lo largo del tiempo. Explicamos la semántica de este enlace mostrando cómo puede ser implementado utilizando los enlaces de dependencia tradicionales entre los nodos de probabilidad que constituyen los nodos del modelo. El resultado final es una representación de I-DID que es significativamente más transparente, semánticamente clara y capaz de ser implementada utilizando los algoritmos estándar para resolver DIDs. Mostramos cómo los IDIDs pueden ser utilizados para modelar la incertidumbre de un agente sobre los modelos de otros, que a su vez pueden ser I-DIDs. La solución al I-DID es una política que prescribe lo que el agente debe hacer con el tiempo, dadas sus creencias sobre el estado físico y otros modelos. Análogos a los DIDs, los I-DIDs pueden ser utilizados para calcular la política de un agente en línea mientras el agente actúa y observa en un entorno poblado por otros agentes interactuantes. 2. ANTECEDENTES: Los IPOMDPS ANIDADOS FINITAMENTE generalizan los POMDPs a entornos multiagentes al incluir los modelos de otros agentes como parte del espacio de estados [9]. Dado que otros agentes también pueden razonar sobre otros, el espacio de estado interactivo está estratégicamente anidado; contiene creencias sobre los modelos de otros agentes y sus creencias sobre los demás. Para simplificar la presentación, consideramos un agente, i, que está interactuando con otro agente, j. Un I-POMDP finitamente anidado del agente i con un nivel de estrategia l se define como la tupla: I-POMDPi,l = ISi,l, A, Ti, Ωi, Oi, Ri donde: • ISi,l denota un conjunto de estados interactivos definido como, ISi,l = S × Mj,l−1, donde Mj,l−1 = {Θj,l−1 ∪ SMj}, para l ≥ 1, e ISi,0 = S, donde S es el conjunto de estados del entorno físico. Θj,l−1 es el conjunto de modelos intencionales computables del agente j: θj,l−1 = bj,l−1, ˆθj donde el marco, ˆθj = A, Ωj, Tj, Oj, Rj, OCj. Aquí, j es Bayesiano racional y OCj es el criterio de optimalidad de j. SMj es el conjunto de modelos subintencionales de j. Ejemplos simples de modelos subintencionales incluyen un modelo sin información [10] y un modelo de juego ficticio [6], ambos de los cuales son independientes de la historia. Damos una construcción recursiva de abajo hacia arriba del espacio de estados interactivo a continuación. Sí,0 = S, Θj,0 = { bj,0, ˆθj | bj,0 ∈ Δ(ISj,0)} Sí,1 = S × {Θj,0 ∪ SMj}, Θj,1 = { bj,1, ˆθj | bj,1 ∈ Δ(ISj,1)} . . . . . . Sí, l = S × {Θj,l−1 ∪ SMj}, Θj,l = { bj,l, ˆθj | bj,l ∈ Δ(ISj,l)} Formulaciones similares de espacios anidados han aparecido en [1, 3]. • A = Ai × Aj es el conjunto de acciones conjuntas de todos los agentes en el entorno; • Ti : S ×A×S → [0, 1], describe el efecto de las acciones conjuntas en los estados físicos del entorno; • Ωi es el conjunto de observaciones del agente i; • Oi : S × A × Ωi → [0, 1] da la probabilidad de las observaciones dadas el estado físico y la acción conjunta; • Ri : ISi × A → R describe las preferencias del agente sobre sus estados interactivos. Normalmente solo importarán los estados físicos. La política del agente es el mapeo, Ω∗ i → Δ(Ai), donde Ω∗ i es el conjunto de todos los historiales de observación del agente i. Dado que la creencia sobre los estados interactivos forma una estadística suficiente [9], la política también puede ser representada como un mapeo del conjunto de todas las creencias del agente i a una distribución sobre sus acciones, Δ(ISi) → Δ(Ai). 2.1 Actualización de Creencias Análogo a los POMDPs, un agente dentro del marco I-POMDP actualiza su creencia a medida que actúa y observa. Sin embargo, hay dos diferencias que complican la actualización de creencias en entornos multiagentes en comparación con los entornos de un solo agente. Primero, dado que el estado del entorno físico depende de las acciones de ambos agentes, la predicción de cómo cambia el estado físico debe hacerse basándose en la predicción de sus acciones. Segundo, los cambios en los modelos de js deben ser incluidos en la actualización de creencias. Específicamente, si j es intencional, entonces se debe incluir una actualización de las creencias de j debido a su acción y observación. En otras palabras, i tiene que actualizar su creencia basándose en su predicción de lo que j observaría y cómo j actualizaría su creencia. Si el modelo de js es subintencional, entonces las observaciones probables de js se añaden al historial de observaciones contenido en el modelo. Formalmente, tenemos: Pr(ist |at−1 i , bt−1 i,l ) = β ISt−1:mt−1 j =θt j bt−1 i,l (ist−1 ) × at−1 j Pr(at−1 j |θt−1 j,l−1)Oi(st , at−1 i , at−1 j , ot i) ×Ti(st−1 , at−1 i , at−1 j , st ) ot j Oj(st , at−1 i , at−1 j , ot j) ×τ(SEθt j (bt−1 j,l−1, at−1 j , ot j) − bt j,l−1) (1) donde β es la constante de normalización, τ es 1 si su argumento es 0, de lo contrario es 0, Pr(at−1 j |θt−1 j,l−1) es la probabilidad de que at−1 j sea Bayes racional para el agente descrito por el modelo θt−1 j,l−1, y SE(·) es una abreviatura para la actualización de creencias. Para una versión de la actualización de creencias cuando el modelo js es subintencional, consulte [9]. Si el agente j también se modela como un I-POMDP, entonces la actualización de creencias invoca la actualización de creencias de j (a través del término SEθt j ( bt−1 j,l−1 , at−1 j , ot j)), lo que a su vez podría invocar la actualización de creencias de is y así sucesivamente. Esta recursión en la anidación de creencias llega al nivel 0. En este nivel, la actualización de creencias del agente se reduce a una actualización de creencias de un POMDP. Para ilustraciones de la actualización de creencias, detalles adicionales sobre I-POMDPs y cómo se comparan con otros marcos multiagentes, consulte [9]. Iteración de Valor Cada estado de creencia en un I-POMDP finitamente anidado tiene un valor asociado que refleja la máxima ganancia que el agente puede esperar en este estado de creencia: Un ( bi,l, θi ) = max ai∈Ai is∈ISi,l ERi(is, ai)bi,l(is)+ γ oi∈Ωi Pr(oi|ai, bi,l)Un−1 ( SEθi (bi,l, ai, oi), θi ) (2) donde, ERi(is, ai) = aj Ri(is, ai, aj)Pr(aj|mj,l−1) (ya que is = (s, mj,l−1)). La ecuación 2 es una base para la iteración de valor en I-POMDPs. La acción óptima del agente, a∗ i, para el caso de horizonte finito con descuento, es un elemento del conjunto de acciones óptimas para el estado de creencia, OPT(θi), definido como: OPT( bi,l, θi ) = argmax ai∈Ai is∈ISi,l ERi(is, ai)bi,l(is) +γ oi∈Ωi Pr(oi|ai, bi,l)Un ( SEθi (bi,l, ai, oi), θi ) (3) 3. Diagramas de influencia interactivos. Una extensión ingenua de los diagramas de influencia (IDs) a entornos poblados por múltiples agentes es posible tratando a los otros agentes como autómatas, representados mediante nodos de probabilidad. Sin embargo, este enfoque asume que las acciones de los agentes están controladas utilizando una distribución de probabilidad que no cambia con el tiempo. Los diagramas de influencia interactivos (I-IDs) adoptan un enfoque más sofisticado al generalizar los IDs para hacerlos aplicables a entornos compartidos con otros agentes que pueden actuar, observar y actualizar sus creencias. 3.1 Sintaxis Además de los nodos habituales de probabilidad, decisión y utilidad, los IIDs incluyen un nuevo tipo de nodo llamado nodo de modelo. Mostramos un nivel general l I-ID en la Fig. 1(a), donde el nodo del modelo (Mj,l−1) está representado con un hexágono. Observamos que la distribución de probabilidad sobre el nodo de probabilidad, S, y el nodo del modelo juntos representan la creencia del agente sobre sus estados interactivos. Además del modelo 1, el modelo de nivel 0 es un POMDP: las acciones de otros agentes se tratan como eventos exógenos y se incorporan en las funciones T, O y R. El Sexto Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) Figura 1: (a) Un nivel genérico l I-ID para el agente i situado con otro agente j. El hexágono es el nodo modelo (Mj,l−1) cuya estructura mostramos en (b). Los miembros del nodo modelo son ellos mismos I-ID (m1 j,l−1, m2 j,l−1; los diagramas no se muestran aquí por simplicidad) cuyos nodos de decisión se asignan a los nodos de probabilidad correspondientes (A1 j , A2 j). Dependiendo del valor del nodo, Mod[Mj], la distribución de cada uno de los nodos de probabilidad se asigna al nodo Aj. (c) El I-ID transformado con el nodo del modelo reemplazado por los nodos de probabilidad y las relaciones entre ellos. Los I-IDs difieren de los IDs al tener un enlace discontinuo (llamado enlace de política en [15]) entre el nodo del modelo y un nodo de probabilidad, Aj, que representa la distribución sobre las acciones de los otros agentes dada su modelo. En ausencia de otros agentes, el nodo del modelo y el nodo de probabilidad, Aj, desaparecen y los I-ID se convierten en ID tradicionales. El nodo del modelo contiene los modelos computacionales alternativos atribuidos por i al otro agente del conjunto, Θj,l−1 ∪ SMj, donde Θj,l−1 y SMj fueron definidos previamente en la Sección 2. Por lo tanto, un modelo en el nodo del modelo puede ser en sí mismo un I-ID o ID, y la recursión termina cuando un modelo es un ID o subintencional. Dado que el nodo del modelo contiene los modelos alternativos del otro agente como sus valores, su representación no es trivial. En particular, algunos de los modelos dentro del nodo son I-IDs que, al resolverse, generan la política óptima de los agentes en sus nodos de decisión. Cada nodo de decisión se asigna al nodo de probabilidad correspondiente, digamos A1 j, de la siguiente manera: si OPT es el conjunto de acciones óptimas obtenidas al resolver el I-ID (o ID), entonces Pr(aj ∈ A1 j) = 1 |OPT| si aj ∈ OPT, 0 en caso contrario. Tomando prestados los conocimientos de trabajos anteriores [8], observamos que el nodo del modelo y el enlace de política discontinua que lo conecta con el nodo de probabilidad, Aj, podrían representarse como se muestra en la Fig. 1(b). El nodo de decisión de cada nivel l − 1 I-ID se transforma en un nodo de probabilidad, como mencionamos anteriormente, de modo que las acciones con el mayor valor en el nodo de decisión se les asignan probabilidades uniformes en el nodo de probabilidad, mientras que al resto se les asigna probabilidad cero. Los diferentes nodos de probabilidad (A1 j , A2 j ), uno para cada modelo, y adicionalmente, el nodo de probabilidad etiquetado Mod[Mj] forman los padres del nodo de probabilidad, Aj. Por lo tanto, hay tantos nodos de acción (A1 j , A2 j ) en Mj,l−1 como el número de modelos en el soporte de las creencias del agente. La tabla de probabilidad condicional del nodo de probabilidad, Aj, es un multiplexor que asume la distribución de cada uno de los nodos de acción (A1 j , A2 j ) dependiendo del valor de Mod[Mj]. Los valores de Mod[Mj] denotan los diferentes modelos de j. En otras palabras, cuando Mod[Mj] tiene el valor m1 j,l−1, el nodo de probabilidad Aj asume la distribución del nodo A1 j, y Aj asume la distribución de A2 j cuando Mod[Mj] tiene el valor m2 j,l−1. La distribución sobre el nodo, Mod[Mj], es la creencia del agente sobre los modelos de j dado un estado físico. Para más agentes, tendremos tantos nodos de modelo como agentes haya. Observa que la Fig. 1(b) aclara la semántica del enlace de política y muestra cómo puede ser representado utilizando los enlaces de dependencia tradicionales. En la Fig. 1(c), mostramos el I-ID transformado cuando el nodo del modelo es reemplazado por los nodos de probabilidad y las relaciones entre ellos. A diferencia de la representación en [15], no hay enlaces de políticas especiales, sino que el I-ID está compuesto únicamente por aquellos tipos de nodos que se encuentran en IDs tradicionales y relaciones de dependencia entre los nodos. Esto permite que los I-IDs sean representados e implementados utilizando herramientas de aplicación convencionales que apuntan a los IDs. Ten en cuenta que podemos ver el nivel l I-ID como un NID. Específicamente, cada uno de los modelos del nivel l − 1 dentro del nodo del modelo son bloques en el NID (ver Fig. 2). Si el nivel l = 1, cada bloque es un ID tradicional, de lo contrario, si l > 1, cada bloque dentro del NID puede ser un NID en sí mismo. Ten en cuenta que dentro de los I-IDs (o IDs) en cada nivel, solo hay un nodo de decisión único. Por lo tanto, nuestro NID no contiene ningún MAID. Figura 2: Un nivel l I-ID representado como un NID. Las probabilidades asignadas a los bloques del NID son creencias sobre modelos js condicionados a un estado físico. 3.2 Solución La solución de un I-ID procede de manera ascendente y se implementa de forma recursiva. Comenzamos resolviendo los modelos de nivel 0, que, si son intencionales, son identificadores tradicionales. Sus soluciones proporcionan distribuciones de probabilidad sobre las acciones de los otros agentes, las cuales se ingresan en los nodos de probabilidad correspondientes encontrados en el nodo del modelo del nivel 1 I-ID. El mapeo de los nodos de decisión de los modelos de nivel 0 a los nodos de probabilidad se realiza de manera que las acciones con el mayor valor en el nodo de decisión se les asignan probabilidades uniformes en el nodo de probabilidad, mientras que el resto se les asigna probabilidad cero. Dadas las distribuciones sobre las acciones dentro de los diferentes nodos de probabilidad (uno para cada modelo del otro agente), el I-ID de nivel 1 se transforma como se muestra en la Fig. 1(c). Durante la transformación, la tabla de probabilidad condicional (CPT) del nodo, Aj, se llena de tal manera que el nodo asume la distribución de cada uno de los nodos de probabilidad dependiendo del valor del nodo, Mod[Mj]. Como mencionamos anteriormente, los valores del nodo Mod[Mj] denotan los diferentes modelos del otro agente, y su distribución es la creencia del agente sobre los modelos de j condicionados al estado físico. El nivel 1 I-ID transformado es un ID tradicional que puede ser resuelto como us816 The Sixth Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) (a) (b) Figura 3: (a) Un I-DID genérico de dos niveles de tiempo para el agente i en un entorno con otro agente j. Observa el enlace de actualización del modelo punteado que denota la actualización de los modelos de j y la distribución de los modelos a lo largo del tiempo. (b) La semántica del enlace de actualización del modelo utilizando el método estándar de maximización de utilidad esperada [18]. Este procedimiento se lleva a cabo hasta el nivel l I-ID cuya solución proporciona el conjunto no vacío de acciones óptimas que el agente debe realizar dada su creencia. Ten en cuenta que, al igual que los IDs, los I-IDs son adecuados para la toma de decisiones en línea cuando se conoce la creencia actual de los agentes. 4. Diagramas de influencia dinámica interactivos (I-DIDs) Los diagramas de influencia dinámica interactivos (I-DIDs) extienden los I-IDs (y NIDs) para permitir la <br>toma de decisiones secuencial</br> a lo largo de varios pasos de tiempo. Así como los DIDs son representaciones gráficas estructuradas de POMDPs, los I-DIDs son los análogos gráficos en línea para I-POMDPs finitamente anidados. Los I-DIDs pueden ser utilizados para optimizar sobre una mirada anticipada finita dadas las creencias iniciales mientras se interactúa con otros agentes, posiblemente similares. 4.1 Sintaxis Representamos un I-DID de dos cortes temporales en la Fig. 3(a). Además de los nodos del modelo y el enlace de política discontinuo, lo que diferencia a un I-DID de un DID es el enlace de actualización del modelo mostrado como una flecha punteada en la Fig. 3(a). Explicamos la semántica del nodo del modelo y el enlace de la política en la sección anterior; a continuación, describimos las actualizaciones del modelo. La actualización del nodo del modelo con el tiempo implica dos pasos: primero, dados los modelos en el tiempo t, identificamos el conjunto actualizado de modelos que residen en el nodo del modelo en el tiempo t + 1. Recuerda de la Sección 2 que el modelo intencional de un agente incluye sus creencias. Debido a que los agentes actúan y reciben observaciones, sus modelos se actualizan para reflejar sus creencias cambiadas. Dado que el conjunto de acciones óptimas para un modelo podría incluir todas las acciones, y el agente podría recibir cualquiera de las |Ωj| posibles observaciones, el conjunto actualizado en el paso de tiempo t + 1 tendrá como máximo |Mt j,l−1||Aj||Ωj| modelos. Aquí, |Mt j,l−1| es el número de modelos en el paso de tiempo t, |Aj| y |Ωj| son los espacios más grandes de acciones y observaciones respectivamente, entre todos los modelos. Segundo, calculamos la nueva distribución sobre los modelos actualizados dados la distribución original y la probabilidad de que el agente realice la acción y reciba la observación que llevó al modelo actualizado. Estos pasos son parte de la actualización de creencias del agente formalizada utilizando la Ecuación 1. En la Fig. 3(b), mostramos cómo se implementa el enlace de actualización del modelo punteado en el I-DID. Si cada uno de los dos modelos de nivel l − 1 atribuidos a j en el paso de tiempo t resulta en una acción, y j podría hacer una de dos posibles observaciones, entonces el nodo del modelo en el paso de tiempo t + 1 contiene cuatro modelos actualizados (mt+1,1 j,l−1 ,mt+1,2 j,l−1 , mt+1,3 j,l−1 , y mt+1,4 j,l−1 ). Estos modelos difieren en sus creencias iniciales, cada una de las cuales es el resultado de j actualizar sus creencias debido a su acción y una posible observación. Los nodos de decisión en cada uno de los I-DIDs o DIDs que representan los modelos de nivel inferior se asignan a la Figura 4 correspondiente: I-DID transformado con los nodos del modelo y el enlace de actualización del modelo reemplazados por los nodos de probabilidad y las relaciones (en negrita) de los nodos de probabilidad, como se mencionó anteriormente. A continuación, describimos cómo se calcula la distribución sobre el conjunto actualizado de modelos (la distribución sobre el nodo de probabilidad Mod[Mt+1 j ] en Mt+1 j,l−1). La probabilidad de que el modelo actualizado js sea, digamos mt+1,1 j,l−1, depende de la probabilidad de que j realice la acción y reciba la observación que condujo a este modelo, y de la distribución previa sobre los modelos en el paso de tiempo t. Dado que el nodo de probabilidad At j asume la distribución de cada uno de los nodos de acción basados en el valor de Mod[Mt j], la probabilidad de la acción está dada por este nodo de probabilidad. Para obtener la probabilidad de una observación posible js, introducimos el nodo de probabilidad Oj, que dependiendo del valor de Mod[Mt j], asume la distribución del nodo de observación en el modelo de nivel inferior denominado Mod[Mt j]. Dado que la probabilidad de las observaciones js depende del estado físico y de las acciones conjuntas de ambos agentes, el nodo Oj está vinculado con St+1, At j y At i. De manera análoga a At j, la tabla de probabilidad condicional de Oj también es un multiplexor modulado por Mod[Mt j]. Finalmente, la distribución de los modelos previos en el tiempo t se obtiene a partir del nodo de probabilidad, Mod[Mt j ] en Mt j,l−1. Por consiguiente, los nodos de probabilidad, Mod[Mt j], At j y Oj, forman los padres de Mod[Mt+1 j] en Mt+1 j,l−1. Ten en cuenta que el enlace de actualización del modelo puede ser reemplazado por los enlaces de dependencia entre los nodos de probabilidad que constituyen los nodos del modelo en las dos etapas temporales. En la Fig. 4 mostramos los dos cortes temporales I-DID con los nodos del modelo reemplazados por los nodos de probabilidad y las relaciones entre ellos. Los nodos de probabilidad y los enlaces de dependencia que no están en negrita son estándar, generalmente encontrados en DIDs. La expansión del I-DID a lo largo de más pasos de tiempo requiere la repetición de los dos pasos de actualizar el conjunto de modelos que forman el 2. Nota que Oj representa la observación j en el tiempo t + 1. El Sexto Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 817 valores del nodo del modelo y añadiendo las relaciones entre los nodos de probabilidad, tantas veces como enlaces de actualización del modelo haya. Observamos que el conjunto posible de modelos del otro agente j crece exponencialmente con el número de pasos de tiempo. Por ejemplo, después de T pasos, puede haber como máximo |Mt=1 j,l−1|(|Aj||Ωj|)T −1 modelos candidatos residiendo en el nodo del modelo. 4.2 Solución Análoga a los I-ID, la solución a un I-DID de nivel l para el agente i expandido a lo largo de T pasos de tiempo puede llevarse a cabo de forma recursiva. Con el propósito de ilustración, dejemos l=1 y T=2. El método de solución utiliza la técnica estándar de anticipación, proyectando las secuencias de acciones y observaciones de los agentes hacia adelante desde el estado de creencia actual [17], y encontrando las posibles creencias que podría tener en el siguiente paso temporal. Dado que el agente i también tiene una creencia sobre los modelos de j, la búsqueda anticipada incluye descubrir los posibles modelos que j podría tener en el futuro. Por consiguiente, cada uno de los modelos subintencionales o de nivel 0 de js (representados utilizando un DID estándar) en el primer paso de tiempo debe resolverse para obtener su conjunto óptimo de acciones. Estas acciones se combinan con el conjunto de observaciones posibles que j podría hacer en ese modelo, lo que resulta en un conjunto actualizado de modelos candidatos (que incluyen las creencias actualizadas) que podrían describir el comportamiento de j. Las creencias sobre este conjunto actualizado de modelos candidatos se calculan utilizando los métodos estándar de inferencia que utilizan las relaciones de dependencia entre los nodos del modelo, como se muestra en la Figura 3(b). Observamos la naturaleza recursiva de esta solución: al resolver el agente de nivel 1 I-DID, los DIDs de nivel 0 deben ser resueltos. Si el anidamiento de los modelos es más profundo, todos los modelos en todos los niveles, comenzando desde el nivel 0, se resuelven de manera ascendente. Breve descripción del algoritmo recursivo para resolver el agente es el Algoritmo para resolver I-DID. Entrada: nivel l ≥ 1 I-ID o nivel 0 ID, T Fase de Expansión 1. Para t desde 1 hasta T − 1, hacer 2. Si l ≥ 1, entonces llenar Mt+1 j,l−1 3. Para cada mt j en el rango (Mt j,l−1) hacer 4. Llame recursivamente al algoritmo con el l - 1 I-ID (o ID) que representa mt j y el horizonte, T - t + 1 5. Mapea el nodo de decisión del I-ID (o ID) resuelto, OPT(mt j), a un nodo de probabilidad Aj 6. Para cada aj en OPT(mt j) hacer 7. Para cada oj en Oj (parte de mt j) hacer 8. Actualizar la creencia js, bt+1 j ← SE(bt j, aj, oj) 9. mt+1 j ← Nuevo I-ID (o ID) con bt+1 j como la creencia inicial 10. Rango(Mt+1 j,l−1) ∪ ← {mt+1 j } 11. Añadir el nodo del modelo, Mt+1 j,l−1, y los enlaces de dependencia entre Mt j,l−1 y Mt+1 j,l−1 (mostrados en la Fig. 3(b)) 12. Agregue los nodos de probabilidad, decisión y utilidad para la franja de tiempo t + 1 y los enlaces de dependencia entre ellos 13. Establezca las CPTs para cada nodo de probabilidad y nodo de utilidad de la Fase de Mirada Adelante 14. Aplica el método estándar de anticipación y respaldo para resolver la Figura 5 expandida de I-DID: Algoritmo para resolver un nivel l ≥ 0 I-DID. Nivel l I-DID expandido durante T pasos de tiempo con otro agente j en la Figura 5. Adoptamos un enfoque de dos fases: Dado un I-ID de nivel l (descrito previamente en la Sección 3) con todos los modelos de niveles inferiores representados también como I-IDs o IDs (si es nivel 0), el primer paso es expandir el I-ID de nivel l a lo largo de T pasos de tiempo añadiendo los enlaces de dependencia y las tablas de probabilidad condicional para cada nodo. Nos enfocamos especialmente en establecer y poblar los nodos del modelo (líneas 3-11). Ten en cuenta que Range(·) devuelve los valores (modelos de nivel inferior) de la variable aleatoria dada como entrada (nodo del modelo). En la segunda fase, utilizamos una técnica estándar de anticipación proyectando las secuencias de acción y observación durante T pasos de tiempo en el futuro, y respaldando los valores de utilidad de las creencias alcanzables. Similar a los I-IDs, los I-DIDs se reducen a DIDs en ausencia de otros agentes. Como mencionamos anteriormente, los modelos de nivel 0 son los DIDs tradicionales. Sus soluciones proporcionan distribuciones de probabilidad sobre las acciones del agente modelado en ese nivel a los I-DIDs en el nivel 1. Dadas las distribuciones de probabilidad sobre las acciones de otros agentes, los IDIDs de nivel 1 pueden resolverse como DIDs y proporcionar distribuciones de probabilidad a modelos de niveles superiores. Suponga que el número de modelos considerados en cada nivel está limitado por un número, M. Resolver un I-DID de nivel l es equivalente a resolver O(Ml) DIDs. Para ilustrar la utilidad de los I-DIDs, los aplicamos a tres dominios de problemas. Describimos, en particular, la formulación del I-DID y las prescripciones óptimas obtenidas al resolverlo. 5.1 Seguimiento-Liderazgo en el Problema del Tigre Multiagente Comenzamos nuestras ilustraciones del uso de I-IDs e I-DIDs con una versión ligeramente modificada del problema del tigre multiagente discutido en [9]. El problema tiene dos agentes, cada uno de los cuales puede abrir la puerta derecha (OR), la puerta izquierda (OL) o escuchar (L). Además de escuchar gruñidos (desde la izquierda (GL) o desde la derecha (GR)) cuando escuchan, los agentes también escuchan chirridos (desde la izquierda (CL), desde la derecha (CR) o sin chirridos (S)), que indican ruidosamente que los otros agentes están abriendo una de las puertas. Cuando se abre cualquier puerta, el tigre persiste en su ubicación original con una probabilidad del 95%. El agente i escucha gruñidos con una fiabilidad del 65% y crujidos con una fiabilidad del 95%. El agente J, por otro lado, escucha gruñidos con una fiabilidad del 95%. Por lo tanto, la configuración es tal que el agente i escucha la apertura de puertas del agente j de manera más confiable que los rugidos de los tigres. Esto sugiere que podría usar acciones de JavaScript como indicación de la ubicación del tigre, como discutimos a continuación. Las preferencias de cada agente son como en el juego de un solo agente discutido en [13]. Las funciones de transición, observación y recompensa se muestran en [16]. Un buen indicador de la utilidad de los métodos normativos para la toma de decisiones como los I-DIDs es la aparición de comportamientos sociales realistas en sus prescripciones. En entornos del persistente problema del tigre multiagente que reflejan situaciones del mundo real, demostramos la seguidismo entre los agentes y, como se muestra en [15], el engaño entre agentes que creen que están en una relación de tipo seguidor-líder. En particular, analizamos las condiciones situacionales y epistemológicas suficientes para su surgimiento. El comportamiento de seguidores, por ejemplo, resulta del agente conociendo sus propias debilidades, evaluando las fortalezas, preferencias y posibles comportamientos del otro, y dándose cuenta de que es mejor para él seguir las acciones de los demás para maximizar sus ganancias. Consideremos una configuración particular del problema del tigre en la que el agente i cree que las preferencias de j están alineadas con las suyas: ambos solo quieren obtener el oro, y la audición de j es más confiable en comparación consigo mismo. Como ejemplo, supongamos que j, al escuchar, puede discernir la ubicación del tigre el 95% de las veces en comparación con su precisión del 65%. Además, el agente i no tiene ninguna información inicial sobre la ubicación de los tigres. En otras palabras, la creencia anidada de un solo nivel, bi,1, asigna 0.5 a cada una de las dos ubicaciones del tigre. Además, considera dos modelos de j, que difieren en los niveles iniciales de creencias planas de j. Esto se representa en el nivel 1 I-ID mostrado en la Fig. 6(a). Según un modelo, j asigna una probabilidad de 0.9 a que el tigre esté detrás de la puerta izquierda, mientras que el otro 818 The Sixth Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) Figura 6: (a) Nivel 1 I-ID del agente i, (b) dos IDs de nivel 0 del agente j cuyos nodos de decisión están mapeados a los nodos de probabilidad, A1 j , A2 j , en (a). El modelo asigna 0.1 a esa ubicación (ver Fig. 6(b)). El agente i está indeciso sobre estos dos modelos de j. Si variamos la capacidad auditiva, y resolvemos el nivel 1 correspondiente I-ID expandido en tres pasos de tiempo, obtenemos las políticas de comportamiento normativas mostradas en la Figura 7 que exhiben comportamiento de seguidores. Si la probabilidad de escuchar correctamente los gruñidos es de 0.65, entonces, como se muestra en la política en la Fig. 7(a), i comienza a seguir condicionalmente las acciones de j: i abre la misma puerta que j abrió anteriormente si su evaluación propia de la ubicación de los tigres confirma la elección de j. Si i pierde la capacidad de interpretar correctamente los gruñidos por completo, sigue ciegamente a j y abre la misma puerta que j abrió anteriormente (Fig. 7(b)). Figura 7: Emergencia de (a) seguidores condicionales, y (b) seguidores ciegos en el problema del tigre. Los comportamientos de interés están en negrita. * es un comodín y representa cualquiera de las observaciones. Observamos que un solo nivel de anidación de creencias, creencias sobre los modelos de los demás, fue suficiente para que surgiera el seguidismo en el problema del tigre. Sin embargo, los requisitos epistemológicos para el surgimiento del liderazgo son más complejos. Para que un agente, digamos j, surja como líder, primero debe surgir el seguidismo en el otro agente i. Como mencionamos anteriormente, si i está seguro de que sus preferencias son idénticas a las de j, y cree que j tiene un mejor sentido del oído, i seguirá las acciones de j con el tiempo. El agente j emerge como líder si cree que lo seguiré, lo que implica que la creencia de j debe estar anidada dos niveles de profundidad para permitirle reconocer su papel de liderazgo. Darse cuenta de que lo seguiré presenta a J una oportunidad para influir en sus acciones en beneficio del bien colectivo o de su interés propio solamente. Por ejemplo, en el problema del tigre, consideremos una situación en la que si tanto i como j abren la puerta correcta, entonces cada uno recibe un pago de 20 que es el doble del original. Si solo j selecciona la puerta correcta, obtiene la recompensa de 10. Por otro lado, si ambos agentes eligen la puerta incorrecta, sus penalizaciones se reducen a la mitad. En este entorno, es tanto en el mejor interés de j como en el beneficio colectivo que utilice su experiencia para seleccionar la puerta correcta, y así ser un buen líder. Sin embargo, considera un problema ligeramente diferente en el que j se beneficia de la pérdida de i y es penalizado si i se beneficia. Específicamente, dejemos que la ganancia se reste de js, indicando que j es antagonista hacia i: si j elige la puerta correcta y i la incorrecta, entonces la pérdida de 100 se convierte en la ganancia de js. El agente J cree que yo incorrectamente pienso que las preferencias de J son aquellas que promueven el bien colectivo y que comienza creyendo con un 99% de confianza dónde está el tigre. Dado que creo que mis preferencias son similares a las de j, y que j comienza creyendo casi con certeza que una de las dos ubicaciones es la correcta (dos modelos de nivel 0 de j), comenzaré siguiendo las acciones de j. Mostramos la política normativa para resolver su I-DID anidado individualmente durante tres pasos de tiempo en la Fig. 8(a). La política demuestra que seguiré ciegamente las acciones de js. Dado que el tigre persiste en su ubicación original con una probabilidad del 0.95, seleccionaré nuevamente la misma puerta. Si j comienza el juego con una probabilidad del 99% de que el tigre esté a la derecha, resolver su I-DID anidado dos niveles profundamente, resulta en la política mostrada en la Fig. 8(b). Aunque j está casi seguro de que OL es la acción correcta, comenzará seleccionando OR, seguido de OL. La intención del agente js es engañar a i, a quien cree que seguirá las acciones de js, para así obtener $110 en el segundo paso de tiempo, lo cual es más de lo que j ganaría si fuera honesto. Figura 8: Emergencia del engaño entre agentes en el problema del tigre. Los comportamientos de interés están en negrita. * denota como antes. (a) El agente es una política que demuestra que seguirá ciegamente las acciones de js. (b) Aunque j está casi seguro de que el tigre está a la derecha, comenzará seleccionando OR, seguido de OL, para engañar a i. 5.2 Altruismo y reciprocidad en el problema del bien público El problema del bien público (PG) [7], consiste en un grupo de M agentes, cada uno de los cuales debe contribuir con algún recurso a una olla pública o guardarlo para sí mismos. Dado que los recursos aportados al fondo público se comparten entre todos los agentes, son menos valiosos para el agente cuando están en el fondo público. Sin embargo, si todos los agentes eligen contribuir con sus recursos, entonces la recompensa para cada agente es mayor que si nadie contribuye. Dado que un agente recibe su parte del bote público independientemente de si ha contribuido o no, la acción dominante es que cada agente no contribuya y en su lugar se aproveche de las contribuciones de los demás. Sin embargo, los comportamientos de los jugadores humanos en simulaciones empíricas del problema de PG difieren de las predicciones normativas. Los experimentos revelan que muchos jugadores inicialmente contribuyen con una gran cantidad al bote público, y continúan contribuyendo cuando se juega el problema del PG repetidamente, aunque en cantidades decrecientes [4]. Muchos de estos experimentos [5] informan que un pequeño grupo central de jugadores contribuye persistentemente al bote público incluso cuando todos los demás están desertando. Estos experimentos también revelan que los jugadores que contribuyen persistentemente tienen preferencias altruistas o recíprocas que coinciden con la cooperación esperada de los demás. Para simplificar, asumimos que el juego se juega entre M = 2 agentes, i y j. Que cada agente esté inicialmente dotado con una cantidad de recursos XT. Mientras que la formulación clásica del juego PG permite que cada agente contribuya con cualquier cantidad de recursos (≤ XT) al bote público, simplificamos el espacio de acciones al permitir dos acciones posibles. Cada agente puede elegir contribuir (C) una cantidad fija de recursos, o no contribuir. La última acción es deThe Sixth Intl. La Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 819 fue señalada como defectuosa (D). Suponemos que las acciones no son observables por otros. El valor de los recursos en la bolsa pública se descuenta por ci para cada agente i, donde ci es el retorno privado marginal. Suponemos que ci < 1 para que el agente no se beneficie lo suficiente como para contribuir al bote público en beneficio propio. Simultáneamente, ciM > 1, lo que hace que la contribución colectiva sea óptima de Pareto. i/j C D C 2ciXT , 2cjXT ciXT − cp, XT + cjXT − P D XT + ciXT − P, cjXT − cp XT , XT Tabla 1: El juego PG de un solo disparo con castigo. Para fomentar las contribuciones, los agentes contribuyentes castigan a los que no colaboran pero incurren en un pequeño costo por administrar el castigo. Sea P la sanción impuesta al agente que se desvía y cp el costo no nulo de castigar al agente contribuyente. Para simplificar, asumimos que el costo de castigar es el mismo para ambos agentes. El juego PG de un solo disparo con castigo se muestra en la Tabla 1. Si ci = cj, cp > 0, y si P > XT − ciXT, entonces la deserción ya no es una acción dominante. Si P < XT − ciXT, entonces la deserción es la acción dominante para ambos. Si P = XT − ciXT, entonces el juego no es resoluble por dominancia. Figura 9: (a) Nivel 1 I-ID del agente i, (b) IDs de nivel 0 del agente j con nodos de decisión mapeados a los nodos de probabilidad, A1 j y A2 j, en (a). Formulamos una versión secuencial del problema PG con castigo desde la perspectiva del agente i. Aunque en el juego de PG repetido, la cantidad en la olla pública se revela a todos los agentes después de cada ronda de acciones, asumimos en nuestra formulación que está oculta para los agentes. Cada agente puede contribuir con una cantidad fija, xc, o desertar. Un agente al realizar una acción recibe una observación de abundante (PY) o escasa (MR) simbolizando el estado de la olla pública. Ten en cuenta que las observaciones también son indirectamente indicativas de las acciones del agente js porque el estado del bote público es influenciado por ellas. La cantidad de recursos en el agente es una olla privada, es perfectamente observable para i. Los pagos son análogos a la Tabla 1. Tomando prestado de las investigaciones empíricas del problema PG [5], construimos IDs de nivel 0 para j que modelan tipos altruistas y no altruistas (Fig. 9(b)). Específicamente, nuestro agente altruista tiene un alto retorno privado marginal (cj está cerca de 1) y no castiga a los demás que incumplen. Que xc = 1 y el agente de nivel 0 sea castigado la mitad de las veces que se desvía. Con una acción restante, ambos tipos de agentes eligen contribuir para evitar ser castigados. Con dos acciones por delante, el tipo altruista elige contribuir, mientras que el otro defecta. Esto se debe a que cj para el tipo altruista es cercano a 1, por lo tanto, el castigo esperado, 0.5P > (1 − cj), que el tipo altruista evita. Debido a que cj para el tipo no altruista es menor, prefiere no contribuir. Con tres pasos por delante, el agente altruista contribuye para evitar el castigo (0.5P > 2(1 − cj)), mientras que el tipo no altruista se desentiende. Para más de tres pasos, mientras el agente altruista continúa contribuyendo al bote público dependiendo de qué tan cercano esté su retorno privado marginal a 1, el tipo no altruista prescribe la deserción. Analizamos las decisiones de un agente altruista i modelado usando un nivel 1 I-DID expandido en 3 pasos de tiempo. i atribuye los dos modelos de nivel 0, mencionados anteriormente, a j (ver Fig. 9). Si creo con una probabilidad de 1 que j es altruista, elijo contribuir en cada uno de los tres pasos. Este comportamiento persiste cuando i no está al tanto de si j es altruista (Fig. 10(a)), y cuando i asigna una alta probabilidad a que j sea del tipo no altruista. Sin embargo, cuando i cree con una probabilidad de 1 que j no es altruista y por lo tanto seguramente va a traicionar, i elige traicionar para evitar ser castigado y porque su beneficio privado marginal es menor que 1. Estos resultados demuestran que el comportamiento de nuestro tipo altruista se asemeja al encontrado experimentalmente. El agente de nivel 1 no altruista elige desertar independientemente de lo altruista que crea que sea el otro agente. Analizamos el comportamiento de un tipo de agente recíproco que se ajusta a la cooperación o defección esperada. El retorno privado marginal del tipo recíproco es similar al del tipo no altruista, sin embargo, obtiene una mayor recompensa cuando su acción es similar a la del otro. Consideramos el caso en el que el agente recíproco i no está seguro de si j es altruista y cree que es probable que el bote público esté medio lleno. Para esta creencia previa, yo elijo defectar. Al recibir una observación de abundancia, decide contribuir, mientras que una observación de escasez lo hace defectar (Fig. 10(b)). Esto se debe a que una observación de abundancia indica que es probable que la olla esté más llena que a la mitad, lo cual resulta de la acción de js para contribuir. Por lo tanto, entre los dos modelos atribuidos a j, es probable que su tipo sea altruista, lo que hace probable que j contribuya nuevamente en el próximo paso de tiempo. El agente i, por lo tanto, elige contribuir para corresponder la acción de js. Un razonamiento análogo lleva a uno a defectar cuando observa una olla escasa. Con una acción por hacer, creo que si j contribuye, también elegiré contribuir para evitar castigo independientemente de sus observaciones. Figura 10: (a) Un agente altruista de nivel 1 siempre contribuye. (b) Un agente recíproco i comienza defraudando y luego elige contribuir o defraudar basándose en su observación de abundancia (indicando que j es probablemente altruista) o escasez (j no es altruista). 5.3 Estrategias en el póker de dos jugadores El póker es un popular juego de cartas de suma cero que ha recibido mucha atención en la comunidad de investigación de IA como un banco de pruebas [2]. El póker se juega entre M ≥ 2 jugadores, en el que cada jugador recibe una mano de cartas de una baraja. Si bien existen varias variantes de póker con diferentes niveles de complejidad, consideramos una versión simple en la que cada jugador tiene tres turnos durante los cuales el jugador puede intercambiar una carta (E), mantener la mano existente (K), retirarse (F) y abandonar el juego, o apostar (C), lo que requiere que todos los jugadores muestren sus manos. Para mantener las cosas simples, dejemos que M = 2, y que cada jugador reciba una mano compuesta por una sola carta sacada del mismo palo. Por lo tanto, durante un enfrentamiento, el jugador que tenga la carta numéricamente más alta (el 2 es el más bajo, el as es el más alto) gana el bote. Durante un intercambio de cartas, la carta descartada se coloca ya sea en la pila L, indicando al otro agente que era una carta de número bajo menor que 8, o en la 820 The Sixth Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) H, indicando que la carta tenía un rango mayor o igual a 8. Ten en cuenta que, por ejemplo, si se descarta una carta de número bajo, la probabilidad de recibir una carta baja a cambio se reduce. Mostramos el nivel 1 I-ID para el Poker simplificado de dos jugadores en la Fig. 11. Consideramos dos modelos (tipos de personalidad) del agente j. El tipo conservador cree que es probable que su oponente tenga una carta de alto número en su mano. Por otro lado, el agente agresivo j cree con alta probabilidad que su oponente tiene una carta de número más bajo. Por lo tanto, los dos tipos difieren en sus creencias sobre la mano de sus oponentes. En ambos estos modelos de nivel 0, se asume que el oponente realiza sus acciones siguiendo una distribución fija y uniforme. Con tres acciones restantes, independientemente de su mano (a menos que sea un as), el agente agresivo elige intercambiar su carta, con la intención de mejorar su mano actual. Esto se debe a que cree que el otro tiene una carta baja, lo que mejora sus posibilidades de obtener una carta alta durante el intercambio. El agente conservador elige quedarse con su carta, sin importar su mano, porque considera que sus posibilidades de obtener una carta alta son escasas, ya que cree que su oponente tiene una. Figura 11: (a) Nivel 1 I-ID del agente i. La observación revela información sobre la mano de js del paso de tiempo anterior, (b) IDs de nivel 0 del agente j cuyos nodos de decisión están mapeados a los nodos de probabilidad, A1 j , A2 j , en (a). La política de un agente de nivel 1 i que cree que cada carta, excepto la suya, tiene la misma probabilidad de estar en su mano (tipo de personalidad neutral) y j podría ser de tipo agresivo o conservador, se muestra en la Figura 12. Su mano contiene la carta numerada 8. El agente comienza por mantener su carta. Al ver que j no intercambió una carta (N), i cree con probabilidad 1 que j es conservador y, por lo tanto, mantendrá sus cartas. i responde manteniendo su carta o intercambiándola porque j tiene igual probabilidad de tener una carta más baja o más alta. Si observo que j descartó su carta en la pila L o H, creo que j es agresivo. Al observar a L, i se da cuenta de que j tenía una carta baja, y es probable que tenga una carta alta después de su intercambio. Dado que la probabilidad de recibir una carta baja es alta en este momento, i decide quedarse con su carta. Al observar la carta H, creyendo que la probabilidad de recibir una carta de número alto es alta, i decide intercambiar su carta. En el paso final, i decide llamar independientemente de su historial de observaciones porque su creencia de que j tiene una carta más alta no es lo suficientemente alta como para concluir que es mejor retirarse y renunciar al pago. Esto se debe en parte al hecho de que una observación de, digamos, L, restablece las creencias del agente en el paso de tiempo anterior sobre las cartas de números bajos solamente. 6. DISCUSIÓN Mostramos cómo los DIDs pueden ser extendidos a los I-DIDs que permiten la <br>toma de decisiones secuenciales</br> en línea en entornos multiagentes inciertos. Nuestra representación gráfica de I-DIDs mejora la Figura 12 anterior: Un agente de nivel 1 es una política de tres pasos en el problema de Poker. i comienza creyendo que j tiene la misma probabilidad de ser agresivo o conservador y podría tener cualquier carta en su mano con igual probabilidad. Trabaja significativamente al ser más transparente, semánticamente claro y capaz de ser resuelto utilizando algoritmos estándar que apuntan a los DIDs. Los I-DIDs extienden los NIDs para permitir la <br>toma de decisiones secuenciales</br> a lo largo de múltiples pasos de tiempo en presencia de otros agentes que interactúan. Los I-DIDs pueden ser vistos como representaciones gráficas concisas para IPOMDPs que proporcionan una forma de explotar la estructura del problema y llevar a cabo la toma de decisiones en línea a medida que el agente actúa y observa dadas sus creencias previas. Actualmente estamos investigando formas de resolver I-DIDs aproximadamente con límites demostrables en la calidad de la solución. Agradecimiento: Agradecemos a Piotr Gmytrasiewicz por algunas discusiones útiles relacionadas con este trabajo. El primer autor desea agradecer el apoyo de una beca UGARF. 7. REFERENCIAS [1] R. J. Aumann. Epistemología interactiva i: Conocimiento. Revista Internacional de Teoría de Juegos, 28:263-300, 1999. [2] D. Billings, A. Davidson, J. Schaeffer y D. Szafron. El desafío del póker. AIJ, 2001. [3] A. Brandenburger y E. Dekel. Jerarquías de creencias y conocimiento común. Revista de Teoría Económica, 59:189-198, 1993. [4] C. Camerer. Teoría de juegos conductual: Experimentos en interacción estratégica. Princeton University Press, 2003. [5] E. Fehr y S. Gachter. Cooperación y castigo en experimentos de bienes públicos. Revista Económica Americana, 90(4):980-994, 2000. [6] D. Fudenberg y D. K. Levine. La Teoría del Aprendizaje en los Juegos. MIT Press, 1998. [7] D. Fudenberg y J. Tirole. Teoría de juegos. MIT Press, 1991. [8] Y. Gal y A. Pfeffer. Un lenguaje para modelar los procesos de toma de decisiones de agentes en juegos. En AAMAS, 2003. [9] P. Gmytrasiewicz y P. Doshi. Un marco para la planificación secuencial en entornos multiagentes. JAIR, 24:49-79, 2005. [10] P. Gmytrasiewicz y E. Durfee. Coordinación racional en entornos multiagente. JAAMAS, 3(4):319-350, 2000. [11] J. C. Harsanyi. Juegos con información incompleta jugados por jugadores bayesianos. Ciencia de la Gestión, 14(3):159-182, 1967. [12] R. A. Howard y J. E. Matheson. Diagramas de influencia. En R. A. Howard y J. E. Matheson, editores, Los Principios y Aplicaciones del Análisis de Decisiones. Grupo de Decisiones Estratégicas, Menlo Park, CA 94025, 1984. [13] L. Kaelbling, M. Littman y A. Cassandra. Planificación y actuación en dominios estocásticos parcialmente observables. Revista de Inteligencia Artificial, 2, 1998. [14] D. Koller y B. Milch. Diagramas de influencia multiagente para representar y resolver juegos. En IJCAI, páginas 1027-1034, 2001. [15] K. Polich y P. Gmytrasiewicz. Diagramas de influencia interactivos y dinámicos. En el taller GTDT, AAMAS, 2006. [16] B. Rathnas, P. Doshi y P. J. Gmytrasiewicz. Soluciones exactas para POMDP interactivos utilizando equivalencia conductual. En la Conferencia de Agentes Autónomos y Sistemas Multiagente (AAMAS), 2006. [17] S. Russell y P. Norvig. Inteligencia Artificial: Un Enfoque Moderno (Segunda Edición). Prentice Hall, 2003. [18] R. D. Shachter. \n\nPrentice Hall, 2003. [18] R. D. Shachter. Evaluando diagramas de influencia. Investigación de Operaciones, 34(6):871-882, 1986. [19] D. Suryadi y P. Gmytrasiewicz. Aprendiendo modelos de otros agentes utilizando diagramas de influencia. En UM, 1999. El Sexto Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 821 ",
            "candidates": [],
            "error": [
                [
                    "toma de decisiones secuenciales",
                    "toma de decisiones secuencial",
                    "toma de decisiones secuenciales",
                    "toma de decisiones secuenciales"
                ]
            ]
        },
        "partially observable multiagent environment": {
            "translated_key": "entornos multiagentes parcialmente observables",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Graphical Models for Online Solutions to Interactive POMDPs Prashant Doshi Dept.",
                "of Computer Science University of Georgia Athens, GA 30602, USA pdoshi@cs.uga.edu Yifeng Zeng Dept. of Computer Science Aalborg University DK-9220 Aalborg, Denmark yfzeng@cs.aau.edu Qiongyu Chen Dept. of Computer Science National Univ. of Singapore 117543, Singapore chenqy@comp.nus.edu.sg ABSTRACT We develop a new graphical representation for interactive partially observable Markov decision processes (I-POMDPs) that is significantly more transparent and semantically clear than the previous representation.",
                "These graphical models called interactive dynamic influence diagrams (I-DIDs) seek to explicitly model the structure that is often present in real-world problems by decomposing the situation into chance and decision variables, and the dependencies between the variables.",
                "I-DIDs generalize DIDs, which may be viewed as graphical representations of POMDPs, to multiagent settings in the same way that I-POMDPs generalize POMDPs.",
                "I-DIDs may be used to compute the policy of an agent online as the agent acts and observes in a setting that is populated by other interacting agents.",
                "Using several examples, we show how I-DIDs may be applied and demonstrate their usefulness.",
                "Categories and Subject Descriptors I.2.11 [Distributed Artificial Intelligence]: Multiagent Systems General Terms Theory 1.",
                "INTRODUCTION Interactive partially observable Markov decision processes (IPOMDPs) [9] provide a framework for sequential decision-making in <br>partially observable multiagent environment</br>s.",
                "They generalize POMDPs [13] to multiagent settings by including the other agents computable models in the state space along with the states of the physical environment.",
                "The models encompass all information influencing the agents behaviors, including their preferences, capabilities, and beliefs, and are thus analogous to types in Bayesian games [11].",
                "I-POMDPs adopt a subjective approach to understanding strategic behavior, rooted in a decision-theoretic framework that takes a decision-makers perspective in the interaction.",
                "In [15], Polich and Gmytrasiewicz introduced interactive dynamic influence diagrams (I-DIDs) as the computational representations of I-POMDPs.",
                "I-DIDs generalize DIDs [12], which may be viewed as computational counterparts of POMDPs, to multiagents settings in the same way that I-POMDPs generalize POMDPs.",
                "I-DIDs contribute to a growing line of work [19] that includes multi-agent influence diagrams (MAIDs) [14], and more recently, networks of influence diagrams (NIDs) [8].",
                "These formalisms seek to explicitly model the structure that is often present in real-world problems by decomposing the situation into chance and decision variables, and the dependencies between the variables.",
                "MAIDs provide an alternative to normal and extensive game forms using a graphical formalism to represent games of imperfect information with a decision node for each agents actions and chance nodes capturing the agents private information.",
                "MAIDs objectively analyze the game, efficiently computing the Nash equilibrium profile by exploiting the independence structure.",
                "NIDs extend MAIDs to include agents uncertainty over the game being played and over models of the other agents.",
                "Each model is a MAID and the network of MAIDs is collapsed, bottom up, into a single MAID for computing the equilibrium of the game keeping in mind the different models of each agent.",
                "Graphical formalisms such as MAIDs and NIDs open up a promising area of research that aims to represent multiagent interactions more transparently.",
                "However, MAIDs provide an analysis of the game from an external viewpoint and the applicability of both is limited to static single play games.",
                "Matters are more complex when we consider interactions that are extended over time, where predictions about others future actions must be made using models that change as the agents act and observe.",
                "I-DIDs address this gap by allowing the representation of other agents models as the values of a special model node.",
                "Both, other agents models and the original agents beliefs over these models are updated over time using special-purpose implementations.",
                "In this paper, we improve on the previous preliminary representation of the I-DID shown in [15] by using the insight that the static I-ID is a type of NID.",
                "Thus, we may utilize NID-specific language constructs such as multiplexers to represent the model node, and subsequently the I-ID, more transparently.",
                "Furthermore, we clarify the semantics of the special purpose policy link introduced in the representation of I-DID by [15], and show that it could be replaced by traditional dependency links.",
                "In the previous representation of the I-DID, the update of the agents belief over the models of others as the agents act and receive observations was denoted using a special link called the model update link that connected the model nodes over time.",
                "We explicate the semantics of this link by showing how it can be implemented using the traditional dependency links between the chance nodes that constitute the model nodes.",
                "The net result is a representation of I-DID that is significantly more transparent, semantically clear, and capable of being implemented using the standard algorithms for solving DIDs.",
                "We show how IDIDs may be used to model an agents uncertainty over others models, that may themselves be I-DIDs.",
                "Solution to the I-DID is a policy that prescribes what the agent should do over time, given its beliefs over the physical state and others models.",
                "Analogous to DIDs, I-DIDs may be used to compute the policy of an agent online as the agent acts and observes in a setting that is populated by other interacting agents. 2.",
                "BACKGROUND: FINITELY NESTED IPOMDPS Interactive POMDPs generalize POMDPs to multiagent settings by including other agents models as part of the state space [9].",
                "Since other agents may also reason about others, the interactive state space is strategically nested; it contains beliefs about other agents models and their beliefs about others.",
                "For simplicity of presentation we consider an agent, i, that is interacting with one other agent, j.",
                "A finitely nested I-POMDP of agent i with a strategy level l is defined as the tuple: I-POMDPi,l = ISi,l, A, Ti, Ωi, Oi, Ri where: • ISi,l denotes a set of interactive states defined as, ISi,l = S × Mj,l−1, where Mj,l−1 = {Θj,l−1 ∪ SMj}, for l ≥ 1, and ISi,0 = S, where S is the set of states of the physical environment.",
                "Θj,l−1 is the set of computable intentional models of agent j: θj,l−1 = bj,l−1, ˆθj where the frame, ˆθj = A, Ωj, Tj, Oj, Rj, OCj .",
                "Here, j is Bayes rational and OCj is js optimality criterion.",
                "SMj is the set of subintentional models of j.",
                "Simple examples of subintentional models include a no-information model [10] and a fictitious play model [6], both of which are history independent.",
                "We give a recursive bottom-up construction of the interactive state space below.",
                "ISi,0 = S, Θj,0 = { bj,0, ˆθj | bj,0 ∈ Δ(ISj,0)} ISi,1 = S × {Θj,0 ∪ SMj}, Θj,1 = { bj,1, ˆθj | bj,1 ∈ Δ(ISj,1)} . . . . . .",
                "ISi,l = S × {Θj,l−1 ∪ SMj}, Θj,l = { bj,l, ˆθj | bj,l ∈ Δ(ISj,l)} Similar formulations of nested spaces have appeared in [1, 3]. • A = Ai × Aj is the set of joint actions of all agents in the environment; • Ti : S ×A×S → [0, 1], describes the effect of the joint actions on the physical states of the environment; • Ωi is the set of observations of agent i; • Oi : S × A × Ωi → [0, 1] gives the likelihood of the observations given the physical state and joint action; • Ri : ISi × A → R describes agent is preferences over its interactive states.",
                "Usually only the physical states will matter.",
                "Agent is policy is the mapping, Ω∗ i → Δ(Ai), where Ω∗ i is the set of all observation histories of agent i.",
                "Since belief over the interactive states forms a sufficient statistic [9], the policy can also be represented as a mapping from the set of all beliefs of agent i to a distribution over its actions, Δ(ISi) → Δ(Ai). 2.1 Belief Update Analogous to POMDPs, an agent within the I-POMDP framework updates its belief as it acts and observes.",
                "However, there are two differences that complicate the belief update in multiagent settings when compared to single agent ones.",
                "First, since the state of the physical environment depends on the actions of both agents, is prediction of how the physical state changes has to be made based on its prediction of js actions.",
                "Second, changes in js models have to be included in is belief update.",
                "Specifically, if j is intentional then an update of js beliefs due to its action and observation has to be included.",
                "In other words, i has to update its belief based on its prediction of what j would observe and how j would update its belief.",
                "If js model is subintentional, then js probable observations are appended to the observation history contained in the model.",
                "Formally, we have: Pr(ist |at−1 i , bt−1 i,l ) = β ISt−1:mt−1 j =θt j bt−1 i,l (ist−1 ) × at−1 j Pr(at−1 j |θt−1 j,l−1)Oi(st , at−1 i , at−1 j , ot i) ×Ti(st−1 , at−1 i , at−1 j , st ) ot j Oj(st , at−1 i , at−1 j , ot j) ×τ(SEθt j (bt−1 j,l−1, at−1 j , ot j) − bt j,l−1) (1) where β is the normalizing constant, τ is 1 if its argument is 0 otherwise it is 0, Pr(at−1 j |θt−1 j,l−1) is the probability that at−1 j is Bayes rational for the agent described by model θt−1 j,l−1, and SE(·) is an abbreviation for the belief update.",
                "For a version of the belief update when js model is subintentional, see [9].",
                "If agent j is also modeled as an I-POMDP, then is belief update invokes js belief update (via the term SEθt j ( bt−1 j,l−1 , at−1 j , ot j)), which in turn could invoke is belief update and so on.",
                "This recursion in belief nesting bottoms out at the 0th level.",
                "At this level, the belief update of the agent reduces to a POMDP belief update. 1 For illustrations of the belief update, additional details on I-POMDPs, and how they compare with other multiagent frameworks, see [9]. 2.2 Value Iteration Each belief state in a finitely nested I-POMDP has an associated value reflecting the maximum payoff the agent can expect in this belief state: Un ( bi,l, θi ) = max ai∈Ai is∈ISi,l ERi(is, ai)bi,l(is)+ γ oi∈Ωi Pr(oi|ai, bi,l)Un−1 ( SEθi (bi,l, ai, oi), θi ) (2) where, ERi(is, ai) = aj Ri(is, ai, aj)Pr(aj|mj,l−1) (since is = (s, mj,l−1)).",
                "Eq. 2 is a basis for value iteration in I-POMDPs.",
                "Agent is optimal action, a∗ i , for the case of finite horizon with discounting, is an element of the set of optimal actions for the belief state, OPT(θi), defined as: OPT( bi,l, θi ) = argmax ai∈Ai is∈ISi,l ERi(is, ai)bi,l(is) +γ oi∈Ωi Pr(oi|ai, bi,l)Un ( SEθi (bi,l, ai, oi), θi ) (3) 3.",
                "INTERACTIVEINFLUENCEDIAGRAMS A naive extension of influence diagrams (IDs) to settings populated by multiple agents is possible by treating other agents as automatons, represented using chance nodes.",
                "However, this approach assumes that the agents actions are controlled using a probability distribution that does not change over time.",
                "Interactive influence diagrams (I-IDs) adopt a more sophisticated approach by generalizing IDs to make them applicable to settings shared with other agents who may act and observe, and update their beliefs. 3.1 Syntax In addition to the usual chance, decision, and utility nodes, IIDs include a new type of node called the model node.",
                "We show a general level l I-ID in Fig. 1(a), where the model node (Mj,l−1) is denoted using a hexagon.",
                "We note that the probability distribution over the chance node, S, and the model node together represents agent is belief over its interactive states.",
                "In addition to the model 1 The 0th level model is a POMDP: Other agents actions are treated as exogenous events and folded into the T, O, and R functions.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 815 Figure 1: (a) A generic level l I-ID for agent i situated with one other agent j.",
                "The hexagon is the model node (Mj,l−1) whose structure we show in (b).",
                "Members of the model node are I-IDs themselves (m1 j,l−1, m2 j,l−1; diagrams not shown here for simplicity) whose decision nodes are mapped to the corresponding chance nodes (A1 j , A2 j ).",
                "Depending on the value of the node, Mod[Mj], the distribution of each of the chance nodes is assigned to the node Aj. (c) The transformed I-ID with the model node replaced by the chance nodes and the relationships between them. node, I-IDs differ from IDs by having a dashed link (called the policy link in [15]) between the model node and a chance node, Aj, that represents the distribution over the other agents actions given its model.",
                "In the absence of other agents, the model node and the chance node, Aj, vanish and I-IDs collapse into traditional IDs.",
                "The model node contains the alternative computational models ascribed by i to the other agent from the set, Θj,l−1 ∪ SMj, where Θj,l−1 and SMj were defined previously in Section 2.",
                "Thus, a model in the model node may itself be an I-ID or ID, and the recursion terminates when a model is an ID or subintentional.",
                "Because the model node contains the alternative models of the other agent as its values, its representation is not trivial.",
                "In particular, some of the models within the node are I-IDs that when solved generate the agents optimal policy in their decision nodes.",
                "Each decision node is mapped to the corresponding chance node, say A1 j , in the following way: if OPT is the set of optimal actions obtained by solving the I-ID (or ID), then Pr(aj ∈ A1 j ) = 1 |OP T | if aj ∈ OPT, 0 otherwise.",
                "Borrowing insights from previous work [8], we observe that the model node and the dashed policy link that connects it to the chance node, Aj, could be represented as shown in Fig. 1(b).",
                "The decision node of each level l − 1 I-ID is transformed into a chance node, as we mentioned previously, so that the actions with the largest value in the decision node are assigned uniform probabilities in the chance node while the rest are assigned zero probability.",
                "The different chance nodes (A1 j , A2 j ), one for each model, and additionally, the chance node labeled Mod[Mj] form the parents of the chance node, Aj.",
                "Thus, there are as many action nodes (A1 j , A2 j ) in Mj,l−1 as the number of models in the support of agent is beliefs.",
                "The conditional probability table of the chance node, Aj, is a multiplexer that assumes the distribution of each of the action nodes (A1 j , A2 j ) depending on the value of Mod[Mj].",
                "The values of Mod[Mj] denote the different models of j.",
                "In other words, when Mod[Mj] has the value m1 j,l−1, the chance node Aj assumes the distribution of the node A1 j , and Aj assumes the distribution of A2 j when Mod[Mj] has the value m2 j,l−1.",
                "The distribution over the node, Mod[Mj], is the agent is belief over the models of j given a physical state.",
                "For more agents, we will have as many model nodes as there are agents.",
                "Notice that Fig. 1(b) clarifies the semantics of the policy link, and shows how it can be represented using the traditional dependency links.",
                "In Fig. 1(c), we show the transformed I-ID when the model node is replaced by the chance nodes and relationships between them.",
                "In contrast to the representation in [15], there are no special-purpose policy links, rather the I-ID is composed of only those types of nodes that are found in traditional IDs and dependency relationships between the nodes.",
                "This allows I-IDs to be represented and implemented using conventional application tools that target IDs.",
                "Note that we may view the level l I-ID as a NID.",
                "Specifically, each of the level l − 1 models within the model node are blocks in the NID (see Fig. 2).",
                "If the level l = 1, each block is a traditional ID, otherwise if l > 1, each block within the NID may itself be a NID.",
                "Note that within the I-IDs (or IDs) at each level, there is only a single decision node.",
                "Thus, our NID does not contain any MAIDs.",
                "Figure 2: A level l I-ID represented as a NID.",
                "The probabilities assigned to the blocks of the NID are is beliefs over js models conditioned on a physical state. 3.2 Solution The solution of an I-ID proceeds in a bottom-up manner, and is implemented recursively.",
                "We start by solving the level 0 models, which, if intentional, are traditional IDs.",
                "Their solutions provide probability distributions over the other agents actions, which are entered in the corresponding chance nodes found in the model node of the level 1 I-ID.",
                "The mapping from the level 0 models decision nodes to the chance nodes is carried out so that actions with the largest value in the decision node are assigned uniform probabilities in the chance node while the rest are assigned zero probability.",
                "Given the distributions over the actions within the different chance nodes (one for each model of the other agent), the level 1 I-ID is transformed as shown in Fig. 1(c).",
                "During the transformation, the conditional probability table (CPT) of the node, Aj, is populated such that the node assumes the distribution of each of the chance nodes depending on the value of the node, Mod[Mj].",
                "As we mentioned previously, the values of the node Mod[Mj] denote the different models of the other agent, and its distribution is the agent is belief over the models of j conditioned on the physical state.",
                "The transformed level 1 I-ID is a traditional ID that may be solved us816 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) (a) (b) Figure 3: (a) A generic two time-slice level l I-DID for agent i in a setting with one other agent j.",
                "Notice the dotted model update link that denotes the update of the models of j and the distribution over the models over time. (b) The semantics of the model update link. ing the standard expected utility maximization method [18].",
                "This procedure is carried out up to the level l I-ID whose solution gives the non-empty set of optimal actions that the agent should perform given its belief.",
                "Notice that analogous to IDs, I-IDs are suitable for online decision-making when the agents current belief is known. 4.",
                "INTERACTIVE DYNAMIC INFLUENCE DIAGRAMS Interactive dynamic influence diagrams (I-DIDs) extend I-IDs (and NIDs) to allow sequential decision-making over several time steps.",
                "Just as DIDs are structured graphical representations of POMDPs, I-DIDs are the graphical online analogs for finitely nested I-POMDPs.",
                "I-DIDs may be used to optimize over a finite look-ahead given initial beliefs while interacting with other, possibly similar, agents. 4.1 Syntax We depict a general two time-slice I-DID in Fig. 3(a).",
                "In addition to the model nodes and the dashed policy link, what differentiates an I-DID from a DID is the model update link shown as a dotted arrow in Fig. 3(a).",
                "We explained the semantics of the model node and the policy link in the previous section; we describe the model updates next.",
                "The update of the model node over time involves two steps: First, given the models at time t, we identify the updated set of models that reside in the model node at time t + 1.",
                "Recall from Section 2 that an agents intentional model includes its belief.",
                "Because the agents act and receive observations, their models are updated to reflect their changed beliefs.",
                "Since the set of optimal actions for a model could include all the actions, and the agent may receive any one of |Ωj| possible observations, the updated set at time step t + 1 will have at most |Mt j,l−1||Aj||Ωj| models.",
                "Here, |Mt j,l−1| is the number of models at time step t, |Aj| and |Ωj| are the largest spaces of actions and observations respectively, among all the models.",
                "Second, we compute the new distribution over the updated models given the original distribution and the probability of the agent performing the action and receiving the observation that led to the updated model.",
                "These steps are a part of agent is belief update formalized using Eq. 1.",
                "In Fig. 3(b), we show how the dotted model update link is implemented in the I-DID.",
                "If each of the two level l − 1 models ascribed to j at time step t results in one action, and j could make one of two possible observations, then the model node at time step t + 1 contains four updated models (mt+1,1 j,l−1 ,mt+1,2 j,l−1 , mt+1,3 j,l−1 , and mt+1,4 j,l−1 ).",
                "These models differ in their initial beliefs, each of which is the result of j updating its beliefs due to its action and a possible observation.",
                "The decision nodes in each of the I-DIDs or DIDs that represent the lower level models are mapped to the corresponding Figure 4: Transformed I-DID with the model nodes and model update link replaced with the chance nodes and the relationships (in bold). chance nodes, as mentioned previously.",
                "Next, we describe how the distribution over the updated set of models (the distribution over the chance node Mod[Mt+1 j ] in Mt+1 j,l−1) is computed.",
                "The probability that js updated model is, say mt+1,1 j,l−1 , depends on the probability of j performing the action and receiving the observation that led to this model, and the prior distribution over the models at time step t. Because the chance node At j assumes the distribution of each of the action nodes based on the value of Mod[Mt j ], the probability of the action is given by this chance node.",
                "In order to obtain the probability of js possible observation, we introduce the chance node Oj, which depending on the value of Mod[Mt j ] assumes the distribution of the observation node in the lower level model denoted by Mod[Mt j ].",
                "Because the probability of js observations depends on the physical state and the joint actions of both agents, the node Oj is linked with St+1 , At j, and At i. 2 Analogous to At j, the conditional probability table of Oj is also a multiplexer modulated by Mod[Mt j ].",
                "Finally, the distribution over the prior models at time t is obtained from the chance node, Mod[Mt j ] in Mt j,l−1.",
                "Consequently, the chance nodes, Mod[Mt j ], At j, and Oj, form the parents of Mod[Mt+1 j ] in Mt+1 j,l−1.",
                "Notice that the model update link may be replaced by the dependency links between the chance nodes that constitute the model nodes in the two time slices.",
                "In Fig. 4 we show the two time-slice I-DID with the model nodes replaced by the chance nodes and the relationships between them.",
                "Chance nodes and dependency links that not in bold are standard, usually found in DIDs.",
                "Expansion of the I-DID over more time steps requires the repetition of the two steps of updating the set of models that form the 2 Note that Oj represents js observation at time t + 1.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 817 values of the model node and adding the relationships between the chance nodes, as many times as there are model update links.",
                "We note that the possible set of models of the other agent j grows exponentially with the number of time steps.",
                "For example, after T steps, there may be at most |Mt=1 j,l−1|(|Aj||Ωj|)T −1 candidate models residing in the model node. 4.2 Solution Analogous to I-IDs, the solution to a level l I-DID for agent i expanded over T time steps may be carried out recursively.",
                "For the purpose of illustration, let l=1 and T=2.",
                "The solution method uses the standard look-ahead technique, projecting the agents action and observation sequences forward from the current belief state [17], and finding the possible beliefs that i could have in the next time step.",
                "Because agent i has a belief over js models as well, the lookahead includes finding out the possible models that j could have in the future.",
                "Consequently, each of js subintentional or level 0 models (represented using a standard DID) in the first time step must be solved to obtain its optimal set of actions.",
                "These actions are combined with the set of possible observations that j could make in that model, resulting in an updated set of candidate models (that include the updated beliefs) that could describe the behavior of j. Beliefs over this updated set of candidate models are calculated using the standard inference methods using the dependency relationships between the model nodes as shown in Fig. 3(b).",
                "We note the recursive nature of this solution: in solving agent is level 1 I-DID, js level 0 DIDs must be solved.",
                "If the nesting of models is deeper, all models at all levels starting from 0 are solved in a bottom-up manner.",
                "We briefly outline the recursive algorithm for solving agent is Algorithm for solving I-DID Input : level l ≥ 1 I-ID or level 0 ID, T Expansion Phase 1.",
                "For t from 1 to T − 1 do 2.",
                "If l ≥ 1 then Populate Mt+1 j,l−1 3.",
                "For each mt j in Range(Mt j,l−1) do 4.",
                "Recursively call algorithm with the l − 1 I-ID (or ID) that represents mt j and the horizon, T − t + 1 5.",
                "Map the decision node of the solved I-ID (or ID), OPT(mt j), to a chance node Aj 6.",
                "For each aj in OPT(mt j) do 7.",
                "For each oj in Oj (part of mt j) do 8.",
                "Update js belief, bt+1 j ← SE(bt j, aj, oj) 9. mt+1 j ← New I-ID (or ID) with bt+1 j as the initial belief 10.",
                "Range(Mt+1 j,l−1) ∪ ← {mt+1 j } 11.",
                "Add the model node, Mt+1 j,l−1, and the dependency links between Mt j,l−1 and Mt+1 j,l−1 (shown in Fig. 3(b)) 12.",
                "Add the chance, decision, and utility nodes for t + 1 time slice and the dependency links between them 13.",
                "Establish the CPTs for each chance node and utility node Look-Ahead Phase 14.",
                "Apply the standard look-ahead and backup method to solve the expanded I-DID Figure 5: Algorithm for solving a level l ≥ 0 I-DID. level l I-DID expanded over T time steps with one other agent j in Fig. 5.",
                "We adopt a two-phase approach: Given an I-ID of level l (described previously in Section 3) with all lower level models also represented as I-IDs or IDs (if level 0), the first step is to expand the level l I-ID over T time steps adding the dependency links and the conditional probability tables for each node.",
                "We particularly focus on establishing and populating the model nodes (lines 3-11).",
                "Note that Range(·) returns the values (lower level models) of the random variable given as input (model node).",
                "In the second phase, we use a standard look-ahead technique projecting the action and observation sequences over T time steps in the future, and backing up the utility values of the reachable beliefs.",
                "Similar to I-IDs, the I-DIDs reduce to DIDs in the absence of other agents.",
                "As we mentioned previously, the 0-th level models are the traditional DIDs.",
                "Their solutions provide probability distributions over actions of the agent modeled at that level to I-DIDs at level 1.",
                "Given probability distributions over other agents actions the level 1 IDIDs can themselves be solved as DIDs, and provide probability distributions to yet higher level models.",
                "Assume that the number of models considered at each level is bound by a number, M. Solving an I-DID of level l in then equivalent to solving O(Ml ) DIDs. 5.",
                "EXAMPLE APPLICATIONS To illustrate the usefulness of I-DIDs, we apply them to three problem domains.",
                "We describe, in particular, the formulation of the I-DID and the optimal prescriptions obtained on solving it. 5.1 Followership-Leadership in the Multiagent Tiger Problem We begin our illustrations of using I-IDs and I-DIDs with a slightly modified version of the multiagent tiger problem discussed in [9].",
                "The problem has two agents, each of which can open the right door (OR), the left door (OL) or listen (L).",
                "In addition to hearing growls (from the left (GL) or from the right (GR)) when they listen, the agents also hear creaks (from the left (CL), from the right (CR), or no creaks (S)), which noisily indicate the other agents opening one of the doors.",
                "When any door is opened, the tiger persists in its original location with a probability of 95%.",
                "Agent i hears growls with a reliability of 65% and creaks with a reliability of 95%.",
                "Agent j, on the other hand, hears growls with a reliability of 95%.",
                "Thus, the setting is such that agent i hears agent j opening doors more reliably than the tigers growls.",
                "This suggests that i could use js actions as an indication of the location of the tiger, as we discuss below.",
                "Each agents preferences are as in the single agent game discussed in [13].",
                "The transition, observation, and reward functions are shown in [16].",
                "A good indicator of the usefulness of normative methods for decision-making like I-DIDs is the emergence of realistic social behaviors in their prescriptions.",
                "In settings of the persistent multiagent tiger problem that reflect real world situations, we demonstrate followership between the agents and, as shown in [15], deception among agents who believe that they are in a follower-leader type of relationship.",
                "In particular, we analyze the situational and epistemological conditions sufficient for their emergence.",
                "The followership behavior, for example, results from the agent knowing its own weaknesses, assessing the strengths, preferences, and possible behaviors of the other, and realizing that its best for it to follow the others actions in order to maximize its payoffs.",
                "Let us consider a particular setting of the tiger problem in which agent i believes that js preferences are aligned with its own - both of them just want to get the gold - and js hearing is more reliable in comparison to itself.",
                "As an example, suppose that j, on listening can discern the tigers location 95% of the times compared to is 65% accuracy.",
                "Additionally, agent i does not have any initial information about the tigers location.",
                "In other words, is single-level nested belief, bi,1, assigns 0.5 to each of the two locations of the tiger.",
                "In addition, i considers two models of j, which differ in js flat level 0 initial beliefs.",
                "This is represented in the level 1 I-ID shown in Fig. 6(a).",
                "According to one model, j assigns a probability of 0.9 that the tiger is behind the left door, while the other 818 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Figure 6: (a) Level 1 I-ID of agent i, (b) two level 0 IDs of agent j whose decision nodes are mapped to the chance nodes, A1 j , A2 j , in (a). model assigns 0.1 to that location (see Fig. 6(b)).",
                "Agent i is undecided on these two models of j.",
                "If we vary is hearing ability, and solve the corresponding level 1 I-ID expanded over three time steps, we obtain the normative behavioral policies shown in Fig 7 that exhibit followership behavior.",
                "If is probability of correctly hearing the growls is 0.65, then as shown in the policy in Fig. 7(a), i begins to conditionally follow js actions: i opens the same door that j opened previously iff is own assessment of the tigers location confirms js pick.",
                "If i loses the ability to correctly interpret the growls completely, it blindly follows j and opens the same door that j opened previously (Fig. 7(b)).",
                "Figure 7: Emergence of (a) conditional followership, and (b) blind followership in the tiger problem.",
                "Behaviors of interest are in bold. * is a wildcard, and denotes any one of the observations.",
                "We observed that a single level of belief nesting - beliefs about the others models - was sufficient for followership to emerge in the tiger problem.",
                "However, the epistemological requirements for the emergence of leadership are more complex.",
                "For an agent, say j, to emerge as a leader, followership must first emerge in the other agent i.",
                "As we mentioned previously, if i is certain that its preferences are identical to those of j, and believes that j has a better sense of hearing, i will follow js actions over time.",
                "Agent j emerges as a leader if it believes that i will follow it, which implies that js belief must be nested two levels deep to enable it to recognize its leadership role.",
                "Realizing that i will follow presents j with an opportunity to influence is actions in the benefit of the collective good or its self-interest alone.",
                "For example, in the tiger problem, let us consider a setting in which if both i and j open the correct door, then each gets a payoff of 20 that is double the original.",
                "If j alone selects the correct door, it gets the payoff of 10.",
                "On the other hand, if both agents pick the wrong door, their penalties are cut in half.",
                "In this setting, it is in both js best interest as well as the collective betterment for j to use its expertise in selecting the correct door, and thus be a good leader.",
                "However, consider a slightly different problem in which j gains from is loss and is penalized if i gains.",
                "Specifically, let is payoff be subtracted from js, indicating that j is antagonistic toward i - if j picks the correct door and i the wrong one, then is loss of 100 becomes js gain.",
                "Agent j believes that i incorrectly thinks that js preferences are those that promote the collective good and that it starts off by believing with 99% confidence where the tiger is.",
                "Because i believes that its preferences are similar to those of j, and that j starts by believing almost surely that one of the two is the correct location (two level 0 models of j), i will start by following js actions.",
                "We show is normative policy on solving its singly-nested I-DID over three time steps in Fig. 8(a).",
                "The policy demonstrates that i will blindly follow js actions.",
                "Since the tiger persists in its original location with a probability of 0.95, i will select the same door again.",
                "If j begins the game with a 99% probability that the tiger is on the right, solving js I-DID nested two levels deep, results in the policy shown in Fig. 8(b).",
                "Even though j is almost certain that OL is the correct action, it will start by selecting OR, followed by OL.",
                "Agent js intention is to deceive i who, it believes, will follow js actions, so as to gain $110 in the second time step, which is more than what j would gain if it were to be honest.",
                "Figure 8: Emergence of deception between agents in the tiger problem.",
                "Behaviors of interest are in bold. * denotes as before. (a) Agent is policy demonstrating that it will blindly follow js actions. (b) Even though j is almost certain that the tiger is on the right, it will start by selecting OR, followed by OL, in order to deceive i. 5.2 Altruism and Reciprocity in the Public Good Problem The public good (PG) problem [7], consists of a group of M agents, each of whom must either contribute some resource to a public pot or keep it for themselves.",
                "Since resources contributed to the public pot are shared among all the agents, they are less valuable to the agent when in the public pot.",
                "However, if all agents choose to contribute their resources, then the payoff to each agent is more than if no one contributes.",
                "Since an agent gets its share of the public pot irrespective of whether it has contributed or not, the dominating action is for each agent to not contribute, and instead free ride on others contributions.",
                "However, behaviors of human players in empirical simulations of the PG problem differ from the normative predictions.",
                "The experiments reveal that many players initially contribute a large amount to the public pot, and continue to contribute when the PG problem is played repeatedly, though in decreasing amounts [4].",
                "Many of these experiments [5] report that a small core group of players persistently contributes to the public pot even when all others are defecting.",
                "These experiments also reveal that players who persistently contribute have altruistic or reciprocal preferences matching expected cooperation of others.",
                "For simplicity, we assume that the game is played between M = 2 agents, i and j.",
                "Let each agent be initially endowed with XT amount of resources.",
                "While the classical PG game formulation permits each agent to contribute any quantity of resources (≤ XT ) to the public pot, we simplify the action space by allowing two possible actions.",
                "Each agent may choose to either contribute (C) a fixed amount of the resources, or not contribute.",
                "The latter action is deThe Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 819 noted as defect (D).",
                "We assume that the actions are not observable to others.",
                "The value of resources in the public pot is discounted by ci for each agent i, where ci is the marginal private return.",
                "We assume that ci < 1 so that the agent does not benefit enough that it contributes to the public pot for private gain.",
                "Simultaneously, ciM > 1, making collective contribution pareto optimal. i/j C D C 2ciXT , 2cjXT ciXT − cp, XT + cjXT − P D XT + ciXT − P, cjXT − cp XT , XT Table 1: The one-shot PG game with punishment.",
                "In order to encourage contributions, the contributing agents punish free riders but incur a small cost for administering the punishment.",
                "Let P be the punishment meted out to the defecting agent and cp the non-zero cost of punishing for the contributing agent.",
                "For simplicity, we assume that the cost of punishing is same for both the agents.",
                "The one-shot PG game with punishment is shown in Table. 1.",
                "Let ci = cj, cp > 0, and if P > XT − ciXT , then defection is no longer a dominating action.",
                "If P < XT − ciXT , then defection is the dominating action for both.",
                "If P = XT − ciXT , then the game is not dominance-solvable.",
                "Figure 9: (a) Level 1 I-ID of agent i, (b) level 0 IDs of agent j with decision nodes mapped to the chance nodes, A1 j and A2 j , in (a).",
                "We formulate a sequential version of the PG problem with punishment from the perspective of agent i.",
                "Though in the repeated PG game, the quantity in the public pot is revealed to all the agents after each round of actions, we assume in our formulation that it is hidden from the agents.",
                "Each agent may contribute a fixed amount, xc, or defect.",
                "An agent on performing an action receives an observation of plenty (PY) or meager (MR) symbolizing the state of the public pot.",
                "Notice that the observations are also indirectly indicative of agent js actions because the state of the public pot is influenced by them.",
                "The amount of resources in agent is private pot, is perfectly observable to i.",
                "The payoffs are analogous to Table. 1.",
                "Borrowing from the empirical investigations of the PG problem [5], we construct level 0 IDs for j that model altruistic and non-altruistic types (Fig. 9(b)).",
                "Specifically, our altruistic agent has a high marginal private return (cj is close to 1) and does not punish others who defect.",
                "Let xc = 1 and the level 0 agent be punished half the times it defects.",
                "With one action remaining, both types of agents choose to contribute to avoid being punished.",
                "With two actions to go, the altruistic type chooses to contribute, while the other defects.",
                "This is because cj for the altruistic type is close to 1, thus the expected punishment, 0.5P > (1 − cj), which the altruistic type avoids.",
                "Because cj for the non-altruistic type is less, it prefers not to contribute.",
                "With three steps to go, the altruistic agent contributes to avoid punishment (0.5P > 2(1 − cj)), and the non-altruistic type defects.",
                "For greater than three steps, while the altruistic agent continues to contribute to the public pot depending on how close its marginal private return is to 1, the non-altruistic type prescribes defection.",
                "We analyzed the decisions of an altruistic agent i modeled using a level 1 I-DID expanded over 3 time steps. i ascribes the two level 0 models, mentioned previously, to j (see Fig. 9).",
                "If i believes with a probability 1 that j is altruistic, i chooses to contribute for each of the three steps.",
                "This behavior persists when i is unaware of whether j is altruistic (Fig. 10(a)), and when i assigns a high probability to j being the non-altruistic type.",
                "However, when i believes with a probability 1 that j is non-altruistic and will thus surely defect, i chooses to defect to avoid being punished and because its marginal private return is less than 1.",
                "These results demonstrate that the behavior of our altruistic type resembles that found experimentally.",
                "The non-altruistic level 1 agent chooses to defect regardless of how likely it believes the other agent to be altruistic.",
                "We analyzed the behavior of a reciprocal agent type that matches expected cooperation or defection.",
                "The reciprocal types marginal private return is similar to that of the non-altruistic type, however, it obtains a greater payoff when its action is similar to that of the other.",
                "We consider the case when the reciprocal agent i is unsure of whether j is altruistic and believes that the public pot is likely to be half full.",
                "For this prior belief, i chooses to defect.",
                "On receiving an observation of plenty, i decides to contribute, while an observation of meager makes it defect (Fig. 10(b)).",
                "This is because an observation of plenty signals that the pot is likely to be greater than half full, which results from js action to contribute.",
                "Thus, among the two models ascribed to j, its type is likely to be altruistic making it likely that j will contribute again in the next time step.",
                "Agent i therefore chooses to contribute to reciprocate js action.",
                "An analogous reasoning leads i to defect when it observes a meager pot.",
                "With one action to go, i believing that j contributes, will choose to contribute too to avoid punishment regardless of its observations.",
                "Figure 10: (a) An altruistic level 1 agent always contributes. (b) A reciprocal agent i starts off by defecting followed by choosing to contribute or defect based on its observation of plenty (indicating that j is likely altruistic) or meager (j is non-altruistic). 5.3 Strategies in Two-Player Poker Poker is a popular zero sum card game that has received much attention among the AI research community as a testbed [2].",
                "Poker is played among M ≥ 2 players in which each player receives a hand of cards from a deck.",
                "While several flavors of Poker with varying complexity exist, we consider a simple version in which each player has three plys during which the player may either exchange a card (E), keep the existing hand (K), fold (F) and withdraw from the game, or call (C), requiring all players to show their hands.",
                "To keep matters simple, let M = 2, and each player receive a hand consisting of a single card drawn from the same suit.",
                "Thus, during a showdown, the player who has the numerically larger card (2 is the lowest, ace is the highest) wins the pot.",
                "During an exchange of cards, the discarded card is placed either in the L pile, indicating to the other agent that it was a low numbered card less than 8, or in the 820 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) H pile, indicating that the card had a rank greater than or equal to 8.",
                "Notice that, for example, if a lower numbered card is discarded, the probability of receiving a low card in exchange is now reduced.",
                "We show the level 1 I-ID for the simplified two-player Poker in Fig. 11.",
                "We considered two models (personality types) of agent j.",
                "The conservative type believes that it is likely that its opponent has a high numbered card in its hand.",
                "On the other hand, the aggressive agent j believes with a high probability that its opponent has a lower numbered card.",
                "Thus, the two types differ in their beliefs over their opponents hand.",
                "In both these level 0 models, the opponent is assumed to perform its actions following a fixed, uniform distribution.",
                "With three actions to go, regardless of its hand (unless it is an ace), the aggressive agent chooses to exchange its card, with the intent of improving on its current hand.",
                "This is because it believes the other to have a low card, which improves its chances of getting a high card during the exchange.",
                "The conservative agent chooses to keep its card, no matter its hand because its chances of getting a high card are slim as it believes that its opponent has one.",
                "Figure 11: (a) Level 1 I-ID of agent i.",
                "The observation reveals information about js hand of the previous time step, (b) level 0 IDs of agent j whose decision nodes are mapped to the chance nodes, A1 j , A2 j , in (a).",
                "The policy of a level 1 agent i who believes that each card except its own has an equal likelihood of being in js hand (neutral personality type) and j could be either an aggressive or conservative type, is shown in Fig. 12. is own hand contains the card numbered 8.",
                "The agent starts by keeping its card.",
                "On seeing that j did not exchange a card (N), i believes with probability 1 that j is conservative and hence will keep its cards. i responds by either keeping its card or exchanging it because j is equally likely to have a lower or higher card.",
                "If i observes that j discarded its card into the L or H pile, i believes that j is aggressive.",
                "On observing L, i realizes that j had a low card, and is likely to have a high card after its exchange.",
                "Because the probability of receiving a low card is high now, i chooses to keep its card.",
                "On observing H, believing that the probability of receiving a high numbered card is high, i chooses to exchange its card.",
                "In the final step, i chooses to call regardless of its observation history because its belief that j has a higher card is not sufficiently high to conclude that its better to fold and relinquish the payoff.",
                "This is partly due to the fact that an observation of, say, L resets the agent is previous time step beliefs over js hand to the low numbered cards only. 6.",
                "DISCUSSION We showed how DIDs may be extended to I-DIDs that enable online sequential decision-making in uncertain multiagent settings.",
                "Our graphical representation of I-DIDs improves on the previous Figure 12: A level 1 agent is three step policy in the Poker problem. i starts by believing that j is equally likely to be aggressive or conservative and could have any card in its hand with equal probability. work significantly by being more transparent, semantically clear, and capable of being solved using standard algorithms that target DIDs.",
                "I-DIDs extend NIDs to allow sequential decision-making over multiple time steps in the presence of other interacting agents.",
                "I-DIDs may be seen as concise graphical representations for IPOMDPs providing a way to exploit problem structure and carry out online decision-making as the agent acts and observes given its prior beliefs.",
                "We are currently investigating ways to solve I-DIDs approximately with provable bounds on the solution quality.",
                "Acknowledgment: We thank Piotr Gmytrasiewicz for some useful discussions related to this work.",
                "The first author would like to acknowledge the support of a UGARF grant. 7.",
                "REFERENCES [1] R. J. Aumann.",
                "Interactive epistemology i: Knowledge.",
                "International Journal of Game Theory, 28:263-300, 1999. [2] D. Billings, A. Davidson, J. Schaeffer, and D. Szafron.",
                "The challenge of poker.",
                "AIJ, 2001. [3] A. Brandenburger and E. Dekel.",
                "Hierarchies of beliefs and common knowledge.",
                "Journal of Economic Theory, 59:189-198, 1993. [4] C. Camerer.",
                "Behavioral Game Theory: Experiments in Strategic Interaction.",
                "Princeton University Press, 2003. [5] E. Fehr and S. Gachter.",
                "Cooperation and punishment in public goods experiments.",
                "American Economic Review, 90(4):980-994, 2000. [6] D. Fudenberg and D. K. Levine.",
                "The Theory of Learning in Games.",
                "MIT Press, 1998. [7] D. Fudenberg and J. Tirole.",
                "Game Theory.",
                "MIT Press, 1991. [8] Y. Gal and A. Pfeffer.",
                "A language for modeling agents decision-making processes in games.",
                "In AAMAS, 2003. [9] P. Gmytrasiewicz and P. Doshi.",
                "A framework for sequential planning in multiagent settings.",
                "JAIR, 24:49-79, 2005. [10] P. Gmytrasiewicz and E. Durfee.",
                "Rational coordination in multi-agent environments.",
                "JAAMAS, 3(4):319-350, 2000. [11] J. C. Harsanyi.",
                "Games with incomplete information played by bayesian players.",
                "Management Science, 14(3):159-182, 1967. [12] R. A. Howard and J. E. Matheson.",
                "Influence diagrams.",
                "In R. A. Howard and J. E. Matheson, editors, The Principles and Applications of Decision Analysis.",
                "Strategic Decisions Group, Menlo Park, CA 94025, 1984. [13] L. Kaelbling, M. Littman, and A. Cassandra.",
                "Planning and acting in partially observable stochastic domains.",
                "Artificial Intelligence Journal, 2, 1998. [14] D. Koller and B. Milch.",
                "Multi-agent influence diagrams for representing and solving games.",
                "In IJCAI, pages 1027-1034, 2001. [15] K. Polich and P. Gmytrasiewicz.",
                "Interactive dynamic influence diagrams.",
                "In GTDT Workshop, AAMAS, 2006. [16] B.",
                "Rathnas., P. Doshi, and P. J. Gmytrasiewicz.",
                "Exact solutions to interactive pomdps using behavioral equivalence.",
                "In Autonomous Agents and Multi-Agent Systems Conference (AAMAS), 2006. [17] S. Russell and P. Norvig.",
                "Artificial Intelligence: A Modern Approach (Second Edition).",
                "Prentice Hall, 2003. [18] R. D. Shachter.",
                "Evaluating influence diagrams.",
                "Operations Research, 34(6):871-882, 1986. [19] D. Suryadi and P. Gmytrasiewicz.",
                "Learning models of other agents using influence diagrams.",
                "In UM, 1999.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 821"
            ],
            "original_annotated_samples": [
                "INTRODUCTION Interactive partially observable Markov decision processes (IPOMDPs) [9] provide a framework for sequential decision-making in <br>partially observable multiagent environment</br>s."
            ],
            "translated_annotated_samples": [
                "Las Procesos de Decisión de Markov Interactivos Parcialmente Observables (IPOMDPs) proporcionan un marco para la toma de decisiones secuenciales en <br>entornos multiagentes parcialmente observables</br>."
            ],
            "translated_text": "Modelos gráficos para soluciones en línea a POMDP interactivos de Prashant Doshi Dept. Departamento de Ciencias de la Computación Universidad de Georgia Athens, GA 30602, EE. UU. pdoshi@cs.uga.edu Yifeng Zeng Dept. de Ciencias de la Computación Universidad de Aalborg DK-9220 Aalborg, Dinamarca yfzeng@cs.aau.edu Qiongyu Chen Dept. de Ciencias de la Computación Universidad Nacional de Singapur 117543, Singapur chenqy@comp.nus.edu.sg RESUMEN Desarrollamos una nueva representación gráfica para procesos de decisión de Markov parcialmente observables interactivos (I-POMDPs) que es significativamente más transparente y semánticamente clara que la representación anterior. Estos modelos gráficos llamados diagramas de influencia dinámica interactiva (I-DIDs) buscan modelar explícitamente la estructura que a menudo está presente en problemas del mundo real al descomponer la situación en variables de probabilidad y decisión, y las dependencias entre las variables. Los I-DIDs generalizan los DIDs, los cuales pueden ser vistos como representaciones gráficas de POMDPs, a entornos multiagentes de la misma manera que los I-POMDPs generalizan los POMDPs. Los I-DIDs pueden ser utilizados para calcular la política de un agente en línea mientras el agente actúa y observa en un entorno poblado por otros agentes que interactúan entre sí. Usando varios ejemplos, mostramos cómo se pueden aplicar los I-DIDs y demostramos su utilidad. Categorías y Descriptores de Asignaturas I.2.11 [Inteligencia Artificial Distribuida]: Sistemas Multiagentes Términos Generales Teoría 1. Las Procesos de Decisión de Markov Interactivos Parcialmente Observables (IPOMDPs) proporcionan un marco para la toma de decisiones secuenciales en <br>entornos multiagentes parcialmente observables</br>. Generalizan los POMDPs [13] a entornos multiagentes al incluir los modelos computables de los otros agentes en el espacio de estados junto con los estados del entorno físico. Los modelos abarcan toda la información que influye en los comportamientos de los agentes, incluyendo sus preferencias, capacidades y creencias, y por lo tanto son análogos a los tipos en los juegos bayesianos [11]. I-POMDPs adoptan un enfoque subjetivo para comprender el comportamiento estratégico, arraigado en un marco de teoría de decisiones que toma la perspectiva de los tomadores de decisiones en la interacción. En [15], Polich y Gmytrasiewicz introdujeron diagramas de influencia dinámica interactivos (I-DIDs) como las representaciones computacionales de I-POMDPs. Los I-DIDs generalizan los DIDs [12], los cuales pueden ser vistos como contrapartes computacionales de POMDPs, a entornos de múltiples agentes de la misma manera que los I-POMDPs generalizan los POMDPs. Los I-DIDs contribuyen a una creciente línea de trabajo [19] que incluye diagramas de influencia multiagente (MAIDs) [14], y más recientemente, redes de diagramas de influencia (NIDs) [8]. Estos formalismos buscan modelar explícitamente la estructura que a menudo está presente en problemas del mundo real al descomponer la situación en variables de azar y decisiones, y las dependencias entre las variables. Los MAIDs proporcionan una alternativa a las formas normales y extensivas de juego utilizando un formalismo gráfico para representar juegos de información imperfecta con un nodo de decisión para las acciones de cada agente y nodos de azar que capturan la información privada de los agentes. Los MAIDs analizan objetivamente el juego, calculando eficientemente el perfil de equilibrio de Nash al explotar la estructura de independencia. NIDs extienden los MAIDs para incluir la incertidumbre de los agentes sobre el juego que se está jugando y sobre los modelos de los otros agentes. Cada modelo es un MAID y la red de MAIDs se colapsa, de abajo hacia arriba, en un solo MAID para calcular el equilibrio del juego teniendo en cuenta los diferentes modelos de cada agente. Los formalismos gráficos como MAIDs y NIDs abren una área de investigación prometedora que tiene como objetivo representar las interacciones multiagente de manera más transparente. Sin embargo, los MAIDs proporcionan un análisis del juego desde un punto de vista externo y la aplicabilidad de ambos está limitada a juegos estáticos de un solo jugador. Los asuntos son más complejos cuando consideramos interacciones que se extienden en el tiempo, donde las predicciones sobre las acciones futuras de otros deben hacerse utilizando modelos que cambian a medida que los agentes actúan y observan. Los I-DIDs abordan esta brecha al permitir la representación de modelos de otros agentes como los valores de un nodo de modelo especial. Tanto los modelos de otros agentes como las creencias originales de los agentes sobre estos modelos se actualizan con el tiempo utilizando implementaciones especializadas. En este documento, mejoramos la representación preliminar previa del I-DID mostrada en [15] utilizando la idea de que el I-ID estático es un tipo de NID. Por lo tanto, podemos utilizar construcciones de lenguaje específicas de NID, como multiplexores, para representar de manera más transparente el nodo del modelo y, posteriormente, el I-ID. Además, aclaramos la semántica del enlace de política de propósito especial introducido en la representación de I-DID por [15], y mostramos que podría ser reemplazado por enlaces de dependencia tradicionales. En la representación previa del I-DID, la actualización de la creencia de los agentes sobre los modelos de los demás a medida que los agentes actúan y reciben observaciones se denotaba utilizando un enlace especial llamado el enlace de actualización del modelo que conectaba los nodos del modelo a lo largo del tiempo. Explicamos la semántica de este enlace mostrando cómo puede ser implementado utilizando los enlaces de dependencia tradicionales entre los nodos de probabilidad que constituyen los nodos del modelo. El resultado final es una representación de I-DID que es significativamente más transparente, semánticamente clara y capaz de ser implementada utilizando los algoritmos estándar para resolver DIDs. Mostramos cómo los IDIDs pueden ser utilizados para modelar la incertidumbre de un agente sobre los modelos de otros, que a su vez pueden ser I-DIDs. La solución al I-DID es una política que prescribe lo que el agente debe hacer con el tiempo, dadas sus creencias sobre el estado físico y otros modelos. Análogos a los DIDs, los I-DIDs pueden ser utilizados para calcular la política de un agente en línea mientras el agente actúa y observa en un entorno poblado por otros agentes interactuantes. 2. ANTECEDENTES: Los IPOMDPS ANIDADOS FINITAMENTE generalizan los POMDPs a entornos multiagentes al incluir los modelos de otros agentes como parte del espacio de estados [9]. Dado que otros agentes también pueden razonar sobre otros, el espacio de estado interactivo está estratégicamente anidado; contiene creencias sobre los modelos de otros agentes y sus creencias sobre los demás. Para simplificar la presentación, consideramos un agente, i, que está interactuando con otro agente, j. Un I-POMDP finitamente anidado del agente i con un nivel de estrategia l se define como la tupla: I-POMDPi,l = ISi,l, A, Ti, Ωi, Oi, Ri donde: • ISi,l denota un conjunto de estados interactivos definido como, ISi,l = S × Mj,l−1, donde Mj,l−1 = {Θj,l−1 ∪ SMj}, para l ≥ 1, e ISi,0 = S, donde S es el conjunto de estados del entorno físico. Θj,l−1 es el conjunto de modelos intencionales computables del agente j: θj,l−1 = bj,l−1, ˆθj donde el marco, ˆθj = A, Ωj, Tj, Oj, Rj, OCj. Aquí, j es Bayesiano racional y OCj es el criterio de optimalidad de j. SMj es el conjunto de modelos subintencionales de j. Ejemplos simples de modelos subintencionales incluyen un modelo sin información [10] y un modelo de juego ficticio [6], ambos de los cuales son independientes de la historia. Damos una construcción recursiva de abajo hacia arriba del espacio de estados interactivo a continuación. Sí,0 = S, Θj,0 = { bj,0, ˆθj | bj,0 ∈ Δ(ISj,0)} Sí,1 = S × {Θj,0 ∪ SMj}, Θj,1 = { bj,1, ˆθj | bj,1 ∈ Δ(ISj,1)} . . . . . . Sí, l = S × {Θj,l−1 ∪ SMj}, Θj,l = { bj,l, ˆθj | bj,l ∈ Δ(ISj,l)} Formulaciones similares de espacios anidados han aparecido en [1, 3]. • A = Ai × Aj es el conjunto de acciones conjuntas de todos los agentes en el entorno; • Ti : S ×A×S → [0, 1], describe el efecto de las acciones conjuntas en los estados físicos del entorno; • Ωi es el conjunto de observaciones del agente i; • Oi : S × A × Ωi → [0, 1] da la probabilidad de las observaciones dadas el estado físico y la acción conjunta; • Ri : ISi × A → R describe las preferencias del agente sobre sus estados interactivos. Normalmente solo importarán los estados físicos. La política del agente es el mapeo, Ω∗ i → Δ(Ai), donde Ω∗ i es el conjunto de todos los historiales de observación del agente i. Dado que la creencia sobre los estados interactivos forma una estadística suficiente [9], la política también puede ser representada como un mapeo del conjunto de todas las creencias del agente i a una distribución sobre sus acciones, Δ(ISi) → Δ(Ai). 2.1 Actualización de Creencias Análogo a los POMDPs, un agente dentro del marco I-POMDP actualiza su creencia a medida que actúa y observa. Sin embargo, hay dos diferencias que complican la actualización de creencias en entornos multiagentes en comparación con los entornos de un solo agente. Primero, dado que el estado del entorno físico depende de las acciones de ambos agentes, la predicción de cómo cambia el estado físico debe hacerse basándose en la predicción de sus acciones. Segundo, los cambios en los modelos de js deben ser incluidos en la actualización de creencias. Específicamente, si j es intencional, entonces se debe incluir una actualización de las creencias de j debido a su acción y observación. En otras palabras, i tiene que actualizar su creencia basándose en su predicción de lo que j observaría y cómo j actualizaría su creencia. Si el modelo de js es subintencional, entonces las observaciones probables de js se añaden al historial de observaciones contenido en el modelo. Formalmente, tenemos: Pr(ist |at−1 i , bt−1 i,l ) = β ISt−1:mt−1 j =θt j bt−1 i,l (ist−1 ) × at−1 j Pr(at−1 j |θt−1 j,l−1)Oi(st , at−1 i , at−1 j , ot i) ×Ti(st−1 , at−1 i , at−1 j , st ) ot j Oj(st , at−1 i , at−1 j , ot j) ×τ(SEθt j (bt−1 j,l−1, at−1 j , ot j) − bt j,l−1) (1) donde β es la constante de normalización, τ es 1 si su argumento es 0, de lo contrario es 0, Pr(at−1 j |θt−1 j,l−1) es la probabilidad de que at−1 j sea Bayes racional para el agente descrito por el modelo θt−1 j,l−1, y SE(·) es una abreviatura para la actualización de creencias. Para una versión de la actualización de creencias cuando el modelo js es subintencional, consulte [9]. Si el agente j también se modela como un I-POMDP, entonces la actualización de creencias invoca la actualización de creencias de j (a través del término SEθt j ( bt−1 j,l−1 , at−1 j , ot j)), lo que a su vez podría invocar la actualización de creencias de is y así sucesivamente. Esta recursión en la anidación de creencias llega al nivel 0. En este nivel, la actualización de creencias del agente se reduce a una actualización de creencias de un POMDP. Para ilustraciones de la actualización de creencias, detalles adicionales sobre I-POMDPs y cómo se comparan con otros marcos multiagentes, consulte [9]. Iteración de Valor Cada estado de creencia en un I-POMDP finitamente anidado tiene un valor asociado que refleja la máxima ganancia que el agente puede esperar en este estado de creencia: Un ( bi,l, θi ) = max ai∈Ai is∈ISi,l ERi(is, ai)bi,l(is)+ γ oi∈Ωi Pr(oi|ai, bi,l)Un−1 ( SEθi (bi,l, ai, oi), θi ) (2) donde, ERi(is, ai) = aj Ri(is, ai, aj)Pr(aj|mj,l−1) (ya que is = (s, mj,l−1)). La ecuación 2 es una base para la iteración de valor en I-POMDPs. La acción óptima del agente, a∗ i, para el caso de horizonte finito con descuento, es un elemento del conjunto de acciones óptimas para el estado de creencia, OPT(θi), definido como: OPT( bi,l, θi ) = argmax ai∈Ai is∈ISi,l ERi(is, ai)bi,l(is) +γ oi∈Ωi Pr(oi|ai, bi,l)Un ( SEθi (bi,l, ai, oi), θi ) (3) 3. Diagramas de influencia interactivos. Una extensión ingenua de los diagramas de influencia (IDs) a entornos poblados por múltiples agentes es posible tratando a los otros agentes como autómatas, representados mediante nodos de probabilidad. Sin embargo, este enfoque asume que las acciones de los agentes están controladas utilizando una distribución de probabilidad que no cambia con el tiempo. Los diagramas de influencia interactivos (I-IDs) adoptan un enfoque más sofisticado al generalizar los IDs para hacerlos aplicables a entornos compartidos con otros agentes que pueden actuar, observar y actualizar sus creencias. 3.1 Sintaxis Además de los nodos habituales de probabilidad, decisión y utilidad, los IIDs incluyen un nuevo tipo de nodo llamado nodo de modelo. Mostramos un nivel general l I-ID en la Fig. 1(a), donde el nodo del modelo (Mj,l−1) está representado con un hexágono. Observamos que la distribución de probabilidad sobre el nodo de probabilidad, S, y el nodo del modelo juntos representan la creencia del agente sobre sus estados interactivos. Además del modelo 1, el modelo de nivel 0 es un POMDP: las acciones de otros agentes se tratan como eventos exógenos y se incorporan en las funciones T, O y R. El Sexto Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) Figura 1: (a) Un nivel genérico l I-ID para el agente i situado con otro agente j. El hexágono es el nodo modelo (Mj,l−1) cuya estructura mostramos en (b). Los miembros del nodo modelo son ellos mismos I-ID (m1 j,l−1, m2 j,l−1; los diagramas no se muestran aquí por simplicidad) cuyos nodos de decisión se asignan a los nodos de probabilidad correspondientes (A1 j , A2 j). Dependiendo del valor del nodo, Mod[Mj], la distribución de cada uno de los nodos de probabilidad se asigna al nodo Aj. (c) El I-ID transformado con el nodo del modelo reemplazado por los nodos de probabilidad y las relaciones entre ellos. Los I-IDs difieren de los IDs al tener un enlace discontinuo (llamado enlace de política en [15]) entre el nodo del modelo y un nodo de probabilidad, Aj, que representa la distribución sobre las acciones de los otros agentes dada su modelo. En ausencia de otros agentes, el nodo del modelo y el nodo de probabilidad, Aj, desaparecen y los I-ID se convierten en ID tradicionales. El nodo del modelo contiene los modelos computacionales alternativos atribuidos por i al otro agente del conjunto, Θj,l−1 ∪ SMj, donde Θj,l−1 y SMj fueron definidos previamente en la Sección 2. Por lo tanto, un modelo en el nodo del modelo puede ser en sí mismo un I-ID o ID, y la recursión termina cuando un modelo es un ID o subintencional. Dado que el nodo del modelo contiene los modelos alternativos del otro agente como sus valores, su representación no es trivial. En particular, algunos de los modelos dentro del nodo son I-IDs que, al resolverse, generan la política óptima de los agentes en sus nodos de decisión. Cada nodo de decisión se asigna al nodo de probabilidad correspondiente, digamos A1 j, de la siguiente manera: si OPT es el conjunto de acciones óptimas obtenidas al resolver el I-ID (o ID), entonces Pr(aj ∈ A1 j) = 1 |OPT| si aj ∈ OPT, 0 en caso contrario. Tomando prestados los conocimientos de trabajos anteriores [8], observamos que el nodo del modelo y el enlace de política discontinua que lo conecta con el nodo de probabilidad, Aj, podrían representarse como se muestra en la Fig. 1(b). El nodo de decisión de cada nivel l − 1 I-ID se transforma en un nodo de probabilidad, como mencionamos anteriormente, de modo que las acciones con el mayor valor en el nodo de decisión se les asignan probabilidades uniformes en el nodo de probabilidad, mientras que al resto se les asigna probabilidad cero. Los diferentes nodos de probabilidad (A1 j , A2 j ), uno para cada modelo, y adicionalmente, el nodo de probabilidad etiquetado Mod[Mj] forman los padres del nodo de probabilidad, Aj. Por lo tanto, hay tantos nodos de acción (A1 j , A2 j ) en Mj,l−1 como el número de modelos en el soporte de las creencias del agente. La tabla de probabilidad condicional del nodo de probabilidad, Aj, es un multiplexor que asume la distribución de cada uno de los nodos de acción (A1 j , A2 j ) dependiendo del valor de Mod[Mj]. Los valores de Mod[Mj] denotan los diferentes modelos de j. En otras palabras, cuando Mod[Mj] tiene el valor m1 j,l−1, el nodo de probabilidad Aj asume la distribución del nodo A1 j, y Aj asume la distribución de A2 j cuando Mod[Mj] tiene el valor m2 j,l−1. La distribución sobre el nodo, Mod[Mj], es la creencia del agente sobre los modelos de j dado un estado físico. Para más agentes, tendremos tantos nodos de modelo como agentes haya. Observa que la Fig. 1(b) aclara la semántica del enlace de política y muestra cómo puede ser representado utilizando los enlaces de dependencia tradicionales. En la Fig. 1(c), mostramos el I-ID transformado cuando el nodo del modelo es reemplazado por los nodos de probabilidad y las relaciones entre ellos. A diferencia de la representación en [15], no hay enlaces de políticas especiales, sino que el I-ID está compuesto únicamente por aquellos tipos de nodos que se encuentran en IDs tradicionales y relaciones de dependencia entre los nodos. Esto permite que los I-IDs sean representados e implementados utilizando herramientas de aplicación convencionales que apuntan a los IDs. Ten en cuenta que podemos ver el nivel l I-ID como un NID. Específicamente, cada uno de los modelos del nivel l − 1 dentro del nodo del modelo son bloques en el NID (ver Fig. 2). Si el nivel l = 1, cada bloque es un ID tradicional, de lo contrario, si l > 1, cada bloque dentro del NID puede ser un NID en sí mismo. Ten en cuenta que dentro de los I-IDs (o IDs) en cada nivel, solo hay un nodo de decisión único. Por lo tanto, nuestro NID no contiene ningún MAID. Figura 2: Un nivel l I-ID representado como un NID. Las probabilidades asignadas a los bloques del NID son creencias sobre modelos js condicionados a un estado físico. 3.2 Solución La solución de un I-ID procede de manera ascendente y se implementa de forma recursiva. Comenzamos resolviendo los modelos de nivel 0, que, si son intencionales, son identificadores tradicionales. Sus soluciones proporcionan distribuciones de probabilidad sobre las acciones de los otros agentes, las cuales se ingresan en los nodos de probabilidad correspondientes encontrados en el nodo del modelo del nivel 1 I-ID. El mapeo de los nodos de decisión de los modelos de nivel 0 a los nodos de probabilidad se realiza de manera que las acciones con el mayor valor en el nodo de decisión se les asignan probabilidades uniformes en el nodo de probabilidad, mientras que el resto se les asigna probabilidad cero. Dadas las distribuciones sobre las acciones dentro de los diferentes nodos de probabilidad (uno para cada modelo del otro agente), el I-ID de nivel 1 se transforma como se muestra en la Fig. 1(c). Durante la transformación, la tabla de probabilidad condicional (CPT) del nodo, Aj, se llena de tal manera que el nodo asume la distribución de cada uno de los nodos de probabilidad dependiendo del valor del nodo, Mod[Mj]. Como mencionamos anteriormente, los valores del nodo Mod[Mj] denotan los diferentes modelos del otro agente, y su distribución es la creencia del agente sobre los modelos de j condicionados al estado físico. El nivel 1 I-ID transformado es un ID tradicional que puede ser resuelto como us816 The Sixth Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) (a) (b) Figura 3: (a) Un I-DID genérico de dos niveles de tiempo para el agente i en un entorno con otro agente j. Observa el enlace de actualización del modelo punteado que denota la actualización de los modelos de j y la distribución de los modelos a lo largo del tiempo. (b) La semántica del enlace de actualización del modelo utilizando el método estándar de maximización de utilidad esperada [18]. Este procedimiento se lleva a cabo hasta el nivel l I-ID cuya solución proporciona el conjunto no vacío de acciones óptimas que el agente debe realizar dada su creencia. Ten en cuenta que, al igual que los IDs, los I-IDs son adecuados para la toma de decisiones en línea cuando se conoce la creencia actual de los agentes. 4. Diagramas de influencia dinámica interactivos (I-DIDs) Los diagramas de influencia dinámica interactivos (I-DIDs) extienden los I-IDs (y NIDs) para permitir la toma de decisiones secuencial a lo largo de varios pasos de tiempo. Así como los DIDs son representaciones gráficas estructuradas de POMDPs, los I-DIDs son los análogos gráficos en línea para I-POMDPs finitamente anidados. Los I-DIDs pueden ser utilizados para optimizar sobre una mirada anticipada finita dadas las creencias iniciales mientras se interactúa con otros agentes, posiblemente similares. 4.1 Sintaxis Representamos un I-DID de dos cortes temporales en la Fig. 3(a). Además de los nodos del modelo y el enlace de política discontinuo, lo que diferencia a un I-DID de un DID es el enlace de actualización del modelo mostrado como una flecha punteada en la Fig. 3(a). Explicamos la semántica del nodo del modelo y el enlace de la política en la sección anterior; a continuación, describimos las actualizaciones del modelo. La actualización del nodo del modelo con el tiempo implica dos pasos: primero, dados los modelos en el tiempo t, identificamos el conjunto actualizado de modelos que residen en el nodo del modelo en el tiempo t + 1. Recuerda de la Sección 2 que el modelo intencional de un agente incluye sus creencias. Debido a que los agentes actúan y reciben observaciones, sus modelos se actualizan para reflejar sus creencias cambiadas. Dado que el conjunto de acciones óptimas para un modelo podría incluir todas las acciones, y el agente podría recibir cualquiera de las |Ωj| posibles observaciones, el conjunto actualizado en el paso de tiempo t + 1 tendrá como máximo |Mt j,l−1||Aj||Ωj| modelos. Aquí, |Mt j,l−1| es el número de modelos en el paso de tiempo t, |Aj| y |Ωj| son los espacios más grandes de acciones y observaciones respectivamente, entre todos los modelos. Segundo, calculamos la nueva distribución sobre los modelos actualizados dados la distribución original y la probabilidad de que el agente realice la acción y reciba la observación que llevó al modelo actualizado. Estos pasos son parte de la actualización de creencias del agente formalizada utilizando la Ecuación 1. En la Fig. 3(b), mostramos cómo se implementa el enlace de actualización del modelo punteado en el I-DID. Si cada uno de los dos modelos de nivel l − 1 atribuidos a j en el paso de tiempo t resulta en una acción, y j podría hacer una de dos posibles observaciones, entonces el nodo del modelo en el paso de tiempo t + 1 contiene cuatro modelos actualizados (mt+1,1 j,l−1 ,mt+1,2 j,l−1 , mt+1,3 j,l−1 , y mt+1,4 j,l−1 ). Estos modelos difieren en sus creencias iniciales, cada una de las cuales es el resultado de j actualizar sus creencias debido a su acción y una posible observación. Los nodos de decisión en cada uno de los I-DIDs o DIDs que representan los modelos de nivel inferior se asignan a la Figura 4 correspondiente: I-DID transformado con los nodos del modelo y el enlace de actualización del modelo reemplazados por los nodos de probabilidad y las relaciones (en negrita) de los nodos de probabilidad, como se mencionó anteriormente. A continuación, describimos cómo se calcula la distribución sobre el conjunto actualizado de modelos (la distribución sobre el nodo de probabilidad Mod[Mt+1 j ] en Mt+1 j,l−1). La probabilidad de que el modelo actualizado js sea, digamos mt+1,1 j,l−1, depende de la probabilidad de que j realice la acción y reciba la observación que condujo a este modelo, y de la distribución previa sobre los modelos en el paso de tiempo t. Dado que el nodo de probabilidad At j asume la distribución de cada uno de los nodos de acción basados en el valor de Mod[Mt j], la probabilidad de la acción está dada por este nodo de probabilidad. Para obtener la probabilidad de una observación posible js, introducimos el nodo de probabilidad Oj, que dependiendo del valor de Mod[Mt j], asume la distribución del nodo de observación en el modelo de nivel inferior denominado Mod[Mt j]. Dado que la probabilidad de las observaciones js depende del estado físico y de las acciones conjuntas de ambos agentes, el nodo Oj está vinculado con St+1, At j y At i. De manera análoga a At j, la tabla de probabilidad condicional de Oj también es un multiplexor modulado por Mod[Mt j]. Finalmente, la distribución de los modelos previos en el tiempo t se obtiene a partir del nodo de probabilidad, Mod[Mt j ] en Mt j,l−1. Por consiguiente, los nodos de probabilidad, Mod[Mt j], At j y Oj, forman los padres de Mod[Mt+1 j] en Mt+1 j,l−1. Ten en cuenta que el enlace de actualización del modelo puede ser reemplazado por los enlaces de dependencia entre los nodos de probabilidad que constituyen los nodos del modelo en las dos etapas temporales. En la Fig. 4 mostramos los dos cortes temporales I-DID con los nodos del modelo reemplazados por los nodos de probabilidad y las relaciones entre ellos. Los nodos de probabilidad y los enlaces de dependencia que no están en negrita son estándar, generalmente encontrados en DIDs. La expansión del I-DID a lo largo de más pasos de tiempo requiere la repetición de los dos pasos de actualizar el conjunto de modelos que forman el 2. Nota que Oj representa la observación j en el tiempo t + 1. El Sexto Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 817 valores del nodo del modelo y añadiendo las relaciones entre los nodos de probabilidad, tantas veces como enlaces de actualización del modelo haya. Observamos que el conjunto posible de modelos del otro agente j crece exponencialmente con el número de pasos de tiempo. Por ejemplo, después de T pasos, puede haber como máximo |Mt=1 j,l−1|(|Aj||Ωj|)T −1 modelos candidatos residiendo en el nodo del modelo. 4.2 Solución Análoga a los I-ID, la solución a un I-DID de nivel l para el agente i expandido a lo largo de T pasos de tiempo puede llevarse a cabo de forma recursiva. Con el propósito de ilustración, dejemos l=1 y T=2. El método de solución utiliza la técnica estándar de anticipación, proyectando las secuencias de acciones y observaciones de los agentes hacia adelante desde el estado de creencia actual [17], y encontrando las posibles creencias que podría tener en el siguiente paso temporal. Dado que el agente i también tiene una creencia sobre los modelos de j, la búsqueda anticipada incluye descubrir los posibles modelos que j podría tener en el futuro. Por consiguiente, cada uno de los modelos subintencionales o de nivel 0 de js (representados utilizando un DID estándar) en el primer paso de tiempo debe resolverse para obtener su conjunto óptimo de acciones. Estas acciones se combinan con el conjunto de observaciones posibles que j podría hacer en ese modelo, lo que resulta en un conjunto actualizado de modelos candidatos (que incluyen las creencias actualizadas) que podrían describir el comportamiento de j. Las creencias sobre este conjunto actualizado de modelos candidatos se calculan utilizando los métodos estándar de inferencia que utilizan las relaciones de dependencia entre los nodos del modelo, como se muestra en la Figura 3(b). Observamos la naturaleza recursiva de esta solución: al resolver el agente de nivel 1 I-DID, los DIDs de nivel 0 deben ser resueltos. Si el anidamiento de los modelos es más profundo, todos los modelos en todos los niveles, comenzando desde el nivel 0, se resuelven de manera ascendente. Breve descripción del algoritmo recursivo para resolver el agente es el Algoritmo para resolver I-DID. Entrada: nivel l ≥ 1 I-ID o nivel 0 ID, T Fase de Expansión 1. Para t desde 1 hasta T − 1, hacer 2. Si l ≥ 1, entonces llenar Mt+1 j,l−1 3. Para cada mt j en el rango (Mt j,l−1) hacer 4. Llame recursivamente al algoritmo con el l - 1 I-ID (o ID) que representa mt j y el horizonte, T - t + 1 5. Mapea el nodo de decisión del I-ID (o ID) resuelto, OPT(mt j), a un nodo de probabilidad Aj 6. Para cada aj en OPT(mt j) hacer 7. Para cada oj en Oj (parte de mt j) hacer 8. Actualizar la creencia js, bt+1 j ← SE(bt j, aj, oj) 9. mt+1 j ← Nuevo I-ID (o ID) con bt+1 j como la creencia inicial 10. Rango(Mt+1 j,l−1) ∪ ← {mt+1 j } 11. Añadir el nodo del modelo, Mt+1 j,l−1, y los enlaces de dependencia entre Mt j,l−1 y Mt+1 j,l−1 (mostrados en la Fig. 3(b)) 12. Agregue los nodos de probabilidad, decisión y utilidad para la franja de tiempo t + 1 y los enlaces de dependencia entre ellos 13. Establezca las CPTs para cada nodo de probabilidad y nodo de utilidad de la Fase de Mirada Adelante 14. Aplica el método estándar de anticipación y respaldo para resolver la Figura 5 expandida de I-DID: Algoritmo para resolver un nivel l ≥ 0 I-DID. Nivel l I-DID expandido durante T pasos de tiempo con otro agente j en la Figura 5. Adoptamos un enfoque de dos fases: Dado un I-ID de nivel l (descrito previamente en la Sección 3) con todos los modelos de niveles inferiores representados también como I-IDs o IDs (si es nivel 0), el primer paso es expandir el I-ID de nivel l a lo largo de T pasos de tiempo añadiendo los enlaces de dependencia y las tablas de probabilidad condicional para cada nodo. Nos enfocamos especialmente en establecer y poblar los nodos del modelo (líneas 3-11). Ten en cuenta que Range(·) devuelve los valores (modelos de nivel inferior) de la variable aleatoria dada como entrada (nodo del modelo). En la segunda fase, utilizamos una técnica estándar de anticipación proyectando las secuencias de acción y observación durante T pasos de tiempo en el futuro, y respaldando los valores de utilidad de las creencias alcanzables. Similar a los I-IDs, los I-DIDs se reducen a DIDs en ausencia de otros agentes. Como mencionamos anteriormente, los modelos de nivel 0 son los DIDs tradicionales. Sus soluciones proporcionan distribuciones de probabilidad sobre las acciones del agente modelado en ese nivel a los I-DIDs en el nivel 1. Dadas las distribuciones de probabilidad sobre las acciones de otros agentes, los IDIDs de nivel 1 pueden resolverse como DIDs y proporcionar distribuciones de probabilidad a modelos de niveles superiores. Suponga que el número de modelos considerados en cada nivel está limitado por un número, M. Resolver un I-DID de nivel l es equivalente a resolver O(Ml) DIDs. Para ilustrar la utilidad de los I-DIDs, los aplicamos a tres dominios de problemas. Describimos, en particular, la formulación del I-DID y las prescripciones óptimas obtenidas al resolverlo. 5.1 Seguimiento-Liderazgo en el Problema del Tigre Multiagente Comenzamos nuestras ilustraciones del uso de I-IDs e I-DIDs con una versión ligeramente modificada del problema del tigre multiagente discutido en [9]. El problema tiene dos agentes, cada uno de los cuales puede abrir la puerta derecha (OR), la puerta izquierda (OL) o escuchar (L). Además de escuchar gruñidos (desde la izquierda (GL) o desde la derecha (GR)) cuando escuchan, los agentes también escuchan chirridos (desde la izquierda (CL), desde la derecha (CR) o sin chirridos (S)), que indican ruidosamente que los otros agentes están abriendo una de las puertas. Cuando se abre cualquier puerta, el tigre persiste en su ubicación original con una probabilidad del 95%. El agente i escucha gruñidos con una fiabilidad del 65% y crujidos con una fiabilidad del 95%. El agente J, por otro lado, escucha gruñidos con una fiabilidad del 95%. Por lo tanto, la configuración es tal que el agente i escucha la apertura de puertas del agente j de manera más confiable que los rugidos de los tigres. Esto sugiere que podría usar acciones de JavaScript como indicación de la ubicación del tigre, como discutimos a continuación. Las preferencias de cada agente son como en el juego de un solo agente discutido en [13]. Las funciones de transición, observación y recompensa se muestran en [16]. Un buen indicador de la utilidad de los métodos normativos para la toma de decisiones como los I-DIDs es la aparición de comportamientos sociales realistas en sus prescripciones. En entornos del persistente problema del tigre multiagente que reflejan situaciones del mundo real, demostramos la seguidismo entre los agentes y, como se muestra en [15], el engaño entre agentes que creen que están en una relación de tipo seguidor-líder. En particular, analizamos las condiciones situacionales y epistemológicas suficientes para su surgimiento. El comportamiento de seguidores, por ejemplo, resulta del agente conociendo sus propias debilidades, evaluando las fortalezas, preferencias y posibles comportamientos del otro, y dándose cuenta de que es mejor para él seguir las acciones de los demás para maximizar sus ganancias. Consideremos una configuración particular del problema del tigre en la que el agente i cree que las preferencias de j están alineadas con las suyas: ambos solo quieren obtener el oro, y la audición de j es más confiable en comparación consigo mismo. Como ejemplo, supongamos que j, al escuchar, puede discernir la ubicación del tigre el 95% de las veces en comparación con su precisión del 65%. Además, el agente i no tiene ninguna información inicial sobre la ubicación de los tigres. En otras palabras, la creencia anidada de un solo nivel, bi,1, asigna 0.5 a cada una de las dos ubicaciones del tigre. Además, considera dos modelos de j, que difieren en los niveles iniciales de creencias planas de j. Esto se representa en el nivel 1 I-ID mostrado en la Fig. 6(a). Según un modelo, j asigna una probabilidad de 0.9 a que el tigre esté detrás de la puerta izquierda, mientras que el otro 818 The Sixth Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) Figura 6: (a) Nivel 1 I-ID del agente i, (b) dos IDs de nivel 0 del agente j cuyos nodos de decisión están mapeados a los nodos de probabilidad, A1 j , A2 j , en (a). El modelo asigna 0.1 a esa ubicación (ver Fig. 6(b)). El agente i está indeciso sobre estos dos modelos de j. Si variamos la capacidad auditiva, y resolvemos el nivel 1 correspondiente I-ID expandido en tres pasos de tiempo, obtenemos las políticas de comportamiento normativas mostradas en la Figura 7 que exhiben comportamiento de seguidores. Si la probabilidad de escuchar correctamente los gruñidos es de 0.65, entonces, como se muestra en la política en la Fig. 7(a), i comienza a seguir condicionalmente las acciones de j: i abre la misma puerta que j abrió anteriormente si su evaluación propia de la ubicación de los tigres confirma la elección de j. Si i pierde la capacidad de interpretar correctamente los gruñidos por completo, sigue ciegamente a j y abre la misma puerta que j abrió anteriormente (Fig. 7(b)). Figura 7: Emergencia de (a) seguidores condicionales, y (b) seguidores ciegos en el problema del tigre. Los comportamientos de interés están en negrita. * es un comodín y representa cualquiera de las observaciones. Observamos que un solo nivel de anidación de creencias, creencias sobre los modelos de los demás, fue suficiente para que surgiera el seguidismo en el problema del tigre. Sin embargo, los requisitos epistemológicos para el surgimiento del liderazgo son más complejos. Para que un agente, digamos j, surja como líder, primero debe surgir el seguidismo en el otro agente i. Como mencionamos anteriormente, si i está seguro de que sus preferencias son idénticas a las de j, y cree que j tiene un mejor sentido del oído, i seguirá las acciones de j con el tiempo. El agente j emerge como líder si cree que lo seguiré, lo que implica que la creencia de j debe estar anidada dos niveles de profundidad para permitirle reconocer su papel de liderazgo. Darse cuenta de que lo seguiré presenta a J una oportunidad para influir en sus acciones en beneficio del bien colectivo o de su interés propio solamente. Por ejemplo, en el problema del tigre, consideremos una situación en la que si tanto i como j abren la puerta correcta, entonces cada uno recibe un pago de 20 que es el doble del original. Si solo j selecciona la puerta correcta, obtiene la recompensa de 10. Por otro lado, si ambos agentes eligen la puerta incorrecta, sus penalizaciones se reducen a la mitad. En este entorno, es tanto en el mejor interés de j como en el beneficio colectivo que utilice su experiencia para seleccionar la puerta correcta, y así ser un buen líder. Sin embargo, considera un problema ligeramente diferente en el que j se beneficia de la pérdida de i y es penalizado si i se beneficia. Específicamente, dejemos que la ganancia se reste de js, indicando que j es antagonista hacia i: si j elige la puerta correcta y i la incorrecta, entonces la pérdida de 100 se convierte en la ganancia de js. El agente J cree que yo incorrectamente pienso que las preferencias de J son aquellas que promueven el bien colectivo y que comienza creyendo con un 99% de confianza dónde está el tigre. Dado que creo que mis preferencias son similares a las de j, y que j comienza creyendo casi con certeza que una de las dos ubicaciones es la correcta (dos modelos de nivel 0 de j), comenzaré siguiendo las acciones de j. Mostramos la política normativa para resolver su I-DID anidado individualmente durante tres pasos de tiempo en la Fig. 8(a). La política demuestra que seguiré ciegamente las acciones de js. Dado que el tigre persiste en su ubicación original con una probabilidad del 0.95, seleccionaré nuevamente la misma puerta. Si j comienza el juego con una probabilidad del 99% de que el tigre esté a la derecha, resolver su I-DID anidado dos niveles profundamente, resulta en la política mostrada en la Fig. 8(b). Aunque j está casi seguro de que OL es la acción correcta, comenzará seleccionando OR, seguido de OL. La intención del agente js es engañar a i, a quien cree que seguirá las acciones de js, para así obtener $110 en el segundo paso de tiempo, lo cual es más de lo que j ganaría si fuera honesto. Figura 8: Emergencia del engaño entre agentes en el problema del tigre. Los comportamientos de interés están en negrita. * denota como antes. (a) El agente es una política que demuestra que seguirá ciegamente las acciones de js. (b) Aunque j está casi seguro de que el tigre está a la derecha, comenzará seleccionando OR, seguido de OL, para engañar a i. 5.2 Altruismo y reciprocidad en el problema del bien público El problema del bien público (PG) [7], consiste en un grupo de M agentes, cada uno de los cuales debe contribuir con algún recurso a una olla pública o guardarlo para sí mismos. Dado que los recursos aportados al fondo público se comparten entre todos los agentes, son menos valiosos para el agente cuando están en el fondo público. Sin embargo, si todos los agentes eligen contribuir con sus recursos, entonces la recompensa para cada agente es mayor que si nadie contribuye. Dado que un agente recibe su parte del bote público independientemente de si ha contribuido o no, la acción dominante es que cada agente no contribuya y en su lugar se aproveche de las contribuciones de los demás. Sin embargo, los comportamientos de los jugadores humanos en simulaciones empíricas del problema de PG difieren de las predicciones normativas. Los experimentos revelan que muchos jugadores inicialmente contribuyen con una gran cantidad al bote público, y continúan contribuyendo cuando se juega el problema del PG repetidamente, aunque en cantidades decrecientes [4]. Muchos de estos experimentos [5] informan que un pequeño grupo central de jugadores contribuye persistentemente al bote público incluso cuando todos los demás están desertando. Estos experimentos también revelan que los jugadores que contribuyen persistentemente tienen preferencias altruistas o recíprocas que coinciden con la cooperación esperada de los demás. Para simplificar, asumimos que el juego se juega entre M = 2 agentes, i y j. Que cada agente esté inicialmente dotado con una cantidad de recursos XT. Mientras que la formulación clásica del juego PG permite que cada agente contribuya con cualquier cantidad de recursos (≤ XT) al bote público, simplificamos el espacio de acciones al permitir dos acciones posibles. Cada agente puede elegir contribuir (C) una cantidad fija de recursos, o no contribuir. La última acción es deThe Sixth Intl. La Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 819 fue señalada como defectuosa (D). Suponemos que las acciones no son observables por otros. El valor de los recursos en la bolsa pública se descuenta por ci para cada agente i, donde ci es el retorno privado marginal. Suponemos que ci < 1 para que el agente no se beneficie lo suficiente como para contribuir al bote público en beneficio propio. Simultáneamente, ciM > 1, lo que hace que la contribución colectiva sea óptima de Pareto. i/j C D C 2ciXT , 2cjXT ciXT − cp, XT + cjXT − P D XT + ciXT − P, cjXT − cp XT , XT Tabla 1: El juego PG de un solo disparo con castigo. Para fomentar las contribuciones, los agentes contribuyentes castigan a los que no colaboran pero incurren en un pequeño costo por administrar el castigo. Sea P la sanción impuesta al agente que se desvía y cp el costo no nulo de castigar al agente contribuyente. Para simplificar, asumimos que el costo de castigar es el mismo para ambos agentes. El juego PG de un solo disparo con castigo se muestra en la Tabla 1. Si ci = cj, cp > 0, y si P > XT − ciXT, entonces la deserción ya no es una acción dominante. Si P < XT − ciXT, entonces la deserción es la acción dominante para ambos. Si P = XT − ciXT, entonces el juego no es resoluble por dominancia. Figura 9: (a) Nivel 1 I-ID del agente i, (b) IDs de nivel 0 del agente j con nodos de decisión mapeados a los nodos de probabilidad, A1 j y A2 j, en (a). Formulamos una versión secuencial del problema PG con castigo desde la perspectiva del agente i. Aunque en el juego de PG repetido, la cantidad en la olla pública se revela a todos los agentes después de cada ronda de acciones, asumimos en nuestra formulación que está oculta para los agentes. Cada agente puede contribuir con una cantidad fija, xc, o desertar. Un agente al realizar una acción recibe una observación de abundante (PY) o escasa (MR) simbolizando el estado de la olla pública. Ten en cuenta que las observaciones también son indirectamente indicativas de las acciones del agente js porque el estado del bote público es influenciado por ellas. La cantidad de recursos en el agente es una olla privada, es perfectamente observable para i. Los pagos son análogos a la Tabla 1. Tomando prestado de las investigaciones empíricas del problema PG [5], construimos IDs de nivel 0 para j que modelan tipos altruistas y no altruistas (Fig. 9(b)). Específicamente, nuestro agente altruista tiene un alto retorno privado marginal (cj está cerca de 1) y no castiga a los demás que incumplen. Que xc = 1 y el agente de nivel 0 sea castigado la mitad de las veces que se desvía. Con una acción restante, ambos tipos de agentes eligen contribuir para evitar ser castigados. Con dos acciones por delante, el tipo altruista elige contribuir, mientras que el otro defecta. Esto se debe a que cj para el tipo altruista es cercano a 1, por lo tanto, el castigo esperado, 0.5P > (1 − cj), que el tipo altruista evita. Debido a que cj para el tipo no altruista es menor, prefiere no contribuir. Con tres pasos por delante, el agente altruista contribuye para evitar el castigo (0.5P > 2(1 − cj)), mientras que el tipo no altruista se desentiende. Para más de tres pasos, mientras el agente altruista continúa contribuyendo al bote público dependiendo de qué tan cercano esté su retorno privado marginal a 1, el tipo no altruista prescribe la deserción. Analizamos las decisiones de un agente altruista i modelado usando un nivel 1 I-DID expandido en 3 pasos de tiempo. i atribuye los dos modelos de nivel 0, mencionados anteriormente, a j (ver Fig. 9). Si creo con una probabilidad de 1 que j es altruista, elijo contribuir en cada uno de los tres pasos. Este comportamiento persiste cuando i no está al tanto de si j es altruista (Fig. 10(a)), y cuando i asigna una alta probabilidad a que j sea del tipo no altruista. Sin embargo, cuando i cree con una probabilidad de 1 que j no es altruista y por lo tanto seguramente va a traicionar, i elige traicionar para evitar ser castigado y porque su beneficio privado marginal es menor que 1. Estos resultados demuestran que el comportamiento de nuestro tipo altruista se asemeja al encontrado experimentalmente. El agente de nivel 1 no altruista elige desertar independientemente de lo altruista que crea que sea el otro agente. Analizamos el comportamiento de un tipo de agente recíproco que se ajusta a la cooperación o defección esperada. El retorno privado marginal del tipo recíproco es similar al del tipo no altruista, sin embargo, obtiene una mayor recompensa cuando su acción es similar a la del otro. Consideramos el caso en el que el agente recíproco i no está seguro de si j es altruista y cree que es probable que el bote público esté medio lleno. Para esta creencia previa, yo elijo defectar. Al recibir una observación de abundancia, decide contribuir, mientras que una observación de escasez lo hace defectar (Fig. 10(b)). Esto se debe a que una observación de abundancia indica que es probable que la olla esté más llena que a la mitad, lo cual resulta de la acción de js para contribuir. Por lo tanto, entre los dos modelos atribuidos a j, es probable que su tipo sea altruista, lo que hace probable que j contribuya nuevamente en el próximo paso de tiempo. El agente i, por lo tanto, elige contribuir para corresponder la acción de js. Un razonamiento análogo lleva a uno a defectar cuando observa una olla escasa. Con una acción por hacer, creo que si j contribuye, también elegiré contribuir para evitar castigo independientemente de sus observaciones. Figura 10: (a) Un agente altruista de nivel 1 siempre contribuye. (b) Un agente recíproco i comienza defraudando y luego elige contribuir o defraudar basándose en su observación de abundancia (indicando que j es probablemente altruista) o escasez (j no es altruista). 5.3 Estrategias en el póker de dos jugadores El póker es un popular juego de cartas de suma cero que ha recibido mucha atención en la comunidad de investigación de IA como un banco de pruebas [2]. El póker se juega entre M ≥ 2 jugadores, en el que cada jugador recibe una mano de cartas de una baraja. Si bien existen varias variantes de póker con diferentes niveles de complejidad, consideramos una versión simple en la que cada jugador tiene tres turnos durante los cuales el jugador puede intercambiar una carta (E), mantener la mano existente (K), retirarse (F) y abandonar el juego, o apostar (C), lo que requiere que todos los jugadores muestren sus manos. Para mantener las cosas simples, dejemos que M = 2, y que cada jugador reciba una mano compuesta por una sola carta sacada del mismo palo. Por lo tanto, durante un enfrentamiento, el jugador que tenga la carta numéricamente más alta (el 2 es el más bajo, el as es el más alto) gana el bote. Durante un intercambio de cartas, la carta descartada se coloca ya sea en la pila L, indicando al otro agente que era una carta de número bajo menor que 8, o en la 820 The Sixth Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) H, indicando que la carta tenía un rango mayor o igual a 8. Ten en cuenta que, por ejemplo, si se descarta una carta de número bajo, la probabilidad de recibir una carta baja a cambio se reduce. Mostramos el nivel 1 I-ID para el Poker simplificado de dos jugadores en la Fig. 11. Consideramos dos modelos (tipos de personalidad) del agente j. El tipo conservador cree que es probable que su oponente tenga una carta de alto número en su mano. Por otro lado, el agente agresivo j cree con alta probabilidad que su oponente tiene una carta de número más bajo. Por lo tanto, los dos tipos difieren en sus creencias sobre la mano de sus oponentes. En ambos estos modelos de nivel 0, se asume que el oponente realiza sus acciones siguiendo una distribución fija y uniforme. Con tres acciones restantes, independientemente de su mano (a menos que sea un as), el agente agresivo elige intercambiar su carta, con la intención de mejorar su mano actual. Esto se debe a que cree que el otro tiene una carta baja, lo que mejora sus posibilidades de obtener una carta alta durante el intercambio. El agente conservador elige quedarse con su carta, sin importar su mano, porque considera que sus posibilidades de obtener una carta alta son escasas, ya que cree que su oponente tiene una. Figura 11: (a) Nivel 1 I-ID del agente i. La observación revela información sobre la mano de js del paso de tiempo anterior, (b) IDs de nivel 0 del agente j cuyos nodos de decisión están mapeados a los nodos de probabilidad, A1 j , A2 j , en (a). La política de un agente de nivel 1 i que cree que cada carta, excepto la suya, tiene la misma probabilidad de estar en su mano (tipo de personalidad neutral) y j podría ser de tipo agresivo o conservador, se muestra en la Figura 12. Su mano contiene la carta numerada 8. El agente comienza por mantener su carta. Al ver que j no intercambió una carta (N), i cree con probabilidad 1 que j es conservador y, por lo tanto, mantendrá sus cartas. i responde manteniendo su carta o intercambiándola porque j tiene igual probabilidad de tener una carta más baja o más alta. Si observo que j descartó su carta en la pila L o H, creo que j es agresivo. Al observar a L, i se da cuenta de que j tenía una carta baja, y es probable que tenga una carta alta después de su intercambio. Dado que la probabilidad de recibir una carta baja es alta en este momento, i decide quedarse con su carta. Al observar la carta H, creyendo que la probabilidad de recibir una carta de número alto es alta, i decide intercambiar su carta. En el paso final, i decide llamar independientemente de su historial de observaciones porque su creencia de que j tiene una carta más alta no es lo suficientemente alta como para concluir que es mejor retirarse y renunciar al pago. Esto se debe en parte al hecho de que una observación de, digamos, L, restablece las creencias del agente en el paso de tiempo anterior sobre las cartas de números bajos solamente. 6. DISCUSIÓN Mostramos cómo los DIDs pueden ser extendidos a los I-DIDs que permiten la toma de decisiones secuenciales en línea en entornos multiagentes inciertos. Nuestra representación gráfica de I-DIDs mejora la Figura 12 anterior: Un agente de nivel 1 es una política de tres pasos en el problema de Poker. i comienza creyendo que j tiene la misma probabilidad de ser agresivo o conservador y podría tener cualquier carta en su mano con igual probabilidad. Trabaja significativamente al ser más transparente, semánticamente claro y capaz de ser resuelto utilizando algoritmos estándar que apuntan a los DIDs. Los I-DIDs extienden los NIDs para permitir la toma de decisiones secuenciales a lo largo de múltiples pasos de tiempo en presencia de otros agentes que interactúan. Los I-DIDs pueden ser vistos como representaciones gráficas concisas para IPOMDPs que proporcionan una forma de explotar la estructura del problema y llevar a cabo la toma de decisiones en línea a medida que el agente actúa y observa dadas sus creencias previas. Actualmente estamos investigando formas de resolver I-DIDs aproximadamente con límites demostrables en la calidad de la solución. Agradecimiento: Agradecemos a Piotr Gmytrasiewicz por algunas discusiones útiles relacionadas con este trabajo. El primer autor desea agradecer el apoyo de una beca UGARF. 7. REFERENCIAS [1] R. J. Aumann. Epistemología interactiva i: Conocimiento. Revista Internacional de Teoría de Juegos, 28:263-300, 1999. [2] D. Billings, A. Davidson, J. Schaeffer y D. Szafron. El desafío del póker. AIJ, 2001. [3] A. Brandenburger y E. Dekel. Jerarquías de creencias y conocimiento común. Revista de Teoría Económica, 59:189-198, 1993. [4] C. Camerer. Teoría de juegos conductual: Experimentos en interacción estratégica. Princeton University Press, 2003. [5] E. Fehr y S. Gachter. Cooperación y castigo en experimentos de bienes públicos. Revista Económica Americana, 90(4):980-994, 2000. [6] D. Fudenberg y D. K. Levine. La Teoría del Aprendizaje en los Juegos. MIT Press, 1998. [7] D. Fudenberg y J. Tirole. Teoría de juegos. MIT Press, 1991. [8] Y. Gal y A. Pfeffer. Un lenguaje para modelar los procesos de toma de decisiones de agentes en juegos. En AAMAS, 2003. [9] P. Gmytrasiewicz y P. Doshi. Un marco para la planificación secuencial en entornos multiagentes. JAIR, 24:49-79, 2005. [10] P. Gmytrasiewicz y E. Durfee. Coordinación racional en entornos multiagente. JAAMAS, 3(4):319-350, 2000. [11] J. C. Harsanyi. Juegos con información incompleta jugados por jugadores bayesianos. Ciencia de la Gestión, 14(3):159-182, 1967. [12] R. A. Howard y J. E. Matheson. Diagramas de influencia. En R. A. Howard y J. E. Matheson, editores, Los Principios y Aplicaciones del Análisis de Decisiones. Grupo de Decisiones Estratégicas, Menlo Park, CA 94025, 1984. [13] L. Kaelbling, M. Littman y A. Cassandra. Planificación y actuación en dominios estocásticos parcialmente observables. Revista de Inteligencia Artificial, 2, 1998. [14] D. Koller y B. Milch. Diagramas de influencia multiagente para representar y resolver juegos. En IJCAI, páginas 1027-1034, 2001. [15] K. Polich y P. Gmytrasiewicz. Diagramas de influencia interactivos y dinámicos. En el taller GTDT, AAMAS, 2006. [16] B. Rathnas, P. Doshi y P. J. Gmytrasiewicz. Soluciones exactas para POMDP interactivos utilizando equivalencia conductual. En la Conferencia de Agentes Autónomos y Sistemas Multiagente (AAMAS), 2006. [17] S. Russell y P. Norvig. Inteligencia Artificial: Un Enfoque Moderno (Segunda Edición). Prentice Hall, 2003. [18] R. D. Shachter. \n\nPrentice Hall, 2003. [18] R. D. Shachter. Evaluando diagramas de influencia. Investigación de Operaciones, 34(6):871-882, 1986. [19] D. Suryadi y P. Gmytrasiewicz. Aprendiendo modelos de otros agentes utilizando diagramas de influencia. En UM, 1999. El Sexto Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 821 ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "multiagent environment": {
            "translated_key": "entornos multiagentes",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Graphical Models for Online Solutions to Interactive POMDPs Prashant Doshi Dept.",
                "of Computer Science University of Georgia Athens, GA 30602, USA pdoshi@cs.uga.edu Yifeng Zeng Dept. of Computer Science Aalborg University DK-9220 Aalborg, Denmark yfzeng@cs.aau.edu Qiongyu Chen Dept. of Computer Science National Univ. of Singapore 117543, Singapore chenqy@comp.nus.edu.sg ABSTRACT We develop a new graphical representation for interactive partially observable Markov decision processes (I-POMDPs) that is significantly more transparent and semantically clear than the previous representation.",
                "These graphical models called interactive dynamic influence diagrams (I-DIDs) seek to explicitly model the structure that is often present in real-world problems by decomposing the situation into chance and decision variables, and the dependencies between the variables.",
                "I-DIDs generalize DIDs, which may be viewed as graphical representations of POMDPs, to multiagent settings in the same way that I-POMDPs generalize POMDPs.",
                "I-DIDs may be used to compute the policy of an agent online as the agent acts and observes in a setting that is populated by other interacting agents.",
                "Using several examples, we show how I-DIDs may be applied and demonstrate their usefulness.",
                "Categories and Subject Descriptors I.2.11 [Distributed Artificial Intelligence]: Multiagent Systems General Terms Theory 1.",
                "INTRODUCTION Interactive partially observable Markov decision processes (IPOMDPs) [9] provide a framework for sequential decision-making in partially observable <br>multiagent environment</br>s.",
                "They generalize POMDPs [13] to multiagent settings by including the other agents computable models in the state space along with the states of the physical environment.",
                "The models encompass all information influencing the agents behaviors, including their preferences, capabilities, and beliefs, and are thus analogous to types in Bayesian games [11].",
                "I-POMDPs adopt a subjective approach to understanding strategic behavior, rooted in a decision-theoretic framework that takes a decision-makers perspective in the interaction.",
                "In [15], Polich and Gmytrasiewicz introduced interactive dynamic influence diagrams (I-DIDs) as the computational representations of I-POMDPs.",
                "I-DIDs generalize DIDs [12], which may be viewed as computational counterparts of POMDPs, to multiagents settings in the same way that I-POMDPs generalize POMDPs.",
                "I-DIDs contribute to a growing line of work [19] that includes multi-agent influence diagrams (MAIDs) [14], and more recently, networks of influence diagrams (NIDs) [8].",
                "These formalisms seek to explicitly model the structure that is often present in real-world problems by decomposing the situation into chance and decision variables, and the dependencies between the variables.",
                "MAIDs provide an alternative to normal and extensive game forms using a graphical formalism to represent games of imperfect information with a decision node for each agents actions and chance nodes capturing the agents private information.",
                "MAIDs objectively analyze the game, efficiently computing the Nash equilibrium profile by exploiting the independence structure.",
                "NIDs extend MAIDs to include agents uncertainty over the game being played and over models of the other agents.",
                "Each model is a MAID and the network of MAIDs is collapsed, bottom up, into a single MAID for computing the equilibrium of the game keeping in mind the different models of each agent.",
                "Graphical formalisms such as MAIDs and NIDs open up a promising area of research that aims to represent multiagent interactions more transparently.",
                "However, MAIDs provide an analysis of the game from an external viewpoint and the applicability of both is limited to static single play games.",
                "Matters are more complex when we consider interactions that are extended over time, where predictions about others future actions must be made using models that change as the agents act and observe.",
                "I-DIDs address this gap by allowing the representation of other agents models as the values of a special model node.",
                "Both, other agents models and the original agents beliefs over these models are updated over time using special-purpose implementations.",
                "In this paper, we improve on the previous preliminary representation of the I-DID shown in [15] by using the insight that the static I-ID is a type of NID.",
                "Thus, we may utilize NID-specific language constructs such as multiplexers to represent the model node, and subsequently the I-ID, more transparently.",
                "Furthermore, we clarify the semantics of the special purpose policy link introduced in the representation of I-DID by [15], and show that it could be replaced by traditional dependency links.",
                "In the previous representation of the I-DID, the update of the agents belief over the models of others as the agents act and receive observations was denoted using a special link called the model update link that connected the model nodes over time.",
                "We explicate the semantics of this link by showing how it can be implemented using the traditional dependency links between the chance nodes that constitute the model nodes.",
                "The net result is a representation of I-DID that is significantly more transparent, semantically clear, and capable of being implemented using the standard algorithms for solving DIDs.",
                "We show how IDIDs may be used to model an agents uncertainty over others models, that may themselves be I-DIDs.",
                "Solution to the I-DID is a policy that prescribes what the agent should do over time, given its beliefs over the physical state and others models.",
                "Analogous to DIDs, I-DIDs may be used to compute the policy of an agent online as the agent acts and observes in a setting that is populated by other interacting agents. 2.",
                "BACKGROUND: FINITELY NESTED IPOMDPS Interactive POMDPs generalize POMDPs to multiagent settings by including other agents models as part of the state space [9].",
                "Since other agents may also reason about others, the interactive state space is strategically nested; it contains beliefs about other agents models and their beliefs about others.",
                "For simplicity of presentation we consider an agent, i, that is interacting with one other agent, j.",
                "A finitely nested I-POMDP of agent i with a strategy level l is defined as the tuple: I-POMDPi,l = ISi,l, A, Ti, Ωi, Oi, Ri where: • ISi,l denotes a set of interactive states defined as, ISi,l = S × Mj,l−1, where Mj,l−1 = {Θj,l−1 ∪ SMj}, for l ≥ 1, and ISi,0 = S, where S is the set of states of the physical environment.",
                "Θj,l−1 is the set of computable intentional models of agent j: θj,l−1 = bj,l−1, ˆθj where the frame, ˆθj = A, Ωj, Tj, Oj, Rj, OCj .",
                "Here, j is Bayes rational and OCj is js optimality criterion.",
                "SMj is the set of subintentional models of j.",
                "Simple examples of subintentional models include a no-information model [10] and a fictitious play model [6], both of which are history independent.",
                "We give a recursive bottom-up construction of the interactive state space below.",
                "ISi,0 = S, Θj,0 = { bj,0, ˆθj | bj,0 ∈ Δ(ISj,0)} ISi,1 = S × {Θj,0 ∪ SMj}, Θj,1 = { bj,1, ˆθj | bj,1 ∈ Δ(ISj,1)} . . . . . .",
                "ISi,l = S × {Θj,l−1 ∪ SMj}, Θj,l = { bj,l, ˆθj | bj,l ∈ Δ(ISj,l)} Similar formulations of nested spaces have appeared in [1, 3]. • A = Ai × Aj is the set of joint actions of all agents in the environment; • Ti : S ×A×S → [0, 1], describes the effect of the joint actions on the physical states of the environment; • Ωi is the set of observations of agent i; • Oi : S × A × Ωi → [0, 1] gives the likelihood of the observations given the physical state and joint action; • Ri : ISi × A → R describes agent is preferences over its interactive states.",
                "Usually only the physical states will matter.",
                "Agent is policy is the mapping, Ω∗ i → Δ(Ai), where Ω∗ i is the set of all observation histories of agent i.",
                "Since belief over the interactive states forms a sufficient statistic [9], the policy can also be represented as a mapping from the set of all beliefs of agent i to a distribution over its actions, Δ(ISi) → Δ(Ai). 2.1 Belief Update Analogous to POMDPs, an agent within the I-POMDP framework updates its belief as it acts and observes.",
                "However, there are two differences that complicate the belief update in multiagent settings when compared to single agent ones.",
                "First, since the state of the physical environment depends on the actions of both agents, is prediction of how the physical state changes has to be made based on its prediction of js actions.",
                "Second, changes in js models have to be included in is belief update.",
                "Specifically, if j is intentional then an update of js beliefs due to its action and observation has to be included.",
                "In other words, i has to update its belief based on its prediction of what j would observe and how j would update its belief.",
                "If js model is subintentional, then js probable observations are appended to the observation history contained in the model.",
                "Formally, we have: Pr(ist |at−1 i , bt−1 i,l ) = β ISt−1:mt−1 j =θt j bt−1 i,l (ist−1 ) × at−1 j Pr(at−1 j |θt−1 j,l−1)Oi(st , at−1 i , at−1 j , ot i) ×Ti(st−1 , at−1 i , at−1 j , st ) ot j Oj(st , at−1 i , at−1 j , ot j) ×τ(SEθt j (bt−1 j,l−1, at−1 j , ot j) − bt j,l−1) (1) where β is the normalizing constant, τ is 1 if its argument is 0 otherwise it is 0, Pr(at−1 j |θt−1 j,l−1) is the probability that at−1 j is Bayes rational for the agent described by model θt−1 j,l−1, and SE(·) is an abbreviation for the belief update.",
                "For a version of the belief update when js model is subintentional, see [9].",
                "If agent j is also modeled as an I-POMDP, then is belief update invokes js belief update (via the term SEθt j ( bt−1 j,l−1 , at−1 j , ot j)), which in turn could invoke is belief update and so on.",
                "This recursion in belief nesting bottoms out at the 0th level.",
                "At this level, the belief update of the agent reduces to a POMDP belief update. 1 For illustrations of the belief update, additional details on I-POMDPs, and how they compare with other multiagent frameworks, see [9]. 2.2 Value Iteration Each belief state in a finitely nested I-POMDP has an associated value reflecting the maximum payoff the agent can expect in this belief state: Un ( bi,l, θi ) = max ai∈Ai is∈ISi,l ERi(is, ai)bi,l(is)+ γ oi∈Ωi Pr(oi|ai, bi,l)Un−1 ( SEθi (bi,l, ai, oi), θi ) (2) where, ERi(is, ai) = aj Ri(is, ai, aj)Pr(aj|mj,l−1) (since is = (s, mj,l−1)).",
                "Eq. 2 is a basis for value iteration in I-POMDPs.",
                "Agent is optimal action, a∗ i , for the case of finite horizon with discounting, is an element of the set of optimal actions for the belief state, OPT(θi), defined as: OPT( bi,l, θi ) = argmax ai∈Ai is∈ISi,l ERi(is, ai)bi,l(is) +γ oi∈Ωi Pr(oi|ai, bi,l)Un ( SEθi (bi,l, ai, oi), θi ) (3) 3.",
                "INTERACTIVEINFLUENCEDIAGRAMS A naive extension of influence diagrams (IDs) to settings populated by multiple agents is possible by treating other agents as automatons, represented using chance nodes.",
                "However, this approach assumes that the agents actions are controlled using a probability distribution that does not change over time.",
                "Interactive influence diagrams (I-IDs) adopt a more sophisticated approach by generalizing IDs to make them applicable to settings shared with other agents who may act and observe, and update their beliefs. 3.1 Syntax In addition to the usual chance, decision, and utility nodes, IIDs include a new type of node called the model node.",
                "We show a general level l I-ID in Fig. 1(a), where the model node (Mj,l−1) is denoted using a hexagon.",
                "We note that the probability distribution over the chance node, S, and the model node together represents agent is belief over its interactive states.",
                "In addition to the model 1 The 0th level model is a POMDP: Other agents actions are treated as exogenous events and folded into the T, O, and R functions.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 815 Figure 1: (a) A generic level l I-ID for agent i situated with one other agent j.",
                "The hexagon is the model node (Mj,l−1) whose structure we show in (b).",
                "Members of the model node are I-IDs themselves (m1 j,l−1, m2 j,l−1; diagrams not shown here for simplicity) whose decision nodes are mapped to the corresponding chance nodes (A1 j , A2 j ).",
                "Depending on the value of the node, Mod[Mj], the distribution of each of the chance nodes is assigned to the node Aj. (c) The transformed I-ID with the model node replaced by the chance nodes and the relationships between them. node, I-IDs differ from IDs by having a dashed link (called the policy link in [15]) between the model node and a chance node, Aj, that represents the distribution over the other agents actions given its model.",
                "In the absence of other agents, the model node and the chance node, Aj, vanish and I-IDs collapse into traditional IDs.",
                "The model node contains the alternative computational models ascribed by i to the other agent from the set, Θj,l−1 ∪ SMj, where Θj,l−1 and SMj were defined previously in Section 2.",
                "Thus, a model in the model node may itself be an I-ID or ID, and the recursion terminates when a model is an ID or subintentional.",
                "Because the model node contains the alternative models of the other agent as its values, its representation is not trivial.",
                "In particular, some of the models within the node are I-IDs that when solved generate the agents optimal policy in their decision nodes.",
                "Each decision node is mapped to the corresponding chance node, say A1 j , in the following way: if OPT is the set of optimal actions obtained by solving the I-ID (or ID), then Pr(aj ∈ A1 j ) = 1 |OP T | if aj ∈ OPT, 0 otherwise.",
                "Borrowing insights from previous work [8], we observe that the model node and the dashed policy link that connects it to the chance node, Aj, could be represented as shown in Fig. 1(b).",
                "The decision node of each level l − 1 I-ID is transformed into a chance node, as we mentioned previously, so that the actions with the largest value in the decision node are assigned uniform probabilities in the chance node while the rest are assigned zero probability.",
                "The different chance nodes (A1 j , A2 j ), one for each model, and additionally, the chance node labeled Mod[Mj] form the parents of the chance node, Aj.",
                "Thus, there are as many action nodes (A1 j , A2 j ) in Mj,l−1 as the number of models in the support of agent is beliefs.",
                "The conditional probability table of the chance node, Aj, is a multiplexer that assumes the distribution of each of the action nodes (A1 j , A2 j ) depending on the value of Mod[Mj].",
                "The values of Mod[Mj] denote the different models of j.",
                "In other words, when Mod[Mj] has the value m1 j,l−1, the chance node Aj assumes the distribution of the node A1 j , and Aj assumes the distribution of A2 j when Mod[Mj] has the value m2 j,l−1.",
                "The distribution over the node, Mod[Mj], is the agent is belief over the models of j given a physical state.",
                "For more agents, we will have as many model nodes as there are agents.",
                "Notice that Fig. 1(b) clarifies the semantics of the policy link, and shows how it can be represented using the traditional dependency links.",
                "In Fig. 1(c), we show the transformed I-ID when the model node is replaced by the chance nodes and relationships between them.",
                "In contrast to the representation in [15], there are no special-purpose policy links, rather the I-ID is composed of only those types of nodes that are found in traditional IDs and dependency relationships between the nodes.",
                "This allows I-IDs to be represented and implemented using conventional application tools that target IDs.",
                "Note that we may view the level l I-ID as a NID.",
                "Specifically, each of the level l − 1 models within the model node are blocks in the NID (see Fig. 2).",
                "If the level l = 1, each block is a traditional ID, otherwise if l > 1, each block within the NID may itself be a NID.",
                "Note that within the I-IDs (or IDs) at each level, there is only a single decision node.",
                "Thus, our NID does not contain any MAIDs.",
                "Figure 2: A level l I-ID represented as a NID.",
                "The probabilities assigned to the blocks of the NID are is beliefs over js models conditioned on a physical state. 3.2 Solution The solution of an I-ID proceeds in a bottom-up manner, and is implemented recursively.",
                "We start by solving the level 0 models, which, if intentional, are traditional IDs.",
                "Their solutions provide probability distributions over the other agents actions, which are entered in the corresponding chance nodes found in the model node of the level 1 I-ID.",
                "The mapping from the level 0 models decision nodes to the chance nodes is carried out so that actions with the largest value in the decision node are assigned uniform probabilities in the chance node while the rest are assigned zero probability.",
                "Given the distributions over the actions within the different chance nodes (one for each model of the other agent), the level 1 I-ID is transformed as shown in Fig. 1(c).",
                "During the transformation, the conditional probability table (CPT) of the node, Aj, is populated such that the node assumes the distribution of each of the chance nodes depending on the value of the node, Mod[Mj].",
                "As we mentioned previously, the values of the node Mod[Mj] denote the different models of the other agent, and its distribution is the agent is belief over the models of j conditioned on the physical state.",
                "The transformed level 1 I-ID is a traditional ID that may be solved us816 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) (a) (b) Figure 3: (a) A generic two time-slice level l I-DID for agent i in a setting with one other agent j.",
                "Notice the dotted model update link that denotes the update of the models of j and the distribution over the models over time. (b) The semantics of the model update link. ing the standard expected utility maximization method [18].",
                "This procedure is carried out up to the level l I-ID whose solution gives the non-empty set of optimal actions that the agent should perform given its belief.",
                "Notice that analogous to IDs, I-IDs are suitable for online decision-making when the agents current belief is known. 4.",
                "INTERACTIVE DYNAMIC INFLUENCE DIAGRAMS Interactive dynamic influence diagrams (I-DIDs) extend I-IDs (and NIDs) to allow sequential decision-making over several time steps.",
                "Just as DIDs are structured graphical representations of POMDPs, I-DIDs are the graphical online analogs for finitely nested I-POMDPs.",
                "I-DIDs may be used to optimize over a finite look-ahead given initial beliefs while interacting with other, possibly similar, agents. 4.1 Syntax We depict a general two time-slice I-DID in Fig. 3(a).",
                "In addition to the model nodes and the dashed policy link, what differentiates an I-DID from a DID is the model update link shown as a dotted arrow in Fig. 3(a).",
                "We explained the semantics of the model node and the policy link in the previous section; we describe the model updates next.",
                "The update of the model node over time involves two steps: First, given the models at time t, we identify the updated set of models that reside in the model node at time t + 1.",
                "Recall from Section 2 that an agents intentional model includes its belief.",
                "Because the agents act and receive observations, their models are updated to reflect their changed beliefs.",
                "Since the set of optimal actions for a model could include all the actions, and the agent may receive any one of |Ωj| possible observations, the updated set at time step t + 1 will have at most |Mt j,l−1||Aj||Ωj| models.",
                "Here, |Mt j,l−1| is the number of models at time step t, |Aj| and |Ωj| are the largest spaces of actions and observations respectively, among all the models.",
                "Second, we compute the new distribution over the updated models given the original distribution and the probability of the agent performing the action and receiving the observation that led to the updated model.",
                "These steps are a part of agent is belief update formalized using Eq. 1.",
                "In Fig. 3(b), we show how the dotted model update link is implemented in the I-DID.",
                "If each of the two level l − 1 models ascribed to j at time step t results in one action, and j could make one of two possible observations, then the model node at time step t + 1 contains four updated models (mt+1,1 j,l−1 ,mt+1,2 j,l−1 , mt+1,3 j,l−1 , and mt+1,4 j,l−1 ).",
                "These models differ in their initial beliefs, each of which is the result of j updating its beliefs due to its action and a possible observation.",
                "The decision nodes in each of the I-DIDs or DIDs that represent the lower level models are mapped to the corresponding Figure 4: Transformed I-DID with the model nodes and model update link replaced with the chance nodes and the relationships (in bold). chance nodes, as mentioned previously.",
                "Next, we describe how the distribution over the updated set of models (the distribution over the chance node Mod[Mt+1 j ] in Mt+1 j,l−1) is computed.",
                "The probability that js updated model is, say mt+1,1 j,l−1 , depends on the probability of j performing the action and receiving the observation that led to this model, and the prior distribution over the models at time step t. Because the chance node At j assumes the distribution of each of the action nodes based on the value of Mod[Mt j ], the probability of the action is given by this chance node.",
                "In order to obtain the probability of js possible observation, we introduce the chance node Oj, which depending on the value of Mod[Mt j ] assumes the distribution of the observation node in the lower level model denoted by Mod[Mt j ].",
                "Because the probability of js observations depends on the physical state and the joint actions of both agents, the node Oj is linked with St+1 , At j, and At i. 2 Analogous to At j, the conditional probability table of Oj is also a multiplexer modulated by Mod[Mt j ].",
                "Finally, the distribution over the prior models at time t is obtained from the chance node, Mod[Mt j ] in Mt j,l−1.",
                "Consequently, the chance nodes, Mod[Mt j ], At j, and Oj, form the parents of Mod[Mt+1 j ] in Mt+1 j,l−1.",
                "Notice that the model update link may be replaced by the dependency links between the chance nodes that constitute the model nodes in the two time slices.",
                "In Fig. 4 we show the two time-slice I-DID with the model nodes replaced by the chance nodes and the relationships between them.",
                "Chance nodes and dependency links that not in bold are standard, usually found in DIDs.",
                "Expansion of the I-DID over more time steps requires the repetition of the two steps of updating the set of models that form the 2 Note that Oj represents js observation at time t + 1.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 817 values of the model node and adding the relationships between the chance nodes, as many times as there are model update links.",
                "We note that the possible set of models of the other agent j grows exponentially with the number of time steps.",
                "For example, after T steps, there may be at most |Mt=1 j,l−1|(|Aj||Ωj|)T −1 candidate models residing in the model node. 4.2 Solution Analogous to I-IDs, the solution to a level l I-DID for agent i expanded over T time steps may be carried out recursively.",
                "For the purpose of illustration, let l=1 and T=2.",
                "The solution method uses the standard look-ahead technique, projecting the agents action and observation sequences forward from the current belief state [17], and finding the possible beliefs that i could have in the next time step.",
                "Because agent i has a belief over js models as well, the lookahead includes finding out the possible models that j could have in the future.",
                "Consequently, each of js subintentional or level 0 models (represented using a standard DID) in the first time step must be solved to obtain its optimal set of actions.",
                "These actions are combined with the set of possible observations that j could make in that model, resulting in an updated set of candidate models (that include the updated beliefs) that could describe the behavior of j. Beliefs over this updated set of candidate models are calculated using the standard inference methods using the dependency relationships between the model nodes as shown in Fig. 3(b).",
                "We note the recursive nature of this solution: in solving agent is level 1 I-DID, js level 0 DIDs must be solved.",
                "If the nesting of models is deeper, all models at all levels starting from 0 are solved in a bottom-up manner.",
                "We briefly outline the recursive algorithm for solving agent is Algorithm for solving I-DID Input : level l ≥ 1 I-ID or level 0 ID, T Expansion Phase 1.",
                "For t from 1 to T − 1 do 2.",
                "If l ≥ 1 then Populate Mt+1 j,l−1 3.",
                "For each mt j in Range(Mt j,l−1) do 4.",
                "Recursively call algorithm with the l − 1 I-ID (or ID) that represents mt j and the horizon, T − t + 1 5.",
                "Map the decision node of the solved I-ID (or ID), OPT(mt j), to a chance node Aj 6.",
                "For each aj in OPT(mt j) do 7.",
                "For each oj in Oj (part of mt j) do 8.",
                "Update js belief, bt+1 j ← SE(bt j, aj, oj) 9. mt+1 j ← New I-ID (or ID) with bt+1 j as the initial belief 10.",
                "Range(Mt+1 j,l−1) ∪ ← {mt+1 j } 11.",
                "Add the model node, Mt+1 j,l−1, and the dependency links between Mt j,l−1 and Mt+1 j,l−1 (shown in Fig. 3(b)) 12.",
                "Add the chance, decision, and utility nodes for t + 1 time slice and the dependency links between them 13.",
                "Establish the CPTs for each chance node and utility node Look-Ahead Phase 14.",
                "Apply the standard look-ahead and backup method to solve the expanded I-DID Figure 5: Algorithm for solving a level l ≥ 0 I-DID. level l I-DID expanded over T time steps with one other agent j in Fig. 5.",
                "We adopt a two-phase approach: Given an I-ID of level l (described previously in Section 3) with all lower level models also represented as I-IDs or IDs (if level 0), the first step is to expand the level l I-ID over T time steps adding the dependency links and the conditional probability tables for each node.",
                "We particularly focus on establishing and populating the model nodes (lines 3-11).",
                "Note that Range(·) returns the values (lower level models) of the random variable given as input (model node).",
                "In the second phase, we use a standard look-ahead technique projecting the action and observation sequences over T time steps in the future, and backing up the utility values of the reachable beliefs.",
                "Similar to I-IDs, the I-DIDs reduce to DIDs in the absence of other agents.",
                "As we mentioned previously, the 0-th level models are the traditional DIDs.",
                "Their solutions provide probability distributions over actions of the agent modeled at that level to I-DIDs at level 1.",
                "Given probability distributions over other agents actions the level 1 IDIDs can themselves be solved as DIDs, and provide probability distributions to yet higher level models.",
                "Assume that the number of models considered at each level is bound by a number, M. Solving an I-DID of level l in then equivalent to solving O(Ml ) DIDs. 5.",
                "EXAMPLE APPLICATIONS To illustrate the usefulness of I-DIDs, we apply them to three problem domains.",
                "We describe, in particular, the formulation of the I-DID and the optimal prescriptions obtained on solving it. 5.1 Followership-Leadership in the Multiagent Tiger Problem We begin our illustrations of using I-IDs and I-DIDs with a slightly modified version of the multiagent tiger problem discussed in [9].",
                "The problem has two agents, each of which can open the right door (OR), the left door (OL) or listen (L).",
                "In addition to hearing growls (from the left (GL) or from the right (GR)) when they listen, the agents also hear creaks (from the left (CL), from the right (CR), or no creaks (S)), which noisily indicate the other agents opening one of the doors.",
                "When any door is opened, the tiger persists in its original location with a probability of 95%.",
                "Agent i hears growls with a reliability of 65% and creaks with a reliability of 95%.",
                "Agent j, on the other hand, hears growls with a reliability of 95%.",
                "Thus, the setting is such that agent i hears agent j opening doors more reliably than the tigers growls.",
                "This suggests that i could use js actions as an indication of the location of the tiger, as we discuss below.",
                "Each agents preferences are as in the single agent game discussed in [13].",
                "The transition, observation, and reward functions are shown in [16].",
                "A good indicator of the usefulness of normative methods for decision-making like I-DIDs is the emergence of realistic social behaviors in their prescriptions.",
                "In settings of the persistent multiagent tiger problem that reflect real world situations, we demonstrate followership between the agents and, as shown in [15], deception among agents who believe that they are in a follower-leader type of relationship.",
                "In particular, we analyze the situational and epistemological conditions sufficient for their emergence.",
                "The followership behavior, for example, results from the agent knowing its own weaknesses, assessing the strengths, preferences, and possible behaviors of the other, and realizing that its best for it to follow the others actions in order to maximize its payoffs.",
                "Let us consider a particular setting of the tiger problem in which agent i believes that js preferences are aligned with its own - both of them just want to get the gold - and js hearing is more reliable in comparison to itself.",
                "As an example, suppose that j, on listening can discern the tigers location 95% of the times compared to is 65% accuracy.",
                "Additionally, agent i does not have any initial information about the tigers location.",
                "In other words, is single-level nested belief, bi,1, assigns 0.5 to each of the two locations of the tiger.",
                "In addition, i considers two models of j, which differ in js flat level 0 initial beliefs.",
                "This is represented in the level 1 I-ID shown in Fig. 6(a).",
                "According to one model, j assigns a probability of 0.9 that the tiger is behind the left door, while the other 818 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Figure 6: (a) Level 1 I-ID of agent i, (b) two level 0 IDs of agent j whose decision nodes are mapped to the chance nodes, A1 j , A2 j , in (a). model assigns 0.1 to that location (see Fig. 6(b)).",
                "Agent i is undecided on these two models of j.",
                "If we vary is hearing ability, and solve the corresponding level 1 I-ID expanded over three time steps, we obtain the normative behavioral policies shown in Fig 7 that exhibit followership behavior.",
                "If is probability of correctly hearing the growls is 0.65, then as shown in the policy in Fig. 7(a), i begins to conditionally follow js actions: i opens the same door that j opened previously iff is own assessment of the tigers location confirms js pick.",
                "If i loses the ability to correctly interpret the growls completely, it blindly follows j and opens the same door that j opened previously (Fig. 7(b)).",
                "Figure 7: Emergence of (a) conditional followership, and (b) blind followership in the tiger problem.",
                "Behaviors of interest are in bold. * is a wildcard, and denotes any one of the observations.",
                "We observed that a single level of belief nesting - beliefs about the others models - was sufficient for followership to emerge in the tiger problem.",
                "However, the epistemological requirements for the emergence of leadership are more complex.",
                "For an agent, say j, to emerge as a leader, followership must first emerge in the other agent i.",
                "As we mentioned previously, if i is certain that its preferences are identical to those of j, and believes that j has a better sense of hearing, i will follow js actions over time.",
                "Agent j emerges as a leader if it believes that i will follow it, which implies that js belief must be nested two levels deep to enable it to recognize its leadership role.",
                "Realizing that i will follow presents j with an opportunity to influence is actions in the benefit of the collective good or its self-interest alone.",
                "For example, in the tiger problem, let us consider a setting in which if both i and j open the correct door, then each gets a payoff of 20 that is double the original.",
                "If j alone selects the correct door, it gets the payoff of 10.",
                "On the other hand, if both agents pick the wrong door, their penalties are cut in half.",
                "In this setting, it is in both js best interest as well as the collective betterment for j to use its expertise in selecting the correct door, and thus be a good leader.",
                "However, consider a slightly different problem in which j gains from is loss and is penalized if i gains.",
                "Specifically, let is payoff be subtracted from js, indicating that j is antagonistic toward i - if j picks the correct door and i the wrong one, then is loss of 100 becomes js gain.",
                "Agent j believes that i incorrectly thinks that js preferences are those that promote the collective good and that it starts off by believing with 99% confidence where the tiger is.",
                "Because i believes that its preferences are similar to those of j, and that j starts by believing almost surely that one of the two is the correct location (two level 0 models of j), i will start by following js actions.",
                "We show is normative policy on solving its singly-nested I-DID over three time steps in Fig. 8(a).",
                "The policy demonstrates that i will blindly follow js actions.",
                "Since the tiger persists in its original location with a probability of 0.95, i will select the same door again.",
                "If j begins the game with a 99% probability that the tiger is on the right, solving js I-DID nested two levels deep, results in the policy shown in Fig. 8(b).",
                "Even though j is almost certain that OL is the correct action, it will start by selecting OR, followed by OL.",
                "Agent js intention is to deceive i who, it believes, will follow js actions, so as to gain $110 in the second time step, which is more than what j would gain if it were to be honest.",
                "Figure 8: Emergence of deception between agents in the tiger problem.",
                "Behaviors of interest are in bold. * denotes as before. (a) Agent is policy demonstrating that it will blindly follow js actions. (b) Even though j is almost certain that the tiger is on the right, it will start by selecting OR, followed by OL, in order to deceive i. 5.2 Altruism and Reciprocity in the Public Good Problem The public good (PG) problem [7], consists of a group of M agents, each of whom must either contribute some resource to a public pot or keep it for themselves.",
                "Since resources contributed to the public pot are shared among all the agents, they are less valuable to the agent when in the public pot.",
                "However, if all agents choose to contribute their resources, then the payoff to each agent is more than if no one contributes.",
                "Since an agent gets its share of the public pot irrespective of whether it has contributed or not, the dominating action is for each agent to not contribute, and instead free ride on others contributions.",
                "However, behaviors of human players in empirical simulations of the PG problem differ from the normative predictions.",
                "The experiments reveal that many players initially contribute a large amount to the public pot, and continue to contribute when the PG problem is played repeatedly, though in decreasing amounts [4].",
                "Many of these experiments [5] report that a small core group of players persistently contributes to the public pot even when all others are defecting.",
                "These experiments also reveal that players who persistently contribute have altruistic or reciprocal preferences matching expected cooperation of others.",
                "For simplicity, we assume that the game is played between M = 2 agents, i and j.",
                "Let each agent be initially endowed with XT amount of resources.",
                "While the classical PG game formulation permits each agent to contribute any quantity of resources (≤ XT ) to the public pot, we simplify the action space by allowing two possible actions.",
                "Each agent may choose to either contribute (C) a fixed amount of the resources, or not contribute.",
                "The latter action is deThe Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 819 noted as defect (D).",
                "We assume that the actions are not observable to others.",
                "The value of resources in the public pot is discounted by ci for each agent i, where ci is the marginal private return.",
                "We assume that ci < 1 so that the agent does not benefit enough that it contributes to the public pot for private gain.",
                "Simultaneously, ciM > 1, making collective contribution pareto optimal. i/j C D C 2ciXT , 2cjXT ciXT − cp, XT + cjXT − P D XT + ciXT − P, cjXT − cp XT , XT Table 1: The one-shot PG game with punishment.",
                "In order to encourage contributions, the contributing agents punish free riders but incur a small cost for administering the punishment.",
                "Let P be the punishment meted out to the defecting agent and cp the non-zero cost of punishing for the contributing agent.",
                "For simplicity, we assume that the cost of punishing is same for both the agents.",
                "The one-shot PG game with punishment is shown in Table. 1.",
                "Let ci = cj, cp > 0, and if P > XT − ciXT , then defection is no longer a dominating action.",
                "If P < XT − ciXT , then defection is the dominating action for both.",
                "If P = XT − ciXT , then the game is not dominance-solvable.",
                "Figure 9: (a) Level 1 I-ID of agent i, (b) level 0 IDs of agent j with decision nodes mapped to the chance nodes, A1 j and A2 j , in (a).",
                "We formulate a sequential version of the PG problem with punishment from the perspective of agent i.",
                "Though in the repeated PG game, the quantity in the public pot is revealed to all the agents after each round of actions, we assume in our formulation that it is hidden from the agents.",
                "Each agent may contribute a fixed amount, xc, or defect.",
                "An agent on performing an action receives an observation of plenty (PY) or meager (MR) symbolizing the state of the public pot.",
                "Notice that the observations are also indirectly indicative of agent js actions because the state of the public pot is influenced by them.",
                "The amount of resources in agent is private pot, is perfectly observable to i.",
                "The payoffs are analogous to Table. 1.",
                "Borrowing from the empirical investigations of the PG problem [5], we construct level 0 IDs for j that model altruistic and non-altruistic types (Fig. 9(b)).",
                "Specifically, our altruistic agent has a high marginal private return (cj is close to 1) and does not punish others who defect.",
                "Let xc = 1 and the level 0 agent be punished half the times it defects.",
                "With one action remaining, both types of agents choose to contribute to avoid being punished.",
                "With two actions to go, the altruistic type chooses to contribute, while the other defects.",
                "This is because cj for the altruistic type is close to 1, thus the expected punishment, 0.5P > (1 − cj), which the altruistic type avoids.",
                "Because cj for the non-altruistic type is less, it prefers not to contribute.",
                "With three steps to go, the altruistic agent contributes to avoid punishment (0.5P > 2(1 − cj)), and the non-altruistic type defects.",
                "For greater than three steps, while the altruistic agent continues to contribute to the public pot depending on how close its marginal private return is to 1, the non-altruistic type prescribes defection.",
                "We analyzed the decisions of an altruistic agent i modeled using a level 1 I-DID expanded over 3 time steps. i ascribes the two level 0 models, mentioned previously, to j (see Fig. 9).",
                "If i believes with a probability 1 that j is altruistic, i chooses to contribute for each of the three steps.",
                "This behavior persists when i is unaware of whether j is altruistic (Fig. 10(a)), and when i assigns a high probability to j being the non-altruistic type.",
                "However, when i believes with a probability 1 that j is non-altruistic and will thus surely defect, i chooses to defect to avoid being punished and because its marginal private return is less than 1.",
                "These results demonstrate that the behavior of our altruistic type resembles that found experimentally.",
                "The non-altruistic level 1 agent chooses to defect regardless of how likely it believes the other agent to be altruistic.",
                "We analyzed the behavior of a reciprocal agent type that matches expected cooperation or defection.",
                "The reciprocal types marginal private return is similar to that of the non-altruistic type, however, it obtains a greater payoff when its action is similar to that of the other.",
                "We consider the case when the reciprocal agent i is unsure of whether j is altruistic and believes that the public pot is likely to be half full.",
                "For this prior belief, i chooses to defect.",
                "On receiving an observation of plenty, i decides to contribute, while an observation of meager makes it defect (Fig. 10(b)).",
                "This is because an observation of plenty signals that the pot is likely to be greater than half full, which results from js action to contribute.",
                "Thus, among the two models ascribed to j, its type is likely to be altruistic making it likely that j will contribute again in the next time step.",
                "Agent i therefore chooses to contribute to reciprocate js action.",
                "An analogous reasoning leads i to defect when it observes a meager pot.",
                "With one action to go, i believing that j contributes, will choose to contribute too to avoid punishment regardless of its observations.",
                "Figure 10: (a) An altruistic level 1 agent always contributes. (b) A reciprocal agent i starts off by defecting followed by choosing to contribute or defect based on its observation of plenty (indicating that j is likely altruistic) or meager (j is non-altruistic). 5.3 Strategies in Two-Player Poker Poker is a popular zero sum card game that has received much attention among the AI research community as a testbed [2].",
                "Poker is played among M ≥ 2 players in which each player receives a hand of cards from a deck.",
                "While several flavors of Poker with varying complexity exist, we consider a simple version in which each player has three plys during which the player may either exchange a card (E), keep the existing hand (K), fold (F) and withdraw from the game, or call (C), requiring all players to show their hands.",
                "To keep matters simple, let M = 2, and each player receive a hand consisting of a single card drawn from the same suit.",
                "Thus, during a showdown, the player who has the numerically larger card (2 is the lowest, ace is the highest) wins the pot.",
                "During an exchange of cards, the discarded card is placed either in the L pile, indicating to the other agent that it was a low numbered card less than 8, or in the 820 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) H pile, indicating that the card had a rank greater than or equal to 8.",
                "Notice that, for example, if a lower numbered card is discarded, the probability of receiving a low card in exchange is now reduced.",
                "We show the level 1 I-ID for the simplified two-player Poker in Fig. 11.",
                "We considered two models (personality types) of agent j.",
                "The conservative type believes that it is likely that its opponent has a high numbered card in its hand.",
                "On the other hand, the aggressive agent j believes with a high probability that its opponent has a lower numbered card.",
                "Thus, the two types differ in their beliefs over their opponents hand.",
                "In both these level 0 models, the opponent is assumed to perform its actions following a fixed, uniform distribution.",
                "With three actions to go, regardless of its hand (unless it is an ace), the aggressive agent chooses to exchange its card, with the intent of improving on its current hand.",
                "This is because it believes the other to have a low card, which improves its chances of getting a high card during the exchange.",
                "The conservative agent chooses to keep its card, no matter its hand because its chances of getting a high card are slim as it believes that its opponent has one.",
                "Figure 11: (a) Level 1 I-ID of agent i.",
                "The observation reveals information about js hand of the previous time step, (b) level 0 IDs of agent j whose decision nodes are mapped to the chance nodes, A1 j , A2 j , in (a).",
                "The policy of a level 1 agent i who believes that each card except its own has an equal likelihood of being in js hand (neutral personality type) and j could be either an aggressive or conservative type, is shown in Fig. 12. is own hand contains the card numbered 8.",
                "The agent starts by keeping its card.",
                "On seeing that j did not exchange a card (N), i believes with probability 1 that j is conservative and hence will keep its cards. i responds by either keeping its card or exchanging it because j is equally likely to have a lower or higher card.",
                "If i observes that j discarded its card into the L or H pile, i believes that j is aggressive.",
                "On observing L, i realizes that j had a low card, and is likely to have a high card after its exchange.",
                "Because the probability of receiving a low card is high now, i chooses to keep its card.",
                "On observing H, believing that the probability of receiving a high numbered card is high, i chooses to exchange its card.",
                "In the final step, i chooses to call regardless of its observation history because its belief that j has a higher card is not sufficiently high to conclude that its better to fold and relinquish the payoff.",
                "This is partly due to the fact that an observation of, say, L resets the agent is previous time step beliefs over js hand to the low numbered cards only. 6.",
                "DISCUSSION We showed how DIDs may be extended to I-DIDs that enable online sequential decision-making in uncertain multiagent settings.",
                "Our graphical representation of I-DIDs improves on the previous Figure 12: A level 1 agent is three step policy in the Poker problem. i starts by believing that j is equally likely to be aggressive or conservative and could have any card in its hand with equal probability. work significantly by being more transparent, semantically clear, and capable of being solved using standard algorithms that target DIDs.",
                "I-DIDs extend NIDs to allow sequential decision-making over multiple time steps in the presence of other interacting agents.",
                "I-DIDs may be seen as concise graphical representations for IPOMDPs providing a way to exploit problem structure and carry out online decision-making as the agent acts and observes given its prior beliefs.",
                "We are currently investigating ways to solve I-DIDs approximately with provable bounds on the solution quality.",
                "Acknowledgment: We thank Piotr Gmytrasiewicz for some useful discussions related to this work.",
                "The first author would like to acknowledge the support of a UGARF grant. 7.",
                "REFERENCES [1] R. J. Aumann.",
                "Interactive epistemology i: Knowledge.",
                "International Journal of Game Theory, 28:263-300, 1999. [2] D. Billings, A. Davidson, J. Schaeffer, and D. Szafron.",
                "The challenge of poker.",
                "AIJ, 2001. [3] A. Brandenburger and E. Dekel.",
                "Hierarchies of beliefs and common knowledge.",
                "Journal of Economic Theory, 59:189-198, 1993. [4] C. Camerer.",
                "Behavioral Game Theory: Experiments in Strategic Interaction.",
                "Princeton University Press, 2003. [5] E. Fehr and S. Gachter.",
                "Cooperation and punishment in public goods experiments.",
                "American Economic Review, 90(4):980-994, 2000. [6] D. Fudenberg and D. K. Levine.",
                "The Theory of Learning in Games.",
                "MIT Press, 1998. [7] D. Fudenberg and J. Tirole.",
                "Game Theory.",
                "MIT Press, 1991. [8] Y. Gal and A. Pfeffer.",
                "A language for modeling agents decision-making processes in games.",
                "In AAMAS, 2003. [9] P. Gmytrasiewicz and P. Doshi.",
                "A framework for sequential planning in multiagent settings.",
                "JAIR, 24:49-79, 2005. [10] P. Gmytrasiewicz and E. Durfee.",
                "Rational coordination in multi-agent environments.",
                "JAAMAS, 3(4):319-350, 2000. [11] J. C. Harsanyi.",
                "Games with incomplete information played by bayesian players.",
                "Management Science, 14(3):159-182, 1967. [12] R. A. Howard and J. E. Matheson.",
                "Influence diagrams.",
                "In R. A. Howard and J. E. Matheson, editors, The Principles and Applications of Decision Analysis.",
                "Strategic Decisions Group, Menlo Park, CA 94025, 1984. [13] L. Kaelbling, M. Littman, and A. Cassandra.",
                "Planning and acting in partially observable stochastic domains.",
                "Artificial Intelligence Journal, 2, 1998. [14] D. Koller and B. Milch.",
                "Multi-agent influence diagrams for representing and solving games.",
                "In IJCAI, pages 1027-1034, 2001. [15] K. Polich and P. Gmytrasiewicz.",
                "Interactive dynamic influence diagrams.",
                "In GTDT Workshop, AAMAS, 2006. [16] B.",
                "Rathnas., P. Doshi, and P. J. Gmytrasiewicz.",
                "Exact solutions to interactive pomdps using behavioral equivalence.",
                "In Autonomous Agents and Multi-Agent Systems Conference (AAMAS), 2006. [17] S. Russell and P. Norvig.",
                "Artificial Intelligence: A Modern Approach (Second Edition).",
                "Prentice Hall, 2003. [18] R. D. Shachter.",
                "Evaluating influence diagrams.",
                "Operations Research, 34(6):871-882, 1986. [19] D. Suryadi and P. Gmytrasiewicz.",
                "Learning models of other agents using influence diagrams.",
                "In UM, 1999.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 821"
            ],
            "original_annotated_samples": [
                "INTRODUCTION Interactive partially observable Markov decision processes (IPOMDPs) [9] provide a framework for sequential decision-making in partially observable <br>multiagent environment</br>s."
            ],
            "translated_annotated_samples": [
                "Las Procesos de Decisión de Markov Interactivos Parcialmente Observables (IPOMDPs) proporcionan un marco para la toma de decisiones secuenciales en <br>entornos multiagentes</br> parcialmente observables."
            ],
            "translated_text": "Modelos gráficos para soluciones en línea a POMDP interactivos de Prashant Doshi Dept. Departamento de Ciencias de la Computación Universidad de Georgia Athens, GA 30602, EE. UU. pdoshi@cs.uga.edu Yifeng Zeng Dept. de Ciencias de la Computación Universidad de Aalborg DK-9220 Aalborg, Dinamarca yfzeng@cs.aau.edu Qiongyu Chen Dept. de Ciencias de la Computación Universidad Nacional de Singapur 117543, Singapur chenqy@comp.nus.edu.sg RESUMEN Desarrollamos una nueva representación gráfica para procesos de decisión de Markov parcialmente observables interactivos (I-POMDPs) que es significativamente más transparente y semánticamente clara que la representación anterior. Estos modelos gráficos llamados diagramas de influencia dinámica interactiva (I-DIDs) buscan modelar explícitamente la estructura que a menudo está presente en problemas del mundo real al descomponer la situación en variables de probabilidad y decisión, y las dependencias entre las variables. Los I-DIDs generalizan los DIDs, los cuales pueden ser vistos como representaciones gráficas de POMDPs, a entornos multiagentes de la misma manera que los I-POMDPs generalizan los POMDPs. Los I-DIDs pueden ser utilizados para calcular la política de un agente en línea mientras el agente actúa y observa en un entorno poblado por otros agentes que interactúan entre sí. Usando varios ejemplos, mostramos cómo se pueden aplicar los I-DIDs y demostramos su utilidad. Categorías y Descriptores de Asignaturas I.2.11 [Inteligencia Artificial Distribuida]: Sistemas Multiagentes Términos Generales Teoría 1. Las Procesos de Decisión de Markov Interactivos Parcialmente Observables (IPOMDPs) proporcionan un marco para la toma de decisiones secuenciales en <br>entornos multiagentes</br> parcialmente observables. Generalizan los POMDPs [13] a entornos multiagentes al incluir los modelos computables de los otros agentes en el espacio de estados junto con los estados del entorno físico. Los modelos abarcan toda la información que influye en los comportamientos de los agentes, incluyendo sus preferencias, capacidades y creencias, y por lo tanto son análogos a los tipos en los juegos bayesianos [11]. I-POMDPs adoptan un enfoque subjetivo para comprender el comportamiento estratégico, arraigado en un marco de teoría de decisiones que toma la perspectiva de los tomadores de decisiones en la interacción. En [15], Polich y Gmytrasiewicz introdujeron diagramas de influencia dinámica interactivos (I-DIDs) como las representaciones computacionales de I-POMDPs. Los I-DIDs generalizan los DIDs [12], los cuales pueden ser vistos como contrapartes computacionales de POMDPs, a entornos de múltiples agentes de la misma manera que los I-POMDPs generalizan los POMDPs. Los I-DIDs contribuyen a una creciente línea de trabajo [19] que incluye diagramas de influencia multiagente (MAIDs) [14], y más recientemente, redes de diagramas de influencia (NIDs) [8]. Estos formalismos buscan modelar explícitamente la estructura que a menudo está presente en problemas del mundo real al descomponer la situación en variables de azar y decisiones, y las dependencias entre las variables. Los MAIDs proporcionan una alternativa a las formas normales y extensivas de juego utilizando un formalismo gráfico para representar juegos de información imperfecta con un nodo de decisión para las acciones de cada agente y nodos de azar que capturan la información privada de los agentes. Los MAIDs analizan objetivamente el juego, calculando eficientemente el perfil de equilibrio de Nash al explotar la estructura de independencia. NIDs extienden los MAIDs para incluir la incertidumbre de los agentes sobre el juego que se está jugando y sobre los modelos de los otros agentes. Cada modelo es un MAID y la red de MAIDs se colapsa, de abajo hacia arriba, en un solo MAID para calcular el equilibrio del juego teniendo en cuenta los diferentes modelos de cada agente. Los formalismos gráficos como MAIDs y NIDs abren una área de investigación prometedora que tiene como objetivo representar las interacciones multiagente de manera más transparente. Sin embargo, los MAIDs proporcionan un análisis del juego desde un punto de vista externo y la aplicabilidad de ambos está limitada a juegos estáticos de un solo jugador. Los asuntos son más complejos cuando consideramos interacciones que se extienden en el tiempo, donde las predicciones sobre las acciones futuras de otros deben hacerse utilizando modelos que cambian a medida que los agentes actúan y observan. Los I-DIDs abordan esta brecha al permitir la representación de modelos de otros agentes como los valores de un nodo de modelo especial. Tanto los modelos de otros agentes como las creencias originales de los agentes sobre estos modelos se actualizan con el tiempo utilizando implementaciones especializadas. En este documento, mejoramos la representación preliminar previa del I-DID mostrada en [15] utilizando la idea de que el I-ID estático es un tipo de NID. Por lo tanto, podemos utilizar construcciones de lenguaje específicas de NID, como multiplexores, para representar de manera más transparente el nodo del modelo y, posteriormente, el I-ID. Además, aclaramos la semántica del enlace de política de propósito especial introducido en la representación de I-DID por [15], y mostramos que podría ser reemplazado por enlaces de dependencia tradicionales. En la representación previa del I-DID, la actualización de la creencia de los agentes sobre los modelos de los demás a medida que los agentes actúan y reciben observaciones se denotaba utilizando un enlace especial llamado el enlace de actualización del modelo que conectaba los nodos del modelo a lo largo del tiempo. Explicamos la semántica de este enlace mostrando cómo puede ser implementado utilizando los enlaces de dependencia tradicionales entre los nodos de probabilidad que constituyen los nodos del modelo. El resultado final es una representación de I-DID que es significativamente más transparente, semánticamente clara y capaz de ser implementada utilizando los algoritmos estándar para resolver DIDs. Mostramos cómo los IDIDs pueden ser utilizados para modelar la incertidumbre de un agente sobre los modelos de otros, que a su vez pueden ser I-DIDs. La solución al I-DID es una política que prescribe lo que el agente debe hacer con el tiempo, dadas sus creencias sobre el estado físico y otros modelos. Análogos a los DIDs, los I-DIDs pueden ser utilizados para calcular la política de un agente en línea mientras el agente actúa y observa en un entorno poblado por otros agentes interactuantes. 2. ANTECEDENTES: Los IPOMDPS ANIDADOS FINITAMENTE generalizan los POMDPs a entornos multiagentes al incluir los modelos de otros agentes como parte del espacio de estados [9]. Dado que otros agentes también pueden razonar sobre otros, el espacio de estado interactivo está estratégicamente anidado; contiene creencias sobre los modelos de otros agentes y sus creencias sobre los demás. Para simplificar la presentación, consideramos un agente, i, que está interactuando con otro agente, j. Un I-POMDP finitamente anidado del agente i con un nivel de estrategia l se define como la tupla: I-POMDPi,l = ISi,l, A, Ti, Ωi, Oi, Ri donde: • ISi,l denota un conjunto de estados interactivos definido como, ISi,l = S × Mj,l−1, donde Mj,l−1 = {Θj,l−1 ∪ SMj}, para l ≥ 1, e ISi,0 = S, donde S es el conjunto de estados del entorno físico. Θj,l−1 es el conjunto de modelos intencionales computables del agente j: θj,l−1 = bj,l−1, ˆθj donde el marco, ˆθj = A, Ωj, Tj, Oj, Rj, OCj. Aquí, j es Bayesiano racional y OCj es el criterio de optimalidad de j. SMj es el conjunto de modelos subintencionales de j. Ejemplos simples de modelos subintencionales incluyen un modelo sin información [10] y un modelo de juego ficticio [6], ambos de los cuales son independientes de la historia. Damos una construcción recursiva de abajo hacia arriba del espacio de estados interactivo a continuación. Sí,0 = S, Θj,0 = { bj,0, ˆθj | bj,0 ∈ Δ(ISj,0)} Sí,1 = S × {Θj,0 ∪ SMj}, Θj,1 = { bj,1, ˆθj | bj,1 ∈ Δ(ISj,1)} . . . . . . Sí, l = S × {Θj,l−1 ∪ SMj}, Θj,l = { bj,l, ˆθj | bj,l ∈ Δ(ISj,l)} Formulaciones similares de espacios anidados han aparecido en [1, 3]. • A = Ai × Aj es el conjunto de acciones conjuntas de todos los agentes en el entorno; • Ti : S ×A×S → [0, 1], describe el efecto de las acciones conjuntas en los estados físicos del entorno; • Ωi es el conjunto de observaciones del agente i; • Oi : S × A × Ωi → [0, 1] da la probabilidad de las observaciones dadas el estado físico y la acción conjunta; • Ri : ISi × A → R describe las preferencias del agente sobre sus estados interactivos. Normalmente solo importarán los estados físicos. La política del agente es el mapeo, Ω∗ i → Δ(Ai), donde Ω∗ i es el conjunto de todos los historiales de observación del agente i. Dado que la creencia sobre los estados interactivos forma una estadística suficiente [9], la política también puede ser representada como un mapeo del conjunto de todas las creencias del agente i a una distribución sobre sus acciones, Δ(ISi) → Δ(Ai). 2.1 Actualización de Creencias Análogo a los POMDPs, un agente dentro del marco I-POMDP actualiza su creencia a medida que actúa y observa. Sin embargo, hay dos diferencias que complican la actualización de creencias en entornos multiagentes en comparación con los entornos de un solo agente. Primero, dado que el estado del entorno físico depende de las acciones de ambos agentes, la predicción de cómo cambia el estado físico debe hacerse basándose en la predicción de sus acciones. Segundo, los cambios en los modelos de js deben ser incluidos en la actualización de creencias. Específicamente, si j es intencional, entonces se debe incluir una actualización de las creencias de j debido a su acción y observación. En otras palabras, i tiene que actualizar su creencia basándose en su predicción de lo que j observaría y cómo j actualizaría su creencia. Si el modelo de js es subintencional, entonces las observaciones probables de js se añaden al historial de observaciones contenido en el modelo. Formalmente, tenemos: Pr(ist |at−1 i , bt−1 i,l ) = β ISt−1:mt−1 j =θt j bt−1 i,l (ist−1 ) × at−1 j Pr(at−1 j |θt−1 j,l−1)Oi(st , at−1 i , at−1 j , ot i) ×Ti(st−1 , at−1 i , at−1 j , st ) ot j Oj(st , at−1 i , at−1 j , ot j) ×τ(SEθt j (bt−1 j,l−1, at−1 j , ot j) − bt j,l−1) (1) donde β es la constante de normalización, τ es 1 si su argumento es 0, de lo contrario es 0, Pr(at−1 j |θt−1 j,l−1) es la probabilidad de que at−1 j sea Bayes racional para el agente descrito por el modelo θt−1 j,l−1, y SE(·) es una abreviatura para la actualización de creencias. Para una versión de la actualización de creencias cuando el modelo js es subintencional, consulte [9]. Si el agente j también se modela como un I-POMDP, entonces la actualización de creencias invoca la actualización de creencias de j (a través del término SEθt j ( bt−1 j,l−1 , at−1 j , ot j)), lo que a su vez podría invocar la actualización de creencias de is y así sucesivamente. Esta recursión en la anidación de creencias llega al nivel 0. En este nivel, la actualización de creencias del agente se reduce a una actualización de creencias de un POMDP. Para ilustraciones de la actualización de creencias, detalles adicionales sobre I-POMDPs y cómo se comparan con otros marcos multiagentes, consulte [9]. Iteración de Valor Cada estado de creencia en un I-POMDP finitamente anidado tiene un valor asociado que refleja la máxima ganancia que el agente puede esperar en este estado de creencia: Un ( bi,l, θi ) = max ai∈Ai is∈ISi,l ERi(is, ai)bi,l(is)+ γ oi∈Ωi Pr(oi|ai, bi,l)Un−1 ( SEθi (bi,l, ai, oi), θi ) (2) donde, ERi(is, ai) = aj Ri(is, ai, aj)Pr(aj|mj,l−1) (ya que is = (s, mj,l−1)). La ecuación 2 es una base para la iteración de valor en I-POMDPs. La acción óptima del agente, a∗ i, para el caso de horizonte finito con descuento, es un elemento del conjunto de acciones óptimas para el estado de creencia, OPT(θi), definido como: OPT( bi,l, θi ) = argmax ai∈Ai is∈ISi,l ERi(is, ai)bi,l(is) +γ oi∈Ωi Pr(oi|ai, bi,l)Un ( SEθi (bi,l, ai, oi), θi ) (3) 3. Diagramas de influencia interactivos. Una extensión ingenua de los diagramas de influencia (IDs) a entornos poblados por múltiples agentes es posible tratando a los otros agentes como autómatas, representados mediante nodos de probabilidad. Sin embargo, este enfoque asume que las acciones de los agentes están controladas utilizando una distribución de probabilidad que no cambia con el tiempo. Los diagramas de influencia interactivos (I-IDs) adoptan un enfoque más sofisticado al generalizar los IDs para hacerlos aplicables a entornos compartidos con otros agentes que pueden actuar, observar y actualizar sus creencias. 3.1 Sintaxis Además de los nodos habituales de probabilidad, decisión y utilidad, los IIDs incluyen un nuevo tipo de nodo llamado nodo de modelo. Mostramos un nivel general l I-ID en la Fig. 1(a), donde el nodo del modelo (Mj,l−1) está representado con un hexágono. Observamos que la distribución de probabilidad sobre el nodo de probabilidad, S, y el nodo del modelo juntos representan la creencia del agente sobre sus estados interactivos. Además del modelo 1, el modelo de nivel 0 es un POMDP: las acciones de otros agentes se tratan como eventos exógenos y se incorporan en las funciones T, O y R. El Sexto Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) Figura 1: (a) Un nivel genérico l I-ID para el agente i situado con otro agente j. El hexágono es el nodo modelo (Mj,l−1) cuya estructura mostramos en (b). Los miembros del nodo modelo son ellos mismos I-ID (m1 j,l−1, m2 j,l−1; los diagramas no se muestran aquí por simplicidad) cuyos nodos de decisión se asignan a los nodos de probabilidad correspondientes (A1 j , A2 j). Dependiendo del valor del nodo, Mod[Mj], la distribución de cada uno de los nodos de probabilidad se asigna al nodo Aj. (c) El I-ID transformado con el nodo del modelo reemplazado por los nodos de probabilidad y las relaciones entre ellos. Los I-IDs difieren de los IDs al tener un enlace discontinuo (llamado enlace de política en [15]) entre el nodo del modelo y un nodo de probabilidad, Aj, que representa la distribución sobre las acciones de los otros agentes dada su modelo. En ausencia de otros agentes, el nodo del modelo y el nodo de probabilidad, Aj, desaparecen y los I-ID se convierten en ID tradicionales. El nodo del modelo contiene los modelos computacionales alternativos atribuidos por i al otro agente del conjunto, Θj,l−1 ∪ SMj, donde Θj,l−1 y SMj fueron definidos previamente en la Sección 2. Por lo tanto, un modelo en el nodo del modelo puede ser en sí mismo un I-ID o ID, y la recursión termina cuando un modelo es un ID o subintencional. Dado que el nodo del modelo contiene los modelos alternativos del otro agente como sus valores, su representación no es trivial. En particular, algunos de los modelos dentro del nodo son I-IDs que, al resolverse, generan la política óptima de los agentes en sus nodos de decisión. Cada nodo de decisión se asigna al nodo de probabilidad correspondiente, digamos A1 j, de la siguiente manera: si OPT es el conjunto de acciones óptimas obtenidas al resolver el I-ID (o ID), entonces Pr(aj ∈ A1 j) = 1 |OPT| si aj ∈ OPT, 0 en caso contrario. Tomando prestados los conocimientos de trabajos anteriores [8], observamos que el nodo del modelo y el enlace de política discontinua que lo conecta con el nodo de probabilidad, Aj, podrían representarse como se muestra en la Fig. 1(b). El nodo de decisión de cada nivel l − 1 I-ID se transforma en un nodo de probabilidad, como mencionamos anteriormente, de modo que las acciones con el mayor valor en el nodo de decisión se les asignan probabilidades uniformes en el nodo de probabilidad, mientras que al resto se les asigna probabilidad cero. Los diferentes nodos de probabilidad (A1 j , A2 j ), uno para cada modelo, y adicionalmente, el nodo de probabilidad etiquetado Mod[Mj] forman los padres del nodo de probabilidad, Aj. Por lo tanto, hay tantos nodos de acción (A1 j , A2 j ) en Mj,l−1 como el número de modelos en el soporte de las creencias del agente. La tabla de probabilidad condicional del nodo de probabilidad, Aj, es un multiplexor que asume la distribución de cada uno de los nodos de acción (A1 j , A2 j ) dependiendo del valor de Mod[Mj]. Los valores de Mod[Mj] denotan los diferentes modelos de j. En otras palabras, cuando Mod[Mj] tiene el valor m1 j,l−1, el nodo de probabilidad Aj asume la distribución del nodo A1 j, y Aj asume la distribución de A2 j cuando Mod[Mj] tiene el valor m2 j,l−1. La distribución sobre el nodo, Mod[Mj], es la creencia del agente sobre los modelos de j dado un estado físico. Para más agentes, tendremos tantos nodos de modelo como agentes haya. Observa que la Fig. 1(b) aclara la semántica del enlace de política y muestra cómo puede ser representado utilizando los enlaces de dependencia tradicionales. En la Fig. 1(c), mostramos el I-ID transformado cuando el nodo del modelo es reemplazado por los nodos de probabilidad y las relaciones entre ellos. A diferencia de la representación en [15], no hay enlaces de políticas especiales, sino que el I-ID está compuesto únicamente por aquellos tipos de nodos que se encuentran en IDs tradicionales y relaciones de dependencia entre los nodos. Esto permite que los I-IDs sean representados e implementados utilizando herramientas de aplicación convencionales que apuntan a los IDs. Ten en cuenta que podemos ver el nivel l I-ID como un NID. Específicamente, cada uno de los modelos del nivel l − 1 dentro del nodo del modelo son bloques en el NID (ver Fig. 2). Si el nivel l = 1, cada bloque es un ID tradicional, de lo contrario, si l > 1, cada bloque dentro del NID puede ser un NID en sí mismo. Ten en cuenta que dentro de los I-IDs (o IDs) en cada nivel, solo hay un nodo de decisión único. Por lo tanto, nuestro NID no contiene ningún MAID. Figura 2: Un nivel l I-ID representado como un NID. Las probabilidades asignadas a los bloques del NID son creencias sobre modelos js condicionados a un estado físico. 3.2 Solución La solución de un I-ID procede de manera ascendente y se implementa de forma recursiva. Comenzamos resolviendo los modelos de nivel 0, que, si son intencionales, son identificadores tradicionales. Sus soluciones proporcionan distribuciones de probabilidad sobre las acciones de los otros agentes, las cuales se ingresan en los nodos de probabilidad correspondientes encontrados en el nodo del modelo del nivel 1 I-ID. El mapeo de los nodos de decisión de los modelos de nivel 0 a los nodos de probabilidad se realiza de manera que las acciones con el mayor valor en el nodo de decisión se les asignan probabilidades uniformes en el nodo de probabilidad, mientras que el resto se les asigna probabilidad cero. Dadas las distribuciones sobre las acciones dentro de los diferentes nodos de probabilidad (uno para cada modelo del otro agente), el I-ID de nivel 1 se transforma como se muestra en la Fig. 1(c). Durante la transformación, la tabla de probabilidad condicional (CPT) del nodo, Aj, se llena de tal manera que el nodo asume la distribución de cada uno de los nodos de probabilidad dependiendo del valor del nodo, Mod[Mj]. Como mencionamos anteriormente, los valores del nodo Mod[Mj] denotan los diferentes modelos del otro agente, y su distribución es la creencia del agente sobre los modelos de j condicionados al estado físico. El nivel 1 I-ID transformado es un ID tradicional que puede ser resuelto como us816 The Sixth Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) (a) (b) Figura 3: (a) Un I-DID genérico de dos niveles de tiempo para el agente i en un entorno con otro agente j. Observa el enlace de actualización del modelo punteado que denota la actualización de los modelos de j y la distribución de los modelos a lo largo del tiempo. (b) La semántica del enlace de actualización del modelo utilizando el método estándar de maximización de utilidad esperada [18]. Este procedimiento se lleva a cabo hasta el nivel l I-ID cuya solución proporciona el conjunto no vacío de acciones óptimas que el agente debe realizar dada su creencia. Ten en cuenta que, al igual que los IDs, los I-IDs son adecuados para la toma de decisiones en línea cuando se conoce la creencia actual de los agentes. 4. Diagramas de influencia dinámica interactivos (I-DIDs) Los diagramas de influencia dinámica interactivos (I-DIDs) extienden los I-IDs (y NIDs) para permitir la toma de decisiones secuencial a lo largo de varios pasos de tiempo. Así como los DIDs son representaciones gráficas estructuradas de POMDPs, los I-DIDs son los análogos gráficos en línea para I-POMDPs finitamente anidados. Los I-DIDs pueden ser utilizados para optimizar sobre una mirada anticipada finita dadas las creencias iniciales mientras se interactúa con otros agentes, posiblemente similares. 4.1 Sintaxis Representamos un I-DID de dos cortes temporales en la Fig. 3(a). Además de los nodos del modelo y el enlace de política discontinuo, lo que diferencia a un I-DID de un DID es el enlace de actualización del modelo mostrado como una flecha punteada en la Fig. 3(a). Explicamos la semántica del nodo del modelo y el enlace de la política en la sección anterior; a continuación, describimos las actualizaciones del modelo. La actualización del nodo del modelo con el tiempo implica dos pasos: primero, dados los modelos en el tiempo t, identificamos el conjunto actualizado de modelos que residen en el nodo del modelo en el tiempo t + 1. Recuerda de la Sección 2 que el modelo intencional de un agente incluye sus creencias. Debido a que los agentes actúan y reciben observaciones, sus modelos se actualizan para reflejar sus creencias cambiadas. Dado que el conjunto de acciones óptimas para un modelo podría incluir todas las acciones, y el agente podría recibir cualquiera de las |Ωj| posibles observaciones, el conjunto actualizado en el paso de tiempo t + 1 tendrá como máximo |Mt j,l−1||Aj||Ωj| modelos. Aquí, |Mt j,l−1| es el número de modelos en el paso de tiempo t, |Aj| y |Ωj| son los espacios más grandes de acciones y observaciones respectivamente, entre todos los modelos. Segundo, calculamos la nueva distribución sobre los modelos actualizados dados la distribución original y la probabilidad de que el agente realice la acción y reciba la observación que llevó al modelo actualizado. Estos pasos son parte de la actualización de creencias del agente formalizada utilizando la Ecuación 1. En la Fig. 3(b), mostramos cómo se implementa el enlace de actualización del modelo punteado en el I-DID. Si cada uno de los dos modelos de nivel l − 1 atribuidos a j en el paso de tiempo t resulta en una acción, y j podría hacer una de dos posibles observaciones, entonces el nodo del modelo en el paso de tiempo t + 1 contiene cuatro modelos actualizados (mt+1,1 j,l−1 ,mt+1,2 j,l−1 , mt+1,3 j,l−1 , y mt+1,4 j,l−1 ). Estos modelos difieren en sus creencias iniciales, cada una de las cuales es el resultado de j actualizar sus creencias debido a su acción y una posible observación. Los nodos de decisión en cada uno de los I-DIDs o DIDs que representan los modelos de nivel inferior se asignan a la Figura 4 correspondiente: I-DID transformado con los nodos del modelo y el enlace de actualización del modelo reemplazados por los nodos de probabilidad y las relaciones (en negrita) de los nodos de probabilidad, como se mencionó anteriormente. A continuación, describimos cómo se calcula la distribución sobre el conjunto actualizado de modelos (la distribución sobre el nodo de probabilidad Mod[Mt+1 j ] en Mt+1 j,l−1). La probabilidad de que el modelo actualizado js sea, digamos mt+1,1 j,l−1, depende de la probabilidad de que j realice la acción y reciba la observación que condujo a este modelo, y de la distribución previa sobre los modelos en el paso de tiempo t. Dado que el nodo de probabilidad At j asume la distribución de cada uno de los nodos de acción basados en el valor de Mod[Mt j], la probabilidad de la acción está dada por este nodo de probabilidad. Para obtener la probabilidad de una observación posible js, introducimos el nodo de probabilidad Oj, que dependiendo del valor de Mod[Mt j], asume la distribución del nodo de observación en el modelo de nivel inferior denominado Mod[Mt j]. Dado que la probabilidad de las observaciones js depende del estado físico y de las acciones conjuntas de ambos agentes, el nodo Oj está vinculado con St+1, At j y At i. De manera análoga a At j, la tabla de probabilidad condicional de Oj también es un multiplexor modulado por Mod[Mt j]. Finalmente, la distribución de los modelos previos en el tiempo t se obtiene a partir del nodo de probabilidad, Mod[Mt j ] en Mt j,l−1. Por consiguiente, los nodos de probabilidad, Mod[Mt j], At j y Oj, forman los padres de Mod[Mt+1 j] en Mt+1 j,l−1. Ten en cuenta que el enlace de actualización del modelo puede ser reemplazado por los enlaces de dependencia entre los nodos de probabilidad que constituyen los nodos del modelo en las dos etapas temporales. En la Fig. 4 mostramos los dos cortes temporales I-DID con los nodos del modelo reemplazados por los nodos de probabilidad y las relaciones entre ellos. Los nodos de probabilidad y los enlaces de dependencia que no están en negrita son estándar, generalmente encontrados en DIDs. La expansión del I-DID a lo largo de más pasos de tiempo requiere la repetición de los dos pasos de actualizar el conjunto de modelos que forman el 2. Nota que Oj representa la observación j en el tiempo t + 1. El Sexto Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 817 valores del nodo del modelo y añadiendo las relaciones entre los nodos de probabilidad, tantas veces como enlaces de actualización del modelo haya. Observamos que el conjunto posible de modelos del otro agente j crece exponencialmente con el número de pasos de tiempo. Por ejemplo, después de T pasos, puede haber como máximo |Mt=1 j,l−1|(|Aj||Ωj|)T −1 modelos candidatos residiendo en el nodo del modelo. 4.2 Solución Análoga a los I-ID, la solución a un I-DID de nivel l para el agente i expandido a lo largo de T pasos de tiempo puede llevarse a cabo de forma recursiva. Con el propósito de ilustración, dejemos l=1 y T=2. El método de solución utiliza la técnica estándar de anticipación, proyectando las secuencias de acciones y observaciones de los agentes hacia adelante desde el estado de creencia actual [17], y encontrando las posibles creencias que podría tener en el siguiente paso temporal. Dado que el agente i también tiene una creencia sobre los modelos de j, la búsqueda anticipada incluye descubrir los posibles modelos que j podría tener en el futuro. Por consiguiente, cada uno de los modelos subintencionales o de nivel 0 de js (representados utilizando un DID estándar) en el primer paso de tiempo debe resolverse para obtener su conjunto óptimo de acciones. Estas acciones se combinan con el conjunto de observaciones posibles que j podría hacer en ese modelo, lo que resulta en un conjunto actualizado de modelos candidatos (que incluyen las creencias actualizadas) que podrían describir el comportamiento de j. Las creencias sobre este conjunto actualizado de modelos candidatos se calculan utilizando los métodos estándar de inferencia que utilizan las relaciones de dependencia entre los nodos del modelo, como se muestra en la Figura 3(b). Observamos la naturaleza recursiva de esta solución: al resolver el agente de nivel 1 I-DID, los DIDs de nivel 0 deben ser resueltos. Si el anidamiento de los modelos es más profundo, todos los modelos en todos los niveles, comenzando desde el nivel 0, se resuelven de manera ascendente. Breve descripción del algoritmo recursivo para resolver el agente es el Algoritmo para resolver I-DID. Entrada: nivel l ≥ 1 I-ID o nivel 0 ID, T Fase de Expansión 1. Para t desde 1 hasta T − 1, hacer 2. Si l ≥ 1, entonces llenar Mt+1 j,l−1 3. Para cada mt j en el rango (Mt j,l−1) hacer 4. Llame recursivamente al algoritmo con el l - 1 I-ID (o ID) que representa mt j y el horizonte, T - t + 1 5. Mapea el nodo de decisión del I-ID (o ID) resuelto, OPT(mt j), a un nodo de probabilidad Aj 6. Para cada aj en OPT(mt j) hacer 7. Para cada oj en Oj (parte de mt j) hacer 8. Actualizar la creencia js, bt+1 j ← SE(bt j, aj, oj) 9. mt+1 j ← Nuevo I-ID (o ID) con bt+1 j como la creencia inicial 10. Rango(Mt+1 j,l−1) ∪ ← {mt+1 j } 11. Añadir el nodo del modelo, Mt+1 j,l−1, y los enlaces de dependencia entre Mt j,l−1 y Mt+1 j,l−1 (mostrados en la Fig. 3(b)) 12. Agregue los nodos de probabilidad, decisión y utilidad para la franja de tiempo t + 1 y los enlaces de dependencia entre ellos 13. Establezca las CPTs para cada nodo de probabilidad y nodo de utilidad de la Fase de Mirada Adelante 14. Aplica el método estándar de anticipación y respaldo para resolver la Figura 5 expandida de I-DID: Algoritmo para resolver un nivel l ≥ 0 I-DID. Nivel l I-DID expandido durante T pasos de tiempo con otro agente j en la Figura 5. Adoptamos un enfoque de dos fases: Dado un I-ID de nivel l (descrito previamente en la Sección 3) con todos los modelos de niveles inferiores representados también como I-IDs o IDs (si es nivel 0), el primer paso es expandir el I-ID de nivel l a lo largo de T pasos de tiempo añadiendo los enlaces de dependencia y las tablas de probabilidad condicional para cada nodo. Nos enfocamos especialmente en establecer y poblar los nodos del modelo (líneas 3-11). Ten en cuenta que Range(·) devuelve los valores (modelos de nivel inferior) de la variable aleatoria dada como entrada (nodo del modelo). En la segunda fase, utilizamos una técnica estándar de anticipación proyectando las secuencias de acción y observación durante T pasos de tiempo en el futuro, y respaldando los valores de utilidad de las creencias alcanzables. Similar a los I-IDs, los I-DIDs se reducen a DIDs en ausencia de otros agentes. Como mencionamos anteriormente, los modelos de nivel 0 son los DIDs tradicionales. Sus soluciones proporcionan distribuciones de probabilidad sobre las acciones del agente modelado en ese nivel a los I-DIDs en el nivel 1. Dadas las distribuciones de probabilidad sobre las acciones de otros agentes, los IDIDs de nivel 1 pueden resolverse como DIDs y proporcionar distribuciones de probabilidad a modelos de niveles superiores. Suponga que el número de modelos considerados en cada nivel está limitado por un número, M. Resolver un I-DID de nivel l es equivalente a resolver O(Ml) DIDs. Para ilustrar la utilidad de los I-DIDs, los aplicamos a tres dominios de problemas. Describimos, en particular, la formulación del I-DID y las prescripciones óptimas obtenidas al resolverlo. 5.1 Seguimiento-Liderazgo en el Problema del Tigre Multiagente Comenzamos nuestras ilustraciones del uso de I-IDs e I-DIDs con una versión ligeramente modificada del problema del tigre multiagente discutido en [9]. El problema tiene dos agentes, cada uno de los cuales puede abrir la puerta derecha (OR), la puerta izquierda (OL) o escuchar (L). Además de escuchar gruñidos (desde la izquierda (GL) o desde la derecha (GR)) cuando escuchan, los agentes también escuchan chirridos (desde la izquierda (CL), desde la derecha (CR) o sin chirridos (S)), que indican ruidosamente que los otros agentes están abriendo una de las puertas. Cuando se abre cualquier puerta, el tigre persiste en su ubicación original con una probabilidad del 95%. El agente i escucha gruñidos con una fiabilidad del 65% y crujidos con una fiabilidad del 95%. El agente J, por otro lado, escucha gruñidos con una fiabilidad del 95%. Por lo tanto, la configuración es tal que el agente i escucha la apertura de puertas del agente j de manera más confiable que los rugidos de los tigres. Esto sugiere que podría usar acciones de JavaScript como indicación de la ubicación del tigre, como discutimos a continuación. Las preferencias de cada agente son como en el juego de un solo agente discutido en [13]. Las funciones de transición, observación y recompensa se muestran en [16]. Un buen indicador de la utilidad de los métodos normativos para la toma de decisiones como los I-DIDs es la aparición de comportamientos sociales realistas en sus prescripciones. En entornos del persistente problema del tigre multiagente que reflejan situaciones del mundo real, demostramos la seguidismo entre los agentes y, como se muestra en [15], el engaño entre agentes que creen que están en una relación de tipo seguidor-líder. En particular, analizamos las condiciones situacionales y epistemológicas suficientes para su surgimiento. El comportamiento de seguidores, por ejemplo, resulta del agente conociendo sus propias debilidades, evaluando las fortalezas, preferencias y posibles comportamientos del otro, y dándose cuenta de que es mejor para él seguir las acciones de los demás para maximizar sus ganancias. Consideremos una configuración particular del problema del tigre en la que el agente i cree que las preferencias de j están alineadas con las suyas: ambos solo quieren obtener el oro, y la audición de j es más confiable en comparación consigo mismo. Como ejemplo, supongamos que j, al escuchar, puede discernir la ubicación del tigre el 95% de las veces en comparación con su precisión del 65%. Además, el agente i no tiene ninguna información inicial sobre la ubicación de los tigres. En otras palabras, la creencia anidada de un solo nivel, bi,1, asigna 0.5 a cada una de las dos ubicaciones del tigre. Además, considera dos modelos de j, que difieren en los niveles iniciales de creencias planas de j. Esto se representa en el nivel 1 I-ID mostrado en la Fig. 6(a). Según un modelo, j asigna una probabilidad de 0.9 a que el tigre esté detrás de la puerta izquierda, mientras que el otro 818 The Sixth Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) Figura 6: (a) Nivel 1 I-ID del agente i, (b) dos IDs de nivel 0 del agente j cuyos nodos de decisión están mapeados a los nodos de probabilidad, A1 j , A2 j , en (a). El modelo asigna 0.1 a esa ubicación (ver Fig. 6(b)). El agente i está indeciso sobre estos dos modelos de j. Si variamos la capacidad auditiva, y resolvemos el nivel 1 correspondiente I-ID expandido en tres pasos de tiempo, obtenemos las políticas de comportamiento normativas mostradas en la Figura 7 que exhiben comportamiento de seguidores. Si la probabilidad de escuchar correctamente los gruñidos es de 0.65, entonces, como se muestra en la política en la Fig. 7(a), i comienza a seguir condicionalmente las acciones de j: i abre la misma puerta que j abrió anteriormente si su evaluación propia de la ubicación de los tigres confirma la elección de j. Si i pierde la capacidad de interpretar correctamente los gruñidos por completo, sigue ciegamente a j y abre la misma puerta que j abrió anteriormente (Fig. 7(b)). Figura 7: Emergencia de (a) seguidores condicionales, y (b) seguidores ciegos en el problema del tigre. Los comportamientos de interés están en negrita. * es un comodín y representa cualquiera de las observaciones. Observamos que un solo nivel de anidación de creencias, creencias sobre los modelos de los demás, fue suficiente para que surgiera el seguidismo en el problema del tigre. Sin embargo, los requisitos epistemológicos para el surgimiento del liderazgo son más complejos. Para que un agente, digamos j, surja como líder, primero debe surgir el seguidismo en el otro agente i. Como mencionamos anteriormente, si i está seguro de que sus preferencias son idénticas a las de j, y cree que j tiene un mejor sentido del oído, i seguirá las acciones de j con el tiempo. El agente j emerge como líder si cree que lo seguiré, lo que implica que la creencia de j debe estar anidada dos niveles de profundidad para permitirle reconocer su papel de liderazgo. Darse cuenta de que lo seguiré presenta a J una oportunidad para influir en sus acciones en beneficio del bien colectivo o de su interés propio solamente. Por ejemplo, en el problema del tigre, consideremos una situación en la que si tanto i como j abren la puerta correcta, entonces cada uno recibe un pago de 20 que es el doble del original. Si solo j selecciona la puerta correcta, obtiene la recompensa de 10. Por otro lado, si ambos agentes eligen la puerta incorrecta, sus penalizaciones se reducen a la mitad. En este entorno, es tanto en el mejor interés de j como en el beneficio colectivo que utilice su experiencia para seleccionar la puerta correcta, y así ser un buen líder. Sin embargo, considera un problema ligeramente diferente en el que j se beneficia de la pérdida de i y es penalizado si i se beneficia. Específicamente, dejemos que la ganancia se reste de js, indicando que j es antagonista hacia i: si j elige la puerta correcta y i la incorrecta, entonces la pérdida de 100 se convierte en la ganancia de js. El agente J cree que yo incorrectamente pienso que las preferencias de J son aquellas que promueven el bien colectivo y que comienza creyendo con un 99% de confianza dónde está el tigre. Dado que creo que mis preferencias son similares a las de j, y que j comienza creyendo casi con certeza que una de las dos ubicaciones es la correcta (dos modelos de nivel 0 de j), comenzaré siguiendo las acciones de j. Mostramos la política normativa para resolver su I-DID anidado individualmente durante tres pasos de tiempo en la Fig. 8(a). La política demuestra que seguiré ciegamente las acciones de js. Dado que el tigre persiste en su ubicación original con una probabilidad del 0.95, seleccionaré nuevamente la misma puerta. Si j comienza el juego con una probabilidad del 99% de que el tigre esté a la derecha, resolver su I-DID anidado dos niveles profundamente, resulta en la política mostrada en la Fig. 8(b). Aunque j está casi seguro de que OL es la acción correcta, comenzará seleccionando OR, seguido de OL. La intención del agente js es engañar a i, a quien cree que seguirá las acciones de js, para así obtener $110 en el segundo paso de tiempo, lo cual es más de lo que j ganaría si fuera honesto. Figura 8: Emergencia del engaño entre agentes en el problema del tigre. Los comportamientos de interés están en negrita. * denota como antes. (a) El agente es una política que demuestra que seguirá ciegamente las acciones de js. (b) Aunque j está casi seguro de que el tigre está a la derecha, comenzará seleccionando OR, seguido de OL, para engañar a i. 5.2 Altruismo y reciprocidad en el problema del bien público El problema del bien público (PG) [7], consiste en un grupo de M agentes, cada uno de los cuales debe contribuir con algún recurso a una olla pública o guardarlo para sí mismos. Dado que los recursos aportados al fondo público se comparten entre todos los agentes, son menos valiosos para el agente cuando están en el fondo público. Sin embargo, si todos los agentes eligen contribuir con sus recursos, entonces la recompensa para cada agente es mayor que si nadie contribuye. Dado que un agente recibe su parte del bote público independientemente de si ha contribuido o no, la acción dominante es que cada agente no contribuya y en su lugar se aproveche de las contribuciones de los demás. Sin embargo, los comportamientos de los jugadores humanos en simulaciones empíricas del problema de PG difieren de las predicciones normativas. Los experimentos revelan que muchos jugadores inicialmente contribuyen con una gran cantidad al bote público, y continúan contribuyendo cuando se juega el problema del PG repetidamente, aunque en cantidades decrecientes [4]. Muchos de estos experimentos [5] informan que un pequeño grupo central de jugadores contribuye persistentemente al bote público incluso cuando todos los demás están desertando. Estos experimentos también revelan que los jugadores que contribuyen persistentemente tienen preferencias altruistas o recíprocas que coinciden con la cooperación esperada de los demás. Para simplificar, asumimos que el juego se juega entre M = 2 agentes, i y j. Que cada agente esté inicialmente dotado con una cantidad de recursos XT. Mientras que la formulación clásica del juego PG permite que cada agente contribuya con cualquier cantidad de recursos (≤ XT) al bote público, simplificamos el espacio de acciones al permitir dos acciones posibles. Cada agente puede elegir contribuir (C) una cantidad fija de recursos, o no contribuir. La última acción es deThe Sixth Intl. La Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 819 fue señalada como defectuosa (D). Suponemos que las acciones no son observables por otros. El valor de los recursos en la bolsa pública se descuenta por ci para cada agente i, donde ci es el retorno privado marginal. Suponemos que ci < 1 para que el agente no se beneficie lo suficiente como para contribuir al bote público en beneficio propio. Simultáneamente, ciM > 1, lo que hace que la contribución colectiva sea óptima de Pareto. i/j C D C 2ciXT , 2cjXT ciXT − cp, XT + cjXT − P D XT + ciXT − P, cjXT − cp XT , XT Tabla 1: El juego PG de un solo disparo con castigo. Para fomentar las contribuciones, los agentes contribuyentes castigan a los que no colaboran pero incurren en un pequeño costo por administrar el castigo. Sea P la sanción impuesta al agente que se desvía y cp el costo no nulo de castigar al agente contribuyente. Para simplificar, asumimos que el costo de castigar es el mismo para ambos agentes. El juego PG de un solo disparo con castigo se muestra en la Tabla 1. Si ci = cj, cp > 0, y si P > XT − ciXT, entonces la deserción ya no es una acción dominante. Si P < XT − ciXT, entonces la deserción es la acción dominante para ambos. Si P = XT − ciXT, entonces el juego no es resoluble por dominancia. Figura 9: (a) Nivel 1 I-ID del agente i, (b) IDs de nivel 0 del agente j con nodos de decisión mapeados a los nodos de probabilidad, A1 j y A2 j, en (a). Formulamos una versión secuencial del problema PG con castigo desde la perspectiva del agente i. Aunque en el juego de PG repetido, la cantidad en la olla pública se revela a todos los agentes después de cada ronda de acciones, asumimos en nuestra formulación que está oculta para los agentes. Cada agente puede contribuir con una cantidad fija, xc, o desertar. Un agente al realizar una acción recibe una observación de abundante (PY) o escasa (MR) simbolizando el estado de la olla pública. Ten en cuenta que las observaciones también son indirectamente indicativas de las acciones del agente js porque el estado del bote público es influenciado por ellas. La cantidad de recursos en el agente es una olla privada, es perfectamente observable para i. Los pagos son análogos a la Tabla 1. Tomando prestado de las investigaciones empíricas del problema PG [5], construimos IDs de nivel 0 para j que modelan tipos altruistas y no altruistas (Fig. 9(b)). Específicamente, nuestro agente altruista tiene un alto retorno privado marginal (cj está cerca de 1) y no castiga a los demás que incumplen. Que xc = 1 y el agente de nivel 0 sea castigado la mitad de las veces que se desvía. Con una acción restante, ambos tipos de agentes eligen contribuir para evitar ser castigados. Con dos acciones por delante, el tipo altruista elige contribuir, mientras que el otro defecta. Esto se debe a que cj para el tipo altruista es cercano a 1, por lo tanto, el castigo esperado, 0.5P > (1 − cj), que el tipo altruista evita. Debido a que cj para el tipo no altruista es menor, prefiere no contribuir. Con tres pasos por delante, el agente altruista contribuye para evitar el castigo (0.5P > 2(1 − cj)), mientras que el tipo no altruista se desentiende. Para más de tres pasos, mientras el agente altruista continúa contribuyendo al bote público dependiendo de qué tan cercano esté su retorno privado marginal a 1, el tipo no altruista prescribe la deserción. Analizamos las decisiones de un agente altruista i modelado usando un nivel 1 I-DID expandido en 3 pasos de tiempo. i atribuye los dos modelos de nivel 0, mencionados anteriormente, a j (ver Fig. 9). Si creo con una probabilidad de 1 que j es altruista, elijo contribuir en cada uno de los tres pasos. Este comportamiento persiste cuando i no está al tanto de si j es altruista (Fig. 10(a)), y cuando i asigna una alta probabilidad a que j sea del tipo no altruista. Sin embargo, cuando i cree con una probabilidad de 1 que j no es altruista y por lo tanto seguramente va a traicionar, i elige traicionar para evitar ser castigado y porque su beneficio privado marginal es menor que 1. Estos resultados demuestran que el comportamiento de nuestro tipo altruista se asemeja al encontrado experimentalmente. El agente de nivel 1 no altruista elige desertar independientemente de lo altruista que crea que sea el otro agente. Analizamos el comportamiento de un tipo de agente recíproco que se ajusta a la cooperación o defección esperada. El retorno privado marginal del tipo recíproco es similar al del tipo no altruista, sin embargo, obtiene una mayor recompensa cuando su acción es similar a la del otro. Consideramos el caso en el que el agente recíproco i no está seguro de si j es altruista y cree que es probable que el bote público esté medio lleno. Para esta creencia previa, yo elijo defectar. Al recibir una observación de abundancia, decide contribuir, mientras que una observación de escasez lo hace defectar (Fig. 10(b)). Esto se debe a que una observación de abundancia indica que es probable que la olla esté más llena que a la mitad, lo cual resulta de la acción de js para contribuir. Por lo tanto, entre los dos modelos atribuidos a j, es probable que su tipo sea altruista, lo que hace probable que j contribuya nuevamente en el próximo paso de tiempo. El agente i, por lo tanto, elige contribuir para corresponder la acción de js. Un razonamiento análogo lleva a uno a defectar cuando observa una olla escasa. Con una acción por hacer, creo que si j contribuye, también elegiré contribuir para evitar castigo independientemente de sus observaciones. Figura 10: (a) Un agente altruista de nivel 1 siempre contribuye. (b) Un agente recíproco i comienza defraudando y luego elige contribuir o defraudar basándose en su observación de abundancia (indicando que j es probablemente altruista) o escasez (j no es altruista). 5.3 Estrategias en el póker de dos jugadores El póker es un popular juego de cartas de suma cero que ha recibido mucha atención en la comunidad de investigación de IA como un banco de pruebas [2]. El póker se juega entre M ≥ 2 jugadores, en el que cada jugador recibe una mano de cartas de una baraja. Si bien existen varias variantes de póker con diferentes niveles de complejidad, consideramos una versión simple en la que cada jugador tiene tres turnos durante los cuales el jugador puede intercambiar una carta (E), mantener la mano existente (K), retirarse (F) y abandonar el juego, o apostar (C), lo que requiere que todos los jugadores muestren sus manos. Para mantener las cosas simples, dejemos que M = 2, y que cada jugador reciba una mano compuesta por una sola carta sacada del mismo palo. Por lo tanto, durante un enfrentamiento, el jugador que tenga la carta numéricamente más alta (el 2 es el más bajo, el as es el más alto) gana el bote. Durante un intercambio de cartas, la carta descartada se coloca ya sea en la pila L, indicando al otro agente que era una carta de número bajo menor que 8, o en la 820 The Sixth Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) H, indicando que la carta tenía un rango mayor o igual a 8. Ten en cuenta que, por ejemplo, si se descarta una carta de número bajo, la probabilidad de recibir una carta baja a cambio se reduce. Mostramos el nivel 1 I-ID para el Poker simplificado de dos jugadores en la Fig. 11. Consideramos dos modelos (tipos de personalidad) del agente j. El tipo conservador cree que es probable que su oponente tenga una carta de alto número en su mano. Por otro lado, el agente agresivo j cree con alta probabilidad que su oponente tiene una carta de número más bajo. Por lo tanto, los dos tipos difieren en sus creencias sobre la mano de sus oponentes. En ambos estos modelos de nivel 0, se asume que el oponente realiza sus acciones siguiendo una distribución fija y uniforme. Con tres acciones restantes, independientemente de su mano (a menos que sea un as), el agente agresivo elige intercambiar su carta, con la intención de mejorar su mano actual. Esto se debe a que cree que el otro tiene una carta baja, lo que mejora sus posibilidades de obtener una carta alta durante el intercambio. El agente conservador elige quedarse con su carta, sin importar su mano, porque considera que sus posibilidades de obtener una carta alta son escasas, ya que cree que su oponente tiene una. Figura 11: (a) Nivel 1 I-ID del agente i. La observación revela información sobre la mano de js del paso de tiempo anterior, (b) IDs de nivel 0 del agente j cuyos nodos de decisión están mapeados a los nodos de probabilidad, A1 j , A2 j , en (a). La política de un agente de nivel 1 i que cree que cada carta, excepto la suya, tiene la misma probabilidad de estar en su mano (tipo de personalidad neutral) y j podría ser de tipo agresivo o conservador, se muestra en la Figura 12. Su mano contiene la carta numerada 8. El agente comienza por mantener su carta. Al ver que j no intercambió una carta (N), i cree con probabilidad 1 que j es conservador y, por lo tanto, mantendrá sus cartas. i responde manteniendo su carta o intercambiándola porque j tiene igual probabilidad de tener una carta más baja o más alta. Si observo que j descartó su carta en la pila L o H, creo que j es agresivo. Al observar a L, i se da cuenta de que j tenía una carta baja, y es probable que tenga una carta alta después de su intercambio. Dado que la probabilidad de recibir una carta baja es alta en este momento, i decide quedarse con su carta. Al observar la carta H, creyendo que la probabilidad de recibir una carta de número alto es alta, i decide intercambiar su carta. En el paso final, i decide llamar independientemente de su historial de observaciones porque su creencia de que j tiene una carta más alta no es lo suficientemente alta como para concluir que es mejor retirarse y renunciar al pago. Esto se debe en parte al hecho de que una observación de, digamos, L, restablece las creencias del agente en el paso de tiempo anterior sobre las cartas de números bajos solamente. 6. DISCUSIÓN Mostramos cómo los DIDs pueden ser extendidos a los I-DIDs que permiten la toma de decisiones secuenciales en línea en entornos multiagentes inciertos. Nuestra representación gráfica de I-DIDs mejora la Figura 12 anterior: Un agente de nivel 1 es una política de tres pasos en el problema de Poker. i comienza creyendo que j tiene la misma probabilidad de ser agresivo o conservador y podría tener cualquier carta en su mano con igual probabilidad. Trabaja significativamente al ser más transparente, semánticamente claro y capaz de ser resuelto utilizando algoritmos estándar que apuntan a los DIDs. Los I-DIDs extienden los NIDs para permitir la toma de decisiones secuenciales a lo largo de múltiples pasos de tiempo en presencia de otros agentes que interactúan. Los I-DIDs pueden ser vistos como representaciones gráficas concisas para IPOMDPs que proporcionan una forma de explotar la estructura del problema y llevar a cabo la toma de decisiones en línea a medida que el agente actúa y observa dadas sus creencias previas. Actualmente estamos investigando formas de resolver I-DIDs aproximadamente con límites demostrables en la calidad de la solución. Agradecimiento: Agradecemos a Piotr Gmytrasiewicz por algunas discusiones útiles relacionadas con este trabajo. El primer autor desea agradecer el apoyo de una beca UGARF. 7. REFERENCIAS [1] R. J. Aumann. Epistemología interactiva i: Conocimiento. Revista Internacional de Teoría de Juegos, 28:263-300, 1999. [2] D. Billings, A. Davidson, J. Schaeffer y D. Szafron. El desafío del póker. AIJ, 2001. [3] A. Brandenburger y E. Dekel. Jerarquías de creencias y conocimiento común. Revista de Teoría Económica, 59:189-198, 1993. [4] C. Camerer. Teoría de juegos conductual: Experimentos en interacción estratégica. Princeton University Press, 2003. [5] E. Fehr y S. Gachter. Cooperación y castigo en experimentos de bienes públicos. Revista Económica Americana, 90(4):980-994, 2000. [6] D. Fudenberg y D. K. Levine. La Teoría del Aprendizaje en los Juegos. MIT Press, 1998. [7] D. Fudenberg y J. Tirole. Teoría de juegos. MIT Press, 1991. [8] Y. Gal y A. Pfeffer. Un lenguaje para modelar los procesos de toma de decisiones de agentes en juegos. En AAMAS, 2003. [9] P. Gmytrasiewicz y P. Doshi. Un marco para la planificación secuencial en entornos multiagentes. JAIR, 24:49-79, 2005. [10] P. Gmytrasiewicz y E. Durfee. Coordinación racional en entornos multiagente. JAAMAS, 3(4):319-350, 2000. [11] J. C. Harsanyi. Juegos con información incompleta jugados por jugadores bayesianos. Ciencia de la Gestión, 14(3):159-182, 1967. [12] R. A. Howard y J. E. Matheson. Diagramas de influencia. En R. A. Howard y J. E. Matheson, editores, Los Principios y Aplicaciones del Análisis de Decisiones. Grupo de Decisiones Estratégicas, Menlo Park, CA 94025, 1984. [13] L. Kaelbling, M. Littman y A. Cassandra. Planificación y actuación en dominios estocásticos parcialmente observables. Revista de Inteligencia Artificial, 2, 1998. [14] D. Koller y B. Milch. Diagramas de influencia multiagente para representar y resolver juegos. En IJCAI, páginas 1027-1034, 2001. [15] K. Polich y P. Gmytrasiewicz. Diagramas de influencia interactivos y dinámicos. En el taller GTDT, AAMAS, 2006. [16] B. Rathnas, P. Doshi y P. J. Gmytrasiewicz. Soluciones exactas para POMDP interactivos utilizando equivalencia conductual. En la Conferencia de Agentes Autónomos y Sistemas Multiagente (AAMAS), 2006. [17] S. Russell y P. Norvig. Inteligencia Artificial: Un Enfoque Moderno (Segunda Edición). Prentice Hall, 2003. [18] R. D. Shachter. \n\nPrentice Hall, 2003. [18] R. D. Shachter. Evaluando diagramas de influencia. Investigación de Operaciones, 34(6):871-882, 1986. [19] D. Suryadi y P. Gmytrasiewicz. Aprendiendo modelos de otros agentes utilizando diagramas de influencia. En UM, 1999. El Sexto Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 821 ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "nash equilibrium profile": {
            "translated_key": "perfil de equilibrio de Nash",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Graphical Models for Online Solutions to Interactive POMDPs Prashant Doshi Dept.",
                "of Computer Science University of Georgia Athens, GA 30602, USA pdoshi@cs.uga.edu Yifeng Zeng Dept. of Computer Science Aalborg University DK-9220 Aalborg, Denmark yfzeng@cs.aau.edu Qiongyu Chen Dept. of Computer Science National Univ. of Singapore 117543, Singapore chenqy@comp.nus.edu.sg ABSTRACT We develop a new graphical representation for interactive partially observable Markov decision processes (I-POMDPs) that is significantly more transparent and semantically clear than the previous representation.",
                "These graphical models called interactive dynamic influence diagrams (I-DIDs) seek to explicitly model the structure that is often present in real-world problems by decomposing the situation into chance and decision variables, and the dependencies between the variables.",
                "I-DIDs generalize DIDs, which may be viewed as graphical representations of POMDPs, to multiagent settings in the same way that I-POMDPs generalize POMDPs.",
                "I-DIDs may be used to compute the policy of an agent online as the agent acts and observes in a setting that is populated by other interacting agents.",
                "Using several examples, we show how I-DIDs may be applied and demonstrate their usefulness.",
                "Categories and Subject Descriptors I.2.11 [Distributed Artificial Intelligence]: Multiagent Systems General Terms Theory 1.",
                "INTRODUCTION Interactive partially observable Markov decision processes (IPOMDPs) [9] provide a framework for sequential decision-making in partially observable multiagent environments.",
                "They generalize POMDPs [13] to multiagent settings by including the other agents computable models in the state space along with the states of the physical environment.",
                "The models encompass all information influencing the agents behaviors, including their preferences, capabilities, and beliefs, and are thus analogous to types in Bayesian games [11].",
                "I-POMDPs adopt a subjective approach to understanding strategic behavior, rooted in a decision-theoretic framework that takes a decision-makers perspective in the interaction.",
                "In [15], Polich and Gmytrasiewicz introduced interactive dynamic influence diagrams (I-DIDs) as the computational representations of I-POMDPs.",
                "I-DIDs generalize DIDs [12], which may be viewed as computational counterparts of POMDPs, to multiagents settings in the same way that I-POMDPs generalize POMDPs.",
                "I-DIDs contribute to a growing line of work [19] that includes multi-agent influence diagrams (MAIDs) [14], and more recently, networks of influence diagrams (NIDs) [8].",
                "These formalisms seek to explicitly model the structure that is often present in real-world problems by decomposing the situation into chance and decision variables, and the dependencies between the variables.",
                "MAIDs provide an alternative to normal and extensive game forms using a graphical formalism to represent games of imperfect information with a decision node for each agents actions and chance nodes capturing the agents private information.",
                "MAIDs objectively analyze the game, efficiently computing the <br>nash equilibrium profile</br> by exploiting the independence structure.",
                "NIDs extend MAIDs to include agents uncertainty over the game being played and over models of the other agents.",
                "Each model is a MAID and the network of MAIDs is collapsed, bottom up, into a single MAID for computing the equilibrium of the game keeping in mind the different models of each agent.",
                "Graphical formalisms such as MAIDs and NIDs open up a promising area of research that aims to represent multiagent interactions more transparently.",
                "However, MAIDs provide an analysis of the game from an external viewpoint and the applicability of both is limited to static single play games.",
                "Matters are more complex when we consider interactions that are extended over time, where predictions about others future actions must be made using models that change as the agents act and observe.",
                "I-DIDs address this gap by allowing the representation of other agents models as the values of a special model node.",
                "Both, other agents models and the original agents beliefs over these models are updated over time using special-purpose implementations.",
                "In this paper, we improve on the previous preliminary representation of the I-DID shown in [15] by using the insight that the static I-ID is a type of NID.",
                "Thus, we may utilize NID-specific language constructs such as multiplexers to represent the model node, and subsequently the I-ID, more transparently.",
                "Furthermore, we clarify the semantics of the special purpose policy link introduced in the representation of I-DID by [15], and show that it could be replaced by traditional dependency links.",
                "In the previous representation of the I-DID, the update of the agents belief over the models of others as the agents act and receive observations was denoted using a special link called the model update link that connected the model nodes over time.",
                "We explicate the semantics of this link by showing how it can be implemented using the traditional dependency links between the chance nodes that constitute the model nodes.",
                "The net result is a representation of I-DID that is significantly more transparent, semantically clear, and capable of being implemented using the standard algorithms for solving DIDs.",
                "We show how IDIDs may be used to model an agents uncertainty over others models, that may themselves be I-DIDs.",
                "Solution to the I-DID is a policy that prescribes what the agent should do over time, given its beliefs over the physical state and others models.",
                "Analogous to DIDs, I-DIDs may be used to compute the policy of an agent online as the agent acts and observes in a setting that is populated by other interacting agents. 2.",
                "BACKGROUND: FINITELY NESTED IPOMDPS Interactive POMDPs generalize POMDPs to multiagent settings by including other agents models as part of the state space [9].",
                "Since other agents may also reason about others, the interactive state space is strategically nested; it contains beliefs about other agents models and their beliefs about others.",
                "For simplicity of presentation we consider an agent, i, that is interacting with one other agent, j.",
                "A finitely nested I-POMDP of agent i with a strategy level l is defined as the tuple: I-POMDPi,l = ISi,l, A, Ti, Ωi, Oi, Ri where: • ISi,l denotes a set of interactive states defined as, ISi,l = S × Mj,l−1, where Mj,l−1 = {Θj,l−1 ∪ SMj}, for l ≥ 1, and ISi,0 = S, where S is the set of states of the physical environment.",
                "Θj,l−1 is the set of computable intentional models of agent j: θj,l−1 = bj,l−1, ˆθj where the frame, ˆθj = A, Ωj, Tj, Oj, Rj, OCj .",
                "Here, j is Bayes rational and OCj is js optimality criterion.",
                "SMj is the set of subintentional models of j.",
                "Simple examples of subintentional models include a no-information model [10] and a fictitious play model [6], both of which are history independent.",
                "We give a recursive bottom-up construction of the interactive state space below.",
                "ISi,0 = S, Θj,0 = { bj,0, ˆθj | bj,0 ∈ Δ(ISj,0)} ISi,1 = S × {Θj,0 ∪ SMj}, Θj,1 = { bj,1, ˆθj | bj,1 ∈ Δ(ISj,1)} . . . . . .",
                "ISi,l = S × {Θj,l−1 ∪ SMj}, Θj,l = { bj,l, ˆθj | bj,l ∈ Δ(ISj,l)} Similar formulations of nested spaces have appeared in [1, 3]. • A = Ai × Aj is the set of joint actions of all agents in the environment; • Ti : S ×A×S → [0, 1], describes the effect of the joint actions on the physical states of the environment; • Ωi is the set of observations of agent i; • Oi : S × A × Ωi → [0, 1] gives the likelihood of the observations given the physical state and joint action; • Ri : ISi × A → R describes agent is preferences over its interactive states.",
                "Usually only the physical states will matter.",
                "Agent is policy is the mapping, Ω∗ i → Δ(Ai), where Ω∗ i is the set of all observation histories of agent i.",
                "Since belief over the interactive states forms a sufficient statistic [9], the policy can also be represented as a mapping from the set of all beliefs of agent i to a distribution over its actions, Δ(ISi) → Δ(Ai). 2.1 Belief Update Analogous to POMDPs, an agent within the I-POMDP framework updates its belief as it acts and observes.",
                "However, there are two differences that complicate the belief update in multiagent settings when compared to single agent ones.",
                "First, since the state of the physical environment depends on the actions of both agents, is prediction of how the physical state changes has to be made based on its prediction of js actions.",
                "Second, changes in js models have to be included in is belief update.",
                "Specifically, if j is intentional then an update of js beliefs due to its action and observation has to be included.",
                "In other words, i has to update its belief based on its prediction of what j would observe and how j would update its belief.",
                "If js model is subintentional, then js probable observations are appended to the observation history contained in the model.",
                "Formally, we have: Pr(ist |at−1 i , bt−1 i,l ) = β ISt−1:mt−1 j =θt j bt−1 i,l (ist−1 ) × at−1 j Pr(at−1 j |θt−1 j,l−1)Oi(st , at−1 i , at−1 j , ot i) ×Ti(st−1 , at−1 i , at−1 j , st ) ot j Oj(st , at−1 i , at−1 j , ot j) ×τ(SEθt j (bt−1 j,l−1, at−1 j , ot j) − bt j,l−1) (1) where β is the normalizing constant, τ is 1 if its argument is 0 otherwise it is 0, Pr(at−1 j |θt−1 j,l−1) is the probability that at−1 j is Bayes rational for the agent described by model θt−1 j,l−1, and SE(·) is an abbreviation for the belief update.",
                "For a version of the belief update when js model is subintentional, see [9].",
                "If agent j is also modeled as an I-POMDP, then is belief update invokes js belief update (via the term SEθt j ( bt−1 j,l−1 , at−1 j , ot j)), which in turn could invoke is belief update and so on.",
                "This recursion in belief nesting bottoms out at the 0th level.",
                "At this level, the belief update of the agent reduces to a POMDP belief update. 1 For illustrations of the belief update, additional details on I-POMDPs, and how they compare with other multiagent frameworks, see [9]. 2.2 Value Iteration Each belief state in a finitely nested I-POMDP has an associated value reflecting the maximum payoff the agent can expect in this belief state: Un ( bi,l, θi ) = max ai∈Ai is∈ISi,l ERi(is, ai)bi,l(is)+ γ oi∈Ωi Pr(oi|ai, bi,l)Un−1 ( SEθi (bi,l, ai, oi), θi ) (2) where, ERi(is, ai) = aj Ri(is, ai, aj)Pr(aj|mj,l−1) (since is = (s, mj,l−1)).",
                "Eq. 2 is a basis for value iteration in I-POMDPs.",
                "Agent is optimal action, a∗ i , for the case of finite horizon with discounting, is an element of the set of optimal actions for the belief state, OPT(θi), defined as: OPT( bi,l, θi ) = argmax ai∈Ai is∈ISi,l ERi(is, ai)bi,l(is) +γ oi∈Ωi Pr(oi|ai, bi,l)Un ( SEθi (bi,l, ai, oi), θi ) (3) 3.",
                "INTERACTIVEINFLUENCEDIAGRAMS A naive extension of influence diagrams (IDs) to settings populated by multiple agents is possible by treating other agents as automatons, represented using chance nodes.",
                "However, this approach assumes that the agents actions are controlled using a probability distribution that does not change over time.",
                "Interactive influence diagrams (I-IDs) adopt a more sophisticated approach by generalizing IDs to make them applicable to settings shared with other agents who may act and observe, and update their beliefs. 3.1 Syntax In addition to the usual chance, decision, and utility nodes, IIDs include a new type of node called the model node.",
                "We show a general level l I-ID in Fig. 1(a), where the model node (Mj,l−1) is denoted using a hexagon.",
                "We note that the probability distribution over the chance node, S, and the model node together represents agent is belief over its interactive states.",
                "In addition to the model 1 The 0th level model is a POMDP: Other agents actions are treated as exogenous events and folded into the T, O, and R functions.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 815 Figure 1: (a) A generic level l I-ID for agent i situated with one other agent j.",
                "The hexagon is the model node (Mj,l−1) whose structure we show in (b).",
                "Members of the model node are I-IDs themselves (m1 j,l−1, m2 j,l−1; diagrams not shown here for simplicity) whose decision nodes are mapped to the corresponding chance nodes (A1 j , A2 j ).",
                "Depending on the value of the node, Mod[Mj], the distribution of each of the chance nodes is assigned to the node Aj. (c) The transformed I-ID with the model node replaced by the chance nodes and the relationships between them. node, I-IDs differ from IDs by having a dashed link (called the policy link in [15]) between the model node and a chance node, Aj, that represents the distribution over the other agents actions given its model.",
                "In the absence of other agents, the model node and the chance node, Aj, vanish and I-IDs collapse into traditional IDs.",
                "The model node contains the alternative computational models ascribed by i to the other agent from the set, Θj,l−1 ∪ SMj, where Θj,l−1 and SMj were defined previously in Section 2.",
                "Thus, a model in the model node may itself be an I-ID or ID, and the recursion terminates when a model is an ID or subintentional.",
                "Because the model node contains the alternative models of the other agent as its values, its representation is not trivial.",
                "In particular, some of the models within the node are I-IDs that when solved generate the agents optimal policy in their decision nodes.",
                "Each decision node is mapped to the corresponding chance node, say A1 j , in the following way: if OPT is the set of optimal actions obtained by solving the I-ID (or ID), then Pr(aj ∈ A1 j ) = 1 |OP T | if aj ∈ OPT, 0 otherwise.",
                "Borrowing insights from previous work [8], we observe that the model node and the dashed policy link that connects it to the chance node, Aj, could be represented as shown in Fig. 1(b).",
                "The decision node of each level l − 1 I-ID is transformed into a chance node, as we mentioned previously, so that the actions with the largest value in the decision node are assigned uniform probabilities in the chance node while the rest are assigned zero probability.",
                "The different chance nodes (A1 j , A2 j ), one for each model, and additionally, the chance node labeled Mod[Mj] form the parents of the chance node, Aj.",
                "Thus, there are as many action nodes (A1 j , A2 j ) in Mj,l−1 as the number of models in the support of agent is beliefs.",
                "The conditional probability table of the chance node, Aj, is a multiplexer that assumes the distribution of each of the action nodes (A1 j , A2 j ) depending on the value of Mod[Mj].",
                "The values of Mod[Mj] denote the different models of j.",
                "In other words, when Mod[Mj] has the value m1 j,l−1, the chance node Aj assumes the distribution of the node A1 j , and Aj assumes the distribution of A2 j when Mod[Mj] has the value m2 j,l−1.",
                "The distribution over the node, Mod[Mj], is the agent is belief over the models of j given a physical state.",
                "For more agents, we will have as many model nodes as there are agents.",
                "Notice that Fig. 1(b) clarifies the semantics of the policy link, and shows how it can be represented using the traditional dependency links.",
                "In Fig. 1(c), we show the transformed I-ID when the model node is replaced by the chance nodes and relationships between them.",
                "In contrast to the representation in [15], there are no special-purpose policy links, rather the I-ID is composed of only those types of nodes that are found in traditional IDs and dependency relationships between the nodes.",
                "This allows I-IDs to be represented and implemented using conventional application tools that target IDs.",
                "Note that we may view the level l I-ID as a NID.",
                "Specifically, each of the level l − 1 models within the model node are blocks in the NID (see Fig. 2).",
                "If the level l = 1, each block is a traditional ID, otherwise if l > 1, each block within the NID may itself be a NID.",
                "Note that within the I-IDs (or IDs) at each level, there is only a single decision node.",
                "Thus, our NID does not contain any MAIDs.",
                "Figure 2: A level l I-ID represented as a NID.",
                "The probabilities assigned to the blocks of the NID are is beliefs over js models conditioned on a physical state. 3.2 Solution The solution of an I-ID proceeds in a bottom-up manner, and is implemented recursively.",
                "We start by solving the level 0 models, which, if intentional, are traditional IDs.",
                "Their solutions provide probability distributions over the other agents actions, which are entered in the corresponding chance nodes found in the model node of the level 1 I-ID.",
                "The mapping from the level 0 models decision nodes to the chance nodes is carried out so that actions with the largest value in the decision node are assigned uniform probabilities in the chance node while the rest are assigned zero probability.",
                "Given the distributions over the actions within the different chance nodes (one for each model of the other agent), the level 1 I-ID is transformed as shown in Fig. 1(c).",
                "During the transformation, the conditional probability table (CPT) of the node, Aj, is populated such that the node assumes the distribution of each of the chance nodes depending on the value of the node, Mod[Mj].",
                "As we mentioned previously, the values of the node Mod[Mj] denote the different models of the other agent, and its distribution is the agent is belief over the models of j conditioned on the physical state.",
                "The transformed level 1 I-ID is a traditional ID that may be solved us816 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) (a) (b) Figure 3: (a) A generic two time-slice level l I-DID for agent i in a setting with one other agent j.",
                "Notice the dotted model update link that denotes the update of the models of j and the distribution over the models over time. (b) The semantics of the model update link. ing the standard expected utility maximization method [18].",
                "This procedure is carried out up to the level l I-ID whose solution gives the non-empty set of optimal actions that the agent should perform given its belief.",
                "Notice that analogous to IDs, I-IDs are suitable for online decision-making when the agents current belief is known. 4.",
                "INTERACTIVE DYNAMIC INFLUENCE DIAGRAMS Interactive dynamic influence diagrams (I-DIDs) extend I-IDs (and NIDs) to allow sequential decision-making over several time steps.",
                "Just as DIDs are structured graphical representations of POMDPs, I-DIDs are the graphical online analogs for finitely nested I-POMDPs.",
                "I-DIDs may be used to optimize over a finite look-ahead given initial beliefs while interacting with other, possibly similar, agents. 4.1 Syntax We depict a general two time-slice I-DID in Fig. 3(a).",
                "In addition to the model nodes and the dashed policy link, what differentiates an I-DID from a DID is the model update link shown as a dotted arrow in Fig. 3(a).",
                "We explained the semantics of the model node and the policy link in the previous section; we describe the model updates next.",
                "The update of the model node over time involves two steps: First, given the models at time t, we identify the updated set of models that reside in the model node at time t + 1.",
                "Recall from Section 2 that an agents intentional model includes its belief.",
                "Because the agents act and receive observations, their models are updated to reflect their changed beliefs.",
                "Since the set of optimal actions for a model could include all the actions, and the agent may receive any one of |Ωj| possible observations, the updated set at time step t + 1 will have at most |Mt j,l−1||Aj||Ωj| models.",
                "Here, |Mt j,l−1| is the number of models at time step t, |Aj| and |Ωj| are the largest spaces of actions and observations respectively, among all the models.",
                "Second, we compute the new distribution over the updated models given the original distribution and the probability of the agent performing the action and receiving the observation that led to the updated model.",
                "These steps are a part of agent is belief update formalized using Eq. 1.",
                "In Fig. 3(b), we show how the dotted model update link is implemented in the I-DID.",
                "If each of the two level l − 1 models ascribed to j at time step t results in one action, and j could make one of two possible observations, then the model node at time step t + 1 contains four updated models (mt+1,1 j,l−1 ,mt+1,2 j,l−1 , mt+1,3 j,l−1 , and mt+1,4 j,l−1 ).",
                "These models differ in their initial beliefs, each of which is the result of j updating its beliefs due to its action and a possible observation.",
                "The decision nodes in each of the I-DIDs or DIDs that represent the lower level models are mapped to the corresponding Figure 4: Transformed I-DID with the model nodes and model update link replaced with the chance nodes and the relationships (in bold). chance nodes, as mentioned previously.",
                "Next, we describe how the distribution over the updated set of models (the distribution over the chance node Mod[Mt+1 j ] in Mt+1 j,l−1) is computed.",
                "The probability that js updated model is, say mt+1,1 j,l−1 , depends on the probability of j performing the action and receiving the observation that led to this model, and the prior distribution over the models at time step t. Because the chance node At j assumes the distribution of each of the action nodes based on the value of Mod[Mt j ], the probability of the action is given by this chance node.",
                "In order to obtain the probability of js possible observation, we introduce the chance node Oj, which depending on the value of Mod[Mt j ] assumes the distribution of the observation node in the lower level model denoted by Mod[Mt j ].",
                "Because the probability of js observations depends on the physical state and the joint actions of both agents, the node Oj is linked with St+1 , At j, and At i. 2 Analogous to At j, the conditional probability table of Oj is also a multiplexer modulated by Mod[Mt j ].",
                "Finally, the distribution over the prior models at time t is obtained from the chance node, Mod[Mt j ] in Mt j,l−1.",
                "Consequently, the chance nodes, Mod[Mt j ], At j, and Oj, form the parents of Mod[Mt+1 j ] in Mt+1 j,l−1.",
                "Notice that the model update link may be replaced by the dependency links between the chance nodes that constitute the model nodes in the two time slices.",
                "In Fig. 4 we show the two time-slice I-DID with the model nodes replaced by the chance nodes and the relationships between them.",
                "Chance nodes and dependency links that not in bold are standard, usually found in DIDs.",
                "Expansion of the I-DID over more time steps requires the repetition of the two steps of updating the set of models that form the 2 Note that Oj represents js observation at time t + 1.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 817 values of the model node and adding the relationships between the chance nodes, as many times as there are model update links.",
                "We note that the possible set of models of the other agent j grows exponentially with the number of time steps.",
                "For example, after T steps, there may be at most |Mt=1 j,l−1|(|Aj||Ωj|)T −1 candidate models residing in the model node. 4.2 Solution Analogous to I-IDs, the solution to a level l I-DID for agent i expanded over T time steps may be carried out recursively.",
                "For the purpose of illustration, let l=1 and T=2.",
                "The solution method uses the standard look-ahead technique, projecting the agents action and observation sequences forward from the current belief state [17], and finding the possible beliefs that i could have in the next time step.",
                "Because agent i has a belief over js models as well, the lookahead includes finding out the possible models that j could have in the future.",
                "Consequently, each of js subintentional or level 0 models (represented using a standard DID) in the first time step must be solved to obtain its optimal set of actions.",
                "These actions are combined with the set of possible observations that j could make in that model, resulting in an updated set of candidate models (that include the updated beliefs) that could describe the behavior of j. Beliefs over this updated set of candidate models are calculated using the standard inference methods using the dependency relationships between the model nodes as shown in Fig. 3(b).",
                "We note the recursive nature of this solution: in solving agent is level 1 I-DID, js level 0 DIDs must be solved.",
                "If the nesting of models is deeper, all models at all levels starting from 0 are solved in a bottom-up manner.",
                "We briefly outline the recursive algorithm for solving agent is Algorithm for solving I-DID Input : level l ≥ 1 I-ID or level 0 ID, T Expansion Phase 1.",
                "For t from 1 to T − 1 do 2.",
                "If l ≥ 1 then Populate Mt+1 j,l−1 3.",
                "For each mt j in Range(Mt j,l−1) do 4.",
                "Recursively call algorithm with the l − 1 I-ID (or ID) that represents mt j and the horizon, T − t + 1 5.",
                "Map the decision node of the solved I-ID (or ID), OPT(mt j), to a chance node Aj 6.",
                "For each aj in OPT(mt j) do 7.",
                "For each oj in Oj (part of mt j) do 8.",
                "Update js belief, bt+1 j ← SE(bt j, aj, oj) 9. mt+1 j ← New I-ID (or ID) with bt+1 j as the initial belief 10.",
                "Range(Mt+1 j,l−1) ∪ ← {mt+1 j } 11.",
                "Add the model node, Mt+1 j,l−1, and the dependency links between Mt j,l−1 and Mt+1 j,l−1 (shown in Fig. 3(b)) 12.",
                "Add the chance, decision, and utility nodes for t + 1 time slice and the dependency links between them 13.",
                "Establish the CPTs for each chance node and utility node Look-Ahead Phase 14.",
                "Apply the standard look-ahead and backup method to solve the expanded I-DID Figure 5: Algorithm for solving a level l ≥ 0 I-DID. level l I-DID expanded over T time steps with one other agent j in Fig. 5.",
                "We adopt a two-phase approach: Given an I-ID of level l (described previously in Section 3) with all lower level models also represented as I-IDs or IDs (if level 0), the first step is to expand the level l I-ID over T time steps adding the dependency links and the conditional probability tables for each node.",
                "We particularly focus on establishing and populating the model nodes (lines 3-11).",
                "Note that Range(·) returns the values (lower level models) of the random variable given as input (model node).",
                "In the second phase, we use a standard look-ahead technique projecting the action and observation sequences over T time steps in the future, and backing up the utility values of the reachable beliefs.",
                "Similar to I-IDs, the I-DIDs reduce to DIDs in the absence of other agents.",
                "As we mentioned previously, the 0-th level models are the traditional DIDs.",
                "Their solutions provide probability distributions over actions of the agent modeled at that level to I-DIDs at level 1.",
                "Given probability distributions over other agents actions the level 1 IDIDs can themselves be solved as DIDs, and provide probability distributions to yet higher level models.",
                "Assume that the number of models considered at each level is bound by a number, M. Solving an I-DID of level l in then equivalent to solving O(Ml ) DIDs. 5.",
                "EXAMPLE APPLICATIONS To illustrate the usefulness of I-DIDs, we apply them to three problem domains.",
                "We describe, in particular, the formulation of the I-DID and the optimal prescriptions obtained on solving it. 5.1 Followership-Leadership in the Multiagent Tiger Problem We begin our illustrations of using I-IDs and I-DIDs with a slightly modified version of the multiagent tiger problem discussed in [9].",
                "The problem has two agents, each of which can open the right door (OR), the left door (OL) or listen (L).",
                "In addition to hearing growls (from the left (GL) or from the right (GR)) when they listen, the agents also hear creaks (from the left (CL), from the right (CR), or no creaks (S)), which noisily indicate the other agents opening one of the doors.",
                "When any door is opened, the tiger persists in its original location with a probability of 95%.",
                "Agent i hears growls with a reliability of 65% and creaks with a reliability of 95%.",
                "Agent j, on the other hand, hears growls with a reliability of 95%.",
                "Thus, the setting is such that agent i hears agent j opening doors more reliably than the tigers growls.",
                "This suggests that i could use js actions as an indication of the location of the tiger, as we discuss below.",
                "Each agents preferences are as in the single agent game discussed in [13].",
                "The transition, observation, and reward functions are shown in [16].",
                "A good indicator of the usefulness of normative methods for decision-making like I-DIDs is the emergence of realistic social behaviors in their prescriptions.",
                "In settings of the persistent multiagent tiger problem that reflect real world situations, we demonstrate followership between the agents and, as shown in [15], deception among agents who believe that they are in a follower-leader type of relationship.",
                "In particular, we analyze the situational and epistemological conditions sufficient for their emergence.",
                "The followership behavior, for example, results from the agent knowing its own weaknesses, assessing the strengths, preferences, and possible behaviors of the other, and realizing that its best for it to follow the others actions in order to maximize its payoffs.",
                "Let us consider a particular setting of the tiger problem in which agent i believes that js preferences are aligned with its own - both of them just want to get the gold - and js hearing is more reliable in comparison to itself.",
                "As an example, suppose that j, on listening can discern the tigers location 95% of the times compared to is 65% accuracy.",
                "Additionally, agent i does not have any initial information about the tigers location.",
                "In other words, is single-level nested belief, bi,1, assigns 0.5 to each of the two locations of the tiger.",
                "In addition, i considers two models of j, which differ in js flat level 0 initial beliefs.",
                "This is represented in the level 1 I-ID shown in Fig. 6(a).",
                "According to one model, j assigns a probability of 0.9 that the tiger is behind the left door, while the other 818 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Figure 6: (a) Level 1 I-ID of agent i, (b) two level 0 IDs of agent j whose decision nodes are mapped to the chance nodes, A1 j , A2 j , in (a). model assigns 0.1 to that location (see Fig. 6(b)).",
                "Agent i is undecided on these two models of j.",
                "If we vary is hearing ability, and solve the corresponding level 1 I-ID expanded over three time steps, we obtain the normative behavioral policies shown in Fig 7 that exhibit followership behavior.",
                "If is probability of correctly hearing the growls is 0.65, then as shown in the policy in Fig. 7(a), i begins to conditionally follow js actions: i opens the same door that j opened previously iff is own assessment of the tigers location confirms js pick.",
                "If i loses the ability to correctly interpret the growls completely, it blindly follows j and opens the same door that j opened previously (Fig. 7(b)).",
                "Figure 7: Emergence of (a) conditional followership, and (b) blind followership in the tiger problem.",
                "Behaviors of interest are in bold. * is a wildcard, and denotes any one of the observations.",
                "We observed that a single level of belief nesting - beliefs about the others models - was sufficient for followership to emerge in the tiger problem.",
                "However, the epistemological requirements for the emergence of leadership are more complex.",
                "For an agent, say j, to emerge as a leader, followership must first emerge in the other agent i.",
                "As we mentioned previously, if i is certain that its preferences are identical to those of j, and believes that j has a better sense of hearing, i will follow js actions over time.",
                "Agent j emerges as a leader if it believes that i will follow it, which implies that js belief must be nested two levels deep to enable it to recognize its leadership role.",
                "Realizing that i will follow presents j with an opportunity to influence is actions in the benefit of the collective good or its self-interest alone.",
                "For example, in the tiger problem, let us consider a setting in which if both i and j open the correct door, then each gets a payoff of 20 that is double the original.",
                "If j alone selects the correct door, it gets the payoff of 10.",
                "On the other hand, if both agents pick the wrong door, their penalties are cut in half.",
                "In this setting, it is in both js best interest as well as the collective betterment for j to use its expertise in selecting the correct door, and thus be a good leader.",
                "However, consider a slightly different problem in which j gains from is loss and is penalized if i gains.",
                "Specifically, let is payoff be subtracted from js, indicating that j is antagonistic toward i - if j picks the correct door and i the wrong one, then is loss of 100 becomes js gain.",
                "Agent j believes that i incorrectly thinks that js preferences are those that promote the collective good and that it starts off by believing with 99% confidence where the tiger is.",
                "Because i believes that its preferences are similar to those of j, and that j starts by believing almost surely that one of the two is the correct location (two level 0 models of j), i will start by following js actions.",
                "We show is normative policy on solving its singly-nested I-DID over three time steps in Fig. 8(a).",
                "The policy demonstrates that i will blindly follow js actions.",
                "Since the tiger persists in its original location with a probability of 0.95, i will select the same door again.",
                "If j begins the game with a 99% probability that the tiger is on the right, solving js I-DID nested two levels deep, results in the policy shown in Fig. 8(b).",
                "Even though j is almost certain that OL is the correct action, it will start by selecting OR, followed by OL.",
                "Agent js intention is to deceive i who, it believes, will follow js actions, so as to gain $110 in the second time step, which is more than what j would gain if it were to be honest.",
                "Figure 8: Emergence of deception between agents in the tiger problem.",
                "Behaviors of interest are in bold. * denotes as before. (a) Agent is policy demonstrating that it will blindly follow js actions. (b) Even though j is almost certain that the tiger is on the right, it will start by selecting OR, followed by OL, in order to deceive i. 5.2 Altruism and Reciprocity in the Public Good Problem The public good (PG) problem [7], consists of a group of M agents, each of whom must either contribute some resource to a public pot or keep it for themselves.",
                "Since resources contributed to the public pot are shared among all the agents, they are less valuable to the agent when in the public pot.",
                "However, if all agents choose to contribute their resources, then the payoff to each agent is more than if no one contributes.",
                "Since an agent gets its share of the public pot irrespective of whether it has contributed or not, the dominating action is for each agent to not contribute, and instead free ride on others contributions.",
                "However, behaviors of human players in empirical simulations of the PG problem differ from the normative predictions.",
                "The experiments reveal that many players initially contribute a large amount to the public pot, and continue to contribute when the PG problem is played repeatedly, though in decreasing amounts [4].",
                "Many of these experiments [5] report that a small core group of players persistently contributes to the public pot even when all others are defecting.",
                "These experiments also reveal that players who persistently contribute have altruistic or reciprocal preferences matching expected cooperation of others.",
                "For simplicity, we assume that the game is played between M = 2 agents, i and j.",
                "Let each agent be initially endowed with XT amount of resources.",
                "While the classical PG game formulation permits each agent to contribute any quantity of resources (≤ XT ) to the public pot, we simplify the action space by allowing two possible actions.",
                "Each agent may choose to either contribute (C) a fixed amount of the resources, or not contribute.",
                "The latter action is deThe Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 819 noted as defect (D).",
                "We assume that the actions are not observable to others.",
                "The value of resources in the public pot is discounted by ci for each agent i, where ci is the marginal private return.",
                "We assume that ci < 1 so that the agent does not benefit enough that it contributes to the public pot for private gain.",
                "Simultaneously, ciM > 1, making collective contribution pareto optimal. i/j C D C 2ciXT , 2cjXT ciXT − cp, XT + cjXT − P D XT + ciXT − P, cjXT − cp XT , XT Table 1: The one-shot PG game with punishment.",
                "In order to encourage contributions, the contributing agents punish free riders but incur a small cost for administering the punishment.",
                "Let P be the punishment meted out to the defecting agent and cp the non-zero cost of punishing for the contributing agent.",
                "For simplicity, we assume that the cost of punishing is same for both the agents.",
                "The one-shot PG game with punishment is shown in Table. 1.",
                "Let ci = cj, cp > 0, and if P > XT − ciXT , then defection is no longer a dominating action.",
                "If P < XT − ciXT , then defection is the dominating action for both.",
                "If P = XT − ciXT , then the game is not dominance-solvable.",
                "Figure 9: (a) Level 1 I-ID of agent i, (b) level 0 IDs of agent j with decision nodes mapped to the chance nodes, A1 j and A2 j , in (a).",
                "We formulate a sequential version of the PG problem with punishment from the perspective of agent i.",
                "Though in the repeated PG game, the quantity in the public pot is revealed to all the agents after each round of actions, we assume in our formulation that it is hidden from the agents.",
                "Each agent may contribute a fixed amount, xc, or defect.",
                "An agent on performing an action receives an observation of plenty (PY) or meager (MR) symbolizing the state of the public pot.",
                "Notice that the observations are also indirectly indicative of agent js actions because the state of the public pot is influenced by them.",
                "The amount of resources in agent is private pot, is perfectly observable to i.",
                "The payoffs are analogous to Table. 1.",
                "Borrowing from the empirical investigations of the PG problem [5], we construct level 0 IDs for j that model altruistic and non-altruistic types (Fig. 9(b)).",
                "Specifically, our altruistic agent has a high marginal private return (cj is close to 1) and does not punish others who defect.",
                "Let xc = 1 and the level 0 agent be punished half the times it defects.",
                "With one action remaining, both types of agents choose to contribute to avoid being punished.",
                "With two actions to go, the altruistic type chooses to contribute, while the other defects.",
                "This is because cj for the altruistic type is close to 1, thus the expected punishment, 0.5P > (1 − cj), which the altruistic type avoids.",
                "Because cj for the non-altruistic type is less, it prefers not to contribute.",
                "With three steps to go, the altruistic agent contributes to avoid punishment (0.5P > 2(1 − cj)), and the non-altruistic type defects.",
                "For greater than three steps, while the altruistic agent continues to contribute to the public pot depending on how close its marginal private return is to 1, the non-altruistic type prescribes defection.",
                "We analyzed the decisions of an altruistic agent i modeled using a level 1 I-DID expanded over 3 time steps. i ascribes the two level 0 models, mentioned previously, to j (see Fig. 9).",
                "If i believes with a probability 1 that j is altruistic, i chooses to contribute for each of the three steps.",
                "This behavior persists when i is unaware of whether j is altruistic (Fig. 10(a)), and when i assigns a high probability to j being the non-altruistic type.",
                "However, when i believes with a probability 1 that j is non-altruistic and will thus surely defect, i chooses to defect to avoid being punished and because its marginal private return is less than 1.",
                "These results demonstrate that the behavior of our altruistic type resembles that found experimentally.",
                "The non-altruistic level 1 agent chooses to defect regardless of how likely it believes the other agent to be altruistic.",
                "We analyzed the behavior of a reciprocal agent type that matches expected cooperation or defection.",
                "The reciprocal types marginal private return is similar to that of the non-altruistic type, however, it obtains a greater payoff when its action is similar to that of the other.",
                "We consider the case when the reciprocal agent i is unsure of whether j is altruistic and believes that the public pot is likely to be half full.",
                "For this prior belief, i chooses to defect.",
                "On receiving an observation of plenty, i decides to contribute, while an observation of meager makes it defect (Fig. 10(b)).",
                "This is because an observation of plenty signals that the pot is likely to be greater than half full, which results from js action to contribute.",
                "Thus, among the two models ascribed to j, its type is likely to be altruistic making it likely that j will contribute again in the next time step.",
                "Agent i therefore chooses to contribute to reciprocate js action.",
                "An analogous reasoning leads i to defect when it observes a meager pot.",
                "With one action to go, i believing that j contributes, will choose to contribute too to avoid punishment regardless of its observations.",
                "Figure 10: (a) An altruistic level 1 agent always contributes. (b) A reciprocal agent i starts off by defecting followed by choosing to contribute or defect based on its observation of plenty (indicating that j is likely altruistic) or meager (j is non-altruistic). 5.3 Strategies in Two-Player Poker Poker is a popular zero sum card game that has received much attention among the AI research community as a testbed [2].",
                "Poker is played among M ≥ 2 players in which each player receives a hand of cards from a deck.",
                "While several flavors of Poker with varying complexity exist, we consider a simple version in which each player has three plys during which the player may either exchange a card (E), keep the existing hand (K), fold (F) and withdraw from the game, or call (C), requiring all players to show their hands.",
                "To keep matters simple, let M = 2, and each player receive a hand consisting of a single card drawn from the same suit.",
                "Thus, during a showdown, the player who has the numerically larger card (2 is the lowest, ace is the highest) wins the pot.",
                "During an exchange of cards, the discarded card is placed either in the L pile, indicating to the other agent that it was a low numbered card less than 8, or in the 820 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) H pile, indicating that the card had a rank greater than or equal to 8.",
                "Notice that, for example, if a lower numbered card is discarded, the probability of receiving a low card in exchange is now reduced.",
                "We show the level 1 I-ID for the simplified two-player Poker in Fig. 11.",
                "We considered two models (personality types) of agent j.",
                "The conservative type believes that it is likely that its opponent has a high numbered card in its hand.",
                "On the other hand, the aggressive agent j believes with a high probability that its opponent has a lower numbered card.",
                "Thus, the two types differ in their beliefs over their opponents hand.",
                "In both these level 0 models, the opponent is assumed to perform its actions following a fixed, uniform distribution.",
                "With three actions to go, regardless of its hand (unless it is an ace), the aggressive agent chooses to exchange its card, with the intent of improving on its current hand.",
                "This is because it believes the other to have a low card, which improves its chances of getting a high card during the exchange.",
                "The conservative agent chooses to keep its card, no matter its hand because its chances of getting a high card are slim as it believes that its opponent has one.",
                "Figure 11: (a) Level 1 I-ID of agent i.",
                "The observation reveals information about js hand of the previous time step, (b) level 0 IDs of agent j whose decision nodes are mapped to the chance nodes, A1 j , A2 j , in (a).",
                "The policy of a level 1 agent i who believes that each card except its own has an equal likelihood of being in js hand (neutral personality type) and j could be either an aggressive or conservative type, is shown in Fig. 12. is own hand contains the card numbered 8.",
                "The agent starts by keeping its card.",
                "On seeing that j did not exchange a card (N), i believes with probability 1 that j is conservative and hence will keep its cards. i responds by either keeping its card or exchanging it because j is equally likely to have a lower or higher card.",
                "If i observes that j discarded its card into the L or H pile, i believes that j is aggressive.",
                "On observing L, i realizes that j had a low card, and is likely to have a high card after its exchange.",
                "Because the probability of receiving a low card is high now, i chooses to keep its card.",
                "On observing H, believing that the probability of receiving a high numbered card is high, i chooses to exchange its card.",
                "In the final step, i chooses to call regardless of its observation history because its belief that j has a higher card is not sufficiently high to conclude that its better to fold and relinquish the payoff.",
                "This is partly due to the fact that an observation of, say, L resets the agent is previous time step beliefs over js hand to the low numbered cards only. 6.",
                "DISCUSSION We showed how DIDs may be extended to I-DIDs that enable online sequential decision-making in uncertain multiagent settings.",
                "Our graphical representation of I-DIDs improves on the previous Figure 12: A level 1 agent is three step policy in the Poker problem. i starts by believing that j is equally likely to be aggressive or conservative and could have any card in its hand with equal probability. work significantly by being more transparent, semantically clear, and capable of being solved using standard algorithms that target DIDs.",
                "I-DIDs extend NIDs to allow sequential decision-making over multiple time steps in the presence of other interacting agents.",
                "I-DIDs may be seen as concise graphical representations for IPOMDPs providing a way to exploit problem structure and carry out online decision-making as the agent acts and observes given its prior beliefs.",
                "We are currently investigating ways to solve I-DIDs approximately with provable bounds on the solution quality.",
                "Acknowledgment: We thank Piotr Gmytrasiewicz for some useful discussions related to this work.",
                "The first author would like to acknowledge the support of a UGARF grant. 7.",
                "REFERENCES [1] R. J. Aumann.",
                "Interactive epistemology i: Knowledge.",
                "International Journal of Game Theory, 28:263-300, 1999. [2] D. Billings, A. Davidson, J. Schaeffer, and D. Szafron.",
                "The challenge of poker.",
                "AIJ, 2001. [3] A. Brandenburger and E. Dekel.",
                "Hierarchies of beliefs and common knowledge.",
                "Journal of Economic Theory, 59:189-198, 1993. [4] C. Camerer.",
                "Behavioral Game Theory: Experiments in Strategic Interaction.",
                "Princeton University Press, 2003. [5] E. Fehr and S. Gachter.",
                "Cooperation and punishment in public goods experiments.",
                "American Economic Review, 90(4):980-994, 2000. [6] D. Fudenberg and D. K. Levine.",
                "The Theory of Learning in Games.",
                "MIT Press, 1998. [7] D. Fudenberg and J. Tirole.",
                "Game Theory.",
                "MIT Press, 1991. [8] Y. Gal and A. Pfeffer.",
                "A language for modeling agents decision-making processes in games.",
                "In AAMAS, 2003. [9] P. Gmytrasiewicz and P. Doshi.",
                "A framework for sequential planning in multiagent settings.",
                "JAIR, 24:49-79, 2005. [10] P. Gmytrasiewicz and E. Durfee.",
                "Rational coordination in multi-agent environments.",
                "JAAMAS, 3(4):319-350, 2000. [11] J. C. Harsanyi.",
                "Games with incomplete information played by bayesian players.",
                "Management Science, 14(3):159-182, 1967. [12] R. A. Howard and J. E. Matheson.",
                "Influence diagrams.",
                "In R. A. Howard and J. E. Matheson, editors, The Principles and Applications of Decision Analysis.",
                "Strategic Decisions Group, Menlo Park, CA 94025, 1984. [13] L. Kaelbling, M. Littman, and A. Cassandra.",
                "Planning and acting in partially observable stochastic domains.",
                "Artificial Intelligence Journal, 2, 1998. [14] D. Koller and B. Milch.",
                "Multi-agent influence diagrams for representing and solving games.",
                "In IJCAI, pages 1027-1034, 2001. [15] K. Polich and P. Gmytrasiewicz.",
                "Interactive dynamic influence diagrams.",
                "In GTDT Workshop, AAMAS, 2006. [16] B.",
                "Rathnas., P. Doshi, and P. J. Gmytrasiewicz.",
                "Exact solutions to interactive pomdps using behavioral equivalence.",
                "In Autonomous Agents and Multi-Agent Systems Conference (AAMAS), 2006. [17] S. Russell and P. Norvig.",
                "Artificial Intelligence: A Modern Approach (Second Edition).",
                "Prentice Hall, 2003. [18] R. D. Shachter.",
                "Evaluating influence diagrams.",
                "Operations Research, 34(6):871-882, 1986. [19] D. Suryadi and P. Gmytrasiewicz.",
                "Learning models of other agents using influence diagrams.",
                "In UM, 1999.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 821"
            ],
            "original_annotated_samples": [
                "MAIDs objectively analyze the game, efficiently computing the <br>nash equilibrium profile</br> by exploiting the independence structure."
            ],
            "translated_annotated_samples": [
                "Los MAIDs analizan objetivamente el juego, calculando eficientemente el <br>perfil de equilibrio de Nash</br> al explotar la estructura de independencia."
            ],
            "translated_text": "Modelos gráficos para soluciones en línea a POMDP interactivos de Prashant Doshi Dept. Departamento de Ciencias de la Computación Universidad de Georgia Athens, GA 30602, EE. UU. pdoshi@cs.uga.edu Yifeng Zeng Dept. de Ciencias de la Computación Universidad de Aalborg DK-9220 Aalborg, Dinamarca yfzeng@cs.aau.edu Qiongyu Chen Dept. de Ciencias de la Computación Universidad Nacional de Singapur 117543, Singapur chenqy@comp.nus.edu.sg RESUMEN Desarrollamos una nueva representación gráfica para procesos de decisión de Markov parcialmente observables interactivos (I-POMDPs) que es significativamente más transparente y semánticamente clara que la representación anterior. Estos modelos gráficos llamados diagramas de influencia dinámica interactiva (I-DIDs) buscan modelar explícitamente la estructura que a menudo está presente en problemas del mundo real al descomponer la situación en variables de probabilidad y decisión, y las dependencias entre las variables. Los I-DIDs generalizan los DIDs, los cuales pueden ser vistos como representaciones gráficas de POMDPs, a entornos multiagentes de la misma manera que los I-POMDPs generalizan los POMDPs. Los I-DIDs pueden ser utilizados para calcular la política de un agente en línea mientras el agente actúa y observa en un entorno poblado por otros agentes que interactúan entre sí. Usando varios ejemplos, mostramos cómo se pueden aplicar los I-DIDs y demostramos su utilidad. Categorías y Descriptores de Asignaturas I.2.11 [Inteligencia Artificial Distribuida]: Sistemas Multiagentes Términos Generales Teoría 1. Las Procesos de Decisión de Markov Interactivos Parcialmente Observables (IPOMDPs) proporcionan un marco para la toma de decisiones secuenciales en entornos multiagentes parcialmente observables. Generalizan los POMDPs [13] a entornos multiagentes al incluir los modelos computables de los otros agentes en el espacio de estados junto con los estados del entorno físico. Los modelos abarcan toda la información que influye en los comportamientos de los agentes, incluyendo sus preferencias, capacidades y creencias, y por lo tanto son análogos a los tipos en los juegos bayesianos [11]. I-POMDPs adoptan un enfoque subjetivo para comprender el comportamiento estratégico, arraigado en un marco de teoría de decisiones que toma la perspectiva de los tomadores de decisiones en la interacción. En [15], Polich y Gmytrasiewicz introdujeron diagramas de influencia dinámica interactivos (I-DIDs) como las representaciones computacionales de I-POMDPs. Los I-DIDs generalizan los DIDs [12], los cuales pueden ser vistos como contrapartes computacionales de POMDPs, a entornos de múltiples agentes de la misma manera que los I-POMDPs generalizan los POMDPs. Los I-DIDs contribuyen a una creciente línea de trabajo [19] que incluye diagramas de influencia multiagente (MAIDs) [14], y más recientemente, redes de diagramas de influencia (NIDs) [8]. Estos formalismos buscan modelar explícitamente la estructura que a menudo está presente en problemas del mundo real al descomponer la situación en variables de azar y decisiones, y las dependencias entre las variables. Los MAIDs proporcionan una alternativa a las formas normales y extensivas de juego utilizando un formalismo gráfico para representar juegos de información imperfecta con un nodo de decisión para las acciones de cada agente y nodos de azar que capturan la información privada de los agentes. Los MAIDs analizan objetivamente el juego, calculando eficientemente el <br>perfil de equilibrio de Nash</br> al explotar la estructura de independencia. NIDs extienden los MAIDs para incluir la incertidumbre de los agentes sobre el juego que se está jugando y sobre los modelos de los otros agentes. Cada modelo es un MAID y la red de MAIDs se colapsa, de abajo hacia arriba, en un solo MAID para calcular el equilibrio del juego teniendo en cuenta los diferentes modelos de cada agente. Los formalismos gráficos como MAIDs y NIDs abren una área de investigación prometedora que tiene como objetivo representar las interacciones multiagente de manera más transparente. Sin embargo, los MAIDs proporcionan un análisis del juego desde un punto de vista externo y la aplicabilidad de ambos está limitada a juegos estáticos de un solo jugador. Los asuntos son más complejos cuando consideramos interacciones que se extienden en el tiempo, donde las predicciones sobre las acciones futuras de otros deben hacerse utilizando modelos que cambian a medida que los agentes actúan y observan. Los I-DIDs abordan esta brecha al permitir la representación de modelos de otros agentes como los valores de un nodo de modelo especial. Tanto los modelos de otros agentes como las creencias originales de los agentes sobre estos modelos se actualizan con el tiempo utilizando implementaciones especializadas. En este documento, mejoramos la representación preliminar previa del I-DID mostrada en [15] utilizando la idea de que el I-ID estático es un tipo de NID. Por lo tanto, podemos utilizar construcciones de lenguaje específicas de NID, como multiplexores, para representar de manera más transparente el nodo del modelo y, posteriormente, el I-ID. Además, aclaramos la semántica del enlace de política de propósito especial introducido en la representación de I-DID por [15], y mostramos que podría ser reemplazado por enlaces de dependencia tradicionales. En la representación previa del I-DID, la actualización de la creencia de los agentes sobre los modelos de los demás a medida que los agentes actúan y reciben observaciones se denotaba utilizando un enlace especial llamado el enlace de actualización del modelo que conectaba los nodos del modelo a lo largo del tiempo. Explicamos la semántica de este enlace mostrando cómo puede ser implementado utilizando los enlaces de dependencia tradicionales entre los nodos de probabilidad que constituyen los nodos del modelo. El resultado final es una representación de I-DID que es significativamente más transparente, semánticamente clara y capaz de ser implementada utilizando los algoritmos estándar para resolver DIDs. Mostramos cómo los IDIDs pueden ser utilizados para modelar la incertidumbre de un agente sobre los modelos de otros, que a su vez pueden ser I-DIDs. La solución al I-DID es una política que prescribe lo que el agente debe hacer con el tiempo, dadas sus creencias sobre el estado físico y otros modelos. Análogos a los DIDs, los I-DIDs pueden ser utilizados para calcular la política de un agente en línea mientras el agente actúa y observa en un entorno poblado por otros agentes interactuantes. 2. ANTECEDENTES: Los IPOMDPS ANIDADOS FINITAMENTE generalizan los POMDPs a entornos multiagentes al incluir los modelos de otros agentes como parte del espacio de estados [9]. Dado que otros agentes también pueden razonar sobre otros, el espacio de estado interactivo está estratégicamente anidado; contiene creencias sobre los modelos de otros agentes y sus creencias sobre los demás. Para simplificar la presentación, consideramos un agente, i, que está interactuando con otro agente, j. Un I-POMDP finitamente anidado del agente i con un nivel de estrategia l se define como la tupla: I-POMDPi,l = ISi,l, A, Ti, Ωi, Oi, Ri donde: • ISi,l denota un conjunto de estados interactivos definido como, ISi,l = S × Mj,l−1, donde Mj,l−1 = {Θj,l−1 ∪ SMj}, para l ≥ 1, e ISi,0 = S, donde S es el conjunto de estados del entorno físico. Θj,l−1 es el conjunto de modelos intencionales computables del agente j: θj,l−1 = bj,l−1, ˆθj donde el marco, ˆθj = A, Ωj, Tj, Oj, Rj, OCj. Aquí, j es Bayesiano racional y OCj es el criterio de optimalidad de j. SMj es el conjunto de modelos subintencionales de j. Ejemplos simples de modelos subintencionales incluyen un modelo sin información [10] y un modelo de juego ficticio [6], ambos de los cuales son independientes de la historia. Damos una construcción recursiva de abajo hacia arriba del espacio de estados interactivo a continuación. Sí,0 = S, Θj,0 = { bj,0, ˆθj | bj,0 ∈ Δ(ISj,0)} Sí,1 = S × {Θj,0 ∪ SMj}, Θj,1 = { bj,1, ˆθj | bj,1 ∈ Δ(ISj,1)} . . . . . . Sí, l = S × {Θj,l−1 ∪ SMj}, Θj,l = { bj,l, ˆθj | bj,l ∈ Δ(ISj,l)} Formulaciones similares de espacios anidados han aparecido en [1, 3]. • A = Ai × Aj es el conjunto de acciones conjuntas de todos los agentes en el entorno; • Ti : S ×A×S → [0, 1], describe el efecto de las acciones conjuntas en los estados físicos del entorno; • Ωi es el conjunto de observaciones del agente i; • Oi : S × A × Ωi → [0, 1] da la probabilidad de las observaciones dadas el estado físico y la acción conjunta; • Ri : ISi × A → R describe las preferencias del agente sobre sus estados interactivos. Normalmente solo importarán los estados físicos. La política del agente es el mapeo, Ω∗ i → Δ(Ai), donde Ω∗ i es el conjunto de todos los historiales de observación del agente i. Dado que la creencia sobre los estados interactivos forma una estadística suficiente [9], la política también puede ser representada como un mapeo del conjunto de todas las creencias del agente i a una distribución sobre sus acciones, Δ(ISi) → Δ(Ai). 2.1 Actualización de Creencias Análogo a los POMDPs, un agente dentro del marco I-POMDP actualiza su creencia a medida que actúa y observa. Sin embargo, hay dos diferencias que complican la actualización de creencias en entornos multiagentes en comparación con los entornos de un solo agente. Primero, dado que el estado del entorno físico depende de las acciones de ambos agentes, la predicción de cómo cambia el estado físico debe hacerse basándose en la predicción de sus acciones. Segundo, los cambios en los modelos de js deben ser incluidos en la actualización de creencias. Específicamente, si j es intencional, entonces se debe incluir una actualización de las creencias de j debido a su acción y observación. En otras palabras, i tiene que actualizar su creencia basándose en su predicción de lo que j observaría y cómo j actualizaría su creencia. Si el modelo de js es subintencional, entonces las observaciones probables de js se añaden al historial de observaciones contenido en el modelo. Formalmente, tenemos: Pr(ist |at−1 i , bt−1 i,l ) = β ISt−1:mt−1 j =θt j bt−1 i,l (ist−1 ) × at−1 j Pr(at−1 j |θt−1 j,l−1)Oi(st , at−1 i , at−1 j , ot i) ×Ti(st−1 , at−1 i , at−1 j , st ) ot j Oj(st , at−1 i , at−1 j , ot j) ×τ(SEθt j (bt−1 j,l−1, at−1 j , ot j) − bt j,l−1) (1) donde β es la constante de normalización, τ es 1 si su argumento es 0, de lo contrario es 0, Pr(at−1 j |θt−1 j,l−1) es la probabilidad de que at−1 j sea Bayes racional para el agente descrito por el modelo θt−1 j,l−1, y SE(·) es una abreviatura para la actualización de creencias. Para una versión de la actualización de creencias cuando el modelo js es subintencional, consulte [9]. Si el agente j también se modela como un I-POMDP, entonces la actualización de creencias invoca la actualización de creencias de j (a través del término SEθt j ( bt−1 j,l−1 , at−1 j , ot j)), lo que a su vez podría invocar la actualización de creencias de is y así sucesivamente. Esta recursión en la anidación de creencias llega al nivel 0. En este nivel, la actualización de creencias del agente se reduce a una actualización de creencias de un POMDP. Para ilustraciones de la actualización de creencias, detalles adicionales sobre I-POMDPs y cómo se comparan con otros marcos multiagentes, consulte [9]. Iteración de Valor Cada estado de creencia en un I-POMDP finitamente anidado tiene un valor asociado que refleja la máxima ganancia que el agente puede esperar en este estado de creencia: Un ( bi,l, θi ) = max ai∈Ai is∈ISi,l ERi(is, ai)bi,l(is)+ γ oi∈Ωi Pr(oi|ai, bi,l)Un−1 ( SEθi (bi,l, ai, oi), θi ) (2) donde, ERi(is, ai) = aj Ri(is, ai, aj)Pr(aj|mj,l−1) (ya que is = (s, mj,l−1)). La ecuación 2 es una base para la iteración de valor en I-POMDPs. La acción óptima del agente, a∗ i, para el caso de horizonte finito con descuento, es un elemento del conjunto de acciones óptimas para el estado de creencia, OPT(θi), definido como: OPT( bi,l, θi ) = argmax ai∈Ai is∈ISi,l ERi(is, ai)bi,l(is) +γ oi∈Ωi Pr(oi|ai, bi,l)Un ( SEθi (bi,l, ai, oi), θi ) (3) 3. Diagramas de influencia interactivos. Una extensión ingenua de los diagramas de influencia (IDs) a entornos poblados por múltiples agentes es posible tratando a los otros agentes como autómatas, representados mediante nodos de probabilidad. Sin embargo, este enfoque asume que las acciones de los agentes están controladas utilizando una distribución de probabilidad que no cambia con el tiempo. Los diagramas de influencia interactivos (I-IDs) adoptan un enfoque más sofisticado al generalizar los IDs para hacerlos aplicables a entornos compartidos con otros agentes que pueden actuar, observar y actualizar sus creencias. 3.1 Sintaxis Además de los nodos habituales de probabilidad, decisión y utilidad, los IIDs incluyen un nuevo tipo de nodo llamado nodo de modelo. Mostramos un nivel general l I-ID en la Fig. 1(a), donde el nodo del modelo (Mj,l−1) está representado con un hexágono. Observamos que la distribución de probabilidad sobre el nodo de probabilidad, S, y el nodo del modelo juntos representan la creencia del agente sobre sus estados interactivos. Además del modelo 1, el modelo de nivel 0 es un POMDP: las acciones de otros agentes se tratan como eventos exógenos y se incorporan en las funciones T, O y R. El Sexto Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) Figura 1: (a) Un nivel genérico l I-ID para el agente i situado con otro agente j. El hexágono es el nodo modelo (Mj,l−1) cuya estructura mostramos en (b). Los miembros del nodo modelo son ellos mismos I-ID (m1 j,l−1, m2 j,l−1; los diagramas no se muestran aquí por simplicidad) cuyos nodos de decisión se asignan a los nodos de probabilidad correspondientes (A1 j , A2 j). Dependiendo del valor del nodo, Mod[Mj], la distribución de cada uno de los nodos de probabilidad se asigna al nodo Aj. (c) El I-ID transformado con el nodo del modelo reemplazado por los nodos de probabilidad y las relaciones entre ellos. Los I-IDs difieren de los IDs al tener un enlace discontinuo (llamado enlace de política en [15]) entre el nodo del modelo y un nodo de probabilidad, Aj, que representa la distribución sobre las acciones de los otros agentes dada su modelo. En ausencia de otros agentes, el nodo del modelo y el nodo de probabilidad, Aj, desaparecen y los I-ID se convierten en ID tradicionales. El nodo del modelo contiene los modelos computacionales alternativos atribuidos por i al otro agente del conjunto, Θj,l−1 ∪ SMj, donde Θj,l−1 y SMj fueron definidos previamente en la Sección 2. Por lo tanto, un modelo en el nodo del modelo puede ser en sí mismo un I-ID o ID, y la recursión termina cuando un modelo es un ID o subintencional. Dado que el nodo del modelo contiene los modelos alternativos del otro agente como sus valores, su representación no es trivial. En particular, algunos de los modelos dentro del nodo son I-IDs que, al resolverse, generan la política óptima de los agentes en sus nodos de decisión. Cada nodo de decisión se asigna al nodo de probabilidad correspondiente, digamos A1 j, de la siguiente manera: si OPT es el conjunto de acciones óptimas obtenidas al resolver el I-ID (o ID), entonces Pr(aj ∈ A1 j) = 1 |OPT| si aj ∈ OPT, 0 en caso contrario. Tomando prestados los conocimientos de trabajos anteriores [8], observamos que el nodo del modelo y el enlace de política discontinua que lo conecta con el nodo de probabilidad, Aj, podrían representarse como se muestra en la Fig. 1(b). El nodo de decisión de cada nivel l − 1 I-ID se transforma en un nodo de probabilidad, como mencionamos anteriormente, de modo que las acciones con el mayor valor en el nodo de decisión se les asignan probabilidades uniformes en el nodo de probabilidad, mientras que al resto se les asigna probabilidad cero. Los diferentes nodos de probabilidad (A1 j , A2 j ), uno para cada modelo, y adicionalmente, el nodo de probabilidad etiquetado Mod[Mj] forman los padres del nodo de probabilidad, Aj. Por lo tanto, hay tantos nodos de acción (A1 j , A2 j ) en Mj,l−1 como el número de modelos en el soporte de las creencias del agente. La tabla de probabilidad condicional del nodo de probabilidad, Aj, es un multiplexor que asume la distribución de cada uno de los nodos de acción (A1 j , A2 j ) dependiendo del valor de Mod[Mj]. Los valores de Mod[Mj] denotan los diferentes modelos de j. En otras palabras, cuando Mod[Mj] tiene el valor m1 j,l−1, el nodo de probabilidad Aj asume la distribución del nodo A1 j, y Aj asume la distribución de A2 j cuando Mod[Mj] tiene el valor m2 j,l−1. La distribución sobre el nodo, Mod[Mj], es la creencia del agente sobre los modelos de j dado un estado físico. Para más agentes, tendremos tantos nodos de modelo como agentes haya. Observa que la Fig. 1(b) aclara la semántica del enlace de política y muestra cómo puede ser representado utilizando los enlaces de dependencia tradicionales. En la Fig. 1(c), mostramos el I-ID transformado cuando el nodo del modelo es reemplazado por los nodos de probabilidad y las relaciones entre ellos. A diferencia de la representación en [15], no hay enlaces de políticas especiales, sino que el I-ID está compuesto únicamente por aquellos tipos de nodos que se encuentran en IDs tradicionales y relaciones de dependencia entre los nodos. Esto permite que los I-IDs sean representados e implementados utilizando herramientas de aplicación convencionales que apuntan a los IDs. Ten en cuenta que podemos ver el nivel l I-ID como un NID. Específicamente, cada uno de los modelos del nivel l − 1 dentro del nodo del modelo son bloques en el NID (ver Fig. 2). Si el nivel l = 1, cada bloque es un ID tradicional, de lo contrario, si l > 1, cada bloque dentro del NID puede ser un NID en sí mismo. Ten en cuenta que dentro de los I-IDs (o IDs) en cada nivel, solo hay un nodo de decisión único. Por lo tanto, nuestro NID no contiene ningún MAID. Figura 2: Un nivel l I-ID representado como un NID. Las probabilidades asignadas a los bloques del NID son creencias sobre modelos js condicionados a un estado físico. 3.2 Solución La solución de un I-ID procede de manera ascendente y se implementa de forma recursiva. Comenzamos resolviendo los modelos de nivel 0, que, si son intencionales, son identificadores tradicionales. Sus soluciones proporcionan distribuciones de probabilidad sobre las acciones de los otros agentes, las cuales se ingresan en los nodos de probabilidad correspondientes encontrados en el nodo del modelo del nivel 1 I-ID. El mapeo de los nodos de decisión de los modelos de nivel 0 a los nodos de probabilidad se realiza de manera que las acciones con el mayor valor en el nodo de decisión se les asignan probabilidades uniformes en el nodo de probabilidad, mientras que el resto se les asigna probabilidad cero. Dadas las distribuciones sobre las acciones dentro de los diferentes nodos de probabilidad (uno para cada modelo del otro agente), el I-ID de nivel 1 se transforma como se muestra en la Fig. 1(c). Durante la transformación, la tabla de probabilidad condicional (CPT) del nodo, Aj, se llena de tal manera que el nodo asume la distribución de cada uno de los nodos de probabilidad dependiendo del valor del nodo, Mod[Mj]. Como mencionamos anteriormente, los valores del nodo Mod[Mj] denotan los diferentes modelos del otro agente, y su distribución es la creencia del agente sobre los modelos de j condicionados al estado físico. El nivel 1 I-ID transformado es un ID tradicional que puede ser resuelto como us816 The Sixth Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) (a) (b) Figura 3: (a) Un I-DID genérico de dos niveles de tiempo para el agente i en un entorno con otro agente j. Observa el enlace de actualización del modelo punteado que denota la actualización de los modelos de j y la distribución de los modelos a lo largo del tiempo. (b) La semántica del enlace de actualización del modelo utilizando el método estándar de maximización de utilidad esperada [18]. Este procedimiento se lleva a cabo hasta el nivel l I-ID cuya solución proporciona el conjunto no vacío de acciones óptimas que el agente debe realizar dada su creencia. Ten en cuenta que, al igual que los IDs, los I-IDs son adecuados para la toma de decisiones en línea cuando se conoce la creencia actual de los agentes. 4. Diagramas de influencia dinámica interactivos (I-DIDs) Los diagramas de influencia dinámica interactivos (I-DIDs) extienden los I-IDs (y NIDs) para permitir la toma de decisiones secuencial a lo largo de varios pasos de tiempo. Así como los DIDs son representaciones gráficas estructuradas de POMDPs, los I-DIDs son los análogos gráficos en línea para I-POMDPs finitamente anidados. Los I-DIDs pueden ser utilizados para optimizar sobre una mirada anticipada finita dadas las creencias iniciales mientras se interactúa con otros agentes, posiblemente similares. 4.1 Sintaxis Representamos un I-DID de dos cortes temporales en la Fig. 3(a). Además de los nodos del modelo y el enlace de política discontinuo, lo que diferencia a un I-DID de un DID es el enlace de actualización del modelo mostrado como una flecha punteada en la Fig. 3(a). Explicamos la semántica del nodo del modelo y el enlace de la política en la sección anterior; a continuación, describimos las actualizaciones del modelo. La actualización del nodo del modelo con el tiempo implica dos pasos: primero, dados los modelos en el tiempo t, identificamos el conjunto actualizado de modelos que residen en el nodo del modelo en el tiempo t + 1. Recuerda de la Sección 2 que el modelo intencional de un agente incluye sus creencias. Debido a que los agentes actúan y reciben observaciones, sus modelos se actualizan para reflejar sus creencias cambiadas. Dado que el conjunto de acciones óptimas para un modelo podría incluir todas las acciones, y el agente podría recibir cualquiera de las |Ωj| posibles observaciones, el conjunto actualizado en el paso de tiempo t + 1 tendrá como máximo |Mt j,l−1||Aj||Ωj| modelos. Aquí, |Mt j,l−1| es el número de modelos en el paso de tiempo t, |Aj| y |Ωj| son los espacios más grandes de acciones y observaciones respectivamente, entre todos los modelos. Segundo, calculamos la nueva distribución sobre los modelos actualizados dados la distribución original y la probabilidad de que el agente realice la acción y reciba la observación que llevó al modelo actualizado. Estos pasos son parte de la actualización de creencias del agente formalizada utilizando la Ecuación 1. En la Fig. 3(b), mostramos cómo se implementa el enlace de actualización del modelo punteado en el I-DID. Si cada uno de los dos modelos de nivel l − 1 atribuidos a j en el paso de tiempo t resulta en una acción, y j podría hacer una de dos posibles observaciones, entonces el nodo del modelo en el paso de tiempo t + 1 contiene cuatro modelos actualizados (mt+1,1 j,l−1 ,mt+1,2 j,l−1 , mt+1,3 j,l−1 , y mt+1,4 j,l−1 ). Estos modelos difieren en sus creencias iniciales, cada una de las cuales es el resultado de j actualizar sus creencias debido a su acción y una posible observación. Los nodos de decisión en cada uno de los I-DIDs o DIDs que representan los modelos de nivel inferior se asignan a la Figura 4 correspondiente: I-DID transformado con los nodos del modelo y el enlace de actualización del modelo reemplazados por los nodos de probabilidad y las relaciones (en negrita) de los nodos de probabilidad, como se mencionó anteriormente. A continuación, describimos cómo se calcula la distribución sobre el conjunto actualizado de modelos (la distribución sobre el nodo de probabilidad Mod[Mt+1 j ] en Mt+1 j,l−1). La probabilidad de que el modelo actualizado js sea, digamos mt+1,1 j,l−1, depende de la probabilidad de que j realice la acción y reciba la observación que condujo a este modelo, y de la distribución previa sobre los modelos en el paso de tiempo t. Dado que el nodo de probabilidad At j asume la distribución de cada uno de los nodos de acción basados en el valor de Mod[Mt j], la probabilidad de la acción está dada por este nodo de probabilidad. Para obtener la probabilidad de una observación posible js, introducimos el nodo de probabilidad Oj, que dependiendo del valor de Mod[Mt j], asume la distribución del nodo de observación en el modelo de nivel inferior denominado Mod[Mt j]. Dado que la probabilidad de las observaciones js depende del estado físico y de las acciones conjuntas de ambos agentes, el nodo Oj está vinculado con St+1, At j y At i. De manera análoga a At j, la tabla de probabilidad condicional de Oj también es un multiplexor modulado por Mod[Mt j]. Finalmente, la distribución de los modelos previos en el tiempo t se obtiene a partir del nodo de probabilidad, Mod[Mt j ] en Mt j,l−1. Por consiguiente, los nodos de probabilidad, Mod[Mt j], At j y Oj, forman los padres de Mod[Mt+1 j] en Mt+1 j,l−1. Ten en cuenta que el enlace de actualización del modelo puede ser reemplazado por los enlaces de dependencia entre los nodos de probabilidad que constituyen los nodos del modelo en las dos etapas temporales. En la Fig. 4 mostramos los dos cortes temporales I-DID con los nodos del modelo reemplazados por los nodos de probabilidad y las relaciones entre ellos. Los nodos de probabilidad y los enlaces de dependencia que no están en negrita son estándar, generalmente encontrados en DIDs. La expansión del I-DID a lo largo de más pasos de tiempo requiere la repetición de los dos pasos de actualizar el conjunto de modelos que forman el 2. Nota que Oj representa la observación j en el tiempo t + 1. El Sexto Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 817 valores del nodo del modelo y añadiendo las relaciones entre los nodos de probabilidad, tantas veces como enlaces de actualización del modelo haya. Observamos que el conjunto posible de modelos del otro agente j crece exponencialmente con el número de pasos de tiempo. Por ejemplo, después de T pasos, puede haber como máximo |Mt=1 j,l−1|(|Aj||Ωj|)T −1 modelos candidatos residiendo en el nodo del modelo. 4.2 Solución Análoga a los I-ID, la solución a un I-DID de nivel l para el agente i expandido a lo largo de T pasos de tiempo puede llevarse a cabo de forma recursiva. Con el propósito de ilustración, dejemos l=1 y T=2. El método de solución utiliza la técnica estándar de anticipación, proyectando las secuencias de acciones y observaciones de los agentes hacia adelante desde el estado de creencia actual [17], y encontrando las posibles creencias que podría tener en el siguiente paso temporal. Dado que el agente i también tiene una creencia sobre los modelos de j, la búsqueda anticipada incluye descubrir los posibles modelos que j podría tener en el futuro. Por consiguiente, cada uno de los modelos subintencionales o de nivel 0 de js (representados utilizando un DID estándar) en el primer paso de tiempo debe resolverse para obtener su conjunto óptimo de acciones. Estas acciones se combinan con el conjunto de observaciones posibles que j podría hacer en ese modelo, lo que resulta en un conjunto actualizado de modelos candidatos (que incluyen las creencias actualizadas) que podrían describir el comportamiento de j. Las creencias sobre este conjunto actualizado de modelos candidatos se calculan utilizando los métodos estándar de inferencia que utilizan las relaciones de dependencia entre los nodos del modelo, como se muestra en la Figura 3(b). Observamos la naturaleza recursiva de esta solución: al resolver el agente de nivel 1 I-DID, los DIDs de nivel 0 deben ser resueltos. Si el anidamiento de los modelos es más profundo, todos los modelos en todos los niveles, comenzando desde el nivel 0, se resuelven de manera ascendente. Breve descripción del algoritmo recursivo para resolver el agente es el Algoritmo para resolver I-DID. Entrada: nivel l ≥ 1 I-ID o nivel 0 ID, T Fase de Expansión 1. Para t desde 1 hasta T − 1, hacer 2. Si l ≥ 1, entonces llenar Mt+1 j,l−1 3. Para cada mt j en el rango (Mt j,l−1) hacer 4. Llame recursivamente al algoritmo con el l - 1 I-ID (o ID) que representa mt j y el horizonte, T - t + 1 5. Mapea el nodo de decisión del I-ID (o ID) resuelto, OPT(mt j), a un nodo de probabilidad Aj 6. Para cada aj en OPT(mt j) hacer 7. Para cada oj en Oj (parte de mt j) hacer 8. Actualizar la creencia js, bt+1 j ← SE(bt j, aj, oj) 9. mt+1 j ← Nuevo I-ID (o ID) con bt+1 j como la creencia inicial 10. Rango(Mt+1 j,l−1) ∪ ← {mt+1 j } 11. Añadir el nodo del modelo, Mt+1 j,l−1, y los enlaces de dependencia entre Mt j,l−1 y Mt+1 j,l−1 (mostrados en la Fig. 3(b)) 12. Agregue los nodos de probabilidad, decisión y utilidad para la franja de tiempo t + 1 y los enlaces de dependencia entre ellos 13. Establezca las CPTs para cada nodo de probabilidad y nodo de utilidad de la Fase de Mirada Adelante 14. Aplica el método estándar de anticipación y respaldo para resolver la Figura 5 expandida de I-DID: Algoritmo para resolver un nivel l ≥ 0 I-DID. Nivel l I-DID expandido durante T pasos de tiempo con otro agente j en la Figura 5. Adoptamos un enfoque de dos fases: Dado un I-ID de nivel l (descrito previamente en la Sección 3) con todos los modelos de niveles inferiores representados también como I-IDs o IDs (si es nivel 0), el primer paso es expandir el I-ID de nivel l a lo largo de T pasos de tiempo añadiendo los enlaces de dependencia y las tablas de probabilidad condicional para cada nodo. Nos enfocamos especialmente en establecer y poblar los nodos del modelo (líneas 3-11). Ten en cuenta que Range(·) devuelve los valores (modelos de nivel inferior) de la variable aleatoria dada como entrada (nodo del modelo). En la segunda fase, utilizamos una técnica estándar de anticipación proyectando las secuencias de acción y observación durante T pasos de tiempo en el futuro, y respaldando los valores de utilidad de las creencias alcanzables. Similar a los I-IDs, los I-DIDs se reducen a DIDs en ausencia de otros agentes. Como mencionamos anteriormente, los modelos de nivel 0 son los DIDs tradicionales. Sus soluciones proporcionan distribuciones de probabilidad sobre las acciones del agente modelado en ese nivel a los I-DIDs en el nivel 1. Dadas las distribuciones de probabilidad sobre las acciones de otros agentes, los IDIDs de nivel 1 pueden resolverse como DIDs y proporcionar distribuciones de probabilidad a modelos de niveles superiores. Suponga que el número de modelos considerados en cada nivel está limitado por un número, M. Resolver un I-DID de nivel l es equivalente a resolver O(Ml) DIDs. Para ilustrar la utilidad de los I-DIDs, los aplicamos a tres dominios de problemas. Describimos, en particular, la formulación del I-DID y las prescripciones óptimas obtenidas al resolverlo. 5.1 Seguimiento-Liderazgo en el Problema del Tigre Multiagente Comenzamos nuestras ilustraciones del uso de I-IDs e I-DIDs con una versión ligeramente modificada del problema del tigre multiagente discutido en [9]. El problema tiene dos agentes, cada uno de los cuales puede abrir la puerta derecha (OR), la puerta izquierda (OL) o escuchar (L). Además de escuchar gruñidos (desde la izquierda (GL) o desde la derecha (GR)) cuando escuchan, los agentes también escuchan chirridos (desde la izquierda (CL), desde la derecha (CR) o sin chirridos (S)), que indican ruidosamente que los otros agentes están abriendo una de las puertas. Cuando se abre cualquier puerta, el tigre persiste en su ubicación original con una probabilidad del 95%. El agente i escucha gruñidos con una fiabilidad del 65% y crujidos con una fiabilidad del 95%. El agente J, por otro lado, escucha gruñidos con una fiabilidad del 95%. Por lo tanto, la configuración es tal que el agente i escucha la apertura de puertas del agente j de manera más confiable que los rugidos de los tigres. Esto sugiere que podría usar acciones de JavaScript como indicación de la ubicación del tigre, como discutimos a continuación. Las preferencias de cada agente son como en el juego de un solo agente discutido en [13]. Las funciones de transición, observación y recompensa se muestran en [16]. Un buen indicador de la utilidad de los métodos normativos para la toma de decisiones como los I-DIDs es la aparición de comportamientos sociales realistas en sus prescripciones. En entornos del persistente problema del tigre multiagente que reflejan situaciones del mundo real, demostramos la seguidismo entre los agentes y, como se muestra en [15], el engaño entre agentes que creen que están en una relación de tipo seguidor-líder. En particular, analizamos las condiciones situacionales y epistemológicas suficientes para su surgimiento. El comportamiento de seguidores, por ejemplo, resulta del agente conociendo sus propias debilidades, evaluando las fortalezas, preferencias y posibles comportamientos del otro, y dándose cuenta de que es mejor para él seguir las acciones de los demás para maximizar sus ganancias. Consideremos una configuración particular del problema del tigre en la que el agente i cree que las preferencias de j están alineadas con las suyas: ambos solo quieren obtener el oro, y la audición de j es más confiable en comparación consigo mismo. Como ejemplo, supongamos que j, al escuchar, puede discernir la ubicación del tigre el 95% de las veces en comparación con su precisión del 65%. Además, el agente i no tiene ninguna información inicial sobre la ubicación de los tigres. En otras palabras, la creencia anidada de un solo nivel, bi,1, asigna 0.5 a cada una de las dos ubicaciones del tigre. Además, considera dos modelos de j, que difieren en los niveles iniciales de creencias planas de j. Esto se representa en el nivel 1 I-ID mostrado en la Fig. 6(a). Según un modelo, j asigna una probabilidad de 0.9 a que el tigre esté detrás de la puerta izquierda, mientras que el otro 818 The Sixth Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) Figura 6: (a) Nivel 1 I-ID del agente i, (b) dos IDs de nivel 0 del agente j cuyos nodos de decisión están mapeados a los nodos de probabilidad, A1 j , A2 j , en (a). El modelo asigna 0.1 a esa ubicación (ver Fig. 6(b)). El agente i está indeciso sobre estos dos modelos de j. Si variamos la capacidad auditiva, y resolvemos el nivel 1 correspondiente I-ID expandido en tres pasos de tiempo, obtenemos las políticas de comportamiento normativas mostradas en la Figura 7 que exhiben comportamiento de seguidores. Si la probabilidad de escuchar correctamente los gruñidos es de 0.65, entonces, como se muestra en la política en la Fig. 7(a), i comienza a seguir condicionalmente las acciones de j: i abre la misma puerta que j abrió anteriormente si su evaluación propia de la ubicación de los tigres confirma la elección de j. Si i pierde la capacidad de interpretar correctamente los gruñidos por completo, sigue ciegamente a j y abre la misma puerta que j abrió anteriormente (Fig. 7(b)). Figura 7: Emergencia de (a) seguidores condicionales, y (b) seguidores ciegos en el problema del tigre. Los comportamientos de interés están en negrita. * es un comodín y representa cualquiera de las observaciones. Observamos que un solo nivel de anidación de creencias, creencias sobre los modelos de los demás, fue suficiente para que surgiera el seguidismo en el problema del tigre. Sin embargo, los requisitos epistemológicos para el surgimiento del liderazgo son más complejos. Para que un agente, digamos j, surja como líder, primero debe surgir el seguidismo en el otro agente i. Como mencionamos anteriormente, si i está seguro de que sus preferencias son idénticas a las de j, y cree que j tiene un mejor sentido del oído, i seguirá las acciones de j con el tiempo. El agente j emerge como líder si cree que lo seguiré, lo que implica que la creencia de j debe estar anidada dos niveles de profundidad para permitirle reconocer su papel de liderazgo. Darse cuenta de que lo seguiré presenta a J una oportunidad para influir en sus acciones en beneficio del bien colectivo o de su interés propio solamente. Por ejemplo, en el problema del tigre, consideremos una situación en la que si tanto i como j abren la puerta correcta, entonces cada uno recibe un pago de 20 que es el doble del original. Si solo j selecciona la puerta correcta, obtiene la recompensa de 10. Por otro lado, si ambos agentes eligen la puerta incorrecta, sus penalizaciones se reducen a la mitad. En este entorno, es tanto en el mejor interés de j como en el beneficio colectivo que utilice su experiencia para seleccionar la puerta correcta, y así ser un buen líder. Sin embargo, considera un problema ligeramente diferente en el que j se beneficia de la pérdida de i y es penalizado si i se beneficia. Específicamente, dejemos que la ganancia se reste de js, indicando que j es antagonista hacia i: si j elige la puerta correcta y i la incorrecta, entonces la pérdida de 100 se convierte en la ganancia de js. El agente J cree que yo incorrectamente pienso que las preferencias de J son aquellas que promueven el bien colectivo y que comienza creyendo con un 99% de confianza dónde está el tigre. Dado que creo que mis preferencias son similares a las de j, y que j comienza creyendo casi con certeza que una de las dos ubicaciones es la correcta (dos modelos de nivel 0 de j), comenzaré siguiendo las acciones de j. Mostramos la política normativa para resolver su I-DID anidado individualmente durante tres pasos de tiempo en la Fig. 8(a). La política demuestra que seguiré ciegamente las acciones de js. Dado que el tigre persiste en su ubicación original con una probabilidad del 0.95, seleccionaré nuevamente la misma puerta. Si j comienza el juego con una probabilidad del 99% de que el tigre esté a la derecha, resolver su I-DID anidado dos niveles profundamente, resulta en la política mostrada en la Fig. 8(b). Aunque j está casi seguro de que OL es la acción correcta, comenzará seleccionando OR, seguido de OL. La intención del agente js es engañar a i, a quien cree que seguirá las acciones de js, para así obtener $110 en el segundo paso de tiempo, lo cual es más de lo que j ganaría si fuera honesto. Figura 8: Emergencia del engaño entre agentes en el problema del tigre. Los comportamientos de interés están en negrita. * denota como antes. (a) El agente es una política que demuestra que seguirá ciegamente las acciones de js. (b) Aunque j está casi seguro de que el tigre está a la derecha, comenzará seleccionando OR, seguido de OL, para engañar a i. 5.2 Altruismo y reciprocidad en el problema del bien público El problema del bien público (PG) [7], consiste en un grupo de M agentes, cada uno de los cuales debe contribuir con algún recurso a una olla pública o guardarlo para sí mismos. Dado que los recursos aportados al fondo público se comparten entre todos los agentes, son menos valiosos para el agente cuando están en el fondo público. Sin embargo, si todos los agentes eligen contribuir con sus recursos, entonces la recompensa para cada agente es mayor que si nadie contribuye. Dado que un agente recibe su parte del bote público independientemente de si ha contribuido o no, la acción dominante es que cada agente no contribuya y en su lugar se aproveche de las contribuciones de los demás. Sin embargo, los comportamientos de los jugadores humanos en simulaciones empíricas del problema de PG difieren de las predicciones normativas. Los experimentos revelan que muchos jugadores inicialmente contribuyen con una gran cantidad al bote público, y continúan contribuyendo cuando se juega el problema del PG repetidamente, aunque en cantidades decrecientes [4]. Muchos de estos experimentos [5] informan que un pequeño grupo central de jugadores contribuye persistentemente al bote público incluso cuando todos los demás están desertando. Estos experimentos también revelan que los jugadores que contribuyen persistentemente tienen preferencias altruistas o recíprocas que coinciden con la cooperación esperada de los demás. Para simplificar, asumimos que el juego se juega entre M = 2 agentes, i y j. Que cada agente esté inicialmente dotado con una cantidad de recursos XT. Mientras que la formulación clásica del juego PG permite que cada agente contribuya con cualquier cantidad de recursos (≤ XT) al bote público, simplificamos el espacio de acciones al permitir dos acciones posibles. Cada agente puede elegir contribuir (C) una cantidad fija de recursos, o no contribuir. La última acción es deThe Sixth Intl. La Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 819 fue señalada como defectuosa (D). Suponemos que las acciones no son observables por otros. El valor de los recursos en la bolsa pública se descuenta por ci para cada agente i, donde ci es el retorno privado marginal. Suponemos que ci < 1 para que el agente no se beneficie lo suficiente como para contribuir al bote público en beneficio propio. Simultáneamente, ciM > 1, lo que hace que la contribución colectiva sea óptima de Pareto. i/j C D C 2ciXT , 2cjXT ciXT − cp, XT + cjXT − P D XT + ciXT − P, cjXT − cp XT , XT Tabla 1: El juego PG de un solo disparo con castigo. Para fomentar las contribuciones, los agentes contribuyentes castigan a los que no colaboran pero incurren en un pequeño costo por administrar el castigo. Sea P la sanción impuesta al agente que se desvía y cp el costo no nulo de castigar al agente contribuyente. Para simplificar, asumimos que el costo de castigar es el mismo para ambos agentes. El juego PG de un solo disparo con castigo se muestra en la Tabla 1. Si ci = cj, cp > 0, y si P > XT − ciXT, entonces la deserción ya no es una acción dominante. Si P < XT − ciXT, entonces la deserción es la acción dominante para ambos. Si P = XT − ciXT, entonces el juego no es resoluble por dominancia. Figura 9: (a) Nivel 1 I-ID del agente i, (b) IDs de nivel 0 del agente j con nodos de decisión mapeados a los nodos de probabilidad, A1 j y A2 j, en (a). Formulamos una versión secuencial del problema PG con castigo desde la perspectiva del agente i. Aunque en el juego de PG repetido, la cantidad en la olla pública se revela a todos los agentes después de cada ronda de acciones, asumimos en nuestra formulación que está oculta para los agentes. Cada agente puede contribuir con una cantidad fija, xc, o desertar. Un agente al realizar una acción recibe una observación de abundante (PY) o escasa (MR) simbolizando el estado de la olla pública. Ten en cuenta que las observaciones también son indirectamente indicativas de las acciones del agente js porque el estado del bote público es influenciado por ellas. La cantidad de recursos en el agente es una olla privada, es perfectamente observable para i. Los pagos son análogos a la Tabla 1. Tomando prestado de las investigaciones empíricas del problema PG [5], construimos IDs de nivel 0 para j que modelan tipos altruistas y no altruistas (Fig. 9(b)). Específicamente, nuestro agente altruista tiene un alto retorno privado marginal (cj está cerca de 1) y no castiga a los demás que incumplen. Que xc = 1 y el agente de nivel 0 sea castigado la mitad de las veces que se desvía. Con una acción restante, ambos tipos de agentes eligen contribuir para evitar ser castigados. Con dos acciones por delante, el tipo altruista elige contribuir, mientras que el otro defecta. Esto se debe a que cj para el tipo altruista es cercano a 1, por lo tanto, el castigo esperado, 0.5P > (1 − cj), que el tipo altruista evita. Debido a que cj para el tipo no altruista es menor, prefiere no contribuir. Con tres pasos por delante, el agente altruista contribuye para evitar el castigo (0.5P > 2(1 − cj)), mientras que el tipo no altruista se desentiende. Para más de tres pasos, mientras el agente altruista continúa contribuyendo al bote público dependiendo de qué tan cercano esté su retorno privado marginal a 1, el tipo no altruista prescribe la deserción. Analizamos las decisiones de un agente altruista i modelado usando un nivel 1 I-DID expandido en 3 pasos de tiempo. i atribuye los dos modelos de nivel 0, mencionados anteriormente, a j (ver Fig. 9). Si creo con una probabilidad de 1 que j es altruista, elijo contribuir en cada uno de los tres pasos. Este comportamiento persiste cuando i no está al tanto de si j es altruista (Fig. 10(a)), y cuando i asigna una alta probabilidad a que j sea del tipo no altruista. Sin embargo, cuando i cree con una probabilidad de 1 que j no es altruista y por lo tanto seguramente va a traicionar, i elige traicionar para evitar ser castigado y porque su beneficio privado marginal es menor que 1. Estos resultados demuestran que el comportamiento de nuestro tipo altruista se asemeja al encontrado experimentalmente. El agente de nivel 1 no altruista elige desertar independientemente de lo altruista que crea que sea el otro agente. Analizamos el comportamiento de un tipo de agente recíproco que se ajusta a la cooperación o defección esperada. El retorno privado marginal del tipo recíproco es similar al del tipo no altruista, sin embargo, obtiene una mayor recompensa cuando su acción es similar a la del otro. Consideramos el caso en el que el agente recíproco i no está seguro de si j es altruista y cree que es probable que el bote público esté medio lleno. Para esta creencia previa, yo elijo defectar. Al recibir una observación de abundancia, decide contribuir, mientras que una observación de escasez lo hace defectar (Fig. 10(b)). Esto se debe a que una observación de abundancia indica que es probable que la olla esté más llena que a la mitad, lo cual resulta de la acción de js para contribuir. Por lo tanto, entre los dos modelos atribuidos a j, es probable que su tipo sea altruista, lo que hace probable que j contribuya nuevamente en el próximo paso de tiempo. El agente i, por lo tanto, elige contribuir para corresponder la acción de js. Un razonamiento análogo lleva a uno a defectar cuando observa una olla escasa. Con una acción por hacer, creo que si j contribuye, también elegiré contribuir para evitar castigo independientemente de sus observaciones. Figura 10: (a) Un agente altruista de nivel 1 siempre contribuye. (b) Un agente recíproco i comienza defraudando y luego elige contribuir o defraudar basándose en su observación de abundancia (indicando que j es probablemente altruista) o escasez (j no es altruista). 5.3 Estrategias en el póker de dos jugadores El póker es un popular juego de cartas de suma cero que ha recibido mucha atención en la comunidad de investigación de IA como un banco de pruebas [2]. El póker se juega entre M ≥ 2 jugadores, en el que cada jugador recibe una mano de cartas de una baraja. Si bien existen varias variantes de póker con diferentes niveles de complejidad, consideramos una versión simple en la que cada jugador tiene tres turnos durante los cuales el jugador puede intercambiar una carta (E), mantener la mano existente (K), retirarse (F) y abandonar el juego, o apostar (C), lo que requiere que todos los jugadores muestren sus manos. Para mantener las cosas simples, dejemos que M = 2, y que cada jugador reciba una mano compuesta por una sola carta sacada del mismo palo. Por lo tanto, durante un enfrentamiento, el jugador que tenga la carta numéricamente más alta (el 2 es el más bajo, el as es el más alto) gana el bote. Durante un intercambio de cartas, la carta descartada se coloca ya sea en la pila L, indicando al otro agente que era una carta de número bajo menor que 8, o en la 820 The Sixth Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) H, indicando que la carta tenía un rango mayor o igual a 8. Ten en cuenta que, por ejemplo, si se descarta una carta de número bajo, la probabilidad de recibir una carta baja a cambio se reduce. Mostramos el nivel 1 I-ID para el Poker simplificado de dos jugadores en la Fig. 11. Consideramos dos modelos (tipos de personalidad) del agente j. El tipo conservador cree que es probable que su oponente tenga una carta de alto número en su mano. Por otro lado, el agente agresivo j cree con alta probabilidad que su oponente tiene una carta de número más bajo. Por lo tanto, los dos tipos difieren en sus creencias sobre la mano de sus oponentes. En ambos estos modelos de nivel 0, se asume que el oponente realiza sus acciones siguiendo una distribución fija y uniforme. Con tres acciones restantes, independientemente de su mano (a menos que sea un as), el agente agresivo elige intercambiar su carta, con la intención de mejorar su mano actual. Esto se debe a que cree que el otro tiene una carta baja, lo que mejora sus posibilidades de obtener una carta alta durante el intercambio. El agente conservador elige quedarse con su carta, sin importar su mano, porque considera que sus posibilidades de obtener una carta alta son escasas, ya que cree que su oponente tiene una. Figura 11: (a) Nivel 1 I-ID del agente i. La observación revela información sobre la mano de js del paso de tiempo anterior, (b) IDs de nivel 0 del agente j cuyos nodos de decisión están mapeados a los nodos de probabilidad, A1 j , A2 j , en (a). La política de un agente de nivel 1 i que cree que cada carta, excepto la suya, tiene la misma probabilidad de estar en su mano (tipo de personalidad neutral) y j podría ser de tipo agresivo o conservador, se muestra en la Figura 12. Su mano contiene la carta numerada 8. El agente comienza por mantener su carta. Al ver que j no intercambió una carta (N), i cree con probabilidad 1 que j es conservador y, por lo tanto, mantendrá sus cartas. i responde manteniendo su carta o intercambiándola porque j tiene igual probabilidad de tener una carta más baja o más alta. Si observo que j descartó su carta en la pila L o H, creo que j es agresivo. Al observar a L, i se da cuenta de que j tenía una carta baja, y es probable que tenga una carta alta después de su intercambio. Dado que la probabilidad de recibir una carta baja es alta en este momento, i decide quedarse con su carta. Al observar la carta H, creyendo que la probabilidad de recibir una carta de número alto es alta, i decide intercambiar su carta. En el paso final, i decide llamar independientemente de su historial de observaciones porque su creencia de que j tiene una carta más alta no es lo suficientemente alta como para concluir que es mejor retirarse y renunciar al pago. Esto se debe en parte al hecho de que una observación de, digamos, L, restablece las creencias del agente en el paso de tiempo anterior sobre las cartas de números bajos solamente. 6. DISCUSIÓN Mostramos cómo los DIDs pueden ser extendidos a los I-DIDs que permiten la toma de decisiones secuenciales en línea en entornos multiagentes inciertos. Nuestra representación gráfica de I-DIDs mejora la Figura 12 anterior: Un agente de nivel 1 es una política de tres pasos en el problema de Poker. i comienza creyendo que j tiene la misma probabilidad de ser agresivo o conservador y podría tener cualquier carta en su mano con igual probabilidad. Trabaja significativamente al ser más transparente, semánticamente claro y capaz de ser resuelto utilizando algoritmos estándar que apuntan a los DIDs. Los I-DIDs extienden los NIDs para permitir la toma de decisiones secuenciales a lo largo de múltiples pasos de tiempo en presencia de otros agentes que interactúan. Los I-DIDs pueden ser vistos como representaciones gráficas concisas para IPOMDPs que proporcionan una forma de explotar la estructura del problema y llevar a cabo la toma de decisiones en línea a medida que el agente actúa y observa dadas sus creencias previas. Actualmente estamos investigando formas de resolver I-DIDs aproximadamente con límites demostrables en la calidad de la solución. Agradecimiento: Agradecemos a Piotr Gmytrasiewicz por algunas discusiones útiles relacionadas con este trabajo. El primer autor desea agradecer el apoyo de una beca UGARF. 7. REFERENCIAS [1] R. J. Aumann. Epistemología interactiva i: Conocimiento. Revista Internacional de Teoría de Juegos, 28:263-300, 1999. [2] D. Billings, A. Davidson, J. Schaeffer y D. Szafron. El desafío del póker. AIJ, 2001. [3] A. Brandenburger y E. Dekel. Jerarquías de creencias y conocimiento común. Revista de Teoría Económica, 59:189-198, 1993. [4] C. Camerer. Teoría de juegos conductual: Experimentos en interacción estratégica. Princeton University Press, 2003. [5] E. Fehr y S. Gachter. Cooperación y castigo en experimentos de bienes públicos. Revista Económica Americana, 90(4):980-994, 2000. [6] D. Fudenberg y D. K. Levine. La Teoría del Aprendizaje en los Juegos. MIT Press, 1998. [7] D. Fudenberg y J. Tirole. Teoría de juegos. MIT Press, 1991. [8] Y. Gal y A. Pfeffer. Un lenguaje para modelar los procesos de toma de decisiones de agentes en juegos. En AAMAS, 2003. [9] P. Gmytrasiewicz y P. Doshi. Un marco para la planificación secuencial en entornos multiagentes. JAIR, 24:49-79, 2005. [10] P. Gmytrasiewicz y E. Durfee. Coordinación racional en entornos multiagente. JAAMAS, 3(4):319-350, 2000. [11] J. C. Harsanyi. Juegos con información incompleta jugados por jugadores bayesianos. Ciencia de la Gestión, 14(3):159-182, 1967. [12] R. A. Howard y J. E. Matheson. Diagramas de influencia. En R. A. Howard y J. E. Matheson, editores, Los Principios y Aplicaciones del Análisis de Decisiones. Grupo de Decisiones Estratégicas, Menlo Park, CA 94025, 1984. [13] L. Kaelbling, M. Littman y A. Cassandra. Planificación y actuación en dominios estocásticos parcialmente observables. Revista de Inteligencia Artificial, 2, 1998. [14] D. Koller y B. Milch. Diagramas de influencia multiagente para representar y resolver juegos. En IJCAI, páginas 1027-1034, 2001. [15] K. Polich y P. Gmytrasiewicz. Diagramas de influencia interactivos y dinámicos. En el taller GTDT, AAMAS, 2006. [16] B. Rathnas, P. Doshi y P. J. Gmytrasiewicz. Soluciones exactas para POMDP interactivos utilizando equivalencia conductual. En la Conferencia de Agentes Autónomos y Sistemas Multiagente (AAMAS), 2006. [17] S. Russell y P. Norvig. Inteligencia Artificial: Un Enfoque Moderno (Segunda Edición). Prentice Hall, 2003. [18] R. D. Shachter. \n\nPrentice Hall, 2003. [18] R. D. Shachter. Evaluando diagramas de influencia. Investigación de Operaciones, 34(6):871-882, 1986. [19] D. Suryadi y P. Gmytrasiewicz. Aprendiendo modelos de otros agentes utilizando diagramas de influencia. En UM, 1999. El Sexto Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 821 ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "independence structure": {
            "translated_key": "estructura de independencia",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Graphical Models for Online Solutions to Interactive POMDPs Prashant Doshi Dept.",
                "of Computer Science University of Georgia Athens, GA 30602, USA pdoshi@cs.uga.edu Yifeng Zeng Dept. of Computer Science Aalborg University DK-9220 Aalborg, Denmark yfzeng@cs.aau.edu Qiongyu Chen Dept. of Computer Science National Univ. of Singapore 117543, Singapore chenqy@comp.nus.edu.sg ABSTRACT We develop a new graphical representation for interactive partially observable Markov decision processes (I-POMDPs) that is significantly more transparent and semantically clear than the previous representation.",
                "These graphical models called interactive dynamic influence diagrams (I-DIDs) seek to explicitly model the structure that is often present in real-world problems by decomposing the situation into chance and decision variables, and the dependencies between the variables.",
                "I-DIDs generalize DIDs, which may be viewed as graphical representations of POMDPs, to multiagent settings in the same way that I-POMDPs generalize POMDPs.",
                "I-DIDs may be used to compute the policy of an agent online as the agent acts and observes in a setting that is populated by other interacting agents.",
                "Using several examples, we show how I-DIDs may be applied and demonstrate their usefulness.",
                "Categories and Subject Descriptors I.2.11 [Distributed Artificial Intelligence]: Multiagent Systems General Terms Theory 1.",
                "INTRODUCTION Interactive partially observable Markov decision processes (IPOMDPs) [9] provide a framework for sequential decision-making in partially observable multiagent environments.",
                "They generalize POMDPs [13] to multiagent settings by including the other agents computable models in the state space along with the states of the physical environment.",
                "The models encompass all information influencing the agents behaviors, including their preferences, capabilities, and beliefs, and are thus analogous to types in Bayesian games [11].",
                "I-POMDPs adopt a subjective approach to understanding strategic behavior, rooted in a decision-theoretic framework that takes a decision-makers perspective in the interaction.",
                "In [15], Polich and Gmytrasiewicz introduced interactive dynamic influence diagrams (I-DIDs) as the computational representations of I-POMDPs.",
                "I-DIDs generalize DIDs [12], which may be viewed as computational counterparts of POMDPs, to multiagents settings in the same way that I-POMDPs generalize POMDPs.",
                "I-DIDs contribute to a growing line of work [19] that includes multi-agent influence diagrams (MAIDs) [14], and more recently, networks of influence diagrams (NIDs) [8].",
                "These formalisms seek to explicitly model the structure that is often present in real-world problems by decomposing the situation into chance and decision variables, and the dependencies between the variables.",
                "MAIDs provide an alternative to normal and extensive game forms using a graphical formalism to represent games of imperfect information with a decision node for each agents actions and chance nodes capturing the agents private information.",
                "MAIDs objectively analyze the game, efficiently computing the Nash equilibrium profile by exploiting the <br>independence structure</br>.",
                "NIDs extend MAIDs to include agents uncertainty over the game being played and over models of the other agents.",
                "Each model is a MAID and the network of MAIDs is collapsed, bottom up, into a single MAID for computing the equilibrium of the game keeping in mind the different models of each agent.",
                "Graphical formalisms such as MAIDs and NIDs open up a promising area of research that aims to represent multiagent interactions more transparently.",
                "However, MAIDs provide an analysis of the game from an external viewpoint and the applicability of both is limited to static single play games.",
                "Matters are more complex when we consider interactions that are extended over time, where predictions about others future actions must be made using models that change as the agents act and observe.",
                "I-DIDs address this gap by allowing the representation of other agents models as the values of a special model node.",
                "Both, other agents models and the original agents beliefs over these models are updated over time using special-purpose implementations.",
                "In this paper, we improve on the previous preliminary representation of the I-DID shown in [15] by using the insight that the static I-ID is a type of NID.",
                "Thus, we may utilize NID-specific language constructs such as multiplexers to represent the model node, and subsequently the I-ID, more transparently.",
                "Furthermore, we clarify the semantics of the special purpose policy link introduced in the representation of I-DID by [15], and show that it could be replaced by traditional dependency links.",
                "In the previous representation of the I-DID, the update of the agents belief over the models of others as the agents act and receive observations was denoted using a special link called the model update link that connected the model nodes over time.",
                "We explicate the semantics of this link by showing how it can be implemented using the traditional dependency links between the chance nodes that constitute the model nodes.",
                "The net result is a representation of I-DID that is significantly more transparent, semantically clear, and capable of being implemented using the standard algorithms for solving DIDs.",
                "We show how IDIDs may be used to model an agents uncertainty over others models, that may themselves be I-DIDs.",
                "Solution to the I-DID is a policy that prescribes what the agent should do over time, given its beliefs over the physical state and others models.",
                "Analogous to DIDs, I-DIDs may be used to compute the policy of an agent online as the agent acts and observes in a setting that is populated by other interacting agents. 2.",
                "BACKGROUND: FINITELY NESTED IPOMDPS Interactive POMDPs generalize POMDPs to multiagent settings by including other agents models as part of the state space [9].",
                "Since other agents may also reason about others, the interactive state space is strategically nested; it contains beliefs about other agents models and their beliefs about others.",
                "For simplicity of presentation we consider an agent, i, that is interacting with one other agent, j.",
                "A finitely nested I-POMDP of agent i with a strategy level l is defined as the tuple: I-POMDPi,l = ISi,l, A, Ti, Ωi, Oi, Ri where: • ISi,l denotes a set of interactive states defined as, ISi,l = S × Mj,l−1, where Mj,l−1 = {Θj,l−1 ∪ SMj}, for l ≥ 1, and ISi,0 = S, where S is the set of states of the physical environment.",
                "Θj,l−1 is the set of computable intentional models of agent j: θj,l−1 = bj,l−1, ˆθj where the frame, ˆθj = A, Ωj, Tj, Oj, Rj, OCj .",
                "Here, j is Bayes rational and OCj is js optimality criterion.",
                "SMj is the set of subintentional models of j.",
                "Simple examples of subintentional models include a no-information model [10] and a fictitious play model [6], both of which are history independent.",
                "We give a recursive bottom-up construction of the interactive state space below.",
                "ISi,0 = S, Θj,0 = { bj,0, ˆθj | bj,0 ∈ Δ(ISj,0)} ISi,1 = S × {Θj,0 ∪ SMj}, Θj,1 = { bj,1, ˆθj | bj,1 ∈ Δ(ISj,1)} . . . . . .",
                "ISi,l = S × {Θj,l−1 ∪ SMj}, Θj,l = { bj,l, ˆθj | bj,l ∈ Δ(ISj,l)} Similar formulations of nested spaces have appeared in [1, 3]. • A = Ai × Aj is the set of joint actions of all agents in the environment; • Ti : S ×A×S → [0, 1], describes the effect of the joint actions on the physical states of the environment; • Ωi is the set of observations of agent i; • Oi : S × A × Ωi → [0, 1] gives the likelihood of the observations given the physical state and joint action; • Ri : ISi × A → R describes agent is preferences over its interactive states.",
                "Usually only the physical states will matter.",
                "Agent is policy is the mapping, Ω∗ i → Δ(Ai), where Ω∗ i is the set of all observation histories of agent i.",
                "Since belief over the interactive states forms a sufficient statistic [9], the policy can also be represented as a mapping from the set of all beliefs of agent i to a distribution over its actions, Δ(ISi) → Δ(Ai). 2.1 Belief Update Analogous to POMDPs, an agent within the I-POMDP framework updates its belief as it acts and observes.",
                "However, there are two differences that complicate the belief update in multiagent settings when compared to single agent ones.",
                "First, since the state of the physical environment depends on the actions of both agents, is prediction of how the physical state changes has to be made based on its prediction of js actions.",
                "Second, changes in js models have to be included in is belief update.",
                "Specifically, if j is intentional then an update of js beliefs due to its action and observation has to be included.",
                "In other words, i has to update its belief based on its prediction of what j would observe and how j would update its belief.",
                "If js model is subintentional, then js probable observations are appended to the observation history contained in the model.",
                "Formally, we have: Pr(ist |at−1 i , bt−1 i,l ) = β ISt−1:mt−1 j =θt j bt−1 i,l (ist−1 ) × at−1 j Pr(at−1 j |θt−1 j,l−1)Oi(st , at−1 i , at−1 j , ot i) ×Ti(st−1 , at−1 i , at−1 j , st ) ot j Oj(st , at−1 i , at−1 j , ot j) ×τ(SEθt j (bt−1 j,l−1, at−1 j , ot j) − bt j,l−1) (1) where β is the normalizing constant, τ is 1 if its argument is 0 otherwise it is 0, Pr(at−1 j |θt−1 j,l−1) is the probability that at−1 j is Bayes rational for the agent described by model θt−1 j,l−1, and SE(·) is an abbreviation for the belief update.",
                "For a version of the belief update when js model is subintentional, see [9].",
                "If agent j is also modeled as an I-POMDP, then is belief update invokes js belief update (via the term SEθt j ( bt−1 j,l−1 , at−1 j , ot j)), which in turn could invoke is belief update and so on.",
                "This recursion in belief nesting bottoms out at the 0th level.",
                "At this level, the belief update of the agent reduces to a POMDP belief update. 1 For illustrations of the belief update, additional details on I-POMDPs, and how they compare with other multiagent frameworks, see [9]. 2.2 Value Iteration Each belief state in a finitely nested I-POMDP has an associated value reflecting the maximum payoff the agent can expect in this belief state: Un ( bi,l, θi ) = max ai∈Ai is∈ISi,l ERi(is, ai)bi,l(is)+ γ oi∈Ωi Pr(oi|ai, bi,l)Un−1 ( SEθi (bi,l, ai, oi), θi ) (2) where, ERi(is, ai) = aj Ri(is, ai, aj)Pr(aj|mj,l−1) (since is = (s, mj,l−1)).",
                "Eq. 2 is a basis for value iteration in I-POMDPs.",
                "Agent is optimal action, a∗ i , for the case of finite horizon with discounting, is an element of the set of optimal actions for the belief state, OPT(θi), defined as: OPT( bi,l, θi ) = argmax ai∈Ai is∈ISi,l ERi(is, ai)bi,l(is) +γ oi∈Ωi Pr(oi|ai, bi,l)Un ( SEθi (bi,l, ai, oi), θi ) (3) 3.",
                "INTERACTIVEINFLUENCEDIAGRAMS A naive extension of influence diagrams (IDs) to settings populated by multiple agents is possible by treating other agents as automatons, represented using chance nodes.",
                "However, this approach assumes that the agents actions are controlled using a probability distribution that does not change over time.",
                "Interactive influence diagrams (I-IDs) adopt a more sophisticated approach by generalizing IDs to make them applicable to settings shared with other agents who may act and observe, and update their beliefs. 3.1 Syntax In addition to the usual chance, decision, and utility nodes, IIDs include a new type of node called the model node.",
                "We show a general level l I-ID in Fig. 1(a), where the model node (Mj,l−1) is denoted using a hexagon.",
                "We note that the probability distribution over the chance node, S, and the model node together represents agent is belief over its interactive states.",
                "In addition to the model 1 The 0th level model is a POMDP: Other agents actions are treated as exogenous events and folded into the T, O, and R functions.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 815 Figure 1: (a) A generic level l I-ID for agent i situated with one other agent j.",
                "The hexagon is the model node (Mj,l−1) whose structure we show in (b).",
                "Members of the model node are I-IDs themselves (m1 j,l−1, m2 j,l−1; diagrams not shown here for simplicity) whose decision nodes are mapped to the corresponding chance nodes (A1 j , A2 j ).",
                "Depending on the value of the node, Mod[Mj], the distribution of each of the chance nodes is assigned to the node Aj. (c) The transformed I-ID with the model node replaced by the chance nodes and the relationships between them. node, I-IDs differ from IDs by having a dashed link (called the policy link in [15]) between the model node and a chance node, Aj, that represents the distribution over the other agents actions given its model.",
                "In the absence of other agents, the model node and the chance node, Aj, vanish and I-IDs collapse into traditional IDs.",
                "The model node contains the alternative computational models ascribed by i to the other agent from the set, Θj,l−1 ∪ SMj, where Θj,l−1 and SMj were defined previously in Section 2.",
                "Thus, a model in the model node may itself be an I-ID or ID, and the recursion terminates when a model is an ID or subintentional.",
                "Because the model node contains the alternative models of the other agent as its values, its representation is not trivial.",
                "In particular, some of the models within the node are I-IDs that when solved generate the agents optimal policy in their decision nodes.",
                "Each decision node is mapped to the corresponding chance node, say A1 j , in the following way: if OPT is the set of optimal actions obtained by solving the I-ID (or ID), then Pr(aj ∈ A1 j ) = 1 |OP T | if aj ∈ OPT, 0 otherwise.",
                "Borrowing insights from previous work [8], we observe that the model node and the dashed policy link that connects it to the chance node, Aj, could be represented as shown in Fig. 1(b).",
                "The decision node of each level l − 1 I-ID is transformed into a chance node, as we mentioned previously, so that the actions with the largest value in the decision node are assigned uniform probabilities in the chance node while the rest are assigned zero probability.",
                "The different chance nodes (A1 j , A2 j ), one for each model, and additionally, the chance node labeled Mod[Mj] form the parents of the chance node, Aj.",
                "Thus, there are as many action nodes (A1 j , A2 j ) in Mj,l−1 as the number of models in the support of agent is beliefs.",
                "The conditional probability table of the chance node, Aj, is a multiplexer that assumes the distribution of each of the action nodes (A1 j , A2 j ) depending on the value of Mod[Mj].",
                "The values of Mod[Mj] denote the different models of j.",
                "In other words, when Mod[Mj] has the value m1 j,l−1, the chance node Aj assumes the distribution of the node A1 j , and Aj assumes the distribution of A2 j when Mod[Mj] has the value m2 j,l−1.",
                "The distribution over the node, Mod[Mj], is the agent is belief over the models of j given a physical state.",
                "For more agents, we will have as many model nodes as there are agents.",
                "Notice that Fig. 1(b) clarifies the semantics of the policy link, and shows how it can be represented using the traditional dependency links.",
                "In Fig. 1(c), we show the transformed I-ID when the model node is replaced by the chance nodes and relationships between them.",
                "In contrast to the representation in [15], there are no special-purpose policy links, rather the I-ID is composed of only those types of nodes that are found in traditional IDs and dependency relationships between the nodes.",
                "This allows I-IDs to be represented and implemented using conventional application tools that target IDs.",
                "Note that we may view the level l I-ID as a NID.",
                "Specifically, each of the level l − 1 models within the model node are blocks in the NID (see Fig. 2).",
                "If the level l = 1, each block is a traditional ID, otherwise if l > 1, each block within the NID may itself be a NID.",
                "Note that within the I-IDs (or IDs) at each level, there is only a single decision node.",
                "Thus, our NID does not contain any MAIDs.",
                "Figure 2: A level l I-ID represented as a NID.",
                "The probabilities assigned to the blocks of the NID are is beliefs over js models conditioned on a physical state. 3.2 Solution The solution of an I-ID proceeds in a bottom-up manner, and is implemented recursively.",
                "We start by solving the level 0 models, which, if intentional, are traditional IDs.",
                "Their solutions provide probability distributions over the other agents actions, which are entered in the corresponding chance nodes found in the model node of the level 1 I-ID.",
                "The mapping from the level 0 models decision nodes to the chance nodes is carried out so that actions with the largest value in the decision node are assigned uniform probabilities in the chance node while the rest are assigned zero probability.",
                "Given the distributions over the actions within the different chance nodes (one for each model of the other agent), the level 1 I-ID is transformed as shown in Fig. 1(c).",
                "During the transformation, the conditional probability table (CPT) of the node, Aj, is populated such that the node assumes the distribution of each of the chance nodes depending on the value of the node, Mod[Mj].",
                "As we mentioned previously, the values of the node Mod[Mj] denote the different models of the other agent, and its distribution is the agent is belief over the models of j conditioned on the physical state.",
                "The transformed level 1 I-ID is a traditional ID that may be solved us816 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) (a) (b) Figure 3: (a) A generic two time-slice level l I-DID for agent i in a setting with one other agent j.",
                "Notice the dotted model update link that denotes the update of the models of j and the distribution over the models over time. (b) The semantics of the model update link. ing the standard expected utility maximization method [18].",
                "This procedure is carried out up to the level l I-ID whose solution gives the non-empty set of optimal actions that the agent should perform given its belief.",
                "Notice that analogous to IDs, I-IDs are suitable for online decision-making when the agents current belief is known. 4.",
                "INTERACTIVE DYNAMIC INFLUENCE DIAGRAMS Interactive dynamic influence diagrams (I-DIDs) extend I-IDs (and NIDs) to allow sequential decision-making over several time steps.",
                "Just as DIDs are structured graphical representations of POMDPs, I-DIDs are the graphical online analogs for finitely nested I-POMDPs.",
                "I-DIDs may be used to optimize over a finite look-ahead given initial beliefs while interacting with other, possibly similar, agents. 4.1 Syntax We depict a general two time-slice I-DID in Fig. 3(a).",
                "In addition to the model nodes and the dashed policy link, what differentiates an I-DID from a DID is the model update link shown as a dotted arrow in Fig. 3(a).",
                "We explained the semantics of the model node and the policy link in the previous section; we describe the model updates next.",
                "The update of the model node over time involves two steps: First, given the models at time t, we identify the updated set of models that reside in the model node at time t + 1.",
                "Recall from Section 2 that an agents intentional model includes its belief.",
                "Because the agents act and receive observations, their models are updated to reflect their changed beliefs.",
                "Since the set of optimal actions for a model could include all the actions, and the agent may receive any one of |Ωj| possible observations, the updated set at time step t + 1 will have at most |Mt j,l−1||Aj||Ωj| models.",
                "Here, |Mt j,l−1| is the number of models at time step t, |Aj| and |Ωj| are the largest spaces of actions and observations respectively, among all the models.",
                "Second, we compute the new distribution over the updated models given the original distribution and the probability of the agent performing the action and receiving the observation that led to the updated model.",
                "These steps are a part of agent is belief update formalized using Eq. 1.",
                "In Fig. 3(b), we show how the dotted model update link is implemented in the I-DID.",
                "If each of the two level l − 1 models ascribed to j at time step t results in one action, and j could make one of two possible observations, then the model node at time step t + 1 contains four updated models (mt+1,1 j,l−1 ,mt+1,2 j,l−1 , mt+1,3 j,l−1 , and mt+1,4 j,l−1 ).",
                "These models differ in their initial beliefs, each of which is the result of j updating its beliefs due to its action and a possible observation.",
                "The decision nodes in each of the I-DIDs or DIDs that represent the lower level models are mapped to the corresponding Figure 4: Transformed I-DID with the model nodes and model update link replaced with the chance nodes and the relationships (in bold). chance nodes, as mentioned previously.",
                "Next, we describe how the distribution over the updated set of models (the distribution over the chance node Mod[Mt+1 j ] in Mt+1 j,l−1) is computed.",
                "The probability that js updated model is, say mt+1,1 j,l−1 , depends on the probability of j performing the action and receiving the observation that led to this model, and the prior distribution over the models at time step t. Because the chance node At j assumes the distribution of each of the action nodes based on the value of Mod[Mt j ], the probability of the action is given by this chance node.",
                "In order to obtain the probability of js possible observation, we introduce the chance node Oj, which depending on the value of Mod[Mt j ] assumes the distribution of the observation node in the lower level model denoted by Mod[Mt j ].",
                "Because the probability of js observations depends on the physical state and the joint actions of both agents, the node Oj is linked with St+1 , At j, and At i. 2 Analogous to At j, the conditional probability table of Oj is also a multiplexer modulated by Mod[Mt j ].",
                "Finally, the distribution over the prior models at time t is obtained from the chance node, Mod[Mt j ] in Mt j,l−1.",
                "Consequently, the chance nodes, Mod[Mt j ], At j, and Oj, form the parents of Mod[Mt+1 j ] in Mt+1 j,l−1.",
                "Notice that the model update link may be replaced by the dependency links between the chance nodes that constitute the model nodes in the two time slices.",
                "In Fig. 4 we show the two time-slice I-DID with the model nodes replaced by the chance nodes and the relationships between them.",
                "Chance nodes and dependency links that not in bold are standard, usually found in DIDs.",
                "Expansion of the I-DID over more time steps requires the repetition of the two steps of updating the set of models that form the 2 Note that Oj represents js observation at time t + 1.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 817 values of the model node and adding the relationships between the chance nodes, as many times as there are model update links.",
                "We note that the possible set of models of the other agent j grows exponentially with the number of time steps.",
                "For example, after T steps, there may be at most |Mt=1 j,l−1|(|Aj||Ωj|)T −1 candidate models residing in the model node. 4.2 Solution Analogous to I-IDs, the solution to a level l I-DID for agent i expanded over T time steps may be carried out recursively.",
                "For the purpose of illustration, let l=1 and T=2.",
                "The solution method uses the standard look-ahead technique, projecting the agents action and observation sequences forward from the current belief state [17], and finding the possible beliefs that i could have in the next time step.",
                "Because agent i has a belief over js models as well, the lookahead includes finding out the possible models that j could have in the future.",
                "Consequently, each of js subintentional or level 0 models (represented using a standard DID) in the first time step must be solved to obtain its optimal set of actions.",
                "These actions are combined with the set of possible observations that j could make in that model, resulting in an updated set of candidate models (that include the updated beliefs) that could describe the behavior of j. Beliefs over this updated set of candidate models are calculated using the standard inference methods using the dependency relationships between the model nodes as shown in Fig. 3(b).",
                "We note the recursive nature of this solution: in solving agent is level 1 I-DID, js level 0 DIDs must be solved.",
                "If the nesting of models is deeper, all models at all levels starting from 0 are solved in a bottom-up manner.",
                "We briefly outline the recursive algorithm for solving agent is Algorithm for solving I-DID Input : level l ≥ 1 I-ID or level 0 ID, T Expansion Phase 1.",
                "For t from 1 to T − 1 do 2.",
                "If l ≥ 1 then Populate Mt+1 j,l−1 3.",
                "For each mt j in Range(Mt j,l−1) do 4.",
                "Recursively call algorithm with the l − 1 I-ID (or ID) that represents mt j and the horizon, T − t + 1 5.",
                "Map the decision node of the solved I-ID (or ID), OPT(mt j), to a chance node Aj 6.",
                "For each aj in OPT(mt j) do 7.",
                "For each oj in Oj (part of mt j) do 8.",
                "Update js belief, bt+1 j ← SE(bt j, aj, oj) 9. mt+1 j ← New I-ID (or ID) with bt+1 j as the initial belief 10.",
                "Range(Mt+1 j,l−1) ∪ ← {mt+1 j } 11.",
                "Add the model node, Mt+1 j,l−1, and the dependency links between Mt j,l−1 and Mt+1 j,l−1 (shown in Fig. 3(b)) 12.",
                "Add the chance, decision, and utility nodes for t + 1 time slice and the dependency links between them 13.",
                "Establish the CPTs for each chance node and utility node Look-Ahead Phase 14.",
                "Apply the standard look-ahead and backup method to solve the expanded I-DID Figure 5: Algorithm for solving a level l ≥ 0 I-DID. level l I-DID expanded over T time steps with one other agent j in Fig. 5.",
                "We adopt a two-phase approach: Given an I-ID of level l (described previously in Section 3) with all lower level models also represented as I-IDs or IDs (if level 0), the first step is to expand the level l I-ID over T time steps adding the dependency links and the conditional probability tables for each node.",
                "We particularly focus on establishing and populating the model nodes (lines 3-11).",
                "Note that Range(·) returns the values (lower level models) of the random variable given as input (model node).",
                "In the second phase, we use a standard look-ahead technique projecting the action and observation sequences over T time steps in the future, and backing up the utility values of the reachable beliefs.",
                "Similar to I-IDs, the I-DIDs reduce to DIDs in the absence of other agents.",
                "As we mentioned previously, the 0-th level models are the traditional DIDs.",
                "Their solutions provide probability distributions over actions of the agent modeled at that level to I-DIDs at level 1.",
                "Given probability distributions over other agents actions the level 1 IDIDs can themselves be solved as DIDs, and provide probability distributions to yet higher level models.",
                "Assume that the number of models considered at each level is bound by a number, M. Solving an I-DID of level l in then equivalent to solving O(Ml ) DIDs. 5.",
                "EXAMPLE APPLICATIONS To illustrate the usefulness of I-DIDs, we apply them to three problem domains.",
                "We describe, in particular, the formulation of the I-DID and the optimal prescriptions obtained on solving it. 5.1 Followership-Leadership in the Multiagent Tiger Problem We begin our illustrations of using I-IDs and I-DIDs with a slightly modified version of the multiagent tiger problem discussed in [9].",
                "The problem has two agents, each of which can open the right door (OR), the left door (OL) or listen (L).",
                "In addition to hearing growls (from the left (GL) or from the right (GR)) when they listen, the agents also hear creaks (from the left (CL), from the right (CR), or no creaks (S)), which noisily indicate the other agents opening one of the doors.",
                "When any door is opened, the tiger persists in its original location with a probability of 95%.",
                "Agent i hears growls with a reliability of 65% and creaks with a reliability of 95%.",
                "Agent j, on the other hand, hears growls with a reliability of 95%.",
                "Thus, the setting is such that agent i hears agent j opening doors more reliably than the tigers growls.",
                "This suggests that i could use js actions as an indication of the location of the tiger, as we discuss below.",
                "Each agents preferences are as in the single agent game discussed in [13].",
                "The transition, observation, and reward functions are shown in [16].",
                "A good indicator of the usefulness of normative methods for decision-making like I-DIDs is the emergence of realistic social behaviors in their prescriptions.",
                "In settings of the persistent multiagent tiger problem that reflect real world situations, we demonstrate followership between the agents and, as shown in [15], deception among agents who believe that they are in a follower-leader type of relationship.",
                "In particular, we analyze the situational and epistemological conditions sufficient for their emergence.",
                "The followership behavior, for example, results from the agent knowing its own weaknesses, assessing the strengths, preferences, and possible behaviors of the other, and realizing that its best for it to follow the others actions in order to maximize its payoffs.",
                "Let us consider a particular setting of the tiger problem in which agent i believes that js preferences are aligned with its own - both of them just want to get the gold - and js hearing is more reliable in comparison to itself.",
                "As an example, suppose that j, on listening can discern the tigers location 95% of the times compared to is 65% accuracy.",
                "Additionally, agent i does not have any initial information about the tigers location.",
                "In other words, is single-level nested belief, bi,1, assigns 0.5 to each of the two locations of the tiger.",
                "In addition, i considers two models of j, which differ in js flat level 0 initial beliefs.",
                "This is represented in the level 1 I-ID shown in Fig. 6(a).",
                "According to one model, j assigns a probability of 0.9 that the tiger is behind the left door, while the other 818 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Figure 6: (a) Level 1 I-ID of agent i, (b) two level 0 IDs of agent j whose decision nodes are mapped to the chance nodes, A1 j , A2 j , in (a). model assigns 0.1 to that location (see Fig. 6(b)).",
                "Agent i is undecided on these two models of j.",
                "If we vary is hearing ability, and solve the corresponding level 1 I-ID expanded over three time steps, we obtain the normative behavioral policies shown in Fig 7 that exhibit followership behavior.",
                "If is probability of correctly hearing the growls is 0.65, then as shown in the policy in Fig. 7(a), i begins to conditionally follow js actions: i opens the same door that j opened previously iff is own assessment of the tigers location confirms js pick.",
                "If i loses the ability to correctly interpret the growls completely, it blindly follows j and opens the same door that j opened previously (Fig. 7(b)).",
                "Figure 7: Emergence of (a) conditional followership, and (b) blind followership in the tiger problem.",
                "Behaviors of interest are in bold. * is a wildcard, and denotes any one of the observations.",
                "We observed that a single level of belief nesting - beliefs about the others models - was sufficient for followership to emerge in the tiger problem.",
                "However, the epistemological requirements for the emergence of leadership are more complex.",
                "For an agent, say j, to emerge as a leader, followership must first emerge in the other agent i.",
                "As we mentioned previously, if i is certain that its preferences are identical to those of j, and believes that j has a better sense of hearing, i will follow js actions over time.",
                "Agent j emerges as a leader if it believes that i will follow it, which implies that js belief must be nested two levels deep to enable it to recognize its leadership role.",
                "Realizing that i will follow presents j with an opportunity to influence is actions in the benefit of the collective good or its self-interest alone.",
                "For example, in the tiger problem, let us consider a setting in which if both i and j open the correct door, then each gets a payoff of 20 that is double the original.",
                "If j alone selects the correct door, it gets the payoff of 10.",
                "On the other hand, if both agents pick the wrong door, their penalties are cut in half.",
                "In this setting, it is in both js best interest as well as the collective betterment for j to use its expertise in selecting the correct door, and thus be a good leader.",
                "However, consider a slightly different problem in which j gains from is loss and is penalized if i gains.",
                "Specifically, let is payoff be subtracted from js, indicating that j is antagonistic toward i - if j picks the correct door and i the wrong one, then is loss of 100 becomes js gain.",
                "Agent j believes that i incorrectly thinks that js preferences are those that promote the collective good and that it starts off by believing with 99% confidence where the tiger is.",
                "Because i believes that its preferences are similar to those of j, and that j starts by believing almost surely that one of the two is the correct location (two level 0 models of j), i will start by following js actions.",
                "We show is normative policy on solving its singly-nested I-DID over three time steps in Fig. 8(a).",
                "The policy demonstrates that i will blindly follow js actions.",
                "Since the tiger persists in its original location with a probability of 0.95, i will select the same door again.",
                "If j begins the game with a 99% probability that the tiger is on the right, solving js I-DID nested two levels deep, results in the policy shown in Fig. 8(b).",
                "Even though j is almost certain that OL is the correct action, it will start by selecting OR, followed by OL.",
                "Agent js intention is to deceive i who, it believes, will follow js actions, so as to gain $110 in the second time step, which is more than what j would gain if it were to be honest.",
                "Figure 8: Emergence of deception between agents in the tiger problem.",
                "Behaviors of interest are in bold. * denotes as before. (a) Agent is policy demonstrating that it will blindly follow js actions. (b) Even though j is almost certain that the tiger is on the right, it will start by selecting OR, followed by OL, in order to deceive i. 5.2 Altruism and Reciprocity in the Public Good Problem The public good (PG) problem [7], consists of a group of M agents, each of whom must either contribute some resource to a public pot or keep it for themselves.",
                "Since resources contributed to the public pot are shared among all the agents, they are less valuable to the agent when in the public pot.",
                "However, if all agents choose to contribute their resources, then the payoff to each agent is more than if no one contributes.",
                "Since an agent gets its share of the public pot irrespective of whether it has contributed or not, the dominating action is for each agent to not contribute, and instead free ride on others contributions.",
                "However, behaviors of human players in empirical simulations of the PG problem differ from the normative predictions.",
                "The experiments reveal that many players initially contribute a large amount to the public pot, and continue to contribute when the PG problem is played repeatedly, though in decreasing amounts [4].",
                "Many of these experiments [5] report that a small core group of players persistently contributes to the public pot even when all others are defecting.",
                "These experiments also reveal that players who persistently contribute have altruistic or reciprocal preferences matching expected cooperation of others.",
                "For simplicity, we assume that the game is played between M = 2 agents, i and j.",
                "Let each agent be initially endowed with XT amount of resources.",
                "While the classical PG game formulation permits each agent to contribute any quantity of resources (≤ XT ) to the public pot, we simplify the action space by allowing two possible actions.",
                "Each agent may choose to either contribute (C) a fixed amount of the resources, or not contribute.",
                "The latter action is deThe Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 819 noted as defect (D).",
                "We assume that the actions are not observable to others.",
                "The value of resources in the public pot is discounted by ci for each agent i, where ci is the marginal private return.",
                "We assume that ci < 1 so that the agent does not benefit enough that it contributes to the public pot for private gain.",
                "Simultaneously, ciM > 1, making collective contribution pareto optimal. i/j C D C 2ciXT , 2cjXT ciXT − cp, XT + cjXT − P D XT + ciXT − P, cjXT − cp XT , XT Table 1: The one-shot PG game with punishment.",
                "In order to encourage contributions, the contributing agents punish free riders but incur a small cost for administering the punishment.",
                "Let P be the punishment meted out to the defecting agent and cp the non-zero cost of punishing for the contributing agent.",
                "For simplicity, we assume that the cost of punishing is same for both the agents.",
                "The one-shot PG game with punishment is shown in Table. 1.",
                "Let ci = cj, cp > 0, and if P > XT − ciXT , then defection is no longer a dominating action.",
                "If P < XT − ciXT , then defection is the dominating action for both.",
                "If P = XT − ciXT , then the game is not dominance-solvable.",
                "Figure 9: (a) Level 1 I-ID of agent i, (b) level 0 IDs of agent j with decision nodes mapped to the chance nodes, A1 j and A2 j , in (a).",
                "We formulate a sequential version of the PG problem with punishment from the perspective of agent i.",
                "Though in the repeated PG game, the quantity in the public pot is revealed to all the agents after each round of actions, we assume in our formulation that it is hidden from the agents.",
                "Each agent may contribute a fixed amount, xc, or defect.",
                "An agent on performing an action receives an observation of plenty (PY) or meager (MR) symbolizing the state of the public pot.",
                "Notice that the observations are also indirectly indicative of agent js actions because the state of the public pot is influenced by them.",
                "The amount of resources in agent is private pot, is perfectly observable to i.",
                "The payoffs are analogous to Table. 1.",
                "Borrowing from the empirical investigations of the PG problem [5], we construct level 0 IDs for j that model altruistic and non-altruistic types (Fig. 9(b)).",
                "Specifically, our altruistic agent has a high marginal private return (cj is close to 1) and does not punish others who defect.",
                "Let xc = 1 and the level 0 agent be punished half the times it defects.",
                "With one action remaining, both types of agents choose to contribute to avoid being punished.",
                "With two actions to go, the altruistic type chooses to contribute, while the other defects.",
                "This is because cj for the altruistic type is close to 1, thus the expected punishment, 0.5P > (1 − cj), which the altruistic type avoids.",
                "Because cj for the non-altruistic type is less, it prefers not to contribute.",
                "With three steps to go, the altruistic agent contributes to avoid punishment (0.5P > 2(1 − cj)), and the non-altruistic type defects.",
                "For greater than three steps, while the altruistic agent continues to contribute to the public pot depending on how close its marginal private return is to 1, the non-altruistic type prescribes defection.",
                "We analyzed the decisions of an altruistic agent i modeled using a level 1 I-DID expanded over 3 time steps. i ascribes the two level 0 models, mentioned previously, to j (see Fig. 9).",
                "If i believes with a probability 1 that j is altruistic, i chooses to contribute for each of the three steps.",
                "This behavior persists when i is unaware of whether j is altruistic (Fig. 10(a)), and when i assigns a high probability to j being the non-altruistic type.",
                "However, when i believes with a probability 1 that j is non-altruistic and will thus surely defect, i chooses to defect to avoid being punished and because its marginal private return is less than 1.",
                "These results demonstrate that the behavior of our altruistic type resembles that found experimentally.",
                "The non-altruistic level 1 agent chooses to defect regardless of how likely it believes the other agent to be altruistic.",
                "We analyzed the behavior of a reciprocal agent type that matches expected cooperation or defection.",
                "The reciprocal types marginal private return is similar to that of the non-altruistic type, however, it obtains a greater payoff when its action is similar to that of the other.",
                "We consider the case when the reciprocal agent i is unsure of whether j is altruistic and believes that the public pot is likely to be half full.",
                "For this prior belief, i chooses to defect.",
                "On receiving an observation of plenty, i decides to contribute, while an observation of meager makes it defect (Fig. 10(b)).",
                "This is because an observation of plenty signals that the pot is likely to be greater than half full, which results from js action to contribute.",
                "Thus, among the two models ascribed to j, its type is likely to be altruistic making it likely that j will contribute again in the next time step.",
                "Agent i therefore chooses to contribute to reciprocate js action.",
                "An analogous reasoning leads i to defect when it observes a meager pot.",
                "With one action to go, i believing that j contributes, will choose to contribute too to avoid punishment regardless of its observations.",
                "Figure 10: (a) An altruistic level 1 agent always contributes. (b) A reciprocal agent i starts off by defecting followed by choosing to contribute or defect based on its observation of plenty (indicating that j is likely altruistic) or meager (j is non-altruistic). 5.3 Strategies in Two-Player Poker Poker is a popular zero sum card game that has received much attention among the AI research community as a testbed [2].",
                "Poker is played among M ≥ 2 players in which each player receives a hand of cards from a deck.",
                "While several flavors of Poker with varying complexity exist, we consider a simple version in which each player has three plys during which the player may either exchange a card (E), keep the existing hand (K), fold (F) and withdraw from the game, or call (C), requiring all players to show their hands.",
                "To keep matters simple, let M = 2, and each player receive a hand consisting of a single card drawn from the same suit.",
                "Thus, during a showdown, the player who has the numerically larger card (2 is the lowest, ace is the highest) wins the pot.",
                "During an exchange of cards, the discarded card is placed either in the L pile, indicating to the other agent that it was a low numbered card less than 8, or in the 820 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) H pile, indicating that the card had a rank greater than or equal to 8.",
                "Notice that, for example, if a lower numbered card is discarded, the probability of receiving a low card in exchange is now reduced.",
                "We show the level 1 I-ID for the simplified two-player Poker in Fig. 11.",
                "We considered two models (personality types) of agent j.",
                "The conservative type believes that it is likely that its opponent has a high numbered card in its hand.",
                "On the other hand, the aggressive agent j believes with a high probability that its opponent has a lower numbered card.",
                "Thus, the two types differ in their beliefs over their opponents hand.",
                "In both these level 0 models, the opponent is assumed to perform its actions following a fixed, uniform distribution.",
                "With three actions to go, regardless of its hand (unless it is an ace), the aggressive agent chooses to exchange its card, with the intent of improving on its current hand.",
                "This is because it believes the other to have a low card, which improves its chances of getting a high card during the exchange.",
                "The conservative agent chooses to keep its card, no matter its hand because its chances of getting a high card are slim as it believes that its opponent has one.",
                "Figure 11: (a) Level 1 I-ID of agent i.",
                "The observation reveals information about js hand of the previous time step, (b) level 0 IDs of agent j whose decision nodes are mapped to the chance nodes, A1 j , A2 j , in (a).",
                "The policy of a level 1 agent i who believes that each card except its own has an equal likelihood of being in js hand (neutral personality type) and j could be either an aggressive or conservative type, is shown in Fig. 12. is own hand contains the card numbered 8.",
                "The agent starts by keeping its card.",
                "On seeing that j did not exchange a card (N), i believes with probability 1 that j is conservative and hence will keep its cards. i responds by either keeping its card or exchanging it because j is equally likely to have a lower or higher card.",
                "If i observes that j discarded its card into the L or H pile, i believes that j is aggressive.",
                "On observing L, i realizes that j had a low card, and is likely to have a high card after its exchange.",
                "Because the probability of receiving a low card is high now, i chooses to keep its card.",
                "On observing H, believing that the probability of receiving a high numbered card is high, i chooses to exchange its card.",
                "In the final step, i chooses to call regardless of its observation history because its belief that j has a higher card is not sufficiently high to conclude that its better to fold and relinquish the payoff.",
                "This is partly due to the fact that an observation of, say, L resets the agent is previous time step beliefs over js hand to the low numbered cards only. 6.",
                "DISCUSSION We showed how DIDs may be extended to I-DIDs that enable online sequential decision-making in uncertain multiagent settings.",
                "Our graphical representation of I-DIDs improves on the previous Figure 12: A level 1 agent is three step policy in the Poker problem. i starts by believing that j is equally likely to be aggressive or conservative and could have any card in its hand with equal probability. work significantly by being more transparent, semantically clear, and capable of being solved using standard algorithms that target DIDs.",
                "I-DIDs extend NIDs to allow sequential decision-making over multiple time steps in the presence of other interacting agents.",
                "I-DIDs may be seen as concise graphical representations for IPOMDPs providing a way to exploit problem structure and carry out online decision-making as the agent acts and observes given its prior beliefs.",
                "We are currently investigating ways to solve I-DIDs approximately with provable bounds on the solution quality.",
                "Acknowledgment: We thank Piotr Gmytrasiewicz for some useful discussions related to this work.",
                "The first author would like to acknowledge the support of a UGARF grant. 7.",
                "REFERENCES [1] R. J. Aumann.",
                "Interactive epistemology i: Knowledge.",
                "International Journal of Game Theory, 28:263-300, 1999. [2] D. Billings, A. Davidson, J. Schaeffer, and D. Szafron.",
                "The challenge of poker.",
                "AIJ, 2001. [3] A. Brandenburger and E. Dekel.",
                "Hierarchies of beliefs and common knowledge.",
                "Journal of Economic Theory, 59:189-198, 1993. [4] C. Camerer.",
                "Behavioral Game Theory: Experiments in Strategic Interaction.",
                "Princeton University Press, 2003. [5] E. Fehr and S. Gachter.",
                "Cooperation and punishment in public goods experiments.",
                "American Economic Review, 90(4):980-994, 2000. [6] D. Fudenberg and D. K. Levine.",
                "The Theory of Learning in Games.",
                "MIT Press, 1998. [7] D. Fudenberg and J. Tirole.",
                "Game Theory.",
                "MIT Press, 1991. [8] Y. Gal and A. Pfeffer.",
                "A language for modeling agents decision-making processes in games.",
                "In AAMAS, 2003. [9] P. Gmytrasiewicz and P. Doshi.",
                "A framework for sequential planning in multiagent settings.",
                "JAIR, 24:49-79, 2005. [10] P. Gmytrasiewicz and E. Durfee.",
                "Rational coordination in multi-agent environments.",
                "JAAMAS, 3(4):319-350, 2000. [11] J. C. Harsanyi.",
                "Games with incomplete information played by bayesian players.",
                "Management Science, 14(3):159-182, 1967. [12] R. A. Howard and J. E. Matheson.",
                "Influence diagrams.",
                "In R. A. Howard and J. E. Matheson, editors, The Principles and Applications of Decision Analysis.",
                "Strategic Decisions Group, Menlo Park, CA 94025, 1984. [13] L. Kaelbling, M. Littman, and A. Cassandra.",
                "Planning and acting in partially observable stochastic domains.",
                "Artificial Intelligence Journal, 2, 1998. [14] D. Koller and B. Milch.",
                "Multi-agent influence diagrams for representing and solving games.",
                "In IJCAI, pages 1027-1034, 2001. [15] K. Polich and P. Gmytrasiewicz.",
                "Interactive dynamic influence diagrams.",
                "In GTDT Workshop, AAMAS, 2006. [16] B.",
                "Rathnas., P. Doshi, and P. J. Gmytrasiewicz.",
                "Exact solutions to interactive pomdps using behavioral equivalence.",
                "In Autonomous Agents and Multi-Agent Systems Conference (AAMAS), 2006. [17] S. Russell and P. Norvig.",
                "Artificial Intelligence: A Modern Approach (Second Edition).",
                "Prentice Hall, 2003. [18] R. D. Shachter.",
                "Evaluating influence diagrams.",
                "Operations Research, 34(6):871-882, 1986. [19] D. Suryadi and P. Gmytrasiewicz.",
                "Learning models of other agents using influence diagrams.",
                "In UM, 1999.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 821"
            ],
            "original_annotated_samples": [
                "MAIDs objectively analyze the game, efficiently computing the Nash equilibrium profile by exploiting the <br>independence structure</br>."
            ],
            "translated_annotated_samples": [
                "Los MAIDs analizan objetivamente el juego, calculando eficientemente el perfil de equilibrio de Nash al explotar la <br>estructura de independencia</br>."
            ],
            "translated_text": "Modelos gráficos para soluciones en línea a POMDP interactivos de Prashant Doshi Dept. Departamento de Ciencias de la Computación Universidad de Georgia Athens, GA 30602, EE. UU. pdoshi@cs.uga.edu Yifeng Zeng Dept. de Ciencias de la Computación Universidad de Aalborg DK-9220 Aalborg, Dinamarca yfzeng@cs.aau.edu Qiongyu Chen Dept. de Ciencias de la Computación Universidad Nacional de Singapur 117543, Singapur chenqy@comp.nus.edu.sg RESUMEN Desarrollamos una nueva representación gráfica para procesos de decisión de Markov parcialmente observables interactivos (I-POMDPs) que es significativamente más transparente y semánticamente clara que la representación anterior. Estos modelos gráficos llamados diagramas de influencia dinámica interactiva (I-DIDs) buscan modelar explícitamente la estructura que a menudo está presente en problemas del mundo real al descomponer la situación en variables de probabilidad y decisión, y las dependencias entre las variables. Los I-DIDs generalizan los DIDs, los cuales pueden ser vistos como representaciones gráficas de POMDPs, a entornos multiagentes de la misma manera que los I-POMDPs generalizan los POMDPs. Los I-DIDs pueden ser utilizados para calcular la política de un agente en línea mientras el agente actúa y observa en un entorno poblado por otros agentes que interactúan entre sí. Usando varios ejemplos, mostramos cómo se pueden aplicar los I-DIDs y demostramos su utilidad. Categorías y Descriptores de Asignaturas I.2.11 [Inteligencia Artificial Distribuida]: Sistemas Multiagentes Términos Generales Teoría 1. Las Procesos de Decisión de Markov Interactivos Parcialmente Observables (IPOMDPs) proporcionan un marco para la toma de decisiones secuenciales en entornos multiagentes parcialmente observables. Generalizan los POMDPs [13] a entornos multiagentes al incluir los modelos computables de los otros agentes en el espacio de estados junto con los estados del entorno físico. Los modelos abarcan toda la información que influye en los comportamientos de los agentes, incluyendo sus preferencias, capacidades y creencias, y por lo tanto son análogos a los tipos en los juegos bayesianos [11]. I-POMDPs adoptan un enfoque subjetivo para comprender el comportamiento estratégico, arraigado en un marco de teoría de decisiones que toma la perspectiva de los tomadores de decisiones en la interacción. En [15], Polich y Gmytrasiewicz introdujeron diagramas de influencia dinámica interactivos (I-DIDs) como las representaciones computacionales de I-POMDPs. Los I-DIDs generalizan los DIDs [12], los cuales pueden ser vistos como contrapartes computacionales de POMDPs, a entornos de múltiples agentes de la misma manera que los I-POMDPs generalizan los POMDPs. Los I-DIDs contribuyen a una creciente línea de trabajo [19] que incluye diagramas de influencia multiagente (MAIDs) [14], y más recientemente, redes de diagramas de influencia (NIDs) [8]. Estos formalismos buscan modelar explícitamente la estructura que a menudo está presente en problemas del mundo real al descomponer la situación en variables de azar y decisiones, y las dependencias entre las variables. Los MAIDs proporcionan una alternativa a las formas normales y extensivas de juego utilizando un formalismo gráfico para representar juegos de información imperfecta con un nodo de decisión para las acciones de cada agente y nodos de azar que capturan la información privada de los agentes. Los MAIDs analizan objetivamente el juego, calculando eficientemente el perfil de equilibrio de Nash al explotar la <br>estructura de independencia</br>. NIDs extienden los MAIDs para incluir la incertidumbre de los agentes sobre el juego que se está jugando y sobre los modelos de los otros agentes. Cada modelo es un MAID y la red de MAIDs se colapsa, de abajo hacia arriba, en un solo MAID para calcular el equilibrio del juego teniendo en cuenta los diferentes modelos de cada agente. Los formalismos gráficos como MAIDs y NIDs abren una área de investigación prometedora que tiene como objetivo representar las interacciones multiagente de manera más transparente. Sin embargo, los MAIDs proporcionan un análisis del juego desde un punto de vista externo y la aplicabilidad de ambos está limitada a juegos estáticos de un solo jugador. Los asuntos son más complejos cuando consideramos interacciones que se extienden en el tiempo, donde las predicciones sobre las acciones futuras de otros deben hacerse utilizando modelos que cambian a medida que los agentes actúan y observan. Los I-DIDs abordan esta brecha al permitir la representación de modelos de otros agentes como los valores de un nodo de modelo especial. Tanto los modelos de otros agentes como las creencias originales de los agentes sobre estos modelos se actualizan con el tiempo utilizando implementaciones especializadas. En este documento, mejoramos la representación preliminar previa del I-DID mostrada en [15] utilizando la idea de que el I-ID estático es un tipo de NID. Por lo tanto, podemos utilizar construcciones de lenguaje específicas de NID, como multiplexores, para representar de manera más transparente el nodo del modelo y, posteriormente, el I-ID. Además, aclaramos la semántica del enlace de política de propósito especial introducido en la representación de I-DID por [15], y mostramos que podría ser reemplazado por enlaces de dependencia tradicionales. En la representación previa del I-DID, la actualización de la creencia de los agentes sobre los modelos de los demás a medida que los agentes actúan y reciben observaciones se denotaba utilizando un enlace especial llamado el enlace de actualización del modelo que conectaba los nodos del modelo a lo largo del tiempo. Explicamos la semántica de este enlace mostrando cómo puede ser implementado utilizando los enlaces de dependencia tradicionales entre los nodos de probabilidad que constituyen los nodos del modelo. El resultado final es una representación de I-DID que es significativamente más transparente, semánticamente clara y capaz de ser implementada utilizando los algoritmos estándar para resolver DIDs. Mostramos cómo los IDIDs pueden ser utilizados para modelar la incertidumbre de un agente sobre los modelos de otros, que a su vez pueden ser I-DIDs. La solución al I-DID es una política que prescribe lo que el agente debe hacer con el tiempo, dadas sus creencias sobre el estado físico y otros modelos. Análogos a los DIDs, los I-DIDs pueden ser utilizados para calcular la política de un agente en línea mientras el agente actúa y observa en un entorno poblado por otros agentes interactuantes. 2. ANTECEDENTES: Los IPOMDPS ANIDADOS FINITAMENTE generalizan los POMDPs a entornos multiagentes al incluir los modelos de otros agentes como parte del espacio de estados [9]. Dado que otros agentes también pueden razonar sobre otros, el espacio de estado interactivo está estratégicamente anidado; contiene creencias sobre los modelos de otros agentes y sus creencias sobre los demás. Para simplificar la presentación, consideramos un agente, i, que está interactuando con otro agente, j. Un I-POMDP finitamente anidado del agente i con un nivel de estrategia l se define como la tupla: I-POMDPi,l = ISi,l, A, Ti, Ωi, Oi, Ri donde: • ISi,l denota un conjunto de estados interactivos definido como, ISi,l = S × Mj,l−1, donde Mj,l−1 = {Θj,l−1 ∪ SMj}, para l ≥ 1, e ISi,0 = S, donde S es el conjunto de estados del entorno físico. Θj,l−1 es el conjunto de modelos intencionales computables del agente j: θj,l−1 = bj,l−1, ˆθj donde el marco, ˆθj = A, Ωj, Tj, Oj, Rj, OCj. Aquí, j es Bayesiano racional y OCj es el criterio de optimalidad de j. SMj es el conjunto de modelos subintencionales de j. Ejemplos simples de modelos subintencionales incluyen un modelo sin información [10] y un modelo de juego ficticio [6], ambos de los cuales son independientes de la historia. Damos una construcción recursiva de abajo hacia arriba del espacio de estados interactivo a continuación. Sí,0 = S, Θj,0 = { bj,0, ˆθj | bj,0 ∈ Δ(ISj,0)} Sí,1 = S × {Θj,0 ∪ SMj}, Θj,1 = { bj,1, ˆθj | bj,1 ∈ Δ(ISj,1)} . . . . . . Sí, l = S × {Θj,l−1 ∪ SMj}, Θj,l = { bj,l, ˆθj | bj,l ∈ Δ(ISj,l)} Formulaciones similares de espacios anidados han aparecido en [1, 3]. • A = Ai × Aj es el conjunto de acciones conjuntas de todos los agentes en el entorno; • Ti : S ×A×S → [0, 1], describe el efecto de las acciones conjuntas en los estados físicos del entorno; • Ωi es el conjunto de observaciones del agente i; • Oi : S × A × Ωi → [0, 1] da la probabilidad de las observaciones dadas el estado físico y la acción conjunta; • Ri : ISi × A → R describe las preferencias del agente sobre sus estados interactivos. Normalmente solo importarán los estados físicos. La política del agente es el mapeo, Ω∗ i → Δ(Ai), donde Ω∗ i es el conjunto de todos los historiales de observación del agente i. Dado que la creencia sobre los estados interactivos forma una estadística suficiente [9], la política también puede ser representada como un mapeo del conjunto de todas las creencias del agente i a una distribución sobre sus acciones, Δ(ISi) → Δ(Ai). 2.1 Actualización de Creencias Análogo a los POMDPs, un agente dentro del marco I-POMDP actualiza su creencia a medida que actúa y observa. Sin embargo, hay dos diferencias que complican la actualización de creencias en entornos multiagentes en comparación con los entornos de un solo agente. Primero, dado que el estado del entorno físico depende de las acciones de ambos agentes, la predicción de cómo cambia el estado físico debe hacerse basándose en la predicción de sus acciones. Segundo, los cambios en los modelos de js deben ser incluidos en la actualización de creencias. Específicamente, si j es intencional, entonces se debe incluir una actualización de las creencias de j debido a su acción y observación. En otras palabras, i tiene que actualizar su creencia basándose en su predicción de lo que j observaría y cómo j actualizaría su creencia. Si el modelo de js es subintencional, entonces las observaciones probables de js se añaden al historial de observaciones contenido en el modelo. Formalmente, tenemos: Pr(ist |at−1 i , bt−1 i,l ) = β ISt−1:mt−1 j =θt j bt−1 i,l (ist−1 ) × at−1 j Pr(at−1 j |θt−1 j,l−1)Oi(st , at−1 i , at−1 j , ot i) ×Ti(st−1 , at−1 i , at−1 j , st ) ot j Oj(st , at−1 i , at−1 j , ot j) ×τ(SEθt j (bt−1 j,l−1, at−1 j , ot j) − bt j,l−1) (1) donde β es la constante de normalización, τ es 1 si su argumento es 0, de lo contrario es 0, Pr(at−1 j |θt−1 j,l−1) es la probabilidad de que at−1 j sea Bayes racional para el agente descrito por el modelo θt−1 j,l−1, y SE(·) es una abreviatura para la actualización de creencias. Para una versión de la actualización de creencias cuando el modelo js es subintencional, consulte [9]. Si el agente j también se modela como un I-POMDP, entonces la actualización de creencias invoca la actualización de creencias de j (a través del término SEθt j ( bt−1 j,l−1 , at−1 j , ot j)), lo que a su vez podría invocar la actualización de creencias de is y así sucesivamente. Esta recursión en la anidación de creencias llega al nivel 0. En este nivel, la actualización de creencias del agente se reduce a una actualización de creencias de un POMDP. Para ilustraciones de la actualización de creencias, detalles adicionales sobre I-POMDPs y cómo se comparan con otros marcos multiagentes, consulte [9]. Iteración de Valor Cada estado de creencia en un I-POMDP finitamente anidado tiene un valor asociado que refleja la máxima ganancia que el agente puede esperar en este estado de creencia: Un ( bi,l, θi ) = max ai∈Ai is∈ISi,l ERi(is, ai)bi,l(is)+ γ oi∈Ωi Pr(oi|ai, bi,l)Un−1 ( SEθi (bi,l, ai, oi), θi ) (2) donde, ERi(is, ai) = aj Ri(is, ai, aj)Pr(aj|mj,l−1) (ya que is = (s, mj,l−1)). La ecuación 2 es una base para la iteración de valor en I-POMDPs. La acción óptima del agente, a∗ i, para el caso de horizonte finito con descuento, es un elemento del conjunto de acciones óptimas para el estado de creencia, OPT(θi), definido como: OPT( bi,l, θi ) = argmax ai∈Ai is∈ISi,l ERi(is, ai)bi,l(is) +γ oi∈Ωi Pr(oi|ai, bi,l)Un ( SEθi (bi,l, ai, oi), θi ) (3) 3. Diagramas de influencia interactivos. Una extensión ingenua de los diagramas de influencia (IDs) a entornos poblados por múltiples agentes es posible tratando a los otros agentes como autómatas, representados mediante nodos de probabilidad. Sin embargo, este enfoque asume que las acciones de los agentes están controladas utilizando una distribución de probabilidad que no cambia con el tiempo. Los diagramas de influencia interactivos (I-IDs) adoptan un enfoque más sofisticado al generalizar los IDs para hacerlos aplicables a entornos compartidos con otros agentes que pueden actuar, observar y actualizar sus creencias. 3.1 Sintaxis Además de los nodos habituales de probabilidad, decisión y utilidad, los IIDs incluyen un nuevo tipo de nodo llamado nodo de modelo. Mostramos un nivel general l I-ID en la Fig. 1(a), donde el nodo del modelo (Mj,l−1) está representado con un hexágono. Observamos que la distribución de probabilidad sobre el nodo de probabilidad, S, y el nodo del modelo juntos representan la creencia del agente sobre sus estados interactivos. Además del modelo 1, el modelo de nivel 0 es un POMDP: las acciones de otros agentes se tratan como eventos exógenos y se incorporan en las funciones T, O y R. El Sexto Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) Figura 1: (a) Un nivel genérico l I-ID para el agente i situado con otro agente j. El hexágono es el nodo modelo (Mj,l−1) cuya estructura mostramos en (b). Los miembros del nodo modelo son ellos mismos I-ID (m1 j,l−1, m2 j,l−1; los diagramas no se muestran aquí por simplicidad) cuyos nodos de decisión se asignan a los nodos de probabilidad correspondientes (A1 j , A2 j). Dependiendo del valor del nodo, Mod[Mj], la distribución de cada uno de los nodos de probabilidad se asigna al nodo Aj. (c) El I-ID transformado con el nodo del modelo reemplazado por los nodos de probabilidad y las relaciones entre ellos. Los I-IDs difieren de los IDs al tener un enlace discontinuo (llamado enlace de política en [15]) entre el nodo del modelo y un nodo de probabilidad, Aj, que representa la distribución sobre las acciones de los otros agentes dada su modelo. En ausencia de otros agentes, el nodo del modelo y el nodo de probabilidad, Aj, desaparecen y los I-ID se convierten en ID tradicionales. El nodo del modelo contiene los modelos computacionales alternativos atribuidos por i al otro agente del conjunto, Θj,l−1 ∪ SMj, donde Θj,l−1 y SMj fueron definidos previamente en la Sección 2. Por lo tanto, un modelo en el nodo del modelo puede ser en sí mismo un I-ID o ID, y la recursión termina cuando un modelo es un ID o subintencional. Dado que el nodo del modelo contiene los modelos alternativos del otro agente como sus valores, su representación no es trivial. En particular, algunos de los modelos dentro del nodo son I-IDs que, al resolverse, generan la política óptima de los agentes en sus nodos de decisión. Cada nodo de decisión se asigna al nodo de probabilidad correspondiente, digamos A1 j, de la siguiente manera: si OPT es el conjunto de acciones óptimas obtenidas al resolver el I-ID (o ID), entonces Pr(aj ∈ A1 j) = 1 |OPT| si aj ∈ OPT, 0 en caso contrario. Tomando prestados los conocimientos de trabajos anteriores [8], observamos que el nodo del modelo y el enlace de política discontinua que lo conecta con el nodo de probabilidad, Aj, podrían representarse como se muestra en la Fig. 1(b). El nodo de decisión de cada nivel l − 1 I-ID se transforma en un nodo de probabilidad, como mencionamos anteriormente, de modo que las acciones con el mayor valor en el nodo de decisión se les asignan probabilidades uniformes en el nodo de probabilidad, mientras que al resto se les asigna probabilidad cero. Los diferentes nodos de probabilidad (A1 j , A2 j ), uno para cada modelo, y adicionalmente, el nodo de probabilidad etiquetado Mod[Mj] forman los padres del nodo de probabilidad, Aj. Por lo tanto, hay tantos nodos de acción (A1 j , A2 j ) en Mj,l−1 como el número de modelos en el soporte de las creencias del agente. La tabla de probabilidad condicional del nodo de probabilidad, Aj, es un multiplexor que asume la distribución de cada uno de los nodos de acción (A1 j , A2 j ) dependiendo del valor de Mod[Mj]. Los valores de Mod[Mj] denotan los diferentes modelos de j. En otras palabras, cuando Mod[Mj] tiene el valor m1 j,l−1, el nodo de probabilidad Aj asume la distribución del nodo A1 j, y Aj asume la distribución de A2 j cuando Mod[Mj] tiene el valor m2 j,l−1. La distribución sobre el nodo, Mod[Mj], es la creencia del agente sobre los modelos de j dado un estado físico. Para más agentes, tendremos tantos nodos de modelo como agentes haya. Observa que la Fig. 1(b) aclara la semántica del enlace de política y muestra cómo puede ser representado utilizando los enlaces de dependencia tradicionales. En la Fig. 1(c), mostramos el I-ID transformado cuando el nodo del modelo es reemplazado por los nodos de probabilidad y las relaciones entre ellos. A diferencia de la representación en [15], no hay enlaces de políticas especiales, sino que el I-ID está compuesto únicamente por aquellos tipos de nodos que se encuentran en IDs tradicionales y relaciones de dependencia entre los nodos. Esto permite que los I-IDs sean representados e implementados utilizando herramientas de aplicación convencionales que apuntan a los IDs. Ten en cuenta que podemos ver el nivel l I-ID como un NID. Específicamente, cada uno de los modelos del nivel l − 1 dentro del nodo del modelo son bloques en el NID (ver Fig. 2). Si el nivel l = 1, cada bloque es un ID tradicional, de lo contrario, si l > 1, cada bloque dentro del NID puede ser un NID en sí mismo. Ten en cuenta que dentro de los I-IDs (o IDs) en cada nivel, solo hay un nodo de decisión único. Por lo tanto, nuestro NID no contiene ningún MAID. Figura 2: Un nivel l I-ID representado como un NID. Las probabilidades asignadas a los bloques del NID son creencias sobre modelos js condicionados a un estado físico. 3.2 Solución La solución de un I-ID procede de manera ascendente y se implementa de forma recursiva. Comenzamos resolviendo los modelos de nivel 0, que, si son intencionales, son identificadores tradicionales. Sus soluciones proporcionan distribuciones de probabilidad sobre las acciones de los otros agentes, las cuales se ingresan en los nodos de probabilidad correspondientes encontrados en el nodo del modelo del nivel 1 I-ID. El mapeo de los nodos de decisión de los modelos de nivel 0 a los nodos de probabilidad se realiza de manera que las acciones con el mayor valor en el nodo de decisión se les asignan probabilidades uniformes en el nodo de probabilidad, mientras que el resto se les asigna probabilidad cero. Dadas las distribuciones sobre las acciones dentro de los diferentes nodos de probabilidad (uno para cada modelo del otro agente), el I-ID de nivel 1 se transforma como se muestra en la Fig. 1(c). Durante la transformación, la tabla de probabilidad condicional (CPT) del nodo, Aj, se llena de tal manera que el nodo asume la distribución de cada uno de los nodos de probabilidad dependiendo del valor del nodo, Mod[Mj]. Como mencionamos anteriormente, los valores del nodo Mod[Mj] denotan los diferentes modelos del otro agente, y su distribución es la creencia del agente sobre los modelos de j condicionados al estado físico. El nivel 1 I-ID transformado es un ID tradicional que puede ser resuelto como us816 The Sixth Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) (a) (b) Figura 3: (a) Un I-DID genérico de dos niveles de tiempo para el agente i en un entorno con otro agente j. Observa el enlace de actualización del modelo punteado que denota la actualización de los modelos de j y la distribución de los modelos a lo largo del tiempo. (b) La semántica del enlace de actualización del modelo utilizando el método estándar de maximización de utilidad esperada [18]. Este procedimiento se lleva a cabo hasta el nivel l I-ID cuya solución proporciona el conjunto no vacío de acciones óptimas que el agente debe realizar dada su creencia. Ten en cuenta que, al igual que los IDs, los I-IDs son adecuados para la toma de decisiones en línea cuando se conoce la creencia actual de los agentes. 4. Diagramas de influencia dinámica interactivos (I-DIDs) Los diagramas de influencia dinámica interactivos (I-DIDs) extienden los I-IDs (y NIDs) para permitir la toma de decisiones secuencial a lo largo de varios pasos de tiempo. Así como los DIDs son representaciones gráficas estructuradas de POMDPs, los I-DIDs son los análogos gráficos en línea para I-POMDPs finitamente anidados. Los I-DIDs pueden ser utilizados para optimizar sobre una mirada anticipada finita dadas las creencias iniciales mientras se interactúa con otros agentes, posiblemente similares. 4.1 Sintaxis Representamos un I-DID de dos cortes temporales en la Fig. 3(a). Además de los nodos del modelo y el enlace de política discontinuo, lo que diferencia a un I-DID de un DID es el enlace de actualización del modelo mostrado como una flecha punteada en la Fig. 3(a). Explicamos la semántica del nodo del modelo y el enlace de la política en la sección anterior; a continuación, describimos las actualizaciones del modelo. La actualización del nodo del modelo con el tiempo implica dos pasos: primero, dados los modelos en el tiempo t, identificamos el conjunto actualizado de modelos que residen en el nodo del modelo en el tiempo t + 1. Recuerda de la Sección 2 que el modelo intencional de un agente incluye sus creencias. Debido a que los agentes actúan y reciben observaciones, sus modelos se actualizan para reflejar sus creencias cambiadas. Dado que el conjunto de acciones óptimas para un modelo podría incluir todas las acciones, y el agente podría recibir cualquiera de las |Ωj| posibles observaciones, el conjunto actualizado en el paso de tiempo t + 1 tendrá como máximo |Mt j,l−1||Aj||Ωj| modelos. Aquí, |Mt j,l−1| es el número de modelos en el paso de tiempo t, |Aj| y |Ωj| son los espacios más grandes de acciones y observaciones respectivamente, entre todos los modelos. Segundo, calculamos la nueva distribución sobre los modelos actualizados dados la distribución original y la probabilidad de que el agente realice la acción y reciba la observación que llevó al modelo actualizado. Estos pasos son parte de la actualización de creencias del agente formalizada utilizando la Ecuación 1. En la Fig. 3(b), mostramos cómo se implementa el enlace de actualización del modelo punteado en el I-DID. Si cada uno de los dos modelos de nivel l − 1 atribuidos a j en el paso de tiempo t resulta en una acción, y j podría hacer una de dos posibles observaciones, entonces el nodo del modelo en el paso de tiempo t + 1 contiene cuatro modelos actualizados (mt+1,1 j,l−1 ,mt+1,2 j,l−1 , mt+1,3 j,l−1 , y mt+1,4 j,l−1 ). Estos modelos difieren en sus creencias iniciales, cada una de las cuales es el resultado de j actualizar sus creencias debido a su acción y una posible observación. Los nodos de decisión en cada uno de los I-DIDs o DIDs que representan los modelos de nivel inferior se asignan a la Figura 4 correspondiente: I-DID transformado con los nodos del modelo y el enlace de actualización del modelo reemplazados por los nodos de probabilidad y las relaciones (en negrita) de los nodos de probabilidad, como se mencionó anteriormente. A continuación, describimos cómo se calcula la distribución sobre el conjunto actualizado de modelos (la distribución sobre el nodo de probabilidad Mod[Mt+1 j ] en Mt+1 j,l−1). La probabilidad de que el modelo actualizado js sea, digamos mt+1,1 j,l−1, depende de la probabilidad de que j realice la acción y reciba la observación que condujo a este modelo, y de la distribución previa sobre los modelos en el paso de tiempo t. Dado que el nodo de probabilidad At j asume la distribución de cada uno de los nodos de acción basados en el valor de Mod[Mt j], la probabilidad de la acción está dada por este nodo de probabilidad. Para obtener la probabilidad de una observación posible js, introducimos el nodo de probabilidad Oj, que dependiendo del valor de Mod[Mt j], asume la distribución del nodo de observación en el modelo de nivel inferior denominado Mod[Mt j]. Dado que la probabilidad de las observaciones js depende del estado físico y de las acciones conjuntas de ambos agentes, el nodo Oj está vinculado con St+1, At j y At i. De manera análoga a At j, la tabla de probabilidad condicional de Oj también es un multiplexor modulado por Mod[Mt j]. Finalmente, la distribución de los modelos previos en el tiempo t se obtiene a partir del nodo de probabilidad, Mod[Mt j ] en Mt j,l−1. Por consiguiente, los nodos de probabilidad, Mod[Mt j], At j y Oj, forman los padres de Mod[Mt+1 j] en Mt+1 j,l−1. Ten en cuenta que el enlace de actualización del modelo puede ser reemplazado por los enlaces de dependencia entre los nodos de probabilidad que constituyen los nodos del modelo en las dos etapas temporales. En la Fig. 4 mostramos los dos cortes temporales I-DID con los nodos del modelo reemplazados por los nodos de probabilidad y las relaciones entre ellos. Los nodos de probabilidad y los enlaces de dependencia que no están en negrita son estándar, generalmente encontrados en DIDs. La expansión del I-DID a lo largo de más pasos de tiempo requiere la repetición de los dos pasos de actualizar el conjunto de modelos que forman el 2. Nota que Oj representa la observación j en el tiempo t + 1. El Sexto Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 817 valores del nodo del modelo y añadiendo las relaciones entre los nodos de probabilidad, tantas veces como enlaces de actualización del modelo haya. Observamos que el conjunto posible de modelos del otro agente j crece exponencialmente con el número de pasos de tiempo. Por ejemplo, después de T pasos, puede haber como máximo |Mt=1 j,l−1|(|Aj||Ωj|)T −1 modelos candidatos residiendo en el nodo del modelo. 4.2 Solución Análoga a los I-ID, la solución a un I-DID de nivel l para el agente i expandido a lo largo de T pasos de tiempo puede llevarse a cabo de forma recursiva. Con el propósito de ilustración, dejemos l=1 y T=2. El método de solución utiliza la técnica estándar de anticipación, proyectando las secuencias de acciones y observaciones de los agentes hacia adelante desde el estado de creencia actual [17], y encontrando las posibles creencias que podría tener en el siguiente paso temporal. Dado que el agente i también tiene una creencia sobre los modelos de j, la búsqueda anticipada incluye descubrir los posibles modelos que j podría tener en el futuro. Por consiguiente, cada uno de los modelos subintencionales o de nivel 0 de js (representados utilizando un DID estándar) en el primer paso de tiempo debe resolverse para obtener su conjunto óptimo de acciones. Estas acciones se combinan con el conjunto de observaciones posibles que j podría hacer en ese modelo, lo que resulta en un conjunto actualizado de modelos candidatos (que incluyen las creencias actualizadas) que podrían describir el comportamiento de j. Las creencias sobre este conjunto actualizado de modelos candidatos se calculan utilizando los métodos estándar de inferencia que utilizan las relaciones de dependencia entre los nodos del modelo, como se muestra en la Figura 3(b). Observamos la naturaleza recursiva de esta solución: al resolver el agente de nivel 1 I-DID, los DIDs de nivel 0 deben ser resueltos. Si el anidamiento de los modelos es más profundo, todos los modelos en todos los niveles, comenzando desde el nivel 0, se resuelven de manera ascendente. Breve descripción del algoritmo recursivo para resolver el agente es el Algoritmo para resolver I-DID. Entrada: nivel l ≥ 1 I-ID o nivel 0 ID, T Fase de Expansión 1. Para t desde 1 hasta T − 1, hacer 2. Si l ≥ 1, entonces llenar Mt+1 j,l−1 3. Para cada mt j en el rango (Mt j,l−1) hacer 4. Llame recursivamente al algoritmo con el l - 1 I-ID (o ID) que representa mt j y el horizonte, T - t + 1 5. Mapea el nodo de decisión del I-ID (o ID) resuelto, OPT(mt j), a un nodo de probabilidad Aj 6. Para cada aj en OPT(mt j) hacer 7. Para cada oj en Oj (parte de mt j) hacer 8. Actualizar la creencia js, bt+1 j ← SE(bt j, aj, oj) 9. mt+1 j ← Nuevo I-ID (o ID) con bt+1 j como la creencia inicial 10. Rango(Mt+1 j,l−1) ∪ ← {mt+1 j } 11. Añadir el nodo del modelo, Mt+1 j,l−1, y los enlaces de dependencia entre Mt j,l−1 y Mt+1 j,l−1 (mostrados en la Fig. 3(b)) 12. Agregue los nodos de probabilidad, decisión y utilidad para la franja de tiempo t + 1 y los enlaces de dependencia entre ellos 13. Establezca las CPTs para cada nodo de probabilidad y nodo de utilidad de la Fase de Mirada Adelante 14. Aplica el método estándar de anticipación y respaldo para resolver la Figura 5 expandida de I-DID: Algoritmo para resolver un nivel l ≥ 0 I-DID. Nivel l I-DID expandido durante T pasos de tiempo con otro agente j en la Figura 5. Adoptamos un enfoque de dos fases: Dado un I-ID de nivel l (descrito previamente en la Sección 3) con todos los modelos de niveles inferiores representados también como I-IDs o IDs (si es nivel 0), el primer paso es expandir el I-ID de nivel l a lo largo de T pasos de tiempo añadiendo los enlaces de dependencia y las tablas de probabilidad condicional para cada nodo. Nos enfocamos especialmente en establecer y poblar los nodos del modelo (líneas 3-11). Ten en cuenta que Range(·) devuelve los valores (modelos de nivel inferior) de la variable aleatoria dada como entrada (nodo del modelo). En la segunda fase, utilizamos una técnica estándar de anticipación proyectando las secuencias de acción y observación durante T pasos de tiempo en el futuro, y respaldando los valores de utilidad de las creencias alcanzables. Similar a los I-IDs, los I-DIDs se reducen a DIDs en ausencia de otros agentes. Como mencionamos anteriormente, los modelos de nivel 0 son los DIDs tradicionales. Sus soluciones proporcionan distribuciones de probabilidad sobre las acciones del agente modelado en ese nivel a los I-DIDs en el nivel 1. Dadas las distribuciones de probabilidad sobre las acciones de otros agentes, los IDIDs de nivel 1 pueden resolverse como DIDs y proporcionar distribuciones de probabilidad a modelos de niveles superiores. Suponga que el número de modelos considerados en cada nivel está limitado por un número, M. Resolver un I-DID de nivel l es equivalente a resolver O(Ml) DIDs. Para ilustrar la utilidad de los I-DIDs, los aplicamos a tres dominios de problemas. Describimos, en particular, la formulación del I-DID y las prescripciones óptimas obtenidas al resolverlo. 5.1 Seguimiento-Liderazgo en el Problema del Tigre Multiagente Comenzamos nuestras ilustraciones del uso de I-IDs e I-DIDs con una versión ligeramente modificada del problema del tigre multiagente discutido en [9]. El problema tiene dos agentes, cada uno de los cuales puede abrir la puerta derecha (OR), la puerta izquierda (OL) o escuchar (L). Además de escuchar gruñidos (desde la izquierda (GL) o desde la derecha (GR)) cuando escuchan, los agentes también escuchan chirridos (desde la izquierda (CL), desde la derecha (CR) o sin chirridos (S)), que indican ruidosamente que los otros agentes están abriendo una de las puertas. Cuando se abre cualquier puerta, el tigre persiste en su ubicación original con una probabilidad del 95%. El agente i escucha gruñidos con una fiabilidad del 65% y crujidos con una fiabilidad del 95%. El agente J, por otro lado, escucha gruñidos con una fiabilidad del 95%. Por lo tanto, la configuración es tal que el agente i escucha la apertura de puertas del agente j de manera más confiable que los rugidos de los tigres. Esto sugiere que podría usar acciones de JavaScript como indicación de la ubicación del tigre, como discutimos a continuación. Las preferencias de cada agente son como en el juego de un solo agente discutido en [13]. Las funciones de transición, observación y recompensa se muestran en [16]. Un buen indicador de la utilidad de los métodos normativos para la toma de decisiones como los I-DIDs es la aparición de comportamientos sociales realistas en sus prescripciones. En entornos del persistente problema del tigre multiagente que reflejan situaciones del mundo real, demostramos la seguidismo entre los agentes y, como se muestra en [15], el engaño entre agentes que creen que están en una relación de tipo seguidor-líder. En particular, analizamos las condiciones situacionales y epistemológicas suficientes para su surgimiento. El comportamiento de seguidores, por ejemplo, resulta del agente conociendo sus propias debilidades, evaluando las fortalezas, preferencias y posibles comportamientos del otro, y dándose cuenta de que es mejor para él seguir las acciones de los demás para maximizar sus ganancias. Consideremos una configuración particular del problema del tigre en la que el agente i cree que las preferencias de j están alineadas con las suyas: ambos solo quieren obtener el oro, y la audición de j es más confiable en comparación consigo mismo. Como ejemplo, supongamos que j, al escuchar, puede discernir la ubicación del tigre el 95% de las veces en comparación con su precisión del 65%. Además, el agente i no tiene ninguna información inicial sobre la ubicación de los tigres. En otras palabras, la creencia anidada de un solo nivel, bi,1, asigna 0.5 a cada una de las dos ubicaciones del tigre. Además, considera dos modelos de j, que difieren en los niveles iniciales de creencias planas de j. Esto se representa en el nivel 1 I-ID mostrado en la Fig. 6(a). Según un modelo, j asigna una probabilidad de 0.9 a que el tigre esté detrás de la puerta izquierda, mientras que el otro 818 The Sixth Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) Figura 6: (a) Nivel 1 I-ID del agente i, (b) dos IDs de nivel 0 del agente j cuyos nodos de decisión están mapeados a los nodos de probabilidad, A1 j , A2 j , en (a). El modelo asigna 0.1 a esa ubicación (ver Fig. 6(b)). El agente i está indeciso sobre estos dos modelos de j. Si variamos la capacidad auditiva, y resolvemos el nivel 1 correspondiente I-ID expandido en tres pasos de tiempo, obtenemos las políticas de comportamiento normativas mostradas en la Figura 7 que exhiben comportamiento de seguidores. Si la probabilidad de escuchar correctamente los gruñidos es de 0.65, entonces, como se muestra en la política en la Fig. 7(a), i comienza a seguir condicionalmente las acciones de j: i abre la misma puerta que j abrió anteriormente si su evaluación propia de la ubicación de los tigres confirma la elección de j. Si i pierde la capacidad de interpretar correctamente los gruñidos por completo, sigue ciegamente a j y abre la misma puerta que j abrió anteriormente (Fig. 7(b)). Figura 7: Emergencia de (a) seguidores condicionales, y (b) seguidores ciegos en el problema del tigre. Los comportamientos de interés están en negrita. * es un comodín y representa cualquiera de las observaciones. Observamos que un solo nivel de anidación de creencias, creencias sobre los modelos de los demás, fue suficiente para que surgiera el seguidismo en el problema del tigre. Sin embargo, los requisitos epistemológicos para el surgimiento del liderazgo son más complejos. Para que un agente, digamos j, surja como líder, primero debe surgir el seguidismo en el otro agente i. Como mencionamos anteriormente, si i está seguro de que sus preferencias son idénticas a las de j, y cree que j tiene un mejor sentido del oído, i seguirá las acciones de j con el tiempo. El agente j emerge como líder si cree que lo seguiré, lo que implica que la creencia de j debe estar anidada dos niveles de profundidad para permitirle reconocer su papel de liderazgo. Darse cuenta de que lo seguiré presenta a J una oportunidad para influir en sus acciones en beneficio del bien colectivo o de su interés propio solamente. Por ejemplo, en el problema del tigre, consideremos una situación en la que si tanto i como j abren la puerta correcta, entonces cada uno recibe un pago de 20 que es el doble del original. Si solo j selecciona la puerta correcta, obtiene la recompensa de 10. Por otro lado, si ambos agentes eligen la puerta incorrecta, sus penalizaciones se reducen a la mitad. En este entorno, es tanto en el mejor interés de j como en el beneficio colectivo que utilice su experiencia para seleccionar la puerta correcta, y así ser un buen líder. Sin embargo, considera un problema ligeramente diferente en el que j se beneficia de la pérdida de i y es penalizado si i se beneficia. Específicamente, dejemos que la ganancia se reste de js, indicando que j es antagonista hacia i: si j elige la puerta correcta y i la incorrecta, entonces la pérdida de 100 se convierte en la ganancia de js. El agente J cree que yo incorrectamente pienso que las preferencias de J son aquellas que promueven el bien colectivo y que comienza creyendo con un 99% de confianza dónde está el tigre. Dado que creo que mis preferencias son similares a las de j, y que j comienza creyendo casi con certeza que una de las dos ubicaciones es la correcta (dos modelos de nivel 0 de j), comenzaré siguiendo las acciones de j. Mostramos la política normativa para resolver su I-DID anidado individualmente durante tres pasos de tiempo en la Fig. 8(a). La política demuestra que seguiré ciegamente las acciones de js. Dado que el tigre persiste en su ubicación original con una probabilidad del 0.95, seleccionaré nuevamente la misma puerta. Si j comienza el juego con una probabilidad del 99% de que el tigre esté a la derecha, resolver su I-DID anidado dos niveles profundamente, resulta en la política mostrada en la Fig. 8(b). Aunque j está casi seguro de que OL es la acción correcta, comenzará seleccionando OR, seguido de OL. La intención del agente js es engañar a i, a quien cree que seguirá las acciones de js, para así obtener $110 en el segundo paso de tiempo, lo cual es más de lo que j ganaría si fuera honesto. Figura 8: Emergencia del engaño entre agentes en el problema del tigre. Los comportamientos de interés están en negrita. * denota como antes. (a) El agente es una política que demuestra que seguirá ciegamente las acciones de js. (b) Aunque j está casi seguro de que el tigre está a la derecha, comenzará seleccionando OR, seguido de OL, para engañar a i. 5.2 Altruismo y reciprocidad en el problema del bien público El problema del bien público (PG) [7], consiste en un grupo de M agentes, cada uno de los cuales debe contribuir con algún recurso a una olla pública o guardarlo para sí mismos. Dado que los recursos aportados al fondo público se comparten entre todos los agentes, son menos valiosos para el agente cuando están en el fondo público. Sin embargo, si todos los agentes eligen contribuir con sus recursos, entonces la recompensa para cada agente es mayor que si nadie contribuye. Dado que un agente recibe su parte del bote público independientemente de si ha contribuido o no, la acción dominante es que cada agente no contribuya y en su lugar se aproveche de las contribuciones de los demás. Sin embargo, los comportamientos de los jugadores humanos en simulaciones empíricas del problema de PG difieren de las predicciones normativas. Los experimentos revelan que muchos jugadores inicialmente contribuyen con una gran cantidad al bote público, y continúan contribuyendo cuando se juega el problema del PG repetidamente, aunque en cantidades decrecientes [4]. Muchos de estos experimentos [5] informan que un pequeño grupo central de jugadores contribuye persistentemente al bote público incluso cuando todos los demás están desertando. Estos experimentos también revelan que los jugadores que contribuyen persistentemente tienen preferencias altruistas o recíprocas que coinciden con la cooperación esperada de los demás. Para simplificar, asumimos que el juego se juega entre M = 2 agentes, i y j. Que cada agente esté inicialmente dotado con una cantidad de recursos XT. Mientras que la formulación clásica del juego PG permite que cada agente contribuya con cualquier cantidad de recursos (≤ XT) al bote público, simplificamos el espacio de acciones al permitir dos acciones posibles. Cada agente puede elegir contribuir (C) una cantidad fija de recursos, o no contribuir. La última acción es deThe Sixth Intl. La Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 819 fue señalada como defectuosa (D). Suponemos que las acciones no son observables por otros. El valor de los recursos en la bolsa pública se descuenta por ci para cada agente i, donde ci es el retorno privado marginal. Suponemos que ci < 1 para que el agente no se beneficie lo suficiente como para contribuir al bote público en beneficio propio. Simultáneamente, ciM > 1, lo que hace que la contribución colectiva sea óptima de Pareto. i/j C D C 2ciXT , 2cjXT ciXT − cp, XT + cjXT − P D XT + ciXT − P, cjXT − cp XT , XT Tabla 1: El juego PG de un solo disparo con castigo. Para fomentar las contribuciones, los agentes contribuyentes castigan a los que no colaboran pero incurren en un pequeño costo por administrar el castigo. Sea P la sanción impuesta al agente que se desvía y cp el costo no nulo de castigar al agente contribuyente. Para simplificar, asumimos que el costo de castigar es el mismo para ambos agentes. El juego PG de un solo disparo con castigo se muestra en la Tabla 1. Si ci = cj, cp > 0, y si P > XT − ciXT, entonces la deserción ya no es una acción dominante. Si P < XT − ciXT, entonces la deserción es la acción dominante para ambos. Si P = XT − ciXT, entonces el juego no es resoluble por dominancia. Figura 9: (a) Nivel 1 I-ID del agente i, (b) IDs de nivel 0 del agente j con nodos de decisión mapeados a los nodos de probabilidad, A1 j y A2 j, en (a). Formulamos una versión secuencial del problema PG con castigo desde la perspectiva del agente i. Aunque en el juego de PG repetido, la cantidad en la olla pública se revela a todos los agentes después de cada ronda de acciones, asumimos en nuestra formulación que está oculta para los agentes. Cada agente puede contribuir con una cantidad fija, xc, o desertar. Un agente al realizar una acción recibe una observación de abundante (PY) o escasa (MR) simbolizando el estado de la olla pública. Ten en cuenta que las observaciones también son indirectamente indicativas de las acciones del agente js porque el estado del bote público es influenciado por ellas. La cantidad de recursos en el agente es una olla privada, es perfectamente observable para i. Los pagos son análogos a la Tabla 1. Tomando prestado de las investigaciones empíricas del problema PG [5], construimos IDs de nivel 0 para j que modelan tipos altruistas y no altruistas (Fig. 9(b)). Específicamente, nuestro agente altruista tiene un alto retorno privado marginal (cj está cerca de 1) y no castiga a los demás que incumplen. Que xc = 1 y el agente de nivel 0 sea castigado la mitad de las veces que se desvía. Con una acción restante, ambos tipos de agentes eligen contribuir para evitar ser castigados. Con dos acciones por delante, el tipo altruista elige contribuir, mientras que el otro defecta. Esto se debe a que cj para el tipo altruista es cercano a 1, por lo tanto, el castigo esperado, 0.5P > (1 − cj), que el tipo altruista evita. Debido a que cj para el tipo no altruista es menor, prefiere no contribuir. Con tres pasos por delante, el agente altruista contribuye para evitar el castigo (0.5P > 2(1 − cj)), mientras que el tipo no altruista se desentiende. Para más de tres pasos, mientras el agente altruista continúa contribuyendo al bote público dependiendo de qué tan cercano esté su retorno privado marginal a 1, el tipo no altruista prescribe la deserción. Analizamos las decisiones de un agente altruista i modelado usando un nivel 1 I-DID expandido en 3 pasos de tiempo. i atribuye los dos modelos de nivel 0, mencionados anteriormente, a j (ver Fig. 9). Si creo con una probabilidad de 1 que j es altruista, elijo contribuir en cada uno de los tres pasos. Este comportamiento persiste cuando i no está al tanto de si j es altruista (Fig. 10(a)), y cuando i asigna una alta probabilidad a que j sea del tipo no altruista. Sin embargo, cuando i cree con una probabilidad de 1 que j no es altruista y por lo tanto seguramente va a traicionar, i elige traicionar para evitar ser castigado y porque su beneficio privado marginal es menor que 1. Estos resultados demuestran que el comportamiento de nuestro tipo altruista se asemeja al encontrado experimentalmente. El agente de nivel 1 no altruista elige desertar independientemente de lo altruista que crea que sea el otro agente. Analizamos el comportamiento de un tipo de agente recíproco que se ajusta a la cooperación o defección esperada. El retorno privado marginal del tipo recíproco es similar al del tipo no altruista, sin embargo, obtiene una mayor recompensa cuando su acción es similar a la del otro. Consideramos el caso en el que el agente recíproco i no está seguro de si j es altruista y cree que es probable que el bote público esté medio lleno. Para esta creencia previa, yo elijo defectar. Al recibir una observación de abundancia, decide contribuir, mientras que una observación de escasez lo hace defectar (Fig. 10(b)). Esto se debe a que una observación de abundancia indica que es probable que la olla esté más llena que a la mitad, lo cual resulta de la acción de js para contribuir. Por lo tanto, entre los dos modelos atribuidos a j, es probable que su tipo sea altruista, lo que hace probable que j contribuya nuevamente en el próximo paso de tiempo. El agente i, por lo tanto, elige contribuir para corresponder la acción de js. Un razonamiento análogo lleva a uno a defectar cuando observa una olla escasa. Con una acción por hacer, creo que si j contribuye, también elegiré contribuir para evitar castigo independientemente de sus observaciones. Figura 10: (a) Un agente altruista de nivel 1 siempre contribuye. (b) Un agente recíproco i comienza defraudando y luego elige contribuir o defraudar basándose en su observación de abundancia (indicando que j es probablemente altruista) o escasez (j no es altruista). 5.3 Estrategias en el póker de dos jugadores El póker es un popular juego de cartas de suma cero que ha recibido mucha atención en la comunidad de investigación de IA como un banco de pruebas [2]. El póker se juega entre M ≥ 2 jugadores, en el que cada jugador recibe una mano de cartas de una baraja. Si bien existen varias variantes de póker con diferentes niveles de complejidad, consideramos una versión simple en la que cada jugador tiene tres turnos durante los cuales el jugador puede intercambiar una carta (E), mantener la mano existente (K), retirarse (F) y abandonar el juego, o apostar (C), lo que requiere que todos los jugadores muestren sus manos. Para mantener las cosas simples, dejemos que M = 2, y que cada jugador reciba una mano compuesta por una sola carta sacada del mismo palo. Por lo tanto, durante un enfrentamiento, el jugador que tenga la carta numéricamente más alta (el 2 es el más bajo, el as es el más alto) gana el bote. Durante un intercambio de cartas, la carta descartada se coloca ya sea en la pila L, indicando al otro agente que era una carta de número bajo menor que 8, o en la 820 The Sixth Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) H, indicando que la carta tenía un rango mayor o igual a 8. Ten en cuenta que, por ejemplo, si se descarta una carta de número bajo, la probabilidad de recibir una carta baja a cambio se reduce. Mostramos el nivel 1 I-ID para el Poker simplificado de dos jugadores en la Fig. 11. Consideramos dos modelos (tipos de personalidad) del agente j. El tipo conservador cree que es probable que su oponente tenga una carta de alto número en su mano. Por otro lado, el agente agresivo j cree con alta probabilidad que su oponente tiene una carta de número más bajo. Por lo tanto, los dos tipos difieren en sus creencias sobre la mano de sus oponentes. En ambos estos modelos de nivel 0, se asume que el oponente realiza sus acciones siguiendo una distribución fija y uniforme. Con tres acciones restantes, independientemente de su mano (a menos que sea un as), el agente agresivo elige intercambiar su carta, con la intención de mejorar su mano actual. Esto se debe a que cree que el otro tiene una carta baja, lo que mejora sus posibilidades de obtener una carta alta durante el intercambio. El agente conservador elige quedarse con su carta, sin importar su mano, porque considera que sus posibilidades de obtener una carta alta son escasas, ya que cree que su oponente tiene una. Figura 11: (a) Nivel 1 I-ID del agente i. La observación revela información sobre la mano de js del paso de tiempo anterior, (b) IDs de nivel 0 del agente j cuyos nodos de decisión están mapeados a los nodos de probabilidad, A1 j , A2 j , en (a). La política de un agente de nivel 1 i que cree que cada carta, excepto la suya, tiene la misma probabilidad de estar en su mano (tipo de personalidad neutral) y j podría ser de tipo agresivo o conservador, se muestra en la Figura 12. Su mano contiene la carta numerada 8. El agente comienza por mantener su carta. Al ver que j no intercambió una carta (N), i cree con probabilidad 1 que j es conservador y, por lo tanto, mantendrá sus cartas. i responde manteniendo su carta o intercambiándola porque j tiene igual probabilidad de tener una carta más baja o más alta. Si observo que j descartó su carta en la pila L o H, creo que j es agresivo. Al observar a L, i se da cuenta de que j tenía una carta baja, y es probable que tenga una carta alta después de su intercambio. Dado que la probabilidad de recibir una carta baja es alta en este momento, i decide quedarse con su carta. Al observar la carta H, creyendo que la probabilidad de recibir una carta de número alto es alta, i decide intercambiar su carta. En el paso final, i decide llamar independientemente de su historial de observaciones porque su creencia de que j tiene una carta más alta no es lo suficientemente alta como para concluir que es mejor retirarse y renunciar al pago. Esto se debe en parte al hecho de que una observación de, digamos, L, restablece las creencias del agente en el paso de tiempo anterior sobre las cartas de números bajos solamente. 6. DISCUSIÓN Mostramos cómo los DIDs pueden ser extendidos a los I-DIDs que permiten la toma de decisiones secuenciales en línea en entornos multiagentes inciertos. Nuestra representación gráfica de I-DIDs mejora la Figura 12 anterior: Un agente de nivel 1 es una política de tres pasos en el problema de Poker. i comienza creyendo que j tiene la misma probabilidad de ser agresivo o conservador y podría tener cualquier carta en su mano con igual probabilidad. Trabaja significativamente al ser más transparente, semánticamente claro y capaz de ser resuelto utilizando algoritmos estándar que apuntan a los DIDs. Los I-DIDs extienden los NIDs para permitir la toma de decisiones secuenciales a lo largo de múltiples pasos de tiempo en presencia de otros agentes que interactúan. Los I-DIDs pueden ser vistos como representaciones gráficas concisas para IPOMDPs que proporcionan una forma de explotar la estructura del problema y llevar a cabo la toma de decisiones en línea a medida que el agente actúa y observa dadas sus creencias previas. Actualmente estamos investigando formas de resolver I-DIDs aproximadamente con límites demostrables en la calidad de la solución. Agradecimiento: Agradecemos a Piotr Gmytrasiewicz por algunas discusiones útiles relacionadas con este trabajo. El primer autor desea agradecer el apoyo de una beca UGARF. 7. REFERENCIAS [1] R. J. Aumann. Epistemología interactiva i: Conocimiento. Revista Internacional de Teoría de Juegos, 28:263-300, 1999. [2] D. Billings, A. Davidson, J. Schaeffer y D. Szafron. El desafío del póker. AIJ, 2001. [3] A. Brandenburger y E. Dekel. Jerarquías de creencias y conocimiento común. Revista de Teoría Económica, 59:189-198, 1993. [4] C. Camerer. Teoría de juegos conductual: Experimentos en interacción estratégica. Princeton University Press, 2003. [5] E. Fehr y S. Gachter. Cooperación y castigo en experimentos de bienes públicos. Revista Económica Americana, 90(4):980-994, 2000. [6] D. Fudenberg y D. K. Levine. La Teoría del Aprendizaje en los Juegos. MIT Press, 1998. [7] D. Fudenberg y J. Tirole. Teoría de juegos. MIT Press, 1991. [8] Y. Gal y A. Pfeffer. Un lenguaje para modelar los procesos de toma de decisiones de agentes en juegos. En AAMAS, 2003. [9] P. Gmytrasiewicz y P. Doshi. Un marco para la planificación secuencial en entornos multiagentes. JAIR, 24:49-79, 2005. [10] P. Gmytrasiewicz y E. Durfee. Coordinación racional en entornos multiagente. JAAMAS, 3(4):319-350, 2000. [11] J. C. Harsanyi. Juegos con información incompleta jugados por jugadores bayesianos. Ciencia de la Gestión, 14(3):159-182, 1967. [12] R. A. Howard y J. E. Matheson. Diagramas de influencia. En R. A. Howard y J. E. Matheson, editores, Los Principios y Aplicaciones del Análisis de Decisiones. Grupo de Decisiones Estratégicas, Menlo Park, CA 94025, 1984. [13] L. Kaelbling, M. Littman y A. Cassandra. Planificación y actuación en dominios estocásticos parcialmente observables. Revista de Inteligencia Artificial, 2, 1998. [14] D. Koller y B. Milch. Diagramas de influencia multiagente para representar y resolver juegos. En IJCAI, páginas 1027-1034, 2001. [15] K. Polich y P. Gmytrasiewicz. Diagramas de influencia interactivos y dinámicos. En el taller GTDT, AAMAS, 2006. [16] B. Rathnas, P. Doshi y P. J. Gmytrasiewicz. Soluciones exactas para POMDP interactivos utilizando equivalencia conductual. En la Conferencia de Agentes Autónomos y Sistemas Multiagente (AAMAS), 2006. [17] S. Russell y P. Norvig. Inteligencia Artificial: Un Enfoque Moderno (Segunda Edición). Prentice Hall, 2003. [18] R. D. Shachter. \n\nPrentice Hall, 2003. [18] R. D. Shachter. Evaluando diagramas de influencia. Investigación de Operaciones, 34(6):871-882, 1986. [19] D. Suryadi y P. Gmytrasiewicz. Aprendiendo modelos de otros agentes utilizando diagramas de influencia. En UM, 1999. El Sexto Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 821 ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "multi-agent influence diagram": {
            "translated_key": "diagramas de influencia multiagente",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Graphical Models for Online Solutions to Interactive POMDPs Prashant Doshi Dept.",
                "of Computer Science University of Georgia Athens, GA 30602, USA pdoshi@cs.uga.edu Yifeng Zeng Dept. of Computer Science Aalborg University DK-9220 Aalborg, Denmark yfzeng@cs.aau.edu Qiongyu Chen Dept. of Computer Science National Univ. of Singapore 117543, Singapore chenqy@comp.nus.edu.sg ABSTRACT We develop a new graphical representation for interactive partially observable Markov decision processes (I-POMDPs) that is significantly more transparent and semantically clear than the previous representation.",
                "These graphical models called interactive dynamic influence diagrams (I-DIDs) seek to explicitly model the structure that is often present in real-world problems by decomposing the situation into chance and decision variables, and the dependencies between the variables.",
                "I-DIDs generalize DIDs, which may be viewed as graphical representations of POMDPs, to multiagent settings in the same way that I-POMDPs generalize POMDPs.",
                "I-DIDs may be used to compute the policy of an agent online as the agent acts and observes in a setting that is populated by other interacting agents.",
                "Using several examples, we show how I-DIDs may be applied and demonstrate their usefulness.",
                "Categories and Subject Descriptors I.2.11 [Distributed Artificial Intelligence]: Multiagent Systems General Terms Theory 1.",
                "INTRODUCTION Interactive partially observable Markov decision processes (IPOMDPs) [9] provide a framework for sequential decision-making in partially observable multiagent environments.",
                "They generalize POMDPs [13] to multiagent settings by including the other agents computable models in the state space along with the states of the physical environment.",
                "The models encompass all information influencing the agents behaviors, including their preferences, capabilities, and beliefs, and are thus analogous to types in Bayesian games [11].",
                "I-POMDPs adopt a subjective approach to understanding strategic behavior, rooted in a decision-theoretic framework that takes a decision-makers perspective in the interaction.",
                "In [15], Polich and Gmytrasiewicz introduced interactive dynamic influence diagrams (I-DIDs) as the computational representations of I-POMDPs.",
                "I-DIDs generalize DIDs [12], which may be viewed as computational counterparts of POMDPs, to multiagents settings in the same way that I-POMDPs generalize POMDPs.",
                "I-DIDs contribute to a growing line of work [19] that includes <br>multi-agent influence diagram</br>s (MAIDs) [14], and more recently, networks of influence diagrams (NIDs) [8].",
                "These formalisms seek to explicitly model the structure that is often present in real-world problems by decomposing the situation into chance and decision variables, and the dependencies between the variables.",
                "MAIDs provide an alternative to normal and extensive game forms using a graphical formalism to represent games of imperfect information with a decision node for each agents actions and chance nodes capturing the agents private information.",
                "MAIDs objectively analyze the game, efficiently computing the Nash equilibrium profile by exploiting the independence structure.",
                "NIDs extend MAIDs to include agents uncertainty over the game being played and over models of the other agents.",
                "Each model is a MAID and the network of MAIDs is collapsed, bottom up, into a single MAID for computing the equilibrium of the game keeping in mind the different models of each agent.",
                "Graphical formalisms such as MAIDs and NIDs open up a promising area of research that aims to represent multiagent interactions more transparently.",
                "However, MAIDs provide an analysis of the game from an external viewpoint and the applicability of both is limited to static single play games.",
                "Matters are more complex when we consider interactions that are extended over time, where predictions about others future actions must be made using models that change as the agents act and observe.",
                "I-DIDs address this gap by allowing the representation of other agents models as the values of a special model node.",
                "Both, other agents models and the original agents beliefs over these models are updated over time using special-purpose implementations.",
                "In this paper, we improve on the previous preliminary representation of the I-DID shown in [15] by using the insight that the static I-ID is a type of NID.",
                "Thus, we may utilize NID-specific language constructs such as multiplexers to represent the model node, and subsequently the I-ID, more transparently.",
                "Furthermore, we clarify the semantics of the special purpose policy link introduced in the representation of I-DID by [15], and show that it could be replaced by traditional dependency links.",
                "In the previous representation of the I-DID, the update of the agents belief over the models of others as the agents act and receive observations was denoted using a special link called the model update link that connected the model nodes over time.",
                "We explicate the semantics of this link by showing how it can be implemented using the traditional dependency links between the chance nodes that constitute the model nodes.",
                "The net result is a representation of I-DID that is significantly more transparent, semantically clear, and capable of being implemented using the standard algorithms for solving DIDs.",
                "We show how IDIDs may be used to model an agents uncertainty over others models, that may themselves be I-DIDs.",
                "Solution to the I-DID is a policy that prescribes what the agent should do over time, given its beliefs over the physical state and others models.",
                "Analogous to DIDs, I-DIDs may be used to compute the policy of an agent online as the agent acts and observes in a setting that is populated by other interacting agents. 2.",
                "BACKGROUND: FINITELY NESTED IPOMDPS Interactive POMDPs generalize POMDPs to multiagent settings by including other agents models as part of the state space [9].",
                "Since other agents may also reason about others, the interactive state space is strategically nested; it contains beliefs about other agents models and their beliefs about others.",
                "For simplicity of presentation we consider an agent, i, that is interacting with one other agent, j.",
                "A finitely nested I-POMDP of agent i with a strategy level l is defined as the tuple: I-POMDPi,l = ISi,l, A, Ti, Ωi, Oi, Ri where: • ISi,l denotes a set of interactive states defined as, ISi,l = S × Mj,l−1, where Mj,l−1 = {Θj,l−1 ∪ SMj}, for l ≥ 1, and ISi,0 = S, where S is the set of states of the physical environment.",
                "Θj,l−1 is the set of computable intentional models of agent j: θj,l−1 = bj,l−1, ˆθj where the frame, ˆθj = A, Ωj, Tj, Oj, Rj, OCj .",
                "Here, j is Bayes rational and OCj is js optimality criterion.",
                "SMj is the set of subintentional models of j.",
                "Simple examples of subintentional models include a no-information model [10] and a fictitious play model [6], both of which are history independent.",
                "We give a recursive bottom-up construction of the interactive state space below.",
                "ISi,0 = S, Θj,0 = { bj,0, ˆθj | bj,0 ∈ Δ(ISj,0)} ISi,1 = S × {Θj,0 ∪ SMj}, Θj,1 = { bj,1, ˆθj | bj,1 ∈ Δ(ISj,1)} . . . . . .",
                "ISi,l = S × {Θj,l−1 ∪ SMj}, Θj,l = { bj,l, ˆθj | bj,l ∈ Δ(ISj,l)} Similar formulations of nested spaces have appeared in [1, 3]. • A = Ai × Aj is the set of joint actions of all agents in the environment; • Ti : S ×A×S → [0, 1], describes the effect of the joint actions on the physical states of the environment; • Ωi is the set of observations of agent i; • Oi : S × A × Ωi → [0, 1] gives the likelihood of the observations given the physical state and joint action; • Ri : ISi × A → R describes agent is preferences over its interactive states.",
                "Usually only the physical states will matter.",
                "Agent is policy is the mapping, Ω∗ i → Δ(Ai), where Ω∗ i is the set of all observation histories of agent i.",
                "Since belief over the interactive states forms a sufficient statistic [9], the policy can also be represented as a mapping from the set of all beliefs of agent i to a distribution over its actions, Δ(ISi) → Δ(Ai). 2.1 Belief Update Analogous to POMDPs, an agent within the I-POMDP framework updates its belief as it acts and observes.",
                "However, there are two differences that complicate the belief update in multiagent settings when compared to single agent ones.",
                "First, since the state of the physical environment depends on the actions of both agents, is prediction of how the physical state changes has to be made based on its prediction of js actions.",
                "Second, changes in js models have to be included in is belief update.",
                "Specifically, if j is intentional then an update of js beliefs due to its action and observation has to be included.",
                "In other words, i has to update its belief based on its prediction of what j would observe and how j would update its belief.",
                "If js model is subintentional, then js probable observations are appended to the observation history contained in the model.",
                "Formally, we have: Pr(ist |at−1 i , bt−1 i,l ) = β ISt−1:mt−1 j =θt j bt−1 i,l (ist−1 ) × at−1 j Pr(at−1 j |θt−1 j,l−1)Oi(st , at−1 i , at−1 j , ot i) ×Ti(st−1 , at−1 i , at−1 j , st ) ot j Oj(st , at−1 i , at−1 j , ot j) ×τ(SEθt j (bt−1 j,l−1, at−1 j , ot j) − bt j,l−1) (1) where β is the normalizing constant, τ is 1 if its argument is 0 otherwise it is 0, Pr(at−1 j |θt−1 j,l−1) is the probability that at−1 j is Bayes rational for the agent described by model θt−1 j,l−1, and SE(·) is an abbreviation for the belief update.",
                "For a version of the belief update when js model is subintentional, see [9].",
                "If agent j is also modeled as an I-POMDP, then is belief update invokes js belief update (via the term SEθt j ( bt−1 j,l−1 , at−1 j , ot j)), which in turn could invoke is belief update and so on.",
                "This recursion in belief nesting bottoms out at the 0th level.",
                "At this level, the belief update of the agent reduces to a POMDP belief update. 1 For illustrations of the belief update, additional details on I-POMDPs, and how they compare with other multiagent frameworks, see [9]. 2.2 Value Iteration Each belief state in a finitely nested I-POMDP has an associated value reflecting the maximum payoff the agent can expect in this belief state: Un ( bi,l, θi ) = max ai∈Ai is∈ISi,l ERi(is, ai)bi,l(is)+ γ oi∈Ωi Pr(oi|ai, bi,l)Un−1 ( SEθi (bi,l, ai, oi), θi ) (2) where, ERi(is, ai) = aj Ri(is, ai, aj)Pr(aj|mj,l−1) (since is = (s, mj,l−1)).",
                "Eq. 2 is a basis for value iteration in I-POMDPs.",
                "Agent is optimal action, a∗ i , for the case of finite horizon with discounting, is an element of the set of optimal actions for the belief state, OPT(θi), defined as: OPT( bi,l, θi ) = argmax ai∈Ai is∈ISi,l ERi(is, ai)bi,l(is) +γ oi∈Ωi Pr(oi|ai, bi,l)Un ( SEθi (bi,l, ai, oi), θi ) (3) 3.",
                "INTERACTIVEINFLUENCEDIAGRAMS A naive extension of influence diagrams (IDs) to settings populated by multiple agents is possible by treating other agents as automatons, represented using chance nodes.",
                "However, this approach assumes that the agents actions are controlled using a probability distribution that does not change over time.",
                "Interactive influence diagrams (I-IDs) adopt a more sophisticated approach by generalizing IDs to make them applicable to settings shared with other agents who may act and observe, and update their beliefs. 3.1 Syntax In addition to the usual chance, decision, and utility nodes, IIDs include a new type of node called the model node.",
                "We show a general level l I-ID in Fig. 1(a), where the model node (Mj,l−1) is denoted using a hexagon.",
                "We note that the probability distribution over the chance node, S, and the model node together represents agent is belief over its interactive states.",
                "In addition to the model 1 The 0th level model is a POMDP: Other agents actions are treated as exogenous events and folded into the T, O, and R functions.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 815 Figure 1: (a) A generic level l I-ID for agent i situated with one other agent j.",
                "The hexagon is the model node (Mj,l−1) whose structure we show in (b).",
                "Members of the model node are I-IDs themselves (m1 j,l−1, m2 j,l−1; diagrams not shown here for simplicity) whose decision nodes are mapped to the corresponding chance nodes (A1 j , A2 j ).",
                "Depending on the value of the node, Mod[Mj], the distribution of each of the chance nodes is assigned to the node Aj. (c) The transformed I-ID with the model node replaced by the chance nodes and the relationships between them. node, I-IDs differ from IDs by having a dashed link (called the policy link in [15]) between the model node and a chance node, Aj, that represents the distribution over the other agents actions given its model.",
                "In the absence of other agents, the model node and the chance node, Aj, vanish and I-IDs collapse into traditional IDs.",
                "The model node contains the alternative computational models ascribed by i to the other agent from the set, Θj,l−1 ∪ SMj, where Θj,l−1 and SMj were defined previously in Section 2.",
                "Thus, a model in the model node may itself be an I-ID or ID, and the recursion terminates when a model is an ID or subintentional.",
                "Because the model node contains the alternative models of the other agent as its values, its representation is not trivial.",
                "In particular, some of the models within the node are I-IDs that when solved generate the agents optimal policy in their decision nodes.",
                "Each decision node is mapped to the corresponding chance node, say A1 j , in the following way: if OPT is the set of optimal actions obtained by solving the I-ID (or ID), then Pr(aj ∈ A1 j ) = 1 |OP T | if aj ∈ OPT, 0 otherwise.",
                "Borrowing insights from previous work [8], we observe that the model node and the dashed policy link that connects it to the chance node, Aj, could be represented as shown in Fig. 1(b).",
                "The decision node of each level l − 1 I-ID is transformed into a chance node, as we mentioned previously, so that the actions with the largest value in the decision node are assigned uniform probabilities in the chance node while the rest are assigned zero probability.",
                "The different chance nodes (A1 j , A2 j ), one for each model, and additionally, the chance node labeled Mod[Mj] form the parents of the chance node, Aj.",
                "Thus, there are as many action nodes (A1 j , A2 j ) in Mj,l−1 as the number of models in the support of agent is beliefs.",
                "The conditional probability table of the chance node, Aj, is a multiplexer that assumes the distribution of each of the action nodes (A1 j , A2 j ) depending on the value of Mod[Mj].",
                "The values of Mod[Mj] denote the different models of j.",
                "In other words, when Mod[Mj] has the value m1 j,l−1, the chance node Aj assumes the distribution of the node A1 j , and Aj assumes the distribution of A2 j when Mod[Mj] has the value m2 j,l−1.",
                "The distribution over the node, Mod[Mj], is the agent is belief over the models of j given a physical state.",
                "For more agents, we will have as many model nodes as there are agents.",
                "Notice that Fig. 1(b) clarifies the semantics of the policy link, and shows how it can be represented using the traditional dependency links.",
                "In Fig. 1(c), we show the transformed I-ID when the model node is replaced by the chance nodes and relationships between them.",
                "In contrast to the representation in [15], there are no special-purpose policy links, rather the I-ID is composed of only those types of nodes that are found in traditional IDs and dependency relationships between the nodes.",
                "This allows I-IDs to be represented and implemented using conventional application tools that target IDs.",
                "Note that we may view the level l I-ID as a NID.",
                "Specifically, each of the level l − 1 models within the model node are blocks in the NID (see Fig. 2).",
                "If the level l = 1, each block is a traditional ID, otherwise if l > 1, each block within the NID may itself be a NID.",
                "Note that within the I-IDs (or IDs) at each level, there is only a single decision node.",
                "Thus, our NID does not contain any MAIDs.",
                "Figure 2: A level l I-ID represented as a NID.",
                "The probabilities assigned to the blocks of the NID are is beliefs over js models conditioned on a physical state. 3.2 Solution The solution of an I-ID proceeds in a bottom-up manner, and is implemented recursively.",
                "We start by solving the level 0 models, which, if intentional, are traditional IDs.",
                "Their solutions provide probability distributions over the other agents actions, which are entered in the corresponding chance nodes found in the model node of the level 1 I-ID.",
                "The mapping from the level 0 models decision nodes to the chance nodes is carried out so that actions with the largest value in the decision node are assigned uniform probabilities in the chance node while the rest are assigned zero probability.",
                "Given the distributions over the actions within the different chance nodes (one for each model of the other agent), the level 1 I-ID is transformed as shown in Fig. 1(c).",
                "During the transformation, the conditional probability table (CPT) of the node, Aj, is populated such that the node assumes the distribution of each of the chance nodes depending on the value of the node, Mod[Mj].",
                "As we mentioned previously, the values of the node Mod[Mj] denote the different models of the other agent, and its distribution is the agent is belief over the models of j conditioned on the physical state.",
                "The transformed level 1 I-ID is a traditional ID that may be solved us816 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) (a) (b) Figure 3: (a) A generic two time-slice level l I-DID for agent i in a setting with one other agent j.",
                "Notice the dotted model update link that denotes the update of the models of j and the distribution over the models over time. (b) The semantics of the model update link. ing the standard expected utility maximization method [18].",
                "This procedure is carried out up to the level l I-ID whose solution gives the non-empty set of optimal actions that the agent should perform given its belief.",
                "Notice that analogous to IDs, I-IDs are suitable for online decision-making when the agents current belief is known. 4.",
                "INTERACTIVE DYNAMIC INFLUENCE DIAGRAMS Interactive dynamic influence diagrams (I-DIDs) extend I-IDs (and NIDs) to allow sequential decision-making over several time steps.",
                "Just as DIDs are structured graphical representations of POMDPs, I-DIDs are the graphical online analogs for finitely nested I-POMDPs.",
                "I-DIDs may be used to optimize over a finite look-ahead given initial beliefs while interacting with other, possibly similar, agents. 4.1 Syntax We depict a general two time-slice I-DID in Fig. 3(a).",
                "In addition to the model nodes and the dashed policy link, what differentiates an I-DID from a DID is the model update link shown as a dotted arrow in Fig. 3(a).",
                "We explained the semantics of the model node and the policy link in the previous section; we describe the model updates next.",
                "The update of the model node over time involves two steps: First, given the models at time t, we identify the updated set of models that reside in the model node at time t + 1.",
                "Recall from Section 2 that an agents intentional model includes its belief.",
                "Because the agents act and receive observations, their models are updated to reflect their changed beliefs.",
                "Since the set of optimal actions for a model could include all the actions, and the agent may receive any one of |Ωj| possible observations, the updated set at time step t + 1 will have at most |Mt j,l−1||Aj||Ωj| models.",
                "Here, |Mt j,l−1| is the number of models at time step t, |Aj| and |Ωj| are the largest spaces of actions and observations respectively, among all the models.",
                "Second, we compute the new distribution over the updated models given the original distribution and the probability of the agent performing the action and receiving the observation that led to the updated model.",
                "These steps are a part of agent is belief update formalized using Eq. 1.",
                "In Fig. 3(b), we show how the dotted model update link is implemented in the I-DID.",
                "If each of the two level l − 1 models ascribed to j at time step t results in one action, and j could make one of two possible observations, then the model node at time step t + 1 contains four updated models (mt+1,1 j,l−1 ,mt+1,2 j,l−1 , mt+1,3 j,l−1 , and mt+1,4 j,l−1 ).",
                "These models differ in their initial beliefs, each of which is the result of j updating its beliefs due to its action and a possible observation.",
                "The decision nodes in each of the I-DIDs or DIDs that represent the lower level models are mapped to the corresponding Figure 4: Transformed I-DID with the model nodes and model update link replaced with the chance nodes and the relationships (in bold). chance nodes, as mentioned previously.",
                "Next, we describe how the distribution over the updated set of models (the distribution over the chance node Mod[Mt+1 j ] in Mt+1 j,l−1) is computed.",
                "The probability that js updated model is, say mt+1,1 j,l−1 , depends on the probability of j performing the action and receiving the observation that led to this model, and the prior distribution over the models at time step t. Because the chance node At j assumes the distribution of each of the action nodes based on the value of Mod[Mt j ], the probability of the action is given by this chance node.",
                "In order to obtain the probability of js possible observation, we introduce the chance node Oj, which depending on the value of Mod[Mt j ] assumes the distribution of the observation node in the lower level model denoted by Mod[Mt j ].",
                "Because the probability of js observations depends on the physical state and the joint actions of both agents, the node Oj is linked with St+1 , At j, and At i. 2 Analogous to At j, the conditional probability table of Oj is also a multiplexer modulated by Mod[Mt j ].",
                "Finally, the distribution over the prior models at time t is obtained from the chance node, Mod[Mt j ] in Mt j,l−1.",
                "Consequently, the chance nodes, Mod[Mt j ], At j, and Oj, form the parents of Mod[Mt+1 j ] in Mt+1 j,l−1.",
                "Notice that the model update link may be replaced by the dependency links between the chance nodes that constitute the model nodes in the two time slices.",
                "In Fig. 4 we show the two time-slice I-DID with the model nodes replaced by the chance nodes and the relationships between them.",
                "Chance nodes and dependency links that not in bold are standard, usually found in DIDs.",
                "Expansion of the I-DID over more time steps requires the repetition of the two steps of updating the set of models that form the 2 Note that Oj represents js observation at time t + 1.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 817 values of the model node and adding the relationships between the chance nodes, as many times as there are model update links.",
                "We note that the possible set of models of the other agent j grows exponentially with the number of time steps.",
                "For example, after T steps, there may be at most |Mt=1 j,l−1|(|Aj||Ωj|)T −1 candidate models residing in the model node. 4.2 Solution Analogous to I-IDs, the solution to a level l I-DID for agent i expanded over T time steps may be carried out recursively.",
                "For the purpose of illustration, let l=1 and T=2.",
                "The solution method uses the standard look-ahead technique, projecting the agents action and observation sequences forward from the current belief state [17], and finding the possible beliefs that i could have in the next time step.",
                "Because agent i has a belief over js models as well, the lookahead includes finding out the possible models that j could have in the future.",
                "Consequently, each of js subintentional or level 0 models (represented using a standard DID) in the first time step must be solved to obtain its optimal set of actions.",
                "These actions are combined with the set of possible observations that j could make in that model, resulting in an updated set of candidate models (that include the updated beliefs) that could describe the behavior of j. Beliefs over this updated set of candidate models are calculated using the standard inference methods using the dependency relationships between the model nodes as shown in Fig. 3(b).",
                "We note the recursive nature of this solution: in solving agent is level 1 I-DID, js level 0 DIDs must be solved.",
                "If the nesting of models is deeper, all models at all levels starting from 0 are solved in a bottom-up manner.",
                "We briefly outline the recursive algorithm for solving agent is Algorithm for solving I-DID Input : level l ≥ 1 I-ID or level 0 ID, T Expansion Phase 1.",
                "For t from 1 to T − 1 do 2.",
                "If l ≥ 1 then Populate Mt+1 j,l−1 3.",
                "For each mt j in Range(Mt j,l−1) do 4.",
                "Recursively call algorithm with the l − 1 I-ID (or ID) that represents mt j and the horizon, T − t + 1 5.",
                "Map the decision node of the solved I-ID (or ID), OPT(mt j), to a chance node Aj 6.",
                "For each aj in OPT(mt j) do 7.",
                "For each oj in Oj (part of mt j) do 8.",
                "Update js belief, bt+1 j ← SE(bt j, aj, oj) 9. mt+1 j ← New I-ID (or ID) with bt+1 j as the initial belief 10.",
                "Range(Mt+1 j,l−1) ∪ ← {mt+1 j } 11.",
                "Add the model node, Mt+1 j,l−1, and the dependency links between Mt j,l−1 and Mt+1 j,l−1 (shown in Fig. 3(b)) 12.",
                "Add the chance, decision, and utility nodes for t + 1 time slice and the dependency links between them 13.",
                "Establish the CPTs for each chance node and utility node Look-Ahead Phase 14.",
                "Apply the standard look-ahead and backup method to solve the expanded I-DID Figure 5: Algorithm for solving a level l ≥ 0 I-DID. level l I-DID expanded over T time steps with one other agent j in Fig. 5.",
                "We adopt a two-phase approach: Given an I-ID of level l (described previously in Section 3) with all lower level models also represented as I-IDs or IDs (if level 0), the first step is to expand the level l I-ID over T time steps adding the dependency links and the conditional probability tables for each node.",
                "We particularly focus on establishing and populating the model nodes (lines 3-11).",
                "Note that Range(·) returns the values (lower level models) of the random variable given as input (model node).",
                "In the second phase, we use a standard look-ahead technique projecting the action and observation sequences over T time steps in the future, and backing up the utility values of the reachable beliefs.",
                "Similar to I-IDs, the I-DIDs reduce to DIDs in the absence of other agents.",
                "As we mentioned previously, the 0-th level models are the traditional DIDs.",
                "Their solutions provide probability distributions over actions of the agent modeled at that level to I-DIDs at level 1.",
                "Given probability distributions over other agents actions the level 1 IDIDs can themselves be solved as DIDs, and provide probability distributions to yet higher level models.",
                "Assume that the number of models considered at each level is bound by a number, M. Solving an I-DID of level l in then equivalent to solving O(Ml ) DIDs. 5.",
                "EXAMPLE APPLICATIONS To illustrate the usefulness of I-DIDs, we apply them to three problem domains.",
                "We describe, in particular, the formulation of the I-DID and the optimal prescriptions obtained on solving it. 5.1 Followership-Leadership in the Multiagent Tiger Problem We begin our illustrations of using I-IDs and I-DIDs with a slightly modified version of the multiagent tiger problem discussed in [9].",
                "The problem has two agents, each of which can open the right door (OR), the left door (OL) or listen (L).",
                "In addition to hearing growls (from the left (GL) or from the right (GR)) when they listen, the agents also hear creaks (from the left (CL), from the right (CR), or no creaks (S)), which noisily indicate the other agents opening one of the doors.",
                "When any door is opened, the tiger persists in its original location with a probability of 95%.",
                "Agent i hears growls with a reliability of 65% and creaks with a reliability of 95%.",
                "Agent j, on the other hand, hears growls with a reliability of 95%.",
                "Thus, the setting is such that agent i hears agent j opening doors more reliably than the tigers growls.",
                "This suggests that i could use js actions as an indication of the location of the tiger, as we discuss below.",
                "Each agents preferences are as in the single agent game discussed in [13].",
                "The transition, observation, and reward functions are shown in [16].",
                "A good indicator of the usefulness of normative methods for decision-making like I-DIDs is the emergence of realistic social behaviors in their prescriptions.",
                "In settings of the persistent multiagent tiger problem that reflect real world situations, we demonstrate followership between the agents and, as shown in [15], deception among agents who believe that they are in a follower-leader type of relationship.",
                "In particular, we analyze the situational and epistemological conditions sufficient for their emergence.",
                "The followership behavior, for example, results from the agent knowing its own weaknesses, assessing the strengths, preferences, and possible behaviors of the other, and realizing that its best for it to follow the others actions in order to maximize its payoffs.",
                "Let us consider a particular setting of the tiger problem in which agent i believes that js preferences are aligned with its own - both of them just want to get the gold - and js hearing is more reliable in comparison to itself.",
                "As an example, suppose that j, on listening can discern the tigers location 95% of the times compared to is 65% accuracy.",
                "Additionally, agent i does not have any initial information about the tigers location.",
                "In other words, is single-level nested belief, bi,1, assigns 0.5 to each of the two locations of the tiger.",
                "In addition, i considers two models of j, which differ in js flat level 0 initial beliefs.",
                "This is represented in the level 1 I-ID shown in Fig. 6(a).",
                "According to one model, j assigns a probability of 0.9 that the tiger is behind the left door, while the other 818 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Figure 6: (a) Level 1 I-ID of agent i, (b) two level 0 IDs of agent j whose decision nodes are mapped to the chance nodes, A1 j , A2 j , in (a). model assigns 0.1 to that location (see Fig. 6(b)).",
                "Agent i is undecided on these two models of j.",
                "If we vary is hearing ability, and solve the corresponding level 1 I-ID expanded over three time steps, we obtain the normative behavioral policies shown in Fig 7 that exhibit followership behavior.",
                "If is probability of correctly hearing the growls is 0.65, then as shown in the policy in Fig. 7(a), i begins to conditionally follow js actions: i opens the same door that j opened previously iff is own assessment of the tigers location confirms js pick.",
                "If i loses the ability to correctly interpret the growls completely, it blindly follows j and opens the same door that j opened previously (Fig. 7(b)).",
                "Figure 7: Emergence of (a) conditional followership, and (b) blind followership in the tiger problem.",
                "Behaviors of interest are in bold. * is a wildcard, and denotes any one of the observations.",
                "We observed that a single level of belief nesting - beliefs about the others models - was sufficient for followership to emerge in the tiger problem.",
                "However, the epistemological requirements for the emergence of leadership are more complex.",
                "For an agent, say j, to emerge as a leader, followership must first emerge in the other agent i.",
                "As we mentioned previously, if i is certain that its preferences are identical to those of j, and believes that j has a better sense of hearing, i will follow js actions over time.",
                "Agent j emerges as a leader if it believes that i will follow it, which implies that js belief must be nested two levels deep to enable it to recognize its leadership role.",
                "Realizing that i will follow presents j with an opportunity to influence is actions in the benefit of the collective good or its self-interest alone.",
                "For example, in the tiger problem, let us consider a setting in which if both i and j open the correct door, then each gets a payoff of 20 that is double the original.",
                "If j alone selects the correct door, it gets the payoff of 10.",
                "On the other hand, if both agents pick the wrong door, their penalties are cut in half.",
                "In this setting, it is in both js best interest as well as the collective betterment for j to use its expertise in selecting the correct door, and thus be a good leader.",
                "However, consider a slightly different problem in which j gains from is loss and is penalized if i gains.",
                "Specifically, let is payoff be subtracted from js, indicating that j is antagonistic toward i - if j picks the correct door and i the wrong one, then is loss of 100 becomes js gain.",
                "Agent j believes that i incorrectly thinks that js preferences are those that promote the collective good and that it starts off by believing with 99% confidence where the tiger is.",
                "Because i believes that its preferences are similar to those of j, and that j starts by believing almost surely that one of the two is the correct location (two level 0 models of j), i will start by following js actions.",
                "We show is normative policy on solving its singly-nested I-DID over three time steps in Fig. 8(a).",
                "The policy demonstrates that i will blindly follow js actions.",
                "Since the tiger persists in its original location with a probability of 0.95, i will select the same door again.",
                "If j begins the game with a 99% probability that the tiger is on the right, solving js I-DID nested two levels deep, results in the policy shown in Fig. 8(b).",
                "Even though j is almost certain that OL is the correct action, it will start by selecting OR, followed by OL.",
                "Agent js intention is to deceive i who, it believes, will follow js actions, so as to gain $110 in the second time step, which is more than what j would gain if it were to be honest.",
                "Figure 8: Emergence of deception between agents in the tiger problem.",
                "Behaviors of interest are in bold. * denotes as before. (a) Agent is policy demonstrating that it will blindly follow js actions. (b) Even though j is almost certain that the tiger is on the right, it will start by selecting OR, followed by OL, in order to deceive i. 5.2 Altruism and Reciprocity in the Public Good Problem The public good (PG) problem [7], consists of a group of M agents, each of whom must either contribute some resource to a public pot or keep it for themselves.",
                "Since resources contributed to the public pot are shared among all the agents, they are less valuable to the agent when in the public pot.",
                "However, if all agents choose to contribute their resources, then the payoff to each agent is more than if no one contributes.",
                "Since an agent gets its share of the public pot irrespective of whether it has contributed or not, the dominating action is for each agent to not contribute, and instead free ride on others contributions.",
                "However, behaviors of human players in empirical simulations of the PG problem differ from the normative predictions.",
                "The experiments reveal that many players initially contribute a large amount to the public pot, and continue to contribute when the PG problem is played repeatedly, though in decreasing amounts [4].",
                "Many of these experiments [5] report that a small core group of players persistently contributes to the public pot even when all others are defecting.",
                "These experiments also reveal that players who persistently contribute have altruistic or reciprocal preferences matching expected cooperation of others.",
                "For simplicity, we assume that the game is played between M = 2 agents, i and j.",
                "Let each agent be initially endowed with XT amount of resources.",
                "While the classical PG game formulation permits each agent to contribute any quantity of resources (≤ XT ) to the public pot, we simplify the action space by allowing two possible actions.",
                "Each agent may choose to either contribute (C) a fixed amount of the resources, or not contribute.",
                "The latter action is deThe Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 819 noted as defect (D).",
                "We assume that the actions are not observable to others.",
                "The value of resources in the public pot is discounted by ci for each agent i, where ci is the marginal private return.",
                "We assume that ci < 1 so that the agent does not benefit enough that it contributes to the public pot for private gain.",
                "Simultaneously, ciM > 1, making collective contribution pareto optimal. i/j C D C 2ciXT , 2cjXT ciXT − cp, XT + cjXT − P D XT + ciXT − P, cjXT − cp XT , XT Table 1: The one-shot PG game with punishment.",
                "In order to encourage contributions, the contributing agents punish free riders but incur a small cost for administering the punishment.",
                "Let P be the punishment meted out to the defecting agent and cp the non-zero cost of punishing for the contributing agent.",
                "For simplicity, we assume that the cost of punishing is same for both the agents.",
                "The one-shot PG game with punishment is shown in Table. 1.",
                "Let ci = cj, cp > 0, and if P > XT − ciXT , then defection is no longer a dominating action.",
                "If P < XT − ciXT , then defection is the dominating action for both.",
                "If P = XT − ciXT , then the game is not dominance-solvable.",
                "Figure 9: (a) Level 1 I-ID of agent i, (b) level 0 IDs of agent j with decision nodes mapped to the chance nodes, A1 j and A2 j , in (a).",
                "We formulate a sequential version of the PG problem with punishment from the perspective of agent i.",
                "Though in the repeated PG game, the quantity in the public pot is revealed to all the agents after each round of actions, we assume in our formulation that it is hidden from the agents.",
                "Each agent may contribute a fixed amount, xc, or defect.",
                "An agent on performing an action receives an observation of plenty (PY) or meager (MR) symbolizing the state of the public pot.",
                "Notice that the observations are also indirectly indicative of agent js actions because the state of the public pot is influenced by them.",
                "The amount of resources in agent is private pot, is perfectly observable to i.",
                "The payoffs are analogous to Table. 1.",
                "Borrowing from the empirical investigations of the PG problem [5], we construct level 0 IDs for j that model altruistic and non-altruistic types (Fig. 9(b)).",
                "Specifically, our altruistic agent has a high marginal private return (cj is close to 1) and does not punish others who defect.",
                "Let xc = 1 and the level 0 agent be punished half the times it defects.",
                "With one action remaining, both types of agents choose to contribute to avoid being punished.",
                "With two actions to go, the altruistic type chooses to contribute, while the other defects.",
                "This is because cj for the altruistic type is close to 1, thus the expected punishment, 0.5P > (1 − cj), which the altruistic type avoids.",
                "Because cj for the non-altruistic type is less, it prefers not to contribute.",
                "With three steps to go, the altruistic agent contributes to avoid punishment (0.5P > 2(1 − cj)), and the non-altruistic type defects.",
                "For greater than three steps, while the altruistic agent continues to contribute to the public pot depending on how close its marginal private return is to 1, the non-altruistic type prescribes defection.",
                "We analyzed the decisions of an altruistic agent i modeled using a level 1 I-DID expanded over 3 time steps. i ascribes the two level 0 models, mentioned previously, to j (see Fig. 9).",
                "If i believes with a probability 1 that j is altruistic, i chooses to contribute for each of the three steps.",
                "This behavior persists when i is unaware of whether j is altruistic (Fig. 10(a)), and when i assigns a high probability to j being the non-altruistic type.",
                "However, when i believes with a probability 1 that j is non-altruistic and will thus surely defect, i chooses to defect to avoid being punished and because its marginal private return is less than 1.",
                "These results demonstrate that the behavior of our altruistic type resembles that found experimentally.",
                "The non-altruistic level 1 agent chooses to defect regardless of how likely it believes the other agent to be altruistic.",
                "We analyzed the behavior of a reciprocal agent type that matches expected cooperation or defection.",
                "The reciprocal types marginal private return is similar to that of the non-altruistic type, however, it obtains a greater payoff when its action is similar to that of the other.",
                "We consider the case when the reciprocal agent i is unsure of whether j is altruistic and believes that the public pot is likely to be half full.",
                "For this prior belief, i chooses to defect.",
                "On receiving an observation of plenty, i decides to contribute, while an observation of meager makes it defect (Fig. 10(b)).",
                "This is because an observation of plenty signals that the pot is likely to be greater than half full, which results from js action to contribute.",
                "Thus, among the two models ascribed to j, its type is likely to be altruistic making it likely that j will contribute again in the next time step.",
                "Agent i therefore chooses to contribute to reciprocate js action.",
                "An analogous reasoning leads i to defect when it observes a meager pot.",
                "With one action to go, i believing that j contributes, will choose to contribute too to avoid punishment regardless of its observations.",
                "Figure 10: (a) An altruistic level 1 agent always contributes. (b) A reciprocal agent i starts off by defecting followed by choosing to contribute or defect based on its observation of plenty (indicating that j is likely altruistic) or meager (j is non-altruistic). 5.3 Strategies in Two-Player Poker Poker is a popular zero sum card game that has received much attention among the AI research community as a testbed [2].",
                "Poker is played among M ≥ 2 players in which each player receives a hand of cards from a deck.",
                "While several flavors of Poker with varying complexity exist, we consider a simple version in which each player has three plys during which the player may either exchange a card (E), keep the existing hand (K), fold (F) and withdraw from the game, or call (C), requiring all players to show their hands.",
                "To keep matters simple, let M = 2, and each player receive a hand consisting of a single card drawn from the same suit.",
                "Thus, during a showdown, the player who has the numerically larger card (2 is the lowest, ace is the highest) wins the pot.",
                "During an exchange of cards, the discarded card is placed either in the L pile, indicating to the other agent that it was a low numbered card less than 8, or in the 820 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) H pile, indicating that the card had a rank greater than or equal to 8.",
                "Notice that, for example, if a lower numbered card is discarded, the probability of receiving a low card in exchange is now reduced.",
                "We show the level 1 I-ID for the simplified two-player Poker in Fig. 11.",
                "We considered two models (personality types) of agent j.",
                "The conservative type believes that it is likely that its opponent has a high numbered card in its hand.",
                "On the other hand, the aggressive agent j believes with a high probability that its opponent has a lower numbered card.",
                "Thus, the two types differ in their beliefs over their opponents hand.",
                "In both these level 0 models, the opponent is assumed to perform its actions following a fixed, uniform distribution.",
                "With three actions to go, regardless of its hand (unless it is an ace), the aggressive agent chooses to exchange its card, with the intent of improving on its current hand.",
                "This is because it believes the other to have a low card, which improves its chances of getting a high card during the exchange.",
                "The conservative agent chooses to keep its card, no matter its hand because its chances of getting a high card are slim as it believes that its opponent has one.",
                "Figure 11: (a) Level 1 I-ID of agent i.",
                "The observation reveals information about js hand of the previous time step, (b) level 0 IDs of agent j whose decision nodes are mapped to the chance nodes, A1 j , A2 j , in (a).",
                "The policy of a level 1 agent i who believes that each card except its own has an equal likelihood of being in js hand (neutral personality type) and j could be either an aggressive or conservative type, is shown in Fig. 12. is own hand contains the card numbered 8.",
                "The agent starts by keeping its card.",
                "On seeing that j did not exchange a card (N), i believes with probability 1 that j is conservative and hence will keep its cards. i responds by either keeping its card or exchanging it because j is equally likely to have a lower or higher card.",
                "If i observes that j discarded its card into the L or H pile, i believes that j is aggressive.",
                "On observing L, i realizes that j had a low card, and is likely to have a high card after its exchange.",
                "Because the probability of receiving a low card is high now, i chooses to keep its card.",
                "On observing H, believing that the probability of receiving a high numbered card is high, i chooses to exchange its card.",
                "In the final step, i chooses to call regardless of its observation history because its belief that j has a higher card is not sufficiently high to conclude that its better to fold and relinquish the payoff.",
                "This is partly due to the fact that an observation of, say, L resets the agent is previous time step beliefs over js hand to the low numbered cards only. 6.",
                "DISCUSSION We showed how DIDs may be extended to I-DIDs that enable online sequential decision-making in uncertain multiagent settings.",
                "Our graphical representation of I-DIDs improves on the previous Figure 12: A level 1 agent is three step policy in the Poker problem. i starts by believing that j is equally likely to be aggressive or conservative and could have any card in its hand with equal probability. work significantly by being more transparent, semantically clear, and capable of being solved using standard algorithms that target DIDs.",
                "I-DIDs extend NIDs to allow sequential decision-making over multiple time steps in the presence of other interacting agents.",
                "I-DIDs may be seen as concise graphical representations for IPOMDPs providing a way to exploit problem structure and carry out online decision-making as the agent acts and observes given its prior beliefs.",
                "We are currently investigating ways to solve I-DIDs approximately with provable bounds on the solution quality.",
                "Acknowledgment: We thank Piotr Gmytrasiewicz for some useful discussions related to this work.",
                "The first author would like to acknowledge the support of a UGARF grant. 7.",
                "REFERENCES [1] R. J. Aumann.",
                "Interactive epistemology i: Knowledge.",
                "International Journal of Game Theory, 28:263-300, 1999. [2] D. Billings, A. Davidson, J. Schaeffer, and D. Szafron.",
                "The challenge of poker.",
                "AIJ, 2001. [3] A. Brandenburger and E. Dekel.",
                "Hierarchies of beliefs and common knowledge.",
                "Journal of Economic Theory, 59:189-198, 1993. [4] C. Camerer.",
                "Behavioral Game Theory: Experiments in Strategic Interaction.",
                "Princeton University Press, 2003. [5] E. Fehr and S. Gachter.",
                "Cooperation and punishment in public goods experiments.",
                "American Economic Review, 90(4):980-994, 2000. [6] D. Fudenberg and D. K. Levine.",
                "The Theory of Learning in Games.",
                "MIT Press, 1998. [7] D. Fudenberg and J. Tirole.",
                "Game Theory.",
                "MIT Press, 1991. [8] Y. Gal and A. Pfeffer.",
                "A language for modeling agents decision-making processes in games.",
                "In AAMAS, 2003. [9] P. Gmytrasiewicz and P. Doshi.",
                "A framework for sequential planning in multiagent settings.",
                "JAIR, 24:49-79, 2005. [10] P. Gmytrasiewicz and E. Durfee.",
                "Rational coordination in multi-agent environments.",
                "JAAMAS, 3(4):319-350, 2000. [11] J. C. Harsanyi.",
                "Games with incomplete information played by bayesian players.",
                "Management Science, 14(3):159-182, 1967. [12] R. A. Howard and J. E. Matheson.",
                "Influence diagrams.",
                "In R. A. Howard and J. E. Matheson, editors, The Principles and Applications of Decision Analysis.",
                "Strategic Decisions Group, Menlo Park, CA 94025, 1984. [13] L. Kaelbling, M. Littman, and A. Cassandra.",
                "Planning and acting in partially observable stochastic domains.",
                "Artificial Intelligence Journal, 2, 1998. [14] D. Koller and B. Milch.",
                "Multi-agent influence diagrams for representing and solving games.",
                "In IJCAI, pages 1027-1034, 2001. [15] K. Polich and P. Gmytrasiewicz.",
                "Interactive dynamic influence diagrams.",
                "In GTDT Workshop, AAMAS, 2006. [16] B.",
                "Rathnas., P. Doshi, and P. J. Gmytrasiewicz.",
                "Exact solutions to interactive pomdps using behavioral equivalence.",
                "In Autonomous Agents and Multi-Agent Systems Conference (AAMAS), 2006. [17] S. Russell and P. Norvig.",
                "Artificial Intelligence: A Modern Approach (Second Edition).",
                "Prentice Hall, 2003. [18] R. D. Shachter.",
                "Evaluating influence diagrams.",
                "Operations Research, 34(6):871-882, 1986. [19] D. Suryadi and P. Gmytrasiewicz.",
                "Learning models of other agents using influence diagrams.",
                "In UM, 1999.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 821"
            ],
            "original_annotated_samples": [
                "I-DIDs contribute to a growing line of work [19] that includes <br>multi-agent influence diagram</br>s (MAIDs) [14], and more recently, networks of influence diagrams (NIDs) [8]."
            ],
            "translated_annotated_samples": [
                "Los I-DIDs contribuyen a una creciente línea de trabajo [19] que incluye <br>diagramas de influencia multiagente</br> (MAIDs) [14], y más recientemente, redes de diagramas de influencia (NIDs) [8]."
            ],
            "translated_text": "Modelos gráficos para soluciones en línea a POMDP interactivos de Prashant Doshi Dept. Departamento de Ciencias de la Computación Universidad de Georgia Athens, GA 30602, EE. UU. pdoshi@cs.uga.edu Yifeng Zeng Dept. de Ciencias de la Computación Universidad de Aalborg DK-9220 Aalborg, Dinamarca yfzeng@cs.aau.edu Qiongyu Chen Dept. de Ciencias de la Computación Universidad Nacional de Singapur 117543, Singapur chenqy@comp.nus.edu.sg RESUMEN Desarrollamos una nueva representación gráfica para procesos de decisión de Markov parcialmente observables interactivos (I-POMDPs) que es significativamente más transparente y semánticamente clara que la representación anterior. Estos modelos gráficos llamados diagramas de influencia dinámica interactiva (I-DIDs) buscan modelar explícitamente la estructura que a menudo está presente en problemas del mundo real al descomponer la situación en variables de probabilidad y decisión, y las dependencias entre las variables. Los I-DIDs generalizan los DIDs, los cuales pueden ser vistos como representaciones gráficas de POMDPs, a entornos multiagentes de la misma manera que los I-POMDPs generalizan los POMDPs. Los I-DIDs pueden ser utilizados para calcular la política de un agente en línea mientras el agente actúa y observa en un entorno poblado por otros agentes que interactúan entre sí. Usando varios ejemplos, mostramos cómo se pueden aplicar los I-DIDs y demostramos su utilidad. Categorías y Descriptores de Asignaturas I.2.11 [Inteligencia Artificial Distribuida]: Sistemas Multiagentes Términos Generales Teoría 1. Las Procesos de Decisión de Markov Interactivos Parcialmente Observables (IPOMDPs) proporcionan un marco para la toma de decisiones secuenciales en entornos multiagentes parcialmente observables. Generalizan los POMDPs [13] a entornos multiagentes al incluir los modelos computables de los otros agentes en el espacio de estados junto con los estados del entorno físico. Los modelos abarcan toda la información que influye en los comportamientos de los agentes, incluyendo sus preferencias, capacidades y creencias, y por lo tanto son análogos a los tipos en los juegos bayesianos [11]. I-POMDPs adoptan un enfoque subjetivo para comprender el comportamiento estratégico, arraigado en un marco de teoría de decisiones que toma la perspectiva de los tomadores de decisiones en la interacción. En [15], Polich y Gmytrasiewicz introdujeron diagramas de influencia dinámica interactivos (I-DIDs) como las representaciones computacionales de I-POMDPs. Los I-DIDs generalizan los DIDs [12], los cuales pueden ser vistos como contrapartes computacionales de POMDPs, a entornos de múltiples agentes de la misma manera que los I-POMDPs generalizan los POMDPs. Los I-DIDs contribuyen a una creciente línea de trabajo [19] que incluye <br>diagramas de influencia multiagente</br> (MAIDs) [14], y más recientemente, redes de diagramas de influencia (NIDs) [8]. Estos formalismos buscan modelar explícitamente la estructura que a menudo está presente en problemas del mundo real al descomponer la situación en variables de azar y decisiones, y las dependencias entre las variables. Los MAIDs proporcionan una alternativa a las formas normales y extensivas de juego utilizando un formalismo gráfico para representar juegos de información imperfecta con un nodo de decisión para las acciones de cada agente y nodos de azar que capturan la información privada de los agentes. Los MAIDs analizan objetivamente el juego, calculando eficientemente el perfil de equilibrio de Nash al explotar la estructura de independencia. NIDs extienden los MAIDs para incluir la incertidumbre de los agentes sobre el juego que se está jugando y sobre los modelos de los otros agentes. Cada modelo es un MAID y la red de MAIDs se colapsa, de abajo hacia arriba, en un solo MAID para calcular el equilibrio del juego teniendo en cuenta los diferentes modelos de cada agente. Los formalismos gráficos como MAIDs y NIDs abren una área de investigación prometedora que tiene como objetivo representar las interacciones multiagente de manera más transparente. Sin embargo, los MAIDs proporcionan un análisis del juego desde un punto de vista externo y la aplicabilidad de ambos está limitada a juegos estáticos de un solo jugador. Los asuntos son más complejos cuando consideramos interacciones que se extienden en el tiempo, donde las predicciones sobre las acciones futuras de otros deben hacerse utilizando modelos que cambian a medida que los agentes actúan y observan. Los I-DIDs abordan esta brecha al permitir la representación de modelos de otros agentes como los valores de un nodo de modelo especial. Tanto los modelos de otros agentes como las creencias originales de los agentes sobre estos modelos se actualizan con el tiempo utilizando implementaciones especializadas. En este documento, mejoramos la representación preliminar previa del I-DID mostrada en [15] utilizando la idea de que el I-ID estático es un tipo de NID. Por lo tanto, podemos utilizar construcciones de lenguaje específicas de NID, como multiplexores, para representar de manera más transparente el nodo del modelo y, posteriormente, el I-ID. Además, aclaramos la semántica del enlace de política de propósito especial introducido en la representación de I-DID por [15], y mostramos que podría ser reemplazado por enlaces de dependencia tradicionales. En la representación previa del I-DID, la actualización de la creencia de los agentes sobre los modelos de los demás a medida que los agentes actúan y reciben observaciones se denotaba utilizando un enlace especial llamado el enlace de actualización del modelo que conectaba los nodos del modelo a lo largo del tiempo. Explicamos la semántica de este enlace mostrando cómo puede ser implementado utilizando los enlaces de dependencia tradicionales entre los nodos de probabilidad que constituyen los nodos del modelo. El resultado final es una representación de I-DID que es significativamente más transparente, semánticamente clara y capaz de ser implementada utilizando los algoritmos estándar para resolver DIDs. Mostramos cómo los IDIDs pueden ser utilizados para modelar la incertidumbre de un agente sobre los modelos de otros, que a su vez pueden ser I-DIDs. La solución al I-DID es una política que prescribe lo que el agente debe hacer con el tiempo, dadas sus creencias sobre el estado físico y otros modelos. Análogos a los DIDs, los I-DIDs pueden ser utilizados para calcular la política de un agente en línea mientras el agente actúa y observa en un entorno poblado por otros agentes interactuantes. 2. ANTECEDENTES: Los IPOMDPS ANIDADOS FINITAMENTE generalizan los POMDPs a entornos multiagentes al incluir los modelos de otros agentes como parte del espacio de estados [9]. Dado que otros agentes también pueden razonar sobre otros, el espacio de estado interactivo está estratégicamente anidado; contiene creencias sobre los modelos de otros agentes y sus creencias sobre los demás. Para simplificar la presentación, consideramos un agente, i, que está interactuando con otro agente, j. Un I-POMDP finitamente anidado del agente i con un nivel de estrategia l se define como la tupla: I-POMDPi,l = ISi,l, A, Ti, Ωi, Oi, Ri donde: • ISi,l denota un conjunto de estados interactivos definido como, ISi,l = S × Mj,l−1, donde Mj,l−1 = {Θj,l−1 ∪ SMj}, para l ≥ 1, e ISi,0 = S, donde S es el conjunto de estados del entorno físico. Θj,l−1 es el conjunto de modelos intencionales computables del agente j: θj,l−1 = bj,l−1, ˆθj donde el marco, ˆθj = A, Ωj, Tj, Oj, Rj, OCj. Aquí, j es Bayesiano racional y OCj es el criterio de optimalidad de j. SMj es el conjunto de modelos subintencionales de j. Ejemplos simples de modelos subintencionales incluyen un modelo sin información [10] y un modelo de juego ficticio [6], ambos de los cuales son independientes de la historia. Damos una construcción recursiva de abajo hacia arriba del espacio de estados interactivo a continuación. Sí,0 = S, Θj,0 = { bj,0, ˆθj | bj,0 ∈ Δ(ISj,0)} Sí,1 = S × {Θj,0 ∪ SMj}, Θj,1 = { bj,1, ˆθj | bj,1 ∈ Δ(ISj,1)} . . . . . . Sí, l = S × {Θj,l−1 ∪ SMj}, Θj,l = { bj,l, ˆθj | bj,l ∈ Δ(ISj,l)} Formulaciones similares de espacios anidados han aparecido en [1, 3]. • A = Ai × Aj es el conjunto de acciones conjuntas de todos los agentes en el entorno; • Ti : S ×A×S → [0, 1], describe el efecto de las acciones conjuntas en los estados físicos del entorno; • Ωi es el conjunto de observaciones del agente i; • Oi : S × A × Ωi → [0, 1] da la probabilidad de las observaciones dadas el estado físico y la acción conjunta; • Ri : ISi × A → R describe las preferencias del agente sobre sus estados interactivos. Normalmente solo importarán los estados físicos. La política del agente es el mapeo, Ω∗ i → Δ(Ai), donde Ω∗ i es el conjunto de todos los historiales de observación del agente i. Dado que la creencia sobre los estados interactivos forma una estadística suficiente [9], la política también puede ser representada como un mapeo del conjunto de todas las creencias del agente i a una distribución sobre sus acciones, Δ(ISi) → Δ(Ai). 2.1 Actualización de Creencias Análogo a los POMDPs, un agente dentro del marco I-POMDP actualiza su creencia a medida que actúa y observa. Sin embargo, hay dos diferencias que complican la actualización de creencias en entornos multiagentes en comparación con los entornos de un solo agente. Primero, dado que el estado del entorno físico depende de las acciones de ambos agentes, la predicción de cómo cambia el estado físico debe hacerse basándose en la predicción de sus acciones. Segundo, los cambios en los modelos de js deben ser incluidos en la actualización de creencias. Específicamente, si j es intencional, entonces se debe incluir una actualización de las creencias de j debido a su acción y observación. En otras palabras, i tiene que actualizar su creencia basándose en su predicción de lo que j observaría y cómo j actualizaría su creencia. Si el modelo de js es subintencional, entonces las observaciones probables de js se añaden al historial de observaciones contenido en el modelo. Formalmente, tenemos: Pr(ist |at−1 i , bt−1 i,l ) = β ISt−1:mt−1 j =θt j bt−1 i,l (ist−1 ) × at−1 j Pr(at−1 j |θt−1 j,l−1)Oi(st , at−1 i , at−1 j , ot i) ×Ti(st−1 , at−1 i , at−1 j , st ) ot j Oj(st , at−1 i , at−1 j , ot j) ×τ(SEθt j (bt−1 j,l−1, at−1 j , ot j) − bt j,l−1) (1) donde β es la constante de normalización, τ es 1 si su argumento es 0, de lo contrario es 0, Pr(at−1 j |θt−1 j,l−1) es la probabilidad de que at−1 j sea Bayes racional para el agente descrito por el modelo θt−1 j,l−1, y SE(·) es una abreviatura para la actualización de creencias. Para una versión de la actualización de creencias cuando el modelo js es subintencional, consulte [9]. Si el agente j también se modela como un I-POMDP, entonces la actualización de creencias invoca la actualización de creencias de j (a través del término SEθt j ( bt−1 j,l−1 , at−1 j , ot j)), lo que a su vez podría invocar la actualización de creencias de is y así sucesivamente. Esta recursión en la anidación de creencias llega al nivel 0. En este nivel, la actualización de creencias del agente se reduce a una actualización de creencias de un POMDP. Para ilustraciones de la actualización de creencias, detalles adicionales sobre I-POMDPs y cómo se comparan con otros marcos multiagentes, consulte [9]. Iteración de Valor Cada estado de creencia en un I-POMDP finitamente anidado tiene un valor asociado que refleja la máxima ganancia que el agente puede esperar en este estado de creencia: Un ( bi,l, θi ) = max ai∈Ai is∈ISi,l ERi(is, ai)bi,l(is)+ γ oi∈Ωi Pr(oi|ai, bi,l)Un−1 ( SEθi (bi,l, ai, oi), θi ) (2) donde, ERi(is, ai) = aj Ri(is, ai, aj)Pr(aj|mj,l−1) (ya que is = (s, mj,l−1)). La ecuación 2 es una base para la iteración de valor en I-POMDPs. La acción óptima del agente, a∗ i, para el caso de horizonte finito con descuento, es un elemento del conjunto de acciones óptimas para el estado de creencia, OPT(θi), definido como: OPT( bi,l, θi ) = argmax ai∈Ai is∈ISi,l ERi(is, ai)bi,l(is) +γ oi∈Ωi Pr(oi|ai, bi,l)Un ( SEθi (bi,l, ai, oi), θi ) (3) 3. Diagramas de influencia interactivos. Una extensión ingenua de los diagramas de influencia (IDs) a entornos poblados por múltiples agentes es posible tratando a los otros agentes como autómatas, representados mediante nodos de probabilidad. Sin embargo, este enfoque asume que las acciones de los agentes están controladas utilizando una distribución de probabilidad que no cambia con el tiempo. Los diagramas de influencia interactivos (I-IDs) adoptan un enfoque más sofisticado al generalizar los IDs para hacerlos aplicables a entornos compartidos con otros agentes que pueden actuar, observar y actualizar sus creencias. 3.1 Sintaxis Además de los nodos habituales de probabilidad, decisión y utilidad, los IIDs incluyen un nuevo tipo de nodo llamado nodo de modelo. Mostramos un nivel general l I-ID en la Fig. 1(a), donde el nodo del modelo (Mj,l−1) está representado con un hexágono. Observamos que la distribución de probabilidad sobre el nodo de probabilidad, S, y el nodo del modelo juntos representan la creencia del agente sobre sus estados interactivos. Además del modelo 1, el modelo de nivel 0 es un POMDP: las acciones de otros agentes se tratan como eventos exógenos y se incorporan en las funciones T, O y R. El Sexto Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) Figura 1: (a) Un nivel genérico l I-ID para el agente i situado con otro agente j. El hexágono es el nodo modelo (Mj,l−1) cuya estructura mostramos en (b). Los miembros del nodo modelo son ellos mismos I-ID (m1 j,l−1, m2 j,l−1; los diagramas no se muestran aquí por simplicidad) cuyos nodos de decisión se asignan a los nodos de probabilidad correspondientes (A1 j , A2 j). Dependiendo del valor del nodo, Mod[Mj], la distribución de cada uno de los nodos de probabilidad se asigna al nodo Aj. (c) El I-ID transformado con el nodo del modelo reemplazado por los nodos de probabilidad y las relaciones entre ellos. Los I-IDs difieren de los IDs al tener un enlace discontinuo (llamado enlace de política en [15]) entre el nodo del modelo y un nodo de probabilidad, Aj, que representa la distribución sobre las acciones de los otros agentes dada su modelo. En ausencia de otros agentes, el nodo del modelo y el nodo de probabilidad, Aj, desaparecen y los I-ID se convierten en ID tradicionales. El nodo del modelo contiene los modelos computacionales alternativos atribuidos por i al otro agente del conjunto, Θj,l−1 ∪ SMj, donde Θj,l−1 y SMj fueron definidos previamente en la Sección 2. Por lo tanto, un modelo en el nodo del modelo puede ser en sí mismo un I-ID o ID, y la recursión termina cuando un modelo es un ID o subintencional. Dado que el nodo del modelo contiene los modelos alternativos del otro agente como sus valores, su representación no es trivial. En particular, algunos de los modelos dentro del nodo son I-IDs que, al resolverse, generan la política óptima de los agentes en sus nodos de decisión. Cada nodo de decisión se asigna al nodo de probabilidad correspondiente, digamos A1 j, de la siguiente manera: si OPT es el conjunto de acciones óptimas obtenidas al resolver el I-ID (o ID), entonces Pr(aj ∈ A1 j) = 1 |OPT| si aj ∈ OPT, 0 en caso contrario. Tomando prestados los conocimientos de trabajos anteriores [8], observamos que el nodo del modelo y el enlace de política discontinua que lo conecta con el nodo de probabilidad, Aj, podrían representarse como se muestra en la Fig. 1(b). El nodo de decisión de cada nivel l − 1 I-ID se transforma en un nodo de probabilidad, como mencionamos anteriormente, de modo que las acciones con el mayor valor en el nodo de decisión se les asignan probabilidades uniformes en el nodo de probabilidad, mientras que al resto se les asigna probabilidad cero. Los diferentes nodos de probabilidad (A1 j , A2 j ), uno para cada modelo, y adicionalmente, el nodo de probabilidad etiquetado Mod[Mj] forman los padres del nodo de probabilidad, Aj. Por lo tanto, hay tantos nodos de acción (A1 j , A2 j ) en Mj,l−1 como el número de modelos en el soporte de las creencias del agente. La tabla de probabilidad condicional del nodo de probabilidad, Aj, es un multiplexor que asume la distribución de cada uno de los nodos de acción (A1 j , A2 j ) dependiendo del valor de Mod[Mj]. Los valores de Mod[Mj] denotan los diferentes modelos de j. En otras palabras, cuando Mod[Mj] tiene el valor m1 j,l−1, el nodo de probabilidad Aj asume la distribución del nodo A1 j, y Aj asume la distribución de A2 j cuando Mod[Mj] tiene el valor m2 j,l−1. La distribución sobre el nodo, Mod[Mj], es la creencia del agente sobre los modelos de j dado un estado físico. Para más agentes, tendremos tantos nodos de modelo como agentes haya. Observa que la Fig. 1(b) aclara la semántica del enlace de política y muestra cómo puede ser representado utilizando los enlaces de dependencia tradicionales. En la Fig. 1(c), mostramos el I-ID transformado cuando el nodo del modelo es reemplazado por los nodos de probabilidad y las relaciones entre ellos. A diferencia de la representación en [15], no hay enlaces de políticas especiales, sino que el I-ID está compuesto únicamente por aquellos tipos de nodos que se encuentran en IDs tradicionales y relaciones de dependencia entre los nodos. Esto permite que los I-IDs sean representados e implementados utilizando herramientas de aplicación convencionales que apuntan a los IDs. Ten en cuenta que podemos ver el nivel l I-ID como un NID. Específicamente, cada uno de los modelos del nivel l − 1 dentro del nodo del modelo son bloques en el NID (ver Fig. 2). Si el nivel l = 1, cada bloque es un ID tradicional, de lo contrario, si l > 1, cada bloque dentro del NID puede ser un NID en sí mismo. Ten en cuenta que dentro de los I-IDs (o IDs) en cada nivel, solo hay un nodo de decisión único. Por lo tanto, nuestro NID no contiene ningún MAID. Figura 2: Un nivel l I-ID representado como un NID. Las probabilidades asignadas a los bloques del NID son creencias sobre modelos js condicionados a un estado físico. 3.2 Solución La solución de un I-ID procede de manera ascendente y se implementa de forma recursiva. Comenzamos resolviendo los modelos de nivel 0, que, si son intencionales, son identificadores tradicionales. Sus soluciones proporcionan distribuciones de probabilidad sobre las acciones de los otros agentes, las cuales se ingresan en los nodos de probabilidad correspondientes encontrados en el nodo del modelo del nivel 1 I-ID. El mapeo de los nodos de decisión de los modelos de nivel 0 a los nodos de probabilidad se realiza de manera que las acciones con el mayor valor en el nodo de decisión se les asignan probabilidades uniformes en el nodo de probabilidad, mientras que el resto se les asigna probabilidad cero. Dadas las distribuciones sobre las acciones dentro de los diferentes nodos de probabilidad (uno para cada modelo del otro agente), el I-ID de nivel 1 se transforma como se muestra en la Fig. 1(c). Durante la transformación, la tabla de probabilidad condicional (CPT) del nodo, Aj, se llena de tal manera que el nodo asume la distribución de cada uno de los nodos de probabilidad dependiendo del valor del nodo, Mod[Mj]. Como mencionamos anteriormente, los valores del nodo Mod[Mj] denotan los diferentes modelos del otro agente, y su distribución es la creencia del agente sobre los modelos de j condicionados al estado físico. El nivel 1 I-ID transformado es un ID tradicional que puede ser resuelto como us816 The Sixth Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) (a) (b) Figura 3: (a) Un I-DID genérico de dos niveles de tiempo para el agente i en un entorno con otro agente j. Observa el enlace de actualización del modelo punteado que denota la actualización de los modelos de j y la distribución de los modelos a lo largo del tiempo. (b) La semántica del enlace de actualización del modelo utilizando el método estándar de maximización de utilidad esperada [18]. Este procedimiento se lleva a cabo hasta el nivel l I-ID cuya solución proporciona el conjunto no vacío de acciones óptimas que el agente debe realizar dada su creencia. Ten en cuenta que, al igual que los IDs, los I-IDs son adecuados para la toma de decisiones en línea cuando se conoce la creencia actual de los agentes. 4. Diagramas de influencia dinámica interactivos (I-DIDs) Los diagramas de influencia dinámica interactivos (I-DIDs) extienden los I-IDs (y NIDs) para permitir la toma de decisiones secuencial a lo largo de varios pasos de tiempo. Así como los DIDs son representaciones gráficas estructuradas de POMDPs, los I-DIDs son los análogos gráficos en línea para I-POMDPs finitamente anidados. Los I-DIDs pueden ser utilizados para optimizar sobre una mirada anticipada finita dadas las creencias iniciales mientras se interactúa con otros agentes, posiblemente similares. 4.1 Sintaxis Representamos un I-DID de dos cortes temporales en la Fig. 3(a). Además de los nodos del modelo y el enlace de política discontinuo, lo que diferencia a un I-DID de un DID es el enlace de actualización del modelo mostrado como una flecha punteada en la Fig. 3(a). Explicamos la semántica del nodo del modelo y el enlace de la política en la sección anterior; a continuación, describimos las actualizaciones del modelo. La actualización del nodo del modelo con el tiempo implica dos pasos: primero, dados los modelos en el tiempo t, identificamos el conjunto actualizado de modelos que residen en el nodo del modelo en el tiempo t + 1. Recuerda de la Sección 2 que el modelo intencional de un agente incluye sus creencias. Debido a que los agentes actúan y reciben observaciones, sus modelos se actualizan para reflejar sus creencias cambiadas. Dado que el conjunto de acciones óptimas para un modelo podría incluir todas las acciones, y el agente podría recibir cualquiera de las |Ωj| posibles observaciones, el conjunto actualizado en el paso de tiempo t + 1 tendrá como máximo |Mt j,l−1||Aj||Ωj| modelos. Aquí, |Mt j,l−1| es el número de modelos en el paso de tiempo t, |Aj| y |Ωj| son los espacios más grandes de acciones y observaciones respectivamente, entre todos los modelos. Segundo, calculamos la nueva distribución sobre los modelos actualizados dados la distribución original y la probabilidad de que el agente realice la acción y reciba la observación que llevó al modelo actualizado. Estos pasos son parte de la actualización de creencias del agente formalizada utilizando la Ecuación 1. En la Fig. 3(b), mostramos cómo se implementa el enlace de actualización del modelo punteado en el I-DID. Si cada uno de los dos modelos de nivel l − 1 atribuidos a j en el paso de tiempo t resulta en una acción, y j podría hacer una de dos posibles observaciones, entonces el nodo del modelo en el paso de tiempo t + 1 contiene cuatro modelos actualizados (mt+1,1 j,l−1 ,mt+1,2 j,l−1 , mt+1,3 j,l−1 , y mt+1,4 j,l−1 ). Estos modelos difieren en sus creencias iniciales, cada una de las cuales es el resultado de j actualizar sus creencias debido a su acción y una posible observación. Los nodos de decisión en cada uno de los I-DIDs o DIDs que representan los modelos de nivel inferior se asignan a la Figura 4 correspondiente: I-DID transformado con los nodos del modelo y el enlace de actualización del modelo reemplazados por los nodos de probabilidad y las relaciones (en negrita) de los nodos de probabilidad, como se mencionó anteriormente. A continuación, describimos cómo se calcula la distribución sobre el conjunto actualizado de modelos (la distribución sobre el nodo de probabilidad Mod[Mt+1 j ] en Mt+1 j,l−1). La probabilidad de que el modelo actualizado js sea, digamos mt+1,1 j,l−1, depende de la probabilidad de que j realice la acción y reciba la observación que condujo a este modelo, y de la distribución previa sobre los modelos en el paso de tiempo t. Dado que el nodo de probabilidad At j asume la distribución de cada uno de los nodos de acción basados en el valor de Mod[Mt j], la probabilidad de la acción está dada por este nodo de probabilidad. Para obtener la probabilidad de una observación posible js, introducimos el nodo de probabilidad Oj, que dependiendo del valor de Mod[Mt j], asume la distribución del nodo de observación en el modelo de nivel inferior denominado Mod[Mt j]. Dado que la probabilidad de las observaciones js depende del estado físico y de las acciones conjuntas de ambos agentes, el nodo Oj está vinculado con St+1, At j y At i. De manera análoga a At j, la tabla de probabilidad condicional de Oj también es un multiplexor modulado por Mod[Mt j]. Finalmente, la distribución de los modelos previos en el tiempo t se obtiene a partir del nodo de probabilidad, Mod[Mt j ] en Mt j,l−1. Por consiguiente, los nodos de probabilidad, Mod[Mt j], At j y Oj, forman los padres de Mod[Mt+1 j] en Mt+1 j,l−1. Ten en cuenta que el enlace de actualización del modelo puede ser reemplazado por los enlaces de dependencia entre los nodos de probabilidad que constituyen los nodos del modelo en las dos etapas temporales. En la Fig. 4 mostramos los dos cortes temporales I-DID con los nodos del modelo reemplazados por los nodos de probabilidad y las relaciones entre ellos. Los nodos de probabilidad y los enlaces de dependencia que no están en negrita son estándar, generalmente encontrados en DIDs. La expansión del I-DID a lo largo de más pasos de tiempo requiere la repetición de los dos pasos de actualizar el conjunto de modelos que forman el 2. Nota que Oj representa la observación j en el tiempo t + 1. El Sexto Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 817 valores del nodo del modelo y añadiendo las relaciones entre los nodos de probabilidad, tantas veces como enlaces de actualización del modelo haya. Observamos que el conjunto posible de modelos del otro agente j crece exponencialmente con el número de pasos de tiempo. Por ejemplo, después de T pasos, puede haber como máximo |Mt=1 j,l−1|(|Aj||Ωj|)T −1 modelos candidatos residiendo en el nodo del modelo. 4.2 Solución Análoga a los I-ID, la solución a un I-DID de nivel l para el agente i expandido a lo largo de T pasos de tiempo puede llevarse a cabo de forma recursiva. Con el propósito de ilustración, dejemos l=1 y T=2. El método de solución utiliza la técnica estándar de anticipación, proyectando las secuencias de acciones y observaciones de los agentes hacia adelante desde el estado de creencia actual [17], y encontrando las posibles creencias que podría tener en el siguiente paso temporal. Dado que el agente i también tiene una creencia sobre los modelos de j, la búsqueda anticipada incluye descubrir los posibles modelos que j podría tener en el futuro. Por consiguiente, cada uno de los modelos subintencionales o de nivel 0 de js (representados utilizando un DID estándar) en el primer paso de tiempo debe resolverse para obtener su conjunto óptimo de acciones. Estas acciones se combinan con el conjunto de observaciones posibles que j podría hacer en ese modelo, lo que resulta en un conjunto actualizado de modelos candidatos (que incluyen las creencias actualizadas) que podrían describir el comportamiento de j. Las creencias sobre este conjunto actualizado de modelos candidatos se calculan utilizando los métodos estándar de inferencia que utilizan las relaciones de dependencia entre los nodos del modelo, como se muestra en la Figura 3(b). Observamos la naturaleza recursiva de esta solución: al resolver el agente de nivel 1 I-DID, los DIDs de nivel 0 deben ser resueltos. Si el anidamiento de los modelos es más profundo, todos los modelos en todos los niveles, comenzando desde el nivel 0, se resuelven de manera ascendente. Breve descripción del algoritmo recursivo para resolver el agente es el Algoritmo para resolver I-DID. Entrada: nivel l ≥ 1 I-ID o nivel 0 ID, T Fase de Expansión 1. Para t desde 1 hasta T − 1, hacer 2. Si l ≥ 1, entonces llenar Mt+1 j,l−1 3. Para cada mt j en el rango (Mt j,l−1) hacer 4. Llame recursivamente al algoritmo con el l - 1 I-ID (o ID) que representa mt j y el horizonte, T - t + 1 5. Mapea el nodo de decisión del I-ID (o ID) resuelto, OPT(mt j), a un nodo de probabilidad Aj 6. Para cada aj en OPT(mt j) hacer 7. Para cada oj en Oj (parte de mt j) hacer 8. Actualizar la creencia js, bt+1 j ← SE(bt j, aj, oj) 9. mt+1 j ← Nuevo I-ID (o ID) con bt+1 j como la creencia inicial 10. Rango(Mt+1 j,l−1) ∪ ← {mt+1 j } 11. Añadir el nodo del modelo, Mt+1 j,l−1, y los enlaces de dependencia entre Mt j,l−1 y Mt+1 j,l−1 (mostrados en la Fig. 3(b)) 12. Agregue los nodos de probabilidad, decisión y utilidad para la franja de tiempo t + 1 y los enlaces de dependencia entre ellos 13. Establezca las CPTs para cada nodo de probabilidad y nodo de utilidad de la Fase de Mirada Adelante 14. Aplica el método estándar de anticipación y respaldo para resolver la Figura 5 expandida de I-DID: Algoritmo para resolver un nivel l ≥ 0 I-DID. Nivel l I-DID expandido durante T pasos de tiempo con otro agente j en la Figura 5. Adoptamos un enfoque de dos fases: Dado un I-ID de nivel l (descrito previamente en la Sección 3) con todos los modelos de niveles inferiores representados también como I-IDs o IDs (si es nivel 0), el primer paso es expandir el I-ID de nivel l a lo largo de T pasos de tiempo añadiendo los enlaces de dependencia y las tablas de probabilidad condicional para cada nodo. Nos enfocamos especialmente en establecer y poblar los nodos del modelo (líneas 3-11). Ten en cuenta que Range(·) devuelve los valores (modelos de nivel inferior) de la variable aleatoria dada como entrada (nodo del modelo). En la segunda fase, utilizamos una técnica estándar de anticipación proyectando las secuencias de acción y observación durante T pasos de tiempo en el futuro, y respaldando los valores de utilidad de las creencias alcanzables. Similar a los I-IDs, los I-DIDs se reducen a DIDs en ausencia de otros agentes. Como mencionamos anteriormente, los modelos de nivel 0 son los DIDs tradicionales. Sus soluciones proporcionan distribuciones de probabilidad sobre las acciones del agente modelado en ese nivel a los I-DIDs en el nivel 1. Dadas las distribuciones de probabilidad sobre las acciones de otros agentes, los IDIDs de nivel 1 pueden resolverse como DIDs y proporcionar distribuciones de probabilidad a modelos de niveles superiores. Suponga que el número de modelos considerados en cada nivel está limitado por un número, M. Resolver un I-DID de nivel l es equivalente a resolver O(Ml) DIDs. Para ilustrar la utilidad de los I-DIDs, los aplicamos a tres dominios de problemas. Describimos, en particular, la formulación del I-DID y las prescripciones óptimas obtenidas al resolverlo. 5.1 Seguimiento-Liderazgo en el Problema del Tigre Multiagente Comenzamos nuestras ilustraciones del uso de I-IDs e I-DIDs con una versión ligeramente modificada del problema del tigre multiagente discutido en [9]. El problema tiene dos agentes, cada uno de los cuales puede abrir la puerta derecha (OR), la puerta izquierda (OL) o escuchar (L). Además de escuchar gruñidos (desde la izquierda (GL) o desde la derecha (GR)) cuando escuchan, los agentes también escuchan chirridos (desde la izquierda (CL), desde la derecha (CR) o sin chirridos (S)), que indican ruidosamente que los otros agentes están abriendo una de las puertas. Cuando se abre cualquier puerta, el tigre persiste en su ubicación original con una probabilidad del 95%. El agente i escucha gruñidos con una fiabilidad del 65% y crujidos con una fiabilidad del 95%. El agente J, por otro lado, escucha gruñidos con una fiabilidad del 95%. Por lo tanto, la configuración es tal que el agente i escucha la apertura de puertas del agente j de manera más confiable que los rugidos de los tigres. Esto sugiere que podría usar acciones de JavaScript como indicación de la ubicación del tigre, como discutimos a continuación. Las preferencias de cada agente son como en el juego de un solo agente discutido en [13]. Las funciones de transición, observación y recompensa se muestran en [16]. Un buen indicador de la utilidad de los métodos normativos para la toma de decisiones como los I-DIDs es la aparición de comportamientos sociales realistas en sus prescripciones. En entornos del persistente problema del tigre multiagente que reflejan situaciones del mundo real, demostramos la seguidismo entre los agentes y, como se muestra en [15], el engaño entre agentes que creen que están en una relación de tipo seguidor-líder. En particular, analizamos las condiciones situacionales y epistemológicas suficientes para su surgimiento. El comportamiento de seguidores, por ejemplo, resulta del agente conociendo sus propias debilidades, evaluando las fortalezas, preferencias y posibles comportamientos del otro, y dándose cuenta de que es mejor para él seguir las acciones de los demás para maximizar sus ganancias. Consideremos una configuración particular del problema del tigre en la que el agente i cree que las preferencias de j están alineadas con las suyas: ambos solo quieren obtener el oro, y la audición de j es más confiable en comparación consigo mismo. Como ejemplo, supongamos que j, al escuchar, puede discernir la ubicación del tigre el 95% de las veces en comparación con su precisión del 65%. Además, el agente i no tiene ninguna información inicial sobre la ubicación de los tigres. En otras palabras, la creencia anidada de un solo nivel, bi,1, asigna 0.5 a cada una de las dos ubicaciones del tigre. Además, considera dos modelos de j, que difieren en los niveles iniciales de creencias planas de j. Esto se representa en el nivel 1 I-ID mostrado en la Fig. 6(a). Según un modelo, j asigna una probabilidad de 0.9 a que el tigre esté detrás de la puerta izquierda, mientras que el otro 818 The Sixth Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) Figura 6: (a) Nivel 1 I-ID del agente i, (b) dos IDs de nivel 0 del agente j cuyos nodos de decisión están mapeados a los nodos de probabilidad, A1 j , A2 j , en (a). El modelo asigna 0.1 a esa ubicación (ver Fig. 6(b)). El agente i está indeciso sobre estos dos modelos de j. Si variamos la capacidad auditiva, y resolvemos el nivel 1 correspondiente I-ID expandido en tres pasos de tiempo, obtenemos las políticas de comportamiento normativas mostradas en la Figura 7 que exhiben comportamiento de seguidores. Si la probabilidad de escuchar correctamente los gruñidos es de 0.65, entonces, como se muestra en la política en la Fig. 7(a), i comienza a seguir condicionalmente las acciones de j: i abre la misma puerta que j abrió anteriormente si su evaluación propia de la ubicación de los tigres confirma la elección de j. Si i pierde la capacidad de interpretar correctamente los gruñidos por completo, sigue ciegamente a j y abre la misma puerta que j abrió anteriormente (Fig. 7(b)). Figura 7: Emergencia de (a) seguidores condicionales, y (b) seguidores ciegos en el problema del tigre. Los comportamientos de interés están en negrita. * es un comodín y representa cualquiera de las observaciones. Observamos que un solo nivel de anidación de creencias, creencias sobre los modelos de los demás, fue suficiente para que surgiera el seguidismo en el problema del tigre. Sin embargo, los requisitos epistemológicos para el surgimiento del liderazgo son más complejos. Para que un agente, digamos j, surja como líder, primero debe surgir el seguidismo en el otro agente i. Como mencionamos anteriormente, si i está seguro de que sus preferencias son idénticas a las de j, y cree que j tiene un mejor sentido del oído, i seguirá las acciones de j con el tiempo. El agente j emerge como líder si cree que lo seguiré, lo que implica que la creencia de j debe estar anidada dos niveles de profundidad para permitirle reconocer su papel de liderazgo. Darse cuenta de que lo seguiré presenta a J una oportunidad para influir en sus acciones en beneficio del bien colectivo o de su interés propio solamente. Por ejemplo, en el problema del tigre, consideremos una situación en la que si tanto i como j abren la puerta correcta, entonces cada uno recibe un pago de 20 que es el doble del original. Si solo j selecciona la puerta correcta, obtiene la recompensa de 10. Por otro lado, si ambos agentes eligen la puerta incorrecta, sus penalizaciones se reducen a la mitad. En este entorno, es tanto en el mejor interés de j como en el beneficio colectivo que utilice su experiencia para seleccionar la puerta correcta, y así ser un buen líder. Sin embargo, considera un problema ligeramente diferente en el que j se beneficia de la pérdida de i y es penalizado si i se beneficia. Específicamente, dejemos que la ganancia se reste de js, indicando que j es antagonista hacia i: si j elige la puerta correcta y i la incorrecta, entonces la pérdida de 100 se convierte en la ganancia de js. El agente J cree que yo incorrectamente pienso que las preferencias de J son aquellas que promueven el bien colectivo y que comienza creyendo con un 99% de confianza dónde está el tigre. Dado que creo que mis preferencias son similares a las de j, y que j comienza creyendo casi con certeza que una de las dos ubicaciones es la correcta (dos modelos de nivel 0 de j), comenzaré siguiendo las acciones de j. Mostramos la política normativa para resolver su I-DID anidado individualmente durante tres pasos de tiempo en la Fig. 8(a). La política demuestra que seguiré ciegamente las acciones de js. Dado que el tigre persiste en su ubicación original con una probabilidad del 0.95, seleccionaré nuevamente la misma puerta. Si j comienza el juego con una probabilidad del 99% de que el tigre esté a la derecha, resolver su I-DID anidado dos niveles profundamente, resulta en la política mostrada en la Fig. 8(b). Aunque j está casi seguro de que OL es la acción correcta, comenzará seleccionando OR, seguido de OL. La intención del agente js es engañar a i, a quien cree que seguirá las acciones de js, para así obtener $110 en el segundo paso de tiempo, lo cual es más de lo que j ganaría si fuera honesto. Figura 8: Emergencia del engaño entre agentes en el problema del tigre. Los comportamientos de interés están en negrita. * denota como antes. (a) El agente es una política que demuestra que seguirá ciegamente las acciones de js. (b) Aunque j está casi seguro de que el tigre está a la derecha, comenzará seleccionando OR, seguido de OL, para engañar a i. 5.2 Altruismo y reciprocidad en el problema del bien público El problema del bien público (PG) [7], consiste en un grupo de M agentes, cada uno de los cuales debe contribuir con algún recurso a una olla pública o guardarlo para sí mismos. Dado que los recursos aportados al fondo público se comparten entre todos los agentes, son menos valiosos para el agente cuando están en el fondo público. Sin embargo, si todos los agentes eligen contribuir con sus recursos, entonces la recompensa para cada agente es mayor que si nadie contribuye. Dado que un agente recibe su parte del bote público independientemente de si ha contribuido o no, la acción dominante es que cada agente no contribuya y en su lugar se aproveche de las contribuciones de los demás. Sin embargo, los comportamientos de los jugadores humanos en simulaciones empíricas del problema de PG difieren de las predicciones normativas. Los experimentos revelan que muchos jugadores inicialmente contribuyen con una gran cantidad al bote público, y continúan contribuyendo cuando se juega el problema del PG repetidamente, aunque en cantidades decrecientes [4]. Muchos de estos experimentos [5] informan que un pequeño grupo central de jugadores contribuye persistentemente al bote público incluso cuando todos los demás están desertando. Estos experimentos también revelan que los jugadores que contribuyen persistentemente tienen preferencias altruistas o recíprocas que coinciden con la cooperación esperada de los demás. Para simplificar, asumimos que el juego se juega entre M = 2 agentes, i y j. Que cada agente esté inicialmente dotado con una cantidad de recursos XT. Mientras que la formulación clásica del juego PG permite que cada agente contribuya con cualquier cantidad de recursos (≤ XT) al bote público, simplificamos el espacio de acciones al permitir dos acciones posibles. Cada agente puede elegir contribuir (C) una cantidad fija de recursos, o no contribuir. La última acción es deThe Sixth Intl. La Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 819 fue señalada como defectuosa (D). Suponemos que las acciones no son observables por otros. El valor de los recursos en la bolsa pública se descuenta por ci para cada agente i, donde ci es el retorno privado marginal. Suponemos que ci < 1 para que el agente no se beneficie lo suficiente como para contribuir al bote público en beneficio propio. Simultáneamente, ciM > 1, lo que hace que la contribución colectiva sea óptima de Pareto. i/j C D C 2ciXT , 2cjXT ciXT − cp, XT + cjXT − P D XT + ciXT − P, cjXT − cp XT , XT Tabla 1: El juego PG de un solo disparo con castigo. Para fomentar las contribuciones, los agentes contribuyentes castigan a los que no colaboran pero incurren en un pequeño costo por administrar el castigo. Sea P la sanción impuesta al agente que se desvía y cp el costo no nulo de castigar al agente contribuyente. Para simplificar, asumimos que el costo de castigar es el mismo para ambos agentes. El juego PG de un solo disparo con castigo se muestra en la Tabla 1. Si ci = cj, cp > 0, y si P > XT − ciXT, entonces la deserción ya no es una acción dominante. Si P < XT − ciXT, entonces la deserción es la acción dominante para ambos. Si P = XT − ciXT, entonces el juego no es resoluble por dominancia. Figura 9: (a) Nivel 1 I-ID del agente i, (b) IDs de nivel 0 del agente j con nodos de decisión mapeados a los nodos de probabilidad, A1 j y A2 j, en (a). Formulamos una versión secuencial del problema PG con castigo desde la perspectiva del agente i. Aunque en el juego de PG repetido, la cantidad en la olla pública se revela a todos los agentes después de cada ronda de acciones, asumimos en nuestra formulación que está oculta para los agentes. Cada agente puede contribuir con una cantidad fija, xc, o desertar. Un agente al realizar una acción recibe una observación de abundante (PY) o escasa (MR) simbolizando el estado de la olla pública. Ten en cuenta que las observaciones también son indirectamente indicativas de las acciones del agente js porque el estado del bote público es influenciado por ellas. La cantidad de recursos en el agente es una olla privada, es perfectamente observable para i. Los pagos son análogos a la Tabla 1. Tomando prestado de las investigaciones empíricas del problema PG [5], construimos IDs de nivel 0 para j que modelan tipos altruistas y no altruistas (Fig. 9(b)). Específicamente, nuestro agente altruista tiene un alto retorno privado marginal (cj está cerca de 1) y no castiga a los demás que incumplen. Que xc = 1 y el agente de nivel 0 sea castigado la mitad de las veces que se desvía. Con una acción restante, ambos tipos de agentes eligen contribuir para evitar ser castigados. Con dos acciones por delante, el tipo altruista elige contribuir, mientras que el otro defecta. Esto se debe a que cj para el tipo altruista es cercano a 1, por lo tanto, el castigo esperado, 0.5P > (1 − cj), que el tipo altruista evita. Debido a que cj para el tipo no altruista es menor, prefiere no contribuir. Con tres pasos por delante, el agente altruista contribuye para evitar el castigo (0.5P > 2(1 − cj)), mientras que el tipo no altruista se desentiende. Para más de tres pasos, mientras el agente altruista continúa contribuyendo al bote público dependiendo de qué tan cercano esté su retorno privado marginal a 1, el tipo no altruista prescribe la deserción. Analizamos las decisiones de un agente altruista i modelado usando un nivel 1 I-DID expandido en 3 pasos de tiempo. i atribuye los dos modelos de nivel 0, mencionados anteriormente, a j (ver Fig. 9). Si creo con una probabilidad de 1 que j es altruista, elijo contribuir en cada uno de los tres pasos. Este comportamiento persiste cuando i no está al tanto de si j es altruista (Fig. 10(a)), y cuando i asigna una alta probabilidad a que j sea del tipo no altruista. Sin embargo, cuando i cree con una probabilidad de 1 que j no es altruista y por lo tanto seguramente va a traicionar, i elige traicionar para evitar ser castigado y porque su beneficio privado marginal es menor que 1. Estos resultados demuestran que el comportamiento de nuestro tipo altruista se asemeja al encontrado experimentalmente. El agente de nivel 1 no altruista elige desertar independientemente de lo altruista que crea que sea el otro agente. Analizamos el comportamiento de un tipo de agente recíproco que se ajusta a la cooperación o defección esperada. El retorno privado marginal del tipo recíproco es similar al del tipo no altruista, sin embargo, obtiene una mayor recompensa cuando su acción es similar a la del otro. Consideramos el caso en el que el agente recíproco i no está seguro de si j es altruista y cree que es probable que el bote público esté medio lleno. Para esta creencia previa, yo elijo defectar. Al recibir una observación de abundancia, decide contribuir, mientras que una observación de escasez lo hace defectar (Fig. 10(b)). Esto se debe a que una observación de abundancia indica que es probable que la olla esté más llena que a la mitad, lo cual resulta de la acción de js para contribuir. Por lo tanto, entre los dos modelos atribuidos a j, es probable que su tipo sea altruista, lo que hace probable que j contribuya nuevamente en el próximo paso de tiempo. El agente i, por lo tanto, elige contribuir para corresponder la acción de js. Un razonamiento análogo lleva a uno a defectar cuando observa una olla escasa. Con una acción por hacer, creo que si j contribuye, también elegiré contribuir para evitar castigo independientemente de sus observaciones. Figura 10: (a) Un agente altruista de nivel 1 siempre contribuye. (b) Un agente recíproco i comienza defraudando y luego elige contribuir o defraudar basándose en su observación de abundancia (indicando que j es probablemente altruista) o escasez (j no es altruista). 5.3 Estrategias en el póker de dos jugadores El póker es un popular juego de cartas de suma cero que ha recibido mucha atención en la comunidad de investigación de IA como un banco de pruebas [2]. El póker se juega entre M ≥ 2 jugadores, en el que cada jugador recibe una mano de cartas de una baraja. Si bien existen varias variantes de póker con diferentes niveles de complejidad, consideramos una versión simple en la que cada jugador tiene tres turnos durante los cuales el jugador puede intercambiar una carta (E), mantener la mano existente (K), retirarse (F) y abandonar el juego, o apostar (C), lo que requiere que todos los jugadores muestren sus manos. Para mantener las cosas simples, dejemos que M = 2, y que cada jugador reciba una mano compuesta por una sola carta sacada del mismo palo. Por lo tanto, durante un enfrentamiento, el jugador que tenga la carta numéricamente más alta (el 2 es el más bajo, el as es el más alto) gana el bote. Durante un intercambio de cartas, la carta descartada se coloca ya sea en la pila L, indicando al otro agente que era una carta de número bajo menor que 8, o en la 820 The Sixth Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) H, indicando que la carta tenía un rango mayor o igual a 8. Ten en cuenta que, por ejemplo, si se descarta una carta de número bajo, la probabilidad de recibir una carta baja a cambio se reduce. Mostramos el nivel 1 I-ID para el Poker simplificado de dos jugadores en la Fig. 11. Consideramos dos modelos (tipos de personalidad) del agente j. El tipo conservador cree que es probable que su oponente tenga una carta de alto número en su mano. Por otro lado, el agente agresivo j cree con alta probabilidad que su oponente tiene una carta de número más bajo. Por lo tanto, los dos tipos difieren en sus creencias sobre la mano de sus oponentes. En ambos estos modelos de nivel 0, se asume que el oponente realiza sus acciones siguiendo una distribución fija y uniforme. Con tres acciones restantes, independientemente de su mano (a menos que sea un as), el agente agresivo elige intercambiar su carta, con la intención de mejorar su mano actual. Esto se debe a que cree que el otro tiene una carta baja, lo que mejora sus posibilidades de obtener una carta alta durante el intercambio. El agente conservador elige quedarse con su carta, sin importar su mano, porque considera que sus posibilidades de obtener una carta alta son escasas, ya que cree que su oponente tiene una. Figura 11: (a) Nivel 1 I-ID del agente i. La observación revela información sobre la mano de js del paso de tiempo anterior, (b) IDs de nivel 0 del agente j cuyos nodos de decisión están mapeados a los nodos de probabilidad, A1 j , A2 j , en (a). La política de un agente de nivel 1 i que cree que cada carta, excepto la suya, tiene la misma probabilidad de estar en su mano (tipo de personalidad neutral) y j podría ser de tipo agresivo o conservador, se muestra en la Figura 12. Su mano contiene la carta numerada 8. El agente comienza por mantener su carta. Al ver que j no intercambió una carta (N), i cree con probabilidad 1 que j es conservador y, por lo tanto, mantendrá sus cartas. i responde manteniendo su carta o intercambiándola porque j tiene igual probabilidad de tener una carta más baja o más alta. Si observo que j descartó su carta en la pila L o H, creo que j es agresivo. Al observar a L, i se da cuenta de que j tenía una carta baja, y es probable que tenga una carta alta después de su intercambio. Dado que la probabilidad de recibir una carta baja es alta en este momento, i decide quedarse con su carta. Al observar la carta H, creyendo que la probabilidad de recibir una carta de número alto es alta, i decide intercambiar su carta. En el paso final, i decide llamar independientemente de su historial de observaciones porque su creencia de que j tiene una carta más alta no es lo suficientemente alta como para concluir que es mejor retirarse y renunciar al pago. Esto se debe en parte al hecho de que una observación de, digamos, L, restablece las creencias del agente en el paso de tiempo anterior sobre las cartas de números bajos solamente. 6. DISCUSIÓN Mostramos cómo los DIDs pueden ser extendidos a los I-DIDs que permiten la toma de decisiones secuenciales en línea en entornos multiagentes inciertos. Nuestra representación gráfica de I-DIDs mejora la Figura 12 anterior: Un agente de nivel 1 es una política de tres pasos en el problema de Poker. i comienza creyendo que j tiene la misma probabilidad de ser agresivo o conservador y podría tener cualquier carta en su mano con igual probabilidad. Trabaja significativamente al ser más transparente, semánticamente claro y capaz de ser resuelto utilizando algoritmos estándar que apuntan a los DIDs. Los I-DIDs extienden los NIDs para permitir la toma de decisiones secuenciales a lo largo de múltiples pasos de tiempo en presencia de otros agentes que interactúan. Los I-DIDs pueden ser vistos como representaciones gráficas concisas para IPOMDPs que proporcionan una forma de explotar la estructura del problema y llevar a cabo la toma de decisiones en línea a medida que el agente actúa y observa dadas sus creencias previas. Actualmente estamos investigando formas de resolver I-DIDs aproximadamente con límites demostrables en la calidad de la solución. Agradecimiento: Agradecemos a Piotr Gmytrasiewicz por algunas discusiones útiles relacionadas con este trabajo. El primer autor desea agradecer el apoyo de una beca UGARF. 7. REFERENCIAS [1] R. J. Aumann. Epistemología interactiva i: Conocimiento. Revista Internacional de Teoría de Juegos, 28:263-300, 1999. [2] D. Billings, A. Davidson, J. Schaeffer y D. Szafron. El desafío del póker. AIJ, 2001. [3] A. Brandenburger y E. Dekel. Jerarquías de creencias y conocimiento común. Revista de Teoría Económica, 59:189-198, 1993. [4] C. Camerer. Teoría de juegos conductual: Experimentos en interacción estratégica. Princeton University Press, 2003. [5] E. Fehr y S. Gachter. Cooperación y castigo en experimentos de bienes públicos. Revista Económica Americana, 90(4):980-994, 2000. [6] D. Fudenberg y D. K. Levine. La Teoría del Aprendizaje en los Juegos. MIT Press, 1998. [7] D. Fudenberg y J. Tirole. Teoría de juegos. MIT Press, 1991. [8] Y. Gal y A. Pfeffer. Un lenguaje para modelar los procesos de toma de decisiones de agentes en juegos. En AAMAS, 2003. [9] P. Gmytrasiewicz y P. Doshi. Un marco para la planificación secuencial en entornos multiagentes. JAIR, 24:49-79, 2005. [10] P. Gmytrasiewicz y E. Durfee. Coordinación racional en entornos multiagente. JAAMAS, 3(4):319-350, 2000. [11] J. C. Harsanyi. Juegos con información incompleta jugados por jugadores bayesianos. Ciencia de la Gestión, 14(3):159-182, 1967. [12] R. A. Howard y J. E. Matheson. Diagramas de influencia. En R. A. Howard y J. E. Matheson, editores, Los Principios y Aplicaciones del Análisis de Decisiones. Grupo de Decisiones Estratégicas, Menlo Park, CA 94025, 1984. [13] L. Kaelbling, M. Littman y A. Cassandra. Planificación y actuación en dominios estocásticos parcialmente observables. Revista de Inteligencia Artificial, 2, 1998. [14] D. Koller y B. Milch. Diagramas de influencia multiagente para representar y resolver juegos. En IJCAI, páginas 1027-1034, 2001. [15] K. Polich y P. Gmytrasiewicz. Diagramas de influencia interactivos y dinámicos. En el taller GTDT, AAMAS, 2006. [16] B. Rathnas, P. Doshi y P. J. Gmytrasiewicz. Soluciones exactas para POMDP interactivos utilizando equivalencia conductual. En la Conferencia de Agentes Autónomos y Sistemas Multiagente (AAMAS), 2006. [17] S. Russell y P. Norvig. Inteligencia Artificial: Un Enfoque Moderno (Segunda Edición). Prentice Hall, 2003. [18] R. D. Shachter. \n\nPrentice Hall, 2003. [18] R. D. Shachter. Evaluando diagramas de influencia. Investigación de Operaciones, 34(6):871-882, 1986. [19] D. Suryadi y P. Gmytrasiewicz. Aprendiendo modelos de otros agentes utilizando diagramas de influencia. En UM, 1999. El Sexto Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 821 ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "networks of influence diagrams": {
            "translated_key": "redes de diagramas de influencia",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Graphical Models for Online Solutions to Interactive POMDPs Prashant Doshi Dept.",
                "of Computer Science University of Georgia Athens, GA 30602, USA pdoshi@cs.uga.edu Yifeng Zeng Dept. of Computer Science Aalborg University DK-9220 Aalborg, Denmark yfzeng@cs.aau.edu Qiongyu Chen Dept. of Computer Science National Univ. of Singapore 117543, Singapore chenqy@comp.nus.edu.sg ABSTRACT We develop a new graphical representation for interactive partially observable Markov decision processes (I-POMDPs) that is significantly more transparent and semantically clear than the previous representation.",
                "These graphical models called interactive dynamic influence diagrams (I-DIDs) seek to explicitly model the structure that is often present in real-world problems by decomposing the situation into chance and decision variables, and the dependencies between the variables.",
                "I-DIDs generalize DIDs, which may be viewed as graphical representations of POMDPs, to multiagent settings in the same way that I-POMDPs generalize POMDPs.",
                "I-DIDs may be used to compute the policy of an agent online as the agent acts and observes in a setting that is populated by other interacting agents.",
                "Using several examples, we show how I-DIDs may be applied and demonstrate their usefulness.",
                "Categories and Subject Descriptors I.2.11 [Distributed Artificial Intelligence]: Multiagent Systems General Terms Theory 1.",
                "INTRODUCTION Interactive partially observable Markov decision processes (IPOMDPs) [9] provide a framework for sequential decision-making in partially observable multiagent environments.",
                "They generalize POMDPs [13] to multiagent settings by including the other agents computable models in the state space along with the states of the physical environment.",
                "The models encompass all information influencing the agents behaviors, including their preferences, capabilities, and beliefs, and are thus analogous to types in Bayesian games [11].",
                "I-POMDPs adopt a subjective approach to understanding strategic behavior, rooted in a decision-theoretic framework that takes a decision-makers perspective in the interaction.",
                "In [15], Polich and Gmytrasiewicz introduced interactive dynamic influence diagrams (I-DIDs) as the computational representations of I-POMDPs.",
                "I-DIDs generalize DIDs [12], which may be viewed as computational counterparts of POMDPs, to multiagents settings in the same way that I-POMDPs generalize POMDPs.",
                "I-DIDs contribute to a growing line of work [19] that includes multi-agent influence diagrams (MAIDs) [14], and more recently, <br>networks of influence diagrams</br> (NIDs) [8].",
                "These formalisms seek to explicitly model the structure that is often present in real-world problems by decomposing the situation into chance and decision variables, and the dependencies between the variables.",
                "MAIDs provide an alternative to normal and extensive game forms using a graphical formalism to represent games of imperfect information with a decision node for each agents actions and chance nodes capturing the agents private information.",
                "MAIDs objectively analyze the game, efficiently computing the Nash equilibrium profile by exploiting the independence structure.",
                "NIDs extend MAIDs to include agents uncertainty over the game being played and over models of the other agents.",
                "Each model is a MAID and the network of MAIDs is collapsed, bottom up, into a single MAID for computing the equilibrium of the game keeping in mind the different models of each agent.",
                "Graphical formalisms such as MAIDs and NIDs open up a promising area of research that aims to represent multiagent interactions more transparently.",
                "However, MAIDs provide an analysis of the game from an external viewpoint and the applicability of both is limited to static single play games.",
                "Matters are more complex when we consider interactions that are extended over time, where predictions about others future actions must be made using models that change as the agents act and observe.",
                "I-DIDs address this gap by allowing the representation of other agents models as the values of a special model node.",
                "Both, other agents models and the original agents beliefs over these models are updated over time using special-purpose implementations.",
                "In this paper, we improve on the previous preliminary representation of the I-DID shown in [15] by using the insight that the static I-ID is a type of NID.",
                "Thus, we may utilize NID-specific language constructs such as multiplexers to represent the model node, and subsequently the I-ID, more transparently.",
                "Furthermore, we clarify the semantics of the special purpose policy link introduced in the representation of I-DID by [15], and show that it could be replaced by traditional dependency links.",
                "In the previous representation of the I-DID, the update of the agents belief over the models of others as the agents act and receive observations was denoted using a special link called the model update link that connected the model nodes over time.",
                "We explicate the semantics of this link by showing how it can be implemented using the traditional dependency links between the chance nodes that constitute the model nodes.",
                "The net result is a representation of I-DID that is significantly more transparent, semantically clear, and capable of being implemented using the standard algorithms for solving DIDs.",
                "We show how IDIDs may be used to model an agents uncertainty over others models, that may themselves be I-DIDs.",
                "Solution to the I-DID is a policy that prescribes what the agent should do over time, given its beliefs over the physical state and others models.",
                "Analogous to DIDs, I-DIDs may be used to compute the policy of an agent online as the agent acts and observes in a setting that is populated by other interacting agents. 2.",
                "BACKGROUND: FINITELY NESTED IPOMDPS Interactive POMDPs generalize POMDPs to multiagent settings by including other agents models as part of the state space [9].",
                "Since other agents may also reason about others, the interactive state space is strategically nested; it contains beliefs about other agents models and their beliefs about others.",
                "For simplicity of presentation we consider an agent, i, that is interacting with one other agent, j.",
                "A finitely nested I-POMDP of agent i with a strategy level l is defined as the tuple: I-POMDPi,l = ISi,l, A, Ti, Ωi, Oi, Ri where: • ISi,l denotes a set of interactive states defined as, ISi,l = S × Mj,l−1, where Mj,l−1 = {Θj,l−1 ∪ SMj}, for l ≥ 1, and ISi,0 = S, where S is the set of states of the physical environment.",
                "Θj,l−1 is the set of computable intentional models of agent j: θj,l−1 = bj,l−1, ˆθj where the frame, ˆθj = A, Ωj, Tj, Oj, Rj, OCj .",
                "Here, j is Bayes rational and OCj is js optimality criterion.",
                "SMj is the set of subintentional models of j.",
                "Simple examples of subintentional models include a no-information model [10] and a fictitious play model [6], both of which are history independent.",
                "We give a recursive bottom-up construction of the interactive state space below.",
                "ISi,0 = S, Θj,0 = { bj,0, ˆθj | bj,0 ∈ Δ(ISj,0)} ISi,1 = S × {Θj,0 ∪ SMj}, Θj,1 = { bj,1, ˆθj | bj,1 ∈ Δ(ISj,1)} . . . . . .",
                "ISi,l = S × {Θj,l−1 ∪ SMj}, Θj,l = { bj,l, ˆθj | bj,l ∈ Δ(ISj,l)} Similar formulations of nested spaces have appeared in [1, 3]. • A = Ai × Aj is the set of joint actions of all agents in the environment; • Ti : S ×A×S → [0, 1], describes the effect of the joint actions on the physical states of the environment; • Ωi is the set of observations of agent i; • Oi : S × A × Ωi → [0, 1] gives the likelihood of the observations given the physical state and joint action; • Ri : ISi × A → R describes agent is preferences over its interactive states.",
                "Usually only the physical states will matter.",
                "Agent is policy is the mapping, Ω∗ i → Δ(Ai), where Ω∗ i is the set of all observation histories of agent i.",
                "Since belief over the interactive states forms a sufficient statistic [9], the policy can also be represented as a mapping from the set of all beliefs of agent i to a distribution over its actions, Δ(ISi) → Δ(Ai). 2.1 Belief Update Analogous to POMDPs, an agent within the I-POMDP framework updates its belief as it acts and observes.",
                "However, there are two differences that complicate the belief update in multiagent settings when compared to single agent ones.",
                "First, since the state of the physical environment depends on the actions of both agents, is prediction of how the physical state changes has to be made based on its prediction of js actions.",
                "Second, changes in js models have to be included in is belief update.",
                "Specifically, if j is intentional then an update of js beliefs due to its action and observation has to be included.",
                "In other words, i has to update its belief based on its prediction of what j would observe and how j would update its belief.",
                "If js model is subintentional, then js probable observations are appended to the observation history contained in the model.",
                "Formally, we have: Pr(ist |at−1 i , bt−1 i,l ) = β ISt−1:mt−1 j =θt j bt−1 i,l (ist−1 ) × at−1 j Pr(at−1 j |θt−1 j,l−1)Oi(st , at−1 i , at−1 j , ot i) ×Ti(st−1 , at−1 i , at−1 j , st ) ot j Oj(st , at−1 i , at−1 j , ot j) ×τ(SEθt j (bt−1 j,l−1, at−1 j , ot j) − bt j,l−1) (1) where β is the normalizing constant, τ is 1 if its argument is 0 otherwise it is 0, Pr(at−1 j |θt−1 j,l−1) is the probability that at−1 j is Bayes rational for the agent described by model θt−1 j,l−1, and SE(·) is an abbreviation for the belief update.",
                "For a version of the belief update when js model is subintentional, see [9].",
                "If agent j is also modeled as an I-POMDP, then is belief update invokes js belief update (via the term SEθt j ( bt−1 j,l−1 , at−1 j , ot j)), which in turn could invoke is belief update and so on.",
                "This recursion in belief nesting bottoms out at the 0th level.",
                "At this level, the belief update of the agent reduces to a POMDP belief update. 1 For illustrations of the belief update, additional details on I-POMDPs, and how they compare with other multiagent frameworks, see [9]. 2.2 Value Iteration Each belief state in a finitely nested I-POMDP has an associated value reflecting the maximum payoff the agent can expect in this belief state: Un ( bi,l, θi ) = max ai∈Ai is∈ISi,l ERi(is, ai)bi,l(is)+ γ oi∈Ωi Pr(oi|ai, bi,l)Un−1 ( SEθi (bi,l, ai, oi), θi ) (2) where, ERi(is, ai) = aj Ri(is, ai, aj)Pr(aj|mj,l−1) (since is = (s, mj,l−1)).",
                "Eq. 2 is a basis for value iteration in I-POMDPs.",
                "Agent is optimal action, a∗ i , for the case of finite horizon with discounting, is an element of the set of optimal actions for the belief state, OPT(θi), defined as: OPT( bi,l, θi ) = argmax ai∈Ai is∈ISi,l ERi(is, ai)bi,l(is) +γ oi∈Ωi Pr(oi|ai, bi,l)Un ( SEθi (bi,l, ai, oi), θi ) (3) 3.",
                "INTERACTIVEINFLUENCEDIAGRAMS A naive extension of influence diagrams (IDs) to settings populated by multiple agents is possible by treating other agents as automatons, represented using chance nodes.",
                "However, this approach assumes that the agents actions are controlled using a probability distribution that does not change over time.",
                "Interactive influence diagrams (I-IDs) adopt a more sophisticated approach by generalizing IDs to make them applicable to settings shared with other agents who may act and observe, and update their beliefs. 3.1 Syntax In addition to the usual chance, decision, and utility nodes, IIDs include a new type of node called the model node.",
                "We show a general level l I-ID in Fig. 1(a), where the model node (Mj,l−1) is denoted using a hexagon.",
                "We note that the probability distribution over the chance node, S, and the model node together represents agent is belief over its interactive states.",
                "In addition to the model 1 The 0th level model is a POMDP: Other agents actions are treated as exogenous events and folded into the T, O, and R functions.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 815 Figure 1: (a) A generic level l I-ID for agent i situated with one other agent j.",
                "The hexagon is the model node (Mj,l−1) whose structure we show in (b).",
                "Members of the model node are I-IDs themselves (m1 j,l−1, m2 j,l−1; diagrams not shown here for simplicity) whose decision nodes are mapped to the corresponding chance nodes (A1 j , A2 j ).",
                "Depending on the value of the node, Mod[Mj], the distribution of each of the chance nodes is assigned to the node Aj. (c) The transformed I-ID with the model node replaced by the chance nodes and the relationships between them. node, I-IDs differ from IDs by having a dashed link (called the policy link in [15]) between the model node and a chance node, Aj, that represents the distribution over the other agents actions given its model.",
                "In the absence of other agents, the model node and the chance node, Aj, vanish and I-IDs collapse into traditional IDs.",
                "The model node contains the alternative computational models ascribed by i to the other agent from the set, Θj,l−1 ∪ SMj, where Θj,l−1 and SMj were defined previously in Section 2.",
                "Thus, a model in the model node may itself be an I-ID or ID, and the recursion terminates when a model is an ID or subintentional.",
                "Because the model node contains the alternative models of the other agent as its values, its representation is not trivial.",
                "In particular, some of the models within the node are I-IDs that when solved generate the agents optimal policy in their decision nodes.",
                "Each decision node is mapped to the corresponding chance node, say A1 j , in the following way: if OPT is the set of optimal actions obtained by solving the I-ID (or ID), then Pr(aj ∈ A1 j ) = 1 |OP T | if aj ∈ OPT, 0 otherwise.",
                "Borrowing insights from previous work [8], we observe that the model node and the dashed policy link that connects it to the chance node, Aj, could be represented as shown in Fig. 1(b).",
                "The decision node of each level l − 1 I-ID is transformed into a chance node, as we mentioned previously, so that the actions with the largest value in the decision node are assigned uniform probabilities in the chance node while the rest are assigned zero probability.",
                "The different chance nodes (A1 j , A2 j ), one for each model, and additionally, the chance node labeled Mod[Mj] form the parents of the chance node, Aj.",
                "Thus, there are as many action nodes (A1 j , A2 j ) in Mj,l−1 as the number of models in the support of agent is beliefs.",
                "The conditional probability table of the chance node, Aj, is a multiplexer that assumes the distribution of each of the action nodes (A1 j , A2 j ) depending on the value of Mod[Mj].",
                "The values of Mod[Mj] denote the different models of j.",
                "In other words, when Mod[Mj] has the value m1 j,l−1, the chance node Aj assumes the distribution of the node A1 j , and Aj assumes the distribution of A2 j when Mod[Mj] has the value m2 j,l−1.",
                "The distribution over the node, Mod[Mj], is the agent is belief over the models of j given a physical state.",
                "For more agents, we will have as many model nodes as there are agents.",
                "Notice that Fig. 1(b) clarifies the semantics of the policy link, and shows how it can be represented using the traditional dependency links.",
                "In Fig. 1(c), we show the transformed I-ID when the model node is replaced by the chance nodes and relationships between them.",
                "In contrast to the representation in [15], there are no special-purpose policy links, rather the I-ID is composed of only those types of nodes that are found in traditional IDs and dependency relationships between the nodes.",
                "This allows I-IDs to be represented and implemented using conventional application tools that target IDs.",
                "Note that we may view the level l I-ID as a NID.",
                "Specifically, each of the level l − 1 models within the model node are blocks in the NID (see Fig. 2).",
                "If the level l = 1, each block is a traditional ID, otherwise if l > 1, each block within the NID may itself be a NID.",
                "Note that within the I-IDs (or IDs) at each level, there is only a single decision node.",
                "Thus, our NID does not contain any MAIDs.",
                "Figure 2: A level l I-ID represented as a NID.",
                "The probabilities assigned to the blocks of the NID are is beliefs over js models conditioned on a physical state. 3.2 Solution The solution of an I-ID proceeds in a bottom-up manner, and is implemented recursively.",
                "We start by solving the level 0 models, which, if intentional, are traditional IDs.",
                "Their solutions provide probability distributions over the other agents actions, which are entered in the corresponding chance nodes found in the model node of the level 1 I-ID.",
                "The mapping from the level 0 models decision nodes to the chance nodes is carried out so that actions with the largest value in the decision node are assigned uniform probabilities in the chance node while the rest are assigned zero probability.",
                "Given the distributions over the actions within the different chance nodes (one for each model of the other agent), the level 1 I-ID is transformed as shown in Fig. 1(c).",
                "During the transformation, the conditional probability table (CPT) of the node, Aj, is populated such that the node assumes the distribution of each of the chance nodes depending on the value of the node, Mod[Mj].",
                "As we mentioned previously, the values of the node Mod[Mj] denote the different models of the other agent, and its distribution is the agent is belief over the models of j conditioned on the physical state.",
                "The transformed level 1 I-ID is a traditional ID that may be solved us816 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) (a) (b) Figure 3: (a) A generic two time-slice level l I-DID for agent i in a setting with one other agent j.",
                "Notice the dotted model update link that denotes the update of the models of j and the distribution over the models over time. (b) The semantics of the model update link. ing the standard expected utility maximization method [18].",
                "This procedure is carried out up to the level l I-ID whose solution gives the non-empty set of optimal actions that the agent should perform given its belief.",
                "Notice that analogous to IDs, I-IDs are suitable for online decision-making when the agents current belief is known. 4.",
                "INTERACTIVE DYNAMIC INFLUENCE DIAGRAMS Interactive dynamic influence diagrams (I-DIDs) extend I-IDs (and NIDs) to allow sequential decision-making over several time steps.",
                "Just as DIDs are structured graphical representations of POMDPs, I-DIDs are the graphical online analogs for finitely nested I-POMDPs.",
                "I-DIDs may be used to optimize over a finite look-ahead given initial beliefs while interacting with other, possibly similar, agents. 4.1 Syntax We depict a general two time-slice I-DID in Fig. 3(a).",
                "In addition to the model nodes and the dashed policy link, what differentiates an I-DID from a DID is the model update link shown as a dotted arrow in Fig. 3(a).",
                "We explained the semantics of the model node and the policy link in the previous section; we describe the model updates next.",
                "The update of the model node over time involves two steps: First, given the models at time t, we identify the updated set of models that reside in the model node at time t + 1.",
                "Recall from Section 2 that an agents intentional model includes its belief.",
                "Because the agents act and receive observations, their models are updated to reflect their changed beliefs.",
                "Since the set of optimal actions for a model could include all the actions, and the agent may receive any one of |Ωj| possible observations, the updated set at time step t + 1 will have at most |Mt j,l−1||Aj||Ωj| models.",
                "Here, |Mt j,l−1| is the number of models at time step t, |Aj| and |Ωj| are the largest spaces of actions and observations respectively, among all the models.",
                "Second, we compute the new distribution over the updated models given the original distribution and the probability of the agent performing the action and receiving the observation that led to the updated model.",
                "These steps are a part of agent is belief update formalized using Eq. 1.",
                "In Fig. 3(b), we show how the dotted model update link is implemented in the I-DID.",
                "If each of the two level l − 1 models ascribed to j at time step t results in one action, and j could make one of two possible observations, then the model node at time step t + 1 contains four updated models (mt+1,1 j,l−1 ,mt+1,2 j,l−1 , mt+1,3 j,l−1 , and mt+1,4 j,l−1 ).",
                "These models differ in their initial beliefs, each of which is the result of j updating its beliefs due to its action and a possible observation.",
                "The decision nodes in each of the I-DIDs or DIDs that represent the lower level models are mapped to the corresponding Figure 4: Transformed I-DID with the model nodes and model update link replaced with the chance nodes and the relationships (in bold). chance nodes, as mentioned previously.",
                "Next, we describe how the distribution over the updated set of models (the distribution over the chance node Mod[Mt+1 j ] in Mt+1 j,l−1) is computed.",
                "The probability that js updated model is, say mt+1,1 j,l−1 , depends on the probability of j performing the action and receiving the observation that led to this model, and the prior distribution over the models at time step t. Because the chance node At j assumes the distribution of each of the action nodes based on the value of Mod[Mt j ], the probability of the action is given by this chance node.",
                "In order to obtain the probability of js possible observation, we introduce the chance node Oj, which depending on the value of Mod[Mt j ] assumes the distribution of the observation node in the lower level model denoted by Mod[Mt j ].",
                "Because the probability of js observations depends on the physical state and the joint actions of both agents, the node Oj is linked with St+1 , At j, and At i. 2 Analogous to At j, the conditional probability table of Oj is also a multiplexer modulated by Mod[Mt j ].",
                "Finally, the distribution over the prior models at time t is obtained from the chance node, Mod[Mt j ] in Mt j,l−1.",
                "Consequently, the chance nodes, Mod[Mt j ], At j, and Oj, form the parents of Mod[Mt+1 j ] in Mt+1 j,l−1.",
                "Notice that the model update link may be replaced by the dependency links between the chance nodes that constitute the model nodes in the two time slices.",
                "In Fig. 4 we show the two time-slice I-DID with the model nodes replaced by the chance nodes and the relationships between them.",
                "Chance nodes and dependency links that not in bold are standard, usually found in DIDs.",
                "Expansion of the I-DID over more time steps requires the repetition of the two steps of updating the set of models that form the 2 Note that Oj represents js observation at time t + 1.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 817 values of the model node and adding the relationships between the chance nodes, as many times as there are model update links.",
                "We note that the possible set of models of the other agent j grows exponentially with the number of time steps.",
                "For example, after T steps, there may be at most |Mt=1 j,l−1|(|Aj||Ωj|)T −1 candidate models residing in the model node. 4.2 Solution Analogous to I-IDs, the solution to a level l I-DID for agent i expanded over T time steps may be carried out recursively.",
                "For the purpose of illustration, let l=1 and T=2.",
                "The solution method uses the standard look-ahead technique, projecting the agents action and observation sequences forward from the current belief state [17], and finding the possible beliefs that i could have in the next time step.",
                "Because agent i has a belief over js models as well, the lookahead includes finding out the possible models that j could have in the future.",
                "Consequently, each of js subintentional or level 0 models (represented using a standard DID) in the first time step must be solved to obtain its optimal set of actions.",
                "These actions are combined with the set of possible observations that j could make in that model, resulting in an updated set of candidate models (that include the updated beliefs) that could describe the behavior of j. Beliefs over this updated set of candidate models are calculated using the standard inference methods using the dependency relationships between the model nodes as shown in Fig. 3(b).",
                "We note the recursive nature of this solution: in solving agent is level 1 I-DID, js level 0 DIDs must be solved.",
                "If the nesting of models is deeper, all models at all levels starting from 0 are solved in a bottom-up manner.",
                "We briefly outline the recursive algorithm for solving agent is Algorithm for solving I-DID Input : level l ≥ 1 I-ID or level 0 ID, T Expansion Phase 1.",
                "For t from 1 to T − 1 do 2.",
                "If l ≥ 1 then Populate Mt+1 j,l−1 3.",
                "For each mt j in Range(Mt j,l−1) do 4.",
                "Recursively call algorithm with the l − 1 I-ID (or ID) that represents mt j and the horizon, T − t + 1 5.",
                "Map the decision node of the solved I-ID (or ID), OPT(mt j), to a chance node Aj 6.",
                "For each aj in OPT(mt j) do 7.",
                "For each oj in Oj (part of mt j) do 8.",
                "Update js belief, bt+1 j ← SE(bt j, aj, oj) 9. mt+1 j ← New I-ID (or ID) with bt+1 j as the initial belief 10.",
                "Range(Mt+1 j,l−1) ∪ ← {mt+1 j } 11.",
                "Add the model node, Mt+1 j,l−1, and the dependency links between Mt j,l−1 and Mt+1 j,l−1 (shown in Fig. 3(b)) 12.",
                "Add the chance, decision, and utility nodes for t + 1 time slice and the dependency links between them 13.",
                "Establish the CPTs for each chance node and utility node Look-Ahead Phase 14.",
                "Apply the standard look-ahead and backup method to solve the expanded I-DID Figure 5: Algorithm for solving a level l ≥ 0 I-DID. level l I-DID expanded over T time steps with one other agent j in Fig. 5.",
                "We adopt a two-phase approach: Given an I-ID of level l (described previously in Section 3) with all lower level models also represented as I-IDs or IDs (if level 0), the first step is to expand the level l I-ID over T time steps adding the dependency links and the conditional probability tables for each node.",
                "We particularly focus on establishing and populating the model nodes (lines 3-11).",
                "Note that Range(·) returns the values (lower level models) of the random variable given as input (model node).",
                "In the second phase, we use a standard look-ahead technique projecting the action and observation sequences over T time steps in the future, and backing up the utility values of the reachable beliefs.",
                "Similar to I-IDs, the I-DIDs reduce to DIDs in the absence of other agents.",
                "As we mentioned previously, the 0-th level models are the traditional DIDs.",
                "Their solutions provide probability distributions over actions of the agent modeled at that level to I-DIDs at level 1.",
                "Given probability distributions over other agents actions the level 1 IDIDs can themselves be solved as DIDs, and provide probability distributions to yet higher level models.",
                "Assume that the number of models considered at each level is bound by a number, M. Solving an I-DID of level l in then equivalent to solving O(Ml ) DIDs. 5.",
                "EXAMPLE APPLICATIONS To illustrate the usefulness of I-DIDs, we apply them to three problem domains.",
                "We describe, in particular, the formulation of the I-DID and the optimal prescriptions obtained on solving it. 5.1 Followership-Leadership in the Multiagent Tiger Problem We begin our illustrations of using I-IDs and I-DIDs with a slightly modified version of the multiagent tiger problem discussed in [9].",
                "The problem has two agents, each of which can open the right door (OR), the left door (OL) or listen (L).",
                "In addition to hearing growls (from the left (GL) or from the right (GR)) when they listen, the agents also hear creaks (from the left (CL), from the right (CR), or no creaks (S)), which noisily indicate the other agents opening one of the doors.",
                "When any door is opened, the tiger persists in its original location with a probability of 95%.",
                "Agent i hears growls with a reliability of 65% and creaks with a reliability of 95%.",
                "Agent j, on the other hand, hears growls with a reliability of 95%.",
                "Thus, the setting is such that agent i hears agent j opening doors more reliably than the tigers growls.",
                "This suggests that i could use js actions as an indication of the location of the tiger, as we discuss below.",
                "Each agents preferences are as in the single agent game discussed in [13].",
                "The transition, observation, and reward functions are shown in [16].",
                "A good indicator of the usefulness of normative methods for decision-making like I-DIDs is the emergence of realistic social behaviors in their prescriptions.",
                "In settings of the persistent multiagent tiger problem that reflect real world situations, we demonstrate followership between the agents and, as shown in [15], deception among agents who believe that they are in a follower-leader type of relationship.",
                "In particular, we analyze the situational and epistemological conditions sufficient for their emergence.",
                "The followership behavior, for example, results from the agent knowing its own weaknesses, assessing the strengths, preferences, and possible behaviors of the other, and realizing that its best for it to follow the others actions in order to maximize its payoffs.",
                "Let us consider a particular setting of the tiger problem in which agent i believes that js preferences are aligned with its own - both of them just want to get the gold - and js hearing is more reliable in comparison to itself.",
                "As an example, suppose that j, on listening can discern the tigers location 95% of the times compared to is 65% accuracy.",
                "Additionally, agent i does not have any initial information about the tigers location.",
                "In other words, is single-level nested belief, bi,1, assigns 0.5 to each of the two locations of the tiger.",
                "In addition, i considers two models of j, which differ in js flat level 0 initial beliefs.",
                "This is represented in the level 1 I-ID shown in Fig. 6(a).",
                "According to one model, j assigns a probability of 0.9 that the tiger is behind the left door, while the other 818 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Figure 6: (a) Level 1 I-ID of agent i, (b) two level 0 IDs of agent j whose decision nodes are mapped to the chance nodes, A1 j , A2 j , in (a). model assigns 0.1 to that location (see Fig. 6(b)).",
                "Agent i is undecided on these two models of j.",
                "If we vary is hearing ability, and solve the corresponding level 1 I-ID expanded over three time steps, we obtain the normative behavioral policies shown in Fig 7 that exhibit followership behavior.",
                "If is probability of correctly hearing the growls is 0.65, then as shown in the policy in Fig. 7(a), i begins to conditionally follow js actions: i opens the same door that j opened previously iff is own assessment of the tigers location confirms js pick.",
                "If i loses the ability to correctly interpret the growls completely, it blindly follows j and opens the same door that j opened previously (Fig. 7(b)).",
                "Figure 7: Emergence of (a) conditional followership, and (b) blind followership in the tiger problem.",
                "Behaviors of interest are in bold. * is a wildcard, and denotes any one of the observations.",
                "We observed that a single level of belief nesting - beliefs about the others models - was sufficient for followership to emerge in the tiger problem.",
                "However, the epistemological requirements for the emergence of leadership are more complex.",
                "For an agent, say j, to emerge as a leader, followership must first emerge in the other agent i.",
                "As we mentioned previously, if i is certain that its preferences are identical to those of j, and believes that j has a better sense of hearing, i will follow js actions over time.",
                "Agent j emerges as a leader if it believes that i will follow it, which implies that js belief must be nested two levels deep to enable it to recognize its leadership role.",
                "Realizing that i will follow presents j with an opportunity to influence is actions in the benefit of the collective good or its self-interest alone.",
                "For example, in the tiger problem, let us consider a setting in which if both i and j open the correct door, then each gets a payoff of 20 that is double the original.",
                "If j alone selects the correct door, it gets the payoff of 10.",
                "On the other hand, if both agents pick the wrong door, their penalties are cut in half.",
                "In this setting, it is in both js best interest as well as the collective betterment for j to use its expertise in selecting the correct door, and thus be a good leader.",
                "However, consider a slightly different problem in which j gains from is loss and is penalized if i gains.",
                "Specifically, let is payoff be subtracted from js, indicating that j is antagonistic toward i - if j picks the correct door and i the wrong one, then is loss of 100 becomes js gain.",
                "Agent j believes that i incorrectly thinks that js preferences are those that promote the collective good and that it starts off by believing with 99% confidence where the tiger is.",
                "Because i believes that its preferences are similar to those of j, and that j starts by believing almost surely that one of the two is the correct location (two level 0 models of j), i will start by following js actions.",
                "We show is normative policy on solving its singly-nested I-DID over three time steps in Fig. 8(a).",
                "The policy demonstrates that i will blindly follow js actions.",
                "Since the tiger persists in its original location with a probability of 0.95, i will select the same door again.",
                "If j begins the game with a 99% probability that the tiger is on the right, solving js I-DID nested two levels deep, results in the policy shown in Fig. 8(b).",
                "Even though j is almost certain that OL is the correct action, it will start by selecting OR, followed by OL.",
                "Agent js intention is to deceive i who, it believes, will follow js actions, so as to gain $110 in the second time step, which is more than what j would gain if it were to be honest.",
                "Figure 8: Emergence of deception between agents in the tiger problem.",
                "Behaviors of interest are in bold. * denotes as before. (a) Agent is policy demonstrating that it will blindly follow js actions. (b) Even though j is almost certain that the tiger is on the right, it will start by selecting OR, followed by OL, in order to deceive i. 5.2 Altruism and Reciprocity in the Public Good Problem The public good (PG) problem [7], consists of a group of M agents, each of whom must either contribute some resource to a public pot or keep it for themselves.",
                "Since resources contributed to the public pot are shared among all the agents, they are less valuable to the agent when in the public pot.",
                "However, if all agents choose to contribute their resources, then the payoff to each agent is more than if no one contributes.",
                "Since an agent gets its share of the public pot irrespective of whether it has contributed or not, the dominating action is for each agent to not contribute, and instead free ride on others contributions.",
                "However, behaviors of human players in empirical simulations of the PG problem differ from the normative predictions.",
                "The experiments reveal that many players initially contribute a large amount to the public pot, and continue to contribute when the PG problem is played repeatedly, though in decreasing amounts [4].",
                "Many of these experiments [5] report that a small core group of players persistently contributes to the public pot even when all others are defecting.",
                "These experiments also reveal that players who persistently contribute have altruistic or reciprocal preferences matching expected cooperation of others.",
                "For simplicity, we assume that the game is played between M = 2 agents, i and j.",
                "Let each agent be initially endowed with XT amount of resources.",
                "While the classical PG game formulation permits each agent to contribute any quantity of resources (≤ XT ) to the public pot, we simplify the action space by allowing two possible actions.",
                "Each agent may choose to either contribute (C) a fixed amount of the resources, or not contribute.",
                "The latter action is deThe Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 819 noted as defect (D).",
                "We assume that the actions are not observable to others.",
                "The value of resources in the public pot is discounted by ci for each agent i, where ci is the marginal private return.",
                "We assume that ci < 1 so that the agent does not benefit enough that it contributes to the public pot for private gain.",
                "Simultaneously, ciM > 1, making collective contribution pareto optimal. i/j C D C 2ciXT , 2cjXT ciXT − cp, XT + cjXT − P D XT + ciXT − P, cjXT − cp XT , XT Table 1: The one-shot PG game with punishment.",
                "In order to encourage contributions, the contributing agents punish free riders but incur a small cost for administering the punishment.",
                "Let P be the punishment meted out to the defecting agent and cp the non-zero cost of punishing for the contributing agent.",
                "For simplicity, we assume that the cost of punishing is same for both the agents.",
                "The one-shot PG game with punishment is shown in Table. 1.",
                "Let ci = cj, cp > 0, and if P > XT − ciXT , then defection is no longer a dominating action.",
                "If P < XT − ciXT , then defection is the dominating action for both.",
                "If P = XT − ciXT , then the game is not dominance-solvable.",
                "Figure 9: (a) Level 1 I-ID of agent i, (b) level 0 IDs of agent j with decision nodes mapped to the chance nodes, A1 j and A2 j , in (a).",
                "We formulate a sequential version of the PG problem with punishment from the perspective of agent i.",
                "Though in the repeated PG game, the quantity in the public pot is revealed to all the agents after each round of actions, we assume in our formulation that it is hidden from the agents.",
                "Each agent may contribute a fixed amount, xc, or defect.",
                "An agent on performing an action receives an observation of plenty (PY) or meager (MR) symbolizing the state of the public pot.",
                "Notice that the observations are also indirectly indicative of agent js actions because the state of the public pot is influenced by them.",
                "The amount of resources in agent is private pot, is perfectly observable to i.",
                "The payoffs are analogous to Table. 1.",
                "Borrowing from the empirical investigations of the PG problem [5], we construct level 0 IDs for j that model altruistic and non-altruistic types (Fig. 9(b)).",
                "Specifically, our altruistic agent has a high marginal private return (cj is close to 1) and does not punish others who defect.",
                "Let xc = 1 and the level 0 agent be punished half the times it defects.",
                "With one action remaining, both types of agents choose to contribute to avoid being punished.",
                "With two actions to go, the altruistic type chooses to contribute, while the other defects.",
                "This is because cj for the altruistic type is close to 1, thus the expected punishment, 0.5P > (1 − cj), which the altruistic type avoids.",
                "Because cj for the non-altruistic type is less, it prefers not to contribute.",
                "With three steps to go, the altruistic agent contributes to avoid punishment (0.5P > 2(1 − cj)), and the non-altruistic type defects.",
                "For greater than three steps, while the altruistic agent continues to contribute to the public pot depending on how close its marginal private return is to 1, the non-altruistic type prescribes defection.",
                "We analyzed the decisions of an altruistic agent i modeled using a level 1 I-DID expanded over 3 time steps. i ascribes the two level 0 models, mentioned previously, to j (see Fig. 9).",
                "If i believes with a probability 1 that j is altruistic, i chooses to contribute for each of the three steps.",
                "This behavior persists when i is unaware of whether j is altruistic (Fig. 10(a)), and when i assigns a high probability to j being the non-altruistic type.",
                "However, when i believes with a probability 1 that j is non-altruistic and will thus surely defect, i chooses to defect to avoid being punished and because its marginal private return is less than 1.",
                "These results demonstrate that the behavior of our altruistic type resembles that found experimentally.",
                "The non-altruistic level 1 agent chooses to defect regardless of how likely it believes the other agent to be altruistic.",
                "We analyzed the behavior of a reciprocal agent type that matches expected cooperation or defection.",
                "The reciprocal types marginal private return is similar to that of the non-altruistic type, however, it obtains a greater payoff when its action is similar to that of the other.",
                "We consider the case when the reciprocal agent i is unsure of whether j is altruistic and believes that the public pot is likely to be half full.",
                "For this prior belief, i chooses to defect.",
                "On receiving an observation of plenty, i decides to contribute, while an observation of meager makes it defect (Fig. 10(b)).",
                "This is because an observation of plenty signals that the pot is likely to be greater than half full, which results from js action to contribute.",
                "Thus, among the two models ascribed to j, its type is likely to be altruistic making it likely that j will contribute again in the next time step.",
                "Agent i therefore chooses to contribute to reciprocate js action.",
                "An analogous reasoning leads i to defect when it observes a meager pot.",
                "With one action to go, i believing that j contributes, will choose to contribute too to avoid punishment regardless of its observations.",
                "Figure 10: (a) An altruistic level 1 agent always contributes. (b) A reciprocal agent i starts off by defecting followed by choosing to contribute or defect based on its observation of plenty (indicating that j is likely altruistic) or meager (j is non-altruistic). 5.3 Strategies in Two-Player Poker Poker is a popular zero sum card game that has received much attention among the AI research community as a testbed [2].",
                "Poker is played among M ≥ 2 players in which each player receives a hand of cards from a deck.",
                "While several flavors of Poker with varying complexity exist, we consider a simple version in which each player has three plys during which the player may either exchange a card (E), keep the existing hand (K), fold (F) and withdraw from the game, or call (C), requiring all players to show their hands.",
                "To keep matters simple, let M = 2, and each player receive a hand consisting of a single card drawn from the same suit.",
                "Thus, during a showdown, the player who has the numerically larger card (2 is the lowest, ace is the highest) wins the pot.",
                "During an exchange of cards, the discarded card is placed either in the L pile, indicating to the other agent that it was a low numbered card less than 8, or in the 820 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) H pile, indicating that the card had a rank greater than or equal to 8.",
                "Notice that, for example, if a lower numbered card is discarded, the probability of receiving a low card in exchange is now reduced.",
                "We show the level 1 I-ID for the simplified two-player Poker in Fig. 11.",
                "We considered two models (personality types) of agent j.",
                "The conservative type believes that it is likely that its opponent has a high numbered card in its hand.",
                "On the other hand, the aggressive agent j believes with a high probability that its opponent has a lower numbered card.",
                "Thus, the two types differ in their beliefs over their opponents hand.",
                "In both these level 0 models, the opponent is assumed to perform its actions following a fixed, uniform distribution.",
                "With three actions to go, regardless of its hand (unless it is an ace), the aggressive agent chooses to exchange its card, with the intent of improving on its current hand.",
                "This is because it believes the other to have a low card, which improves its chances of getting a high card during the exchange.",
                "The conservative agent chooses to keep its card, no matter its hand because its chances of getting a high card are slim as it believes that its opponent has one.",
                "Figure 11: (a) Level 1 I-ID of agent i.",
                "The observation reveals information about js hand of the previous time step, (b) level 0 IDs of agent j whose decision nodes are mapped to the chance nodes, A1 j , A2 j , in (a).",
                "The policy of a level 1 agent i who believes that each card except its own has an equal likelihood of being in js hand (neutral personality type) and j could be either an aggressive or conservative type, is shown in Fig. 12. is own hand contains the card numbered 8.",
                "The agent starts by keeping its card.",
                "On seeing that j did not exchange a card (N), i believes with probability 1 that j is conservative and hence will keep its cards. i responds by either keeping its card or exchanging it because j is equally likely to have a lower or higher card.",
                "If i observes that j discarded its card into the L or H pile, i believes that j is aggressive.",
                "On observing L, i realizes that j had a low card, and is likely to have a high card after its exchange.",
                "Because the probability of receiving a low card is high now, i chooses to keep its card.",
                "On observing H, believing that the probability of receiving a high numbered card is high, i chooses to exchange its card.",
                "In the final step, i chooses to call regardless of its observation history because its belief that j has a higher card is not sufficiently high to conclude that its better to fold and relinquish the payoff.",
                "This is partly due to the fact that an observation of, say, L resets the agent is previous time step beliefs over js hand to the low numbered cards only. 6.",
                "DISCUSSION We showed how DIDs may be extended to I-DIDs that enable online sequential decision-making in uncertain multiagent settings.",
                "Our graphical representation of I-DIDs improves on the previous Figure 12: A level 1 agent is three step policy in the Poker problem. i starts by believing that j is equally likely to be aggressive or conservative and could have any card in its hand with equal probability. work significantly by being more transparent, semantically clear, and capable of being solved using standard algorithms that target DIDs.",
                "I-DIDs extend NIDs to allow sequential decision-making over multiple time steps in the presence of other interacting agents.",
                "I-DIDs may be seen as concise graphical representations for IPOMDPs providing a way to exploit problem structure and carry out online decision-making as the agent acts and observes given its prior beliefs.",
                "We are currently investigating ways to solve I-DIDs approximately with provable bounds on the solution quality.",
                "Acknowledgment: We thank Piotr Gmytrasiewicz for some useful discussions related to this work.",
                "The first author would like to acknowledge the support of a UGARF grant. 7.",
                "REFERENCES [1] R. J. Aumann.",
                "Interactive epistemology i: Knowledge.",
                "International Journal of Game Theory, 28:263-300, 1999. [2] D. Billings, A. Davidson, J. Schaeffer, and D. Szafron.",
                "The challenge of poker.",
                "AIJ, 2001. [3] A. Brandenburger and E. Dekel.",
                "Hierarchies of beliefs and common knowledge.",
                "Journal of Economic Theory, 59:189-198, 1993. [4] C. Camerer.",
                "Behavioral Game Theory: Experiments in Strategic Interaction.",
                "Princeton University Press, 2003. [5] E. Fehr and S. Gachter.",
                "Cooperation and punishment in public goods experiments.",
                "American Economic Review, 90(4):980-994, 2000. [6] D. Fudenberg and D. K. Levine.",
                "The Theory of Learning in Games.",
                "MIT Press, 1998. [7] D. Fudenberg and J. Tirole.",
                "Game Theory.",
                "MIT Press, 1991. [8] Y. Gal and A. Pfeffer.",
                "A language for modeling agents decision-making processes in games.",
                "In AAMAS, 2003. [9] P. Gmytrasiewicz and P. Doshi.",
                "A framework for sequential planning in multiagent settings.",
                "JAIR, 24:49-79, 2005. [10] P. Gmytrasiewicz and E. Durfee.",
                "Rational coordination in multi-agent environments.",
                "JAAMAS, 3(4):319-350, 2000. [11] J. C. Harsanyi.",
                "Games with incomplete information played by bayesian players.",
                "Management Science, 14(3):159-182, 1967. [12] R. A. Howard and J. E. Matheson.",
                "Influence diagrams.",
                "In R. A. Howard and J. E. Matheson, editors, The Principles and Applications of Decision Analysis.",
                "Strategic Decisions Group, Menlo Park, CA 94025, 1984. [13] L. Kaelbling, M. Littman, and A. Cassandra.",
                "Planning and acting in partially observable stochastic domains.",
                "Artificial Intelligence Journal, 2, 1998. [14] D. Koller and B. Milch.",
                "Multi-agent influence diagrams for representing and solving games.",
                "In IJCAI, pages 1027-1034, 2001. [15] K. Polich and P. Gmytrasiewicz.",
                "Interactive dynamic influence diagrams.",
                "In GTDT Workshop, AAMAS, 2006. [16] B.",
                "Rathnas., P. Doshi, and P. J. Gmytrasiewicz.",
                "Exact solutions to interactive pomdps using behavioral equivalence.",
                "In Autonomous Agents and Multi-Agent Systems Conference (AAMAS), 2006. [17] S. Russell and P. Norvig.",
                "Artificial Intelligence: A Modern Approach (Second Edition).",
                "Prentice Hall, 2003. [18] R. D. Shachter.",
                "Evaluating influence diagrams.",
                "Operations Research, 34(6):871-882, 1986. [19] D. Suryadi and P. Gmytrasiewicz.",
                "Learning models of other agents using influence diagrams.",
                "In UM, 1999.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 821"
            ],
            "original_annotated_samples": [
                "I-DIDs contribute to a growing line of work [19] that includes multi-agent influence diagrams (MAIDs) [14], and more recently, <br>networks of influence diagrams</br> (NIDs) [8]."
            ],
            "translated_annotated_samples": [
                "Los I-DIDs contribuyen a una creciente línea de trabajo [19] que incluye diagramas de influencia multiagente (MAIDs) [14], y más recientemente, <br>redes de diagramas de influencia</br> (NIDs) [8]."
            ],
            "translated_text": "Modelos gráficos para soluciones en línea a POMDP interactivos de Prashant Doshi Dept. Departamento de Ciencias de la Computación Universidad de Georgia Athens, GA 30602, EE. UU. pdoshi@cs.uga.edu Yifeng Zeng Dept. de Ciencias de la Computación Universidad de Aalborg DK-9220 Aalborg, Dinamarca yfzeng@cs.aau.edu Qiongyu Chen Dept. de Ciencias de la Computación Universidad Nacional de Singapur 117543, Singapur chenqy@comp.nus.edu.sg RESUMEN Desarrollamos una nueva representación gráfica para procesos de decisión de Markov parcialmente observables interactivos (I-POMDPs) que es significativamente más transparente y semánticamente clara que la representación anterior. Estos modelos gráficos llamados diagramas de influencia dinámica interactiva (I-DIDs) buscan modelar explícitamente la estructura que a menudo está presente en problemas del mundo real al descomponer la situación en variables de probabilidad y decisión, y las dependencias entre las variables. Los I-DIDs generalizan los DIDs, los cuales pueden ser vistos como representaciones gráficas de POMDPs, a entornos multiagentes de la misma manera que los I-POMDPs generalizan los POMDPs. Los I-DIDs pueden ser utilizados para calcular la política de un agente en línea mientras el agente actúa y observa en un entorno poblado por otros agentes que interactúan entre sí. Usando varios ejemplos, mostramos cómo se pueden aplicar los I-DIDs y demostramos su utilidad. Categorías y Descriptores de Asignaturas I.2.11 [Inteligencia Artificial Distribuida]: Sistemas Multiagentes Términos Generales Teoría 1. Las Procesos de Decisión de Markov Interactivos Parcialmente Observables (IPOMDPs) proporcionan un marco para la toma de decisiones secuenciales en entornos multiagentes parcialmente observables. Generalizan los POMDPs [13] a entornos multiagentes al incluir los modelos computables de los otros agentes en el espacio de estados junto con los estados del entorno físico. Los modelos abarcan toda la información que influye en los comportamientos de los agentes, incluyendo sus preferencias, capacidades y creencias, y por lo tanto son análogos a los tipos en los juegos bayesianos [11]. I-POMDPs adoptan un enfoque subjetivo para comprender el comportamiento estratégico, arraigado en un marco de teoría de decisiones que toma la perspectiva de los tomadores de decisiones en la interacción. En [15], Polich y Gmytrasiewicz introdujeron diagramas de influencia dinámica interactivos (I-DIDs) como las representaciones computacionales de I-POMDPs. Los I-DIDs generalizan los DIDs [12], los cuales pueden ser vistos como contrapartes computacionales de POMDPs, a entornos de múltiples agentes de la misma manera que los I-POMDPs generalizan los POMDPs. Los I-DIDs contribuyen a una creciente línea de trabajo [19] que incluye diagramas de influencia multiagente (MAIDs) [14], y más recientemente, <br>redes de diagramas de influencia</br> (NIDs) [8]. Estos formalismos buscan modelar explícitamente la estructura que a menudo está presente en problemas del mundo real al descomponer la situación en variables de azar y decisiones, y las dependencias entre las variables. Los MAIDs proporcionan una alternativa a las formas normales y extensivas de juego utilizando un formalismo gráfico para representar juegos de información imperfecta con un nodo de decisión para las acciones de cada agente y nodos de azar que capturan la información privada de los agentes. Los MAIDs analizan objetivamente el juego, calculando eficientemente el perfil de equilibrio de Nash al explotar la estructura de independencia. NIDs extienden los MAIDs para incluir la incertidumbre de los agentes sobre el juego que se está jugando y sobre los modelos de los otros agentes. Cada modelo es un MAID y la red de MAIDs se colapsa, de abajo hacia arriba, en un solo MAID para calcular el equilibrio del juego teniendo en cuenta los diferentes modelos de cada agente. Los formalismos gráficos como MAIDs y NIDs abren una área de investigación prometedora que tiene como objetivo representar las interacciones multiagente de manera más transparente. Sin embargo, los MAIDs proporcionan un análisis del juego desde un punto de vista externo y la aplicabilidad de ambos está limitada a juegos estáticos de un solo jugador. Los asuntos son más complejos cuando consideramos interacciones que se extienden en el tiempo, donde las predicciones sobre las acciones futuras de otros deben hacerse utilizando modelos que cambian a medida que los agentes actúan y observan. Los I-DIDs abordan esta brecha al permitir la representación de modelos de otros agentes como los valores de un nodo de modelo especial. Tanto los modelos de otros agentes como las creencias originales de los agentes sobre estos modelos se actualizan con el tiempo utilizando implementaciones especializadas. En este documento, mejoramos la representación preliminar previa del I-DID mostrada en [15] utilizando la idea de que el I-ID estático es un tipo de NID. Por lo tanto, podemos utilizar construcciones de lenguaje específicas de NID, como multiplexores, para representar de manera más transparente el nodo del modelo y, posteriormente, el I-ID. Además, aclaramos la semántica del enlace de política de propósito especial introducido en la representación de I-DID por [15], y mostramos que podría ser reemplazado por enlaces de dependencia tradicionales. En la representación previa del I-DID, la actualización de la creencia de los agentes sobre los modelos de los demás a medida que los agentes actúan y reciben observaciones se denotaba utilizando un enlace especial llamado el enlace de actualización del modelo que conectaba los nodos del modelo a lo largo del tiempo. Explicamos la semántica de este enlace mostrando cómo puede ser implementado utilizando los enlaces de dependencia tradicionales entre los nodos de probabilidad que constituyen los nodos del modelo. El resultado final es una representación de I-DID que es significativamente más transparente, semánticamente clara y capaz de ser implementada utilizando los algoritmos estándar para resolver DIDs. Mostramos cómo los IDIDs pueden ser utilizados para modelar la incertidumbre de un agente sobre los modelos de otros, que a su vez pueden ser I-DIDs. La solución al I-DID es una política que prescribe lo que el agente debe hacer con el tiempo, dadas sus creencias sobre el estado físico y otros modelos. Análogos a los DIDs, los I-DIDs pueden ser utilizados para calcular la política de un agente en línea mientras el agente actúa y observa en un entorno poblado por otros agentes interactuantes. 2. ANTECEDENTES: Los IPOMDPS ANIDADOS FINITAMENTE generalizan los POMDPs a entornos multiagentes al incluir los modelos de otros agentes como parte del espacio de estados [9]. Dado que otros agentes también pueden razonar sobre otros, el espacio de estado interactivo está estratégicamente anidado; contiene creencias sobre los modelos de otros agentes y sus creencias sobre los demás. Para simplificar la presentación, consideramos un agente, i, que está interactuando con otro agente, j. Un I-POMDP finitamente anidado del agente i con un nivel de estrategia l se define como la tupla: I-POMDPi,l = ISi,l, A, Ti, Ωi, Oi, Ri donde: • ISi,l denota un conjunto de estados interactivos definido como, ISi,l = S × Mj,l−1, donde Mj,l−1 = {Θj,l−1 ∪ SMj}, para l ≥ 1, e ISi,0 = S, donde S es el conjunto de estados del entorno físico. Θj,l−1 es el conjunto de modelos intencionales computables del agente j: θj,l−1 = bj,l−1, ˆθj donde el marco, ˆθj = A, Ωj, Tj, Oj, Rj, OCj. Aquí, j es Bayesiano racional y OCj es el criterio de optimalidad de j. SMj es el conjunto de modelos subintencionales de j. Ejemplos simples de modelos subintencionales incluyen un modelo sin información [10] y un modelo de juego ficticio [6], ambos de los cuales son independientes de la historia. Damos una construcción recursiva de abajo hacia arriba del espacio de estados interactivo a continuación. Sí,0 = S, Θj,0 = { bj,0, ˆθj | bj,0 ∈ Δ(ISj,0)} Sí,1 = S × {Θj,0 ∪ SMj}, Θj,1 = { bj,1, ˆθj | bj,1 ∈ Δ(ISj,1)} . . . . . . Sí, l = S × {Θj,l−1 ∪ SMj}, Θj,l = { bj,l, ˆθj | bj,l ∈ Δ(ISj,l)} Formulaciones similares de espacios anidados han aparecido en [1, 3]. • A = Ai × Aj es el conjunto de acciones conjuntas de todos los agentes en el entorno; • Ti : S ×A×S → [0, 1], describe el efecto de las acciones conjuntas en los estados físicos del entorno; • Ωi es el conjunto de observaciones del agente i; • Oi : S × A × Ωi → [0, 1] da la probabilidad de las observaciones dadas el estado físico y la acción conjunta; • Ri : ISi × A → R describe las preferencias del agente sobre sus estados interactivos. Normalmente solo importarán los estados físicos. La política del agente es el mapeo, Ω∗ i → Δ(Ai), donde Ω∗ i es el conjunto de todos los historiales de observación del agente i. Dado que la creencia sobre los estados interactivos forma una estadística suficiente [9], la política también puede ser representada como un mapeo del conjunto de todas las creencias del agente i a una distribución sobre sus acciones, Δ(ISi) → Δ(Ai). 2.1 Actualización de Creencias Análogo a los POMDPs, un agente dentro del marco I-POMDP actualiza su creencia a medida que actúa y observa. Sin embargo, hay dos diferencias que complican la actualización de creencias en entornos multiagentes en comparación con los entornos de un solo agente. Primero, dado que el estado del entorno físico depende de las acciones de ambos agentes, la predicción de cómo cambia el estado físico debe hacerse basándose en la predicción de sus acciones. Segundo, los cambios en los modelos de js deben ser incluidos en la actualización de creencias. Específicamente, si j es intencional, entonces se debe incluir una actualización de las creencias de j debido a su acción y observación. En otras palabras, i tiene que actualizar su creencia basándose en su predicción de lo que j observaría y cómo j actualizaría su creencia. Si el modelo de js es subintencional, entonces las observaciones probables de js se añaden al historial de observaciones contenido en el modelo. Formalmente, tenemos: Pr(ist |at−1 i , bt−1 i,l ) = β ISt−1:mt−1 j =θt j bt−1 i,l (ist−1 ) × at−1 j Pr(at−1 j |θt−1 j,l−1)Oi(st , at−1 i , at−1 j , ot i) ×Ti(st−1 , at−1 i , at−1 j , st ) ot j Oj(st , at−1 i , at−1 j , ot j) ×τ(SEθt j (bt−1 j,l−1, at−1 j , ot j) − bt j,l−1) (1) donde β es la constante de normalización, τ es 1 si su argumento es 0, de lo contrario es 0, Pr(at−1 j |θt−1 j,l−1) es la probabilidad de que at−1 j sea Bayes racional para el agente descrito por el modelo θt−1 j,l−1, y SE(·) es una abreviatura para la actualización de creencias. Para una versión de la actualización de creencias cuando el modelo js es subintencional, consulte [9]. Si el agente j también se modela como un I-POMDP, entonces la actualización de creencias invoca la actualización de creencias de j (a través del término SEθt j ( bt−1 j,l−1 , at−1 j , ot j)), lo que a su vez podría invocar la actualización de creencias de is y así sucesivamente. Esta recursión en la anidación de creencias llega al nivel 0. En este nivel, la actualización de creencias del agente se reduce a una actualización de creencias de un POMDP. Para ilustraciones de la actualización de creencias, detalles adicionales sobre I-POMDPs y cómo se comparan con otros marcos multiagentes, consulte [9]. Iteración de Valor Cada estado de creencia en un I-POMDP finitamente anidado tiene un valor asociado que refleja la máxima ganancia que el agente puede esperar en este estado de creencia: Un ( bi,l, θi ) = max ai∈Ai is∈ISi,l ERi(is, ai)bi,l(is)+ γ oi∈Ωi Pr(oi|ai, bi,l)Un−1 ( SEθi (bi,l, ai, oi), θi ) (2) donde, ERi(is, ai) = aj Ri(is, ai, aj)Pr(aj|mj,l−1) (ya que is = (s, mj,l−1)). La ecuación 2 es una base para la iteración de valor en I-POMDPs. La acción óptima del agente, a∗ i, para el caso de horizonte finito con descuento, es un elemento del conjunto de acciones óptimas para el estado de creencia, OPT(θi), definido como: OPT( bi,l, θi ) = argmax ai∈Ai is∈ISi,l ERi(is, ai)bi,l(is) +γ oi∈Ωi Pr(oi|ai, bi,l)Un ( SEθi (bi,l, ai, oi), θi ) (3) 3. Diagramas de influencia interactivos. Una extensión ingenua de los diagramas de influencia (IDs) a entornos poblados por múltiples agentes es posible tratando a los otros agentes como autómatas, representados mediante nodos de probabilidad. Sin embargo, este enfoque asume que las acciones de los agentes están controladas utilizando una distribución de probabilidad que no cambia con el tiempo. Los diagramas de influencia interactivos (I-IDs) adoptan un enfoque más sofisticado al generalizar los IDs para hacerlos aplicables a entornos compartidos con otros agentes que pueden actuar, observar y actualizar sus creencias. 3.1 Sintaxis Además de los nodos habituales de probabilidad, decisión y utilidad, los IIDs incluyen un nuevo tipo de nodo llamado nodo de modelo. Mostramos un nivel general l I-ID en la Fig. 1(a), donde el nodo del modelo (Mj,l−1) está representado con un hexágono. Observamos que la distribución de probabilidad sobre el nodo de probabilidad, S, y el nodo del modelo juntos representan la creencia del agente sobre sus estados interactivos. Además del modelo 1, el modelo de nivel 0 es un POMDP: las acciones de otros agentes se tratan como eventos exógenos y se incorporan en las funciones T, O y R. El Sexto Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) Figura 1: (a) Un nivel genérico l I-ID para el agente i situado con otro agente j. El hexágono es el nodo modelo (Mj,l−1) cuya estructura mostramos en (b). Los miembros del nodo modelo son ellos mismos I-ID (m1 j,l−1, m2 j,l−1; los diagramas no se muestran aquí por simplicidad) cuyos nodos de decisión se asignan a los nodos de probabilidad correspondientes (A1 j , A2 j). Dependiendo del valor del nodo, Mod[Mj], la distribución de cada uno de los nodos de probabilidad se asigna al nodo Aj. (c) El I-ID transformado con el nodo del modelo reemplazado por los nodos de probabilidad y las relaciones entre ellos. Los I-IDs difieren de los IDs al tener un enlace discontinuo (llamado enlace de política en [15]) entre el nodo del modelo y un nodo de probabilidad, Aj, que representa la distribución sobre las acciones de los otros agentes dada su modelo. En ausencia de otros agentes, el nodo del modelo y el nodo de probabilidad, Aj, desaparecen y los I-ID se convierten en ID tradicionales. El nodo del modelo contiene los modelos computacionales alternativos atribuidos por i al otro agente del conjunto, Θj,l−1 ∪ SMj, donde Θj,l−1 y SMj fueron definidos previamente en la Sección 2. Por lo tanto, un modelo en el nodo del modelo puede ser en sí mismo un I-ID o ID, y la recursión termina cuando un modelo es un ID o subintencional. Dado que el nodo del modelo contiene los modelos alternativos del otro agente como sus valores, su representación no es trivial. En particular, algunos de los modelos dentro del nodo son I-IDs que, al resolverse, generan la política óptima de los agentes en sus nodos de decisión. Cada nodo de decisión se asigna al nodo de probabilidad correspondiente, digamos A1 j, de la siguiente manera: si OPT es el conjunto de acciones óptimas obtenidas al resolver el I-ID (o ID), entonces Pr(aj ∈ A1 j) = 1 |OPT| si aj ∈ OPT, 0 en caso contrario. Tomando prestados los conocimientos de trabajos anteriores [8], observamos que el nodo del modelo y el enlace de política discontinua que lo conecta con el nodo de probabilidad, Aj, podrían representarse como se muestra en la Fig. 1(b). El nodo de decisión de cada nivel l − 1 I-ID se transforma en un nodo de probabilidad, como mencionamos anteriormente, de modo que las acciones con el mayor valor en el nodo de decisión se les asignan probabilidades uniformes en el nodo de probabilidad, mientras que al resto se les asigna probabilidad cero. Los diferentes nodos de probabilidad (A1 j , A2 j ), uno para cada modelo, y adicionalmente, el nodo de probabilidad etiquetado Mod[Mj] forman los padres del nodo de probabilidad, Aj. Por lo tanto, hay tantos nodos de acción (A1 j , A2 j ) en Mj,l−1 como el número de modelos en el soporte de las creencias del agente. La tabla de probabilidad condicional del nodo de probabilidad, Aj, es un multiplexor que asume la distribución de cada uno de los nodos de acción (A1 j , A2 j ) dependiendo del valor de Mod[Mj]. Los valores de Mod[Mj] denotan los diferentes modelos de j. En otras palabras, cuando Mod[Mj] tiene el valor m1 j,l−1, el nodo de probabilidad Aj asume la distribución del nodo A1 j, y Aj asume la distribución de A2 j cuando Mod[Mj] tiene el valor m2 j,l−1. La distribución sobre el nodo, Mod[Mj], es la creencia del agente sobre los modelos de j dado un estado físico. Para más agentes, tendremos tantos nodos de modelo como agentes haya. Observa que la Fig. 1(b) aclara la semántica del enlace de política y muestra cómo puede ser representado utilizando los enlaces de dependencia tradicionales. En la Fig. 1(c), mostramos el I-ID transformado cuando el nodo del modelo es reemplazado por los nodos de probabilidad y las relaciones entre ellos. A diferencia de la representación en [15], no hay enlaces de políticas especiales, sino que el I-ID está compuesto únicamente por aquellos tipos de nodos que se encuentran en IDs tradicionales y relaciones de dependencia entre los nodos. Esto permite que los I-IDs sean representados e implementados utilizando herramientas de aplicación convencionales que apuntan a los IDs. Ten en cuenta que podemos ver el nivel l I-ID como un NID. Específicamente, cada uno de los modelos del nivel l − 1 dentro del nodo del modelo son bloques en el NID (ver Fig. 2). Si el nivel l = 1, cada bloque es un ID tradicional, de lo contrario, si l > 1, cada bloque dentro del NID puede ser un NID en sí mismo. Ten en cuenta que dentro de los I-IDs (o IDs) en cada nivel, solo hay un nodo de decisión único. Por lo tanto, nuestro NID no contiene ningún MAID. Figura 2: Un nivel l I-ID representado como un NID. Las probabilidades asignadas a los bloques del NID son creencias sobre modelos js condicionados a un estado físico. 3.2 Solución La solución de un I-ID procede de manera ascendente y se implementa de forma recursiva. Comenzamos resolviendo los modelos de nivel 0, que, si son intencionales, son identificadores tradicionales. Sus soluciones proporcionan distribuciones de probabilidad sobre las acciones de los otros agentes, las cuales se ingresan en los nodos de probabilidad correspondientes encontrados en el nodo del modelo del nivel 1 I-ID. El mapeo de los nodos de decisión de los modelos de nivel 0 a los nodos de probabilidad se realiza de manera que las acciones con el mayor valor en el nodo de decisión se les asignan probabilidades uniformes en el nodo de probabilidad, mientras que el resto se les asigna probabilidad cero. Dadas las distribuciones sobre las acciones dentro de los diferentes nodos de probabilidad (uno para cada modelo del otro agente), el I-ID de nivel 1 se transforma como se muestra en la Fig. 1(c). Durante la transformación, la tabla de probabilidad condicional (CPT) del nodo, Aj, se llena de tal manera que el nodo asume la distribución de cada uno de los nodos de probabilidad dependiendo del valor del nodo, Mod[Mj]. Como mencionamos anteriormente, los valores del nodo Mod[Mj] denotan los diferentes modelos del otro agente, y su distribución es la creencia del agente sobre los modelos de j condicionados al estado físico. El nivel 1 I-ID transformado es un ID tradicional que puede ser resuelto como us816 The Sixth Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) (a) (b) Figura 3: (a) Un I-DID genérico de dos niveles de tiempo para el agente i en un entorno con otro agente j. Observa el enlace de actualización del modelo punteado que denota la actualización de los modelos de j y la distribución de los modelos a lo largo del tiempo. (b) La semántica del enlace de actualización del modelo utilizando el método estándar de maximización de utilidad esperada [18]. Este procedimiento se lleva a cabo hasta el nivel l I-ID cuya solución proporciona el conjunto no vacío de acciones óptimas que el agente debe realizar dada su creencia. Ten en cuenta que, al igual que los IDs, los I-IDs son adecuados para la toma de decisiones en línea cuando se conoce la creencia actual de los agentes. 4. Diagramas de influencia dinámica interactivos (I-DIDs) Los diagramas de influencia dinámica interactivos (I-DIDs) extienden los I-IDs (y NIDs) para permitir la toma de decisiones secuencial a lo largo de varios pasos de tiempo. Así como los DIDs son representaciones gráficas estructuradas de POMDPs, los I-DIDs son los análogos gráficos en línea para I-POMDPs finitamente anidados. Los I-DIDs pueden ser utilizados para optimizar sobre una mirada anticipada finita dadas las creencias iniciales mientras se interactúa con otros agentes, posiblemente similares. 4.1 Sintaxis Representamos un I-DID de dos cortes temporales en la Fig. 3(a). Además de los nodos del modelo y el enlace de política discontinuo, lo que diferencia a un I-DID de un DID es el enlace de actualización del modelo mostrado como una flecha punteada en la Fig. 3(a). Explicamos la semántica del nodo del modelo y el enlace de la política en la sección anterior; a continuación, describimos las actualizaciones del modelo. La actualización del nodo del modelo con el tiempo implica dos pasos: primero, dados los modelos en el tiempo t, identificamos el conjunto actualizado de modelos que residen en el nodo del modelo en el tiempo t + 1. Recuerda de la Sección 2 que el modelo intencional de un agente incluye sus creencias. Debido a que los agentes actúan y reciben observaciones, sus modelos se actualizan para reflejar sus creencias cambiadas. Dado que el conjunto de acciones óptimas para un modelo podría incluir todas las acciones, y el agente podría recibir cualquiera de las |Ωj| posibles observaciones, el conjunto actualizado en el paso de tiempo t + 1 tendrá como máximo |Mt j,l−1||Aj||Ωj| modelos. Aquí, |Mt j,l−1| es el número de modelos en el paso de tiempo t, |Aj| y |Ωj| son los espacios más grandes de acciones y observaciones respectivamente, entre todos los modelos. Segundo, calculamos la nueva distribución sobre los modelos actualizados dados la distribución original y la probabilidad de que el agente realice la acción y reciba la observación que llevó al modelo actualizado. Estos pasos son parte de la actualización de creencias del agente formalizada utilizando la Ecuación 1. En la Fig. 3(b), mostramos cómo se implementa el enlace de actualización del modelo punteado en el I-DID. Si cada uno de los dos modelos de nivel l − 1 atribuidos a j en el paso de tiempo t resulta en una acción, y j podría hacer una de dos posibles observaciones, entonces el nodo del modelo en el paso de tiempo t + 1 contiene cuatro modelos actualizados (mt+1,1 j,l−1 ,mt+1,2 j,l−1 , mt+1,3 j,l−1 , y mt+1,4 j,l−1 ). Estos modelos difieren en sus creencias iniciales, cada una de las cuales es el resultado de j actualizar sus creencias debido a su acción y una posible observación. Los nodos de decisión en cada uno de los I-DIDs o DIDs que representan los modelos de nivel inferior se asignan a la Figura 4 correspondiente: I-DID transformado con los nodos del modelo y el enlace de actualización del modelo reemplazados por los nodos de probabilidad y las relaciones (en negrita) de los nodos de probabilidad, como se mencionó anteriormente. A continuación, describimos cómo se calcula la distribución sobre el conjunto actualizado de modelos (la distribución sobre el nodo de probabilidad Mod[Mt+1 j ] en Mt+1 j,l−1). La probabilidad de que el modelo actualizado js sea, digamos mt+1,1 j,l−1, depende de la probabilidad de que j realice la acción y reciba la observación que condujo a este modelo, y de la distribución previa sobre los modelos en el paso de tiempo t. Dado que el nodo de probabilidad At j asume la distribución de cada uno de los nodos de acción basados en el valor de Mod[Mt j], la probabilidad de la acción está dada por este nodo de probabilidad. Para obtener la probabilidad de una observación posible js, introducimos el nodo de probabilidad Oj, que dependiendo del valor de Mod[Mt j], asume la distribución del nodo de observación en el modelo de nivel inferior denominado Mod[Mt j]. Dado que la probabilidad de las observaciones js depende del estado físico y de las acciones conjuntas de ambos agentes, el nodo Oj está vinculado con St+1, At j y At i. De manera análoga a At j, la tabla de probabilidad condicional de Oj también es un multiplexor modulado por Mod[Mt j]. Finalmente, la distribución de los modelos previos en el tiempo t se obtiene a partir del nodo de probabilidad, Mod[Mt j ] en Mt j,l−1. Por consiguiente, los nodos de probabilidad, Mod[Mt j], At j y Oj, forman los padres de Mod[Mt+1 j] en Mt+1 j,l−1. Ten en cuenta que el enlace de actualización del modelo puede ser reemplazado por los enlaces de dependencia entre los nodos de probabilidad que constituyen los nodos del modelo en las dos etapas temporales. En la Fig. 4 mostramos los dos cortes temporales I-DID con los nodos del modelo reemplazados por los nodos de probabilidad y las relaciones entre ellos. Los nodos de probabilidad y los enlaces de dependencia que no están en negrita son estándar, generalmente encontrados en DIDs. La expansión del I-DID a lo largo de más pasos de tiempo requiere la repetición de los dos pasos de actualizar el conjunto de modelos que forman el 2. Nota que Oj representa la observación j en el tiempo t + 1. El Sexto Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 817 valores del nodo del modelo y añadiendo las relaciones entre los nodos de probabilidad, tantas veces como enlaces de actualización del modelo haya. Observamos que el conjunto posible de modelos del otro agente j crece exponencialmente con el número de pasos de tiempo. Por ejemplo, después de T pasos, puede haber como máximo |Mt=1 j,l−1|(|Aj||Ωj|)T −1 modelos candidatos residiendo en el nodo del modelo. 4.2 Solución Análoga a los I-ID, la solución a un I-DID de nivel l para el agente i expandido a lo largo de T pasos de tiempo puede llevarse a cabo de forma recursiva. Con el propósito de ilustración, dejemos l=1 y T=2. El método de solución utiliza la técnica estándar de anticipación, proyectando las secuencias de acciones y observaciones de los agentes hacia adelante desde el estado de creencia actual [17], y encontrando las posibles creencias que podría tener en el siguiente paso temporal. Dado que el agente i también tiene una creencia sobre los modelos de j, la búsqueda anticipada incluye descubrir los posibles modelos que j podría tener en el futuro. Por consiguiente, cada uno de los modelos subintencionales o de nivel 0 de js (representados utilizando un DID estándar) en el primer paso de tiempo debe resolverse para obtener su conjunto óptimo de acciones. Estas acciones se combinan con el conjunto de observaciones posibles que j podría hacer en ese modelo, lo que resulta en un conjunto actualizado de modelos candidatos (que incluyen las creencias actualizadas) que podrían describir el comportamiento de j. Las creencias sobre este conjunto actualizado de modelos candidatos se calculan utilizando los métodos estándar de inferencia que utilizan las relaciones de dependencia entre los nodos del modelo, como se muestra en la Figura 3(b). Observamos la naturaleza recursiva de esta solución: al resolver el agente de nivel 1 I-DID, los DIDs de nivel 0 deben ser resueltos. Si el anidamiento de los modelos es más profundo, todos los modelos en todos los niveles, comenzando desde el nivel 0, se resuelven de manera ascendente. Breve descripción del algoritmo recursivo para resolver el agente es el Algoritmo para resolver I-DID. Entrada: nivel l ≥ 1 I-ID o nivel 0 ID, T Fase de Expansión 1. Para t desde 1 hasta T − 1, hacer 2. Si l ≥ 1, entonces llenar Mt+1 j,l−1 3. Para cada mt j en el rango (Mt j,l−1) hacer 4. Llame recursivamente al algoritmo con el l - 1 I-ID (o ID) que representa mt j y el horizonte, T - t + 1 5. Mapea el nodo de decisión del I-ID (o ID) resuelto, OPT(mt j), a un nodo de probabilidad Aj 6. Para cada aj en OPT(mt j) hacer 7. Para cada oj en Oj (parte de mt j) hacer 8. Actualizar la creencia js, bt+1 j ← SE(bt j, aj, oj) 9. mt+1 j ← Nuevo I-ID (o ID) con bt+1 j como la creencia inicial 10. Rango(Mt+1 j,l−1) ∪ ← {mt+1 j } 11. Añadir el nodo del modelo, Mt+1 j,l−1, y los enlaces de dependencia entre Mt j,l−1 y Mt+1 j,l−1 (mostrados en la Fig. 3(b)) 12. Agregue los nodos de probabilidad, decisión y utilidad para la franja de tiempo t + 1 y los enlaces de dependencia entre ellos 13. Establezca las CPTs para cada nodo de probabilidad y nodo de utilidad de la Fase de Mirada Adelante 14. Aplica el método estándar de anticipación y respaldo para resolver la Figura 5 expandida de I-DID: Algoritmo para resolver un nivel l ≥ 0 I-DID. Nivel l I-DID expandido durante T pasos de tiempo con otro agente j en la Figura 5. Adoptamos un enfoque de dos fases: Dado un I-ID de nivel l (descrito previamente en la Sección 3) con todos los modelos de niveles inferiores representados también como I-IDs o IDs (si es nivel 0), el primer paso es expandir el I-ID de nivel l a lo largo de T pasos de tiempo añadiendo los enlaces de dependencia y las tablas de probabilidad condicional para cada nodo. Nos enfocamos especialmente en establecer y poblar los nodos del modelo (líneas 3-11). Ten en cuenta que Range(·) devuelve los valores (modelos de nivel inferior) de la variable aleatoria dada como entrada (nodo del modelo). En la segunda fase, utilizamos una técnica estándar de anticipación proyectando las secuencias de acción y observación durante T pasos de tiempo en el futuro, y respaldando los valores de utilidad de las creencias alcanzables. Similar a los I-IDs, los I-DIDs se reducen a DIDs en ausencia de otros agentes. Como mencionamos anteriormente, los modelos de nivel 0 son los DIDs tradicionales. Sus soluciones proporcionan distribuciones de probabilidad sobre las acciones del agente modelado en ese nivel a los I-DIDs en el nivel 1. Dadas las distribuciones de probabilidad sobre las acciones de otros agentes, los IDIDs de nivel 1 pueden resolverse como DIDs y proporcionar distribuciones de probabilidad a modelos de niveles superiores. Suponga que el número de modelos considerados en cada nivel está limitado por un número, M. Resolver un I-DID de nivel l es equivalente a resolver O(Ml) DIDs. Para ilustrar la utilidad de los I-DIDs, los aplicamos a tres dominios de problemas. Describimos, en particular, la formulación del I-DID y las prescripciones óptimas obtenidas al resolverlo. 5.1 Seguimiento-Liderazgo en el Problema del Tigre Multiagente Comenzamos nuestras ilustraciones del uso de I-IDs e I-DIDs con una versión ligeramente modificada del problema del tigre multiagente discutido en [9]. El problema tiene dos agentes, cada uno de los cuales puede abrir la puerta derecha (OR), la puerta izquierda (OL) o escuchar (L). Además de escuchar gruñidos (desde la izquierda (GL) o desde la derecha (GR)) cuando escuchan, los agentes también escuchan chirridos (desde la izquierda (CL), desde la derecha (CR) o sin chirridos (S)), que indican ruidosamente que los otros agentes están abriendo una de las puertas. Cuando se abre cualquier puerta, el tigre persiste en su ubicación original con una probabilidad del 95%. El agente i escucha gruñidos con una fiabilidad del 65% y crujidos con una fiabilidad del 95%. El agente J, por otro lado, escucha gruñidos con una fiabilidad del 95%. Por lo tanto, la configuración es tal que el agente i escucha la apertura de puertas del agente j de manera más confiable que los rugidos de los tigres. Esto sugiere que podría usar acciones de JavaScript como indicación de la ubicación del tigre, como discutimos a continuación. Las preferencias de cada agente son como en el juego de un solo agente discutido en [13]. Las funciones de transición, observación y recompensa se muestran en [16]. Un buen indicador de la utilidad de los métodos normativos para la toma de decisiones como los I-DIDs es la aparición de comportamientos sociales realistas en sus prescripciones. En entornos del persistente problema del tigre multiagente que reflejan situaciones del mundo real, demostramos la seguidismo entre los agentes y, como se muestra en [15], el engaño entre agentes que creen que están en una relación de tipo seguidor-líder. En particular, analizamos las condiciones situacionales y epistemológicas suficientes para su surgimiento. El comportamiento de seguidores, por ejemplo, resulta del agente conociendo sus propias debilidades, evaluando las fortalezas, preferencias y posibles comportamientos del otro, y dándose cuenta de que es mejor para él seguir las acciones de los demás para maximizar sus ganancias. Consideremos una configuración particular del problema del tigre en la que el agente i cree que las preferencias de j están alineadas con las suyas: ambos solo quieren obtener el oro, y la audición de j es más confiable en comparación consigo mismo. Como ejemplo, supongamos que j, al escuchar, puede discernir la ubicación del tigre el 95% de las veces en comparación con su precisión del 65%. Además, el agente i no tiene ninguna información inicial sobre la ubicación de los tigres. En otras palabras, la creencia anidada de un solo nivel, bi,1, asigna 0.5 a cada una de las dos ubicaciones del tigre. Además, considera dos modelos de j, que difieren en los niveles iniciales de creencias planas de j. Esto se representa en el nivel 1 I-ID mostrado en la Fig. 6(a). Según un modelo, j asigna una probabilidad de 0.9 a que el tigre esté detrás de la puerta izquierda, mientras que el otro 818 The Sixth Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) Figura 6: (a) Nivel 1 I-ID del agente i, (b) dos IDs de nivel 0 del agente j cuyos nodos de decisión están mapeados a los nodos de probabilidad, A1 j , A2 j , en (a). El modelo asigna 0.1 a esa ubicación (ver Fig. 6(b)). El agente i está indeciso sobre estos dos modelos de j. Si variamos la capacidad auditiva, y resolvemos el nivel 1 correspondiente I-ID expandido en tres pasos de tiempo, obtenemos las políticas de comportamiento normativas mostradas en la Figura 7 que exhiben comportamiento de seguidores. Si la probabilidad de escuchar correctamente los gruñidos es de 0.65, entonces, como se muestra en la política en la Fig. 7(a), i comienza a seguir condicionalmente las acciones de j: i abre la misma puerta que j abrió anteriormente si su evaluación propia de la ubicación de los tigres confirma la elección de j. Si i pierde la capacidad de interpretar correctamente los gruñidos por completo, sigue ciegamente a j y abre la misma puerta que j abrió anteriormente (Fig. 7(b)). Figura 7: Emergencia de (a) seguidores condicionales, y (b) seguidores ciegos en el problema del tigre. Los comportamientos de interés están en negrita. * es un comodín y representa cualquiera de las observaciones. Observamos que un solo nivel de anidación de creencias, creencias sobre los modelos de los demás, fue suficiente para que surgiera el seguidismo en el problema del tigre. Sin embargo, los requisitos epistemológicos para el surgimiento del liderazgo son más complejos. Para que un agente, digamos j, surja como líder, primero debe surgir el seguidismo en el otro agente i. Como mencionamos anteriormente, si i está seguro de que sus preferencias son idénticas a las de j, y cree que j tiene un mejor sentido del oído, i seguirá las acciones de j con el tiempo. El agente j emerge como líder si cree que lo seguiré, lo que implica que la creencia de j debe estar anidada dos niveles de profundidad para permitirle reconocer su papel de liderazgo. Darse cuenta de que lo seguiré presenta a J una oportunidad para influir en sus acciones en beneficio del bien colectivo o de su interés propio solamente. Por ejemplo, en el problema del tigre, consideremos una situación en la que si tanto i como j abren la puerta correcta, entonces cada uno recibe un pago de 20 que es el doble del original. Si solo j selecciona la puerta correcta, obtiene la recompensa de 10. Por otro lado, si ambos agentes eligen la puerta incorrecta, sus penalizaciones se reducen a la mitad. En este entorno, es tanto en el mejor interés de j como en el beneficio colectivo que utilice su experiencia para seleccionar la puerta correcta, y así ser un buen líder. Sin embargo, considera un problema ligeramente diferente en el que j se beneficia de la pérdida de i y es penalizado si i se beneficia. Específicamente, dejemos que la ganancia se reste de js, indicando que j es antagonista hacia i: si j elige la puerta correcta y i la incorrecta, entonces la pérdida de 100 se convierte en la ganancia de js. El agente J cree que yo incorrectamente pienso que las preferencias de J son aquellas que promueven el bien colectivo y que comienza creyendo con un 99% de confianza dónde está el tigre. Dado que creo que mis preferencias son similares a las de j, y que j comienza creyendo casi con certeza que una de las dos ubicaciones es la correcta (dos modelos de nivel 0 de j), comenzaré siguiendo las acciones de j. Mostramos la política normativa para resolver su I-DID anidado individualmente durante tres pasos de tiempo en la Fig. 8(a). La política demuestra que seguiré ciegamente las acciones de js. Dado que el tigre persiste en su ubicación original con una probabilidad del 0.95, seleccionaré nuevamente la misma puerta. Si j comienza el juego con una probabilidad del 99% de que el tigre esté a la derecha, resolver su I-DID anidado dos niveles profundamente, resulta en la política mostrada en la Fig. 8(b). Aunque j está casi seguro de que OL es la acción correcta, comenzará seleccionando OR, seguido de OL. La intención del agente js es engañar a i, a quien cree que seguirá las acciones de js, para así obtener $110 en el segundo paso de tiempo, lo cual es más de lo que j ganaría si fuera honesto. Figura 8: Emergencia del engaño entre agentes en el problema del tigre. Los comportamientos de interés están en negrita. * denota como antes. (a) El agente es una política que demuestra que seguirá ciegamente las acciones de js. (b) Aunque j está casi seguro de que el tigre está a la derecha, comenzará seleccionando OR, seguido de OL, para engañar a i. 5.2 Altruismo y reciprocidad en el problema del bien público El problema del bien público (PG) [7], consiste en un grupo de M agentes, cada uno de los cuales debe contribuir con algún recurso a una olla pública o guardarlo para sí mismos. Dado que los recursos aportados al fondo público se comparten entre todos los agentes, son menos valiosos para el agente cuando están en el fondo público. Sin embargo, si todos los agentes eligen contribuir con sus recursos, entonces la recompensa para cada agente es mayor que si nadie contribuye. Dado que un agente recibe su parte del bote público independientemente de si ha contribuido o no, la acción dominante es que cada agente no contribuya y en su lugar se aproveche de las contribuciones de los demás. Sin embargo, los comportamientos de los jugadores humanos en simulaciones empíricas del problema de PG difieren de las predicciones normativas. Los experimentos revelan que muchos jugadores inicialmente contribuyen con una gran cantidad al bote público, y continúan contribuyendo cuando se juega el problema del PG repetidamente, aunque en cantidades decrecientes [4]. Muchos de estos experimentos [5] informan que un pequeño grupo central de jugadores contribuye persistentemente al bote público incluso cuando todos los demás están desertando. Estos experimentos también revelan que los jugadores que contribuyen persistentemente tienen preferencias altruistas o recíprocas que coinciden con la cooperación esperada de los demás. Para simplificar, asumimos que el juego se juega entre M = 2 agentes, i y j. Que cada agente esté inicialmente dotado con una cantidad de recursos XT. Mientras que la formulación clásica del juego PG permite que cada agente contribuya con cualquier cantidad de recursos (≤ XT) al bote público, simplificamos el espacio de acciones al permitir dos acciones posibles. Cada agente puede elegir contribuir (C) una cantidad fija de recursos, o no contribuir. La última acción es deThe Sixth Intl. La Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 819 fue señalada como defectuosa (D). Suponemos que las acciones no son observables por otros. El valor de los recursos en la bolsa pública se descuenta por ci para cada agente i, donde ci es el retorno privado marginal. Suponemos que ci < 1 para que el agente no se beneficie lo suficiente como para contribuir al bote público en beneficio propio. Simultáneamente, ciM > 1, lo que hace que la contribución colectiva sea óptima de Pareto. i/j C D C 2ciXT , 2cjXT ciXT − cp, XT + cjXT − P D XT + ciXT − P, cjXT − cp XT , XT Tabla 1: El juego PG de un solo disparo con castigo. Para fomentar las contribuciones, los agentes contribuyentes castigan a los que no colaboran pero incurren en un pequeño costo por administrar el castigo. Sea P la sanción impuesta al agente que se desvía y cp el costo no nulo de castigar al agente contribuyente. Para simplificar, asumimos que el costo de castigar es el mismo para ambos agentes. El juego PG de un solo disparo con castigo se muestra en la Tabla 1. Si ci = cj, cp > 0, y si P > XT − ciXT, entonces la deserción ya no es una acción dominante. Si P < XT − ciXT, entonces la deserción es la acción dominante para ambos. Si P = XT − ciXT, entonces el juego no es resoluble por dominancia. Figura 9: (a) Nivel 1 I-ID del agente i, (b) IDs de nivel 0 del agente j con nodos de decisión mapeados a los nodos de probabilidad, A1 j y A2 j, en (a). Formulamos una versión secuencial del problema PG con castigo desde la perspectiva del agente i. Aunque en el juego de PG repetido, la cantidad en la olla pública se revela a todos los agentes después de cada ronda de acciones, asumimos en nuestra formulación que está oculta para los agentes. Cada agente puede contribuir con una cantidad fija, xc, o desertar. Un agente al realizar una acción recibe una observación de abundante (PY) o escasa (MR) simbolizando el estado de la olla pública. Ten en cuenta que las observaciones también son indirectamente indicativas de las acciones del agente js porque el estado del bote público es influenciado por ellas. La cantidad de recursos en el agente es una olla privada, es perfectamente observable para i. Los pagos son análogos a la Tabla 1. Tomando prestado de las investigaciones empíricas del problema PG [5], construimos IDs de nivel 0 para j que modelan tipos altruistas y no altruistas (Fig. 9(b)). Específicamente, nuestro agente altruista tiene un alto retorno privado marginal (cj está cerca de 1) y no castiga a los demás que incumplen. Que xc = 1 y el agente de nivel 0 sea castigado la mitad de las veces que se desvía. Con una acción restante, ambos tipos de agentes eligen contribuir para evitar ser castigados. Con dos acciones por delante, el tipo altruista elige contribuir, mientras que el otro defecta. Esto se debe a que cj para el tipo altruista es cercano a 1, por lo tanto, el castigo esperado, 0.5P > (1 − cj), que el tipo altruista evita. Debido a que cj para el tipo no altruista es menor, prefiere no contribuir. Con tres pasos por delante, el agente altruista contribuye para evitar el castigo (0.5P > 2(1 − cj)), mientras que el tipo no altruista se desentiende. Para más de tres pasos, mientras el agente altruista continúa contribuyendo al bote público dependiendo de qué tan cercano esté su retorno privado marginal a 1, el tipo no altruista prescribe la deserción. Analizamos las decisiones de un agente altruista i modelado usando un nivel 1 I-DID expandido en 3 pasos de tiempo. i atribuye los dos modelos de nivel 0, mencionados anteriormente, a j (ver Fig. 9). Si creo con una probabilidad de 1 que j es altruista, elijo contribuir en cada uno de los tres pasos. Este comportamiento persiste cuando i no está al tanto de si j es altruista (Fig. 10(a)), y cuando i asigna una alta probabilidad a que j sea del tipo no altruista. Sin embargo, cuando i cree con una probabilidad de 1 que j no es altruista y por lo tanto seguramente va a traicionar, i elige traicionar para evitar ser castigado y porque su beneficio privado marginal es menor que 1. Estos resultados demuestran que el comportamiento de nuestro tipo altruista se asemeja al encontrado experimentalmente. El agente de nivel 1 no altruista elige desertar independientemente de lo altruista que crea que sea el otro agente. Analizamos el comportamiento de un tipo de agente recíproco que se ajusta a la cooperación o defección esperada. El retorno privado marginal del tipo recíproco es similar al del tipo no altruista, sin embargo, obtiene una mayor recompensa cuando su acción es similar a la del otro. Consideramos el caso en el que el agente recíproco i no está seguro de si j es altruista y cree que es probable que el bote público esté medio lleno. Para esta creencia previa, yo elijo defectar. Al recibir una observación de abundancia, decide contribuir, mientras que una observación de escasez lo hace defectar (Fig. 10(b)). Esto se debe a que una observación de abundancia indica que es probable que la olla esté más llena que a la mitad, lo cual resulta de la acción de js para contribuir. Por lo tanto, entre los dos modelos atribuidos a j, es probable que su tipo sea altruista, lo que hace probable que j contribuya nuevamente en el próximo paso de tiempo. El agente i, por lo tanto, elige contribuir para corresponder la acción de js. Un razonamiento análogo lleva a uno a defectar cuando observa una olla escasa. Con una acción por hacer, creo que si j contribuye, también elegiré contribuir para evitar castigo independientemente de sus observaciones. Figura 10: (a) Un agente altruista de nivel 1 siempre contribuye. (b) Un agente recíproco i comienza defraudando y luego elige contribuir o defraudar basándose en su observación de abundancia (indicando que j es probablemente altruista) o escasez (j no es altruista). 5.3 Estrategias en el póker de dos jugadores El póker es un popular juego de cartas de suma cero que ha recibido mucha atención en la comunidad de investigación de IA como un banco de pruebas [2]. El póker se juega entre M ≥ 2 jugadores, en el que cada jugador recibe una mano de cartas de una baraja. Si bien existen varias variantes de póker con diferentes niveles de complejidad, consideramos una versión simple en la que cada jugador tiene tres turnos durante los cuales el jugador puede intercambiar una carta (E), mantener la mano existente (K), retirarse (F) y abandonar el juego, o apostar (C), lo que requiere que todos los jugadores muestren sus manos. Para mantener las cosas simples, dejemos que M = 2, y que cada jugador reciba una mano compuesta por una sola carta sacada del mismo palo. Por lo tanto, durante un enfrentamiento, el jugador que tenga la carta numéricamente más alta (el 2 es el más bajo, el as es el más alto) gana el bote. Durante un intercambio de cartas, la carta descartada se coloca ya sea en la pila L, indicando al otro agente que era una carta de número bajo menor que 8, o en la 820 The Sixth Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) H, indicando que la carta tenía un rango mayor o igual a 8. Ten en cuenta que, por ejemplo, si se descarta una carta de número bajo, la probabilidad de recibir una carta baja a cambio se reduce. Mostramos el nivel 1 I-ID para el Poker simplificado de dos jugadores en la Fig. 11. Consideramos dos modelos (tipos de personalidad) del agente j. El tipo conservador cree que es probable que su oponente tenga una carta de alto número en su mano. Por otro lado, el agente agresivo j cree con alta probabilidad que su oponente tiene una carta de número más bajo. Por lo tanto, los dos tipos difieren en sus creencias sobre la mano de sus oponentes. En ambos estos modelos de nivel 0, se asume que el oponente realiza sus acciones siguiendo una distribución fija y uniforme. Con tres acciones restantes, independientemente de su mano (a menos que sea un as), el agente agresivo elige intercambiar su carta, con la intención de mejorar su mano actual. Esto se debe a que cree que el otro tiene una carta baja, lo que mejora sus posibilidades de obtener una carta alta durante el intercambio. El agente conservador elige quedarse con su carta, sin importar su mano, porque considera que sus posibilidades de obtener una carta alta son escasas, ya que cree que su oponente tiene una. Figura 11: (a) Nivel 1 I-ID del agente i. La observación revela información sobre la mano de js del paso de tiempo anterior, (b) IDs de nivel 0 del agente j cuyos nodos de decisión están mapeados a los nodos de probabilidad, A1 j , A2 j , en (a). La política de un agente de nivel 1 i que cree que cada carta, excepto la suya, tiene la misma probabilidad de estar en su mano (tipo de personalidad neutral) y j podría ser de tipo agresivo o conservador, se muestra en la Figura 12. Su mano contiene la carta numerada 8. El agente comienza por mantener su carta. Al ver que j no intercambió una carta (N), i cree con probabilidad 1 que j es conservador y, por lo tanto, mantendrá sus cartas. i responde manteniendo su carta o intercambiándola porque j tiene igual probabilidad de tener una carta más baja o más alta. Si observo que j descartó su carta en la pila L o H, creo que j es agresivo. Al observar a L, i se da cuenta de que j tenía una carta baja, y es probable que tenga una carta alta después de su intercambio. Dado que la probabilidad de recibir una carta baja es alta en este momento, i decide quedarse con su carta. Al observar la carta H, creyendo que la probabilidad de recibir una carta de número alto es alta, i decide intercambiar su carta. En el paso final, i decide llamar independientemente de su historial de observaciones porque su creencia de que j tiene una carta más alta no es lo suficientemente alta como para concluir que es mejor retirarse y renunciar al pago. Esto se debe en parte al hecho de que una observación de, digamos, L, restablece las creencias del agente en el paso de tiempo anterior sobre las cartas de números bajos solamente. 6. DISCUSIÓN Mostramos cómo los DIDs pueden ser extendidos a los I-DIDs que permiten la toma de decisiones secuenciales en línea en entornos multiagentes inciertos. Nuestra representación gráfica de I-DIDs mejora la Figura 12 anterior: Un agente de nivel 1 es una política de tres pasos en el problema de Poker. i comienza creyendo que j tiene la misma probabilidad de ser agresivo o conservador y podría tener cualquier carta en su mano con igual probabilidad. Trabaja significativamente al ser más transparente, semánticamente claro y capaz de ser resuelto utilizando algoritmos estándar que apuntan a los DIDs. Los I-DIDs extienden los NIDs para permitir la toma de decisiones secuenciales a lo largo de múltiples pasos de tiempo en presencia de otros agentes que interactúan. Los I-DIDs pueden ser vistos como representaciones gráficas concisas para IPOMDPs que proporcionan una forma de explotar la estructura del problema y llevar a cabo la toma de decisiones en línea a medida que el agente actúa y observa dadas sus creencias previas. Actualmente estamos investigando formas de resolver I-DIDs aproximadamente con límites demostrables en la calidad de la solución. Agradecimiento: Agradecemos a Piotr Gmytrasiewicz por algunas discusiones útiles relacionadas con este trabajo. El primer autor desea agradecer el apoyo de una beca UGARF. 7. REFERENCIAS [1] R. J. Aumann. Epistemología interactiva i: Conocimiento. Revista Internacional de Teoría de Juegos, 28:263-300, 1999. [2] D. Billings, A. Davidson, J. Schaeffer y D. Szafron. El desafío del póker. AIJ, 2001. [3] A. Brandenburger y E. Dekel. Jerarquías de creencias y conocimiento común. Revista de Teoría Económica, 59:189-198, 1993. [4] C. Camerer. Teoría de juegos conductual: Experimentos en interacción estratégica. Princeton University Press, 2003. [5] E. Fehr y S. Gachter. Cooperación y castigo en experimentos de bienes públicos. Revista Económica Americana, 90(4):980-994, 2000. [6] D. Fudenberg y D. K. Levine. La Teoría del Aprendizaje en los Juegos. MIT Press, 1998. [7] D. Fudenberg y J. Tirole. Teoría de juegos. MIT Press, 1991. [8] Y. Gal y A. Pfeffer. Un lenguaje para modelar los procesos de toma de decisiones de agentes en juegos. En AAMAS, 2003. [9] P. Gmytrasiewicz y P. Doshi. Un marco para la planificación secuencial en entornos multiagentes. JAIR, 24:49-79, 2005. [10] P. Gmytrasiewicz y E. Durfee. Coordinación racional en entornos multiagente. JAAMAS, 3(4):319-350, 2000. [11] J. C. Harsanyi. Juegos con información incompleta jugados por jugadores bayesianos. Ciencia de la Gestión, 14(3):159-182, 1967. [12] R. A. Howard y J. E. Matheson. Diagramas de influencia. En R. A. Howard y J. E. Matheson, editores, Los Principios y Aplicaciones del Análisis de Decisiones. Grupo de Decisiones Estratégicas, Menlo Park, CA 94025, 1984. [13] L. Kaelbling, M. Littman y A. Cassandra. Planificación y actuación en dominios estocásticos parcialmente observables. Revista de Inteligencia Artificial, 2, 1998. [14] D. Koller y B. Milch. Diagramas de influencia multiagente para representar y resolver juegos. En IJCAI, páginas 1027-1034, 2001. [15] K. Polich y P. Gmytrasiewicz. Diagramas de influencia interactivos y dinámicos. En el taller GTDT, AAMAS, 2006. [16] B. Rathnas, P. Doshi y P. J. Gmytrasiewicz. Soluciones exactas para POMDP interactivos utilizando equivalencia conductual. En la Conferencia de Agentes Autónomos y Sistemas Multiagente (AAMAS), 2006. [17] S. Russell y P. Norvig. Inteligencia Artificial: Un Enfoque Moderno (Segunda Edición). Prentice Hall, 2003. [18] R. D. Shachter. \n\nPrentice Hall, 2003. [18] R. D. Shachter. Evaluando diagramas de influencia. Investigación de Operaciones, 34(6):871-882, 1986. [19] D. Suryadi y P. Gmytrasiewicz. Aprendiendo modelos de otros agentes utilizando diagramas de influencia. En UM, 1999. El Sexto Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 821 ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "influence diagram network": {
            "translated_key": "Diagrama de influencia de red",
            "is_in_text": false,
            "original_annotated_sentences": [
                "Graphical Models for Online Solutions to Interactive POMDPs Prashant Doshi Dept.",
                "of Computer Science University of Georgia Athens, GA 30602, USA pdoshi@cs.uga.edu Yifeng Zeng Dept. of Computer Science Aalborg University DK-9220 Aalborg, Denmark yfzeng@cs.aau.edu Qiongyu Chen Dept. of Computer Science National Univ. of Singapore 117543, Singapore chenqy@comp.nus.edu.sg ABSTRACT We develop a new graphical representation for interactive partially observable Markov decision processes (I-POMDPs) that is significantly more transparent and semantically clear than the previous representation.",
                "These graphical models called interactive dynamic influence diagrams (I-DIDs) seek to explicitly model the structure that is often present in real-world problems by decomposing the situation into chance and decision variables, and the dependencies between the variables.",
                "I-DIDs generalize DIDs, which may be viewed as graphical representations of POMDPs, to multiagent settings in the same way that I-POMDPs generalize POMDPs.",
                "I-DIDs may be used to compute the policy of an agent online as the agent acts and observes in a setting that is populated by other interacting agents.",
                "Using several examples, we show how I-DIDs may be applied and demonstrate their usefulness.",
                "Categories and Subject Descriptors I.2.11 [Distributed Artificial Intelligence]: Multiagent Systems General Terms Theory 1.",
                "INTRODUCTION Interactive partially observable Markov decision processes (IPOMDPs) [9] provide a framework for sequential decision-making in partially observable multiagent environments.",
                "They generalize POMDPs [13] to multiagent settings by including the other agents computable models in the state space along with the states of the physical environment.",
                "The models encompass all information influencing the agents behaviors, including their preferences, capabilities, and beliefs, and are thus analogous to types in Bayesian games [11].",
                "I-POMDPs adopt a subjective approach to understanding strategic behavior, rooted in a decision-theoretic framework that takes a decision-makers perspective in the interaction.",
                "In [15], Polich and Gmytrasiewicz introduced interactive dynamic influence diagrams (I-DIDs) as the computational representations of I-POMDPs.",
                "I-DIDs generalize DIDs [12], which may be viewed as computational counterparts of POMDPs, to multiagents settings in the same way that I-POMDPs generalize POMDPs.",
                "I-DIDs contribute to a growing line of work [19] that includes multi-agent influence diagrams (MAIDs) [14], and more recently, networks of influence diagrams (NIDs) [8].",
                "These formalisms seek to explicitly model the structure that is often present in real-world problems by decomposing the situation into chance and decision variables, and the dependencies between the variables.",
                "MAIDs provide an alternative to normal and extensive game forms using a graphical formalism to represent games of imperfect information with a decision node for each agents actions and chance nodes capturing the agents private information.",
                "MAIDs objectively analyze the game, efficiently computing the Nash equilibrium profile by exploiting the independence structure.",
                "NIDs extend MAIDs to include agents uncertainty over the game being played and over models of the other agents.",
                "Each model is a MAID and the network of MAIDs is collapsed, bottom up, into a single MAID for computing the equilibrium of the game keeping in mind the different models of each agent.",
                "Graphical formalisms such as MAIDs and NIDs open up a promising area of research that aims to represent multiagent interactions more transparently.",
                "However, MAIDs provide an analysis of the game from an external viewpoint and the applicability of both is limited to static single play games.",
                "Matters are more complex when we consider interactions that are extended over time, where predictions about others future actions must be made using models that change as the agents act and observe.",
                "I-DIDs address this gap by allowing the representation of other agents models as the values of a special model node.",
                "Both, other agents models and the original agents beliefs over these models are updated over time using special-purpose implementations.",
                "In this paper, we improve on the previous preliminary representation of the I-DID shown in [15] by using the insight that the static I-ID is a type of NID.",
                "Thus, we may utilize NID-specific language constructs such as multiplexers to represent the model node, and subsequently the I-ID, more transparently.",
                "Furthermore, we clarify the semantics of the special purpose policy link introduced in the representation of I-DID by [15], and show that it could be replaced by traditional dependency links.",
                "In the previous representation of the I-DID, the update of the agents belief over the models of others as the agents act and receive observations was denoted using a special link called the model update link that connected the model nodes over time.",
                "We explicate the semantics of this link by showing how it can be implemented using the traditional dependency links between the chance nodes that constitute the model nodes.",
                "The net result is a representation of I-DID that is significantly more transparent, semantically clear, and capable of being implemented using the standard algorithms for solving DIDs.",
                "We show how IDIDs may be used to model an agents uncertainty over others models, that may themselves be I-DIDs.",
                "Solution to the I-DID is a policy that prescribes what the agent should do over time, given its beliefs over the physical state and others models.",
                "Analogous to DIDs, I-DIDs may be used to compute the policy of an agent online as the agent acts and observes in a setting that is populated by other interacting agents. 2.",
                "BACKGROUND: FINITELY NESTED IPOMDPS Interactive POMDPs generalize POMDPs to multiagent settings by including other agents models as part of the state space [9].",
                "Since other agents may also reason about others, the interactive state space is strategically nested; it contains beliefs about other agents models and their beliefs about others.",
                "For simplicity of presentation we consider an agent, i, that is interacting with one other agent, j.",
                "A finitely nested I-POMDP of agent i with a strategy level l is defined as the tuple: I-POMDPi,l = ISi,l, A, Ti, Ωi, Oi, Ri where: • ISi,l denotes a set of interactive states defined as, ISi,l = S × Mj,l−1, where Mj,l−1 = {Θj,l−1 ∪ SMj}, for l ≥ 1, and ISi,0 = S, where S is the set of states of the physical environment.",
                "Θj,l−1 is the set of computable intentional models of agent j: θj,l−1 = bj,l−1, ˆθj where the frame, ˆθj = A, Ωj, Tj, Oj, Rj, OCj .",
                "Here, j is Bayes rational and OCj is js optimality criterion.",
                "SMj is the set of subintentional models of j.",
                "Simple examples of subintentional models include a no-information model [10] and a fictitious play model [6], both of which are history independent.",
                "We give a recursive bottom-up construction of the interactive state space below.",
                "ISi,0 = S, Θj,0 = { bj,0, ˆθj | bj,0 ∈ Δ(ISj,0)} ISi,1 = S × {Θj,0 ∪ SMj}, Θj,1 = { bj,1, ˆθj | bj,1 ∈ Δ(ISj,1)} . . . . . .",
                "ISi,l = S × {Θj,l−1 ∪ SMj}, Θj,l = { bj,l, ˆθj | bj,l ∈ Δ(ISj,l)} Similar formulations of nested spaces have appeared in [1, 3]. • A = Ai × Aj is the set of joint actions of all agents in the environment; • Ti : S ×A×S → [0, 1], describes the effect of the joint actions on the physical states of the environment; • Ωi is the set of observations of agent i; • Oi : S × A × Ωi → [0, 1] gives the likelihood of the observations given the physical state and joint action; • Ri : ISi × A → R describes agent is preferences over its interactive states.",
                "Usually only the physical states will matter.",
                "Agent is policy is the mapping, Ω∗ i → Δ(Ai), where Ω∗ i is the set of all observation histories of agent i.",
                "Since belief over the interactive states forms a sufficient statistic [9], the policy can also be represented as a mapping from the set of all beliefs of agent i to a distribution over its actions, Δ(ISi) → Δ(Ai). 2.1 Belief Update Analogous to POMDPs, an agent within the I-POMDP framework updates its belief as it acts and observes.",
                "However, there are two differences that complicate the belief update in multiagent settings when compared to single agent ones.",
                "First, since the state of the physical environment depends on the actions of both agents, is prediction of how the physical state changes has to be made based on its prediction of js actions.",
                "Second, changes in js models have to be included in is belief update.",
                "Specifically, if j is intentional then an update of js beliefs due to its action and observation has to be included.",
                "In other words, i has to update its belief based on its prediction of what j would observe and how j would update its belief.",
                "If js model is subintentional, then js probable observations are appended to the observation history contained in the model.",
                "Formally, we have: Pr(ist |at−1 i , bt−1 i,l ) = β ISt−1:mt−1 j =θt j bt−1 i,l (ist−1 ) × at−1 j Pr(at−1 j |θt−1 j,l−1)Oi(st , at−1 i , at−1 j , ot i) ×Ti(st−1 , at−1 i , at−1 j , st ) ot j Oj(st , at−1 i , at−1 j , ot j) ×τ(SEθt j (bt−1 j,l−1, at−1 j , ot j) − bt j,l−1) (1) where β is the normalizing constant, τ is 1 if its argument is 0 otherwise it is 0, Pr(at−1 j |θt−1 j,l−1) is the probability that at−1 j is Bayes rational for the agent described by model θt−1 j,l−1, and SE(·) is an abbreviation for the belief update.",
                "For a version of the belief update when js model is subintentional, see [9].",
                "If agent j is also modeled as an I-POMDP, then is belief update invokes js belief update (via the term SEθt j ( bt−1 j,l−1 , at−1 j , ot j)), which in turn could invoke is belief update and so on.",
                "This recursion in belief nesting bottoms out at the 0th level.",
                "At this level, the belief update of the agent reduces to a POMDP belief update. 1 For illustrations of the belief update, additional details on I-POMDPs, and how they compare with other multiagent frameworks, see [9]. 2.2 Value Iteration Each belief state in a finitely nested I-POMDP has an associated value reflecting the maximum payoff the agent can expect in this belief state: Un ( bi,l, θi ) = max ai∈Ai is∈ISi,l ERi(is, ai)bi,l(is)+ γ oi∈Ωi Pr(oi|ai, bi,l)Un−1 ( SEθi (bi,l, ai, oi), θi ) (2) where, ERi(is, ai) = aj Ri(is, ai, aj)Pr(aj|mj,l−1) (since is = (s, mj,l−1)).",
                "Eq. 2 is a basis for value iteration in I-POMDPs.",
                "Agent is optimal action, a∗ i , for the case of finite horizon with discounting, is an element of the set of optimal actions for the belief state, OPT(θi), defined as: OPT( bi,l, θi ) = argmax ai∈Ai is∈ISi,l ERi(is, ai)bi,l(is) +γ oi∈Ωi Pr(oi|ai, bi,l)Un ( SEθi (bi,l, ai, oi), θi ) (3) 3.",
                "INTERACTIVEINFLUENCEDIAGRAMS A naive extension of influence diagrams (IDs) to settings populated by multiple agents is possible by treating other agents as automatons, represented using chance nodes.",
                "However, this approach assumes that the agents actions are controlled using a probability distribution that does not change over time.",
                "Interactive influence diagrams (I-IDs) adopt a more sophisticated approach by generalizing IDs to make them applicable to settings shared with other agents who may act and observe, and update their beliefs. 3.1 Syntax In addition to the usual chance, decision, and utility nodes, IIDs include a new type of node called the model node.",
                "We show a general level l I-ID in Fig. 1(a), where the model node (Mj,l−1) is denoted using a hexagon.",
                "We note that the probability distribution over the chance node, S, and the model node together represents agent is belief over its interactive states.",
                "In addition to the model 1 The 0th level model is a POMDP: Other agents actions are treated as exogenous events and folded into the T, O, and R functions.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 815 Figure 1: (a) A generic level l I-ID for agent i situated with one other agent j.",
                "The hexagon is the model node (Mj,l−1) whose structure we show in (b).",
                "Members of the model node are I-IDs themselves (m1 j,l−1, m2 j,l−1; diagrams not shown here for simplicity) whose decision nodes are mapped to the corresponding chance nodes (A1 j , A2 j ).",
                "Depending on the value of the node, Mod[Mj], the distribution of each of the chance nodes is assigned to the node Aj. (c) The transformed I-ID with the model node replaced by the chance nodes and the relationships between them. node, I-IDs differ from IDs by having a dashed link (called the policy link in [15]) between the model node and a chance node, Aj, that represents the distribution over the other agents actions given its model.",
                "In the absence of other agents, the model node and the chance node, Aj, vanish and I-IDs collapse into traditional IDs.",
                "The model node contains the alternative computational models ascribed by i to the other agent from the set, Θj,l−1 ∪ SMj, where Θj,l−1 and SMj were defined previously in Section 2.",
                "Thus, a model in the model node may itself be an I-ID or ID, and the recursion terminates when a model is an ID or subintentional.",
                "Because the model node contains the alternative models of the other agent as its values, its representation is not trivial.",
                "In particular, some of the models within the node are I-IDs that when solved generate the agents optimal policy in their decision nodes.",
                "Each decision node is mapped to the corresponding chance node, say A1 j , in the following way: if OPT is the set of optimal actions obtained by solving the I-ID (or ID), then Pr(aj ∈ A1 j ) = 1 |OP T | if aj ∈ OPT, 0 otherwise.",
                "Borrowing insights from previous work [8], we observe that the model node and the dashed policy link that connects it to the chance node, Aj, could be represented as shown in Fig. 1(b).",
                "The decision node of each level l − 1 I-ID is transformed into a chance node, as we mentioned previously, so that the actions with the largest value in the decision node are assigned uniform probabilities in the chance node while the rest are assigned zero probability.",
                "The different chance nodes (A1 j , A2 j ), one for each model, and additionally, the chance node labeled Mod[Mj] form the parents of the chance node, Aj.",
                "Thus, there are as many action nodes (A1 j , A2 j ) in Mj,l−1 as the number of models in the support of agent is beliefs.",
                "The conditional probability table of the chance node, Aj, is a multiplexer that assumes the distribution of each of the action nodes (A1 j , A2 j ) depending on the value of Mod[Mj].",
                "The values of Mod[Mj] denote the different models of j.",
                "In other words, when Mod[Mj] has the value m1 j,l−1, the chance node Aj assumes the distribution of the node A1 j , and Aj assumes the distribution of A2 j when Mod[Mj] has the value m2 j,l−1.",
                "The distribution over the node, Mod[Mj], is the agent is belief over the models of j given a physical state.",
                "For more agents, we will have as many model nodes as there are agents.",
                "Notice that Fig. 1(b) clarifies the semantics of the policy link, and shows how it can be represented using the traditional dependency links.",
                "In Fig. 1(c), we show the transformed I-ID when the model node is replaced by the chance nodes and relationships between them.",
                "In contrast to the representation in [15], there are no special-purpose policy links, rather the I-ID is composed of only those types of nodes that are found in traditional IDs and dependency relationships between the nodes.",
                "This allows I-IDs to be represented and implemented using conventional application tools that target IDs.",
                "Note that we may view the level l I-ID as a NID.",
                "Specifically, each of the level l − 1 models within the model node are blocks in the NID (see Fig. 2).",
                "If the level l = 1, each block is a traditional ID, otherwise if l > 1, each block within the NID may itself be a NID.",
                "Note that within the I-IDs (or IDs) at each level, there is only a single decision node.",
                "Thus, our NID does not contain any MAIDs.",
                "Figure 2: A level l I-ID represented as a NID.",
                "The probabilities assigned to the blocks of the NID are is beliefs over js models conditioned on a physical state. 3.2 Solution The solution of an I-ID proceeds in a bottom-up manner, and is implemented recursively.",
                "We start by solving the level 0 models, which, if intentional, are traditional IDs.",
                "Their solutions provide probability distributions over the other agents actions, which are entered in the corresponding chance nodes found in the model node of the level 1 I-ID.",
                "The mapping from the level 0 models decision nodes to the chance nodes is carried out so that actions with the largest value in the decision node are assigned uniform probabilities in the chance node while the rest are assigned zero probability.",
                "Given the distributions over the actions within the different chance nodes (one for each model of the other agent), the level 1 I-ID is transformed as shown in Fig. 1(c).",
                "During the transformation, the conditional probability table (CPT) of the node, Aj, is populated such that the node assumes the distribution of each of the chance nodes depending on the value of the node, Mod[Mj].",
                "As we mentioned previously, the values of the node Mod[Mj] denote the different models of the other agent, and its distribution is the agent is belief over the models of j conditioned on the physical state.",
                "The transformed level 1 I-ID is a traditional ID that may be solved us816 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) (a) (b) Figure 3: (a) A generic two time-slice level l I-DID for agent i in a setting with one other agent j.",
                "Notice the dotted model update link that denotes the update of the models of j and the distribution over the models over time. (b) The semantics of the model update link. ing the standard expected utility maximization method [18].",
                "This procedure is carried out up to the level l I-ID whose solution gives the non-empty set of optimal actions that the agent should perform given its belief.",
                "Notice that analogous to IDs, I-IDs are suitable for online decision-making when the agents current belief is known. 4.",
                "INTERACTIVE DYNAMIC INFLUENCE DIAGRAMS Interactive dynamic influence diagrams (I-DIDs) extend I-IDs (and NIDs) to allow sequential decision-making over several time steps.",
                "Just as DIDs are structured graphical representations of POMDPs, I-DIDs are the graphical online analogs for finitely nested I-POMDPs.",
                "I-DIDs may be used to optimize over a finite look-ahead given initial beliefs while interacting with other, possibly similar, agents. 4.1 Syntax We depict a general two time-slice I-DID in Fig. 3(a).",
                "In addition to the model nodes and the dashed policy link, what differentiates an I-DID from a DID is the model update link shown as a dotted arrow in Fig. 3(a).",
                "We explained the semantics of the model node and the policy link in the previous section; we describe the model updates next.",
                "The update of the model node over time involves two steps: First, given the models at time t, we identify the updated set of models that reside in the model node at time t + 1.",
                "Recall from Section 2 that an agents intentional model includes its belief.",
                "Because the agents act and receive observations, their models are updated to reflect their changed beliefs.",
                "Since the set of optimal actions for a model could include all the actions, and the agent may receive any one of |Ωj| possible observations, the updated set at time step t + 1 will have at most |Mt j,l−1||Aj||Ωj| models.",
                "Here, |Mt j,l−1| is the number of models at time step t, |Aj| and |Ωj| are the largest spaces of actions and observations respectively, among all the models.",
                "Second, we compute the new distribution over the updated models given the original distribution and the probability of the agent performing the action and receiving the observation that led to the updated model.",
                "These steps are a part of agent is belief update formalized using Eq. 1.",
                "In Fig. 3(b), we show how the dotted model update link is implemented in the I-DID.",
                "If each of the two level l − 1 models ascribed to j at time step t results in one action, and j could make one of two possible observations, then the model node at time step t + 1 contains four updated models (mt+1,1 j,l−1 ,mt+1,2 j,l−1 , mt+1,3 j,l−1 , and mt+1,4 j,l−1 ).",
                "These models differ in their initial beliefs, each of which is the result of j updating its beliefs due to its action and a possible observation.",
                "The decision nodes in each of the I-DIDs or DIDs that represent the lower level models are mapped to the corresponding Figure 4: Transformed I-DID with the model nodes and model update link replaced with the chance nodes and the relationships (in bold). chance nodes, as mentioned previously.",
                "Next, we describe how the distribution over the updated set of models (the distribution over the chance node Mod[Mt+1 j ] in Mt+1 j,l−1) is computed.",
                "The probability that js updated model is, say mt+1,1 j,l−1 , depends on the probability of j performing the action and receiving the observation that led to this model, and the prior distribution over the models at time step t. Because the chance node At j assumes the distribution of each of the action nodes based on the value of Mod[Mt j ], the probability of the action is given by this chance node.",
                "In order to obtain the probability of js possible observation, we introduce the chance node Oj, which depending on the value of Mod[Mt j ] assumes the distribution of the observation node in the lower level model denoted by Mod[Mt j ].",
                "Because the probability of js observations depends on the physical state and the joint actions of both agents, the node Oj is linked with St+1 , At j, and At i. 2 Analogous to At j, the conditional probability table of Oj is also a multiplexer modulated by Mod[Mt j ].",
                "Finally, the distribution over the prior models at time t is obtained from the chance node, Mod[Mt j ] in Mt j,l−1.",
                "Consequently, the chance nodes, Mod[Mt j ], At j, and Oj, form the parents of Mod[Mt+1 j ] in Mt+1 j,l−1.",
                "Notice that the model update link may be replaced by the dependency links between the chance nodes that constitute the model nodes in the two time slices.",
                "In Fig. 4 we show the two time-slice I-DID with the model nodes replaced by the chance nodes and the relationships between them.",
                "Chance nodes and dependency links that not in bold are standard, usually found in DIDs.",
                "Expansion of the I-DID over more time steps requires the repetition of the two steps of updating the set of models that form the 2 Note that Oj represents js observation at time t + 1.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 817 values of the model node and adding the relationships between the chance nodes, as many times as there are model update links.",
                "We note that the possible set of models of the other agent j grows exponentially with the number of time steps.",
                "For example, after T steps, there may be at most |Mt=1 j,l−1|(|Aj||Ωj|)T −1 candidate models residing in the model node. 4.2 Solution Analogous to I-IDs, the solution to a level l I-DID for agent i expanded over T time steps may be carried out recursively.",
                "For the purpose of illustration, let l=1 and T=2.",
                "The solution method uses the standard look-ahead technique, projecting the agents action and observation sequences forward from the current belief state [17], and finding the possible beliefs that i could have in the next time step.",
                "Because agent i has a belief over js models as well, the lookahead includes finding out the possible models that j could have in the future.",
                "Consequently, each of js subintentional or level 0 models (represented using a standard DID) in the first time step must be solved to obtain its optimal set of actions.",
                "These actions are combined with the set of possible observations that j could make in that model, resulting in an updated set of candidate models (that include the updated beliefs) that could describe the behavior of j. Beliefs over this updated set of candidate models are calculated using the standard inference methods using the dependency relationships between the model nodes as shown in Fig. 3(b).",
                "We note the recursive nature of this solution: in solving agent is level 1 I-DID, js level 0 DIDs must be solved.",
                "If the nesting of models is deeper, all models at all levels starting from 0 are solved in a bottom-up manner.",
                "We briefly outline the recursive algorithm for solving agent is Algorithm for solving I-DID Input : level l ≥ 1 I-ID or level 0 ID, T Expansion Phase 1.",
                "For t from 1 to T − 1 do 2.",
                "If l ≥ 1 then Populate Mt+1 j,l−1 3.",
                "For each mt j in Range(Mt j,l−1) do 4.",
                "Recursively call algorithm with the l − 1 I-ID (or ID) that represents mt j and the horizon, T − t + 1 5.",
                "Map the decision node of the solved I-ID (or ID), OPT(mt j), to a chance node Aj 6.",
                "For each aj in OPT(mt j) do 7.",
                "For each oj in Oj (part of mt j) do 8.",
                "Update js belief, bt+1 j ← SE(bt j, aj, oj) 9. mt+1 j ← New I-ID (or ID) with bt+1 j as the initial belief 10.",
                "Range(Mt+1 j,l−1) ∪ ← {mt+1 j } 11.",
                "Add the model node, Mt+1 j,l−1, and the dependency links between Mt j,l−1 and Mt+1 j,l−1 (shown in Fig. 3(b)) 12.",
                "Add the chance, decision, and utility nodes for t + 1 time slice and the dependency links between them 13.",
                "Establish the CPTs for each chance node and utility node Look-Ahead Phase 14.",
                "Apply the standard look-ahead and backup method to solve the expanded I-DID Figure 5: Algorithm for solving a level l ≥ 0 I-DID. level l I-DID expanded over T time steps with one other agent j in Fig. 5.",
                "We adopt a two-phase approach: Given an I-ID of level l (described previously in Section 3) with all lower level models also represented as I-IDs or IDs (if level 0), the first step is to expand the level l I-ID over T time steps adding the dependency links and the conditional probability tables for each node.",
                "We particularly focus on establishing and populating the model nodes (lines 3-11).",
                "Note that Range(·) returns the values (lower level models) of the random variable given as input (model node).",
                "In the second phase, we use a standard look-ahead technique projecting the action and observation sequences over T time steps in the future, and backing up the utility values of the reachable beliefs.",
                "Similar to I-IDs, the I-DIDs reduce to DIDs in the absence of other agents.",
                "As we mentioned previously, the 0-th level models are the traditional DIDs.",
                "Their solutions provide probability distributions over actions of the agent modeled at that level to I-DIDs at level 1.",
                "Given probability distributions over other agents actions the level 1 IDIDs can themselves be solved as DIDs, and provide probability distributions to yet higher level models.",
                "Assume that the number of models considered at each level is bound by a number, M. Solving an I-DID of level l in then equivalent to solving O(Ml ) DIDs. 5.",
                "EXAMPLE APPLICATIONS To illustrate the usefulness of I-DIDs, we apply them to three problem domains.",
                "We describe, in particular, the formulation of the I-DID and the optimal prescriptions obtained on solving it. 5.1 Followership-Leadership in the Multiagent Tiger Problem We begin our illustrations of using I-IDs and I-DIDs with a slightly modified version of the multiagent tiger problem discussed in [9].",
                "The problem has two agents, each of which can open the right door (OR), the left door (OL) or listen (L).",
                "In addition to hearing growls (from the left (GL) or from the right (GR)) when they listen, the agents also hear creaks (from the left (CL), from the right (CR), or no creaks (S)), which noisily indicate the other agents opening one of the doors.",
                "When any door is opened, the tiger persists in its original location with a probability of 95%.",
                "Agent i hears growls with a reliability of 65% and creaks with a reliability of 95%.",
                "Agent j, on the other hand, hears growls with a reliability of 95%.",
                "Thus, the setting is such that agent i hears agent j opening doors more reliably than the tigers growls.",
                "This suggests that i could use js actions as an indication of the location of the tiger, as we discuss below.",
                "Each agents preferences are as in the single agent game discussed in [13].",
                "The transition, observation, and reward functions are shown in [16].",
                "A good indicator of the usefulness of normative methods for decision-making like I-DIDs is the emergence of realistic social behaviors in their prescriptions.",
                "In settings of the persistent multiagent tiger problem that reflect real world situations, we demonstrate followership between the agents and, as shown in [15], deception among agents who believe that they are in a follower-leader type of relationship.",
                "In particular, we analyze the situational and epistemological conditions sufficient for their emergence.",
                "The followership behavior, for example, results from the agent knowing its own weaknesses, assessing the strengths, preferences, and possible behaviors of the other, and realizing that its best for it to follow the others actions in order to maximize its payoffs.",
                "Let us consider a particular setting of the tiger problem in which agent i believes that js preferences are aligned with its own - both of them just want to get the gold - and js hearing is more reliable in comparison to itself.",
                "As an example, suppose that j, on listening can discern the tigers location 95% of the times compared to is 65% accuracy.",
                "Additionally, agent i does not have any initial information about the tigers location.",
                "In other words, is single-level nested belief, bi,1, assigns 0.5 to each of the two locations of the tiger.",
                "In addition, i considers two models of j, which differ in js flat level 0 initial beliefs.",
                "This is represented in the level 1 I-ID shown in Fig. 6(a).",
                "According to one model, j assigns a probability of 0.9 that the tiger is behind the left door, while the other 818 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Figure 6: (a) Level 1 I-ID of agent i, (b) two level 0 IDs of agent j whose decision nodes are mapped to the chance nodes, A1 j , A2 j , in (a). model assigns 0.1 to that location (see Fig. 6(b)).",
                "Agent i is undecided on these two models of j.",
                "If we vary is hearing ability, and solve the corresponding level 1 I-ID expanded over three time steps, we obtain the normative behavioral policies shown in Fig 7 that exhibit followership behavior.",
                "If is probability of correctly hearing the growls is 0.65, then as shown in the policy in Fig. 7(a), i begins to conditionally follow js actions: i opens the same door that j opened previously iff is own assessment of the tigers location confirms js pick.",
                "If i loses the ability to correctly interpret the growls completely, it blindly follows j and opens the same door that j opened previously (Fig. 7(b)).",
                "Figure 7: Emergence of (a) conditional followership, and (b) blind followership in the tiger problem.",
                "Behaviors of interest are in bold. * is a wildcard, and denotes any one of the observations.",
                "We observed that a single level of belief nesting - beliefs about the others models - was sufficient for followership to emerge in the tiger problem.",
                "However, the epistemological requirements for the emergence of leadership are more complex.",
                "For an agent, say j, to emerge as a leader, followership must first emerge in the other agent i.",
                "As we mentioned previously, if i is certain that its preferences are identical to those of j, and believes that j has a better sense of hearing, i will follow js actions over time.",
                "Agent j emerges as a leader if it believes that i will follow it, which implies that js belief must be nested two levels deep to enable it to recognize its leadership role.",
                "Realizing that i will follow presents j with an opportunity to influence is actions in the benefit of the collective good or its self-interest alone.",
                "For example, in the tiger problem, let us consider a setting in which if both i and j open the correct door, then each gets a payoff of 20 that is double the original.",
                "If j alone selects the correct door, it gets the payoff of 10.",
                "On the other hand, if both agents pick the wrong door, their penalties are cut in half.",
                "In this setting, it is in both js best interest as well as the collective betterment for j to use its expertise in selecting the correct door, and thus be a good leader.",
                "However, consider a slightly different problem in which j gains from is loss and is penalized if i gains.",
                "Specifically, let is payoff be subtracted from js, indicating that j is antagonistic toward i - if j picks the correct door and i the wrong one, then is loss of 100 becomes js gain.",
                "Agent j believes that i incorrectly thinks that js preferences are those that promote the collective good and that it starts off by believing with 99% confidence where the tiger is.",
                "Because i believes that its preferences are similar to those of j, and that j starts by believing almost surely that one of the two is the correct location (two level 0 models of j), i will start by following js actions.",
                "We show is normative policy on solving its singly-nested I-DID over three time steps in Fig. 8(a).",
                "The policy demonstrates that i will blindly follow js actions.",
                "Since the tiger persists in its original location with a probability of 0.95, i will select the same door again.",
                "If j begins the game with a 99% probability that the tiger is on the right, solving js I-DID nested two levels deep, results in the policy shown in Fig. 8(b).",
                "Even though j is almost certain that OL is the correct action, it will start by selecting OR, followed by OL.",
                "Agent js intention is to deceive i who, it believes, will follow js actions, so as to gain $110 in the second time step, which is more than what j would gain if it were to be honest.",
                "Figure 8: Emergence of deception between agents in the tiger problem.",
                "Behaviors of interest are in bold. * denotes as before. (a) Agent is policy demonstrating that it will blindly follow js actions. (b) Even though j is almost certain that the tiger is on the right, it will start by selecting OR, followed by OL, in order to deceive i. 5.2 Altruism and Reciprocity in the Public Good Problem The public good (PG) problem [7], consists of a group of M agents, each of whom must either contribute some resource to a public pot or keep it for themselves.",
                "Since resources contributed to the public pot are shared among all the agents, they are less valuable to the agent when in the public pot.",
                "However, if all agents choose to contribute their resources, then the payoff to each agent is more than if no one contributes.",
                "Since an agent gets its share of the public pot irrespective of whether it has contributed or not, the dominating action is for each agent to not contribute, and instead free ride on others contributions.",
                "However, behaviors of human players in empirical simulations of the PG problem differ from the normative predictions.",
                "The experiments reveal that many players initially contribute a large amount to the public pot, and continue to contribute when the PG problem is played repeatedly, though in decreasing amounts [4].",
                "Many of these experiments [5] report that a small core group of players persistently contributes to the public pot even when all others are defecting.",
                "These experiments also reveal that players who persistently contribute have altruistic or reciprocal preferences matching expected cooperation of others.",
                "For simplicity, we assume that the game is played between M = 2 agents, i and j.",
                "Let each agent be initially endowed with XT amount of resources.",
                "While the classical PG game formulation permits each agent to contribute any quantity of resources (≤ XT ) to the public pot, we simplify the action space by allowing two possible actions.",
                "Each agent may choose to either contribute (C) a fixed amount of the resources, or not contribute.",
                "The latter action is deThe Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 819 noted as defect (D).",
                "We assume that the actions are not observable to others.",
                "The value of resources in the public pot is discounted by ci for each agent i, where ci is the marginal private return.",
                "We assume that ci < 1 so that the agent does not benefit enough that it contributes to the public pot for private gain.",
                "Simultaneously, ciM > 1, making collective contribution pareto optimal. i/j C D C 2ciXT , 2cjXT ciXT − cp, XT + cjXT − P D XT + ciXT − P, cjXT − cp XT , XT Table 1: The one-shot PG game with punishment.",
                "In order to encourage contributions, the contributing agents punish free riders but incur a small cost for administering the punishment.",
                "Let P be the punishment meted out to the defecting agent and cp the non-zero cost of punishing for the contributing agent.",
                "For simplicity, we assume that the cost of punishing is same for both the agents.",
                "The one-shot PG game with punishment is shown in Table. 1.",
                "Let ci = cj, cp > 0, and if P > XT − ciXT , then defection is no longer a dominating action.",
                "If P < XT − ciXT , then defection is the dominating action for both.",
                "If P = XT − ciXT , then the game is not dominance-solvable.",
                "Figure 9: (a) Level 1 I-ID of agent i, (b) level 0 IDs of agent j with decision nodes mapped to the chance nodes, A1 j and A2 j , in (a).",
                "We formulate a sequential version of the PG problem with punishment from the perspective of agent i.",
                "Though in the repeated PG game, the quantity in the public pot is revealed to all the agents after each round of actions, we assume in our formulation that it is hidden from the agents.",
                "Each agent may contribute a fixed amount, xc, or defect.",
                "An agent on performing an action receives an observation of plenty (PY) or meager (MR) symbolizing the state of the public pot.",
                "Notice that the observations are also indirectly indicative of agent js actions because the state of the public pot is influenced by them.",
                "The amount of resources in agent is private pot, is perfectly observable to i.",
                "The payoffs are analogous to Table. 1.",
                "Borrowing from the empirical investigations of the PG problem [5], we construct level 0 IDs for j that model altruistic and non-altruistic types (Fig. 9(b)).",
                "Specifically, our altruistic agent has a high marginal private return (cj is close to 1) and does not punish others who defect.",
                "Let xc = 1 and the level 0 agent be punished half the times it defects.",
                "With one action remaining, both types of agents choose to contribute to avoid being punished.",
                "With two actions to go, the altruistic type chooses to contribute, while the other defects.",
                "This is because cj for the altruistic type is close to 1, thus the expected punishment, 0.5P > (1 − cj), which the altruistic type avoids.",
                "Because cj for the non-altruistic type is less, it prefers not to contribute.",
                "With three steps to go, the altruistic agent contributes to avoid punishment (0.5P > 2(1 − cj)), and the non-altruistic type defects.",
                "For greater than three steps, while the altruistic agent continues to contribute to the public pot depending on how close its marginal private return is to 1, the non-altruistic type prescribes defection.",
                "We analyzed the decisions of an altruistic agent i modeled using a level 1 I-DID expanded over 3 time steps. i ascribes the two level 0 models, mentioned previously, to j (see Fig. 9).",
                "If i believes with a probability 1 that j is altruistic, i chooses to contribute for each of the three steps.",
                "This behavior persists when i is unaware of whether j is altruistic (Fig. 10(a)), and when i assigns a high probability to j being the non-altruistic type.",
                "However, when i believes with a probability 1 that j is non-altruistic and will thus surely defect, i chooses to defect to avoid being punished and because its marginal private return is less than 1.",
                "These results demonstrate that the behavior of our altruistic type resembles that found experimentally.",
                "The non-altruistic level 1 agent chooses to defect regardless of how likely it believes the other agent to be altruistic.",
                "We analyzed the behavior of a reciprocal agent type that matches expected cooperation or defection.",
                "The reciprocal types marginal private return is similar to that of the non-altruistic type, however, it obtains a greater payoff when its action is similar to that of the other.",
                "We consider the case when the reciprocal agent i is unsure of whether j is altruistic and believes that the public pot is likely to be half full.",
                "For this prior belief, i chooses to defect.",
                "On receiving an observation of plenty, i decides to contribute, while an observation of meager makes it defect (Fig. 10(b)).",
                "This is because an observation of plenty signals that the pot is likely to be greater than half full, which results from js action to contribute.",
                "Thus, among the two models ascribed to j, its type is likely to be altruistic making it likely that j will contribute again in the next time step.",
                "Agent i therefore chooses to contribute to reciprocate js action.",
                "An analogous reasoning leads i to defect when it observes a meager pot.",
                "With one action to go, i believing that j contributes, will choose to contribute too to avoid punishment regardless of its observations.",
                "Figure 10: (a) An altruistic level 1 agent always contributes. (b) A reciprocal agent i starts off by defecting followed by choosing to contribute or defect based on its observation of plenty (indicating that j is likely altruistic) or meager (j is non-altruistic). 5.3 Strategies in Two-Player Poker Poker is a popular zero sum card game that has received much attention among the AI research community as a testbed [2].",
                "Poker is played among M ≥ 2 players in which each player receives a hand of cards from a deck.",
                "While several flavors of Poker with varying complexity exist, we consider a simple version in which each player has three plys during which the player may either exchange a card (E), keep the existing hand (K), fold (F) and withdraw from the game, or call (C), requiring all players to show their hands.",
                "To keep matters simple, let M = 2, and each player receive a hand consisting of a single card drawn from the same suit.",
                "Thus, during a showdown, the player who has the numerically larger card (2 is the lowest, ace is the highest) wins the pot.",
                "During an exchange of cards, the discarded card is placed either in the L pile, indicating to the other agent that it was a low numbered card less than 8, or in the 820 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) H pile, indicating that the card had a rank greater than or equal to 8.",
                "Notice that, for example, if a lower numbered card is discarded, the probability of receiving a low card in exchange is now reduced.",
                "We show the level 1 I-ID for the simplified two-player Poker in Fig. 11.",
                "We considered two models (personality types) of agent j.",
                "The conservative type believes that it is likely that its opponent has a high numbered card in its hand.",
                "On the other hand, the aggressive agent j believes with a high probability that its opponent has a lower numbered card.",
                "Thus, the two types differ in their beliefs over their opponents hand.",
                "In both these level 0 models, the opponent is assumed to perform its actions following a fixed, uniform distribution.",
                "With three actions to go, regardless of its hand (unless it is an ace), the aggressive agent chooses to exchange its card, with the intent of improving on its current hand.",
                "This is because it believes the other to have a low card, which improves its chances of getting a high card during the exchange.",
                "The conservative agent chooses to keep its card, no matter its hand because its chances of getting a high card are slim as it believes that its opponent has one.",
                "Figure 11: (a) Level 1 I-ID of agent i.",
                "The observation reveals information about js hand of the previous time step, (b) level 0 IDs of agent j whose decision nodes are mapped to the chance nodes, A1 j , A2 j , in (a).",
                "The policy of a level 1 agent i who believes that each card except its own has an equal likelihood of being in js hand (neutral personality type) and j could be either an aggressive or conservative type, is shown in Fig. 12. is own hand contains the card numbered 8.",
                "The agent starts by keeping its card.",
                "On seeing that j did not exchange a card (N), i believes with probability 1 that j is conservative and hence will keep its cards. i responds by either keeping its card or exchanging it because j is equally likely to have a lower or higher card.",
                "If i observes that j discarded its card into the L or H pile, i believes that j is aggressive.",
                "On observing L, i realizes that j had a low card, and is likely to have a high card after its exchange.",
                "Because the probability of receiving a low card is high now, i chooses to keep its card.",
                "On observing H, believing that the probability of receiving a high numbered card is high, i chooses to exchange its card.",
                "In the final step, i chooses to call regardless of its observation history because its belief that j has a higher card is not sufficiently high to conclude that its better to fold and relinquish the payoff.",
                "This is partly due to the fact that an observation of, say, L resets the agent is previous time step beliefs over js hand to the low numbered cards only. 6.",
                "DISCUSSION We showed how DIDs may be extended to I-DIDs that enable online sequential decision-making in uncertain multiagent settings.",
                "Our graphical representation of I-DIDs improves on the previous Figure 12: A level 1 agent is three step policy in the Poker problem. i starts by believing that j is equally likely to be aggressive or conservative and could have any card in its hand with equal probability. work significantly by being more transparent, semantically clear, and capable of being solved using standard algorithms that target DIDs.",
                "I-DIDs extend NIDs to allow sequential decision-making over multiple time steps in the presence of other interacting agents.",
                "I-DIDs may be seen as concise graphical representations for IPOMDPs providing a way to exploit problem structure and carry out online decision-making as the agent acts and observes given its prior beliefs.",
                "We are currently investigating ways to solve I-DIDs approximately with provable bounds on the solution quality.",
                "Acknowledgment: We thank Piotr Gmytrasiewicz for some useful discussions related to this work.",
                "The first author would like to acknowledge the support of a UGARF grant. 7.",
                "REFERENCES [1] R. J. Aumann.",
                "Interactive epistemology i: Knowledge.",
                "International Journal of Game Theory, 28:263-300, 1999. [2] D. Billings, A. Davidson, J. Schaeffer, and D. Szafron.",
                "The challenge of poker.",
                "AIJ, 2001. [3] A. Brandenburger and E. Dekel.",
                "Hierarchies of beliefs and common knowledge.",
                "Journal of Economic Theory, 59:189-198, 1993. [4] C. Camerer.",
                "Behavioral Game Theory: Experiments in Strategic Interaction.",
                "Princeton University Press, 2003. [5] E. Fehr and S. Gachter.",
                "Cooperation and punishment in public goods experiments.",
                "American Economic Review, 90(4):980-994, 2000. [6] D. Fudenberg and D. K. Levine.",
                "The Theory of Learning in Games.",
                "MIT Press, 1998. [7] D. Fudenberg and J. Tirole.",
                "Game Theory.",
                "MIT Press, 1991. [8] Y. Gal and A. Pfeffer.",
                "A language for modeling agents decision-making processes in games.",
                "In AAMAS, 2003. [9] P. Gmytrasiewicz and P. Doshi.",
                "A framework for sequential planning in multiagent settings.",
                "JAIR, 24:49-79, 2005. [10] P. Gmytrasiewicz and E. Durfee.",
                "Rational coordination in multi-agent environments.",
                "JAAMAS, 3(4):319-350, 2000. [11] J. C. Harsanyi.",
                "Games with incomplete information played by bayesian players.",
                "Management Science, 14(3):159-182, 1967. [12] R. A. Howard and J. E. Matheson.",
                "Influence diagrams.",
                "In R. A. Howard and J. E. Matheson, editors, The Principles and Applications of Decision Analysis.",
                "Strategic Decisions Group, Menlo Park, CA 94025, 1984. [13] L. Kaelbling, M. Littman, and A. Cassandra.",
                "Planning and acting in partially observable stochastic domains.",
                "Artificial Intelligence Journal, 2, 1998. [14] D. Koller and B. Milch.",
                "Multi-agent influence diagrams for representing and solving games.",
                "In IJCAI, pages 1027-1034, 2001. [15] K. Polich and P. Gmytrasiewicz.",
                "Interactive dynamic influence diagrams.",
                "In GTDT Workshop, AAMAS, 2006. [16] B.",
                "Rathnas., P. Doshi, and P. J. Gmytrasiewicz.",
                "Exact solutions to interactive pomdps using behavioral equivalence.",
                "In Autonomous Agents and Multi-Agent Systems Conference (AAMAS), 2006. [17] S. Russell and P. Norvig.",
                "Artificial Intelligence: A Modern Approach (Second Edition).",
                "Prentice Hall, 2003. [18] R. D. Shachter.",
                "Evaluating influence diagrams.",
                "Operations Research, 34(6):871-882, 1986. [19] D. Suryadi and P. Gmytrasiewicz.",
                "Learning models of other agents using influence diagrams.",
                "In UM, 1999.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 821"
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [],
            "translated_text": "",
            "candidates": [],
            "error": [
                []
            ]
        },
        "multiplexer": {
            "translated_key": "multiplexor",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Graphical Models for Online Solutions to Interactive POMDPs Prashant Doshi Dept.",
                "of Computer Science University of Georgia Athens, GA 30602, USA pdoshi@cs.uga.edu Yifeng Zeng Dept. of Computer Science Aalborg University DK-9220 Aalborg, Denmark yfzeng@cs.aau.edu Qiongyu Chen Dept. of Computer Science National Univ. of Singapore 117543, Singapore chenqy@comp.nus.edu.sg ABSTRACT We develop a new graphical representation for interactive partially observable Markov decision processes (I-POMDPs) that is significantly more transparent and semantically clear than the previous representation.",
                "These graphical models called interactive dynamic influence diagrams (I-DIDs) seek to explicitly model the structure that is often present in real-world problems by decomposing the situation into chance and decision variables, and the dependencies between the variables.",
                "I-DIDs generalize DIDs, which may be viewed as graphical representations of POMDPs, to multiagent settings in the same way that I-POMDPs generalize POMDPs.",
                "I-DIDs may be used to compute the policy of an agent online as the agent acts and observes in a setting that is populated by other interacting agents.",
                "Using several examples, we show how I-DIDs may be applied and demonstrate their usefulness.",
                "Categories and Subject Descriptors I.2.11 [Distributed Artificial Intelligence]: Multiagent Systems General Terms Theory 1.",
                "INTRODUCTION Interactive partially observable Markov decision processes (IPOMDPs) [9] provide a framework for sequential decision-making in partially observable multiagent environments.",
                "They generalize POMDPs [13] to multiagent settings by including the other agents computable models in the state space along with the states of the physical environment.",
                "The models encompass all information influencing the agents behaviors, including their preferences, capabilities, and beliefs, and are thus analogous to types in Bayesian games [11].",
                "I-POMDPs adopt a subjective approach to understanding strategic behavior, rooted in a decision-theoretic framework that takes a decision-makers perspective in the interaction.",
                "In [15], Polich and Gmytrasiewicz introduced interactive dynamic influence diagrams (I-DIDs) as the computational representations of I-POMDPs.",
                "I-DIDs generalize DIDs [12], which may be viewed as computational counterparts of POMDPs, to multiagents settings in the same way that I-POMDPs generalize POMDPs.",
                "I-DIDs contribute to a growing line of work [19] that includes multi-agent influence diagrams (MAIDs) [14], and more recently, networks of influence diagrams (NIDs) [8].",
                "These formalisms seek to explicitly model the structure that is often present in real-world problems by decomposing the situation into chance and decision variables, and the dependencies between the variables.",
                "MAIDs provide an alternative to normal and extensive game forms using a graphical formalism to represent games of imperfect information with a decision node for each agents actions and chance nodes capturing the agents private information.",
                "MAIDs objectively analyze the game, efficiently computing the Nash equilibrium profile by exploiting the independence structure.",
                "NIDs extend MAIDs to include agents uncertainty over the game being played and over models of the other agents.",
                "Each model is a MAID and the network of MAIDs is collapsed, bottom up, into a single MAID for computing the equilibrium of the game keeping in mind the different models of each agent.",
                "Graphical formalisms such as MAIDs and NIDs open up a promising area of research that aims to represent multiagent interactions more transparently.",
                "However, MAIDs provide an analysis of the game from an external viewpoint and the applicability of both is limited to static single play games.",
                "Matters are more complex when we consider interactions that are extended over time, where predictions about others future actions must be made using models that change as the agents act and observe.",
                "I-DIDs address this gap by allowing the representation of other agents models as the values of a special model node.",
                "Both, other agents models and the original agents beliefs over these models are updated over time using special-purpose implementations.",
                "In this paper, we improve on the previous preliminary representation of the I-DID shown in [15] by using the insight that the static I-ID is a type of NID.",
                "Thus, we may utilize NID-specific language constructs such as multiplexers to represent the model node, and subsequently the I-ID, more transparently.",
                "Furthermore, we clarify the semantics of the special purpose policy link introduced in the representation of I-DID by [15], and show that it could be replaced by traditional dependency links.",
                "In the previous representation of the I-DID, the update of the agents belief over the models of others as the agents act and receive observations was denoted using a special link called the model update link that connected the model nodes over time.",
                "We explicate the semantics of this link by showing how it can be implemented using the traditional dependency links between the chance nodes that constitute the model nodes.",
                "The net result is a representation of I-DID that is significantly more transparent, semantically clear, and capable of being implemented using the standard algorithms for solving DIDs.",
                "We show how IDIDs may be used to model an agents uncertainty over others models, that may themselves be I-DIDs.",
                "Solution to the I-DID is a policy that prescribes what the agent should do over time, given its beliefs over the physical state and others models.",
                "Analogous to DIDs, I-DIDs may be used to compute the policy of an agent online as the agent acts and observes in a setting that is populated by other interacting agents. 2.",
                "BACKGROUND: FINITELY NESTED IPOMDPS Interactive POMDPs generalize POMDPs to multiagent settings by including other agents models as part of the state space [9].",
                "Since other agents may also reason about others, the interactive state space is strategically nested; it contains beliefs about other agents models and their beliefs about others.",
                "For simplicity of presentation we consider an agent, i, that is interacting with one other agent, j.",
                "A finitely nested I-POMDP of agent i with a strategy level l is defined as the tuple: I-POMDPi,l = ISi,l, A, Ti, Ωi, Oi, Ri where: • ISi,l denotes a set of interactive states defined as, ISi,l = S × Mj,l−1, where Mj,l−1 = {Θj,l−1 ∪ SMj}, for l ≥ 1, and ISi,0 = S, where S is the set of states of the physical environment.",
                "Θj,l−1 is the set of computable intentional models of agent j: θj,l−1 = bj,l−1, ˆθj where the frame, ˆθj = A, Ωj, Tj, Oj, Rj, OCj .",
                "Here, j is Bayes rational and OCj is js optimality criterion.",
                "SMj is the set of subintentional models of j.",
                "Simple examples of subintentional models include a no-information model [10] and a fictitious play model [6], both of which are history independent.",
                "We give a recursive bottom-up construction of the interactive state space below.",
                "ISi,0 = S, Θj,0 = { bj,0, ˆθj | bj,0 ∈ Δ(ISj,0)} ISi,1 = S × {Θj,0 ∪ SMj}, Θj,1 = { bj,1, ˆθj | bj,1 ∈ Δ(ISj,1)} . . . . . .",
                "ISi,l = S × {Θj,l−1 ∪ SMj}, Θj,l = { bj,l, ˆθj | bj,l ∈ Δ(ISj,l)} Similar formulations of nested spaces have appeared in [1, 3]. • A = Ai × Aj is the set of joint actions of all agents in the environment; • Ti : S ×A×S → [0, 1], describes the effect of the joint actions on the physical states of the environment; • Ωi is the set of observations of agent i; • Oi : S × A × Ωi → [0, 1] gives the likelihood of the observations given the physical state and joint action; • Ri : ISi × A → R describes agent is preferences over its interactive states.",
                "Usually only the physical states will matter.",
                "Agent is policy is the mapping, Ω∗ i → Δ(Ai), where Ω∗ i is the set of all observation histories of agent i.",
                "Since belief over the interactive states forms a sufficient statistic [9], the policy can also be represented as a mapping from the set of all beliefs of agent i to a distribution over its actions, Δ(ISi) → Δ(Ai). 2.1 Belief Update Analogous to POMDPs, an agent within the I-POMDP framework updates its belief as it acts and observes.",
                "However, there are two differences that complicate the belief update in multiagent settings when compared to single agent ones.",
                "First, since the state of the physical environment depends on the actions of both agents, is prediction of how the physical state changes has to be made based on its prediction of js actions.",
                "Second, changes in js models have to be included in is belief update.",
                "Specifically, if j is intentional then an update of js beliefs due to its action and observation has to be included.",
                "In other words, i has to update its belief based on its prediction of what j would observe and how j would update its belief.",
                "If js model is subintentional, then js probable observations are appended to the observation history contained in the model.",
                "Formally, we have: Pr(ist |at−1 i , bt−1 i,l ) = β ISt−1:mt−1 j =θt j bt−1 i,l (ist−1 ) × at−1 j Pr(at−1 j |θt−1 j,l−1)Oi(st , at−1 i , at−1 j , ot i) ×Ti(st−1 , at−1 i , at−1 j , st ) ot j Oj(st , at−1 i , at−1 j , ot j) ×τ(SEθt j (bt−1 j,l−1, at−1 j , ot j) − bt j,l−1) (1) where β is the normalizing constant, τ is 1 if its argument is 0 otherwise it is 0, Pr(at−1 j |θt−1 j,l−1) is the probability that at−1 j is Bayes rational for the agent described by model θt−1 j,l−1, and SE(·) is an abbreviation for the belief update.",
                "For a version of the belief update when js model is subintentional, see [9].",
                "If agent j is also modeled as an I-POMDP, then is belief update invokes js belief update (via the term SEθt j ( bt−1 j,l−1 , at−1 j , ot j)), which in turn could invoke is belief update and so on.",
                "This recursion in belief nesting bottoms out at the 0th level.",
                "At this level, the belief update of the agent reduces to a POMDP belief update. 1 For illustrations of the belief update, additional details on I-POMDPs, and how they compare with other multiagent frameworks, see [9]. 2.2 Value Iteration Each belief state in a finitely nested I-POMDP has an associated value reflecting the maximum payoff the agent can expect in this belief state: Un ( bi,l, θi ) = max ai∈Ai is∈ISi,l ERi(is, ai)bi,l(is)+ γ oi∈Ωi Pr(oi|ai, bi,l)Un−1 ( SEθi (bi,l, ai, oi), θi ) (2) where, ERi(is, ai) = aj Ri(is, ai, aj)Pr(aj|mj,l−1) (since is = (s, mj,l−1)).",
                "Eq. 2 is a basis for value iteration in I-POMDPs.",
                "Agent is optimal action, a∗ i , for the case of finite horizon with discounting, is an element of the set of optimal actions for the belief state, OPT(θi), defined as: OPT( bi,l, θi ) = argmax ai∈Ai is∈ISi,l ERi(is, ai)bi,l(is) +γ oi∈Ωi Pr(oi|ai, bi,l)Un ( SEθi (bi,l, ai, oi), θi ) (3) 3.",
                "INTERACTIVEINFLUENCEDIAGRAMS A naive extension of influence diagrams (IDs) to settings populated by multiple agents is possible by treating other agents as automatons, represented using chance nodes.",
                "However, this approach assumes that the agents actions are controlled using a probability distribution that does not change over time.",
                "Interactive influence diagrams (I-IDs) adopt a more sophisticated approach by generalizing IDs to make them applicable to settings shared with other agents who may act and observe, and update their beliefs. 3.1 Syntax In addition to the usual chance, decision, and utility nodes, IIDs include a new type of node called the model node.",
                "We show a general level l I-ID in Fig. 1(a), where the model node (Mj,l−1) is denoted using a hexagon.",
                "We note that the probability distribution over the chance node, S, and the model node together represents agent is belief over its interactive states.",
                "In addition to the model 1 The 0th level model is a POMDP: Other agents actions are treated as exogenous events and folded into the T, O, and R functions.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 815 Figure 1: (a) A generic level l I-ID for agent i situated with one other agent j.",
                "The hexagon is the model node (Mj,l−1) whose structure we show in (b).",
                "Members of the model node are I-IDs themselves (m1 j,l−1, m2 j,l−1; diagrams not shown here for simplicity) whose decision nodes are mapped to the corresponding chance nodes (A1 j , A2 j ).",
                "Depending on the value of the node, Mod[Mj], the distribution of each of the chance nodes is assigned to the node Aj. (c) The transformed I-ID with the model node replaced by the chance nodes and the relationships between them. node, I-IDs differ from IDs by having a dashed link (called the policy link in [15]) between the model node and a chance node, Aj, that represents the distribution over the other agents actions given its model.",
                "In the absence of other agents, the model node and the chance node, Aj, vanish and I-IDs collapse into traditional IDs.",
                "The model node contains the alternative computational models ascribed by i to the other agent from the set, Θj,l−1 ∪ SMj, where Θj,l−1 and SMj were defined previously in Section 2.",
                "Thus, a model in the model node may itself be an I-ID or ID, and the recursion terminates when a model is an ID or subintentional.",
                "Because the model node contains the alternative models of the other agent as its values, its representation is not trivial.",
                "In particular, some of the models within the node are I-IDs that when solved generate the agents optimal policy in their decision nodes.",
                "Each decision node is mapped to the corresponding chance node, say A1 j , in the following way: if OPT is the set of optimal actions obtained by solving the I-ID (or ID), then Pr(aj ∈ A1 j ) = 1 |OP T | if aj ∈ OPT, 0 otherwise.",
                "Borrowing insights from previous work [8], we observe that the model node and the dashed policy link that connects it to the chance node, Aj, could be represented as shown in Fig. 1(b).",
                "The decision node of each level l − 1 I-ID is transformed into a chance node, as we mentioned previously, so that the actions with the largest value in the decision node are assigned uniform probabilities in the chance node while the rest are assigned zero probability.",
                "The different chance nodes (A1 j , A2 j ), one for each model, and additionally, the chance node labeled Mod[Mj] form the parents of the chance node, Aj.",
                "Thus, there are as many action nodes (A1 j , A2 j ) in Mj,l−1 as the number of models in the support of agent is beliefs.",
                "The conditional probability table of the chance node, Aj, is a <br>multiplexer</br> that assumes the distribution of each of the action nodes (A1 j , A2 j ) depending on the value of Mod[Mj].",
                "The values of Mod[Mj] denote the different models of j.",
                "In other words, when Mod[Mj] has the value m1 j,l−1, the chance node Aj assumes the distribution of the node A1 j , and Aj assumes the distribution of A2 j when Mod[Mj] has the value m2 j,l−1.",
                "The distribution over the node, Mod[Mj], is the agent is belief over the models of j given a physical state.",
                "For more agents, we will have as many model nodes as there are agents.",
                "Notice that Fig. 1(b) clarifies the semantics of the policy link, and shows how it can be represented using the traditional dependency links.",
                "In Fig. 1(c), we show the transformed I-ID when the model node is replaced by the chance nodes and relationships between them.",
                "In contrast to the representation in [15], there are no special-purpose policy links, rather the I-ID is composed of only those types of nodes that are found in traditional IDs and dependency relationships between the nodes.",
                "This allows I-IDs to be represented and implemented using conventional application tools that target IDs.",
                "Note that we may view the level l I-ID as a NID.",
                "Specifically, each of the level l − 1 models within the model node are blocks in the NID (see Fig. 2).",
                "If the level l = 1, each block is a traditional ID, otherwise if l > 1, each block within the NID may itself be a NID.",
                "Note that within the I-IDs (or IDs) at each level, there is only a single decision node.",
                "Thus, our NID does not contain any MAIDs.",
                "Figure 2: A level l I-ID represented as a NID.",
                "The probabilities assigned to the blocks of the NID are is beliefs over js models conditioned on a physical state. 3.2 Solution The solution of an I-ID proceeds in a bottom-up manner, and is implemented recursively.",
                "We start by solving the level 0 models, which, if intentional, are traditional IDs.",
                "Their solutions provide probability distributions over the other agents actions, which are entered in the corresponding chance nodes found in the model node of the level 1 I-ID.",
                "The mapping from the level 0 models decision nodes to the chance nodes is carried out so that actions with the largest value in the decision node are assigned uniform probabilities in the chance node while the rest are assigned zero probability.",
                "Given the distributions over the actions within the different chance nodes (one for each model of the other agent), the level 1 I-ID is transformed as shown in Fig. 1(c).",
                "During the transformation, the conditional probability table (CPT) of the node, Aj, is populated such that the node assumes the distribution of each of the chance nodes depending on the value of the node, Mod[Mj].",
                "As we mentioned previously, the values of the node Mod[Mj] denote the different models of the other agent, and its distribution is the agent is belief over the models of j conditioned on the physical state.",
                "The transformed level 1 I-ID is a traditional ID that may be solved us816 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) (a) (b) Figure 3: (a) A generic two time-slice level l I-DID for agent i in a setting with one other agent j.",
                "Notice the dotted model update link that denotes the update of the models of j and the distribution over the models over time. (b) The semantics of the model update link. ing the standard expected utility maximization method [18].",
                "This procedure is carried out up to the level l I-ID whose solution gives the non-empty set of optimal actions that the agent should perform given its belief.",
                "Notice that analogous to IDs, I-IDs are suitable for online decision-making when the agents current belief is known. 4.",
                "INTERACTIVE DYNAMIC INFLUENCE DIAGRAMS Interactive dynamic influence diagrams (I-DIDs) extend I-IDs (and NIDs) to allow sequential decision-making over several time steps.",
                "Just as DIDs are structured graphical representations of POMDPs, I-DIDs are the graphical online analogs for finitely nested I-POMDPs.",
                "I-DIDs may be used to optimize over a finite look-ahead given initial beliefs while interacting with other, possibly similar, agents. 4.1 Syntax We depict a general two time-slice I-DID in Fig. 3(a).",
                "In addition to the model nodes and the dashed policy link, what differentiates an I-DID from a DID is the model update link shown as a dotted arrow in Fig. 3(a).",
                "We explained the semantics of the model node and the policy link in the previous section; we describe the model updates next.",
                "The update of the model node over time involves two steps: First, given the models at time t, we identify the updated set of models that reside in the model node at time t + 1.",
                "Recall from Section 2 that an agents intentional model includes its belief.",
                "Because the agents act and receive observations, their models are updated to reflect their changed beliefs.",
                "Since the set of optimal actions for a model could include all the actions, and the agent may receive any one of |Ωj| possible observations, the updated set at time step t + 1 will have at most |Mt j,l−1||Aj||Ωj| models.",
                "Here, |Mt j,l−1| is the number of models at time step t, |Aj| and |Ωj| are the largest spaces of actions and observations respectively, among all the models.",
                "Second, we compute the new distribution over the updated models given the original distribution and the probability of the agent performing the action and receiving the observation that led to the updated model.",
                "These steps are a part of agent is belief update formalized using Eq. 1.",
                "In Fig. 3(b), we show how the dotted model update link is implemented in the I-DID.",
                "If each of the two level l − 1 models ascribed to j at time step t results in one action, and j could make one of two possible observations, then the model node at time step t + 1 contains four updated models (mt+1,1 j,l−1 ,mt+1,2 j,l−1 , mt+1,3 j,l−1 , and mt+1,4 j,l−1 ).",
                "These models differ in their initial beliefs, each of which is the result of j updating its beliefs due to its action and a possible observation.",
                "The decision nodes in each of the I-DIDs or DIDs that represent the lower level models are mapped to the corresponding Figure 4: Transformed I-DID with the model nodes and model update link replaced with the chance nodes and the relationships (in bold). chance nodes, as mentioned previously.",
                "Next, we describe how the distribution over the updated set of models (the distribution over the chance node Mod[Mt+1 j ] in Mt+1 j,l−1) is computed.",
                "The probability that js updated model is, say mt+1,1 j,l−1 , depends on the probability of j performing the action and receiving the observation that led to this model, and the prior distribution over the models at time step t. Because the chance node At j assumes the distribution of each of the action nodes based on the value of Mod[Mt j ], the probability of the action is given by this chance node.",
                "In order to obtain the probability of js possible observation, we introduce the chance node Oj, which depending on the value of Mod[Mt j ] assumes the distribution of the observation node in the lower level model denoted by Mod[Mt j ].",
                "Because the probability of js observations depends on the physical state and the joint actions of both agents, the node Oj is linked with St+1 , At j, and At i. 2 Analogous to At j, the conditional probability table of Oj is also a <br>multiplexer</br> modulated by Mod[Mt j ].",
                "Finally, the distribution over the prior models at time t is obtained from the chance node, Mod[Mt j ] in Mt j,l−1.",
                "Consequently, the chance nodes, Mod[Mt j ], At j, and Oj, form the parents of Mod[Mt+1 j ] in Mt+1 j,l−1.",
                "Notice that the model update link may be replaced by the dependency links between the chance nodes that constitute the model nodes in the two time slices.",
                "In Fig. 4 we show the two time-slice I-DID with the model nodes replaced by the chance nodes and the relationships between them.",
                "Chance nodes and dependency links that not in bold are standard, usually found in DIDs.",
                "Expansion of the I-DID over more time steps requires the repetition of the two steps of updating the set of models that form the 2 Note that Oj represents js observation at time t + 1.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 817 values of the model node and adding the relationships between the chance nodes, as many times as there are model update links.",
                "We note that the possible set of models of the other agent j grows exponentially with the number of time steps.",
                "For example, after T steps, there may be at most |Mt=1 j,l−1|(|Aj||Ωj|)T −1 candidate models residing in the model node. 4.2 Solution Analogous to I-IDs, the solution to a level l I-DID for agent i expanded over T time steps may be carried out recursively.",
                "For the purpose of illustration, let l=1 and T=2.",
                "The solution method uses the standard look-ahead technique, projecting the agents action and observation sequences forward from the current belief state [17], and finding the possible beliefs that i could have in the next time step.",
                "Because agent i has a belief over js models as well, the lookahead includes finding out the possible models that j could have in the future.",
                "Consequently, each of js subintentional or level 0 models (represented using a standard DID) in the first time step must be solved to obtain its optimal set of actions.",
                "These actions are combined with the set of possible observations that j could make in that model, resulting in an updated set of candidate models (that include the updated beliefs) that could describe the behavior of j. Beliefs over this updated set of candidate models are calculated using the standard inference methods using the dependency relationships between the model nodes as shown in Fig. 3(b).",
                "We note the recursive nature of this solution: in solving agent is level 1 I-DID, js level 0 DIDs must be solved.",
                "If the nesting of models is deeper, all models at all levels starting from 0 are solved in a bottom-up manner.",
                "We briefly outline the recursive algorithm for solving agent is Algorithm for solving I-DID Input : level l ≥ 1 I-ID or level 0 ID, T Expansion Phase 1.",
                "For t from 1 to T − 1 do 2.",
                "If l ≥ 1 then Populate Mt+1 j,l−1 3.",
                "For each mt j in Range(Mt j,l−1) do 4.",
                "Recursively call algorithm with the l − 1 I-ID (or ID) that represents mt j and the horizon, T − t + 1 5.",
                "Map the decision node of the solved I-ID (or ID), OPT(mt j), to a chance node Aj 6.",
                "For each aj in OPT(mt j) do 7.",
                "For each oj in Oj (part of mt j) do 8.",
                "Update js belief, bt+1 j ← SE(bt j, aj, oj) 9. mt+1 j ← New I-ID (or ID) with bt+1 j as the initial belief 10.",
                "Range(Mt+1 j,l−1) ∪ ← {mt+1 j } 11.",
                "Add the model node, Mt+1 j,l−1, and the dependency links between Mt j,l−1 and Mt+1 j,l−1 (shown in Fig. 3(b)) 12.",
                "Add the chance, decision, and utility nodes for t + 1 time slice and the dependency links between them 13.",
                "Establish the CPTs for each chance node and utility node Look-Ahead Phase 14.",
                "Apply the standard look-ahead and backup method to solve the expanded I-DID Figure 5: Algorithm for solving a level l ≥ 0 I-DID. level l I-DID expanded over T time steps with one other agent j in Fig. 5.",
                "We adopt a two-phase approach: Given an I-ID of level l (described previously in Section 3) with all lower level models also represented as I-IDs or IDs (if level 0), the first step is to expand the level l I-ID over T time steps adding the dependency links and the conditional probability tables for each node.",
                "We particularly focus on establishing and populating the model nodes (lines 3-11).",
                "Note that Range(·) returns the values (lower level models) of the random variable given as input (model node).",
                "In the second phase, we use a standard look-ahead technique projecting the action and observation sequences over T time steps in the future, and backing up the utility values of the reachable beliefs.",
                "Similar to I-IDs, the I-DIDs reduce to DIDs in the absence of other agents.",
                "As we mentioned previously, the 0-th level models are the traditional DIDs.",
                "Their solutions provide probability distributions over actions of the agent modeled at that level to I-DIDs at level 1.",
                "Given probability distributions over other agents actions the level 1 IDIDs can themselves be solved as DIDs, and provide probability distributions to yet higher level models.",
                "Assume that the number of models considered at each level is bound by a number, M. Solving an I-DID of level l in then equivalent to solving O(Ml ) DIDs. 5.",
                "EXAMPLE APPLICATIONS To illustrate the usefulness of I-DIDs, we apply them to three problem domains.",
                "We describe, in particular, the formulation of the I-DID and the optimal prescriptions obtained on solving it. 5.1 Followership-Leadership in the Multiagent Tiger Problem We begin our illustrations of using I-IDs and I-DIDs with a slightly modified version of the multiagent tiger problem discussed in [9].",
                "The problem has two agents, each of which can open the right door (OR), the left door (OL) or listen (L).",
                "In addition to hearing growls (from the left (GL) or from the right (GR)) when they listen, the agents also hear creaks (from the left (CL), from the right (CR), or no creaks (S)), which noisily indicate the other agents opening one of the doors.",
                "When any door is opened, the tiger persists in its original location with a probability of 95%.",
                "Agent i hears growls with a reliability of 65% and creaks with a reliability of 95%.",
                "Agent j, on the other hand, hears growls with a reliability of 95%.",
                "Thus, the setting is such that agent i hears agent j opening doors more reliably than the tigers growls.",
                "This suggests that i could use js actions as an indication of the location of the tiger, as we discuss below.",
                "Each agents preferences are as in the single agent game discussed in [13].",
                "The transition, observation, and reward functions are shown in [16].",
                "A good indicator of the usefulness of normative methods for decision-making like I-DIDs is the emergence of realistic social behaviors in their prescriptions.",
                "In settings of the persistent multiagent tiger problem that reflect real world situations, we demonstrate followership between the agents and, as shown in [15], deception among agents who believe that they are in a follower-leader type of relationship.",
                "In particular, we analyze the situational and epistemological conditions sufficient for their emergence.",
                "The followership behavior, for example, results from the agent knowing its own weaknesses, assessing the strengths, preferences, and possible behaviors of the other, and realizing that its best for it to follow the others actions in order to maximize its payoffs.",
                "Let us consider a particular setting of the tiger problem in which agent i believes that js preferences are aligned with its own - both of them just want to get the gold - and js hearing is more reliable in comparison to itself.",
                "As an example, suppose that j, on listening can discern the tigers location 95% of the times compared to is 65% accuracy.",
                "Additionally, agent i does not have any initial information about the tigers location.",
                "In other words, is single-level nested belief, bi,1, assigns 0.5 to each of the two locations of the tiger.",
                "In addition, i considers two models of j, which differ in js flat level 0 initial beliefs.",
                "This is represented in the level 1 I-ID shown in Fig. 6(a).",
                "According to one model, j assigns a probability of 0.9 that the tiger is behind the left door, while the other 818 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Figure 6: (a) Level 1 I-ID of agent i, (b) two level 0 IDs of agent j whose decision nodes are mapped to the chance nodes, A1 j , A2 j , in (a). model assigns 0.1 to that location (see Fig. 6(b)).",
                "Agent i is undecided on these two models of j.",
                "If we vary is hearing ability, and solve the corresponding level 1 I-ID expanded over three time steps, we obtain the normative behavioral policies shown in Fig 7 that exhibit followership behavior.",
                "If is probability of correctly hearing the growls is 0.65, then as shown in the policy in Fig. 7(a), i begins to conditionally follow js actions: i opens the same door that j opened previously iff is own assessment of the tigers location confirms js pick.",
                "If i loses the ability to correctly interpret the growls completely, it blindly follows j and opens the same door that j opened previously (Fig. 7(b)).",
                "Figure 7: Emergence of (a) conditional followership, and (b) blind followership in the tiger problem.",
                "Behaviors of interest are in bold. * is a wildcard, and denotes any one of the observations.",
                "We observed that a single level of belief nesting - beliefs about the others models - was sufficient for followership to emerge in the tiger problem.",
                "However, the epistemological requirements for the emergence of leadership are more complex.",
                "For an agent, say j, to emerge as a leader, followership must first emerge in the other agent i.",
                "As we mentioned previously, if i is certain that its preferences are identical to those of j, and believes that j has a better sense of hearing, i will follow js actions over time.",
                "Agent j emerges as a leader if it believes that i will follow it, which implies that js belief must be nested two levels deep to enable it to recognize its leadership role.",
                "Realizing that i will follow presents j with an opportunity to influence is actions in the benefit of the collective good or its self-interest alone.",
                "For example, in the tiger problem, let us consider a setting in which if both i and j open the correct door, then each gets a payoff of 20 that is double the original.",
                "If j alone selects the correct door, it gets the payoff of 10.",
                "On the other hand, if both agents pick the wrong door, their penalties are cut in half.",
                "In this setting, it is in both js best interest as well as the collective betterment for j to use its expertise in selecting the correct door, and thus be a good leader.",
                "However, consider a slightly different problem in which j gains from is loss and is penalized if i gains.",
                "Specifically, let is payoff be subtracted from js, indicating that j is antagonistic toward i - if j picks the correct door and i the wrong one, then is loss of 100 becomes js gain.",
                "Agent j believes that i incorrectly thinks that js preferences are those that promote the collective good and that it starts off by believing with 99% confidence where the tiger is.",
                "Because i believes that its preferences are similar to those of j, and that j starts by believing almost surely that one of the two is the correct location (two level 0 models of j), i will start by following js actions.",
                "We show is normative policy on solving its singly-nested I-DID over three time steps in Fig. 8(a).",
                "The policy demonstrates that i will blindly follow js actions.",
                "Since the tiger persists in its original location with a probability of 0.95, i will select the same door again.",
                "If j begins the game with a 99% probability that the tiger is on the right, solving js I-DID nested two levels deep, results in the policy shown in Fig. 8(b).",
                "Even though j is almost certain that OL is the correct action, it will start by selecting OR, followed by OL.",
                "Agent js intention is to deceive i who, it believes, will follow js actions, so as to gain $110 in the second time step, which is more than what j would gain if it were to be honest.",
                "Figure 8: Emergence of deception between agents in the tiger problem.",
                "Behaviors of interest are in bold. * denotes as before. (a) Agent is policy demonstrating that it will blindly follow js actions. (b) Even though j is almost certain that the tiger is on the right, it will start by selecting OR, followed by OL, in order to deceive i. 5.2 Altruism and Reciprocity in the Public Good Problem The public good (PG) problem [7], consists of a group of M agents, each of whom must either contribute some resource to a public pot or keep it for themselves.",
                "Since resources contributed to the public pot are shared among all the agents, they are less valuable to the agent when in the public pot.",
                "However, if all agents choose to contribute their resources, then the payoff to each agent is more than if no one contributes.",
                "Since an agent gets its share of the public pot irrespective of whether it has contributed or not, the dominating action is for each agent to not contribute, and instead free ride on others contributions.",
                "However, behaviors of human players in empirical simulations of the PG problem differ from the normative predictions.",
                "The experiments reveal that many players initially contribute a large amount to the public pot, and continue to contribute when the PG problem is played repeatedly, though in decreasing amounts [4].",
                "Many of these experiments [5] report that a small core group of players persistently contributes to the public pot even when all others are defecting.",
                "These experiments also reveal that players who persistently contribute have altruistic or reciprocal preferences matching expected cooperation of others.",
                "For simplicity, we assume that the game is played between M = 2 agents, i and j.",
                "Let each agent be initially endowed with XT amount of resources.",
                "While the classical PG game formulation permits each agent to contribute any quantity of resources (≤ XT ) to the public pot, we simplify the action space by allowing two possible actions.",
                "Each agent may choose to either contribute (C) a fixed amount of the resources, or not contribute.",
                "The latter action is deThe Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 819 noted as defect (D).",
                "We assume that the actions are not observable to others.",
                "The value of resources in the public pot is discounted by ci for each agent i, where ci is the marginal private return.",
                "We assume that ci < 1 so that the agent does not benefit enough that it contributes to the public pot for private gain.",
                "Simultaneously, ciM > 1, making collective contribution pareto optimal. i/j C D C 2ciXT , 2cjXT ciXT − cp, XT + cjXT − P D XT + ciXT − P, cjXT − cp XT , XT Table 1: The one-shot PG game with punishment.",
                "In order to encourage contributions, the contributing agents punish free riders but incur a small cost for administering the punishment.",
                "Let P be the punishment meted out to the defecting agent and cp the non-zero cost of punishing for the contributing agent.",
                "For simplicity, we assume that the cost of punishing is same for both the agents.",
                "The one-shot PG game with punishment is shown in Table. 1.",
                "Let ci = cj, cp > 0, and if P > XT − ciXT , then defection is no longer a dominating action.",
                "If P < XT − ciXT , then defection is the dominating action for both.",
                "If P = XT − ciXT , then the game is not dominance-solvable.",
                "Figure 9: (a) Level 1 I-ID of agent i, (b) level 0 IDs of agent j with decision nodes mapped to the chance nodes, A1 j and A2 j , in (a).",
                "We formulate a sequential version of the PG problem with punishment from the perspective of agent i.",
                "Though in the repeated PG game, the quantity in the public pot is revealed to all the agents after each round of actions, we assume in our formulation that it is hidden from the agents.",
                "Each agent may contribute a fixed amount, xc, or defect.",
                "An agent on performing an action receives an observation of plenty (PY) or meager (MR) symbolizing the state of the public pot.",
                "Notice that the observations are also indirectly indicative of agent js actions because the state of the public pot is influenced by them.",
                "The amount of resources in agent is private pot, is perfectly observable to i.",
                "The payoffs are analogous to Table. 1.",
                "Borrowing from the empirical investigations of the PG problem [5], we construct level 0 IDs for j that model altruistic and non-altruistic types (Fig. 9(b)).",
                "Specifically, our altruistic agent has a high marginal private return (cj is close to 1) and does not punish others who defect.",
                "Let xc = 1 and the level 0 agent be punished half the times it defects.",
                "With one action remaining, both types of agents choose to contribute to avoid being punished.",
                "With two actions to go, the altruistic type chooses to contribute, while the other defects.",
                "This is because cj for the altruistic type is close to 1, thus the expected punishment, 0.5P > (1 − cj), which the altruistic type avoids.",
                "Because cj for the non-altruistic type is less, it prefers not to contribute.",
                "With three steps to go, the altruistic agent contributes to avoid punishment (0.5P > 2(1 − cj)), and the non-altruistic type defects.",
                "For greater than three steps, while the altruistic agent continues to contribute to the public pot depending on how close its marginal private return is to 1, the non-altruistic type prescribes defection.",
                "We analyzed the decisions of an altruistic agent i modeled using a level 1 I-DID expanded over 3 time steps. i ascribes the two level 0 models, mentioned previously, to j (see Fig. 9).",
                "If i believes with a probability 1 that j is altruistic, i chooses to contribute for each of the three steps.",
                "This behavior persists when i is unaware of whether j is altruistic (Fig. 10(a)), and when i assigns a high probability to j being the non-altruistic type.",
                "However, when i believes with a probability 1 that j is non-altruistic and will thus surely defect, i chooses to defect to avoid being punished and because its marginal private return is less than 1.",
                "These results demonstrate that the behavior of our altruistic type resembles that found experimentally.",
                "The non-altruistic level 1 agent chooses to defect regardless of how likely it believes the other agent to be altruistic.",
                "We analyzed the behavior of a reciprocal agent type that matches expected cooperation or defection.",
                "The reciprocal types marginal private return is similar to that of the non-altruistic type, however, it obtains a greater payoff when its action is similar to that of the other.",
                "We consider the case when the reciprocal agent i is unsure of whether j is altruistic and believes that the public pot is likely to be half full.",
                "For this prior belief, i chooses to defect.",
                "On receiving an observation of plenty, i decides to contribute, while an observation of meager makes it defect (Fig. 10(b)).",
                "This is because an observation of plenty signals that the pot is likely to be greater than half full, which results from js action to contribute.",
                "Thus, among the two models ascribed to j, its type is likely to be altruistic making it likely that j will contribute again in the next time step.",
                "Agent i therefore chooses to contribute to reciprocate js action.",
                "An analogous reasoning leads i to defect when it observes a meager pot.",
                "With one action to go, i believing that j contributes, will choose to contribute too to avoid punishment regardless of its observations.",
                "Figure 10: (a) An altruistic level 1 agent always contributes. (b) A reciprocal agent i starts off by defecting followed by choosing to contribute or defect based on its observation of plenty (indicating that j is likely altruistic) or meager (j is non-altruistic). 5.3 Strategies in Two-Player Poker Poker is a popular zero sum card game that has received much attention among the AI research community as a testbed [2].",
                "Poker is played among M ≥ 2 players in which each player receives a hand of cards from a deck.",
                "While several flavors of Poker with varying complexity exist, we consider a simple version in which each player has three plys during which the player may either exchange a card (E), keep the existing hand (K), fold (F) and withdraw from the game, or call (C), requiring all players to show their hands.",
                "To keep matters simple, let M = 2, and each player receive a hand consisting of a single card drawn from the same suit.",
                "Thus, during a showdown, the player who has the numerically larger card (2 is the lowest, ace is the highest) wins the pot.",
                "During an exchange of cards, the discarded card is placed either in the L pile, indicating to the other agent that it was a low numbered card less than 8, or in the 820 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) H pile, indicating that the card had a rank greater than or equal to 8.",
                "Notice that, for example, if a lower numbered card is discarded, the probability of receiving a low card in exchange is now reduced.",
                "We show the level 1 I-ID for the simplified two-player Poker in Fig. 11.",
                "We considered two models (personality types) of agent j.",
                "The conservative type believes that it is likely that its opponent has a high numbered card in its hand.",
                "On the other hand, the aggressive agent j believes with a high probability that its opponent has a lower numbered card.",
                "Thus, the two types differ in their beliefs over their opponents hand.",
                "In both these level 0 models, the opponent is assumed to perform its actions following a fixed, uniform distribution.",
                "With three actions to go, regardless of its hand (unless it is an ace), the aggressive agent chooses to exchange its card, with the intent of improving on its current hand.",
                "This is because it believes the other to have a low card, which improves its chances of getting a high card during the exchange.",
                "The conservative agent chooses to keep its card, no matter its hand because its chances of getting a high card are slim as it believes that its opponent has one.",
                "Figure 11: (a) Level 1 I-ID of agent i.",
                "The observation reveals information about js hand of the previous time step, (b) level 0 IDs of agent j whose decision nodes are mapped to the chance nodes, A1 j , A2 j , in (a).",
                "The policy of a level 1 agent i who believes that each card except its own has an equal likelihood of being in js hand (neutral personality type) and j could be either an aggressive or conservative type, is shown in Fig. 12. is own hand contains the card numbered 8.",
                "The agent starts by keeping its card.",
                "On seeing that j did not exchange a card (N), i believes with probability 1 that j is conservative and hence will keep its cards. i responds by either keeping its card or exchanging it because j is equally likely to have a lower or higher card.",
                "If i observes that j discarded its card into the L or H pile, i believes that j is aggressive.",
                "On observing L, i realizes that j had a low card, and is likely to have a high card after its exchange.",
                "Because the probability of receiving a low card is high now, i chooses to keep its card.",
                "On observing H, believing that the probability of receiving a high numbered card is high, i chooses to exchange its card.",
                "In the final step, i chooses to call regardless of its observation history because its belief that j has a higher card is not sufficiently high to conclude that its better to fold and relinquish the payoff.",
                "This is partly due to the fact that an observation of, say, L resets the agent is previous time step beliefs over js hand to the low numbered cards only. 6.",
                "DISCUSSION We showed how DIDs may be extended to I-DIDs that enable online sequential decision-making in uncertain multiagent settings.",
                "Our graphical representation of I-DIDs improves on the previous Figure 12: A level 1 agent is three step policy in the Poker problem. i starts by believing that j is equally likely to be aggressive or conservative and could have any card in its hand with equal probability. work significantly by being more transparent, semantically clear, and capable of being solved using standard algorithms that target DIDs.",
                "I-DIDs extend NIDs to allow sequential decision-making over multiple time steps in the presence of other interacting agents.",
                "I-DIDs may be seen as concise graphical representations for IPOMDPs providing a way to exploit problem structure and carry out online decision-making as the agent acts and observes given its prior beliefs.",
                "We are currently investigating ways to solve I-DIDs approximately with provable bounds on the solution quality.",
                "Acknowledgment: We thank Piotr Gmytrasiewicz for some useful discussions related to this work.",
                "The first author would like to acknowledge the support of a UGARF grant. 7.",
                "REFERENCES [1] R. J. Aumann.",
                "Interactive epistemology i: Knowledge.",
                "International Journal of Game Theory, 28:263-300, 1999. [2] D. Billings, A. Davidson, J. Schaeffer, and D. Szafron.",
                "The challenge of poker.",
                "AIJ, 2001. [3] A. Brandenburger and E. Dekel.",
                "Hierarchies of beliefs and common knowledge.",
                "Journal of Economic Theory, 59:189-198, 1993. [4] C. Camerer.",
                "Behavioral Game Theory: Experiments in Strategic Interaction.",
                "Princeton University Press, 2003. [5] E. Fehr and S. Gachter.",
                "Cooperation and punishment in public goods experiments.",
                "American Economic Review, 90(4):980-994, 2000. [6] D. Fudenberg and D. K. Levine.",
                "The Theory of Learning in Games.",
                "MIT Press, 1998. [7] D. Fudenberg and J. Tirole.",
                "Game Theory.",
                "MIT Press, 1991. [8] Y. Gal and A. Pfeffer.",
                "A language for modeling agents decision-making processes in games.",
                "In AAMAS, 2003. [9] P. Gmytrasiewicz and P. Doshi.",
                "A framework for sequential planning in multiagent settings.",
                "JAIR, 24:49-79, 2005. [10] P. Gmytrasiewicz and E. Durfee.",
                "Rational coordination in multi-agent environments.",
                "JAAMAS, 3(4):319-350, 2000. [11] J. C. Harsanyi.",
                "Games with incomplete information played by bayesian players.",
                "Management Science, 14(3):159-182, 1967. [12] R. A. Howard and J. E. Matheson.",
                "Influence diagrams.",
                "In R. A. Howard and J. E. Matheson, editors, The Principles and Applications of Decision Analysis.",
                "Strategic Decisions Group, Menlo Park, CA 94025, 1984. [13] L. Kaelbling, M. Littman, and A. Cassandra.",
                "Planning and acting in partially observable stochastic domains.",
                "Artificial Intelligence Journal, 2, 1998. [14] D. Koller and B. Milch.",
                "Multi-agent influence diagrams for representing and solving games.",
                "In IJCAI, pages 1027-1034, 2001. [15] K. Polich and P. Gmytrasiewicz.",
                "Interactive dynamic influence diagrams.",
                "In GTDT Workshop, AAMAS, 2006. [16] B.",
                "Rathnas., P. Doshi, and P. J. Gmytrasiewicz.",
                "Exact solutions to interactive pomdps using behavioral equivalence.",
                "In Autonomous Agents and Multi-Agent Systems Conference (AAMAS), 2006. [17] S. Russell and P. Norvig.",
                "Artificial Intelligence: A Modern Approach (Second Edition).",
                "Prentice Hall, 2003. [18] R. D. Shachter.",
                "Evaluating influence diagrams.",
                "Operations Research, 34(6):871-882, 1986. [19] D. Suryadi and P. Gmytrasiewicz.",
                "Learning models of other agents using influence diagrams.",
                "In UM, 1999.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 821"
            ],
            "original_annotated_samples": [
                "The conditional probability table of the chance node, Aj, is a <br>multiplexer</br> that assumes the distribution of each of the action nodes (A1 j , A2 j ) depending on the value of Mod[Mj].",
                "Because the probability of js observations depends on the physical state and the joint actions of both agents, the node Oj is linked with St+1 , At j, and At i. 2 Analogous to At j, the conditional probability table of Oj is also a <br>multiplexer</br> modulated by Mod[Mt j ]."
            ],
            "translated_annotated_samples": [
                "La tabla de probabilidad condicional del nodo de probabilidad, Aj, es un <br>multiplexor</br> que asume la distribución de cada uno de los nodos de acción (A1 j , A2 j ) dependiendo del valor de Mod[Mj].",
                "Dado que la probabilidad de las observaciones js depende del estado físico y de las acciones conjuntas de ambos agentes, el nodo Oj está vinculado con St+1, At j y At i. De manera análoga a At j, la tabla de probabilidad condicional de Oj también es un <br>multiplexor</br> modulado por Mod[Mt j]."
            ],
            "translated_text": "Modelos gráficos para soluciones en línea a POMDP interactivos de Prashant Doshi Dept. Departamento de Ciencias de la Computación Universidad de Georgia Athens, GA 30602, EE. UU. pdoshi@cs.uga.edu Yifeng Zeng Dept. de Ciencias de la Computación Universidad de Aalborg DK-9220 Aalborg, Dinamarca yfzeng@cs.aau.edu Qiongyu Chen Dept. de Ciencias de la Computación Universidad Nacional de Singapur 117543, Singapur chenqy@comp.nus.edu.sg RESUMEN Desarrollamos una nueva representación gráfica para procesos de decisión de Markov parcialmente observables interactivos (I-POMDPs) que es significativamente más transparente y semánticamente clara que la representación anterior. Estos modelos gráficos llamados diagramas de influencia dinámica interactiva (I-DIDs) buscan modelar explícitamente la estructura que a menudo está presente en problemas del mundo real al descomponer la situación en variables de probabilidad y decisión, y las dependencias entre las variables. Los I-DIDs generalizan los DIDs, los cuales pueden ser vistos como representaciones gráficas de POMDPs, a entornos multiagentes de la misma manera que los I-POMDPs generalizan los POMDPs. Los I-DIDs pueden ser utilizados para calcular la política de un agente en línea mientras el agente actúa y observa en un entorno poblado por otros agentes que interactúan entre sí. Usando varios ejemplos, mostramos cómo se pueden aplicar los I-DIDs y demostramos su utilidad. Categorías y Descriptores de Asignaturas I.2.11 [Inteligencia Artificial Distribuida]: Sistemas Multiagentes Términos Generales Teoría 1. Las Procesos de Decisión de Markov Interactivos Parcialmente Observables (IPOMDPs) proporcionan un marco para la toma de decisiones secuenciales en entornos multiagentes parcialmente observables. Generalizan los POMDPs [13] a entornos multiagentes al incluir los modelos computables de los otros agentes en el espacio de estados junto con los estados del entorno físico. Los modelos abarcan toda la información que influye en los comportamientos de los agentes, incluyendo sus preferencias, capacidades y creencias, y por lo tanto son análogos a los tipos en los juegos bayesianos [11]. I-POMDPs adoptan un enfoque subjetivo para comprender el comportamiento estratégico, arraigado en un marco de teoría de decisiones que toma la perspectiva de los tomadores de decisiones en la interacción. En [15], Polich y Gmytrasiewicz introdujeron diagramas de influencia dinámica interactivos (I-DIDs) como las representaciones computacionales de I-POMDPs. Los I-DIDs generalizan los DIDs [12], los cuales pueden ser vistos como contrapartes computacionales de POMDPs, a entornos de múltiples agentes de la misma manera que los I-POMDPs generalizan los POMDPs. Los I-DIDs contribuyen a una creciente línea de trabajo [19] que incluye diagramas de influencia multiagente (MAIDs) [14], y más recientemente, redes de diagramas de influencia (NIDs) [8]. Estos formalismos buscan modelar explícitamente la estructura que a menudo está presente en problemas del mundo real al descomponer la situación en variables de azar y decisiones, y las dependencias entre las variables. Los MAIDs proporcionan una alternativa a las formas normales y extensivas de juego utilizando un formalismo gráfico para representar juegos de información imperfecta con un nodo de decisión para las acciones de cada agente y nodos de azar que capturan la información privada de los agentes. Los MAIDs analizan objetivamente el juego, calculando eficientemente el perfil de equilibrio de Nash al explotar la estructura de independencia. NIDs extienden los MAIDs para incluir la incertidumbre de los agentes sobre el juego que se está jugando y sobre los modelos de los otros agentes. Cada modelo es un MAID y la red de MAIDs se colapsa, de abajo hacia arriba, en un solo MAID para calcular el equilibrio del juego teniendo en cuenta los diferentes modelos de cada agente. Los formalismos gráficos como MAIDs y NIDs abren una área de investigación prometedora que tiene como objetivo representar las interacciones multiagente de manera más transparente. Sin embargo, los MAIDs proporcionan un análisis del juego desde un punto de vista externo y la aplicabilidad de ambos está limitada a juegos estáticos de un solo jugador. Los asuntos son más complejos cuando consideramos interacciones que se extienden en el tiempo, donde las predicciones sobre las acciones futuras de otros deben hacerse utilizando modelos que cambian a medida que los agentes actúan y observan. Los I-DIDs abordan esta brecha al permitir la representación de modelos de otros agentes como los valores de un nodo de modelo especial. Tanto los modelos de otros agentes como las creencias originales de los agentes sobre estos modelos se actualizan con el tiempo utilizando implementaciones especializadas. En este documento, mejoramos la representación preliminar previa del I-DID mostrada en [15] utilizando la idea de que el I-ID estático es un tipo de NID. Por lo tanto, podemos utilizar construcciones de lenguaje específicas de NID, como multiplexores, para representar de manera más transparente el nodo del modelo y, posteriormente, el I-ID. Además, aclaramos la semántica del enlace de política de propósito especial introducido en la representación de I-DID por [15], y mostramos que podría ser reemplazado por enlaces de dependencia tradicionales. En la representación previa del I-DID, la actualización de la creencia de los agentes sobre los modelos de los demás a medida que los agentes actúan y reciben observaciones se denotaba utilizando un enlace especial llamado el enlace de actualización del modelo que conectaba los nodos del modelo a lo largo del tiempo. Explicamos la semántica de este enlace mostrando cómo puede ser implementado utilizando los enlaces de dependencia tradicionales entre los nodos de probabilidad que constituyen los nodos del modelo. El resultado final es una representación de I-DID que es significativamente más transparente, semánticamente clara y capaz de ser implementada utilizando los algoritmos estándar para resolver DIDs. Mostramos cómo los IDIDs pueden ser utilizados para modelar la incertidumbre de un agente sobre los modelos de otros, que a su vez pueden ser I-DIDs. La solución al I-DID es una política que prescribe lo que el agente debe hacer con el tiempo, dadas sus creencias sobre el estado físico y otros modelos. Análogos a los DIDs, los I-DIDs pueden ser utilizados para calcular la política de un agente en línea mientras el agente actúa y observa en un entorno poblado por otros agentes interactuantes. 2. ANTECEDENTES: Los IPOMDPS ANIDADOS FINITAMENTE generalizan los POMDPs a entornos multiagentes al incluir los modelos de otros agentes como parte del espacio de estados [9]. Dado que otros agentes también pueden razonar sobre otros, el espacio de estado interactivo está estratégicamente anidado; contiene creencias sobre los modelos de otros agentes y sus creencias sobre los demás. Para simplificar la presentación, consideramos un agente, i, que está interactuando con otro agente, j. Un I-POMDP finitamente anidado del agente i con un nivel de estrategia l se define como la tupla: I-POMDPi,l = ISi,l, A, Ti, Ωi, Oi, Ri donde: • ISi,l denota un conjunto de estados interactivos definido como, ISi,l = S × Mj,l−1, donde Mj,l−1 = {Θj,l−1 ∪ SMj}, para l ≥ 1, e ISi,0 = S, donde S es el conjunto de estados del entorno físico. Θj,l−1 es el conjunto de modelos intencionales computables del agente j: θj,l−1 = bj,l−1, ˆθj donde el marco, ˆθj = A, Ωj, Tj, Oj, Rj, OCj. Aquí, j es Bayesiano racional y OCj es el criterio de optimalidad de j. SMj es el conjunto de modelos subintencionales de j. Ejemplos simples de modelos subintencionales incluyen un modelo sin información [10] y un modelo de juego ficticio [6], ambos de los cuales son independientes de la historia. Damos una construcción recursiva de abajo hacia arriba del espacio de estados interactivo a continuación. Sí,0 = S, Θj,0 = { bj,0, ˆθj | bj,0 ∈ Δ(ISj,0)} Sí,1 = S × {Θj,0 ∪ SMj}, Θj,1 = { bj,1, ˆθj | bj,1 ∈ Δ(ISj,1)} . . . . . . Sí, l = S × {Θj,l−1 ∪ SMj}, Θj,l = { bj,l, ˆθj | bj,l ∈ Δ(ISj,l)} Formulaciones similares de espacios anidados han aparecido en [1, 3]. • A = Ai × Aj es el conjunto de acciones conjuntas de todos los agentes en el entorno; • Ti : S ×A×S → [0, 1], describe el efecto de las acciones conjuntas en los estados físicos del entorno; • Ωi es el conjunto de observaciones del agente i; • Oi : S × A × Ωi → [0, 1] da la probabilidad de las observaciones dadas el estado físico y la acción conjunta; • Ri : ISi × A → R describe las preferencias del agente sobre sus estados interactivos. Normalmente solo importarán los estados físicos. La política del agente es el mapeo, Ω∗ i → Δ(Ai), donde Ω∗ i es el conjunto de todos los historiales de observación del agente i. Dado que la creencia sobre los estados interactivos forma una estadística suficiente [9], la política también puede ser representada como un mapeo del conjunto de todas las creencias del agente i a una distribución sobre sus acciones, Δ(ISi) → Δ(Ai). 2.1 Actualización de Creencias Análogo a los POMDPs, un agente dentro del marco I-POMDP actualiza su creencia a medida que actúa y observa. Sin embargo, hay dos diferencias que complican la actualización de creencias en entornos multiagentes en comparación con los entornos de un solo agente. Primero, dado que el estado del entorno físico depende de las acciones de ambos agentes, la predicción de cómo cambia el estado físico debe hacerse basándose en la predicción de sus acciones. Segundo, los cambios en los modelos de js deben ser incluidos en la actualización de creencias. Específicamente, si j es intencional, entonces se debe incluir una actualización de las creencias de j debido a su acción y observación. En otras palabras, i tiene que actualizar su creencia basándose en su predicción de lo que j observaría y cómo j actualizaría su creencia. Si el modelo de js es subintencional, entonces las observaciones probables de js se añaden al historial de observaciones contenido en el modelo. Formalmente, tenemos: Pr(ist |at−1 i , bt−1 i,l ) = β ISt−1:mt−1 j =θt j bt−1 i,l (ist−1 ) × at−1 j Pr(at−1 j |θt−1 j,l−1)Oi(st , at−1 i , at−1 j , ot i) ×Ti(st−1 , at−1 i , at−1 j , st ) ot j Oj(st , at−1 i , at−1 j , ot j) ×τ(SEθt j (bt−1 j,l−1, at−1 j , ot j) − bt j,l−1) (1) donde β es la constante de normalización, τ es 1 si su argumento es 0, de lo contrario es 0, Pr(at−1 j |θt−1 j,l−1) es la probabilidad de que at−1 j sea Bayes racional para el agente descrito por el modelo θt−1 j,l−1, y SE(·) es una abreviatura para la actualización de creencias. Para una versión de la actualización de creencias cuando el modelo js es subintencional, consulte [9]. Si el agente j también se modela como un I-POMDP, entonces la actualización de creencias invoca la actualización de creencias de j (a través del término SEθt j ( bt−1 j,l−1 , at−1 j , ot j)), lo que a su vez podría invocar la actualización de creencias de is y así sucesivamente. Esta recursión en la anidación de creencias llega al nivel 0. En este nivel, la actualización de creencias del agente se reduce a una actualización de creencias de un POMDP. Para ilustraciones de la actualización de creencias, detalles adicionales sobre I-POMDPs y cómo se comparan con otros marcos multiagentes, consulte [9]. Iteración de Valor Cada estado de creencia en un I-POMDP finitamente anidado tiene un valor asociado que refleja la máxima ganancia que el agente puede esperar en este estado de creencia: Un ( bi,l, θi ) = max ai∈Ai is∈ISi,l ERi(is, ai)bi,l(is)+ γ oi∈Ωi Pr(oi|ai, bi,l)Un−1 ( SEθi (bi,l, ai, oi), θi ) (2) donde, ERi(is, ai) = aj Ri(is, ai, aj)Pr(aj|mj,l−1) (ya que is = (s, mj,l−1)). La ecuación 2 es una base para la iteración de valor en I-POMDPs. La acción óptima del agente, a∗ i, para el caso de horizonte finito con descuento, es un elemento del conjunto de acciones óptimas para el estado de creencia, OPT(θi), definido como: OPT( bi,l, θi ) = argmax ai∈Ai is∈ISi,l ERi(is, ai)bi,l(is) +γ oi∈Ωi Pr(oi|ai, bi,l)Un ( SEθi (bi,l, ai, oi), θi ) (3) 3. Diagramas de influencia interactivos. Una extensión ingenua de los diagramas de influencia (IDs) a entornos poblados por múltiples agentes es posible tratando a los otros agentes como autómatas, representados mediante nodos de probabilidad. Sin embargo, este enfoque asume que las acciones de los agentes están controladas utilizando una distribución de probabilidad que no cambia con el tiempo. Los diagramas de influencia interactivos (I-IDs) adoptan un enfoque más sofisticado al generalizar los IDs para hacerlos aplicables a entornos compartidos con otros agentes que pueden actuar, observar y actualizar sus creencias. 3.1 Sintaxis Además de los nodos habituales de probabilidad, decisión y utilidad, los IIDs incluyen un nuevo tipo de nodo llamado nodo de modelo. Mostramos un nivel general l I-ID en la Fig. 1(a), donde el nodo del modelo (Mj,l−1) está representado con un hexágono. Observamos que la distribución de probabilidad sobre el nodo de probabilidad, S, y el nodo del modelo juntos representan la creencia del agente sobre sus estados interactivos. Además del modelo 1, el modelo de nivel 0 es un POMDP: las acciones de otros agentes se tratan como eventos exógenos y se incorporan en las funciones T, O y R. El Sexto Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) Figura 1: (a) Un nivel genérico l I-ID para el agente i situado con otro agente j. El hexágono es el nodo modelo (Mj,l−1) cuya estructura mostramos en (b). Los miembros del nodo modelo son ellos mismos I-ID (m1 j,l−1, m2 j,l−1; los diagramas no se muestran aquí por simplicidad) cuyos nodos de decisión se asignan a los nodos de probabilidad correspondientes (A1 j , A2 j). Dependiendo del valor del nodo, Mod[Mj], la distribución de cada uno de los nodos de probabilidad se asigna al nodo Aj. (c) El I-ID transformado con el nodo del modelo reemplazado por los nodos de probabilidad y las relaciones entre ellos. Los I-IDs difieren de los IDs al tener un enlace discontinuo (llamado enlace de política en [15]) entre el nodo del modelo y un nodo de probabilidad, Aj, que representa la distribución sobre las acciones de los otros agentes dada su modelo. En ausencia de otros agentes, el nodo del modelo y el nodo de probabilidad, Aj, desaparecen y los I-ID se convierten en ID tradicionales. El nodo del modelo contiene los modelos computacionales alternativos atribuidos por i al otro agente del conjunto, Θj,l−1 ∪ SMj, donde Θj,l−1 y SMj fueron definidos previamente en la Sección 2. Por lo tanto, un modelo en el nodo del modelo puede ser en sí mismo un I-ID o ID, y la recursión termina cuando un modelo es un ID o subintencional. Dado que el nodo del modelo contiene los modelos alternativos del otro agente como sus valores, su representación no es trivial. En particular, algunos de los modelos dentro del nodo son I-IDs que, al resolverse, generan la política óptima de los agentes en sus nodos de decisión. Cada nodo de decisión se asigna al nodo de probabilidad correspondiente, digamos A1 j, de la siguiente manera: si OPT es el conjunto de acciones óptimas obtenidas al resolver el I-ID (o ID), entonces Pr(aj ∈ A1 j) = 1 |OPT| si aj ∈ OPT, 0 en caso contrario. Tomando prestados los conocimientos de trabajos anteriores [8], observamos que el nodo del modelo y el enlace de política discontinua que lo conecta con el nodo de probabilidad, Aj, podrían representarse como se muestra en la Fig. 1(b). El nodo de decisión de cada nivel l − 1 I-ID se transforma en un nodo de probabilidad, como mencionamos anteriormente, de modo que las acciones con el mayor valor en el nodo de decisión se les asignan probabilidades uniformes en el nodo de probabilidad, mientras que al resto se les asigna probabilidad cero. Los diferentes nodos de probabilidad (A1 j , A2 j ), uno para cada modelo, y adicionalmente, el nodo de probabilidad etiquetado Mod[Mj] forman los padres del nodo de probabilidad, Aj. Por lo tanto, hay tantos nodos de acción (A1 j , A2 j ) en Mj,l−1 como el número de modelos en el soporte de las creencias del agente. La tabla de probabilidad condicional del nodo de probabilidad, Aj, es un <br>multiplexor</br> que asume la distribución de cada uno de los nodos de acción (A1 j , A2 j ) dependiendo del valor de Mod[Mj]. Los valores de Mod[Mj] denotan los diferentes modelos de j. En otras palabras, cuando Mod[Mj] tiene el valor m1 j,l−1, el nodo de probabilidad Aj asume la distribución del nodo A1 j, y Aj asume la distribución de A2 j cuando Mod[Mj] tiene el valor m2 j,l−1. La distribución sobre el nodo, Mod[Mj], es la creencia del agente sobre los modelos de j dado un estado físico. Para más agentes, tendremos tantos nodos de modelo como agentes haya. Observa que la Fig. 1(b) aclara la semántica del enlace de política y muestra cómo puede ser representado utilizando los enlaces de dependencia tradicionales. En la Fig. 1(c), mostramos el I-ID transformado cuando el nodo del modelo es reemplazado por los nodos de probabilidad y las relaciones entre ellos. A diferencia de la representación en [15], no hay enlaces de políticas especiales, sino que el I-ID está compuesto únicamente por aquellos tipos de nodos que se encuentran en IDs tradicionales y relaciones de dependencia entre los nodos. Esto permite que los I-IDs sean representados e implementados utilizando herramientas de aplicación convencionales que apuntan a los IDs. Ten en cuenta que podemos ver el nivel l I-ID como un NID. Específicamente, cada uno de los modelos del nivel l − 1 dentro del nodo del modelo son bloques en el NID (ver Fig. 2). Si el nivel l = 1, cada bloque es un ID tradicional, de lo contrario, si l > 1, cada bloque dentro del NID puede ser un NID en sí mismo. Ten en cuenta que dentro de los I-IDs (o IDs) en cada nivel, solo hay un nodo de decisión único. Por lo tanto, nuestro NID no contiene ningún MAID. Figura 2: Un nivel l I-ID representado como un NID. Las probabilidades asignadas a los bloques del NID son creencias sobre modelos js condicionados a un estado físico. 3.2 Solución La solución de un I-ID procede de manera ascendente y se implementa de forma recursiva. Comenzamos resolviendo los modelos de nivel 0, que, si son intencionales, son identificadores tradicionales. Sus soluciones proporcionan distribuciones de probabilidad sobre las acciones de los otros agentes, las cuales se ingresan en los nodos de probabilidad correspondientes encontrados en el nodo del modelo del nivel 1 I-ID. El mapeo de los nodos de decisión de los modelos de nivel 0 a los nodos de probabilidad se realiza de manera que las acciones con el mayor valor en el nodo de decisión se les asignan probabilidades uniformes en el nodo de probabilidad, mientras que el resto se les asigna probabilidad cero. Dadas las distribuciones sobre las acciones dentro de los diferentes nodos de probabilidad (uno para cada modelo del otro agente), el I-ID de nivel 1 se transforma como se muestra en la Fig. 1(c). Durante la transformación, la tabla de probabilidad condicional (CPT) del nodo, Aj, se llena de tal manera que el nodo asume la distribución de cada uno de los nodos de probabilidad dependiendo del valor del nodo, Mod[Mj]. Como mencionamos anteriormente, los valores del nodo Mod[Mj] denotan los diferentes modelos del otro agente, y su distribución es la creencia del agente sobre los modelos de j condicionados al estado físico. El nivel 1 I-ID transformado es un ID tradicional que puede ser resuelto como us816 The Sixth Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) (a) (b) Figura 3: (a) Un I-DID genérico de dos niveles de tiempo para el agente i en un entorno con otro agente j. Observa el enlace de actualización del modelo punteado que denota la actualización de los modelos de j y la distribución de los modelos a lo largo del tiempo. (b) La semántica del enlace de actualización del modelo utilizando el método estándar de maximización de utilidad esperada [18]. Este procedimiento se lleva a cabo hasta el nivel l I-ID cuya solución proporciona el conjunto no vacío de acciones óptimas que el agente debe realizar dada su creencia. Ten en cuenta que, al igual que los IDs, los I-IDs son adecuados para la toma de decisiones en línea cuando se conoce la creencia actual de los agentes. 4. Diagramas de influencia dinámica interactivos (I-DIDs) Los diagramas de influencia dinámica interactivos (I-DIDs) extienden los I-IDs (y NIDs) para permitir la toma de decisiones secuencial a lo largo de varios pasos de tiempo. Así como los DIDs son representaciones gráficas estructuradas de POMDPs, los I-DIDs son los análogos gráficos en línea para I-POMDPs finitamente anidados. Los I-DIDs pueden ser utilizados para optimizar sobre una mirada anticipada finita dadas las creencias iniciales mientras se interactúa con otros agentes, posiblemente similares. 4.1 Sintaxis Representamos un I-DID de dos cortes temporales en la Fig. 3(a). Además de los nodos del modelo y el enlace de política discontinuo, lo que diferencia a un I-DID de un DID es el enlace de actualización del modelo mostrado como una flecha punteada en la Fig. 3(a). Explicamos la semántica del nodo del modelo y el enlace de la política en la sección anterior; a continuación, describimos las actualizaciones del modelo. La actualización del nodo del modelo con el tiempo implica dos pasos: primero, dados los modelos en el tiempo t, identificamos el conjunto actualizado de modelos que residen en el nodo del modelo en el tiempo t + 1. Recuerda de la Sección 2 que el modelo intencional de un agente incluye sus creencias. Debido a que los agentes actúan y reciben observaciones, sus modelos se actualizan para reflejar sus creencias cambiadas. Dado que el conjunto de acciones óptimas para un modelo podría incluir todas las acciones, y el agente podría recibir cualquiera de las |Ωj| posibles observaciones, el conjunto actualizado en el paso de tiempo t + 1 tendrá como máximo |Mt j,l−1||Aj||Ωj| modelos. Aquí, |Mt j,l−1| es el número de modelos en el paso de tiempo t, |Aj| y |Ωj| son los espacios más grandes de acciones y observaciones respectivamente, entre todos los modelos. Segundo, calculamos la nueva distribución sobre los modelos actualizados dados la distribución original y la probabilidad de que el agente realice la acción y reciba la observación que llevó al modelo actualizado. Estos pasos son parte de la actualización de creencias del agente formalizada utilizando la Ecuación 1. En la Fig. 3(b), mostramos cómo se implementa el enlace de actualización del modelo punteado en el I-DID. Si cada uno de los dos modelos de nivel l − 1 atribuidos a j en el paso de tiempo t resulta en una acción, y j podría hacer una de dos posibles observaciones, entonces el nodo del modelo en el paso de tiempo t + 1 contiene cuatro modelos actualizados (mt+1,1 j,l−1 ,mt+1,2 j,l−1 , mt+1,3 j,l−1 , y mt+1,4 j,l−1 ). Estos modelos difieren en sus creencias iniciales, cada una de las cuales es el resultado de j actualizar sus creencias debido a su acción y una posible observación. Los nodos de decisión en cada uno de los I-DIDs o DIDs que representan los modelos de nivel inferior se asignan a la Figura 4 correspondiente: I-DID transformado con los nodos del modelo y el enlace de actualización del modelo reemplazados por los nodos de probabilidad y las relaciones (en negrita) de los nodos de probabilidad, como se mencionó anteriormente. A continuación, describimos cómo se calcula la distribución sobre el conjunto actualizado de modelos (la distribución sobre el nodo de probabilidad Mod[Mt+1 j ] en Mt+1 j,l−1). La probabilidad de que el modelo actualizado js sea, digamos mt+1,1 j,l−1, depende de la probabilidad de que j realice la acción y reciba la observación que condujo a este modelo, y de la distribución previa sobre los modelos en el paso de tiempo t. Dado que el nodo de probabilidad At j asume la distribución de cada uno de los nodos de acción basados en el valor de Mod[Mt j], la probabilidad de la acción está dada por este nodo de probabilidad. Para obtener la probabilidad de una observación posible js, introducimos el nodo de probabilidad Oj, que dependiendo del valor de Mod[Mt j], asume la distribución del nodo de observación en el modelo de nivel inferior denominado Mod[Mt j]. Dado que la probabilidad de las observaciones js depende del estado físico y de las acciones conjuntas de ambos agentes, el nodo Oj está vinculado con St+1, At j y At i. De manera análoga a At j, la tabla de probabilidad condicional de Oj también es un <br>multiplexor</br> modulado por Mod[Mt j]. Finalmente, la distribución de los modelos previos en el tiempo t se obtiene a partir del nodo de probabilidad, Mod[Mt j ] en Mt j,l−1. Por consiguiente, los nodos de probabilidad, Mod[Mt j], At j y Oj, forman los padres de Mod[Mt+1 j] en Mt+1 j,l−1. Ten en cuenta que el enlace de actualización del modelo puede ser reemplazado por los enlaces de dependencia entre los nodos de probabilidad que constituyen los nodos del modelo en las dos etapas temporales. En la Fig. 4 mostramos los dos cortes temporales I-DID con los nodos del modelo reemplazados por los nodos de probabilidad y las relaciones entre ellos. Los nodos de probabilidad y los enlaces de dependencia que no están en negrita son estándar, generalmente encontrados en DIDs. La expansión del I-DID a lo largo de más pasos de tiempo requiere la repetición de los dos pasos de actualizar el conjunto de modelos que forman el 2. Nota que Oj representa la observación j en el tiempo t + 1. El Sexto Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 817 valores del nodo del modelo y añadiendo las relaciones entre los nodos de probabilidad, tantas veces como enlaces de actualización del modelo haya. Observamos que el conjunto posible de modelos del otro agente j crece exponencialmente con el número de pasos de tiempo. Por ejemplo, después de T pasos, puede haber como máximo |Mt=1 j,l−1|(|Aj||Ωj|)T −1 modelos candidatos residiendo en el nodo del modelo. 4.2 Solución Análoga a los I-ID, la solución a un I-DID de nivel l para el agente i expandido a lo largo de T pasos de tiempo puede llevarse a cabo de forma recursiva. Con el propósito de ilustración, dejemos l=1 y T=2. El método de solución utiliza la técnica estándar de anticipación, proyectando las secuencias de acciones y observaciones de los agentes hacia adelante desde el estado de creencia actual [17], y encontrando las posibles creencias que podría tener en el siguiente paso temporal. Dado que el agente i también tiene una creencia sobre los modelos de j, la búsqueda anticipada incluye descubrir los posibles modelos que j podría tener en el futuro. Por consiguiente, cada uno de los modelos subintencionales o de nivel 0 de js (representados utilizando un DID estándar) en el primer paso de tiempo debe resolverse para obtener su conjunto óptimo de acciones. Estas acciones se combinan con el conjunto de observaciones posibles que j podría hacer en ese modelo, lo que resulta en un conjunto actualizado de modelos candidatos (que incluyen las creencias actualizadas) que podrían describir el comportamiento de j. Las creencias sobre este conjunto actualizado de modelos candidatos se calculan utilizando los métodos estándar de inferencia que utilizan las relaciones de dependencia entre los nodos del modelo, como se muestra en la Figura 3(b). Observamos la naturaleza recursiva de esta solución: al resolver el agente de nivel 1 I-DID, los DIDs de nivel 0 deben ser resueltos. Si el anidamiento de los modelos es más profundo, todos los modelos en todos los niveles, comenzando desde el nivel 0, se resuelven de manera ascendente. Breve descripción del algoritmo recursivo para resolver el agente es el Algoritmo para resolver I-DID. Entrada: nivel l ≥ 1 I-ID o nivel 0 ID, T Fase de Expansión 1. Para t desde 1 hasta T − 1, hacer 2. Si l ≥ 1, entonces llenar Mt+1 j,l−1 3. Para cada mt j en el rango (Mt j,l−1) hacer 4. Llame recursivamente al algoritmo con el l - 1 I-ID (o ID) que representa mt j y el horizonte, T - t + 1 5. Mapea el nodo de decisión del I-ID (o ID) resuelto, OPT(mt j), a un nodo de probabilidad Aj 6. Para cada aj en OPT(mt j) hacer 7. Para cada oj en Oj (parte de mt j) hacer 8. Actualizar la creencia js, bt+1 j ← SE(bt j, aj, oj) 9. mt+1 j ← Nuevo I-ID (o ID) con bt+1 j como la creencia inicial 10. Rango(Mt+1 j,l−1) ∪ ← {mt+1 j } 11. Añadir el nodo del modelo, Mt+1 j,l−1, y los enlaces de dependencia entre Mt j,l−1 y Mt+1 j,l−1 (mostrados en la Fig. 3(b)) 12. Agregue los nodos de probabilidad, decisión y utilidad para la franja de tiempo t + 1 y los enlaces de dependencia entre ellos 13. Establezca las CPTs para cada nodo de probabilidad y nodo de utilidad de la Fase de Mirada Adelante 14. Aplica el método estándar de anticipación y respaldo para resolver la Figura 5 expandida de I-DID: Algoritmo para resolver un nivel l ≥ 0 I-DID. Nivel l I-DID expandido durante T pasos de tiempo con otro agente j en la Figura 5. Adoptamos un enfoque de dos fases: Dado un I-ID de nivel l (descrito previamente en la Sección 3) con todos los modelos de niveles inferiores representados también como I-IDs o IDs (si es nivel 0), el primer paso es expandir el I-ID de nivel l a lo largo de T pasos de tiempo añadiendo los enlaces de dependencia y las tablas de probabilidad condicional para cada nodo. Nos enfocamos especialmente en establecer y poblar los nodos del modelo (líneas 3-11). Ten en cuenta que Range(·) devuelve los valores (modelos de nivel inferior) de la variable aleatoria dada como entrada (nodo del modelo). En la segunda fase, utilizamos una técnica estándar de anticipación proyectando las secuencias de acción y observación durante T pasos de tiempo en el futuro, y respaldando los valores de utilidad de las creencias alcanzables. Similar a los I-IDs, los I-DIDs se reducen a DIDs en ausencia de otros agentes. Como mencionamos anteriormente, los modelos de nivel 0 son los DIDs tradicionales. Sus soluciones proporcionan distribuciones de probabilidad sobre las acciones del agente modelado en ese nivel a los I-DIDs en el nivel 1. Dadas las distribuciones de probabilidad sobre las acciones de otros agentes, los IDIDs de nivel 1 pueden resolverse como DIDs y proporcionar distribuciones de probabilidad a modelos de niveles superiores. Suponga que el número de modelos considerados en cada nivel está limitado por un número, M. Resolver un I-DID de nivel l es equivalente a resolver O(Ml) DIDs. Para ilustrar la utilidad de los I-DIDs, los aplicamos a tres dominios de problemas. Describimos, en particular, la formulación del I-DID y las prescripciones óptimas obtenidas al resolverlo. 5.1 Seguimiento-Liderazgo en el Problema del Tigre Multiagente Comenzamos nuestras ilustraciones del uso de I-IDs e I-DIDs con una versión ligeramente modificada del problema del tigre multiagente discutido en [9]. El problema tiene dos agentes, cada uno de los cuales puede abrir la puerta derecha (OR), la puerta izquierda (OL) o escuchar (L). Además de escuchar gruñidos (desde la izquierda (GL) o desde la derecha (GR)) cuando escuchan, los agentes también escuchan chirridos (desde la izquierda (CL), desde la derecha (CR) o sin chirridos (S)), que indican ruidosamente que los otros agentes están abriendo una de las puertas. Cuando se abre cualquier puerta, el tigre persiste en su ubicación original con una probabilidad del 95%. El agente i escucha gruñidos con una fiabilidad del 65% y crujidos con una fiabilidad del 95%. El agente J, por otro lado, escucha gruñidos con una fiabilidad del 95%. Por lo tanto, la configuración es tal que el agente i escucha la apertura de puertas del agente j de manera más confiable que los rugidos de los tigres. Esto sugiere que podría usar acciones de JavaScript como indicación de la ubicación del tigre, como discutimos a continuación. Las preferencias de cada agente son como en el juego de un solo agente discutido en [13]. Las funciones de transición, observación y recompensa se muestran en [16]. Un buen indicador de la utilidad de los métodos normativos para la toma de decisiones como los I-DIDs es la aparición de comportamientos sociales realistas en sus prescripciones. En entornos del persistente problema del tigre multiagente que reflejan situaciones del mundo real, demostramos la seguidismo entre los agentes y, como se muestra en [15], el engaño entre agentes que creen que están en una relación de tipo seguidor-líder. En particular, analizamos las condiciones situacionales y epistemológicas suficientes para su surgimiento. El comportamiento de seguidores, por ejemplo, resulta del agente conociendo sus propias debilidades, evaluando las fortalezas, preferencias y posibles comportamientos del otro, y dándose cuenta de que es mejor para él seguir las acciones de los demás para maximizar sus ganancias. Consideremos una configuración particular del problema del tigre en la que el agente i cree que las preferencias de j están alineadas con las suyas: ambos solo quieren obtener el oro, y la audición de j es más confiable en comparación consigo mismo. Como ejemplo, supongamos que j, al escuchar, puede discernir la ubicación del tigre el 95% de las veces en comparación con su precisión del 65%. Además, el agente i no tiene ninguna información inicial sobre la ubicación de los tigres. En otras palabras, la creencia anidada de un solo nivel, bi,1, asigna 0.5 a cada una de las dos ubicaciones del tigre. Además, considera dos modelos de j, que difieren en los niveles iniciales de creencias planas de j. Esto se representa en el nivel 1 I-ID mostrado en la Fig. 6(a). Según un modelo, j asigna una probabilidad de 0.9 a que el tigre esté detrás de la puerta izquierda, mientras que el otro 818 The Sixth Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) Figura 6: (a) Nivel 1 I-ID del agente i, (b) dos IDs de nivel 0 del agente j cuyos nodos de decisión están mapeados a los nodos de probabilidad, A1 j , A2 j , en (a). El modelo asigna 0.1 a esa ubicación (ver Fig. 6(b)). El agente i está indeciso sobre estos dos modelos de j. Si variamos la capacidad auditiva, y resolvemos el nivel 1 correspondiente I-ID expandido en tres pasos de tiempo, obtenemos las políticas de comportamiento normativas mostradas en la Figura 7 que exhiben comportamiento de seguidores. Si la probabilidad de escuchar correctamente los gruñidos es de 0.65, entonces, como se muestra en la política en la Fig. 7(a), i comienza a seguir condicionalmente las acciones de j: i abre la misma puerta que j abrió anteriormente si su evaluación propia de la ubicación de los tigres confirma la elección de j. Si i pierde la capacidad de interpretar correctamente los gruñidos por completo, sigue ciegamente a j y abre la misma puerta que j abrió anteriormente (Fig. 7(b)). Figura 7: Emergencia de (a) seguidores condicionales, y (b) seguidores ciegos en el problema del tigre. Los comportamientos de interés están en negrita. * es un comodín y representa cualquiera de las observaciones. Observamos que un solo nivel de anidación de creencias, creencias sobre los modelos de los demás, fue suficiente para que surgiera el seguidismo en el problema del tigre. Sin embargo, los requisitos epistemológicos para el surgimiento del liderazgo son más complejos. Para que un agente, digamos j, surja como líder, primero debe surgir el seguidismo en el otro agente i. Como mencionamos anteriormente, si i está seguro de que sus preferencias son idénticas a las de j, y cree que j tiene un mejor sentido del oído, i seguirá las acciones de j con el tiempo. El agente j emerge como líder si cree que lo seguiré, lo que implica que la creencia de j debe estar anidada dos niveles de profundidad para permitirle reconocer su papel de liderazgo. Darse cuenta de que lo seguiré presenta a J una oportunidad para influir en sus acciones en beneficio del bien colectivo o de su interés propio solamente. Por ejemplo, en el problema del tigre, consideremos una situación en la que si tanto i como j abren la puerta correcta, entonces cada uno recibe un pago de 20 que es el doble del original. Si solo j selecciona la puerta correcta, obtiene la recompensa de 10. Por otro lado, si ambos agentes eligen la puerta incorrecta, sus penalizaciones se reducen a la mitad. En este entorno, es tanto en el mejor interés de j como en el beneficio colectivo que utilice su experiencia para seleccionar la puerta correcta, y así ser un buen líder. Sin embargo, considera un problema ligeramente diferente en el que j se beneficia de la pérdida de i y es penalizado si i se beneficia. Específicamente, dejemos que la ganancia se reste de js, indicando que j es antagonista hacia i: si j elige la puerta correcta y i la incorrecta, entonces la pérdida de 100 se convierte en la ganancia de js. El agente J cree que yo incorrectamente pienso que las preferencias de J son aquellas que promueven el bien colectivo y que comienza creyendo con un 99% de confianza dónde está el tigre. Dado que creo que mis preferencias son similares a las de j, y que j comienza creyendo casi con certeza que una de las dos ubicaciones es la correcta (dos modelos de nivel 0 de j), comenzaré siguiendo las acciones de j. Mostramos la política normativa para resolver su I-DID anidado individualmente durante tres pasos de tiempo en la Fig. 8(a). La política demuestra que seguiré ciegamente las acciones de js. Dado que el tigre persiste en su ubicación original con una probabilidad del 0.95, seleccionaré nuevamente la misma puerta. Si j comienza el juego con una probabilidad del 99% de que el tigre esté a la derecha, resolver su I-DID anidado dos niveles profundamente, resulta en la política mostrada en la Fig. 8(b). Aunque j está casi seguro de que OL es la acción correcta, comenzará seleccionando OR, seguido de OL. La intención del agente js es engañar a i, a quien cree que seguirá las acciones de js, para así obtener $110 en el segundo paso de tiempo, lo cual es más de lo que j ganaría si fuera honesto. Figura 8: Emergencia del engaño entre agentes en el problema del tigre. Los comportamientos de interés están en negrita. * denota como antes. (a) El agente es una política que demuestra que seguirá ciegamente las acciones de js. (b) Aunque j está casi seguro de que el tigre está a la derecha, comenzará seleccionando OR, seguido de OL, para engañar a i. 5.2 Altruismo y reciprocidad en el problema del bien público El problema del bien público (PG) [7], consiste en un grupo de M agentes, cada uno de los cuales debe contribuir con algún recurso a una olla pública o guardarlo para sí mismos. Dado que los recursos aportados al fondo público se comparten entre todos los agentes, son menos valiosos para el agente cuando están en el fondo público. Sin embargo, si todos los agentes eligen contribuir con sus recursos, entonces la recompensa para cada agente es mayor que si nadie contribuye. Dado que un agente recibe su parte del bote público independientemente de si ha contribuido o no, la acción dominante es que cada agente no contribuya y en su lugar se aproveche de las contribuciones de los demás. Sin embargo, los comportamientos de los jugadores humanos en simulaciones empíricas del problema de PG difieren de las predicciones normativas. Los experimentos revelan que muchos jugadores inicialmente contribuyen con una gran cantidad al bote público, y continúan contribuyendo cuando se juega el problema del PG repetidamente, aunque en cantidades decrecientes [4]. Muchos de estos experimentos [5] informan que un pequeño grupo central de jugadores contribuye persistentemente al bote público incluso cuando todos los demás están desertando. Estos experimentos también revelan que los jugadores que contribuyen persistentemente tienen preferencias altruistas o recíprocas que coinciden con la cooperación esperada de los demás. Para simplificar, asumimos que el juego se juega entre M = 2 agentes, i y j. Que cada agente esté inicialmente dotado con una cantidad de recursos XT. Mientras que la formulación clásica del juego PG permite que cada agente contribuya con cualquier cantidad de recursos (≤ XT) al bote público, simplificamos el espacio de acciones al permitir dos acciones posibles. Cada agente puede elegir contribuir (C) una cantidad fija de recursos, o no contribuir. La última acción es deThe Sixth Intl. La Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 819 fue señalada como defectuosa (D). Suponemos que las acciones no son observables por otros. El valor de los recursos en la bolsa pública se descuenta por ci para cada agente i, donde ci es el retorno privado marginal. Suponemos que ci < 1 para que el agente no se beneficie lo suficiente como para contribuir al bote público en beneficio propio. Simultáneamente, ciM > 1, lo que hace que la contribución colectiva sea óptima de Pareto. i/j C D C 2ciXT , 2cjXT ciXT − cp, XT + cjXT − P D XT + ciXT − P, cjXT − cp XT , XT Tabla 1: El juego PG de un solo disparo con castigo. Para fomentar las contribuciones, los agentes contribuyentes castigan a los que no colaboran pero incurren en un pequeño costo por administrar el castigo. Sea P la sanción impuesta al agente que se desvía y cp el costo no nulo de castigar al agente contribuyente. Para simplificar, asumimos que el costo de castigar es el mismo para ambos agentes. El juego PG de un solo disparo con castigo se muestra en la Tabla 1. Si ci = cj, cp > 0, y si P > XT − ciXT, entonces la deserción ya no es una acción dominante. Si P < XT − ciXT, entonces la deserción es la acción dominante para ambos. Si P = XT − ciXT, entonces el juego no es resoluble por dominancia. Figura 9: (a) Nivel 1 I-ID del agente i, (b) IDs de nivel 0 del agente j con nodos de decisión mapeados a los nodos de probabilidad, A1 j y A2 j, en (a). Formulamos una versión secuencial del problema PG con castigo desde la perspectiva del agente i. Aunque en el juego de PG repetido, la cantidad en la olla pública se revela a todos los agentes después de cada ronda de acciones, asumimos en nuestra formulación que está oculta para los agentes. Cada agente puede contribuir con una cantidad fija, xc, o desertar. Un agente al realizar una acción recibe una observación de abundante (PY) o escasa (MR) simbolizando el estado de la olla pública. Ten en cuenta que las observaciones también son indirectamente indicativas de las acciones del agente js porque el estado del bote público es influenciado por ellas. La cantidad de recursos en el agente es una olla privada, es perfectamente observable para i. Los pagos son análogos a la Tabla 1. Tomando prestado de las investigaciones empíricas del problema PG [5], construimos IDs de nivel 0 para j que modelan tipos altruistas y no altruistas (Fig. 9(b)). Específicamente, nuestro agente altruista tiene un alto retorno privado marginal (cj está cerca de 1) y no castiga a los demás que incumplen. Que xc = 1 y el agente de nivel 0 sea castigado la mitad de las veces que se desvía. Con una acción restante, ambos tipos de agentes eligen contribuir para evitar ser castigados. Con dos acciones por delante, el tipo altruista elige contribuir, mientras que el otro defecta. Esto se debe a que cj para el tipo altruista es cercano a 1, por lo tanto, el castigo esperado, 0.5P > (1 − cj), que el tipo altruista evita. Debido a que cj para el tipo no altruista es menor, prefiere no contribuir. Con tres pasos por delante, el agente altruista contribuye para evitar el castigo (0.5P > 2(1 − cj)), mientras que el tipo no altruista se desentiende. Para más de tres pasos, mientras el agente altruista continúa contribuyendo al bote público dependiendo de qué tan cercano esté su retorno privado marginal a 1, el tipo no altruista prescribe la deserción. Analizamos las decisiones de un agente altruista i modelado usando un nivel 1 I-DID expandido en 3 pasos de tiempo. i atribuye los dos modelos de nivel 0, mencionados anteriormente, a j (ver Fig. 9). Si creo con una probabilidad de 1 que j es altruista, elijo contribuir en cada uno de los tres pasos. Este comportamiento persiste cuando i no está al tanto de si j es altruista (Fig. 10(a)), y cuando i asigna una alta probabilidad a que j sea del tipo no altruista. Sin embargo, cuando i cree con una probabilidad de 1 que j no es altruista y por lo tanto seguramente va a traicionar, i elige traicionar para evitar ser castigado y porque su beneficio privado marginal es menor que 1. Estos resultados demuestran que el comportamiento de nuestro tipo altruista se asemeja al encontrado experimentalmente. El agente de nivel 1 no altruista elige desertar independientemente de lo altruista que crea que sea el otro agente. Analizamos el comportamiento de un tipo de agente recíproco que se ajusta a la cooperación o defección esperada. El retorno privado marginal del tipo recíproco es similar al del tipo no altruista, sin embargo, obtiene una mayor recompensa cuando su acción es similar a la del otro. Consideramos el caso en el que el agente recíproco i no está seguro de si j es altruista y cree que es probable que el bote público esté medio lleno. Para esta creencia previa, yo elijo defectar. Al recibir una observación de abundancia, decide contribuir, mientras que una observación de escasez lo hace defectar (Fig. 10(b)). Esto se debe a que una observación de abundancia indica que es probable que la olla esté más llena que a la mitad, lo cual resulta de la acción de js para contribuir. Por lo tanto, entre los dos modelos atribuidos a j, es probable que su tipo sea altruista, lo que hace probable que j contribuya nuevamente en el próximo paso de tiempo. El agente i, por lo tanto, elige contribuir para corresponder la acción de js. Un razonamiento análogo lleva a uno a defectar cuando observa una olla escasa. Con una acción por hacer, creo que si j contribuye, también elegiré contribuir para evitar castigo independientemente de sus observaciones. Figura 10: (a) Un agente altruista de nivel 1 siempre contribuye. (b) Un agente recíproco i comienza defraudando y luego elige contribuir o defraudar basándose en su observación de abundancia (indicando que j es probablemente altruista) o escasez (j no es altruista). 5.3 Estrategias en el póker de dos jugadores El póker es un popular juego de cartas de suma cero que ha recibido mucha atención en la comunidad de investigación de IA como un banco de pruebas [2]. El póker se juega entre M ≥ 2 jugadores, en el que cada jugador recibe una mano de cartas de una baraja. Si bien existen varias variantes de póker con diferentes niveles de complejidad, consideramos una versión simple en la que cada jugador tiene tres turnos durante los cuales el jugador puede intercambiar una carta (E), mantener la mano existente (K), retirarse (F) y abandonar el juego, o apostar (C), lo que requiere que todos los jugadores muestren sus manos. Para mantener las cosas simples, dejemos que M = 2, y que cada jugador reciba una mano compuesta por una sola carta sacada del mismo palo. Por lo tanto, durante un enfrentamiento, el jugador que tenga la carta numéricamente más alta (el 2 es el más bajo, el as es el más alto) gana el bote. Durante un intercambio de cartas, la carta descartada se coloca ya sea en la pila L, indicando al otro agente que era una carta de número bajo menor que 8, o en la 820 The Sixth Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) H, indicando que la carta tenía un rango mayor o igual a 8. Ten en cuenta que, por ejemplo, si se descarta una carta de número bajo, la probabilidad de recibir una carta baja a cambio se reduce. Mostramos el nivel 1 I-ID para el Poker simplificado de dos jugadores en la Fig. 11. Consideramos dos modelos (tipos de personalidad) del agente j. El tipo conservador cree que es probable que su oponente tenga una carta de alto número en su mano. Por otro lado, el agente agresivo j cree con alta probabilidad que su oponente tiene una carta de número más bajo. Por lo tanto, los dos tipos difieren en sus creencias sobre la mano de sus oponentes. En ambos estos modelos de nivel 0, se asume que el oponente realiza sus acciones siguiendo una distribución fija y uniforme. Con tres acciones restantes, independientemente de su mano (a menos que sea un as), el agente agresivo elige intercambiar su carta, con la intención de mejorar su mano actual. Esto se debe a que cree que el otro tiene una carta baja, lo que mejora sus posibilidades de obtener una carta alta durante el intercambio. El agente conservador elige quedarse con su carta, sin importar su mano, porque considera que sus posibilidades de obtener una carta alta son escasas, ya que cree que su oponente tiene una. Figura 11: (a) Nivel 1 I-ID del agente i. La observación revela información sobre la mano de js del paso de tiempo anterior, (b) IDs de nivel 0 del agente j cuyos nodos de decisión están mapeados a los nodos de probabilidad, A1 j , A2 j , en (a). La política de un agente de nivel 1 i que cree que cada carta, excepto la suya, tiene la misma probabilidad de estar en su mano (tipo de personalidad neutral) y j podría ser de tipo agresivo o conservador, se muestra en la Figura 12. Su mano contiene la carta numerada 8. El agente comienza por mantener su carta. Al ver que j no intercambió una carta (N), i cree con probabilidad 1 que j es conservador y, por lo tanto, mantendrá sus cartas. i responde manteniendo su carta o intercambiándola porque j tiene igual probabilidad de tener una carta más baja o más alta. Si observo que j descartó su carta en la pila L o H, creo que j es agresivo. Al observar a L, i se da cuenta de que j tenía una carta baja, y es probable que tenga una carta alta después de su intercambio. Dado que la probabilidad de recibir una carta baja es alta en este momento, i decide quedarse con su carta. Al observar la carta H, creyendo que la probabilidad de recibir una carta de número alto es alta, i decide intercambiar su carta. En el paso final, i decide llamar independientemente de su historial de observaciones porque su creencia de que j tiene una carta más alta no es lo suficientemente alta como para concluir que es mejor retirarse y renunciar al pago. Esto se debe en parte al hecho de que una observación de, digamos, L, restablece las creencias del agente en el paso de tiempo anterior sobre las cartas de números bajos solamente. 6. DISCUSIÓN Mostramos cómo los DIDs pueden ser extendidos a los I-DIDs que permiten la toma de decisiones secuenciales en línea en entornos multiagentes inciertos. Nuestra representación gráfica de I-DIDs mejora la Figura 12 anterior: Un agente de nivel 1 es una política de tres pasos en el problema de Poker. i comienza creyendo que j tiene la misma probabilidad de ser agresivo o conservador y podría tener cualquier carta en su mano con igual probabilidad. Trabaja significativamente al ser más transparente, semánticamente claro y capaz de ser resuelto utilizando algoritmos estándar que apuntan a los DIDs. Los I-DIDs extienden los NIDs para permitir la toma de decisiones secuenciales a lo largo de múltiples pasos de tiempo en presencia de otros agentes que interactúan. Los I-DIDs pueden ser vistos como representaciones gráficas concisas para IPOMDPs que proporcionan una forma de explotar la estructura del problema y llevar a cabo la toma de decisiones en línea a medida que el agente actúa y observa dadas sus creencias previas. Actualmente estamos investigando formas de resolver I-DIDs aproximadamente con límites demostrables en la calidad de la solución. Agradecimiento: Agradecemos a Piotr Gmytrasiewicz por algunas discusiones útiles relacionadas con este trabajo. El primer autor desea agradecer el apoyo de una beca UGARF. 7. REFERENCIAS [1] R. J. Aumann. Epistemología interactiva i: Conocimiento. Revista Internacional de Teoría de Juegos, 28:263-300, 1999. [2] D. Billings, A. Davidson, J. Schaeffer y D. Szafron. El desafío del póker. AIJ, 2001. [3] A. Brandenburger y E. Dekel. Jerarquías de creencias y conocimiento común. Revista de Teoría Económica, 59:189-198, 1993. [4] C. Camerer. Teoría de juegos conductual: Experimentos en interacción estratégica. Princeton University Press, 2003. [5] E. Fehr y S. Gachter. Cooperación y castigo en experimentos de bienes públicos. Revista Económica Americana, 90(4):980-994, 2000. [6] D. Fudenberg y D. K. Levine. La Teoría del Aprendizaje en los Juegos. MIT Press, 1998. [7] D. Fudenberg y J. Tirole. Teoría de juegos. MIT Press, 1991. [8] Y. Gal y A. Pfeffer. Un lenguaje para modelar los procesos de toma de decisiones de agentes en juegos. En AAMAS, 2003. [9] P. Gmytrasiewicz y P. Doshi. Un marco para la planificación secuencial en entornos multiagentes. JAIR, 24:49-79, 2005. [10] P. Gmytrasiewicz y E. Durfee. Coordinación racional en entornos multiagente. JAAMAS, 3(4):319-350, 2000. [11] J. C. Harsanyi. Juegos con información incompleta jugados por jugadores bayesianos. Ciencia de la Gestión, 14(3):159-182, 1967. [12] R. A. Howard y J. E. Matheson. Diagramas de influencia. En R. A. Howard y J. E. Matheson, editores, Los Principios y Aplicaciones del Análisis de Decisiones. Grupo de Decisiones Estratégicas, Menlo Park, CA 94025, 1984. [13] L. Kaelbling, M. Littman y A. Cassandra. Planificación y actuación en dominios estocásticos parcialmente observables. Revista de Inteligencia Artificial, 2, 1998. [14] D. Koller y B. Milch. Diagramas de influencia multiagente para representar y resolver juegos. En IJCAI, páginas 1027-1034, 2001. [15] K. Polich y P. Gmytrasiewicz. Diagramas de influencia interactivos y dinámicos. En el taller GTDT, AAMAS, 2006. [16] B. Rathnas, P. Doshi y P. J. Gmytrasiewicz. Soluciones exactas para POMDP interactivos utilizando equivalencia conductual. En la Conferencia de Agentes Autónomos y Sistemas Multiagente (AAMAS), 2006. [17] S. Russell y P. Norvig. Inteligencia Artificial: Un Enfoque Moderno (Segunda Edición). Prentice Hall, 2003. [18] R. D. Shachter. \n\nPrentice Hall, 2003. [18] R. D. Shachter. Evaluando diagramas de influencia. Investigación de Operaciones, 34(6):871-882, 1986. [19] D. Suryadi y P. Gmytrasiewicz. Aprendiendo modelos de otros agentes utilizando diagramas de influencia. En UM, 1999. El Sexto Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 821 ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "policy link": {
            "translated_key": "enlace de política",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Graphical Models for Online Solutions to Interactive POMDPs Prashant Doshi Dept.",
                "of Computer Science University of Georgia Athens, GA 30602, USA pdoshi@cs.uga.edu Yifeng Zeng Dept. of Computer Science Aalborg University DK-9220 Aalborg, Denmark yfzeng@cs.aau.edu Qiongyu Chen Dept. of Computer Science National Univ. of Singapore 117543, Singapore chenqy@comp.nus.edu.sg ABSTRACT We develop a new graphical representation for interactive partially observable Markov decision processes (I-POMDPs) that is significantly more transparent and semantically clear than the previous representation.",
                "These graphical models called interactive dynamic influence diagrams (I-DIDs) seek to explicitly model the structure that is often present in real-world problems by decomposing the situation into chance and decision variables, and the dependencies between the variables.",
                "I-DIDs generalize DIDs, which may be viewed as graphical representations of POMDPs, to multiagent settings in the same way that I-POMDPs generalize POMDPs.",
                "I-DIDs may be used to compute the policy of an agent online as the agent acts and observes in a setting that is populated by other interacting agents.",
                "Using several examples, we show how I-DIDs may be applied and demonstrate their usefulness.",
                "Categories and Subject Descriptors I.2.11 [Distributed Artificial Intelligence]: Multiagent Systems General Terms Theory 1.",
                "INTRODUCTION Interactive partially observable Markov decision processes (IPOMDPs) [9] provide a framework for sequential decision-making in partially observable multiagent environments.",
                "They generalize POMDPs [13] to multiagent settings by including the other agents computable models in the state space along with the states of the physical environment.",
                "The models encompass all information influencing the agents behaviors, including their preferences, capabilities, and beliefs, and are thus analogous to types in Bayesian games [11].",
                "I-POMDPs adopt a subjective approach to understanding strategic behavior, rooted in a decision-theoretic framework that takes a decision-makers perspective in the interaction.",
                "In [15], Polich and Gmytrasiewicz introduced interactive dynamic influence diagrams (I-DIDs) as the computational representations of I-POMDPs.",
                "I-DIDs generalize DIDs [12], which may be viewed as computational counterparts of POMDPs, to multiagents settings in the same way that I-POMDPs generalize POMDPs.",
                "I-DIDs contribute to a growing line of work [19] that includes multi-agent influence diagrams (MAIDs) [14], and more recently, networks of influence diagrams (NIDs) [8].",
                "These formalisms seek to explicitly model the structure that is often present in real-world problems by decomposing the situation into chance and decision variables, and the dependencies between the variables.",
                "MAIDs provide an alternative to normal and extensive game forms using a graphical formalism to represent games of imperfect information with a decision node for each agents actions and chance nodes capturing the agents private information.",
                "MAIDs objectively analyze the game, efficiently computing the Nash equilibrium profile by exploiting the independence structure.",
                "NIDs extend MAIDs to include agents uncertainty over the game being played and over models of the other agents.",
                "Each model is a MAID and the network of MAIDs is collapsed, bottom up, into a single MAID for computing the equilibrium of the game keeping in mind the different models of each agent.",
                "Graphical formalisms such as MAIDs and NIDs open up a promising area of research that aims to represent multiagent interactions more transparently.",
                "However, MAIDs provide an analysis of the game from an external viewpoint and the applicability of both is limited to static single play games.",
                "Matters are more complex when we consider interactions that are extended over time, where predictions about others future actions must be made using models that change as the agents act and observe.",
                "I-DIDs address this gap by allowing the representation of other agents models as the values of a special model node.",
                "Both, other agents models and the original agents beliefs over these models are updated over time using special-purpose implementations.",
                "In this paper, we improve on the previous preliminary representation of the I-DID shown in [15] by using the insight that the static I-ID is a type of NID.",
                "Thus, we may utilize NID-specific language constructs such as multiplexers to represent the model node, and subsequently the I-ID, more transparently.",
                "Furthermore, we clarify the semantics of the special purpose <br>policy link</br> introduced in the representation of I-DID by [15], and show that it could be replaced by traditional dependency links.",
                "In the previous representation of the I-DID, the update of the agents belief over the models of others as the agents act and receive observations was denoted using a special link called the model update link that connected the model nodes over time.",
                "We explicate the semantics of this link by showing how it can be implemented using the traditional dependency links between the chance nodes that constitute the model nodes.",
                "The net result is a representation of I-DID that is significantly more transparent, semantically clear, and capable of being implemented using the standard algorithms for solving DIDs.",
                "We show how IDIDs may be used to model an agents uncertainty over others models, that may themselves be I-DIDs.",
                "Solution to the I-DID is a policy that prescribes what the agent should do over time, given its beliefs over the physical state and others models.",
                "Analogous to DIDs, I-DIDs may be used to compute the policy of an agent online as the agent acts and observes in a setting that is populated by other interacting agents. 2.",
                "BACKGROUND: FINITELY NESTED IPOMDPS Interactive POMDPs generalize POMDPs to multiagent settings by including other agents models as part of the state space [9].",
                "Since other agents may also reason about others, the interactive state space is strategically nested; it contains beliefs about other agents models and their beliefs about others.",
                "For simplicity of presentation we consider an agent, i, that is interacting with one other agent, j.",
                "A finitely nested I-POMDP of agent i with a strategy level l is defined as the tuple: I-POMDPi,l = ISi,l, A, Ti, Ωi, Oi, Ri where: • ISi,l denotes a set of interactive states defined as, ISi,l = S × Mj,l−1, where Mj,l−1 = {Θj,l−1 ∪ SMj}, for l ≥ 1, and ISi,0 = S, where S is the set of states of the physical environment.",
                "Θj,l−1 is the set of computable intentional models of agent j: θj,l−1 = bj,l−1, ˆθj where the frame, ˆθj = A, Ωj, Tj, Oj, Rj, OCj .",
                "Here, j is Bayes rational and OCj is js optimality criterion.",
                "SMj is the set of subintentional models of j.",
                "Simple examples of subintentional models include a no-information model [10] and a fictitious play model [6], both of which are history independent.",
                "We give a recursive bottom-up construction of the interactive state space below.",
                "ISi,0 = S, Θj,0 = { bj,0, ˆθj | bj,0 ∈ Δ(ISj,0)} ISi,1 = S × {Θj,0 ∪ SMj}, Θj,1 = { bj,1, ˆθj | bj,1 ∈ Δ(ISj,1)} . . . . . .",
                "ISi,l = S × {Θj,l−1 ∪ SMj}, Θj,l = { bj,l, ˆθj | bj,l ∈ Δ(ISj,l)} Similar formulations of nested spaces have appeared in [1, 3]. • A = Ai × Aj is the set of joint actions of all agents in the environment; • Ti : S ×A×S → [0, 1], describes the effect of the joint actions on the physical states of the environment; • Ωi is the set of observations of agent i; • Oi : S × A × Ωi → [0, 1] gives the likelihood of the observations given the physical state and joint action; • Ri : ISi × A → R describes agent is preferences over its interactive states.",
                "Usually only the physical states will matter.",
                "Agent is policy is the mapping, Ω∗ i → Δ(Ai), where Ω∗ i is the set of all observation histories of agent i.",
                "Since belief over the interactive states forms a sufficient statistic [9], the policy can also be represented as a mapping from the set of all beliefs of agent i to a distribution over its actions, Δ(ISi) → Δ(Ai). 2.1 Belief Update Analogous to POMDPs, an agent within the I-POMDP framework updates its belief as it acts and observes.",
                "However, there are two differences that complicate the belief update in multiagent settings when compared to single agent ones.",
                "First, since the state of the physical environment depends on the actions of both agents, is prediction of how the physical state changes has to be made based on its prediction of js actions.",
                "Second, changes in js models have to be included in is belief update.",
                "Specifically, if j is intentional then an update of js beliefs due to its action and observation has to be included.",
                "In other words, i has to update its belief based on its prediction of what j would observe and how j would update its belief.",
                "If js model is subintentional, then js probable observations are appended to the observation history contained in the model.",
                "Formally, we have: Pr(ist |at−1 i , bt−1 i,l ) = β ISt−1:mt−1 j =θt j bt−1 i,l (ist−1 ) × at−1 j Pr(at−1 j |θt−1 j,l−1)Oi(st , at−1 i , at−1 j , ot i) ×Ti(st−1 , at−1 i , at−1 j , st ) ot j Oj(st , at−1 i , at−1 j , ot j) ×τ(SEθt j (bt−1 j,l−1, at−1 j , ot j) − bt j,l−1) (1) where β is the normalizing constant, τ is 1 if its argument is 0 otherwise it is 0, Pr(at−1 j |θt−1 j,l−1) is the probability that at−1 j is Bayes rational for the agent described by model θt−1 j,l−1, and SE(·) is an abbreviation for the belief update.",
                "For a version of the belief update when js model is subintentional, see [9].",
                "If agent j is also modeled as an I-POMDP, then is belief update invokes js belief update (via the term SEθt j ( bt−1 j,l−1 , at−1 j , ot j)), which in turn could invoke is belief update and so on.",
                "This recursion in belief nesting bottoms out at the 0th level.",
                "At this level, the belief update of the agent reduces to a POMDP belief update. 1 For illustrations of the belief update, additional details on I-POMDPs, and how they compare with other multiagent frameworks, see [9]. 2.2 Value Iteration Each belief state in a finitely nested I-POMDP has an associated value reflecting the maximum payoff the agent can expect in this belief state: Un ( bi,l, θi ) = max ai∈Ai is∈ISi,l ERi(is, ai)bi,l(is)+ γ oi∈Ωi Pr(oi|ai, bi,l)Un−1 ( SEθi (bi,l, ai, oi), θi ) (2) where, ERi(is, ai) = aj Ri(is, ai, aj)Pr(aj|mj,l−1) (since is = (s, mj,l−1)).",
                "Eq. 2 is a basis for value iteration in I-POMDPs.",
                "Agent is optimal action, a∗ i , for the case of finite horizon with discounting, is an element of the set of optimal actions for the belief state, OPT(θi), defined as: OPT( bi,l, θi ) = argmax ai∈Ai is∈ISi,l ERi(is, ai)bi,l(is) +γ oi∈Ωi Pr(oi|ai, bi,l)Un ( SEθi (bi,l, ai, oi), θi ) (3) 3.",
                "INTERACTIVEINFLUENCEDIAGRAMS A naive extension of influence diagrams (IDs) to settings populated by multiple agents is possible by treating other agents as automatons, represented using chance nodes.",
                "However, this approach assumes that the agents actions are controlled using a probability distribution that does not change over time.",
                "Interactive influence diagrams (I-IDs) adopt a more sophisticated approach by generalizing IDs to make them applicable to settings shared with other agents who may act and observe, and update their beliefs. 3.1 Syntax In addition to the usual chance, decision, and utility nodes, IIDs include a new type of node called the model node.",
                "We show a general level l I-ID in Fig. 1(a), where the model node (Mj,l−1) is denoted using a hexagon.",
                "We note that the probability distribution over the chance node, S, and the model node together represents agent is belief over its interactive states.",
                "In addition to the model 1 The 0th level model is a POMDP: Other agents actions are treated as exogenous events and folded into the T, O, and R functions.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 815 Figure 1: (a) A generic level l I-ID for agent i situated with one other agent j.",
                "The hexagon is the model node (Mj,l−1) whose structure we show in (b).",
                "Members of the model node are I-IDs themselves (m1 j,l−1, m2 j,l−1; diagrams not shown here for simplicity) whose decision nodes are mapped to the corresponding chance nodes (A1 j , A2 j ).",
                "Depending on the value of the node, Mod[Mj], the distribution of each of the chance nodes is assigned to the node Aj. (c) The transformed I-ID with the model node replaced by the chance nodes and the relationships between them. node, I-IDs differ from IDs by having a dashed link (called the <br>policy link</br> in [15]) between the model node and a chance node, Aj, that represents the distribution over the other agents actions given its model.",
                "In the absence of other agents, the model node and the chance node, Aj, vanish and I-IDs collapse into traditional IDs.",
                "The model node contains the alternative computational models ascribed by i to the other agent from the set, Θj,l−1 ∪ SMj, where Θj,l−1 and SMj were defined previously in Section 2.",
                "Thus, a model in the model node may itself be an I-ID or ID, and the recursion terminates when a model is an ID or subintentional.",
                "Because the model node contains the alternative models of the other agent as its values, its representation is not trivial.",
                "In particular, some of the models within the node are I-IDs that when solved generate the agents optimal policy in their decision nodes.",
                "Each decision node is mapped to the corresponding chance node, say A1 j , in the following way: if OPT is the set of optimal actions obtained by solving the I-ID (or ID), then Pr(aj ∈ A1 j ) = 1 |OP T | if aj ∈ OPT, 0 otherwise.",
                "Borrowing insights from previous work [8], we observe that the model node and the dashed <br>policy link</br> that connects it to the chance node, Aj, could be represented as shown in Fig. 1(b).",
                "The decision node of each level l − 1 I-ID is transformed into a chance node, as we mentioned previously, so that the actions with the largest value in the decision node are assigned uniform probabilities in the chance node while the rest are assigned zero probability.",
                "The different chance nodes (A1 j , A2 j ), one for each model, and additionally, the chance node labeled Mod[Mj] form the parents of the chance node, Aj.",
                "Thus, there are as many action nodes (A1 j , A2 j ) in Mj,l−1 as the number of models in the support of agent is beliefs.",
                "The conditional probability table of the chance node, Aj, is a multiplexer that assumes the distribution of each of the action nodes (A1 j , A2 j ) depending on the value of Mod[Mj].",
                "The values of Mod[Mj] denote the different models of j.",
                "In other words, when Mod[Mj] has the value m1 j,l−1, the chance node Aj assumes the distribution of the node A1 j , and Aj assumes the distribution of A2 j when Mod[Mj] has the value m2 j,l−1.",
                "The distribution over the node, Mod[Mj], is the agent is belief over the models of j given a physical state.",
                "For more agents, we will have as many model nodes as there are agents.",
                "Notice that Fig. 1(b) clarifies the semantics of the <br>policy link</br>, and shows how it can be represented using the traditional dependency links.",
                "In Fig. 1(c), we show the transformed I-ID when the model node is replaced by the chance nodes and relationships between them.",
                "In contrast to the representation in [15], there are no special-purpose policy links, rather the I-ID is composed of only those types of nodes that are found in traditional IDs and dependency relationships between the nodes.",
                "This allows I-IDs to be represented and implemented using conventional application tools that target IDs.",
                "Note that we may view the level l I-ID as a NID.",
                "Specifically, each of the level l − 1 models within the model node are blocks in the NID (see Fig. 2).",
                "If the level l = 1, each block is a traditional ID, otherwise if l > 1, each block within the NID may itself be a NID.",
                "Note that within the I-IDs (or IDs) at each level, there is only a single decision node.",
                "Thus, our NID does not contain any MAIDs.",
                "Figure 2: A level l I-ID represented as a NID.",
                "The probabilities assigned to the blocks of the NID are is beliefs over js models conditioned on a physical state. 3.2 Solution The solution of an I-ID proceeds in a bottom-up manner, and is implemented recursively.",
                "We start by solving the level 0 models, which, if intentional, are traditional IDs.",
                "Their solutions provide probability distributions over the other agents actions, which are entered in the corresponding chance nodes found in the model node of the level 1 I-ID.",
                "The mapping from the level 0 models decision nodes to the chance nodes is carried out so that actions with the largest value in the decision node are assigned uniform probabilities in the chance node while the rest are assigned zero probability.",
                "Given the distributions over the actions within the different chance nodes (one for each model of the other agent), the level 1 I-ID is transformed as shown in Fig. 1(c).",
                "During the transformation, the conditional probability table (CPT) of the node, Aj, is populated such that the node assumes the distribution of each of the chance nodes depending on the value of the node, Mod[Mj].",
                "As we mentioned previously, the values of the node Mod[Mj] denote the different models of the other agent, and its distribution is the agent is belief over the models of j conditioned on the physical state.",
                "The transformed level 1 I-ID is a traditional ID that may be solved us816 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) (a) (b) Figure 3: (a) A generic two time-slice level l I-DID for agent i in a setting with one other agent j.",
                "Notice the dotted model update link that denotes the update of the models of j and the distribution over the models over time. (b) The semantics of the model update link. ing the standard expected utility maximization method [18].",
                "This procedure is carried out up to the level l I-ID whose solution gives the non-empty set of optimal actions that the agent should perform given its belief.",
                "Notice that analogous to IDs, I-IDs are suitable for online decision-making when the agents current belief is known. 4.",
                "INTERACTIVE DYNAMIC INFLUENCE DIAGRAMS Interactive dynamic influence diagrams (I-DIDs) extend I-IDs (and NIDs) to allow sequential decision-making over several time steps.",
                "Just as DIDs are structured graphical representations of POMDPs, I-DIDs are the graphical online analogs for finitely nested I-POMDPs.",
                "I-DIDs may be used to optimize over a finite look-ahead given initial beliefs while interacting with other, possibly similar, agents. 4.1 Syntax We depict a general two time-slice I-DID in Fig. 3(a).",
                "In addition to the model nodes and the dashed <br>policy link</br>, what differentiates an I-DID from a DID is the model update link shown as a dotted arrow in Fig. 3(a).",
                "We explained the semantics of the model node and the <br>policy link</br> in the previous section; we describe the model updates next.",
                "The update of the model node over time involves two steps: First, given the models at time t, we identify the updated set of models that reside in the model node at time t + 1.",
                "Recall from Section 2 that an agents intentional model includes its belief.",
                "Because the agents act and receive observations, their models are updated to reflect their changed beliefs.",
                "Since the set of optimal actions for a model could include all the actions, and the agent may receive any one of |Ωj| possible observations, the updated set at time step t + 1 will have at most |Mt j,l−1||Aj||Ωj| models.",
                "Here, |Mt j,l−1| is the number of models at time step t, |Aj| and |Ωj| are the largest spaces of actions and observations respectively, among all the models.",
                "Second, we compute the new distribution over the updated models given the original distribution and the probability of the agent performing the action and receiving the observation that led to the updated model.",
                "These steps are a part of agent is belief update formalized using Eq. 1.",
                "In Fig. 3(b), we show how the dotted model update link is implemented in the I-DID.",
                "If each of the two level l − 1 models ascribed to j at time step t results in one action, and j could make one of two possible observations, then the model node at time step t + 1 contains four updated models (mt+1,1 j,l−1 ,mt+1,2 j,l−1 , mt+1,3 j,l−1 , and mt+1,4 j,l−1 ).",
                "These models differ in their initial beliefs, each of which is the result of j updating its beliefs due to its action and a possible observation.",
                "The decision nodes in each of the I-DIDs or DIDs that represent the lower level models are mapped to the corresponding Figure 4: Transformed I-DID with the model nodes and model update link replaced with the chance nodes and the relationships (in bold). chance nodes, as mentioned previously.",
                "Next, we describe how the distribution over the updated set of models (the distribution over the chance node Mod[Mt+1 j ] in Mt+1 j,l−1) is computed.",
                "The probability that js updated model is, say mt+1,1 j,l−1 , depends on the probability of j performing the action and receiving the observation that led to this model, and the prior distribution over the models at time step t. Because the chance node At j assumes the distribution of each of the action nodes based on the value of Mod[Mt j ], the probability of the action is given by this chance node.",
                "In order to obtain the probability of js possible observation, we introduce the chance node Oj, which depending on the value of Mod[Mt j ] assumes the distribution of the observation node in the lower level model denoted by Mod[Mt j ].",
                "Because the probability of js observations depends on the physical state and the joint actions of both agents, the node Oj is linked with St+1 , At j, and At i. 2 Analogous to At j, the conditional probability table of Oj is also a multiplexer modulated by Mod[Mt j ].",
                "Finally, the distribution over the prior models at time t is obtained from the chance node, Mod[Mt j ] in Mt j,l−1.",
                "Consequently, the chance nodes, Mod[Mt j ], At j, and Oj, form the parents of Mod[Mt+1 j ] in Mt+1 j,l−1.",
                "Notice that the model update link may be replaced by the dependency links between the chance nodes that constitute the model nodes in the two time slices.",
                "In Fig. 4 we show the two time-slice I-DID with the model nodes replaced by the chance nodes and the relationships between them.",
                "Chance nodes and dependency links that not in bold are standard, usually found in DIDs.",
                "Expansion of the I-DID over more time steps requires the repetition of the two steps of updating the set of models that form the 2 Note that Oj represents js observation at time t + 1.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 817 values of the model node and adding the relationships between the chance nodes, as many times as there are model update links.",
                "We note that the possible set of models of the other agent j grows exponentially with the number of time steps.",
                "For example, after T steps, there may be at most |Mt=1 j,l−1|(|Aj||Ωj|)T −1 candidate models residing in the model node. 4.2 Solution Analogous to I-IDs, the solution to a level l I-DID for agent i expanded over T time steps may be carried out recursively.",
                "For the purpose of illustration, let l=1 and T=2.",
                "The solution method uses the standard look-ahead technique, projecting the agents action and observation sequences forward from the current belief state [17], and finding the possible beliefs that i could have in the next time step.",
                "Because agent i has a belief over js models as well, the lookahead includes finding out the possible models that j could have in the future.",
                "Consequently, each of js subintentional or level 0 models (represented using a standard DID) in the first time step must be solved to obtain its optimal set of actions.",
                "These actions are combined with the set of possible observations that j could make in that model, resulting in an updated set of candidate models (that include the updated beliefs) that could describe the behavior of j. Beliefs over this updated set of candidate models are calculated using the standard inference methods using the dependency relationships between the model nodes as shown in Fig. 3(b).",
                "We note the recursive nature of this solution: in solving agent is level 1 I-DID, js level 0 DIDs must be solved.",
                "If the nesting of models is deeper, all models at all levels starting from 0 are solved in a bottom-up manner.",
                "We briefly outline the recursive algorithm for solving agent is Algorithm for solving I-DID Input : level l ≥ 1 I-ID or level 0 ID, T Expansion Phase 1.",
                "For t from 1 to T − 1 do 2.",
                "If l ≥ 1 then Populate Mt+1 j,l−1 3.",
                "For each mt j in Range(Mt j,l−1) do 4.",
                "Recursively call algorithm with the l − 1 I-ID (or ID) that represents mt j and the horizon, T − t + 1 5.",
                "Map the decision node of the solved I-ID (or ID), OPT(mt j), to a chance node Aj 6.",
                "For each aj in OPT(mt j) do 7.",
                "For each oj in Oj (part of mt j) do 8.",
                "Update js belief, bt+1 j ← SE(bt j, aj, oj) 9. mt+1 j ← New I-ID (or ID) with bt+1 j as the initial belief 10.",
                "Range(Mt+1 j,l−1) ∪ ← {mt+1 j } 11.",
                "Add the model node, Mt+1 j,l−1, and the dependency links between Mt j,l−1 and Mt+1 j,l−1 (shown in Fig. 3(b)) 12.",
                "Add the chance, decision, and utility nodes for t + 1 time slice and the dependency links between them 13.",
                "Establish the CPTs for each chance node and utility node Look-Ahead Phase 14.",
                "Apply the standard look-ahead and backup method to solve the expanded I-DID Figure 5: Algorithm for solving a level l ≥ 0 I-DID. level l I-DID expanded over T time steps with one other agent j in Fig. 5.",
                "We adopt a two-phase approach: Given an I-ID of level l (described previously in Section 3) with all lower level models also represented as I-IDs or IDs (if level 0), the first step is to expand the level l I-ID over T time steps adding the dependency links and the conditional probability tables for each node.",
                "We particularly focus on establishing and populating the model nodes (lines 3-11).",
                "Note that Range(·) returns the values (lower level models) of the random variable given as input (model node).",
                "In the second phase, we use a standard look-ahead technique projecting the action and observation sequences over T time steps in the future, and backing up the utility values of the reachable beliefs.",
                "Similar to I-IDs, the I-DIDs reduce to DIDs in the absence of other agents.",
                "As we mentioned previously, the 0-th level models are the traditional DIDs.",
                "Their solutions provide probability distributions over actions of the agent modeled at that level to I-DIDs at level 1.",
                "Given probability distributions over other agents actions the level 1 IDIDs can themselves be solved as DIDs, and provide probability distributions to yet higher level models.",
                "Assume that the number of models considered at each level is bound by a number, M. Solving an I-DID of level l in then equivalent to solving O(Ml ) DIDs. 5.",
                "EXAMPLE APPLICATIONS To illustrate the usefulness of I-DIDs, we apply them to three problem domains.",
                "We describe, in particular, the formulation of the I-DID and the optimal prescriptions obtained on solving it. 5.1 Followership-Leadership in the Multiagent Tiger Problem We begin our illustrations of using I-IDs and I-DIDs with a slightly modified version of the multiagent tiger problem discussed in [9].",
                "The problem has two agents, each of which can open the right door (OR), the left door (OL) or listen (L).",
                "In addition to hearing growls (from the left (GL) or from the right (GR)) when they listen, the agents also hear creaks (from the left (CL), from the right (CR), or no creaks (S)), which noisily indicate the other agents opening one of the doors.",
                "When any door is opened, the tiger persists in its original location with a probability of 95%.",
                "Agent i hears growls with a reliability of 65% and creaks with a reliability of 95%.",
                "Agent j, on the other hand, hears growls with a reliability of 95%.",
                "Thus, the setting is such that agent i hears agent j opening doors more reliably than the tigers growls.",
                "This suggests that i could use js actions as an indication of the location of the tiger, as we discuss below.",
                "Each agents preferences are as in the single agent game discussed in [13].",
                "The transition, observation, and reward functions are shown in [16].",
                "A good indicator of the usefulness of normative methods for decision-making like I-DIDs is the emergence of realistic social behaviors in their prescriptions.",
                "In settings of the persistent multiagent tiger problem that reflect real world situations, we demonstrate followership between the agents and, as shown in [15], deception among agents who believe that they are in a follower-leader type of relationship.",
                "In particular, we analyze the situational and epistemological conditions sufficient for their emergence.",
                "The followership behavior, for example, results from the agent knowing its own weaknesses, assessing the strengths, preferences, and possible behaviors of the other, and realizing that its best for it to follow the others actions in order to maximize its payoffs.",
                "Let us consider a particular setting of the tiger problem in which agent i believes that js preferences are aligned with its own - both of them just want to get the gold - and js hearing is more reliable in comparison to itself.",
                "As an example, suppose that j, on listening can discern the tigers location 95% of the times compared to is 65% accuracy.",
                "Additionally, agent i does not have any initial information about the tigers location.",
                "In other words, is single-level nested belief, bi,1, assigns 0.5 to each of the two locations of the tiger.",
                "In addition, i considers two models of j, which differ in js flat level 0 initial beliefs.",
                "This is represented in the level 1 I-ID shown in Fig. 6(a).",
                "According to one model, j assigns a probability of 0.9 that the tiger is behind the left door, while the other 818 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Figure 6: (a) Level 1 I-ID of agent i, (b) two level 0 IDs of agent j whose decision nodes are mapped to the chance nodes, A1 j , A2 j , in (a). model assigns 0.1 to that location (see Fig. 6(b)).",
                "Agent i is undecided on these two models of j.",
                "If we vary is hearing ability, and solve the corresponding level 1 I-ID expanded over three time steps, we obtain the normative behavioral policies shown in Fig 7 that exhibit followership behavior.",
                "If is probability of correctly hearing the growls is 0.65, then as shown in the policy in Fig. 7(a), i begins to conditionally follow js actions: i opens the same door that j opened previously iff is own assessment of the tigers location confirms js pick.",
                "If i loses the ability to correctly interpret the growls completely, it blindly follows j and opens the same door that j opened previously (Fig. 7(b)).",
                "Figure 7: Emergence of (a) conditional followership, and (b) blind followership in the tiger problem.",
                "Behaviors of interest are in bold. * is a wildcard, and denotes any one of the observations.",
                "We observed that a single level of belief nesting - beliefs about the others models - was sufficient for followership to emerge in the tiger problem.",
                "However, the epistemological requirements for the emergence of leadership are more complex.",
                "For an agent, say j, to emerge as a leader, followership must first emerge in the other agent i.",
                "As we mentioned previously, if i is certain that its preferences are identical to those of j, and believes that j has a better sense of hearing, i will follow js actions over time.",
                "Agent j emerges as a leader if it believes that i will follow it, which implies that js belief must be nested two levels deep to enable it to recognize its leadership role.",
                "Realizing that i will follow presents j with an opportunity to influence is actions in the benefit of the collective good or its self-interest alone.",
                "For example, in the tiger problem, let us consider a setting in which if both i and j open the correct door, then each gets a payoff of 20 that is double the original.",
                "If j alone selects the correct door, it gets the payoff of 10.",
                "On the other hand, if both agents pick the wrong door, their penalties are cut in half.",
                "In this setting, it is in both js best interest as well as the collective betterment for j to use its expertise in selecting the correct door, and thus be a good leader.",
                "However, consider a slightly different problem in which j gains from is loss and is penalized if i gains.",
                "Specifically, let is payoff be subtracted from js, indicating that j is antagonistic toward i - if j picks the correct door and i the wrong one, then is loss of 100 becomes js gain.",
                "Agent j believes that i incorrectly thinks that js preferences are those that promote the collective good and that it starts off by believing with 99% confidence where the tiger is.",
                "Because i believes that its preferences are similar to those of j, and that j starts by believing almost surely that one of the two is the correct location (two level 0 models of j), i will start by following js actions.",
                "We show is normative policy on solving its singly-nested I-DID over three time steps in Fig. 8(a).",
                "The policy demonstrates that i will blindly follow js actions.",
                "Since the tiger persists in its original location with a probability of 0.95, i will select the same door again.",
                "If j begins the game with a 99% probability that the tiger is on the right, solving js I-DID nested two levels deep, results in the policy shown in Fig. 8(b).",
                "Even though j is almost certain that OL is the correct action, it will start by selecting OR, followed by OL.",
                "Agent js intention is to deceive i who, it believes, will follow js actions, so as to gain $110 in the second time step, which is more than what j would gain if it were to be honest.",
                "Figure 8: Emergence of deception between agents in the tiger problem.",
                "Behaviors of interest are in bold. * denotes as before. (a) Agent is policy demonstrating that it will blindly follow js actions. (b) Even though j is almost certain that the tiger is on the right, it will start by selecting OR, followed by OL, in order to deceive i. 5.2 Altruism and Reciprocity in the Public Good Problem The public good (PG) problem [7], consists of a group of M agents, each of whom must either contribute some resource to a public pot or keep it for themselves.",
                "Since resources contributed to the public pot are shared among all the agents, they are less valuable to the agent when in the public pot.",
                "However, if all agents choose to contribute their resources, then the payoff to each agent is more than if no one contributes.",
                "Since an agent gets its share of the public pot irrespective of whether it has contributed or not, the dominating action is for each agent to not contribute, and instead free ride on others contributions.",
                "However, behaviors of human players in empirical simulations of the PG problem differ from the normative predictions.",
                "The experiments reveal that many players initially contribute a large amount to the public pot, and continue to contribute when the PG problem is played repeatedly, though in decreasing amounts [4].",
                "Many of these experiments [5] report that a small core group of players persistently contributes to the public pot even when all others are defecting.",
                "These experiments also reveal that players who persistently contribute have altruistic or reciprocal preferences matching expected cooperation of others.",
                "For simplicity, we assume that the game is played between M = 2 agents, i and j.",
                "Let each agent be initially endowed with XT amount of resources.",
                "While the classical PG game formulation permits each agent to contribute any quantity of resources (≤ XT ) to the public pot, we simplify the action space by allowing two possible actions.",
                "Each agent may choose to either contribute (C) a fixed amount of the resources, or not contribute.",
                "The latter action is deThe Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 819 noted as defect (D).",
                "We assume that the actions are not observable to others.",
                "The value of resources in the public pot is discounted by ci for each agent i, where ci is the marginal private return.",
                "We assume that ci < 1 so that the agent does not benefit enough that it contributes to the public pot for private gain.",
                "Simultaneously, ciM > 1, making collective contribution pareto optimal. i/j C D C 2ciXT , 2cjXT ciXT − cp, XT + cjXT − P D XT + ciXT − P, cjXT − cp XT , XT Table 1: The one-shot PG game with punishment.",
                "In order to encourage contributions, the contributing agents punish free riders but incur a small cost for administering the punishment.",
                "Let P be the punishment meted out to the defecting agent and cp the non-zero cost of punishing for the contributing agent.",
                "For simplicity, we assume that the cost of punishing is same for both the agents.",
                "The one-shot PG game with punishment is shown in Table. 1.",
                "Let ci = cj, cp > 0, and if P > XT − ciXT , then defection is no longer a dominating action.",
                "If P < XT − ciXT , then defection is the dominating action for both.",
                "If P = XT − ciXT , then the game is not dominance-solvable.",
                "Figure 9: (a) Level 1 I-ID of agent i, (b) level 0 IDs of agent j with decision nodes mapped to the chance nodes, A1 j and A2 j , in (a).",
                "We formulate a sequential version of the PG problem with punishment from the perspective of agent i.",
                "Though in the repeated PG game, the quantity in the public pot is revealed to all the agents after each round of actions, we assume in our formulation that it is hidden from the agents.",
                "Each agent may contribute a fixed amount, xc, or defect.",
                "An agent on performing an action receives an observation of plenty (PY) or meager (MR) symbolizing the state of the public pot.",
                "Notice that the observations are also indirectly indicative of agent js actions because the state of the public pot is influenced by them.",
                "The amount of resources in agent is private pot, is perfectly observable to i.",
                "The payoffs are analogous to Table. 1.",
                "Borrowing from the empirical investigations of the PG problem [5], we construct level 0 IDs for j that model altruistic and non-altruistic types (Fig. 9(b)).",
                "Specifically, our altruistic agent has a high marginal private return (cj is close to 1) and does not punish others who defect.",
                "Let xc = 1 and the level 0 agent be punished half the times it defects.",
                "With one action remaining, both types of agents choose to contribute to avoid being punished.",
                "With two actions to go, the altruistic type chooses to contribute, while the other defects.",
                "This is because cj for the altruistic type is close to 1, thus the expected punishment, 0.5P > (1 − cj), which the altruistic type avoids.",
                "Because cj for the non-altruistic type is less, it prefers not to contribute.",
                "With three steps to go, the altruistic agent contributes to avoid punishment (0.5P > 2(1 − cj)), and the non-altruistic type defects.",
                "For greater than three steps, while the altruistic agent continues to contribute to the public pot depending on how close its marginal private return is to 1, the non-altruistic type prescribes defection.",
                "We analyzed the decisions of an altruistic agent i modeled using a level 1 I-DID expanded over 3 time steps. i ascribes the two level 0 models, mentioned previously, to j (see Fig. 9).",
                "If i believes with a probability 1 that j is altruistic, i chooses to contribute for each of the three steps.",
                "This behavior persists when i is unaware of whether j is altruistic (Fig. 10(a)), and when i assigns a high probability to j being the non-altruistic type.",
                "However, when i believes with a probability 1 that j is non-altruistic and will thus surely defect, i chooses to defect to avoid being punished and because its marginal private return is less than 1.",
                "These results demonstrate that the behavior of our altruistic type resembles that found experimentally.",
                "The non-altruistic level 1 agent chooses to defect regardless of how likely it believes the other agent to be altruistic.",
                "We analyzed the behavior of a reciprocal agent type that matches expected cooperation or defection.",
                "The reciprocal types marginal private return is similar to that of the non-altruistic type, however, it obtains a greater payoff when its action is similar to that of the other.",
                "We consider the case when the reciprocal agent i is unsure of whether j is altruistic and believes that the public pot is likely to be half full.",
                "For this prior belief, i chooses to defect.",
                "On receiving an observation of plenty, i decides to contribute, while an observation of meager makes it defect (Fig. 10(b)).",
                "This is because an observation of plenty signals that the pot is likely to be greater than half full, which results from js action to contribute.",
                "Thus, among the two models ascribed to j, its type is likely to be altruistic making it likely that j will contribute again in the next time step.",
                "Agent i therefore chooses to contribute to reciprocate js action.",
                "An analogous reasoning leads i to defect when it observes a meager pot.",
                "With one action to go, i believing that j contributes, will choose to contribute too to avoid punishment regardless of its observations.",
                "Figure 10: (a) An altruistic level 1 agent always contributes. (b) A reciprocal agent i starts off by defecting followed by choosing to contribute or defect based on its observation of plenty (indicating that j is likely altruistic) or meager (j is non-altruistic). 5.3 Strategies in Two-Player Poker Poker is a popular zero sum card game that has received much attention among the AI research community as a testbed [2].",
                "Poker is played among M ≥ 2 players in which each player receives a hand of cards from a deck.",
                "While several flavors of Poker with varying complexity exist, we consider a simple version in which each player has three plys during which the player may either exchange a card (E), keep the existing hand (K), fold (F) and withdraw from the game, or call (C), requiring all players to show their hands.",
                "To keep matters simple, let M = 2, and each player receive a hand consisting of a single card drawn from the same suit.",
                "Thus, during a showdown, the player who has the numerically larger card (2 is the lowest, ace is the highest) wins the pot.",
                "During an exchange of cards, the discarded card is placed either in the L pile, indicating to the other agent that it was a low numbered card less than 8, or in the 820 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) H pile, indicating that the card had a rank greater than or equal to 8.",
                "Notice that, for example, if a lower numbered card is discarded, the probability of receiving a low card in exchange is now reduced.",
                "We show the level 1 I-ID for the simplified two-player Poker in Fig. 11.",
                "We considered two models (personality types) of agent j.",
                "The conservative type believes that it is likely that its opponent has a high numbered card in its hand.",
                "On the other hand, the aggressive agent j believes with a high probability that its opponent has a lower numbered card.",
                "Thus, the two types differ in their beliefs over their opponents hand.",
                "In both these level 0 models, the opponent is assumed to perform its actions following a fixed, uniform distribution.",
                "With three actions to go, regardless of its hand (unless it is an ace), the aggressive agent chooses to exchange its card, with the intent of improving on its current hand.",
                "This is because it believes the other to have a low card, which improves its chances of getting a high card during the exchange.",
                "The conservative agent chooses to keep its card, no matter its hand because its chances of getting a high card are slim as it believes that its opponent has one.",
                "Figure 11: (a) Level 1 I-ID of agent i.",
                "The observation reveals information about js hand of the previous time step, (b) level 0 IDs of agent j whose decision nodes are mapped to the chance nodes, A1 j , A2 j , in (a).",
                "The policy of a level 1 agent i who believes that each card except its own has an equal likelihood of being in js hand (neutral personality type) and j could be either an aggressive or conservative type, is shown in Fig. 12. is own hand contains the card numbered 8.",
                "The agent starts by keeping its card.",
                "On seeing that j did not exchange a card (N), i believes with probability 1 that j is conservative and hence will keep its cards. i responds by either keeping its card or exchanging it because j is equally likely to have a lower or higher card.",
                "If i observes that j discarded its card into the L or H pile, i believes that j is aggressive.",
                "On observing L, i realizes that j had a low card, and is likely to have a high card after its exchange.",
                "Because the probability of receiving a low card is high now, i chooses to keep its card.",
                "On observing H, believing that the probability of receiving a high numbered card is high, i chooses to exchange its card.",
                "In the final step, i chooses to call regardless of its observation history because its belief that j has a higher card is not sufficiently high to conclude that its better to fold and relinquish the payoff.",
                "This is partly due to the fact that an observation of, say, L resets the agent is previous time step beliefs over js hand to the low numbered cards only. 6.",
                "DISCUSSION We showed how DIDs may be extended to I-DIDs that enable online sequential decision-making in uncertain multiagent settings.",
                "Our graphical representation of I-DIDs improves on the previous Figure 12: A level 1 agent is three step policy in the Poker problem. i starts by believing that j is equally likely to be aggressive or conservative and could have any card in its hand with equal probability. work significantly by being more transparent, semantically clear, and capable of being solved using standard algorithms that target DIDs.",
                "I-DIDs extend NIDs to allow sequential decision-making over multiple time steps in the presence of other interacting agents.",
                "I-DIDs may be seen as concise graphical representations for IPOMDPs providing a way to exploit problem structure and carry out online decision-making as the agent acts and observes given its prior beliefs.",
                "We are currently investigating ways to solve I-DIDs approximately with provable bounds on the solution quality.",
                "Acknowledgment: We thank Piotr Gmytrasiewicz for some useful discussions related to this work.",
                "The first author would like to acknowledge the support of a UGARF grant. 7.",
                "REFERENCES [1] R. J. Aumann.",
                "Interactive epistemology i: Knowledge.",
                "International Journal of Game Theory, 28:263-300, 1999. [2] D. Billings, A. Davidson, J. Schaeffer, and D. Szafron.",
                "The challenge of poker.",
                "AIJ, 2001. [3] A. Brandenburger and E. Dekel.",
                "Hierarchies of beliefs and common knowledge.",
                "Journal of Economic Theory, 59:189-198, 1993. [4] C. Camerer.",
                "Behavioral Game Theory: Experiments in Strategic Interaction.",
                "Princeton University Press, 2003. [5] E. Fehr and S. Gachter.",
                "Cooperation and punishment in public goods experiments.",
                "American Economic Review, 90(4):980-994, 2000. [6] D. Fudenberg and D. K. Levine.",
                "The Theory of Learning in Games.",
                "MIT Press, 1998. [7] D. Fudenberg and J. Tirole.",
                "Game Theory.",
                "MIT Press, 1991. [8] Y. Gal and A. Pfeffer.",
                "A language for modeling agents decision-making processes in games.",
                "In AAMAS, 2003. [9] P. Gmytrasiewicz and P. Doshi.",
                "A framework for sequential planning in multiagent settings.",
                "JAIR, 24:49-79, 2005. [10] P. Gmytrasiewicz and E. Durfee.",
                "Rational coordination in multi-agent environments.",
                "JAAMAS, 3(4):319-350, 2000. [11] J. C. Harsanyi.",
                "Games with incomplete information played by bayesian players.",
                "Management Science, 14(3):159-182, 1967. [12] R. A. Howard and J. E. Matheson.",
                "Influence diagrams.",
                "In R. A. Howard and J. E. Matheson, editors, The Principles and Applications of Decision Analysis.",
                "Strategic Decisions Group, Menlo Park, CA 94025, 1984. [13] L. Kaelbling, M. Littman, and A. Cassandra.",
                "Planning and acting in partially observable stochastic domains.",
                "Artificial Intelligence Journal, 2, 1998. [14] D. Koller and B. Milch.",
                "Multi-agent influence diagrams for representing and solving games.",
                "In IJCAI, pages 1027-1034, 2001. [15] K. Polich and P. Gmytrasiewicz.",
                "Interactive dynamic influence diagrams.",
                "In GTDT Workshop, AAMAS, 2006. [16] B.",
                "Rathnas., P. Doshi, and P. J. Gmytrasiewicz.",
                "Exact solutions to interactive pomdps using behavioral equivalence.",
                "In Autonomous Agents and Multi-Agent Systems Conference (AAMAS), 2006. [17] S. Russell and P. Norvig.",
                "Artificial Intelligence: A Modern Approach (Second Edition).",
                "Prentice Hall, 2003. [18] R. D. Shachter.",
                "Evaluating influence diagrams.",
                "Operations Research, 34(6):871-882, 1986. [19] D. Suryadi and P. Gmytrasiewicz.",
                "Learning models of other agents using influence diagrams.",
                "In UM, 1999.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 821"
            ],
            "original_annotated_samples": [
                "Furthermore, we clarify the semantics of the special purpose <br>policy link</br> introduced in the representation of I-DID by [15], and show that it could be replaced by traditional dependency links.",
                "Depending on the value of the node, Mod[Mj], the distribution of each of the chance nodes is assigned to the node Aj. (c) The transformed I-ID with the model node replaced by the chance nodes and the relationships between them. node, I-IDs differ from IDs by having a dashed link (called the <br>policy link</br> in [15]) between the model node and a chance node, Aj, that represents the distribution over the other agents actions given its model.",
                "Borrowing insights from previous work [8], we observe that the model node and the dashed <br>policy link</br> that connects it to the chance node, Aj, could be represented as shown in Fig. 1(b).",
                "Notice that Fig. 1(b) clarifies the semantics of the <br>policy link</br>, and shows how it can be represented using the traditional dependency links.",
                "In addition to the model nodes and the dashed <br>policy link</br>, what differentiates an I-DID from a DID is the model update link shown as a dotted arrow in Fig. 3(a)."
            ],
            "translated_annotated_samples": [
                "Además, aclaramos la semántica del <br>enlace de política</br> de propósito especial introducido en la representación de I-DID por [15], y mostramos que podría ser reemplazado por enlaces de dependencia tradicionales.",
                "Dependiendo del valor del nodo, Mod[Mj], la distribución de cada uno de los nodos de probabilidad se asigna al nodo Aj. (c) El I-ID transformado con el nodo del modelo reemplazado por los nodos de probabilidad y las relaciones entre ellos. Los I-IDs difieren de los IDs al tener un enlace discontinuo (llamado <br>enlace de política</br> en [15]) entre el nodo del modelo y un nodo de probabilidad, Aj, que representa la distribución sobre las acciones de los otros agentes dada su modelo.",
                "Tomando prestados los conocimientos de trabajos anteriores [8], observamos que el nodo del modelo y el <br>enlace de política</br> discontinua que lo conecta con el nodo de probabilidad, Aj, podrían representarse como se muestra en la Fig. 1(b).",
                "Observa que la Fig. 1(b) aclara la semántica del <br>enlace de política</br> y muestra cómo puede ser representado utilizando los enlaces de dependencia tradicionales.",
                "Además de los nodos del modelo y el <br>enlace de política</br> discontinuo, lo que diferencia a un I-DID de un DID es el enlace de actualización del modelo mostrado como una flecha punteada en la Fig. 3(a)."
            ],
            "translated_text": "Modelos gráficos para soluciones en línea a POMDP interactivos de Prashant Doshi Dept. Departamento de Ciencias de la Computación Universidad de Georgia Athens, GA 30602, EE. UU. pdoshi@cs.uga.edu Yifeng Zeng Dept. de Ciencias de la Computación Universidad de Aalborg DK-9220 Aalborg, Dinamarca yfzeng@cs.aau.edu Qiongyu Chen Dept. de Ciencias de la Computación Universidad Nacional de Singapur 117543, Singapur chenqy@comp.nus.edu.sg RESUMEN Desarrollamos una nueva representación gráfica para procesos de decisión de Markov parcialmente observables interactivos (I-POMDPs) que es significativamente más transparente y semánticamente clara que la representación anterior. Estos modelos gráficos llamados diagramas de influencia dinámica interactiva (I-DIDs) buscan modelar explícitamente la estructura que a menudo está presente en problemas del mundo real al descomponer la situación en variables de probabilidad y decisión, y las dependencias entre las variables. Los I-DIDs generalizan los DIDs, los cuales pueden ser vistos como representaciones gráficas de POMDPs, a entornos multiagentes de la misma manera que los I-POMDPs generalizan los POMDPs. Los I-DIDs pueden ser utilizados para calcular la política de un agente en línea mientras el agente actúa y observa en un entorno poblado por otros agentes que interactúan entre sí. Usando varios ejemplos, mostramos cómo se pueden aplicar los I-DIDs y demostramos su utilidad. Categorías y Descriptores de Asignaturas I.2.11 [Inteligencia Artificial Distribuida]: Sistemas Multiagentes Términos Generales Teoría 1. Las Procesos de Decisión de Markov Interactivos Parcialmente Observables (IPOMDPs) proporcionan un marco para la toma de decisiones secuenciales en entornos multiagentes parcialmente observables. Generalizan los POMDPs [13] a entornos multiagentes al incluir los modelos computables de los otros agentes en el espacio de estados junto con los estados del entorno físico. Los modelos abarcan toda la información que influye en los comportamientos de los agentes, incluyendo sus preferencias, capacidades y creencias, y por lo tanto son análogos a los tipos en los juegos bayesianos [11]. I-POMDPs adoptan un enfoque subjetivo para comprender el comportamiento estratégico, arraigado en un marco de teoría de decisiones que toma la perspectiva de los tomadores de decisiones en la interacción. En [15], Polich y Gmytrasiewicz introdujeron diagramas de influencia dinámica interactivos (I-DIDs) como las representaciones computacionales de I-POMDPs. Los I-DIDs generalizan los DIDs [12], los cuales pueden ser vistos como contrapartes computacionales de POMDPs, a entornos de múltiples agentes de la misma manera que los I-POMDPs generalizan los POMDPs. Los I-DIDs contribuyen a una creciente línea de trabajo [19] que incluye diagramas de influencia multiagente (MAIDs) [14], y más recientemente, redes de diagramas de influencia (NIDs) [8]. Estos formalismos buscan modelar explícitamente la estructura que a menudo está presente en problemas del mundo real al descomponer la situación en variables de azar y decisiones, y las dependencias entre las variables. Los MAIDs proporcionan una alternativa a las formas normales y extensivas de juego utilizando un formalismo gráfico para representar juegos de información imperfecta con un nodo de decisión para las acciones de cada agente y nodos de azar que capturan la información privada de los agentes. Los MAIDs analizan objetivamente el juego, calculando eficientemente el perfil de equilibrio de Nash al explotar la estructura de independencia. NIDs extienden los MAIDs para incluir la incertidumbre de los agentes sobre el juego que se está jugando y sobre los modelos de los otros agentes. Cada modelo es un MAID y la red de MAIDs se colapsa, de abajo hacia arriba, en un solo MAID para calcular el equilibrio del juego teniendo en cuenta los diferentes modelos de cada agente. Los formalismos gráficos como MAIDs y NIDs abren una área de investigación prometedora que tiene como objetivo representar las interacciones multiagente de manera más transparente. Sin embargo, los MAIDs proporcionan un análisis del juego desde un punto de vista externo y la aplicabilidad de ambos está limitada a juegos estáticos de un solo jugador. Los asuntos son más complejos cuando consideramos interacciones que se extienden en el tiempo, donde las predicciones sobre las acciones futuras de otros deben hacerse utilizando modelos que cambian a medida que los agentes actúan y observan. Los I-DIDs abordan esta brecha al permitir la representación de modelos de otros agentes como los valores de un nodo de modelo especial. Tanto los modelos de otros agentes como las creencias originales de los agentes sobre estos modelos se actualizan con el tiempo utilizando implementaciones especializadas. En este documento, mejoramos la representación preliminar previa del I-DID mostrada en [15] utilizando la idea de que el I-ID estático es un tipo de NID. Por lo tanto, podemos utilizar construcciones de lenguaje específicas de NID, como multiplexores, para representar de manera más transparente el nodo del modelo y, posteriormente, el I-ID. Además, aclaramos la semántica del <br>enlace de política</br> de propósito especial introducido en la representación de I-DID por [15], y mostramos que podría ser reemplazado por enlaces de dependencia tradicionales. En la representación previa del I-DID, la actualización de la creencia de los agentes sobre los modelos de los demás a medida que los agentes actúan y reciben observaciones se denotaba utilizando un enlace especial llamado el enlace de actualización del modelo que conectaba los nodos del modelo a lo largo del tiempo. Explicamos la semántica de este enlace mostrando cómo puede ser implementado utilizando los enlaces de dependencia tradicionales entre los nodos de probabilidad que constituyen los nodos del modelo. El resultado final es una representación de I-DID que es significativamente más transparente, semánticamente clara y capaz de ser implementada utilizando los algoritmos estándar para resolver DIDs. Mostramos cómo los IDIDs pueden ser utilizados para modelar la incertidumbre de un agente sobre los modelos de otros, que a su vez pueden ser I-DIDs. La solución al I-DID es una política que prescribe lo que el agente debe hacer con el tiempo, dadas sus creencias sobre el estado físico y otros modelos. Análogos a los DIDs, los I-DIDs pueden ser utilizados para calcular la política de un agente en línea mientras el agente actúa y observa en un entorno poblado por otros agentes interactuantes. 2. ANTECEDENTES: Los IPOMDPS ANIDADOS FINITAMENTE generalizan los POMDPs a entornos multiagentes al incluir los modelos de otros agentes como parte del espacio de estados [9]. Dado que otros agentes también pueden razonar sobre otros, el espacio de estado interactivo está estratégicamente anidado; contiene creencias sobre los modelos de otros agentes y sus creencias sobre los demás. Para simplificar la presentación, consideramos un agente, i, que está interactuando con otro agente, j. Un I-POMDP finitamente anidado del agente i con un nivel de estrategia l se define como la tupla: I-POMDPi,l = ISi,l, A, Ti, Ωi, Oi, Ri donde: • ISi,l denota un conjunto de estados interactivos definido como, ISi,l = S × Mj,l−1, donde Mj,l−1 = {Θj,l−1 ∪ SMj}, para l ≥ 1, e ISi,0 = S, donde S es el conjunto de estados del entorno físico. Θj,l−1 es el conjunto de modelos intencionales computables del agente j: θj,l−1 = bj,l−1, ˆθj donde el marco, ˆθj = A, Ωj, Tj, Oj, Rj, OCj. Aquí, j es Bayesiano racional y OCj es el criterio de optimalidad de j. SMj es el conjunto de modelos subintencionales de j. Ejemplos simples de modelos subintencionales incluyen un modelo sin información [10] y un modelo de juego ficticio [6], ambos de los cuales son independientes de la historia. Damos una construcción recursiva de abajo hacia arriba del espacio de estados interactivo a continuación. Sí,0 = S, Θj,0 = { bj,0, ˆθj | bj,0 ∈ Δ(ISj,0)} Sí,1 = S × {Θj,0 ∪ SMj}, Θj,1 = { bj,1, ˆθj | bj,1 ∈ Δ(ISj,1)} . . . . . . Sí, l = S × {Θj,l−1 ∪ SMj}, Θj,l = { bj,l, ˆθj | bj,l ∈ Δ(ISj,l)} Formulaciones similares de espacios anidados han aparecido en [1, 3]. • A = Ai × Aj es el conjunto de acciones conjuntas de todos los agentes en el entorno; • Ti : S ×A×S → [0, 1], describe el efecto de las acciones conjuntas en los estados físicos del entorno; • Ωi es el conjunto de observaciones del agente i; • Oi : S × A × Ωi → [0, 1] da la probabilidad de las observaciones dadas el estado físico y la acción conjunta; • Ri : ISi × A → R describe las preferencias del agente sobre sus estados interactivos. Normalmente solo importarán los estados físicos. La política del agente es el mapeo, Ω∗ i → Δ(Ai), donde Ω∗ i es el conjunto de todos los historiales de observación del agente i. Dado que la creencia sobre los estados interactivos forma una estadística suficiente [9], la política también puede ser representada como un mapeo del conjunto de todas las creencias del agente i a una distribución sobre sus acciones, Δ(ISi) → Δ(Ai). 2.1 Actualización de Creencias Análogo a los POMDPs, un agente dentro del marco I-POMDP actualiza su creencia a medida que actúa y observa. Sin embargo, hay dos diferencias que complican la actualización de creencias en entornos multiagentes en comparación con los entornos de un solo agente. Primero, dado que el estado del entorno físico depende de las acciones de ambos agentes, la predicción de cómo cambia el estado físico debe hacerse basándose en la predicción de sus acciones. Segundo, los cambios en los modelos de js deben ser incluidos en la actualización de creencias. Específicamente, si j es intencional, entonces se debe incluir una actualización de las creencias de j debido a su acción y observación. En otras palabras, i tiene que actualizar su creencia basándose en su predicción de lo que j observaría y cómo j actualizaría su creencia. Si el modelo de js es subintencional, entonces las observaciones probables de js se añaden al historial de observaciones contenido en el modelo. Formalmente, tenemos: Pr(ist |at−1 i , bt−1 i,l ) = β ISt−1:mt−1 j =θt j bt−1 i,l (ist−1 ) × at−1 j Pr(at−1 j |θt−1 j,l−1)Oi(st , at−1 i , at−1 j , ot i) ×Ti(st−1 , at−1 i , at−1 j , st ) ot j Oj(st , at−1 i , at−1 j , ot j) ×τ(SEθt j (bt−1 j,l−1, at−1 j , ot j) − bt j,l−1) (1) donde β es la constante de normalización, τ es 1 si su argumento es 0, de lo contrario es 0, Pr(at−1 j |θt−1 j,l−1) es la probabilidad de que at−1 j sea Bayes racional para el agente descrito por el modelo θt−1 j,l−1, y SE(·) es una abreviatura para la actualización de creencias. Para una versión de la actualización de creencias cuando el modelo js es subintencional, consulte [9]. Si el agente j también se modela como un I-POMDP, entonces la actualización de creencias invoca la actualización de creencias de j (a través del término SEθt j ( bt−1 j,l−1 , at−1 j , ot j)), lo que a su vez podría invocar la actualización de creencias de is y así sucesivamente. Esta recursión en la anidación de creencias llega al nivel 0. En este nivel, la actualización de creencias del agente se reduce a una actualización de creencias de un POMDP. Para ilustraciones de la actualización de creencias, detalles adicionales sobre I-POMDPs y cómo se comparan con otros marcos multiagentes, consulte [9]. Iteración de Valor Cada estado de creencia en un I-POMDP finitamente anidado tiene un valor asociado que refleja la máxima ganancia que el agente puede esperar en este estado de creencia: Un ( bi,l, θi ) = max ai∈Ai is∈ISi,l ERi(is, ai)bi,l(is)+ γ oi∈Ωi Pr(oi|ai, bi,l)Un−1 ( SEθi (bi,l, ai, oi), θi ) (2) donde, ERi(is, ai) = aj Ri(is, ai, aj)Pr(aj|mj,l−1) (ya que is = (s, mj,l−1)). La ecuación 2 es una base para la iteración de valor en I-POMDPs. La acción óptima del agente, a∗ i, para el caso de horizonte finito con descuento, es un elemento del conjunto de acciones óptimas para el estado de creencia, OPT(θi), definido como: OPT( bi,l, θi ) = argmax ai∈Ai is∈ISi,l ERi(is, ai)bi,l(is) +γ oi∈Ωi Pr(oi|ai, bi,l)Un ( SEθi (bi,l, ai, oi), θi ) (3) 3. Diagramas de influencia interactivos. Una extensión ingenua de los diagramas de influencia (IDs) a entornos poblados por múltiples agentes es posible tratando a los otros agentes como autómatas, representados mediante nodos de probabilidad. Sin embargo, este enfoque asume que las acciones de los agentes están controladas utilizando una distribución de probabilidad que no cambia con el tiempo. Los diagramas de influencia interactivos (I-IDs) adoptan un enfoque más sofisticado al generalizar los IDs para hacerlos aplicables a entornos compartidos con otros agentes que pueden actuar, observar y actualizar sus creencias. 3.1 Sintaxis Además de los nodos habituales de probabilidad, decisión y utilidad, los IIDs incluyen un nuevo tipo de nodo llamado nodo de modelo. Mostramos un nivel general l I-ID en la Fig. 1(a), donde el nodo del modelo (Mj,l−1) está representado con un hexágono. Observamos que la distribución de probabilidad sobre el nodo de probabilidad, S, y el nodo del modelo juntos representan la creencia del agente sobre sus estados interactivos. Además del modelo 1, el modelo de nivel 0 es un POMDP: las acciones de otros agentes se tratan como eventos exógenos y se incorporan en las funciones T, O y R. El Sexto Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) Figura 1: (a) Un nivel genérico l I-ID para el agente i situado con otro agente j. El hexágono es el nodo modelo (Mj,l−1) cuya estructura mostramos en (b). Los miembros del nodo modelo son ellos mismos I-ID (m1 j,l−1, m2 j,l−1; los diagramas no se muestran aquí por simplicidad) cuyos nodos de decisión se asignan a los nodos de probabilidad correspondientes (A1 j , A2 j). Dependiendo del valor del nodo, Mod[Mj], la distribución de cada uno de los nodos de probabilidad se asigna al nodo Aj. (c) El I-ID transformado con el nodo del modelo reemplazado por los nodos de probabilidad y las relaciones entre ellos. Los I-IDs difieren de los IDs al tener un enlace discontinuo (llamado <br>enlace de política</br> en [15]) entre el nodo del modelo y un nodo de probabilidad, Aj, que representa la distribución sobre las acciones de los otros agentes dada su modelo. En ausencia de otros agentes, el nodo del modelo y el nodo de probabilidad, Aj, desaparecen y los I-ID se convierten en ID tradicionales. El nodo del modelo contiene los modelos computacionales alternativos atribuidos por i al otro agente del conjunto, Θj,l−1 ∪ SMj, donde Θj,l−1 y SMj fueron definidos previamente en la Sección 2. Por lo tanto, un modelo en el nodo del modelo puede ser en sí mismo un I-ID o ID, y la recursión termina cuando un modelo es un ID o subintencional. Dado que el nodo del modelo contiene los modelos alternativos del otro agente como sus valores, su representación no es trivial. En particular, algunos de los modelos dentro del nodo son I-IDs que, al resolverse, generan la política óptima de los agentes en sus nodos de decisión. Cada nodo de decisión se asigna al nodo de probabilidad correspondiente, digamos A1 j, de la siguiente manera: si OPT es el conjunto de acciones óptimas obtenidas al resolver el I-ID (o ID), entonces Pr(aj ∈ A1 j) = 1 |OPT| si aj ∈ OPT, 0 en caso contrario. Tomando prestados los conocimientos de trabajos anteriores [8], observamos que el nodo del modelo y el <br>enlace de política</br> discontinua que lo conecta con el nodo de probabilidad, Aj, podrían representarse como se muestra en la Fig. 1(b). El nodo de decisión de cada nivel l − 1 I-ID se transforma en un nodo de probabilidad, como mencionamos anteriormente, de modo que las acciones con el mayor valor en el nodo de decisión se les asignan probabilidades uniformes en el nodo de probabilidad, mientras que al resto se les asigna probabilidad cero. Los diferentes nodos de probabilidad (A1 j , A2 j ), uno para cada modelo, y adicionalmente, el nodo de probabilidad etiquetado Mod[Mj] forman los padres del nodo de probabilidad, Aj. Por lo tanto, hay tantos nodos de acción (A1 j , A2 j ) en Mj,l−1 como el número de modelos en el soporte de las creencias del agente. La tabla de probabilidad condicional del nodo de probabilidad, Aj, es un multiplexor que asume la distribución de cada uno de los nodos de acción (A1 j , A2 j ) dependiendo del valor de Mod[Mj]. Los valores de Mod[Mj] denotan los diferentes modelos de j. En otras palabras, cuando Mod[Mj] tiene el valor m1 j,l−1, el nodo de probabilidad Aj asume la distribución del nodo A1 j, y Aj asume la distribución de A2 j cuando Mod[Mj] tiene el valor m2 j,l−1. La distribución sobre el nodo, Mod[Mj], es la creencia del agente sobre los modelos de j dado un estado físico. Para más agentes, tendremos tantos nodos de modelo como agentes haya. Observa que la Fig. 1(b) aclara la semántica del <br>enlace de política</br> y muestra cómo puede ser representado utilizando los enlaces de dependencia tradicionales. En la Fig. 1(c), mostramos el I-ID transformado cuando el nodo del modelo es reemplazado por los nodos de probabilidad y las relaciones entre ellos. A diferencia de la representación en [15], no hay enlaces de políticas especiales, sino que el I-ID está compuesto únicamente por aquellos tipos de nodos que se encuentran en IDs tradicionales y relaciones de dependencia entre los nodos. Esto permite que los I-IDs sean representados e implementados utilizando herramientas de aplicación convencionales que apuntan a los IDs. Ten en cuenta que podemos ver el nivel l I-ID como un NID. Específicamente, cada uno de los modelos del nivel l − 1 dentro del nodo del modelo son bloques en el NID (ver Fig. 2). Si el nivel l = 1, cada bloque es un ID tradicional, de lo contrario, si l > 1, cada bloque dentro del NID puede ser un NID en sí mismo. Ten en cuenta que dentro de los I-IDs (o IDs) en cada nivel, solo hay un nodo de decisión único. Por lo tanto, nuestro NID no contiene ningún MAID. Figura 2: Un nivel l I-ID representado como un NID. Las probabilidades asignadas a los bloques del NID son creencias sobre modelos js condicionados a un estado físico. 3.2 Solución La solución de un I-ID procede de manera ascendente y se implementa de forma recursiva. Comenzamos resolviendo los modelos de nivel 0, que, si son intencionales, son identificadores tradicionales. Sus soluciones proporcionan distribuciones de probabilidad sobre las acciones de los otros agentes, las cuales se ingresan en los nodos de probabilidad correspondientes encontrados en el nodo del modelo del nivel 1 I-ID. El mapeo de los nodos de decisión de los modelos de nivel 0 a los nodos de probabilidad se realiza de manera que las acciones con el mayor valor en el nodo de decisión se les asignan probabilidades uniformes en el nodo de probabilidad, mientras que el resto se les asigna probabilidad cero. Dadas las distribuciones sobre las acciones dentro de los diferentes nodos de probabilidad (uno para cada modelo del otro agente), el I-ID de nivel 1 se transforma como se muestra en la Fig. 1(c). Durante la transformación, la tabla de probabilidad condicional (CPT) del nodo, Aj, se llena de tal manera que el nodo asume la distribución de cada uno de los nodos de probabilidad dependiendo del valor del nodo, Mod[Mj]. Como mencionamos anteriormente, los valores del nodo Mod[Mj] denotan los diferentes modelos del otro agente, y su distribución es la creencia del agente sobre los modelos de j condicionados al estado físico. El nivel 1 I-ID transformado es un ID tradicional que puede ser resuelto como us816 The Sixth Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) (a) (b) Figura 3: (a) Un I-DID genérico de dos niveles de tiempo para el agente i en un entorno con otro agente j. Observa el enlace de actualización del modelo punteado que denota la actualización de los modelos de j y la distribución de los modelos a lo largo del tiempo. (b) La semántica del enlace de actualización del modelo utilizando el método estándar de maximización de utilidad esperada [18]. Este procedimiento se lleva a cabo hasta el nivel l I-ID cuya solución proporciona el conjunto no vacío de acciones óptimas que el agente debe realizar dada su creencia. Ten en cuenta que, al igual que los IDs, los I-IDs son adecuados para la toma de decisiones en línea cuando se conoce la creencia actual de los agentes. 4. Diagramas de influencia dinámica interactivos (I-DIDs) Los diagramas de influencia dinámica interactivos (I-DIDs) extienden los I-IDs (y NIDs) para permitir la toma de decisiones secuencial a lo largo de varios pasos de tiempo. Así como los DIDs son representaciones gráficas estructuradas de POMDPs, los I-DIDs son los análogos gráficos en línea para I-POMDPs finitamente anidados. Los I-DIDs pueden ser utilizados para optimizar sobre una mirada anticipada finita dadas las creencias iniciales mientras se interactúa con otros agentes, posiblemente similares. 4.1 Sintaxis Representamos un I-DID de dos cortes temporales en la Fig. 3(a). Además de los nodos del modelo y el <br>enlace de política</br> discontinuo, lo que diferencia a un I-DID de un DID es el enlace de actualización del modelo mostrado como una flecha punteada en la Fig. 3(a). ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "dependency link": {
            "translated_key": "enlaces de dependencia",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Graphical Models for Online Solutions to Interactive POMDPs Prashant Doshi Dept.",
                "of Computer Science University of Georgia Athens, GA 30602, USA pdoshi@cs.uga.edu Yifeng Zeng Dept. of Computer Science Aalborg University DK-9220 Aalborg, Denmark yfzeng@cs.aau.edu Qiongyu Chen Dept. of Computer Science National Univ. of Singapore 117543, Singapore chenqy@comp.nus.edu.sg ABSTRACT We develop a new graphical representation for interactive partially observable Markov decision processes (I-POMDPs) that is significantly more transparent and semantically clear than the previous representation.",
                "These graphical models called interactive dynamic influence diagrams (I-DIDs) seek to explicitly model the structure that is often present in real-world problems by decomposing the situation into chance and decision variables, and the dependencies between the variables.",
                "I-DIDs generalize DIDs, which may be viewed as graphical representations of POMDPs, to multiagent settings in the same way that I-POMDPs generalize POMDPs.",
                "I-DIDs may be used to compute the policy of an agent online as the agent acts and observes in a setting that is populated by other interacting agents.",
                "Using several examples, we show how I-DIDs may be applied and demonstrate their usefulness.",
                "Categories and Subject Descriptors I.2.11 [Distributed Artificial Intelligence]: Multiagent Systems General Terms Theory 1.",
                "INTRODUCTION Interactive partially observable Markov decision processes (IPOMDPs) [9] provide a framework for sequential decision-making in partially observable multiagent environments.",
                "They generalize POMDPs [13] to multiagent settings by including the other agents computable models in the state space along with the states of the physical environment.",
                "The models encompass all information influencing the agents behaviors, including their preferences, capabilities, and beliefs, and are thus analogous to types in Bayesian games [11].",
                "I-POMDPs adopt a subjective approach to understanding strategic behavior, rooted in a decision-theoretic framework that takes a decision-makers perspective in the interaction.",
                "In [15], Polich and Gmytrasiewicz introduced interactive dynamic influence diagrams (I-DIDs) as the computational representations of I-POMDPs.",
                "I-DIDs generalize DIDs [12], which may be viewed as computational counterparts of POMDPs, to multiagents settings in the same way that I-POMDPs generalize POMDPs.",
                "I-DIDs contribute to a growing line of work [19] that includes multi-agent influence diagrams (MAIDs) [14], and more recently, networks of influence diagrams (NIDs) [8].",
                "These formalisms seek to explicitly model the structure that is often present in real-world problems by decomposing the situation into chance and decision variables, and the dependencies between the variables.",
                "MAIDs provide an alternative to normal and extensive game forms using a graphical formalism to represent games of imperfect information with a decision node for each agents actions and chance nodes capturing the agents private information.",
                "MAIDs objectively analyze the game, efficiently computing the Nash equilibrium profile by exploiting the independence structure.",
                "NIDs extend MAIDs to include agents uncertainty over the game being played and over models of the other agents.",
                "Each model is a MAID and the network of MAIDs is collapsed, bottom up, into a single MAID for computing the equilibrium of the game keeping in mind the different models of each agent.",
                "Graphical formalisms such as MAIDs and NIDs open up a promising area of research that aims to represent multiagent interactions more transparently.",
                "However, MAIDs provide an analysis of the game from an external viewpoint and the applicability of both is limited to static single play games.",
                "Matters are more complex when we consider interactions that are extended over time, where predictions about others future actions must be made using models that change as the agents act and observe.",
                "I-DIDs address this gap by allowing the representation of other agents models as the values of a special model node.",
                "Both, other agents models and the original agents beliefs over these models are updated over time using special-purpose implementations.",
                "In this paper, we improve on the previous preliminary representation of the I-DID shown in [15] by using the insight that the static I-ID is a type of NID.",
                "Thus, we may utilize NID-specific language constructs such as multiplexers to represent the model node, and subsequently the I-ID, more transparently.",
                "Furthermore, we clarify the semantics of the special purpose policy link introduced in the representation of I-DID by [15], and show that it could be replaced by traditional <br>dependency link</br>s.",
                "In the previous representation of the I-DID, the update of the agents belief over the models of others as the agents act and receive observations was denoted using a special link called the model update link that connected the model nodes over time.",
                "We explicate the semantics of this link by showing how it can be implemented using the traditional <br>dependency link</br>s between the chance nodes that constitute the model nodes.",
                "The net result is a representation of I-DID that is significantly more transparent, semantically clear, and capable of being implemented using the standard algorithms for solving DIDs.",
                "We show how IDIDs may be used to model an agents uncertainty over others models, that may themselves be I-DIDs.",
                "Solution to the I-DID is a policy that prescribes what the agent should do over time, given its beliefs over the physical state and others models.",
                "Analogous to DIDs, I-DIDs may be used to compute the policy of an agent online as the agent acts and observes in a setting that is populated by other interacting agents. 2.",
                "BACKGROUND: FINITELY NESTED IPOMDPS Interactive POMDPs generalize POMDPs to multiagent settings by including other agents models as part of the state space [9].",
                "Since other agents may also reason about others, the interactive state space is strategically nested; it contains beliefs about other agents models and their beliefs about others.",
                "For simplicity of presentation we consider an agent, i, that is interacting with one other agent, j.",
                "A finitely nested I-POMDP of agent i with a strategy level l is defined as the tuple: I-POMDPi,l = ISi,l, A, Ti, Ωi, Oi, Ri where: • ISi,l denotes a set of interactive states defined as, ISi,l = S × Mj,l−1, where Mj,l−1 = {Θj,l−1 ∪ SMj}, for l ≥ 1, and ISi,0 = S, where S is the set of states of the physical environment.",
                "Θj,l−1 is the set of computable intentional models of agent j: θj,l−1 = bj,l−1, ˆθj where the frame, ˆθj = A, Ωj, Tj, Oj, Rj, OCj .",
                "Here, j is Bayes rational and OCj is js optimality criterion.",
                "SMj is the set of subintentional models of j.",
                "Simple examples of subintentional models include a no-information model [10] and a fictitious play model [6], both of which are history independent.",
                "We give a recursive bottom-up construction of the interactive state space below.",
                "ISi,0 = S, Θj,0 = { bj,0, ˆθj | bj,0 ∈ Δ(ISj,0)} ISi,1 = S × {Θj,0 ∪ SMj}, Θj,1 = { bj,1, ˆθj | bj,1 ∈ Δ(ISj,1)} . . . . . .",
                "ISi,l = S × {Θj,l−1 ∪ SMj}, Θj,l = { bj,l, ˆθj | bj,l ∈ Δ(ISj,l)} Similar formulations of nested spaces have appeared in [1, 3]. • A = Ai × Aj is the set of joint actions of all agents in the environment; • Ti : S ×A×S → [0, 1], describes the effect of the joint actions on the physical states of the environment; • Ωi is the set of observations of agent i; • Oi : S × A × Ωi → [0, 1] gives the likelihood of the observations given the physical state and joint action; • Ri : ISi × A → R describes agent is preferences over its interactive states.",
                "Usually only the physical states will matter.",
                "Agent is policy is the mapping, Ω∗ i → Δ(Ai), where Ω∗ i is the set of all observation histories of agent i.",
                "Since belief over the interactive states forms a sufficient statistic [9], the policy can also be represented as a mapping from the set of all beliefs of agent i to a distribution over its actions, Δ(ISi) → Δ(Ai). 2.1 Belief Update Analogous to POMDPs, an agent within the I-POMDP framework updates its belief as it acts and observes.",
                "However, there are two differences that complicate the belief update in multiagent settings when compared to single agent ones.",
                "First, since the state of the physical environment depends on the actions of both agents, is prediction of how the physical state changes has to be made based on its prediction of js actions.",
                "Second, changes in js models have to be included in is belief update.",
                "Specifically, if j is intentional then an update of js beliefs due to its action and observation has to be included.",
                "In other words, i has to update its belief based on its prediction of what j would observe and how j would update its belief.",
                "If js model is subintentional, then js probable observations are appended to the observation history contained in the model.",
                "Formally, we have: Pr(ist |at−1 i , bt−1 i,l ) = β ISt−1:mt−1 j =θt j bt−1 i,l (ist−1 ) × at−1 j Pr(at−1 j |θt−1 j,l−1)Oi(st , at−1 i , at−1 j , ot i) ×Ti(st−1 , at−1 i , at−1 j , st ) ot j Oj(st , at−1 i , at−1 j , ot j) ×τ(SEθt j (bt−1 j,l−1, at−1 j , ot j) − bt j,l−1) (1) where β is the normalizing constant, τ is 1 if its argument is 0 otherwise it is 0, Pr(at−1 j |θt−1 j,l−1) is the probability that at−1 j is Bayes rational for the agent described by model θt−1 j,l−1, and SE(·) is an abbreviation for the belief update.",
                "For a version of the belief update when js model is subintentional, see [9].",
                "If agent j is also modeled as an I-POMDP, then is belief update invokes js belief update (via the term SEθt j ( bt−1 j,l−1 , at−1 j , ot j)), which in turn could invoke is belief update and so on.",
                "This recursion in belief nesting bottoms out at the 0th level.",
                "At this level, the belief update of the agent reduces to a POMDP belief update. 1 For illustrations of the belief update, additional details on I-POMDPs, and how they compare with other multiagent frameworks, see [9]. 2.2 Value Iteration Each belief state in a finitely nested I-POMDP has an associated value reflecting the maximum payoff the agent can expect in this belief state: Un ( bi,l, θi ) = max ai∈Ai is∈ISi,l ERi(is, ai)bi,l(is)+ γ oi∈Ωi Pr(oi|ai, bi,l)Un−1 ( SEθi (bi,l, ai, oi), θi ) (2) where, ERi(is, ai) = aj Ri(is, ai, aj)Pr(aj|mj,l−1) (since is = (s, mj,l−1)).",
                "Eq. 2 is a basis for value iteration in I-POMDPs.",
                "Agent is optimal action, a∗ i , for the case of finite horizon with discounting, is an element of the set of optimal actions for the belief state, OPT(θi), defined as: OPT( bi,l, θi ) = argmax ai∈Ai is∈ISi,l ERi(is, ai)bi,l(is) +γ oi∈Ωi Pr(oi|ai, bi,l)Un ( SEθi (bi,l, ai, oi), θi ) (3) 3.",
                "INTERACTIVEINFLUENCEDIAGRAMS A naive extension of influence diagrams (IDs) to settings populated by multiple agents is possible by treating other agents as automatons, represented using chance nodes.",
                "However, this approach assumes that the agents actions are controlled using a probability distribution that does not change over time.",
                "Interactive influence diagrams (I-IDs) adopt a more sophisticated approach by generalizing IDs to make them applicable to settings shared with other agents who may act and observe, and update their beliefs. 3.1 Syntax In addition to the usual chance, decision, and utility nodes, IIDs include a new type of node called the model node.",
                "We show a general level l I-ID in Fig. 1(a), where the model node (Mj,l−1) is denoted using a hexagon.",
                "We note that the probability distribution over the chance node, S, and the model node together represents agent is belief over its interactive states.",
                "In addition to the model 1 The 0th level model is a POMDP: Other agents actions are treated as exogenous events and folded into the T, O, and R functions.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 815 Figure 1: (a) A generic level l I-ID for agent i situated with one other agent j.",
                "The hexagon is the model node (Mj,l−1) whose structure we show in (b).",
                "Members of the model node are I-IDs themselves (m1 j,l−1, m2 j,l−1; diagrams not shown here for simplicity) whose decision nodes are mapped to the corresponding chance nodes (A1 j , A2 j ).",
                "Depending on the value of the node, Mod[Mj], the distribution of each of the chance nodes is assigned to the node Aj. (c) The transformed I-ID with the model node replaced by the chance nodes and the relationships between them. node, I-IDs differ from IDs by having a dashed link (called the policy link in [15]) between the model node and a chance node, Aj, that represents the distribution over the other agents actions given its model.",
                "In the absence of other agents, the model node and the chance node, Aj, vanish and I-IDs collapse into traditional IDs.",
                "The model node contains the alternative computational models ascribed by i to the other agent from the set, Θj,l−1 ∪ SMj, where Θj,l−1 and SMj were defined previously in Section 2.",
                "Thus, a model in the model node may itself be an I-ID or ID, and the recursion terminates when a model is an ID or subintentional.",
                "Because the model node contains the alternative models of the other agent as its values, its representation is not trivial.",
                "In particular, some of the models within the node are I-IDs that when solved generate the agents optimal policy in their decision nodes.",
                "Each decision node is mapped to the corresponding chance node, say A1 j , in the following way: if OPT is the set of optimal actions obtained by solving the I-ID (or ID), then Pr(aj ∈ A1 j ) = 1 |OP T | if aj ∈ OPT, 0 otherwise.",
                "Borrowing insights from previous work [8], we observe that the model node and the dashed policy link that connects it to the chance node, Aj, could be represented as shown in Fig. 1(b).",
                "The decision node of each level l − 1 I-ID is transformed into a chance node, as we mentioned previously, so that the actions with the largest value in the decision node are assigned uniform probabilities in the chance node while the rest are assigned zero probability.",
                "The different chance nodes (A1 j , A2 j ), one for each model, and additionally, the chance node labeled Mod[Mj] form the parents of the chance node, Aj.",
                "Thus, there are as many action nodes (A1 j , A2 j ) in Mj,l−1 as the number of models in the support of agent is beliefs.",
                "The conditional probability table of the chance node, Aj, is a multiplexer that assumes the distribution of each of the action nodes (A1 j , A2 j ) depending on the value of Mod[Mj].",
                "The values of Mod[Mj] denote the different models of j.",
                "In other words, when Mod[Mj] has the value m1 j,l−1, the chance node Aj assumes the distribution of the node A1 j , and Aj assumes the distribution of A2 j when Mod[Mj] has the value m2 j,l−1.",
                "The distribution over the node, Mod[Mj], is the agent is belief over the models of j given a physical state.",
                "For more agents, we will have as many model nodes as there are agents.",
                "Notice that Fig. 1(b) clarifies the semantics of the policy link, and shows how it can be represented using the traditional <br>dependency link</br>s.",
                "In Fig. 1(c), we show the transformed I-ID when the model node is replaced by the chance nodes and relationships between them.",
                "In contrast to the representation in [15], there are no special-purpose policy links, rather the I-ID is composed of only those types of nodes that are found in traditional IDs and dependency relationships between the nodes.",
                "This allows I-IDs to be represented and implemented using conventional application tools that target IDs.",
                "Note that we may view the level l I-ID as a NID.",
                "Specifically, each of the level l − 1 models within the model node are blocks in the NID (see Fig. 2).",
                "If the level l = 1, each block is a traditional ID, otherwise if l > 1, each block within the NID may itself be a NID.",
                "Note that within the I-IDs (or IDs) at each level, there is only a single decision node.",
                "Thus, our NID does not contain any MAIDs.",
                "Figure 2: A level l I-ID represented as a NID.",
                "The probabilities assigned to the blocks of the NID are is beliefs over js models conditioned on a physical state. 3.2 Solution The solution of an I-ID proceeds in a bottom-up manner, and is implemented recursively.",
                "We start by solving the level 0 models, which, if intentional, are traditional IDs.",
                "Their solutions provide probability distributions over the other agents actions, which are entered in the corresponding chance nodes found in the model node of the level 1 I-ID.",
                "The mapping from the level 0 models decision nodes to the chance nodes is carried out so that actions with the largest value in the decision node are assigned uniform probabilities in the chance node while the rest are assigned zero probability.",
                "Given the distributions over the actions within the different chance nodes (one for each model of the other agent), the level 1 I-ID is transformed as shown in Fig. 1(c).",
                "During the transformation, the conditional probability table (CPT) of the node, Aj, is populated such that the node assumes the distribution of each of the chance nodes depending on the value of the node, Mod[Mj].",
                "As we mentioned previously, the values of the node Mod[Mj] denote the different models of the other agent, and its distribution is the agent is belief over the models of j conditioned on the physical state.",
                "The transformed level 1 I-ID is a traditional ID that may be solved us816 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) (a) (b) Figure 3: (a) A generic two time-slice level l I-DID for agent i in a setting with one other agent j.",
                "Notice the dotted model update link that denotes the update of the models of j and the distribution over the models over time. (b) The semantics of the model update link. ing the standard expected utility maximization method [18].",
                "This procedure is carried out up to the level l I-ID whose solution gives the non-empty set of optimal actions that the agent should perform given its belief.",
                "Notice that analogous to IDs, I-IDs are suitable for online decision-making when the agents current belief is known. 4.",
                "INTERACTIVE DYNAMIC INFLUENCE DIAGRAMS Interactive dynamic influence diagrams (I-DIDs) extend I-IDs (and NIDs) to allow sequential decision-making over several time steps.",
                "Just as DIDs are structured graphical representations of POMDPs, I-DIDs are the graphical online analogs for finitely nested I-POMDPs.",
                "I-DIDs may be used to optimize over a finite look-ahead given initial beliefs while interacting with other, possibly similar, agents. 4.1 Syntax We depict a general two time-slice I-DID in Fig. 3(a).",
                "In addition to the model nodes and the dashed policy link, what differentiates an I-DID from a DID is the model update link shown as a dotted arrow in Fig. 3(a).",
                "We explained the semantics of the model node and the policy link in the previous section; we describe the model updates next.",
                "The update of the model node over time involves two steps: First, given the models at time t, we identify the updated set of models that reside in the model node at time t + 1.",
                "Recall from Section 2 that an agents intentional model includes its belief.",
                "Because the agents act and receive observations, their models are updated to reflect their changed beliefs.",
                "Since the set of optimal actions for a model could include all the actions, and the agent may receive any one of |Ωj| possible observations, the updated set at time step t + 1 will have at most |Mt j,l−1||Aj||Ωj| models.",
                "Here, |Mt j,l−1| is the number of models at time step t, |Aj| and |Ωj| are the largest spaces of actions and observations respectively, among all the models.",
                "Second, we compute the new distribution over the updated models given the original distribution and the probability of the agent performing the action and receiving the observation that led to the updated model.",
                "These steps are a part of agent is belief update formalized using Eq. 1.",
                "In Fig. 3(b), we show how the dotted model update link is implemented in the I-DID.",
                "If each of the two level l − 1 models ascribed to j at time step t results in one action, and j could make one of two possible observations, then the model node at time step t + 1 contains four updated models (mt+1,1 j,l−1 ,mt+1,2 j,l−1 , mt+1,3 j,l−1 , and mt+1,4 j,l−1 ).",
                "These models differ in their initial beliefs, each of which is the result of j updating its beliefs due to its action and a possible observation.",
                "The decision nodes in each of the I-DIDs or DIDs that represent the lower level models are mapped to the corresponding Figure 4: Transformed I-DID with the model nodes and model update link replaced with the chance nodes and the relationships (in bold). chance nodes, as mentioned previously.",
                "Next, we describe how the distribution over the updated set of models (the distribution over the chance node Mod[Mt+1 j ] in Mt+1 j,l−1) is computed.",
                "The probability that js updated model is, say mt+1,1 j,l−1 , depends on the probability of j performing the action and receiving the observation that led to this model, and the prior distribution over the models at time step t. Because the chance node At j assumes the distribution of each of the action nodes based on the value of Mod[Mt j ], the probability of the action is given by this chance node.",
                "In order to obtain the probability of js possible observation, we introduce the chance node Oj, which depending on the value of Mod[Mt j ] assumes the distribution of the observation node in the lower level model denoted by Mod[Mt j ].",
                "Because the probability of js observations depends on the physical state and the joint actions of both agents, the node Oj is linked with St+1 , At j, and At i. 2 Analogous to At j, the conditional probability table of Oj is also a multiplexer modulated by Mod[Mt j ].",
                "Finally, the distribution over the prior models at time t is obtained from the chance node, Mod[Mt j ] in Mt j,l−1.",
                "Consequently, the chance nodes, Mod[Mt j ], At j, and Oj, form the parents of Mod[Mt+1 j ] in Mt+1 j,l−1.",
                "Notice that the model update link may be replaced by the <br>dependency link</br>s between the chance nodes that constitute the model nodes in the two time slices.",
                "In Fig. 4 we show the two time-slice I-DID with the model nodes replaced by the chance nodes and the relationships between them.",
                "Chance nodes and <br>dependency link</br>s that not in bold are standard, usually found in DIDs.",
                "Expansion of the I-DID over more time steps requires the repetition of the two steps of updating the set of models that form the 2 Note that Oj represents js observation at time t + 1.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 817 values of the model node and adding the relationships between the chance nodes, as many times as there are model update links.",
                "We note that the possible set of models of the other agent j grows exponentially with the number of time steps.",
                "For example, after T steps, there may be at most |Mt=1 j,l−1|(|Aj||Ωj|)T −1 candidate models residing in the model node. 4.2 Solution Analogous to I-IDs, the solution to a level l I-DID for agent i expanded over T time steps may be carried out recursively.",
                "For the purpose of illustration, let l=1 and T=2.",
                "The solution method uses the standard look-ahead technique, projecting the agents action and observation sequences forward from the current belief state [17], and finding the possible beliefs that i could have in the next time step.",
                "Because agent i has a belief over js models as well, the lookahead includes finding out the possible models that j could have in the future.",
                "Consequently, each of js subintentional or level 0 models (represented using a standard DID) in the first time step must be solved to obtain its optimal set of actions.",
                "These actions are combined with the set of possible observations that j could make in that model, resulting in an updated set of candidate models (that include the updated beliefs) that could describe the behavior of j. Beliefs over this updated set of candidate models are calculated using the standard inference methods using the dependency relationships between the model nodes as shown in Fig. 3(b).",
                "We note the recursive nature of this solution: in solving agent is level 1 I-DID, js level 0 DIDs must be solved.",
                "If the nesting of models is deeper, all models at all levels starting from 0 are solved in a bottom-up manner.",
                "We briefly outline the recursive algorithm for solving agent is Algorithm for solving I-DID Input : level l ≥ 1 I-ID or level 0 ID, T Expansion Phase 1.",
                "For t from 1 to T − 1 do 2.",
                "If l ≥ 1 then Populate Mt+1 j,l−1 3.",
                "For each mt j in Range(Mt j,l−1) do 4.",
                "Recursively call algorithm with the l − 1 I-ID (or ID) that represents mt j and the horizon, T − t + 1 5.",
                "Map the decision node of the solved I-ID (or ID), OPT(mt j), to a chance node Aj 6.",
                "For each aj in OPT(mt j) do 7.",
                "For each oj in Oj (part of mt j) do 8.",
                "Update js belief, bt+1 j ← SE(bt j, aj, oj) 9. mt+1 j ← New I-ID (or ID) with bt+1 j as the initial belief 10.",
                "Range(Mt+1 j,l−1) ∪ ← {mt+1 j } 11.",
                "Add the model node, Mt+1 j,l−1, and the <br>dependency link</br>s between Mt j,l−1 and Mt+1 j,l−1 (shown in Fig. 3(b)) 12.",
                "Add the chance, decision, and utility nodes for t + 1 time slice and the <br>dependency link</br>s between them 13.",
                "Establish the CPTs for each chance node and utility node Look-Ahead Phase 14.",
                "Apply the standard look-ahead and backup method to solve the expanded I-DID Figure 5: Algorithm for solving a level l ≥ 0 I-DID. level l I-DID expanded over T time steps with one other agent j in Fig. 5.",
                "We adopt a two-phase approach: Given an I-ID of level l (described previously in Section 3) with all lower level models also represented as I-IDs or IDs (if level 0), the first step is to expand the level l I-ID over T time steps adding the <br>dependency link</br>s and the conditional probability tables for each node.",
                "We particularly focus on establishing and populating the model nodes (lines 3-11).",
                "Note that Range(·) returns the values (lower level models) of the random variable given as input (model node).",
                "In the second phase, we use a standard look-ahead technique projecting the action and observation sequences over T time steps in the future, and backing up the utility values of the reachable beliefs.",
                "Similar to I-IDs, the I-DIDs reduce to DIDs in the absence of other agents.",
                "As we mentioned previously, the 0-th level models are the traditional DIDs.",
                "Their solutions provide probability distributions over actions of the agent modeled at that level to I-DIDs at level 1.",
                "Given probability distributions over other agents actions the level 1 IDIDs can themselves be solved as DIDs, and provide probability distributions to yet higher level models.",
                "Assume that the number of models considered at each level is bound by a number, M. Solving an I-DID of level l in then equivalent to solving O(Ml ) DIDs. 5.",
                "EXAMPLE APPLICATIONS To illustrate the usefulness of I-DIDs, we apply them to three problem domains.",
                "We describe, in particular, the formulation of the I-DID and the optimal prescriptions obtained on solving it. 5.1 Followership-Leadership in the Multiagent Tiger Problem We begin our illustrations of using I-IDs and I-DIDs with a slightly modified version of the multiagent tiger problem discussed in [9].",
                "The problem has two agents, each of which can open the right door (OR), the left door (OL) or listen (L).",
                "In addition to hearing growls (from the left (GL) or from the right (GR)) when they listen, the agents also hear creaks (from the left (CL), from the right (CR), or no creaks (S)), which noisily indicate the other agents opening one of the doors.",
                "When any door is opened, the tiger persists in its original location with a probability of 95%.",
                "Agent i hears growls with a reliability of 65% and creaks with a reliability of 95%.",
                "Agent j, on the other hand, hears growls with a reliability of 95%.",
                "Thus, the setting is such that agent i hears agent j opening doors more reliably than the tigers growls.",
                "This suggests that i could use js actions as an indication of the location of the tiger, as we discuss below.",
                "Each agents preferences are as in the single agent game discussed in [13].",
                "The transition, observation, and reward functions are shown in [16].",
                "A good indicator of the usefulness of normative methods for decision-making like I-DIDs is the emergence of realistic social behaviors in their prescriptions.",
                "In settings of the persistent multiagent tiger problem that reflect real world situations, we demonstrate followership between the agents and, as shown in [15], deception among agents who believe that they are in a follower-leader type of relationship.",
                "In particular, we analyze the situational and epistemological conditions sufficient for their emergence.",
                "The followership behavior, for example, results from the agent knowing its own weaknesses, assessing the strengths, preferences, and possible behaviors of the other, and realizing that its best for it to follow the others actions in order to maximize its payoffs.",
                "Let us consider a particular setting of the tiger problem in which agent i believes that js preferences are aligned with its own - both of them just want to get the gold - and js hearing is more reliable in comparison to itself.",
                "As an example, suppose that j, on listening can discern the tigers location 95% of the times compared to is 65% accuracy.",
                "Additionally, agent i does not have any initial information about the tigers location.",
                "In other words, is single-level nested belief, bi,1, assigns 0.5 to each of the two locations of the tiger.",
                "In addition, i considers two models of j, which differ in js flat level 0 initial beliefs.",
                "This is represented in the level 1 I-ID shown in Fig. 6(a).",
                "According to one model, j assigns a probability of 0.9 that the tiger is behind the left door, while the other 818 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Figure 6: (a) Level 1 I-ID of agent i, (b) two level 0 IDs of agent j whose decision nodes are mapped to the chance nodes, A1 j , A2 j , in (a). model assigns 0.1 to that location (see Fig. 6(b)).",
                "Agent i is undecided on these two models of j.",
                "If we vary is hearing ability, and solve the corresponding level 1 I-ID expanded over three time steps, we obtain the normative behavioral policies shown in Fig 7 that exhibit followership behavior.",
                "If is probability of correctly hearing the growls is 0.65, then as shown in the policy in Fig. 7(a), i begins to conditionally follow js actions: i opens the same door that j opened previously iff is own assessment of the tigers location confirms js pick.",
                "If i loses the ability to correctly interpret the growls completely, it blindly follows j and opens the same door that j opened previously (Fig. 7(b)).",
                "Figure 7: Emergence of (a) conditional followership, and (b) blind followership in the tiger problem.",
                "Behaviors of interest are in bold. * is a wildcard, and denotes any one of the observations.",
                "We observed that a single level of belief nesting - beliefs about the others models - was sufficient for followership to emerge in the tiger problem.",
                "However, the epistemological requirements for the emergence of leadership are more complex.",
                "For an agent, say j, to emerge as a leader, followership must first emerge in the other agent i.",
                "As we mentioned previously, if i is certain that its preferences are identical to those of j, and believes that j has a better sense of hearing, i will follow js actions over time.",
                "Agent j emerges as a leader if it believes that i will follow it, which implies that js belief must be nested two levels deep to enable it to recognize its leadership role.",
                "Realizing that i will follow presents j with an opportunity to influence is actions in the benefit of the collective good or its self-interest alone.",
                "For example, in the tiger problem, let us consider a setting in which if both i and j open the correct door, then each gets a payoff of 20 that is double the original.",
                "If j alone selects the correct door, it gets the payoff of 10.",
                "On the other hand, if both agents pick the wrong door, their penalties are cut in half.",
                "In this setting, it is in both js best interest as well as the collective betterment for j to use its expertise in selecting the correct door, and thus be a good leader.",
                "However, consider a slightly different problem in which j gains from is loss and is penalized if i gains.",
                "Specifically, let is payoff be subtracted from js, indicating that j is antagonistic toward i - if j picks the correct door and i the wrong one, then is loss of 100 becomes js gain.",
                "Agent j believes that i incorrectly thinks that js preferences are those that promote the collective good and that it starts off by believing with 99% confidence where the tiger is.",
                "Because i believes that its preferences are similar to those of j, and that j starts by believing almost surely that one of the two is the correct location (two level 0 models of j), i will start by following js actions.",
                "We show is normative policy on solving its singly-nested I-DID over three time steps in Fig. 8(a).",
                "The policy demonstrates that i will blindly follow js actions.",
                "Since the tiger persists in its original location with a probability of 0.95, i will select the same door again.",
                "If j begins the game with a 99% probability that the tiger is on the right, solving js I-DID nested two levels deep, results in the policy shown in Fig. 8(b).",
                "Even though j is almost certain that OL is the correct action, it will start by selecting OR, followed by OL.",
                "Agent js intention is to deceive i who, it believes, will follow js actions, so as to gain $110 in the second time step, which is more than what j would gain if it were to be honest.",
                "Figure 8: Emergence of deception between agents in the tiger problem.",
                "Behaviors of interest are in bold. * denotes as before. (a) Agent is policy demonstrating that it will blindly follow js actions. (b) Even though j is almost certain that the tiger is on the right, it will start by selecting OR, followed by OL, in order to deceive i. 5.2 Altruism and Reciprocity in the Public Good Problem The public good (PG) problem [7], consists of a group of M agents, each of whom must either contribute some resource to a public pot or keep it for themselves.",
                "Since resources contributed to the public pot are shared among all the agents, they are less valuable to the agent when in the public pot.",
                "However, if all agents choose to contribute their resources, then the payoff to each agent is more than if no one contributes.",
                "Since an agent gets its share of the public pot irrespective of whether it has contributed or not, the dominating action is for each agent to not contribute, and instead free ride on others contributions.",
                "However, behaviors of human players in empirical simulations of the PG problem differ from the normative predictions.",
                "The experiments reveal that many players initially contribute a large amount to the public pot, and continue to contribute when the PG problem is played repeatedly, though in decreasing amounts [4].",
                "Many of these experiments [5] report that a small core group of players persistently contributes to the public pot even when all others are defecting.",
                "These experiments also reveal that players who persistently contribute have altruistic or reciprocal preferences matching expected cooperation of others.",
                "For simplicity, we assume that the game is played between M = 2 agents, i and j.",
                "Let each agent be initially endowed with XT amount of resources.",
                "While the classical PG game formulation permits each agent to contribute any quantity of resources (≤ XT ) to the public pot, we simplify the action space by allowing two possible actions.",
                "Each agent may choose to either contribute (C) a fixed amount of the resources, or not contribute.",
                "The latter action is deThe Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 819 noted as defect (D).",
                "We assume that the actions are not observable to others.",
                "The value of resources in the public pot is discounted by ci for each agent i, where ci is the marginal private return.",
                "We assume that ci < 1 so that the agent does not benefit enough that it contributes to the public pot for private gain.",
                "Simultaneously, ciM > 1, making collective contribution pareto optimal. i/j C D C 2ciXT , 2cjXT ciXT − cp, XT + cjXT − P D XT + ciXT − P, cjXT − cp XT , XT Table 1: The one-shot PG game with punishment.",
                "In order to encourage contributions, the contributing agents punish free riders but incur a small cost for administering the punishment.",
                "Let P be the punishment meted out to the defecting agent and cp the non-zero cost of punishing for the contributing agent.",
                "For simplicity, we assume that the cost of punishing is same for both the agents.",
                "The one-shot PG game with punishment is shown in Table. 1.",
                "Let ci = cj, cp > 0, and if P > XT − ciXT , then defection is no longer a dominating action.",
                "If P < XT − ciXT , then defection is the dominating action for both.",
                "If P = XT − ciXT , then the game is not dominance-solvable.",
                "Figure 9: (a) Level 1 I-ID of agent i, (b) level 0 IDs of agent j with decision nodes mapped to the chance nodes, A1 j and A2 j , in (a).",
                "We formulate a sequential version of the PG problem with punishment from the perspective of agent i.",
                "Though in the repeated PG game, the quantity in the public pot is revealed to all the agents after each round of actions, we assume in our formulation that it is hidden from the agents.",
                "Each agent may contribute a fixed amount, xc, or defect.",
                "An agent on performing an action receives an observation of plenty (PY) or meager (MR) symbolizing the state of the public pot.",
                "Notice that the observations are also indirectly indicative of agent js actions because the state of the public pot is influenced by them.",
                "The amount of resources in agent is private pot, is perfectly observable to i.",
                "The payoffs are analogous to Table. 1.",
                "Borrowing from the empirical investigations of the PG problem [5], we construct level 0 IDs for j that model altruistic and non-altruistic types (Fig. 9(b)).",
                "Specifically, our altruistic agent has a high marginal private return (cj is close to 1) and does not punish others who defect.",
                "Let xc = 1 and the level 0 agent be punished half the times it defects.",
                "With one action remaining, both types of agents choose to contribute to avoid being punished.",
                "With two actions to go, the altruistic type chooses to contribute, while the other defects.",
                "This is because cj for the altruistic type is close to 1, thus the expected punishment, 0.5P > (1 − cj), which the altruistic type avoids.",
                "Because cj for the non-altruistic type is less, it prefers not to contribute.",
                "With three steps to go, the altruistic agent contributes to avoid punishment (0.5P > 2(1 − cj)), and the non-altruistic type defects.",
                "For greater than three steps, while the altruistic agent continues to contribute to the public pot depending on how close its marginal private return is to 1, the non-altruistic type prescribes defection.",
                "We analyzed the decisions of an altruistic agent i modeled using a level 1 I-DID expanded over 3 time steps. i ascribes the two level 0 models, mentioned previously, to j (see Fig. 9).",
                "If i believes with a probability 1 that j is altruistic, i chooses to contribute for each of the three steps.",
                "This behavior persists when i is unaware of whether j is altruistic (Fig. 10(a)), and when i assigns a high probability to j being the non-altruistic type.",
                "However, when i believes with a probability 1 that j is non-altruistic and will thus surely defect, i chooses to defect to avoid being punished and because its marginal private return is less than 1.",
                "These results demonstrate that the behavior of our altruistic type resembles that found experimentally.",
                "The non-altruistic level 1 agent chooses to defect regardless of how likely it believes the other agent to be altruistic.",
                "We analyzed the behavior of a reciprocal agent type that matches expected cooperation or defection.",
                "The reciprocal types marginal private return is similar to that of the non-altruistic type, however, it obtains a greater payoff when its action is similar to that of the other.",
                "We consider the case when the reciprocal agent i is unsure of whether j is altruistic and believes that the public pot is likely to be half full.",
                "For this prior belief, i chooses to defect.",
                "On receiving an observation of plenty, i decides to contribute, while an observation of meager makes it defect (Fig. 10(b)).",
                "This is because an observation of plenty signals that the pot is likely to be greater than half full, which results from js action to contribute.",
                "Thus, among the two models ascribed to j, its type is likely to be altruistic making it likely that j will contribute again in the next time step.",
                "Agent i therefore chooses to contribute to reciprocate js action.",
                "An analogous reasoning leads i to defect when it observes a meager pot.",
                "With one action to go, i believing that j contributes, will choose to contribute too to avoid punishment regardless of its observations.",
                "Figure 10: (a) An altruistic level 1 agent always contributes. (b) A reciprocal agent i starts off by defecting followed by choosing to contribute or defect based on its observation of plenty (indicating that j is likely altruistic) or meager (j is non-altruistic). 5.3 Strategies in Two-Player Poker Poker is a popular zero sum card game that has received much attention among the AI research community as a testbed [2].",
                "Poker is played among M ≥ 2 players in which each player receives a hand of cards from a deck.",
                "While several flavors of Poker with varying complexity exist, we consider a simple version in which each player has three plys during which the player may either exchange a card (E), keep the existing hand (K), fold (F) and withdraw from the game, or call (C), requiring all players to show their hands.",
                "To keep matters simple, let M = 2, and each player receive a hand consisting of a single card drawn from the same suit.",
                "Thus, during a showdown, the player who has the numerically larger card (2 is the lowest, ace is the highest) wins the pot.",
                "During an exchange of cards, the discarded card is placed either in the L pile, indicating to the other agent that it was a low numbered card less than 8, or in the 820 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) H pile, indicating that the card had a rank greater than or equal to 8.",
                "Notice that, for example, if a lower numbered card is discarded, the probability of receiving a low card in exchange is now reduced.",
                "We show the level 1 I-ID for the simplified two-player Poker in Fig. 11.",
                "We considered two models (personality types) of agent j.",
                "The conservative type believes that it is likely that its opponent has a high numbered card in its hand.",
                "On the other hand, the aggressive agent j believes with a high probability that its opponent has a lower numbered card.",
                "Thus, the two types differ in their beliefs over their opponents hand.",
                "In both these level 0 models, the opponent is assumed to perform its actions following a fixed, uniform distribution.",
                "With three actions to go, regardless of its hand (unless it is an ace), the aggressive agent chooses to exchange its card, with the intent of improving on its current hand.",
                "This is because it believes the other to have a low card, which improves its chances of getting a high card during the exchange.",
                "The conservative agent chooses to keep its card, no matter its hand because its chances of getting a high card are slim as it believes that its opponent has one.",
                "Figure 11: (a) Level 1 I-ID of agent i.",
                "The observation reveals information about js hand of the previous time step, (b) level 0 IDs of agent j whose decision nodes are mapped to the chance nodes, A1 j , A2 j , in (a).",
                "The policy of a level 1 agent i who believes that each card except its own has an equal likelihood of being in js hand (neutral personality type) and j could be either an aggressive or conservative type, is shown in Fig. 12. is own hand contains the card numbered 8.",
                "The agent starts by keeping its card.",
                "On seeing that j did not exchange a card (N), i believes with probability 1 that j is conservative and hence will keep its cards. i responds by either keeping its card or exchanging it because j is equally likely to have a lower or higher card.",
                "If i observes that j discarded its card into the L or H pile, i believes that j is aggressive.",
                "On observing L, i realizes that j had a low card, and is likely to have a high card after its exchange.",
                "Because the probability of receiving a low card is high now, i chooses to keep its card.",
                "On observing H, believing that the probability of receiving a high numbered card is high, i chooses to exchange its card.",
                "In the final step, i chooses to call regardless of its observation history because its belief that j has a higher card is not sufficiently high to conclude that its better to fold and relinquish the payoff.",
                "This is partly due to the fact that an observation of, say, L resets the agent is previous time step beliefs over js hand to the low numbered cards only. 6.",
                "DISCUSSION We showed how DIDs may be extended to I-DIDs that enable online sequential decision-making in uncertain multiagent settings.",
                "Our graphical representation of I-DIDs improves on the previous Figure 12: A level 1 agent is three step policy in the Poker problem. i starts by believing that j is equally likely to be aggressive or conservative and could have any card in its hand with equal probability. work significantly by being more transparent, semantically clear, and capable of being solved using standard algorithms that target DIDs.",
                "I-DIDs extend NIDs to allow sequential decision-making over multiple time steps in the presence of other interacting agents.",
                "I-DIDs may be seen as concise graphical representations for IPOMDPs providing a way to exploit problem structure and carry out online decision-making as the agent acts and observes given its prior beliefs.",
                "We are currently investigating ways to solve I-DIDs approximately with provable bounds on the solution quality.",
                "Acknowledgment: We thank Piotr Gmytrasiewicz for some useful discussions related to this work.",
                "The first author would like to acknowledge the support of a UGARF grant. 7.",
                "REFERENCES [1] R. J. Aumann.",
                "Interactive epistemology i: Knowledge.",
                "International Journal of Game Theory, 28:263-300, 1999. [2] D. Billings, A. Davidson, J. Schaeffer, and D. Szafron.",
                "The challenge of poker.",
                "AIJ, 2001. [3] A. Brandenburger and E. Dekel.",
                "Hierarchies of beliefs and common knowledge.",
                "Journal of Economic Theory, 59:189-198, 1993. [4] C. Camerer.",
                "Behavioral Game Theory: Experiments in Strategic Interaction.",
                "Princeton University Press, 2003. [5] E. Fehr and S. Gachter.",
                "Cooperation and punishment in public goods experiments.",
                "American Economic Review, 90(4):980-994, 2000. [6] D. Fudenberg and D. K. Levine.",
                "The Theory of Learning in Games.",
                "MIT Press, 1998. [7] D. Fudenberg and J. Tirole.",
                "Game Theory.",
                "MIT Press, 1991. [8] Y. Gal and A. Pfeffer.",
                "A language for modeling agents decision-making processes in games.",
                "In AAMAS, 2003. [9] P. Gmytrasiewicz and P. Doshi.",
                "A framework for sequential planning in multiagent settings.",
                "JAIR, 24:49-79, 2005. [10] P. Gmytrasiewicz and E. Durfee.",
                "Rational coordination in multi-agent environments.",
                "JAAMAS, 3(4):319-350, 2000. [11] J. C. Harsanyi.",
                "Games with incomplete information played by bayesian players.",
                "Management Science, 14(3):159-182, 1967. [12] R. A. Howard and J. E. Matheson.",
                "Influence diagrams.",
                "In R. A. Howard and J. E. Matheson, editors, The Principles and Applications of Decision Analysis.",
                "Strategic Decisions Group, Menlo Park, CA 94025, 1984. [13] L. Kaelbling, M. Littman, and A. Cassandra.",
                "Planning and acting in partially observable stochastic domains.",
                "Artificial Intelligence Journal, 2, 1998. [14] D. Koller and B. Milch.",
                "Multi-agent influence diagrams for representing and solving games.",
                "In IJCAI, pages 1027-1034, 2001. [15] K. Polich and P. Gmytrasiewicz.",
                "Interactive dynamic influence diagrams.",
                "In GTDT Workshop, AAMAS, 2006. [16] B.",
                "Rathnas., P. Doshi, and P. J. Gmytrasiewicz.",
                "Exact solutions to interactive pomdps using behavioral equivalence.",
                "In Autonomous Agents and Multi-Agent Systems Conference (AAMAS), 2006. [17] S. Russell and P. Norvig.",
                "Artificial Intelligence: A Modern Approach (Second Edition).",
                "Prentice Hall, 2003. [18] R. D. Shachter.",
                "Evaluating influence diagrams.",
                "Operations Research, 34(6):871-882, 1986. [19] D. Suryadi and P. Gmytrasiewicz.",
                "Learning models of other agents using influence diagrams.",
                "In UM, 1999.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 821"
            ],
            "original_annotated_samples": [
                "Furthermore, we clarify the semantics of the special purpose policy link introduced in the representation of I-DID by [15], and show that it could be replaced by traditional <br>dependency link</br>s.",
                "We explicate the semantics of this link by showing how it can be implemented using the traditional <br>dependency link</br>s between the chance nodes that constitute the model nodes.",
                "Notice that Fig. 1(b) clarifies the semantics of the policy link, and shows how it can be represented using the traditional <br>dependency link</br>s.",
                "Notice that the model update link may be replaced by the <br>dependency link</br>s between the chance nodes that constitute the model nodes in the two time slices.",
                "Chance nodes and <br>dependency link</br>s that not in bold are standard, usually found in DIDs."
            ],
            "translated_annotated_samples": [
                "Además, aclaramos la semántica del enlace de política de propósito especial introducido en la representación de I-DID por [15], y mostramos que podría ser reemplazado por <br>enlaces de dependencia</br> tradicionales.",
                "Explicamos la semántica de este enlace mostrando cómo puede ser implementado utilizando los <br>enlaces de dependencia</br> tradicionales entre los nodos de probabilidad que constituyen los nodos del modelo.",
                "Observa que la Fig. 1(b) aclara la semántica del enlace de política y muestra cómo puede ser representado utilizando los <br>enlaces de dependencia</br> tradicionales.",
                "Ten en cuenta que el enlace de actualización del modelo puede ser reemplazado por los <br>enlaces de dependencia</br> entre los nodos de probabilidad que constituyen los nodos del modelo en las dos etapas temporales.",
                "Los nodos de probabilidad y los <br>enlaces de dependencia</br> que no están en negrita son estándar, generalmente encontrados en DIDs."
            ],
            "translated_text": "Modelos gráficos para soluciones en línea a POMDP interactivos de Prashant Doshi Dept. Departamento de Ciencias de la Computación Universidad de Georgia Athens, GA 30602, EE. UU. pdoshi@cs.uga.edu Yifeng Zeng Dept. de Ciencias de la Computación Universidad de Aalborg DK-9220 Aalborg, Dinamarca yfzeng@cs.aau.edu Qiongyu Chen Dept. de Ciencias de la Computación Universidad Nacional de Singapur 117543, Singapur chenqy@comp.nus.edu.sg RESUMEN Desarrollamos una nueva representación gráfica para procesos de decisión de Markov parcialmente observables interactivos (I-POMDPs) que es significativamente más transparente y semánticamente clara que la representación anterior. Estos modelos gráficos llamados diagramas de influencia dinámica interactiva (I-DIDs) buscan modelar explícitamente la estructura que a menudo está presente en problemas del mundo real al descomponer la situación en variables de probabilidad y decisión, y las dependencias entre las variables. Los I-DIDs generalizan los DIDs, los cuales pueden ser vistos como representaciones gráficas de POMDPs, a entornos multiagentes de la misma manera que los I-POMDPs generalizan los POMDPs. Los I-DIDs pueden ser utilizados para calcular la política de un agente en línea mientras el agente actúa y observa en un entorno poblado por otros agentes que interactúan entre sí. Usando varios ejemplos, mostramos cómo se pueden aplicar los I-DIDs y demostramos su utilidad. Categorías y Descriptores de Asignaturas I.2.11 [Inteligencia Artificial Distribuida]: Sistemas Multiagentes Términos Generales Teoría 1. Las Procesos de Decisión de Markov Interactivos Parcialmente Observables (IPOMDPs) proporcionan un marco para la toma de decisiones secuenciales en entornos multiagentes parcialmente observables. Generalizan los POMDPs [13] a entornos multiagentes al incluir los modelos computables de los otros agentes en el espacio de estados junto con los estados del entorno físico. Los modelos abarcan toda la información que influye en los comportamientos de los agentes, incluyendo sus preferencias, capacidades y creencias, y por lo tanto son análogos a los tipos en los juegos bayesianos [11]. I-POMDPs adoptan un enfoque subjetivo para comprender el comportamiento estratégico, arraigado en un marco de teoría de decisiones que toma la perspectiva de los tomadores de decisiones en la interacción. En [15], Polich y Gmytrasiewicz introdujeron diagramas de influencia dinámica interactivos (I-DIDs) como las representaciones computacionales de I-POMDPs. Los I-DIDs generalizan los DIDs [12], los cuales pueden ser vistos como contrapartes computacionales de POMDPs, a entornos de múltiples agentes de la misma manera que los I-POMDPs generalizan los POMDPs. Los I-DIDs contribuyen a una creciente línea de trabajo [19] que incluye diagramas de influencia multiagente (MAIDs) [14], y más recientemente, redes de diagramas de influencia (NIDs) [8]. Estos formalismos buscan modelar explícitamente la estructura que a menudo está presente en problemas del mundo real al descomponer la situación en variables de azar y decisiones, y las dependencias entre las variables. Los MAIDs proporcionan una alternativa a las formas normales y extensivas de juego utilizando un formalismo gráfico para representar juegos de información imperfecta con un nodo de decisión para las acciones de cada agente y nodos de azar que capturan la información privada de los agentes. Los MAIDs analizan objetivamente el juego, calculando eficientemente el perfil de equilibrio de Nash al explotar la estructura de independencia. NIDs extienden los MAIDs para incluir la incertidumbre de los agentes sobre el juego que se está jugando y sobre los modelos de los otros agentes. Cada modelo es un MAID y la red de MAIDs se colapsa, de abajo hacia arriba, en un solo MAID para calcular el equilibrio del juego teniendo en cuenta los diferentes modelos de cada agente. Los formalismos gráficos como MAIDs y NIDs abren una área de investigación prometedora que tiene como objetivo representar las interacciones multiagente de manera más transparente. Sin embargo, los MAIDs proporcionan un análisis del juego desde un punto de vista externo y la aplicabilidad de ambos está limitada a juegos estáticos de un solo jugador. Los asuntos son más complejos cuando consideramos interacciones que se extienden en el tiempo, donde las predicciones sobre las acciones futuras de otros deben hacerse utilizando modelos que cambian a medida que los agentes actúan y observan. Los I-DIDs abordan esta brecha al permitir la representación de modelos de otros agentes como los valores de un nodo de modelo especial. Tanto los modelos de otros agentes como las creencias originales de los agentes sobre estos modelos se actualizan con el tiempo utilizando implementaciones especializadas. En este documento, mejoramos la representación preliminar previa del I-DID mostrada en [15] utilizando la idea de que el I-ID estático es un tipo de NID. Por lo tanto, podemos utilizar construcciones de lenguaje específicas de NID, como multiplexores, para representar de manera más transparente el nodo del modelo y, posteriormente, el I-ID. Además, aclaramos la semántica del enlace de política de propósito especial introducido en la representación de I-DID por [15], y mostramos que podría ser reemplazado por <br>enlaces de dependencia</br> tradicionales. En la representación previa del I-DID, la actualización de la creencia de los agentes sobre los modelos de los demás a medida que los agentes actúan y reciben observaciones se denotaba utilizando un enlace especial llamado el enlace de actualización del modelo que conectaba los nodos del modelo a lo largo del tiempo. Explicamos la semántica de este enlace mostrando cómo puede ser implementado utilizando los <br>enlaces de dependencia</br> tradicionales entre los nodos de probabilidad que constituyen los nodos del modelo. El resultado final es una representación de I-DID que es significativamente más transparente, semánticamente clara y capaz de ser implementada utilizando los algoritmos estándar para resolver DIDs. Mostramos cómo los IDIDs pueden ser utilizados para modelar la incertidumbre de un agente sobre los modelos de otros, que a su vez pueden ser I-DIDs. La solución al I-DID es una política que prescribe lo que el agente debe hacer con el tiempo, dadas sus creencias sobre el estado físico y otros modelos. Análogos a los DIDs, los I-DIDs pueden ser utilizados para calcular la política de un agente en línea mientras el agente actúa y observa en un entorno poblado por otros agentes interactuantes. 2. ANTECEDENTES: Los IPOMDPS ANIDADOS FINITAMENTE generalizan los POMDPs a entornos multiagentes al incluir los modelos de otros agentes como parte del espacio de estados [9]. Dado que otros agentes también pueden razonar sobre otros, el espacio de estado interactivo está estratégicamente anidado; contiene creencias sobre los modelos de otros agentes y sus creencias sobre los demás. Para simplificar la presentación, consideramos un agente, i, que está interactuando con otro agente, j. Un I-POMDP finitamente anidado del agente i con un nivel de estrategia l se define como la tupla: I-POMDPi,l = ISi,l, A, Ti, Ωi, Oi, Ri donde: • ISi,l denota un conjunto de estados interactivos definido como, ISi,l = S × Mj,l−1, donde Mj,l−1 = {Θj,l−1 ∪ SMj}, para l ≥ 1, e ISi,0 = S, donde S es el conjunto de estados del entorno físico. Θj,l−1 es el conjunto de modelos intencionales computables del agente j: θj,l−1 = bj,l−1, ˆθj donde el marco, ˆθj = A, Ωj, Tj, Oj, Rj, OCj. Aquí, j es Bayesiano racional y OCj es el criterio de optimalidad de j. SMj es el conjunto de modelos subintencionales de j. Ejemplos simples de modelos subintencionales incluyen un modelo sin información [10] y un modelo de juego ficticio [6], ambos de los cuales son independientes de la historia. Damos una construcción recursiva de abajo hacia arriba del espacio de estados interactivo a continuación. Sí,0 = S, Θj,0 = { bj,0, ˆθj | bj,0 ∈ Δ(ISj,0)} Sí,1 = S × {Θj,0 ∪ SMj}, Θj,1 = { bj,1, ˆθj | bj,1 ∈ Δ(ISj,1)} . . . . . . Sí, l = S × {Θj,l−1 ∪ SMj}, Θj,l = { bj,l, ˆθj | bj,l ∈ Δ(ISj,l)} Formulaciones similares de espacios anidados han aparecido en [1, 3]. • A = Ai × Aj es el conjunto de acciones conjuntas de todos los agentes en el entorno; • Ti : S ×A×S → [0, 1], describe el efecto de las acciones conjuntas en los estados físicos del entorno; • Ωi es el conjunto de observaciones del agente i; • Oi : S × A × Ωi → [0, 1] da la probabilidad de las observaciones dadas el estado físico y la acción conjunta; • Ri : ISi × A → R describe las preferencias del agente sobre sus estados interactivos. Normalmente solo importarán los estados físicos. La política del agente es el mapeo, Ω∗ i → Δ(Ai), donde Ω∗ i es el conjunto de todos los historiales de observación del agente i. Dado que la creencia sobre los estados interactivos forma una estadística suficiente [9], la política también puede ser representada como un mapeo del conjunto de todas las creencias del agente i a una distribución sobre sus acciones, Δ(ISi) → Δ(Ai). 2.1 Actualización de Creencias Análogo a los POMDPs, un agente dentro del marco I-POMDP actualiza su creencia a medida que actúa y observa. Sin embargo, hay dos diferencias que complican la actualización de creencias en entornos multiagentes en comparación con los entornos de un solo agente. Primero, dado que el estado del entorno físico depende de las acciones de ambos agentes, la predicción de cómo cambia el estado físico debe hacerse basándose en la predicción de sus acciones. Segundo, los cambios en los modelos de js deben ser incluidos en la actualización de creencias. Específicamente, si j es intencional, entonces se debe incluir una actualización de las creencias de j debido a su acción y observación. En otras palabras, i tiene que actualizar su creencia basándose en su predicción de lo que j observaría y cómo j actualizaría su creencia. Si el modelo de js es subintencional, entonces las observaciones probables de js se añaden al historial de observaciones contenido en el modelo. Formalmente, tenemos: Pr(ist |at−1 i , bt−1 i,l ) = β ISt−1:mt−1 j =θt j bt−1 i,l (ist−1 ) × at−1 j Pr(at−1 j |θt−1 j,l−1)Oi(st , at−1 i , at−1 j , ot i) ×Ti(st−1 , at−1 i , at−1 j , st ) ot j Oj(st , at−1 i , at−1 j , ot j) ×τ(SEθt j (bt−1 j,l−1, at−1 j , ot j) − bt j,l−1) (1) donde β es la constante de normalización, τ es 1 si su argumento es 0, de lo contrario es 0, Pr(at−1 j |θt−1 j,l−1) es la probabilidad de que at−1 j sea Bayes racional para el agente descrito por el modelo θt−1 j,l−1, y SE(·) es una abreviatura para la actualización de creencias. Para una versión de la actualización de creencias cuando el modelo js es subintencional, consulte [9]. Si el agente j también se modela como un I-POMDP, entonces la actualización de creencias invoca la actualización de creencias de j (a través del término SEθt j ( bt−1 j,l−1 , at−1 j , ot j)), lo que a su vez podría invocar la actualización de creencias de is y así sucesivamente. Esta recursión en la anidación de creencias llega al nivel 0. En este nivel, la actualización de creencias del agente se reduce a una actualización de creencias de un POMDP. Para ilustraciones de la actualización de creencias, detalles adicionales sobre I-POMDPs y cómo se comparan con otros marcos multiagentes, consulte [9]. Iteración de Valor Cada estado de creencia en un I-POMDP finitamente anidado tiene un valor asociado que refleja la máxima ganancia que el agente puede esperar en este estado de creencia: Un ( bi,l, θi ) = max ai∈Ai is∈ISi,l ERi(is, ai)bi,l(is)+ γ oi∈Ωi Pr(oi|ai, bi,l)Un−1 ( SEθi (bi,l, ai, oi), θi ) (2) donde, ERi(is, ai) = aj Ri(is, ai, aj)Pr(aj|mj,l−1) (ya que is = (s, mj,l−1)). La ecuación 2 es una base para la iteración de valor en I-POMDPs. La acción óptima del agente, a∗ i, para el caso de horizonte finito con descuento, es un elemento del conjunto de acciones óptimas para el estado de creencia, OPT(θi), definido como: OPT( bi,l, θi ) = argmax ai∈Ai is∈ISi,l ERi(is, ai)bi,l(is) +γ oi∈Ωi Pr(oi|ai, bi,l)Un ( SEθi (bi,l, ai, oi), θi ) (3) 3. Diagramas de influencia interactivos. Una extensión ingenua de los diagramas de influencia (IDs) a entornos poblados por múltiples agentes es posible tratando a los otros agentes como autómatas, representados mediante nodos de probabilidad. Sin embargo, este enfoque asume que las acciones de los agentes están controladas utilizando una distribución de probabilidad que no cambia con el tiempo. Los diagramas de influencia interactivos (I-IDs) adoptan un enfoque más sofisticado al generalizar los IDs para hacerlos aplicables a entornos compartidos con otros agentes que pueden actuar, observar y actualizar sus creencias. 3.1 Sintaxis Además de los nodos habituales de probabilidad, decisión y utilidad, los IIDs incluyen un nuevo tipo de nodo llamado nodo de modelo. Mostramos un nivel general l I-ID en la Fig. 1(a), donde el nodo del modelo (Mj,l−1) está representado con un hexágono. Observamos que la distribución de probabilidad sobre el nodo de probabilidad, S, y el nodo del modelo juntos representan la creencia del agente sobre sus estados interactivos. Además del modelo 1, el modelo de nivel 0 es un POMDP: las acciones de otros agentes se tratan como eventos exógenos y se incorporan en las funciones T, O y R. El Sexto Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) Figura 1: (a) Un nivel genérico l I-ID para el agente i situado con otro agente j. El hexágono es el nodo modelo (Mj,l−1) cuya estructura mostramos en (b). Los miembros del nodo modelo son ellos mismos I-ID (m1 j,l−1, m2 j,l−1; los diagramas no se muestran aquí por simplicidad) cuyos nodos de decisión se asignan a los nodos de probabilidad correspondientes (A1 j , A2 j). Dependiendo del valor del nodo, Mod[Mj], la distribución de cada uno de los nodos de probabilidad se asigna al nodo Aj. (c) El I-ID transformado con el nodo del modelo reemplazado por los nodos de probabilidad y las relaciones entre ellos. Los I-IDs difieren de los IDs al tener un enlace discontinuo (llamado enlace de política en [15]) entre el nodo del modelo y un nodo de probabilidad, Aj, que representa la distribución sobre las acciones de los otros agentes dada su modelo. En ausencia de otros agentes, el nodo del modelo y el nodo de probabilidad, Aj, desaparecen y los I-ID se convierten en ID tradicionales. El nodo del modelo contiene los modelos computacionales alternativos atribuidos por i al otro agente del conjunto, Θj,l−1 ∪ SMj, donde Θj,l−1 y SMj fueron definidos previamente en la Sección 2. Por lo tanto, un modelo en el nodo del modelo puede ser en sí mismo un I-ID o ID, y la recursión termina cuando un modelo es un ID o subintencional. Dado que el nodo del modelo contiene los modelos alternativos del otro agente como sus valores, su representación no es trivial. En particular, algunos de los modelos dentro del nodo son I-IDs que, al resolverse, generan la política óptima de los agentes en sus nodos de decisión. Cada nodo de decisión se asigna al nodo de probabilidad correspondiente, digamos A1 j, de la siguiente manera: si OPT es el conjunto de acciones óptimas obtenidas al resolver el I-ID (o ID), entonces Pr(aj ∈ A1 j) = 1 |OPT| si aj ∈ OPT, 0 en caso contrario. Tomando prestados los conocimientos de trabajos anteriores [8], observamos que el nodo del modelo y el enlace de política discontinua que lo conecta con el nodo de probabilidad, Aj, podrían representarse como se muestra en la Fig. 1(b). El nodo de decisión de cada nivel l − 1 I-ID se transforma en un nodo de probabilidad, como mencionamos anteriormente, de modo que las acciones con el mayor valor en el nodo de decisión se les asignan probabilidades uniformes en el nodo de probabilidad, mientras que al resto se les asigna probabilidad cero. Los diferentes nodos de probabilidad (A1 j , A2 j ), uno para cada modelo, y adicionalmente, el nodo de probabilidad etiquetado Mod[Mj] forman los padres del nodo de probabilidad, Aj. Por lo tanto, hay tantos nodos de acción (A1 j , A2 j ) en Mj,l−1 como el número de modelos en el soporte de las creencias del agente. La tabla de probabilidad condicional del nodo de probabilidad, Aj, es un multiplexor que asume la distribución de cada uno de los nodos de acción (A1 j , A2 j ) dependiendo del valor de Mod[Mj]. Los valores de Mod[Mj] denotan los diferentes modelos de j. En otras palabras, cuando Mod[Mj] tiene el valor m1 j,l−1, el nodo de probabilidad Aj asume la distribución del nodo A1 j, y Aj asume la distribución de A2 j cuando Mod[Mj] tiene el valor m2 j,l−1. La distribución sobre el nodo, Mod[Mj], es la creencia del agente sobre los modelos de j dado un estado físico. Para más agentes, tendremos tantos nodos de modelo como agentes haya. Observa que la Fig. 1(b) aclara la semántica del enlace de política y muestra cómo puede ser representado utilizando los <br>enlaces de dependencia</br> tradicionales. En la Fig. 1(c), mostramos el I-ID transformado cuando el nodo del modelo es reemplazado por los nodos de probabilidad y las relaciones entre ellos. A diferencia de la representación en [15], no hay enlaces de políticas especiales, sino que el I-ID está compuesto únicamente por aquellos tipos de nodos que se encuentran en IDs tradicionales y relaciones de dependencia entre los nodos. Esto permite que los I-IDs sean representados e implementados utilizando herramientas de aplicación convencionales que apuntan a los IDs. Ten en cuenta que podemos ver el nivel l I-ID como un NID. Específicamente, cada uno de los modelos del nivel l − 1 dentro del nodo del modelo son bloques en el NID (ver Fig. 2). Si el nivel l = 1, cada bloque es un ID tradicional, de lo contrario, si l > 1, cada bloque dentro del NID puede ser un NID en sí mismo. Ten en cuenta que dentro de los I-IDs (o IDs) en cada nivel, solo hay un nodo de decisión único. Por lo tanto, nuestro NID no contiene ningún MAID. Figura 2: Un nivel l I-ID representado como un NID. Las probabilidades asignadas a los bloques del NID son creencias sobre modelos js condicionados a un estado físico. 3.2 Solución La solución de un I-ID procede de manera ascendente y se implementa de forma recursiva. Comenzamos resolviendo los modelos de nivel 0, que, si son intencionales, son identificadores tradicionales. Sus soluciones proporcionan distribuciones de probabilidad sobre las acciones de los otros agentes, las cuales se ingresan en los nodos de probabilidad correspondientes encontrados en el nodo del modelo del nivel 1 I-ID. El mapeo de los nodos de decisión de los modelos de nivel 0 a los nodos de probabilidad se realiza de manera que las acciones con el mayor valor en el nodo de decisión se les asignan probabilidades uniformes en el nodo de probabilidad, mientras que el resto se les asigna probabilidad cero. Dadas las distribuciones sobre las acciones dentro de los diferentes nodos de probabilidad (uno para cada modelo del otro agente), el I-ID de nivel 1 se transforma como se muestra en la Fig. 1(c). Durante la transformación, la tabla de probabilidad condicional (CPT) del nodo, Aj, se llena de tal manera que el nodo asume la distribución de cada uno de los nodos de probabilidad dependiendo del valor del nodo, Mod[Mj]. Como mencionamos anteriormente, los valores del nodo Mod[Mj] denotan los diferentes modelos del otro agente, y su distribución es la creencia del agente sobre los modelos de j condicionados al estado físico. El nivel 1 I-ID transformado es un ID tradicional que puede ser resuelto como us816 The Sixth Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) (a) (b) Figura 3: (a) Un I-DID genérico de dos niveles de tiempo para el agente i en un entorno con otro agente j. Observa el enlace de actualización del modelo punteado que denota la actualización de los modelos de j y la distribución de los modelos a lo largo del tiempo. (b) La semántica del enlace de actualización del modelo utilizando el método estándar de maximización de utilidad esperada [18]. Este procedimiento se lleva a cabo hasta el nivel l I-ID cuya solución proporciona el conjunto no vacío de acciones óptimas que el agente debe realizar dada su creencia. Ten en cuenta que, al igual que los IDs, los I-IDs son adecuados para la toma de decisiones en línea cuando se conoce la creencia actual de los agentes. 4. Diagramas de influencia dinámica interactivos (I-DIDs) Los diagramas de influencia dinámica interactivos (I-DIDs) extienden los I-IDs (y NIDs) para permitir la toma de decisiones secuencial a lo largo de varios pasos de tiempo. Así como los DIDs son representaciones gráficas estructuradas de POMDPs, los I-DIDs son los análogos gráficos en línea para I-POMDPs finitamente anidados. Los I-DIDs pueden ser utilizados para optimizar sobre una mirada anticipada finita dadas las creencias iniciales mientras se interactúa con otros agentes, posiblemente similares. 4.1 Sintaxis Representamos un I-DID de dos cortes temporales en la Fig. 3(a). Además de los nodos del modelo y el enlace de política discontinuo, lo que diferencia a un I-DID de un DID es el enlace de actualización del modelo mostrado como una flecha punteada en la Fig. 3(a). Explicamos la semántica del nodo del modelo y el enlace de la política en la sección anterior; a continuación, describimos las actualizaciones del modelo. La actualización del nodo del modelo con el tiempo implica dos pasos: primero, dados los modelos en el tiempo t, identificamos el conjunto actualizado de modelos que residen en el nodo del modelo en el tiempo t + 1. Recuerda de la Sección 2 que el modelo intencional de un agente incluye sus creencias. Debido a que los agentes actúan y reciben observaciones, sus modelos se actualizan para reflejar sus creencias cambiadas. Dado que el conjunto de acciones óptimas para un modelo podría incluir todas las acciones, y el agente podría recibir cualquiera de las |Ωj| posibles observaciones, el conjunto actualizado en el paso de tiempo t + 1 tendrá como máximo |Mt j,l−1||Aj||Ωj| modelos. Aquí, |Mt j,l−1| es el número de modelos en el paso de tiempo t, |Aj| y |Ωj| son los espacios más grandes de acciones y observaciones respectivamente, entre todos los modelos. Segundo, calculamos la nueva distribución sobre los modelos actualizados dados la distribución original y la probabilidad de que el agente realice la acción y reciba la observación que llevó al modelo actualizado. Estos pasos son parte de la actualización de creencias del agente formalizada utilizando la Ecuación 1. En la Fig. 3(b), mostramos cómo se implementa el enlace de actualización del modelo punteado en el I-DID. Si cada uno de los dos modelos de nivel l − 1 atribuidos a j en el paso de tiempo t resulta en una acción, y j podría hacer una de dos posibles observaciones, entonces el nodo del modelo en el paso de tiempo t + 1 contiene cuatro modelos actualizados (mt+1,1 j,l−1 ,mt+1,2 j,l−1 , mt+1,3 j,l−1 , y mt+1,4 j,l−1 ). Estos modelos difieren en sus creencias iniciales, cada una de las cuales es el resultado de j actualizar sus creencias debido a su acción y una posible observación. Los nodos de decisión en cada uno de los I-DIDs o DIDs que representan los modelos de nivel inferior se asignan a la Figura 4 correspondiente: I-DID transformado con los nodos del modelo y el enlace de actualización del modelo reemplazados por los nodos de probabilidad y las relaciones (en negrita) de los nodos de probabilidad, como se mencionó anteriormente. A continuación, describimos cómo se calcula la distribución sobre el conjunto actualizado de modelos (la distribución sobre el nodo de probabilidad Mod[Mt+1 j ] en Mt+1 j,l−1). La probabilidad de que el modelo actualizado js sea, digamos mt+1,1 j,l−1, depende de la probabilidad de que j realice la acción y reciba la observación que condujo a este modelo, y de la distribución previa sobre los modelos en el paso de tiempo t. Dado que el nodo de probabilidad At j asume la distribución de cada uno de los nodos de acción basados en el valor de Mod[Mt j], la probabilidad de la acción está dada por este nodo de probabilidad. Para obtener la probabilidad de una observación posible js, introducimos el nodo de probabilidad Oj, que dependiendo del valor de Mod[Mt j], asume la distribución del nodo de observación en el modelo de nivel inferior denominado Mod[Mt j]. Dado que la probabilidad de las observaciones js depende del estado físico y de las acciones conjuntas de ambos agentes, el nodo Oj está vinculado con St+1, At j y At i. De manera análoga a At j, la tabla de probabilidad condicional de Oj también es un multiplexor modulado por Mod[Mt j]. Finalmente, la distribución de los modelos previos en el tiempo t se obtiene a partir del nodo de probabilidad, Mod[Mt j ] en Mt j,l−1. Por consiguiente, los nodos de probabilidad, Mod[Mt j], At j y Oj, forman los padres de Mod[Mt+1 j] en Mt+1 j,l−1. Ten en cuenta que el enlace de actualización del modelo puede ser reemplazado por los <br>enlaces de dependencia</br> entre los nodos de probabilidad que constituyen los nodos del modelo en las dos etapas temporales. En la Fig. 4 mostramos los dos cortes temporales I-DID con los nodos del modelo reemplazados por los nodos de probabilidad y las relaciones entre ellos. Los nodos de probabilidad y los <br>enlaces de dependencia</br> que no están en negrita son estándar, generalmente encontrados en DIDs. ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "influence diagram": {
            "translated_key": "diagramas de influencia",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Graphical Models for Online Solutions to Interactive POMDPs Prashant Doshi Dept.",
                "of Computer Science University of Georgia Athens, GA 30602, USA pdoshi@cs.uga.edu Yifeng Zeng Dept. of Computer Science Aalborg University DK-9220 Aalborg, Denmark yfzeng@cs.aau.edu Qiongyu Chen Dept. of Computer Science National Univ. of Singapore 117543, Singapore chenqy@comp.nus.edu.sg ABSTRACT We develop a new graphical representation for interactive partially observable Markov decision processes (I-POMDPs) that is significantly more transparent and semantically clear than the previous representation.",
                "These graphical models called interactive dynamic <br>influence diagram</br>s (I-DIDs) seek to explicitly model the structure that is often present in real-world problems by decomposing the situation into chance and decision variables, and the dependencies between the variables.",
                "I-DIDs generalize DIDs, which may be viewed as graphical representations of POMDPs, to multiagent settings in the same way that I-POMDPs generalize POMDPs.",
                "I-DIDs may be used to compute the policy of an agent online as the agent acts and observes in a setting that is populated by other interacting agents.",
                "Using several examples, we show how I-DIDs may be applied and demonstrate their usefulness.",
                "Categories and Subject Descriptors I.2.11 [Distributed Artificial Intelligence]: Multiagent Systems General Terms Theory 1.",
                "INTRODUCTION Interactive partially observable Markov decision processes (IPOMDPs) [9] provide a framework for sequential decision-making in partially observable multiagent environments.",
                "They generalize POMDPs [13] to multiagent settings by including the other agents computable models in the state space along with the states of the physical environment.",
                "The models encompass all information influencing the agents behaviors, including their preferences, capabilities, and beliefs, and are thus analogous to types in Bayesian games [11].",
                "I-POMDPs adopt a subjective approach to understanding strategic behavior, rooted in a decision-theoretic framework that takes a decision-makers perspective in the interaction.",
                "In [15], Polich and Gmytrasiewicz introduced interactive dynamic <br>influence diagram</br>s (I-DIDs) as the computational representations of I-POMDPs.",
                "I-DIDs generalize DIDs [12], which may be viewed as computational counterparts of POMDPs, to multiagents settings in the same way that I-POMDPs generalize POMDPs.",
                "I-DIDs contribute to a growing line of work [19] that includes multi-agent <br>influence diagram</br>s (MAIDs) [14], and more recently, networks of <br>influence diagram</br>s (NIDs) [8].",
                "These formalisms seek to explicitly model the structure that is often present in real-world problems by decomposing the situation into chance and decision variables, and the dependencies between the variables.",
                "MAIDs provide an alternative to normal and extensive game forms using a graphical formalism to represent games of imperfect information with a decision node for each agents actions and chance nodes capturing the agents private information.",
                "MAIDs objectively analyze the game, efficiently computing the Nash equilibrium profile by exploiting the independence structure.",
                "NIDs extend MAIDs to include agents uncertainty over the game being played and over models of the other agents.",
                "Each model is a MAID and the network of MAIDs is collapsed, bottom up, into a single MAID for computing the equilibrium of the game keeping in mind the different models of each agent.",
                "Graphical formalisms such as MAIDs and NIDs open up a promising area of research that aims to represent multiagent interactions more transparently.",
                "However, MAIDs provide an analysis of the game from an external viewpoint and the applicability of both is limited to static single play games.",
                "Matters are more complex when we consider interactions that are extended over time, where predictions about others future actions must be made using models that change as the agents act and observe.",
                "I-DIDs address this gap by allowing the representation of other agents models as the values of a special model node.",
                "Both, other agents models and the original agents beliefs over these models are updated over time using special-purpose implementations.",
                "In this paper, we improve on the previous preliminary representation of the I-DID shown in [15] by using the insight that the static I-ID is a type of NID.",
                "Thus, we may utilize NID-specific language constructs such as multiplexers to represent the model node, and subsequently the I-ID, more transparently.",
                "Furthermore, we clarify the semantics of the special purpose policy link introduced in the representation of I-DID by [15], and show that it could be replaced by traditional dependency links.",
                "In the previous representation of the I-DID, the update of the agents belief over the models of others as the agents act and receive observations was denoted using a special link called the model update link that connected the model nodes over time.",
                "We explicate the semantics of this link by showing how it can be implemented using the traditional dependency links between the chance nodes that constitute the model nodes.",
                "The net result is a representation of I-DID that is significantly more transparent, semantically clear, and capable of being implemented using the standard algorithms for solving DIDs.",
                "We show how IDIDs may be used to model an agents uncertainty over others models, that may themselves be I-DIDs.",
                "Solution to the I-DID is a policy that prescribes what the agent should do over time, given its beliefs over the physical state and others models.",
                "Analogous to DIDs, I-DIDs may be used to compute the policy of an agent online as the agent acts and observes in a setting that is populated by other interacting agents. 2.",
                "BACKGROUND: FINITELY NESTED IPOMDPS Interactive POMDPs generalize POMDPs to multiagent settings by including other agents models as part of the state space [9].",
                "Since other agents may also reason about others, the interactive state space is strategically nested; it contains beliefs about other agents models and their beliefs about others.",
                "For simplicity of presentation we consider an agent, i, that is interacting with one other agent, j.",
                "A finitely nested I-POMDP of agent i with a strategy level l is defined as the tuple: I-POMDPi,l = ISi,l, A, Ti, Ωi, Oi, Ri where: • ISi,l denotes a set of interactive states defined as, ISi,l = S × Mj,l−1, where Mj,l−1 = {Θj,l−1 ∪ SMj}, for l ≥ 1, and ISi,0 = S, where S is the set of states of the physical environment.",
                "Θj,l−1 is the set of computable intentional models of agent j: θj,l−1 = bj,l−1, ˆθj where the frame, ˆθj = A, Ωj, Tj, Oj, Rj, OCj .",
                "Here, j is Bayes rational and OCj is js optimality criterion.",
                "SMj is the set of subintentional models of j.",
                "Simple examples of subintentional models include a no-information model [10] and a fictitious play model [6], both of which are history independent.",
                "We give a recursive bottom-up construction of the interactive state space below.",
                "ISi,0 = S, Θj,0 = { bj,0, ˆθj | bj,0 ∈ Δ(ISj,0)} ISi,1 = S × {Θj,0 ∪ SMj}, Θj,1 = { bj,1, ˆθj | bj,1 ∈ Δ(ISj,1)} . . . . . .",
                "ISi,l = S × {Θj,l−1 ∪ SMj}, Θj,l = { bj,l, ˆθj | bj,l ∈ Δ(ISj,l)} Similar formulations of nested spaces have appeared in [1, 3]. • A = Ai × Aj is the set of joint actions of all agents in the environment; • Ti : S ×A×S → [0, 1], describes the effect of the joint actions on the physical states of the environment; • Ωi is the set of observations of agent i; • Oi : S × A × Ωi → [0, 1] gives the likelihood of the observations given the physical state and joint action; • Ri : ISi × A → R describes agent is preferences over its interactive states.",
                "Usually only the physical states will matter.",
                "Agent is policy is the mapping, Ω∗ i → Δ(Ai), where Ω∗ i is the set of all observation histories of agent i.",
                "Since belief over the interactive states forms a sufficient statistic [9], the policy can also be represented as a mapping from the set of all beliefs of agent i to a distribution over its actions, Δ(ISi) → Δ(Ai). 2.1 Belief Update Analogous to POMDPs, an agent within the I-POMDP framework updates its belief as it acts and observes.",
                "However, there are two differences that complicate the belief update in multiagent settings when compared to single agent ones.",
                "First, since the state of the physical environment depends on the actions of both agents, is prediction of how the physical state changes has to be made based on its prediction of js actions.",
                "Second, changes in js models have to be included in is belief update.",
                "Specifically, if j is intentional then an update of js beliefs due to its action and observation has to be included.",
                "In other words, i has to update its belief based on its prediction of what j would observe and how j would update its belief.",
                "If js model is subintentional, then js probable observations are appended to the observation history contained in the model.",
                "Formally, we have: Pr(ist |at−1 i , bt−1 i,l ) = β ISt−1:mt−1 j =θt j bt−1 i,l (ist−1 ) × at−1 j Pr(at−1 j |θt−1 j,l−1)Oi(st , at−1 i , at−1 j , ot i) ×Ti(st−1 , at−1 i , at−1 j , st ) ot j Oj(st , at−1 i , at−1 j , ot j) ×τ(SEθt j (bt−1 j,l−1, at−1 j , ot j) − bt j,l−1) (1) where β is the normalizing constant, τ is 1 if its argument is 0 otherwise it is 0, Pr(at−1 j |θt−1 j,l−1) is the probability that at−1 j is Bayes rational for the agent described by model θt−1 j,l−1, and SE(·) is an abbreviation for the belief update.",
                "For a version of the belief update when js model is subintentional, see [9].",
                "If agent j is also modeled as an I-POMDP, then is belief update invokes js belief update (via the term SEθt j ( bt−1 j,l−1 , at−1 j , ot j)), which in turn could invoke is belief update and so on.",
                "This recursion in belief nesting bottoms out at the 0th level.",
                "At this level, the belief update of the agent reduces to a POMDP belief update. 1 For illustrations of the belief update, additional details on I-POMDPs, and how they compare with other multiagent frameworks, see [9]. 2.2 Value Iteration Each belief state in a finitely nested I-POMDP has an associated value reflecting the maximum payoff the agent can expect in this belief state: Un ( bi,l, θi ) = max ai∈Ai is∈ISi,l ERi(is, ai)bi,l(is)+ γ oi∈Ωi Pr(oi|ai, bi,l)Un−1 ( SEθi (bi,l, ai, oi), θi ) (2) where, ERi(is, ai) = aj Ri(is, ai, aj)Pr(aj|mj,l−1) (since is = (s, mj,l−1)).",
                "Eq. 2 is a basis for value iteration in I-POMDPs.",
                "Agent is optimal action, a∗ i , for the case of finite horizon with discounting, is an element of the set of optimal actions for the belief state, OPT(θi), defined as: OPT( bi,l, θi ) = argmax ai∈Ai is∈ISi,l ERi(is, ai)bi,l(is) +γ oi∈Ωi Pr(oi|ai, bi,l)Un ( SEθi (bi,l, ai, oi), θi ) (3) 3.",
                "INTERACTIVEINFLUENCEDIAGRAMS A naive extension of <br>influence diagram</br>s (IDs) to settings populated by multiple agents is possible by treating other agents as automatons, represented using chance nodes.",
                "However, this approach assumes that the agents actions are controlled using a probability distribution that does not change over time.",
                "Interactive <br>influence diagram</br>s (I-IDs) adopt a more sophisticated approach by generalizing IDs to make them applicable to settings shared with other agents who may act and observe, and update their beliefs. 3.1 Syntax In addition to the usual chance, decision, and utility nodes, IIDs include a new type of node called the model node.",
                "We show a general level l I-ID in Fig. 1(a), where the model node (Mj,l−1) is denoted using a hexagon.",
                "We note that the probability distribution over the chance node, S, and the model node together represents agent is belief over its interactive states.",
                "In addition to the model 1 The 0th level model is a POMDP: Other agents actions are treated as exogenous events and folded into the T, O, and R functions.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 815 Figure 1: (a) A generic level l I-ID for agent i situated with one other agent j.",
                "The hexagon is the model node (Mj,l−1) whose structure we show in (b).",
                "Members of the model node are I-IDs themselves (m1 j,l−1, m2 j,l−1; diagrams not shown here for simplicity) whose decision nodes are mapped to the corresponding chance nodes (A1 j , A2 j ).",
                "Depending on the value of the node, Mod[Mj], the distribution of each of the chance nodes is assigned to the node Aj. (c) The transformed I-ID with the model node replaced by the chance nodes and the relationships between them. node, I-IDs differ from IDs by having a dashed link (called the policy link in [15]) between the model node and a chance node, Aj, that represents the distribution over the other agents actions given its model.",
                "In the absence of other agents, the model node and the chance node, Aj, vanish and I-IDs collapse into traditional IDs.",
                "The model node contains the alternative computational models ascribed by i to the other agent from the set, Θj,l−1 ∪ SMj, where Θj,l−1 and SMj were defined previously in Section 2.",
                "Thus, a model in the model node may itself be an I-ID or ID, and the recursion terminates when a model is an ID or subintentional.",
                "Because the model node contains the alternative models of the other agent as its values, its representation is not trivial.",
                "In particular, some of the models within the node are I-IDs that when solved generate the agents optimal policy in their decision nodes.",
                "Each decision node is mapped to the corresponding chance node, say A1 j , in the following way: if OPT is the set of optimal actions obtained by solving the I-ID (or ID), then Pr(aj ∈ A1 j ) = 1 |OP T | if aj ∈ OPT, 0 otherwise.",
                "Borrowing insights from previous work [8], we observe that the model node and the dashed policy link that connects it to the chance node, Aj, could be represented as shown in Fig. 1(b).",
                "The decision node of each level l − 1 I-ID is transformed into a chance node, as we mentioned previously, so that the actions with the largest value in the decision node are assigned uniform probabilities in the chance node while the rest are assigned zero probability.",
                "The different chance nodes (A1 j , A2 j ), one for each model, and additionally, the chance node labeled Mod[Mj] form the parents of the chance node, Aj.",
                "Thus, there are as many action nodes (A1 j , A2 j ) in Mj,l−1 as the number of models in the support of agent is beliefs.",
                "The conditional probability table of the chance node, Aj, is a multiplexer that assumes the distribution of each of the action nodes (A1 j , A2 j ) depending on the value of Mod[Mj].",
                "The values of Mod[Mj] denote the different models of j.",
                "In other words, when Mod[Mj] has the value m1 j,l−1, the chance node Aj assumes the distribution of the node A1 j , and Aj assumes the distribution of A2 j when Mod[Mj] has the value m2 j,l−1.",
                "The distribution over the node, Mod[Mj], is the agent is belief over the models of j given a physical state.",
                "For more agents, we will have as many model nodes as there are agents.",
                "Notice that Fig. 1(b) clarifies the semantics of the policy link, and shows how it can be represented using the traditional dependency links.",
                "In Fig. 1(c), we show the transformed I-ID when the model node is replaced by the chance nodes and relationships between them.",
                "In contrast to the representation in [15], there are no special-purpose policy links, rather the I-ID is composed of only those types of nodes that are found in traditional IDs and dependency relationships between the nodes.",
                "This allows I-IDs to be represented and implemented using conventional application tools that target IDs.",
                "Note that we may view the level l I-ID as a NID.",
                "Specifically, each of the level l − 1 models within the model node are blocks in the NID (see Fig. 2).",
                "If the level l = 1, each block is a traditional ID, otherwise if l > 1, each block within the NID may itself be a NID.",
                "Note that within the I-IDs (or IDs) at each level, there is only a single decision node.",
                "Thus, our NID does not contain any MAIDs.",
                "Figure 2: A level l I-ID represented as a NID.",
                "The probabilities assigned to the blocks of the NID are is beliefs over js models conditioned on a physical state. 3.2 Solution The solution of an I-ID proceeds in a bottom-up manner, and is implemented recursively.",
                "We start by solving the level 0 models, which, if intentional, are traditional IDs.",
                "Their solutions provide probability distributions over the other agents actions, which are entered in the corresponding chance nodes found in the model node of the level 1 I-ID.",
                "The mapping from the level 0 models decision nodes to the chance nodes is carried out so that actions with the largest value in the decision node are assigned uniform probabilities in the chance node while the rest are assigned zero probability.",
                "Given the distributions over the actions within the different chance nodes (one for each model of the other agent), the level 1 I-ID is transformed as shown in Fig. 1(c).",
                "During the transformation, the conditional probability table (CPT) of the node, Aj, is populated such that the node assumes the distribution of each of the chance nodes depending on the value of the node, Mod[Mj].",
                "As we mentioned previously, the values of the node Mod[Mj] denote the different models of the other agent, and its distribution is the agent is belief over the models of j conditioned on the physical state.",
                "The transformed level 1 I-ID is a traditional ID that may be solved us816 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) (a) (b) Figure 3: (a) A generic two time-slice level l I-DID for agent i in a setting with one other agent j.",
                "Notice the dotted model update link that denotes the update of the models of j and the distribution over the models over time. (b) The semantics of the model update link. ing the standard expected utility maximization method [18].",
                "This procedure is carried out up to the level l I-ID whose solution gives the non-empty set of optimal actions that the agent should perform given its belief.",
                "Notice that analogous to IDs, I-IDs are suitable for online decision-making when the agents current belief is known. 4.",
                "INTERACTIVE DYNAMIC INFLUENCE DIAGRAMS Interactive dynamic <br>influence diagram</br>s (I-DIDs) extend I-IDs (and NIDs) to allow sequential decision-making over several time steps.",
                "Just as DIDs are structured graphical representations of POMDPs, I-DIDs are the graphical online analogs for finitely nested I-POMDPs.",
                "I-DIDs may be used to optimize over a finite look-ahead given initial beliefs while interacting with other, possibly similar, agents. 4.1 Syntax We depict a general two time-slice I-DID in Fig. 3(a).",
                "In addition to the model nodes and the dashed policy link, what differentiates an I-DID from a DID is the model update link shown as a dotted arrow in Fig. 3(a).",
                "We explained the semantics of the model node and the policy link in the previous section; we describe the model updates next.",
                "The update of the model node over time involves two steps: First, given the models at time t, we identify the updated set of models that reside in the model node at time t + 1.",
                "Recall from Section 2 that an agents intentional model includes its belief.",
                "Because the agents act and receive observations, their models are updated to reflect their changed beliefs.",
                "Since the set of optimal actions for a model could include all the actions, and the agent may receive any one of |Ωj| possible observations, the updated set at time step t + 1 will have at most |Mt j,l−1||Aj||Ωj| models.",
                "Here, |Mt j,l−1| is the number of models at time step t, |Aj| and |Ωj| are the largest spaces of actions and observations respectively, among all the models.",
                "Second, we compute the new distribution over the updated models given the original distribution and the probability of the agent performing the action and receiving the observation that led to the updated model.",
                "These steps are a part of agent is belief update formalized using Eq. 1.",
                "In Fig. 3(b), we show how the dotted model update link is implemented in the I-DID.",
                "If each of the two level l − 1 models ascribed to j at time step t results in one action, and j could make one of two possible observations, then the model node at time step t + 1 contains four updated models (mt+1,1 j,l−1 ,mt+1,2 j,l−1 , mt+1,3 j,l−1 , and mt+1,4 j,l−1 ).",
                "These models differ in their initial beliefs, each of which is the result of j updating its beliefs due to its action and a possible observation.",
                "The decision nodes in each of the I-DIDs or DIDs that represent the lower level models are mapped to the corresponding Figure 4: Transformed I-DID with the model nodes and model update link replaced with the chance nodes and the relationships (in bold). chance nodes, as mentioned previously.",
                "Next, we describe how the distribution over the updated set of models (the distribution over the chance node Mod[Mt+1 j ] in Mt+1 j,l−1) is computed.",
                "The probability that js updated model is, say mt+1,1 j,l−1 , depends on the probability of j performing the action and receiving the observation that led to this model, and the prior distribution over the models at time step t. Because the chance node At j assumes the distribution of each of the action nodes based on the value of Mod[Mt j ], the probability of the action is given by this chance node.",
                "In order to obtain the probability of js possible observation, we introduce the chance node Oj, which depending on the value of Mod[Mt j ] assumes the distribution of the observation node in the lower level model denoted by Mod[Mt j ].",
                "Because the probability of js observations depends on the physical state and the joint actions of both agents, the node Oj is linked with St+1 , At j, and At i. 2 Analogous to At j, the conditional probability table of Oj is also a multiplexer modulated by Mod[Mt j ].",
                "Finally, the distribution over the prior models at time t is obtained from the chance node, Mod[Mt j ] in Mt j,l−1.",
                "Consequently, the chance nodes, Mod[Mt j ], At j, and Oj, form the parents of Mod[Mt+1 j ] in Mt+1 j,l−1.",
                "Notice that the model update link may be replaced by the dependency links between the chance nodes that constitute the model nodes in the two time slices.",
                "In Fig. 4 we show the two time-slice I-DID with the model nodes replaced by the chance nodes and the relationships between them.",
                "Chance nodes and dependency links that not in bold are standard, usually found in DIDs.",
                "Expansion of the I-DID over more time steps requires the repetition of the two steps of updating the set of models that form the 2 Note that Oj represents js observation at time t + 1.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 817 values of the model node and adding the relationships between the chance nodes, as many times as there are model update links.",
                "We note that the possible set of models of the other agent j grows exponentially with the number of time steps.",
                "For example, after T steps, there may be at most |Mt=1 j,l−1|(|Aj||Ωj|)T −1 candidate models residing in the model node. 4.2 Solution Analogous to I-IDs, the solution to a level l I-DID for agent i expanded over T time steps may be carried out recursively.",
                "For the purpose of illustration, let l=1 and T=2.",
                "The solution method uses the standard look-ahead technique, projecting the agents action and observation sequences forward from the current belief state [17], and finding the possible beliefs that i could have in the next time step.",
                "Because agent i has a belief over js models as well, the lookahead includes finding out the possible models that j could have in the future.",
                "Consequently, each of js subintentional or level 0 models (represented using a standard DID) in the first time step must be solved to obtain its optimal set of actions.",
                "These actions are combined with the set of possible observations that j could make in that model, resulting in an updated set of candidate models (that include the updated beliefs) that could describe the behavior of j. Beliefs over this updated set of candidate models are calculated using the standard inference methods using the dependency relationships between the model nodes as shown in Fig. 3(b).",
                "We note the recursive nature of this solution: in solving agent is level 1 I-DID, js level 0 DIDs must be solved.",
                "If the nesting of models is deeper, all models at all levels starting from 0 are solved in a bottom-up manner.",
                "We briefly outline the recursive algorithm for solving agent is Algorithm for solving I-DID Input : level l ≥ 1 I-ID or level 0 ID, T Expansion Phase 1.",
                "For t from 1 to T − 1 do 2.",
                "If l ≥ 1 then Populate Mt+1 j,l−1 3.",
                "For each mt j in Range(Mt j,l−1) do 4.",
                "Recursively call algorithm with the l − 1 I-ID (or ID) that represents mt j and the horizon, T − t + 1 5.",
                "Map the decision node of the solved I-ID (or ID), OPT(mt j), to a chance node Aj 6.",
                "For each aj in OPT(mt j) do 7.",
                "For each oj in Oj (part of mt j) do 8.",
                "Update js belief, bt+1 j ← SE(bt j, aj, oj) 9. mt+1 j ← New I-ID (or ID) with bt+1 j as the initial belief 10.",
                "Range(Mt+1 j,l−1) ∪ ← {mt+1 j } 11.",
                "Add the model node, Mt+1 j,l−1, and the dependency links between Mt j,l−1 and Mt+1 j,l−1 (shown in Fig. 3(b)) 12.",
                "Add the chance, decision, and utility nodes for t + 1 time slice and the dependency links between them 13.",
                "Establish the CPTs for each chance node and utility node Look-Ahead Phase 14.",
                "Apply the standard look-ahead and backup method to solve the expanded I-DID Figure 5: Algorithm for solving a level l ≥ 0 I-DID. level l I-DID expanded over T time steps with one other agent j in Fig. 5.",
                "We adopt a two-phase approach: Given an I-ID of level l (described previously in Section 3) with all lower level models also represented as I-IDs or IDs (if level 0), the first step is to expand the level l I-ID over T time steps adding the dependency links and the conditional probability tables for each node.",
                "We particularly focus on establishing and populating the model nodes (lines 3-11).",
                "Note that Range(·) returns the values (lower level models) of the random variable given as input (model node).",
                "In the second phase, we use a standard look-ahead technique projecting the action and observation sequences over T time steps in the future, and backing up the utility values of the reachable beliefs.",
                "Similar to I-IDs, the I-DIDs reduce to DIDs in the absence of other agents.",
                "As we mentioned previously, the 0-th level models are the traditional DIDs.",
                "Their solutions provide probability distributions over actions of the agent modeled at that level to I-DIDs at level 1.",
                "Given probability distributions over other agents actions the level 1 IDIDs can themselves be solved as DIDs, and provide probability distributions to yet higher level models.",
                "Assume that the number of models considered at each level is bound by a number, M. Solving an I-DID of level l in then equivalent to solving O(Ml ) DIDs. 5.",
                "EXAMPLE APPLICATIONS To illustrate the usefulness of I-DIDs, we apply them to three problem domains.",
                "We describe, in particular, the formulation of the I-DID and the optimal prescriptions obtained on solving it. 5.1 Followership-Leadership in the Multiagent Tiger Problem We begin our illustrations of using I-IDs and I-DIDs with a slightly modified version of the multiagent tiger problem discussed in [9].",
                "The problem has two agents, each of which can open the right door (OR), the left door (OL) or listen (L).",
                "In addition to hearing growls (from the left (GL) or from the right (GR)) when they listen, the agents also hear creaks (from the left (CL), from the right (CR), or no creaks (S)), which noisily indicate the other agents opening one of the doors.",
                "When any door is opened, the tiger persists in its original location with a probability of 95%.",
                "Agent i hears growls with a reliability of 65% and creaks with a reliability of 95%.",
                "Agent j, on the other hand, hears growls with a reliability of 95%.",
                "Thus, the setting is such that agent i hears agent j opening doors more reliably than the tigers growls.",
                "This suggests that i could use js actions as an indication of the location of the tiger, as we discuss below.",
                "Each agents preferences are as in the single agent game discussed in [13].",
                "The transition, observation, and reward functions are shown in [16].",
                "A good indicator of the usefulness of normative methods for decision-making like I-DIDs is the emergence of realistic social behaviors in their prescriptions.",
                "In settings of the persistent multiagent tiger problem that reflect real world situations, we demonstrate followership between the agents and, as shown in [15], deception among agents who believe that they are in a follower-leader type of relationship.",
                "In particular, we analyze the situational and epistemological conditions sufficient for their emergence.",
                "The followership behavior, for example, results from the agent knowing its own weaknesses, assessing the strengths, preferences, and possible behaviors of the other, and realizing that its best for it to follow the others actions in order to maximize its payoffs.",
                "Let us consider a particular setting of the tiger problem in which agent i believes that js preferences are aligned with its own - both of them just want to get the gold - and js hearing is more reliable in comparison to itself.",
                "As an example, suppose that j, on listening can discern the tigers location 95% of the times compared to is 65% accuracy.",
                "Additionally, agent i does not have any initial information about the tigers location.",
                "In other words, is single-level nested belief, bi,1, assigns 0.5 to each of the two locations of the tiger.",
                "In addition, i considers two models of j, which differ in js flat level 0 initial beliefs.",
                "This is represented in the level 1 I-ID shown in Fig. 6(a).",
                "According to one model, j assigns a probability of 0.9 that the tiger is behind the left door, while the other 818 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Figure 6: (a) Level 1 I-ID of agent i, (b) two level 0 IDs of agent j whose decision nodes are mapped to the chance nodes, A1 j , A2 j , in (a). model assigns 0.1 to that location (see Fig. 6(b)).",
                "Agent i is undecided on these two models of j.",
                "If we vary is hearing ability, and solve the corresponding level 1 I-ID expanded over three time steps, we obtain the normative behavioral policies shown in Fig 7 that exhibit followership behavior.",
                "If is probability of correctly hearing the growls is 0.65, then as shown in the policy in Fig. 7(a), i begins to conditionally follow js actions: i opens the same door that j opened previously iff is own assessment of the tigers location confirms js pick.",
                "If i loses the ability to correctly interpret the growls completely, it blindly follows j and opens the same door that j opened previously (Fig. 7(b)).",
                "Figure 7: Emergence of (a) conditional followership, and (b) blind followership in the tiger problem.",
                "Behaviors of interest are in bold. * is a wildcard, and denotes any one of the observations.",
                "We observed that a single level of belief nesting - beliefs about the others models - was sufficient for followership to emerge in the tiger problem.",
                "However, the epistemological requirements for the emergence of leadership are more complex.",
                "For an agent, say j, to emerge as a leader, followership must first emerge in the other agent i.",
                "As we mentioned previously, if i is certain that its preferences are identical to those of j, and believes that j has a better sense of hearing, i will follow js actions over time.",
                "Agent j emerges as a leader if it believes that i will follow it, which implies that js belief must be nested two levels deep to enable it to recognize its leadership role.",
                "Realizing that i will follow presents j with an opportunity to influence is actions in the benefit of the collective good or its self-interest alone.",
                "For example, in the tiger problem, let us consider a setting in which if both i and j open the correct door, then each gets a payoff of 20 that is double the original.",
                "If j alone selects the correct door, it gets the payoff of 10.",
                "On the other hand, if both agents pick the wrong door, their penalties are cut in half.",
                "In this setting, it is in both js best interest as well as the collective betterment for j to use its expertise in selecting the correct door, and thus be a good leader.",
                "However, consider a slightly different problem in which j gains from is loss and is penalized if i gains.",
                "Specifically, let is payoff be subtracted from js, indicating that j is antagonistic toward i - if j picks the correct door and i the wrong one, then is loss of 100 becomes js gain.",
                "Agent j believes that i incorrectly thinks that js preferences are those that promote the collective good and that it starts off by believing with 99% confidence where the tiger is.",
                "Because i believes that its preferences are similar to those of j, and that j starts by believing almost surely that one of the two is the correct location (two level 0 models of j), i will start by following js actions.",
                "We show is normative policy on solving its singly-nested I-DID over three time steps in Fig. 8(a).",
                "The policy demonstrates that i will blindly follow js actions.",
                "Since the tiger persists in its original location with a probability of 0.95, i will select the same door again.",
                "If j begins the game with a 99% probability that the tiger is on the right, solving js I-DID nested two levels deep, results in the policy shown in Fig. 8(b).",
                "Even though j is almost certain that OL is the correct action, it will start by selecting OR, followed by OL.",
                "Agent js intention is to deceive i who, it believes, will follow js actions, so as to gain $110 in the second time step, which is more than what j would gain if it were to be honest.",
                "Figure 8: Emergence of deception between agents in the tiger problem.",
                "Behaviors of interest are in bold. * denotes as before. (a) Agent is policy demonstrating that it will blindly follow js actions. (b) Even though j is almost certain that the tiger is on the right, it will start by selecting OR, followed by OL, in order to deceive i. 5.2 Altruism and Reciprocity in the Public Good Problem The public good (PG) problem [7], consists of a group of M agents, each of whom must either contribute some resource to a public pot or keep it for themselves.",
                "Since resources contributed to the public pot are shared among all the agents, they are less valuable to the agent when in the public pot.",
                "However, if all agents choose to contribute their resources, then the payoff to each agent is more than if no one contributes.",
                "Since an agent gets its share of the public pot irrespective of whether it has contributed or not, the dominating action is for each agent to not contribute, and instead free ride on others contributions.",
                "However, behaviors of human players in empirical simulations of the PG problem differ from the normative predictions.",
                "The experiments reveal that many players initially contribute a large amount to the public pot, and continue to contribute when the PG problem is played repeatedly, though in decreasing amounts [4].",
                "Many of these experiments [5] report that a small core group of players persistently contributes to the public pot even when all others are defecting.",
                "These experiments also reveal that players who persistently contribute have altruistic or reciprocal preferences matching expected cooperation of others.",
                "For simplicity, we assume that the game is played between M = 2 agents, i and j.",
                "Let each agent be initially endowed with XT amount of resources.",
                "While the classical PG game formulation permits each agent to contribute any quantity of resources (≤ XT ) to the public pot, we simplify the action space by allowing two possible actions.",
                "Each agent may choose to either contribute (C) a fixed amount of the resources, or not contribute.",
                "The latter action is deThe Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 819 noted as defect (D).",
                "We assume that the actions are not observable to others.",
                "The value of resources in the public pot is discounted by ci for each agent i, where ci is the marginal private return.",
                "We assume that ci < 1 so that the agent does not benefit enough that it contributes to the public pot for private gain.",
                "Simultaneously, ciM > 1, making collective contribution pareto optimal. i/j C D C 2ciXT , 2cjXT ciXT − cp, XT + cjXT − P D XT + ciXT − P, cjXT − cp XT , XT Table 1: The one-shot PG game with punishment.",
                "In order to encourage contributions, the contributing agents punish free riders but incur a small cost for administering the punishment.",
                "Let P be the punishment meted out to the defecting agent and cp the non-zero cost of punishing for the contributing agent.",
                "For simplicity, we assume that the cost of punishing is same for both the agents.",
                "The one-shot PG game with punishment is shown in Table. 1.",
                "Let ci = cj, cp > 0, and if P > XT − ciXT , then defection is no longer a dominating action.",
                "If P < XT − ciXT , then defection is the dominating action for both.",
                "If P = XT − ciXT , then the game is not dominance-solvable.",
                "Figure 9: (a) Level 1 I-ID of agent i, (b) level 0 IDs of agent j with decision nodes mapped to the chance nodes, A1 j and A2 j , in (a).",
                "We formulate a sequential version of the PG problem with punishment from the perspective of agent i.",
                "Though in the repeated PG game, the quantity in the public pot is revealed to all the agents after each round of actions, we assume in our formulation that it is hidden from the agents.",
                "Each agent may contribute a fixed amount, xc, or defect.",
                "An agent on performing an action receives an observation of plenty (PY) or meager (MR) symbolizing the state of the public pot.",
                "Notice that the observations are also indirectly indicative of agent js actions because the state of the public pot is influenced by them.",
                "The amount of resources in agent is private pot, is perfectly observable to i.",
                "The payoffs are analogous to Table. 1.",
                "Borrowing from the empirical investigations of the PG problem [5], we construct level 0 IDs for j that model altruistic and non-altruistic types (Fig. 9(b)).",
                "Specifically, our altruistic agent has a high marginal private return (cj is close to 1) and does not punish others who defect.",
                "Let xc = 1 and the level 0 agent be punished half the times it defects.",
                "With one action remaining, both types of agents choose to contribute to avoid being punished.",
                "With two actions to go, the altruistic type chooses to contribute, while the other defects.",
                "This is because cj for the altruistic type is close to 1, thus the expected punishment, 0.5P > (1 − cj), which the altruistic type avoids.",
                "Because cj for the non-altruistic type is less, it prefers not to contribute.",
                "With three steps to go, the altruistic agent contributes to avoid punishment (0.5P > 2(1 − cj)), and the non-altruistic type defects.",
                "For greater than three steps, while the altruistic agent continues to contribute to the public pot depending on how close its marginal private return is to 1, the non-altruistic type prescribes defection.",
                "We analyzed the decisions of an altruistic agent i modeled using a level 1 I-DID expanded over 3 time steps. i ascribes the two level 0 models, mentioned previously, to j (see Fig. 9).",
                "If i believes with a probability 1 that j is altruistic, i chooses to contribute for each of the three steps.",
                "This behavior persists when i is unaware of whether j is altruistic (Fig. 10(a)), and when i assigns a high probability to j being the non-altruistic type.",
                "However, when i believes with a probability 1 that j is non-altruistic and will thus surely defect, i chooses to defect to avoid being punished and because its marginal private return is less than 1.",
                "These results demonstrate that the behavior of our altruistic type resembles that found experimentally.",
                "The non-altruistic level 1 agent chooses to defect regardless of how likely it believes the other agent to be altruistic.",
                "We analyzed the behavior of a reciprocal agent type that matches expected cooperation or defection.",
                "The reciprocal types marginal private return is similar to that of the non-altruistic type, however, it obtains a greater payoff when its action is similar to that of the other.",
                "We consider the case when the reciprocal agent i is unsure of whether j is altruistic and believes that the public pot is likely to be half full.",
                "For this prior belief, i chooses to defect.",
                "On receiving an observation of plenty, i decides to contribute, while an observation of meager makes it defect (Fig. 10(b)).",
                "This is because an observation of plenty signals that the pot is likely to be greater than half full, which results from js action to contribute.",
                "Thus, among the two models ascribed to j, its type is likely to be altruistic making it likely that j will contribute again in the next time step.",
                "Agent i therefore chooses to contribute to reciprocate js action.",
                "An analogous reasoning leads i to defect when it observes a meager pot.",
                "With one action to go, i believing that j contributes, will choose to contribute too to avoid punishment regardless of its observations.",
                "Figure 10: (a) An altruistic level 1 agent always contributes. (b) A reciprocal agent i starts off by defecting followed by choosing to contribute or defect based on its observation of plenty (indicating that j is likely altruistic) or meager (j is non-altruistic). 5.3 Strategies in Two-Player Poker Poker is a popular zero sum card game that has received much attention among the AI research community as a testbed [2].",
                "Poker is played among M ≥ 2 players in which each player receives a hand of cards from a deck.",
                "While several flavors of Poker with varying complexity exist, we consider a simple version in which each player has three plys during which the player may either exchange a card (E), keep the existing hand (K), fold (F) and withdraw from the game, or call (C), requiring all players to show their hands.",
                "To keep matters simple, let M = 2, and each player receive a hand consisting of a single card drawn from the same suit.",
                "Thus, during a showdown, the player who has the numerically larger card (2 is the lowest, ace is the highest) wins the pot.",
                "During an exchange of cards, the discarded card is placed either in the L pile, indicating to the other agent that it was a low numbered card less than 8, or in the 820 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) H pile, indicating that the card had a rank greater than or equal to 8.",
                "Notice that, for example, if a lower numbered card is discarded, the probability of receiving a low card in exchange is now reduced.",
                "We show the level 1 I-ID for the simplified two-player Poker in Fig. 11.",
                "We considered two models (personality types) of agent j.",
                "The conservative type believes that it is likely that its opponent has a high numbered card in its hand.",
                "On the other hand, the aggressive agent j believes with a high probability that its opponent has a lower numbered card.",
                "Thus, the two types differ in their beliefs over their opponents hand.",
                "In both these level 0 models, the opponent is assumed to perform its actions following a fixed, uniform distribution.",
                "With three actions to go, regardless of its hand (unless it is an ace), the aggressive agent chooses to exchange its card, with the intent of improving on its current hand.",
                "This is because it believes the other to have a low card, which improves its chances of getting a high card during the exchange.",
                "The conservative agent chooses to keep its card, no matter its hand because its chances of getting a high card are slim as it believes that its opponent has one.",
                "Figure 11: (a) Level 1 I-ID of agent i.",
                "The observation reveals information about js hand of the previous time step, (b) level 0 IDs of agent j whose decision nodes are mapped to the chance nodes, A1 j , A2 j , in (a).",
                "The policy of a level 1 agent i who believes that each card except its own has an equal likelihood of being in js hand (neutral personality type) and j could be either an aggressive or conservative type, is shown in Fig. 12. is own hand contains the card numbered 8.",
                "The agent starts by keeping its card.",
                "On seeing that j did not exchange a card (N), i believes with probability 1 that j is conservative and hence will keep its cards. i responds by either keeping its card or exchanging it because j is equally likely to have a lower or higher card.",
                "If i observes that j discarded its card into the L or H pile, i believes that j is aggressive.",
                "On observing L, i realizes that j had a low card, and is likely to have a high card after its exchange.",
                "Because the probability of receiving a low card is high now, i chooses to keep its card.",
                "On observing H, believing that the probability of receiving a high numbered card is high, i chooses to exchange its card.",
                "In the final step, i chooses to call regardless of its observation history because its belief that j has a higher card is not sufficiently high to conclude that its better to fold and relinquish the payoff.",
                "This is partly due to the fact that an observation of, say, L resets the agent is previous time step beliefs over js hand to the low numbered cards only. 6.",
                "DISCUSSION We showed how DIDs may be extended to I-DIDs that enable online sequential decision-making in uncertain multiagent settings.",
                "Our graphical representation of I-DIDs improves on the previous Figure 12: A level 1 agent is three step policy in the Poker problem. i starts by believing that j is equally likely to be aggressive or conservative and could have any card in its hand with equal probability. work significantly by being more transparent, semantically clear, and capable of being solved using standard algorithms that target DIDs.",
                "I-DIDs extend NIDs to allow sequential decision-making over multiple time steps in the presence of other interacting agents.",
                "I-DIDs may be seen as concise graphical representations for IPOMDPs providing a way to exploit problem structure and carry out online decision-making as the agent acts and observes given its prior beliefs.",
                "We are currently investigating ways to solve I-DIDs approximately with provable bounds on the solution quality.",
                "Acknowledgment: We thank Piotr Gmytrasiewicz for some useful discussions related to this work.",
                "The first author would like to acknowledge the support of a UGARF grant. 7.",
                "REFERENCES [1] R. J. Aumann.",
                "Interactive epistemology i: Knowledge.",
                "International Journal of Game Theory, 28:263-300, 1999. [2] D. Billings, A. Davidson, J. Schaeffer, and D. Szafron.",
                "The challenge of poker.",
                "AIJ, 2001. [3] A. Brandenburger and E. Dekel.",
                "Hierarchies of beliefs and common knowledge.",
                "Journal of Economic Theory, 59:189-198, 1993. [4] C. Camerer.",
                "Behavioral Game Theory: Experiments in Strategic Interaction.",
                "Princeton University Press, 2003. [5] E. Fehr and S. Gachter.",
                "Cooperation and punishment in public goods experiments.",
                "American Economic Review, 90(4):980-994, 2000. [6] D. Fudenberg and D. K. Levine.",
                "The Theory of Learning in Games.",
                "MIT Press, 1998. [7] D. Fudenberg and J. Tirole.",
                "Game Theory.",
                "MIT Press, 1991. [8] Y. Gal and A. Pfeffer.",
                "A language for modeling agents decision-making processes in games.",
                "In AAMAS, 2003. [9] P. Gmytrasiewicz and P. Doshi.",
                "A framework for sequential planning in multiagent settings.",
                "JAIR, 24:49-79, 2005. [10] P. Gmytrasiewicz and E. Durfee.",
                "Rational coordination in multi-agent environments.",
                "JAAMAS, 3(4):319-350, 2000. [11] J. C. Harsanyi.",
                "Games with incomplete information played by bayesian players.",
                "Management Science, 14(3):159-182, 1967. [12] R. A. Howard and J. E. Matheson.",
                "Influence diagrams.",
                "In R. A. Howard and J. E. Matheson, editors, The Principles and Applications of Decision Analysis.",
                "Strategic Decisions Group, Menlo Park, CA 94025, 1984. [13] L. Kaelbling, M. Littman, and A. Cassandra.",
                "Planning and acting in partially observable stochastic domains.",
                "Artificial Intelligence Journal, 2, 1998. [14] D. Koller and B. Milch.",
                "Multi-agent <br>influence diagram</br>s for representing and solving games.",
                "In IJCAI, pages 1027-1034, 2001. [15] K. Polich and P. Gmytrasiewicz.",
                "Interactive dynamic <br>influence diagram</br>s.",
                "In GTDT Workshop, AAMAS, 2006. [16] B.",
                "Rathnas., P. Doshi, and P. J. Gmytrasiewicz.",
                "Exact solutions to interactive pomdps using behavioral equivalence.",
                "In Autonomous Agents and Multi-Agent Systems Conference (AAMAS), 2006. [17] S. Russell and P. Norvig.",
                "Artificial Intelligence: A Modern Approach (Second Edition).",
                "Prentice Hall, 2003. [18] R. D. Shachter.",
                "Evaluating <br>influence diagram</br>s.",
                "Operations Research, 34(6):871-882, 1986. [19] D. Suryadi and P. Gmytrasiewicz.",
                "Learning models of other agents using <br>influence diagram</br>s.",
                "In UM, 1999.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 821"
            ],
            "original_annotated_samples": [
                "These graphical models called interactive dynamic <br>influence diagram</br>s (I-DIDs) seek to explicitly model the structure that is often present in real-world problems by decomposing the situation into chance and decision variables, and the dependencies between the variables.",
                "In [15], Polich and Gmytrasiewicz introduced interactive dynamic <br>influence diagram</br>s (I-DIDs) as the computational representations of I-POMDPs.",
                "I-DIDs contribute to a growing line of work [19] that includes multi-agent <br>influence diagram</br>s (MAIDs) [14], and more recently, networks of <br>influence diagram</br>s (NIDs) [8].",
                "INTERACTIVEINFLUENCEDIAGRAMS A naive extension of <br>influence diagram</br>s (IDs) to settings populated by multiple agents is possible by treating other agents as automatons, represented using chance nodes.",
                "Interactive <br>influence diagram</br>s (I-IDs) adopt a more sophisticated approach by generalizing IDs to make them applicable to settings shared with other agents who may act and observe, and update their beliefs. 3.1 Syntax In addition to the usual chance, decision, and utility nodes, IIDs include a new type of node called the model node."
            ],
            "translated_annotated_samples": [
                "Estos modelos gráficos llamados <br>diagramas de influencia</br> dinámica interactiva (I-DIDs) buscan modelar explícitamente la estructura que a menudo está presente en problemas del mundo real al descomponer la situación en variables de probabilidad y decisión, y las dependencias entre las variables.",
                "En [15], Polich y Gmytrasiewicz introdujeron <br>diagramas de influencia</br> dinámica interactivos (I-DIDs) como las representaciones computacionales de I-POMDPs.",
                "Los I-DIDs contribuyen a una creciente línea de trabajo [19] que incluye <br>diagramas de influencia</br> multiagente (MAIDs) [14], y más recientemente, redes de <br>diagramas de influencia</br> (NIDs) [8].",
                "Diagramas de influencia interactivos. Una extensión ingenua de los <br>diagramas de influencia</br> (IDs) a entornos poblados por múltiples agentes es posible tratando a los otros agentes como autómatas, representados mediante nodos de probabilidad.",
                "Los <br>diagramas de influencia</br> interactivos (I-IDs) adoptan un enfoque más sofisticado al generalizar los IDs para hacerlos aplicables a entornos compartidos con otros agentes que pueden actuar, observar y actualizar sus creencias. 3.1 Sintaxis Además de los nodos habituales de probabilidad, decisión y utilidad, los IIDs incluyen un nuevo tipo de nodo llamado nodo de modelo."
            ],
            "translated_text": "Modelos gráficos para soluciones en línea a POMDP interactivos de Prashant Doshi Dept. Departamento de Ciencias de la Computación Universidad de Georgia Athens, GA 30602, EE. UU. pdoshi@cs.uga.edu Yifeng Zeng Dept. de Ciencias de la Computación Universidad de Aalborg DK-9220 Aalborg, Dinamarca yfzeng@cs.aau.edu Qiongyu Chen Dept. de Ciencias de la Computación Universidad Nacional de Singapur 117543, Singapur chenqy@comp.nus.edu.sg RESUMEN Desarrollamos una nueva representación gráfica para procesos de decisión de Markov parcialmente observables interactivos (I-POMDPs) que es significativamente más transparente y semánticamente clara que la representación anterior. Estos modelos gráficos llamados <br>diagramas de influencia</br> dinámica interactiva (I-DIDs) buscan modelar explícitamente la estructura que a menudo está presente en problemas del mundo real al descomponer la situación en variables de probabilidad y decisión, y las dependencias entre las variables. Los I-DIDs generalizan los DIDs, los cuales pueden ser vistos como representaciones gráficas de POMDPs, a entornos multiagentes de la misma manera que los I-POMDPs generalizan los POMDPs. Los I-DIDs pueden ser utilizados para calcular la política de un agente en línea mientras el agente actúa y observa en un entorno poblado por otros agentes que interactúan entre sí. Usando varios ejemplos, mostramos cómo se pueden aplicar los I-DIDs y demostramos su utilidad. Categorías y Descriptores de Asignaturas I.2.11 [Inteligencia Artificial Distribuida]: Sistemas Multiagentes Términos Generales Teoría 1. Las Procesos de Decisión de Markov Interactivos Parcialmente Observables (IPOMDPs) proporcionan un marco para la toma de decisiones secuenciales en entornos multiagentes parcialmente observables. Generalizan los POMDPs [13] a entornos multiagentes al incluir los modelos computables de los otros agentes en el espacio de estados junto con los estados del entorno físico. Los modelos abarcan toda la información que influye en los comportamientos de los agentes, incluyendo sus preferencias, capacidades y creencias, y por lo tanto son análogos a los tipos en los juegos bayesianos [11]. I-POMDPs adoptan un enfoque subjetivo para comprender el comportamiento estratégico, arraigado en un marco de teoría de decisiones que toma la perspectiva de los tomadores de decisiones en la interacción. En [15], Polich y Gmytrasiewicz introdujeron <br>diagramas de influencia</br> dinámica interactivos (I-DIDs) como las representaciones computacionales de I-POMDPs. Los I-DIDs generalizan los DIDs [12], los cuales pueden ser vistos como contrapartes computacionales de POMDPs, a entornos de múltiples agentes de la misma manera que los I-POMDPs generalizan los POMDPs. Los I-DIDs contribuyen a una creciente línea de trabajo [19] que incluye <br>diagramas de influencia</br> multiagente (MAIDs) [14], y más recientemente, redes de <br>diagramas de influencia</br> (NIDs) [8]. Estos formalismos buscan modelar explícitamente la estructura que a menudo está presente en problemas del mundo real al descomponer la situación en variables de azar y decisiones, y las dependencias entre las variables. Los MAIDs proporcionan una alternativa a las formas normales y extensivas de juego utilizando un formalismo gráfico para representar juegos de información imperfecta con un nodo de decisión para las acciones de cada agente y nodos de azar que capturan la información privada de los agentes. Los MAIDs analizan objetivamente el juego, calculando eficientemente el perfil de equilibrio de Nash al explotar la estructura de independencia. NIDs extienden los MAIDs para incluir la incertidumbre de los agentes sobre el juego que se está jugando y sobre los modelos de los otros agentes. Cada modelo es un MAID y la red de MAIDs se colapsa, de abajo hacia arriba, en un solo MAID para calcular el equilibrio del juego teniendo en cuenta los diferentes modelos de cada agente. Los formalismos gráficos como MAIDs y NIDs abren una área de investigación prometedora que tiene como objetivo representar las interacciones multiagente de manera más transparente. Sin embargo, los MAIDs proporcionan un análisis del juego desde un punto de vista externo y la aplicabilidad de ambos está limitada a juegos estáticos de un solo jugador. Los asuntos son más complejos cuando consideramos interacciones que se extienden en el tiempo, donde las predicciones sobre las acciones futuras de otros deben hacerse utilizando modelos que cambian a medida que los agentes actúan y observan. Los I-DIDs abordan esta brecha al permitir la representación de modelos de otros agentes como los valores de un nodo de modelo especial. Tanto los modelos de otros agentes como las creencias originales de los agentes sobre estos modelos se actualizan con el tiempo utilizando implementaciones especializadas. En este documento, mejoramos la representación preliminar previa del I-DID mostrada en [15] utilizando la idea de que el I-ID estático es un tipo de NID. Por lo tanto, podemos utilizar construcciones de lenguaje específicas de NID, como multiplexores, para representar de manera más transparente el nodo del modelo y, posteriormente, el I-ID. Además, aclaramos la semántica del enlace de política de propósito especial introducido en la representación de I-DID por [15], y mostramos que podría ser reemplazado por enlaces de dependencia tradicionales. En la representación previa del I-DID, la actualización de la creencia de los agentes sobre los modelos de los demás a medida que los agentes actúan y reciben observaciones se denotaba utilizando un enlace especial llamado el enlace de actualización del modelo que conectaba los nodos del modelo a lo largo del tiempo. Explicamos la semántica de este enlace mostrando cómo puede ser implementado utilizando los enlaces de dependencia tradicionales entre los nodos de probabilidad que constituyen los nodos del modelo. El resultado final es una representación de I-DID que es significativamente más transparente, semánticamente clara y capaz de ser implementada utilizando los algoritmos estándar para resolver DIDs. Mostramos cómo los IDIDs pueden ser utilizados para modelar la incertidumbre de un agente sobre los modelos de otros, que a su vez pueden ser I-DIDs. La solución al I-DID es una política que prescribe lo que el agente debe hacer con el tiempo, dadas sus creencias sobre el estado físico y otros modelos. Análogos a los DIDs, los I-DIDs pueden ser utilizados para calcular la política de un agente en línea mientras el agente actúa y observa en un entorno poblado por otros agentes interactuantes. 2. ANTECEDENTES: Los IPOMDPS ANIDADOS FINITAMENTE generalizan los POMDPs a entornos multiagentes al incluir los modelos de otros agentes como parte del espacio de estados [9]. Dado que otros agentes también pueden razonar sobre otros, el espacio de estado interactivo está estratégicamente anidado; contiene creencias sobre los modelos de otros agentes y sus creencias sobre los demás. Para simplificar la presentación, consideramos un agente, i, que está interactuando con otro agente, j. Un I-POMDP finitamente anidado del agente i con un nivel de estrategia l se define como la tupla: I-POMDPi,l = ISi,l, A, Ti, Ωi, Oi, Ri donde: • ISi,l denota un conjunto de estados interactivos definido como, ISi,l = S × Mj,l−1, donde Mj,l−1 = {Θj,l−1 ∪ SMj}, para l ≥ 1, e ISi,0 = S, donde S es el conjunto de estados del entorno físico. Θj,l−1 es el conjunto de modelos intencionales computables del agente j: θj,l−1 = bj,l−1, ˆθj donde el marco, ˆθj = A, Ωj, Tj, Oj, Rj, OCj. Aquí, j es Bayesiano racional y OCj es el criterio de optimalidad de j. SMj es el conjunto de modelos subintencionales de j. Ejemplos simples de modelos subintencionales incluyen un modelo sin información [10] y un modelo de juego ficticio [6], ambos de los cuales son independientes de la historia. Damos una construcción recursiva de abajo hacia arriba del espacio de estados interactivo a continuación. Sí,0 = S, Θj,0 = { bj,0, ˆθj | bj,0 ∈ Δ(ISj,0)} Sí,1 = S × {Θj,0 ∪ SMj}, Θj,1 = { bj,1, ˆθj | bj,1 ∈ Δ(ISj,1)} . . . . . . Sí, l = S × {Θj,l−1 ∪ SMj}, Θj,l = { bj,l, ˆθj | bj,l ∈ Δ(ISj,l)} Formulaciones similares de espacios anidados han aparecido en [1, 3]. • A = Ai × Aj es el conjunto de acciones conjuntas de todos los agentes en el entorno; • Ti : S ×A×S → [0, 1], describe el efecto de las acciones conjuntas en los estados físicos del entorno; • Ωi es el conjunto de observaciones del agente i; • Oi : S × A × Ωi → [0, 1] da la probabilidad de las observaciones dadas el estado físico y la acción conjunta; • Ri : ISi × A → R describe las preferencias del agente sobre sus estados interactivos. Normalmente solo importarán los estados físicos. La política del agente es el mapeo, Ω∗ i → Δ(Ai), donde Ω∗ i es el conjunto de todos los historiales de observación del agente i. Dado que la creencia sobre los estados interactivos forma una estadística suficiente [9], la política también puede ser representada como un mapeo del conjunto de todas las creencias del agente i a una distribución sobre sus acciones, Δ(ISi) → Δ(Ai). 2.1 Actualización de Creencias Análogo a los POMDPs, un agente dentro del marco I-POMDP actualiza su creencia a medida que actúa y observa. Sin embargo, hay dos diferencias que complican la actualización de creencias en entornos multiagentes en comparación con los entornos de un solo agente. Primero, dado que el estado del entorno físico depende de las acciones de ambos agentes, la predicción de cómo cambia el estado físico debe hacerse basándose en la predicción de sus acciones. Segundo, los cambios en los modelos de js deben ser incluidos en la actualización de creencias. Específicamente, si j es intencional, entonces se debe incluir una actualización de las creencias de j debido a su acción y observación. En otras palabras, i tiene que actualizar su creencia basándose en su predicción de lo que j observaría y cómo j actualizaría su creencia. Si el modelo de js es subintencional, entonces las observaciones probables de js se añaden al historial de observaciones contenido en el modelo. Formalmente, tenemos: Pr(ist |at−1 i , bt−1 i,l ) = β ISt−1:mt−1 j =θt j bt−1 i,l (ist−1 ) × at−1 j Pr(at−1 j |θt−1 j,l−1)Oi(st , at−1 i , at−1 j , ot i) ×Ti(st−1 , at−1 i , at−1 j , st ) ot j Oj(st , at−1 i , at−1 j , ot j) ×τ(SEθt j (bt−1 j,l−1, at−1 j , ot j) − bt j,l−1) (1) donde β es la constante de normalización, τ es 1 si su argumento es 0, de lo contrario es 0, Pr(at−1 j |θt−1 j,l−1) es la probabilidad de que at−1 j sea Bayes racional para el agente descrito por el modelo θt−1 j,l−1, y SE(·) es una abreviatura para la actualización de creencias. Para una versión de la actualización de creencias cuando el modelo js es subintencional, consulte [9]. Si el agente j también se modela como un I-POMDP, entonces la actualización de creencias invoca la actualización de creencias de j (a través del término SEθt j ( bt−1 j,l−1 , at−1 j , ot j)), lo que a su vez podría invocar la actualización de creencias de is y así sucesivamente. Esta recursión en la anidación de creencias llega al nivel 0. En este nivel, la actualización de creencias del agente se reduce a una actualización de creencias de un POMDP. Para ilustraciones de la actualización de creencias, detalles adicionales sobre I-POMDPs y cómo se comparan con otros marcos multiagentes, consulte [9]. Iteración de Valor Cada estado de creencia en un I-POMDP finitamente anidado tiene un valor asociado que refleja la máxima ganancia que el agente puede esperar en este estado de creencia: Un ( bi,l, θi ) = max ai∈Ai is∈ISi,l ERi(is, ai)bi,l(is)+ γ oi∈Ωi Pr(oi|ai, bi,l)Un−1 ( SEθi (bi,l, ai, oi), θi ) (2) donde, ERi(is, ai) = aj Ri(is, ai, aj)Pr(aj|mj,l−1) (ya que is = (s, mj,l−1)). La ecuación 2 es una base para la iteración de valor en I-POMDPs. La acción óptima del agente, a∗ i, para el caso de horizonte finito con descuento, es un elemento del conjunto de acciones óptimas para el estado de creencia, OPT(θi), definido como: OPT( bi,l, θi ) = argmax ai∈Ai is∈ISi,l ERi(is, ai)bi,l(is) +γ oi∈Ωi Pr(oi|ai, bi,l)Un ( SEθi (bi,l, ai, oi), θi ) (3) 3. Diagramas de influencia interactivos. Una extensión ingenua de los <br>diagramas de influencia</br> (IDs) a entornos poblados por múltiples agentes es posible tratando a los otros agentes como autómatas, representados mediante nodos de probabilidad. Sin embargo, este enfoque asume que las acciones de los agentes están controladas utilizando una distribución de probabilidad que no cambia con el tiempo. Los <br>diagramas de influencia</br> interactivos (I-IDs) adoptan un enfoque más sofisticado al generalizar los IDs para hacerlos aplicables a entornos compartidos con otros agentes que pueden actuar, observar y actualizar sus creencias. 3.1 Sintaxis Además de los nodos habituales de probabilidad, decisión y utilidad, los IIDs incluyen un nuevo tipo de nodo llamado nodo de modelo. ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "interactive influence diagram": {
            "translated_key": "diagrama de influencia interactivo",
            "is_in_text": false,
            "original_annotated_sentences": [
                "Graphical Models for Online Solutions to Interactive POMDPs Prashant Doshi Dept.",
                "of Computer Science University of Georgia Athens, GA 30602, USA pdoshi@cs.uga.edu Yifeng Zeng Dept. of Computer Science Aalborg University DK-9220 Aalborg, Denmark yfzeng@cs.aau.edu Qiongyu Chen Dept. of Computer Science National Univ. of Singapore 117543, Singapore chenqy@comp.nus.edu.sg ABSTRACT We develop a new graphical representation for interactive partially observable Markov decision processes (I-POMDPs) that is significantly more transparent and semantically clear than the previous representation.",
                "These graphical models called interactive dynamic influence diagrams (I-DIDs) seek to explicitly model the structure that is often present in real-world problems by decomposing the situation into chance and decision variables, and the dependencies between the variables.",
                "I-DIDs generalize DIDs, which may be viewed as graphical representations of POMDPs, to multiagent settings in the same way that I-POMDPs generalize POMDPs.",
                "I-DIDs may be used to compute the policy of an agent online as the agent acts and observes in a setting that is populated by other interacting agents.",
                "Using several examples, we show how I-DIDs may be applied and demonstrate their usefulness.",
                "Categories and Subject Descriptors I.2.11 [Distributed Artificial Intelligence]: Multiagent Systems General Terms Theory 1.",
                "INTRODUCTION Interactive partially observable Markov decision processes (IPOMDPs) [9] provide a framework for sequential decision-making in partially observable multiagent environments.",
                "They generalize POMDPs [13] to multiagent settings by including the other agents computable models in the state space along with the states of the physical environment.",
                "The models encompass all information influencing the agents behaviors, including their preferences, capabilities, and beliefs, and are thus analogous to types in Bayesian games [11].",
                "I-POMDPs adopt a subjective approach to understanding strategic behavior, rooted in a decision-theoretic framework that takes a decision-makers perspective in the interaction.",
                "In [15], Polich and Gmytrasiewicz introduced interactive dynamic influence diagrams (I-DIDs) as the computational representations of I-POMDPs.",
                "I-DIDs generalize DIDs [12], which may be viewed as computational counterparts of POMDPs, to multiagents settings in the same way that I-POMDPs generalize POMDPs.",
                "I-DIDs contribute to a growing line of work [19] that includes multi-agent influence diagrams (MAIDs) [14], and more recently, networks of influence diagrams (NIDs) [8].",
                "These formalisms seek to explicitly model the structure that is often present in real-world problems by decomposing the situation into chance and decision variables, and the dependencies between the variables.",
                "MAIDs provide an alternative to normal and extensive game forms using a graphical formalism to represent games of imperfect information with a decision node for each agents actions and chance nodes capturing the agents private information.",
                "MAIDs objectively analyze the game, efficiently computing the Nash equilibrium profile by exploiting the independence structure.",
                "NIDs extend MAIDs to include agents uncertainty over the game being played and over models of the other agents.",
                "Each model is a MAID and the network of MAIDs is collapsed, bottom up, into a single MAID for computing the equilibrium of the game keeping in mind the different models of each agent.",
                "Graphical formalisms such as MAIDs and NIDs open up a promising area of research that aims to represent multiagent interactions more transparently.",
                "However, MAIDs provide an analysis of the game from an external viewpoint and the applicability of both is limited to static single play games.",
                "Matters are more complex when we consider interactions that are extended over time, where predictions about others future actions must be made using models that change as the agents act and observe.",
                "I-DIDs address this gap by allowing the representation of other agents models as the values of a special model node.",
                "Both, other agents models and the original agents beliefs over these models are updated over time using special-purpose implementations.",
                "In this paper, we improve on the previous preliminary representation of the I-DID shown in [15] by using the insight that the static I-ID is a type of NID.",
                "Thus, we may utilize NID-specific language constructs such as multiplexers to represent the model node, and subsequently the I-ID, more transparently.",
                "Furthermore, we clarify the semantics of the special purpose policy link introduced in the representation of I-DID by [15], and show that it could be replaced by traditional dependency links.",
                "In the previous representation of the I-DID, the update of the agents belief over the models of others as the agents act and receive observations was denoted using a special link called the model update link that connected the model nodes over time.",
                "We explicate the semantics of this link by showing how it can be implemented using the traditional dependency links between the chance nodes that constitute the model nodes.",
                "The net result is a representation of I-DID that is significantly more transparent, semantically clear, and capable of being implemented using the standard algorithms for solving DIDs.",
                "We show how IDIDs may be used to model an agents uncertainty over others models, that may themselves be I-DIDs.",
                "Solution to the I-DID is a policy that prescribes what the agent should do over time, given its beliefs over the physical state and others models.",
                "Analogous to DIDs, I-DIDs may be used to compute the policy of an agent online as the agent acts and observes in a setting that is populated by other interacting agents. 2.",
                "BACKGROUND: FINITELY NESTED IPOMDPS Interactive POMDPs generalize POMDPs to multiagent settings by including other agents models as part of the state space [9].",
                "Since other agents may also reason about others, the interactive state space is strategically nested; it contains beliefs about other agents models and their beliefs about others.",
                "For simplicity of presentation we consider an agent, i, that is interacting with one other agent, j.",
                "A finitely nested I-POMDP of agent i with a strategy level l is defined as the tuple: I-POMDPi,l = ISi,l, A, Ti, Ωi, Oi, Ri where: • ISi,l denotes a set of interactive states defined as, ISi,l = S × Mj,l−1, where Mj,l−1 = {Θj,l−1 ∪ SMj}, for l ≥ 1, and ISi,0 = S, where S is the set of states of the physical environment.",
                "Θj,l−1 is the set of computable intentional models of agent j: θj,l−1 = bj,l−1, ˆθj where the frame, ˆθj = A, Ωj, Tj, Oj, Rj, OCj .",
                "Here, j is Bayes rational and OCj is js optimality criterion.",
                "SMj is the set of subintentional models of j.",
                "Simple examples of subintentional models include a no-information model [10] and a fictitious play model [6], both of which are history independent.",
                "We give a recursive bottom-up construction of the interactive state space below.",
                "ISi,0 = S, Θj,0 = { bj,0, ˆθj | bj,0 ∈ Δ(ISj,0)} ISi,1 = S × {Θj,0 ∪ SMj}, Θj,1 = { bj,1, ˆθj | bj,1 ∈ Δ(ISj,1)} . . . . . .",
                "ISi,l = S × {Θj,l−1 ∪ SMj}, Θj,l = { bj,l, ˆθj | bj,l ∈ Δ(ISj,l)} Similar formulations of nested spaces have appeared in [1, 3]. • A = Ai × Aj is the set of joint actions of all agents in the environment; • Ti : S ×A×S → [0, 1], describes the effect of the joint actions on the physical states of the environment; • Ωi is the set of observations of agent i; • Oi : S × A × Ωi → [0, 1] gives the likelihood of the observations given the physical state and joint action; • Ri : ISi × A → R describes agent is preferences over its interactive states.",
                "Usually only the physical states will matter.",
                "Agent is policy is the mapping, Ω∗ i → Δ(Ai), where Ω∗ i is the set of all observation histories of agent i.",
                "Since belief over the interactive states forms a sufficient statistic [9], the policy can also be represented as a mapping from the set of all beliefs of agent i to a distribution over its actions, Δ(ISi) → Δ(Ai). 2.1 Belief Update Analogous to POMDPs, an agent within the I-POMDP framework updates its belief as it acts and observes.",
                "However, there are two differences that complicate the belief update in multiagent settings when compared to single agent ones.",
                "First, since the state of the physical environment depends on the actions of both agents, is prediction of how the physical state changes has to be made based on its prediction of js actions.",
                "Second, changes in js models have to be included in is belief update.",
                "Specifically, if j is intentional then an update of js beliefs due to its action and observation has to be included.",
                "In other words, i has to update its belief based on its prediction of what j would observe and how j would update its belief.",
                "If js model is subintentional, then js probable observations are appended to the observation history contained in the model.",
                "Formally, we have: Pr(ist |at−1 i , bt−1 i,l ) = β ISt−1:mt−1 j =θt j bt−1 i,l (ist−1 ) × at−1 j Pr(at−1 j |θt−1 j,l−1)Oi(st , at−1 i , at−1 j , ot i) ×Ti(st−1 , at−1 i , at−1 j , st ) ot j Oj(st , at−1 i , at−1 j , ot j) ×τ(SEθt j (bt−1 j,l−1, at−1 j , ot j) − bt j,l−1) (1) where β is the normalizing constant, τ is 1 if its argument is 0 otherwise it is 0, Pr(at−1 j |θt−1 j,l−1) is the probability that at−1 j is Bayes rational for the agent described by model θt−1 j,l−1, and SE(·) is an abbreviation for the belief update.",
                "For a version of the belief update when js model is subintentional, see [9].",
                "If agent j is also modeled as an I-POMDP, then is belief update invokes js belief update (via the term SEθt j ( bt−1 j,l−1 , at−1 j , ot j)), which in turn could invoke is belief update and so on.",
                "This recursion in belief nesting bottoms out at the 0th level.",
                "At this level, the belief update of the agent reduces to a POMDP belief update. 1 For illustrations of the belief update, additional details on I-POMDPs, and how they compare with other multiagent frameworks, see [9]. 2.2 Value Iteration Each belief state in a finitely nested I-POMDP has an associated value reflecting the maximum payoff the agent can expect in this belief state: Un ( bi,l, θi ) = max ai∈Ai is∈ISi,l ERi(is, ai)bi,l(is)+ γ oi∈Ωi Pr(oi|ai, bi,l)Un−1 ( SEθi (bi,l, ai, oi), θi ) (2) where, ERi(is, ai) = aj Ri(is, ai, aj)Pr(aj|mj,l−1) (since is = (s, mj,l−1)).",
                "Eq. 2 is a basis for value iteration in I-POMDPs.",
                "Agent is optimal action, a∗ i , for the case of finite horizon with discounting, is an element of the set of optimal actions for the belief state, OPT(θi), defined as: OPT( bi,l, θi ) = argmax ai∈Ai is∈ISi,l ERi(is, ai)bi,l(is) +γ oi∈Ωi Pr(oi|ai, bi,l)Un ( SEθi (bi,l, ai, oi), θi ) (3) 3.",
                "INTERACTIVEINFLUENCEDIAGRAMS A naive extension of influence diagrams (IDs) to settings populated by multiple agents is possible by treating other agents as automatons, represented using chance nodes.",
                "However, this approach assumes that the agents actions are controlled using a probability distribution that does not change over time.",
                "Interactive influence diagrams (I-IDs) adopt a more sophisticated approach by generalizing IDs to make them applicable to settings shared with other agents who may act and observe, and update their beliefs. 3.1 Syntax In addition to the usual chance, decision, and utility nodes, IIDs include a new type of node called the model node.",
                "We show a general level l I-ID in Fig. 1(a), where the model node (Mj,l−1) is denoted using a hexagon.",
                "We note that the probability distribution over the chance node, S, and the model node together represents agent is belief over its interactive states.",
                "In addition to the model 1 The 0th level model is a POMDP: Other agents actions are treated as exogenous events and folded into the T, O, and R functions.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 815 Figure 1: (a) A generic level l I-ID for agent i situated with one other agent j.",
                "The hexagon is the model node (Mj,l−1) whose structure we show in (b).",
                "Members of the model node are I-IDs themselves (m1 j,l−1, m2 j,l−1; diagrams not shown here for simplicity) whose decision nodes are mapped to the corresponding chance nodes (A1 j , A2 j ).",
                "Depending on the value of the node, Mod[Mj], the distribution of each of the chance nodes is assigned to the node Aj. (c) The transformed I-ID with the model node replaced by the chance nodes and the relationships between them. node, I-IDs differ from IDs by having a dashed link (called the policy link in [15]) between the model node and a chance node, Aj, that represents the distribution over the other agents actions given its model.",
                "In the absence of other agents, the model node and the chance node, Aj, vanish and I-IDs collapse into traditional IDs.",
                "The model node contains the alternative computational models ascribed by i to the other agent from the set, Θj,l−1 ∪ SMj, where Θj,l−1 and SMj were defined previously in Section 2.",
                "Thus, a model in the model node may itself be an I-ID or ID, and the recursion terminates when a model is an ID or subintentional.",
                "Because the model node contains the alternative models of the other agent as its values, its representation is not trivial.",
                "In particular, some of the models within the node are I-IDs that when solved generate the agents optimal policy in their decision nodes.",
                "Each decision node is mapped to the corresponding chance node, say A1 j , in the following way: if OPT is the set of optimal actions obtained by solving the I-ID (or ID), then Pr(aj ∈ A1 j ) = 1 |OP T | if aj ∈ OPT, 0 otherwise.",
                "Borrowing insights from previous work [8], we observe that the model node and the dashed policy link that connects it to the chance node, Aj, could be represented as shown in Fig. 1(b).",
                "The decision node of each level l − 1 I-ID is transformed into a chance node, as we mentioned previously, so that the actions with the largest value in the decision node are assigned uniform probabilities in the chance node while the rest are assigned zero probability.",
                "The different chance nodes (A1 j , A2 j ), one for each model, and additionally, the chance node labeled Mod[Mj] form the parents of the chance node, Aj.",
                "Thus, there are as many action nodes (A1 j , A2 j ) in Mj,l−1 as the number of models in the support of agent is beliefs.",
                "The conditional probability table of the chance node, Aj, is a multiplexer that assumes the distribution of each of the action nodes (A1 j , A2 j ) depending on the value of Mod[Mj].",
                "The values of Mod[Mj] denote the different models of j.",
                "In other words, when Mod[Mj] has the value m1 j,l−1, the chance node Aj assumes the distribution of the node A1 j , and Aj assumes the distribution of A2 j when Mod[Mj] has the value m2 j,l−1.",
                "The distribution over the node, Mod[Mj], is the agent is belief over the models of j given a physical state.",
                "For more agents, we will have as many model nodes as there are agents.",
                "Notice that Fig. 1(b) clarifies the semantics of the policy link, and shows how it can be represented using the traditional dependency links.",
                "In Fig. 1(c), we show the transformed I-ID when the model node is replaced by the chance nodes and relationships between them.",
                "In contrast to the representation in [15], there are no special-purpose policy links, rather the I-ID is composed of only those types of nodes that are found in traditional IDs and dependency relationships between the nodes.",
                "This allows I-IDs to be represented and implemented using conventional application tools that target IDs.",
                "Note that we may view the level l I-ID as a NID.",
                "Specifically, each of the level l − 1 models within the model node are blocks in the NID (see Fig. 2).",
                "If the level l = 1, each block is a traditional ID, otherwise if l > 1, each block within the NID may itself be a NID.",
                "Note that within the I-IDs (or IDs) at each level, there is only a single decision node.",
                "Thus, our NID does not contain any MAIDs.",
                "Figure 2: A level l I-ID represented as a NID.",
                "The probabilities assigned to the blocks of the NID are is beliefs over js models conditioned on a physical state. 3.2 Solution The solution of an I-ID proceeds in a bottom-up manner, and is implemented recursively.",
                "We start by solving the level 0 models, which, if intentional, are traditional IDs.",
                "Their solutions provide probability distributions over the other agents actions, which are entered in the corresponding chance nodes found in the model node of the level 1 I-ID.",
                "The mapping from the level 0 models decision nodes to the chance nodes is carried out so that actions with the largest value in the decision node are assigned uniform probabilities in the chance node while the rest are assigned zero probability.",
                "Given the distributions over the actions within the different chance nodes (one for each model of the other agent), the level 1 I-ID is transformed as shown in Fig. 1(c).",
                "During the transformation, the conditional probability table (CPT) of the node, Aj, is populated such that the node assumes the distribution of each of the chance nodes depending on the value of the node, Mod[Mj].",
                "As we mentioned previously, the values of the node Mod[Mj] denote the different models of the other agent, and its distribution is the agent is belief over the models of j conditioned on the physical state.",
                "The transformed level 1 I-ID is a traditional ID that may be solved us816 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) (a) (b) Figure 3: (a) A generic two time-slice level l I-DID for agent i in a setting with one other agent j.",
                "Notice the dotted model update link that denotes the update of the models of j and the distribution over the models over time. (b) The semantics of the model update link. ing the standard expected utility maximization method [18].",
                "This procedure is carried out up to the level l I-ID whose solution gives the non-empty set of optimal actions that the agent should perform given its belief.",
                "Notice that analogous to IDs, I-IDs are suitable for online decision-making when the agents current belief is known. 4.",
                "INTERACTIVE DYNAMIC INFLUENCE DIAGRAMS Interactive dynamic influence diagrams (I-DIDs) extend I-IDs (and NIDs) to allow sequential decision-making over several time steps.",
                "Just as DIDs are structured graphical representations of POMDPs, I-DIDs are the graphical online analogs for finitely nested I-POMDPs.",
                "I-DIDs may be used to optimize over a finite look-ahead given initial beliefs while interacting with other, possibly similar, agents. 4.1 Syntax We depict a general two time-slice I-DID in Fig. 3(a).",
                "In addition to the model nodes and the dashed policy link, what differentiates an I-DID from a DID is the model update link shown as a dotted arrow in Fig. 3(a).",
                "We explained the semantics of the model node and the policy link in the previous section; we describe the model updates next.",
                "The update of the model node over time involves two steps: First, given the models at time t, we identify the updated set of models that reside in the model node at time t + 1.",
                "Recall from Section 2 that an agents intentional model includes its belief.",
                "Because the agents act and receive observations, their models are updated to reflect their changed beliefs.",
                "Since the set of optimal actions for a model could include all the actions, and the agent may receive any one of |Ωj| possible observations, the updated set at time step t + 1 will have at most |Mt j,l−1||Aj||Ωj| models.",
                "Here, |Mt j,l−1| is the number of models at time step t, |Aj| and |Ωj| are the largest spaces of actions and observations respectively, among all the models.",
                "Second, we compute the new distribution over the updated models given the original distribution and the probability of the agent performing the action and receiving the observation that led to the updated model.",
                "These steps are a part of agent is belief update formalized using Eq. 1.",
                "In Fig. 3(b), we show how the dotted model update link is implemented in the I-DID.",
                "If each of the two level l − 1 models ascribed to j at time step t results in one action, and j could make one of two possible observations, then the model node at time step t + 1 contains four updated models (mt+1,1 j,l−1 ,mt+1,2 j,l−1 , mt+1,3 j,l−1 , and mt+1,4 j,l−1 ).",
                "These models differ in their initial beliefs, each of which is the result of j updating its beliefs due to its action and a possible observation.",
                "The decision nodes in each of the I-DIDs or DIDs that represent the lower level models are mapped to the corresponding Figure 4: Transformed I-DID with the model nodes and model update link replaced with the chance nodes and the relationships (in bold). chance nodes, as mentioned previously.",
                "Next, we describe how the distribution over the updated set of models (the distribution over the chance node Mod[Mt+1 j ] in Mt+1 j,l−1) is computed.",
                "The probability that js updated model is, say mt+1,1 j,l−1 , depends on the probability of j performing the action and receiving the observation that led to this model, and the prior distribution over the models at time step t. Because the chance node At j assumes the distribution of each of the action nodes based on the value of Mod[Mt j ], the probability of the action is given by this chance node.",
                "In order to obtain the probability of js possible observation, we introduce the chance node Oj, which depending on the value of Mod[Mt j ] assumes the distribution of the observation node in the lower level model denoted by Mod[Mt j ].",
                "Because the probability of js observations depends on the physical state and the joint actions of both agents, the node Oj is linked with St+1 , At j, and At i. 2 Analogous to At j, the conditional probability table of Oj is also a multiplexer modulated by Mod[Mt j ].",
                "Finally, the distribution over the prior models at time t is obtained from the chance node, Mod[Mt j ] in Mt j,l−1.",
                "Consequently, the chance nodes, Mod[Mt j ], At j, and Oj, form the parents of Mod[Mt+1 j ] in Mt+1 j,l−1.",
                "Notice that the model update link may be replaced by the dependency links between the chance nodes that constitute the model nodes in the two time slices.",
                "In Fig. 4 we show the two time-slice I-DID with the model nodes replaced by the chance nodes and the relationships between them.",
                "Chance nodes and dependency links that not in bold are standard, usually found in DIDs.",
                "Expansion of the I-DID over more time steps requires the repetition of the two steps of updating the set of models that form the 2 Note that Oj represents js observation at time t + 1.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 817 values of the model node and adding the relationships between the chance nodes, as many times as there are model update links.",
                "We note that the possible set of models of the other agent j grows exponentially with the number of time steps.",
                "For example, after T steps, there may be at most |Mt=1 j,l−1|(|Aj||Ωj|)T −1 candidate models residing in the model node. 4.2 Solution Analogous to I-IDs, the solution to a level l I-DID for agent i expanded over T time steps may be carried out recursively.",
                "For the purpose of illustration, let l=1 and T=2.",
                "The solution method uses the standard look-ahead technique, projecting the agents action and observation sequences forward from the current belief state [17], and finding the possible beliefs that i could have in the next time step.",
                "Because agent i has a belief over js models as well, the lookahead includes finding out the possible models that j could have in the future.",
                "Consequently, each of js subintentional or level 0 models (represented using a standard DID) in the first time step must be solved to obtain its optimal set of actions.",
                "These actions are combined with the set of possible observations that j could make in that model, resulting in an updated set of candidate models (that include the updated beliefs) that could describe the behavior of j. Beliefs over this updated set of candidate models are calculated using the standard inference methods using the dependency relationships between the model nodes as shown in Fig. 3(b).",
                "We note the recursive nature of this solution: in solving agent is level 1 I-DID, js level 0 DIDs must be solved.",
                "If the nesting of models is deeper, all models at all levels starting from 0 are solved in a bottom-up manner.",
                "We briefly outline the recursive algorithm for solving agent is Algorithm for solving I-DID Input : level l ≥ 1 I-ID or level 0 ID, T Expansion Phase 1.",
                "For t from 1 to T − 1 do 2.",
                "If l ≥ 1 then Populate Mt+1 j,l−1 3.",
                "For each mt j in Range(Mt j,l−1) do 4.",
                "Recursively call algorithm with the l − 1 I-ID (or ID) that represents mt j and the horizon, T − t + 1 5.",
                "Map the decision node of the solved I-ID (or ID), OPT(mt j), to a chance node Aj 6.",
                "For each aj in OPT(mt j) do 7.",
                "For each oj in Oj (part of mt j) do 8.",
                "Update js belief, bt+1 j ← SE(bt j, aj, oj) 9. mt+1 j ← New I-ID (or ID) with bt+1 j as the initial belief 10.",
                "Range(Mt+1 j,l−1) ∪ ← {mt+1 j } 11.",
                "Add the model node, Mt+1 j,l−1, and the dependency links between Mt j,l−1 and Mt+1 j,l−1 (shown in Fig. 3(b)) 12.",
                "Add the chance, decision, and utility nodes for t + 1 time slice and the dependency links between them 13.",
                "Establish the CPTs for each chance node and utility node Look-Ahead Phase 14.",
                "Apply the standard look-ahead and backup method to solve the expanded I-DID Figure 5: Algorithm for solving a level l ≥ 0 I-DID. level l I-DID expanded over T time steps with one other agent j in Fig. 5.",
                "We adopt a two-phase approach: Given an I-ID of level l (described previously in Section 3) with all lower level models also represented as I-IDs or IDs (if level 0), the first step is to expand the level l I-ID over T time steps adding the dependency links and the conditional probability tables for each node.",
                "We particularly focus on establishing and populating the model nodes (lines 3-11).",
                "Note that Range(·) returns the values (lower level models) of the random variable given as input (model node).",
                "In the second phase, we use a standard look-ahead technique projecting the action and observation sequences over T time steps in the future, and backing up the utility values of the reachable beliefs.",
                "Similar to I-IDs, the I-DIDs reduce to DIDs in the absence of other agents.",
                "As we mentioned previously, the 0-th level models are the traditional DIDs.",
                "Their solutions provide probability distributions over actions of the agent modeled at that level to I-DIDs at level 1.",
                "Given probability distributions over other agents actions the level 1 IDIDs can themselves be solved as DIDs, and provide probability distributions to yet higher level models.",
                "Assume that the number of models considered at each level is bound by a number, M. Solving an I-DID of level l in then equivalent to solving O(Ml ) DIDs. 5.",
                "EXAMPLE APPLICATIONS To illustrate the usefulness of I-DIDs, we apply them to three problem domains.",
                "We describe, in particular, the formulation of the I-DID and the optimal prescriptions obtained on solving it. 5.1 Followership-Leadership in the Multiagent Tiger Problem We begin our illustrations of using I-IDs and I-DIDs with a slightly modified version of the multiagent tiger problem discussed in [9].",
                "The problem has two agents, each of which can open the right door (OR), the left door (OL) or listen (L).",
                "In addition to hearing growls (from the left (GL) or from the right (GR)) when they listen, the agents also hear creaks (from the left (CL), from the right (CR), or no creaks (S)), which noisily indicate the other agents opening one of the doors.",
                "When any door is opened, the tiger persists in its original location with a probability of 95%.",
                "Agent i hears growls with a reliability of 65% and creaks with a reliability of 95%.",
                "Agent j, on the other hand, hears growls with a reliability of 95%.",
                "Thus, the setting is such that agent i hears agent j opening doors more reliably than the tigers growls.",
                "This suggests that i could use js actions as an indication of the location of the tiger, as we discuss below.",
                "Each agents preferences are as in the single agent game discussed in [13].",
                "The transition, observation, and reward functions are shown in [16].",
                "A good indicator of the usefulness of normative methods for decision-making like I-DIDs is the emergence of realistic social behaviors in their prescriptions.",
                "In settings of the persistent multiagent tiger problem that reflect real world situations, we demonstrate followership between the agents and, as shown in [15], deception among agents who believe that they are in a follower-leader type of relationship.",
                "In particular, we analyze the situational and epistemological conditions sufficient for their emergence.",
                "The followership behavior, for example, results from the agent knowing its own weaknesses, assessing the strengths, preferences, and possible behaviors of the other, and realizing that its best for it to follow the others actions in order to maximize its payoffs.",
                "Let us consider a particular setting of the tiger problem in which agent i believes that js preferences are aligned with its own - both of them just want to get the gold - and js hearing is more reliable in comparison to itself.",
                "As an example, suppose that j, on listening can discern the tigers location 95% of the times compared to is 65% accuracy.",
                "Additionally, agent i does not have any initial information about the tigers location.",
                "In other words, is single-level nested belief, bi,1, assigns 0.5 to each of the two locations of the tiger.",
                "In addition, i considers two models of j, which differ in js flat level 0 initial beliefs.",
                "This is represented in the level 1 I-ID shown in Fig. 6(a).",
                "According to one model, j assigns a probability of 0.9 that the tiger is behind the left door, while the other 818 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Figure 6: (a) Level 1 I-ID of agent i, (b) two level 0 IDs of agent j whose decision nodes are mapped to the chance nodes, A1 j , A2 j , in (a). model assigns 0.1 to that location (see Fig. 6(b)).",
                "Agent i is undecided on these two models of j.",
                "If we vary is hearing ability, and solve the corresponding level 1 I-ID expanded over three time steps, we obtain the normative behavioral policies shown in Fig 7 that exhibit followership behavior.",
                "If is probability of correctly hearing the growls is 0.65, then as shown in the policy in Fig. 7(a), i begins to conditionally follow js actions: i opens the same door that j opened previously iff is own assessment of the tigers location confirms js pick.",
                "If i loses the ability to correctly interpret the growls completely, it blindly follows j and opens the same door that j opened previously (Fig. 7(b)).",
                "Figure 7: Emergence of (a) conditional followership, and (b) blind followership in the tiger problem.",
                "Behaviors of interest are in bold. * is a wildcard, and denotes any one of the observations.",
                "We observed that a single level of belief nesting - beliefs about the others models - was sufficient for followership to emerge in the tiger problem.",
                "However, the epistemological requirements for the emergence of leadership are more complex.",
                "For an agent, say j, to emerge as a leader, followership must first emerge in the other agent i.",
                "As we mentioned previously, if i is certain that its preferences are identical to those of j, and believes that j has a better sense of hearing, i will follow js actions over time.",
                "Agent j emerges as a leader if it believes that i will follow it, which implies that js belief must be nested two levels deep to enable it to recognize its leadership role.",
                "Realizing that i will follow presents j with an opportunity to influence is actions in the benefit of the collective good or its self-interest alone.",
                "For example, in the tiger problem, let us consider a setting in which if both i and j open the correct door, then each gets a payoff of 20 that is double the original.",
                "If j alone selects the correct door, it gets the payoff of 10.",
                "On the other hand, if both agents pick the wrong door, their penalties are cut in half.",
                "In this setting, it is in both js best interest as well as the collective betterment for j to use its expertise in selecting the correct door, and thus be a good leader.",
                "However, consider a slightly different problem in which j gains from is loss and is penalized if i gains.",
                "Specifically, let is payoff be subtracted from js, indicating that j is antagonistic toward i - if j picks the correct door and i the wrong one, then is loss of 100 becomes js gain.",
                "Agent j believes that i incorrectly thinks that js preferences are those that promote the collective good and that it starts off by believing with 99% confidence where the tiger is.",
                "Because i believes that its preferences are similar to those of j, and that j starts by believing almost surely that one of the two is the correct location (two level 0 models of j), i will start by following js actions.",
                "We show is normative policy on solving its singly-nested I-DID over three time steps in Fig. 8(a).",
                "The policy demonstrates that i will blindly follow js actions.",
                "Since the tiger persists in its original location with a probability of 0.95, i will select the same door again.",
                "If j begins the game with a 99% probability that the tiger is on the right, solving js I-DID nested two levels deep, results in the policy shown in Fig. 8(b).",
                "Even though j is almost certain that OL is the correct action, it will start by selecting OR, followed by OL.",
                "Agent js intention is to deceive i who, it believes, will follow js actions, so as to gain $110 in the second time step, which is more than what j would gain if it were to be honest.",
                "Figure 8: Emergence of deception between agents in the tiger problem.",
                "Behaviors of interest are in bold. * denotes as before. (a) Agent is policy demonstrating that it will blindly follow js actions. (b) Even though j is almost certain that the tiger is on the right, it will start by selecting OR, followed by OL, in order to deceive i. 5.2 Altruism and Reciprocity in the Public Good Problem The public good (PG) problem [7], consists of a group of M agents, each of whom must either contribute some resource to a public pot or keep it for themselves.",
                "Since resources contributed to the public pot are shared among all the agents, they are less valuable to the agent when in the public pot.",
                "However, if all agents choose to contribute their resources, then the payoff to each agent is more than if no one contributes.",
                "Since an agent gets its share of the public pot irrespective of whether it has contributed or not, the dominating action is for each agent to not contribute, and instead free ride on others contributions.",
                "However, behaviors of human players in empirical simulations of the PG problem differ from the normative predictions.",
                "The experiments reveal that many players initially contribute a large amount to the public pot, and continue to contribute when the PG problem is played repeatedly, though in decreasing amounts [4].",
                "Many of these experiments [5] report that a small core group of players persistently contributes to the public pot even when all others are defecting.",
                "These experiments also reveal that players who persistently contribute have altruistic or reciprocal preferences matching expected cooperation of others.",
                "For simplicity, we assume that the game is played between M = 2 agents, i and j.",
                "Let each agent be initially endowed with XT amount of resources.",
                "While the classical PG game formulation permits each agent to contribute any quantity of resources (≤ XT ) to the public pot, we simplify the action space by allowing two possible actions.",
                "Each agent may choose to either contribute (C) a fixed amount of the resources, or not contribute.",
                "The latter action is deThe Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 819 noted as defect (D).",
                "We assume that the actions are not observable to others.",
                "The value of resources in the public pot is discounted by ci for each agent i, where ci is the marginal private return.",
                "We assume that ci < 1 so that the agent does not benefit enough that it contributes to the public pot for private gain.",
                "Simultaneously, ciM > 1, making collective contribution pareto optimal. i/j C D C 2ciXT , 2cjXT ciXT − cp, XT + cjXT − P D XT + ciXT − P, cjXT − cp XT , XT Table 1: The one-shot PG game with punishment.",
                "In order to encourage contributions, the contributing agents punish free riders but incur a small cost for administering the punishment.",
                "Let P be the punishment meted out to the defecting agent and cp the non-zero cost of punishing for the contributing agent.",
                "For simplicity, we assume that the cost of punishing is same for both the agents.",
                "The one-shot PG game with punishment is shown in Table. 1.",
                "Let ci = cj, cp > 0, and if P > XT − ciXT , then defection is no longer a dominating action.",
                "If P < XT − ciXT , then defection is the dominating action for both.",
                "If P = XT − ciXT , then the game is not dominance-solvable.",
                "Figure 9: (a) Level 1 I-ID of agent i, (b) level 0 IDs of agent j with decision nodes mapped to the chance nodes, A1 j and A2 j , in (a).",
                "We formulate a sequential version of the PG problem with punishment from the perspective of agent i.",
                "Though in the repeated PG game, the quantity in the public pot is revealed to all the agents after each round of actions, we assume in our formulation that it is hidden from the agents.",
                "Each agent may contribute a fixed amount, xc, or defect.",
                "An agent on performing an action receives an observation of plenty (PY) or meager (MR) symbolizing the state of the public pot.",
                "Notice that the observations are also indirectly indicative of agent js actions because the state of the public pot is influenced by them.",
                "The amount of resources in agent is private pot, is perfectly observable to i.",
                "The payoffs are analogous to Table. 1.",
                "Borrowing from the empirical investigations of the PG problem [5], we construct level 0 IDs for j that model altruistic and non-altruistic types (Fig. 9(b)).",
                "Specifically, our altruistic agent has a high marginal private return (cj is close to 1) and does not punish others who defect.",
                "Let xc = 1 and the level 0 agent be punished half the times it defects.",
                "With one action remaining, both types of agents choose to contribute to avoid being punished.",
                "With two actions to go, the altruistic type chooses to contribute, while the other defects.",
                "This is because cj for the altruistic type is close to 1, thus the expected punishment, 0.5P > (1 − cj), which the altruistic type avoids.",
                "Because cj for the non-altruistic type is less, it prefers not to contribute.",
                "With three steps to go, the altruistic agent contributes to avoid punishment (0.5P > 2(1 − cj)), and the non-altruistic type defects.",
                "For greater than three steps, while the altruistic agent continues to contribute to the public pot depending on how close its marginal private return is to 1, the non-altruistic type prescribes defection.",
                "We analyzed the decisions of an altruistic agent i modeled using a level 1 I-DID expanded over 3 time steps. i ascribes the two level 0 models, mentioned previously, to j (see Fig. 9).",
                "If i believes with a probability 1 that j is altruistic, i chooses to contribute for each of the three steps.",
                "This behavior persists when i is unaware of whether j is altruistic (Fig. 10(a)), and when i assigns a high probability to j being the non-altruistic type.",
                "However, when i believes with a probability 1 that j is non-altruistic and will thus surely defect, i chooses to defect to avoid being punished and because its marginal private return is less than 1.",
                "These results demonstrate that the behavior of our altruistic type resembles that found experimentally.",
                "The non-altruistic level 1 agent chooses to defect regardless of how likely it believes the other agent to be altruistic.",
                "We analyzed the behavior of a reciprocal agent type that matches expected cooperation or defection.",
                "The reciprocal types marginal private return is similar to that of the non-altruistic type, however, it obtains a greater payoff when its action is similar to that of the other.",
                "We consider the case when the reciprocal agent i is unsure of whether j is altruistic and believes that the public pot is likely to be half full.",
                "For this prior belief, i chooses to defect.",
                "On receiving an observation of plenty, i decides to contribute, while an observation of meager makes it defect (Fig. 10(b)).",
                "This is because an observation of plenty signals that the pot is likely to be greater than half full, which results from js action to contribute.",
                "Thus, among the two models ascribed to j, its type is likely to be altruistic making it likely that j will contribute again in the next time step.",
                "Agent i therefore chooses to contribute to reciprocate js action.",
                "An analogous reasoning leads i to defect when it observes a meager pot.",
                "With one action to go, i believing that j contributes, will choose to contribute too to avoid punishment regardless of its observations.",
                "Figure 10: (a) An altruistic level 1 agent always contributes. (b) A reciprocal agent i starts off by defecting followed by choosing to contribute or defect based on its observation of plenty (indicating that j is likely altruistic) or meager (j is non-altruistic). 5.3 Strategies in Two-Player Poker Poker is a popular zero sum card game that has received much attention among the AI research community as a testbed [2].",
                "Poker is played among M ≥ 2 players in which each player receives a hand of cards from a deck.",
                "While several flavors of Poker with varying complexity exist, we consider a simple version in which each player has three plys during which the player may either exchange a card (E), keep the existing hand (K), fold (F) and withdraw from the game, or call (C), requiring all players to show their hands.",
                "To keep matters simple, let M = 2, and each player receive a hand consisting of a single card drawn from the same suit.",
                "Thus, during a showdown, the player who has the numerically larger card (2 is the lowest, ace is the highest) wins the pot.",
                "During an exchange of cards, the discarded card is placed either in the L pile, indicating to the other agent that it was a low numbered card less than 8, or in the 820 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) H pile, indicating that the card had a rank greater than or equal to 8.",
                "Notice that, for example, if a lower numbered card is discarded, the probability of receiving a low card in exchange is now reduced.",
                "We show the level 1 I-ID for the simplified two-player Poker in Fig. 11.",
                "We considered two models (personality types) of agent j.",
                "The conservative type believes that it is likely that its opponent has a high numbered card in its hand.",
                "On the other hand, the aggressive agent j believes with a high probability that its opponent has a lower numbered card.",
                "Thus, the two types differ in their beliefs over their opponents hand.",
                "In both these level 0 models, the opponent is assumed to perform its actions following a fixed, uniform distribution.",
                "With three actions to go, regardless of its hand (unless it is an ace), the aggressive agent chooses to exchange its card, with the intent of improving on its current hand.",
                "This is because it believes the other to have a low card, which improves its chances of getting a high card during the exchange.",
                "The conservative agent chooses to keep its card, no matter its hand because its chances of getting a high card are slim as it believes that its opponent has one.",
                "Figure 11: (a) Level 1 I-ID of agent i.",
                "The observation reveals information about js hand of the previous time step, (b) level 0 IDs of agent j whose decision nodes are mapped to the chance nodes, A1 j , A2 j , in (a).",
                "The policy of a level 1 agent i who believes that each card except its own has an equal likelihood of being in js hand (neutral personality type) and j could be either an aggressive or conservative type, is shown in Fig. 12. is own hand contains the card numbered 8.",
                "The agent starts by keeping its card.",
                "On seeing that j did not exchange a card (N), i believes with probability 1 that j is conservative and hence will keep its cards. i responds by either keeping its card or exchanging it because j is equally likely to have a lower or higher card.",
                "If i observes that j discarded its card into the L or H pile, i believes that j is aggressive.",
                "On observing L, i realizes that j had a low card, and is likely to have a high card after its exchange.",
                "Because the probability of receiving a low card is high now, i chooses to keep its card.",
                "On observing H, believing that the probability of receiving a high numbered card is high, i chooses to exchange its card.",
                "In the final step, i chooses to call regardless of its observation history because its belief that j has a higher card is not sufficiently high to conclude that its better to fold and relinquish the payoff.",
                "This is partly due to the fact that an observation of, say, L resets the agent is previous time step beliefs over js hand to the low numbered cards only. 6.",
                "DISCUSSION We showed how DIDs may be extended to I-DIDs that enable online sequential decision-making in uncertain multiagent settings.",
                "Our graphical representation of I-DIDs improves on the previous Figure 12: A level 1 agent is three step policy in the Poker problem. i starts by believing that j is equally likely to be aggressive or conservative and could have any card in its hand with equal probability. work significantly by being more transparent, semantically clear, and capable of being solved using standard algorithms that target DIDs.",
                "I-DIDs extend NIDs to allow sequential decision-making over multiple time steps in the presence of other interacting agents.",
                "I-DIDs may be seen as concise graphical representations for IPOMDPs providing a way to exploit problem structure and carry out online decision-making as the agent acts and observes given its prior beliefs.",
                "We are currently investigating ways to solve I-DIDs approximately with provable bounds on the solution quality.",
                "Acknowledgment: We thank Piotr Gmytrasiewicz for some useful discussions related to this work.",
                "The first author would like to acknowledge the support of a UGARF grant. 7.",
                "REFERENCES [1] R. J. Aumann.",
                "Interactive epistemology i: Knowledge.",
                "International Journal of Game Theory, 28:263-300, 1999. [2] D. Billings, A. Davidson, J. Schaeffer, and D. Szafron.",
                "The challenge of poker.",
                "AIJ, 2001. [3] A. Brandenburger and E. Dekel.",
                "Hierarchies of beliefs and common knowledge.",
                "Journal of Economic Theory, 59:189-198, 1993. [4] C. Camerer.",
                "Behavioral Game Theory: Experiments in Strategic Interaction.",
                "Princeton University Press, 2003. [5] E. Fehr and S. Gachter.",
                "Cooperation and punishment in public goods experiments.",
                "American Economic Review, 90(4):980-994, 2000. [6] D. Fudenberg and D. K. Levine.",
                "The Theory of Learning in Games.",
                "MIT Press, 1998. [7] D. Fudenberg and J. Tirole.",
                "Game Theory.",
                "MIT Press, 1991. [8] Y. Gal and A. Pfeffer.",
                "A language for modeling agents decision-making processes in games.",
                "In AAMAS, 2003. [9] P. Gmytrasiewicz and P. Doshi.",
                "A framework for sequential planning in multiagent settings.",
                "JAIR, 24:49-79, 2005. [10] P. Gmytrasiewicz and E. Durfee.",
                "Rational coordination in multi-agent environments.",
                "JAAMAS, 3(4):319-350, 2000. [11] J. C. Harsanyi.",
                "Games with incomplete information played by bayesian players.",
                "Management Science, 14(3):159-182, 1967. [12] R. A. Howard and J. E. Matheson.",
                "Influence diagrams.",
                "In R. A. Howard and J. E. Matheson, editors, The Principles and Applications of Decision Analysis.",
                "Strategic Decisions Group, Menlo Park, CA 94025, 1984. [13] L. Kaelbling, M. Littman, and A. Cassandra.",
                "Planning and acting in partially observable stochastic domains.",
                "Artificial Intelligence Journal, 2, 1998. [14] D. Koller and B. Milch.",
                "Multi-agent influence diagrams for representing and solving games.",
                "In IJCAI, pages 1027-1034, 2001. [15] K. Polich and P. Gmytrasiewicz.",
                "Interactive dynamic influence diagrams.",
                "In GTDT Workshop, AAMAS, 2006. [16] B.",
                "Rathnas., P. Doshi, and P. J. Gmytrasiewicz.",
                "Exact solutions to interactive pomdps using behavioral equivalence.",
                "In Autonomous Agents and Multi-Agent Systems Conference (AAMAS), 2006. [17] S. Russell and P. Norvig.",
                "Artificial Intelligence: A Modern Approach (Second Edition).",
                "Prentice Hall, 2003. [18] R. D. Shachter.",
                "Evaluating influence diagrams.",
                "Operations Research, 34(6):871-882, 1986. [19] D. Suryadi and P. Gmytrasiewicz.",
                "Learning models of other agents using influence diagrams.",
                "In UM, 1999.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 821"
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [],
            "translated_text": "",
            "candidates": [],
            "error": [
                []
            ]
        },
        "online sequential decision-making": {
            "translated_key": "toma de decisiones secuenciales en línea",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Graphical Models for Online Solutions to Interactive POMDPs Prashant Doshi Dept.",
                "of Computer Science University of Georgia Athens, GA 30602, USA pdoshi@cs.uga.edu Yifeng Zeng Dept. of Computer Science Aalborg University DK-9220 Aalborg, Denmark yfzeng@cs.aau.edu Qiongyu Chen Dept. of Computer Science National Univ. of Singapore 117543, Singapore chenqy@comp.nus.edu.sg ABSTRACT We develop a new graphical representation for interactive partially observable Markov decision processes (I-POMDPs) that is significantly more transparent and semantically clear than the previous representation.",
                "These graphical models called interactive dynamic influence diagrams (I-DIDs) seek to explicitly model the structure that is often present in real-world problems by decomposing the situation into chance and decision variables, and the dependencies between the variables.",
                "I-DIDs generalize DIDs, which may be viewed as graphical representations of POMDPs, to multiagent settings in the same way that I-POMDPs generalize POMDPs.",
                "I-DIDs may be used to compute the policy of an agent online as the agent acts and observes in a setting that is populated by other interacting agents.",
                "Using several examples, we show how I-DIDs may be applied and demonstrate their usefulness.",
                "Categories and Subject Descriptors I.2.11 [Distributed Artificial Intelligence]: Multiagent Systems General Terms Theory 1.",
                "INTRODUCTION Interactive partially observable Markov decision processes (IPOMDPs) [9] provide a framework for sequential decision-making in partially observable multiagent environments.",
                "They generalize POMDPs [13] to multiagent settings by including the other agents computable models in the state space along with the states of the physical environment.",
                "The models encompass all information influencing the agents behaviors, including their preferences, capabilities, and beliefs, and are thus analogous to types in Bayesian games [11].",
                "I-POMDPs adopt a subjective approach to understanding strategic behavior, rooted in a decision-theoretic framework that takes a decision-makers perspective in the interaction.",
                "In [15], Polich and Gmytrasiewicz introduced interactive dynamic influence diagrams (I-DIDs) as the computational representations of I-POMDPs.",
                "I-DIDs generalize DIDs [12], which may be viewed as computational counterparts of POMDPs, to multiagents settings in the same way that I-POMDPs generalize POMDPs.",
                "I-DIDs contribute to a growing line of work [19] that includes multi-agent influence diagrams (MAIDs) [14], and more recently, networks of influence diagrams (NIDs) [8].",
                "These formalisms seek to explicitly model the structure that is often present in real-world problems by decomposing the situation into chance and decision variables, and the dependencies between the variables.",
                "MAIDs provide an alternative to normal and extensive game forms using a graphical formalism to represent games of imperfect information with a decision node for each agents actions and chance nodes capturing the agents private information.",
                "MAIDs objectively analyze the game, efficiently computing the Nash equilibrium profile by exploiting the independence structure.",
                "NIDs extend MAIDs to include agents uncertainty over the game being played and over models of the other agents.",
                "Each model is a MAID and the network of MAIDs is collapsed, bottom up, into a single MAID for computing the equilibrium of the game keeping in mind the different models of each agent.",
                "Graphical formalisms such as MAIDs and NIDs open up a promising area of research that aims to represent multiagent interactions more transparently.",
                "However, MAIDs provide an analysis of the game from an external viewpoint and the applicability of both is limited to static single play games.",
                "Matters are more complex when we consider interactions that are extended over time, where predictions about others future actions must be made using models that change as the agents act and observe.",
                "I-DIDs address this gap by allowing the representation of other agents models as the values of a special model node.",
                "Both, other agents models and the original agents beliefs over these models are updated over time using special-purpose implementations.",
                "In this paper, we improve on the previous preliminary representation of the I-DID shown in [15] by using the insight that the static I-ID is a type of NID.",
                "Thus, we may utilize NID-specific language constructs such as multiplexers to represent the model node, and subsequently the I-ID, more transparently.",
                "Furthermore, we clarify the semantics of the special purpose policy link introduced in the representation of I-DID by [15], and show that it could be replaced by traditional dependency links.",
                "In the previous representation of the I-DID, the update of the agents belief over the models of others as the agents act and receive observations was denoted using a special link called the model update link that connected the model nodes over time.",
                "We explicate the semantics of this link by showing how it can be implemented using the traditional dependency links between the chance nodes that constitute the model nodes.",
                "The net result is a representation of I-DID that is significantly more transparent, semantically clear, and capable of being implemented using the standard algorithms for solving DIDs.",
                "We show how IDIDs may be used to model an agents uncertainty over others models, that may themselves be I-DIDs.",
                "Solution to the I-DID is a policy that prescribes what the agent should do over time, given its beliefs over the physical state and others models.",
                "Analogous to DIDs, I-DIDs may be used to compute the policy of an agent online as the agent acts and observes in a setting that is populated by other interacting agents. 2.",
                "BACKGROUND: FINITELY NESTED IPOMDPS Interactive POMDPs generalize POMDPs to multiagent settings by including other agents models as part of the state space [9].",
                "Since other agents may also reason about others, the interactive state space is strategically nested; it contains beliefs about other agents models and their beliefs about others.",
                "For simplicity of presentation we consider an agent, i, that is interacting with one other agent, j.",
                "A finitely nested I-POMDP of agent i with a strategy level l is defined as the tuple: I-POMDPi,l = ISi,l, A, Ti, Ωi, Oi, Ri where: • ISi,l denotes a set of interactive states defined as, ISi,l = S × Mj,l−1, where Mj,l−1 = {Θj,l−1 ∪ SMj}, for l ≥ 1, and ISi,0 = S, where S is the set of states of the physical environment.",
                "Θj,l−1 is the set of computable intentional models of agent j: θj,l−1 = bj,l−1, ˆθj where the frame, ˆθj = A, Ωj, Tj, Oj, Rj, OCj .",
                "Here, j is Bayes rational and OCj is js optimality criterion.",
                "SMj is the set of subintentional models of j.",
                "Simple examples of subintentional models include a no-information model [10] and a fictitious play model [6], both of which are history independent.",
                "We give a recursive bottom-up construction of the interactive state space below.",
                "ISi,0 = S, Θj,0 = { bj,0, ˆθj | bj,0 ∈ Δ(ISj,0)} ISi,1 = S × {Θj,0 ∪ SMj}, Θj,1 = { bj,1, ˆθj | bj,1 ∈ Δ(ISj,1)} . . . . . .",
                "ISi,l = S × {Θj,l−1 ∪ SMj}, Θj,l = { bj,l, ˆθj | bj,l ∈ Δ(ISj,l)} Similar formulations of nested spaces have appeared in [1, 3]. • A = Ai × Aj is the set of joint actions of all agents in the environment; • Ti : S ×A×S → [0, 1], describes the effect of the joint actions on the physical states of the environment; • Ωi is the set of observations of agent i; • Oi : S × A × Ωi → [0, 1] gives the likelihood of the observations given the physical state and joint action; • Ri : ISi × A → R describes agent is preferences over its interactive states.",
                "Usually only the physical states will matter.",
                "Agent is policy is the mapping, Ω∗ i → Δ(Ai), where Ω∗ i is the set of all observation histories of agent i.",
                "Since belief over the interactive states forms a sufficient statistic [9], the policy can also be represented as a mapping from the set of all beliefs of agent i to a distribution over its actions, Δ(ISi) → Δ(Ai). 2.1 Belief Update Analogous to POMDPs, an agent within the I-POMDP framework updates its belief as it acts and observes.",
                "However, there are two differences that complicate the belief update in multiagent settings when compared to single agent ones.",
                "First, since the state of the physical environment depends on the actions of both agents, is prediction of how the physical state changes has to be made based on its prediction of js actions.",
                "Second, changes in js models have to be included in is belief update.",
                "Specifically, if j is intentional then an update of js beliefs due to its action and observation has to be included.",
                "In other words, i has to update its belief based on its prediction of what j would observe and how j would update its belief.",
                "If js model is subintentional, then js probable observations are appended to the observation history contained in the model.",
                "Formally, we have: Pr(ist |at−1 i , bt−1 i,l ) = β ISt−1:mt−1 j =θt j bt−1 i,l (ist−1 ) × at−1 j Pr(at−1 j |θt−1 j,l−1)Oi(st , at−1 i , at−1 j , ot i) ×Ti(st−1 , at−1 i , at−1 j , st ) ot j Oj(st , at−1 i , at−1 j , ot j) ×τ(SEθt j (bt−1 j,l−1, at−1 j , ot j) − bt j,l−1) (1) where β is the normalizing constant, τ is 1 if its argument is 0 otherwise it is 0, Pr(at−1 j |θt−1 j,l−1) is the probability that at−1 j is Bayes rational for the agent described by model θt−1 j,l−1, and SE(·) is an abbreviation for the belief update.",
                "For a version of the belief update when js model is subintentional, see [9].",
                "If agent j is also modeled as an I-POMDP, then is belief update invokes js belief update (via the term SEθt j ( bt−1 j,l−1 , at−1 j , ot j)), which in turn could invoke is belief update and so on.",
                "This recursion in belief nesting bottoms out at the 0th level.",
                "At this level, the belief update of the agent reduces to a POMDP belief update. 1 For illustrations of the belief update, additional details on I-POMDPs, and how they compare with other multiagent frameworks, see [9]. 2.2 Value Iteration Each belief state in a finitely nested I-POMDP has an associated value reflecting the maximum payoff the agent can expect in this belief state: Un ( bi,l, θi ) = max ai∈Ai is∈ISi,l ERi(is, ai)bi,l(is)+ γ oi∈Ωi Pr(oi|ai, bi,l)Un−1 ( SEθi (bi,l, ai, oi), θi ) (2) where, ERi(is, ai) = aj Ri(is, ai, aj)Pr(aj|mj,l−1) (since is = (s, mj,l−1)).",
                "Eq. 2 is a basis for value iteration in I-POMDPs.",
                "Agent is optimal action, a∗ i , for the case of finite horizon with discounting, is an element of the set of optimal actions for the belief state, OPT(θi), defined as: OPT( bi,l, θi ) = argmax ai∈Ai is∈ISi,l ERi(is, ai)bi,l(is) +γ oi∈Ωi Pr(oi|ai, bi,l)Un ( SEθi (bi,l, ai, oi), θi ) (3) 3.",
                "INTERACTIVEINFLUENCEDIAGRAMS A naive extension of influence diagrams (IDs) to settings populated by multiple agents is possible by treating other agents as automatons, represented using chance nodes.",
                "However, this approach assumes that the agents actions are controlled using a probability distribution that does not change over time.",
                "Interactive influence diagrams (I-IDs) adopt a more sophisticated approach by generalizing IDs to make them applicable to settings shared with other agents who may act and observe, and update their beliefs. 3.1 Syntax In addition to the usual chance, decision, and utility nodes, IIDs include a new type of node called the model node.",
                "We show a general level l I-ID in Fig. 1(a), where the model node (Mj,l−1) is denoted using a hexagon.",
                "We note that the probability distribution over the chance node, S, and the model node together represents agent is belief over its interactive states.",
                "In addition to the model 1 The 0th level model is a POMDP: Other agents actions are treated as exogenous events and folded into the T, O, and R functions.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 815 Figure 1: (a) A generic level l I-ID for agent i situated with one other agent j.",
                "The hexagon is the model node (Mj,l−1) whose structure we show in (b).",
                "Members of the model node are I-IDs themselves (m1 j,l−1, m2 j,l−1; diagrams not shown here for simplicity) whose decision nodes are mapped to the corresponding chance nodes (A1 j , A2 j ).",
                "Depending on the value of the node, Mod[Mj], the distribution of each of the chance nodes is assigned to the node Aj. (c) The transformed I-ID with the model node replaced by the chance nodes and the relationships between them. node, I-IDs differ from IDs by having a dashed link (called the policy link in [15]) between the model node and a chance node, Aj, that represents the distribution over the other agents actions given its model.",
                "In the absence of other agents, the model node and the chance node, Aj, vanish and I-IDs collapse into traditional IDs.",
                "The model node contains the alternative computational models ascribed by i to the other agent from the set, Θj,l−1 ∪ SMj, where Θj,l−1 and SMj were defined previously in Section 2.",
                "Thus, a model in the model node may itself be an I-ID or ID, and the recursion terminates when a model is an ID or subintentional.",
                "Because the model node contains the alternative models of the other agent as its values, its representation is not trivial.",
                "In particular, some of the models within the node are I-IDs that when solved generate the agents optimal policy in their decision nodes.",
                "Each decision node is mapped to the corresponding chance node, say A1 j , in the following way: if OPT is the set of optimal actions obtained by solving the I-ID (or ID), then Pr(aj ∈ A1 j ) = 1 |OP T | if aj ∈ OPT, 0 otherwise.",
                "Borrowing insights from previous work [8], we observe that the model node and the dashed policy link that connects it to the chance node, Aj, could be represented as shown in Fig. 1(b).",
                "The decision node of each level l − 1 I-ID is transformed into a chance node, as we mentioned previously, so that the actions with the largest value in the decision node are assigned uniform probabilities in the chance node while the rest are assigned zero probability.",
                "The different chance nodes (A1 j , A2 j ), one for each model, and additionally, the chance node labeled Mod[Mj] form the parents of the chance node, Aj.",
                "Thus, there are as many action nodes (A1 j , A2 j ) in Mj,l−1 as the number of models in the support of agent is beliefs.",
                "The conditional probability table of the chance node, Aj, is a multiplexer that assumes the distribution of each of the action nodes (A1 j , A2 j ) depending on the value of Mod[Mj].",
                "The values of Mod[Mj] denote the different models of j.",
                "In other words, when Mod[Mj] has the value m1 j,l−1, the chance node Aj assumes the distribution of the node A1 j , and Aj assumes the distribution of A2 j when Mod[Mj] has the value m2 j,l−1.",
                "The distribution over the node, Mod[Mj], is the agent is belief over the models of j given a physical state.",
                "For more agents, we will have as many model nodes as there are agents.",
                "Notice that Fig. 1(b) clarifies the semantics of the policy link, and shows how it can be represented using the traditional dependency links.",
                "In Fig. 1(c), we show the transformed I-ID when the model node is replaced by the chance nodes and relationships between them.",
                "In contrast to the representation in [15], there are no special-purpose policy links, rather the I-ID is composed of only those types of nodes that are found in traditional IDs and dependency relationships between the nodes.",
                "This allows I-IDs to be represented and implemented using conventional application tools that target IDs.",
                "Note that we may view the level l I-ID as a NID.",
                "Specifically, each of the level l − 1 models within the model node are blocks in the NID (see Fig. 2).",
                "If the level l = 1, each block is a traditional ID, otherwise if l > 1, each block within the NID may itself be a NID.",
                "Note that within the I-IDs (or IDs) at each level, there is only a single decision node.",
                "Thus, our NID does not contain any MAIDs.",
                "Figure 2: A level l I-ID represented as a NID.",
                "The probabilities assigned to the blocks of the NID are is beliefs over js models conditioned on a physical state. 3.2 Solution The solution of an I-ID proceeds in a bottom-up manner, and is implemented recursively.",
                "We start by solving the level 0 models, which, if intentional, are traditional IDs.",
                "Their solutions provide probability distributions over the other agents actions, which are entered in the corresponding chance nodes found in the model node of the level 1 I-ID.",
                "The mapping from the level 0 models decision nodes to the chance nodes is carried out so that actions with the largest value in the decision node are assigned uniform probabilities in the chance node while the rest are assigned zero probability.",
                "Given the distributions over the actions within the different chance nodes (one for each model of the other agent), the level 1 I-ID is transformed as shown in Fig. 1(c).",
                "During the transformation, the conditional probability table (CPT) of the node, Aj, is populated such that the node assumes the distribution of each of the chance nodes depending on the value of the node, Mod[Mj].",
                "As we mentioned previously, the values of the node Mod[Mj] denote the different models of the other agent, and its distribution is the agent is belief over the models of j conditioned on the physical state.",
                "The transformed level 1 I-ID is a traditional ID that may be solved us816 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) (a) (b) Figure 3: (a) A generic two time-slice level l I-DID for agent i in a setting with one other agent j.",
                "Notice the dotted model update link that denotes the update of the models of j and the distribution over the models over time. (b) The semantics of the model update link. ing the standard expected utility maximization method [18].",
                "This procedure is carried out up to the level l I-ID whose solution gives the non-empty set of optimal actions that the agent should perform given its belief.",
                "Notice that analogous to IDs, I-IDs are suitable for online decision-making when the agents current belief is known. 4.",
                "INTERACTIVE DYNAMIC INFLUENCE DIAGRAMS Interactive dynamic influence diagrams (I-DIDs) extend I-IDs (and NIDs) to allow sequential decision-making over several time steps.",
                "Just as DIDs are structured graphical representations of POMDPs, I-DIDs are the graphical online analogs for finitely nested I-POMDPs.",
                "I-DIDs may be used to optimize over a finite look-ahead given initial beliefs while interacting with other, possibly similar, agents. 4.1 Syntax We depict a general two time-slice I-DID in Fig. 3(a).",
                "In addition to the model nodes and the dashed policy link, what differentiates an I-DID from a DID is the model update link shown as a dotted arrow in Fig. 3(a).",
                "We explained the semantics of the model node and the policy link in the previous section; we describe the model updates next.",
                "The update of the model node over time involves two steps: First, given the models at time t, we identify the updated set of models that reside in the model node at time t + 1.",
                "Recall from Section 2 that an agents intentional model includes its belief.",
                "Because the agents act and receive observations, their models are updated to reflect their changed beliefs.",
                "Since the set of optimal actions for a model could include all the actions, and the agent may receive any one of |Ωj| possible observations, the updated set at time step t + 1 will have at most |Mt j,l−1||Aj||Ωj| models.",
                "Here, |Mt j,l−1| is the number of models at time step t, |Aj| and |Ωj| are the largest spaces of actions and observations respectively, among all the models.",
                "Second, we compute the new distribution over the updated models given the original distribution and the probability of the agent performing the action and receiving the observation that led to the updated model.",
                "These steps are a part of agent is belief update formalized using Eq. 1.",
                "In Fig. 3(b), we show how the dotted model update link is implemented in the I-DID.",
                "If each of the two level l − 1 models ascribed to j at time step t results in one action, and j could make one of two possible observations, then the model node at time step t + 1 contains four updated models (mt+1,1 j,l−1 ,mt+1,2 j,l−1 , mt+1,3 j,l−1 , and mt+1,4 j,l−1 ).",
                "These models differ in their initial beliefs, each of which is the result of j updating its beliefs due to its action and a possible observation.",
                "The decision nodes in each of the I-DIDs or DIDs that represent the lower level models are mapped to the corresponding Figure 4: Transformed I-DID with the model nodes and model update link replaced with the chance nodes and the relationships (in bold). chance nodes, as mentioned previously.",
                "Next, we describe how the distribution over the updated set of models (the distribution over the chance node Mod[Mt+1 j ] in Mt+1 j,l−1) is computed.",
                "The probability that js updated model is, say mt+1,1 j,l−1 , depends on the probability of j performing the action and receiving the observation that led to this model, and the prior distribution over the models at time step t. Because the chance node At j assumes the distribution of each of the action nodes based on the value of Mod[Mt j ], the probability of the action is given by this chance node.",
                "In order to obtain the probability of js possible observation, we introduce the chance node Oj, which depending on the value of Mod[Mt j ] assumes the distribution of the observation node in the lower level model denoted by Mod[Mt j ].",
                "Because the probability of js observations depends on the physical state and the joint actions of both agents, the node Oj is linked with St+1 , At j, and At i. 2 Analogous to At j, the conditional probability table of Oj is also a multiplexer modulated by Mod[Mt j ].",
                "Finally, the distribution over the prior models at time t is obtained from the chance node, Mod[Mt j ] in Mt j,l−1.",
                "Consequently, the chance nodes, Mod[Mt j ], At j, and Oj, form the parents of Mod[Mt+1 j ] in Mt+1 j,l−1.",
                "Notice that the model update link may be replaced by the dependency links between the chance nodes that constitute the model nodes in the two time slices.",
                "In Fig. 4 we show the two time-slice I-DID with the model nodes replaced by the chance nodes and the relationships between them.",
                "Chance nodes and dependency links that not in bold are standard, usually found in DIDs.",
                "Expansion of the I-DID over more time steps requires the repetition of the two steps of updating the set of models that form the 2 Note that Oj represents js observation at time t + 1.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 817 values of the model node and adding the relationships between the chance nodes, as many times as there are model update links.",
                "We note that the possible set of models of the other agent j grows exponentially with the number of time steps.",
                "For example, after T steps, there may be at most |Mt=1 j,l−1|(|Aj||Ωj|)T −1 candidate models residing in the model node. 4.2 Solution Analogous to I-IDs, the solution to a level l I-DID for agent i expanded over T time steps may be carried out recursively.",
                "For the purpose of illustration, let l=1 and T=2.",
                "The solution method uses the standard look-ahead technique, projecting the agents action and observation sequences forward from the current belief state [17], and finding the possible beliefs that i could have in the next time step.",
                "Because agent i has a belief over js models as well, the lookahead includes finding out the possible models that j could have in the future.",
                "Consequently, each of js subintentional or level 0 models (represented using a standard DID) in the first time step must be solved to obtain its optimal set of actions.",
                "These actions are combined with the set of possible observations that j could make in that model, resulting in an updated set of candidate models (that include the updated beliefs) that could describe the behavior of j. Beliefs over this updated set of candidate models are calculated using the standard inference methods using the dependency relationships between the model nodes as shown in Fig. 3(b).",
                "We note the recursive nature of this solution: in solving agent is level 1 I-DID, js level 0 DIDs must be solved.",
                "If the nesting of models is deeper, all models at all levels starting from 0 are solved in a bottom-up manner.",
                "We briefly outline the recursive algorithm for solving agent is Algorithm for solving I-DID Input : level l ≥ 1 I-ID or level 0 ID, T Expansion Phase 1.",
                "For t from 1 to T − 1 do 2.",
                "If l ≥ 1 then Populate Mt+1 j,l−1 3.",
                "For each mt j in Range(Mt j,l−1) do 4.",
                "Recursively call algorithm with the l − 1 I-ID (or ID) that represents mt j and the horizon, T − t + 1 5.",
                "Map the decision node of the solved I-ID (or ID), OPT(mt j), to a chance node Aj 6.",
                "For each aj in OPT(mt j) do 7.",
                "For each oj in Oj (part of mt j) do 8.",
                "Update js belief, bt+1 j ← SE(bt j, aj, oj) 9. mt+1 j ← New I-ID (or ID) with bt+1 j as the initial belief 10.",
                "Range(Mt+1 j,l−1) ∪ ← {mt+1 j } 11.",
                "Add the model node, Mt+1 j,l−1, and the dependency links between Mt j,l−1 and Mt+1 j,l−1 (shown in Fig. 3(b)) 12.",
                "Add the chance, decision, and utility nodes for t + 1 time slice and the dependency links between them 13.",
                "Establish the CPTs for each chance node and utility node Look-Ahead Phase 14.",
                "Apply the standard look-ahead and backup method to solve the expanded I-DID Figure 5: Algorithm for solving a level l ≥ 0 I-DID. level l I-DID expanded over T time steps with one other agent j in Fig. 5.",
                "We adopt a two-phase approach: Given an I-ID of level l (described previously in Section 3) with all lower level models also represented as I-IDs or IDs (if level 0), the first step is to expand the level l I-ID over T time steps adding the dependency links and the conditional probability tables for each node.",
                "We particularly focus on establishing and populating the model nodes (lines 3-11).",
                "Note that Range(·) returns the values (lower level models) of the random variable given as input (model node).",
                "In the second phase, we use a standard look-ahead technique projecting the action and observation sequences over T time steps in the future, and backing up the utility values of the reachable beliefs.",
                "Similar to I-IDs, the I-DIDs reduce to DIDs in the absence of other agents.",
                "As we mentioned previously, the 0-th level models are the traditional DIDs.",
                "Their solutions provide probability distributions over actions of the agent modeled at that level to I-DIDs at level 1.",
                "Given probability distributions over other agents actions the level 1 IDIDs can themselves be solved as DIDs, and provide probability distributions to yet higher level models.",
                "Assume that the number of models considered at each level is bound by a number, M. Solving an I-DID of level l in then equivalent to solving O(Ml ) DIDs. 5.",
                "EXAMPLE APPLICATIONS To illustrate the usefulness of I-DIDs, we apply them to three problem domains.",
                "We describe, in particular, the formulation of the I-DID and the optimal prescriptions obtained on solving it. 5.1 Followership-Leadership in the Multiagent Tiger Problem We begin our illustrations of using I-IDs and I-DIDs with a slightly modified version of the multiagent tiger problem discussed in [9].",
                "The problem has two agents, each of which can open the right door (OR), the left door (OL) or listen (L).",
                "In addition to hearing growls (from the left (GL) or from the right (GR)) when they listen, the agents also hear creaks (from the left (CL), from the right (CR), or no creaks (S)), which noisily indicate the other agents opening one of the doors.",
                "When any door is opened, the tiger persists in its original location with a probability of 95%.",
                "Agent i hears growls with a reliability of 65% and creaks with a reliability of 95%.",
                "Agent j, on the other hand, hears growls with a reliability of 95%.",
                "Thus, the setting is such that agent i hears agent j opening doors more reliably than the tigers growls.",
                "This suggests that i could use js actions as an indication of the location of the tiger, as we discuss below.",
                "Each agents preferences are as in the single agent game discussed in [13].",
                "The transition, observation, and reward functions are shown in [16].",
                "A good indicator of the usefulness of normative methods for decision-making like I-DIDs is the emergence of realistic social behaviors in their prescriptions.",
                "In settings of the persistent multiagent tiger problem that reflect real world situations, we demonstrate followership between the agents and, as shown in [15], deception among agents who believe that they are in a follower-leader type of relationship.",
                "In particular, we analyze the situational and epistemological conditions sufficient for their emergence.",
                "The followership behavior, for example, results from the agent knowing its own weaknesses, assessing the strengths, preferences, and possible behaviors of the other, and realizing that its best for it to follow the others actions in order to maximize its payoffs.",
                "Let us consider a particular setting of the tiger problem in which agent i believes that js preferences are aligned with its own - both of them just want to get the gold - and js hearing is more reliable in comparison to itself.",
                "As an example, suppose that j, on listening can discern the tigers location 95% of the times compared to is 65% accuracy.",
                "Additionally, agent i does not have any initial information about the tigers location.",
                "In other words, is single-level nested belief, bi,1, assigns 0.5 to each of the two locations of the tiger.",
                "In addition, i considers two models of j, which differ in js flat level 0 initial beliefs.",
                "This is represented in the level 1 I-ID shown in Fig. 6(a).",
                "According to one model, j assigns a probability of 0.9 that the tiger is behind the left door, while the other 818 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Figure 6: (a) Level 1 I-ID of agent i, (b) two level 0 IDs of agent j whose decision nodes are mapped to the chance nodes, A1 j , A2 j , in (a). model assigns 0.1 to that location (see Fig. 6(b)).",
                "Agent i is undecided on these two models of j.",
                "If we vary is hearing ability, and solve the corresponding level 1 I-ID expanded over three time steps, we obtain the normative behavioral policies shown in Fig 7 that exhibit followership behavior.",
                "If is probability of correctly hearing the growls is 0.65, then as shown in the policy in Fig. 7(a), i begins to conditionally follow js actions: i opens the same door that j opened previously iff is own assessment of the tigers location confirms js pick.",
                "If i loses the ability to correctly interpret the growls completely, it blindly follows j and opens the same door that j opened previously (Fig. 7(b)).",
                "Figure 7: Emergence of (a) conditional followership, and (b) blind followership in the tiger problem.",
                "Behaviors of interest are in bold. * is a wildcard, and denotes any one of the observations.",
                "We observed that a single level of belief nesting - beliefs about the others models - was sufficient for followership to emerge in the tiger problem.",
                "However, the epistemological requirements for the emergence of leadership are more complex.",
                "For an agent, say j, to emerge as a leader, followership must first emerge in the other agent i.",
                "As we mentioned previously, if i is certain that its preferences are identical to those of j, and believes that j has a better sense of hearing, i will follow js actions over time.",
                "Agent j emerges as a leader if it believes that i will follow it, which implies that js belief must be nested two levels deep to enable it to recognize its leadership role.",
                "Realizing that i will follow presents j with an opportunity to influence is actions in the benefit of the collective good or its self-interest alone.",
                "For example, in the tiger problem, let us consider a setting in which if both i and j open the correct door, then each gets a payoff of 20 that is double the original.",
                "If j alone selects the correct door, it gets the payoff of 10.",
                "On the other hand, if both agents pick the wrong door, their penalties are cut in half.",
                "In this setting, it is in both js best interest as well as the collective betterment for j to use its expertise in selecting the correct door, and thus be a good leader.",
                "However, consider a slightly different problem in which j gains from is loss and is penalized if i gains.",
                "Specifically, let is payoff be subtracted from js, indicating that j is antagonistic toward i - if j picks the correct door and i the wrong one, then is loss of 100 becomes js gain.",
                "Agent j believes that i incorrectly thinks that js preferences are those that promote the collective good and that it starts off by believing with 99% confidence where the tiger is.",
                "Because i believes that its preferences are similar to those of j, and that j starts by believing almost surely that one of the two is the correct location (two level 0 models of j), i will start by following js actions.",
                "We show is normative policy on solving its singly-nested I-DID over three time steps in Fig. 8(a).",
                "The policy demonstrates that i will blindly follow js actions.",
                "Since the tiger persists in its original location with a probability of 0.95, i will select the same door again.",
                "If j begins the game with a 99% probability that the tiger is on the right, solving js I-DID nested two levels deep, results in the policy shown in Fig. 8(b).",
                "Even though j is almost certain that OL is the correct action, it will start by selecting OR, followed by OL.",
                "Agent js intention is to deceive i who, it believes, will follow js actions, so as to gain $110 in the second time step, which is more than what j would gain if it were to be honest.",
                "Figure 8: Emergence of deception between agents in the tiger problem.",
                "Behaviors of interest are in bold. * denotes as before. (a) Agent is policy demonstrating that it will blindly follow js actions. (b) Even though j is almost certain that the tiger is on the right, it will start by selecting OR, followed by OL, in order to deceive i. 5.2 Altruism and Reciprocity in the Public Good Problem The public good (PG) problem [7], consists of a group of M agents, each of whom must either contribute some resource to a public pot or keep it for themselves.",
                "Since resources contributed to the public pot are shared among all the agents, they are less valuable to the agent when in the public pot.",
                "However, if all agents choose to contribute their resources, then the payoff to each agent is more than if no one contributes.",
                "Since an agent gets its share of the public pot irrespective of whether it has contributed or not, the dominating action is for each agent to not contribute, and instead free ride on others contributions.",
                "However, behaviors of human players in empirical simulations of the PG problem differ from the normative predictions.",
                "The experiments reveal that many players initially contribute a large amount to the public pot, and continue to contribute when the PG problem is played repeatedly, though in decreasing amounts [4].",
                "Many of these experiments [5] report that a small core group of players persistently contributes to the public pot even when all others are defecting.",
                "These experiments also reveal that players who persistently contribute have altruistic or reciprocal preferences matching expected cooperation of others.",
                "For simplicity, we assume that the game is played between M = 2 agents, i and j.",
                "Let each agent be initially endowed with XT amount of resources.",
                "While the classical PG game formulation permits each agent to contribute any quantity of resources (≤ XT ) to the public pot, we simplify the action space by allowing two possible actions.",
                "Each agent may choose to either contribute (C) a fixed amount of the resources, or not contribute.",
                "The latter action is deThe Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 819 noted as defect (D).",
                "We assume that the actions are not observable to others.",
                "The value of resources in the public pot is discounted by ci for each agent i, where ci is the marginal private return.",
                "We assume that ci < 1 so that the agent does not benefit enough that it contributes to the public pot for private gain.",
                "Simultaneously, ciM > 1, making collective contribution pareto optimal. i/j C D C 2ciXT , 2cjXT ciXT − cp, XT + cjXT − P D XT + ciXT − P, cjXT − cp XT , XT Table 1: The one-shot PG game with punishment.",
                "In order to encourage contributions, the contributing agents punish free riders but incur a small cost for administering the punishment.",
                "Let P be the punishment meted out to the defecting agent and cp the non-zero cost of punishing for the contributing agent.",
                "For simplicity, we assume that the cost of punishing is same for both the agents.",
                "The one-shot PG game with punishment is shown in Table. 1.",
                "Let ci = cj, cp > 0, and if P > XT − ciXT , then defection is no longer a dominating action.",
                "If P < XT − ciXT , then defection is the dominating action for both.",
                "If P = XT − ciXT , then the game is not dominance-solvable.",
                "Figure 9: (a) Level 1 I-ID of agent i, (b) level 0 IDs of agent j with decision nodes mapped to the chance nodes, A1 j and A2 j , in (a).",
                "We formulate a sequential version of the PG problem with punishment from the perspective of agent i.",
                "Though in the repeated PG game, the quantity in the public pot is revealed to all the agents after each round of actions, we assume in our formulation that it is hidden from the agents.",
                "Each agent may contribute a fixed amount, xc, or defect.",
                "An agent on performing an action receives an observation of plenty (PY) or meager (MR) symbolizing the state of the public pot.",
                "Notice that the observations are also indirectly indicative of agent js actions because the state of the public pot is influenced by them.",
                "The amount of resources in agent is private pot, is perfectly observable to i.",
                "The payoffs are analogous to Table. 1.",
                "Borrowing from the empirical investigations of the PG problem [5], we construct level 0 IDs for j that model altruistic and non-altruistic types (Fig. 9(b)).",
                "Specifically, our altruistic agent has a high marginal private return (cj is close to 1) and does not punish others who defect.",
                "Let xc = 1 and the level 0 agent be punished half the times it defects.",
                "With one action remaining, both types of agents choose to contribute to avoid being punished.",
                "With two actions to go, the altruistic type chooses to contribute, while the other defects.",
                "This is because cj for the altruistic type is close to 1, thus the expected punishment, 0.5P > (1 − cj), which the altruistic type avoids.",
                "Because cj for the non-altruistic type is less, it prefers not to contribute.",
                "With three steps to go, the altruistic agent contributes to avoid punishment (0.5P > 2(1 − cj)), and the non-altruistic type defects.",
                "For greater than three steps, while the altruistic agent continues to contribute to the public pot depending on how close its marginal private return is to 1, the non-altruistic type prescribes defection.",
                "We analyzed the decisions of an altruistic agent i modeled using a level 1 I-DID expanded over 3 time steps. i ascribes the two level 0 models, mentioned previously, to j (see Fig. 9).",
                "If i believes with a probability 1 that j is altruistic, i chooses to contribute for each of the three steps.",
                "This behavior persists when i is unaware of whether j is altruistic (Fig. 10(a)), and when i assigns a high probability to j being the non-altruistic type.",
                "However, when i believes with a probability 1 that j is non-altruistic and will thus surely defect, i chooses to defect to avoid being punished and because its marginal private return is less than 1.",
                "These results demonstrate that the behavior of our altruistic type resembles that found experimentally.",
                "The non-altruistic level 1 agent chooses to defect regardless of how likely it believes the other agent to be altruistic.",
                "We analyzed the behavior of a reciprocal agent type that matches expected cooperation or defection.",
                "The reciprocal types marginal private return is similar to that of the non-altruistic type, however, it obtains a greater payoff when its action is similar to that of the other.",
                "We consider the case when the reciprocal agent i is unsure of whether j is altruistic and believes that the public pot is likely to be half full.",
                "For this prior belief, i chooses to defect.",
                "On receiving an observation of plenty, i decides to contribute, while an observation of meager makes it defect (Fig. 10(b)).",
                "This is because an observation of plenty signals that the pot is likely to be greater than half full, which results from js action to contribute.",
                "Thus, among the two models ascribed to j, its type is likely to be altruistic making it likely that j will contribute again in the next time step.",
                "Agent i therefore chooses to contribute to reciprocate js action.",
                "An analogous reasoning leads i to defect when it observes a meager pot.",
                "With one action to go, i believing that j contributes, will choose to contribute too to avoid punishment regardless of its observations.",
                "Figure 10: (a) An altruistic level 1 agent always contributes. (b) A reciprocal agent i starts off by defecting followed by choosing to contribute or defect based on its observation of plenty (indicating that j is likely altruistic) or meager (j is non-altruistic). 5.3 Strategies in Two-Player Poker Poker is a popular zero sum card game that has received much attention among the AI research community as a testbed [2].",
                "Poker is played among M ≥ 2 players in which each player receives a hand of cards from a deck.",
                "While several flavors of Poker with varying complexity exist, we consider a simple version in which each player has three plys during which the player may either exchange a card (E), keep the existing hand (K), fold (F) and withdraw from the game, or call (C), requiring all players to show their hands.",
                "To keep matters simple, let M = 2, and each player receive a hand consisting of a single card drawn from the same suit.",
                "Thus, during a showdown, the player who has the numerically larger card (2 is the lowest, ace is the highest) wins the pot.",
                "During an exchange of cards, the discarded card is placed either in the L pile, indicating to the other agent that it was a low numbered card less than 8, or in the 820 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) H pile, indicating that the card had a rank greater than or equal to 8.",
                "Notice that, for example, if a lower numbered card is discarded, the probability of receiving a low card in exchange is now reduced.",
                "We show the level 1 I-ID for the simplified two-player Poker in Fig. 11.",
                "We considered two models (personality types) of agent j.",
                "The conservative type believes that it is likely that its opponent has a high numbered card in its hand.",
                "On the other hand, the aggressive agent j believes with a high probability that its opponent has a lower numbered card.",
                "Thus, the two types differ in their beliefs over their opponents hand.",
                "In both these level 0 models, the opponent is assumed to perform its actions following a fixed, uniform distribution.",
                "With three actions to go, regardless of its hand (unless it is an ace), the aggressive agent chooses to exchange its card, with the intent of improving on its current hand.",
                "This is because it believes the other to have a low card, which improves its chances of getting a high card during the exchange.",
                "The conservative agent chooses to keep its card, no matter its hand because its chances of getting a high card are slim as it believes that its opponent has one.",
                "Figure 11: (a) Level 1 I-ID of agent i.",
                "The observation reveals information about js hand of the previous time step, (b) level 0 IDs of agent j whose decision nodes are mapped to the chance nodes, A1 j , A2 j , in (a).",
                "The policy of a level 1 agent i who believes that each card except its own has an equal likelihood of being in js hand (neutral personality type) and j could be either an aggressive or conservative type, is shown in Fig. 12. is own hand contains the card numbered 8.",
                "The agent starts by keeping its card.",
                "On seeing that j did not exchange a card (N), i believes with probability 1 that j is conservative and hence will keep its cards. i responds by either keeping its card or exchanging it because j is equally likely to have a lower or higher card.",
                "If i observes that j discarded its card into the L or H pile, i believes that j is aggressive.",
                "On observing L, i realizes that j had a low card, and is likely to have a high card after its exchange.",
                "Because the probability of receiving a low card is high now, i chooses to keep its card.",
                "On observing H, believing that the probability of receiving a high numbered card is high, i chooses to exchange its card.",
                "In the final step, i chooses to call regardless of its observation history because its belief that j has a higher card is not sufficiently high to conclude that its better to fold and relinquish the payoff.",
                "This is partly due to the fact that an observation of, say, L resets the agent is previous time step beliefs over js hand to the low numbered cards only. 6.",
                "DISCUSSION We showed how DIDs may be extended to I-DIDs that enable <br>online sequential decision-making</br> in uncertain multiagent settings.",
                "Our graphical representation of I-DIDs improves on the previous Figure 12: A level 1 agent is three step policy in the Poker problem. i starts by believing that j is equally likely to be aggressive or conservative and could have any card in its hand with equal probability. work significantly by being more transparent, semantically clear, and capable of being solved using standard algorithms that target DIDs.",
                "I-DIDs extend NIDs to allow sequential decision-making over multiple time steps in the presence of other interacting agents.",
                "I-DIDs may be seen as concise graphical representations for IPOMDPs providing a way to exploit problem structure and carry out online decision-making as the agent acts and observes given its prior beliefs.",
                "We are currently investigating ways to solve I-DIDs approximately with provable bounds on the solution quality.",
                "Acknowledgment: We thank Piotr Gmytrasiewicz for some useful discussions related to this work.",
                "The first author would like to acknowledge the support of a UGARF grant. 7.",
                "REFERENCES [1] R. J. Aumann.",
                "Interactive epistemology i: Knowledge.",
                "International Journal of Game Theory, 28:263-300, 1999. [2] D. Billings, A. Davidson, J. Schaeffer, and D. Szafron.",
                "The challenge of poker.",
                "AIJ, 2001. [3] A. Brandenburger and E. Dekel.",
                "Hierarchies of beliefs and common knowledge.",
                "Journal of Economic Theory, 59:189-198, 1993. [4] C. Camerer.",
                "Behavioral Game Theory: Experiments in Strategic Interaction.",
                "Princeton University Press, 2003. [5] E. Fehr and S. Gachter.",
                "Cooperation and punishment in public goods experiments.",
                "American Economic Review, 90(4):980-994, 2000. [6] D. Fudenberg and D. K. Levine.",
                "The Theory of Learning in Games.",
                "MIT Press, 1998. [7] D. Fudenberg and J. Tirole.",
                "Game Theory.",
                "MIT Press, 1991. [8] Y. Gal and A. Pfeffer.",
                "A language for modeling agents decision-making processes in games.",
                "In AAMAS, 2003. [9] P. Gmytrasiewicz and P. Doshi.",
                "A framework for sequential planning in multiagent settings.",
                "JAIR, 24:49-79, 2005. [10] P. Gmytrasiewicz and E. Durfee.",
                "Rational coordination in multi-agent environments.",
                "JAAMAS, 3(4):319-350, 2000. [11] J. C. Harsanyi.",
                "Games with incomplete information played by bayesian players.",
                "Management Science, 14(3):159-182, 1967. [12] R. A. Howard and J. E. Matheson.",
                "Influence diagrams.",
                "In R. A. Howard and J. E. Matheson, editors, The Principles and Applications of Decision Analysis.",
                "Strategic Decisions Group, Menlo Park, CA 94025, 1984. [13] L. Kaelbling, M. Littman, and A. Cassandra.",
                "Planning and acting in partially observable stochastic domains.",
                "Artificial Intelligence Journal, 2, 1998. [14] D. Koller and B. Milch.",
                "Multi-agent influence diagrams for representing and solving games.",
                "In IJCAI, pages 1027-1034, 2001. [15] K. Polich and P. Gmytrasiewicz.",
                "Interactive dynamic influence diagrams.",
                "In GTDT Workshop, AAMAS, 2006. [16] B.",
                "Rathnas., P. Doshi, and P. J. Gmytrasiewicz.",
                "Exact solutions to interactive pomdps using behavioral equivalence.",
                "In Autonomous Agents and Multi-Agent Systems Conference (AAMAS), 2006. [17] S. Russell and P. Norvig.",
                "Artificial Intelligence: A Modern Approach (Second Edition).",
                "Prentice Hall, 2003. [18] R. D. Shachter.",
                "Evaluating influence diagrams.",
                "Operations Research, 34(6):871-882, 1986. [19] D. Suryadi and P. Gmytrasiewicz.",
                "Learning models of other agents using influence diagrams.",
                "In UM, 1999.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 821"
            ],
            "original_annotated_samples": [
                "DISCUSSION We showed how DIDs may be extended to I-DIDs that enable <br>online sequential decision-making</br> in uncertain multiagent settings."
            ],
            "translated_annotated_samples": [
                "DISCUSIÓN Mostramos cómo los DIDs pueden ser extendidos a los I-DIDs que permiten la <br>toma de decisiones secuenciales en línea</br> en entornos multiagentes inciertos."
            ],
            "translated_text": "Modelos gráficos para soluciones en línea a POMDP interactivos de Prashant Doshi Dept. Departamento de Ciencias de la Computación Universidad de Georgia Athens, GA 30602, EE. UU. pdoshi@cs.uga.edu Yifeng Zeng Dept. de Ciencias de la Computación Universidad de Aalborg DK-9220 Aalborg, Dinamarca yfzeng@cs.aau.edu Qiongyu Chen Dept. de Ciencias de la Computación Universidad Nacional de Singapur 117543, Singapur chenqy@comp.nus.edu.sg RESUMEN Desarrollamos una nueva representación gráfica para procesos de decisión de Markov parcialmente observables interactivos (I-POMDPs) que es significativamente más transparente y semánticamente clara que la representación anterior. Estos modelos gráficos llamados diagramas de influencia dinámica interactiva (I-DIDs) buscan modelar explícitamente la estructura que a menudo está presente en problemas del mundo real al descomponer la situación en variables de probabilidad y decisión, y las dependencias entre las variables. Los I-DIDs generalizan los DIDs, los cuales pueden ser vistos como representaciones gráficas de POMDPs, a entornos multiagentes de la misma manera que los I-POMDPs generalizan los POMDPs. Los I-DIDs pueden ser utilizados para calcular la política de un agente en línea mientras el agente actúa y observa en un entorno poblado por otros agentes que interactúan entre sí. Usando varios ejemplos, mostramos cómo se pueden aplicar los I-DIDs y demostramos su utilidad. Categorías y Descriptores de Asignaturas I.2.11 [Inteligencia Artificial Distribuida]: Sistemas Multiagentes Términos Generales Teoría 1. Las Procesos de Decisión de Markov Interactivos Parcialmente Observables (IPOMDPs) proporcionan un marco para la toma de decisiones secuenciales en entornos multiagentes parcialmente observables. Generalizan los POMDPs [13] a entornos multiagentes al incluir los modelos computables de los otros agentes en el espacio de estados junto con los estados del entorno físico. Los modelos abarcan toda la información que influye en los comportamientos de los agentes, incluyendo sus preferencias, capacidades y creencias, y por lo tanto son análogos a los tipos en los juegos bayesianos [11]. I-POMDPs adoptan un enfoque subjetivo para comprender el comportamiento estratégico, arraigado en un marco de teoría de decisiones que toma la perspectiva de los tomadores de decisiones en la interacción. En [15], Polich y Gmytrasiewicz introdujeron diagramas de influencia dinámica interactivos (I-DIDs) como las representaciones computacionales de I-POMDPs. Los I-DIDs generalizan los DIDs [12], los cuales pueden ser vistos como contrapartes computacionales de POMDPs, a entornos de múltiples agentes de la misma manera que los I-POMDPs generalizan los POMDPs. Los I-DIDs contribuyen a una creciente línea de trabajo [19] que incluye diagramas de influencia multiagente (MAIDs) [14], y más recientemente, redes de diagramas de influencia (NIDs) [8]. Estos formalismos buscan modelar explícitamente la estructura que a menudo está presente en problemas del mundo real al descomponer la situación en variables de azar y decisiones, y las dependencias entre las variables. Los MAIDs proporcionan una alternativa a las formas normales y extensivas de juego utilizando un formalismo gráfico para representar juegos de información imperfecta con un nodo de decisión para las acciones de cada agente y nodos de azar que capturan la información privada de los agentes. Los MAIDs analizan objetivamente el juego, calculando eficientemente el perfil de equilibrio de Nash al explotar la estructura de independencia. NIDs extienden los MAIDs para incluir la incertidumbre de los agentes sobre el juego que se está jugando y sobre los modelos de los otros agentes. Cada modelo es un MAID y la red de MAIDs se colapsa, de abajo hacia arriba, en un solo MAID para calcular el equilibrio del juego teniendo en cuenta los diferentes modelos de cada agente. Los formalismos gráficos como MAIDs y NIDs abren una área de investigación prometedora que tiene como objetivo representar las interacciones multiagente de manera más transparente. Sin embargo, los MAIDs proporcionan un análisis del juego desde un punto de vista externo y la aplicabilidad de ambos está limitada a juegos estáticos de un solo jugador. Los asuntos son más complejos cuando consideramos interacciones que se extienden en el tiempo, donde las predicciones sobre las acciones futuras de otros deben hacerse utilizando modelos que cambian a medida que los agentes actúan y observan. Los I-DIDs abordan esta brecha al permitir la representación de modelos de otros agentes como los valores de un nodo de modelo especial. Tanto los modelos de otros agentes como las creencias originales de los agentes sobre estos modelos se actualizan con el tiempo utilizando implementaciones especializadas. En este documento, mejoramos la representación preliminar previa del I-DID mostrada en [15] utilizando la idea de que el I-ID estático es un tipo de NID. Por lo tanto, podemos utilizar construcciones de lenguaje específicas de NID, como multiplexores, para representar de manera más transparente el nodo del modelo y, posteriormente, el I-ID. Además, aclaramos la semántica del enlace de política de propósito especial introducido en la representación de I-DID por [15], y mostramos que podría ser reemplazado por enlaces de dependencia tradicionales. En la representación previa del I-DID, la actualización de la creencia de los agentes sobre los modelos de los demás a medida que los agentes actúan y reciben observaciones se denotaba utilizando un enlace especial llamado el enlace de actualización del modelo que conectaba los nodos del modelo a lo largo del tiempo. Explicamos la semántica de este enlace mostrando cómo puede ser implementado utilizando los enlaces de dependencia tradicionales entre los nodos de probabilidad que constituyen los nodos del modelo. El resultado final es una representación de I-DID que es significativamente más transparente, semánticamente clara y capaz de ser implementada utilizando los algoritmos estándar para resolver DIDs. Mostramos cómo los IDIDs pueden ser utilizados para modelar la incertidumbre de un agente sobre los modelos de otros, que a su vez pueden ser I-DIDs. La solución al I-DID es una política que prescribe lo que el agente debe hacer con el tiempo, dadas sus creencias sobre el estado físico y otros modelos. Análogos a los DIDs, los I-DIDs pueden ser utilizados para calcular la política de un agente en línea mientras el agente actúa y observa en un entorno poblado por otros agentes interactuantes. 2. ANTECEDENTES: Los IPOMDPS ANIDADOS FINITAMENTE generalizan los POMDPs a entornos multiagentes al incluir los modelos de otros agentes como parte del espacio de estados [9]. Dado que otros agentes también pueden razonar sobre otros, el espacio de estado interactivo está estratégicamente anidado; contiene creencias sobre los modelos de otros agentes y sus creencias sobre los demás. Para simplificar la presentación, consideramos un agente, i, que está interactuando con otro agente, j. Un I-POMDP finitamente anidado del agente i con un nivel de estrategia l se define como la tupla: I-POMDPi,l = ISi,l, A, Ti, Ωi, Oi, Ri donde: • ISi,l denota un conjunto de estados interactivos definido como, ISi,l = S × Mj,l−1, donde Mj,l−1 = {Θj,l−1 ∪ SMj}, para l ≥ 1, e ISi,0 = S, donde S es el conjunto de estados del entorno físico. Θj,l−1 es el conjunto de modelos intencionales computables del agente j: θj,l−1 = bj,l−1, ˆθj donde el marco, ˆθj = A, Ωj, Tj, Oj, Rj, OCj. Aquí, j es Bayesiano racional y OCj es el criterio de optimalidad de j. SMj es el conjunto de modelos subintencionales de j. Ejemplos simples de modelos subintencionales incluyen un modelo sin información [10] y un modelo de juego ficticio [6], ambos de los cuales son independientes de la historia. Damos una construcción recursiva de abajo hacia arriba del espacio de estados interactivo a continuación. Sí,0 = S, Θj,0 = { bj,0, ˆθj | bj,0 ∈ Δ(ISj,0)} Sí,1 = S × {Θj,0 ∪ SMj}, Θj,1 = { bj,1, ˆθj | bj,1 ∈ Δ(ISj,1)} . . . . . . Sí, l = S × {Θj,l−1 ∪ SMj}, Θj,l = { bj,l, ˆθj | bj,l ∈ Δ(ISj,l)} Formulaciones similares de espacios anidados han aparecido en [1, 3]. • A = Ai × Aj es el conjunto de acciones conjuntas de todos los agentes en el entorno; • Ti : S ×A×S → [0, 1], describe el efecto de las acciones conjuntas en los estados físicos del entorno; • Ωi es el conjunto de observaciones del agente i; • Oi : S × A × Ωi → [0, 1] da la probabilidad de las observaciones dadas el estado físico y la acción conjunta; • Ri : ISi × A → R describe las preferencias del agente sobre sus estados interactivos. Normalmente solo importarán los estados físicos. La política del agente es el mapeo, Ω∗ i → Δ(Ai), donde Ω∗ i es el conjunto de todos los historiales de observación del agente i. Dado que la creencia sobre los estados interactivos forma una estadística suficiente [9], la política también puede ser representada como un mapeo del conjunto de todas las creencias del agente i a una distribución sobre sus acciones, Δ(ISi) → Δ(Ai). 2.1 Actualización de Creencias Análogo a los POMDPs, un agente dentro del marco I-POMDP actualiza su creencia a medida que actúa y observa. Sin embargo, hay dos diferencias que complican la actualización de creencias en entornos multiagentes en comparación con los entornos de un solo agente. Primero, dado que el estado del entorno físico depende de las acciones de ambos agentes, la predicción de cómo cambia el estado físico debe hacerse basándose en la predicción de sus acciones. Segundo, los cambios en los modelos de js deben ser incluidos en la actualización de creencias. Específicamente, si j es intencional, entonces se debe incluir una actualización de las creencias de j debido a su acción y observación. En otras palabras, i tiene que actualizar su creencia basándose en su predicción de lo que j observaría y cómo j actualizaría su creencia. Si el modelo de js es subintencional, entonces las observaciones probables de js se añaden al historial de observaciones contenido en el modelo. Formalmente, tenemos: Pr(ist |at−1 i , bt−1 i,l ) = β ISt−1:mt−1 j =θt j bt−1 i,l (ist−1 ) × at−1 j Pr(at−1 j |θt−1 j,l−1)Oi(st , at−1 i , at−1 j , ot i) ×Ti(st−1 , at−1 i , at−1 j , st ) ot j Oj(st , at−1 i , at−1 j , ot j) ×τ(SEθt j (bt−1 j,l−1, at−1 j , ot j) − bt j,l−1) (1) donde β es la constante de normalización, τ es 1 si su argumento es 0, de lo contrario es 0, Pr(at−1 j |θt−1 j,l−1) es la probabilidad de que at−1 j sea Bayes racional para el agente descrito por el modelo θt−1 j,l−1, y SE(·) es una abreviatura para la actualización de creencias. Para una versión de la actualización de creencias cuando el modelo js es subintencional, consulte [9]. Si el agente j también se modela como un I-POMDP, entonces la actualización de creencias invoca la actualización de creencias de j (a través del término SEθt j ( bt−1 j,l−1 , at−1 j , ot j)), lo que a su vez podría invocar la actualización de creencias de is y así sucesivamente. Esta recursión en la anidación de creencias llega al nivel 0. En este nivel, la actualización de creencias del agente se reduce a una actualización de creencias de un POMDP. Para ilustraciones de la actualización de creencias, detalles adicionales sobre I-POMDPs y cómo se comparan con otros marcos multiagentes, consulte [9]. Iteración de Valor Cada estado de creencia en un I-POMDP finitamente anidado tiene un valor asociado que refleja la máxima ganancia que el agente puede esperar en este estado de creencia: Un ( bi,l, θi ) = max ai∈Ai is∈ISi,l ERi(is, ai)bi,l(is)+ γ oi∈Ωi Pr(oi|ai, bi,l)Un−1 ( SEθi (bi,l, ai, oi), θi ) (2) donde, ERi(is, ai) = aj Ri(is, ai, aj)Pr(aj|mj,l−1) (ya que is = (s, mj,l−1)). La ecuación 2 es una base para la iteración de valor en I-POMDPs. La acción óptima del agente, a∗ i, para el caso de horizonte finito con descuento, es un elemento del conjunto de acciones óptimas para el estado de creencia, OPT(θi), definido como: OPT( bi,l, θi ) = argmax ai∈Ai is∈ISi,l ERi(is, ai)bi,l(is) +γ oi∈Ωi Pr(oi|ai, bi,l)Un ( SEθi (bi,l, ai, oi), θi ) (3) 3. Diagramas de influencia interactivos. Una extensión ingenua de los diagramas de influencia (IDs) a entornos poblados por múltiples agentes es posible tratando a los otros agentes como autómatas, representados mediante nodos de probabilidad. Sin embargo, este enfoque asume que las acciones de los agentes están controladas utilizando una distribución de probabilidad que no cambia con el tiempo. Los diagramas de influencia interactivos (I-IDs) adoptan un enfoque más sofisticado al generalizar los IDs para hacerlos aplicables a entornos compartidos con otros agentes que pueden actuar, observar y actualizar sus creencias. 3.1 Sintaxis Además de los nodos habituales de probabilidad, decisión y utilidad, los IIDs incluyen un nuevo tipo de nodo llamado nodo de modelo. Mostramos un nivel general l I-ID en la Fig. 1(a), donde el nodo del modelo (Mj,l−1) está representado con un hexágono. Observamos que la distribución de probabilidad sobre el nodo de probabilidad, S, y el nodo del modelo juntos representan la creencia del agente sobre sus estados interactivos. Además del modelo 1, el modelo de nivel 0 es un POMDP: las acciones de otros agentes se tratan como eventos exógenos y se incorporan en las funciones T, O y R. El Sexto Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) Figura 1: (a) Un nivel genérico l I-ID para el agente i situado con otro agente j. El hexágono es el nodo modelo (Mj,l−1) cuya estructura mostramos en (b). Los miembros del nodo modelo son ellos mismos I-ID (m1 j,l−1, m2 j,l−1; los diagramas no se muestran aquí por simplicidad) cuyos nodos de decisión se asignan a los nodos de probabilidad correspondientes (A1 j , A2 j). Dependiendo del valor del nodo, Mod[Mj], la distribución de cada uno de los nodos de probabilidad se asigna al nodo Aj. (c) El I-ID transformado con el nodo del modelo reemplazado por los nodos de probabilidad y las relaciones entre ellos. Los I-IDs difieren de los IDs al tener un enlace discontinuo (llamado enlace de política en [15]) entre el nodo del modelo y un nodo de probabilidad, Aj, que representa la distribución sobre las acciones de los otros agentes dada su modelo. En ausencia de otros agentes, el nodo del modelo y el nodo de probabilidad, Aj, desaparecen y los I-ID se convierten en ID tradicionales. El nodo del modelo contiene los modelos computacionales alternativos atribuidos por i al otro agente del conjunto, Θj,l−1 ∪ SMj, donde Θj,l−1 y SMj fueron definidos previamente en la Sección 2. Por lo tanto, un modelo en el nodo del modelo puede ser en sí mismo un I-ID o ID, y la recursión termina cuando un modelo es un ID o subintencional. Dado que el nodo del modelo contiene los modelos alternativos del otro agente como sus valores, su representación no es trivial. En particular, algunos de los modelos dentro del nodo son I-IDs que, al resolverse, generan la política óptima de los agentes en sus nodos de decisión. Cada nodo de decisión se asigna al nodo de probabilidad correspondiente, digamos A1 j, de la siguiente manera: si OPT es el conjunto de acciones óptimas obtenidas al resolver el I-ID (o ID), entonces Pr(aj ∈ A1 j) = 1 |OPT| si aj ∈ OPT, 0 en caso contrario. Tomando prestados los conocimientos de trabajos anteriores [8], observamos que el nodo del modelo y el enlace de política discontinua que lo conecta con el nodo de probabilidad, Aj, podrían representarse como se muestra en la Fig. 1(b). El nodo de decisión de cada nivel l − 1 I-ID se transforma en un nodo de probabilidad, como mencionamos anteriormente, de modo que las acciones con el mayor valor en el nodo de decisión se les asignan probabilidades uniformes en el nodo de probabilidad, mientras que al resto se les asigna probabilidad cero. Los diferentes nodos de probabilidad (A1 j , A2 j ), uno para cada modelo, y adicionalmente, el nodo de probabilidad etiquetado Mod[Mj] forman los padres del nodo de probabilidad, Aj. Por lo tanto, hay tantos nodos de acción (A1 j , A2 j ) en Mj,l−1 como el número de modelos en el soporte de las creencias del agente. La tabla de probabilidad condicional del nodo de probabilidad, Aj, es un multiplexor que asume la distribución de cada uno de los nodos de acción (A1 j , A2 j ) dependiendo del valor de Mod[Mj]. Los valores de Mod[Mj] denotan los diferentes modelos de j. En otras palabras, cuando Mod[Mj] tiene el valor m1 j,l−1, el nodo de probabilidad Aj asume la distribución del nodo A1 j, y Aj asume la distribución de A2 j cuando Mod[Mj] tiene el valor m2 j,l−1. La distribución sobre el nodo, Mod[Mj], es la creencia del agente sobre los modelos de j dado un estado físico. Para más agentes, tendremos tantos nodos de modelo como agentes haya. Observa que la Fig. 1(b) aclara la semántica del enlace de política y muestra cómo puede ser representado utilizando los enlaces de dependencia tradicionales. En la Fig. 1(c), mostramos el I-ID transformado cuando el nodo del modelo es reemplazado por los nodos de probabilidad y las relaciones entre ellos. A diferencia de la representación en [15], no hay enlaces de políticas especiales, sino que el I-ID está compuesto únicamente por aquellos tipos de nodos que se encuentran en IDs tradicionales y relaciones de dependencia entre los nodos. Esto permite que los I-IDs sean representados e implementados utilizando herramientas de aplicación convencionales que apuntan a los IDs. Ten en cuenta que podemos ver el nivel l I-ID como un NID. Específicamente, cada uno de los modelos del nivel l − 1 dentro del nodo del modelo son bloques en el NID (ver Fig. 2). Si el nivel l = 1, cada bloque es un ID tradicional, de lo contrario, si l > 1, cada bloque dentro del NID puede ser un NID en sí mismo. Ten en cuenta que dentro de los I-IDs (o IDs) en cada nivel, solo hay un nodo de decisión único. Por lo tanto, nuestro NID no contiene ningún MAID. Figura 2: Un nivel l I-ID representado como un NID. Las probabilidades asignadas a los bloques del NID son creencias sobre modelos js condicionados a un estado físico. 3.2 Solución La solución de un I-ID procede de manera ascendente y se implementa de forma recursiva. Comenzamos resolviendo los modelos de nivel 0, que, si son intencionales, son identificadores tradicionales. Sus soluciones proporcionan distribuciones de probabilidad sobre las acciones de los otros agentes, las cuales se ingresan en los nodos de probabilidad correspondientes encontrados en el nodo del modelo del nivel 1 I-ID. El mapeo de los nodos de decisión de los modelos de nivel 0 a los nodos de probabilidad se realiza de manera que las acciones con el mayor valor en el nodo de decisión se les asignan probabilidades uniformes en el nodo de probabilidad, mientras que el resto se les asigna probabilidad cero. Dadas las distribuciones sobre las acciones dentro de los diferentes nodos de probabilidad (uno para cada modelo del otro agente), el I-ID de nivel 1 se transforma como se muestra en la Fig. 1(c). Durante la transformación, la tabla de probabilidad condicional (CPT) del nodo, Aj, se llena de tal manera que el nodo asume la distribución de cada uno de los nodos de probabilidad dependiendo del valor del nodo, Mod[Mj]. Como mencionamos anteriormente, los valores del nodo Mod[Mj] denotan los diferentes modelos del otro agente, y su distribución es la creencia del agente sobre los modelos de j condicionados al estado físico. El nivel 1 I-ID transformado es un ID tradicional que puede ser resuelto como us816 The Sixth Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) (a) (b) Figura 3: (a) Un I-DID genérico de dos niveles de tiempo para el agente i en un entorno con otro agente j. Observa el enlace de actualización del modelo punteado que denota la actualización de los modelos de j y la distribución de los modelos a lo largo del tiempo. (b) La semántica del enlace de actualización del modelo utilizando el método estándar de maximización de utilidad esperada [18]. Este procedimiento se lleva a cabo hasta el nivel l I-ID cuya solución proporciona el conjunto no vacío de acciones óptimas que el agente debe realizar dada su creencia. Ten en cuenta que, al igual que los IDs, los I-IDs son adecuados para la toma de decisiones en línea cuando se conoce la creencia actual de los agentes. 4. Diagramas de influencia dinámica interactivos (I-DIDs) Los diagramas de influencia dinámica interactivos (I-DIDs) extienden los I-IDs (y NIDs) para permitir la toma de decisiones secuencial a lo largo de varios pasos de tiempo. Así como los DIDs son representaciones gráficas estructuradas de POMDPs, los I-DIDs son los análogos gráficos en línea para I-POMDPs finitamente anidados. Los I-DIDs pueden ser utilizados para optimizar sobre una mirada anticipada finita dadas las creencias iniciales mientras se interactúa con otros agentes, posiblemente similares. 4.1 Sintaxis Representamos un I-DID de dos cortes temporales en la Fig. 3(a). Además de los nodos del modelo y el enlace de política discontinuo, lo que diferencia a un I-DID de un DID es el enlace de actualización del modelo mostrado como una flecha punteada en la Fig. 3(a). Explicamos la semántica del nodo del modelo y el enlace de la política en la sección anterior; a continuación, describimos las actualizaciones del modelo. La actualización del nodo del modelo con el tiempo implica dos pasos: primero, dados los modelos en el tiempo t, identificamos el conjunto actualizado de modelos que residen en el nodo del modelo en el tiempo t + 1. Recuerda de la Sección 2 que el modelo intencional de un agente incluye sus creencias. Debido a que los agentes actúan y reciben observaciones, sus modelos se actualizan para reflejar sus creencias cambiadas. Dado que el conjunto de acciones óptimas para un modelo podría incluir todas las acciones, y el agente podría recibir cualquiera de las |Ωj| posibles observaciones, el conjunto actualizado en el paso de tiempo t + 1 tendrá como máximo |Mt j,l−1||Aj||Ωj| modelos. Aquí, |Mt j,l−1| es el número de modelos en el paso de tiempo t, |Aj| y |Ωj| son los espacios más grandes de acciones y observaciones respectivamente, entre todos los modelos. Segundo, calculamos la nueva distribución sobre los modelos actualizados dados la distribución original y la probabilidad de que el agente realice la acción y reciba la observación que llevó al modelo actualizado. Estos pasos son parte de la actualización de creencias del agente formalizada utilizando la Ecuación 1. En la Fig. 3(b), mostramos cómo se implementa el enlace de actualización del modelo punteado en el I-DID. Si cada uno de los dos modelos de nivel l − 1 atribuidos a j en el paso de tiempo t resulta en una acción, y j podría hacer una de dos posibles observaciones, entonces el nodo del modelo en el paso de tiempo t + 1 contiene cuatro modelos actualizados (mt+1,1 j,l−1 ,mt+1,2 j,l−1 , mt+1,3 j,l−1 , y mt+1,4 j,l−1 ). Estos modelos difieren en sus creencias iniciales, cada una de las cuales es el resultado de j actualizar sus creencias debido a su acción y una posible observación. Los nodos de decisión en cada uno de los I-DIDs o DIDs que representan los modelos de nivel inferior se asignan a la Figura 4 correspondiente: I-DID transformado con los nodos del modelo y el enlace de actualización del modelo reemplazados por los nodos de probabilidad y las relaciones (en negrita) de los nodos de probabilidad, como se mencionó anteriormente. A continuación, describimos cómo se calcula la distribución sobre el conjunto actualizado de modelos (la distribución sobre el nodo de probabilidad Mod[Mt+1 j ] en Mt+1 j,l−1). La probabilidad de que el modelo actualizado js sea, digamos mt+1,1 j,l−1, depende de la probabilidad de que j realice la acción y reciba la observación que condujo a este modelo, y de la distribución previa sobre los modelos en el paso de tiempo t. Dado que el nodo de probabilidad At j asume la distribución de cada uno de los nodos de acción basados en el valor de Mod[Mt j], la probabilidad de la acción está dada por este nodo de probabilidad. Para obtener la probabilidad de una observación posible js, introducimos el nodo de probabilidad Oj, que dependiendo del valor de Mod[Mt j], asume la distribución del nodo de observación en el modelo de nivel inferior denominado Mod[Mt j]. Dado que la probabilidad de las observaciones js depende del estado físico y de las acciones conjuntas de ambos agentes, el nodo Oj está vinculado con St+1, At j y At i. De manera análoga a At j, la tabla de probabilidad condicional de Oj también es un multiplexor modulado por Mod[Mt j]. Finalmente, la distribución de los modelos previos en el tiempo t se obtiene a partir del nodo de probabilidad, Mod[Mt j ] en Mt j,l−1. Por consiguiente, los nodos de probabilidad, Mod[Mt j], At j y Oj, forman los padres de Mod[Mt+1 j] en Mt+1 j,l−1. Ten en cuenta que el enlace de actualización del modelo puede ser reemplazado por los enlaces de dependencia entre los nodos de probabilidad que constituyen los nodos del modelo en las dos etapas temporales. En la Fig. 4 mostramos los dos cortes temporales I-DID con los nodos del modelo reemplazados por los nodos de probabilidad y las relaciones entre ellos. Los nodos de probabilidad y los enlaces de dependencia que no están en negrita son estándar, generalmente encontrados en DIDs. La expansión del I-DID a lo largo de más pasos de tiempo requiere la repetición de los dos pasos de actualizar el conjunto de modelos que forman el 2. Nota que Oj representa la observación j en el tiempo t + 1. El Sexto Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 817 valores del nodo del modelo y añadiendo las relaciones entre los nodos de probabilidad, tantas veces como enlaces de actualización del modelo haya. Observamos que el conjunto posible de modelos del otro agente j crece exponencialmente con el número de pasos de tiempo. Por ejemplo, después de T pasos, puede haber como máximo |Mt=1 j,l−1|(|Aj||Ωj|)T −1 modelos candidatos residiendo en el nodo del modelo. 4.2 Solución Análoga a los I-ID, la solución a un I-DID de nivel l para el agente i expandido a lo largo de T pasos de tiempo puede llevarse a cabo de forma recursiva. Con el propósito de ilustración, dejemos l=1 y T=2. El método de solución utiliza la técnica estándar de anticipación, proyectando las secuencias de acciones y observaciones de los agentes hacia adelante desde el estado de creencia actual [17], y encontrando las posibles creencias que podría tener en el siguiente paso temporal. Dado que el agente i también tiene una creencia sobre los modelos de j, la búsqueda anticipada incluye descubrir los posibles modelos que j podría tener en el futuro. Por consiguiente, cada uno de los modelos subintencionales o de nivel 0 de js (representados utilizando un DID estándar) en el primer paso de tiempo debe resolverse para obtener su conjunto óptimo de acciones. Estas acciones se combinan con el conjunto de observaciones posibles que j podría hacer en ese modelo, lo que resulta en un conjunto actualizado de modelos candidatos (que incluyen las creencias actualizadas) que podrían describir el comportamiento de j. Las creencias sobre este conjunto actualizado de modelos candidatos se calculan utilizando los métodos estándar de inferencia que utilizan las relaciones de dependencia entre los nodos del modelo, como se muestra en la Figura 3(b). Observamos la naturaleza recursiva de esta solución: al resolver el agente de nivel 1 I-DID, los DIDs de nivel 0 deben ser resueltos. Si el anidamiento de los modelos es más profundo, todos los modelos en todos los niveles, comenzando desde el nivel 0, se resuelven de manera ascendente. Breve descripción del algoritmo recursivo para resolver el agente es el Algoritmo para resolver I-DID. Entrada: nivel l ≥ 1 I-ID o nivel 0 ID, T Fase de Expansión 1. Para t desde 1 hasta T − 1, hacer 2. Si l ≥ 1, entonces llenar Mt+1 j,l−1 3. Para cada mt j en el rango (Mt j,l−1) hacer 4. Llame recursivamente al algoritmo con el l - 1 I-ID (o ID) que representa mt j y el horizonte, T - t + 1 5. Mapea el nodo de decisión del I-ID (o ID) resuelto, OPT(mt j), a un nodo de probabilidad Aj 6. Para cada aj en OPT(mt j) hacer 7. Para cada oj en Oj (parte de mt j) hacer 8. Actualizar la creencia js, bt+1 j ← SE(bt j, aj, oj) 9. mt+1 j ← Nuevo I-ID (o ID) con bt+1 j como la creencia inicial 10. Rango(Mt+1 j,l−1) ∪ ← {mt+1 j } 11. Añadir el nodo del modelo, Mt+1 j,l−1, y los enlaces de dependencia entre Mt j,l−1 y Mt+1 j,l−1 (mostrados en la Fig. 3(b)) 12. Agregue los nodos de probabilidad, decisión y utilidad para la franja de tiempo t + 1 y los enlaces de dependencia entre ellos 13. Establezca las CPTs para cada nodo de probabilidad y nodo de utilidad de la Fase de Mirada Adelante 14. Aplica el método estándar de anticipación y respaldo para resolver la Figura 5 expandida de I-DID: Algoritmo para resolver un nivel l ≥ 0 I-DID. Nivel l I-DID expandido durante T pasos de tiempo con otro agente j en la Figura 5. Adoptamos un enfoque de dos fases: Dado un I-ID de nivel l (descrito previamente en la Sección 3) con todos los modelos de niveles inferiores representados también como I-IDs o IDs (si es nivel 0), el primer paso es expandir el I-ID de nivel l a lo largo de T pasos de tiempo añadiendo los enlaces de dependencia y las tablas de probabilidad condicional para cada nodo. Nos enfocamos especialmente en establecer y poblar los nodos del modelo (líneas 3-11). Ten en cuenta que Range(·) devuelve los valores (modelos de nivel inferior) de la variable aleatoria dada como entrada (nodo del modelo). En la segunda fase, utilizamos una técnica estándar de anticipación proyectando las secuencias de acción y observación durante T pasos de tiempo en el futuro, y respaldando los valores de utilidad de las creencias alcanzables. Similar a los I-IDs, los I-DIDs se reducen a DIDs en ausencia de otros agentes. Como mencionamos anteriormente, los modelos de nivel 0 son los DIDs tradicionales. Sus soluciones proporcionan distribuciones de probabilidad sobre las acciones del agente modelado en ese nivel a los I-DIDs en el nivel 1. Dadas las distribuciones de probabilidad sobre las acciones de otros agentes, los IDIDs de nivel 1 pueden resolverse como DIDs y proporcionar distribuciones de probabilidad a modelos de niveles superiores. Suponga que el número de modelos considerados en cada nivel está limitado por un número, M. Resolver un I-DID de nivel l es equivalente a resolver O(Ml) DIDs. Para ilustrar la utilidad de los I-DIDs, los aplicamos a tres dominios de problemas. Describimos, en particular, la formulación del I-DID y las prescripciones óptimas obtenidas al resolverlo. 5.1 Seguimiento-Liderazgo en el Problema del Tigre Multiagente Comenzamos nuestras ilustraciones del uso de I-IDs e I-DIDs con una versión ligeramente modificada del problema del tigre multiagente discutido en [9]. El problema tiene dos agentes, cada uno de los cuales puede abrir la puerta derecha (OR), la puerta izquierda (OL) o escuchar (L). Además de escuchar gruñidos (desde la izquierda (GL) o desde la derecha (GR)) cuando escuchan, los agentes también escuchan chirridos (desde la izquierda (CL), desde la derecha (CR) o sin chirridos (S)), que indican ruidosamente que los otros agentes están abriendo una de las puertas. Cuando se abre cualquier puerta, el tigre persiste en su ubicación original con una probabilidad del 95%. El agente i escucha gruñidos con una fiabilidad del 65% y crujidos con una fiabilidad del 95%. El agente J, por otro lado, escucha gruñidos con una fiabilidad del 95%. Por lo tanto, la configuración es tal que el agente i escucha la apertura de puertas del agente j de manera más confiable que los rugidos de los tigres. Esto sugiere que podría usar acciones de JavaScript como indicación de la ubicación del tigre, como discutimos a continuación. Las preferencias de cada agente son como en el juego de un solo agente discutido en [13]. Las funciones de transición, observación y recompensa se muestran en [16]. Un buen indicador de la utilidad de los métodos normativos para la toma de decisiones como los I-DIDs es la aparición de comportamientos sociales realistas en sus prescripciones. En entornos del persistente problema del tigre multiagente que reflejan situaciones del mundo real, demostramos la seguidismo entre los agentes y, como se muestra en [15], el engaño entre agentes que creen que están en una relación de tipo seguidor-líder. En particular, analizamos las condiciones situacionales y epistemológicas suficientes para su surgimiento. El comportamiento de seguidores, por ejemplo, resulta del agente conociendo sus propias debilidades, evaluando las fortalezas, preferencias y posibles comportamientos del otro, y dándose cuenta de que es mejor para él seguir las acciones de los demás para maximizar sus ganancias. Consideremos una configuración particular del problema del tigre en la que el agente i cree que las preferencias de j están alineadas con las suyas: ambos solo quieren obtener el oro, y la audición de j es más confiable en comparación consigo mismo. Como ejemplo, supongamos que j, al escuchar, puede discernir la ubicación del tigre el 95% de las veces en comparación con su precisión del 65%. Además, el agente i no tiene ninguna información inicial sobre la ubicación de los tigres. En otras palabras, la creencia anidada de un solo nivel, bi,1, asigna 0.5 a cada una de las dos ubicaciones del tigre. Además, considera dos modelos de j, que difieren en los niveles iniciales de creencias planas de j. Esto se representa en el nivel 1 I-ID mostrado en la Fig. 6(a). Según un modelo, j asigna una probabilidad de 0.9 a que el tigre esté detrás de la puerta izquierda, mientras que el otro 818 The Sixth Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) Figura 6: (a) Nivel 1 I-ID del agente i, (b) dos IDs de nivel 0 del agente j cuyos nodos de decisión están mapeados a los nodos de probabilidad, A1 j , A2 j , en (a). El modelo asigna 0.1 a esa ubicación (ver Fig. 6(b)). El agente i está indeciso sobre estos dos modelos de j. Si variamos la capacidad auditiva, y resolvemos el nivel 1 correspondiente I-ID expandido en tres pasos de tiempo, obtenemos las políticas de comportamiento normativas mostradas en la Figura 7 que exhiben comportamiento de seguidores. Si la probabilidad de escuchar correctamente los gruñidos es de 0.65, entonces, como se muestra en la política en la Fig. 7(a), i comienza a seguir condicionalmente las acciones de j: i abre la misma puerta que j abrió anteriormente si su evaluación propia de la ubicación de los tigres confirma la elección de j. Si i pierde la capacidad de interpretar correctamente los gruñidos por completo, sigue ciegamente a j y abre la misma puerta que j abrió anteriormente (Fig. 7(b)). Figura 7: Emergencia de (a) seguidores condicionales, y (b) seguidores ciegos en el problema del tigre. Los comportamientos de interés están en negrita. * es un comodín y representa cualquiera de las observaciones. Observamos que un solo nivel de anidación de creencias, creencias sobre los modelos de los demás, fue suficiente para que surgiera el seguidismo en el problema del tigre. Sin embargo, los requisitos epistemológicos para el surgimiento del liderazgo son más complejos. Para que un agente, digamos j, surja como líder, primero debe surgir el seguidismo en el otro agente i. Como mencionamos anteriormente, si i está seguro de que sus preferencias son idénticas a las de j, y cree que j tiene un mejor sentido del oído, i seguirá las acciones de j con el tiempo. El agente j emerge como líder si cree que lo seguiré, lo que implica que la creencia de j debe estar anidada dos niveles de profundidad para permitirle reconocer su papel de liderazgo. Darse cuenta de que lo seguiré presenta a J una oportunidad para influir en sus acciones en beneficio del bien colectivo o de su interés propio solamente. Por ejemplo, en el problema del tigre, consideremos una situación en la que si tanto i como j abren la puerta correcta, entonces cada uno recibe un pago de 20 que es el doble del original. Si solo j selecciona la puerta correcta, obtiene la recompensa de 10. Por otro lado, si ambos agentes eligen la puerta incorrecta, sus penalizaciones se reducen a la mitad. En este entorno, es tanto en el mejor interés de j como en el beneficio colectivo que utilice su experiencia para seleccionar la puerta correcta, y así ser un buen líder. Sin embargo, considera un problema ligeramente diferente en el que j se beneficia de la pérdida de i y es penalizado si i se beneficia. Específicamente, dejemos que la ganancia se reste de js, indicando que j es antagonista hacia i: si j elige la puerta correcta y i la incorrecta, entonces la pérdida de 100 se convierte en la ganancia de js. El agente J cree que yo incorrectamente pienso que las preferencias de J son aquellas que promueven el bien colectivo y que comienza creyendo con un 99% de confianza dónde está el tigre. Dado que creo que mis preferencias son similares a las de j, y que j comienza creyendo casi con certeza que una de las dos ubicaciones es la correcta (dos modelos de nivel 0 de j), comenzaré siguiendo las acciones de j. Mostramos la política normativa para resolver su I-DID anidado individualmente durante tres pasos de tiempo en la Fig. 8(a). La política demuestra que seguiré ciegamente las acciones de js. Dado que el tigre persiste en su ubicación original con una probabilidad del 0.95, seleccionaré nuevamente la misma puerta. Si j comienza el juego con una probabilidad del 99% de que el tigre esté a la derecha, resolver su I-DID anidado dos niveles profundamente, resulta en la política mostrada en la Fig. 8(b). Aunque j está casi seguro de que OL es la acción correcta, comenzará seleccionando OR, seguido de OL. La intención del agente js es engañar a i, a quien cree que seguirá las acciones de js, para así obtener $110 en el segundo paso de tiempo, lo cual es más de lo que j ganaría si fuera honesto. Figura 8: Emergencia del engaño entre agentes en el problema del tigre. Los comportamientos de interés están en negrita. * denota como antes. (a) El agente es una política que demuestra que seguirá ciegamente las acciones de js. (b) Aunque j está casi seguro de que el tigre está a la derecha, comenzará seleccionando OR, seguido de OL, para engañar a i. 5.2 Altruismo y reciprocidad en el problema del bien público El problema del bien público (PG) [7], consiste en un grupo de M agentes, cada uno de los cuales debe contribuir con algún recurso a una olla pública o guardarlo para sí mismos. Dado que los recursos aportados al fondo público se comparten entre todos los agentes, son menos valiosos para el agente cuando están en el fondo público. Sin embargo, si todos los agentes eligen contribuir con sus recursos, entonces la recompensa para cada agente es mayor que si nadie contribuye. Dado que un agente recibe su parte del bote público independientemente de si ha contribuido o no, la acción dominante es que cada agente no contribuya y en su lugar se aproveche de las contribuciones de los demás. Sin embargo, los comportamientos de los jugadores humanos en simulaciones empíricas del problema de PG difieren de las predicciones normativas. Los experimentos revelan que muchos jugadores inicialmente contribuyen con una gran cantidad al bote público, y continúan contribuyendo cuando se juega el problema del PG repetidamente, aunque en cantidades decrecientes [4]. Muchos de estos experimentos [5] informan que un pequeño grupo central de jugadores contribuye persistentemente al bote público incluso cuando todos los demás están desertando. Estos experimentos también revelan que los jugadores que contribuyen persistentemente tienen preferencias altruistas o recíprocas que coinciden con la cooperación esperada de los demás. Para simplificar, asumimos que el juego se juega entre M = 2 agentes, i y j. Que cada agente esté inicialmente dotado con una cantidad de recursos XT. Mientras que la formulación clásica del juego PG permite que cada agente contribuya con cualquier cantidad de recursos (≤ XT) al bote público, simplificamos el espacio de acciones al permitir dos acciones posibles. Cada agente puede elegir contribuir (C) una cantidad fija de recursos, o no contribuir. La última acción es deThe Sixth Intl. La Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 819 fue señalada como defectuosa (D). Suponemos que las acciones no son observables por otros. El valor de los recursos en la bolsa pública se descuenta por ci para cada agente i, donde ci es el retorno privado marginal. Suponemos que ci < 1 para que el agente no se beneficie lo suficiente como para contribuir al bote público en beneficio propio. Simultáneamente, ciM > 1, lo que hace que la contribución colectiva sea óptima de Pareto. i/j C D C 2ciXT , 2cjXT ciXT − cp, XT + cjXT − P D XT + ciXT − P, cjXT − cp XT , XT Tabla 1: El juego PG de un solo disparo con castigo. Para fomentar las contribuciones, los agentes contribuyentes castigan a los que no colaboran pero incurren en un pequeño costo por administrar el castigo. Sea P la sanción impuesta al agente que se desvía y cp el costo no nulo de castigar al agente contribuyente. Para simplificar, asumimos que el costo de castigar es el mismo para ambos agentes. El juego PG de un solo disparo con castigo se muestra en la Tabla 1. Si ci = cj, cp > 0, y si P > XT − ciXT, entonces la deserción ya no es una acción dominante. Si P < XT − ciXT, entonces la deserción es la acción dominante para ambos. Si P = XT − ciXT, entonces el juego no es resoluble por dominancia. Figura 9: (a) Nivel 1 I-ID del agente i, (b) IDs de nivel 0 del agente j con nodos de decisión mapeados a los nodos de probabilidad, A1 j y A2 j, en (a). Formulamos una versión secuencial del problema PG con castigo desde la perspectiva del agente i. Aunque en el juego de PG repetido, la cantidad en la olla pública se revela a todos los agentes después de cada ronda de acciones, asumimos en nuestra formulación que está oculta para los agentes. Cada agente puede contribuir con una cantidad fija, xc, o desertar. Un agente al realizar una acción recibe una observación de abundante (PY) o escasa (MR) simbolizando el estado de la olla pública. Ten en cuenta que las observaciones también son indirectamente indicativas de las acciones del agente js porque el estado del bote público es influenciado por ellas. La cantidad de recursos en el agente es una olla privada, es perfectamente observable para i. Los pagos son análogos a la Tabla 1. Tomando prestado de las investigaciones empíricas del problema PG [5], construimos IDs de nivel 0 para j que modelan tipos altruistas y no altruistas (Fig. 9(b)). Específicamente, nuestro agente altruista tiene un alto retorno privado marginal (cj está cerca de 1) y no castiga a los demás que incumplen. Que xc = 1 y el agente de nivel 0 sea castigado la mitad de las veces que se desvía. Con una acción restante, ambos tipos de agentes eligen contribuir para evitar ser castigados. Con dos acciones por delante, el tipo altruista elige contribuir, mientras que el otro defecta. Esto se debe a que cj para el tipo altruista es cercano a 1, por lo tanto, el castigo esperado, 0.5P > (1 − cj), que el tipo altruista evita. Debido a que cj para el tipo no altruista es menor, prefiere no contribuir. Con tres pasos por delante, el agente altruista contribuye para evitar el castigo (0.5P > 2(1 − cj)), mientras que el tipo no altruista se desentiende. Para más de tres pasos, mientras el agente altruista continúa contribuyendo al bote público dependiendo de qué tan cercano esté su retorno privado marginal a 1, el tipo no altruista prescribe la deserción. Analizamos las decisiones de un agente altruista i modelado usando un nivel 1 I-DID expandido en 3 pasos de tiempo. i atribuye los dos modelos de nivel 0, mencionados anteriormente, a j (ver Fig. 9). Si creo con una probabilidad de 1 que j es altruista, elijo contribuir en cada uno de los tres pasos. Este comportamiento persiste cuando i no está al tanto de si j es altruista (Fig. 10(a)), y cuando i asigna una alta probabilidad a que j sea del tipo no altruista. Sin embargo, cuando i cree con una probabilidad de 1 que j no es altruista y por lo tanto seguramente va a traicionar, i elige traicionar para evitar ser castigado y porque su beneficio privado marginal es menor que 1. Estos resultados demuestran que el comportamiento de nuestro tipo altruista se asemeja al encontrado experimentalmente. El agente de nivel 1 no altruista elige desertar independientemente de lo altruista que crea que sea el otro agente. Analizamos el comportamiento de un tipo de agente recíproco que se ajusta a la cooperación o defección esperada. El retorno privado marginal del tipo recíproco es similar al del tipo no altruista, sin embargo, obtiene una mayor recompensa cuando su acción es similar a la del otro. Consideramos el caso en el que el agente recíproco i no está seguro de si j es altruista y cree que es probable que el bote público esté medio lleno. Para esta creencia previa, yo elijo defectar. Al recibir una observación de abundancia, decide contribuir, mientras que una observación de escasez lo hace defectar (Fig. 10(b)). Esto se debe a que una observación de abundancia indica que es probable que la olla esté más llena que a la mitad, lo cual resulta de la acción de js para contribuir. Por lo tanto, entre los dos modelos atribuidos a j, es probable que su tipo sea altruista, lo que hace probable que j contribuya nuevamente en el próximo paso de tiempo. El agente i, por lo tanto, elige contribuir para corresponder la acción de js. Un razonamiento análogo lleva a uno a defectar cuando observa una olla escasa. Con una acción por hacer, creo que si j contribuye, también elegiré contribuir para evitar castigo independientemente de sus observaciones. Figura 10: (a) Un agente altruista de nivel 1 siempre contribuye. (b) Un agente recíproco i comienza defraudando y luego elige contribuir o defraudar basándose en su observación de abundancia (indicando que j es probablemente altruista) o escasez (j no es altruista). 5.3 Estrategias en el póker de dos jugadores El póker es un popular juego de cartas de suma cero que ha recibido mucha atención en la comunidad de investigación de IA como un banco de pruebas [2]. El póker se juega entre M ≥ 2 jugadores, en el que cada jugador recibe una mano de cartas de una baraja. Si bien existen varias variantes de póker con diferentes niveles de complejidad, consideramos una versión simple en la que cada jugador tiene tres turnos durante los cuales el jugador puede intercambiar una carta (E), mantener la mano existente (K), retirarse (F) y abandonar el juego, o apostar (C), lo que requiere que todos los jugadores muestren sus manos. Para mantener las cosas simples, dejemos que M = 2, y que cada jugador reciba una mano compuesta por una sola carta sacada del mismo palo. Por lo tanto, durante un enfrentamiento, el jugador que tenga la carta numéricamente más alta (el 2 es el más bajo, el as es el más alto) gana el bote. Durante un intercambio de cartas, la carta descartada se coloca ya sea en la pila L, indicando al otro agente que era una carta de número bajo menor que 8, o en la 820 The Sixth Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) H, indicando que la carta tenía un rango mayor o igual a 8. Ten en cuenta que, por ejemplo, si se descarta una carta de número bajo, la probabilidad de recibir una carta baja a cambio se reduce. Mostramos el nivel 1 I-ID para el Poker simplificado de dos jugadores en la Fig. 11. Consideramos dos modelos (tipos de personalidad) del agente j. El tipo conservador cree que es probable que su oponente tenga una carta de alto número en su mano. Por otro lado, el agente agresivo j cree con alta probabilidad que su oponente tiene una carta de número más bajo. Por lo tanto, los dos tipos difieren en sus creencias sobre la mano de sus oponentes. En ambos estos modelos de nivel 0, se asume que el oponente realiza sus acciones siguiendo una distribución fija y uniforme. Con tres acciones restantes, independientemente de su mano (a menos que sea un as), el agente agresivo elige intercambiar su carta, con la intención de mejorar su mano actual. Esto se debe a que cree que el otro tiene una carta baja, lo que mejora sus posibilidades de obtener una carta alta durante el intercambio. El agente conservador elige quedarse con su carta, sin importar su mano, porque considera que sus posibilidades de obtener una carta alta son escasas, ya que cree que su oponente tiene una. Figura 11: (a) Nivel 1 I-ID del agente i. La observación revela información sobre la mano de js del paso de tiempo anterior, (b) IDs de nivel 0 del agente j cuyos nodos de decisión están mapeados a los nodos de probabilidad, A1 j , A2 j , en (a). La política de un agente de nivel 1 i que cree que cada carta, excepto la suya, tiene la misma probabilidad de estar en su mano (tipo de personalidad neutral) y j podría ser de tipo agresivo o conservador, se muestra en la Figura 12. Su mano contiene la carta numerada 8. El agente comienza por mantener su carta. Al ver que j no intercambió una carta (N), i cree con probabilidad 1 que j es conservador y, por lo tanto, mantendrá sus cartas. i responde manteniendo su carta o intercambiándola porque j tiene igual probabilidad de tener una carta más baja o más alta. Si observo que j descartó su carta en la pila L o H, creo que j es agresivo. Al observar a L, i se da cuenta de que j tenía una carta baja, y es probable que tenga una carta alta después de su intercambio. Dado que la probabilidad de recibir una carta baja es alta en este momento, i decide quedarse con su carta. Al observar la carta H, creyendo que la probabilidad de recibir una carta de número alto es alta, i decide intercambiar su carta. En el paso final, i decide llamar independientemente de su historial de observaciones porque su creencia de que j tiene una carta más alta no es lo suficientemente alta como para concluir que es mejor retirarse y renunciar al pago. Esto se debe en parte al hecho de que una observación de, digamos, L, restablece las creencias del agente en el paso de tiempo anterior sobre las cartas de números bajos solamente. 6. DISCUSIÓN Mostramos cómo los DIDs pueden ser extendidos a los I-DIDs que permiten la <br>toma de decisiones secuenciales en línea</br> en entornos multiagentes inciertos. Nuestra representación gráfica de I-DIDs mejora la Figura 12 anterior: Un agente de nivel 1 es una política de tres pasos en el problema de Poker. i comienza creyendo que j tiene la misma probabilidad de ser agresivo o conservador y podría tener cualquier carta en su mano con igual probabilidad. Trabaja significativamente al ser más transparente, semánticamente claro y capaz de ser resuelto utilizando algoritmos estándar que apuntan a los DIDs. Los I-DIDs extienden los NIDs para permitir la toma de decisiones secuenciales a lo largo de múltiples pasos de tiempo en presencia de otros agentes que interactúan. Los I-DIDs pueden ser vistos como representaciones gráficas concisas para IPOMDPs que proporcionan una forma de explotar la estructura del problema y llevar a cabo la toma de decisiones en línea a medida que el agente actúa y observa dadas sus creencias previas. Actualmente estamos investigando formas de resolver I-DIDs aproximadamente con límites demostrables en la calidad de la solución. Agradecimiento: Agradecemos a Piotr Gmytrasiewicz por algunas discusiones útiles relacionadas con este trabajo. El primer autor desea agradecer el apoyo de una beca UGARF. 7. REFERENCIAS [1] R. J. Aumann. Epistemología interactiva i: Conocimiento. Revista Internacional de Teoría de Juegos, 28:263-300, 1999. [2] D. Billings, A. Davidson, J. Schaeffer y D. Szafron. El desafío del póker. AIJ, 2001. [3] A. Brandenburger y E. Dekel. Jerarquías de creencias y conocimiento común. Revista de Teoría Económica, 59:189-198, 1993. [4] C. Camerer. Teoría de juegos conductual: Experimentos en interacción estratégica. Princeton University Press, 2003. [5] E. Fehr y S. Gachter. Cooperación y castigo en experimentos de bienes públicos. Revista Económica Americana, 90(4):980-994, 2000. [6] D. Fudenberg y D. K. Levine. La Teoría del Aprendizaje en los Juegos. MIT Press, 1998. [7] D. Fudenberg y J. Tirole. Teoría de juegos. MIT Press, 1991. [8] Y. Gal y A. Pfeffer. Un lenguaje para modelar los procesos de toma de decisiones de agentes en juegos. En AAMAS, 2003. [9] P. Gmytrasiewicz y P. Doshi. Un marco para la planificación secuencial en entornos multiagentes. JAIR, 24:49-79, 2005. [10] P. Gmytrasiewicz y E. Durfee. Coordinación racional en entornos multiagente. JAAMAS, 3(4):319-350, 2000. [11] J. C. Harsanyi. Juegos con información incompleta jugados por jugadores bayesianos. Ciencia de la Gestión, 14(3):159-182, 1967. [12] R. A. Howard y J. E. Matheson. Diagramas de influencia. En R. A. Howard y J. E. Matheson, editores, Los Principios y Aplicaciones del Análisis de Decisiones. Grupo de Decisiones Estratégicas, Menlo Park, CA 94025, 1984. [13] L. Kaelbling, M. Littman y A. Cassandra. Planificación y actuación en dominios estocásticos parcialmente observables. Revista de Inteligencia Artificial, 2, 1998. [14] D. Koller y B. Milch. Diagramas de influencia multiagente para representar y resolver juegos. En IJCAI, páginas 1027-1034, 2001. [15] K. Polich y P. Gmytrasiewicz. Diagramas de influencia interactivos y dinámicos. En el taller GTDT, AAMAS, 2006. [16] B. Rathnas, P. Doshi y P. J. Gmytrasiewicz. Soluciones exactas para POMDP interactivos utilizando equivalencia conductual. En la Conferencia de Agentes Autónomos y Sistemas Multiagente (AAMAS), 2006. [17] S. Russell y P. Norvig. Inteligencia Artificial: Un Enfoque Moderno (Segunda Edición). Prentice Hall, 2003. [18] R. D. Shachter. \n\nPrentice Hall, 2003. [18] R. D. Shachter. Evaluando diagramas de influencia. Investigación de Operaciones, 34(6):871-882, 1986. [19] D. Suryadi y P. Gmytrasiewicz. Aprendiendo modelos de otros agentes utilizando diagramas de influencia. En UM, 1999. El Sexto Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 821 ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "dynamic inﬂuence diagram": {
            "translated_key": "diagrama de influencia dinámica",
            "is_in_text": false,
            "original_annotated_sentences": [
                "Graphical Models for Online Solutions to Interactive POMDPs Prashant Doshi Dept.",
                "of Computer Science University of Georgia Athens, GA 30602, USA pdoshi@cs.uga.edu Yifeng Zeng Dept. of Computer Science Aalborg University DK-9220 Aalborg, Denmark yfzeng@cs.aau.edu Qiongyu Chen Dept. of Computer Science National Univ. of Singapore 117543, Singapore chenqy@comp.nus.edu.sg ABSTRACT We develop a new graphical representation for interactive partially observable Markov decision processes (I-POMDPs) that is significantly more transparent and semantically clear than the previous representation.",
                "These graphical models called interactive dynamic influence diagrams (I-DIDs) seek to explicitly model the structure that is often present in real-world problems by decomposing the situation into chance and decision variables, and the dependencies between the variables.",
                "I-DIDs generalize DIDs, which may be viewed as graphical representations of POMDPs, to multiagent settings in the same way that I-POMDPs generalize POMDPs.",
                "I-DIDs may be used to compute the policy of an agent online as the agent acts and observes in a setting that is populated by other interacting agents.",
                "Using several examples, we show how I-DIDs may be applied and demonstrate their usefulness.",
                "Categories and Subject Descriptors I.2.11 [Distributed Artificial Intelligence]: Multiagent Systems General Terms Theory 1.",
                "INTRODUCTION Interactive partially observable Markov decision processes (IPOMDPs) [9] provide a framework for sequential decision-making in partially observable multiagent environments.",
                "They generalize POMDPs [13] to multiagent settings by including the other agents computable models in the state space along with the states of the physical environment.",
                "The models encompass all information influencing the agents behaviors, including their preferences, capabilities, and beliefs, and are thus analogous to types in Bayesian games [11].",
                "I-POMDPs adopt a subjective approach to understanding strategic behavior, rooted in a decision-theoretic framework that takes a decision-makers perspective in the interaction.",
                "In [15], Polich and Gmytrasiewicz introduced interactive dynamic influence diagrams (I-DIDs) as the computational representations of I-POMDPs.",
                "I-DIDs generalize DIDs [12], which may be viewed as computational counterparts of POMDPs, to multiagents settings in the same way that I-POMDPs generalize POMDPs.",
                "I-DIDs contribute to a growing line of work [19] that includes multi-agent influence diagrams (MAIDs) [14], and more recently, networks of influence diagrams (NIDs) [8].",
                "These formalisms seek to explicitly model the structure that is often present in real-world problems by decomposing the situation into chance and decision variables, and the dependencies between the variables.",
                "MAIDs provide an alternative to normal and extensive game forms using a graphical formalism to represent games of imperfect information with a decision node for each agents actions and chance nodes capturing the agents private information.",
                "MAIDs objectively analyze the game, efficiently computing the Nash equilibrium profile by exploiting the independence structure.",
                "NIDs extend MAIDs to include agents uncertainty over the game being played and over models of the other agents.",
                "Each model is a MAID and the network of MAIDs is collapsed, bottom up, into a single MAID for computing the equilibrium of the game keeping in mind the different models of each agent.",
                "Graphical formalisms such as MAIDs and NIDs open up a promising area of research that aims to represent multiagent interactions more transparently.",
                "However, MAIDs provide an analysis of the game from an external viewpoint and the applicability of both is limited to static single play games.",
                "Matters are more complex when we consider interactions that are extended over time, where predictions about others future actions must be made using models that change as the agents act and observe.",
                "I-DIDs address this gap by allowing the representation of other agents models as the values of a special model node.",
                "Both, other agents models and the original agents beliefs over these models are updated over time using special-purpose implementations.",
                "In this paper, we improve on the previous preliminary representation of the I-DID shown in [15] by using the insight that the static I-ID is a type of NID.",
                "Thus, we may utilize NID-specific language constructs such as multiplexers to represent the model node, and subsequently the I-ID, more transparently.",
                "Furthermore, we clarify the semantics of the special purpose policy link introduced in the representation of I-DID by [15], and show that it could be replaced by traditional dependency links.",
                "In the previous representation of the I-DID, the update of the agents belief over the models of others as the agents act and receive observations was denoted using a special link called the model update link that connected the model nodes over time.",
                "We explicate the semantics of this link by showing how it can be implemented using the traditional dependency links between the chance nodes that constitute the model nodes.",
                "The net result is a representation of I-DID that is significantly more transparent, semantically clear, and capable of being implemented using the standard algorithms for solving DIDs.",
                "We show how IDIDs may be used to model an agents uncertainty over others models, that may themselves be I-DIDs.",
                "Solution to the I-DID is a policy that prescribes what the agent should do over time, given its beliefs over the physical state and others models.",
                "Analogous to DIDs, I-DIDs may be used to compute the policy of an agent online as the agent acts and observes in a setting that is populated by other interacting agents. 2.",
                "BACKGROUND: FINITELY NESTED IPOMDPS Interactive POMDPs generalize POMDPs to multiagent settings by including other agents models as part of the state space [9].",
                "Since other agents may also reason about others, the interactive state space is strategically nested; it contains beliefs about other agents models and their beliefs about others.",
                "For simplicity of presentation we consider an agent, i, that is interacting with one other agent, j.",
                "A finitely nested I-POMDP of agent i with a strategy level l is defined as the tuple: I-POMDPi,l = ISi,l, A, Ti, Ωi, Oi, Ri where: • ISi,l denotes a set of interactive states defined as, ISi,l = S × Mj,l−1, where Mj,l−1 = {Θj,l−1 ∪ SMj}, for l ≥ 1, and ISi,0 = S, where S is the set of states of the physical environment.",
                "Θj,l−1 is the set of computable intentional models of agent j: θj,l−1 = bj,l−1, ˆθj where the frame, ˆθj = A, Ωj, Tj, Oj, Rj, OCj .",
                "Here, j is Bayes rational and OCj is js optimality criterion.",
                "SMj is the set of subintentional models of j.",
                "Simple examples of subintentional models include a no-information model [10] and a fictitious play model [6], both of which are history independent.",
                "We give a recursive bottom-up construction of the interactive state space below.",
                "ISi,0 = S, Θj,0 = { bj,0, ˆθj | bj,0 ∈ Δ(ISj,0)} ISi,1 = S × {Θj,0 ∪ SMj}, Θj,1 = { bj,1, ˆθj | bj,1 ∈ Δ(ISj,1)} . . . . . .",
                "ISi,l = S × {Θj,l−1 ∪ SMj}, Θj,l = { bj,l, ˆθj | bj,l ∈ Δ(ISj,l)} Similar formulations of nested spaces have appeared in [1, 3]. • A = Ai × Aj is the set of joint actions of all agents in the environment; • Ti : S ×A×S → [0, 1], describes the effect of the joint actions on the physical states of the environment; • Ωi is the set of observations of agent i; • Oi : S × A × Ωi → [0, 1] gives the likelihood of the observations given the physical state and joint action; • Ri : ISi × A → R describes agent is preferences over its interactive states.",
                "Usually only the physical states will matter.",
                "Agent is policy is the mapping, Ω∗ i → Δ(Ai), where Ω∗ i is the set of all observation histories of agent i.",
                "Since belief over the interactive states forms a sufficient statistic [9], the policy can also be represented as a mapping from the set of all beliefs of agent i to a distribution over its actions, Δ(ISi) → Δ(Ai). 2.1 Belief Update Analogous to POMDPs, an agent within the I-POMDP framework updates its belief as it acts and observes.",
                "However, there are two differences that complicate the belief update in multiagent settings when compared to single agent ones.",
                "First, since the state of the physical environment depends on the actions of both agents, is prediction of how the physical state changes has to be made based on its prediction of js actions.",
                "Second, changes in js models have to be included in is belief update.",
                "Specifically, if j is intentional then an update of js beliefs due to its action and observation has to be included.",
                "In other words, i has to update its belief based on its prediction of what j would observe and how j would update its belief.",
                "If js model is subintentional, then js probable observations are appended to the observation history contained in the model.",
                "Formally, we have: Pr(ist |at−1 i , bt−1 i,l ) = β ISt−1:mt−1 j =θt j bt−1 i,l (ist−1 ) × at−1 j Pr(at−1 j |θt−1 j,l−1)Oi(st , at−1 i , at−1 j , ot i) ×Ti(st−1 , at−1 i , at−1 j , st ) ot j Oj(st , at−1 i , at−1 j , ot j) ×τ(SEθt j (bt−1 j,l−1, at−1 j , ot j) − bt j,l−1) (1) where β is the normalizing constant, τ is 1 if its argument is 0 otherwise it is 0, Pr(at−1 j |θt−1 j,l−1) is the probability that at−1 j is Bayes rational for the agent described by model θt−1 j,l−1, and SE(·) is an abbreviation for the belief update.",
                "For a version of the belief update when js model is subintentional, see [9].",
                "If agent j is also modeled as an I-POMDP, then is belief update invokes js belief update (via the term SEθt j ( bt−1 j,l−1 , at−1 j , ot j)), which in turn could invoke is belief update and so on.",
                "This recursion in belief nesting bottoms out at the 0th level.",
                "At this level, the belief update of the agent reduces to a POMDP belief update. 1 For illustrations of the belief update, additional details on I-POMDPs, and how they compare with other multiagent frameworks, see [9]. 2.2 Value Iteration Each belief state in a finitely nested I-POMDP has an associated value reflecting the maximum payoff the agent can expect in this belief state: Un ( bi,l, θi ) = max ai∈Ai is∈ISi,l ERi(is, ai)bi,l(is)+ γ oi∈Ωi Pr(oi|ai, bi,l)Un−1 ( SEθi (bi,l, ai, oi), θi ) (2) where, ERi(is, ai) = aj Ri(is, ai, aj)Pr(aj|mj,l−1) (since is = (s, mj,l−1)).",
                "Eq. 2 is a basis for value iteration in I-POMDPs.",
                "Agent is optimal action, a∗ i , for the case of finite horizon with discounting, is an element of the set of optimal actions for the belief state, OPT(θi), defined as: OPT( bi,l, θi ) = argmax ai∈Ai is∈ISi,l ERi(is, ai)bi,l(is) +γ oi∈Ωi Pr(oi|ai, bi,l)Un ( SEθi (bi,l, ai, oi), θi ) (3) 3.",
                "INTERACTIVEINFLUENCEDIAGRAMS A naive extension of influence diagrams (IDs) to settings populated by multiple agents is possible by treating other agents as automatons, represented using chance nodes.",
                "However, this approach assumes that the agents actions are controlled using a probability distribution that does not change over time.",
                "Interactive influence diagrams (I-IDs) adopt a more sophisticated approach by generalizing IDs to make them applicable to settings shared with other agents who may act and observe, and update their beliefs. 3.1 Syntax In addition to the usual chance, decision, and utility nodes, IIDs include a new type of node called the model node.",
                "We show a general level l I-ID in Fig. 1(a), where the model node (Mj,l−1) is denoted using a hexagon.",
                "We note that the probability distribution over the chance node, S, and the model node together represents agent is belief over its interactive states.",
                "In addition to the model 1 The 0th level model is a POMDP: Other agents actions are treated as exogenous events and folded into the T, O, and R functions.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 815 Figure 1: (a) A generic level l I-ID for agent i situated with one other agent j.",
                "The hexagon is the model node (Mj,l−1) whose structure we show in (b).",
                "Members of the model node are I-IDs themselves (m1 j,l−1, m2 j,l−1; diagrams not shown here for simplicity) whose decision nodes are mapped to the corresponding chance nodes (A1 j , A2 j ).",
                "Depending on the value of the node, Mod[Mj], the distribution of each of the chance nodes is assigned to the node Aj. (c) The transformed I-ID with the model node replaced by the chance nodes and the relationships between them. node, I-IDs differ from IDs by having a dashed link (called the policy link in [15]) between the model node and a chance node, Aj, that represents the distribution over the other agents actions given its model.",
                "In the absence of other agents, the model node and the chance node, Aj, vanish and I-IDs collapse into traditional IDs.",
                "The model node contains the alternative computational models ascribed by i to the other agent from the set, Θj,l−1 ∪ SMj, where Θj,l−1 and SMj were defined previously in Section 2.",
                "Thus, a model in the model node may itself be an I-ID or ID, and the recursion terminates when a model is an ID or subintentional.",
                "Because the model node contains the alternative models of the other agent as its values, its representation is not trivial.",
                "In particular, some of the models within the node are I-IDs that when solved generate the agents optimal policy in their decision nodes.",
                "Each decision node is mapped to the corresponding chance node, say A1 j , in the following way: if OPT is the set of optimal actions obtained by solving the I-ID (or ID), then Pr(aj ∈ A1 j ) = 1 |OP T | if aj ∈ OPT, 0 otherwise.",
                "Borrowing insights from previous work [8], we observe that the model node and the dashed policy link that connects it to the chance node, Aj, could be represented as shown in Fig. 1(b).",
                "The decision node of each level l − 1 I-ID is transformed into a chance node, as we mentioned previously, so that the actions with the largest value in the decision node are assigned uniform probabilities in the chance node while the rest are assigned zero probability.",
                "The different chance nodes (A1 j , A2 j ), one for each model, and additionally, the chance node labeled Mod[Mj] form the parents of the chance node, Aj.",
                "Thus, there are as many action nodes (A1 j , A2 j ) in Mj,l−1 as the number of models in the support of agent is beliefs.",
                "The conditional probability table of the chance node, Aj, is a multiplexer that assumes the distribution of each of the action nodes (A1 j , A2 j ) depending on the value of Mod[Mj].",
                "The values of Mod[Mj] denote the different models of j.",
                "In other words, when Mod[Mj] has the value m1 j,l−1, the chance node Aj assumes the distribution of the node A1 j , and Aj assumes the distribution of A2 j when Mod[Mj] has the value m2 j,l−1.",
                "The distribution over the node, Mod[Mj], is the agent is belief over the models of j given a physical state.",
                "For more agents, we will have as many model nodes as there are agents.",
                "Notice that Fig. 1(b) clarifies the semantics of the policy link, and shows how it can be represented using the traditional dependency links.",
                "In Fig. 1(c), we show the transformed I-ID when the model node is replaced by the chance nodes and relationships between them.",
                "In contrast to the representation in [15], there are no special-purpose policy links, rather the I-ID is composed of only those types of nodes that are found in traditional IDs and dependency relationships between the nodes.",
                "This allows I-IDs to be represented and implemented using conventional application tools that target IDs.",
                "Note that we may view the level l I-ID as a NID.",
                "Specifically, each of the level l − 1 models within the model node are blocks in the NID (see Fig. 2).",
                "If the level l = 1, each block is a traditional ID, otherwise if l > 1, each block within the NID may itself be a NID.",
                "Note that within the I-IDs (or IDs) at each level, there is only a single decision node.",
                "Thus, our NID does not contain any MAIDs.",
                "Figure 2: A level l I-ID represented as a NID.",
                "The probabilities assigned to the blocks of the NID are is beliefs over js models conditioned on a physical state. 3.2 Solution The solution of an I-ID proceeds in a bottom-up manner, and is implemented recursively.",
                "We start by solving the level 0 models, which, if intentional, are traditional IDs.",
                "Their solutions provide probability distributions over the other agents actions, which are entered in the corresponding chance nodes found in the model node of the level 1 I-ID.",
                "The mapping from the level 0 models decision nodes to the chance nodes is carried out so that actions with the largest value in the decision node are assigned uniform probabilities in the chance node while the rest are assigned zero probability.",
                "Given the distributions over the actions within the different chance nodes (one for each model of the other agent), the level 1 I-ID is transformed as shown in Fig. 1(c).",
                "During the transformation, the conditional probability table (CPT) of the node, Aj, is populated such that the node assumes the distribution of each of the chance nodes depending on the value of the node, Mod[Mj].",
                "As we mentioned previously, the values of the node Mod[Mj] denote the different models of the other agent, and its distribution is the agent is belief over the models of j conditioned on the physical state.",
                "The transformed level 1 I-ID is a traditional ID that may be solved us816 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) (a) (b) Figure 3: (a) A generic two time-slice level l I-DID for agent i in a setting with one other agent j.",
                "Notice the dotted model update link that denotes the update of the models of j and the distribution over the models over time. (b) The semantics of the model update link. ing the standard expected utility maximization method [18].",
                "This procedure is carried out up to the level l I-ID whose solution gives the non-empty set of optimal actions that the agent should perform given its belief.",
                "Notice that analogous to IDs, I-IDs are suitable for online decision-making when the agents current belief is known. 4.",
                "INTERACTIVE DYNAMIC INFLUENCE DIAGRAMS Interactive dynamic influence diagrams (I-DIDs) extend I-IDs (and NIDs) to allow sequential decision-making over several time steps.",
                "Just as DIDs are structured graphical representations of POMDPs, I-DIDs are the graphical online analogs for finitely nested I-POMDPs.",
                "I-DIDs may be used to optimize over a finite look-ahead given initial beliefs while interacting with other, possibly similar, agents. 4.1 Syntax We depict a general two time-slice I-DID in Fig. 3(a).",
                "In addition to the model nodes and the dashed policy link, what differentiates an I-DID from a DID is the model update link shown as a dotted arrow in Fig. 3(a).",
                "We explained the semantics of the model node and the policy link in the previous section; we describe the model updates next.",
                "The update of the model node over time involves two steps: First, given the models at time t, we identify the updated set of models that reside in the model node at time t + 1.",
                "Recall from Section 2 that an agents intentional model includes its belief.",
                "Because the agents act and receive observations, their models are updated to reflect their changed beliefs.",
                "Since the set of optimal actions for a model could include all the actions, and the agent may receive any one of |Ωj| possible observations, the updated set at time step t + 1 will have at most |Mt j,l−1||Aj||Ωj| models.",
                "Here, |Mt j,l−1| is the number of models at time step t, |Aj| and |Ωj| are the largest spaces of actions and observations respectively, among all the models.",
                "Second, we compute the new distribution over the updated models given the original distribution and the probability of the agent performing the action and receiving the observation that led to the updated model.",
                "These steps are a part of agent is belief update formalized using Eq. 1.",
                "In Fig. 3(b), we show how the dotted model update link is implemented in the I-DID.",
                "If each of the two level l − 1 models ascribed to j at time step t results in one action, and j could make one of two possible observations, then the model node at time step t + 1 contains four updated models (mt+1,1 j,l−1 ,mt+1,2 j,l−1 , mt+1,3 j,l−1 , and mt+1,4 j,l−1 ).",
                "These models differ in their initial beliefs, each of which is the result of j updating its beliefs due to its action and a possible observation.",
                "The decision nodes in each of the I-DIDs or DIDs that represent the lower level models are mapped to the corresponding Figure 4: Transformed I-DID with the model nodes and model update link replaced with the chance nodes and the relationships (in bold). chance nodes, as mentioned previously.",
                "Next, we describe how the distribution over the updated set of models (the distribution over the chance node Mod[Mt+1 j ] in Mt+1 j,l−1) is computed.",
                "The probability that js updated model is, say mt+1,1 j,l−1 , depends on the probability of j performing the action and receiving the observation that led to this model, and the prior distribution over the models at time step t. Because the chance node At j assumes the distribution of each of the action nodes based on the value of Mod[Mt j ], the probability of the action is given by this chance node.",
                "In order to obtain the probability of js possible observation, we introduce the chance node Oj, which depending on the value of Mod[Mt j ] assumes the distribution of the observation node in the lower level model denoted by Mod[Mt j ].",
                "Because the probability of js observations depends on the physical state and the joint actions of both agents, the node Oj is linked with St+1 , At j, and At i. 2 Analogous to At j, the conditional probability table of Oj is also a multiplexer modulated by Mod[Mt j ].",
                "Finally, the distribution over the prior models at time t is obtained from the chance node, Mod[Mt j ] in Mt j,l−1.",
                "Consequently, the chance nodes, Mod[Mt j ], At j, and Oj, form the parents of Mod[Mt+1 j ] in Mt+1 j,l−1.",
                "Notice that the model update link may be replaced by the dependency links between the chance nodes that constitute the model nodes in the two time slices.",
                "In Fig. 4 we show the two time-slice I-DID with the model nodes replaced by the chance nodes and the relationships between them.",
                "Chance nodes and dependency links that not in bold are standard, usually found in DIDs.",
                "Expansion of the I-DID over more time steps requires the repetition of the two steps of updating the set of models that form the 2 Note that Oj represents js observation at time t + 1.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 817 values of the model node and adding the relationships between the chance nodes, as many times as there are model update links.",
                "We note that the possible set of models of the other agent j grows exponentially with the number of time steps.",
                "For example, after T steps, there may be at most |Mt=1 j,l−1|(|Aj||Ωj|)T −1 candidate models residing in the model node. 4.2 Solution Analogous to I-IDs, the solution to a level l I-DID for agent i expanded over T time steps may be carried out recursively.",
                "For the purpose of illustration, let l=1 and T=2.",
                "The solution method uses the standard look-ahead technique, projecting the agents action and observation sequences forward from the current belief state [17], and finding the possible beliefs that i could have in the next time step.",
                "Because agent i has a belief over js models as well, the lookahead includes finding out the possible models that j could have in the future.",
                "Consequently, each of js subintentional or level 0 models (represented using a standard DID) in the first time step must be solved to obtain its optimal set of actions.",
                "These actions are combined with the set of possible observations that j could make in that model, resulting in an updated set of candidate models (that include the updated beliefs) that could describe the behavior of j. Beliefs over this updated set of candidate models are calculated using the standard inference methods using the dependency relationships between the model nodes as shown in Fig. 3(b).",
                "We note the recursive nature of this solution: in solving agent is level 1 I-DID, js level 0 DIDs must be solved.",
                "If the nesting of models is deeper, all models at all levels starting from 0 are solved in a bottom-up manner.",
                "We briefly outline the recursive algorithm for solving agent is Algorithm for solving I-DID Input : level l ≥ 1 I-ID or level 0 ID, T Expansion Phase 1.",
                "For t from 1 to T − 1 do 2.",
                "If l ≥ 1 then Populate Mt+1 j,l−1 3.",
                "For each mt j in Range(Mt j,l−1) do 4.",
                "Recursively call algorithm with the l − 1 I-ID (or ID) that represents mt j and the horizon, T − t + 1 5.",
                "Map the decision node of the solved I-ID (or ID), OPT(mt j), to a chance node Aj 6.",
                "For each aj in OPT(mt j) do 7.",
                "For each oj in Oj (part of mt j) do 8.",
                "Update js belief, bt+1 j ← SE(bt j, aj, oj) 9. mt+1 j ← New I-ID (or ID) with bt+1 j as the initial belief 10.",
                "Range(Mt+1 j,l−1) ∪ ← {mt+1 j } 11.",
                "Add the model node, Mt+1 j,l−1, and the dependency links between Mt j,l−1 and Mt+1 j,l−1 (shown in Fig. 3(b)) 12.",
                "Add the chance, decision, and utility nodes for t + 1 time slice and the dependency links between them 13.",
                "Establish the CPTs for each chance node and utility node Look-Ahead Phase 14.",
                "Apply the standard look-ahead and backup method to solve the expanded I-DID Figure 5: Algorithm for solving a level l ≥ 0 I-DID. level l I-DID expanded over T time steps with one other agent j in Fig. 5.",
                "We adopt a two-phase approach: Given an I-ID of level l (described previously in Section 3) with all lower level models also represented as I-IDs or IDs (if level 0), the first step is to expand the level l I-ID over T time steps adding the dependency links and the conditional probability tables for each node.",
                "We particularly focus on establishing and populating the model nodes (lines 3-11).",
                "Note that Range(·) returns the values (lower level models) of the random variable given as input (model node).",
                "In the second phase, we use a standard look-ahead technique projecting the action and observation sequences over T time steps in the future, and backing up the utility values of the reachable beliefs.",
                "Similar to I-IDs, the I-DIDs reduce to DIDs in the absence of other agents.",
                "As we mentioned previously, the 0-th level models are the traditional DIDs.",
                "Their solutions provide probability distributions over actions of the agent modeled at that level to I-DIDs at level 1.",
                "Given probability distributions over other agents actions the level 1 IDIDs can themselves be solved as DIDs, and provide probability distributions to yet higher level models.",
                "Assume that the number of models considered at each level is bound by a number, M. Solving an I-DID of level l in then equivalent to solving O(Ml ) DIDs. 5.",
                "EXAMPLE APPLICATIONS To illustrate the usefulness of I-DIDs, we apply them to three problem domains.",
                "We describe, in particular, the formulation of the I-DID and the optimal prescriptions obtained on solving it. 5.1 Followership-Leadership in the Multiagent Tiger Problem We begin our illustrations of using I-IDs and I-DIDs with a slightly modified version of the multiagent tiger problem discussed in [9].",
                "The problem has two agents, each of which can open the right door (OR), the left door (OL) or listen (L).",
                "In addition to hearing growls (from the left (GL) or from the right (GR)) when they listen, the agents also hear creaks (from the left (CL), from the right (CR), or no creaks (S)), which noisily indicate the other agents opening one of the doors.",
                "When any door is opened, the tiger persists in its original location with a probability of 95%.",
                "Agent i hears growls with a reliability of 65% and creaks with a reliability of 95%.",
                "Agent j, on the other hand, hears growls with a reliability of 95%.",
                "Thus, the setting is such that agent i hears agent j opening doors more reliably than the tigers growls.",
                "This suggests that i could use js actions as an indication of the location of the tiger, as we discuss below.",
                "Each agents preferences are as in the single agent game discussed in [13].",
                "The transition, observation, and reward functions are shown in [16].",
                "A good indicator of the usefulness of normative methods for decision-making like I-DIDs is the emergence of realistic social behaviors in their prescriptions.",
                "In settings of the persistent multiagent tiger problem that reflect real world situations, we demonstrate followership between the agents and, as shown in [15], deception among agents who believe that they are in a follower-leader type of relationship.",
                "In particular, we analyze the situational and epistemological conditions sufficient for their emergence.",
                "The followership behavior, for example, results from the agent knowing its own weaknesses, assessing the strengths, preferences, and possible behaviors of the other, and realizing that its best for it to follow the others actions in order to maximize its payoffs.",
                "Let us consider a particular setting of the tiger problem in which agent i believes that js preferences are aligned with its own - both of them just want to get the gold - and js hearing is more reliable in comparison to itself.",
                "As an example, suppose that j, on listening can discern the tigers location 95% of the times compared to is 65% accuracy.",
                "Additionally, agent i does not have any initial information about the tigers location.",
                "In other words, is single-level nested belief, bi,1, assigns 0.5 to each of the two locations of the tiger.",
                "In addition, i considers two models of j, which differ in js flat level 0 initial beliefs.",
                "This is represented in the level 1 I-ID shown in Fig. 6(a).",
                "According to one model, j assigns a probability of 0.9 that the tiger is behind the left door, while the other 818 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Figure 6: (a) Level 1 I-ID of agent i, (b) two level 0 IDs of agent j whose decision nodes are mapped to the chance nodes, A1 j , A2 j , in (a). model assigns 0.1 to that location (see Fig. 6(b)).",
                "Agent i is undecided on these two models of j.",
                "If we vary is hearing ability, and solve the corresponding level 1 I-ID expanded over three time steps, we obtain the normative behavioral policies shown in Fig 7 that exhibit followership behavior.",
                "If is probability of correctly hearing the growls is 0.65, then as shown in the policy in Fig. 7(a), i begins to conditionally follow js actions: i opens the same door that j opened previously iff is own assessment of the tigers location confirms js pick.",
                "If i loses the ability to correctly interpret the growls completely, it blindly follows j and opens the same door that j opened previously (Fig. 7(b)).",
                "Figure 7: Emergence of (a) conditional followership, and (b) blind followership in the tiger problem.",
                "Behaviors of interest are in bold. * is a wildcard, and denotes any one of the observations.",
                "We observed that a single level of belief nesting - beliefs about the others models - was sufficient for followership to emerge in the tiger problem.",
                "However, the epistemological requirements for the emergence of leadership are more complex.",
                "For an agent, say j, to emerge as a leader, followership must first emerge in the other agent i.",
                "As we mentioned previously, if i is certain that its preferences are identical to those of j, and believes that j has a better sense of hearing, i will follow js actions over time.",
                "Agent j emerges as a leader if it believes that i will follow it, which implies that js belief must be nested two levels deep to enable it to recognize its leadership role.",
                "Realizing that i will follow presents j with an opportunity to influence is actions in the benefit of the collective good or its self-interest alone.",
                "For example, in the tiger problem, let us consider a setting in which if both i and j open the correct door, then each gets a payoff of 20 that is double the original.",
                "If j alone selects the correct door, it gets the payoff of 10.",
                "On the other hand, if both agents pick the wrong door, their penalties are cut in half.",
                "In this setting, it is in both js best interest as well as the collective betterment for j to use its expertise in selecting the correct door, and thus be a good leader.",
                "However, consider a slightly different problem in which j gains from is loss and is penalized if i gains.",
                "Specifically, let is payoff be subtracted from js, indicating that j is antagonistic toward i - if j picks the correct door and i the wrong one, then is loss of 100 becomes js gain.",
                "Agent j believes that i incorrectly thinks that js preferences are those that promote the collective good and that it starts off by believing with 99% confidence where the tiger is.",
                "Because i believes that its preferences are similar to those of j, and that j starts by believing almost surely that one of the two is the correct location (two level 0 models of j), i will start by following js actions.",
                "We show is normative policy on solving its singly-nested I-DID over three time steps in Fig. 8(a).",
                "The policy demonstrates that i will blindly follow js actions.",
                "Since the tiger persists in its original location with a probability of 0.95, i will select the same door again.",
                "If j begins the game with a 99% probability that the tiger is on the right, solving js I-DID nested two levels deep, results in the policy shown in Fig. 8(b).",
                "Even though j is almost certain that OL is the correct action, it will start by selecting OR, followed by OL.",
                "Agent js intention is to deceive i who, it believes, will follow js actions, so as to gain $110 in the second time step, which is more than what j would gain if it were to be honest.",
                "Figure 8: Emergence of deception between agents in the tiger problem.",
                "Behaviors of interest are in bold. * denotes as before. (a) Agent is policy demonstrating that it will blindly follow js actions. (b) Even though j is almost certain that the tiger is on the right, it will start by selecting OR, followed by OL, in order to deceive i. 5.2 Altruism and Reciprocity in the Public Good Problem The public good (PG) problem [7], consists of a group of M agents, each of whom must either contribute some resource to a public pot or keep it for themselves.",
                "Since resources contributed to the public pot are shared among all the agents, they are less valuable to the agent when in the public pot.",
                "However, if all agents choose to contribute their resources, then the payoff to each agent is more than if no one contributes.",
                "Since an agent gets its share of the public pot irrespective of whether it has contributed or not, the dominating action is for each agent to not contribute, and instead free ride on others contributions.",
                "However, behaviors of human players in empirical simulations of the PG problem differ from the normative predictions.",
                "The experiments reveal that many players initially contribute a large amount to the public pot, and continue to contribute when the PG problem is played repeatedly, though in decreasing amounts [4].",
                "Many of these experiments [5] report that a small core group of players persistently contributes to the public pot even when all others are defecting.",
                "These experiments also reveal that players who persistently contribute have altruistic or reciprocal preferences matching expected cooperation of others.",
                "For simplicity, we assume that the game is played between M = 2 agents, i and j.",
                "Let each agent be initially endowed with XT amount of resources.",
                "While the classical PG game formulation permits each agent to contribute any quantity of resources (≤ XT ) to the public pot, we simplify the action space by allowing two possible actions.",
                "Each agent may choose to either contribute (C) a fixed amount of the resources, or not contribute.",
                "The latter action is deThe Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 819 noted as defect (D).",
                "We assume that the actions are not observable to others.",
                "The value of resources in the public pot is discounted by ci for each agent i, where ci is the marginal private return.",
                "We assume that ci < 1 so that the agent does not benefit enough that it contributes to the public pot for private gain.",
                "Simultaneously, ciM > 1, making collective contribution pareto optimal. i/j C D C 2ciXT , 2cjXT ciXT − cp, XT + cjXT − P D XT + ciXT − P, cjXT − cp XT , XT Table 1: The one-shot PG game with punishment.",
                "In order to encourage contributions, the contributing agents punish free riders but incur a small cost for administering the punishment.",
                "Let P be the punishment meted out to the defecting agent and cp the non-zero cost of punishing for the contributing agent.",
                "For simplicity, we assume that the cost of punishing is same for both the agents.",
                "The one-shot PG game with punishment is shown in Table. 1.",
                "Let ci = cj, cp > 0, and if P > XT − ciXT , then defection is no longer a dominating action.",
                "If P < XT − ciXT , then defection is the dominating action for both.",
                "If P = XT − ciXT , then the game is not dominance-solvable.",
                "Figure 9: (a) Level 1 I-ID of agent i, (b) level 0 IDs of agent j with decision nodes mapped to the chance nodes, A1 j and A2 j , in (a).",
                "We formulate a sequential version of the PG problem with punishment from the perspective of agent i.",
                "Though in the repeated PG game, the quantity in the public pot is revealed to all the agents after each round of actions, we assume in our formulation that it is hidden from the agents.",
                "Each agent may contribute a fixed amount, xc, or defect.",
                "An agent on performing an action receives an observation of plenty (PY) or meager (MR) symbolizing the state of the public pot.",
                "Notice that the observations are also indirectly indicative of agent js actions because the state of the public pot is influenced by them.",
                "The amount of resources in agent is private pot, is perfectly observable to i.",
                "The payoffs are analogous to Table. 1.",
                "Borrowing from the empirical investigations of the PG problem [5], we construct level 0 IDs for j that model altruistic and non-altruistic types (Fig. 9(b)).",
                "Specifically, our altruistic agent has a high marginal private return (cj is close to 1) and does not punish others who defect.",
                "Let xc = 1 and the level 0 agent be punished half the times it defects.",
                "With one action remaining, both types of agents choose to contribute to avoid being punished.",
                "With two actions to go, the altruistic type chooses to contribute, while the other defects.",
                "This is because cj for the altruistic type is close to 1, thus the expected punishment, 0.5P > (1 − cj), which the altruistic type avoids.",
                "Because cj for the non-altruistic type is less, it prefers not to contribute.",
                "With three steps to go, the altruistic agent contributes to avoid punishment (0.5P > 2(1 − cj)), and the non-altruistic type defects.",
                "For greater than three steps, while the altruistic agent continues to contribute to the public pot depending on how close its marginal private return is to 1, the non-altruistic type prescribes defection.",
                "We analyzed the decisions of an altruistic agent i modeled using a level 1 I-DID expanded over 3 time steps. i ascribes the two level 0 models, mentioned previously, to j (see Fig. 9).",
                "If i believes with a probability 1 that j is altruistic, i chooses to contribute for each of the three steps.",
                "This behavior persists when i is unaware of whether j is altruistic (Fig. 10(a)), and when i assigns a high probability to j being the non-altruistic type.",
                "However, when i believes with a probability 1 that j is non-altruistic and will thus surely defect, i chooses to defect to avoid being punished and because its marginal private return is less than 1.",
                "These results demonstrate that the behavior of our altruistic type resembles that found experimentally.",
                "The non-altruistic level 1 agent chooses to defect regardless of how likely it believes the other agent to be altruistic.",
                "We analyzed the behavior of a reciprocal agent type that matches expected cooperation or defection.",
                "The reciprocal types marginal private return is similar to that of the non-altruistic type, however, it obtains a greater payoff when its action is similar to that of the other.",
                "We consider the case when the reciprocal agent i is unsure of whether j is altruistic and believes that the public pot is likely to be half full.",
                "For this prior belief, i chooses to defect.",
                "On receiving an observation of plenty, i decides to contribute, while an observation of meager makes it defect (Fig. 10(b)).",
                "This is because an observation of plenty signals that the pot is likely to be greater than half full, which results from js action to contribute.",
                "Thus, among the two models ascribed to j, its type is likely to be altruistic making it likely that j will contribute again in the next time step.",
                "Agent i therefore chooses to contribute to reciprocate js action.",
                "An analogous reasoning leads i to defect when it observes a meager pot.",
                "With one action to go, i believing that j contributes, will choose to contribute too to avoid punishment regardless of its observations.",
                "Figure 10: (a) An altruistic level 1 agent always contributes. (b) A reciprocal agent i starts off by defecting followed by choosing to contribute or defect based on its observation of plenty (indicating that j is likely altruistic) or meager (j is non-altruistic). 5.3 Strategies in Two-Player Poker Poker is a popular zero sum card game that has received much attention among the AI research community as a testbed [2].",
                "Poker is played among M ≥ 2 players in which each player receives a hand of cards from a deck.",
                "While several flavors of Poker with varying complexity exist, we consider a simple version in which each player has three plys during which the player may either exchange a card (E), keep the existing hand (K), fold (F) and withdraw from the game, or call (C), requiring all players to show their hands.",
                "To keep matters simple, let M = 2, and each player receive a hand consisting of a single card drawn from the same suit.",
                "Thus, during a showdown, the player who has the numerically larger card (2 is the lowest, ace is the highest) wins the pot.",
                "During an exchange of cards, the discarded card is placed either in the L pile, indicating to the other agent that it was a low numbered card less than 8, or in the 820 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) H pile, indicating that the card had a rank greater than or equal to 8.",
                "Notice that, for example, if a lower numbered card is discarded, the probability of receiving a low card in exchange is now reduced.",
                "We show the level 1 I-ID for the simplified two-player Poker in Fig. 11.",
                "We considered two models (personality types) of agent j.",
                "The conservative type believes that it is likely that its opponent has a high numbered card in its hand.",
                "On the other hand, the aggressive agent j believes with a high probability that its opponent has a lower numbered card.",
                "Thus, the two types differ in their beliefs over their opponents hand.",
                "In both these level 0 models, the opponent is assumed to perform its actions following a fixed, uniform distribution.",
                "With three actions to go, regardless of its hand (unless it is an ace), the aggressive agent chooses to exchange its card, with the intent of improving on its current hand.",
                "This is because it believes the other to have a low card, which improves its chances of getting a high card during the exchange.",
                "The conservative agent chooses to keep its card, no matter its hand because its chances of getting a high card are slim as it believes that its opponent has one.",
                "Figure 11: (a) Level 1 I-ID of agent i.",
                "The observation reveals information about js hand of the previous time step, (b) level 0 IDs of agent j whose decision nodes are mapped to the chance nodes, A1 j , A2 j , in (a).",
                "The policy of a level 1 agent i who believes that each card except its own has an equal likelihood of being in js hand (neutral personality type) and j could be either an aggressive or conservative type, is shown in Fig. 12. is own hand contains the card numbered 8.",
                "The agent starts by keeping its card.",
                "On seeing that j did not exchange a card (N), i believes with probability 1 that j is conservative and hence will keep its cards. i responds by either keeping its card or exchanging it because j is equally likely to have a lower or higher card.",
                "If i observes that j discarded its card into the L or H pile, i believes that j is aggressive.",
                "On observing L, i realizes that j had a low card, and is likely to have a high card after its exchange.",
                "Because the probability of receiving a low card is high now, i chooses to keep its card.",
                "On observing H, believing that the probability of receiving a high numbered card is high, i chooses to exchange its card.",
                "In the final step, i chooses to call regardless of its observation history because its belief that j has a higher card is not sufficiently high to conclude that its better to fold and relinquish the payoff.",
                "This is partly due to the fact that an observation of, say, L resets the agent is previous time step beliefs over js hand to the low numbered cards only. 6.",
                "DISCUSSION We showed how DIDs may be extended to I-DIDs that enable online sequential decision-making in uncertain multiagent settings.",
                "Our graphical representation of I-DIDs improves on the previous Figure 12: A level 1 agent is three step policy in the Poker problem. i starts by believing that j is equally likely to be aggressive or conservative and could have any card in its hand with equal probability. work significantly by being more transparent, semantically clear, and capable of being solved using standard algorithms that target DIDs.",
                "I-DIDs extend NIDs to allow sequential decision-making over multiple time steps in the presence of other interacting agents.",
                "I-DIDs may be seen as concise graphical representations for IPOMDPs providing a way to exploit problem structure and carry out online decision-making as the agent acts and observes given its prior beliefs.",
                "We are currently investigating ways to solve I-DIDs approximately with provable bounds on the solution quality.",
                "Acknowledgment: We thank Piotr Gmytrasiewicz for some useful discussions related to this work.",
                "The first author would like to acknowledge the support of a UGARF grant. 7.",
                "REFERENCES [1] R. J. Aumann.",
                "Interactive epistemology i: Knowledge.",
                "International Journal of Game Theory, 28:263-300, 1999. [2] D. Billings, A. Davidson, J. Schaeffer, and D. Szafron.",
                "The challenge of poker.",
                "AIJ, 2001. [3] A. Brandenburger and E. Dekel.",
                "Hierarchies of beliefs and common knowledge.",
                "Journal of Economic Theory, 59:189-198, 1993. [4] C. Camerer.",
                "Behavioral Game Theory: Experiments in Strategic Interaction.",
                "Princeton University Press, 2003. [5] E. Fehr and S. Gachter.",
                "Cooperation and punishment in public goods experiments.",
                "American Economic Review, 90(4):980-994, 2000. [6] D. Fudenberg and D. K. Levine.",
                "The Theory of Learning in Games.",
                "MIT Press, 1998. [7] D. Fudenberg and J. Tirole.",
                "Game Theory.",
                "MIT Press, 1991. [8] Y. Gal and A. Pfeffer.",
                "A language for modeling agents decision-making processes in games.",
                "In AAMAS, 2003. [9] P. Gmytrasiewicz and P. Doshi.",
                "A framework for sequential planning in multiagent settings.",
                "JAIR, 24:49-79, 2005. [10] P. Gmytrasiewicz and E. Durfee.",
                "Rational coordination in multi-agent environments.",
                "JAAMAS, 3(4):319-350, 2000. [11] J. C. Harsanyi.",
                "Games with incomplete information played by bayesian players.",
                "Management Science, 14(3):159-182, 1967. [12] R. A. Howard and J. E. Matheson.",
                "Influence diagrams.",
                "In R. A. Howard and J. E. Matheson, editors, The Principles and Applications of Decision Analysis.",
                "Strategic Decisions Group, Menlo Park, CA 94025, 1984. [13] L. Kaelbling, M. Littman, and A. Cassandra.",
                "Planning and acting in partially observable stochastic domains.",
                "Artificial Intelligence Journal, 2, 1998. [14] D. Koller and B. Milch.",
                "Multi-agent influence diagrams for representing and solving games.",
                "In IJCAI, pages 1027-1034, 2001. [15] K. Polich and P. Gmytrasiewicz.",
                "Interactive dynamic influence diagrams.",
                "In GTDT Workshop, AAMAS, 2006. [16] B.",
                "Rathnas., P. Doshi, and P. J. Gmytrasiewicz.",
                "Exact solutions to interactive pomdps using behavioral equivalence.",
                "In Autonomous Agents and Multi-Agent Systems Conference (AAMAS), 2006. [17] S. Russell and P. Norvig.",
                "Artificial Intelligence: A Modern Approach (Second Edition).",
                "Prentice Hall, 2003. [18] R. D. Shachter.",
                "Evaluating influence diagrams.",
                "Operations Research, 34(6):871-882, 1986. [19] D. Suryadi and P. Gmytrasiewicz.",
                "Learning models of other agents using influence diagrams.",
                "In UM, 1999.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 821"
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [],
            "translated_text": "",
            "candidates": [],
            "error": [
                []
            ]
        },
        "decision-make": {
            "translated_key": "tomadores de decisiones",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Graphical Models for Online Solutions to Interactive POMDPs Prashant Doshi Dept.",
                "of Computer Science University of Georgia Athens, GA 30602, USA pdoshi@cs.uga.edu Yifeng Zeng Dept. of Computer Science Aalborg University DK-9220 Aalborg, Denmark yfzeng@cs.aau.edu Qiongyu Chen Dept. of Computer Science National Univ. of Singapore 117543, Singapore chenqy@comp.nus.edu.sg ABSTRACT We develop a new graphical representation for interactive partially observable Markov decision processes (I-POMDPs) that is significantly more transparent and semantically clear than the previous representation.",
                "These graphical models called interactive dynamic influence diagrams (I-DIDs) seek to explicitly model the structure that is often present in real-world problems by decomposing the situation into chance and decision variables, and the dependencies between the variables.",
                "I-DIDs generalize DIDs, which may be viewed as graphical representations of POMDPs, to multiagent settings in the same way that I-POMDPs generalize POMDPs.",
                "I-DIDs may be used to compute the policy of an agent online as the agent acts and observes in a setting that is populated by other interacting agents.",
                "Using several examples, we show how I-DIDs may be applied and demonstrate their usefulness.",
                "Categories and Subject Descriptors I.2.11 [Distributed Artificial Intelligence]: Multiagent Systems General Terms Theory 1.",
                "INTRODUCTION Interactive partially observable Markov decision processes (IPOMDPs) [9] provide a framework for sequential decision-making in partially observable multiagent environments.",
                "They generalize POMDPs [13] to multiagent settings by including the other agents computable models in the state space along with the states of the physical environment.",
                "The models encompass all information influencing the agents behaviors, including their preferences, capabilities, and beliefs, and are thus analogous to types in Bayesian games [11].",
                "I-POMDPs adopt a subjective approach to understanding strategic behavior, rooted in a decision-theoretic framework that takes a <br>decision-make</br>rs perspective in the interaction.",
                "In [15], Polich and Gmytrasiewicz introduced interactive dynamic influence diagrams (I-DIDs) as the computational representations of I-POMDPs.",
                "I-DIDs generalize DIDs [12], which may be viewed as computational counterparts of POMDPs, to multiagents settings in the same way that I-POMDPs generalize POMDPs.",
                "I-DIDs contribute to a growing line of work [19] that includes multi-agent influence diagrams (MAIDs) [14], and more recently, networks of influence diagrams (NIDs) [8].",
                "These formalisms seek to explicitly model the structure that is often present in real-world problems by decomposing the situation into chance and decision variables, and the dependencies between the variables.",
                "MAIDs provide an alternative to normal and extensive game forms using a graphical formalism to represent games of imperfect information with a decision node for each agents actions and chance nodes capturing the agents private information.",
                "MAIDs objectively analyze the game, efficiently computing the Nash equilibrium profile by exploiting the independence structure.",
                "NIDs extend MAIDs to include agents uncertainty over the game being played and over models of the other agents.",
                "Each model is a MAID and the network of MAIDs is collapsed, bottom up, into a single MAID for computing the equilibrium of the game keeping in mind the different models of each agent.",
                "Graphical formalisms such as MAIDs and NIDs open up a promising area of research that aims to represent multiagent interactions more transparently.",
                "However, MAIDs provide an analysis of the game from an external viewpoint and the applicability of both is limited to static single play games.",
                "Matters are more complex when we consider interactions that are extended over time, where predictions about others future actions must be made using models that change as the agents act and observe.",
                "I-DIDs address this gap by allowing the representation of other agents models as the values of a special model node.",
                "Both, other agents models and the original agents beliefs over these models are updated over time using special-purpose implementations.",
                "In this paper, we improve on the previous preliminary representation of the I-DID shown in [15] by using the insight that the static I-ID is a type of NID.",
                "Thus, we may utilize NID-specific language constructs such as multiplexers to represent the model node, and subsequently the I-ID, more transparently.",
                "Furthermore, we clarify the semantics of the special purpose policy link introduced in the representation of I-DID by [15], and show that it could be replaced by traditional dependency links.",
                "In the previous representation of the I-DID, the update of the agents belief over the models of others as the agents act and receive observations was denoted using a special link called the model update link that connected the model nodes over time.",
                "We explicate the semantics of this link by showing how it can be implemented using the traditional dependency links between the chance nodes that constitute the model nodes.",
                "The net result is a representation of I-DID that is significantly more transparent, semantically clear, and capable of being implemented using the standard algorithms for solving DIDs.",
                "We show how IDIDs may be used to model an agents uncertainty over others models, that may themselves be I-DIDs.",
                "Solution to the I-DID is a policy that prescribes what the agent should do over time, given its beliefs over the physical state and others models.",
                "Analogous to DIDs, I-DIDs may be used to compute the policy of an agent online as the agent acts and observes in a setting that is populated by other interacting agents. 2.",
                "BACKGROUND: FINITELY NESTED IPOMDPS Interactive POMDPs generalize POMDPs to multiagent settings by including other agents models as part of the state space [9].",
                "Since other agents may also reason about others, the interactive state space is strategically nested; it contains beliefs about other agents models and their beliefs about others.",
                "For simplicity of presentation we consider an agent, i, that is interacting with one other agent, j.",
                "A finitely nested I-POMDP of agent i with a strategy level l is defined as the tuple: I-POMDPi,l = ISi,l, A, Ti, Ωi, Oi, Ri where: • ISi,l denotes a set of interactive states defined as, ISi,l = S × Mj,l−1, where Mj,l−1 = {Θj,l−1 ∪ SMj}, for l ≥ 1, and ISi,0 = S, where S is the set of states of the physical environment.",
                "Θj,l−1 is the set of computable intentional models of agent j: θj,l−1 = bj,l−1, ˆθj where the frame, ˆθj = A, Ωj, Tj, Oj, Rj, OCj .",
                "Here, j is Bayes rational and OCj is js optimality criterion.",
                "SMj is the set of subintentional models of j.",
                "Simple examples of subintentional models include a no-information model [10] and a fictitious play model [6], both of which are history independent.",
                "We give a recursive bottom-up construction of the interactive state space below.",
                "ISi,0 = S, Θj,0 = { bj,0, ˆθj | bj,0 ∈ Δ(ISj,0)} ISi,1 = S × {Θj,0 ∪ SMj}, Θj,1 = { bj,1, ˆθj | bj,1 ∈ Δ(ISj,1)} . . . . . .",
                "ISi,l = S × {Θj,l−1 ∪ SMj}, Θj,l = { bj,l, ˆθj | bj,l ∈ Δ(ISj,l)} Similar formulations of nested spaces have appeared in [1, 3]. • A = Ai × Aj is the set of joint actions of all agents in the environment; • Ti : S ×A×S → [0, 1], describes the effect of the joint actions on the physical states of the environment; • Ωi is the set of observations of agent i; • Oi : S × A × Ωi → [0, 1] gives the likelihood of the observations given the physical state and joint action; • Ri : ISi × A → R describes agent is preferences over its interactive states.",
                "Usually only the physical states will matter.",
                "Agent is policy is the mapping, Ω∗ i → Δ(Ai), where Ω∗ i is the set of all observation histories of agent i.",
                "Since belief over the interactive states forms a sufficient statistic [9], the policy can also be represented as a mapping from the set of all beliefs of agent i to a distribution over its actions, Δ(ISi) → Δ(Ai). 2.1 Belief Update Analogous to POMDPs, an agent within the I-POMDP framework updates its belief as it acts and observes.",
                "However, there are two differences that complicate the belief update in multiagent settings when compared to single agent ones.",
                "First, since the state of the physical environment depends on the actions of both agents, is prediction of how the physical state changes has to be made based on its prediction of js actions.",
                "Second, changes in js models have to be included in is belief update.",
                "Specifically, if j is intentional then an update of js beliefs due to its action and observation has to be included.",
                "In other words, i has to update its belief based on its prediction of what j would observe and how j would update its belief.",
                "If js model is subintentional, then js probable observations are appended to the observation history contained in the model.",
                "Formally, we have: Pr(ist |at−1 i , bt−1 i,l ) = β ISt−1:mt−1 j =θt j bt−1 i,l (ist−1 ) × at−1 j Pr(at−1 j |θt−1 j,l−1)Oi(st , at−1 i , at−1 j , ot i) ×Ti(st−1 , at−1 i , at−1 j , st ) ot j Oj(st , at−1 i , at−1 j , ot j) ×τ(SEθt j (bt−1 j,l−1, at−1 j , ot j) − bt j,l−1) (1) where β is the normalizing constant, τ is 1 if its argument is 0 otherwise it is 0, Pr(at−1 j |θt−1 j,l−1) is the probability that at−1 j is Bayes rational for the agent described by model θt−1 j,l−1, and SE(·) is an abbreviation for the belief update.",
                "For a version of the belief update when js model is subintentional, see [9].",
                "If agent j is also modeled as an I-POMDP, then is belief update invokes js belief update (via the term SEθt j ( bt−1 j,l−1 , at−1 j , ot j)), which in turn could invoke is belief update and so on.",
                "This recursion in belief nesting bottoms out at the 0th level.",
                "At this level, the belief update of the agent reduces to a POMDP belief update. 1 For illustrations of the belief update, additional details on I-POMDPs, and how they compare with other multiagent frameworks, see [9]. 2.2 Value Iteration Each belief state in a finitely nested I-POMDP has an associated value reflecting the maximum payoff the agent can expect in this belief state: Un ( bi,l, θi ) = max ai∈Ai is∈ISi,l ERi(is, ai)bi,l(is)+ γ oi∈Ωi Pr(oi|ai, bi,l)Un−1 ( SEθi (bi,l, ai, oi), θi ) (2) where, ERi(is, ai) = aj Ri(is, ai, aj)Pr(aj|mj,l−1) (since is = (s, mj,l−1)).",
                "Eq. 2 is a basis for value iteration in I-POMDPs.",
                "Agent is optimal action, a∗ i , for the case of finite horizon with discounting, is an element of the set of optimal actions for the belief state, OPT(θi), defined as: OPT( bi,l, θi ) = argmax ai∈Ai is∈ISi,l ERi(is, ai)bi,l(is) +γ oi∈Ωi Pr(oi|ai, bi,l)Un ( SEθi (bi,l, ai, oi), θi ) (3) 3.",
                "INTERACTIVEINFLUENCEDIAGRAMS A naive extension of influence diagrams (IDs) to settings populated by multiple agents is possible by treating other agents as automatons, represented using chance nodes.",
                "However, this approach assumes that the agents actions are controlled using a probability distribution that does not change over time.",
                "Interactive influence diagrams (I-IDs) adopt a more sophisticated approach by generalizing IDs to make them applicable to settings shared with other agents who may act and observe, and update their beliefs. 3.1 Syntax In addition to the usual chance, decision, and utility nodes, IIDs include a new type of node called the model node.",
                "We show a general level l I-ID in Fig. 1(a), where the model node (Mj,l−1) is denoted using a hexagon.",
                "We note that the probability distribution over the chance node, S, and the model node together represents agent is belief over its interactive states.",
                "In addition to the model 1 The 0th level model is a POMDP: Other agents actions are treated as exogenous events and folded into the T, O, and R functions.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 815 Figure 1: (a) A generic level l I-ID for agent i situated with one other agent j.",
                "The hexagon is the model node (Mj,l−1) whose structure we show in (b).",
                "Members of the model node are I-IDs themselves (m1 j,l−1, m2 j,l−1; diagrams not shown here for simplicity) whose decision nodes are mapped to the corresponding chance nodes (A1 j , A2 j ).",
                "Depending on the value of the node, Mod[Mj], the distribution of each of the chance nodes is assigned to the node Aj. (c) The transformed I-ID with the model node replaced by the chance nodes and the relationships between them. node, I-IDs differ from IDs by having a dashed link (called the policy link in [15]) between the model node and a chance node, Aj, that represents the distribution over the other agents actions given its model.",
                "In the absence of other agents, the model node and the chance node, Aj, vanish and I-IDs collapse into traditional IDs.",
                "The model node contains the alternative computational models ascribed by i to the other agent from the set, Θj,l−1 ∪ SMj, where Θj,l−1 and SMj were defined previously in Section 2.",
                "Thus, a model in the model node may itself be an I-ID or ID, and the recursion terminates when a model is an ID or subintentional.",
                "Because the model node contains the alternative models of the other agent as its values, its representation is not trivial.",
                "In particular, some of the models within the node are I-IDs that when solved generate the agents optimal policy in their decision nodes.",
                "Each decision node is mapped to the corresponding chance node, say A1 j , in the following way: if OPT is the set of optimal actions obtained by solving the I-ID (or ID), then Pr(aj ∈ A1 j ) = 1 |OP T | if aj ∈ OPT, 0 otherwise.",
                "Borrowing insights from previous work [8], we observe that the model node and the dashed policy link that connects it to the chance node, Aj, could be represented as shown in Fig. 1(b).",
                "The decision node of each level l − 1 I-ID is transformed into a chance node, as we mentioned previously, so that the actions with the largest value in the decision node are assigned uniform probabilities in the chance node while the rest are assigned zero probability.",
                "The different chance nodes (A1 j , A2 j ), one for each model, and additionally, the chance node labeled Mod[Mj] form the parents of the chance node, Aj.",
                "Thus, there are as many action nodes (A1 j , A2 j ) in Mj,l−1 as the number of models in the support of agent is beliefs.",
                "The conditional probability table of the chance node, Aj, is a multiplexer that assumes the distribution of each of the action nodes (A1 j , A2 j ) depending on the value of Mod[Mj].",
                "The values of Mod[Mj] denote the different models of j.",
                "In other words, when Mod[Mj] has the value m1 j,l−1, the chance node Aj assumes the distribution of the node A1 j , and Aj assumes the distribution of A2 j when Mod[Mj] has the value m2 j,l−1.",
                "The distribution over the node, Mod[Mj], is the agent is belief over the models of j given a physical state.",
                "For more agents, we will have as many model nodes as there are agents.",
                "Notice that Fig. 1(b) clarifies the semantics of the policy link, and shows how it can be represented using the traditional dependency links.",
                "In Fig. 1(c), we show the transformed I-ID when the model node is replaced by the chance nodes and relationships between them.",
                "In contrast to the representation in [15], there are no special-purpose policy links, rather the I-ID is composed of only those types of nodes that are found in traditional IDs and dependency relationships between the nodes.",
                "This allows I-IDs to be represented and implemented using conventional application tools that target IDs.",
                "Note that we may view the level l I-ID as a NID.",
                "Specifically, each of the level l − 1 models within the model node are blocks in the NID (see Fig. 2).",
                "If the level l = 1, each block is a traditional ID, otherwise if l > 1, each block within the NID may itself be a NID.",
                "Note that within the I-IDs (or IDs) at each level, there is only a single decision node.",
                "Thus, our NID does not contain any MAIDs.",
                "Figure 2: A level l I-ID represented as a NID.",
                "The probabilities assigned to the blocks of the NID are is beliefs over js models conditioned on a physical state. 3.2 Solution The solution of an I-ID proceeds in a bottom-up manner, and is implemented recursively.",
                "We start by solving the level 0 models, which, if intentional, are traditional IDs.",
                "Their solutions provide probability distributions over the other agents actions, which are entered in the corresponding chance nodes found in the model node of the level 1 I-ID.",
                "The mapping from the level 0 models decision nodes to the chance nodes is carried out so that actions with the largest value in the decision node are assigned uniform probabilities in the chance node while the rest are assigned zero probability.",
                "Given the distributions over the actions within the different chance nodes (one for each model of the other agent), the level 1 I-ID is transformed as shown in Fig. 1(c).",
                "During the transformation, the conditional probability table (CPT) of the node, Aj, is populated such that the node assumes the distribution of each of the chance nodes depending on the value of the node, Mod[Mj].",
                "As we mentioned previously, the values of the node Mod[Mj] denote the different models of the other agent, and its distribution is the agent is belief over the models of j conditioned on the physical state.",
                "The transformed level 1 I-ID is a traditional ID that may be solved us816 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) (a) (b) Figure 3: (a) A generic two time-slice level l I-DID for agent i in a setting with one other agent j.",
                "Notice the dotted model update link that denotes the update of the models of j and the distribution over the models over time. (b) The semantics of the model update link. ing the standard expected utility maximization method [18].",
                "This procedure is carried out up to the level l I-ID whose solution gives the non-empty set of optimal actions that the agent should perform given its belief.",
                "Notice that analogous to IDs, I-IDs are suitable for online decision-making when the agents current belief is known. 4.",
                "INTERACTIVE DYNAMIC INFLUENCE DIAGRAMS Interactive dynamic influence diagrams (I-DIDs) extend I-IDs (and NIDs) to allow sequential decision-making over several time steps.",
                "Just as DIDs are structured graphical representations of POMDPs, I-DIDs are the graphical online analogs for finitely nested I-POMDPs.",
                "I-DIDs may be used to optimize over a finite look-ahead given initial beliefs while interacting with other, possibly similar, agents. 4.1 Syntax We depict a general two time-slice I-DID in Fig. 3(a).",
                "In addition to the model nodes and the dashed policy link, what differentiates an I-DID from a DID is the model update link shown as a dotted arrow in Fig. 3(a).",
                "We explained the semantics of the model node and the policy link in the previous section; we describe the model updates next.",
                "The update of the model node over time involves two steps: First, given the models at time t, we identify the updated set of models that reside in the model node at time t + 1.",
                "Recall from Section 2 that an agents intentional model includes its belief.",
                "Because the agents act and receive observations, their models are updated to reflect their changed beliefs.",
                "Since the set of optimal actions for a model could include all the actions, and the agent may receive any one of |Ωj| possible observations, the updated set at time step t + 1 will have at most |Mt j,l−1||Aj||Ωj| models.",
                "Here, |Mt j,l−1| is the number of models at time step t, |Aj| and |Ωj| are the largest spaces of actions and observations respectively, among all the models.",
                "Second, we compute the new distribution over the updated models given the original distribution and the probability of the agent performing the action and receiving the observation that led to the updated model.",
                "These steps are a part of agent is belief update formalized using Eq. 1.",
                "In Fig. 3(b), we show how the dotted model update link is implemented in the I-DID.",
                "If each of the two level l − 1 models ascribed to j at time step t results in one action, and j could make one of two possible observations, then the model node at time step t + 1 contains four updated models (mt+1,1 j,l−1 ,mt+1,2 j,l−1 , mt+1,3 j,l−1 , and mt+1,4 j,l−1 ).",
                "These models differ in their initial beliefs, each of which is the result of j updating its beliefs due to its action and a possible observation.",
                "The decision nodes in each of the I-DIDs or DIDs that represent the lower level models are mapped to the corresponding Figure 4: Transformed I-DID with the model nodes and model update link replaced with the chance nodes and the relationships (in bold). chance nodes, as mentioned previously.",
                "Next, we describe how the distribution over the updated set of models (the distribution over the chance node Mod[Mt+1 j ] in Mt+1 j,l−1) is computed.",
                "The probability that js updated model is, say mt+1,1 j,l−1 , depends on the probability of j performing the action and receiving the observation that led to this model, and the prior distribution over the models at time step t. Because the chance node At j assumes the distribution of each of the action nodes based on the value of Mod[Mt j ], the probability of the action is given by this chance node.",
                "In order to obtain the probability of js possible observation, we introduce the chance node Oj, which depending on the value of Mod[Mt j ] assumes the distribution of the observation node in the lower level model denoted by Mod[Mt j ].",
                "Because the probability of js observations depends on the physical state and the joint actions of both agents, the node Oj is linked with St+1 , At j, and At i. 2 Analogous to At j, the conditional probability table of Oj is also a multiplexer modulated by Mod[Mt j ].",
                "Finally, the distribution over the prior models at time t is obtained from the chance node, Mod[Mt j ] in Mt j,l−1.",
                "Consequently, the chance nodes, Mod[Mt j ], At j, and Oj, form the parents of Mod[Mt+1 j ] in Mt+1 j,l−1.",
                "Notice that the model update link may be replaced by the dependency links between the chance nodes that constitute the model nodes in the two time slices.",
                "In Fig. 4 we show the two time-slice I-DID with the model nodes replaced by the chance nodes and the relationships between them.",
                "Chance nodes and dependency links that not in bold are standard, usually found in DIDs.",
                "Expansion of the I-DID over more time steps requires the repetition of the two steps of updating the set of models that form the 2 Note that Oj represents js observation at time t + 1.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 817 values of the model node and adding the relationships between the chance nodes, as many times as there are model update links.",
                "We note that the possible set of models of the other agent j grows exponentially with the number of time steps.",
                "For example, after T steps, there may be at most |Mt=1 j,l−1|(|Aj||Ωj|)T −1 candidate models residing in the model node. 4.2 Solution Analogous to I-IDs, the solution to a level l I-DID for agent i expanded over T time steps may be carried out recursively.",
                "For the purpose of illustration, let l=1 and T=2.",
                "The solution method uses the standard look-ahead technique, projecting the agents action and observation sequences forward from the current belief state [17], and finding the possible beliefs that i could have in the next time step.",
                "Because agent i has a belief over js models as well, the lookahead includes finding out the possible models that j could have in the future.",
                "Consequently, each of js subintentional or level 0 models (represented using a standard DID) in the first time step must be solved to obtain its optimal set of actions.",
                "These actions are combined with the set of possible observations that j could make in that model, resulting in an updated set of candidate models (that include the updated beliefs) that could describe the behavior of j. Beliefs over this updated set of candidate models are calculated using the standard inference methods using the dependency relationships between the model nodes as shown in Fig. 3(b).",
                "We note the recursive nature of this solution: in solving agent is level 1 I-DID, js level 0 DIDs must be solved.",
                "If the nesting of models is deeper, all models at all levels starting from 0 are solved in a bottom-up manner.",
                "We briefly outline the recursive algorithm for solving agent is Algorithm for solving I-DID Input : level l ≥ 1 I-ID or level 0 ID, T Expansion Phase 1.",
                "For t from 1 to T − 1 do 2.",
                "If l ≥ 1 then Populate Mt+1 j,l−1 3.",
                "For each mt j in Range(Mt j,l−1) do 4.",
                "Recursively call algorithm with the l − 1 I-ID (or ID) that represents mt j and the horizon, T − t + 1 5.",
                "Map the decision node of the solved I-ID (or ID), OPT(mt j), to a chance node Aj 6.",
                "For each aj in OPT(mt j) do 7.",
                "For each oj in Oj (part of mt j) do 8.",
                "Update js belief, bt+1 j ← SE(bt j, aj, oj) 9. mt+1 j ← New I-ID (or ID) with bt+1 j as the initial belief 10.",
                "Range(Mt+1 j,l−1) ∪ ← {mt+1 j } 11.",
                "Add the model node, Mt+1 j,l−1, and the dependency links between Mt j,l−1 and Mt+1 j,l−1 (shown in Fig. 3(b)) 12.",
                "Add the chance, decision, and utility nodes for t + 1 time slice and the dependency links between them 13.",
                "Establish the CPTs for each chance node and utility node Look-Ahead Phase 14.",
                "Apply the standard look-ahead and backup method to solve the expanded I-DID Figure 5: Algorithm for solving a level l ≥ 0 I-DID. level l I-DID expanded over T time steps with one other agent j in Fig. 5.",
                "We adopt a two-phase approach: Given an I-ID of level l (described previously in Section 3) with all lower level models also represented as I-IDs or IDs (if level 0), the first step is to expand the level l I-ID over T time steps adding the dependency links and the conditional probability tables for each node.",
                "We particularly focus on establishing and populating the model nodes (lines 3-11).",
                "Note that Range(·) returns the values (lower level models) of the random variable given as input (model node).",
                "In the second phase, we use a standard look-ahead technique projecting the action and observation sequences over T time steps in the future, and backing up the utility values of the reachable beliefs.",
                "Similar to I-IDs, the I-DIDs reduce to DIDs in the absence of other agents.",
                "As we mentioned previously, the 0-th level models are the traditional DIDs.",
                "Their solutions provide probability distributions over actions of the agent modeled at that level to I-DIDs at level 1.",
                "Given probability distributions over other agents actions the level 1 IDIDs can themselves be solved as DIDs, and provide probability distributions to yet higher level models.",
                "Assume that the number of models considered at each level is bound by a number, M. Solving an I-DID of level l in then equivalent to solving O(Ml ) DIDs. 5.",
                "EXAMPLE APPLICATIONS To illustrate the usefulness of I-DIDs, we apply them to three problem domains.",
                "We describe, in particular, the formulation of the I-DID and the optimal prescriptions obtained on solving it. 5.1 Followership-Leadership in the Multiagent Tiger Problem We begin our illustrations of using I-IDs and I-DIDs with a slightly modified version of the multiagent tiger problem discussed in [9].",
                "The problem has two agents, each of which can open the right door (OR), the left door (OL) or listen (L).",
                "In addition to hearing growls (from the left (GL) or from the right (GR)) when they listen, the agents also hear creaks (from the left (CL), from the right (CR), or no creaks (S)), which noisily indicate the other agents opening one of the doors.",
                "When any door is opened, the tiger persists in its original location with a probability of 95%.",
                "Agent i hears growls with a reliability of 65% and creaks with a reliability of 95%.",
                "Agent j, on the other hand, hears growls with a reliability of 95%.",
                "Thus, the setting is such that agent i hears agent j opening doors more reliably than the tigers growls.",
                "This suggests that i could use js actions as an indication of the location of the tiger, as we discuss below.",
                "Each agents preferences are as in the single agent game discussed in [13].",
                "The transition, observation, and reward functions are shown in [16].",
                "A good indicator of the usefulness of normative methods for decision-making like I-DIDs is the emergence of realistic social behaviors in their prescriptions.",
                "In settings of the persistent multiagent tiger problem that reflect real world situations, we demonstrate followership between the agents and, as shown in [15], deception among agents who believe that they are in a follower-leader type of relationship.",
                "In particular, we analyze the situational and epistemological conditions sufficient for their emergence.",
                "The followership behavior, for example, results from the agent knowing its own weaknesses, assessing the strengths, preferences, and possible behaviors of the other, and realizing that its best for it to follow the others actions in order to maximize its payoffs.",
                "Let us consider a particular setting of the tiger problem in which agent i believes that js preferences are aligned with its own - both of them just want to get the gold - and js hearing is more reliable in comparison to itself.",
                "As an example, suppose that j, on listening can discern the tigers location 95% of the times compared to is 65% accuracy.",
                "Additionally, agent i does not have any initial information about the tigers location.",
                "In other words, is single-level nested belief, bi,1, assigns 0.5 to each of the two locations of the tiger.",
                "In addition, i considers two models of j, which differ in js flat level 0 initial beliefs.",
                "This is represented in the level 1 I-ID shown in Fig. 6(a).",
                "According to one model, j assigns a probability of 0.9 that the tiger is behind the left door, while the other 818 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Figure 6: (a) Level 1 I-ID of agent i, (b) two level 0 IDs of agent j whose decision nodes are mapped to the chance nodes, A1 j , A2 j , in (a). model assigns 0.1 to that location (see Fig. 6(b)).",
                "Agent i is undecided on these two models of j.",
                "If we vary is hearing ability, and solve the corresponding level 1 I-ID expanded over three time steps, we obtain the normative behavioral policies shown in Fig 7 that exhibit followership behavior.",
                "If is probability of correctly hearing the growls is 0.65, then as shown in the policy in Fig. 7(a), i begins to conditionally follow js actions: i opens the same door that j opened previously iff is own assessment of the tigers location confirms js pick.",
                "If i loses the ability to correctly interpret the growls completely, it blindly follows j and opens the same door that j opened previously (Fig. 7(b)).",
                "Figure 7: Emergence of (a) conditional followership, and (b) blind followership in the tiger problem.",
                "Behaviors of interest are in bold. * is a wildcard, and denotes any one of the observations.",
                "We observed that a single level of belief nesting - beliefs about the others models - was sufficient for followership to emerge in the tiger problem.",
                "However, the epistemological requirements for the emergence of leadership are more complex.",
                "For an agent, say j, to emerge as a leader, followership must first emerge in the other agent i.",
                "As we mentioned previously, if i is certain that its preferences are identical to those of j, and believes that j has a better sense of hearing, i will follow js actions over time.",
                "Agent j emerges as a leader if it believes that i will follow it, which implies that js belief must be nested two levels deep to enable it to recognize its leadership role.",
                "Realizing that i will follow presents j with an opportunity to influence is actions in the benefit of the collective good or its self-interest alone.",
                "For example, in the tiger problem, let us consider a setting in which if both i and j open the correct door, then each gets a payoff of 20 that is double the original.",
                "If j alone selects the correct door, it gets the payoff of 10.",
                "On the other hand, if both agents pick the wrong door, their penalties are cut in half.",
                "In this setting, it is in both js best interest as well as the collective betterment for j to use its expertise in selecting the correct door, and thus be a good leader.",
                "However, consider a slightly different problem in which j gains from is loss and is penalized if i gains.",
                "Specifically, let is payoff be subtracted from js, indicating that j is antagonistic toward i - if j picks the correct door and i the wrong one, then is loss of 100 becomes js gain.",
                "Agent j believes that i incorrectly thinks that js preferences are those that promote the collective good and that it starts off by believing with 99% confidence where the tiger is.",
                "Because i believes that its preferences are similar to those of j, and that j starts by believing almost surely that one of the two is the correct location (two level 0 models of j), i will start by following js actions.",
                "We show is normative policy on solving its singly-nested I-DID over three time steps in Fig. 8(a).",
                "The policy demonstrates that i will blindly follow js actions.",
                "Since the tiger persists in its original location with a probability of 0.95, i will select the same door again.",
                "If j begins the game with a 99% probability that the tiger is on the right, solving js I-DID nested two levels deep, results in the policy shown in Fig. 8(b).",
                "Even though j is almost certain that OL is the correct action, it will start by selecting OR, followed by OL.",
                "Agent js intention is to deceive i who, it believes, will follow js actions, so as to gain $110 in the second time step, which is more than what j would gain if it were to be honest.",
                "Figure 8: Emergence of deception between agents in the tiger problem.",
                "Behaviors of interest are in bold. * denotes as before. (a) Agent is policy demonstrating that it will blindly follow js actions. (b) Even though j is almost certain that the tiger is on the right, it will start by selecting OR, followed by OL, in order to deceive i. 5.2 Altruism and Reciprocity in the Public Good Problem The public good (PG) problem [7], consists of a group of M agents, each of whom must either contribute some resource to a public pot or keep it for themselves.",
                "Since resources contributed to the public pot are shared among all the agents, they are less valuable to the agent when in the public pot.",
                "However, if all agents choose to contribute their resources, then the payoff to each agent is more than if no one contributes.",
                "Since an agent gets its share of the public pot irrespective of whether it has contributed or not, the dominating action is for each agent to not contribute, and instead free ride on others contributions.",
                "However, behaviors of human players in empirical simulations of the PG problem differ from the normative predictions.",
                "The experiments reveal that many players initially contribute a large amount to the public pot, and continue to contribute when the PG problem is played repeatedly, though in decreasing amounts [4].",
                "Many of these experiments [5] report that a small core group of players persistently contributes to the public pot even when all others are defecting.",
                "These experiments also reveal that players who persistently contribute have altruistic or reciprocal preferences matching expected cooperation of others.",
                "For simplicity, we assume that the game is played between M = 2 agents, i and j.",
                "Let each agent be initially endowed with XT amount of resources.",
                "While the classical PG game formulation permits each agent to contribute any quantity of resources (≤ XT ) to the public pot, we simplify the action space by allowing two possible actions.",
                "Each agent may choose to either contribute (C) a fixed amount of the resources, or not contribute.",
                "The latter action is deThe Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 819 noted as defect (D).",
                "We assume that the actions are not observable to others.",
                "The value of resources in the public pot is discounted by ci for each agent i, where ci is the marginal private return.",
                "We assume that ci < 1 so that the agent does not benefit enough that it contributes to the public pot for private gain.",
                "Simultaneously, ciM > 1, making collective contribution pareto optimal. i/j C D C 2ciXT , 2cjXT ciXT − cp, XT + cjXT − P D XT + ciXT − P, cjXT − cp XT , XT Table 1: The one-shot PG game with punishment.",
                "In order to encourage contributions, the contributing agents punish free riders but incur a small cost for administering the punishment.",
                "Let P be the punishment meted out to the defecting agent and cp the non-zero cost of punishing for the contributing agent.",
                "For simplicity, we assume that the cost of punishing is same for both the agents.",
                "The one-shot PG game with punishment is shown in Table. 1.",
                "Let ci = cj, cp > 0, and if P > XT − ciXT , then defection is no longer a dominating action.",
                "If P < XT − ciXT , then defection is the dominating action for both.",
                "If P = XT − ciXT , then the game is not dominance-solvable.",
                "Figure 9: (a) Level 1 I-ID of agent i, (b) level 0 IDs of agent j with decision nodes mapped to the chance nodes, A1 j and A2 j , in (a).",
                "We formulate a sequential version of the PG problem with punishment from the perspective of agent i.",
                "Though in the repeated PG game, the quantity in the public pot is revealed to all the agents after each round of actions, we assume in our formulation that it is hidden from the agents.",
                "Each agent may contribute a fixed amount, xc, or defect.",
                "An agent on performing an action receives an observation of plenty (PY) or meager (MR) symbolizing the state of the public pot.",
                "Notice that the observations are also indirectly indicative of agent js actions because the state of the public pot is influenced by them.",
                "The amount of resources in agent is private pot, is perfectly observable to i.",
                "The payoffs are analogous to Table. 1.",
                "Borrowing from the empirical investigations of the PG problem [5], we construct level 0 IDs for j that model altruistic and non-altruistic types (Fig. 9(b)).",
                "Specifically, our altruistic agent has a high marginal private return (cj is close to 1) and does not punish others who defect.",
                "Let xc = 1 and the level 0 agent be punished half the times it defects.",
                "With one action remaining, both types of agents choose to contribute to avoid being punished.",
                "With two actions to go, the altruistic type chooses to contribute, while the other defects.",
                "This is because cj for the altruistic type is close to 1, thus the expected punishment, 0.5P > (1 − cj), which the altruistic type avoids.",
                "Because cj for the non-altruistic type is less, it prefers not to contribute.",
                "With three steps to go, the altruistic agent contributes to avoid punishment (0.5P > 2(1 − cj)), and the non-altruistic type defects.",
                "For greater than three steps, while the altruistic agent continues to contribute to the public pot depending on how close its marginal private return is to 1, the non-altruistic type prescribes defection.",
                "We analyzed the decisions of an altruistic agent i modeled using a level 1 I-DID expanded over 3 time steps. i ascribes the two level 0 models, mentioned previously, to j (see Fig. 9).",
                "If i believes with a probability 1 that j is altruistic, i chooses to contribute for each of the three steps.",
                "This behavior persists when i is unaware of whether j is altruistic (Fig. 10(a)), and when i assigns a high probability to j being the non-altruistic type.",
                "However, when i believes with a probability 1 that j is non-altruistic and will thus surely defect, i chooses to defect to avoid being punished and because its marginal private return is less than 1.",
                "These results demonstrate that the behavior of our altruistic type resembles that found experimentally.",
                "The non-altruistic level 1 agent chooses to defect regardless of how likely it believes the other agent to be altruistic.",
                "We analyzed the behavior of a reciprocal agent type that matches expected cooperation or defection.",
                "The reciprocal types marginal private return is similar to that of the non-altruistic type, however, it obtains a greater payoff when its action is similar to that of the other.",
                "We consider the case when the reciprocal agent i is unsure of whether j is altruistic and believes that the public pot is likely to be half full.",
                "For this prior belief, i chooses to defect.",
                "On receiving an observation of plenty, i decides to contribute, while an observation of meager makes it defect (Fig. 10(b)).",
                "This is because an observation of plenty signals that the pot is likely to be greater than half full, which results from js action to contribute.",
                "Thus, among the two models ascribed to j, its type is likely to be altruistic making it likely that j will contribute again in the next time step.",
                "Agent i therefore chooses to contribute to reciprocate js action.",
                "An analogous reasoning leads i to defect when it observes a meager pot.",
                "With one action to go, i believing that j contributes, will choose to contribute too to avoid punishment regardless of its observations.",
                "Figure 10: (a) An altruistic level 1 agent always contributes. (b) A reciprocal agent i starts off by defecting followed by choosing to contribute or defect based on its observation of plenty (indicating that j is likely altruistic) or meager (j is non-altruistic). 5.3 Strategies in Two-Player Poker Poker is a popular zero sum card game that has received much attention among the AI research community as a testbed [2].",
                "Poker is played among M ≥ 2 players in which each player receives a hand of cards from a deck.",
                "While several flavors of Poker with varying complexity exist, we consider a simple version in which each player has three plys during which the player may either exchange a card (E), keep the existing hand (K), fold (F) and withdraw from the game, or call (C), requiring all players to show their hands.",
                "To keep matters simple, let M = 2, and each player receive a hand consisting of a single card drawn from the same suit.",
                "Thus, during a showdown, the player who has the numerically larger card (2 is the lowest, ace is the highest) wins the pot.",
                "During an exchange of cards, the discarded card is placed either in the L pile, indicating to the other agent that it was a low numbered card less than 8, or in the 820 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) H pile, indicating that the card had a rank greater than or equal to 8.",
                "Notice that, for example, if a lower numbered card is discarded, the probability of receiving a low card in exchange is now reduced.",
                "We show the level 1 I-ID for the simplified two-player Poker in Fig. 11.",
                "We considered two models (personality types) of agent j.",
                "The conservative type believes that it is likely that its opponent has a high numbered card in its hand.",
                "On the other hand, the aggressive agent j believes with a high probability that its opponent has a lower numbered card.",
                "Thus, the two types differ in their beliefs over their opponents hand.",
                "In both these level 0 models, the opponent is assumed to perform its actions following a fixed, uniform distribution.",
                "With three actions to go, regardless of its hand (unless it is an ace), the aggressive agent chooses to exchange its card, with the intent of improving on its current hand.",
                "This is because it believes the other to have a low card, which improves its chances of getting a high card during the exchange.",
                "The conservative agent chooses to keep its card, no matter its hand because its chances of getting a high card are slim as it believes that its opponent has one.",
                "Figure 11: (a) Level 1 I-ID of agent i.",
                "The observation reveals information about js hand of the previous time step, (b) level 0 IDs of agent j whose decision nodes are mapped to the chance nodes, A1 j , A2 j , in (a).",
                "The policy of a level 1 agent i who believes that each card except its own has an equal likelihood of being in js hand (neutral personality type) and j could be either an aggressive or conservative type, is shown in Fig. 12. is own hand contains the card numbered 8.",
                "The agent starts by keeping its card.",
                "On seeing that j did not exchange a card (N), i believes with probability 1 that j is conservative and hence will keep its cards. i responds by either keeping its card or exchanging it because j is equally likely to have a lower or higher card.",
                "If i observes that j discarded its card into the L or H pile, i believes that j is aggressive.",
                "On observing L, i realizes that j had a low card, and is likely to have a high card after its exchange.",
                "Because the probability of receiving a low card is high now, i chooses to keep its card.",
                "On observing H, believing that the probability of receiving a high numbered card is high, i chooses to exchange its card.",
                "In the final step, i chooses to call regardless of its observation history because its belief that j has a higher card is not sufficiently high to conclude that its better to fold and relinquish the payoff.",
                "This is partly due to the fact that an observation of, say, L resets the agent is previous time step beliefs over js hand to the low numbered cards only. 6.",
                "DISCUSSION We showed how DIDs may be extended to I-DIDs that enable online sequential decision-making in uncertain multiagent settings.",
                "Our graphical representation of I-DIDs improves on the previous Figure 12: A level 1 agent is three step policy in the Poker problem. i starts by believing that j is equally likely to be aggressive or conservative and could have any card in its hand with equal probability. work significantly by being more transparent, semantically clear, and capable of being solved using standard algorithms that target DIDs.",
                "I-DIDs extend NIDs to allow sequential decision-making over multiple time steps in the presence of other interacting agents.",
                "I-DIDs may be seen as concise graphical representations for IPOMDPs providing a way to exploit problem structure and carry out online decision-making as the agent acts and observes given its prior beliefs.",
                "We are currently investigating ways to solve I-DIDs approximately with provable bounds on the solution quality.",
                "Acknowledgment: We thank Piotr Gmytrasiewicz for some useful discussions related to this work.",
                "The first author would like to acknowledge the support of a UGARF grant. 7.",
                "REFERENCES [1] R. J. Aumann.",
                "Interactive epistemology i: Knowledge.",
                "International Journal of Game Theory, 28:263-300, 1999. [2] D. Billings, A. Davidson, J. Schaeffer, and D. Szafron.",
                "The challenge of poker.",
                "AIJ, 2001. [3] A. Brandenburger and E. Dekel.",
                "Hierarchies of beliefs and common knowledge.",
                "Journal of Economic Theory, 59:189-198, 1993. [4] C. Camerer.",
                "Behavioral Game Theory: Experiments in Strategic Interaction.",
                "Princeton University Press, 2003. [5] E. Fehr and S. Gachter.",
                "Cooperation and punishment in public goods experiments.",
                "American Economic Review, 90(4):980-994, 2000. [6] D. Fudenberg and D. K. Levine.",
                "The Theory of Learning in Games.",
                "MIT Press, 1998. [7] D. Fudenberg and J. Tirole.",
                "Game Theory.",
                "MIT Press, 1991. [8] Y. Gal and A. Pfeffer.",
                "A language for modeling agents decision-making processes in games.",
                "In AAMAS, 2003. [9] P. Gmytrasiewicz and P. Doshi.",
                "A framework for sequential planning in multiagent settings.",
                "JAIR, 24:49-79, 2005. [10] P. Gmytrasiewicz and E. Durfee.",
                "Rational coordination in multi-agent environments.",
                "JAAMAS, 3(4):319-350, 2000. [11] J. C. Harsanyi.",
                "Games with incomplete information played by bayesian players.",
                "Management Science, 14(3):159-182, 1967. [12] R. A. Howard and J. E. Matheson.",
                "Influence diagrams.",
                "In R. A. Howard and J. E. Matheson, editors, The Principles and Applications of Decision Analysis.",
                "Strategic Decisions Group, Menlo Park, CA 94025, 1984. [13] L. Kaelbling, M. Littman, and A. Cassandra.",
                "Planning and acting in partially observable stochastic domains.",
                "Artificial Intelligence Journal, 2, 1998. [14] D. Koller and B. Milch.",
                "Multi-agent influence diagrams for representing and solving games.",
                "In IJCAI, pages 1027-1034, 2001. [15] K. Polich and P. Gmytrasiewicz.",
                "Interactive dynamic influence diagrams.",
                "In GTDT Workshop, AAMAS, 2006. [16] B.",
                "Rathnas., P. Doshi, and P. J. Gmytrasiewicz.",
                "Exact solutions to interactive pomdps using behavioral equivalence.",
                "In Autonomous Agents and Multi-Agent Systems Conference (AAMAS), 2006. [17] S. Russell and P. Norvig.",
                "Artificial Intelligence: A Modern Approach (Second Edition).",
                "Prentice Hall, 2003. [18] R. D. Shachter.",
                "Evaluating influence diagrams.",
                "Operations Research, 34(6):871-882, 1986. [19] D. Suryadi and P. Gmytrasiewicz.",
                "Learning models of other agents using influence diagrams.",
                "In UM, 1999.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 821"
            ],
            "original_annotated_samples": [
                "I-POMDPs adopt a subjective approach to understanding strategic behavior, rooted in a decision-theoretic framework that takes a <br>decision-make</br>rs perspective in the interaction."
            ],
            "translated_annotated_samples": [
                "I-POMDPs adoptan un enfoque subjetivo para comprender el comportamiento estratégico, arraigado en un marco de teoría de decisiones que toma la perspectiva de los <br>tomadores de decisiones</br> en la interacción."
            ],
            "translated_text": "Modelos gráficos para soluciones en línea a POMDP interactivos de Prashant Doshi Dept. Departamento de Ciencias de la Computación Universidad de Georgia Athens, GA 30602, EE. UU. pdoshi@cs.uga.edu Yifeng Zeng Dept. de Ciencias de la Computación Universidad de Aalborg DK-9220 Aalborg, Dinamarca yfzeng@cs.aau.edu Qiongyu Chen Dept. de Ciencias de la Computación Universidad Nacional de Singapur 117543, Singapur chenqy@comp.nus.edu.sg RESUMEN Desarrollamos una nueva representación gráfica para procesos de decisión de Markov parcialmente observables interactivos (I-POMDPs) que es significativamente más transparente y semánticamente clara que la representación anterior. Estos modelos gráficos llamados diagramas de influencia dinámica interactiva (I-DIDs) buscan modelar explícitamente la estructura que a menudo está presente en problemas del mundo real al descomponer la situación en variables de probabilidad y decisión, y las dependencias entre las variables. Los I-DIDs generalizan los DIDs, los cuales pueden ser vistos como representaciones gráficas de POMDPs, a entornos multiagentes de la misma manera que los I-POMDPs generalizan los POMDPs. Los I-DIDs pueden ser utilizados para calcular la política de un agente en línea mientras el agente actúa y observa en un entorno poblado por otros agentes que interactúan entre sí. Usando varios ejemplos, mostramos cómo se pueden aplicar los I-DIDs y demostramos su utilidad. Categorías y Descriptores de Asignaturas I.2.11 [Inteligencia Artificial Distribuida]: Sistemas Multiagentes Términos Generales Teoría 1. Las Procesos de Decisión de Markov Interactivos Parcialmente Observables (IPOMDPs) proporcionan un marco para la toma de decisiones secuenciales en entornos multiagentes parcialmente observables. Generalizan los POMDPs [13] a entornos multiagentes al incluir los modelos computables de los otros agentes en el espacio de estados junto con los estados del entorno físico. Los modelos abarcan toda la información que influye en los comportamientos de los agentes, incluyendo sus preferencias, capacidades y creencias, y por lo tanto son análogos a los tipos en los juegos bayesianos [11]. I-POMDPs adoptan un enfoque subjetivo para comprender el comportamiento estratégico, arraigado en un marco de teoría de decisiones que toma la perspectiva de los <br>tomadores de decisiones</br> en la interacción. En [15], Polich y Gmytrasiewicz introdujeron diagramas de influencia dinámica interactivos (I-DIDs) como las representaciones computacionales de I-POMDPs. Los I-DIDs generalizan los DIDs [12], los cuales pueden ser vistos como contrapartes computacionales de POMDPs, a entornos de múltiples agentes de la misma manera que los I-POMDPs generalizan los POMDPs. Los I-DIDs contribuyen a una creciente línea de trabajo [19] que incluye diagramas de influencia multiagente (MAIDs) [14], y más recientemente, redes de diagramas de influencia (NIDs) [8]. Estos formalismos buscan modelar explícitamente la estructura que a menudo está presente en problemas del mundo real al descomponer la situación en variables de azar y decisiones, y las dependencias entre las variables. Los MAIDs proporcionan una alternativa a las formas normales y extensivas de juego utilizando un formalismo gráfico para representar juegos de información imperfecta con un nodo de decisión para las acciones de cada agente y nodos de azar que capturan la información privada de los agentes. Los MAIDs analizan objetivamente el juego, calculando eficientemente el perfil de equilibrio de Nash al explotar la estructura de independencia. NIDs extienden los MAIDs para incluir la incertidumbre de los agentes sobre el juego que se está jugando y sobre los modelos de los otros agentes. Cada modelo es un MAID y la red de MAIDs se colapsa, de abajo hacia arriba, en un solo MAID para calcular el equilibrio del juego teniendo en cuenta los diferentes modelos de cada agente. Los formalismos gráficos como MAIDs y NIDs abren una área de investigación prometedora que tiene como objetivo representar las interacciones multiagente de manera más transparente. Sin embargo, los MAIDs proporcionan un análisis del juego desde un punto de vista externo y la aplicabilidad de ambos está limitada a juegos estáticos de un solo jugador. Los asuntos son más complejos cuando consideramos interacciones que se extienden en el tiempo, donde las predicciones sobre las acciones futuras de otros deben hacerse utilizando modelos que cambian a medida que los agentes actúan y observan. Los I-DIDs abordan esta brecha al permitir la representación de modelos de otros agentes como los valores de un nodo de modelo especial. Tanto los modelos de otros agentes como las creencias originales de los agentes sobre estos modelos se actualizan con el tiempo utilizando implementaciones especializadas. En este documento, mejoramos la representación preliminar previa del I-DID mostrada en [15] utilizando la idea de que el I-ID estático es un tipo de NID. Por lo tanto, podemos utilizar construcciones de lenguaje específicas de NID, como multiplexores, para representar de manera más transparente el nodo del modelo y, posteriormente, el I-ID. Además, aclaramos la semántica del enlace de política de propósito especial introducido en la representación de I-DID por [15], y mostramos que podría ser reemplazado por enlaces de dependencia tradicionales. En la representación previa del I-DID, la actualización de la creencia de los agentes sobre los modelos de los demás a medida que los agentes actúan y reciben observaciones se denotaba utilizando un enlace especial llamado el enlace de actualización del modelo que conectaba los nodos del modelo a lo largo del tiempo. Explicamos la semántica de este enlace mostrando cómo puede ser implementado utilizando los enlaces de dependencia tradicionales entre los nodos de probabilidad que constituyen los nodos del modelo. El resultado final es una representación de I-DID que es significativamente más transparente, semánticamente clara y capaz de ser implementada utilizando los algoritmos estándar para resolver DIDs. Mostramos cómo los IDIDs pueden ser utilizados para modelar la incertidumbre de un agente sobre los modelos de otros, que a su vez pueden ser I-DIDs. La solución al I-DID es una política que prescribe lo que el agente debe hacer con el tiempo, dadas sus creencias sobre el estado físico y otros modelos. Análogos a los DIDs, los I-DIDs pueden ser utilizados para calcular la política de un agente en línea mientras el agente actúa y observa en un entorno poblado por otros agentes interactuantes. 2. ANTECEDENTES: Los IPOMDPS ANIDADOS FINITAMENTE generalizan los POMDPs a entornos multiagentes al incluir los modelos de otros agentes como parte del espacio de estados [9]. Dado que otros agentes también pueden razonar sobre otros, el espacio de estado interactivo está estratégicamente anidado; contiene creencias sobre los modelos de otros agentes y sus creencias sobre los demás. Para simplificar la presentación, consideramos un agente, i, que está interactuando con otro agente, j. Un I-POMDP finitamente anidado del agente i con un nivel de estrategia l se define como la tupla: I-POMDPi,l = ISi,l, A, Ti, Ωi, Oi, Ri donde: • ISi,l denota un conjunto de estados interactivos definido como, ISi,l = S × Mj,l−1, donde Mj,l−1 = {Θj,l−1 ∪ SMj}, para l ≥ 1, e ISi,0 = S, donde S es el conjunto de estados del entorno físico. Θj,l−1 es el conjunto de modelos intencionales computables del agente j: θj,l−1 = bj,l−1, ˆθj donde el marco, ˆθj = A, Ωj, Tj, Oj, Rj, OCj. Aquí, j es Bayesiano racional y OCj es el criterio de optimalidad de j. SMj es el conjunto de modelos subintencionales de j. Ejemplos simples de modelos subintencionales incluyen un modelo sin información [10] y un modelo de juego ficticio [6], ambos de los cuales son independientes de la historia. Damos una construcción recursiva de abajo hacia arriba del espacio de estados interactivo a continuación. Sí,0 = S, Θj,0 = { bj,0, ˆθj | bj,0 ∈ Δ(ISj,0)} Sí,1 = S × {Θj,0 ∪ SMj}, Θj,1 = { bj,1, ˆθj | bj,1 ∈ Δ(ISj,1)} . . . . . . Sí, l = S × {Θj,l−1 ∪ SMj}, Θj,l = { bj,l, ˆθj | bj,l ∈ Δ(ISj,l)} Formulaciones similares de espacios anidados han aparecido en [1, 3]. • A = Ai × Aj es el conjunto de acciones conjuntas de todos los agentes en el entorno; • Ti : S ×A×S → [0, 1], describe el efecto de las acciones conjuntas en los estados físicos del entorno; • Ωi es el conjunto de observaciones del agente i; • Oi : S × A × Ωi → [0, 1] da la probabilidad de las observaciones dadas el estado físico y la acción conjunta; • Ri : ISi × A → R describe las preferencias del agente sobre sus estados interactivos. Normalmente solo importarán los estados físicos. La política del agente es el mapeo, Ω∗ i → Δ(Ai), donde Ω∗ i es el conjunto de todos los historiales de observación del agente i. Dado que la creencia sobre los estados interactivos forma una estadística suficiente [9], la política también puede ser representada como un mapeo del conjunto de todas las creencias del agente i a una distribución sobre sus acciones, Δ(ISi) → Δ(Ai). 2.1 Actualización de Creencias Análogo a los POMDPs, un agente dentro del marco I-POMDP actualiza su creencia a medida que actúa y observa. Sin embargo, hay dos diferencias que complican la actualización de creencias en entornos multiagentes en comparación con los entornos de un solo agente. Primero, dado que el estado del entorno físico depende de las acciones de ambos agentes, la predicción de cómo cambia el estado físico debe hacerse basándose en la predicción de sus acciones. Segundo, los cambios en los modelos de js deben ser incluidos en la actualización de creencias. Específicamente, si j es intencional, entonces se debe incluir una actualización de las creencias de j debido a su acción y observación. En otras palabras, i tiene que actualizar su creencia basándose en su predicción de lo que j observaría y cómo j actualizaría su creencia. Si el modelo de js es subintencional, entonces las observaciones probables de js se añaden al historial de observaciones contenido en el modelo. Formalmente, tenemos: Pr(ist |at−1 i , bt−1 i,l ) = β ISt−1:mt−1 j =θt j bt−1 i,l (ist−1 ) × at−1 j Pr(at−1 j |θt−1 j,l−1)Oi(st , at−1 i , at−1 j , ot i) ×Ti(st−1 , at−1 i , at−1 j , st ) ot j Oj(st , at−1 i , at−1 j , ot j) ×τ(SEθt j (bt−1 j,l−1, at−1 j , ot j) − bt j,l−1) (1) donde β es la constante de normalización, τ es 1 si su argumento es 0, de lo contrario es 0, Pr(at−1 j |θt−1 j,l−1) es la probabilidad de que at−1 j sea Bayes racional para el agente descrito por el modelo θt−1 j,l−1, y SE(·) es una abreviatura para la actualización de creencias. Para una versión de la actualización de creencias cuando el modelo js es subintencional, consulte [9]. Si el agente j también se modela como un I-POMDP, entonces la actualización de creencias invoca la actualización de creencias de j (a través del término SEθt j ( bt−1 j,l−1 , at−1 j , ot j)), lo que a su vez podría invocar la actualización de creencias de is y así sucesivamente. Esta recursión en la anidación de creencias llega al nivel 0. En este nivel, la actualización de creencias del agente se reduce a una actualización de creencias de un POMDP. Para ilustraciones de la actualización de creencias, detalles adicionales sobre I-POMDPs y cómo se comparan con otros marcos multiagentes, consulte [9]. Iteración de Valor Cada estado de creencia en un I-POMDP finitamente anidado tiene un valor asociado que refleja la máxima ganancia que el agente puede esperar en este estado de creencia: Un ( bi,l, θi ) = max ai∈Ai is∈ISi,l ERi(is, ai)bi,l(is)+ γ oi∈Ωi Pr(oi|ai, bi,l)Un−1 ( SEθi (bi,l, ai, oi), θi ) (2) donde, ERi(is, ai) = aj Ri(is, ai, aj)Pr(aj|mj,l−1) (ya que is = (s, mj,l−1)). La ecuación 2 es una base para la iteración de valor en I-POMDPs. La acción óptima del agente, a∗ i, para el caso de horizonte finito con descuento, es un elemento del conjunto de acciones óptimas para el estado de creencia, OPT(θi), definido como: OPT( bi,l, θi ) = argmax ai∈Ai is∈ISi,l ERi(is, ai)bi,l(is) +γ oi∈Ωi Pr(oi|ai, bi,l)Un ( SEθi (bi,l, ai, oi), θi ) (3) 3. Diagramas de influencia interactivos. Una extensión ingenua de los diagramas de influencia (IDs) a entornos poblados por múltiples agentes es posible tratando a los otros agentes como autómatas, representados mediante nodos de probabilidad. Sin embargo, este enfoque asume que las acciones de los agentes están controladas utilizando una distribución de probabilidad que no cambia con el tiempo. Los diagramas de influencia interactivos (I-IDs) adoptan un enfoque más sofisticado al generalizar los IDs para hacerlos aplicables a entornos compartidos con otros agentes que pueden actuar, observar y actualizar sus creencias. 3.1 Sintaxis Además de los nodos habituales de probabilidad, decisión y utilidad, los IIDs incluyen un nuevo tipo de nodo llamado nodo de modelo. Mostramos un nivel general l I-ID en la Fig. 1(a), donde el nodo del modelo (Mj,l−1) está representado con un hexágono. Observamos que la distribución de probabilidad sobre el nodo de probabilidad, S, y el nodo del modelo juntos representan la creencia del agente sobre sus estados interactivos. Además del modelo 1, el modelo de nivel 0 es un POMDP: las acciones de otros agentes se tratan como eventos exógenos y se incorporan en las funciones T, O y R. El Sexto Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) Figura 1: (a) Un nivel genérico l I-ID para el agente i situado con otro agente j. El hexágono es el nodo modelo (Mj,l−1) cuya estructura mostramos en (b). Los miembros del nodo modelo son ellos mismos I-ID (m1 j,l−1, m2 j,l−1; los diagramas no se muestran aquí por simplicidad) cuyos nodos de decisión se asignan a los nodos de probabilidad correspondientes (A1 j , A2 j). Dependiendo del valor del nodo, Mod[Mj], la distribución de cada uno de los nodos de probabilidad se asigna al nodo Aj. (c) El I-ID transformado con el nodo del modelo reemplazado por los nodos de probabilidad y las relaciones entre ellos. Los I-IDs difieren de los IDs al tener un enlace discontinuo (llamado enlace de política en [15]) entre el nodo del modelo y un nodo de probabilidad, Aj, que representa la distribución sobre las acciones de los otros agentes dada su modelo. En ausencia de otros agentes, el nodo del modelo y el nodo de probabilidad, Aj, desaparecen y los I-ID se convierten en ID tradicionales. El nodo del modelo contiene los modelos computacionales alternativos atribuidos por i al otro agente del conjunto, Θj,l−1 ∪ SMj, donde Θj,l−1 y SMj fueron definidos previamente en la Sección 2. Por lo tanto, un modelo en el nodo del modelo puede ser en sí mismo un I-ID o ID, y la recursión termina cuando un modelo es un ID o subintencional. Dado que el nodo del modelo contiene los modelos alternativos del otro agente como sus valores, su representación no es trivial. En particular, algunos de los modelos dentro del nodo son I-IDs que, al resolverse, generan la política óptima de los agentes en sus nodos de decisión. Cada nodo de decisión se asigna al nodo de probabilidad correspondiente, digamos A1 j, de la siguiente manera: si OPT es el conjunto de acciones óptimas obtenidas al resolver el I-ID (o ID), entonces Pr(aj ∈ A1 j) = 1 |OPT| si aj ∈ OPT, 0 en caso contrario. Tomando prestados los conocimientos de trabajos anteriores [8], observamos que el nodo del modelo y el enlace de política discontinua que lo conecta con el nodo de probabilidad, Aj, podrían representarse como se muestra en la Fig. 1(b). El nodo de decisión de cada nivel l − 1 I-ID se transforma en un nodo de probabilidad, como mencionamos anteriormente, de modo que las acciones con el mayor valor en el nodo de decisión se les asignan probabilidades uniformes en el nodo de probabilidad, mientras que al resto se les asigna probabilidad cero. Los diferentes nodos de probabilidad (A1 j , A2 j ), uno para cada modelo, y adicionalmente, el nodo de probabilidad etiquetado Mod[Mj] forman los padres del nodo de probabilidad, Aj. Por lo tanto, hay tantos nodos de acción (A1 j , A2 j ) en Mj,l−1 como el número de modelos en el soporte de las creencias del agente. La tabla de probabilidad condicional del nodo de probabilidad, Aj, es un multiplexor que asume la distribución de cada uno de los nodos de acción (A1 j , A2 j ) dependiendo del valor de Mod[Mj]. Los valores de Mod[Mj] denotan los diferentes modelos de j. En otras palabras, cuando Mod[Mj] tiene el valor m1 j,l−1, el nodo de probabilidad Aj asume la distribución del nodo A1 j, y Aj asume la distribución de A2 j cuando Mod[Mj] tiene el valor m2 j,l−1. La distribución sobre el nodo, Mod[Mj], es la creencia del agente sobre los modelos de j dado un estado físico. Para más agentes, tendremos tantos nodos de modelo como agentes haya. Observa que la Fig. 1(b) aclara la semántica del enlace de política y muestra cómo puede ser representado utilizando los enlaces de dependencia tradicionales. En la Fig. 1(c), mostramos el I-ID transformado cuando el nodo del modelo es reemplazado por los nodos de probabilidad y las relaciones entre ellos. A diferencia de la representación en [15], no hay enlaces de políticas especiales, sino que el I-ID está compuesto únicamente por aquellos tipos de nodos que se encuentran en IDs tradicionales y relaciones de dependencia entre los nodos. Esto permite que los I-IDs sean representados e implementados utilizando herramientas de aplicación convencionales que apuntan a los IDs. Ten en cuenta que podemos ver el nivel l I-ID como un NID. Específicamente, cada uno de los modelos del nivel l − 1 dentro del nodo del modelo son bloques en el NID (ver Fig. 2). Si el nivel l = 1, cada bloque es un ID tradicional, de lo contrario, si l > 1, cada bloque dentro del NID puede ser un NID en sí mismo. Ten en cuenta que dentro de los I-IDs (o IDs) en cada nivel, solo hay un nodo de decisión único. Por lo tanto, nuestro NID no contiene ningún MAID. Figura 2: Un nivel l I-ID representado como un NID. Las probabilidades asignadas a los bloques del NID son creencias sobre modelos js condicionados a un estado físico. 3.2 Solución La solución de un I-ID procede de manera ascendente y se implementa de forma recursiva. Comenzamos resolviendo los modelos de nivel 0, que, si son intencionales, son identificadores tradicionales. Sus soluciones proporcionan distribuciones de probabilidad sobre las acciones de los otros agentes, las cuales se ingresan en los nodos de probabilidad correspondientes encontrados en el nodo del modelo del nivel 1 I-ID. El mapeo de los nodos de decisión de los modelos de nivel 0 a los nodos de probabilidad se realiza de manera que las acciones con el mayor valor en el nodo de decisión se les asignan probabilidades uniformes en el nodo de probabilidad, mientras que el resto se les asigna probabilidad cero. Dadas las distribuciones sobre las acciones dentro de los diferentes nodos de probabilidad (uno para cada modelo del otro agente), el I-ID de nivel 1 se transforma como se muestra en la Fig. 1(c). Durante la transformación, la tabla de probabilidad condicional (CPT) del nodo, Aj, se llena de tal manera que el nodo asume la distribución de cada uno de los nodos de probabilidad dependiendo del valor del nodo, Mod[Mj]. Como mencionamos anteriormente, los valores del nodo Mod[Mj] denotan los diferentes modelos del otro agente, y su distribución es la creencia del agente sobre los modelos de j condicionados al estado físico. El nivel 1 I-ID transformado es un ID tradicional que puede ser resuelto como us816 The Sixth Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) (a) (b) Figura 3: (a) Un I-DID genérico de dos niveles de tiempo para el agente i en un entorno con otro agente j. Observa el enlace de actualización del modelo punteado que denota la actualización de los modelos de j y la distribución de los modelos a lo largo del tiempo. (b) La semántica del enlace de actualización del modelo utilizando el método estándar de maximización de utilidad esperada [18]. Este procedimiento se lleva a cabo hasta el nivel l I-ID cuya solución proporciona el conjunto no vacío de acciones óptimas que el agente debe realizar dada su creencia. Ten en cuenta que, al igual que los IDs, los I-IDs son adecuados para la toma de decisiones en línea cuando se conoce la creencia actual de los agentes. 4. Diagramas de influencia dinámica interactivos (I-DIDs) Los diagramas de influencia dinámica interactivos (I-DIDs) extienden los I-IDs (y NIDs) para permitir la toma de decisiones secuencial a lo largo de varios pasos de tiempo. Así como los DIDs son representaciones gráficas estructuradas de POMDPs, los I-DIDs son los análogos gráficos en línea para I-POMDPs finitamente anidados. Los I-DIDs pueden ser utilizados para optimizar sobre una mirada anticipada finita dadas las creencias iniciales mientras se interactúa con otros agentes, posiblemente similares. 4.1 Sintaxis Representamos un I-DID de dos cortes temporales en la Fig. 3(a). Además de los nodos del modelo y el enlace de política discontinuo, lo que diferencia a un I-DID de un DID es el enlace de actualización del modelo mostrado como una flecha punteada en la Fig. 3(a). Explicamos la semántica del nodo del modelo y el enlace de la política en la sección anterior; a continuación, describimos las actualizaciones del modelo. La actualización del nodo del modelo con el tiempo implica dos pasos: primero, dados los modelos en el tiempo t, identificamos el conjunto actualizado de modelos que residen en el nodo del modelo en el tiempo t + 1. Recuerda de la Sección 2 que el modelo intencional de un agente incluye sus creencias. Debido a que los agentes actúan y reciben observaciones, sus modelos se actualizan para reflejar sus creencias cambiadas. Dado que el conjunto de acciones óptimas para un modelo podría incluir todas las acciones, y el agente podría recibir cualquiera de las |Ωj| posibles observaciones, el conjunto actualizado en el paso de tiempo t + 1 tendrá como máximo |Mt j,l−1||Aj||Ωj| modelos. Aquí, |Mt j,l−1| es el número de modelos en el paso de tiempo t, |Aj| y |Ωj| son los espacios más grandes de acciones y observaciones respectivamente, entre todos los modelos. Segundo, calculamos la nueva distribución sobre los modelos actualizados dados la distribución original y la probabilidad de que el agente realice la acción y reciba la observación que llevó al modelo actualizado. Estos pasos son parte de la actualización de creencias del agente formalizada utilizando la Ecuación 1. En la Fig. 3(b), mostramos cómo se implementa el enlace de actualización del modelo punteado en el I-DID. Si cada uno de los dos modelos de nivel l − 1 atribuidos a j en el paso de tiempo t resulta en una acción, y j podría hacer una de dos posibles observaciones, entonces el nodo del modelo en el paso de tiempo t + 1 contiene cuatro modelos actualizados (mt+1,1 j,l−1 ,mt+1,2 j,l−1 , mt+1,3 j,l−1 , y mt+1,4 j,l−1 ). Estos modelos difieren en sus creencias iniciales, cada una de las cuales es el resultado de j actualizar sus creencias debido a su acción y una posible observación. Los nodos de decisión en cada uno de los I-DIDs o DIDs que representan los modelos de nivel inferior se asignan a la Figura 4 correspondiente: I-DID transformado con los nodos del modelo y el enlace de actualización del modelo reemplazados por los nodos de probabilidad y las relaciones (en negrita) de los nodos de probabilidad, como se mencionó anteriormente. A continuación, describimos cómo se calcula la distribución sobre el conjunto actualizado de modelos (la distribución sobre el nodo de probabilidad Mod[Mt+1 j ] en Mt+1 j,l−1). La probabilidad de que el modelo actualizado js sea, digamos mt+1,1 j,l−1, depende de la probabilidad de que j realice la acción y reciba la observación que condujo a este modelo, y de la distribución previa sobre los modelos en el paso de tiempo t. Dado que el nodo de probabilidad At j asume la distribución de cada uno de los nodos de acción basados en el valor de Mod[Mt j], la probabilidad de la acción está dada por este nodo de probabilidad. Para obtener la probabilidad de una observación posible js, introducimos el nodo de probabilidad Oj, que dependiendo del valor de Mod[Mt j], asume la distribución del nodo de observación en el modelo de nivel inferior denominado Mod[Mt j]. Dado que la probabilidad de las observaciones js depende del estado físico y de las acciones conjuntas de ambos agentes, el nodo Oj está vinculado con St+1, At j y At i. De manera análoga a At j, la tabla de probabilidad condicional de Oj también es un multiplexor modulado por Mod[Mt j]. Finalmente, la distribución de los modelos previos en el tiempo t se obtiene a partir del nodo de probabilidad, Mod[Mt j ] en Mt j,l−1. Por consiguiente, los nodos de probabilidad, Mod[Mt j], At j y Oj, forman los padres de Mod[Mt+1 j] en Mt+1 j,l−1. Ten en cuenta que el enlace de actualización del modelo puede ser reemplazado por los enlaces de dependencia entre los nodos de probabilidad que constituyen los nodos del modelo en las dos etapas temporales. En la Fig. 4 mostramos los dos cortes temporales I-DID con los nodos del modelo reemplazados por los nodos de probabilidad y las relaciones entre ellos. Los nodos de probabilidad y los enlaces de dependencia que no están en negrita son estándar, generalmente encontrados en DIDs. La expansión del I-DID a lo largo de más pasos de tiempo requiere la repetición de los dos pasos de actualizar el conjunto de modelos que forman el 2. Nota que Oj representa la observación j en el tiempo t + 1. El Sexto Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 817 valores del nodo del modelo y añadiendo las relaciones entre los nodos de probabilidad, tantas veces como enlaces de actualización del modelo haya. Observamos que el conjunto posible de modelos del otro agente j crece exponencialmente con el número de pasos de tiempo. Por ejemplo, después de T pasos, puede haber como máximo |Mt=1 j,l−1|(|Aj||Ωj|)T −1 modelos candidatos residiendo en el nodo del modelo. 4.2 Solución Análoga a los I-ID, la solución a un I-DID de nivel l para el agente i expandido a lo largo de T pasos de tiempo puede llevarse a cabo de forma recursiva. Con el propósito de ilustración, dejemos l=1 y T=2. El método de solución utiliza la técnica estándar de anticipación, proyectando las secuencias de acciones y observaciones de los agentes hacia adelante desde el estado de creencia actual [17], y encontrando las posibles creencias que podría tener en el siguiente paso temporal. Dado que el agente i también tiene una creencia sobre los modelos de j, la búsqueda anticipada incluye descubrir los posibles modelos que j podría tener en el futuro. Por consiguiente, cada uno de los modelos subintencionales o de nivel 0 de js (representados utilizando un DID estándar) en el primer paso de tiempo debe resolverse para obtener su conjunto óptimo de acciones. Estas acciones se combinan con el conjunto de observaciones posibles que j podría hacer en ese modelo, lo que resulta en un conjunto actualizado de modelos candidatos (que incluyen las creencias actualizadas) que podrían describir el comportamiento de j. Las creencias sobre este conjunto actualizado de modelos candidatos se calculan utilizando los métodos estándar de inferencia que utilizan las relaciones de dependencia entre los nodos del modelo, como se muestra en la Figura 3(b). Observamos la naturaleza recursiva de esta solución: al resolver el agente de nivel 1 I-DID, los DIDs de nivel 0 deben ser resueltos. Si el anidamiento de los modelos es más profundo, todos los modelos en todos los niveles, comenzando desde el nivel 0, se resuelven de manera ascendente. Breve descripción del algoritmo recursivo para resolver el agente es el Algoritmo para resolver I-DID. Entrada: nivel l ≥ 1 I-ID o nivel 0 ID, T Fase de Expansión 1. Para t desde 1 hasta T − 1, hacer 2. Si l ≥ 1, entonces llenar Mt+1 j,l−1 3. Para cada mt j en el rango (Mt j,l−1) hacer 4. Llame recursivamente al algoritmo con el l - 1 I-ID (o ID) que representa mt j y el horizonte, T - t + 1 5. Mapea el nodo de decisión del I-ID (o ID) resuelto, OPT(mt j), a un nodo de probabilidad Aj 6. Para cada aj en OPT(mt j) hacer 7. Para cada oj en Oj (parte de mt j) hacer 8. Actualizar la creencia js, bt+1 j ← SE(bt j, aj, oj) 9. mt+1 j ← Nuevo I-ID (o ID) con bt+1 j como la creencia inicial 10. Rango(Mt+1 j,l−1) ∪ ← {mt+1 j } 11. Añadir el nodo del modelo, Mt+1 j,l−1, y los enlaces de dependencia entre Mt j,l−1 y Mt+1 j,l−1 (mostrados en la Fig. 3(b)) 12. Agregue los nodos de probabilidad, decisión y utilidad para la franja de tiempo t + 1 y los enlaces de dependencia entre ellos 13. Establezca las CPTs para cada nodo de probabilidad y nodo de utilidad de la Fase de Mirada Adelante 14. Aplica el método estándar de anticipación y respaldo para resolver la Figura 5 expandida de I-DID: Algoritmo para resolver un nivel l ≥ 0 I-DID. Nivel l I-DID expandido durante T pasos de tiempo con otro agente j en la Figura 5. Adoptamos un enfoque de dos fases: Dado un I-ID de nivel l (descrito previamente en la Sección 3) con todos los modelos de niveles inferiores representados también como I-IDs o IDs (si es nivel 0), el primer paso es expandir el I-ID de nivel l a lo largo de T pasos de tiempo añadiendo los enlaces de dependencia y las tablas de probabilidad condicional para cada nodo. Nos enfocamos especialmente en establecer y poblar los nodos del modelo (líneas 3-11). Ten en cuenta que Range(·) devuelve los valores (modelos de nivel inferior) de la variable aleatoria dada como entrada (nodo del modelo). En la segunda fase, utilizamos una técnica estándar de anticipación proyectando las secuencias de acción y observación durante T pasos de tiempo en el futuro, y respaldando los valores de utilidad de las creencias alcanzables. Similar a los I-IDs, los I-DIDs se reducen a DIDs en ausencia de otros agentes. Como mencionamos anteriormente, los modelos de nivel 0 son los DIDs tradicionales. Sus soluciones proporcionan distribuciones de probabilidad sobre las acciones del agente modelado en ese nivel a los I-DIDs en el nivel 1. Dadas las distribuciones de probabilidad sobre las acciones de otros agentes, los IDIDs de nivel 1 pueden resolverse como DIDs y proporcionar distribuciones de probabilidad a modelos de niveles superiores. Suponga que el número de modelos considerados en cada nivel está limitado por un número, M. Resolver un I-DID de nivel l es equivalente a resolver O(Ml) DIDs. Para ilustrar la utilidad de los I-DIDs, los aplicamos a tres dominios de problemas. Describimos, en particular, la formulación del I-DID y las prescripciones óptimas obtenidas al resolverlo. 5.1 Seguimiento-Liderazgo en el Problema del Tigre Multiagente Comenzamos nuestras ilustraciones del uso de I-IDs e I-DIDs con una versión ligeramente modificada del problema del tigre multiagente discutido en [9]. El problema tiene dos agentes, cada uno de los cuales puede abrir la puerta derecha (OR), la puerta izquierda (OL) o escuchar (L). Además de escuchar gruñidos (desde la izquierda (GL) o desde la derecha (GR)) cuando escuchan, los agentes también escuchan chirridos (desde la izquierda (CL), desde la derecha (CR) o sin chirridos (S)), que indican ruidosamente que los otros agentes están abriendo una de las puertas. Cuando se abre cualquier puerta, el tigre persiste en su ubicación original con una probabilidad del 95%. El agente i escucha gruñidos con una fiabilidad del 65% y crujidos con una fiabilidad del 95%. El agente J, por otro lado, escucha gruñidos con una fiabilidad del 95%. Por lo tanto, la configuración es tal que el agente i escucha la apertura de puertas del agente j de manera más confiable que los rugidos de los tigres. Esto sugiere que podría usar acciones de JavaScript como indicación de la ubicación del tigre, como discutimos a continuación. Las preferencias de cada agente son como en el juego de un solo agente discutido en [13]. Las funciones de transición, observación y recompensa se muestran en [16]. Un buen indicador de la utilidad de los métodos normativos para la toma de decisiones como los I-DIDs es la aparición de comportamientos sociales realistas en sus prescripciones. En entornos del persistente problema del tigre multiagente que reflejan situaciones del mundo real, demostramos la seguidismo entre los agentes y, como se muestra en [15], el engaño entre agentes que creen que están en una relación de tipo seguidor-líder. En particular, analizamos las condiciones situacionales y epistemológicas suficientes para su surgimiento. El comportamiento de seguidores, por ejemplo, resulta del agente conociendo sus propias debilidades, evaluando las fortalezas, preferencias y posibles comportamientos del otro, y dándose cuenta de que es mejor para él seguir las acciones de los demás para maximizar sus ganancias. Consideremos una configuración particular del problema del tigre en la que el agente i cree que las preferencias de j están alineadas con las suyas: ambos solo quieren obtener el oro, y la audición de j es más confiable en comparación consigo mismo. Como ejemplo, supongamos que j, al escuchar, puede discernir la ubicación del tigre el 95% de las veces en comparación con su precisión del 65%. Además, el agente i no tiene ninguna información inicial sobre la ubicación de los tigres. En otras palabras, la creencia anidada de un solo nivel, bi,1, asigna 0.5 a cada una de las dos ubicaciones del tigre. Además, considera dos modelos de j, que difieren en los niveles iniciales de creencias planas de j. Esto se representa en el nivel 1 I-ID mostrado en la Fig. 6(a). Según un modelo, j asigna una probabilidad de 0.9 a que el tigre esté detrás de la puerta izquierda, mientras que el otro 818 The Sixth Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) Figura 6: (a) Nivel 1 I-ID del agente i, (b) dos IDs de nivel 0 del agente j cuyos nodos de decisión están mapeados a los nodos de probabilidad, A1 j , A2 j , en (a). El modelo asigna 0.1 a esa ubicación (ver Fig. 6(b)). El agente i está indeciso sobre estos dos modelos de j. Si variamos la capacidad auditiva, y resolvemos el nivel 1 correspondiente I-ID expandido en tres pasos de tiempo, obtenemos las políticas de comportamiento normativas mostradas en la Figura 7 que exhiben comportamiento de seguidores. Si la probabilidad de escuchar correctamente los gruñidos es de 0.65, entonces, como se muestra en la política en la Fig. 7(a), i comienza a seguir condicionalmente las acciones de j: i abre la misma puerta que j abrió anteriormente si su evaluación propia de la ubicación de los tigres confirma la elección de j. Si i pierde la capacidad de interpretar correctamente los gruñidos por completo, sigue ciegamente a j y abre la misma puerta que j abrió anteriormente (Fig. 7(b)). Figura 7: Emergencia de (a) seguidores condicionales, y (b) seguidores ciegos en el problema del tigre. Los comportamientos de interés están en negrita. * es un comodín y representa cualquiera de las observaciones. Observamos que un solo nivel de anidación de creencias, creencias sobre los modelos de los demás, fue suficiente para que surgiera el seguidismo en el problema del tigre. Sin embargo, los requisitos epistemológicos para el surgimiento del liderazgo son más complejos. Para que un agente, digamos j, surja como líder, primero debe surgir el seguidismo en el otro agente i. Como mencionamos anteriormente, si i está seguro de que sus preferencias son idénticas a las de j, y cree que j tiene un mejor sentido del oído, i seguirá las acciones de j con el tiempo. El agente j emerge como líder si cree que lo seguiré, lo que implica que la creencia de j debe estar anidada dos niveles de profundidad para permitirle reconocer su papel de liderazgo. Darse cuenta de que lo seguiré presenta a J una oportunidad para influir en sus acciones en beneficio del bien colectivo o de su interés propio solamente. Por ejemplo, en el problema del tigre, consideremos una situación en la que si tanto i como j abren la puerta correcta, entonces cada uno recibe un pago de 20 que es el doble del original. Si solo j selecciona la puerta correcta, obtiene la recompensa de 10. Por otro lado, si ambos agentes eligen la puerta incorrecta, sus penalizaciones se reducen a la mitad. En este entorno, es tanto en el mejor interés de j como en el beneficio colectivo que utilice su experiencia para seleccionar la puerta correcta, y así ser un buen líder. Sin embargo, considera un problema ligeramente diferente en el que j se beneficia de la pérdida de i y es penalizado si i se beneficia. Específicamente, dejemos que la ganancia se reste de js, indicando que j es antagonista hacia i: si j elige la puerta correcta y i la incorrecta, entonces la pérdida de 100 se convierte en la ganancia de js. El agente J cree que yo incorrectamente pienso que las preferencias de J son aquellas que promueven el bien colectivo y que comienza creyendo con un 99% de confianza dónde está el tigre. Dado que creo que mis preferencias son similares a las de j, y que j comienza creyendo casi con certeza que una de las dos ubicaciones es la correcta (dos modelos de nivel 0 de j), comenzaré siguiendo las acciones de j. Mostramos la política normativa para resolver su I-DID anidado individualmente durante tres pasos de tiempo en la Fig. 8(a). La política demuestra que seguiré ciegamente las acciones de js. Dado que el tigre persiste en su ubicación original con una probabilidad del 0.95, seleccionaré nuevamente la misma puerta. Si j comienza el juego con una probabilidad del 99% de que el tigre esté a la derecha, resolver su I-DID anidado dos niveles profundamente, resulta en la política mostrada en la Fig. 8(b). Aunque j está casi seguro de que OL es la acción correcta, comenzará seleccionando OR, seguido de OL. La intención del agente js es engañar a i, a quien cree que seguirá las acciones de js, para así obtener $110 en el segundo paso de tiempo, lo cual es más de lo que j ganaría si fuera honesto. Figura 8: Emergencia del engaño entre agentes en el problema del tigre. Los comportamientos de interés están en negrita. * denota como antes. (a) El agente es una política que demuestra que seguirá ciegamente las acciones de js. (b) Aunque j está casi seguro de que el tigre está a la derecha, comenzará seleccionando OR, seguido de OL, para engañar a i. 5.2 Altruismo y reciprocidad en el problema del bien público El problema del bien público (PG) [7], consiste en un grupo de M agentes, cada uno de los cuales debe contribuir con algún recurso a una olla pública o guardarlo para sí mismos. Dado que los recursos aportados al fondo público se comparten entre todos los agentes, son menos valiosos para el agente cuando están en el fondo público. Sin embargo, si todos los agentes eligen contribuir con sus recursos, entonces la recompensa para cada agente es mayor que si nadie contribuye. Dado que un agente recibe su parte del bote público independientemente de si ha contribuido o no, la acción dominante es que cada agente no contribuya y en su lugar se aproveche de las contribuciones de los demás. Sin embargo, los comportamientos de los jugadores humanos en simulaciones empíricas del problema de PG difieren de las predicciones normativas. Los experimentos revelan que muchos jugadores inicialmente contribuyen con una gran cantidad al bote público, y continúan contribuyendo cuando se juega el problema del PG repetidamente, aunque en cantidades decrecientes [4]. Muchos de estos experimentos [5] informan que un pequeño grupo central de jugadores contribuye persistentemente al bote público incluso cuando todos los demás están desertando. Estos experimentos también revelan que los jugadores que contribuyen persistentemente tienen preferencias altruistas o recíprocas que coinciden con la cooperación esperada de los demás. Para simplificar, asumimos que el juego se juega entre M = 2 agentes, i y j. Que cada agente esté inicialmente dotado con una cantidad de recursos XT. Mientras que la formulación clásica del juego PG permite que cada agente contribuya con cualquier cantidad de recursos (≤ XT) al bote público, simplificamos el espacio de acciones al permitir dos acciones posibles. Cada agente puede elegir contribuir (C) una cantidad fija de recursos, o no contribuir. La última acción es deThe Sixth Intl. La Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 819 fue señalada como defectuosa (D). Suponemos que las acciones no son observables por otros. El valor de los recursos en la bolsa pública se descuenta por ci para cada agente i, donde ci es el retorno privado marginal. Suponemos que ci < 1 para que el agente no se beneficie lo suficiente como para contribuir al bote público en beneficio propio. Simultáneamente, ciM > 1, lo que hace que la contribución colectiva sea óptima de Pareto. i/j C D C 2ciXT , 2cjXT ciXT − cp, XT + cjXT − P D XT + ciXT − P, cjXT − cp XT , XT Tabla 1: El juego PG de un solo disparo con castigo. Para fomentar las contribuciones, los agentes contribuyentes castigan a los que no colaboran pero incurren en un pequeño costo por administrar el castigo. Sea P la sanción impuesta al agente que se desvía y cp el costo no nulo de castigar al agente contribuyente. Para simplificar, asumimos que el costo de castigar es el mismo para ambos agentes. El juego PG de un solo disparo con castigo se muestra en la Tabla 1. Si ci = cj, cp > 0, y si P > XT − ciXT, entonces la deserción ya no es una acción dominante. Si P < XT − ciXT, entonces la deserción es la acción dominante para ambos. Si P = XT − ciXT, entonces el juego no es resoluble por dominancia. Figura 9: (a) Nivel 1 I-ID del agente i, (b) IDs de nivel 0 del agente j con nodos de decisión mapeados a los nodos de probabilidad, A1 j y A2 j, en (a). Formulamos una versión secuencial del problema PG con castigo desde la perspectiva del agente i. Aunque en el juego de PG repetido, la cantidad en la olla pública se revela a todos los agentes después de cada ronda de acciones, asumimos en nuestra formulación que está oculta para los agentes. Cada agente puede contribuir con una cantidad fija, xc, o desertar. Un agente al realizar una acción recibe una observación de abundante (PY) o escasa (MR) simbolizando el estado de la olla pública. Ten en cuenta que las observaciones también son indirectamente indicativas de las acciones del agente js porque el estado del bote público es influenciado por ellas. La cantidad de recursos en el agente es una olla privada, es perfectamente observable para i. Los pagos son análogos a la Tabla 1. Tomando prestado de las investigaciones empíricas del problema PG [5], construimos IDs de nivel 0 para j que modelan tipos altruistas y no altruistas (Fig. 9(b)). Específicamente, nuestro agente altruista tiene un alto retorno privado marginal (cj está cerca de 1) y no castiga a los demás que incumplen. Que xc = 1 y el agente de nivel 0 sea castigado la mitad de las veces que se desvía. Con una acción restante, ambos tipos de agentes eligen contribuir para evitar ser castigados. Con dos acciones por delante, el tipo altruista elige contribuir, mientras que el otro defecta. Esto se debe a que cj para el tipo altruista es cercano a 1, por lo tanto, el castigo esperado, 0.5P > (1 − cj), que el tipo altruista evita. Debido a que cj para el tipo no altruista es menor, prefiere no contribuir. Con tres pasos por delante, el agente altruista contribuye para evitar el castigo (0.5P > 2(1 − cj)), mientras que el tipo no altruista se desentiende. Para más de tres pasos, mientras el agente altruista continúa contribuyendo al bote público dependiendo de qué tan cercano esté su retorno privado marginal a 1, el tipo no altruista prescribe la deserción. Analizamos las decisiones de un agente altruista i modelado usando un nivel 1 I-DID expandido en 3 pasos de tiempo. i atribuye los dos modelos de nivel 0, mencionados anteriormente, a j (ver Fig. 9). Si creo con una probabilidad de 1 que j es altruista, elijo contribuir en cada uno de los tres pasos. Este comportamiento persiste cuando i no está al tanto de si j es altruista (Fig. 10(a)), y cuando i asigna una alta probabilidad a que j sea del tipo no altruista. Sin embargo, cuando i cree con una probabilidad de 1 que j no es altruista y por lo tanto seguramente va a traicionar, i elige traicionar para evitar ser castigado y porque su beneficio privado marginal es menor que 1. Estos resultados demuestran que el comportamiento de nuestro tipo altruista se asemeja al encontrado experimentalmente. El agente de nivel 1 no altruista elige desertar independientemente de lo altruista que crea que sea el otro agente. Analizamos el comportamiento de un tipo de agente recíproco que se ajusta a la cooperación o defección esperada. El retorno privado marginal del tipo recíproco es similar al del tipo no altruista, sin embargo, obtiene una mayor recompensa cuando su acción es similar a la del otro. Consideramos el caso en el que el agente recíproco i no está seguro de si j es altruista y cree que es probable que el bote público esté medio lleno. Para esta creencia previa, yo elijo defectar. Al recibir una observación de abundancia, decide contribuir, mientras que una observación de escasez lo hace defectar (Fig. 10(b)). Esto se debe a que una observación de abundancia indica que es probable que la olla esté más llena que a la mitad, lo cual resulta de la acción de js para contribuir. Por lo tanto, entre los dos modelos atribuidos a j, es probable que su tipo sea altruista, lo que hace probable que j contribuya nuevamente en el próximo paso de tiempo. El agente i, por lo tanto, elige contribuir para corresponder la acción de js. Un razonamiento análogo lleva a uno a defectar cuando observa una olla escasa. Con una acción por hacer, creo que si j contribuye, también elegiré contribuir para evitar castigo independientemente de sus observaciones. Figura 10: (a) Un agente altruista de nivel 1 siempre contribuye. (b) Un agente recíproco i comienza defraudando y luego elige contribuir o defraudar basándose en su observación de abundancia (indicando que j es probablemente altruista) o escasez (j no es altruista). 5.3 Estrategias en el póker de dos jugadores El póker es un popular juego de cartas de suma cero que ha recibido mucha atención en la comunidad de investigación de IA como un banco de pruebas [2]. El póker se juega entre M ≥ 2 jugadores, en el que cada jugador recibe una mano de cartas de una baraja. Si bien existen varias variantes de póker con diferentes niveles de complejidad, consideramos una versión simple en la que cada jugador tiene tres turnos durante los cuales el jugador puede intercambiar una carta (E), mantener la mano existente (K), retirarse (F) y abandonar el juego, o apostar (C), lo que requiere que todos los jugadores muestren sus manos. Para mantener las cosas simples, dejemos que M = 2, y que cada jugador reciba una mano compuesta por una sola carta sacada del mismo palo. Por lo tanto, durante un enfrentamiento, el jugador que tenga la carta numéricamente más alta (el 2 es el más bajo, el as es el más alto) gana el bote. Durante un intercambio de cartas, la carta descartada se coloca ya sea en la pila L, indicando al otro agente que era una carta de número bajo menor que 8, o en la 820 The Sixth Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) H, indicando que la carta tenía un rango mayor o igual a 8. Ten en cuenta que, por ejemplo, si se descarta una carta de número bajo, la probabilidad de recibir una carta baja a cambio se reduce. Mostramos el nivel 1 I-ID para el Poker simplificado de dos jugadores en la Fig. 11. Consideramos dos modelos (tipos de personalidad) del agente j. El tipo conservador cree que es probable que su oponente tenga una carta de alto número en su mano. Por otro lado, el agente agresivo j cree con alta probabilidad que su oponente tiene una carta de número más bajo. Por lo tanto, los dos tipos difieren en sus creencias sobre la mano de sus oponentes. En ambos estos modelos de nivel 0, se asume que el oponente realiza sus acciones siguiendo una distribución fija y uniforme. Con tres acciones restantes, independientemente de su mano (a menos que sea un as), el agente agresivo elige intercambiar su carta, con la intención de mejorar su mano actual. Esto se debe a que cree que el otro tiene una carta baja, lo que mejora sus posibilidades de obtener una carta alta durante el intercambio. El agente conservador elige quedarse con su carta, sin importar su mano, porque considera que sus posibilidades de obtener una carta alta son escasas, ya que cree que su oponente tiene una. Figura 11: (a) Nivel 1 I-ID del agente i. La observación revela información sobre la mano de js del paso de tiempo anterior, (b) IDs de nivel 0 del agente j cuyos nodos de decisión están mapeados a los nodos de probabilidad, A1 j , A2 j , en (a). La política de un agente de nivel 1 i que cree que cada carta, excepto la suya, tiene la misma probabilidad de estar en su mano (tipo de personalidad neutral) y j podría ser de tipo agresivo o conservador, se muestra en la Figura 12. Su mano contiene la carta numerada 8. El agente comienza por mantener su carta. Al ver que j no intercambió una carta (N), i cree con probabilidad 1 que j es conservador y, por lo tanto, mantendrá sus cartas. i responde manteniendo su carta o intercambiándola porque j tiene igual probabilidad de tener una carta más baja o más alta. Si observo que j descartó su carta en la pila L o H, creo que j es agresivo. Al observar a L, i se da cuenta de que j tenía una carta baja, y es probable que tenga una carta alta después de su intercambio. Dado que la probabilidad de recibir una carta baja es alta en este momento, i decide quedarse con su carta. Al observar la carta H, creyendo que la probabilidad de recibir una carta de número alto es alta, i decide intercambiar su carta. En el paso final, i decide llamar independientemente de su historial de observaciones porque su creencia de que j tiene una carta más alta no es lo suficientemente alta como para concluir que es mejor retirarse y renunciar al pago. Esto se debe en parte al hecho de que una observación de, digamos, L, restablece las creencias del agente en el paso de tiempo anterior sobre las cartas de números bajos solamente. 6. DISCUSIÓN Mostramos cómo los DIDs pueden ser extendidos a los I-DIDs que permiten la toma de decisiones secuenciales en línea en entornos multiagentes inciertos. Nuestra representación gráfica de I-DIDs mejora la Figura 12 anterior: Un agente de nivel 1 es una política de tres pasos en el problema de Poker. i comienza creyendo que j tiene la misma probabilidad de ser agresivo o conservador y podría tener cualquier carta en su mano con igual probabilidad. Trabaja significativamente al ser más transparente, semánticamente claro y capaz de ser resuelto utilizando algoritmos estándar que apuntan a los DIDs. Los I-DIDs extienden los NIDs para permitir la toma de decisiones secuenciales a lo largo de múltiples pasos de tiempo en presencia de otros agentes que interactúan. Los I-DIDs pueden ser vistos como representaciones gráficas concisas para IPOMDPs que proporcionan una forma de explotar la estructura del problema y llevar a cabo la toma de decisiones en línea a medida que el agente actúa y observa dadas sus creencias previas. Actualmente estamos investigando formas de resolver I-DIDs aproximadamente con límites demostrables en la calidad de la solución. Agradecimiento: Agradecemos a Piotr Gmytrasiewicz por algunas discusiones útiles relacionadas con este trabajo. El primer autor desea agradecer el apoyo de una beca UGARF. 7. REFERENCIAS [1] R. J. Aumann. Epistemología interactiva i: Conocimiento. Revista Internacional de Teoría de Juegos, 28:263-300, 1999. [2] D. Billings, A. Davidson, J. Schaeffer y D. Szafron. El desafío del póker. AIJ, 2001. [3] A. Brandenburger y E. Dekel. Jerarquías de creencias y conocimiento común. Revista de Teoría Económica, 59:189-198, 1993. [4] C. Camerer. Teoría de juegos conductual: Experimentos en interacción estratégica. Princeton University Press, 2003. [5] E. Fehr y S. Gachter. Cooperación y castigo en experimentos de bienes públicos. Revista Económica Americana, 90(4):980-994, 2000. [6] D. Fudenberg y D. K. Levine. La Teoría del Aprendizaje en los Juegos. MIT Press, 1998. [7] D. Fudenberg y J. Tirole. Teoría de juegos. MIT Press, 1991. [8] Y. Gal y A. Pfeffer. Un lenguaje para modelar los procesos de toma de decisiones de agentes en juegos. En AAMAS, 2003. [9] P. Gmytrasiewicz y P. Doshi. Un marco para la planificación secuencial en entornos multiagentes. JAIR, 24:49-79, 2005. [10] P. Gmytrasiewicz y E. Durfee. Coordinación racional en entornos multiagente. JAAMAS, 3(4):319-350, 2000. [11] J. C. Harsanyi. Juegos con información incompleta jugados por jugadores bayesianos. Ciencia de la Gestión, 14(3):159-182, 1967. [12] R. A. Howard y J. E. Matheson. Diagramas de influencia. En R. A. Howard y J. E. Matheson, editores, Los Principios y Aplicaciones del Análisis de Decisiones. Grupo de Decisiones Estratégicas, Menlo Park, CA 94025, 1984. [13] L. Kaelbling, M. Littman y A. Cassandra. Planificación y actuación en dominios estocásticos parcialmente observables. Revista de Inteligencia Artificial, 2, 1998. [14] D. Koller y B. Milch. Diagramas de influencia multiagente para representar y resolver juegos. En IJCAI, páginas 1027-1034, 2001. [15] K. Polich y P. Gmytrasiewicz. Diagramas de influencia interactivos y dinámicos. En el taller GTDT, AAMAS, 2006. [16] B. Rathnas, P. Doshi y P. J. Gmytrasiewicz. Soluciones exactas para POMDP interactivos utilizando equivalencia conductual. En la Conferencia de Agentes Autónomos y Sistemas Multiagente (AAMAS), 2006. [17] S. Russell y P. Norvig. Inteligencia Artificial: Un Enfoque Moderno (Segunda Edición). Prentice Hall, 2003. [18] R. D. Shachter. \n\nPrentice Hall, 2003. [18] R. D. Shachter. Evaluando diagramas de influencia. Investigación de Operaciones, 34(6):871-882, 1986. [19] D. Suryadi y P. Gmytrasiewicz. Aprendiendo modelos de otros agentes utilizando diagramas de influencia. En UM, 1999. El Sexto Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 821 ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "agent model": {
            "translated_key": "",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Graphical Models for Online Solutions to Interactive POMDPs Prashant Doshi Dept.",
                "of Computer Science University of Georgia Athens, GA 30602, USA pdoshi@cs.uga.edu Yifeng Zeng Dept. of Computer Science Aalborg University DK-9220 Aalborg, Denmark yfzeng@cs.aau.edu Qiongyu Chen Dept. of Computer Science National Univ. of Singapore 117543, Singapore chenqy@comp.nus.edu.sg ABSTRACT We develop a new graphical representation for interactive partially observable Markov decision processes (I-POMDPs) that is significantly more transparent and semantically clear than the previous representation.",
                "These graphical models called interactive dynamic influence diagrams (I-DIDs) seek to explicitly model the structure that is often present in real-world problems by decomposing the situation into chance and decision variables, and the dependencies between the variables.",
                "I-DIDs generalize DIDs, which may be viewed as graphical representations of POMDPs, to multiagent settings in the same way that I-POMDPs generalize POMDPs.",
                "I-DIDs may be used to compute the policy of an agent online as the agent acts and observes in a setting that is populated by other interacting agents.",
                "Using several examples, we show how I-DIDs may be applied and demonstrate their usefulness.",
                "Categories and Subject Descriptors I.2.11 [Distributed Artificial Intelligence]: Multiagent Systems General Terms Theory 1.",
                "INTRODUCTION Interactive partially observable Markov decision processes (IPOMDPs) [9] provide a framework for sequential decision-making in partially observable multiagent environments.",
                "They generalize POMDPs [13] to multiagent settings by including the other agents computable models in the state space along with the states of the physical environment.",
                "The models encompass all information influencing the agents behaviors, including their preferences, capabilities, and beliefs, and are thus analogous to types in Bayesian games [11].",
                "I-POMDPs adopt a subjective approach to understanding strategic behavior, rooted in a decision-theoretic framework that takes a decision-makers perspective in the interaction.",
                "In [15], Polich and Gmytrasiewicz introduced interactive dynamic influence diagrams (I-DIDs) as the computational representations of I-POMDPs.",
                "I-DIDs generalize DIDs [12], which may be viewed as computational counterparts of POMDPs, to multiagents settings in the same way that I-POMDPs generalize POMDPs.",
                "I-DIDs contribute to a growing line of work [19] that includes multi-agent influence diagrams (MAIDs) [14], and more recently, networks of influence diagrams (NIDs) [8].",
                "These formalisms seek to explicitly model the structure that is often present in real-world problems by decomposing the situation into chance and decision variables, and the dependencies between the variables.",
                "MAIDs provide an alternative to normal and extensive game forms using a graphical formalism to represent games of imperfect information with a decision node for each agents actions and chance nodes capturing the agents private information.",
                "MAIDs objectively analyze the game, efficiently computing the Nash equilibrium profile by exploiting the independence structure.",
                "NIDs extend MAIDs to include agents uncertainty over the game being played and over models of the other agents.",
                "Each model is a MAID and the network of MAIDs is collapsed, bottom up, into a single MAID for computing the equilibrium of the game keeping in mind the different models of each agent.",
                "Graphical formalisms such as MAIDs and NIDs open up a promising area of research that aims to represent multiagent interactions more transparently.",
                "However, MAIDs provide an analysis of the game from an external viewpoint and the applicability of both is limited to static single play games.",
                "Matters are more complex when we consider interactions that are extended over time, where predictions about others future actions must be made using models that change as the agents act and observe.",
                "I-DIDs address this gap by allowing the representation of other agents models as the values of a special model node.",
                "Both, other agents models and the original agents beliefs over these models are updated over time using special-purpose implementations.",
                "In this paper, we improve on the previous preliminary representation of the I-DID shown in [15] by using the insight that the static I-ID is a type of NID.",
                "Thus, we may utilize NID-specific language constructs such as multiplexers to represent the model node, and subsequently the I-ID, more transparently.",
                "Furthermore, we clarify the semantics of the special purpose policy link introduced in the representation of I-DID by [15], and show that it could be replaced by traditional dependency links.",
                "In the previous representation of the I-DID, the update of the agents belief over the models of others as the agents act and receive observations was denoted using a special link called the model update link that connected the model nodes over time.",
                "We explicate the semantics of this link by showing how it can be implemented using the traditional dependency links between the chance nodes that constitute the model nodes.",
                "The net result is a representation of I-DID that is significantly more transparent, semantically clear, and capable of being implemented using the standard algorithms for solving DIDs.",
                "We show how IDIDs may be used to model an agents uncertainty over others models, that may themselves be I-DIDs.",
                "Solution to the I-DID is a policy that prescribes what the agent should do over time, given its beliefs over the physical state and others models.",
                "Analogous to DIDs, I-DIDs may be used to compute the policy of an agent online as the agent acts and observes in a setting that is populated by other interacting agents. 2.",
                "BACKGROUND: FINITELY NESTED IPOMDPS Interactive POMDPs generalize POMDPs to multiagent settings by including other agents models as part of the state space [9].",
                "Since other agents may also reason about others, the interactive state space is strategically nested; it contains beliefs about other agents models and their beliefs about others.",
                "For simplicity of presentation we consider an agent, i, that is interacting with one other agent, j.",
                "A finitely nested I-POMDP of agent i with a strategy level l is defined as the tuple: I-POMDPi,l = ISi,l, A, Ti, Ωi, Oi, Ri where: • ISi,l denotes a set of interactive states defined as, ISi,l = S × Mj,l−1, where Mj,l−1 = {Θj,l−1 ∪ SMj}, for l ≥ 1, and ISi,0 = S, where S is the set of states of the physical environment.",
                "Θj,l−1 is the set of computable intentional models of agent j: θj,l−1 = bj,l−1, ˆθj where the frame, ˆθj = A, Ωj, Tj, Oj, Rj, OCj .",
                "Here, j is Bayes rational and OCj is js optimality criterion.",
                "SMj is the set of subintentional models of j.",
                "Simple examples of subintentional models include a no-information model [10] and a fictitious play model [6], both of which are history independent.",
                "We give a recursive bottom-up construction of the interactive state space below.",
                "ISi,0 = S, Θj,0 = { bj,0, ˆθj | bj,0 ∈ Δ(ISj,0)} ISi,1 = S × {Θj,0 ∪ SMj}, Θj,1 = { bj,1, ˆθj | bj,1 ∈ Δ(ISj,1)} . . . . . .",
                "ISi,l = S × {Θj,l−1 ∪ SMj}, Θj,l = { bj,l, ˆθj | bj,l ∈ Δ(ISj,l)} Similar formulations of nested spaces have appeared in [1, 3]. • A = Ai × Aj is the set of joint actions of all agents in the environment; • Ti : S ×A×S → [0, 1], describes the effect of the joint actions on the physical states of the environment; • Ωi is the set of observations of agent i; • Oi : S × A × Ωi → [0, 1] gives the likelihood of the observations given the physical state and joint action; • Ri : ISi × A → R describes agent is preferences over its interactive states.",
                "Usually only the physical states will matter.",
                "Agent is policy is the mapping, Ω∗ i → Δ(Ai), where Ω∗ i is the set of all observation histories of agent i.",
                "Since belief over the interactive states forms a sufficient statistic [9], the policy can also be represented as a mapping from the set of all beliefs of agent i to a distribution over its actions, Δ(ISi) → Δ(Ai). 2.1 Belief Update Analogous to POMDPs, an agent within the I-POMDP framework updates its belief as it acts and observes.",
                "However, there are two differences that complicate the belief update in multiagent settings when compared to single agent ones.",
                "First, since the state of the physical environment depends on the actions of both agents, is prediction of how the physical state changes has to be made based on its prediction of js actions.",
                "Second, changes in js models have to be included in is belief update.",
                "Specifically, if j is intentional then an update of js beliefs due to its action and observation has to be included.",
                "In other words, i has to update its belief based on its prediction of what j would observe and how j would update its belief.",
                "If js model is subintentional, then js probable observations are appended to the observation history contained in the model.",
                "Formally, we have: Pr(ist |at−1 i , bt−1 i,l ) = β ISt−1:mt−1 j =θt j bt−1 i,l (ist−1 ) × at−1 j Pr(at−1 j |θt−1 j,l−1)Oi(st , at−1 i , at−1 j , ot i) ×Ti(st−1 , at−1 i , at−1 j , st ) ot j Oj(st , at−1 i , at−1 j , ot j) ×τ(SEθt j (bt−1 j,l−1, at−1 j , ot j) − bt j,l−1) (1) where β is the normalizing constant, τ is 1 if its argument is 0 otherwise it is 0, Pr(at−1 j |θt−1 j,l−1) is the probability that at−1 j is Bayes rational for the agent described by model θt−1 j,l−1, and SE(·) is an abbreviation for the belief update.",
                "For a version of the belief update when js model is subintentional, see [9].",
                "If agent j is also modeled as an I-POMDP, then is belief update invokes js belief update (via the term SEθt j ( bt−1 j,l−1 , at−1 j , ot j)), which in turn could invoke is belief update and so on.",
                "This recursion in belief nesting bottoms out at the 0th level.",
                "At this level, the belief update of the agent reduces to a POMDP belief update. 1 For illustrations of the belief update, additional details on I-POMDPs, and how they compare with other multiagent frameworks, see [9]. 2.2 Value Iteration Each belief state in a finitely nested I-POMDP has an associated value reflecting the maximum payoff the agent can expect in this belief state: Un ( bi,l, θi ) = max ai∈Ai is∈ISi,l ERi(is, ai)bi,l(is)+ γ oi∈Ωi Pr(oi|ai, bi,l)Un−1 ( SEθi (bi,l, ai, oi), θi ) (2) where, ERi(is, ai) = aj Ri(is, ai, aj)Pr(aj|mj,l−1) (since is = (s, mj,l−1)).",
                "Eq. 2 is a basis for value iteration in I-POMDPs.",
                "Agent is optimal action, a∗ i , for the case of finite horizon with discounting, is an element of the set of optimal actions for the belief state, OPT(θi), defined as: OPT( bi,l, θi ) = argmax ai∈Ai is∈ISi,l ERi(is, ai)bi,l(is) +γ oi∈Ωi Pr(oi|ai, bi,l)Un ( SEθi (bi,l, ai, oi), θi ) (3) 3.",
                "INTERACTIVEINFLUENCEDIAGRAMS A naive extension of influence diagrams (IDs) to settings populated by multiple agents is possible by treating other agents as automatons, represented using chance nodes.",
                "However, this approach assumes that the agents actions are controlled using a probability distribution that does not change over time.",
                "Interactive influence diagrams (I-IDs) adopt a more sophisticated approach by generalizing IDs to make them applicable to settings shared with other agents who may act and observe, and update their beliefs. 3.1 Syntax In addition to the usual chance, decision, and utility nodes, IIDs include a new type of node called the model node.",
                "We show a general level l I-ID in Fig. 1(a), where the model node (Mj,l−1) is denoted using a hexagon.",
                "We note that the probability distribution over the chance node, S, and the model node together represents agent is belief over its interactive states.",
                "In addition to the model 1 The 0th level model is a POMDP: Other agents actions are treated as exogenous events and folded into the T, O, and R functions.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 815 Figure 1: (a) A generic level l I-ID for agent i situated with one other agent j.",
                "The hexagon is the model node (Mj,l−1) whose structure we show in (b).",
                "Members of the model node are I-IDs themselves (m1 j,l−1, m2 j,l−1; diagrams not shown here for simplicity) whose decision nodes are mapped to the corresponding chance nodes (A1 j , A2 j ).",
                "Depending on the value of the node, Mod[Mj], the distribution of each of the chance nodes is assigned to the node Aj. (c) The transformed I-ID with the model node replaced by the chance nodes and the relationships between them. node, I-IDs differ from IDs by having a dashed link (called the policy link in [15]) between the model node and a chance node, Aj, that represents the distribution over the other agents actions given its model.",
                "In the absence of other agents, the model node and the chance node, Aj, vanish and I-IDs collapse into traditional IDs.",
                "The model node contains the alternative computational models ascribed by i to the other agent from the set, Θj,l−1 ∪ SMj, where Θj,l−1 and SMj were defined previously in Section 2.",
                "Thus, a model in the model node may itself be an I-ID or ID, and the recursion terminates when a model is an ID or subintentional.",
                "Because the model node contains the alternative models of the other agent as its values, its representation is not trivial.",
                "In particular, some of the models within the node are I-IDs that when solved generate the agents optimal policy in their decision nodes.",
                "Each decision node is mapped to the corresponding chance node, say A1 j , in the following way: if OPT is the set of optimal actions obtained by solving the I-ID (or ID), then Pr(aj ∈ A1 j ) = 1 |OP T | if aj ∈ OPT, 0 otherwise.",
                "Borrowing insights from previous work [8], we observe that the model node and the dashed policy link that connects it to the chance node, Aj, could be represented as shown in Fig. 1(b).",
                "The decision node of each level l − 1 I-ID is transformed into a chance node, as we mentioned previously, so that the actions with the largest value in the decision node are assigned uniform probabilities in the chance node while the rest are assigned zero probability.",
                "The different chance nodes (A1 j , A2 j ), one for each model, and additionally, the chance node labeled Mod[Mj] form the parents of the chance node, Aj.",
                "Thus, there are as many action nodes (A1 j , A2 j ) in Mj,l−1 as the number of models in the support of agent is beliefs.",
                "The conditional probability table of the chance node, Aj, is a multiplexer that assumes the distribution of each of the action nodes (A1 j , A2 j ) depending on the value of Mod[Mj].",
                "The values of Mod[Mj] denote the different models of j.",
                "In other words, when Mod[Mj] has the value m1 j,l−1, the chance node Aj assumes the distribution of the node A1 j , and Aj assumes the distribution of A2 j when Mod[Mj] has the value m2 j,l−1.",
                "The distribution over the node, Mod[Mj], is the agent is belief over the models of j given a physical state.",
                "For more agents, we will have as many model nodes as there are agents.",
                "Notice that Fig. 1(b) clarifies the semantics of the policy link, and shows how it can be represented using the traditional dependency links.",
                "In Fig. 1(c), we show the transformed I-ID when the model node is replaced by the chance nodes and relationships between them.",
                "In contrast to the representation in [15], there are no special-purpose policy links, rather the I-ID is composed of only those types of nodes that are found in traditional IDs and dependency relationships between the nodes.",
                "This allows I-IDs to be represented and implemented using conventional application tools that target IDs.",
                "Note that we may view the level l I-ID as a NID.",
                "Specifically, each of the level l − 1 models within the model node are blocks in the NID (see Fig. 2).",
                "If the level l = 1, each block is a traditional ID, otherwise if l > 1, each block within the NID may itself be a NID.",
                "Note that within the I-IDs (or IDs) at each level, there is only a single decision node.",
                "Thus, our NID does not contain any MAIDs.",
                "Figure 2: A level l I-ID represented as a NID.",
                "The probabilities assigned to the blocks of the NID are is beliefs over js models conditioned on a physical state. 3.2 Solution The solution of an I-ID proceeds in a bottom-up manner, and is implemented recursively.",
                "We start by solving the level 0 models, which, if intentional, are traditional IDs.",
                "Their solutions provide probability distributions over the other agents actions, which are entered in the corresponding chance nodes found in the model node of the level 1 I-ID.",
                "The mapping from the level 0 models decision nodes to the chance nodes is carried out so that actions with the largest value in the decision node are assigned uniform probabilities in the chance node while the rest are assigned zero probability.",
                "Given the distributions over the actions within the different chance nodes (one for each model of the other agent), the level 1 I-ID is transformed as shown in Fig. 1(c).",
                "During the transformation, the conditional probability table (CPT) of the node, Aj, is populated such that the node assumes the distribution of each of the chance nodes depending on the value of the node, Mod[Mj].",
                "As we mentioned previously, the values of the node Mod[Mj] denote the different models of the other agent, and its distribution is the agent is belief over the models of j conditioned on the physical state.",
                "The transformed level 1 I-ID is a traditional ID that may be solved us816 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) (a) (b) Figure 3: (a) A generic two time-slice level l I-DID for agent i in a setting with one other agent j.",
                "Notice the dotted model update link that denotes the update of the models of j and the distribution over the models over time. (b) The semantics of the model update link. ing the standard expected utility maximization method [18].",
                "This procedure is carried out up to the level l I-ID whose solution gives the non-empty set of optimal actions that the agent should perform given its belief.",
                "Notice that analogous to IDs, I-IDs are suitable for online decision-making when the agents current belief is known. 4.",
                "INTERACTIVE DYNAMIC INFLUENCE DIAGRAMS Interactive dynamic influence diagrams (I-DIDs) extend I-IDs (and NIDs) to allow sequential decision-making over several time steps.",
                "Just as DIDs are structured graphical representations of POMDPs, I-DIDs are the graphical online analogs for finitely nested I-POMDPs.",
                "I-DIDs may be used to optimize over a finite look-ahead given initial beliefs while interacting with other, possibly similar, agents. 4.1 Syntax We depict a general two time-slice I-DID in Fig. 3(a).",
                "In addition to the model nodes and the dashed policy link, what differentiates an I-DID from a DID is the model update link shown as a dotted arrow in Fig. 3(a).",
                "We explained the semantics of the model node and the policy link in the previous section; we describe the model updates next.",
                "The update of the model node over time involves two steps: First, given the models at time t, we identify the updated set of models that reside in the model node at time t + 1.",
                "Recall from Section 2 that an agents intentional model includes its belief.",
                "Because the agents act and receive observations, their models are updated to reflect their changed beliefs.",
                "Since the set of optimal actions for a model could include all the actions, and the agent may receive any one of |Ωj| possible observations, the updated set at time step t + 1 will have at most |Mt j,l−1||Aj||Ωj| models.",
                "Here, |Mt j,l−1| is the number of models at time step t, |Aj| and |Ωj| are the largest spaces of actions and observations respectively, among all the models.",
                "Second, we compute the new distribution over the updated models given the original distribution and the probability of the agent performing the action and receiving the observation that led to the updated model.",
                "These steps are a part of agent is belief update formalized using Eq. 1.",
                "In Fig. 3(b), we show how the dotted model update link is implemented in the I-DID.",
                "If each of the two level l − 1 models ascribed to j at time step t results in one action, and j could make one of two possible observations, then the model node at time step t + 1 contains four updated models (mt+1,1 j,l−1 ,mt+1,2 j,l−1 , mt+1,3 j,l−1 , and mt+1,4 j,l−1 ).",
                "These models differ in their initial beliefs, each of which is the result of j updating its beliefs due to its action and a possible observation.",
                "The decision nodes in each of the I-DIDs or DIDs that represent the lower level models are mapped to the corresponding Figure 4: Transformed I-DID with the model nodes and model update link replaced with the chance nodes and the relationships (in bold). chance nodes, as mentioned previously.",
                "Next, we describe how the distribution over the updated set of models (the distribution over the chance node Mod[Mt+1 j ] in Mt+1 j,l−1) is computed.",
                "The probability that js updated model is, say mt+1,1 j,l−1 , depends on the probability of j performing the action and receiving the observation that led to this model, and the prior distribution over the models at time step t. Because the chance node At j assumes the distribution of each of the action nodes based on the value of Mod[Mt j ], the probability of the action is given by this chance node.",
                "In order to obtain the probability of js possible observation, we introduce the chance node Oj, which depending on the value of Mod[Mt j ] assumes the distribution of the observation node in the lower level model denoted by Mod[Mt j ].",
                "Because the probability of js observations depends on the physical state and the joint actions of both agents, the node Oj is linked with St+1 , At j, and At i. 2 Analogous to At j, the conditional probability table of Oj is also a multiplexer modulated by Mod[Mt j ].",
                "Finally, the distribution over the prior models at time t is obtained from the chance node, Mod[Mt j ] in Mt j,l−1.",
                "Consequently, the chance nodes, Mod[Mt j ], At j, and Oj, form the parents of Mod[Mt+1 j ] in Mt+1 j,l−1.",
                "Notice that the model update link may be replaced by the dependency links between the chance nodes that constitute the model nodes in the two time slices.",
                "In Fig. 4 we show the two time-slice I-DID with the model nodes replaced by the chance nodes and the relationships between them.",
                "Chance nodes and dependency links that not in bold are standard, usually found in DIDs.",
                "Expansion of the I-DID over more time steps requires the repetition of the two steps of updating the set of models that form the 2 Note that Oj represents js observation at time t + 1.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 817 values of the model node and adding the relationships between the chance nodes, as many times as there are model update links.",
                "We note that the possible set of models of the other agent j grows exponentially with the number of time steps.",
                "For example, after T steps, there may be at most |Mt=1 j,l−1|(|Aj||Ωj|)T −1 candidate models residing in the model node. 4.2 Solution Analogous to I-IDs, the solution to a level l I-DID for agent i expanded over T time steps may be carried out recursively.",
                "For the purpose of illustration, let l=1 and T=2.",
                "The solution method uses the standard look-ahead technique, projecting the agents action and observation sequences forward from the current belief state [17], and finding the possible beliefs that i could have in the next time step.",
                "Because agent i has a belief over js models as well, the lookahead includes finding out the possible models that j could have in the future.",
                "Consequently, each of js subintentional or level 0 models (represented using a standard DID) in the first time step must be solved to obtain its optimal set of actions.",
                "These actions are combined with the set of possible observations that j could make in that model, resulting in an updated set of candidate models (that include the updated beliefs) that could describe the behavior of j. Beliefs over this updated set of candidate models are calculated using the standard inference methods using the dependency relationships between the model nodes as shown in Fig. 3(b).",
                "We note the recursive nature of this solution: in solving agent is level 1 I-DID, js level 0 DIDs must be solved.",
                "If the nesting of models is deeper, all models at all levels starting from 0 are solved in a bottom-up manner.",
                "We briefly outline the recursive algorithm for solving agent is Algorithm for solving I-DID Input : level l ≥ 1 I-ID or level 0 ID, T Expansion Phase 1.",
                "For t from 1 to T − 1 do 2.",
                "If l ≥ 1 then Populate Mt+1 j,l−1 3.",
                "For each mt j in Range(Mt j,l−1) do 4.",
                "Recursively call algorithm with the l − 1 I-ID (or ID) that represents mt j and the horizon, T − t + 1 5.",
                "Map the decision node of the solved I-ID (or ID), OPT(mt j), to a chance node Aj 6.",
                "For each aj in OPT(mt j) do 7.",
                "For each oj in Oj (part of mt j) do 8.",
                "Update js belief, bt+1 j ← SE(bt j, aj, oj) 9. mt+1 j ← New I-ID (or ID) with bt+1 j as the initial belief 10.",
                "Range(Mt+1 j,l−1) ∪ ← {mt+1 j } 11.",
                "Add the model node, Mt+1 j,l−1, and the dependency links between Mt j,l−1 and Mt+1 j,l−1 (shown in Fig. 3(b)) 12.",
                "Add the chance, decision, and utility nodes for t + 1 time slice and the dependency links between them 13.",
                "Establish the CPTs for each chance node and utility node Look-Ahead Phase 14.",
                "Apply the standard look-ahead and backup method to solve the expanded I-DID Figure 5: Algorithm for solving a level l ≥ 0 I-DID. level l I-DID expanded over T time steps with one other agent j in Fig. 5.",
                "We adopt a two-phase approach: Given an I-ID of level l (described previously in Section 3) with all lower level models also represented as I-IDs or IDs (if level 0), the first step is to expand the level l I-ID over T time steps adding the dependency links and the conditional probability tables for each node.",
                "We particularly focus on establishing and populating the model nodes (lines 3-11).",
                "Note that Range(·) returns the values (lower level models) of the random variable given as input (model node).",
                "In the second phase, we use a standard look-ahead technique projecting the action and observation sequences over T time steps in the future, and backing up the utility values of the reachable beliefs.",
                "Similar to I-IDs, the I-DIDs reduce to DIDs in the absence of other agents.",
                "As we mentioned previously, the 0-th level models are the traditional DIDs.",
                "Their solutions provide probability distributions over actions of the <br>agent model</br>ed at that level to I-DIDs at level 1.",
                "Given probability distributions over other agents actions the level 1 IDIDs can themselves be solved as DIDs, and provide probability distributions to yet higher level models.",
                "Assume that the number of models considered at each level is bound by a number, M. Solving an I-DID of level l in then equivalent to solving O(Ml ) DIDs. 5.",
                "EXAMPLE APPLICATIONS To illustrate the usefulness of I-DIDs, we apply them to three problem domains.",
                "We describe, in particular, the formulation of the I-DID and the optimal prescriptions obtained on solving it. 5.1 Followership-Leadership in the Multiagent Tiger Problem We begin our illustrations of using I-IDs and I-DIDs with a slightly modified version of the multiagent tiger problem discussed in [9].",
                "The problem has two agents, each of which can open the right door (OR), the left door (OL) or listen (L).",
                "In addition to hearing growls (from the left (GL) or from the right (GR)) when they listen, the agents also hear creaks (from the left (CL), from the right (CR), or no creaks (S)), which noisily indicate the other agents opening one of the doors.",
                "When any door is opened, the tiger persists in its original location with a probability of 95%.",
                "Agent i hears growls with a reliability of 65% and creaks with a reliability of 95%.",
                "Agent j, on the other hand, hears growls with a reliability of 95%.",
                "Thus, the setting is such that agent i hears agent j opening doors more reliably than the tigers growls.",
                "This suggests that i could use js actions as an indication of the location of the tiger, as we discuss below.",
                "Each agents preferences are as in the single agent game discussed in [13].",
                "The transition, observation, and reward functions are shown in [16].",
                "A good indicator of the usefulness of normative methods for decision-making like I-DIDs is the emergence of realistic social behaviors in their prescriptions.",
                "In settings of the persistent multiagent tiger problem that reflect real world situations, we demonstrate followership between the agents and, as shown in [15], deception among agents who believe that they are in a follower-leader type of relationship.",
                "In particular, we analyze the situational and epistemological conditions sufficient for their emergence.",
                "The followership behavior, for example, results from the agent knowing its own weaknesses, assessing the strengths, preferences, and possible behaviors of the other, and realizing that its best for it to follow the others actions in order to maximize its payoffs.",
                "Let us consider a particular setting of the tiger problem in which agent i believes that js preferences are aligned with its own - both of them just want to get the gold - and js hearing is more reliable in comparison to itself.",
                "As an example, suppose that j, on listening can discern the tigers location 95% of the times compared to is 65% accuracy.",
                "Additionally, agent i does not have any initial information about the tigers location.",
                "In other words, is single-level nested belief, bi,1, assigns 0.5 to each of the two locations of the tiger.",
                "In addition, i considers two models of j, which differ in js flat level 0 initial beliefs.",
                "This is represented in the level 1 I-ID shown in Fig. 6(a).",
                "According to one model, j assigns a probability of 0.9 that the tiger is behind the left door, while the other 818 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Figure 6: (a) Level 1 I-ID of agent i, (b) two level 0 IDs of agent j whose decision nodes are mapped to the chance nodes, A1 j , A2 j , in (a). model assigns 0.1 to that location (see Fig. 6(b)).",
                "Agent i is undecided on these two models of j.",
                "If we vary is hearing ability, and solve the corresponding level 1 I-ID expanded over three time steps, we obtain the normative behavioral policies shown in Fig 7 that exhibit followership behavior.",
                "If is probability of correctly hearing the growls is 0.65, then as shown in the policy in Fig. 7(a), i begins to conditionally follow js actions: i opens the same door that j opened previously iff is own assessment of the tigers location confirms js pick.",
                "If i loses the ability to correctly interpret the growls completely, it blindly follows j and opens the same door that j opened previously (Fig. 7(b)).",
                "Figure 7: Emergence of (a) conditional followership, and (b) blind followership in the tiger problem.",
                "Behaviors of interest are in bold. * is a wildcard, and denotes any one of the observations.",
                "We observed that a single level of belief nesting - beliefs about the others models - was sufficient for followership to emerge in the tiger problem.",
                "However, the epistemological requirements for the emergence of leadership are more complex.",
                "For an agent, say j, to emerge as a leader, followership must first emerge in the other agent i.",
                "As we mentioned previously, if i is certain that its preferences are identical to those of j, and believes that j has a better sense of hearing, i will follow js actions over time.",
                "Agent j emerges as a leader if it believes that i will follow it, which implies that js belief must be nested two levels deep to enable it to recognize its leadership role.",
                "Realizing that i will follow presents j with an opportunity to influence is actions in the benefit of the collective good or its self-interest alone.",
                "For example, in the tiger problem, let us consider a setting in which if both i and j open the correct door, then each gets a payoff of 20 that is double the original.",
                "If j alone selects the correct door, it gets the payoff of 10.",
                "On the other hand, if both agents pick the wrong door, their penalties are cut in half.",
                "In this setting, it is in both js best interest as well as the collective betterment for j to use its expertise in selecting the correct door, and thus be a good leader.",
                "However, consider a slightly different problem in which j gains from is loss and is penalized if i gains.",
                "Specifically, let is payoff be subtracted from js, indicating that j is antagonistic toward i - if j picks the correct door and i the wrong one, then is loss of 100 becomes js gain.",
                "Agent j believes that i incorrectly thinks that js preferences are those that promote the collective good and that it starts off by believing with 99% confidence where the tiger is.",
                "Because i believes that its preferences are similar to those of j, and that j starts by believing almost surely that one of the two is the correct location (two level 0 models of j), i will start by following js actions.",
                "We show is normative policy on solving its singly-nested I-DID over three time steps in Fig. 8(a).",
                "The policy demonstrates that i will blindly follow js actions.",
                "Since the tiger persists in its original location with a probability of 0.95, i will select the same door again.",
                "If j begins the game with a 99% probability that the tiger is on the right, solving js I-DID nested two levels deep, results in the policy shown in Fig. 8(b).",
                "Even though j is almost certain that OL is the correct action, it will start by selecting OR, followed by OL.",
                "Agent js intention is to deceive i who, it believes, will follow js actions, so as to gain $110 in the second time step, which is more than what j would gain if it were to be honest.",
                "Figure 8: Emergence of deception between agents in the tiger problem.",
                "Behaviors of interest are in bold. * denotes as before. (a) Agent is policy demonstrating that it will blindly follow js actions. (b) Even though j is almost certain that the tiger is on the right, it will start by selecting OR, followed by OL, in order to deceive i. 5.2 Altruism and Reciprocity in the Public Good Problem The public good (PG) problem [7], consists of a group of M agents, each of whom must either contribute some resource to a public pot or keep it for themselves.",
                "Since resources contributed to the public pot are shared among all the agents, they are less valuable to the agent when in the public pot.",
                "However, if all agents choose to contribute their resources, then the payoff to each agent is more than if no one contributes.",
                "Since an agent gets its share of the public pot irrespective of whether it has contributed or not, the dominating action is for each agent to not contribute, and instead free ride on others contributions.",
                "However, behaviors of human players in empirical simulations of the PG problem differ from the normative predictions.",
                "The experiments reveal that many players initially contribute a large amount to the public pot, and continue to contribute when the PG problem is played repeatedly, though in decreasing amounts [4].",
                "Many of these experiments [5] report that a small core group of players persistently contributes to the public pot even when all others are defecting.",
                "These experiments also reveal that players who persistently contribute have altruistic or reciprocal preferences matching expected cooperation of others.",
                "For simplicity, we assume that the game is played between M = 2 agents, i and j.",
                "Let each agent be initially endowed with XT amount of resources.",
                "While the classical PG game formulation permits each agent to contribute any quantity of resources (≤ XT ) to the public pot, we simplify the action space by allowing two possible actions.",
                "Each agent may choose to either contribute (C) a fixed amount of the resources, or not contribute.",
                "The latter action is deThe Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 819 noted as defect (D).",
                "We assume that the actions are not observable to others.",
                "The value of resources in the public pot is discounted by ci for each agent i, where ci is the marginal private return.",
                "We assume that ci < 1 so that the agent does not benefit enough that it contributes to the public pot for private gain.",
                "Simultaneously, ciM > 1, making collective contribution pareto optimal. i/j C D C 2ciXT , 2cjXT ciXT − cp, XT + cjXT − P D XT + ciXT − P, cjXT − cp XT , XT Table 1: The one-shot PG game with punishment.",
                "In order to encourage contributions, the contributing agents punish free riders but incur a small cost for administering the punishment.",
                "Let P be the punishment meted out to the defecting agent and cp the non-zero cost of punishing for the contributing agent.",
                "For simplicity, we assume that the cost of punishing is same for both the agents.",
                "The one-shot PG game with punishment is shown in Table. 1.",
                "Let ci = cj, cp > 0, and if P > XT − ciXT , then defection is no longer a dominating action.",
                "If P < XT − ciXT , then defection is the dominating action for both.",
                "If P = XT − ciXT , then the game is not dominance-solvable.",
                "Figure 9: (a) Level 1 I-ID of agent i, (b) level 0 IDs of agent j with decision nodes mapped to the chance nodes, A1 j and A2 j , in (a).",
                "We formulate a sequential version of the PG problem with punishment from the perspective of agent i.",
                "Though in the repeated PG game, the quantity in the public pot is revealed to all the agents after each round of actions, we assume in our formulation that it is hidden from the agents.",
                "Each agent may contribute a fixed amount, xc, or defect.",
                "An agent on performing an action receives an observation of plenty (PY) or meager (MR) symbolizing the state of the public pot.",
                "Notice that the observations are also indirectly indicative of agent js actions because the state of the public pot is influenced by them.",
                "The amount of resources in agent is private pot, is perfectly observable to i.",
                "The payoffs are analogous to Table. 1.",
                "Borrowing from the empirical investigations of the PG problem [5], we construct level 0 IDs for j that model altruistic and non-altruistic types (Fig. 9(b)).",
                "Specifically, our altruistic agent has a high marginal private return (cj is close to 1) and does not punish others who defect.",
                "Let xc = 1 and the level 0 agent be punished half the times it defects.",
                "With one action remaining, both types of agents choose to contribute to avoid being punished.",
                "With two actions to go, the altruistic type chooses to contribute, while the other defects.",
                "This is because cj for the altruistic type is close to 1, thus the expected punishment, 0.5P > (1 − cj), which the altruistic type avoids.",
                "Because cj for the non-altruistic type is less, it prefers not to contribute.",
                "With three steps to go, the altruistic agent contributes to avoid punishment (0.5P > 2(1 − cj)), and the non-altruistic type defects.",
                "For greater than three steps, while the altruistic agent continues to contribute to the public pot depending on how close its marginal private return is to 1, the non-altruistic type prescribes defection.",
                "We analyzed the decisions of an altruistic agent i modeled using a level 1 I-DID expanded over 3 time steps. i ascribes the two level 0 models, mentioned previously, to j (see Fig. 9).",
                "If i believes with a probability 1 that j is altruistic, i chooses to contribute for each of the three steps.",
                "This behavior persists when i is unaware of whether j is altruistic (Fig. 10(a)), and when i assigns a high probability to j being the non-altruistic type.",
                "However, when i believes with a probability 1 that j is non-altruistic and will thus surely defect, i chooses to defect to avoid being punished and because its marginal private return is less than 1.",
                "These results demonstrate that the behavior of our altruistic type resembles that found experimentally.",
                "The non-altruistic level 1 agent chooses to defect regardless of how likely it believes the other agent to be altruistic.",
                "We analyzed the behavior of a reciprocal agent type that matches expected cooperation or defection.",
                "The reciprocal types marginal private return is similar to that of the non-altruistic type, however, it obtains a greater payoff when its action is similar to that of the other.",
                "We consider the case when the reciprocal agent i is unsure of whether j is altruistic and believes that the public pot is likely to be half full.",
                "For this prior belief, i chooses to defect.",
                "On receiving an observation of plenty, i decides to contribute, while an observation of meager makes it defect (Fig. 10(b)).",
                "This is because an observation of plenty signals that the pot is likely to be greater than half full, which results from js action to contribute.",
                "Thus, among the two models ascribed to j, its type is likely to be altruistic making it likely that j will contribute again in the next time step.",
                "Agent i therefore chooses to contribute to reciprocate js action.",
                "An analogous reasoning leads i to defect when it observes a meager pot.",
                "With one action to go, i believing that j contributes, will choose to contribute too to avoid punishment regardless of its observations.",
                "Figure 10: (a) An altruistic level 1 agent always contributes. (b) A reciprocal agent i starts off by defecting followed by choosing to contribute or defect based on its observation of plenty (indicating that j is likely altruistic) or meager (j is non-altruistic). 5.3 Strategies in Two-Player Poker Poker is a popular zero sum card game that has received much attention among the AI research community as a testbed [2].",
                "Poker is played among M ≥ 2 players in which each player receives a hand of cards from a deck.",
                "While several flavors of Poker with varying complexity exist, we consider a simple version in which each player has three plys during which the player may either exchange a card (E), keep the existing hand (K), fold (F) and withdraw from the game, or call (C), requiring all players to show their hands.",
                "To keep matters simple, let M = 2, and each player receive a hand consisting of a single card drawn from the same suit.",
                "Thus, during a showdown, the player who has the numerically larger card (2 is the lowest, ace is the highest) wins the pot.",
                "During an exchange of cards, the discarded card is placed either in the L pile, indicating to the other agent that it was a low numbered card less than 8, or in the 820 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) H pile, indicating that the card had a rank greater than or equal to 8.",
                "Notice that, for example, if a lower numbered card is discarded, the probability of receiving a low card in exchange is now reduced.",
                "We show the level 1 I-ID for the simplified two-player Poker in Fig. 11.",
                "We considered two models (personality types) of agent j.",
                "The conservative type believes that it is likely that its opponent has a high numbered card in its hand.",
                "On the other hand, the aggressive agent j believes with a high probability that its opponent has a lower numbered card.",
                "Thus, the two types differ in their beliefs over their opponents hand.",
                "In both these level 0 models, the opponent is assumed to perform its actions following a fixed, uniform distribution.",
                "With three actions to go, regardless of its hand (unless it is an ace), the aggressive agent chooses to exchange its card, with the intent of improving on its current hand.",
                "This is because it believes the other to have a low card, which improves its chances of getting a high card during the exchange.",
                "The conservative agent chooses to keep its card, no matter its hand because its chances of getting a high card are slim as it believes that its opponent has one.",
                "Figure 11: (a) Level 1 I-ID of agent i.",
                "The observation reveals information about js hand of the previous time step, (b) level 0 IDs of agent j whose decision nodes are mapped to the chance nodes, A1 j , A2 j , in (a).",
                "The policy of a level 1 agent i who believes that each card except its own has an equal likelihood of being in js hand (neutral personality type) and j could be either an aggressive or conservative type, is shown in Fig. 12. is own hand contains the card numbered 8.",
                "The agent starts by keeping its card.",
                "On seeing that j did not exchange a card (N), i believes with probability 1 that j is conservative and hence will keep its cards. i responds by either keeping its card or exchanging it because j is equally likely to have a lower or higher card.",
                "If i observes that j discarded its card into the L or H pile, i believes that j is aggressive.",
                "On observing L, i realizes that j had a low card, and is likely to have a high card after its exchange.",
                "Because the probability of receiving a low card is high now, i chooses to keep its card.",
                "On observing H, believing that the probability of receiving a high numbered card is high, i chooses to exchange its card.",
                "In the final step, i chooses to call regardless of its observation history because its belief that j has a higher card is not sufficiently high to conclude that its better to fold and relinquish the payoff.",
                "This is partly due to the fact that an observation of, say, L resets the agent is previous time step beliefs over js hand to the low numbered cards only. 6.",
                "DISCUSSION We showed how DIDs may be extended to I-DIDs that enable online sequential decision-making in uncertain multiagent settings.",
                "Our graphical representation of I-DIDs improves on the previous Figure 12: A level 1 agent is three step policy in the Poker problem. i starts by believing that j is equally likely to be aggressive or conservative and could have any card in its hand with equal probability. work significantly by being more transparent, semantically clear, and capable of being solved using standard algorithms that target DIDs.",
                "I-DIDs extend NIDs to allow sequential decision-making over multiple time steps in the presence of other interacting agents.",
                "I-DIDs may be seen as concise graphical representations for IPOMDPs providing a way to exploit problem structure and carry out online decision-making as the agent acts and observes given its prior beliefs.",
                "We are currently investigating ways to solve I-DIDs approximately with provable bounds on the solution quality.",
                "Acknowledgment: We thank Piotr Gmytrasiewicz for some useful discussions related to this work.",
                "The first author would like to acknowledge the support of a UGARF grant. 7.",
                "REFERENCES [1] R. J. Aumann.",
                "Interactive epistemology i: Knowledge.",
                "International Journal of Game Theory, 28:263-300, 1999. [2] D. Billings, A. Davidson, J. Schaeffer, and D. Szafron.",
                "The challenge of poker.",
                "AIJ, 2001. [3] A. Brandenburger and E. Dekel.",
                "Hierarchies of beliefs and common knowledge.",
                "Journal of Economic Theory, 59:189-198, 1993. [4] C. Camerer.",
                "Behavioral Game Theory: Experiments in Strategic Interaction.",
                "Princeton University Press, 2003. [5] E. Fehr and S. Gachter.",
                "Cooperation and punishment in public goods experiments.",
                "American Economic Review, 90(4):980-994, 2000. [6] D. Fudenberg and D. K. Levine.",
                "The Theory of Learning in Games.",
                "MIT Press, 1998. [7] D. Fudenberg and J. Tirole.",
                "Game Theory.",
                "MIT Press, 1991. [8] Y. Gal and A. Pfeffer.",
                "A language for modeling agents decision-making processes in games.",
                "In AAMAS, 2003. [9] P. Gmytrasiewicz and P. Doshi.",
                "A framework for sequential planning in multiagent settings.",
                "JAIR, 24:49-79, 2005. [10] P. Gmytrasiewicz and E. Durfee.",
                "Rational coordination in multi-agent environments.",
                "JAAMAS, 3(4):319-350, 2000. [11] J. C. Harsanyi.",
                "Games with incomplete information played by bayesian players.",
                "Management Science, 14(3):159-182, 1967. [12] R. A. Howard and J. E. Matheson.",
                "Influence diagrams.",
                "In R. A. Howard and J. E. Matheson, editors, The Principles and Applications of Decision Analysis.",
                "Strategic Decisions Group, Menlo Park, CA 94025, 1984. [13] L. Kaelbling, M. Littman, and A. Cassandra.",
                "Planning and acting in partially observable stochastic domains.",
                "Artificial Intelligence Journal, 2, 1998. [14] D. Koller and B. Milch.",
                "Multi-agent influence diagrams for representing and solving games.",
                "In IJCAI, pages 1027-1034, 2001. [15] K. Polich and P. Gmytrasiewicz.",
                "Interactive dynamic influence diagrams.",
                "In GTDT Workshop, AAMAS, 2006. [16] B.",
                "Rathnas., P. Doshi, and P. J. Gmytrasiewicz.",
                "Exact solutions to interactive pomdps using behavioral equivalence.",
                "In Autonomous Agents and Multi-Agent Systems Conference (AAMAS), 2006. [17] S. Russell and P. Norvig.",
                "Artificial Intelligence: A Modern Approach (Second Edition).",
                "Prentice Hall, 2003. [18] R. D. Shachter.",
                "Evaluating influence diagrams.",
                "Operations Research, 34(6):871-882, 1986. [19] D. Suryadi and P. Gmytrasiewicz.",
                "Learning models of other agents using influence diagrams.",
                "In UM, 1999.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 821"
            ],
            "original_annotated_samples": [
                "Their solutions provide probability distributions over actions of the <br>agent model</br>ed at that level to I-DIDs at level 1."
            ],
            "translated_annotated_samples": [
                "Sus soluciones proporcionan distribuciones de probabilidad sobre las acciones del agente modelado en ese nivel a los I-DIDs en el nivel 1."
            ],
            "translated_text": "Modelos gráficos para soluciones en línea a POMDP interactivos de Prashant Doshi Dept. Departamento de Ciencias de la Computación Universidad de Georgia Athens, GA 30602, EE. UU. pdoshi@cs.uga.edu Yifeng Zeng Dept. de Ciencias de la Computación Universidad de Aalborg DK-9220 Aalborg, Dinamarca yfzeng@cs.aau.edu Qiongyu Chen Dept. de Ciencias de la Computación Universidad Nacional de Singapur 117543, Singapur chenqy@comp.nus.edu.sg RESUMEN Desarrollamos una nueva representación gráfica para procesos de decisión de Markov parcialmente observables interactivos (I-POMDPs) que es significativamente más transparente y semánticamente clara que la representación anterior. Estos modelos gráficos llamados diagramas de influencia dinámica interactiva (I-DIDs) buscan modelar explícitamente la estructura que a menudo está presente en problemas del mundo real al descomponer la situación en variables de probabilidad y decisión, y las dependencias entre las variables. Los I-DIDs generalizan los DIDs, los cuales pueden ser vistos como representaciones gráficas de POMDPs, a entornos multiagentes de la misma manera que los I-POMDPs generalizan los POMDPs. Los I-DIDs pueden ser utilizados para calcular la política de un agente en línea mientras el agente actúa y observa en un entorno poblado por otros agentes que interactúan entre sí. Usando varios ejemplos, mostramos cómo se pueden aplicar los I-DIDs y demostramos su utilidad. Categorías y Descriptores de Asignaturas I.2.11 [Inteligencia Artificial Distribuida]: Sistemas Multiagentes Términos Generales Teoría 1. Las Procesos de Decisión de Markov Interactivos Parcialmente Observables (IPOMDPs) proporcionan un marco para la toma de decisiones secuenciales en entornos multiagentes parcialmente observables. Generalizan los POMDPs [13] a entornos multiagentes al incluir los modelos computables de los otros agentes en el espacio de estados junto con los estados del entorno físico. Los modelos abarcan toda la información que influye en los comportamientos de los agentes, incluyendo sus preferencias, capacidades y creencias, y por lo tanto son análogos a los tipos en los juegos bayesianos [11]. I-POMDPs adoptan un enfoque subjetivo para comprender el comportamiento estratégico, arraigado en un marco de teoría de decisiones que toma la perspectiva de los tomadores de decisiones en la interacción. En [15], Polich y Gmytrasiewicz introdujeron diagramas de influencia dinámica interactivos (I-DIDs) como las representaciones computacionales de I-POMDPs. Los I-DIDs generalizan los DIDs [12], los cuales pueden ser vistos como contrapartes computacionales de POMDPs, a entornos de múltiples agentes de la misma manera que los I-POMDPs generalizan los POMDPs. Los I-DIDs contribuyen a una creciente línea de trabajo [19] que incluye diagramas de influencia multiagente (MAIDs) [14], y más recientemente, redes de diagramas de influencia (NIDs) [8]. Estos formalismos buscan modelar explícitamente la estructura que a menudo está presente en problemas del mundo real al descomponer la situación en variables de azar y decisiones, y las dependencias entre las variables. Los MAIDs proporcionan una alternativa a las formas normales y extensivas de juego utilizando un formalismo gráfico para representar juegos de información imperfecta con un nodo de decisión para las acciones de cada agente y nodos de azar que capturan la información privada de los agentes. Los MAIDs analizan objetivamente el juego, calculando eficientemente el perfil de equilibrio de Nash al explotar la estructura de independencia. NIDs extienden los MAIDs para incluir la incertidumbre de los agentes sobre el juego que se está jugando y sobre los modelos de los otros agentes. Cada modelo es un MAID y la red de MAIDs se colapsa, de abajo hacia arriba, en un solo MAID para calcular el equilibrio del juego teniendo en cuenta los diferentes modelos de cada agente. Los formalismos gráficos como MAIDs y NIDs abren una área de investigación prometedora que tiene como objetivo representar las interacciones multiagente de manera más transparente. Sin embargo, los MAIDs proporcionan un análisis del juego desde un punto de vista externo y la aplicabilidad de ambos está limitada a juegos estáticos de un solo jugador. Los asuntos son más complejos cuando consideramos interacciones que se extienden en el tiempo, donde las predicciones sobre las acciones futuras de otros deben hacerse utilizando modelos que cambian a medida que los agentes actúan y observan. Los I-DIDs abordan esta brecha al permitir la representación de modelos de otros agentes como los valores de un nodo de modelo especial. Tanto los modelos de otros agentes como las creencias originales de los agentes sobre estos modelos se actualizan con el tiempo utilizando implementaciones especializadas. En este documento, mejoramos la representación preliminar previa del I-DID mostrada en [15] utilizando la idea de que el I-ID estático es un tipo de NID. Por lo tanto, podemos utilizar construcciones de lenguaje específicas de NID, como multiplexores, para representar de manera más transparente el nodo del modelo y, posteriormente, el I-ID. Además, aclaramos la semántica del enlace de política de propósito especial introducido en la representación de I-DID por [15], y mostramos que podría ser reemplazado por enlaces de dependencia tradicionales. En la representación previa del I-DID, la actualización de la creencia de los agentes sobre los modelos de los demás a medida que los agentes actúan y reciben observaciones se denotaba utilizando un enlace especial llamado el enlace de actualización del modelo que conectaba los nodos del modelo a lo largo del tiempo. Explicamos la semántica de este enlace mostrando cómo puede ser implementado utilizando los enlaces de dependencia tradicionales entre los nodos de probabilidad que constituyen los nodos del modelo. El resultado final es una representación de I-DID que es significativamente más transparente, semánticamente clara y capaz de ser implementada utilizando los algoritmos estándar para resolver DIDs. Mostramos cómo los IDIDs pueden ser utilizados para modelar la incertidumbre de un agente sobre los modelos de otros, que a su vez pueden ser I-DIDs. La solución al I-DID es una política que prescribe lo que el agente debe hacer con el tiempo, dadas sus creencias sobre el estado físico y otros modelos. Análogos a los DIDs, los I-DIDs pueden ser utilizados para calcular la política de un agente en línea mientras el agente actúa y observa en un entorno poblado por otros agentes interactuantes. 2. ANTECEDENTES: Los IPOMDPS ANIDADOS FINITAMENTE generalizan los POMDPs a entornos multiagentes al incluir los modelos de otros agentes como parte del espacio de estados [9]. Dado que otros agentes también pueden razonar sobre otros, el espacio de estado interactivo está estratégicamente anidado; contiene creencias sobre los modelos de otros agentes y sus creencias sobre los demás. Para simplificar la presentación, consideramos un agente, i, que está interactuando con otro agente, j. Un I-POMDP finitamente anidado del agente i con un nivel de estrategia l se define como la tupla: I-POMDPi,l = ISi,l, A, Ti, Ωi, Oi, Ri donde: • ISi,l denota un conjunto de estados interactivos definido como, ISi,l = S × Mj,l−1, donde Mj,l−1 = {Θj,l−1 ∪ SMj}, para l ≥ 1, e ISi,0 = S, donde S es el conjunto de estados del entorno físico. Θj,l−1 es el conjunto de modelos intencionales computables del agente j: θj,l−1 = bj,l−1, ˆθj donde el marco, ˆθj = A, Ωj, Tj, Oj, Rj, OCj. Aquí, j es Bayesiano racional y OCj es el criterio de optimalidad de j. SMj es el conjunto de modelos subintencionales de j. Ejemplos simples de modelos subintencionales incluyen un modelo sin información [10] y un modelo de juego ficticio [6], ambos de los cuales son independientes de la historia. Damos una construcción recursiva de abajo hacia arriba del espacio de estados interactivo a continuación. Sí,0 = S, Θj,0 = { bj,0, ˆθj | bj,0 ∈ Δ(ISj,0)} Sí,1 = S × {Θj,0 ∪ SMj}, Θj,1 = { bj,1, ˆθj | bj,1 ∈ Δ(ISj,1)} . . . . . . Sí, l = S × {Θj,l−1 ∪ SMj}, Θj,l = { bj,l, ˆθj | bj,l ∈ Δ(ISj,l)} Formulaciones similares de espacios anidados han aparecido en [1, 3]. • A = Ai × Aj es el conjunto de acciones conjuntas de todos los agentes en el entorno; • Ti : S ×A×S → [0, 1], describe el efecto de las acciones conjuntas en los estados físicos del entorno; • Ωi es el conjunto de observaciones del agente i; • Oi : S × A × Ωi → [0, 1] da la probabilidad de las observaciones dadas el estado físico y la acción conjunta; • Ri : ISi × A → R describe las preferencias del agente sobre sus estados interactivos. Normalmente solo importarán los estados físicos. La política del agente es el mapeo, Ω∗ i → Δ(Ai), donde Ω∗ i es el conjunto de todos los historiales de observación del agente i. Dado que la creencia sobre los estados interactivos forma una estadística suficiente [9], la política también puede ser representada como un mapeo del conjunto de todas las creencias del agente i a una distribución sobre sus acciones, Δ(ISi) → Δ(Ai). 2.1 Actualización de Creencias Análogo a los POMDPs, un agente dentro del marco I-POMDP actualiza su creencia a medida que actúa y observa. Sin embargo, hay dos diferencias que complican la actualización de creencias en entornos multiagentes en comparación con los entornos de un solo agente. Primero, dado que el estado del entorno físico depende de las acciones de ambos agentes, la predicción de cómo cambia el estado físico debe hacerse basándose en la predicción de sus acciones. Segundo, los cambios en los modelos de js deben ser incluidos en la actualización de creencias. Específicamente, si j es intencional, entonces se debe incluir una actualización de las creencias de j debido a su acción y observación. En otras palabras, i tiene que actualizar su creencia basándose en su predicción de lo que j observaría y cómo j actualizaría su creencia. Si el modelo de js es subintencional, entonces las observaciones probables de js se añaden al historial de observaciones contenido en el modelo. Formalmente, tenemos: Pr(ist |at−1 i , bt−1 i,l ) = β ISt−1:mt−1 j =θt j bt−1 i,l (ist−1 ) × at−1 j Pr(at−1 j |θt−1 j,l−1)Oi(st , at−1 i , at−1 j , ot i) ×Ti(st−1 , at−1 i , at−1 j , st ) ot j Oj(st , at−1 i , at−1 j , ot j) ×τ(SEθt j (bt−1 j,l−1, at−1 j , ot j) − bt j,l−1) (1) donde β es la constante de normalización, τ es 1 si su argumento es 0, de lo contrario es 0, Pr(at−1 j |θt−1 j,l−1) es la probabilidad de que at−1 j sea Bayes racional para el agente descrito por el modelo θt−1 j,l−1, y SE(·) es una abreviatura para la actualización de creencias. Para una versión de la actualización de creencias cuando el modelo js es subintencional, consulte [9]. Si el agente j también se modela como un I-POMDP, entonces la actualización de creencias invoca la actualización de creencias de j (a través del término SEθt j ( bt−1 j,l−1 , at−1 j , ot j)), lo que a su vez podría invocar la actualización de creencias de is y así sucesivamente. Esta recursión en la anidación de creencias llega al nivel 0. En este nivel, la actualización de creencias del agente se reduce a una actualización de creencias de un POMDP. Para ilustraciones de la actualización de creencias, detalles adicionales sobre I-POMDPs y cómo se comparan con otros marcos multiagentes, consulte [9]. Iteración de Valor Cada estado de creencia en un I-POMDP finitamente anidado tiene un valor asociado que refleja la máxima ganancia que el agente puede esperar en este estado de creencia: Un ( bi,l, θi ) = max ai∈Ai is∈ISi,l ERi(is, ai)bi,l(is)+ γ oi∈Ωi Pr(oi|ai, bi,l)Un−1 ( SEθi (bi,l, ai, oi), θi ) (2) donde, ERi(is, ai) = aj Ri(is, ai, aj)Pr(aj|mj,l−1) (ya que is = (s, mj,l−1)). La ecuación 2 es una base para la iteración de valor en I-POMDPs. La acción óptima del agente, a∗ i, para el caso de horizonte finito con descuento, es un elemento del conjunto de acciones óptimas para el estado de creencia, OPT(θi), definido como: OPT( bi,l, θi ) = argmax ai∈Ai is∈ISi,l ERi(is, ai)bi,l(is) +γ oi∈Ωi Pr(oi|ai, bi,l)Un ( SEθi (bi,l, ai, oi), θi ) (3) 3. Diagramas de influencia interactivos. Una extensión ingenua de los diagramas de influencia (IDs) a entornos poblados por múltiples agentes es posible tratando a los otros agentes como autómatas, representados mediante nodos de probabilidad. Sin embargo, este enfoque asume que las acciones de los agentes están controladas utilizando una distribución de probabilidad que no cambia con el tiempo. Los diagramas de influencia interactivos (I-IDs) adoptan un enfoque más sofisticado al generalizar los IDs para hacerlos aplicables a entornos compartidos con otros agentes que pueden actuar, observar y actualizar sus creencias. 3.1 Sintaxis Además de los nodos habituales de probabilidad, decisión y utilidad, los IIDs incluyen un nuevo tipo de nodo llamado nodo de modelo. Mostramos un nivel general l I-ID en la Fig. 1(a), donde el nodo del modelo (Mj,l−1) está representado con un hexágono. Observamos que la distribución de probabilidad sobre el nodo de probabilidad, S, y el nodo del modelo juntos representan la creencia del agente sobre sus estados interactivos. Además del modelo 1, el modelo de nivel 0 es un POMDP: las acciones de otros agentes se tratan como eventos exógenos y se incorporan en las funciones T, O y R. El Sexto Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) Figura 1: (a) Un nivel genérico l I-ID para el agente i situado con otro agente j. El hexágono es el nodo modelo (Mj,l−1) cuya estructura mostramos en (b). Los miembros del nodo modelo son ellos mismos I-ID (m1 j,l−1, m2 j,l−1; los diagramas no se muestran aquí por simplicidad) cuyos nodos de decisión se asignan a los nodos de probabilidad correspondientes (A1 j , A2 j). Dependiendo del valor del nodo, Mod[Mj], la distribución de cada uno de los nodos de probabilidad se asigna al nodo Aj. (c) El I-ID transformado con el nodo del modelo reemplazado por los nodos de probabilidad y las relaciones entre ellos. Los I-IDs difieren de los IDs al tener un enlace discontinuo (llamado enlace de política en [15]) entre el nodo del modelo y un nodo de probabilidad, Aj, que representa la distribución sobre las acciones de los otros agentes dada su modelo. En ausencia de otros agentes, el nodo del modelo y el nodo de probabilidad, Aj, desaparecen y los I-ID se convierten en ID tradicionales. El nodo del modelo contiene los modelos computacionales alternativos atribuidos por i al otro agente del conjunto, Θj,l−1 ∪ SMj, donde Θj,l−1 y SMj fueron definidos previamente en la Sección 2. Por lo tanto, un modelo en el nodo del modelo puede ser en sí mismo un I-ID o ID, y la recursión termina cuando un modelo es un ID o subintencional. Dado que el nodo del modelo contiene los modelos alternativos del otro agente como sus valores, su representación no es trivial. En particular, algunos de los modelos dentro del nodo son I-IDs que, al resolverse, generan la política óptima de los agentes en sus nodos de decisión. Cada nodo de decisión se asigna al nodo de probabilidad correspondiente, digamos A1 j, de la siguiente manera: si OPT es el conjunto de acciones óptimas obtenidas al resolver el I-ID (o ID), entonces Pr(aj ∈ A1 j) = 1 |OPT| si aj ∈ OPT, 0 en caso contrario. Tomando prestados los conocimientos de trabajos anteriores [8], observamos que el nodo del modelo y el enlace de política discontinua que lo conecta con el nodo de probabilidad, Aj, podrían representarse como se muestra en la Fig. 1(b). El nodo de decisión de cada nivel l − 1 I-ID se transforma en un nodo de probabilidad, como mencionamos anteriormente, de modo que las acciones con el mayor valor en el nodo de decisión se les asignan probabilidades uniformes en el nodo de probabilidad, mientras que al resto se les asigna probabilidad cero. Los diferentes nodos de probabilidad (A1 j , A2 j ), uno para cada modelo, y adicionalmente, el nodo de probabilidad etiquetado Mod[Mj] forman los padres del nodo de probabilidad, Aj. Por lo tanto, hay tantos nodos de acción (A1 j , A2 j ) en Mj,l−1 como el número de modelos en el soporte de las creencias del agente. La tabla de probabilidad condicional del nodo de probabilidad, Aj, es un multiplexor que asume la distribución de cada uno de los nodos de acción (A1 j , A2 j ) dependiendo del valor de Mod[Mj]. Los valores de Mod[Mj] denotan los diferentes modelos de j. En otras palabras, cuando Mod[Mj] tiene el valor m1 j,l−1, el nodo de probabilidad Aj asume la distribución del nodo A1 j, y Aj asume la distribución de A2 j cuando Mod[Mj] tiene el valor m2 j,l−1. La distribución sobre el nodo, Mod[Mj], es la creencia del agente sobre los modelos de j dado un estado físico. Para más agentes, tendremos tantos nodos de modelo como agentes haya. Observa que la Fig. 1(b) aclara la semántica del enlace de política y muestra cómo puede ser representado utilizando los enlaces de dependencia tradicionales. En la Fig. 1(c), mostramos el I-ID transformado cuando el nodo del modelo es reemplazado por los nodos de probabilidad y las relaciones entre ellos. A diferencia de la representación en [15], no hay enlaces de políticas especiales, sino que el I-ID está compuesto únicamente por aquellos tipos de nodos que se encuentran en IDs tradicionales y relaciones de dependencia entre los nodos. Esto permite que los I-IDs sean representados e implementados utilizando herramientas de aplicación convencionales que apuntan a los IDs. Ten en cuenta que podemos ver el nivel l I-ID como un NID. Específicamente, cada uno de los modelos del nivel l − 1 dentro del nodo del modelo son bloques en el NID (ver Fig. 2). Si el nivel l = 1, cada bloque es un ID tradicional, de lo contrario, si l > 1, cada bloque dentro del NID puede ser un NID en sí mismo. Ten en cuenta que dentro de los I-IDs (o IDs) en cada nivel, solo hay un nodo de decisión único. Por lo tanto, nuestro NID no contiene ningún MAID. Figura 2: Un nivel l I-ID representado como un NID. Las probabilidades asignadas a los bloques del NID son creencias sobre modelos js condicionados a un estado físico. 3.2 Solución La solución de un I-ID procede de manera ascendente y se implementa de forma recursiva. Comenzamos resolviendo los modelos de nivel 0, que, si son intencionales, son identificadores tradicionales. Sus soluciones proporcionan distribuciones de probabilidad sobre las acciones de los otros agentes, las cuales se ingresan en los nodos de probabilidad correspondientes encontrados en el nodo del modelo del nivel 1 I-ID. El mapeo de los nodos de decisión de los modelos de nivel 0 a los nodos de probabilidad se realiza de manera que las acciones con el mayor valor en el nodo de decisión se les asignan probabilidades uniformes en el nodo de probabilidad, mientras que el resto se les asigna probabilidad cero. Dadas las distribuciones sobre las acciones dentro de los diferentes nodos de probabilidad (uno para cada modelo del otro agente), el I-ID de nivel 1 se transforma como se muestra en la Fig. 1(c). Durante la transformación, la tabla de probabilidad condicional (CPT) del nodo, Aj, se llena de tal manera que el nodo asume la distribución de cada uno de los nodos de probabilidad dependiendo del valor del nodo, Mod[Mj]. Como mencionamos anteriormente, los valores del nodo Mod[Mj] denotan los diferentes modelos del otro agente, y su distribución es la creencia del agente sobre los modelos de j condicionados al estado físico. El nivel 1 I-ID transformado es un ID tradicional que puede ser resuelto como us816 The Sixth Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) (a) (b) Figura 3: (a) Un I-DID genérico de dos niveles de tiempo para el agente i en un entorno con otro agente j. Observa el enlace de actualización del modelo punteado que denota la actualización de los modelos de j y la distribución de los modelos a lo largo del tiempo. (b) La semántica del enlace de actualización del modelo utilizando el método estándar de maximización de utilidad esperada [18]. Este procedimiento se lleva a cabo hasta el nivel l I-ID cuya solución proporciona el conjunto no vacío de acciones óptimas que el agente debe realizar dada su creencia. Ten en cuenta que, al igual que los IDs, los I-IDs son adecuados para la toma de decisiones en línea cuando se conoce la creencia actual de los agentes. 4. Diagramas de influencia dinámica interactivos (I-DIDs) Los diagramas de influencia dinámica interactivos (I-DIDs) extienden los I-IDs (y NIDs) para permitir la toma de decisiones secuencial a lo largo de varios pasos de tiempo. Así como los DIDs son representaciones gráficas estructuradas de POMDPs, los I-DIDs son los análogos gráficos en línea para I-POMDPs finitamente anidados. Los I-DIDs pueden ser utilizados para optimizar sobre una mirada anticipada finita dadas las creencias iniciales mientras se interactúa con otros agentes, posiblemente similares. 4.1 Sintaxis Representamos un I-DID de dos cortes temporales en la Fig. 3(a). Además de los nodos del modelo y el enlace de política discontinuo, lo que diferencia a un I-DID de un DID es el enlace de actualización del modelo mostrado como una flecha punteada en la Fig. 3(a). Explicamos la semántica del nodo del modelo y el enlace de la política en la sección anterior; a continuación, describimos las actualizaciones del modelo. La actualización del nodo del modelo con el tiempo implica dos pasos: primero, dados los modelos en el tiempo t, identificamos el conjunto actualizado de modelos que residen en el nodo del modelo en el tiempo t + 1. Recuerda de la Sección 2 que el modelo intencional de un agente incluye sus creencias. Debido a que los agentes actúan y reciben observaciones, sus modelos se actualizan para reflejar sus creencias cambiadas. Dado que el conjunto de acciones óptimas para un modelo podría incluir todas las acciones, y el agente podría recibir cualquiera de las |Ωj| posibles observaciones, el conjunto actualizado en el paso de tiempo t + 1 tendrá como máximo |Mt j,l−1||Aj||Ωj| modelos. Aquí, |Mt j,l−1| es el número de modelos en el paso de tiempo t, |Aj| y |Ωj| son los espacios más grandes de acciones y observaciones respectivamente, entre todos los modelos. Segundo, calculamos la nueva distribución sobre los modelos actualizados dados la distribución original y la probabilidad de que el agente realice la acción y reciba la observación que llevó al modelo actualizado. Estos pasos son parte de la actualización de creencias del agente formalizada utilizando la Ecuación 1. En la Fig. 3(b), mostramos cómo se implementa el enlace de actualización del modelo punteado en el I-DID. Si cada uno de los dos modelos de nivel l − 1 atribuidos a j en el paso de tiempo t resulta en una acción, y j podría hacer una de dos posibles observaciones, entonces el nodo del modelo en el paso de tiempo t + 1 contiene cuatro modelos actualizados (mt+1,1 j,l−1 ,mt+1,2 j,l−1 , mt+1,3 j,l−1 , y mt+1,4 j,l−1 ). Estos modelos difieren en sus creencias iniciales, cada una de las cuales es el resultado de j actualizar sus creencias debido a su acción y una posible observación. Los nodos de decisión en cada uno de los I-DIDs o DIDs que representan los modelos de nivel inferior se asignan a la Figura 4 correspondiente: I-DID transformado con los nodos del modelo y el enlace de actualización del modelo reemplazados por los nodos de probabilidad y las relaciones (en negrita) de los nodos de probabilidad, como se mencionó anteriormente. A continuación, describimos cómo se calcula la distribución sobre el conjunto actualizado de modelos (la distribución sobre el nodo de probabilidad Mod[Mt+1 j ] en Mt+1 j,l−1). La probabilidad de que el modelo actualizado js sea, digamos mt+1,1 j,l−1, depende de la probabilidad de que j realice la acción y reciba la observación que condujo a este modelo, y de la distribución previa sobre los modelos en el paso de tiempo t. Dado que el nodo de probabilidad At j asume la distribución de cada uno de los nodos de acción basados en el valor de Mod[Mt j], la probabilidad de la acción está dada por este nodo de probabilidad. Para obtener la probabilidad de una observación posible js, introducimos el nodo de probabilidad Oj, que dependiendo del valor de Mod[Mt j], asume la distribución del nodo de observación en el modelo de nivel inferior denominado Mod[Mt j]. Dado que la probabilidad de las observaciones js depende del estado físico y de las acciones conjuntas de ambos agentes, el nodo Oj está vinculado con St+1, At j y At i. De manera análoga a At j, la tabla de probabilidad condicional de Oj también es un multiplexor modulado por Mod[Mt j]. Finalmente, la distribución de los modelos previos en el tiempo t se obtiene a partir del nodo de probabilidad, Mod[Mt j ] en Mt j,l−1. Por consiguiente, los nodos de probabilidad, Mod[Mt j], At j y Oj, forman los padres de Mod[Mt+1 j] en Mt+1 j,l−1. Ten en cuenta que el enlace de actualización del modelo puede ser reemplazado por los enlaces de dependencia entre los nodos de probabilidad que constituyen los nodos del modelo en las dos etapas temporales. En la Fig. 4 mostramos los dos cortes temporales I-DID con los nodos del modelo reemplazados por los nodos de probabilidad y las relaciones entre ellos. Los nodos de probabilidad y los enlaces de dependencia que no están en negrita son estándar, generalmente encontrados en DIDs. La expansión del I-DID a lo largo de más pasos de tiempo requiere la repetición de los dos pasos de actualizar el conjunto de modelos que forman el 2. Nota que Oj representa la observación j en el tiempo t + 1. El Sexto Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 817 valores del nodo del modelo y añadiendo las relaciones entre los nodos de probabilidad, tantas veces como enlaces de actualización del modelo haya. Observamos que el conjunto posible de modelos del otro agente j crece exponencialmente con el número de pasos de tiempo. Por ejemplo, después de T pasos, puede haber como máximo |Mt=1 j,l−1|(|Aj||Ωj|)T −1 modelos candidatos residiendo en el nodo del modelo. 4.2 Solución Análoga a los I-ID, la solución a un I-DID de nivel l para el agente i expandido a lo largo de T pasos de tiempo puede llevarse a cabo de forma recursiva. Con el propósito de ilustración, dejemos l=1 y T=2. El método de solución utiliza la técnica estándar de anticipación, proyectando las secuencias de acciones y observaciones de los agentes hacia adelante desde el estado de creencia actual [17], y encontrando las posibles creencias que podría tener en el siguiente paso temporal. Dado que el agente i también tiene una creencia sobre los modelos de j, la búsqueda anticipada incluye descubrir los posibles modelos que j podría tener en el futuro. Por consiguiente, cada uno de los modelos subintencionales o de nivel 0 de js (representados utilizando un DID estándar) en el primer paso de tiempo debe resolverse para obtener su conjunto óptimo de acciones. Estas acciones se combinan con el conjunto de observaciones posibles que j podría hacer en ese modelo, lo que resulta en un conjunto actualizado de modelos candidatos (que incluyen las creencias actualizadas) que podrían describir el comportamiento de j. Las creencias sobre este conjunto actualizado de modelos candidatos se calculan utilizando los métodos estándar de inferencia que utilizan las relaciones de dependencia entre los nodos del modelo, como se muestra en la Figura 3(b). Observamos la naturaleza recursiva de esta solución: al resolver el agente de nivel 1 I-DID, los DIDs de nivel 0 deben ser resueltos. Si el anidamiento de los modelos es más profundo, todos los modelos en todos los niveles, comenzando desde el nivel 0, se resuelven de manera ascendente. Breve descripción del algoritmo recursivo para resolver el agente es el Algoritmo para resolver I-DID. Entrada: nivel l ≥ 1 I-ID o nivel 0 ID, T Fase de Expansión 1. Para t desde 1 hasta T − 1, hacer 2. Si l ≥ 1, entonces llenar Mt+1 j,l−1 3. Para cada mt j en el rango (Mt j,l−1) hacer 4. Llame recursivamente al algoritmo con el l - 1 I-ID (o ID) que representa mt j y el horizonte, T - t + 1 5. Mapea el nodo de decisión del I-ID (o ID) resuelto, OPT(mt j), a un nodo de probabilidad Aj 6. Para cada aj en OPT(mt j) hacer 7. Para cada oj en Oj (parte de mt j) hacer 8. Actualizar la creencia js, bt+1 j ← SE(bt j, aj, oj) 9. mt+1 j ← Nuevo I-ID (o ID) con bt+1 j como la creencia inicial 10. Rango(Mt+1 j,l−1) ∪ ← {mt+1 j } 11. Añadir el nodo del modelo, Mt+1 j,l−1, y los enlaces de dependencia entre Mt j,l−1 y Mt+1 j,l−1 (mostrados en la Fig. 3(b)) 12. Agregue los nodos de probabilidad, decisión y utilidad para la franja de tiempo t + 1 y los enlaces de dependencia entre ellos 13. Establezca las CPTs para cada nodo de probabilidad y nodo de utilidad de la Fase de Mirada Adelante 14. Aplica el método estándar de anticipación y respaldo para resolver la Figura 5 expandida de I-DID: Algoritmo para resolver un nivel l ≥ 0 I-DID. Nivel l I-DID expandido durante T pasos de tiempo con otro agente j en la Figura 5. Adoptamos un enfoque de dos fases: Dado un I-ID de nivel l (descrito previamente en la Sección 3) con todos los modelos de niveles inferiores representados también como I-IDs o IDs (si es nivel 0), el primer paso es expandir el I-ID de nivel l a lo largo de T pasos de tiempo añadiendo los enlaces de dependencia y las tablas de probabilidad condicional para cada nodo. Nos enfocamos especialmente en establecer y poblar los nodos del modelo (líneas 3-11). Ten en cuenta que Range(·) devuelve los valores (modelos de nivel inferior) de la variable aleatoria dada como entrada (nodo del modelo). En la segunda fase, utilizamos una técnica estándar de anticipación proyectando las secuencias de acción y observación durante T pasos de tiempo en el futuro, y respaldando los valores de utilidad de las creencias alcanzables. Similar a los I-IDs, los I-DIDs se reducen a DIDs en ausencia de otros agentes. Como mencionamos anteriormente, los modelos de nivel 0 son los DIDs tradicionales. Sus soluciones proporcionan distribuciones de probabilidad sobre las acciones del agente modelado en ese nivel a los I-DIDs en el nivel 1. Dadas las distribuciones de probabilidad sobre las acciones de otros agentes, los IDIDs de nivel 1 pueden resolverse como DIDs y proporcionar distribuciones de probabilidad a modelos de niveles superiores. Suponga que el número de modelos considerados en cada nivel está limitado por un número, M. Resolver un I-DID de nivel l es equivalente a resolver O(Ml) DIDs. Para ilustrar la utilidad de los I-DIDs, los aplicamos a tres dominios de problemas. Describimos, en particular, la formulación del I-DID y las prescripciones óptimas obtenidas al resolverlo. 5.1 Seguimiento-Liderazgo en el Problema del Tigre Multiagente Comenzamos nuestras ilustraciones del uso de I-IDs e I-DIDs con una versión ligeramente modificada del problema del tigre multiagente discutido en [9]. El problema tiene dos agentes, cada uno de los cuales puede abrir la puerta derecha (OR), la puerta izquierda (OL) o escuchar (L). Además de escuchar gruñidos (desde la izquierda (GL) o desde la derecha (GR)) cuando escuchan, los agentes también escuchan chirridos (desde la izquierda (CL), desde la derecha (CR) o sin chirridos (S)), que indican ruidosamente que los otros agentes están abriendo una de las puertas. Cuando se abre cualquier puerta, el tigre persiste en su ubicación original con una probabilidad del 95%. El agente i escucha gruñidos con una fiabilidad del 65% y crujidos con una fiabilidad del 95%. El agente J, por otro lado, escucha gruñidos con una fiabilidad del 95%. Por lo tanto, la configuración es tal que el agente i escucha la apertura de puertas del agente j de manera más confiable que los rugidos de los tigres. Esto sugiere que podría usar acciones de JavaScript como indicación de la ubicación del tigre, como discutimos a continuación. Las preferencias de cada agente son como en el juego de un solo agente discutido en [13]. Las funciones de transición, observación y recompensa se muestran en [16]. Un buen indicador de la utilidad de los métodos normativos para la toma de decisiones como los I-DIDs es la aparición de comportamientos sociales realistas en sus prescripciones. En entornos del persistente problema del tigre multiagente que reflejan situaciones del mundo real, demostramos la seguidismo entre los agentes y, como se muestra en [15], el engaño entre agentes que creen que están en una relación de tipo seguidor-líder. En particular, analizamos las condiciones situacionales y epistemológicas suficientes para su surgimiento. El comportamiento de seguidores, por ejemplo, resulta del agente conociendo sus propias debilidades, evaluando las fortalezas, preferencias y posibles comportamientos del otro, y dándose cuenta de que es mejor para él seguir las acciones de los demás para maximizar sus ganancias. Consideremos una configuración particular del problema del tigre en la que el agente i cree que las preferencias de j están alineadas con las suyas: ambos solo quieren obtener el oro, y la audición de j es más confiable en comparación consigo mismo. Como ejemplo, supongamos que j, al escuchar, puede discernir la ubicación del tigre el 95% de las veces en comparación con su precisión del 65%. Además, el agente i no tiene ninguna información inicial sobre la ubicación de los tigres. En otras palabras, la creencia anidada de un solo nivel, bi,1, asigna 0.5 a cada una de las dos ubicaciones del tigre. Además, considera dos modelos de j, que difieren en los niveles iniciales de creencias planas de j. Esto se representa en el nivel 1 I-ID mostrado en la Fig. 6(a). Según un modelo, j asigna una probabilidad de 0.9 a que el tigre esté detrás de la puerta izquierda, mientras que el otro 818 The Sixth Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) Figura 6: (a) Nivel 1 I-ID del agente i, (b) dos IDs de nivel 0 del agente j cuyos nodos de decisión están mapeados a los nodos de probabilidad, A1 j , A2 j , en (a). El modelo asigna 0.1 a esa ubicación (ver Fig. 6(b)). El agente i está indeciso sobre estos dos modelos de j. Si variamos la capacidad auditiva, y resolvemos el nivel 1 correspondiente I-ID expandido en tres pasos de tiempo, obtenemos las políticas de comportamiento normativas mostradas en la Figura 7 que exhiben comportamiento de seguidores. Si la probabilidad de escuchar correctamente los gruñidos es de 0.65, entonces, como se muestra en la política en la Fig. 7(a), i comienza a seguir condicionalmente las acciones de j: i abre la misma puerta que j abrió anteriormente si su evaluación propia de la ubicación de los tigres confirma la elección de j. Si i pierde la capacidad de interpretar correctamente los gruñidos por completo, sigue ciegamente a j y abre la misma puerta que j abrió anteriormente (Fig. 7(b)). Figura 7: Emergencia de (a) seguidores condicionales, y (b) seguidores ciegos en el problema del tigre. Los comportamientos de interés están en negrita. * es un comodín y representa cualquiera de las observaciones. Observamos que un solo nivel de anidación de creencias, creencias sobre los modelos de los demás, fue suficiente para que surgiera el seguidismo en el problema del tigre. Sin embargo, los requisitos epistemológicos para el surgimiento del liderazgo son más complejos. Para que un agente, digamos j, surja como líder, primero debe surgir el seguidismo en el otro agente i. Como mencionamos anteriormente, si i está seguro de que sus preferencias son idénticas a las de j, y cree que j tiene un mejor sentido del oído, i seguirá las acciones de j con el tiempo. El agente j emerge como líder si cree que lo seguiré, lo que implica que la creencia de j debe estar anidada dos niveles de profundidad para permitirle reconocer su papel de liderazgo. Darse cuenta de que lo seguiré presenta a J una oportunidad para influir en sus acciones en beneficio del bien colectivo o de su interés propio solamente. Por ejemplo, en el problema del tigre, consideremos una situación en la que si tanto i como j abren la puerta correcta, entonces cada uno recibe un pago de 20 que es el doble del original. Si solo j selecciona la puerta correcta, obtiene la recompensa de 10. Por otro lado, si ambos agentes eligen la puerta incorrecta, sus penalizaciones se reducen a la mitad. En este entorno, es tanto en el mejor interés de j como en el beneficio colectivo que utilice su experiencia para seleccionar la puerta correcta, y así ser un buen líder. Sin embargo, considera un problema ligeramente diferente en el que j se beneficia de la pérdida de i y es penalizado si i se beneficia. Específicamente, dejemos que la ganancia se reste de js, indicando que j es antagonista hacia i: si j elige la puerta correcta y i la incorrecta, entonces la pérdida de 100 se convierte en la ganancia de js. El agente J cree que yo incorrectamente pienso que las preferencias de J son aquellas que promueven el bien colectivo y que comienza creyendo con un 99% de confianza dónde está el tigre. Dado que creo que mis preferencias son similares a las de j, y que j comienza creyendo casi con certeza que una de las dos ubicaciones es la correcta (dos modelos de nivel 0 de j), comenzaré siguiendo las acciones de j. Mostramos la política normativa para resolver su I-DID anidado individualmente durante tres pasos de tiempo en la Fig. 8(a). La política demuestra que seguiré ciegamente las acciones de js. Dado que el tigre persiste en su ubicación original con una probabilidad del 0.95, seleccionaré nuevamente la misma puerta. Si j comienza el juego con una probabilidad del 99% de que el tigre esté a la derecha, resolver su I-DID anidado dos niveles profundamente, resulta en la política mostrada en la Fig. 8(b). Aunque j está casi seguro de que OL es la acción correcta, comenzará seleccionando OR, seguido de OL. La intención del agente js es engañar a i, a quien cree que seguirá las acciones de js, para así obtener $110 en el segundo paso de tiempo, lo cual es más de lo que j ganaría si fuera honesto. Figura 8: Emergencia del engaño entre agentes en el problema del tigre. Los comportamientos de interés están en negrita. * denota como antes. (a) El agente es una política que demuestra que seguirá ciegamente las acciones de js. (b) Aunque j está casi seguro de que el tigre está a la derecha, comenzará seleccionando OR, seguido de OL, para engañar a i. 5.2 Altruismo y reciprocidad en el problema del bien público El problema del bien público (PG) [7], consiste en un grupo de M agentes, cada uno de los cuales debe contribuir con algún recurso a una olla pública o guardarlo para sí mismos. Dado que los recursos aportados al fondo público se comparten entre todos los agentes, son menos valiosos para el agente cuando están en el fondo público. Sin embargo, si todos los agentes eligen contribuir con sus recursos, entonces la recompensa para cada agente es mayor que si nadie contribuye. Dado que un agente recibe su parte del bote público independientemente de si ha contribuido o no, la acción dominante es que cada agente no contribuya y en su lugar se aproveche de las contribuciones de los demás. Sin embargo, los comportamientos de los jugadores humanos en simulaciones empíricas del problema de PG difieren de las predicciones normativas. Los experimentos revelan que muchos jugadores inicialmente contribuyen con una gran cantidad al bote público, y continúan contribuyendo cuando se juega el problema del PG repetidamente, aunque en cantidades decrecientes [4]. Muchos de estos experimentos [5] informan que un pequeño grupo central de jugadores contribuye persistentemente al bote público incluso cuando todos los demás están desertando. Estos experimentos también revelan que los jugadores que contribuyen persistentemente tienen preferencias altruistas o recíprocas que coinciden con la cooperación esperada de los demás. Para simplificar, asumimos que el juego se juega entre M = 2 agentes, i y j. Que cada agente esté inicialmente dotado con una cantidad de recursos XT. Mientras que la formulación clásica del juego PG permite que cada agente contribuya con cualquier cantidad de recursos (≤ XT) al bote público, simplificamos el espacio de acciones al permitir dos acciones posibles. Cada agente puede elegir contribuir (C) una cantidad fija de recursos, o no contribuir. La última acción es deThe Sixth Intl. La Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 819 fue señalada como defectuosa (D). Suponemos que las acciones no son observables por otros. El valor de los recursos en la bolsa pública se descuenta por ci para cada agente i, donde ci es el retorno privado marginal. Suponemos que ci < 1 para que el agente no se beneficie lo suficiente como para contribuir al bote público en beneficio propio. Simultáneamente, ciM > 1, lo que hace que la contribución colectiva sea óptima de Pareto. i/j C D C 2ciXT , 2cjXT ciXT − cp, XT + cjXT − P D XT + ciXT − P, cjXT − cp XT , XT Tabla 1: El juego PG de un solo disparo con castigo. Para fomentar las contribuciones, los agentes contribuyentes castigan a los que no colaboran pero incurren en un pequeño costo por administrar el castigo. Sea P la sanción impuesta al agente que se desvía y cp el costo no nulo de castigar al agente contribuyente. Para simplificar, asumimos que el costo de castigar es el mismo para ambos agentes. El juego PG de un solo disparo con castigo se muestra en la Tabla 1. Si ci = cj, cp > 0, y si P > XT − ciXT, entonces la deserción ya no es una acción dominante. Si P < XT − ciXT, entonces la deserción es la acción dominante para ambos. Si P = XT − ciXT, entonces el juego no es resoluble por dominancia. Figura 9: (a) Nivel 1 I-ID del agente i, (b) IDs de nivel 0 del agente j con nodos de decisión mapeados a los nodos de probabilidad, A1 j y A2 j, en (a). Formulamos una versión secuencial del problema PG con castigo desde la perspectiva del agente i. Aunque en el juego de PG repetido, la cantidad en la olla pública se revela a todos los agentes después de cada ronda de acciones, asumimos en nuestra formulación que está oculta para los agentes. Cada agente puede contribuir con una cantidad fija, xc, o desertar. Un agente al realizar una acción recibe una observación de abundante (PY) o escasa (MR) simbolizando el estado de la olla pública. Ten en cuenta que las observaciones también son indirectamente indicativas de las acciones del agente js porque el estado del bote público es influenciado por ellas. La cantidad de recursos en el agente es una olla privada, es perfectamente observable para i. Los pagos son análogos a la Tabla 1. Tomando prestado de las investigaciones empíricas del problema PG [5], construimos IDs de nivel 0 para j que modelan tipos altruistas y no altruistas (Fig. 9(b)). Específicamente, nuestro agente altruista tiene un alto retorno privado marginal (cj está cerca de 1) y no castiga a los demás que incumplen. Que xc = 1 y el agente de nivel 0 sea castigado la mitad de las veces que se desvía. Con una acción restante, ambos tipos de agentes eligen contribuir para evitar ser castigados. Con dos acciones por delante, el tipo altruista elige contribuir, mientras que el otro defecta. Esto se debe a que cj para el tipo altruista es cercano a 1, por lo tanto, el castigo esperado, 0.5P > (1 − cj), que el tipo altruista evita. Debido a que cj para el tipo no altruista es menor, prefiere no contribuir. Con tres pasos por delante, el agente altruista contribuye para evitar el castigo (0.5P > 2(1 − cj)), mientras que el tipo no altruista se desentiende. Para más de tres pasos, mientras el agente altruista continúa contribuyendo al bote público dependiendo de qué tan cercano esté su retorno privado marginal a 1, el tipo no altruista prescribe la deserción. Analizamos las decisiones de un agente altruista i modelado usando un nivel 1 I-DID expandido en 3 pasos de tiempo. i atribuye los dos modelos de nivel 0, mencionados anteriormente, a j (ver Fig. 9). Si creo con una probabilidad de 1 que j es altruista, elijo contribuir en cada uno de los tres pasos. Este comportamiento persiste cuando i no está al tanto de si j es altruista (Fig. 10(a)), y cuando i asigna una alta probabilidad a que j sea del tipo no altruista. Sin embargo, cuando i cree con una probabilidad de 1 que j no es altruista y por lo tanto seguramente va a traicionar, i elige traicionar para evitar ser castigado y porque su beneficio privado marginal es menor que 1. Estos resultados demuestran que el comportamiento de nuestro tipo altruista se asemeja al encontrado experimentalmente. El agente de nivel 1 no altruista elige desertar independientemente de lo altruista que crea que sea el otro agente. Analizamos el comportamiento de un tipo de agente recíproco que se ajusta a la cooperación o defección esperada. El retorno privado marginal del tipo recíproco es similar al del tipo no altruista, sin embargo, obtiene una mayor recompensa cuando su acción es similar a la del otro. Consideramos el caso en el que el agente recíproco i no está seguro de si j es altruista y cree que es probable que el bote público esté medio lleno. Para esta creencia previa, yo elijo defectar. Al recibir una observación de abundancia, decide contribuir, mientras que una observación de escasez lo hace defectar (Fig. 10(b)). Esto se debe a que una observación de abundancia indica que es probable que la olla esté más llena que a la mitad, lo cual resulta de la acción de js para contribuir. Por lo tanto, entre los dos modelos atribuidos a j, es probable que su tipo sea altruista, lo que hace probable que j contribuya nuevamente en el próximo paso de tiempo. El agente i, por lo tanto, elige contribuir para corresponder la acción de js. Un razonamiento análogo lleva a uno a defectar cuando observa una olla escasa. Con una acción por hacer, creo que si j contribuye, también elegiré contribuir para evitar castigo independientemente de sus observaciones. Figura 10: (a) Un agente altruista de nivel 1 siempre contribuye. (b) Un agente recíproco i comienza defraudando y luego elige contribuir o defraudar basándose en su observación de abundancia (indicando que j es probablemente altruista) o escasez (j no es altruista). 5.3 Estrategias en el póker de dos jugadores El póker es un popular juego de cartas de suma cero que ha recibido mucha atención en la comunidad de investigación de IA como un banco de pruebas [2]. El póker se juega entre M ≥ 2 jugadores, en el que cada jugador recibe una mano de cartas de una baraja. Si bien existen varias variantes de póker con diferentes niveles de complejidad, consideramos una versión simple en la que cada jugador tiene tres turnos durante los cuales el jugador puede intercambiar una carta (E), mantener la mano existente (K), retirarse (F) y abandonar el juego, o apostar (C), lo que requiere que todos los jugadores muestren sus manos. Para mantener las cosas simples, dejemos que M = 2, y que cada jugador reciba una mano compuesta por una sola carta sacada del mismo palo. Por lo tanto, durante un enfrentamiento, el jugador que tenga la carta numéricamente más alta (el 2 es el más bajo, el as es el más alto) gana el bote. Durante un intercambio de cartas, la carta descartada se coloca ya sea en la pila L, indicando al otro agente que era una carta de número bajo menor que 8, o en la 820 The Sixth Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) H, indicando que la carta tenía un rango mayor o igual a 8. Ten en cuenta que, por ejemplo, si se descarta una carta de número bajo, la probabilidad de recibir una carta baja a cambio se reduce. Mostramos el nivel 1 I-ID para el Poker simplificado de dos jugadores en la Fig. 11. Consideramos dos modelos (tipos de personalidad) del agente j. El tipo conservador cree que es probable que su oponente tenga una carta de alto número en su mano. Por otro lado, el agente agresivo j cree con alta probabilidad que su oponente tiene una carta de número más bajo. Por lo tanto, los dos tipos difieren en sus creencias sobre la mano de sus oponentes. En ambos estos modelos de nivel 0, se asume que el oponente realiza sus acciones siguiendo una distribución fija y uniforme. Con tres acciones restantes, independientemente de su mano (a menos que sea un as), el agente agresivo elige intercambiar su carta, con la intención de mejorar su mano actual. Esto se debe a que cree que el otro tiene una carta baja, lo que mejora sus posibilidades de obtener una carta alta durante el intercambio. El agente conservador elige quedarse con su carta, sin importar su mano, porque considera que sus posibilidades de obtener una carta alta son escasas, ya que cree que su oponente tiene una. Figura 11: (a) Nivel 1 I-ID del agente i. La observación revela información sobre la mano de js del paso de tiempo anterior, (b) IDs de nivel 0 del agente j cuyos nodos de decisión están mapeados a los nodos de probabilidad, A1 j , A2 j , en (a). La política de un agente de nivel 1 i que cree que cada carta, excepto la suya, tiene la misma probabilidad de estar en su mano (tipo de personalidad neutral) y j podría ser de tipo agresivo o conservador, se muestra en la Figura 12. Su mano contiene la carta numerada 8. El agente comienza por mantener su carta. Al ver que j no intercambió una carta (N), i cree con probabilidad 1 que j es conservador y, por lo tanto, mantendrá sus cartas. i responde manteniendo su carta o intercambiándola porque j tiene igual probabilidad de tener una carta más baja o más alta. Si observo que j descartó su carta en la pila L o H, creo que j es agresivo. Al observar a L, i se da cuenta de que j tenía una carta baja, y es probable que tenga una carta alta después de su intercambio. Dado que la probabilidad de recibir una carta baja es alta en este momento, i decide quedarse con su carta. Al observar la carta H, creyendo que la probabilidad de recibir una carta de número alto es alta, i decide intercambiar su carta. En el paso final, i decide llamar independientemente de su historial de observaciones porque su creencia de que j tiene una carta más alta no es lo suficientemente alta como para concluir que es mejor retirarse y renunciar al pago. Esto se debe en parte al hecho de que una observación de, digamos, L, restablece las creencias del agente en el paso de tiempo anterior sobre las cartas de números bajos solamente. 6. DISCUSIÓN Mostramos cómo los DIDs pueden ser extendidos a los I-DIDs que permiten la toma de decisiones secuenciales en línea en entornos multiagentes inciertos. Nuestra representación gráfica de I-DIDs mejora la Figura 12 anterior: Un agente de nivel 1 es una política de tres pasos en el problema de Poker. i comienza creyendo que j tiene la misma probabilidad de ser agresivo o conservador y podría tener cualquier carta en su mano con igual probabilidad. Trabaja significativamente al ser más transparente, semánticamente claro y capaz de ser resuelto utilizando algoritmos estándar que apuntan a los DIDs. Los I-DIDs extienden los NIDs para permitir la toma de decisiones secuenciales a lo largo de múltiples pasos de tiempo en presencia de otros agentes que interactúan. Los I-DIDs pueden ser vistos como representaciones gráficas concisas para IPOMDPs que proporcionan una forma de explotar la estructura del problema y llevar a cabo la toma de decisiones en línea a medida que el agente actúa y observa dadas sus creencias previas. Actualmente estamos investigando formas de resolver I-DIDs aproximadamente con límites demostrables en la calidad de la solución. Agradecimiento: Agradecemos a Piotr Gmytrasiewicz por algunas discusiones útiles relacionadas con este trabajo. El primer autor desea agradecer el apoyo de una beca UGARF. 7. REFERENCIAS [1] R. J. Aumann. Epistemología interactiva i: Conocimiento. Revista Internacional de Teoría de Juegos, 28:263-300, 1999. [2] D. Billings, A. Davidson, J. Schaeffer y D. Szafron. El desafío del póker. AIJ, 2001. [3] A. Brandenburger y E. Dekel. Jerarquías de creencias y conocimiento común. Revista de Teoría Económica, 59:189-198, 1993. [4] C. Camerer. Teoría de juegos conductual: Experimentos en interacción estratégica. Princeton University Press, 2003. [5] E. Fehr y S. Gachter. Cooperación y castigo en experimentos de bienes públicos. Revista Económica Americana, 90(4):980-994, 2000. [6] D. Fudenberg y D. K. Levine. La Teoría del Aprendizaje en los Juegos. MIT Press, 1998. [7] D. Fudenberg y J. Tirole. Teoría de juegos. MIT Press, 1991. [8] Y. Gal y A. Pfeffer. Un lenguaje para modelar los procesos de toma de decisiones de agentes en juegos. En AAMAS, 2003. [9] P. Gmytrasiewicz y P. Doshi. Un marco para la planificación secuencial en entornos multiagentes. JAIR, 24:49-79, 2005. [10] P. Gmytrasiewicz y E. Durfee. Coordinación racional en entornos multiagente. JAAMAS, 3(4):319-350, 2000. [11] J. C. Harsanyi. Juegos con información incompleta jugados por jugadores bayesianos. Ciencia de la Gestión, 14(3):159-182, 1967. [12] R. A. Howard y J. E. Matheson. Diagramas de influencia. En R. A. Howard y J. E. Matheson, editores, Los Principios y Aplicaciones del Análisis de Decisiones. Grupo de Decisiones Estratégicas, Menlo Park, CA 94025, 1984. [13] L. Kaelbling, M. Littman y A. Cassandra. Planificación y actuación en dominios estocásticos parcialmente observables. Revista de Inteligencia Artificial, 2, 1998. [14] D. Koller y B. Milch. Diagramas de influencia multiagente para representar y resolver juegos. En IJCAI, páginas 1027-1034, 2001. [15] K. Polich y P. Gmytrasiewicz. Diagramas de influencia interactivos y dinámicos. En el taller GTDT, AAMAS, 2006. [16] B. Rathnas, P. Doshi y P. J. Gmytrasiewicz. Soluciones exactas para POMDP interactivos utilizando equivalencia conductual. En la Conferencia de Agentes Autónomos y Sistemas Multiagente (AAMAS), 2006. [17] S. Russell y P. Norvig. Inteligencia Artificial: Un Enfoque Moderno (Segunda Edición). Prentice Hall, 2003. [18] R. D. Shachter. \n\nPrentice Hall, 2003. [18] R. D. Shachter. Evaluando diagramas de influencia. Investigación de Operaciones, 34(6):871-882, 1986. [19] D. Suryadi y P. Gmytrasiewicz. Aprendiendo modelos de otros agentes utilizando diagramas de influencia. En UM, 1999. El Sexto Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 821 ",
            "candidates": [],
            "error": [
                []
            ]
        }
    }
}