{
    "id": "H-25",
    "original_text": "Term Feedback for Information Retrieval with Language Models Bin Tan† , Atulya Velivelli‡ , Hui Fang† , ChengXiang Zhai† Dept. of Computer Science† , Dept. of Electrical and Computer Engineering‡ University of Illinois at Urbana-Champaign bintan@cs.uiuc.edu, velivell@ifp.uiuc.edu, hfang@cs.uiuc.edu, czhai@cs.uiuc.edu ABSTRACT In this paper we study term-based feedback for information retrieval in the language modeling approach. With term feedback a user directly judges the relevance of individual terms without interaction with feedback documents, taking full control of the query expansion process. We propose a cluster-based method for selecting terms to present to the user for judgment, as well as effective algorithms for constructing refined query language models from user term feedback. Our algorithms are shown to bring significant improvement in retrieval accuracy over a non-feedback baseline, and achieve comparable performance to relevance feedback. They are helpful even when there are no relevant documents in the top. Categories and Subject Descriptors H.3.3 [Information Search and Retrieval]: Retrieval models General Terms Algorithms 1. INTRODUCTION In the language modeling approach to information retrieval, feedback is often modeled as estimating an improved query model or relevance model based on a set of feedback documents [25, 13]. This is in line with the traditional way of doing relevance feedback - presenting a user with documents/passages for relevance judgment and then extracting terms from the judged documents or passages to expand the initial query. It is an indirect way of seeking users assistance for query model construction, in the sense that the refined query model (based on terms) is learned through feedback documents/passages, which are high-level structures of terms. It has the disadvantage that irrelevant terms, which occur along with relevant ones in the judged content, may be erroneously used for query expansion, causing undesired effects. For example, for the TREC query Hubble telescope achievements, when a relevant document talks more about the telescopes repair than its discoveries, irrelevant terms such as spacewalk can be added into the modified query. We can consider a more direct way to involve a user in query model improvement, without an intermediary step of document feedback that can introduce noise. The idea is to present a (reasonable) number of individual terms to the user and ask him/her to judge the relevance of each term or directly specify their probabilities in the query model. This strategy has been discussed in [15], but to our knowledge, it has not been seriously studied in existing language modeling literature. Compared to traditional relevance feedback, this term-based approach to interactive query model refinement has several advantages. First, the user has better control of the final query model through direct manipulation of terms: he/she can dictate which terms are relevant, irrelevant, and possibly, to what degree. This avoids the risk of bringing unwanted terms into the query model, although sometimes the user introduces low-quality terms. Second, because a term takes less time to judge than a documents full text or summary, and as few as around 20 presented terms can bring significant improvement in retrieval performance (as we will show later), term feedback makes it faster to gather user feedback. This is especially helpful for interactive adhoc search. Third, sometimes there are no relevant documents in the top N of the initially retrieved results if the topic is hard. This is often true when N is constrained to be small, which arises from the fact that the user is unwilling to judge too many documents. In this case, relevance feedback is useless, as no relevant document can be leveraged on, but term feedback is still often helpful, by allowing relevant terms to be picked from irrelevant documents. During our participation in the TREC 2005 HARD Track and continued study afterward, we explored how to exploit term feedback from the user to construct improved query models for information retrieval in the language modeling approach. We identified two key subtasks of term-based feedback, i.e., pre-feedback presentation term selection and post-feedback query model construction, with effective algorithms developed for both. We imposed a secondary cluster structure on terms and found that a cluster view sheds additional insight into the users information need, and provides a good way of utilizing term feedback. Through experiments we found that term feedback improves significantly over the nonfeedback baseline, even though the user often makes mistakes in relevance judgment. Among our algorithms, the one with best retrieval performance is TCFB, the combination of TFB, the direct term feedback algorithm, and CFB, the cluster-based feedback algorithm. We also varied the number of feedback terms and observed reasonable improvement even at low numbers. Finally, by comparing term feedback with document-level feedback, we found it to be a viable alternative to the latter with competitive retrieval performance. The rest of the paper is organized as follows. Section 2 discusses some related work. Section 4 outlines our general approach to term feedback. We present our method for presentation term selection in Section 3 and algorithms for query model construction in Section 5. The experiment results are given in Section 6. Section 7 concludes this paper. 2. RELATED WORK Relevance feedback[17, 19] has long been recognized as an effective method for improving retrieval performance. Normally, the top N documents retrieved using the original query are presented to the user for judgment, after which terms are extracted from the judged relevant documents, weighted by their potential of attracting more relevant documents, and added into the query model. The expanded query usually represents the users information need better than the original one, which is often just a short keyword query. A second iteration of retrieval using this modified query usually produces significant increase in retrieval accuracy. In cases where true relevance judgment is unavailable and all top N documents are assumed to be relevant, it is called blind or pseudo feedback[5, 16] and usually still brings performance improvement. Because document is a large text unit, when it is used for relevance feedback many irrelevant terms can be introduced into the feedback process. To overcome this, passage feedback is proposed and shown to improve feedback performance[1, 23]. A more direct solution is to ask the user for their relevance judgment of feedback terms. For example, in some relevance feedback systems such as [12], there is an interaction step that allows the user to add or remove expansion terms after they are automatically extracted from relevant documents. This is categorized as interactive query expansion, where the original query is augmented with user-provided terms, which can come from direct user input (free-form text or keywords)[22, 7, 10] or user selection of system-suggested terms (using thesauri[6, 22] or extracted from feedback documents[6, 22, 12, 4, 7]). In many cases term relevance feedback has been found to effectively improve retrieval performance[6, 22, 12, 4, 10]. For example, the study in [12] shows that the user prefers to have explicit knowledge and direct control of which terms are used for query expansion, and the penetrable interface that provides this freedom is shown to perform better than other interfaces. However, in some other cases there is no significant benefit[3, 14], even if the user likes interacting with expansion terms. In a simulated study carried out in [18], the author compares the retrieval performance of interactive query expansion and automatic query expansion with a simulated study, and suggests that the potential benefits of the former can be hard to achieve. The user is found to be not good at identifying useful terms for query expansion, when a simple term presentation interface is unable to provide sufficient semantic context of the feedback terms. Our work differs from the previous ones in two important aspects. First, when we choose terms to present to the user for relevance judgment, we not only consider single-term value (e.g., the relative frequency of a term in the top documents, which can be measured by metrics such as Robertson Selection Value and Simplified Kullback-Leibler Distance as listed in [24]), but also examine the cluster structure of the terms, so as to produce a balanced coverage of the different topic aspects. Second, with the language modelling framework, we allow an elaborate construction of the updated query model, by setting different probabilities for different terms based on whether it is a query term, its significance in the top documents, and its cluster membership. Although techniques for adjusting query term weights exist for vector space models[17] and probablistic relevance models[9], most of the aforementioned works do not use them, choosing to just append feedback terms to the original query (thus using equal weights for them), which can lead to poorer retrieval performance. The combination of the two aspects allows our method to perform much better than the baseline. The usual way for feedback term presentation is just to display the terms in a list. There have been some works on alternative user interfaces. [8] arranges terms in a hierarchy, and [11] compares three different interfaces, including terms + checkboxes, terms + context (sentences) + checkboxes, sentences + input text box. In both studies, however, there is no significant performance difference. In our work we adopt the simplest approach of terms + checkboxes. We focus on term presentation and query model construction from feedback terms, and believe using contexts to improve feedback term quality should be orthogonal to our method. 3. GENERAL APPROACH We follow the language modeling approach, and base our method on the KL-divergence retrieval model proposed in [25]. With this model, the retrieval task involves estimating a query language model θq from a given query, a document language model θd from each document, and calculating their KL-divergence D(θq||θd), which is then used to score the documents. [25] treats relevance feedback as a query model re-estimation problem, i.e., computing an updated query model θq given the original query text and the extra evidence carried by the judged relevant documents. We adopt this view, and cast our task as updating the query model from user term feedback. There are two key subtasks here: First, how to choose the best terms to present to the user for judgment, in order to gather maximal evidence about the users information need. Second, how to compute an updated query model based on this term feedback evidence, so that it captures the users information need and translates into good retrieval performance. 4. PRESENTATION TERM SELECTION Proper selection of terms to be presented to the user for judgment is crucial to the success of term feedback. If the terms are poorly chosen and there are few relevant ones, the user will have a hard time looking for useful terms to help clarify his/her information need. If the relevant terms are plentiful, but all concentrate on a single aspect of the query topic, then we will only be able to get feedback on that aspect and missing others, resulting in a breadth loss in retrieved results. Therefore, it is important to carefully select presentation terms to maximize expected gain from user feedback, i.e., those that can potentially reveal most evidence of the users information need. This is similar to active feedback[21], which suggests that a retrieval system should actively probe the users information need, and in the case of relevance feedback, the feedback documents should be chosen to maximize learning benefits (e.g. diversely so as to increase coverage). In our approach, the top N documents from an initial retrieval using the original query form the source of feedback terms: all terms that appear in them are considered candidates to present to the user. These documents serve as pseudo-feedback, since they provide a much richer context than the original query (usually very short), while the user is not asked to judge their relevance. Due to the latter reason, it is possible to make N quite large (e.g., in our experiments we set N = 60) to increase its coverage of different aspects in the topic. The simplest way of selecting feedback terms is to choose the most frequent M terms from the N documents. This method, however, has two drawbacks. First, a lot of common noisy terms will be selected due to their high frequencies in the document collection, unless a stop-word list is used for filtering. Second, the presentation list will tend to be filled by terms from major aspects of the topic; those from a minor aspect are likely to be missed due to their relatively low frequencies. We solve the above problems by two corresponding measures. First, we introduce a background model θB that is estimated from collection statistics and explains the common terms, so that they are much less likely to appear in the presentation list. Second, the terms are selected from multiple clusters in the pseudo-feedback documents, to ensure sufficient representation of different aspects of the topic. We rely on the mixture multinomial model, which is used for theme discovery in [26]. Specifically, we assume the N documents contain K clusters {Ci| i = 1, 2, · · · K}, each characterized by a multinomial word distribution (also known as unigram language model) θi and corresponding to an aspect of the topic. The documents are regarded as sampled from a mixture of K + 1 components, including the K clusters and the background model: p(w|d) = λBp(w|θB) + (1 − λB) K i=1 πd,ip(w|θi) where w is a word, λB is the mixture weight for the background model θB, and πd,i is the document-specific mixture weight for the i-th cluster model θi. We then estimate the cluster models by maximizing the probability of the pseudo-feedback documents being generated from the multinomial mixture model: log p(D|Λ) = d∈D w∈V c(w; d) log p(w|d) where D = {di| i = 1, 2, · · · N} is the set of the N documents, V is the vocabulary, c(w; d) is ws frequency in d and Λ = {θi| i = 1, 2, · · · K} ∪ {πdij | i = 1, 2, · · · N, j = 1, 2, · · · K} is the set of model parameters to estimate. The cluster models can be efficiently estimated using the Expectation-Maximization (EM) algorithm. For its details, we refer the reader to [26]. Table 1 shows the cluster models for TREC query Transportation tunnel disasters (K = 3). Note that only the middle cluster is relevant. Table 1: Cluster models for topic 363 Transportation tunnel disasters Cluster 1 Cluster 2 Cluster 3 tunnel 0.0768 tunnel 0.0935 tunnel 0.0454 transport 0.0364 fire 0.0295 transport 0.0406 traffic 0.0206 truck 0.0236 toll 0.0166 railwai 0.0186 french 0.0220 amtrak 0.0153 harbor 0.0146 smoke 0.0157 train 0.0129 rail 0.0140 car 0.0154 airport 0.0122 bridg 0.0139 italian 0.0152 turnpik 0.0105 kilomet 0.0136 firefight 0.0144 lui 0.0095 truck 0.0133 blaze 0.0127 jersei 0.0093 construct 0.0131 blanc 0.0121 pass 0.0087 · · · · · · · · · From each of the K estimated clusters, we choose the L = M/K terms with highest probabilities to form a total of M presentation terms. If a term happens to be in top L in multiple clusters, we assign it to the cluster where it has highest probability and let the other clusters take one more term as compensation. We also filter out terms in the original query text because they tend to always be relevant when the query is short. The selected terms are then presented to the user for judgment. A sample (completed) feedback form is shown in Figure 1. In this study we only deal with binary judgment: a presented term is by default unchecked, and a user may check it to indicate relevance. We also do not explicitly exploit negative feedback (i.e., penalizing irrelevant terms), because with binary feedback an unchecked term is not necessarily irrelevant (maybe the user is unsure about its relevance). We could ask the user for finer judgment (e.g., choosing from highly relevant, somewhat relevant, do not know, somewhat irrelevant and highly irrelevant), but binary feedback is more compact, taking less space to display and less user effort to make judgment. 5. ESTIMATING QUERY MODELS FROM TERM FEEDBACK In this section, we present several algorithms for exploiting term feedback. The algorithms take as input the original query q, the clusters {θi} as generated by the theme discovery algorithm, the set of feedback terms T and their relevance judgment R, and outputs an updated query language model θq that makes best use of the feedback evidence to capture the users information need. First we describe our notations: • θq: The original query model, derived from query terms only: p(w|θq) = c(w; q) |q| where c(w; q) is the count of w in q, and |q| = w∈q c(w; q) is the query length. • θq : The updated query model which we need to estimate from term feedback. • θi (i = 1, 2, . . . K): The unigram language model of cluster Ci, as estimated using the theme discovery algorithm. • T = {ti,j} (i = 1 . . . K, j = 1 . . . L): The set of terms presented to the user for judgment. ti,j is the j-th term chosen from cluster Ci. • R = {δw|w ∈ T}: δw is an indicator variable that is 1 if w is judged relevant or 0 otherwise. 5.1 TFB (Direct Term Feedback) This is a straight-forward form of term feedback that does not involve any secondary structure. We give a weight of 1 to terms judged relevant by the user, a weight of μ to query terms, zero weight to other terms, and then apply normalization: p(w|θq ) = δw + μ c(w; q) w ∈T δw + μ|q| where w ∈T δw is the total number of terms that are judged relevant. We call this method TFB (direct Term FeedBack). If we let μ = 1, this approach is equivalent to appending the relevant terms after the original query, which is what standard query expansion (without term reweighting) does. If we set μ > 1, we are putting more emphasis on the query terms than the checked ones. Note that the result model will be more biased toward θq if the original query is long or the user feedback is weak, which makes sense, as we can trust more on the original query in either case. Figure 1: Filled clarification form for Topic 363 363 transportation tunnel disasters Please select all terms that are relevant to the topic. traffic railway harbor rail bridge kilometer construct swiss cross link kong hong river project meter shanghai fire truck french smoke car italian firefights blaze blanc mont victim franc rescue driver chamonix emerge toll amtrak train airport turnpike lui jersey pass rome z center electron road boston speed bu submit 5.2 CFB (Cluster Feedback) Here we exploit the cluster structure that played an important role when we selected the presentation terms. The clusters represent different aspects of the query topic, each of which may or may not be relevant. If we are able to identify the relevant clusters, we can combine them to generate a query model that is good at discovering documents belonging to these clusters (instead of the irrelevant ones). We could ask the user to directly judge the relevance of a cluster after viewing representative terms in that cluster, but this would sometimes be a difficult task for the user, who has to guess the semantics of a cluster via its set of terms, which may not be well connected to one another due to a lack of context. Therefore, we propose to learn cluster feedback indirectly, inferring the relevance of a cluster through the relevance of its feedback terms. Because each cluster has an equal number of terms presented to the user, the simplest measure of a clusters relevance is the number of terms that are judged relevant in it. Intuitively, the more terms are marked relevant in a cluster, the closer the cluster is to the query topic, and the more the cluster should participate in query modification. If we combine the cluster models using weights determined this way and then interpolate with the original query model, we get the following formula for query updating, which we call CFB (Cluster FeedBack): p(w|θq ) = λp(w|θq) + (1 − λ) K i=1 L j=1 δti,j K k=1 L j=1 δtk,j p(w|θi) where L j=1 δti,j is the number of relevant terms in cluster Ci, and K k=1 L j=1 δtk,j is the total number of relevant terms. We note that when there is only one cluster (K = 1), the above formula degenerates to p(w|θq ) = λp(w|θq) + (1 − λ)p(w|θ1) which is merely pseudo-feedback of the form proposed in [25]. 5.3 TCFB (Term-cluster Feedback) TFB and CFB both have their drawbacks. TFB assigns non-zero probabilities to the presented terms that are marked relevant, but completely ignores (a lot more) others, which may be left unchecked due to the users ignorance, or simply not included in the presentation list, but we should be able to infer their relevance from the checked ones. For example, in Figure 1, since as many as 5 terms in the middle cluster (the third and fourth columns) are checked, we should have high confidence in the relevance of other terms in that cluster. CFB remedies TFBs problem by treating the terms in a cluster collectively, so that unchecked/unpresented terms receive weights when presented terms in their clusters are judged as relevant, but it does not distinguish which terms in a cluster are presented or judged. Intuitively, the judged relevant terms should receive larger weights because they are explicitly indicated as relevant by the user. Therefore, we try to combine the two methods, hoping to get the best out of both. We do this by interpolating the TFB model with the CFB model, and call it TCFB: p(w|θq ) = αp(w|θqT F B ) + (1 − α)p(w|θqCF B ) 6. EXPERIMENTS In this section, we describe our experiment results. We first describe our experiment setup and present an overview of various methods performance. Then we discuss the effects of varying the parameter setting in the algorithms, as well as the number of presentation terms. Next we analyze user term feedback behavior and its relation to retrieval performance. Finally we compare term feedback to relevance feedback and show that it has its particular advantage. 6.1 Experiment Setup and Basic Results We took the opportunity of TREC 2005 HARD Track[2] for the evaluation of our algorithms. The tracks used the AQUAINT collection, a 3GB corpus of English newswire text. The topics included 50 ones previously known to be hard, i.e. with low retrieval performance. It is for these hard topics that user feedback is most helpful, as it can provide information to disambiguate the queries; with easy topics the user may be unwilling to spend efforts for feedback if the automatic retrieval results are good enough. Participants of the track were able to submit custom-designed clarification forms (CF) to solicit feedback from human assessors provided by Table 2: Retrieval performance for different methods and CF types. The last row is the percentage of MAP improvement over the baseline. The parameter settings μ = 4, λ = 0.1, α = 0.3 are near optimal. Baseline TFB1C TFB3C TFB6C CFB1C CFB3C CFB6C TCFB1C TCFB3C TCFB6C MAP 0.219 0.288 0.288 0.278 0.254 0.305 0.301 0.274 0.309 0.304 Pr@30 0.393 0.467 0.475 0.457 0.399 0.480 0.473 0.431 0.491 0.473 RR 4339 4753 4762 4740 4600 4907 4872 4767 4947 4906 % 0% 31.5% 31.5% 26.9% 16.0% 39.3% 37.4% 25.1% 41.1% 38.8% Table 3: MAP variation with the number of presented terms. # terms TFB1C TFB3C TFB6C CFB3C CFB6C TCFB3C TCFB6C 6 0.245 0.240 0.227 0.279 0.279 0.281 0.274 12 0.261 0.261 0.242 0.299 0.286 0.297 0.281 18 0.275 0.274 0.256 0.301 0.282 0.300 0.286 24 0.276 0.281 0.265 0.303 0.292 0.305 0.292 30 0.280 0.285 0.270 0.304 0.296 0.307 0.296 36 0.282 0.288 0.272 0.307 0.297 0.309 0.297 42 0.283 0.288 0.275 0.306 0.298 0.309 0.300 48 0.288 0.288 0.278 0.305 0.301 0.309 0.303 NIST. We designed three sets of clarification forms for term feedback, differing in the choice of K, the number of clusters, and L, the number of presented terms from each cluster. They are: 1× 48, a big cluster with 48 terms, 3 × 16, 3 clusters with 16 terms each, and 6 × 8, 6 clusters with 8 terms each. The total number of presented terms (M) is fixed at 48, so by comparing the performance of different types of clarification forms we can know the effects of different degree of clustering. For each topic, an assessor would complete the forms ordered by 6 × 8, 1 × 48 and 3 × 16, spending up to three minutes on each form. The sample clarification form shown in Figure 1 is of type 3 × 16. It is a simple and compact interface in which the user can check relevant terms. The form is self-explanatory; there is no need for extra user training on how to use it. Our initinal queries are constructed only using the topic title descriptions, which are on average 2.7 words in length. As our baseline we use the KL divergence retrieval method implemented in the Lemur Toolkit1 with 5 pseudo-feedback documents. We stem the terms, choose Dirichlet smoothing with a prior of 2000, and truncate query language models to 50 terms (these settings are used throughout the experiments). For all other parameters we use Lemurs default settings. The baseline turns out to perform above average among the track participants. After an initial run using this baseline retrieval method, we take the top 60 documents for each topic and apply the theme discovery algorithm to output the clusters (1, 3, or 6 of them), based on which we generate clarification forms. After user feedback is received, we run the term feedback algorithms (TFB, CFB or TCFB) to estimate updated query models, which are then used for a second iteration of retrieval. We evaluate the different retrieval methods performance on their rankings of the top 1000 documents. The evaluation metrics we adopt include mean average (non-interpolated) precision (MAP), precision at top 30 (Pr@30) and total relevant retrieved (RR). Table 2 shows the performance of various methods and configurations of K × L. The suffixes (1C, 3C, 6C) after TFB,CFB,TCFB stand for the number of clusters (K). For example, TCFB3C means the TCFB method on the 3 × 16 clarification forms. From Table 2 we can make the following observations: 1 http://www.lemurproject.com 1. All methods perform considerably better than the pseudofeedback baseline, with TCFB3C achieving a highest 41.1% improvement in MAP, indicating significant contribution of term feedback for clarification of the users information need. In other words, term feedback is truly helpful for improving retrieval accuracy. 2. For TFB, the performance is almost equal on the 1 × 48 and 3 × 16 clarification forms in terms of MAP (although the latter is slightly better in Pr@30 and RR), and a little worse on the 6 × 8 ones. 3. Both CFB3C and CFB6C perform better than their TFB counterparts in all three metrics, suggesting that feedback on a secondary cluster structure is indeed beneficial. CFB1C is actually worse because it cannot adjust the weight of its (single) cluster from term feedback and it is merely pseudofeedback. 4. Although TCFB is just a simple mixture of TFB and CFB by interpolation, it is able to outperform both. This supports our speculation that TCFB overcomes the drawbacks of TFB (paying attention only to checked terms) and CFB (not distinguishing checked and unchecked terms in a cluster). Except for TCFB6C v.s. CFB6C, the performance advantage of TCFB over TFB/CFB is significant at p < 0.05 using the Wilcoxon signed rank test. This is not true in the case of TFB v.s. CFB, each of which is better than the other in nearly half of the topics. 6.2 Reduction of Presentation Terms In some situations we may have to reduce the number of presentation terms due to limits in display space or user feedback efforts. It is interesting to know whether our algorithms performance deteriorates when the user is presented with fewer terms. Because the presentation terms within each cluster are generated in decreasing order of their frequencies, the presentation list forms a subset of the original one if its size is reduced2 . Therefore, we can easily simulate what happens when the number of presentation terms decreases 2 There are complexities arising from terms appearing in top L of multiple clusters, but these are exceptions from M to M : we will keep all judgments of the top L = M /K terms in each cluster and discard those of others. Table 3 shows the performance of various algorithms as the number of presentation terms ranges from 6 to 48. We find that the performance of TFB is more susceptible to presentation term reduction than that of CFB or TCFB. For example, at 12 terms the MAP of TFB3C is 90.6% of that at 48 terms, while the numbers for CFB3C and TCFB3C are 98.0% and 96.1% respectively. We conjecture the reason to be that while TFBs performance heavily depends on how many good terms are chosen for query expansion, CFB only needs a rough estimate of cluster weights to work. Also, the 3 × 16 clarification forms seem to be more robust than the 6 × 8 ones: at 12 terms the MAP of TFB6C is 87.1% of that at 48 terms, lower than 90.6% for TFB3C. Similarly, for CFB it is 95.0% against 98.0%. This is natual, as for a large cluster number of 6, it is easier to get into the situation where each cluster gets too few presentation terms to make topic diversification useful. Overall, we are surprised to see that the algorithms are still able to perform reasonably well when the number of presentation terms is small. For example, at only 12 terms CFB3C (the clarification form is of size 3 × 4) can still improve 36.5% over the baseline, dropping slightly from 39.3% at 48 terms. 6.3 User Feedback Analysis In this part we study several aspects of users term feedback behavior, and whether they are connected to retrieval performance. Figure 2: Clarification form completion time distributions 0−30 30−60 60−90 90−120 120−150 150−180 0 5 10 15 20 25 30 35 completion time (seconds) #topics 1×48 3×16 6×8 Figure 2 shows the distribution of time needed to complete a clarification form3 . We see that the user is usually able to finish term feedback within a reasonably short amount of time: for more than half of the topics the clarification form is completed in just 1 minute, and only a small fraction of topics (less than 10% for 1 × 48 and 3 × 16) take more than 2 minutes. This suggests that term feedback is suitable for interactive ad-hoc retrieval, where a user usually does not want to spend too much time on providing feedback. We find that a user often makes mistakes when judging term relevance. Sometimes a relevant term may be left out because its connection to the query topic is not obvious to the user. Other times a dubious term may be included but turns out to be irrelevant. Take the topic in Figure 1 for example. There was a fire disaster in Mont 3 The maximal time is 180 seconds, as the NIST assessor would be forced to submit the form at that moment. Table 4: Term selection statistics (topic average) CF Type 1 × 48 3 × 16 6 × 8 # checked terms 14.8 13.3 11.2 # rel. terms 15.0 12.6 11.2 # rel. checked terms 7.9 6.9 5.9 precision 0.534 0.519 0.527 recall 0.526 0.548 0.527 Blanc Tunnel between France and Italy in 1999, but the user failed to select such keywords as mont, blanc, french and italian due to his/her ignorance of the event. Indeed, without proper context it would be hard to make perfect judgment. What is then, the extent to which the user is good at term feedback? Does it have serious impact on retrieval performance? To answer these questions, we need a measure of individual terms true relevance. We adopt the Simplified KL Divergence metric used in [24] to decide query expansion terms as our term relevance measure: σKLD(w) = p(w|R) log p(w|R) p(w|¬R) where p(w|R) is the probability that a relevant document contains term w, and p(w|¬R) is the probability that an irrelevant document contains w, both of which can be easily computed via maximum likelihood estimate given document-level relevance judgment. If σKLD(w) > 0, w is more likely to appear in relevant documents than irrelevant ones. We consider a term relevant if its Simplified KL Divergence value is greater than a certain threshold σ0. We can then define precision and recall of user term judgment accordingly: precision is the fraction of terms checked by the user that are relevant; recall is the fraction of presented relevant terms that are checked by the user. Table 4 shows the number of checked terms, relevant terms and relevant checked terms when σ0 is set to 1.0, as well as the precision/recall of user term judgment. Note that when the clarification forms contain more clusters, fewer terms are checked: 14.8 for 1 × 48, 13.3 for 3 × 16 and 11.2 for 6×8. Similar pattern holds for relevant terms and relevant checked terms. There seems to be a trade-off between increasing topic diversity by clustering and losing extra relevant terms: when there are more clusters, each of them gets fewer terms to present, which can hurt a major relevant cluster that contains many relevant terms. Therefore, it is not always helpful to have more clusters, e.g., TFB6C is actually worse than TFB1C. The major finding we can make from Table 4 is that the user is not particularly good at identifying relevant terms, which echoes the discovery in [18]. In the case of 3 × 16 clarification forms, the average number of terms checked as relevant by the user is 13.3 per topic, and the average number of relevant terms whose σKLD value exceed 1.0 is 12.6. The user is able to recognize only 6.9 of these terms on average. Indeed, the precision and recall of user feedback terms (as defined previously) are far from perfect. On the other hand, If the user had correctly checked all such relevant terms, the performance of our algorithms would have increased a lot, as shown in Table 5. We see that TFB gets big improvement when there is an oracle who checks all relevant terms, while CFB meets a bottleneck around MAP of 0.325, since all it does is adjust cluster weights, and when the learned weights are close to being accurate, it cannot benefit more from term feedback. Also note that TCFB fails to outperform TFB, probably because TFB is sufficiently accurate. Table 5: Change of MAP when using all (and only) relevant terms (σKLD > 1.0) for feedback. original term feedback relevant term feedback TF1 0.288 0.354 TF3 0.288 0.354 TF6 0.278 0.346 CF3 0.305 0.325 CF6 0.301 0.326 TCF3 0.309 0.345 TCF6 0.304 0.341 6.4 Comparison with Relevance Feedback Now we compare term feedback with document-level relevance feedback, in which the user is presented with the top N documents from an initial retrieval and asked to judge their relevance. The feedback process is simulated using document relevance judgment from NIST. We use the mixture model based feedback method proposed in [25], with mixture noise set to 0.95 and feedback coefficient set to 0.9. Comparative evaluation of relevance feedback against other methods is complicated by the fact that some documents have already been viewed during feedback, so it makes no sense to include them in the retrieval results of the second run. However, this does not hold for term feedback. Thus, to make it fair w.r.t. users information gain, if the feedback documents are relevant, they should be kept in the top of the ranking; if they are irrelevant, they should be left out. Therefore, we use relevance feedback to produce a ranking of top 1000 retrieved documents but with every feedback document excluded, and then prepend the relevant feedback documents at the front. Table 6 shows the performance of relevance feedback for different values of N and compares it with TCFB3C. Table 6: Performance of relevance feedback for different number of feedback documents (N). N MAP Pr@30 RR 5 0.302 0.586 4779 10 0.345 0.670 4916 20 0.389 0.772 5004 TCFB3C 0.309 0.491 4947 We see that the performance of TCFB3C is comparable to that of relevance feedback using 5 documents. Although it is poorer than when there are 10 feedback documents in terms of MAP and Pr@30, it does retrieve more documents (4947) when going down the ranked list. We try to compare the quality of automatically inserted terms in relevance feedback with that of manually selected terms in term feedback. This is done by truncating the relevance feedback modified query model to a size equal to the number of checked terms for the same topic. We can then compare the terms in the truncated model with the checked terms. Figure 3 shows the distribution of the terms σKLD scores. We find that term feedback tends to produce expansion terms of higher quality(those with σKLD > 1) compared to relevance feedback (with 10 feedback documents). This does not contradict the fact that the latter yields higher retrieval performance. Actually, when we use the truncated query model instead of the intact one refined from relevance feedback, the MAP is only 0.304. The truth Figure 3: Comparison of expansion term quality between relevance feedback (with 10 feedback documents) and term feedback (with 3 × 16 CFs) −1−0 0−1 1−2 2−3 3−4 4−5 5−6 0 50 100 150 200 250 300 350 σKLD #terms relevance feedback term feedback is, although there are many unwanted terms in the expanded query model from feedback documents, there are also more relevant terms than what the user can possibly select from the list of presentation terms generated with pseudo-feedback documents, and the positive effects often outweights the negative ones. We are interested to know under what circumstances term feedback has advantage over relevance feedback. One such situation is when none of the top N feedback documents is relevant, rendering relevance feedback useless. This is not infrequent, as one might have thought: out of the 50 topics, there are 13 such cases when N = 5, 10 when N = 10, and still 3 when N = 20. When this happens, one can only back off to the original retrieval method; the power of relevance feedback is lost. Surprisingly, in 11 out of 13 such cases where relevance feedback seems impossible, the user is able to check at least 2 relevant terms from the 3 × 16 clarification form (we consider term t to be relevant if σKLD(t) > 1.0). Furthermore, in 10 out of them TCFB3C outperforms the pseudo-feedback baseline, increasing MAP from 0.076 to 0.146 on average (these are particularly hard topics). We think that there are two possible explanations for this phenomenon of term feedback being active even when relevance feedback does not work: First, even if none of the top N (suppose it is a small number) documents are relevant, we may still find relevant documents in top 60, which is more inclusive but usually unreachable when people are doing relevance feedback in interactive ad-hoc search, from which we can draw feedback terms. This is true for topic 367 piracy, where the top 10 feedback documents are all about software piracy, yet there are documents between 10-60 that are about piracy on the seas (which is about the real information need), contributing terms such as pirate, ship for selection in the clarification form. Second, for some topics, a document needs to meet some special condition in order to be relevant. The top N documents may be related to the topic, but nonetheless irrelevant. In this case, we may still extract useful terms from these documents, even if they do not qualify as relevant ones. For example, in topic 639 consumer online shopping, a document needs to mention what contributes to shopping growth to really match the specified information need, hence none of the top 10 feedback documents are regarded as relevant. But nevertheless, the feedback terms such as retail, commerce are good for query expansion. 7. CONCLUSIONS In this paper we studied the use of term feedback for interactive information retrieval in the language modeling approach. We proposed a cluster-based method for selecting presentation terms as well as algorithms to estimate refined query models from user term feedback. We saw significant improvement in retrieval accuracy brought by term feedback, in spite of the fact that a user often makes mistakes in relevance judgment that hurts its performance. We found the best-performing algorithm to be TCFB, which benefits from the combination of directly observed term evidence with TFB and indirectly learned cluster relevance with CFB. When we reduced the number of presentation terms, term feedback is still able to keep much of its performance gain over the baseline. Finally, we compared term feedback to document-level relevance feedback, and found that TCFB3Cs performance is on a par with the latter with 5 feedback documents. We regarded term feedback as a viable alternative to traditional relevance feedback, especially when there are no relevant documents in the top. We propose to extend our work in several ways. First, we want to study whether the use of various contexts can help the user to better identify term relevance, while not sacrificing the simplicity and compactness of term feedback. Second, currently all terms are presented to the user in a single batch. We could instead consider iterative term feedback, by presenting a small number of terms first, and show more terms after receiving user feedback or stop when the refined query is good enough. The presented terms should be selected dynamically to maximize learning benefits at any moment. Third, we have plans to incorporate term feedback into our UCAIR toolbar[20], an Internet Explorer plugin, to make it work for web search. We are also interested in studying how to combine term feedback with relevance feedback or implicit feedback. We could, for example, allow the user to dynamically modify terms in a language model learned from feedback documents. 8. ACKNOWLEDGMENT This work is supported in part by the National Science Foundation grants IIS-0347933 and IIS-0428472. 9. REFERENCES [1] J. Allan. Relevance feedback with too much data. In Proceedings of the 18th annual international ACM SIGIR conference on research and development in information retrieval, pages 337-343, 1995. [2] J. Allan. HARD track overview in TREC 2005 - High Accuracy Retrieval from Documents. In The Fourteenth Text REtrieval Conference, 2005. [3] P. Anick. Using terminological feedback for web search refinement: a log-based study. In Proceedings of the 26th annual international ACM SIGIR conference on research and development in informaion retrieval, pages 88-95, 2003. [4] P. G. Anick and S. Tipirneni. The paraphrase search assistant: terminological feedback for iterative information seeking. In Proceedings of the 22nd annual international ACM SIGIR conference on research and development in information retrieval, pages 153-159, 1999. [5] C. Buckley, G. Salton, J. Allan, and A. Singhal. Automatic query expansion using SMART. In Proceedings of the Third Text REtrieval Conference, 1994. [6] D. Harman. Towards interactive query expansion. In Proceedings of the 11th annual international ACM SIGIR conference on research and development in information retrieval, pages 321-331, 1988. [7] N. A. Jaleel, A. Corrada-Emmanuel, Q. Li, X. Liu, C. Wade, and J. Allan. UMass at TREC 2003: HARD and QA. In TREC, pages 715-725, 2003. [8] H. Joho, C. Coverson, M. Sanderson, and M. Beaulieu. Hierarchical presentation of expansion terms. In Proceedings of the 2002 ACM symposium on applied computing, pages 645-649, 2002. [9] K. S. Jones, S. Walker, and S. E. Robertson. A probabilistic model of information retrieval: development and status. Technical Report 446, Computer Laboratory, University of Cambridge, 1998. [10] D. Kelly, V. D. Dollu, and X. Fu. The loquacious user: a document-independent source of terms for query expansion. In Proceedings of the 28th annual international ACM SIGIR conference on research and development in information retrieval, pages 457-464, 2005. [11] D. Kelly and X. Fu. Elicitation of term relevance feedback: an investigation of term source and context. In Proceedings of the 29th annual international ACM SIGIR conference on research and development in information retrieval, 2006. [12] J. Koenemann and N. Belkin. A case for interaction: A study of interactive information retrieval behavior and effectiveness. In Proceedings of the SIGCHI conference on human factors in computing systems, pages 205-212, 1996. [13] V. Lavrenko and W. B. Croft. Relevance-based language models. In Research and Development in Information Retrieval, pages 120-127, 2001. [14] Y. Nemeth, B. Shapira, and M. Taeib-Maimon. Evaluation of the real and perceived value of automatic and interactive query expansion. In Proceedings of the 27th annual international ACM SIGIR conference on research and development in information retrieval, pages 526-527, 2004. [15] J. Ponte. A Language Modeling Approach to Information Retrieval. PhD thesis, University of Massachusetts at Amherst, 1998. [16] S. E. Robertson, S. Walker, S. Jones, M. Beaulieu, and M. Gatford. Okapi at TREC-3. In Proceedings of the Third Text REtrieval Conference, 1994. [17] J. Rocchio. Relevance feedback in information retrieval. In The SMART retrieval system, pages 313-323. 1971. [18] I. Ruthven. Re-examining the potential effectiveness of interactive query expansion. In Proceedings of the 26th annual international ACM SIGIR conference on research and development in informaion retrieval, pages 213-220, 2003. [19] G. Salton and C. Buckley. Improving retrieval performance by relevance feedback. Journal of the American Society for Information Science, 41:288-297, 1990. [20] X. Shen, B. Tan, and C. Zhai. Implicit user modeling for personalized search. In Proceedings of the 14th ACM international conference on information and knowledge management, pages 824-831, 2005. [21] X. Shen and C. Zhai. Active feedback in ad-hoc information retrieval. In Proceedings of the 28th annual international ACM SIGIR conference on research and development in information retrieval, pages 59-66, 2005. [22] A. Spink. Term relevance feedback and query expansion: relation to design. In Proceedings of the 17th annual international ACM SIGIR conference on research and development in information retrieval, pages 81-90, 1994. [23] J. Xu and W. B. Croft. Query expansion using local and global document analysis. In Proceedings of the 19th annual international ACM SIGIR conference on research and development in information retrieval, pages 4-11, 1996. [24] H. Zaragoza, N. Craswell, M. Taylor, S. Saria, and S. Robertson. Microsoft cambridge at TREC-13: Web and HARD tracks. In Proceedings of the 13th Text REtrieval Conference, 2004. [25] C. Zhai and J. Lafferty. Model-based feedback in the language modeling approach to information retrieval. In Proceedings of the tenth international conference on information and knowledge management, pages 403-410, 2001. [26] C. Zhai, A. Velivelli, and B. Yu. A cross-collection mixture model for comparative text mining. In Proceedings of the tenth ACM SIGKDD international conference on knowledge discovery and data mining, pages 743-748, 2004.",
    "original_translation": "Comentarios sobre el término para la recuperación de información con modelos de lenguaje Bin Tan†, Atulya Velivelli‡, Hui Fang†, ChengXiang Zhai† Dept. En este documento estudiamos la retroalimentación basada en términos para la recuperación de información en el enfoque de modelado de lenguaje. Con la retroalimentación de términos, un usuario juzga directamente la relevancia de términos individuales sin interacción con documentos de retroalimentación, tomando el control total del proceso de expansión de la consulta. Proponemos un método basado en clusters para seleccionar términos para presentar al usuario para su evaluación, así como algoritmos efectivos para construir modelos de lenguaje de consulta refinados a partir de la retroalimentación de términos del usuario. Nuestros algoritmos han demostrado proporcionar una mejora significativa en la precisión de recuperación en comparación con una línea base sin retroalimentación, y logran un rendimiento comparable al de la retroalimentación de relevancia. Son útiles incluso cuando no hay documentos relevantes en la parte superior. Categorías y Descriptores de Asignaturas H.3.3 [Búsqueda y Recuperación de Información]: Modelos de recuperación Términos generales Algoritmos 1. En el enfoque de modelado del lenguaje para la recuperación de información, la retroalimentación a menudo se modela como la estimación de un modelo de consulta mejorado o un modelo de relevancia basado en un conjunto de documentos de retroalimentación [25, 13]. Esto está en línea con la forma tradicional de realizar retroalimentación de relevancia: presentar al usuario documentos/pasajes para su juicio de relevancia y luego extraer términos de los documentos o pasajes juzgados para expandir la consulta inicial. Es una forma indirecta de buscar la ayuda de los usuarios para la construcción del modelo de consulta, en el sentido de que el modelo de consulta refinado (basado en términos) se aprende a través de documentos/pasajes de retroalimentación, que son estructuras de alto nivel de términos. Tiene la desventaja de que los términos irrelevantes, que ocurren junto con los relevantes en el contenido evaluado, pueden ser utilizados erróneamente para la expansión de la consulta, causando efectos no deseados. Por ejemplo, para la consulta TREC del telescopio Hubble logros, cuando un documento relevante habla más sobre la reparación de los telescopios que sobre sus descubrimientos, términos irrelevantes como caminata espacial pueden ser agregados a la consulta modificada. Podemos considerar una forma más directa de involucrar a un usuario en la mejora del modelo de consulta, sin un paso intermedio de retroalimentación de documentos que pueda introducir ruido. La idea es presentar al usuario un número (razonable) de términos individuales y pedirle que juzgue la relevancia de cada término o especifique directamente sus probabilidades en el modelo de consulta. Esta estrategia ha sido discutida en [15], pero hasta donde sabemos, no ha sido estudiada seriamente en la literatura existente de modelado de lenguaje. En comparación con la retroalimentación de relevancia tradicional, este enfoque basado en términos para el refinamiento del modelo de consulta interactivo tiene varias ventajas. Primero, el usuario tiene un mejor control del modelo de consulta final a través de la manipulación directa de términos: él/ella puede dictar qué términos son relevantes, irrelevantes y posiblemente, en qué medida. Esto evita el riesgo de introducir términos no deseados en el modelo de consulta, aunque a veces el usuario introduce términos de baja calidad. Segundo, dado que un término tarda menos tiempo en ser evaluado que el texto completo o el resumen de un documento, y con tan solo alrededor de 20 términos presentados se puede lograr una mejora significativa en el rendimiento de recuperación (como demostraremos más adelante), el feedback de términos permite recopilar la retroalimentación del usuario de manera más rápida. Esto es especialmente útil para la búsqueda interactiva ad hoc. En tercer lugar, a veces no hay documentos relevantes en los primeros N resultados recuperados inicialmente si el tema es difícil. Esto suele ser cierto cuando N está limitado a ser pequeño, lo cual surge del hecho de que el usuario no está dispuesto a evaluar demasiados documentos. En este caso, la retroalimentación de relevancia es inútil, ya que no se puede aprovechar ningún documento relevante, pero la retroalimentación de términos sigue siendo útil, al permitir seleccionar términos relevantes de documentos irrelevantes. Durante nuestra participación en la pista HARD de TREC 2005 y el estudio continuado posteriormente, exploramos cómo aprovechar la retroalimentación de términos por parte del usuario para construir modelos de consulta mejorados para la recuperación de información en el enfoque de modelado de lenguaje. Identificamos dos sub tareas clave del feedback basado en términos, es decir, la selección de términos para la presentación previa al feedback y la construcción del modelo de consulta posterior al feedback, con algoritmos efectivos desarrollados para ambos. Impusimos una estructura de clúster secundaria en los términos y descubrimos que una vista de clúster arroja una mayor comprensión de la necesidad de información de los usuarios, y proporciona una buena forma de utilizar la retroalimentación de términos. A través de experimentos encontramos que el feedback a término mejora significativamente sobre la línea base sin feedback, a pesar de que el usuario a menudo comete errores en la evaluación de relevancia. Entre nuestros algoritmos, el que tiene el mejor rendimiento de recuperación es TCFB, la combinación de TFB, el algoritmo de retroalimentación de términos directos, y CFB, el algoritmo de retroalimentación basado en clústeres. También variamos el número de términos de retroalimentación y observamos una mejora razonable incluso con números bajos. Finalmente, al comparar la retroalimentación a nivel de términos con la retroalimentación a nivel de documentos, encontramos que es una alternativa viable a esta última con un rendimiento competitivo en la recuperación. El resto del documento está organizado de la siguiente manera. La sección 2 discute algunos trabajos relacionados. La sección 4 describe nuestro enfoque general para la retroalimentación de términos. Presentamos nuestro método para la selección de términos de presentación en la Sección 3 y los algoritmos para la construcción del modelo de consulta en la Sección 5. Los resultados del experimento se presentan en la Sección 6. La sección 7 concluye este documento. 2. TRABAJO RELACIONADO La retroalimentación de relevancia[17, 19] ha sido reconocida durante mucho tiempo como un método efectivo para mejorar el rendimiento de recuperación. Normalmente, los N documentos principales recuperados utilizando la consulta original se presentan al usuario para su evaluación, después de lo cual se extraen términos de los documentos relevantes evaluados, ponderados por su potencial de atraer más documentos relevantes, y se añaden al modelo de consulta. La consulta ampliada suele representar mejor la necesidad de información de los usuarios que la original, que a menudo es solo una consulta de palabras clave cortas. Una segunda iteración de recuperación utilizando esta consulta modificada generalmente produce un aumento significativo en la precisión de la recuperación. En los casos en los que no se dispone de una evaluación de relevancia real y se asume que todos los documentos principales N son relevantes, se denomina retroalimentación ciega o pseudo [5, 16] y generalmente sigue mejorando el rendimiento. Debido a que un documento es una unidad de texto grande, cuando se utiliza para retroalimentación de relevancia, muchos términos irrelevantes pueden ser introducidos en el proceso de retroalimentación. Para superar esto, se propone el feedback de pasajes y se muestra que mejora el rendimiento del feedback. Una solución más directa es pedir al usuario su juicio de relevancia sobre los términos de retroalimentación. Por ejemplo, en algunos sistemas de retroalimentación de relevancia como [12], hay un paso de interacción que permite al usuario agregar o eliminar términos de expansión después de que son extraídos automáticamente de los documentos relevantes. Esto se clasifica como expansión interactiva de consultas, donde la consulta original se amplía con términos proporcionados por el usuario, que pueden provenir de la entrada directa del usuario (texto libre o palabras clave) o de la selección del usuario de términos sugeridos por el sistema (utilizando tesauros o extraídos de documentos de retroalimentación). En muchos casos, se ha encontrado que la retroalimentación de relevancia de términos mejora de manera efectiva el rendimiento de recuperación [6, 22, 12, 4, 10]. Por ejemplo, el estudio en [12] muestra que el usuario prefiere tener un conocimiento explícito y control directo de qué términos se utilizan para la expansión de la consulta, y se demuestra que la interfaz penetrable que proporciona esta libertad tiene un mejor rendimiento que otras interfaces. Sin embargo, en algunos otros casos no hay un beneficio significativo[3, 14], incluso si al usuario le gusta interactuar con los términos de expansión. En un estudio simulado realizado en [18], el autor compara el rendimiento de recuperación de la expansión interactiva de consultas y la expansión automática de consultas con un estudio simulado, y sugiere que los posibles beneficios de la primera pueden ser difíciles de lograr. Se descubre que el usuario no es bueno identificando términos útiles para la expansión de la consulta, cuando una interfaz de presentación de términos simple no puede proporcionar suficiente contexto semántico de los términos de retroalimentación. Nuestro trabajo difiere de los anteriores en dos aspectos importantes. Primero, al elegir los términos para presentar al usuario para la evaluación de relevancia, no solo consideramos el valor de un solo término (por ejemplo, la frecuencia relativa de un término en los documentos principales, que puede medirse mediante métricas como el Valor de Selección de Robertson y la Distancia Kullback-Leibler Simplificada como se detalla en [24]), sino que también examinamos la estructura de los grupos de términos, con el fin de producir una cobertura equilibrada de los diferentes aspectos del tema. Segundo, con el marco de modelado del lenguaje, permitimos una construcción detallada del modelo de consulta actualizado, estableciendo diferentes probabilidades para diferentes términos según si es un término de consulta, su importancia en los documentos principales y su pertenencia a un clúster. Aunque existen técnicas para ajustar los pesos de los términos de consulta en modelos de espacio vectorial y modelos de relevancia probabilística, la mayoría de los trabajos mencionados no las utilizan, optando por simplemente añadir términos de retroalimentación a la consulta original (usando pesos iguales para ellos), lo que puede llevar a un rendimiento de recuperación más pobre. La combinación de los dos aspectos permite que nuestro método funcione mucho mejor que el punto de referencia. La forma habitual de presentar los términos de retroalimentación es simplemente mostrar los términos en una lista. Ha habido algunos trabajos sobre interfaces de usuario alternativas. [8] organiza los términos en una jerarquía, y [11] compara tres interfaces diferentes, incluyendo términos + casillas de verificación, términos + contexto (oraciones) + casillas de verificación, oraciones + cuadro de texto de entrada. En ambos estudios, sin embargo, no hay una diferencia significativa en el rendimiento. En nuestro trabajo adoptamos el enfoque más simple de términos + casillas de verificación. Nos enfocamos en la presentación de términos y la construcción de modelos de consulta a partir de términos de retroalimentación, y creemos que el uso de contextos para mejorar la calidad de los términos de retroalimentación debería ser ortogonal a nuestro método. 3. Enfoque general: Seguimos el enfoque de modelado del lenguaje y basamos nuestro método en el modelo de recuperación de divergencia KL propuesto en [25]. Con este modelo, la tarea de recuperación implica estimar un modelo de lenguaje de consulta θq a partir de una consulta dada, un modelo de lenguaje de documento θd de cada documento, y calcular su divergencia KL D(θq||θd), que luego se utiliza para puntuar los documentos. [25] trata la retroalimentación de relevancia como un problema de reestimación del modelo de consulta, es decir, calcular un modelo de consulta actualizado θq dado el texto de la consulta original y la evidencia adicional proporcionada por los documentos relevantes juzgados. Adoptamos esta perspectiva y planteamos nuestra tarea como la actualización del modelo de consulta a partir de la retroalimentación de términos de usuario. Aquí hay dos tareas clave: Primero, cómo elegir los mejores términos para presentar al usuario para su evaluación, con el fin de recopilar la máxima evidencia sobre la necesidad de información del usuario. Segundo, cómo calcular un modelo de consulta actualizado basado en esta retroalimentación de términos, de manera que capture la necesidad de información de los usuarios y se traduzca en un buen rendimiento de recuperación. 4. SELECCIÓN DE TÉRMINOS DE PRESENTACIÓN La selección adecuada de los términos que se presentarán al usuario para su evaluación es crucial para el éxito de la retroalimentación de términos. Si los términos están mal elegidos y hay pocos relevantes, el usuario tendrá dificultades para buscar términos útiles que ayuden a aclarar su necesidad de información. Si los términos relevantes son abundantes, pero todos se centran en un solo aspecto del tema de la consulta, entonces solo podremos obtener retroalimentación sobre ese aspecto y perderemos otros, lo que resultará en una pérdida de amplitud en los resultados recuperados. Por lo tanto, es importante seleccionar cuidadosamente los términos de presentación para maximizar la ganancia esperada de la retroalimentación del usuario, es decir, aquellos que pueden revelar potencialmente la mayor evidencia de la necesidad de información del usuario. Esto es similar al feedback activo[21], que sugiere que un sistema de recuperación debería sondear activamente la necesidad de información de los usuarios, y en el caso del feedback de relevancia, los documentos de feedback deberían ser elegidos para maximizar los beneficios de aprendizaje (por ejemplo, de manera diversa para aumentar la cobertura). En nuestro enfoque, los N documentos principales de una recuperación inicial utilizando la consulta original forman la fuente de términos de retroalimentación: todos los términos que aparecen en ellos se consideran candidatos para presentar al usuario. Estos documentos sirven como pseudo-retroalimentación, ya que proporcionan un contexto mucho más amplio que la consulta original (generalmente muy breve), sin que se le pida al usuario que juzgue su relevancia. Debido a esta última razón, es posible hacer que N sea bastante grande (por ejemplo, en nuestros experimentos establecimos N = 60) para aumentar su cobertura de diferentes aspectos en el tema. La forma más sencilla de seleccionar términos de retroalimentación es elegir los M términos más frecuentes de los N documentos. Este método, sin embargo, tiene dos inconvenientes. Primero, se seleccionarán muchos términos ruidosos comunes debido a sus altas frecuencias en la colección de documentos, a menos que se utilice una lista de palabras vacías para filtrar. En segundo lugar, la lista de presentación tiende a estar compuesta por términos de los aspectos principales del tema; es probable que se pasen por alto aquellos de un aspecto menor debido a sus frecuencias relativamente bajas. Resolvemos los problemas anteriores mediante dos medidas correspondientes. Primero, introducimos un modelo de fondo θB que se estima a partir de estadísticas de la colección y explica los términos comunes, de modo que es mucho menos probable que aparezcan en la lista de presentación. Segundo, los términos se seleccionan de múltiples grupos en los documentos de retroalimentación simulada, para garantizar una representación suficiente de diferentes aspectos del tema. Nos basamos en el modelo multinomial de mezcla, que se utiliza para el descubrimiento de temas en [26]. Específicamente, asumimos que los N documentos contienen K grupos {Ci| i = 1, 2, · · · K}, cada uno caracterizado por una distribución de palabras multinomial (también conocida como modelo de lenguaje unigrama) θi y correspondiente a un aspecto del tema. Los documentos se consideran como muestreados de una mezcla de K + 1 componentes, incluidos los K grupos y el modelo de fondo: p(w|d) = λBp(w|θB) + (1 − λB) K i=1 πd,ip(w|θi) donde w es una palabra, λB es el peso de la mezcla para el modelo de fondo θB, y πd,i es el peso de la mezcla específico del documento para el modelo de cluster i-ésimo θi. Luego estimamos los modelos de clúster maximizando la probabilidad de que los documentos de pseudo-retroalimentación sean generados a partir del modelo de mezcla multinomial: log p(D|Λ) = d∈D w∈V c(w; d) log p(w|d) donde D = {di| i = 1, 2, · · · N} es el conjunto de los N documentos, V es el vocabulario, c(w; d) es la frecuencia de w en d y Λ = {θi| i = 1, 2, · · · K} ∪ {πdij | i = 1, 2, · · · N, j = 1, 2, · · · K} es el conjunto de parámetros del modelo a estimar. Los modelos de conglomerados pueden ser estimados eficientemente utilizando el algoritmo de Expectation-Maximization (EM). Para más detalles, remitimos al lector a [26]. La Tabla 1 muestra los modelos de clúster para la consulta de TREC sobre desastres en túneles de transporte (K = 3). Ten en cuenta que solo el grupo central es relevante. Tabla 1: Modelos de clúster para el tema 363 Desastres en túneles de transporte Clúster 1 Clúster 2 Clúster 3 túnel 0.0768 túnel 0.0935 túnel 0.0454 transporte 0.0364 fuego 0.0295 transporte 0.0406 tráfico 0.0206 camión 0.0236 peaje 0.0166 ferrocarril 0.0186 francés 0.0220 Amtrak 0.0153 puerto 0.0146 humo 0.0157 tren 0.0129 riel 0.0140 coche 0.0154 aeropuerto 0.0122 puente 0.0139 italiano 0.0152 autopista 0.0105 kilómetro 0.0136 incendio 0.0144 lui 0.0095 camión 0.0133 llamas 0.0127 Jersey 0.0093 construcción 0.0131 blanco 0.0121 paso 0.0087 · · · · · · · · · De cada uno de los K clústeres estimados, elegimos los L = M/K términos con las probabilidades más altas para formar un total de M términos de presentación. Si un término resulta estar en el top L en múltiples grupos, lo asignamos al grupo donde tenga la probabilidad más alta y permitimos que los otros grupos tomen un término adicional como compensación. También filtramos los términos en el texto de la consulta original porque tienden a ser siempre relevantes cuando la consulta es corta. Los términos seleccionados son luego presentados al usuario para su evaluación. Se muestra un formulario de retroalimentación (completado) de muestra en la Figura 1. En este estudio solo tratamos con juicios binarios: un término presentado está por defecto sin marcar, y un usuario puede marcarlo para indicar relevancia. Tampoco explotamos explícitamente la retroalimentación negativa (es decir, penalizar términos irrelevantes), porque con la retroalimentación binaria un término no verificado no es necesariamente irrelevante (quizás el usuario no está seguro de su relevancia). Podríamos pedir al usuario un juicio más detallado (por ejemplo, eligiendo entre altamente relevante, algo relevante, no sé, algo irrelevante y altamente irrelevante), pero la retroalimentación binaria es más compacta, ocupando menos espacio para mostrar y requiriendo menos esfuerzo por parte del usuario para emitir un juicio. 5. ESTIMACIÓN DE MODELOS DE CONSULTA A PARTIR DE RETROALIMENTACIÓN DE TÉRMINOS En esta sección, presentamos varios algoritmos para aprovechar la retroalimentación de términos. Los algoritmos toman como entrada la consulta original q, los grupos {θi} generados por el algoritmo de descubrimiento de temas, el conjunto de términos de retroalimentación T y su juicio de relevancia R, y producen un modelo de lenguaje de consulta actualizado θq que hace el mejor uso de la evidencia de retroalimentación para capturar la necesidad de información de los usuarios. Primero describimos nuestras notaciones: • θq: El modelo de consulta original, derivado solo de los términos de la consulta: p(w|θq) = c(w; q) |q| donde c(w; q) es la cantidad de veces que w aparece en q, y |q| = w∈q c(w; q) es la longitud de la consulta. • θq: El modelo de consulta actualizado que necesitamos estimar a partir de la retroalimentación de términos. • θi (i = 1, 2, . . . K): El modelo de lenguaje unigrama del clúster Ci, estimado utilizando el algoritmo de descubrimiento de temas. • T = {ti,j} (i = 1 . . . K, j = 1 . . . \n\nK, j = 1 . . . El conjunto de términos presentados al usuario para su evaluación. ti,j es el j-ésimo término elegido del grupo Ci. • R = {δw|w ∈ T}: δw es una variable indicadora que es 1 si w es considerado relevante o 0 en caso contrario. 5.1 TFB (Retroalimentación Directa de Términos) Esta es una forma directa de retroalimentación de términos que no implica ninguna estructura secundaria. Asignamos un peso de 1 a los términos considerados relevantes por el usuario, un peso de μ a los términos de la consulta, un peso de cero a los demás términos, y luego aplicamos la normalización: p(w|θq ) = δw + μ c(w; q) w ∈T δw + μ|q| donde w ∈T δw es el número total de términos considerados relevantes. Llamamos a este método TFB (retroalimentación directa de términos). Si dejamos que μ = 1, este enfoque es equivalente a agregar los términos relevantes después de la consulta original, que es lo que hace la expansión estándar de consultas (sin reponderación de términos). Si establecemos μ > 1, estamos poniendo más énfasis en los términos de consulta que en los verificados. Ten en cuenta que el modelo de resultado será más sesgado hacia θq si la consulta original es larga o el feedback del usuario es débil, lo cual tiene sentido, ya que podemos confiar más en la consulta original en ambos casos. Figura 1: Formulario de aclaración completado para el Tema 363 363 desastres en túneles de transporte. Por favor, seleccione todos los términos relevantes para el tema. tráfico ferrocarril puerto puente ferroviario kilómetro construir suizo enlace cruzado hongkonés río proyecto metro camión de bomberos francés humo coche italiano bomberos incendio blanco montaña víctima francés rescate conductor chamonix emerger peaje tren amtrak aeropuerto turnpike jersey pass rome z centro electrón carretera boston velocidad bu enviar 5.2 CFB (Retroalimentación de Clúster) Aquí explotamos la estructura de clúster que jugó un papel importante cuando seleccionamos los términos de presentación. Los grupos representan diferentes aspectos del tema de la consulta, cada uno de los cuales puede ser relevante o no. Si somos capaces de identificar los grupos relevantes, podemos combinarlos para generar un modelo de consulta que sea bueno para descubrir documentos pertenecientes a estos grupos (en lugar de los irrelevantes). Podríamos pedir al usuario que juzgue directamente la relevancia de un grupo después de ver los términos representativos en ese grupo, pero a veces esto sería una tarea difícil para el usuario, quien tendría que adivinar la semántica de un grupo a través de su conjunto de términos, los cuales podrían no estar bien conectados entre sí debido a la falta de contexto. Por lo tanto, proponemos aprender retroalimentación de clúster de forma indirecta, inferir la relevancia de un clúster a través de la relevancia de sus términos de retroalimentación. Dado que cada grupo tiene un número igual de términos presentados al usuario, la medida más simple de la relevancia de un grupo es el número de términos que se consideran relevantes en él. Intuitivamente, cuantos más términos estén marcados como relevantes en un grupo, más cerca estará el grupo del tema de la consulta y más debería participar en la modificación de la consulta. Si combinamos los modelos de clúster utilizando pesos determinados de esta manera y luego interpolamos con el modelo de consulta original, obtenemos la siguiente fórmula para la actualización de la consulta, que llamamos CFB (Retroalimentación de Clúster): p(w|θq) = λp(w|θq) + (1 − λ) K i=1 L j=1 δti,j K k=1 L j=1 δtk,j p(w|θi) donde L j=1 δti,j es el número de términos relevantes en el clúster Ci, y K k=1 L j=1 δtk,j es el número total de términos relevantes. Observamos que cuando solo hay un clúster (K = 1), la fórmula anterior se degrada a p(w|θq ) = λp(w|θq) + (1 − λ)p(w|θ1) que es simplemente una pseudo-retroalimentación de la forma propuesta en [25]. TCFB (Retroalimentación Término-Clúster) TFB y CFB ambos tienen sus inconvenientes. TFB asigna probabilidades no nulas a los términos presentados que están marcados como relevantes, pero ignora por completo (muchos más) otros, que pueden quedar sin marcar debido a la ignorancia de los usuarios, o simplemente no estar incluidos en la lista de presentación, pero deberíamos ser capaces de inferir su relevancia a partir de los marcados. Por ejemplo, en la Figura 1, dado que se revisan hasta 5 términos en el grupo central (las tercera y cuarta columnas), deberíamos tener una alta confianza en la relevancia de otros términos en ese grupo. CFB resuelve el problema de TFB tratando los términos en un grupo de manera colectiva, de modo que los términos no verificados/no presentados reciban pesos cuando los términos presentados en sus grupos son considerados relevantes, pero no distingue qué términos en un grupo son presentados o considerados. Intuitivamente, los términos considerados relevantes deberían recibir pesos mayores porque son indicados explícitamente como relevantes por el usuario. Por lo tanto, intentamos combinar los dos métodos, con la esperanza de obtener lo mejor de ambos. Lo hacemos interpolando el modelo TFB con el modelo CFB, y lo llamamos TCFB: p(w|θq ) = αp(w|θqT F B ) + (1 − α)p(w|θqCF B ) 6. EXPERIMENTOS En esta sección, describimos los resultados de nuestro experimento. Primero describimos nuestra configuración experimental y presentamos una visión general del rendimiento de varios métodos. Luego discutimos los efectos de variar la configuración de los parámetros en los algoritmos, así como el número de términos de presentación. A continuación analizamos el comportamiento de retroalimentación de los usuarios y su relación con el rendimiento de recuperación. Finalmente comparamos la retroalimentación de términos con la retroalimentación de relevancia y mostramos que tiene su ventaja particular. 6.1 Configuración del Experimento y Resultados Básicos Aprovechamos la oportunidad de la pista HARD de TREC 2005[2] para la evaluación de nuestros algoritmos. Las pistas utilizaron la colección AQUAINT, un corpus de 3GB de texto de noticias en inglés. Los temas incluyeron 50 que se sabía que eran difíciles, es decir, con bajo rendimiento de recuperación. Es para estos temas difíciles que la retroalimentación del usuario es más útil, ya que puede proporcionar información para desambiguar las consultas; con temas fáciles, es posible que el usuario no esté dispuesto a esforzarse por dar retroalimentación si los resultados de recuperación automática son lo suficientemente buenos. Los participantes de la pista pudieron enviar formularios de aclaración (CF) diseñados a medida para solicitar retroalimentación de los evaluadores humanos proporcionados por la Tabla 2: Rendimiento de recuperación para diferentes métodos y tipos de CF. La última fila es el porcentaje de mejora del MAP sobre la línea base. Los ajustes de parámetros μ = 4, λ = 0.1, α = 0.3 son casi óptimos. Variación de MAP con el número de términos presentados. # términos TFB1C TFB3C TFB6C CFB3C CFB6C TCFB3C TCFB6C 6 0.245 0.240 0.227 0.279 0.279 0.281 0.274 12 0.261 0.261 0.242 0.299 0.286 0.297 0.281 18 0.275 0.274 0.256 0.301 0.282 0.300 0.286 24 0.276 0.281 0.265 0.303 0.292 0.305 0.292 30 0.280 0.285 0.270 0.304 0.296 0.307 0.296 36 0.282 0.288 0.272 0.307 0.297 0.309 0.297 42 0.283 0.288 0.275 0.306 0.298 0.309 0.300 48 0.288 0.288 0.278 0.305 0.301 0.309 0.303 NIST. Diseñamos tres conjuntos de formularios de aclaración para la retroalimentación de términos, diferenciados en la elección de K, el número de grupos, y L, el número de términos presentados de cada grupo. Son: 1 × 48, un gran grupo con 48 términos, 3 × 16, 3 grupos con 16 términos cada uno, y 6 × 8, 6 grupos con 8 términos cada uno. El número total de términos presentados (M) está fijo en 48, por lo que al comparar el rendimiento de diferentes tipos de formas de clarificación podemos conocer los efectos de diferentes grados de agrupamiento. Para cada tema, un evaluador completaría los formularios en el orden de 6 × 8, 1 × 48 y 3 × 16, dedicando hasta tres minutos en cada formulario. El formulario de aclaración de muestra mostrado en la Figura 1 es de tipo 3 × 16. Es una interfaz simple y compacta en la que el usuario puede consultar términos relevantes. El formulario es autoexplicativo; no es necesario brindar capacitación adicional al usuario sobre cómo utilizarlo. Nuestras consultas iniciales se construyen solo utilizando las descripciones de los títulos de los temas, que tienen en promedio 2.7 palabras de longitud. Como base utilizamos el método de recuperación de divergencia KL implementado en la herramienta Lemur con 5 documentos de retroalimentación pseudo. Aplicamos el suavizado de Dirichlet con una prior de 2000, y truncamos los modelos de lenguaje de consulta a 50 términos (estas configuraciones se utilizan en todos los experimentos). Para todos los demás parámetros, utilizamos la configuración predeterminada de Lemurs. El punto de referencia resulta tener un rendimiento por encima del promedio entre los participantes de la pista. Después de una ejecución inicial utilizando este método de recuperación de referencia, tomamos los 60 documentos principales para cada tema y aplicamos el algoritmo de descubrimiento de temas para producir los grupos (1, 3 o 6 de ellos), en base a los cuales generamos formularios de aclaración. Después de recibir la retroalimentación del usuario, ejecutamos los algoritmos de retroalimentación de términos (TFB, CFB o TCFB) para estimar modelos de consulta actualizados, los cuales luego se utilizan para una segunda iteración de recuperación. Evaluamos el rendimiento de los diferentes métodos de recuperación en sus clasificaciones de los 1000 documentos principales. Las métricas de evaluación que adoptamos incluyen la precisión media promedio (no interpolada) (MAP), precisión en los primeros 30 (Pr@30) y total relevante recuperado (RR). La Tabla 2 muestra el rendimiento de varios métodos y configuraciones de K × L. Los sufijos (1C, 3C, 6C) después de TFB, CFB, TCFB representan el número de grupos (K). Por ejemplo, TCFB3C significa el método TCFB en los formularios de aclaración de 3 × 16. De la Tabla 2 podemos hacer las siguientes observaciones: 1 http://www.lemurproject.com 1. Todos los métodos tienen un rendimiento considerablemente mejor que la línea base de pseudoretroalimentación, con TCFB3C logrando una mejora máxima del 41.1% en el MAP, lo que indica una contribución significativa de la retroalimentación de términos para la clarificación de la necesidad de información de los usuarios. En otras palabras, el feedback a término es realmente útil para mejorar la precisión de recuperación. 2. Para TFB, el rendimiento es casi igual en los formularios de aclaración de 1 × 48 y 3 × 16 en términos de MAP (aunque este último es ligeramente mejor en Pr@30 y RR), y un poco peor en los de 6 × 8. 3. Tanto CFB3C como CFB6C tienen un mejor rendimiento que sus contrapartes TFB en las tres métricas, lo que sugiere que el feedback sobre una estructura de clúster secundaria es realmente beneficioso. CFB1C es en realidad peor porque no puede ajustar el peso de su (único) grupo a partir de la retroalimentación de términos y es simplemente pseudoretroalimentación. 4. Aunque TCFB es solo una simple mezcla de TFB y CFB por interpolación, logra superar a ambos. Esto respalda nuestra especulación de que TCFB supera las desventajas de TFB (prestando atención solo a los términos marcados) y CFB (sin distinguir los términos marcados y no marcados en un grupo). Excepto por TCFB6C v.s. CFB6C, la ventaja de rendimiento de TCFB sobre TFB/CFB es significativa con p < 0.05 utilizando la prueba de rango con signo de Wilcoxon. Esto no es cierto en el caso de TFB v.s. CFB, cada uno de los cuales es mejor que el otro en casi la mitad de los temas. 6.2 Reducción de Términos de Presentación En algunas situaciones podemos tener que reducir el número de términos de presentación debido a limitaciones en el espacio de visualización o esfuerzos de retroalimentación del usuario. Es interesante saber si el rendimiento de nuestros algoritmos se deteriora cuando al usuario se le presentan menos términos. Dado que los términos de presentación dentro de cada grupo se generan en orden decreciente de sus frecuencias, la lista de presentación forma un subconjunto del original si su tamaño se reduce2. Por lo tanto, podemos simular fácilmente lo que sucede cuando el número de términos de presentación disminuye 2. Hay complejidades que surgen de los términos que aparecen en la parte superior L de múltiples grupos, pero estas son excepciones de M a M: mantendremos todos los juicios de los términos superiores L = M / K en cada grupo y descartaremos los de los demás. La Tabla 3 muestra el rendimiento de varios algoritmos a medida que el número de términos de presentación varía de 6 a 48. Observamos que el rendimiento de TFB es más susceptible a la reducción de términos de presentación que el de CFB o TCFB. Por ejemplo, en 12 términos el MAP de TFB3C es el 90.6% del que se obtiene en 48 términos, mientras que los números para CFB3C y TCFB3C son del 98.0% y 96.1% respectivamente. Conjeturamos que la razón es que mientras el rendimiento de TFB depende en gran medida de cuántos términos buenos se elijan para la expansión de la consulta, CFB solo necesita una estimación aproximada de los pesos de los clústeres para funcionar. Además, las formas de aclaración de 3 × 16 parecen ser más robustas que las de 6 × 8: con 12 términos, el MAP de TFB6C es el 87.1% del obtenido con 48 términos, por debajo del 90.6% de TFB3C. De manera similar, para CFB es del 95.0% en comparación con el 98.0%. Esto es natural, ya que para un gran número de 6 clusters, es más fácil llegar a la situación en la que cada cluster recibe muy pocos términos de presentación para que la diversificación de temas sea útil. En general, nos sorprende ver que los algoritmos aún pueden funcionar razonablemente bien cuando el número de términos de presentación es pequeño. Por ejemplo, con solo 12 términos, CFB3C (el formulario de aclaración es de tamaño 3 × 4) aún puede mejorar un 36.5% sobre el valor base, disminuyendo ligeramente desde el 39.3% con 48 términos. En el Análisis de Retroalimentación del Usuario, estudiamos varios aspectos del comportamiento de retroalimentación de los usuarios en términos, y si están conectados al rendimiento de recuperación. Figura 2: Distribuciones de tiempo de completación del formulario de aclaración 0−30 30−60 60−90 90−120 120−150 150−180 0 5 10 15 20 25 30 35 tiempo de completación (segundos) #temas 1×48 3×16 6×8 La Figura 2 muestra la distribución del tiempo necesario para completar un formulario de aclaración. Observamos que el usuario suele ser capaz de completar el feedback del término en un tiempo razonablemente corto: para más de la mitad de los temas, el formulario de aclaración se completa en solo 1 minuto, y solo una pequeña fracción de temas (menos del 10% para 1 × 48 y 3 × 16) tarda más de 2 minutos. Esto sugiere que la retroalimentación a corto plazo es adecuada para la recuperación interactiva ad-hoc, donde un usuario generalmente no desea dedicar demasiado tiempo a proporcionar retroalimentación. Observamos que un usuario a menudo comete errores al juzgar la relevancia de un término. A veces, un término relevante puede ser omitido porque su conexión con el tema de la consulta no es obvia para el usuario. En otras ocasiones, puede incluirse un término dudoso que resulta ser irrelevante. Toma como ejemplo el tema en la Figura 1. Hubo un desastre de incendio en Mont 3. El tiempo máximo es de 180 segundos, ya que el evaluador del NIST estaría obligado a enviar el formulario en ese momento. Tabla 4: Estadísticas de selección de términos (promedio del tema) CF Tipo 1 × 48 3 × 16 6 × 8 # términos revisados 14.8 13.3 11.2 # términos relevantes 15.0 12.6 11.2 # términos relevantes revisados 7.9 6.9 5.9 precisión 0.534 0.519 0.527 recuperación 0.526 0.548 0.527 El usuario no seleccionó palabras clave como mont, blanc, francés e italiano debido a su desconocimiento del evento del Túnel del Mont Blanc entre Francia e Italia en 1999. De hecho, sin un contexto adecuado sería difícil hacer un juicio perfecto. ¿Cuál es entonces, el nivel en el que el usuario es bueno dando retroalimentación? ¿Tiene un impacto serio en el rendimiento de recuperación? Para responder a estas preguntas, necesitamos una medida de la verdadera relevancia de los términos individuales. Adoptamos la métrica de Divergencia KL Simplificada utilizada en [24] para decidir los términos de expansión de la consulta como nuestra medida de relevancia de términos: σKLD(w) = p(w|R) log p(w|R) p(w|¬R) donde p(w|R) es la probabilidad de que un documento relevante contenga el término w, y p(w|¬R) es la probabilidad de que un documento irrelevante contenga w, ambas pueden ser fácilmente calculadas mediante una estimación de máxima verosimilitud dada la relevancia a nivel de documento. Si σKLD(w) > 0, w es más probable que aparezca en documentos relevantes que en irrelevantes. Consideramos un término relevante si su valor de Divergencia KL Simplificada es mayor que un umbral determinado σ0. Podemos entonces definir la precisión y la exhaustividad del juicio de términos del usuario de la siguiente manera: la precisión es la fracción de términos revisados por el usuario que son relevantes; la exhaustividad es la fracción de términos relevantes presentados que son revisados por el usuario. La Tabla 4 muestra el número de términos revisados, términos relevantes y términos relevantes revisados cuando σ0 se establece en 1.0, así como la precisión/recuperación del juicio de términos del usuario. Ten en cuenta que cuando los formularios de aclaración contienen más grupos, se revisan menos términos: 14.8 para 1 × 48, 13.3 para 3 × 16 y 11.2 para 6×8. El patrón similar se mantiene para los términos relevantes y los términos relevantes verificados. Parece haber un compromiso entre aumentar la diversidad de temas mediante el agrupamiento y perder términos adicionales relevantes: cuando hay más grupos, cada uno recibe menos términos para presentar, lo que puede perjudicar a un grupo relevante importante que contiene muchos términos relevantes. Por lo tanto, no siempre es útil tener más grupos, por ejemplo, TFB6C es en realidad peor que TFB1C. El hallazgo principal que podemos obtener de la Tabla 4 es que el usuario no es particularmente bueno identificando términos relevantes, lo cual coincide con el descubrimiento en [18]. En el caso de 3 formularios de aclaración de 16, el número promedio de términos marcados como relevantes por el usuario es de 13.3 por tema, y el número promedio de términos relevantes cuyo valor de σKLD excede 1.0 es de 12.6. El usuario es capaz de reconocer solo 6.9 de estos términos en promedio. De hecho, la precisión y la exhaustividad de los términos de retroalimentación de los usuarios (como se definió anteriormente) están lejos de ser perfectas. Por otro lado, si el usuario hubiera verificado correctamente todos esos términos relevantes, el rendimiento de nuestros algoritmos habría aumentado considerablemente, como se muestra en la Tabla 5. Vemos que TFB experimenta una gran mejora cuando hay un oráculo que verifica todos los términos relevantes, mientras que CFB se encuentra con un cuello de botella alrededor de un MAP de 0.325, ya que todo lo que hace es ajustar los pesos de los clústeres, y cuando los pesos aprendidos están cerca de ser precisos, no puede beneficiarse más del feedback de términos. También hay que tener en cuenta que TCFB no logra superar a TFB, probablemente porque TFB es lo suficientemente preciso. Tabla 5: Cambio de MAP al usar todos (y solo) los términos relevantes (σKLD > 1.0) para retroalimentación. retroalimentación de término original retroalimentación de término relevante TF1 0.288 0.354 TF3 0.288 0.354 TF6 0.278 0.346 CF3 0.305 0.325 CF6 0.301 0.326 TCF3 0.309 0.345 TCF6 0.304 0.341 6.4 Comparación con Retroalimentación de Relevancia Ahora comparamos la retroalimentación de términos con la retroalimentación de relevancia a nivel de documento, en la que al usuario se le presentan los N mejores documentos de una recuperación inicial y se le pide que juzgue su relevancia. El proceso de retroalimentación se simula utilizando la evaluación de relevancia de documentos de NIST. Utilizamos el método de retroalimentación basado en el modelo de mezcla propuesto en [25], con el ruido de mezcla establecido en 0.95 y el coeficiente de retroalimentación establecido en 0.9. La evaluación comparativa del feedback de relevancia frente a otros métodos se complica por el hecho de que algunos documentos ya han sido vistos durante el feedback, por lo que no tiene sentido incluirlos en los resultados de recuperación de la segunda ejecución. Sin embargo, esto no se aplica al término retroalimentación. Por lo tanto, para que sea justo en cuanto a la ganancia de información de los usuarios, si los documentos de retroalimentación son relevantes, deben mantenerse en la parte superior del ranking; si son irrelevantes, deben ser excluidos. Por lo tanto, utilizamos retroalimentación de relevancia para producir un ranking de los 1000 documentos recuperados, pero excluyendo cada documento de retroalimentación, y luego añadimos los documentos relevantes de retroalimentación al principio. La tabla 6 muestra el rendimiento de la retroalimentación de relevancia para diferentes valores de N y lo compara con TCFB3C. Tabla 6: Rendimiento de la retroalimentación de relevancia para diferentes números de documentos de retroalimentación (N). Vemos que el rendimiento de TCFB3C es comparable al de la retroalimentación de relevancia utilizando 5 documentos. Aunque es menos eficiente que cuando hay 10 documentos de retroalimentación en términos de MAP y Pr@30, sí recupera más documentos (4947) al descender por la lista clasificada. Intentamos comparar la calidad de los términos insertados automáticamente en la retroalimentación de relevancia con la de los términos seleccionados manualmente en la retroalimentación de términos. Esto se logra truncando el modelo de consulta modificado por retroalimentación de relevancia a un tamaño igual al número de términos verificados para el mismo tema. Podemos comparar los términos en el modelo truncado con los términos verificados. La Figura 3 muestra la distribución de los puntajes σKLD de los términos. Observamos que el término \"feedback\" tiende a producir términos de expansión de mayor calidad (aquellos con σKLD > 1) en comparación con el \"relevance feedback\" (con 10 documentos de feedback). Esto no contradice el hecho de que este último produce un rendimiento de recuperación más alto. De hecho, cuando usamos el modelo de consulta truncada en lugar del modelo íntegro refinado a partir de la retroalimentación de relevancia, el MAP es solo 0.304. La verdad Figura 3: Comparación de la calidad del término de expansión entre retroalimentación de relevancia (con 10 documentos de retroalimentación) y retroalimentación de términos (con 3 × 16 CFs) −1−0 0−1 1−2 2−3 3−4 4−5 5−6 0 50 100 150 200 250 300 350 σKLD #términos retroalimentación de relevancia retroalimentación de términos es que, aunque hay muchos términos no deseados en el modelo de consulta expandido a partir de los documentos de retroalimentación, también hay más términos relevantes de los que el usuario puede seleccionar de la lista de términos de presentación generados con documentos de pseudo-retroalimentación, y los efectos positivos a menudo superan a los negativos. Estamos interesados en saber en qué circunstancias la retroalimentación basada en términos tiene ventajas sobre la retroalimentación basada en relevancia. Una de esas situaciones es cuando ninguno de los documentos de retroalimentación principales es relevante, lo que hace que la retroalimentación de relevancia sea inútil. Esto no es infrecuente, como uno podría haber pensado: de los 50 temas, hay 13 casos así cuando N = 5, 10 cuando N = 10, y aún 3 cuando N = 20. Cuando esto sucede, solo se puede retroceder al método original de recuperación; se pierde el poder de la retroalimentación de relevancia. Sorprendentemente, en 11 de 13 casos en los que parece imposible el feedback de relevancia, el usuario puede marcar al menos 2 términos relevantes de los 3 × 16 del formulario de aclaración (consideramos que el término t es relevante si σKLD(t) > 1.0). Además, en 10 de ellos TCFB3C supera la línea base de pseudo-retroalimentación, aumentando el MAP de 0.076 a 0.146 en promedio (estos son temas particularmente difíciles). Creemos que hay dos posibles explicaciones para este fenómeno de que la retroalimentación de términos esté activa incluso cuando la retroalimentación de relevancia no funciona: Primero, incluso si ninguno de los primeros N (supongamos que es un número pequeño) documentos son relevantes, aún podemos encontrar documentos relevantes en los primeros 60, que son más inclusivos pero generalmente inalcanzables cuando las personas están realizando retroalimentación de relevancia en la búsqueda interactiva ad-hoc, de donde podemos extraer términos de retroalimentación. Esto es cierto para el tema 367 de piratería, donde los 10 documentos principales de retroalimentación son todos sobre piratería de software, sin embargo, hay documentos entre el 10 y el 60 que tratan sobre piratería en los mares (que es la necesidad de información real), contribuyendo términos como pirata, barco para la selección en el formulario de aclaración. Segundo, para algunos temas, un documento debe cumplir con ciertas condiciones especiales para ser relevante. Los N documentos principales pueden estar relacionados con el tema, pero no obstante ser irrelevantes. En este caso, aún podemos extraer términos útiles de estos documentos, incluso si no califican como relevantes. Por ejemplo, en el tema 639 de compras en línea de consumidores, un documento necesita mencionar qué contribuye al crecimiento de las compras para realmente coincidir con la necesidad de información especificada, por lo tanto, ninguno de los 10 documentos de retroalimentación principales se considera relevante. Sin embargo, los términos de retroalimentación como venta al por menor, comercio son buenos para la expansión de consultas. 7. CONCLUSIONES En este artículo estudiamos el uso de la retroalimentación de términos para la recuperación interactiva de información en el enfoque de modelado del lenguaje. Propusimos un método basado en clusters para seleccionar términos de presentación, así como algoritmos para estimar modelos de consulta refinados a partir de la retroalimentación de términos de usuario. Observamos una mejora significativa en la precisión de recuperación proporcionada por la retroalimentación de términos, a pesar de que un usuario a menudo comete errores en la evaluación de relevancia que perjudican su rendimiento. Encontramos que el algoritmo de mejor rendimiento es TCFB, el cual se beneficia de la combinación de la evidencia de términos observados directamente con TFB y la relevancia de clúster aprendida indirectamente con CFB. Cuando redujimos el número de términos de presentación, el feedback del término aún puede mantener gran parte de su mejora de rendimiento sobre el valor base. Finalmente, comparamos la retroalimentación de términos con la retroalimentación de relevancia a nivel de documento, y encontramos que el rendimiento de TCFB3C está a la par con este último con 5 documentos de retroalimentación. Consideramos el término \"feedback\" como una alternativa viable al feedback de relevancia tradicional, especialmente cuando no hay documentos relevantes en la parte superior. Proponemos ampliar nuestro trabajo de varias maneras. Primero, queremos estudiar si el uso de varios contextos puede ayudar al usuario a identificar mejor la relevancia de los términos, sin sacrificar la simplicidad y la concisión de la retroalimentación de los términos. Segundo, actualmente todos los términos se presentan al usuario en un solo lote. En su lugar, podríamos considerar la retroalimentación iterativa de términos, presentando primero un pequeño número de términos y mostrando más términos después de recibir la retroalimentación del usuario o detenernos cuando la consulta refinada sea lo suficientemente buena. Los términos presentados deben ser seleccionados dinámicamente para maximizar los beneficios de aprendizaje en cualquier momento. Tercero, tenemos planes de incorporar retroalimentación de términos en nuestra barra de herramientas UCAIR[20], un complemento de Internet Explorer, para que funcione en búsquedas web. También estamos interesados en estudiar cómo combinar la retroalimentación de términos con la retroalimentación de relevancia o la retroalimentación implícita. Por ejemplo, podríamos permitir al usuario modificar dinámicamente los términos en un modelo de lenguaje aprendido a partir de documentos de retroalimentación. 8. AGRADECIMIENTO Este trabajo cuenta con el apoyo parcial de las becas de la Fundación Nacional de Ciencias IIS-0347933 e IIS-0428472. 9. REFERENCIAS [1] J. Allan. Retroalimentación de relevancia con demasiados datos. En Actas de la 18ª conferencia internacional anual de ACM SIGIR sobre investigación y desarrollo en recuperación de información, páginas 337-343, 1995. [2] J. Allan. Resumen de la pista HARD en TREC 2005 - Recuperación de alta precisión de documentos. En la Decimocuarta Conferencia de Recuperación de Información de 2005. [3] P. Anick. Utilizando retroalimentación terminológica para refinar la búsqueda web: un estudio basado en registros. En Actas de la 26ª conferencia internacional anual de ACM SIGIR sobre investigación y desarrollo en recuperación de información, páginas 88-95, 2003. [4] P. G. Anick y S. Tipirneni. El asistente de búsqueda de paráfrasis: retroalimentación terminológica para la búsqueda iterativa de información. En Actas de la 22ª conferencia internacional anual de ACM SIGIR sobre investigación y desarrollo en recuperación de información, páginas 153-159, 1999. [5] C. Buckley, G. Salton, J. Allan y A. Singhal. Expansión automática de consultas utilizando SMART. En Actas de la Tercera Conferencia de Recuperación de Texto, 1994. [6] D. Harman. Hacia la expansión interactiva de consultas. En Actas de la 11ª conferencia internacional anual de ACM SIGIR sobre investigación y desarrollo en recuperación de información, páginas 321-331, 1988. [7] N. A. Jaleel, A. Corrada-Emmanuel, Q. Li, X. Liu, C. Wade y J. Allan. UMass en TREC 2003: HARD y QA. En TREC, páginas 715-725, 2003. [8] H. Joho, C. Coverson, M. Sanderson y M. Beaulieu. Presentación jerárquica de términos de expansión. En Actas del simposio de informática aplicada de ACM de 2002, páginas 645-649, 2002. [9] K. S. Jones, S. Walker y S. E. Robertson. Un modelo probabilístico de recuperación de información: desarrollo y estado actual. Informe técnico 446, Laboratorio de Computación, Universidad de Cambridge, 1998. [10] D. Kelly, V. D. Dollu y X. Fu. El usuario locuaz: una fuente de términos independiente del documento para la expansión de consultas. En Actas de la 28ª conferencia internacional anual de ACM SIGIR sobre investigación y desarrollo en recuperación de información, páginas 457-464, 2005. [11] D. Kelly y X. Fu. Obtención de retroalimentación de relevancia de términos: una investigación sobre la fuente y el contexto de los términos. En Actas de la 29ª conferencia internacional anual de ACM SIGIR sobre investigación y desarrollo en recuperación de información, 2006. [12] J. Koenemann y N. Belkin. Un caso para la interacción: Un estudio del comportamiento y la efectividad de la recuperación de información interactiva. En Actas de la conferencia SIGCHI sobre factores humanos en sistemas informáticos, páginas 205-212, 1996. [13] V. Lavrenko y W. B. Croft. Modelos de lenguaje basados en relevancia. En Investigación y Desarrollo en Recuperación de Información, páginas 120-127, 2001. [14] Y. Nemeth, B. Shapira y M. Taeib-Maimon. Evaluación del valor real y percibido de la expansión automática e interactiva de consultas. En Actas de la 27ª conferencia internacional anual de ACM SIGIR sobre investigación y desarrollo en recuperación de información, páginas 526-527, 2004. [15] J. Ponte. Un enfoque de modelado del lenguaje para la recuperación de información. Tesis doctoral, Universidad de Massachusetts en Amherst, 1998. [16] S. E. Robertson, S. Walker, S. Jones, M. Beaulieu y M. Gatford. Okapi en TREC-3. En Actas de la Tercera Conferencia de Recuperación de Texto, 1994. [17] J. Rocchio. Retroalimentación de relevancia en la recuperación de información. En el sistema de recuperación SMART, páginas 313-323. 1971. [18] I. Ruthven. Reexaminando la efectividad potencial de la expansión interactiva de consultas. En Actas de la 26ª conferencia internacional anual de ACM SIGIR sobre investigación y desarrollo en recuperación de información, páginas 213-220, 2003. [19] G. Salton y C. Buckley. Mejorando el rendimiento de recuperación mediante retroalimentación de relevancia. Revista de la Sociedad Americana de Ciencia de la Información, 41:288-297, 1990. [20] X. Shen, B. Tan y C. Zhai. Modelado implícito de usuario para búsqueda personalizada. En Actas de la 14ª conferencia internacional de ACM sobre información y gestión del conocimiento, páginas 824-831, 2005. [21] X. Shen y C. Zhai. Retroalimentación activa en la recuperación de información ad-hoc. En Actas de la 28ª conferencia internacional anual de ACM SIGIR sobre investigación y desarrollo en recuperación de información, páginas 59-66, 2005. [22] A. Spink. Retroalimentación de relevancia de términos y expansión de consultas: relación con el diseño. En Actas de la 17ª conferencia internacional anual de ACM SIGIR sobre investigación y desarrollo en recuperación de información, páginas 81-90, 1994. [23] J. Xu y W. B. Croft. Expansión de consulta utilizando análisis local y global de documentos. En Actas de la 19ª conferencia internacional anual de ACM SIGIR sobre investigación y desarrollo en recuperación de información, páginas 4-11, 1996. [24] H. Zaragoza, N. Craswell, M. Taylor, S. Saria y S. Robertson. Microsoft Cambridge en TREC-13: pistas Web y HARD. En Actas de la 13ª Conferencia de Recuperación de Información de Texto, 2004. [25] C. Zhai y J. Lafferty. Retroalimentación basada en modelos en el enfoque de modelado del lenguaje para la recuperación de información. En Actas de la décima conferencia internacional sobre gestión de la información y el conocimiento, páginas 403-410, 2001. [26] C. Zhai, A. Velivelli y B. Yu. Un modelo de mezcla de colecciones cruzadas para la minería de textos comparativa. En Actas de la décima conferencia internacional de ACM SIGKDD sobre descubrimiento de conocimiento y minería de datos, páginas 743-748, 2004.",
    "original_sentences": [
        "Term Feedback for Information Retrieval with Language Models Bin Tan† , Atulya Velivelli‡ , Hui Fang† , ChengXiang Zhai† Dept.",
        "of Computer Science† , Dept. of Electrical and Computer Engineering‡ University of Illinois at Urbana-Champaign bintan@cs.uiuc.edu, velivell@ifp.uiuc.edu, hfang@cs.uiuc.edu, czhai@cs.uiuc.edu ABSTRACT In this paper we study term-based feedback for information retrieval in the language modeling approach.",
        "With term feedback a user directly judges the relevance of individual terms without interaction with feedback documents, taking full control of the query expansion process.",
        "We propose a cluster-based method for selecting terms to present to the user for judgment, as well as effective algorithms for constructing refined query language models from user term feedback.",
        "Our algorithms are shown to bring significant improvement in retrieval accuracy over a non-feedback baseline, and achieve comparable performance to relevance feedback.",
        "They are helpful even when there are no relevant documents in the top.",
        "Categories and Subject Descriptors H.3.3 [Information Search and Retrieval]: Retrieval models General Terms Algorithms 1.",
        "INTRODUCTION In the language modeling approach to information retrieval, feedback is often modeled as estimating an improved query model or relevance model based on a set of feedback documents [25, 13].",
        "This is in line with the traditional way of doing relevance feedback - presenting a user with documents/passages for relevance judgment and then extracting terms from the judged documents or passages to expand the initial query.",
        "It is an indirect way of seeking users assistance for query model construction, in the sense that the refined query model (based on terms) is learned through feedback documents/passages, which are high-level structures of terms.",
        "It has the disadvantage that irrelevant terms, which occur along with relevant ones in the judged content, may be erroneously used for query expansion, causing undesired effects.",
        "For example, for the TREC query Hubble telescope achievements, when a relevant document talks more about the telescopes repair than its discoveries, irrelevant terms such as spacewalk can be added into the modified query.",
        "We can consider a more direct way to involve a user in query model improvement, without an intermediary step of document feedback that can introduce noise.",
        "The idea is to present a (reasonable) number of individual terms to the user and ask him/her to judge the relevance of each term or directly specify their probabilities in the query model.",
        "This strategy has been discussed in [15], but to our knowledge, it has not been seriously studied in existing language modeling literature.",
        "Compared to traditional relevance feedback, this term-based approach to interactive query model refinement has several advantages.",
        "First, the user has better control of the final query model through direct manipulation of terms: he/she can dictate which terms are relevant, irrelevant, and possibly, to what degree.",
        "This avoids the risk of bringing unwanted terms into the query model, although sometimes the user introduces low-quality terms.",
        "Second, because a term takes less time to judge than a documents full text or summary, and as few as around 20 presented terms can bring significant improvement in retrieval performance (as we will show later), term feedback makes it faster to gather user feedback.",
        "This is especially helpful for interactive adhoc search.",
        "Third, sometimes there are no relevant documents in the top N of the initially retrieved results if the topic is hard.",
        "This is often true when N is constrained to be small, which arises from the fact that the user is unwilling to judge too many documents.",
        "In this case, relevance feedback is useless, as no relevant document can be leveraged on, but term feedback is still often helpful, by allowing relevant terms to be picked from irrelevant documents.",
        "During our participation in the TREC 2005 HARD Track and continued study afterward, we explored how to exploit term feedback from the user to construct improved query models for information retrieval in the language modeling approach.",
        "We identified two key subtasks of term-based feedback, i.e., pre-feedback presentation term selection and post-feedback query model construction, with effective algorithms developed for both.",
        "We imposed a secondary cluster structure on terms and found that a cluster view sheds additional insight into the users information need, and provides a good way of utilizing term feedback.",
        "Through experiments we found that term feedback improves significantly over the nonfeedback baseline, even though the user often makes mistakes in relevance judgment.",
        "Among our algorithms, the one with best retrieval performance is TCFB, the combination of TFB, the direct term feedback algorithm, and CFB, the cluster-based feedback algorithm.",
        "We also varied the number of feedback terms and observed reasonable improvement even at low numbers.",
        "Finally, by comparing term feedback with document-level feedback, we found it to be a viable alternative to the latter with competitive retrieval performance.",
        "The rest of the paper is organized as follows.",
        "Section 2 discusses some related work.",
        "Section 4 outlines our general approach to term feedback.",
        "We present our method for presentation term selection in Section 3 and algorithms for query model construction in Section 5.",
        "The experiment results are given in Section 6.",
        "Section 7 concludes this paper. 2.",
        "RELATED WORK Relevance feedback[17, 19] has long been recognized as an effective method for improving retrieval performance.",
        "Normally, the top N documents retrieved using the original query are presented to the user for judgment, after which terms are extracted from the judged relevant documents, weighted by their potential of attracting more relevant documents, and added into the query model.",
        "The expanded query usually represents the users information need better than the original one, which is often just a short keyword query.",
        "A second iteration of retrieval using this modified query usually produces significant increase in retrieval accuracy.",
        "In cases where true relevance judgment is unavailable and all top N documents are assumed to be relevant, it is called blind or pseudo feedback[5, 16] and usually still brings performance improvement.",
        "Because document is a large text unit, when it is used for relevance feedback many irrelevant terms can be introduced into the feedback process.",
        "To overcome this, passage feedback is proposed and shown to improve feedback performance[1, 23].",
        "A more direct solution is to ask the user for their relevance judgment of feedback terms.",
        "For example, in some relevance feedback systems such as [12], there is an interaction step that allows the user to add or remove expansion terms after they are automatically extracted from relevant documents.",
        "This is categorized as interactive query expansion, where the original query is augmented with user-provided terms, which can come from direct user input (free-form text or keywords)[22, 7, 10] or user selection of system-suggested terms (using thesauri[6, 22] or extracted from feedback documents[6, 22, 12, 4, 7]).",
        "In many cases term relevance feedback has been found to effectively improve retrieval performance[6, 22, 12, 4, 10].",
        "For example, the study in [12] shows that the user prefers to have explicit knowledge and direct control of which terms are used for query expansion, and the penetrable interface that provides this freedom is shown to perform better than other interfaces.",
        "However, in some other cases there is no significant benefit[3, 14], even if the user likes interacting with expansion terms.",
        "In a simulated study carried out in [18], the author compares the retrieval performance of interactive query expansion and automatic query expansion with a simulated study, and suggests that the potential benefits of the former can be hard to achieve.",
        "The user is found to be not good at identifying useful terms for query expansion, when a simple term presentation interface is unable to provide sufficient semantic context of the feedback terms.",
        "Our work differs from the previous ones in two important aspects.",
        "First, when we choose terms to present to the user for relevance judgment, we not only consider single-term value (e.g., the relative frequency of a term in the top documents, which can be measured by metrics such as Robertson Selection Value and Simplified Kullback-Leibler Distance as listed in [24]), but also examine the cluster structure of the terms, so as to produce a balanced coverage of the different topic aspects.",
        "Second, with the language modelling framework, we allow an elaborate construction of the updated query model, by setting different probabilities for different terms based on whether it is a query term, its significance in the top documents, and its cluster membership.",
        "Although techniques for adjusting query term weights exist for vector space models[17] and probablistic relevance models[9], most of the aforementioned works do not use them, choosing to just append feedback terms to the original query (thus using equal weights for them), which can lead to poorer retrieval performance.",
        "The combination of the two aspects allows our method to perform much better than the baseline.",
        "The usual way for feedback term presentation is just to display the terms in a list.",
        "There have been some works on alternative user interfaces. [8] arranges terms in a hierarchy, and [11] compares three different interfaces, including terms + checkboxes, terms + context (sentences) + checkboxes, sentences + input text box.",
        "In both studies, however, there is no significant performance difference.",
        "In our work we adopt the simplest approach of terms + checkboxes.",
        "We focus on term presentation and query model construction from feedback terms, and believe using contexts to improve feedback term quality should be orthogonal to our method. 3.",
        "GENERAL APPROACH We follow the language modeling approach, and base our method on the KL-divergence retrieval model proposed in [25].",
        "With this model, the retrieval task involves estimating a query language model θq from a given query, a document language model θd from each document, and calculating their KL-divergence D(θq||θd), which is then used to score the documents. [25] treats relevance feedback as a query model re-estimation problem, i.e., computing an updated query model θq given the original query text and the extra evidence carried by the judged relevant documents.",
        "We adopt this view, and cast our task as updating the query model from user term feedback.",
        "There are two key subtasks here: First, how to choose the best terms to present to the user for judgment, in order to gather maximal evidence about the users information need.",
        "Second, how to compute an updated query model based on this term feedback evidence, so that it captures the users information need and translates into good retrieval performance. 4.",
        "PRESENTATION TERM SELECTION Proper selection of terms to be presented to the user for judgment is crucial to the success of term feedback.",
        "If the terms are poorly chosen and there are few relevant ones, the user will have a hard time looking for useful terms to help clarify his/her information need.",
        "If the relevant terms are plentiful, but all concentrate on a single aspect of the query topic, then we will only be able to get feedback on that aspect and missing others, resulting in a breadth loss in retrieved results.",
        "Therefore, it is important to carefully select presentation terms to maximize expected gain from user feedback, i.e., those that can potentially reveal most evidence of the users information need.",
        "This is similar to active feedback[21], which suggests that a retrieval system should actively probe the users information need, and in the case of relevance feedback, the feedback documents should be chosen to maximize learning benefits (e.g. diversely so as to increase coverage).",
        "In our approach, the top N documents from an initial retrieval using the original query form the source of feedback terms: all terms that appear in them are considered candidates to present to the user.",
        "These documents serve as pseudo-feedback, since they provide a much richer context than the original query (usually very short), while the user is not asked to judge their relevance.",
        "Due to the latter reason, it is possible to make N quite large (e.g., in our experiments we set N = 60) to increase its coverage of different aspects in the topic.",
        "The simplest way of selecting feedback terms is to choose the most frequent M terms from the N documents.",
        "This method, however, has two drawbacks.",
        "First, a lot of common noisy terms will be selected due to their high frequencies in the document collection, unless a stop-word list is used for filtering.",
        "Second, the presentation list will tend to be filled by terms from major aspects of the topic; those from a minor aspect are likely to be missed due to their relatively low frequencies.",
        "We solve the above problems by two corresponding measures.",
        "First, we introduce a background model θB that is estimated from collection statistics and explains the common terms, so that they are much less likely to appear in the presentation list.",
        "Second, the terms are selected from multiple clusters in the pseudo-feedback documents, to ensure sufficient representation of different aspects of the topic.",
        "We rely on the mixture multinomial model, which is used for theme discovery in [26].",
        "Specifically, we assume the N documents contain K clusters {Ci| i = 1, 2, · · · K}, each characterized by a multinomial word distribution (also known as unigram language model) θi and corresponding to an aspect of the topic.",
        "The documents are regarded as sampled from a mixture of K + 1 components, including the K clusters and the background model: p(w|d) = λBp(w|θB) + (1 − λB) K i=1 πd,ip(w|θi) where w is a word, λB is the mixture weight for the background model θB, and πd,i is the document-specific mixture weight for the i-th cluster model θi.",
        "We then estimate the cluster models by maximizing the probability of the pseudo-feedback documents being generated from the multinomial mixture model: log p(D|Λ) = d∈D w∈V c(w; d) log p(w|d) where D = {di| i = 1, 2, · · · N} is the set of the N documents, V is the vocabulary, c(w; d) is ws frequency in d and Λ = {θi| i = 1, 2, · · · K} ∪ {πdij | i = 1, 2, · · · N, j = 1, 2, · · · K} is the set of model parameters to estimate.",
        "The cluster models can be efficiently estimated using the Expectation-Maximization (EM) algorithm.",
        "For its details, we refer the reader to [26].",
        "Table 1 shows the cluster models for TREC query Transportation tunnel disasters (K = 3).",
        "Note that only the middle cluster is relevant.",
        "Table 1: Cluster models for topic 363 Transportation tunnel disasters Cluster 1 Cluster 2 Cluster 3 tunnel 0.0768 tunnel 0.0935 tunnel 0.0454 transport 0.0364 fire 0.0295 transport 0.0406 traffic 0.0206 truck 0.0236 toll 0.0166 railwai 0.0186 french 0.0220 amtrak 0.0153 harbor 0.0146 smoke 0.0157 train 0.0129 rail 0.0140 car 0.0154 airport 0.0122 bridg 0.0139 italian 0.0152 turnpik 0.0105 kilomet 0.0136 firefight 0.0144 lui 0.0095 truck 0.0133 blaze 0.0127 jersei 0.0093 construct 0.0131 blanc 0.0121 pass 0.0087 · · · · · · · · · From each of the K estimated clusters, we choose the L = M/K terms with highest probabilities to form a total of M presentation terms.",
        "If a term happens to be in top L in multiple clusters, we assign it to the cluster where it has highest probability and let the other clusters take one more term as compensation.",
        "We also filter out terms in the original query text because they tend to always be relevant when the query is short.",
        "The selected terms are then presented to the user for judgment.",
        "A sample (completed) feedback form is shown in Figure 1.",
        "In this study we only deal with binary judgment: a presented term is by default unchecked, and a user may check it to indicate relevance.",
        "We also do not explicitly exploit negative feedback (i.e., penalizing irrelevant terms), because with binary feedback an unchecked term is not necessarily irrelevant (maybe the user is unsure about its relevance).",
        "We could ask the user for finer judgment (e.g., choosing from highly relevant, somewhat relevant, do not know, somewhat irrelevant and highly irrelevant), but binary feedback is more compact, taking less space to display and less user effort to make judgment. 5.",
        "ESTIMATING QUERY MODELS FROM TERM FEEDBACK In this section, we present several algorithms for exploiting term feedback.",
        "The algorithms take as input the original query q, the clusters {θi} as generated by the theme discovery algorithm, the set of feedback terms T and their relevance judgment R, and outputs an updated query language model θq that makes best use of the feedback evidence to capture the users information need.",
        "First we describe our notations: • θq: The original query model, derived from query terms only: p(w|θq) = c(w; q) |q| where c(w; q) is the count of w in q, and |q| = w∈q c(w; q) is the query length. • θq : The updated query model which we need to estimate from term feedback. • θi (i = 1, 2, . . .",
        "K): The unigram language model of cluster Ci, as estimated using the theme discovery algorithm. • T = {ti,j} (i = 1 . . .",
        "K, j = 1 . . .",
        "L): The set of terms presented to the user for judgment. ti,j is the j-th term chosen from cluster Ci. • R = {δw|w ∈ T}: δw is an indicator variable that is 1 if w is judged relevant or 0 otherwise. 5.1 TFB (Direct Term Feedback) This is a straight-forward form of term feedback that does not involve any secondary structure.",
        "We give a weight of 1 to terms judged relevant by the user, a weight of μ to query terms, zero weight to other terms, and then apply normalization: p(w|θq ) = δw + μ c(w; q) w ∈T δw + μ|q| where w ∈T δw is the total number of terms that are judged relevant.",
        "We call this method TFB (direct Term FeedBack).",
        "If we let μ = 1, this approach is equivalent to appending the relevant terms after the original query, which is what standard query expansion (without term reweighting) does.",
        "If we set μ > 1, we are putting more emphasis on the query terms than the checked ones.",
        "Note that the result model will be more biased toward θq if the original query is long or the user feedback is weak, which makes sense, as we can trust more on the original query in either case.",
        "Figure 1: Filled clarification form for Topic 363 363 transportation tunnel disasters Please select all terms that are relevant to the topic. traffic railway harbor rail bridge kilometer construct swiss cross link kong hong river project meter shanghai fire truck french smoke car italian firefights blaze blanc mont victim franc rescue driver chamonix emerge toll amtrak train airport turnpike lui jersey pass rome z center electron road boston speed bu submit 5.2 CFB (Cluster Feedback) Here we exploit the cluster structure that played an important role when we selected the presentation terms.",
        "The clusters represent different aspects of the query topic, each of which may or may not be relevant.",
        "If we are able to identify the relevant clusters, we can combine them to generate a query model that is good at discovering documents belonging to these clusters (instead of the irrelevant ones).",
        "We could ask the user to directly judge the relevance of a cluster after viewing representative terms in that cluster, but this would sometimes be a difficult task for the user, who has to guess the semantics of a cluster via its set of terms, which may not be well connected to one another due to a lack of context.",
        "Therefore, we propose to learn cluster feedback indirectly, inferring the relevance of a cluster through the relevance of its feedback terms.",
        "Because each cluster has an equal number of terms presented to the user, the simplest measure of a clusters relevance is the number of terms that are judged relevant in it.",
        "Intuitively, the more terms are marked relevant in a cluster, the closer the cluster is to the query topic, and the more the cluster should participate in query modification.",
        "If we combine the cluster models using weights determined this way and then interpolate with the original query model, we get the following formula for query updating, which we call CFB (Cluster FeedBack): p(w|θq ) = λp(w|θq) + (1 − λ) K i=1 L j=1 δti,j K k=1 L j=1 δtk,j p(w|θi) where L j=1 δti,j is the number of relevant terms in cluster Ci, and K k=1 L j=1 δtk,j is the total number of relevant terms.",
        "We note that when there is only one cluster (K = 1), the above formula degenerates to p(w|θq ) = λp(w|θq) + (1 − λ)p(w|θ1) which is merely pseudo-feedback of the form proposed in [25]. 5.3 TCFB (Term-cluster Feedback) TFB and CFB both have their drawbacks.",
        "TFB assigns non-zero probabilities to the presented terms that are marked relevant, but completely ignores (a lot more) others, which may be left unchecked due to the users ignorance, or simply not included in the presentation list, but we should be able to infer their relevance from the checked ones.",
        "For example, in Figure 1, since as many as 5 terms in the middle cluster (the third and fourth columns) are checked, we should have high confidence in the relevance of other terms in that cluster.",
        "CFB remedies TFBs problem by treating the terms in a cluster collectively, so that unchecked/unpresented terms receive weights when presented terms in their clusters are judged as relevant, but it does not distinguish which terms in a cluster are presented or judged.",
        "Intuitively, the judged relevant terms should receive larger weights because they are explicitly indicated as relevant by the user.",
        "Therefore, we try to combine the two methods, hoping to get the best out of both.",
        "We do this by interpolating the TFB model with the CFB model, and call it TCFB: p(w|θq ) = αp(w|θqT F B ) + (1 − α)p(w|θqCF B ) 6.",
        "EXPERIMENTS In this section, we describe our experiment results.",
        "We first describe our experiment setup and present an overview of various methods performance.",
        "Then we discuss the effects of varying the parameter setting in the algorithms, as well as the number of presentation terms.",
        "Next we analyze user term feedback behavior and its relation to retrieval performance.",
        "Finally we compare term feedback to relevance feedback and show that it has its particular advantage. 6.1 Experiment Setup and Basic Results We took the opportunity of TREC 2005 HARD Track[2] for the evaluation of our algorithms.",
        "The tracks used the AQUAINT collection, a 3GB corpus of English newswire text.",
        "The topics included 50 ones previously known to be hard, i.e. with low retrieval performance.",
        "It is for these hard topics that user feedback is most helpful, as it can provide information to disambiguate the queries; with easy topics the user may be unwilling to spend efforts for feedback if the automatic retrieval results are good enough.",
        "Participants of the track were able to submit custom-designed clarification forms (CF) to solicit feedback from human assessors provided by Table 2: Retrieval performance for different methods and CF types.",
        "The last row is the percentage of MAP improvement over the baseline.",
        "The parameter settings μ = 4, λ = 0.1, α = 0.3 are near optimal.",
        "Baseline TFB1C TFB3C TFB6C CFB1C CFB3C CFB6C TCFB1C TCFB3C TCFB6C MAP 0.219 0.288 0.288 0.278 0.254 0.305 0.301 0.274 0.309 0.304 Pr@30 0.393 0.467 0.475 0.457 0.399 0.480 0.473 0.431 0.491 0.473 RR 4339 4753 4762 4740 4600 4907 4872 4767 4947 4906 % 0% 31.5% 31.5% 26.9% 16.0% 39.3% 37.4% 25.1% 41.1% 38.8% Table 3: MAP variation with the number of presented terms. # terms TFB1C TFB3C TFB6C CFB3C CFB6C TCFB3C TCFB6C 6 0.245 0.240 0.227 0.279 0.279 0.281 0.274 12 0.261 0.261 0.242 0.299 0.286 0.297 0.281 18 0.275 0.274 0.256 0.301 0.282 0.300 0.286 24 0.276 0.281 0.265 0.303 0.292 0.305 0.292 30 0.280 0.285 0.270 0.304 0.296 0.307 0.296 36 0.282 0.288 0.272 0.307 0.297 0.309 0.297 42 0.283 0.288 0.275 0.306 0.298 0.309 0.300 48 0.288 0.288 0.278 0.305 0.301 0.309 0.303 NIST.",
        "We designed three sets of clarification forms for term feedback, differing in the choice of K, the number of clusters, and L, the number of presented terms from each cluster.",
        "They are: 1× 48, a big cluster with 48 terms, 3 × 16, 3 clusters with 16 terms each, and 6 × 8, 6 clusters with 8 terms each.",
        "The total number of presented terms (M) is fixed at 48, so by comparing the performance of different types of clarification forms we can know the effects of different degree of clustering.",
        "For each topic, an assessor would complete the forms ordered by 6 × 8, 1 × 48 and 3 × 16, spending up to three minutes on each form.",
        "The sample clarification form shown in Figure 1 is of type 3 × 16.",
        "It is a simple and compact interface in which the user can check relevant terms.",
        "The form is self-explanatory; there is no need for extra user training on how to use it.",
        "Our initinal queries are constructed only using the topic title descriptions, which are on average 2.7 words in length.",
        "As our baseline we use the KL divergence retrieval method implemented in the Lemur Toolkit1 with 5 pseudo-feedback documents.",
        "We stem the terms, choose Dirichlet smoothing with a prior of 2000, and truncate query language models to 50 terms (these settings are used throughout the experiments).",
        "For all other parameters we use Lemurs default settings.",
        "The baseline turns out to perform above average among the track participants.",
        "After an initial run using this baseline retrieval method, we take the top 60 documents for each topic and apply the theme discovery algorithm to output the clusters (1, 3, or 6 of them), based on which we generate clarification forms.",
        "After user feedback is received, we run the term feedback algorithms (TFB, CFB or TCFB) to estimate updated query models, which are then used for a second iteration of retrieval.",
        "We evaluate the different retrieval methods performance on their rankings of the top 1000 documents.",
        "The evaluation metrics we adopt include mean average (non-interpolated) precision (MAP), precision at top 30 (Pr@30) and total relevant retrieved (RR).",
        "Table 2 shows the performance of various methods and configurations of K × L. The suffixes (1C, 3C, 6C) after TFB,CFB,TCFB stand for the number of clusters (K).",
        "For example, TCFB3C means the TCFB method on the 3 × 16 clarification forms.",
        "From Table 2 we can make the following observations: 1 http://www.lemurproject.com 1.",
        "All methods perform considerably better than the pseudofeedback baseline, with TCFB3C achieving a highest 41.1% improvement in MAP, indicating significant contribution of term feedback for clarification of the users information need.",
        "In other words, term feedback is truly helpful for improving retrieval accuracy. 2.",
        "For TFB, the performance is almost equal on the 1 × 48 and 3 × 16 clarification forms in terms of MAP (although the latter is slightly better in Pr@30 and RR), and a little worse on the 6 × 8 ones. 3.",
        "Both CFB3C and CFB6C perform better than their TFB counterparts in all three metrics, suggesting that feedback on a secondary cluster structure is indeed beneficial.",
        "CFB1C is actually worse because it cannot adjust the weight of its (single) cluster from term feedback and it is merely pseudofeedback. 4.",
        "Although TCFB is just a simple mixture of TFB and CFB by interpolation, it is able to outperform both.",
        "This supports our speculation that TCFB overcomes the drawbacks of TFB (paying attention only to checked terms) and CFB (not distinguishing checked and unchecked terms in a cluster).",
        "Except for TCFB6C v.s.",
        "CFB6C, the performance advantage of TCFB over TFB/CFB is significant at p < 0.05 using the Wilcoxon signed rank test.",
        "This is not true in the case of TFB v.s.",
        "CFB, each of which is better than the other in nearly half of the topics. 6.2 Reduction of Presentation Terms In some situations we may have to reduce the number of presentation terms due to limits in display space or user feedback efforts.",
        "It is interesting to know whether our algorithms performance deteriorates when the user is presented with fewer terms.",
        "Because the presentation terms within each cluster are generated in decreasing order of their frequencies, the presentation list forms a subset of the original one if its size is reduced2 .",
        "Therefore, we can easily simulate what happens when the number of presentation terms decreases 2 There are complexities arising from terms appearing in top L of multiple clusters, but these are exceptions from M to M : we will keep all judgments of the top L = M /K terms in each cluster and discard those of others.",
        "Table 3 shows the performance of various algorithms as the number of presentation terms ranges from 6 to 48.",
        "We find that the performance of TFB is more susceptible to presentation term reduction than that of CFB or TCFB.",
        "For example, at 12 terms the MAP of TFB3C is 90.6% of that at 48 terms, while the numbers for CFB3C and TCFB3C are 98.0% and 96.1% respectively.",
        "We conjecture the reason to be that while TFBs performance heavily depends on how many good terms are chosen for query expansion, CFB only needs a rough estimate of cluster weights to work.",
        "Also, the 3 × 16 clarification forms seem to be more robust than the 6 × 8 ones: at 12 terms the MAP of TFB6C is 87.1% of that at 48 terms, lower than 90.6% for TFB3C.",
        "Similarly, for CFB it is 95.0% against 98.0%.",
        "This is natual, as for a large cluster number of 6, it is easier to get into the situation where each cluster gets too few presentation terms to make topic diversification useful.",
        "Overall, we are surprised to see that the algorithms are still able to perform reasonably well when the number of presentation terms is small.",
        "For example, at only 12 terms CFB3C (the clarification form is of size 3 × 4) can still improve 36.5% over the baseline, dropping slightly from 39.3% at 48 terms. 6.3 User Feedback Analysis In this part we study several aspects of users term feedback behavior, and whether they are connected to retrieval performance.",
        "Figure 2: Clarification form completion time distributions 0−30 30−60 60−90 90−120 120−150 150−180 0 5 10 15 20 25 30 35 completion time (seconds) #topics 1×48 3×16 6×8 Figure 2 shows the distribution of time needed to complete a clarification form3 .",
        "We see that the user is usually able to finish term feedback within a reasonably short amount of time: for more than half of the topics the clarification form is completed in just 1 minute, and only a small fraction of topics (less than 10% for 1 × 48 and 3 × 16) take more than 2 minutes.",
        "This suggests that term feedback is suitable for interactive ad-hoc retrieval, where a user usually does not want to spend too much time on providing feedback.",
        "We find that a user often makes mistakes when judging term relevance.",
        "Sometimes a relevant term may be left out because its connection to the query topic is not obvious to the user.",
        "Other times a dubious term may be included but turns out to be irrelevant.",
        "Take the topic in Figure 1 for example.",
        "There was a fire disaster in Mont 3 The maximal time is 180 seconds, as the NIST assessor would be forced to submit the form at that moment.",
        "Table 4: Term selection statistics (topic average) CF Type 1 × 48 3 × 16 6 × 8 # checked terms 14.8 13.3 11.2 # rel. terms 15.0 12.6 11.2 # rel. checked terms 7.9 6.9 5.9 precision 0.534 0.519 0.527 recall 0.526 0.548 0.527 Blanc Tunnel between France and Italy in 1999, but the user failed to select such keywords as mont, blanc, french and italian due to his/her ignorance of the event.",
        "Indeed, without proper context it would be hard to make perfect judgment.",
        "What is then, the extent to which the user is good at term feedback?",
        "Does it have serious impact on retrieval performance?",
        "To answer these questions, we need a measure of individual terms true relevance.",
        "We adopt the Simplified KL Divergence metric used in [24] to decide query expansion terms as our term relevance measure: σKLD(w) = p(w|R) log p(w|R) p(w|¬R) where p(w|R) is the probability that a relevant document contains term w, and p(w|¬R) is the probability that an irrelevant document contains w, both of which can be easily computed via maximum likelihood estimate given document-level relevance judgment.",
        "If σKLD(w) > 0, w is more likely to appear in relevant documents than irrelevant ones.",
        "We consider a term relevant if its Simplified KL Divergence value is greater than a certain threshold σ0.",
        "We can then define precision and recall of user term judgment accordingly: precision is the fraction of terms checked by the user that are relevant; recall is the fraction of presented relevant terms that are checked by the user.",
        "Table 4 shows the number of checked terms, relevant terms and relevant checked terms when σ0 is set to 1.0, as well as the precision/recall of user term judgment.",
        "Note that when the clarification forms contain more clusters, fewer terms are checked: 14.8 for 1 × 48, 13.3 for 3 × 16 and 11.2 for 6×8.",
        "Similar pattern holds for relevant terms and relevant checked terms.",
        "There seems to be a trade-off between increasing topic diversity by clustering and losing extra relevant terms: when there are more clusters, each of them gets fewer terms to present, which can hurt a major relevant cluster that contains many relevant terms.",
        "Therefore, it is not always helpful to have more clusters, e.g., TFB6C is actually worse than TFB1C.",
        "The major finding we can make from Table 4 is that the user is not particularly good at identifying relevant terms, which echoes the discovery in [18].",
        "In the case of 3 × 16 clarification forms, the average number of terms checked as relevant by the user is 13.3 per topic, and the average number of relevant terms whose σKLD value exceed 1.0 is 12.6.",
        "The user is able to recognize only 6.9 of these terms on average.",
        "Indeed, the precision and recall of user feedback terms (as defined previously) are far from perfect.",
        "On the other hand, If the user had correctly checked all such relevant terms, the performance of our algorithms would have increased a lot, as shown in Table 5.",
        "We see that TFB gets big improvement when there is an oracle who checks all relevant terms, while CFB meets a bottleneck around MAP of 0.325, since all it does is adjust cluster weights, and when the learned weights are close to being accurate, it cannot benefit more from term feedback.",
        "Also note that TCFB fails to outperform TFB, probably because TFB is sufficiently accurate.",
        "Table 5: Change of MAP when using all (and only) relevant terms (σKLD > 1.0) for feedback. original term feedback relevant term feedback TF1 0.288 0.354 TF3 0.288 0.354 TF6 0.278 0.346 CF3 0.305 0.325 CF6 0.301 0.326 TCF3 0.309 0.345 TCF6 0.304 0.341 6.4 Comparison with Relevance Feedback Now we compare term feedback with document-level relevance feedback, in which the user is presented with the top N documents from an initial retrieval and asked to judge their relevance.",
        "The feedback process is simulated using document relevance judgment from NIST.",
        "We use the mixture model based feedback method proposed in [25], with mixture noise set to 0.95 and feedback coefficient set to 0.9.",
        "Comparative evaluation of relevance feedback against other methods is complicated by the fact that some documents have already been viewed during feedback, so it makes no sense to include them in the retrieval results of the second run.",
        "However, this does not hold for term feedback.",
        "Thus, to make it fair w.r.t. users information gain, if the feedback documents are relevant, they should be kept in the top of the ranking; if they are irrelevant, they should be left out.",
        "Therefore, we use relevance feedback to produce a ranking of top 1000 retrieved documents but with every feedback document excluded, and then prepend the relevant feedback documents at the front.",
        "Table 6 shows the performance of relevance feedback for different values of N and compares it with TCFB3C.",
        "Table 6: Performance of relevance feedback for different number of feedback documents (N).",
        "N MAP Pr@30 RR 5 0.302 0.586 4779 10 0.345 0.670 4916 20 0.389 0.772 5004 TCFB3C 0.309 0.491 4947 We see that the performance of TCFB3C is comparable to that of relevance feedback using 5 documents.",
        "Although it is poorer than when there are 10 feedback documents in terms of MAP and Pr@30, it does retrieve more documents (4947) when going down the ranked list.",
        "We try to compare the quality of automatically inserted terms in relevance feedback with that of manually selected terms in term feedback.",
        "This is done by truncating the relevance feedback modified query model to a size equal to the number of checked terms for the same topic.",
        "We can then compare the terms in the truncated model with the checked terms.",
        "Figure 3 shows the distribution of the terms σKLD scores.",
        "We find that term feedback tends to produce expansion terms of higher quality(those with σKLD > 1) compared to relevance feedback (with 10 feedback documents).",
        "This does not contradict the fact that the latter yields higher retrieval performance.",
        "Actually, when we use the truncated query model instead of the intact one refined from relevance feedback, the MAP is only 0.304.",
        "The truth Figure 3: Comparison of expansion term quality between relevance feedback (with 10 feedback documents) and term feedback (with 3 × 16 CFs) −1−0 0−1 1−2 2−3 3−4 4−5 5−6 0 50 100 150 200 250 300 350 σKLD #terms relevance feedback term feedback is, although there are many unwanted terms in the expanded query model from feedback documents, there are also more relevant terms than what the user can possibly select from the list of presentation terms generated with pseudo-feedback documents, and the positive effects often outweights the negative ones.",
        "We are interested to know under what circumstances term feedback has advantage over relevance feedback.",
        "One such situation is when none of the top N feedback documents is relevant, rendering relevance feedback useless.",
        "This is not infrequent, as one might have thought: out of the 50 topics, there are 13 such cases when N = 5, 10 when N = 10, and still 3 when N = 20.",
        "When this happens, one can only back off to the original retrieval method; the power of relevance feedback is lost.",
        "Surprisingly, in 11 out of 13 such cases where relevance feedback seems impossible, the user is able to check at least 2 relevant terms from the 3 × 16 clarification form (we consider term t to be relevant if σKLD(t) > 1.0).",
        "Furthermore, in 10 out of them TCFB3C outperforms the pseudo-feedback baseline, increasing MAP from 0.076 to 0.146 on average (these are particularly hard topics).",
        "We think that there are two possible explanations for this phenomenon of term feedback being active even when relevance feedback does not work: First, even if none of the top N (suppose it is a small number) documents are relevant, we may still find relevant documents in top 60, which is more inclusive but usually unreachable when people are doing relevance feedback in interactive ad-hoc search, from which we can draw feedback terms.",
        "This is true for topic 367 piracy, where the top 10 feedback documents are all about software piracy, yet there are documents between 10-60 that are about piracy on the seas (which is about the real information need), contributing terms such as pirate, ship for selection in the clarification form.",
        "Second, for some topics, a document needs to meet some special condition in order to be relevant.",
        "The top N documents may be related to the topic, but nonetheless irrelevant.",
        "In this case, we may still extract useful terms from these documents, even if they do not qualify as relevant ones.",
        "For example, in topic 639 consumer online shopping, a document needs to mention what contributes to shopping growth to really match the specified information need, hence none of the top 10 feedback documents are regarded as relevant.",
        "But nevertheless, the feedback terms such as retail, commerce are good for query expansion. 7.",
        "CONCLUSIONS In this paper we studied the use of term feedback for interactive information retrieval in the language modeling approach.",
        "We proposed a cluster-based method for selecting presentation terms as well as algorithms to estimate refined query models from user term feedback.",
        "We saw significant improvement in retrieval accuracy brought by term feedback, in spite of the fact that a user often makes mistakes in relevance judgment that hurts its performance.",
        "We found the best-performing algorithm to be TCFB, which benefits from the combination of directly observed term evidence with TFB and indirectly learned cluster relevance with CFB.",
        "When we reduced the number of presentation terms, term feedback is still able to keep much of its performance gain over the baseline.",
        "Finally, we compared term feedback to document-level relevance feedback, and found that TCFB3Cs performance is on a par with the latter with 5 feedback documents.",
        "We regarded term feedback as a viable alternative to traditional relevance feedback, especially when there are no relevant documents in the top.",
        "We propose to extend our work in several ways.",
        "First, we want to study whether the use of various contexts can help the user to better identify term relevance, while not sacrificing the simplicity and compactness of term feedback.",
        "Second, currently all terms are presented to the user in a single batch.",
        "We could instead consider iterative term feedback, by presenting a small number of terms first, and show more terms after receiving user feedback or stop when the refined query is good enough.",
        "The presented terms should be selected dynamically to maximize learning benefits at any moment.",
        "Third, we have plans to incorporate term feedback into our UCAIR toolbar[20], an Internet Explorer plugin, to make it work for web search.",
        "We are also interested in studying how to combine term feedback with relevance feedback or implicit feedback.",
        "We could, for example, allow the user to dynamically modify terms in a language model learned from feedback documents. 8.",
        "ACKNOWLEDGMENT This work is supported in part by the National Science Foundation grants IIS-0347933 and IIS-0428472. 9.",
        "REFERENCES [1] J. Allan.",
        "Relevance feedback with too much data.",
        "In Proceedings of the 18th annual international ACM SIGIR conference on research and development in information retrieval, pages 337-343, 1995. [2] J. Allan.",
        "HARD track overview in TREC 2005 - High Accuracy Retrieval from Documents.",
        "In The Fourteenth Text REtrieval Conference, 2005. [3] P. Anick.",
        "Using terminological feedback for web search refinement: a log-based study.",
        "In Proceedings of the 26th annual international ACM SIGIR conference on research and development in informaion retrieval, pages 88-95, 2003. [4] P. G. Anick and S. Tipirneni.",
        "The paraphrase search assistant: terminological feedback for iterative information seeking.",
        "In Proceedings of the 22nd annual international ACM SIGIR conference on research and development in information retrieval, pages 153-159, 1999. [5] C. Buckley, G. Salton, J. Allan, and A. Singhal.",
        "Automatic query expansion using SMART.",
        "In Proceedings of the Third Text REtrieval Conference, 1994. [6] D. Harman.",
        "Towards interactive query expansion.",
        "In Proceedings of the 11th annual international ACM SIGIR conference on research and development in information retrieval, pages 321-331, 1988. [7] N. A. Jaleel, A. Corrada-Emmanuel, Q. Li, X. Liu, C. Wade, and J. Allan.",
        "UMass at TREC 2003: HARD and QA.",
        "In TREC, pages 715-725, 2003. [8] H. Joho, C. Coverson, M. Sanderson, and M. Beaulieu.",
        "Hierarchical presentation of expansion terms.",
        "In Proceedings of the 2002 ACM symposium on applied computing, pages 645-649, 2002. [9] K. S. Jones, S. Walker, and S. E. Robertson.",
        "A probabilistic model of information retrieval: development and status.",
        "Technical Report 446, Computer Laboratory, University of Cambridge, 1998. [10] D. Kelly, V. D. Dollu, and X. Fu.",
        "The loquacious user: a document-independent source of terms for query expansion.",
        "In Proceedings of the 28th annual international ACM SIGIR conference on research and development in information retrieval, pages 457-464, 2005. [11] D. Kelly and X. Fu.",
        "Elicitation of term relevance feedback: an investigation of term source and context.",
        "In Proceedings of the 29th annual international ACM SIGIR conference on research and development in information retrieval, 2006. [12] J. Koenemann and N. Belkin.",
        "A case for interaction: A study of interactive information retrieval behavior and effectiveness.",
        "In Proceedings of the SIGCHI conference on human factors in computing systems, pages 205-212, 1996. [13] V. Lavrenko and W. B. Croft.",
        "Relevance-based language models.",
        "In Research and Development in Information Retrieval, pages 120-127, 2001. [14] Y. Nemeth, B. Shapira, and M. Taeib-Maimon.",
        "Evaluation of the real and perceived value of automatic and interactive query expansion.",
        "In Proceedings of the 27th annual international ACM SIGIR conference on research and development in information retrieval, pages 526-527, 2004. [15] J. Ponte.",
        "A Language Modeling Approach to Information Retrieval.",
        "PhD thesis, University of Massachusetts at Amherst, 1998. [16] S. E. Robertson, S. Walker, S. Jones, M. Beaulieu, and M. Gatford.",
        "Okapi at TREC-3.",
        "In Proceedings of the Third Text REtrieval Conference, 1994. [17] J. Rocchio.",
        "Relevance feedback in information retrieval.",
        "In The SMART retrieval system, pages 313-323. 1971. [18] I. Ruthven.",
        "Re-examining the potential effectiveness of interactive query expansion.",
        "In Proceedings of the 26th annual international ACM SIGIR conference on research and development in informaion retrieval, pages 213-220, 2003. [19] G. Salton and C. Buckley.",
        "Improving retrieval performance by relevance feedback.",
        "Journal of the American Society for Information Science, 41:288-297, 1990. [20] X. Shen, B. Tan, and C. Zhai.",
        "Implicit user modeling for personalized search.",
        "In Proceedings of the 14th ACM international conference on information and knowledge management, pages 824-831, 2005. [21] X. Shen and C. Zhai.",
        "Active feedback in ad-hoc information retrieval.",
        "In Proceedings of the 28th annual international ACM SIGIR conference on research and development in information retrieval, pages 59-66, 2005. [22] A. Spink.",
        "Term relevance feedback and query expansion: relation to design.",
        "In Proceedings of the 17th annual international ACM SIGIR conference on research and development in information retrieval, pages 81-90, 1994. [23] J. Xu and W. B. Croft.",
        "Query expansion using local and global document analysis.",
        "In Proceedings of the 19th annual international ACM SIGIR conference on research and development in information retrieval, pages 4-11, 1996. [24] H. Zaragoza, N. Craswell, M. Taylor, S. Saria, and S. Robertson.",
        "Microsoft cambridge at TREC-13: Web and HARD tracks.",
        "In Proceedings of the 13th Text REtrieval Conference, 2004. [25] C. Zhai and J. Lafferty.",
        "Model-based feedback in the language modeling approach to information retrieval.",
        "In Proceedings of the tenth international conference on information and knowledge management, pages 403-410, 2001. [26] C. Zhai, A. Velivelli, and B. Yu.",
        "A cross-collection mixture model for comparative text mining.",
        "In Proceedings of the tenth ACM SIGKDD international conference on knowledge discovery and data mining, pages 743-748, 2004."
    ],
    "translated_text_sentences": [
        "Comentarios sobre el término para la recuperación de información con modelos de lenguaje Bin Tan†, Atulya Velivelli‡, Hui Fang†, ChengXiang Zhai† Dept.",
        "En este documento estudiamos la retroalimentación basada en términos para la recuperación de información en el enfoque de modelado de lenguaje.",
        "Con la retroalimentación de términos, un usuario juzga directamente la relevancia de términos individuales sin interacción con documentos de retroalimentación, tomando el control total del proceso de expansión de la consulta.",
        "Proponemos un método basado en clusters para seleccionar términos para presentar al usuario para su evaluación, así como algoritmos efectivos para construir modelos de lenguaje de consulta refinados a partir de la retroalimentación de términos del usuario.",
        "Nuestros algoritmos han demostrado proporcionar una mejora significativa en la precisión de recuperación en comparación con una línea base sin retroalimentación, y logran un rendimiento comparable al de la retroalimentación de relevancia.",
        "Son útiles incluso cuando no hay documentos relevantes en la parte superior.",
        "Categorías y Descriptores de Asignaturas H.3.3 [Búsqueda y Recuperación de Información]: Modelos de recuperación Términos generales Algoritmos 1.",
        "En el enfoque de modelado del lenguaje para la recuperación de información, la retroalimentación a menudo se modela como la estimación de un modelo de consulta mejorado o un modelo de relevancia basado en un conjunto de documentos de retroalimentación [25, 13].",
        "Esto está en línea con la forma tradicional de realizar retroalimentación de relevancia: presentar al usuario documentos/pasajes para su juicio de relevancia y luego extraer términos de los documentos o pasajes juzgados para expandir la consulta inicial.",
        "Es una forma indirecta de buscar la ayuda de los usuarios para la construcción del modelo de consulta, en el sentido de que el modelo de consulta refinado (basado en términos) se aprende a través de documentos/pasajes de retroalimentación, que son estructuras de alto nivel de términos.",
        "Tiene la desventaja de que los términos irrelevantes, que ocurren junto con los relevantes en el contenido evaluado, pueden ser utilizados erróneamente para la expansión de la consulta, causando efectos no deseados.",
        "Por ejemplo, para la consulta TREC del telescopio Hubble logros, cuando un documento relevante habla más sobre la reparación de los telescopios que sobre sus descubrimientos, términos irrelevantes como caminata espacial pueden ser agregados a la consulta modificada.",
        "Podemos considerar una forma más directa de involucrar a un usuario en la mejora del modelo de consulta, sin un paso intermedio de retroalimentación de documentos que pueda introducir ruido.",
        "La idea es presentar al usuario un número (razonable) de términos individuales y pedirle que juzgue la relevancia de cada término o especifique directamente sus probabilidades en el modelo de consulta.",
        "Esta estrategia ha sido discutida en [15], pero hasta donde sabemos, no ha sido estudiada seriamente en la literatura existente de modelado de lenguaje.",
        "En comparación con la retroalimentación de relevancia tradicional, este enfoque basado en términos para el refinamiento del modelo de consulta interactivo tiene varias ventajas.",
        "Primero, el usuario tiene un mejor control del modelo de consulta final a través de la manipulación directa de términos: él/ella puede dictar qué términos son relevantes, irrelevantes y posiblemente, en qué medida.",
        "Esto evita el riesgo de introducir términos no deseados en el modelo de consulta, aunque a veces el usuario introduce términos de baja calidad.",
        "Segundo, dado que un término tarda menos tiempo en ser evaluado que el texto completo o el resumen de un documento, y con tan solo alrededor de 20 términos presentados se puede lograr una mejora significativa en el rendimiento de recuperación (como demostraremos más adelante), el feedback de términos permite recopilar la retroalimentación del usuario de manera más rápida.",
        "Esto es especialmente útil para la búsqueda interactiva ad hoc.",
        "En tercer lugar, a veces no hay documentos relevantes en los primeros N resultados recuperados inicialmente si el tema es difícil.",
        "Esto suele ser cierto cuando N está limitado a ser pequeño, lo cual surge del hecho de que el usuario no está dispuesto a evaluar demasiados documentos.",
        "En este caso, la retroalimentación de relevancia es inútil, ya que no se puede aprovechar ningún documento relevante, pero la retroalimentación de términos sigue siendo útil, al permitir seleccionar términos relevantes de documentos irrelevantes.",
        "Durante nuestra participación en la pista HARD de TREC 2005 y el estudio continuado posteriormente, exploramos cómo aprovechar la retroalimentación de términos por parte del usuario para construir modelos de consulta mejorados para la recuperación de información en el enfoque de modelado de lenguaje.",
        "Identificamos dos sub tareas clave del feedback basado en términos, es decir, la selección de términos para la presentación previa al feedback y la construcción del modelo de consulta posterior al feedback, con algoritmos efectivos desarrollados para ambos.",
        "Impusimos una estructura de clúster secundaria en los términos y descubrimos que una vista de clúster arroja una mayor comprensión de la necesidad de información de los usuarios, y proporciona una buena forma de utilizar la retroalimentación de términos.",
        "A través de experimentos encontramos que el feedback a término mejora significativamente sobre la línea base sin feedback, a pesar de que el usuario a menudo comete errores en la evaluación de relevancia.",
        "Entre nuestros algoritmos, el que tiene el mejor rendimiento de recuperación es TCFB, la combinación de TFB, el algoritmo de retroalimentación de términos directos, y CFB, el algoritmo de retroalimentación basado en clústeres.",
        "También variamos el número de términos de retroalimentación y observamos una mejora razonable incluso con números bajos.",
        "Finalmente, al comparar la retroalimentación a nivel de términos con la retroalimentación a nivel de documentos, encontramos que es una alternativa viable a esta última con un rendimiento competitivo en la recuperación.",
        "El resto del documento está organizado de la siguiente manera.",
        "La sección 2 discute algunos trabajos relacionados.",
        "La sección 4 describe nuestro enfoque general para la retroalimentación de términos.",
        "Presentamos nuestro método para la selección de términos de presentación en la Sección 3 y los algoritmos para la construcción del modelo de consulta en la Sección 5.",
        "Los resultados del experimento se presentan en la Sección 6.",
        "La sección 7 concluye este documento. 2.",
        "TRABAJO RELACIONADO La retroalimentación de relevancia[17, 19] ha sido reconocida durante mucho tiempo como un método efectivo para mejorar el rendimiento de recuperación.",
        "Normalmente, los N documentos principales recuperados utilizando la consulta original se presentan al usuario para su evaluación, después de lo cual se extraen términos de los documentos relevantes evaluados, ponderados por su potencial de atraer más documentos relevantes, y se añaden al modelo de consulta.",
        "La consulta ampliada suele representar mejor la necesidad de información de los usuarios que la original, que a menudo es solo una consulta de palabras clave cortas.",
        "Una segunda iteración de recuperación utilizando esta consulta modificada generalmente produce un aumento significativo en la precisión de la recuperación.",
        "En los casos en los que no se dispone de una evaluación de relevancia real y se asume que todos los documentos principales N son relevantes, se denomina retroalimentación ciega o pseudo [5, 16] y generalmente sigue mejorando el rendimiento.",
        "Debido a que un documento es una unidad de texto grande, cuando se utiliza para retroalimentación de relevancia, muchos términos irrelevantes pueden ser introducidos en el proceso de retroalimentación.",
        "Para superar esto, se propone el feedback de pasajes y se muestra que mejora el rendimiento del feedback.",
        "Una solución más directa es pedir al usuario su juicio de relevancia sobre los términos de retroalimentación.",
        "Por ejemplo, en algunos sistemas de retroalimentación de relevancia como [12], hay un paso de interacción que permite al usuario agregar o eliminar términos de expansión después de que son extraídos automáticamente de los documentos relevantes.",
        "Esto se clasifica como expansión interactiva de consultas, donde la consulta original se amplía con términos proporcionados por el usuario, que pueden provenir de la entrada directa del usuario (texto libre o palabras clave) o de la selección del usuario de términos sugeridos por el sistema (utilizando tesauros o extraídos de documentos de retroalimentación).",
        "En muchos casos, se ha encontrado que la retroalimentación de relevancia de términos mejora de manera efectiva el rendimiento de recuperación [6, 22, 12, 4, 10].",
        "Por ejemplo, el estudio en [12] muestra que el usuario prefiere tener un conocimiento explícito y control directo de qué términos se utilizan para la expansión de la consulta, y se demuestra que la interfaz penetrable que proporciona esta libertad tiene un mejor rendimiento que otras interfaces.",
        "Sin embargo, en algunos otros casos no hay un beneficio significativo[3, 14], incluso si al usuario le gusta interactuar con los términos de expansión.",
        "En un estudio simulado realizado en [18], el autor compara el rendimiento de recuperación de la expansión interactiva de consultas y la expansión automática de consultas con un estudio simulado, y sugiere que los posibles beneficios de la primera pueden ser difíciles de lograr.",
        "Se descubre que el usuario no es bueno identificando términos útiles para la expansión de la consulta, cuando una interfaz de presentación de términos simple no puede proporcionar suficiente contexto semántico de los términos de retroalimentación.",
        "Nuestro trabajo difiere de los anteriores en dos aspectos importantes.",
        "Primero, al elegir los términos para presentar al usuario para la evaluación de relevancia, no solo consideramos el valor de un solo término (por ejemplo, la frecuencia relativa de un término en los documentos principales, que puede medirse mediante métricas como el Valor de Selección de Robertson y la Distancia Kullback-Leibler Simplificada como se detalla en [24]), sino que también examinamos la estructura de los grupos de términos, con el fin de producir una cobertura equilibrada de los diferentes aspectos del tema.",
        "Segundo, con el marco de modelado del lenguaje, permitimos una construcción detallada del modelo de consulta actualizado, estableciendo diferentes probabilidades para diferentes términos según si es un término de consulta, su importancia en los documentos principales y su pertenencia a un clúster.",
        "Aunque existen técnicas para ajustar los pesos de los términos de consulta en modelos de espacio vectorial y modelos de relevancia probabilística, la mayoría de los trabajos mencionados no las utilizan, optando por simplemente añadir términos de retroalimentación a la consulta original (usando pesos iguales para ellos), lo que puede llevar a un rendimiento de recuperación más pobre.",
        "La combinación de los dos aspectos permite que nuestro método funcione mucho mejor que el punto de referencia.",
        "La forma habitual de presentar los términos de retroalimentación es simplemente mostrar los términos en una lista.",
        "Ha habido algunos trabajos sobre interfaces de usuario alternativas. [8] organiza los términos en una jerarquía, y [11] compara tres interfaces diferentes, incluyendo términos + casillas de verificación, términos + contexto (oraciones) + casillas de verificación, oraciones + cuadro de texto de entrada.",
        "En ambos estudios, sin embargo, no hay una diferencia significativa en el rendimiento.",
        "En nuestro trabajo adoptamos el enfoque más simple de términos + casillas de verificación.",
        "Nos enfocamos en la presentación de términos y la construcción de modelos de consulta a partir de términos de retroalimentación, y creemos que el uso de contextos para mejorar la calidad de los términos de retroalimentación debería ser ortogonal a nuestro método. 3.",
        "Enfoque general: Seguimos el enfoque de modelado del lenguaje y basamos nuestro método en el modelo de recuperación de divergencia KL propuesto en [25].",
        "Con este modelo, la tarea de recuperación implica estimar un modelo de lenguaje de consulta θq a partir de una consulta dada, un modelo de lenguaje de documento θd de cada documento, y calcular su divergencia KL D(θq||θd), que luego se utiliza para puntuar los documentos. [25] trata la retroalimentación de relevancia como un problema de reestimación del modelo de consulta, es decir, calcular un modelo de consulta actualizado θq dado el texto de la consulta original y la evidencia adicional proporcionada por los documentos relevantes juzgados.",
        "Adoptamos esta perspectiva y planteamos nuestra tarea como la actualización del modelo de consulta a partir de la retroalimentación de términos de usuario.",
        "Aquí hay dos tareas clave: Primero, cómo elegir los mejores términos para presentar al usuario para su evaluación, con el fin de recopilar la máxima evidencia sobre la necesidad de información del usuario.",
        "Segundo, cómo calcular un modelo de consulta actualizado basado en esta retroalimentación de términos, de manera que capture la necesidad de información de los usuarios y se traduzca en un buen rendimiento de recuperación. 4.",
        "SELECCIÓN DE TÉRMINOS DE PRESENTACIÓN La selección adecuada de los términos que se presentarán al usuario para su evaluación es crucial para el éxito de la retroalimentación de términos.",
        "Si los términos están mal elegidos y hay pocos relevantes, el usuario tendrá dificultades para buscar términos útiles que ayuden a aclarar su necesidad de información.",
        "Si los términos relevantes son abundantes, pero todos se centran en un solo aspecto del tema de la consulta, entonces solo podremos obtener retroalimentación sobre ese aspecto y perderemos otros, lo que resultará en una pérdida de amplitud en los resultados recuperados.",
        "Por lo tanto, es importante seleccionar cuidadosamente los términos de presentación para maximizar la ganancia esperada de la retroalimentación del usuario, es decir, aquellos que pueden revelar potencialmente la mayor evidencia de la necesidad de información del usuario.",
        "Esto es similar al feedback activo[21], que sugiere que un sistema de recuperación debería sondear activamente la necesidad de información de los usuarios, y en el caso del feedback de relevancia, los documentos de feedback deberían ser elegidos para maximizar los beneficios de aprendizaje (por ejemplo, de manera diversa para aumentar la cobertura).",
        "En nuestro enfoque, los N documentos principales de una recuperación inicial utilizando la consulta original forman la fuente de términos de retroalimentación: todos los términos que aparecen en ellos se consideran candidatos para presentar al usuario.",
        "Estos documentos sirven como pseudo-retroalimentación, ya que proporcionan un contexto mucho más amplio que la consulta original (generalmente muy breve), sin que se le pida al usuario que juzgue su relevancia.",
        "Debido a esta última razón, es posible hacer que N sea bastante grande (por ejemplo, en nuestros experimentos establecimos N = 60) para aumentar su cobertura de diferentes aspectos en el tema.",
        "La forma más sencilla de seleccionar términos de retroalimentación es elegir los M términos más frecuentes de los N documentos.",
        "Este método, sin embargo, tiene dos inconvenientes.",
        "Primero, se seleccionarán muchos términos ruidosos comunes debido a sus altas frecuencias en la colección de documentos, a menos que se utilice una lista de palabras vacías para filtrar.",
        "En segundo lugar, la lista de presentación tiende a estar compuesta por términos de los aspectos principales del tema; es probable que se pasen por alto aquellos de un aspecto menor debido a sus frecuencias relativamente bajas.",
        "Resolvemos los problemas anteriores mediante dos medidas correspondientes.",
        "Primero, introducimos un modelo de fondo θB que se estima a partir de estadísticas de la colección y explica los términos comunes, de modo que es mucho menos probable que aparezcan en la lista de presentación.",
        "Segundo, los términos se seleccionan de múltiples grupos en los documentos de retroalimentación simulada, para garantizar una representación suficiente de diferentes aspectos del tema.",
        "Nos basamos en el modelo multinomial de mezcla, que se utiliza para el descubrimiento de temas en [26].",
        "Específicamente, asumimos que los N documentos contienen K grupos {Ci| i = 1, 2, · · · K}, cada uno caracterizado por una distribución de palabras multinomial (también conocida como modelo de lenguaje unigrama) θi y correspondiente a un aspecto del tema.",
        "Los documentos se consideran como muestreados de una mezcla de K + 1 componentes, incluidos los K grupos y el modelo de fondo: p(w|d) = λBp(w|θB) + (1 − λB) K i=1 πd,ip(w|θi) donde w es una palabra, λB es el peso de la mezcla para el modelo de fondo θB, y πd,i es el peso de la mezcla específico del documento para el modelo de cluster i-ésimo θi.",
        "Luego estimamos los modelos de clúster maximizando la probabilidad de que los documentos de pseudo-retroalimentación sean generados a partir del modelo de mezcla multinomial: log p(D|Λ) = d∈D w∈V c(w; d) log p(w|d) donde D = {di| i = 1, 2, · · · N} es el conjunto de los N documentos, V es el vocabulario, c(w; d) es la frecuencia de w en d y Λ = {θi| i = 1, 2, · · · K} ∪ {πdij | i = 1, 2, · · · N, j = 1, 2, · · · K} es el conjunto de parámetros del modelo a estimar.",
        "Los modelos de conglomerados pueden ser estimados eficientemente utilizando el algoritmo de Expectation-Maximization (EM).",
        "Para más detalles, remitimos al lector a [26].",
        "La Tabla 1 muestra los modelos de clúster para la consulta de TREC sobre desastres en túneles de transporte (K = 3).",
        "Ten en cuenta que solo el grupo central es relevante.",
        "Tabla 1: Modelos de clúster para el tema 363 Desastres en túneles de transporte Clúster 1 Clúster 2 Clúster 3 túnel 0.0768 túnel 0.0935 túnel 0.0454 transporte 0.0364 fuego 0.0295 transporte 0.0406 tráfico 0.0206 camión 0.0236 peaje 0.0166 ferrocarril 0.0186 francés 0.0220 Amtrak 0.0153 puerto 0.0146 humo 0.0157 tren 0.0129 riel 0.0140 coche 0.0154 aeropuerto 0.0122 puente 0.0139 italiano 0.0152 autopista 0.0105 kilómetro 0.0136 incendio 0.0144 lui 0.0095 camión 0.0133 llamas 0.0127 Jersey 0.0093 construcción 0.0131 blanco 0.0121 paso 0.0087 · · · · · · · · · De cada uno de los K clústeres estimados, elegimos los L = M/K términos con las probabilidades más altas para formar un total de M términos de presentación.",
        "Si un término resulta estar en el top L en múltiples grupos, lo asignamos al grupo donde tenga la probabilidad más alta y permitimos que los otros grupos tomen un término adicional como compensación.",
        "También filtramos los términos en el texto de la consulta original porque tienden a ser siempre relevantes cuando la consulta es corta.",
        "Los términos seleccionados son luego presentados al usuario para su evaluación.",
        "Se muestra un formulario de retroalimentación (completado) de muestra en la Figura 1.",
        "En este estudio solo tratamos con juicios binarios: un término presentado está por defecto sin marcar, y un usuario puede marcarlo para indicar relevancia.",
        "Tampoco explotamos explícitamente la retroalimentación negativa (es decir, penalizar términos irrelevantes), porque con la retroalimentación binaria un término no verificado no es necesariamente irrelevante (quizás el usuario no está seguro de su relevancia).",
        "Podríamos pedir al usuario un juicio más detallado (por ejemplo, eligiendo entre altamente relevante, algo relevante, no sé, algo irrelevante y altamente irrelevante), pero la retroalimentación binaria es más compacta, ocupando menos espacio para mostrar y requiriendo menos esfuerzo por parte del usuario para emitir un juicio. 5.",
        "ESTIMACIÓN DE MODELOS DE CONSULTA A PARTIR DE RETROALIMENTACIÓN DE TÉRMINOS En esta sección, presentamos varios algoritmos para aprovechar la retroalimentación de términos.",
        "Los algoritmos toman como entrada la consulta original q, los grupos {θi} generados por el algoritmo de descubrimiento de temas, el conjunto de términos de retroalimentación T y su juicio de relevancia R, y producen un modelo de lenguaje de consulta actualizado θq que hace el mejor uso de la evidencia de retroalimentación para capturar la necesidad de información de los usuarios.",
        "Primero describimos nuestras notaciones: • θq: El modelo de consulta original, derivado solo de los términos de la consulta: p(w|θq) = c(w; q) |q| donde c(w; q) es la cantidad de veces que w aparece en q, y |q| = w∈q c(w; q) es la longitud de la consulta. • θq: El modelo de consulta actualizado que necesitamos estimar a partir de la retroalimentación de términos. • θi (i = 1, 2, . . .",
        "K): El modelo de lenguaje unigrama del clúster Ci, estimado utilizando el algoritmo de descubrimiento de temas. • T = {ti,j} (i = 1 . . .",
        "K, j = 1 . . . \n\nK, j = 1 . . .",
        "El conjunto de términos presentados al usuario para su evaluación. ti,j es el j-ésimo término elegido del grupo Ci. • R = {δw|w ∈ T}: δw es una variable indicadora que es 1 si w es considerado relevante o 0 en caso contrario. 5.1 TFB (Retroalimentación Directa de Términos) Esta es una forma directa de retroalimentación de términos que no implica ninguna estructura secundaria.",
        "Asignamos un peso de 1 a los términos considerados relevantes por el usuario, un peso de μ a los términos de la consulta, un peso de cero a los demás términos, y luego aplicamos la normalización: p(w|θq ) = δw + μ c(w; q) w ∈T δw + μ|q| donde w ∈T δw es el número total de términos considerados relevantes.",
        "Llamamos a este método TFB (retroalimentación directa de términos).",
        "Si dejamos que μ = 1, este enfoque es equivalente a agregar los términos relevantes después de la consulta original, que es lo que hace la expansión estándar de consultas (sin reponderación de términos).",
        "Si establecemos μ > 1, estamos poniendo más énfasis en los términos de consulta que en los verificados.",
        "Ten en cuenta que el modelo de resultado será más sesgado hacia θq si la consulta original es larga o el feedback del usuario es débil, lo cual tiene sentido, ya que podemos confiar más en la consulta original en ambos casos.",
        "Figura 1: Formulario de aclaración completado para el Tema 363 363 desastres en túneles de transporte. Por favor, seleccione todos los términos relevantes para el tema. tráfico ferrocarril puerto puente ferroviario kilómetro construir suizo enlace cruzado hongkonés río proyecto metro camión de bomberos francés humo coche italiano bomberos incendio blanco montaña víctima francés rescate conductor chamonix emerger peaje tren amtrak aeropuerto turnpike jersey pass rome z centro electrón carretera boston velocidad bu enviar 5.2 CFB (Retroalimentación de Clúster) Aquí explotamos la estructura de clúster que jugó un papel importante cuando seleccionamos los términos de presentación.",
        "Los grupos representan diferentes aspectos del tema de la consulta, cada uno de los cuales puede ser relevante o no.",
        "Si somos capaces de identificar los grupos relevantes, podemos combinarlos para generar un modelo de consulta que sea bueno para descubrir documentos pertenecientes a estos grupos (en lugar de los irrelevantes).",
        "Podríamos pedir al usuario que juzgue directamente la relevancia de un grupo después de ver los términos representativos en ese grupo, pero a veces esto sería una tarea difícil para el usuario, quien tendría que adivinar la semántica de un grupo a través de su conjunto de términos, los cuales podrían no estar bien conectados entre sí debido a la falta de contexto.",
        "Por lo tanto, proponemos aprender retroalimentación de clúster de forma indirecta, inferir la relevancia de un clúster a través de la relevancia de sus términos de retroalimentación.",
        "Dado que cada grupo tiene un número igual de términos presentados al usuario, la medida más simple de la relevancia de un grupo es el número de términos que se consideran relevantes en él.",
        "Intuitivamente, cuantos más términos estén marcados como relevantes en un grupo, más cerca estará el grupo del tema de la consulta y más debería participar en la modificación de la consulta.",
        "Si combinamos los modelos de clúster utilizando pesos determinados de esta manera y luego interpolamos con el modelo de consulta original, obtenemos la siguiente fórmula para la actualización de la consulta, que llamamos CFB (Retroalimentación de Clúster): p(w|θq) = λp(w|θq) + (1 − λ) K i=1 L j=1 δti,j K k=1 L j=1 δtk,j p(w|θi) donde L j=1 δti,j es el número de términos relevantes en el clúster Ci, y K k=1 L j=1 δtk,j es el número total de términos relevantes.",
        "Observamos que cuando solo hay un clúster (K = 1), la fórmula anterior se degrada a p(w|θq ) = λp(w|θq) + (1 − λ)p(w|θ1) que es simplemente una pseudo-retroalimentación de la forma propuesta en [25]. TCFB (Retroalimentación Término-Clúster) TFB y CFB ambos tienen sus inconvenientes.",
        "TFB asigna probabilidades no nulas a los términos presentados que están marcados como relevantes, pero ignora por completo (muchos más) otros, que pueden quedar sin marcar debido a la ignorancia de los usuarios, o simplemente no estar incluidos en la lista de presentación, pero deberíamos ser capaces de inferir su relevancia a partir de los marcados.",
        "Por ejemplo, en la Figura 1, dado que se revisan hasta 5 términos en el grupo central (las tercera y cuarta columnas), deberíamos tener una alta confianza en la relevancia de otros términos en ese grupo.",
        "CFB resuelve el problema de TFB tratando los términos en un grupo de manera colectiva, de modo que los términos no verificados/no presentados reciban pesos cuando los términos presentados en sus grupos son considerados relevantes, pero no distingue qué términos en un grupo son presentados o considerados.",
        "Intuitivamente, los términos considerados relevantes deberían recibir pesos mayores porque son indicados explícitamente como relevantes por el usuario.",
        "Por lo tanto, intentamos combinar los dos métodos, con la esperanza de obtener lo mejor de ambos.",
        "Lo hacemos interpolando el modelo TFB con el modelo CFB, y lo llamamos TCFB: p(w|θq ) = αp(w|θqT F B ) + (1 − α)p(w|θqCF B ) 6.",
        "EXPERIMENTOS En esta sección, describimos los resultados de nuestro experimento.",
        "Primero describimos nuestra configuración experimental y presentamos una visión general del rendimiento de varios métodos.",
        "Luego discutimos los efectos de variar la configuración de los parámetros en los algoritmos, así como el número de términos de presentación.",
        "A continuación analizamos el comportamiento de retroalimentación de los usuarios y su relación con el rendimiento de recuperación.",
        "Finalmente comparamos la retroalimentación de términos con la retroalimentación de relevancia y mostramos que tiene su ventaja particular. 6.1 Configuración del Experimento y Resultados Básicos Aprovechamos la oportunidad de la pista HARD de TREC 2005[2] para la evaluación de nuestros algoritmos.",
        "Las pistas utilizaron la colección AQUAINT, un corpus de 3GB de texto de noticias en inglés.",
        "Los temas incluyeron 50 que se sabía que eran difíciles, es decir, con bajo rendimiento de recuperación.",
        "Es para estos temas difíciles que la retroalimentación del usuario es más útil, ya que puede proporcionar información para desambiguar las consultas; con temas fáciles, es posible que el usuario no esté dispuesto a esforzarse por dar retroalimentación si los resultados de recuperación automática son lo suficientemente buenos.",
        "Los participantes de la pista pudieron enviar formularios de aclaración (CF) diseñados a medida para solicitar retroalimentación de los evaluadores humanos proporcionados por la Tabla 2: Rendimiento de recuperación para diferentes métodos y tipos de CF.",
        "La última fila es el porcentaje de mejora del MAP sobre la línea base.",
        "Los ajustes de parámetros μ = 4, λ = 0.1, α = 0.3 son casi óptimos.",
        "Variación de MAP con el número de términos presentados. # términos TFB1C TFB3C TFB6C CFB3C CFB6C TCFB3C TCFB6C 6 0.245 0.240 0.227 0.279 0.279 0.281 0.274 12 0.261 0.261 0.242 0.299 0.286 0.297 0.281 18 0.275 0.274 0.256 0.301 0.282 0.300 0.286 24 0.276 0.281 0.265 0.303 0.292 0.305 0.292 30 0.280 0.285 0.270 0.304 0.296 0.307 0.296 36 0.282 0.288 0.272 0.307 0.297 0.309 0.297 42 0.283 0.288 0.275 0.306 0.298 0.309 0.300 48 0.288 0.288 0.278 0.305 0.301 0.309 0.303 NIST.",
        "Diseñamos tres conjuntos de formularios de aclaración para la retroalimentación de términos, diferenciados en la elección de K, el número de grupos, y L, el número de términos presentados de cada grupo.",
        "Son: 1 × 48, un gran grupo con 48 términos, 3 × 16, 3 grupos con 16 términos cada uno, y 6 × 8, 6 grupos con 8 términos cada uno.",
        "El número total de términos presentados (M) está fijo en 48, por lo que al comparar el rendimiento de diferentes tipos de formas de clarificación podemos conocer los efectos de diferentes grados de agrupamiento.",
        "Para cada tema, un evaluador completaría los formularios en el orden de 6 × 8, 1 × 48 y 3 × 16, dedicando hasta tres minutos en cada formulario.",
        "El formulario de aclaración de muestra mostrado en la Figura 1 es de tipo 3 × 16.",
        "Es una interfaz simple y compacta en la que el usuario puede consultar términos relevantes.",
        "El formulario es autoexplicativo; no es necesario brindar capacitación adicional al usuario sobre cómo utilizarlo.",
        "Nuestras consultas iniciales se construyen solo utilizando las descripciones de los títulos de los temas, que tienen en promedio 2.7 palabras de longitud.",
        "Como base utilizamos el método de recuperación de divergencia KL implementado en la herramienta Lemur con 5 documentos de retroalimentación pseudo.",
        "Aplicamos el suavizado de Dirichlet con una prior de 2000, y truncamos los modelos de lenguaje de consulta a 50 términos (estas configuraciones se utilizan en todos los experimentos).",
        "Para todos los demás parámetros, utilizamos la configuración predeterminada de Lemurs.",
        "El punto de referencia resulta tener un rendimiento por encima del promedio entre los participantes de la pista.",
        "Después de una ejecución inicial utilizando este método de recuperación de referencia, tomamos los 60 documentos principales para cada tema y aplicamos el algoritmo de descubrimiento de temas para producir los grupos (1, 3 o 6 de ellos), en base a los cuales generamos formularios de aclaración.",
        "Después de recibir la retroalimentación del usuario, ejecutamos los algoritmos de retroalimentación de términos (TFB, CFB o TCFB) para estimar modelos de consulta actualizados, los cuales luego se utilizan para una segunda iteración de recuperación.",
        "Evaluamos el rendimiento de los diferentes métodos de recuperación en sus clasificaciones de los 1000 documentos principales.",
        "Las métricas de evaluación que adoptamos incluyen la precisión media promedio (no interpolada) (MAP), precisión en los primeros 30 (Pr@30) y total relevante recuperado (RR).",
        "La Tabla 2 muestra el rendimiento de varios métodos y configuraciones de K × L. Los sufijos (1C, 3C, 6C) después de TFB, CFB, TCFB representan el número de grupos (K).",
        "Por ejemplo, TCFB3C significa el método TCFB en los formularios de aclaración de 3 × 16.",
        "De la Tabla 2 podemos hacer las siguientes observaciones: 1 http://www.lemurproject.com 1.",
        "Todos los métodos tienen un rendimiento considerablemente mejor que la línea base de pseudoretroalimentación, con TCFB3C logrando una mejora máxima del 41.1% en el MAP, lo que indica una contribución significativa de la retroalimentación de términos para la clarificación de la necesidad de información de los usuarios.",
        "En otras palabras, el feedback a término es realmente útil para mejorar la precisión de recuperación. 2.",
        "Para TFB, el rendimiento es casi igual en los formularios de aclaración de 1 × 48 y 3 × 16 en términos de MAP (aunque este último es ligeramente mejor en Pr@30 y RR), y un poco peor en los de 6 × 8. 3.",
        "Tanto CFB3C como CFB6C tienen un mejor rendimiento que sus contrapartes TFB en las tres métricas, lo que sugiere que el feedback sobre una estructura de clúster secundaria es realmente beneficioso.",
        "CFB1C es en realidad peor porque no puede ajustar el peso de su (único) grupo a partir de la retroalimentación de términos y es simplemente pseudoretroalimentación. 4.",
        "Aunque TCFB es solo una simple mezcla de TFB y CFB por interpolación, logra superar a ambos.",
        "Esto respalda nuestra especulación de que TCFB supera las desventajas de TFB (prestando atención solo a los términos marcados) y CFB (sin distinguir los términos marcados y no marcados en un grupo).",
        "Excepto por TCFB6C v.s.",
        "CFB6C, la ventaja de rendimiento de TCFB sobre TFB/CFB es significativa con p < 0.05 utilizando la prueba de rango con signo de Wilcoxon.",
        "Esto no es cierto en el caso de TFB v.s.",
        "CFB, cada uno de los cuales es mejor que el otro en casi la mitad de los temas. 6.2 Reducción de Términos de Presentación En algunas situaciones podemos tener que reducir el número de términos de presentación debido a limitaciones en el espacio de visualización o esfuerzos de retroalimentación del usuario.",
        "Es interesante saber si el rendimiento de nuestros algoritmos se deteriora cuando al usuario se le presentan menos términos.",
        "Dado que los términos de presentación dentro de cada grupo se generan en orden decreciente de sus frecuencias, la lista de presentación forma un subconjunto del original si su tamaño se reduce2.",
        "Por lo tanto, podemos simular fácilmente lo que sucede cuando el número de términos de presentación disminuye 2. Hay complejidades que surgen de los términos que aparecen en la parte superior L de múltiples grupos, pero estas son excepciones de M a M: mantendremos todos los juicios de los términos superiores L = M / K en cada grupo y descartaremos los de los demás.",
        "La Tabla 3 muestra el rendimiento de varios algoritmos a medida que el número de términos de presentación varía de 6 a 48.",
        "Observamos que el rendimiento de TFB es más susceptible a la reducción de términos de presentación que el de CFB o TCFB.",
        "Por ejemplo, en 12 términos el MAP de TFB3C es el 90.6% del que se obtiene en 48 términos, mientras que los números para CFB3C y TCFB3C son del 98.0% y 96.1% respectivamente.",
        "Conjeturamos que la razón es que mientras el rendimiento de TFB depende en gran medida de cuántos términos buenos se elijan para la expansión de la consulta, CFB solo necesita una estimación aproximada de los pesos de los clústeres para funcionar.",
        "Además, las formas de aclaración de 3 × 16 parecen ser más robustas que las de 6 × 8: con 12 términos, el MAP de TFB6C es el 87.1% del obtenido con 48 términos, por debajo del 90.6% de TFB3C.",
        "De manera similar, para CFB es del 95.0% en comparación con el 98.0%.",
        "Esto es natural, ya que para un gran número de 6 clusters, es más fácil llegar a la situación en la que cada cluster recibe muy pocos términos de presentación para que la diversificación de temas sea útil.",
        "En general, nos sorprende ver que los algoritmos aún pueden funcionar razonablemente bien cuando el número de términos de presentación es pequeño.",
        "Por ejemplo, con solo 12 términos, CFB3C (el formulario de aclaración es de tamaño 3 × 4) aún puede mejorar un 36.5% sobre el valor base, disminuyendo ligeramente desde el 39.3% con 48 términos. En el Análisis de Retroalimentación del Usuario, estudiamos varios aspectos del comportamiento de retroalimentación de los usuarios en términos, y si están conectados al rendimiento de recuperación.",
        "Figura 2: Distribuciones de tiempo de completación del formulario de aclaración 0−30 30−60 60−90 90−120 120−150 150−180 0 5 10 15 20 25 30 35 tiempo de completación (segundos) #temas 1×48 3×16 6×8 La Figura 2 muestra la distribución del tiempo necesario para completar un formulario de aclaración.",
        "Observamos que el usuario suele ser capaz de completar el feedback del término en un tiempo razonablemente corto: para más de la mitad de los temas, el formulario de aclaración se completa en solo 1 minuto, y solo una pequeña fracción de temas (menos del 10% para 1 × 48 y 3 × 16) tarda más de 2 minutos.",
        "Esto sugiere que la retroalimentación a corto plazo es adecuada para la recuperación interactiva ad-hoc, donde un usuario generalmente no desea dedicar demasiado tiempo a proporcionar retroalimentación.",
        "Observamos que un usuario a menudo comete errores al juzgar la relevancia de un término.",
        "A veces, un término relevante puede ser omitido porque su conexión con el tema de la consulta no es obvia para el usuario.",
        "En otras ocasiones, puede incluirse un término dudoso que resulta ser irrelevante.",
        "Toma como ejemplo el tema en la Figura 1.",
        "Hubo un desastre de incendio en Mont 3. El tiempo máximo es de 180 segundos, ya que el evaluador del NIST estaría obligado a enviar el formulario en ese momento.",
        "Tabla 4: Estadísticas de selección de términos (promedio del tema) CF Tipo 1 × 48 3 × 16 6 × 8 # términos revisados 14.8 13.3 11.2 # términos relevantes 15.0 12.6 11.2 # términos relevantes revisados 7.9 6.9 5.9 precisión 0.534 0.519 0.527 recuperación 0.526 0.548 0.527 El usuario no seleccionó palabras clave como mont, blanc, francés e italiano debido a su desconocimiento del evento del Túnel del Mont Blanc entre Francia e Italia en 1999.",
        "De hecho, sin un contexto adecuado sería difícil hacer un juicio perfecto.",
        "¿Cuál es entonces, el nivel en el que el usuario es bueno dando retroalimentación?",
        "¿Tiene un impacto serio en el rendimiento de recuperación?",
        "Para responder a estas preguntas, necesitamos una medida de la verdadera relevancia de los términos individuales.",
        "Adoptamos la métrica de Divergencia KL Simplificada utilizada en [24] para decidir los términos de expansión de la consulta como nuestra medida de relevancia de términos: σKLD(w) = p(w|R) log p(w|R) p(w|¬R) donde p(w|R) es la probabilidad de que un documento relevante contenga el término w, y p(w|¬R) es la probabilidad de que un documento irrelevante contenga w, ambas pueden ser fácilmente calculadas mediante una estimación de máxima verosimilitud dada la relevancia a nivel de documento.",
        "Si σKLD(w) > 0, w es más probable que aparezca en documentos relevantes que en irrelevantes.",
        "Consideramos un término relevante si su valor de Divergencia KL Simplificada es mayor que un umbral determinado σ0.",
        "Podemos entonces definir la precisión y la exhaustividad del juicio de términos del usuario de la siguiente manera: la precisión es la fracción de términos revisados por el usuario que son relevantes; la exhaustividad es la fracción de términos relevantes presentados que son revisados por el usuario.",
        "La Tabla 4 muestra el número de términos revisados, términos relevantes y términos relevantes revisados cuando σ0 se establece en 1.0, así como la precisión/recuperación del juicio de términos del usuario.",
        "Ten en cuenta que cuando los formularios de aclaración contienen más grupos, se revisan menos términos: 14.8 para 1 × 48, 13.3 para 3 × 16 y 11.2 para 6×8.",
        "El patrón similar se mantiene para los términos relevantes y los términos relevantes verificados.",
        "Parece haber un compromiso entre aumentar la diversidad de temas mediante el agrupamiento y perder términos adicionales relevantes: cuando hay más grupos, cada uno recibe menos términos para presentar, lo que puede perjudicar a un grupo relevante importante que contiene muchos términos relevantes.",
        "Por lo tanto, no siempre es útil tener más grupos, por ejemplo, TFB6C es en realidad peor que TFB1C.",
        "El hallazgo principal que podemos obtener de la Tabla 4 es que el usuario no es particularmente bueno identificando términos relevantes, lo cual coincide con el descubrimiento en [18].",
        "En el caso de 3 formularios de aclaración de 16, el número promedio de términos marcados como relevantes por el usuario es de 13.3 por tema, y el número promedio de términos relevantes cuyo valor de σKLD excede 1.0 es de 12.6.",
        "El usuario es capaz de reconocer solo 6.9 de estos términos en promedio.",
        "De hecho, la precisión y la exhaustividad de los términos de retroalimentación de los usuarios (como se definió anteriormente) están lejos de ser perfectas.",
        "Por otro lado, si el usuario hubiera verificado correctamente todos esos términos relevantes, el rendimiento de nuestros algoritmos habría aumentado considerablemente, como se muestra en la Tabla 5.",
        "Vemos que TFB experimenta una gran mejora cuando hay un oráculo que verifica todos los términos relevantes, mientras que CFB se encuentra con un cuello de botella alrededor de un MAP de 0.325, ya que todo lo que hace es ajustar los pesos de los clústeres, y cuando los pesos aprendidos están cerca de ser precisos, no puede beneficiarse más del feedback de términos.",
        "También hay que tener en cuenta que TCFB no logra superar a TFB, probablemente porque TFB es lo suficientemente preciso.",
        "Tabla 5: Cambio de MAP al usar todos (y solo) los términos relevantes (σKLD > 1.0) para retroalimentación. retroalimentación de término original retroalimentación de término relevante TF1 0.288 0.354 TF3 0.288 0.354 TF6 0.278 0.346 CF3 0.305 0.325 CF6 0.301 0.326 TCF3 0.309 0.345 TCF6 0.304 0.341 6.4 Comparación con Retroalimentación de Relevancia Ahora comparamos la retroalimentación de términos con la retroalimentación de relevancia a nivel de documento, en la que al usuario se le presentan los N mejores documentos de una recuperación inicial y se le pide que juzgue su relevancia.",
        "El proceso de retroalimentación se simula utilizando la evaluación de relevancia de documentos de NIST.",
        "Utilizamos el método de retroalimentación basado en el modelo de mezcla propuesto en [25], con el ruido de mezcla establecido en 0.95 y el coeficiente de retroalimentación establecido en 0.9.",
        "La evaluación comparativa del feedback de relevancia frente a otros métodos se complica por el hecho de que algunos documentos ya han sido vistos durante el feedback, por lo que no tiene sentido incluirlos en los resultados de recuperación de la segunda ejecución.",
        "Sin embargo, esto no se aplica al término retroalimentación.",
        "Por lo tanto, para que sea justo en cuanto a la ganancia de información de los usuarios, si los documentos de retroalimentación son relevantes, deben mantenerse en la parte superior del ranking; si son irrelevantes, deben ser excluidos.",
        "Por lo tanto, utilizamos retroalimentación de relevancia para producir un ranking de los 1000 documentos recuperados, pero excluyendo cada documento de retroalimentación, y luego añadimos los documentos relevantes de retroalimentación al principio.",
        "La tabla 6 muestra el rendimiento de la retroalimentación de relevancia para diferentes valores de N y lo compara con TCFB3C.",
        "Tabla 6: Rendimiento de la retroalimentación de relevancia para diferentes números de documentos de retroalimentación (N).",
        "Vemos que el rendimiento de TCFB3C es comparable al de la retroalimentación de relevancia utilizando 5 documentos.",
        "Aunque es menos eficiente que cuando hay 10 documentos de retroalimentación en términos de MAP y Pr@30, sí recupera más documentos (4947) al descender por la lista clasificada.",
        "Intentamos comparar la calidad de los términos insertados automáticamente en la retroalimentación de relevancia con la de los términos seleccionados manualmente en la retroalimentación de términos.",
        "Esto se logra truncando el modelo de consulta modificado por retroalimentación de relevancia a un tamaño igual al número de términos verificados para el mismo tema.",
        "Podemos comparar los términos en el modelo truncado con los términos verificados.",
        "La Figura 3 muestra la distribución de los puntajes σKLD de los términos.",
        "Observamos que el término \"feedback\" tiende a producir términos de expansión de mayor calidad (aquellos con σKLD > 1) en comparación con el \"relevance feedback\" (con 10 documentos de feedback).",
        "Esto no contradice el hecho de que este último produce un rendimiento de recuperación más alto.",
        "De hecho, cuando usamos el modelo de consulta truncada en lugar del modelo íntegro refinado a partir de la retroalimentación de relevancia, el MAP es solo 0.304.",
        "La verdad Figura 3: Comparación de la calidad del término de expansión entre retroalimentación de relevancia (con 10 documentos de retroalimentación) y retroalimentación de términos (con 3 × 16 CFs) −1−0 0−1 1−2 2−3 3−4 4−5 5−6 0 50 100 150 200 250 300 350 σKLD #términos retroalimentación de relevancia retroalimentación de términos es que, aunque hay muchos términos no deseados en el modelo de consulta expandido a partir de los documentos de retroalimentación, también hay más términos relevantes de los que el usuario puede seleccionar de la lista de términos de presentación generados con documentos de pseudo-retroalimentación, y los efectos positivos a menudo superan a los negativos.",
        "Estamos interesados en saber en qué circunstancias la retroalimentación basada en términos tiene ventajas sobre la retroalimentación basada en relevancia.",
        "Una de esas situaciones es cuando ninguno de los documentos de retroalimentación principales es relevante, lo que hace que la retroalimentación de relevancia sea inútil.",
        "Esto no es infrecuente, como uno podría haber pensado: de los 50 temas, hay 13 casos así cuando N = 5, 10 cuando N = 10, y aún 3 cuando N = 20.",
        "Cuando esto sucede, solo se puede retroceder al método original de recuperación; se pierde el poder de la retroalimentación de relevancia.",
        "Sorprendentemente, en 11 de 13 casos en los que parece imposible el feedback de relevancia, el usuario puede marcar al menos 2 términos relevantes de los 3 × 16 del formulario de aclaración (consideramos que el término t es relevante si σKLD(t) > 1.0).",
        "Además, en 10 de ellos TCFB3C supera la línea base de pseudo-retroalimentación, aumentando el MAP de 0.076 a 0.146 en promedio (estos son temas particularmente difíciles).",
        "Creemos que hay dos posibles explicaciones para este fenómeno de que la retroalimentación de términos esté activa incluso cuando la retroalimentación de relevancia no funciona: Primero, incluso si ninguno de los primeros N (supongamos que es un número pequeño) documentos son relevantes, aún podemos encontrar documentos relevantes en los primeros 60, que son más inclusivos pero generalmente inalcanzables cuando las personas están realizando retroalimentación de relevancia en la búsqueda interactiva ad-hoc, de donde podemos extraer términos de retroalimentación.",
        "Esto es cierto para el tema 367 de piratería, donde los 10 documentos principales de retroalimentación son todos sobre piratería de software, sin embargo, hay documentos entre el 10 y el 60 que tratan sobre piratería en los mares (que es la necesidad de información real), contribuyendo términos como pirata, barco para la selección en el formulario de aclaración.",
        "Segundo, para algunos temas, un documento debe cumplir con ciertas condiciones especiales para ser relevante.",
        "Los N documentos principales pueden estar relacionados con el tema, pero no obstante ser irrelevantes.",
        "En este caso, aún podemos extraer términos útiles de estos documentos, incluso si no califican como relevantes.",
        "Por ejemplo, en el tema 639 de compras en línea de consumidores, un documento necesita mencionar qué contribuye al crecimiento de las compras para realmente coincidir con la necesidad de información especificada, por lo tanto, ninguno de los 10 documentos de retroalimentación principales se considera relevante.",
        "Sin embargo, los términos de retroalimentación como venta al por menor, comercio son buenos para la expansión de consultas. 7.",
        "CONCLUSIONES En este artículo estudiamos el uso de la retroalimentación de términos para la recuperación interactiva de información en el enfoque de modelado del lenguaje.",
        "Propusimos un método basado en clusters para seleccionar términos de presentación, así como algoritmos para estimar modelos de consulta refinados a partir de la retroalimentación de términos de usuario.",
        "Observamos una mejora significativa en la precisión de recuperación proporcionada por la retroalimentación de términos, a pesar de que un usuario a menudo comete errores en la evaluación de relevancia que perjudican su rendimiento.",
        "Encontramos que el algoritmo de mejor rendimiento es TCFB, el cual se beneficia de la combinación de la evidencia de términos observados directamente con TFB y la relevancia de clúster aprendida indirectamente con CFB.",
        "Cuando redujimos el número de términos de presentación, el feedback del término aún puede mantener gran parte de su mejora de rendimiento sobre el valor base.",
        "Finalmente, comparamos la retroalimentación de términos con la retroalimentación de relevancia a nivel de documento, y encontramos que el rendimiento de TCFB3C está a la par con este último con 5 documentos de retroalimentación.",
        "Consideramos el término \"feedback\" como una alternativa viable al feedback de relevancia tradicional, especialmente cuando no hay documentos relevantes en la parte superior.",
        "Proponemos ampliar nuestro trabajo de varias maneras.",
        "Primero, queremos estudiar si el uso de varios contextos puede ayudar al usuario a identificar mejor la relevancia de los términos, sin sacrificar la simplicidad y la concisión de la retroalimentación de los términos.",
        "Segundo, actualmente todos los términos se presentan al usuario en un solo lote.",
        "En su lugar, podríamos considerar la retroalimentación iterativa de términos, presentando primero un pequeño número de términos y mostrando más términos después de recibir la retroalimentación del usuario o detenernos cuando la consulta refinada sea lo suficientemente buena.",
        "Los términos presentados deben ser seleccionados dinámicamente para maximizar los beneficios de aprendizaje en cualquier momento.",
        "Tercero, tenemos planes de incorporar retroalimentación de términos en nuestra barra de herramientas UCAIR[20], un complemento de Internet Explorer, para que funcione en búsquedas web.",
        "También estamos interesados en estudiar cómo combinar la retroalimentación de términos con la retroalimentación de relevancia o la retroalimentación implícita.",
        "Por ejemplo, podríamos permitir al usuario modificar dinámicamente los términos en un modelo de lenguaje aprendido a partir de documentos de retroalimentación. 8.",
        "AGRADECIMIENTO Este trabajo cuenta con el apoyo parcial de las becas de la Fundación Nacional de Ciencias IIS-0347933 e IIS-0428472. 9.",
        "REFERENCIAS [1] J. Allan.",
        "Retroalimentación de relevancia con demasiados datos.",
        "En Actas de la 18ª conferencia internacional anual de ACM SIGIR sobre investigación y desarrollo en recuperación de información, páginas 337-343, 1995. [2] J. Allan.",
        "Resumen de la pista HARD en TREC 2005 - Recuperación de alta precisión de documentos.",
        "En la Decimocuarta Conferencia de Recuperación de Información de 2005. [3] P. Anick.",
        "Utilizando retroalimentación terminológica para refinar la búsqueda web: un estudio basado en registros.",
        "En Actas de la 26ª conferencia internacional anual de ACM SIGIR sobre investigación y desarrollo en recuperación de información, páginas 88-95, 2003. [4] P. G. Anick y S. Tipirneni.",
        "El asistente de búsqueda de paráfrasis: retroalimentación terminológica para la búsqueda iterativa de información.",
        "En Actas de la 22ª conferencia internacional anual de ACM SIGIR sobre investigación y desarrollo en recuperación de información, páginas 153-159, 1999. [5] C. Buckley, G. Salton, J. Allan y A. Singhal.",
        "Expansión automática de consultas utilizando SMART.",
        "En Actas de la Tercera Conferencia de Recuperación de Texto, 1994. [6] D. Harman.",
        "Hacia la expansión interactiva de consultas.",
        "En Actas de la 11ª conferencia internacional anual de ACM SIGIR sobre investigación y desarrollo en recuperación de información, páginas 321-331, 1988. [7] N. A. Jaleel, A. Corrada-Emmanuel, Q. Li, X. Liu, C. Wade y J. Allan.",
        "UMass en TREC 2003: HARD y QA.",
        "En TREC, páginas 715-725, 2003. [8] H. Joho, C. Coverson, M. Sanderson y M. Beaulieu.",
        "Presentación jerárquica de términos de expansión.",
        "En Actas del simposio de informática aplicada de ACM de 2002, páginas 645-649, 2002. [9] K. S. Jones, S. Walker y S. E. Robertson.",
        "Un modelo probabilístico de recuperación de información: desarrollo y estado actual.",
        "Informe técnico 446, Laboratorio de Computación, Universidad de Cambridge, 1998. [10] D. Kelly, V. D. Dollu y X. Fu.",
        "El usuario locuaz: una fuente de términos independiente del documento para la expansión de consultas.",
        "En Actas de la 28ª conferencia internacional anual de ACM SIGIR sobre investigación y desarrollo en recuperación de información, páginas 457-464, 2005. [11] D. Kelly y X. Fu.",
        "Obtención de retroalimentación de relevancia de términos: una investigación sobre la fuente y el contexto de los términos.",
        "En Actas de la 29ª conferencia internacional anual de ACM SIGIR sobre investigación y desarrollo en recuperación de información, 2006. [12] J. Koenemann y N. Belkin.",
        "Un caso para la interacción: Un estudio del comportamiento y la efectividad de la recuperación de información interactiva.",
        "En Actas de la conferencia SIGCHI sobre factores humanos en sistemas informáticos, páginas 205-212, 1996. [13] V. Lavrenko y W. B. Croft.",
        "Modelos de lenguaje basados en relevancia.",
        "En Investigación y Desarrollo en Recuperación de Información, páginas 120-127, 2001. [14] Y. Nemeth, B. Shapira y M. Taeib-Maimon.",
        "Evaluación del valor real y percibido de la expansión automática e interactiva de consultas.",
        "En Actas de la 27ª conferencia internacional anual de ACM SIGIR sobre investigación y desarrollo en recuperación de información, páginas 526-527, 2004. [15] J. Ponte.",
        "Un enfoque de modelado del lenguaje para la recuperación de información.",
        "Tesis doctoral, Universidad de Massachusetts en Amherst, 1998. [16] S. E. Robertson, S. Walker, S. Jones, M. Beaulieu y M. Gatford.",
        "Okapi en TREC-3.",
        "En Actas de la Tercera Conferencia de Recuperación de Texto, 1994. [17] J. Rocchio.",
        "Retroalimentación de relevancia en la recuperación de información.",
        "En el sistema de recuperación SMART, páginas 313-323. 1971. [18] I. Ruthven.",
        "Reexaminando la efectividad potencial de la expansión interactiva de consultas.",
        "En Actas de la 26ª conferencia internacional anual de ACM SIGIR sobre investigación y desarrollo en recuperación de información, páginas 213-220, 2003. [19] G. Salton y C. Buckley.",
        "Mejorando el rendimiento de recuperación mediante retroalimentación de relevancia.",
        "Revista de la Sociedad Americana de Ciencia de la Información, 41:288-297, 1990. [20] X. Shen, B. Tan y C. Zhai.",
        "Modelado implícito de usuario para búsqueda personalizada.",
        "En Actas de la 14ª conferencia internacional de ACM sobre información y gestión del conocimiento, páginas 824-831, 2005. [21] X. Shen y C. Zhai.",
        "Retroalimentación activa en la recuperación de información ad-hoc.",
        "En Actas de la 28ª conferencia internacional anual de ACM SIGIR sobre investigación y desarrollo en recuperación de información, páginas 59-66, 2005. [22] A. Spink.",
        "Retroalimentación de relevancia de términos y expansión de consultas: relación con el diseño.",
        "En Actas de la 17ª conferencia internacional anual de ACM SIGIR sobre investigación y desarrollo en recuperación de información, páginas 81-90, 1994. [23] J. Xu y W. B. Croft.",
        "Expansión de consulta utilizando análisis local y global de documentos.",
        "En Actas de la 19ª conferencia internacional anual de ACM SIGIR sobre investigación y desarrollo en recuperación de información, páginas 4-11, 1996. [24] H. Zaragoza, N. Craswell, M. Taylor, S. Saria y S. Robertson.",
        "Microsoft Cambridge en TREC-13: pistas Web y HARD.",
        "En Actas de la 13ª Conferencia de Recuperación de Información de Texto, 2004. [25] C. Zhai y J. Lafferty.",
        "Retroalimentación basada en modelos en el enfoque de modelado del lenguaje para la recuperación de información.",
        "En Actas de la décima conferencia internacional sobre gestión de la información y el conocimiento, páginas 403-410, 2001. [26] C. Zhai, A. Velivelli y B. Yu.",
        "Un modelo de mezcla de colecciones cruzadas para la minería de textos comparativa.",
        "En Actas de la décima conferencia internacional de ACM SIGKDD sobre descubrimiento de conocimiento y minería de datos, páginas 743-748, 2004."
    ],
    "error_count": 6,
    "keys": {
        "term-based feedback": {
            "translated_key": "",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Term Feedback for Information Retrieval with Language Models Bin Tan† , Atulya Velivelli‡ , Hui Fang† , ChengXiang Zhai† Dept.",
                "of Computer Science† , Dept. of Electrical and Computer Engineering‡ University of Illinois at Urbana-Champaign bintan@cs.uiuc.edu, velivell@ifp.uiuc.edu, hfang@cs.uiuc.edu, czhai@cs.uiuc.edu ABSTRACT In this paper we study <br>term-based feedback</br> for information retrieval in the language modeling approach.",
                "With term feedback a user directly judges the relevance of individual terms without interaction with feedback documents, taking full control of the query expansion process.",
                "We propose a cluster-based method for selecting terms to present to the user for judgment, as well as effective algorithms for constructing refined query language models from user term feedback.",
                "Our algorithms are shown to bring significant improvement in retrieval accuracy over a non-feedback baseline, and achieve comparable performance to relevance feedback.",
                "They are helpful even when there are no relevant documents in the top.",
                "Categories and Subject Descriptors H.3.3 [Information Search and Retrieval]: Retrieval models General Terms Algorithms 1.",
                "INTRODUCTION In the language modeling approach to information retrieval, feedback is often modeled as estimating an improved query model or relevance model based on a set of feedback documents [25, 13].",
                "This is in line with the traditional way of doing relevance feedback - presenting a user with documents/passages for relevance judgment and then extracting terms from the judged documents or passages to expand the initial query.",
                "It is an indirect way of seeking users assistance for query model construction, in the sense that the refined query model (based on terms) is learned through feedback documents/passages, which are high-level structures of terms.",
                "It has the disadvantage that irrelevant terms, which occur along with relevant ones in the judged content, may be erroneously used for query expansion, causing undesired effects.",
                "For example, for the TREC query Hubble telescope achievements, when a relevant document talks more about the telescopes repair than its discoveries, irrelevant terms such as spacewalk can be added into the modified query.",
                "We can consider a more direct way to involve a user in query model improvement, without an intermediary step of document feedback that can introduce noise.",
                "The idea is to present a (reasonable) number of individual terms to the user and ask him/her to judge the relevance of each term or directly specify their probabilities in the query model.",
                "This strategy has been discussed in [15], but to our knowledge, it has not been seriously studied in existing language modeling literature.",
                "Compared to traditional relevance feedback, this term-based approach to interactive query model refinement has several advantages.",
                "First, the user has better control of the final query model through direct manipulation of terms: he/she can dictate which terms are relevant, irrelevant, and possibly, to what degree.",
                "This avoids the risk of bringing unwanted terms into the query model, although sometimes the user introduces low-quality terms.",
                "Second, because a term takes less time to judge than a documents full text or summary, and as few as around 20 presented terms can bring significant improvement in retrieval performance (as we will show later), term feedback makes it faster to gather user feedback.",
                "This is especially helpful for interactive adhoc search.",
                "Third, sometimes there are no relevant documents in the top N of the initially retrieved results if the topic is hard.",
                "This is often true when N is constrained to be small, which arises from the fact that the user is unwilling to judge too many documents.",
                "In this case, relevance feedback is useless, as no relevant document can be leveraged on, but term feedback is still often helpful, by allowing relevant terms to be picked from irrelevant documents.",
                "During our participation in the TREC 2005 HARD Track and continued study afterward, we explored how to exploit term feedback from the user to construct improved query models for information retrieval in the language modeling approach.",
                "We identified two key subtasks of <br>term-based feedback</br>, i.e., pre-feedback presentation term selection and post-feedback query model construction, with effective algorithms developed for both.",
                "We imposed a secondary cluster structure on terms and found that a cluster view sheds additional insight into the users information need, and provides a good way of utilizing term feedback.",
                "Through experiments we found that term feedback improves significantly over the nonfeedback baseline, even though the user often makes mistakes in relevance judgment.",
                "Among our algorithms, the one with best retrieval performance is TCFB, the combination of TFB, the direct term feedback algorithm, and CFB, the cluster-based feedback algorithm.",
                "We also varied the number of feedback terms and observed reasonable improvement even at low numbers.",
                "Finally, by comparing term feedback with document-level feedback, we found it to be a viable alternative to the latter with competitive retrieval performance.",
                "The rest of the paper is organized as follows.",
                "Section 2 discusses some related work.",
                "Section 4 outlines our general approach to term feedback.",
                "We present our method for presentation term selection in Section 3 and algorithms for query model construction in Section 5.",
                "The experiment results are given in Section 6.",
                "Section 7 concludes this paper. 2.",
                "RELATED WORK Relevance feedback[17, 19] has long been recognized as an effective method for improving retrieval performance.",
                "Normally, the top N documents retrieved using the original query are presented to the user for judgment, after which terms are extracted from the judged relevant documents, weighted by their potential of attracting more relevant documents, and added into the query model.",
                "The expanded query usually represents the users information need better than the original one, which is often just a short keyword query.",
                "A second iteration of retrieval using this modified query usually produces significant increase in retrieval accuracy.",
                "In cases where true relevance judgment is unavailable and all top N documents are assumed to be relevant, it is called blind or pseudo feedback[5, 16] and usually still brings performance improvement.",
                "Because document is a large text unit, when it is used for relevance feedback many irrelevant terms can be introduced into the feedback process.",
                "To overcome this, passage feedback is proposed and shown to improve feedback performance[1, 23].",
                "A more direct solution is to ask the user for their relevance judgment of feedback terms.",
                "For example, in some relevance feedback systems such as [12], there is an interaction step that allows the user to add or remove expansion terms after they are automatically extracted from relevant documents.",
                "This is categorized as interactive query expansion, where the original query is augmented with user-provided terms, which can come from direct user input (free-form text or keywords)[22, 7, 10] or user selection of system-suggested terms (using thesauri[6, 22] or extracted from feedback documents[6, 22, 12, 4, 7]).",
                "In many cases term relevance feedback has been found to effectively improve retrieval performance[6, 22, 12, 4, 10].",
                "For example, the study in [12] shows that the user prefers to have explicit knowledge and direct control of which terms are used for query expansion, and the penetrable interface that provides this freedom is shown to perform better than other interfaces.",
                "However, in some other cases there is no significant benefit[3, 14], even if the user likes interacting with expansion terms.",
                "In a simulated study carried out in [18], the author compares the retrieval performance of interactive query expansion and automatic query expansion with a simulated study, and suggests that the potential benefits of the former can be hard to achieve.",
                "The user is found to be not good at identifying useful terms for query expansion, when a simple term presentation interface is unable to provide sufficient semantic context of the feedback terms.",
                "Our work differs from the previous ones in two important aspects.",
                "First, when we choose terms to present to the user for relevance judgment, we not only consider single-term value (e.g., the relative frequency of a term in the top documents, which can be measured by metrics such as Robertson Selection Value and Simplified Kullback-Leibler Distance as listed in [24]), but also examine the cluster structure of the terms, so as to produce a balanced coverage of the different topic aspects.",
                "Second, with the language modelling framework, we allow an elaborate construction of the updated query model, by setting different probabilities for different terms based on whether it is a query term, its significance in the top documents, and its cluster membership.",
                "Although techniques for adjusting query term weights exist for vector space models[17] and probablistic relevance models[9], most of the aforementioned works do not use them, choosing to just append feedback terms to the original query (thus using equal weights for them), which can lead to poorer retrieval performance.",
                "The combination of the two aspects allows our method to perform much better than the baseline.",
                "The usual way for feedback term presentation is just to display the terms in a list.",
                "There have been some works on alternative user interfaces. [8] arranges terms in a hierarchy, and [11] compares three different interfaces, including terms + checkboxes, terms + context (sentences) + checkboxes, sentences + input text box.",
                "In both studies, however, there is no significant performance difference.",
                "In our work we adopt the simplest approach of terms + checkboxes.",
                "We focus on term presentation and query model construction from feedback terms, and believe using contexts to improve feedback term quality should be orthogonal to our method. 3.",
                "GENERAL APPROACH We follow the language modeling approach, and base our method on the KL-divergence retrieval model proposed in [25].",
                "With this model, the retrieval task involves estimating a query language model θq from a given query, a document language model θd from each document, and calculating their KL-divergence D(θq||θd), which is then used to score the documents. [25] treats relevance feedback as a query model re-estimation problem, i.e., computing an updated query model θq given the original query text and the extra evidence carried by the judged relevant documents.",
                "We adopt this view, and cast our task as updating the query model from user term feedback.",
                "There are two key subtasks here: First, how to choose the best terms to present to the user for judgment, in order to gather maximal evidence about the users information need.",
                "Second, how to compute an updated query model based on this term feedback evidence, so that it captures the users information need and translates into good retrieval performance. 4.",
                "PRESENTATION TERM SELECTION Proper selection of terms to be presented to the user for judgment is crucial to the success of term feedback.",
                "If the terms are poorly chosen and there are few relevant ones, the user will have a hard time looking for useful terms to help clarify his/her information need.",
                "If the relevant terms are plentiful, but all concentrate on a single aspect of the query topic, then we will only be able to get feedback on that aspect and missing others, resulting in a breadth loss in retrieved results.",
                "Therefore, it is important to carefully select presentation terms to maximize expected gain from user feedback, i.e., those that can potentially reveal most evidence of the users information need.",
                "This is similar to active feedback[21], which suggests that a retrieval system should actively probe the users information need, and in the case of relevance feedback, the feedback documents should be chosen to maximize learning benefits (e.g. diversely so as to increase coverage).",
                "In our approach, the top N documents from an initial retrieval using the original query form the source of feedback terms: all terms that appear in them are considered candidates to present to the user.",
                "These documents serve as pseudo-feedback, since they provide a much richer context than the original query (usually very short), while the user is not asked to judge their relevance.",
                "Due to the latter reason, it is possible to make N quite large (e.g., in our experiments we set N = 60) to increase its coverage of different aspects in the topic.",
                "The simplest way of selecting feedback terms is to choose the most frequent M terms from the N documents.",
                "This method, however, has two drawbacks.",
                "First, a lot of common noisy terms will be selected due to their high frequencies in the document collection, unless a stop-word list is used for filtering.",
                "Second, the presentation list will tend to be filled by terms from major aspects of the topic; those from a minor aspect are likely to be missed due to their relatively low frequencies.",
                "We solve the above problems by two corresponding measures.",
                "First, we introduce a background model θB that is estimated from collection statistics and explains the common terms, so that they are much less likely to appear in the presentation list.",
                "Second, the terms are selected from multiple clusters in the pseudo-feedback documents, to ensure sufficient representation of different aspects of the topic.",
                "We rely on the mixture multinomial model, which is used for theme discovery in [26].",
                "Specifically, we assume the N documents contain K clusters {Ci| i = 1, 2, · · · K}, each characterized by a multinomial word distribution (also known as unigram language model) θi and corresponding to an aspect of the topic.",
                "The documents are regarded as sampled from a mixture of K + 1 components, including the K clusters and the background model: p(w|d) = λBp(w|θB) + (1 − λB) K i=1 πd,ip(w|θi) where w is a word, λB is the mixture weight for the background model θB, and πd,i is the document-specific mixture weight for the i-th cluster model θi.",
                "We then estimate the cluster models by maximizing the probability of the pseudo-feedback documents being generated from the multinomial mixture model: log p(D|Λ) = d∈D w∈V c(w; d) log p(w|d) where D = {di| i = 1, 2, · · · N} is the set of the N documents, V is the vocabulary, c(w; d) is ws frequency in d and Λ = {θi| i = 1, 2, · · · K} ∪ {πdij | i = 1, 2, · · · N, j = 1, 2, · · · K} is the set of model parameters to estimate.",
                "The cluster models can be efficiently estimated using the Expectation-Maximization (EM) algorithm.",
                "For its details, we refer the reader to [26].",
                "Table 1 shows the cluster models for TREC query Transportation tunnel disasters (K = 3).",
                "Note that only the middle cluster is relevant.",
                "Table 1: Cluster models for topic 363 Transportation tunnel disasters Cluster 1 Cluster 2 Cluster 3 tunnel 0.0768 tunnel 0.0935 tunnel 0.0454 transport 0.0364 fire 0.0295 transport 0.0406 traffic 0.0206 truck 0.0236 toll 0.0166 railwai 0.0186 french 0.0220 amtrak 0.0153 harbor 0.0146 smoke 0.0157 train 0.0129 rail 0.0140 car 0.0154 airport 0.0122 bridg 0.0139 italian 0.0152 turnpik 0.0105 kilomet 0.0136 firefight 0.0144 lui 0.0095 truck 0.0133 blaze 0.0127 jersei 0.0093 construct 0.0131 blanc 0.0121 pass 0.0087 · · · · · · · · · From each of the K estimated clusters, we choose the L = M/K terms with highest probabilities to form a total of M presentation terms.",
                "If a term happens to be in top L in multiple clusters, we assign it to the cluster where it has highest probability and let the other clusters take one more term as compensation.",
                "We also filter out terms in the original query text because they tend to always be relevant when the query is short.",
                "The selected terms are then presented to the user for judgment.",
                "A sample (completed) feedback form is shown in Figure 1.",
                "In this study we only deal with binary judgment: a presented term is by default unchecked, and a user may check it to indicate relevance.",
                "We also do not explicitly exploit negative feedback (i.e., penalizing irrelevant terms), because with binary feedback an unchecked term is not necessarily irrelevant (maybe the user is unsure about its relevance).",
                "We could ask the user for finer judgment (e.g., choosing from highly relevant, somewhat relevant, do not know, somewhat irrelevant and highly irrelevant), but binary feedback is more compact, taking less space to display and less user effort to make judgment. 5.",
                "ESTIMATING QUERY MODELS FROM TERM FEEDBACK In this section, we present several algorithms for exploiting term feedback.",
                "The algorithms take as input the original query q, the clusters {θi} as generated by the theme discovery algorithm, the set of feedback terms T and their relevance judgment R, and outputs an updated query language model θq that makes best use of the feedback evidence to capture the users information need.",
                "First we describe our notations: • θq: The original query model, derived from query terms only: p(w|θq) = c(w; q) |q| where c(w; q) is the count of w in q, and |q| = w∈q c(w; q) is the query length. • θq : The updated query model which we need to estimate from term feedback. • θi (i = 1, 2, . . .",
                "K): The unigram language model of cluster Ci, as estimated using the theme discovery algorithm. • T = {ti,j} (i = 1 . . .",
                "K, j = 1 . . .",
                "L): The set of terms presented to the user for judgment. ti,j is the j-th term chosen from cluster Ci. • R = {δw|w ∈ T}: δw is an indicator variable that is 1 if w is judged relevant or 0 otherwise. 5.1 TFB (Direct Term Feedback) This is a straight-forward form of term feedback that does not involve any secondary structure.",
                "We give a weight of 1 to terms judged relevant by the user, a weight of μ to query terms, zero weight to other terms, and then apply normalization: p(w|θq ) = δw + μ c(w; q) w ∈T δw + μ|q| where w ∈T δw is the total number of terms that are judged relevant.",
                "We call this method TFB (direct Term FeedBack).",
                "If we let μ = 1, this approach is equivalent to appending the relevant terms after the original query, which is what standard query expansion (without term reweighting) does.",
                "If we set μ > 1, we are putting more emphasis on the query terms than the checked ones.",
                "Note that the result model will be more biased toward θq if the original query is long or the user feedback is weak, which makes sense, as we can trust more on the original query in either case.",
                "Figure 1: Filled clarification form for Topic 363 363 transportation tunnel disasters Please select all terms that are relevant to the topic. traffic railway harbor rail bridge kilometer construct swiss cross link kong hong river project meter shanghai fire truck french smoke car italian firefights blaze blanc mont victim franc rescue driver chamonix emerge toll amtrak train airport turnpike lui jersey pass rome z center electron road boston speed bu submit 5.2 CFB (Cluster Feedback) Here we exploit the cluster structure that played an important role when we selected the presentation terms.",
                "The clusters represent different aspects of the query topic, each of which may or may not be relevant.",
                "If we are able to identify the relevant clusters, we can combine them to generate a query model that is good at discovering documents belonging to these clusters (instead of the irrelevant ones).",
                "We could ask the user to directly judge the relevance of a cluster after viewing representative terms in that cluster, but this would sometimes be a difficult task for the user, who has to guess the semantics of a cluster via its set of terms, which may not be well connected to one another due to a lack of context.",
                "Therefore, we propose to learn cluster feedback indirectly, inferring the relevance of a cluster through the relevance of its feedback terms.",
                "Because each cluster has an equal number of terms presented to the user, the simplest measure of a clusters relevance is the number of terms that are judged relevant in it.",
                "Intuitively, the more terms are marked relevant in a cluster, the closer the cluster is to the query topic, and the more the cluster should participate in query modification.",
                "If we combine the cluster models using weights determined this way and then interpolate with the original query model, we get the following formula for query updating, which we call CFB (Cluster FeedBack): p(w|θq ) = λp(w|θq) + (1 − λ) K i=1 L j=1 δti,j K k=1 L j=1 δtk,j p(w|θi) where L j=1 δti,j is the number of relevant terms in cluster Ci, and K k=1 L j=1 δtk,j is the total number of relevant terms.",
                "We note that when there is only one cluster (K = 1), the above formula degenerates to p(w|θq ) = λp(w|θq) + (1 − λ)p(w|θ1) which is merely pseudo-feedback of the form proposed in [25]. 5.3 TCFB (Term-cluster Feedback) TFB and CFB both have their drawbacks.",
                "TFB assigns non-zero probabilities to the presented terms that are marked relevant, but completely ignores (a lot more) others, which may be left unchecked due to the users ignorance, or simply not included in the presentation list, but we should be able to infer their relevance from the checked ones.",
                "For example, in Figure 1, since as many as 5 terms in the middle cluster (the third and fourth columns) are checked, we should have high confidence in the relevance of other terms in that cluster.",
                "CFB remedies TFBs problem by treating the terms in a cluster collectively, so that unchecked/unpresented terms receive weights when presented terms in their clusters are judged as relevant, but it does not distinguish which terms in a cluster are presented or judged.",
                "Intuitively, the judged relevant terms should receive larger weights because they are explicitly indicated as relevant by the user.",
                "Therefore, we try to combine the two methods, hoping to get the best out of both.",
                "We do this by interpolating the TFB model with the CFB model, and call it TCFB: p(w|θq ) = αp(w|θqT F B ) + (1 − α)p(w|θqCF B ) 6.",
                "EXPERIMENTS In this section, we describe our experiment results.",
                "We first describe our experiment setup and present an overview of various methods performance.",
                "Then we discuss the effects of varying the parameter setting in the algorithms, as well as the number of presentation terms.",
                "Next we analyze user term feedback behavior and its relation to retrieval performance.",
                "Finally we compare term feedback to relevance feedback and show that it has its particular advantage. 6.1 Experiment Setup and Basic Results We took the opportunity of TREC 2005 HARD Track[2] for the evaluation of our algorithms.",
                "The tracks used the AQUAINT collection, a 3GB corpus of English newswire text.",
                "The topics included 50 ones previously known to be hard, i.e. with low retrieval performance.",
                "It is for these hard topics that user feedback is most helpful, as it can provide information to disambiguate the queries; with easy topics the user may be unwilling to spend efforts for feedback if the automatic retrieval results are good enough.",
                "Participants of the track were able to submit custom-designed clarification forms (CF) to solicit feedback from human assessors provided by Table 2: Retrieval performance for different methods and CF types.",
                "The last row is the percentage of MAP improvement over the baseline.",
                "The parameter settings μ = 4, λ = 0.1, α = 0.3 are near optimal.",
                "Baseline TFB1C TFB3C TFB6C CFB1C CFB3C CFB6C TCFB1C TCFB3C TCFB6C MAP 0.219 0.288 0.288 0.278 0.254 0.305 0.301 0.274 0.309 0.304 Pr@30 0.393 0.467 0.475 0.457 0.399 0.480 0.473 0.431 0.491 0.473 RR 4339 4753 4762 4740 4600 4907 4872 4767 4947 4906 % 0% 31.5% 31.5% 26.9% 16.0% 39.3% 37.4% 25.1% 41.1% 38.8% Table 3: MAP variation with the number of presented terms. # terms TFB1C TFB3C TFB6C CFB3C CFB6C TCFB3C TCFB6C 6 0.245 0.240 0.227 0.279 0.279 0.281 0.274 12 0.261 0.261 0.242 0.299 0.286 0.297 0.281 18 0.275 0.274 0.256 0.301 0.282 0.300 0.286 24 0.276 0.281 0.265 0.303 0.292 0.305 0.292 30 0.280 0.285 0.270 0.304 0.296 0.307 0.296 36 0.282 0.288 0.272 0.307 0.297 0.309 0.297 42 0.283 0.288 0.275 0.306 0.298 0.309 0.300 48 0.288 0.288 0.278 0.305 0.301 0.309 0.303 NIST.",
                "We designed three sets of clarification forms for term feedback, differing in the choice of K, the number of clusters, and L, the number of presented terms from each cluster.",
                "They are: 1× 48, a big cluster with 48 terms, 3 × 16, 3 clusters with 16 terms each, and 6 × 8, 6 clusters with 8 terms each.",
                "The total number of presented terms (M) is fixed at 48, so by comparing the performance of different types of clarification forms we can know the effects of different degree of clustering.",
                "For each topic, an assessor would complete the forms ordered by 6 × 8, 1 × 48 and 3 × 16, spending up to three minutes on each form.",
                "The sample clarification form shown in Figure 1 is of type 3 × 16.",
                "It is a simple and compact interface in which the user can check relevant terms.",
                "The form is self-explanatory; there is no need for extra user training on how to use it.",
                "Our initinal queries are constructed only using the topic title descriptions, which are on average 2.7 words in length.",
                "As our baseline we use the KL divergence retrieval method implemented in the Lemur Toolkit1 with 5 pseudo-feedback documents.",
                "We stem the terms, choose Dirichlet smoothing with a prior of 2000, and truncate query language models to 50 terms (these settings are used throughout the experiments).",
                "For all other parameters we use Lemurs default settings.",
                "The baseline turns out to perform above average among the track participants.",
                "After an initial run using this baseline retrieval method, we take the top 60 documents for each topic and apply the theme discovery algorithm to output the clusters (1, 3, or 6 of them), based on which we generate clarification forms.",
                "After user feedback is received, we run the term feedback algorithms (TFB, CFB or TCFB) to estimate updated query models, which are then used for a second iteration of retrieval.",
                "We evaluate the different retrieval methods performance on their rankings of the top 1000 documents.",
                "The evaluation metrics we adopt include mean average (non-interpolated) precision (MAP), precision at top 30 (Pr@30) and total relevant retrieved (RR).",
                "Table 2 shows the performance of various methods and configurations of K × L. The suffixes (1C, 3C, 6C) after TFB,CFB,TCFB stand for the number of clusters (K).",
                "For example, TCFB3C means the TCFB method on the 3 × 16 clarification forms.",
                "From Table 2 we can make the following observations: 1 http://www.lemurproject.com 1.",
                "All methods perform considerably better than the pseudofeedback baseline, with TCFB3C achieving a highest 41.1% improvement in MAP, indicating significant contribution of term feedback for clarification of the users information need.",
                "In other words, term feedback is truly helpful for improving retrieval accuracy. 2.",
                "For TFB, the performance is almost equal on the 1 × 48 and 3 × 16 clarification forms in terms of MAP (although the latter is slightly better in Pr@30 and RR), and a little worse on the 6 × 8 ones. 3.",
                "Both CFB3C and CFB6C perform better than their TFB counterparts in all three metrics, suggesting that feedback on a secondary cluster structure is indeed beneficial.",
                "CFB1C is actually worse because it cannot adjust the weight of its (single) cluster from term feedback and it is merely pseudofeedback. 4.",
                "Although TCFB is just a simple mixture of TFB and CFB by interpolation, it is able to outperform both.",
                "This supports our speculation that TCFB overcomes the drawbacks of TFB (paying attention only to checked terms) and CFB (not distinguishing checked and unchecked terms in a cluster).",
                "Except for TCFB6C v.s.",
                "CFB6C, the performance advantage of TCFB over TFB/CFB is significant at p < 0.05 using the Wilcoxon signed rank test.",
                "This is not true in the case of TFB v.s.",
                "CFB, each of which is better than the other in nearly half of the topics. 6.2 Reduction of Presentation Terms In some situations we may have to reduce the number of presentation terms due to limits in display space or user feedback efforts.",
                "It is interesting to know whether our algorithms performance deteriorates when the user is presented with fewer terms.",
                "Because the presentation terms within each cluster are generated in decreasing order of their frequencies, the presentation list forms a subset of the original one if its size is reduced2 .",
                "Therefore, we can easily simulate what happens when the number of presentation terms decreases 2 There are complexities arising from terms appearing in top L of multiple clusters, but these are exceptions from M to M : we will keep all judgments of the top L = M /K terms in each cluster and discard those of others.",
                "Table 3 shows the performance of various algorithms as the number of presentation terms ranges from 6 to 48.",
                "We find that the performance of TFB is more susceptible to presentation term reduction than that of CFB or TCFB.",
                "For example, at 12 terms the MAP of TFB3C is 90.6% of that at 48 terms, while the numbers for CFB3C and TCFB3C are 98.0% and 96.1% respectively.",
                "We conjecture the reason to be that while TFBs performance heavily depends on how many good terms are chosen for query expansion, CFB only needs a rough estimate of cluster weights to work.",
                "Also, the 3 × 16 clarification forms seem to be more robust than the 6 × 8 ones: at 12 terms the MAP of TFB6C is 87.1% of that at 48 terms, lower than 90.6% for TFB3C.",
                "Similarly, for CFB it is 95.0% against 98.0%.",
                "This is natual, as for a large cluster number of 6, it is easier to get into the situation where each cluster gets too few presentation terms to make topic diversification useful.",
                "Overall, we are surprised to see that the algorithms are still able to perform reasonably well when the number of presentation terms is small.",
                "For example, at only 12 terms CFB3C (the clarification form is of size 3 × 4) can still improve 36.5% over the baseline, dropping slightly from 39.3% at 48 terms. 6.3 User Feedback Analysis In this part we study several aspects of users term feedback behavior, and whether they are connected to retrieval performance.",
                "Figure 2: Clarification form completion time distributions 0−30 30−60 60−90 90−120 120−150 150−180 0 5 10 15 20 25 30 35 completion time (seconds) #topics 1×48 3×16 6×8 Figure 2 shows the distribution of time needed to complete a clarification form3 .",
                "We see that the user is usually able to finish term feedback within a reasonably short amount of time: for more than half of the topics the clarification form is completed in just 1 minute, and only a small fraction of topics (less than 10% for 1 × 48 and 3 × 16) take more than 2 minutes.",
                "This suggests that term feedback is suitable for interactive ad-hoc retrieval, where a user usually does not want to spend too much time on providing feedback.",
                "We find that a user often makes mistakes when judging term relevance.",
                "Sometimes a relevant term may be left out because its connection to the query topic is not obvious to the user.",
                "Other times a dubious term may be included but turns out to be irrelevant.",
                "Take the topic in Figure 1 for example.",
                "There was a fire disaster in Mont 3 The maximal time is 180 seconds, as the NIST assessor would be forced to submit the form at that moment.",
                "Table 4: Term selection statistics (topic average) CF Type 1 × 48 3 × 16 6 × 8 # checked terms 14.8 13.3 11.2 # rel. terms 15.0 12.6 11.2 # rel. checked terms 7.9 6.9 5.9 precision 0.534 0.519 0.527 recall 0.526 0.548 0.527 Blanc Tunnel between France and Italy in 1999, but the user failed to select such keywords as mont, blanc, french and italian due to his/her ignorance of the event.",
                "Indeed, without proper context it would be hard to make perfect judgment.",
                "What is then, the extent to which the user is good at term feedback?",
                "Does it have serious impact on retrieval performance?",
                "To answer these questions, we need a measure of individual terms true relevance.",
                "We adopt the Simplified KL Divergence metric used in [24] to decide query expansion terms as our term relevance measure: σKLD(w) = p(w|R) log p(w|R) p(w|¬R) where p(w|R) is the probability that a relevant document contains term w, and p(w|¬R) is the probability that an irrelevant document contains w, both of which can be easily computed via maximum likelihood estimate given document-level relevance judgment.",
                "If σKLD(w) > 0, w is more likely to appear in relevant documents than irrelevant ones.",
                "We consider a term relevant if its Simplified KL Divergence value is greater than a certain threshold σ0.",
                "We can then define precision and recall of user term judgment accordingly: precision is the fraction of terms checked by the user that are relevant; recall is the fraction of presented relevant terms that are checked by the user.",
                "Table 4 shows the number of checked terms, relevant terms and relevant checked terms when σ0 is set to 1.0, as well as the precision/recall of user term judgment.",
                "Note that when the clarification forms contain more clusters, fewer terms are checked: 14.8 for 1 × 48, 13.3 for 3 × 16 and 11.2 for 6×8.",
                "Similar pattern holds for relevant terms and relevant checked terms.",
                "There seems to be a trade-off between increasing topic diversity by clustering and losing extra relevant terms: when there are more clusters, each of them gets fewer terms to present, which can hurt a major relevant cluster that contains many relevant terms.",
                "Therefore, it is not always helpful to have more clusters, e.g., TFB6C is actually worse than TFB1C.",
                "The major finding we can make from Table 4 is that the user is not particularly good at identifying relevant terms, which echoes the discovery in [18].",
                "In the case of 3 × 16 clarification forms, the average number of terms checked as relevant by the user is 13.3 per topic, and the average number of relevant terms whose σKLD value exceed 1.0 is 12.6.",
                "The user is able to recognize only 6.9 of these terms on average.",
                "Indeed, the precision and recall of user feedback terms (as defined previously) are far from perfect.",
                "On the other hand, If the user had correctly checked all such relevant terms, the performance of our algorithms would have increased a lot, as shown in Table 5.",
                "We see that TFB gets big improvement when there is an oracle who checks all relevant terms, while CFB meets a bottleneck around MAP of 0.325, since all it does is adjust cluster weights, and when the learned weights are close to being accurate, it cannot benefit more from term feedback.",
                "Also note that TCFB fails to outperform TFB, probably because TFB is sufficiently accurate.",
                "Table 5: Change of MAP when using all (and only) relevant terms (σKLD > 1.0) for feedback. original term feedback relevant term feedback TF1 0.288 0.354 TF3 0.288 0.354 TF6 0.278 0.346 CF3 0.305 0.325 CF6 0.301 0.326 TCF3 0.309 0.345 TCF6 0.304 0.341 6.4 Comparison with Relevance Feedback Now we compare term feedback with document-level relevance feedback, in which the user is presented with the top N documents from an initial retrieval and asked to judge their relevance.",
                "The feedback process is simulated using document relevance judgment from NIST.",
                "We use the mixture model based feedback method proposed in [25], with mixture noise set to 0.95 and feedback coefficient set to 0.9.",
                "Comparative evaluation of relevance feedback against other methods is complicated by the fact that some documents have already been viewed during feedback, so it makes no sense to include them in the retrieval results of the second run.",
                "However, this does not hold for term feedback.",
                "Thus, to make it fair w.r.t. users information gain, if the feedback documents are relevant, they should be kept in the top of the ranking; if they are irrelevant, they should be left out.",
                "Therefore, we use relevance feedback to produce a ranking of top 1000 retrieved documents but with every feedback document excluded, and then prepend the relevant feedback documents at the front.",
                "Table 6 shows the performance of relevance feedback for different values of N and compares it with TCFB3C.",
                "Table 6: Performance of relevance feedback for different number of feedback documents (N).",
                "N MAP Pr@30 RR 5 0.302 0.586 4779 10 0.345 0.670 4916 20 0.389 0.772 5004 TCFB3C 0.309 0.491 4947 We see that the performance of TCFB3C is comparable to that of relevance feedback using 5 documents.",
                "Although it is poorer than when there are 10 feedback documents in terms of MAP and Pr@30, it does retrieve more documents (4947) when going down the ranked list.",
                "We try to compare the quality of automatically inserted terms in relevance feedback with that of manually selected terms in term feedback.",
                "This is done by truncating the relevance feedback modified query model to a size equal to the number of checked terms for the same topic.",
                "We can then compare the terms in the truncated model with the checked terms.",
                "Figure 3 shows the distribution of the terms σKLD scores.",
                "We find that term feedback tends to produce expansion terms of higher quality(those with σKLD > 1) compared to relevance feedback (with 10 feedback documents).",
                "This does not contradict the fact that the latter yields higher retrieval performance.",
                "Actually, when we use the truncated query model instead of the intact one refined from relevance feedback, the MAP is only 0.304.",
                "The truth Figure 3: Comparison of expansion term quality between relevance feedback (with 10 feedback documents) and term feedback (with 3 × 16 CFs) −1−0 0−1 1−2 2−3 3−4 4−5 5−6 0 50 100 150 200 250 300 350 σKLD #terms relevance feedback term feedback is, although there are many unwanted terms in the expanded query model from feedback documents, there are also more relevant terms than what the user can possibly select from the list of presentation terms generated with pseudo-feedback documents, and the positive effects often outweights the negative ones.",
                "We are interested to know under what circumstances term feedback has advantage over relevance feedback.",
                "One such situation is when none of the top N feedback documents is relevant, rendering relevance feedback useless.",
                "This is not infrequent, as one might have thought: out of the 50 topics, there are 13 such cases when N = 5, 10 when N = 10, and still 3 when N = 20.",
                "When this happens, one can only back off to the original retrieval method; the power of relevance feedback is lost.",
                "Surprisingly, in 11 out of 13 such cases where relevance feedback seems impossible, the user is able to check at least 2 relevant terms from the 3 × 16 clarification form (we consider term t to be relevant if σKLD(t) > 1.0).",
                "Furthermore, in 10 out of them TCFB3C outperforms the pseudo-feedback baseline, increasing MAP from 0.076 to 0.146 on average (these are particularly hard topics).",
                "We think that there are two possible explanations for this phenomenon of term feedback being active even when relevance feedback does not work: First, even if none of the top N (suppose it is a small number) documents are relevant, we may still find relevant documents in top 60, which is more inclusive but usually unreachable when people are doing relevance feedback in interactive ad-hoc search, from which we can draw feedback terms.",
                "This is true for topic 367 piracy, where the top 10 feedback documents are all about software piracy, yet there are documents between 10-60 that are about piracy on the seas (which is about the real information need), contributing terms such as pirate, ship for selection in the clarification form.",
                "Second, for some topics, a document needs to meet some special condition in order to be relevant.",
                "The top N documents may be related to the topic, but nonetheless irrelevant.",
                "In this case, we may still extract useful terms from these documents, even if they do not qualify as relevant ones.",
                "For example, in topic 639 consumer online shopping, a document needs to mention what contributes to shopping growth to really match the specified information need, hence none of the top 10 feedback documents are regarded as relevant.",
                "But nevertheless, the feedback terms such as retail, commerce are good for query expansion. 7.",
                "CONCLUSIONS In this paper we studied the use of term feedback for interactive information retrieval in the language modeling approach.",
                "We proposed a cluster-based method for selecting presentation terms as well as algorithms to estimate refined query models from user term feedback.",
                "We saw significant improvement in retrieval accuracy brought by term feedback, in spite of the fact that a user often makes mistakes in relevance judgment that hurts its performance.",
                "We found the best-performing algorithm to be TCFB, which benefits from the combination of directly observed term evidence with TFB and indirectly learned cluster relevance with CFB.",
                "When we reduced the number of presentation terms, term feedback is still able to keep much of its performance gain over the baseline.",
                "Finally, we compared term feedback to document-level relevance feedback, and found that TCFB3Cs performance is on a par with the latter with 5 feedback documents.",
                "We regarded term feedback as a viable alternative to traditional relevance feedback, especially when there are no relevant documents in the top.",
                "We propose to extend our work in several ways.",
                "First, we want to study whether the use of various contexts can help the user to better identify term relevance, while not sacrificing the simplicity and compactness of term feedback.",
                "Second, currently all terms are presented to the user in a single batch.",
                "We could instead consider iterative term feedback, by presenting a small number of terms first, and show more terms after receiving user feedback or stop when the refined query is good enough.",
                "The presented terms should be selected dynamically to maximize learning benefits at any moment.",
                "Third, we have plans to incorporate term feedback into our UCAIR toolbar[20], an Internet Explorer plugin, to make it work for web search.",
                "We are also interested in studying how to combine term feedback with relevance feedback or implicit feedback.",
                "We could, for example, allow the user to dynamically modify terms in a language model learned from feedback documents. 8.",
                "ACKNOWLEDGMENT This work is supported in part by the National Science Foundation grants IIS-0347933 and IIS-0428472. 9.",
                "REFERENCES [1] J. Allan.",
                "Relevance feedback with too much data.",
                "In Proceedings of the 18th annual international ACM SIGIR conference on research and development in information retrieval, pages 337-343, 1995. [2] J. Allan.",
                "HARD track overview in TREC 2005 - High Accuracy Retrieval from Documents.",
                "In The Fourteenth Text REtrieval Conference, 2005. [3] P. Anick.",
                "Using terminological feedback for web search refinement: a log-based study.",
                "In Proceedings of the 26th annual international ACM SIGIR conference on research and development in informaion retrieval, pages 88-95, 2003. [4] P. G. Anick and S. Tipirneni.",
                "The paraphrase search assistant: terminological feedback for iterative information seeking.",
                "In Proceedings of the 22nd annual international ACM SIGIR conference on research and development in information retrieval, pages 153-159, 1999. [5] C. Buckley, G. Salton, J. Allan, and A. Singhal.",
                "Automatic query expansion using SMART.",
                "In Proceedings of the Third Text REtrieval Conference, 1994. [6] D. Harman.",
                "Towards interactive query expansion.",
                "In Proceedings of the 11th annual international ACM SIGIR conference on research and development in information retrieval, pages 321-331, 1988. [7] N. A. Jaleel, A. Corrada-Emmanuel, Q. Li, X. Liu, C. Wade, and J. Allan.",
                "UMass at TREC 2003: HARD and QA.",
                "In TREC, pages 715-725, 2003. [8] H. Joho, C. Coverson, M. Sanderson, and M. Beaulieu.",
                "Hierarchical presentation of expansion terms.",
                "In Proceedings of the 2002 ACM symposium on applied computing, pages 645-649, 2002. [9] K. S. Jones, S. Walker, and S. E. Robertson.",
                "A probabilistic model of information retrieval: development and status.",
                "Technical Report 446, Computer Laboratory, University of Cambridge, 1998. [10] D. Kelly, V. D. Dollu, and X. Fu.",
                "The loquacious user: a document-independent source of terms for query expansion.",
                "In Proceedings of the 28th annual international ACM SIGIR conference on research and development in information retrieval, pages 457-464, 2005. [11] D. Kelly and X. Fu.",
                "Elicitation of term relevance feedback: an investigation of term source and context.",
                "In Proceedings of the 29th annual international ACM SIGIR conference on research and development in information retrieval, 2006. [12] J. Koenemann and N. Belkin.",
                "A case for interaction: A study of interactive information retrieval behavior and effectiveness.",
                "In Proceedings of the SIGCHI conference on human factors in computing systems, pages 205-212, 1996. [13] V. Lavrenko and W. B. Croft.",
                "Relevance-based language models.",
                "In Research and Development in Information Retrieval, pages 120-127, 2001. [14] Y. Nemeth, B. Shapira, and M. Taeib-Maimon.",
                "Evaluation of the real and perceived value of automatic and interactive query expansion.",
                "In Proceedings of the 27th annual international ACM SIGIR conference on research and development in information retrieval, pages 526-527, 2004. [15] J. Ponte.",
                "A Language Modeling Approach to Information Retrieval.",
                "PhD thesis, University of Massachusetts at Amherst, 1998. [16] S. E. Robertson, S. Walker, S. Jones, M. Beaulieu, and M. Gatford.",
                "Okapi at TREC-3.",
                "In Proceedings of the Third Text REtrieval Conference, 1994. [17] J. Rocchio.",
                "Relevance feedback in information retrieval.",
                "In The SMART retrieval system, pages 313-323. 1971. [18] I. Ruthven.",
                "Re-examining the potential effectiveness of interactive query expansion.",
                "In Proceedings of the 26th annual international ACM SIGIR conference on research and development in informaion retrieval, pages 213-220, 2003. [19] G. Salton and C. Buckley.",
                "Improving retrieval performance by relevance feedback.",
                "Journal of the American Society for Information Science, 41:288-297, 1990. [20] X. Shen, B. Tan, and C. Zhai.",
                "Implicit user modeling for personalized search.",
                "In Proceedings of the 14th ACM international conference on information and knowledge management, pages 824-831, 2005. [21] X. Shen and C. Zhai.",
                "Active feedback in ad-hoc information retrieval.",
                "In Proceedings of the 28th annual international ACM SIGIR conference on research and development in information retrieval, pages 59-66, 2005. [22] A. Spink.",
                "Term relevance feedback and query expansion: relation to design.",
                "In Proceedings of the 17th annual international ACM SIGIR conference on research and development in information retrieval, pages 81-90, 1994. [23] J. Xu and W. B. Croft.",
                "Query expansion using local and global document analysis.",
                "In Proceedings of the 19th annual international ACM SIGIR conference on research and development in information retrieval, pages 4-11, 1996. [24] H. Zaragoza, N. Craswell, M. Taylor, S. Saria, and S. Robertson.",
                "Microsoft cambridge at TREC-13: Web and HARD tracks.",
                "In Proceedings of the 13th Text REtrieval Conference, 2004. [25] C. Zhai and J. Lafferty.",
                "Model-based feedback in the language modeling approach to information retrieval.",
                "In Proceedings of the tenth international conference on information and knowledge management, pages 403-410, 2001. [26] C. Zhai, A. Velivelli, and B. Yu.",
                "A cross-collection mixture model for comparative text mining.",
                "In Proceedings of the tenth ACM SIGKDD international conference on knowledge discovery and data mining, pages 743-748, 2004."
            ],
            "original_annotated_samples": [
                "of Computer Science† , Dept. of Electrical and Computer Engineering‡ University of Illinois at Urbana-Champaign bintan@cs.uiuc.edu, velivell@ifp.uiuc.edu, hfang@cs.uiuc.edu, czhai@cs.uiuc.edu ABSTRACT In this paper we study <br>term-based feedback</br> for information retrieval in the language modeling approach.",
                "We identified two key subtasks of <br>term-based feedback</br>, i.e., pre-feedback presentation term selection and post-feedback query model construction, with effective algorithms developed for both."
            ],
            "translated_annotated_samples": [
                "En este documento estudiamos la <br>retroalimentación basada en términos</br> para la recuperación de información en el enfoque de modelado de lenguaje.",
                "Identificamos dos sub tareas clave del <br>feedback basado en términos</br>, es decir, la selección de términos para la presentación previa al feedback y la construcción del modelo de consulta posterior al feedback, con algoritmos efectivos desarrollados para ambos."
            ],
            "translated_text": "Comentarios sobre el término para la recuperación de información con modelos de lenguaje Bin Tan†, Atulya Velivelli‡, Hui Fang†, ChengXiang Zhai† Dept. En este documento estudiamos la <br>retroalimentación basada en términos</br> para la recuperación de información en el enfoque de modelado de lenguaje. Con la retroalimentación de términos, un usuario juzga directamente la relevancia de términos individuales sin interacción con documentos de retroalimentación, tomando el control total del proceso de expansión de la consulta. Proponemos un método basado en clusters para seleccionar términos para presentar al usuario para su evaluación, así como algoritmos efectivos para construir modelos de lenguaje de consulta refinados a partir de la retroalimentación de términos del usuario. Nuestros algoritmos han demostrado proporcionar una mejora significativa en la precisión de recuperación en comparación con una línea base sin retroalimentación, y logran un rendimiento comparable al de la retroalimentación de relevancia. Son útiles incluso cuando no hay documentos relevantes en la parte superior. Categorías y Descriptores de Asignaturas H.3.3 [Búsqueda y Recuperación de Información]: Modelos de recuperación Términos generales Algoritmos 1. En el enfoque de modelado del lenguaje para la recuperación de información, la retroalimentación a menudo se modela como la estimación de un modelo de consulta mejorado o un modelo de relevancia basado en un conjunto de documentos de retroalimentación [25, 13]. Esto está en línea con la forma tradicional de realizar retroalimentación de relevancia: presentar al usuario documentos/pasajes para su juicio de relevancia y luego extraer términos de los documentos o pasajes juzgados para expandir la consulta inicial. Es una forma indirecta de buscar la ayuda de los usuarios para la construcción del modelo de consulta, en el sentido de que el modelo de consulta refinado (basado en términos) se aprende a través de documentos/pasajes de retroalimentación, que son estructuras de alto nivel de términos. Tiene la desventaja de que los términos irrelevantes, que ocurren junto con los relevantes en el contenido evaluado, pueden ser utilizados erróneamente para la expansión de la consulta, causando efectos no deseados. Por ejemplo, para la consulta TREC del telescopio Hubble logros, cuando un documento relevante habla más sobre la reparación de los telescopios que sobre sus descubrimientos, términos irrelevantes como caminata espacial pueden ser agregados a la consulta modificada. Podemos considerar una forma más directa de involucrar a un usuario en la mejora del modelo de consulta, sin un paso intermedio de retroalimentación de documentos que pueda introducir ruido. La idea es presentar al usuario un número (razonable) de términos individuales y pedirle que juzgue la relevancia de cada término o especifique directamente sus probabilidades en el modelo de consulta. Esta estrategia ha sido discutida en [15], pero hasta donde sabemos, no ha sido estudiada seriamente en la literatura existente de modelado de lenguaje. En comparación con la retroalimentación de relevancia tradicional, este enfoque basado en términos para el refinamiento del modelo de consulta interactivo tiene varias ventajas. Primero, el usuario tiene un mejor control del modelo de consulta final a través de la manipulación directa de términos: él/ella puede dictar qué términos son relevantes, irrelevantes y posiblemente, en qué medida. Esto evita el riesgo de introducir términos no deseados en el modelo de consulta, aunque a veces el usuario introduce términos de baja calidad. Segundo, dado que un término tarda menos tiempo en ser evaluado que el texto completo o el resumen de un documento, y con tan solo alrededor de 20 términos presentados se puede lograr una mejora significativa en el rendimiento de recuperación (como demostraremos más adelante), el feedback de términos permite recopilar la retroalimentación del usuario de manera más rápida. Esto es especialmente útil para la búsqueda interactiva ad hoc. En tercer lugar, a veces no hay documentos relevantes en los primeros N resultados recuperados inicialmente si el tema es difícil. Esto suele ser cierto cuando N está limitado a ser pequeño, lo cual surge del hecho de que el usuario no está dispuesto a evaluar demasiados documentos. En este caso, la retroalimentación de relevancia es inútil, ya que no se puede aprovechar ningún documento relevante, pero la retroalimentación de términos sigue siendo útil, al permitir seleccionar términos relevantes de documentos irrelevantes. Durante nuestra participación en la pista HARD de TREC 2005 y el estudio continuado posteriormente, exploramos cómo aprovechar la retroalimentación de términos por parte del usuario para construir modelos de consulta mejorados para la recuperación de información en el enfoque de modelado de lenguaje. Identificamos dos sub tareas clave del <br>feedback basado en términos</br>, es decir, la selección de términos para la presentación previa al feedback y la construcción del modelo de consulta posterior al feedback, con algoritmos efectivos desarrollados para ambos. Impusimos una estructura de clúster secundaria en los términos y descubrimos que una vista de clúster arroja una mayor comprensión de la necesidad de información de los usuarios, y proporciona una buena forma de utilizar la retroalimentación de términos. A través de experimentos encontramos que el feedback a término mejora significativamente sobre la línea base sin feedback, a pesar de que el usuario a menudo comete errores en la evaluación de relevancia. Entre nuestros algoritmos, el que tiene el mejor rendimiento de recuperación es TCFB, la combinación de TFB, el algoritmo de retroalimentación de términos directos, y CFB, el algoritmo de retroalimentación basado en clústeres. También variamos el número de términos de retroalimentación y observamos una mejora razonable incluso con números bajos. Finalmente, al comparar la retroalimentación a nivel de términos con la retroalimentación a nivel de documentos, encontramos que es una alternativa viable a esta última con un rendimiento competitivo en la recuperación. El resto del documento está organizado de la siguiente manera. La sección 2 discute algunos trabajos relacionados. La sección 4 describe nuestro enfoque general para la retroalimentación de términos. Presentamos nuestro método para la selección de términos de presentación en la Sección 3 y los algoritmos para la construcción del modelo de consulta en la Sección 5. Los resultados del experimento se presentan en la Sección 6. La sección 7 concluye este documento. 2. TRABAJO RELACIONADO La retroalimentación de relevancia[17, 19] ha sido reconocida durante mucho tiempo como un método efectivo para mejorar el rendimiento de recuperación. Normalmente, los N documentos principales recuperados utilizando la consulta original se presentan al usuario para su evaluación, después de lo cual se extraen términos de los documentos relevantes evaluados, ponderados por su potencial de atraer más documentos relevantes, y se añaden al modelo de consulta. La consulta ampliada suele representar mejor la necesidad de información de los usuarios que la original, que a menudo es solo una consulta de palabras clave cortas. Una segunda iteración de recuperación utilizando esta consulta modificada generalmente produce un aumento significativo en la precisión de la recuperación. En los casos en los que no se dispone de una evaluación de relevancia real y se asume que todos los documentos principales N son relevantes, se denomina retroalimentación ciega o pseudo [5, 16] y generalmente sigue mejorando el rendimiento. Debido a que un documento es una unidad de texto grande, cuando se utiliza para retroalimentación de relevancia, muchos términos irrelevantes pueden ser introducidos en el proceso de retroalimentación. Para superar esto, se propone el feedback de pasajes y se muestra que mejora el rendimiento del feedback. Una solución más directa es pedir al usuario su juicio de relevancia sobre los términos de retroalimentación. Por ejemplo, en algunos sistemas de retroalimentación de relevancia como [12], hay un paso de interacción que permite al usuario agregar o eliminar términos de expansión después de que son extraídos automáticamente de los documentos relevantes. Esto se clasifica como expansión interactiva de consultas, donde la consulta original se amplía con términos proporcionados por el usuario, que pueden provenir de la entrada directa del usuario (texto libre o palabras clave) o de la selección del usuario de términos sugeridos por el sistema (utilizando tesauros o extraídos de documentos de retroalimentación). En muchos casos, se ha encontrado que la retroalimentación de relevancia de términos mejora de manera efectiva el rendimiento de recuperación [6, 22, 12, 4, 10]. Por ejemplo, el estudio en [12] muestra que el usuario prefiere tener un conocimiento explícito y control directo de qué términos se utilizan para la expansión de la consulta, y se demuestra que la interfaz penetrable que proporciona esta libertad tiene un mejor rendimiento que otras interfaces. Sin embargo, en algunos otros casos no hay un beneficio significativo[3, 14], incluso si al usuario le gusta interactuar con los términos de expansión. En un estudio simulado realizado en [18], el autor compara el rendimiento de recuperación de la expansión interactiva de consultas y la expansión automática de consultas con un estudio simulado, y sugiere que los posibles beneficios de la primera pueden ser difíciles de lograr. Se descubre que el usuario no es bueno identificando términos útiles para la expansión de la consulta, cuando una interfaz de presentación de términos simple no puede proporcionar suficiente contexto semántico de los términos de retroalimentación. Nuestro trabajo difiere de los anteriores en dos aspectos importantes. Primero, al elegir los términos para presentar al usuario para la evaluación de relevancia, no solo consideramos el valor de un solo término (por ejemplo, la frecuencia relativa de un término en los documentos principales, que puede medirse mediante métricas como el Valor de Selección de Robertson y la Distancia Kullback-Leibler Simplificada como se detalla en [24]), sino que también examinamos la estructura de los grupos de términos, con el fin de producir una cobertura equilibrada de los diferentes aspectos del tema. Segundo, con el marco de modelado del lenguaje, permitimos una construcción detallada del modelo de consulta actualizado, estableciendo diferentes probabilidades para diferentes términos según si es un término de consulta, su importancia en los documentos principales y su pertenencia a un clúster. Aunque existen técnicas para ajustar los pesos de los términos de consulta en modelos de espacio vectorial y modelos de relevancia probabilística, la mayoría de los trabajos mencionados no las utilizan, optando por simplemente añadir términos de retroalimentación a la consulta original (usando pesos iguales para ellos), lo que puede llevar a un rendimiento de recuperación más pobre. La combinación de los dos aspectos permite que nuestro método funcione mucho mejor que el punto de referencia. La forma habitual de presentar los términos de retroalimentación es simplemente mostrar los términos en una lista. Ha habido algunos trabajos sobre interfaces de usuario alternativas. [8] organiza los términos en una jerarquía, y [11] compara tres interfaces diferentes, incluyendo términos + casillas de verificación, términos + contexto (oraciones) + casillas de verificación, oraciones + cuadro de texto de entrada. En ambos estudios, sin embargo, no hay una diferencia significativa en el rendimiento. En nuestro trabajo adoptamos el enfoque más simple de términos + casillas de verificación. Nos enfocamos en la presentación de términos y la construcción de modelos de consulta a partir de términos de retroalimentación, y creemos que el uso de contextos para mejorar la calidad de los términos de retroalimentación debería ser ortogonal a nuestro método. 3. Enfoque general: Seguimos el enfoque de modelado del lenguaje y basamos nuestro método en el modelo de recuperación de divergencia KL propuesto en [25]. Con este modelo, la tarea de recuperación implica estimar un modelo de lenguaje de consulta θq a partir de una consulta dada, un modelo de lenguaje de documento θd de cada documento, y calcular su divergencia KL D(θq||θd), que luego se utiliza para puntuar los documentos. [25] trata la retroalimentación de relevancia como un problema de reestimación del modelo de consulta, es decir, calcular un modelo de consulta actualizado θq dado el texto de la consulta original y la evidencia adicional proporcionada por los documentos relevantes juzgados. Adoptamos esta perspectiva y planteamos nuestra tarea como la actualización del modelo de consulta a partir de la retroalimentación de términos de usuario. Aquí hay dos tareas clave: Primero, cómo elegir los mejores términos para presentar al usuario para su evaluación, con el fin de recopilar la máxima evidencia sobre la necesidad de información del usuario. Segundo, cómo calcular un modelo de consulta actualizado basado en esta retroalimentación de términos, de manera que capture la necesidad de información de los usuarios y se traduzca en un buen rendimiento de recuperación. 4. SELECCIÓN DE TÉRMINOS DE PRESENTACIÓN La selección adecuada de los términos que se presentarán al usuario para su evaluación es crucial para el éxito de la retroalimentación de términos. Si los términos están mal elegidos y hay pocos relevantes, el usuario tendrá dificultades para buscar términos útiles que ayuden a aclarar su necesidad de información. Si los términos relevantes son abundantes, pero todos se centran en un solo aspecto del tema de la consulta, entonces solo podremos obtener retroalimentación sobre ese aspecto y perderemos otros, lo que resultará en una pérdida de amplitud en los resultados recuperados. Por lo tanto, es importante seleccionar cuidadosamente los términos de presentación para maximizar la ganancia esperada de la retroalimentación del usuario, es decir, aquellos que pueden revelar potencialmente la mayor evidencia de la necesidad de información del usuario. Esto es similar al feedback activo[21], que sugiere que un sistema de recuperación debería sondear activamente la necesidad de información de los usuarios, y en el caso del feedback de relevancia, los documentos de feedback deberían ser elegidos para maximizar los beneficios de aprendizaje (por ejemplo, de manera diversa para aumentar la cobertura). En nuestro enfoque, los N documentos principales de una recuperación inicial utilizando la consulta original forman la fuente de términos de retroalimentación: todos los términos que aparecen en ellos se consideran candidatos para presentar al usuario. Estos documentos sirven como pseudo-retroalimentación, ya que proporcionan un contexto mucho más amplio que la consulta original (generalmente muy breve), sin que se le pida al usuario que juzgue su relevancia. Debido a esta última razón, es posible hacer que N sea bastante grande (por ejemplo, en nuestros experimentos establecimos N = 60) para aumentar su cobertura de diferentes aspectos en el tema. La forma más sencilla de seleccionar términos de retroalimentación es elegir los M términos más frecuentes de los N documentos. Este método, sin embargo, tiene dos inconvenientes. Primero, se seleccionarán muchos términos ruidosos comunes debido a sus altas frecuencias en la colección de documentos, a menos que se utilice una lista de palabras vacías para filtrar. En segundo lugar, la lista de presentación tiende a estar compuesta por términos de los aspectos principales del tema; es probable que se pasen por alto aquellos de un aspecto menor debido a sus frecuencias relativamente bajas. Resolvemos los problemas anteriores mediante dos medidas correspondientes. Primero, introducimos un modelo de fondo θB que se estima a partir de estadísticas de la colección y explica los términos comunes, de modo que es mucho menos probable que aparezcan en la lista de presentación. Segundo, los términos se seleccionan de múltiples grupos en los documentos de retroalimentación simulada, para garantizar una representación suficiente de diferentes aspectos del tema. Nos basamos en el modelo multinomial de mezcla, que se utiliza para el descubrimiento de temas en [26]. Específicamente, asumimos que los N documentos contienen K grupos {Ci| i = 1, 2, · · · K}, cada uno caracterizado por una distribución de palabras multinomial (también conocida como modelo de lenguaje unigrama) θi y correspondiente a un aspecto del tema. Los documentos se consideran como muestreados de una mezcla de K + 1 componentes, incluidos los K grupos y el modelo de fondo: p(w|d) = λBp(w|θB) + (1 − λB) K i=1 πd,ip(w|θi) donde w es una palabra, λB es el peso de la mezcla para el modelo de fondo θB, y πd,i es el peso de la mezcla específico del documento para el modelo de cluster i-ésimo θi. Luego estimamos los modelos de clúster maximizando la probabilidad de que los documentos de pseudo-retroalimentación sean generados a partir del modelo de mezcla multinomial: log p(D|Λ) = d∈D w∈V c(w; d) log p(w|d) donde D = {di| i = 1, 2, · · · N} es el conjunto de los N documentos, V es el vocabulario, c(w; d) es la frecuencia de w en d y Λ = {θi| i = 1, 2, · · · K} ∪ {πdij | i = 1, 2, · · · N, j = 1, 2, · · · K} es el conjunto de parámetros del modelo a estimar. Los modelos de conglomerados pueden ser estimados eficientemente utilizando el algoritmo de Expectation-Maximization (EM). Para más detalles, remitimos al lector a [26]. La Tabla 1 muestra los modelos de clúster para la consulta de TREC sobre desastres en túneles de transporte (K = 3). Ten en cuenta que solo el grupo central es relevante. Tabla 1: Modelos de clúster para el tema 363 Desastres en túneles de transporte Clúster 1 Clúster 2 Clúster 3 túnel 0.0768 túnel 0.0935 túnel 0.0454 transporte 0.0364 fuego 0.0295 transporte 0.0406 tráfico 0.0206 camión 0.0236 peaje 0.0166 ferrocarril 0.0186 francés 0.0220 Amtrak 0.0153 puerto 0.0146 humo 0.0157 tren 0.0129 riel 0.0140 coche 0.0154 aeropuerto 0.0122 puente 0.0139 italiano 0.0152 autopista 0.0105 kilómetro 0.0136 incendio 0.0144 lui 0.0095 camión 0.0133 llamas 0.0127 Jersey 0.0093 construcción 0.0131 blanco 0.0121 paso 0.0087 · · · · · · · · · De cada uno de los K clústeres estimados, elegimos los L = M/K términos con las probabilidades más altas para formar un total de M términos de presentación. Si un término resulta estar en el top L en múltiples grupos, lo asignamos al grupo donde tenga la probabilidad más alta y permitimos que los otros grupos tomen un término adicional como compensación. También filtramos los términos en el texto de la consulta original porque tienden a ser siempre relevantes cuando la consulta es corta. Los términos seleccionados son luego presentados al usuario para su evaluación. Se muestra un formulario de retroalimentación (completado) de muestra en la Figura 1. En este estudio solo tratamos con juicios binarios: un término presentado está por defecto sin marcar, y un usuario puede marcarlo para indicar relevancia. Tampoco explotamos explícitamente la retroalimentación negativa (es decir, penalizar términos irrelevantes), porque con la retroalimentación binaria un término no verificado no es necesariamente irrelevante (quizás el usuario no está seguro de su relevancia). Podríamos pedir al usuario un juicio más detallado (por ejemplo, eligiendo entre altamente relevante, algo relevante, no sé, algo irrelevante y altamente irrelevante), pero la retroalimentación binaria es más compacta, ocupando menos espacio para mostrar y requiriendo menos esfuerzo por parte del usuario para emitir un juicio. 5. ESTIMACIÓN DE MODELOS DE CONSULTA A PARTIR DE RETROALIMENTACIÓN DE TÉRMINOS En esta sección, presentamos varios algoritmos para aprovechar la retroalimentación de términos. Los algoritmos toman como entrada la consulta original q, los grupos {θi} generados por el algoritmo de descubrimiento de temas, el conjunto de términos de retroalimentación T y su juicio de relevancia R, y producen un modelo de lenguaje de consulta actualizado θq que hace el mejor uso de la evidencia de retroalimentación para capturar la necesidad de información de los usuarios. Primero describimos nuestras notaciones: • θq: El modelo de consulta original, derivado solo de los términos de la consulta: p(w|θq) = c(w; q) |q| donde c(w; q) es la cantidad de veces que w aparece en q, y |q| = w∈q c(w; q) es la longitud de la consulta. • θq: El modelo de consulta actualizado que necesitamos estimar a partir de la retroalimentación de términos. • θi (i = 1, 2, . . . K): El modelo de lenguaje unigrama del clúster Ci, estimado utilizando el algoritmo de descubrimiento de temas. • T = {ti,j} (i = 1 . . . K, j = 1 . . . \n\nK, j = 1 . . . El conjunto de términos presentados al usuario para su evaluación. ti,j es el j-ésimo término elegido del grupo Ci. • R = {δw|w ∈ T}: δw es una variable indicadora que es 1 si w es considerado relevante o 0 en caso contrario. 5.1 TFB (Retroalimentación Directa de Términos) Esta es una forma directa de retroalimentación de términos que no implica ninguna estructura secundaria. Asignamos un peso de 1 a los términos considerados relevantes por el usuario, un peso de μ a los términos de la consulta, un peso de cero a los demás términos, y luego aplicamos la normalización: p(w|θq ) = δw + μ c(w; q) w ∈T δw + μ|q| donde w ∈T δw es el número total de términos considerados relevantes. Llamamos a este método TFB (retroalimentación directa de términos). Si dejamos que μ = 1, este enfoque es equivalente a agregar los términos relevantes después de la consulta original, que es lo que hace la expansión estándar de consultas (sin reponderación de términos). Si establecemos μ > 1, estamos poniendo más énfasis en los términos de consulta que en los verificados. Ten en cuenta que el modelo de resultado será más sesgado hacia θq si la consulta original es larga o el feedback del usuario es débil, lo cual tiene sentido, ya que podemos confiar más en la consulta original en ambos casos. Figura 1: Formulario de aclaración completado para el Tema 363 363 desastres en túneles de transporte. Por favor, seleccione todos los términos relevantes para el tema. tráfico ferrocarril puerto puente ferroviario kilómetro construir suizo enlace cruzado hongkonés río proyecto metro camión de bomberos francés humo coche italiano bomberos incendio blanco montaña víctima francés rescate conductor chamonix emerger peaje tren amtrak aeropuerto turnpike jersey pass rome z centro electrón carretera boston velocidad bu enviar 5.2 CFB (Retroalimentación de Clúster) Aquí explotamos la estructura de clúster que jugó un papel importante cuando seleccionamos los términos de presentación. Los grupos representan diferentes aspectos del tema de la consulta, cada uno de los cuales puede ser relevante o no. Si somos capaces de identificar los grupos relevantes, podemos combinarlos para generar un modelo de consulta que sea bueno para descubrir documentos pertenecientes a estos grupos (en lugar de los irrelevantes). Podríamos pedir al usuario que juzgue directamente la relevancia de un grupo después de ver los términos representativos en ese grupo, pero a veces esto sería una tarea difícil para el usuario, quien tendría que adivinar la semántica de un grupo a través de su conjunto de términos, los cuales podrían no estar bien conectados entre sí debido a la falta de contexto. Por lo tanto, proponemos aprender retroalimentación de clúster de forma indirecta, inferir la relevancia de un clúster a través de la relevancia de sus términos de retroalimentación. Dado que cada grupo tiene un número igual de términos presentados al usuario, la medida más simple de la relevancia de un grupo es el número de términos que se consideran relevantes en él. Intuitivamente, cuantos más términos estén marcados como relevantes en un grupo, más cerca estará el grupo del tema de la consulta y más debería participar en la modificación de la consulta. Si combinamos los modelos de clúster utilizando pesos determinados de esta manera y luego interpolamos con el modelo de consulta original, obtenemos la siguiente fórmula para la actualización de la consulta, que llamamos CFB (Retroalimentación de Clúster): p(w|θq) = λp(w|θq) + (1 − λ) K i=1 L j=1 δti,j K k=1 L j=1 δtk,j p(w|θi) donde L j=1 δti,j es el número de términos relevantes en el clúster Ci, y K k=1 L j=1 δtk,j es el número total de términos relevantes. Observamos que cuando solo hay un clúster (K = 1), la fórmula anterior se degrada a p(w|θq ) = λp(w|θq) + (1 − λ)p(w|θ1) que es simplemente una pseudo-retroalimentación de la forma propuesta en [25]. TCFB (Retroalimentación Término-Clúster) TFB y CFB ambos tienen sus inconvenientes. TFB asigna probabilidades no nulas a los términos presentados que están marcados como relevantes, pero ignora por completo (muchos más) otros, que pueden quedar sin marcar debido a la ignorancia de los usuarios, o simplemente no estar incluidos en la lista de presentación, pero deberíamos ser capaces de inferir su relevancia a partir de los marcados. Por ejemplo, en la Figura 1, dado que se revisan hasta 5 términos en el grupo central (las tercera y cuarta columnas), deberíamos tener una alta confianza en la relevancia de otros términos en ese grupo. CFB resuelve el problema de TFB tratando los términos en un grupo de manera colectiva, de modo que los términos no verificados/no presentados reciban pesos cuando los términos presentados en sus grupos son considerados relevantes, pero no distingue qué términos en un grupo son presentados o considerados. Intuitivamente, los términos considerados relevantes deberían recibir pesos mayores porque son indicados explícitamente como relevantes por el usuario. Por lo tanto, intentamos combinar los dos métodos, con la esperanza de obtener lo mejor de ambos. Lo hacemos interpolando el modelo TFB con el modelo CFB, y lo llamamos TCFB: p(w|θq ) = αp(w|θqT F B ) + (1 − α)p(w|θqCF B ) 6. EXPERIMENTOS En esta sección, describimos los resultados de nuestro experimento. Primero describimos nuestra configuración experimental y presentamos una visión general del rendimiento de varios métodos. Luego discutimos los efectos de variar la configuración de los parámetros en los algoritmos, así como el número de términos de presentación. A continuación analizamos el comportamiento de retroalimentación de los usuarios y su relación con el rendimiento de recuperación. Finalmente comparamos la retroalimentación de términos con la retroalimentación de relevancia y mostramos que tiene su ventaja particular. 6.1 Configuración del Experimento y Resultados Básicos Aprovechamos la oportunidad de la pista HARD de TREC 2005[2] para la evaluación de nuestros algoritmos. Las pistas utilizaron la colección AQUAINT, un corpus de 3GB de texto de noticias en inglés. Los temas incluyeron 50 que se sabía que eran difíciles, es decir, con bajo rendimiento de recuperación. Es para estos temas difíciles que la retroalimentación del usuario es más útil, ya que puede proporcionar información para desambiguar las consultas; con temas fáciles, es posible que el usuario no esté dispuesto a esforzarse por dar retroalimentación si los resultados de recuperación automática son lo suficientemente buenos. Los participantes de la pista pudieron enviar formularios de aclaración (CF) diseñados a medida para solicitar retroalimentación de los evaluadores humanos proporcionados por la Tabla 2: Rendimiento de recuperación para diferentes métodos y tipos de CF. La última fila es el porcentaje de mejora del MAP sobre la línea base. Los ajustes de parámetros μ = 4, λ = 0.1, α = 0.3 son casi óptimos. Variación de MAP con el número de términos presentados. # términos TFB1C TFB3C TFB6C CFB3C CFB6C TCFB3C TCFB6C 6 0.245 0.240 0.227 0.279 0.279 0.281 0.274 12 0.261 0.261 0.242 0.299 0.286 0.297 0.281 18 0.275 0.274 0.256 0.301 0.282 0.300 0.286 24 0.276 0.281 0.265 0.303 0.292 0.305 0.292 30 0.280 0.285 0.270 0.304 0.296 0.307 0.296 36 0.282 0.288 0.272 0.307 0.297 0.309 0.297 42 0.283 0.288 0.275 0.306 0.298 0.309 0.300 48 0.288 0.288 0.278 0.305 0.301 0.309 0.303 NIST. Diseñamos tres conjuntos de formularios de aclaración para la retroalimentación de términos, diferenciados en la elección de K, el número de grupos, y L, el número de términos presentados de cada grupo. Son: 1 × 48, un gran grupo con 48 términos, 3 × 16, 3 grupos con 16 términos cada uno, y 6 × 8, 6 grupos con 8 términos cada uno. El número total de términos presentados (M) está fijo en 48, por lo que al comparar el rendimiento de diferentes tipos de formas de clarificación podemos conocer los efectos de diferentes grados de agrupamiento. Para cada tema, un evaluador completaría los formularios en el orden de 6 × 8, 1 × 48 y 3 × 16, dedicando hasta tres minutos en cada formulario. El formulario de aclaración de muestra mostrado en la Figura 1 es de tipo 3 × 16. Es una interfaz simple y compacta en la que el usuario puede consultar términos relevantes. El formulario es autoexplicativo; no es necesario brindar capacitación adicional al usuario sobre cómo utilizarlo. Nuestras consultas iniciales se construyen solo utilizando las descripciones de los títulos de los temas, que tienen en promedio 2.7 palabras de longitud. Como base utilizamos el método de recuperación de divergencia KL implementado en la herramienta Lemur con 5 documentos de retroalimentación pseudo. Aplicamos el suavizado de Dirichlet con una prior de 2000, y truncamos los modelos de lenguaje de consulta a 50 términos (estas configuraciones se utilizan en todos los experimentos). Para todos los demás parámetros, utilizamos la configuración predeterminada de Lemurs. El punto de referencia resulta tener un rendimiento por encima del promedio entre los participantes de la pista. Después de una ejecución inicial utilizando este método de recuperación de referencia, tomamos los 60 documentos principales para cada tema y aplicamos el algoritmo de descubrimiento de temas para producir los grupos (1, 3 o 6 de ellos), en base a los cuales generamos formularios de aclaración. Después de recibir la retroalimentación del usuario, ejecutamos los algoritmos de retroalimentación de términos (TFB, CFB o TCFB) para estimar modelos de consulta actualizados, los cuales luego se utilizan para una segunda iteración de recuperación. Evaluamos el rendimiento de los diferentes métodos de recuperación en sus clasificaciones de los 1000 documentos principales. Las métricas de evaluación que adoptamos incluyen la precisión media promedio (no interpolada) (MAP), precisión en los primeros 30 (Pr@30) y total relevante recuperado (RR). La Tabla 2 muestra el rendimiento de varios métodos y configuraciones de K × L. Los sufijos (1C, 3C, 6C) después de TFB, CFB, TCFB representan el número de grupos (K). Por ejemplo, TCFB3C significa el método TCFB en los formularios de aclaración de 3 × 16. De la Tabla 2 podemos hacer las siguientes observaciones: 1 http://www.lemurproject.com 1. Todos los métodos tienen un rendimiento considerablemente mejor que la línea base de pseudoretroalimentación, con TCFB3C logrando una mejora máxima del 41.1% en el MAP, lo que indica una contribución significativa de la retroalimentación de términos para la clarificación de la necesidad de información de los usuarios. En otras palabras, el feedback a término es realmente útil para mejorar la precisión de recuperación. 2. Para TFB, el rendimiento es casi igual en los formularios de aclaración de 1 × 48 y 3 × 16 en términos de MAP (aunque este último es ligeramente mejor en Pr@30 y RR), y un poco peor en los de 6 × 8. 3. Tanto CFB3C como CFB6C tienen un mejor rendimiento que sus contrapartes TFB en las tres métricas, lo que sugiere que el feedback sobre una estructura de clúster secundaria es realmente beneficioso. CFB1C es en realidad peor porque no puede ajustar el peso de su (único) grupo a partir de la retroalimentación de términos y es simplemente pseudoretroalimentación. 4. Aunque TCFB es solo una simple mezcla de TFB y CFB por interpolación, logra superar a ambos. Esto respalda nuestra especulación de que TCFB supera las desventajas de TFB (prestando atención solo a los términos marcados) y CFB (sin distinguir los términos marcados y no marcados en un grupo). Excepto por TCFB6C v.s. CFB6C, la ventaja de rendimiento de TCFB sobre TFB/CFB es significativa con p < 0.05 utilizando la prueba de rango con signo de Wilcoxon. Esto no es cierto en el caso de TFB v.s. CFB, cada uno de los cuales es mejor que el otro en casi la mitad de los temas. 6.2 Reducción de Términos de Presentación En algunas situaciones podemos tener que reducir el número de términos de presentación debido a limitaciones en el espacio de visualización o esfuerzos de retroalimentación del usuario. Es interesante saber si el rendimiento de nuestros algoritmos se deteriora cuando al usuario se le presentan menos términos. Dado que los términos de presentación dentro de cada grupo se generan en orden decreciente de sus frecuencias, la lista de presentación forma un subconjunto del original si su tamaño se reduce2. Por lo tanto, podemos simular fácilmente lo que sucede cuando el número de términos de presentación disminuye 2. Hay complejidades que surgen de los términos que aparecen en la parte superior L de múltiples grupos, pero estas son excepciones de M a M: mantendremos todos los juicios de los términos superiores L = M / K en cada grupo y descartaremos los de los demás. La Tabla 3 muestra el rendimiento de varios algoritmos a medida que el número de términos de presentación varía de 6 a 48. Observamos que el rendimiento de TFB es más susceptible a la reducción de términos de presentación que el de CFB o TCFB. Por ejemplo, en 12 términos el MAP de TFB3C es el 90.6% del que se obtiene en 48 términos, mientras que los números para CFB3C y TCFB3C son del 98.0% y 96.1% respectivamente. Conjeturamos que la razón es que mientras el rendimiento de TFB depende en gran medida de cuántos términos buenos se elijan para la expansión de la consulta, CFB solo necesita una estimación aproximada de los pesos de los clústeres para funcionar. Además, las formas de aclaración de 3 × 16 parecen ser más robustas que las de 6 × 8: con 12 términos, el MAP de TFB6C es el 87.1% del obtenido con 48 términos, por debajo del 90.6% de TFB3C. De manera similar, para CFB es del 95.0% en comparación con el 98.0%. Esto es natural, ya que para un gran número de 6 clusters, es más fácil llegar a la situación en la que cada cluster recibe muy pocos términos de presentación para que la diversificación de temas sea útil. En general, nos sorprende ver que los algoritmos aún pueden funcionar razonablemente bien cuando el número de términos de presentación es pequeño. Por ejemplo, con solo 12 términos, CFB3C (el formulario de aclaración es de tamaño 3 × 4) aún puede mejorar un 36.5% sobre el valor base, disminuyendo ligeramente desde el 39.3% con 48 términos. En el Análisis de Retroalimentación del Usuario, estudiamos varios aspectos del comportamiento de retroalimentación de los usuarios en términos, y si están conectados al rendimiento de recuperación. Figura 2: Distribuciones de tiempo de completación del formulario de aclaración 0−30 30−60 60−90 90−120 120−150 150−180 0 5 10 15 20 25 30 35 tiempo de completación (segundos) #temas 1×48 3×16 6×8 La Figura 2 muestra la distribución del tiempo necesario para completar un formulario de aclaración. Observamos que el usuario suele ser capaz de completar el feedback del término en un tiempo razonablemente corto: para más de la mitad de los temas, el formulario de aclaración se completa en solo 1 minuto, y solo una pequeña fracción de temas (menos del 10% para 1 × 48 y 3 × 16) tarda más de 2 minutos. Esto sugiere que la retroalimentación a corto plazo es adecuada para la recuperación interactiva ad-hoc, donde un usuario generalmente no desea dedicar demasiado tiempo a proporcionar retroalimentación. Observamos que un usuario a menudo comete errores al juzgar la relevancia de un término. A veces, un término relevante puede ser omitido porque su conexión con el tema de la consulta no es obvia para el usuario. En otras ocasiones, puede incluirse un término dudoso que resulta ser irrelevante. Toma como ejemplo el tema en la Figura 1. Hubo un desastre de incendio en Mont 3. El tiempo máximo es de 180 segundos, ya que el evaluador del NIST estaría obligado a enviar el formulario en ese momento. Tabla 4: Estadísticas de selección de términos (promedio del tema) CF Tipo 1 × 48 3 × 16 6 × 8 # términos revisados 14.8 13.3 11.2 # términos relevantes 15.0 12.6 11.2 # términos relevantes revisados 7.9 6.9 5.9 precisión 0.534 0.519 0.527 recuperación 0.526 0.548 0.527 El usuario no seleccionó palabras clave como mont, blanc, francés e italiano debido a su desconocimiento del evento del Túnel del Mont Blanc entre Francia e Italia en 1999. De hecho, sin un contexto adecuado sería difícil hacer un juicio perfecto. ¿Cuál es entonces, el nivel en el que el usuario es bueno dando retroalimentación? ¿Tiene un impacto serio en el rendimiento de recuperación? Para responder a estas preguntas, necesitamos una medida de la verdadera relevancia de los términos individuales. Adoptamos la métrica de Divergencia KL Simplificada utilizada en [24] para decidir los términos de expansión de la consulta como nuestra medida de relevancia de términos: σKLD(w) = p(w|R) log p(w|R) p(w|¬R) donde p(w|R) es la probabilidad de que un documento relevante contenga el término w, y p(w|¬R) es la probabilidad de que un documento irrelevante contenga w, ambas pueden ser fácilmente calculadas mediante una estimación de máxima verosimilitud dada la relevancia a nivel de documento. Si σKLD(w) > 0, w es más probable que aparezca en documentos relevantes que en irrelevantes. Consideramos un término relevante si su valor de Divergencia KL Simplificada es mayor que un umbral determinado σ0. Podemos entonces definir la precisión y la exhaustividad del juicio de términos del usuario de la siguiente manera: la precisión es la fracción de términos revisados por el usuario que son relevantes; la exhaustividad es la fracción de términos relevantes presentados que son revisados por el usuario. La Tabla 4 muestra el número de términos revisados, términos relevantes y términos relevantes revisados cuando σ0 se establece en 1.0, así como la precisión/recuperación del juicio de términos del usuario. Ten en cuenta que cuando los formularios de aclaración contienen más grupos, se revisan menos términos: 14.8 para 1 × 48, 13.3 para 3 × 16 y 11.2 para 6×8. El patrón similar se mantiene para los términos relevantes y los términos relevantes verificados. Parece haber un compromiso entre aumentar la diversidad de temas mediante el agrupamiento y perder términos adicionales relevantes: cuando hay más grupos, cada uno recibe menos términos para presentar, lo que puede perjudicar a un grupo relevante importante que contiene muchos términos relevantes. Por lo tanto, no siempre es útil tener más grupos, por ejemplo, TFB6C es en realidad peor que TFB1C. El hallazgo principal que podemos obtener de la Tabla 4 es que el usuario no es particularmente bueno identificando términos relevantes, lo cual coincide con el descubrimiento en [18]. En el caso de 3 formularios de aclaración de 16, el número promedio de términos marcados como relevantes por el usuario es de 13.3 por tema, y el número promedio de términos relevantes cuyo valor de σKLD excede 1.0 es de 12.6. El usuario es capaz de reconocer solo 6.9 de estos términos en promedio. De hecho, la precisión y la exhaustividad de los términos de retroalimentación de los usuarios (como se definió anteriormente) están lejos de ser perfectas. Por otro lado, si el usuario hubiera verificado correctamente todos esos términos relevantes, el rendimiento de nuestros algoritmos habría aumentado considerablemente, como se muestra en la Tabla 5. Vemos que TFB experimenta una gran mejora cuando hay un oráculo que verifica todos los términos relevantes, mientras que CFB se encuentra con un cuello de botella alrededor de un MAP de 0.325, ya que todo lo que hace es ajustar los pesos de los clústeres, y cuando los pesos aprendidos están cerca de ser precisos, no puede beneficiarse más del feedback de términos. También hay que tener en cuenta que TCFB no logra superar a TFB, probablemente porque TFB es lo suficientemente preciso. Tabla 5: Cambio de MAP al usar todos (y solo) los términos relevantes (σKLD > 1.0) para retroalimentación. retroalimentación de término original retroalimentación de término relevante TF1 0.288 0.354 TF3 0.288 0.354 TF6 0.278 0.346 CF3 0.305 0.325 CF6 0.301 0.326 TCF3 0.309 0.345 TCF6 0.304 0.341 6.4 Comparación con Retroalimentación de Relevancia Ahora comparamos la retroalimentación de términos con la retroalimentación de relevancia a nivel de documento, en la que al usuario se le presentan los N mejores documentos de una recuperación inicial y se le pide que juzgue su relevancia. El proceso de retroalimentación se simula utilizando la evaluación de relevancia de documentos de NIST. Utilizamos el método de retroalimentación basado en el modelo de mezcla propuesto en [25], con el ruido de mezcla establecido en 0.95 y el coeficiente de retroalimentación establecido en 0.9. La evaluación comparativa del feedback de relevancia frente a otros métodos se complica por el hecho de que algunos documentos ya han sido vistos durante el feedback, por lo que no tiene sentido incluirlos en los resultados de recuperación de la segunda ejecución. Sin embargo, esto no se aplica al término retroalimentación. Por lo tanto, para que sea justo en cuanto a la ganancia de información de los usuarios, si los documentos de retroalimentación son relevantes, deben mantenerse en la parte superior del ranking; si son irrelevantes, deben ser excluidos. Por lo tanto, utilizamos retroalimentación de relevancia para producir un ranking de los 1000 documentos recuperados, pero excluyendo cada documento de retroalimentación, y luego añadimos los documentos relevantes de retroalimentación al principio. La tabla 6 muestra el rendimiento de la retroalimentación de relevancia para diferentes valores de N y lo compara con TCFB3C. Tabla 6: Rendimiento de la retroalimentación de relevancia para diferentes números de documentos de retroalimentación (N). Vemos que el rendimiento de TCFB3C es comparable al de la retroalimentación de relevancia utilizando 5 documentos. Aunque es menos eficiente que cuando hay 10 documentos de retroalimentación en términos de MAP y Pr@30, sí recupera más documentos (4947) al descender por la lista clasificada. Intentamos comparar la calidad de los términos insertados automáticamente en la retroalimentación de relevancia con la de los términos seleccionados manualmente en la retroalimentación de términos. Esto se logra truncando el modelo de consulta modificado por retroalimentación de relevancia a un tamaño igual al número de términos verificados para el mismo tema. Podemos comparar los términos en el modelo truncado con los términos verificados. La Figura 3 muestra la distribución de los puntajes σKLD de los términos. Observamos que el término \"feedback\" tiende a producir términos de expansión de mayor calidad (aquellos con σKLD > 1) en comparación con el \"relevance feedback\" (con 10 documentos de feedback). Esto no contradice el hecho de que este último produce un rendimiento de recuperación más alto. De hecho, cuando usamos el modelo de consulta truncada en lugar del modelo íntegro refinado a partir de la retroalimentación de relevancia, el MAP es solo 0.304. La verdad Figura 3: Comparación de la calidad del término de expansión entre retroalimentación de relevancia (con 10 documentos de retroalimentación) y retroalimentación de términos (con 3 × 16 CFs) −1−0 0−1 1−2 2−3 3−4 4−5 5−6 0 50 100 150 200 250 300 350 σKLD #términos retroalimentación de relevancia retroalimentación de términos es que, aunque hay muchos términos no deseados en el modelo de consulta expandido a partir de los documentos de retroalimentación, también hay más términos relevantes de los que el usuario puede seleccionar de la lista de términos de presentación generados con documentos de pseudo-retroalimentación, y los efectos positivos a menudo superan a los negativos. Estamos interesados en saber en qué circunstancias la retroalimentación basada en términos tiene ventajas sobre la retroalimentación basada en relevancia. Una de esas situaciones es cuando ninguno de los documentos de retroalimentación principales es relevante, lo que hace que la retroalimentación de relevancia sea inútil. Esto no es infrecuente, como uno podría haber pensado: de los 50 temas, hay 13 casos así cuando N = 5, 10 cuando N = 10, y aún 3 cuando N = 20. Cuando esto sucede, solo se puede retroceder al método original de recuperación; se pierde el poder de la retroalimentación de relevancia. Sorprendentemente, en 11 de 13 casos en los que parece imposible el feedback de relevancia, el usuario puede marcar al menos 2 términos relevantes de los 3 × 16 del formulario de aclaración (consideramos que el término t es relevante si σKLD(t) > 1.0). Además, en 10 de ellos TCFB3C supera la línea base de pseudo-retroalimentación, aumentando el MAP de 0.076 a 0.146 en promedio (estos son temas particularmente difíciles). Creemos que hay dos posibles explicaciones para este fenómeno de que la retroalimentación de términos esté activa incluso cuando la retroalimentación de relevancia no funciona: Primero, incluso si ninguno de los primeros N (supongamos que es un número pequeño) documentos son relevantes, aún podemos encontrar documentos relevantes en los primeros 60, que son más inclusivos pero generalmente inalcanzables cuando las personas están realizando retroalimentación de relevancia en la búsqueda interactiva ad-hoc, de donde podemos extraer términos de retroalimentación. Esto es cierto para el tema 367 de piratería, donde los 10 documentos principales de retroalimentación son todos sobre piratería de software, sin embargo, hay documentos entre el 10 y el 60 que tratan sobre piratería en los mares (que es la necesidad de información real), contribuyendo términos como pirata, barco para la selección en el formulario de aclaración. Segundo, para algunos temas, un documento debe cumplir con ciertas condiciones especiales para ser relevante. Los N documentos principales pueden estar relacionados con el tema, pero no obstante ser irrelevantes. En este caso, aún podemos extraer términos útiles de estos documentos, incluso si no califican como relevantes. Por ejemplo, en el tema 639 de compras en línea de consumidores, un documento necesita mencionar qué contribuye al crecimiento de las compras para realmente coincidir con la necesidad de información especificada, por lo tanto, ninguno de los 10 documentos de retroalimentación principales se considera relevante. Sin embargo, los términos de retroalimentación como venta al por menor, comercio son buenos para la expansión de consultas. 7. CONCLUSIONES En este artículo estudiamos el uso de la retroalimentación de términos para la recuperación interactiva de información en el enfoque de modelado del lenguaje. Propusimos un método basado en clusters para seleccionar términos de presentación, así como algoritmos para estimar modelos de consulta refinados a partir de la retroalimentación de términos de usuario. Observamos una mejora significativa en la precisión de recuperación proporcionada por la retroalimentación de términos, a pesar de que un usuario a menudo comete errores en la evaluación de relevancia que perjudican su rendimiento. Encontramos que el algoritmo de mejor rendimiento es TCFB, el cual se beneficia de la combinación de la evidencia de términos observados directamente con TFB y la relevancia de clúster aprendida indirectamente con CFB. Cuando redujimos el número de términos de presentación, el feedback del término aún puede mantener gran parte de su mejora de rendimiento sobre el valor base. Finalmente, comparamos la retroalimentación de términos con la retroalimentación de relevancia a nivel de documento, y encontramos que el rendimiento de TCFB3C está a la par con este último con 5 documentos de retroalimentación. Consideramos el término \"feedback\" como una alternativa viable al feedback de relevancia tradicional, especialmente cuando no hay documentos relevantes en la parte superior. Proponemos ampliar nuestro trabajo de varias maneras. Primero, queremos estudiar si el uso de varios contextos puede ayudar al usuario a identificar mejor la relevancia de los términos, sin sacrificar la simplicidad y la concisión de la retroalimentación de los términos. Segundo, actualmente todos los términos se presentan al usuario en un solo lote. En su lugar, podríamos considerar la retroalimentación iterativa de términos, presentando primero un pequeño número de términos y mostrando más términos después de recibir la retroalimentación del usuario o detenernos cuando la consulta refinada sea lo suficientemente buena. Los términos presentados deben ser seleccionados dinámicamente para maximizar los beneficios de aprendizaje en cualquier momento. Tercero, tenemos planes de incorporar retroalimentación de términos en nuestra barra de herramientas UCAIR[20], un complemento de Internet Explorer, para que funcione en búsquedas web. También estamos interesados en estudiar cómo combinar la retroalimentación de términos con la retroalimentación de relevancia o la retroalimentación implícita. Por ejemplo, podríamos permitir al usuario modificar dinámicamente los términos en un modelo de lenguaje aprendido a partir de documentos de retroalimentación. 8. AGRADECIMIENTO Este trabajo cuenta con el apoyo parcial de las becas de la Fundación Nacional de Ciencias IIS-0347933 e IIS-0428472. 9. REFERENCIAS [1] J. Allan. Retroalimentación de relevancia con demasiados datos. En Actas de la 18ª conferencia internacional anual de ACM SIGIR sobre investigación y desarrollo en recuperación de información, páginas 337-343, 1995. [2] J. Allan. Resumen de la pista HARD en TREC 2005 - Recuperación de alta precisión de documentos. En la Decimocuarta Conferencia de Recuperación de Información de 2005. [3] P. Anick. Utilizando retroalimentación terminológica para refinar la búsqueda web: un estudio basado en registros. En Actas de la 26ª conferencia internacional anual de ACM SIGIR sobre investigación y desarrollo en recuperación de información, páginas 88-95, 2003. [4] P. G. Anick y S. Tipirneni. El asistente de búsqueda de paráfrasis: retroalimentación terminológica para la búsqueda iterativa de información. En Actas de la 22ª conferencia internacional anual de ACM SIGIR sobre investigación y desarrollo en recuperación de información, páginas 153-159, 1999. [5] C. Buckley, G. Salton, J. Allan y A. Singhal. Expansión automática de consultas utilizando SMART. En Actas de la Tercera Conferencia de Recuperación de Texto, 1994. [6] D. Harman. Hacia la expansión interactiva de consultas. En Actas de la 11ª conferencia internacional anual de ACM SIGIR sobre investigación y desarrollo en recuperación de información, páginas 321-331, 1988. [7] N. A. Jaleel, A. Corrada-Emmanuel, Q. Li, X. Liu, C. Wade y J. Allan. UMass en TREC 2003: HARD y QA. En TREC, páginas 715-725, 2003. [8] H. Joho, C. Coverson, M. Sanderson y M. Beaulieu. Presentación jerárquica de términos de expansión. En Actas del simposio de informática aplicada de ACM de 2002, páginas 645-649, 2002. [9] K. S. Jones, S. Walker y S. E. Robertson. Un modelo probabilístico de recuperación de información: desarrollo y estado actual. Informe técnico 446, Laboratorio de Computación, Universidad de Cambridge, 1998. [10] D. Kelly, V. D. Dollu y X. Fu. El usuario locuaz: una fuente de términos independiente del documento para la expansión de consultas. En Actas de la 28ª conferencia internacional anual de ACM SIGIR sobre investigación y desarrollo en recuperación de información, páginas 457-464, 2005. [11] D. Kelly y X. Fu. Obtención de retroalimentación de relevancia de términos: una investigación sobre la fuente y el contexto de los términos. En Actas de la 29ª conferencia internacional anual de ACM SIGIR sobre investigación y desarrollo en recuperación de información, 2006. [12] J. Koenemann y N. Belkin. Un caso para la interacción: Un estudio del comportamiento y la efectividad de la recuperación de información interactiva. En Actas de la conferencia SIGCHI sobre factores humanos en sistemas informáticos, páginas 205-212, 1996. [13] V. Lavrenko y W. B. Croft. Modelos de lenguaje basados en relevancia. En Investigación y Desarrollo en Recuperación de Información, páginas 120-127, 2001. [14] Y. Nemeth, B. Shapira y M. Taeib-Maimon. Evaluación del valor real y percibido de la expansión automática e interactiva de consultas. En Actas de la 27ª conferencia internacional anual de ACM SIGIR sobre investigación y desarrollo en recuperación de información, páginas 526-527, 2004. [15] J. Ponte. Un enfoque de modelado del lenguaje para la recuperación de información. Tesis doctoral, Universidad de Massachusetts en Amherst, 1998. [16] S. E. Robertson, S. Walker, S. Jones, M. Beaulieu y M. Gatford. Okapi en TREC-3. En Actas de la Tercera Conferencia de Recuperación de Texto, 1994. [17] J. Rocchio. Retroalimentación de relevancia en la recuperación de información. En el sistema de recuperación SMART, páginas 313-323. 1971. [18] I. Ruthven. Reexaminando la efectividad potencial de la expansión interactiva de consultas. En Actas de la 26ª conferencia internacional anual de ACM SIGIR sobre investigación y desarrollo en recuperación de información, páginas 213-220, 2003. [19] G. Salton y C. Buckley. Mejorando el rendimiento de recuperación mediante retroalimentación de relevancia. Revista de la Sociedad Americana de Ciencia de la Información, 41:288-297, 1990. [20] X. Shen, B. Tan y C. Zhai. Modelado implícito de usuario para búsqueda personalizada. En Actas de la 14ª conferencia internacional de ACM sobre información y gestión del conocimiento, páginas 824-831, 2005. [21] X. Shen y C. Zhai. Retroalimentación activa en la recuperación de información ad-hoc. En Actas de la 28ª conferencia internacional anual de ACM SIGIR sobre investigación y desarrollo en recuperación de información, páginas 59-66, 2005. [22] A. Spink. Retroalimentación de relevancia de términos y expansión de consultas: relación con el diseño. En Actas de la 17ª conferencia internacional anual de ACM SIGIR sobre investigación y desarrollo en recuperación de información, páginas 81-90, 1994. [23] J. Xu y W. B. Croft. Expansión de consulta utilizando análisis local y global de documentos. En Actas de la 19ª conferencia internacional anual de ACM SIGIR sobre investigación y desarrollo en recuperación de información, páginas 4-11, 1996. [24] H. Zaragoza, N. Craswell, M. Taylor, S. Saria y S. Robertson. Microsoft Cambridge en TREC-13: pistas Web y HARD. En Actas de la 13ª Conferencia de Recuperación de Información de Texto, 2004. [25] C. Zhai y J. Lafferty. Retroalimentación basada en modelos en el enfoque de modelado del lenguaje para la recuperación de información. En Actas de la décima conferencia internacional sobre gestión de la información y el conocimiento, páginas 403-410, 2001. [26] C. Zhai, A. Velivelli y B. Yu. Un modelo de mezcla de colecciones cruzadas para la minería de textos comparativa. En Actas de la décima conferencia internacional de ACM SIGKDD sobre descubrimiento de conocimiento y minería de datos, páginas 743-748, 2004. ",
            "candidates": [],
            "error": [
                [
                    "retroalimentación basada en términos",
                    "feedback basado en términos"
                ]
            ]
        },
        "information retrieval": {
            "translated_key": "",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Term Feedback for <br>information retrieval</br> with Language Models Bin Tan† , Atulya Velivelli‡ , Hui Fang† , ChengXiang Zhai† Dept.",
                "of Computer Science† , Dept. of Electrical and Computer Engineering‡ University of Illinois at Urbana-Champaign bintan@cs.uiuc.edu, velivell@ifp.uiuc.edu, hfang@cs.uiuc.edu, czhai@cs.uiuc.edu ABSTRACT In this paper we study term-based feedback for <br>information retrieval</br> in the language modeling approach.",
                "With term feedback a user directly judges the relevance of individual terms without interaction with feedback documents, taking full control of the query expansion process.",
                "We propose a cluster-based method for selecting terms to present to the user for judgment, as well as effective algorithms for constructing refined query language models from user term feedback.",
                "Our algorithms are shown to bring significant improvement in retrieval accuracy over a non-feedback baseline, and achieve comparable performance to relevance feedback.",
                "They are helpful even when there are no relevant documents in the top.",
                "Categories and Subject Descriptors H.3.3 [Information Search and Retrieval]: Retrieval models General Terms Algorithms 1.",
                "INTRODUCTION In the language modeling approach to <br>information retrieval</br>, feedback is often modeled as estimating an improved query model or relevance model based on a set of feedback documents [25, 13].",
                "This is in line with the traditional way of doing relevance feedback - presenting a user with documents/passages for relevance judgment and then extracting terms from the judged documents or passages to expand the initial query.",
                "It is an indirect way of seeking users assistance for query model construction, in the sense that the refined query model (based on terms) is learned through feedback documents/passages, which are high-level structures of terms.",
                "It has the disadvantage that irrelevant terms, which occur along with relevant ones in the judged content, may be erroneously used for query expansion, causing undesired effects.",
                "For example, for the TREC query Hubble telescope achievements, when a relevant document talks more about the telescopes repair than its discoveries, irrelevant terms such as spacewalk can be added into the modified query.",
                "We can consider a more direct way to involve a user in query model improvement, without an intermediary step of document feedback that can introduce noise.",
                "The idea is to present a (reasonable) number of individual terms to the user and ask him/her to judge the relevance of each term or directly specify their probabilities in the query model.",
                "This strategy has been discussed in [15], but to our knowledge, it has not been seriously studied in existing language modeling literature.",
                "Compared to traditional relevance feedback, this term-based approach to interactive query model refinement has several advantages.",
                "First, the user has better control of the final query model through direct manipulation of terms: he/she can dictate which terms are relevant, irrelevant, and possibly, to what degree.",
                "This avoids the risk of bringing unwanted terms into the query model, although sometimes the user introduces low-quality terms.",
                "Second, because a term takes less time to judge than a documents full text or summary, and as few as around 20 presented terms can bring significant improvement in retrieval performance (as we will show later), term feedback makes it faster to gather user feedback.",
                "This is especially helpful for interactive adhoc search.",
                "Third, sometimes there are no relevant documents in the top N of the initially retrieved results if the topic is hard.",
                "This is often true when N is constrained to be small, which arises from the fact that the user is unwilling to judge too many documents.",
                "In this case, relevance feedback is useless, as no relevant document can be leveraged on, but term feedback is still often helpful, by allowing relevant terms to be picked from irrelevant documents.",
                "During our participation in the TREC 2005 HARD Track and continued study afterward, we explored how to exploit term feedback from the user to construct improved query models for <br>information retrieval</br> in the language modeling approach.",
                "We identified two key subtasks of term-based feedback, i.e., pre-feedback presentation term selection and post-feedback query model construction, with effective algorithms developed for both.",
                "We imposed a secondary cluster structure on terms and found that a cluster view sheds additional insight into the users information need, and provides a good way of utilizing term feedback.",
                "Through experiments we found that term feedback improves significantly over the nonfeedback baseline, even though the user often makes mistakes in relevance judgment.",
                "Among our algorithms, the one with best retrieval performance is TCFB, the combination of TFB, the direct term feedback algorithm, and CFB, the cluster-based feedback algorithm.",
                "We also varied the number of feedback terms and observed reasonable improvement even at low numbers.",
                "Finally, by comparing term feedback with document-level feedback, we found it to be a viable alternative to the latter with competitive retrieval performance.",
                "The rest of the paper is organized as follows.",
                "Section 2 discusses some related work.",
                "Section 4 outlines our general approach to term feedback.",
                "We present our method for presentation term selection in Section 3 and algorithms for query model construction in Section 5.",
                "The experiment results are given in Section 6.",
                "Section 7 concludes this paper. 2.",
                "RELATED WORK Relevance feedback[17, 19] has long been recognized as an effective method for improving retrieval performance.",
                "Normally, the top N documents retrieved using the original query are presented to the user for judgment, after which terms are extracted from the judged relevant documents, weighted by their potential of attracting more relevant documents, and added into the query model.",
                "The expanded query usually represents the users information need better than the original one, which is often just a short keyword query.",
                "A second iteration of retrieval using this modified query usually produces significant increase in retrieval accuracy.",
                "In cases where true relevance judgment is unavailable and all top N documents are assumed to be relevant, it is called blind or pseudo feedback[5, 16] and usually still brings performance improvement.",
                "Because document is a large text unit, when it is used for relevance feedback many irrelevant terms can be introduced into the feedback process.",
                "To overcome this, passage feedback is proposed and shown to improve feedback performance[1, 23].",
                "A more direct solution is to ask the user for their relevance judgment of feedback terms.",
                "For example, in some relevance feedback systems such as [12], there is an interaction step that allows the user to add or remove expansion terms after they are automatically extracted from relevant documents.",
                "This is categorized as interactive query expansion, where the original query is augmented with user-provided terms, which can come from direct user input (free-form text or keywords)[22, 7, 10] or user selection of system-suggested terms (using thesauri[6, 22] or extracted from feedback documents[6, 22, 12, 4, 7]).",
                "In many cases term relevance feedback has been found to effectively improve retrieval performance[6, 22, 12, 4, 10].",
                "For example, the study in [12] shows that the user prefers to have explicit knowledge and direct control of which terms are used for query expansion, and the penetrable interface that provides this freedom is shown to perform better than other interfaces.",
                "However, in some other cases there is no significant benefit[3, 14], even if the user likes interacting with expansion terms.",
                "In a simulated study carried out in [18], the author compares the retrieval performance of interactive query expansion and automatic query expansion with a simulated study, and suggests that the potential benefits of the former can be hard to achieve.",
                "The user is found to be not good at identifying useful terms for query expansion, when a simple term presentation interface is unable to provide sufficient semantic context of the feedback terms.",
                "Our work differs from the previous ones in two important aspects.",
                "First, when we choose terms to present to the user for relevance judgment, we not only consider single-term value (e.g., the relative frequency of a term in the top documents, which can be measured by metrics such as Robertson Selection Value and Simplified Kullback-Leibler Distance as listed in [24]), but also examine the cluster structure of the terms, so as to produce a balanced coverage of the different topic aspects.",
                "Second, with the language modelling framework, we allow an elaborate construction of the updated query model, by setting different probabilities for different terms based on whether it is a query term, its significance in the top documents, and its cluster membership.",
                "Although techniques for adjusting query term weights exist for vector space models[17] and probablistic relevance models[9], most of the aforementioned works do not use them, choosing to just append feedback terms to the original query (thus using equal weights for them), which can lead to poorer retrieval performance.",
                "The combination of the two aspects allows our method to perform much better than the baseline.",
                "The usual way for feedback term presentation is just to display the terms in a list.",
                "There have been some works on alternative user interfaces. [8] arranges terms in a hierarchy, and [11] compares three different interfaces, including terms + checkboxes, terms + context (sentences) + checkboxes, sentences + input text box.",
                "In both studies, however, there is no significant performance difference.",
                "In our work we adopt the simplest approach of terms + checkboxes.",
                "We focus on term presentation and query model construction from feedback terms, and believe using contexts to improve feedback term quality should be orthogonal to our method. 3.",
                "GENERAL APPROACH We follow the language modeling approach, and base our method on the KL-divergence retrieval model proposed in [25].",
                "With this model, the retrieval task involves estimating a query language model θq from a given query, a document language model θd from each document, and calculating their KL-divergence D(θq||θd), which is then used to score the documents. [25] treats relevance feedback as a query model re-estimation problem, i.e., computing an updated query model θq given the original query text and the extra evidence carried by the judged relevant documents.",
                "We adopt this view, and cast our task as updating the query model from user term feedback.",
                "There are two key subtasks here: First, how to choose the best terms to present to the user for judgment, in order to gather maximal evidence about the users information need.",
                "Second, how to compute an updated query model based on this term feedback evidence, so that it captures the users information need and translates into good retrieval performance. 4.",
                "PRESENTATION TERM SELECTION Proper selection of terms to be presented to the user for judgment is crucial to the success of term feedback.",
                "If the terms are poorly chosen and there are few relevant ones, the user will have a hard time looking for useful terms to help clarify his/her information need.",
                "If the relevant terms are plentiful, but all concentrate on a single aspect of the query topic, then we will only be able to get feedback on that aspect and missing others, resulting in a breadth loss in retrieved results.",
                "Therefore, it is important to carefully select presentation terms to maximize expected gain from user feedback, i.e., those that can potentially reveal most evidence of the users information need.",
                "This is similar to active feedback[21], which suggests that a retrieval system should actively probe the users information need, and in the case of relevance feedback, the feedback documents should be chosen to maximize learning benefits (e.g. diversely so as to increase coverage).",
                "In our approach, the top N documents from an initial retrieval using the original query form the source of feedback terms: all terms that appear in them are considered candidates to present to the user.",
                "These documents serve as pseudo-feedback, since they provide a much richer context than the original query (usually very short), while the user is not asked to judge their relevance.",
                "Due to the latter reason, it is possible to make N quite large (e.g., in our experiments we set N = 60) to increase its coverage of different aspects in the topic.",
                "The simplest way of selecting feedback terms is to choose the most frequent M terms from the N documents.",
                "This method, however, has two drawbacks.",
                "First, a lot of common noisy terms will be selected due to their high frequencies in the document collection, unless a stop-word list is used for filtering.",
                "Second, the presentation list will tend to be filled by terms from major aspects of the topic; those from a minor aspect are likely to be missed due to their relatively low frequencies.",
                "We solve the above problems by two corresponding measures.",
                "First, we introduce a background model θB that is estimated from collection statistics and explains the common terms, so that they are much less likely to appear in the presentation list.",
                "Second, the terms are selected from multiple clusters in the pseudo-feedback documents, to ensure sufficient representation of different aspects of the topic.",
                "We rely on the mixture multinomial model, which is used for theme discovery in [26].",
                "Specifically, we assume the N documents contain K clusters {Ci| i = 1, 2, · · · K}, each characterized by a multinomial word distribution (also known as unigram language model) θi and corresponding to an aspect of the topic.",
                "The documents are regarded as sampled from a mixture of K + 1 components, including the K clusters and the background model: p(w|d) = λBp(w|θB) + (1 − λB) K i=1 πd,ip(w|θi) where w is a word, λB is the mixture weight for the background model θB, and πd,i is the document-specific mixture weight for the i-th cluster model θi.",
                "We then estimate the cluster models by maximizing the probability of the pseudo-feedback documents being generated from the multinomial mixture model: log p(D|Λ) = d∈D w∈V c(w; d) log p(w|d) where D = {di| i = 1, 2, · · · N} is the set of the N documents, V is the vocabulary, c(w; d) is ws frequency in d and Λ = {θi| i = 1, 2, · · · K} ∪ {πdij | i = 1, 2, · · · N, j = 1, 2, · · · K} is the set of model parameters to estimate.",
                "The cluster models can be efficiently estimated using the Expectation-Maximization (EM) algorithm.",
                "For its details, we refer the reader to [26].",
                "Table 1 shows the cluster models for TREC query Transportation tunnel disasters (K = 3).",
                "Note that only the middle cluster is relevant.",
                "Table 1: Cluster models for topic 363 Transportation tunnel disasters Cluster 1 Cluster 2 Cluster 3 tunnel 0.0768 tunnel 0.0935 tunnel 0.0454 transport 0.0364 fire 0.0295 transport 0.0406 traffic 0.0206 truck 0.0236 toll 0.0166 railwai 0.0186 french 0.0220 amtrak 0.0153 harbor 0.0146 smoke 0.0157 train 0.0129 rail 0.0140 car 0.0154 airport 0.0122 bridg 0.0139 italian 0.0152 turnpik 0.0105 kilomet 0.0136 firefight 0.0144 lui 0.0095 truck 0.0133 blaze 0.0127 jersei 0.0093 construct 0.0131 blanc 0.0121 pass 0.0087 · · · · · · · · · From each of the K estimated clusters, we choose the L = M/K terms with highest probabilities to form a total of M presentation terms.",
                "If a term happens to be in top L in multiple clusters, we assign it to the cluster where it has highest probability and let the other clusters take one more term as compensation.",
                "We also filter out terms in the original query text because they tend to always be relevant when the query is short.",
                "The selected terms are then presented to the user for judgment.",
                "A sample (completed) feedback form is shown in Figure 1.",
                "In this study we only deal with binary judgment: a presented term is by default unchecked, and a user may check it to indicate relevance.",
                "We also do not explicitly exploit negative feedback (i.e., penalizing irrelevant terms), because with binary feedback an unchecked term is not necessarily irrelevant (maybe the user is unsure about its relevance).",
                "We could ask the user for finer judgment (e.g., choosing from highly relevant, somewhat relevant, do not know, somewhat irrelevant and highly irrelevant), but binary feedback is more compact, taking less space to display and less user effort to make judgment. 5.",
                "ESTIMATING QUERY MODELS FROM TERM FEEDBACK In this section, we present several algorithms for exploiting term feedback.",
                "The algorithms take as input the original query q, the clusters {θi} as generated by the theme discovery algorithm, the set of feedback terms T and their relevance judgment R, and outputs an updated query language model θq that makes best use of the feedback evidence to capture the users information need.",
                "First we describe our notations: • θq: The original query model, derived from query terms only: p(w|θq) = c(w; q) |q| where c(w; q) is the count of w in q, and |q| = w∈q c(w; q) is the query length. • θq : The updated query model which we need to estimate from term feedback. • θi (i = 1, 2, . . .",
                "K): The unigram language model of cluster Ci, as estimated using the theme discovery algorithm. • T = {ti,j} (i = 1 . . .",
                "K, j = 1 . . .",
                "L): The set of terms presented to the user for judgment. ti,j is the j-th term chosen from cluster Ci. • R = {δw|w ∈ T}: δw is an indicator variable that is 1 if w is judged relevant or 0 otherwise. 5.1 TFB (Direct Term Feedback) This is a straight-forward form of term feedback that does not involve any secondary structure.",
                "We give a weight of 1 to terms judged relevant by the user, a weight of μ to query terms, zero weight to other terms, and then apply normalization: p(w|θq ) = δw + μ c(w; q) w ∈T δw + μ|q| where w ∈T δw is the total number of terms that are judged relevant.",
                "We call this method TFB (direct Term FeedBack).",
                "If we let μ = 1, this approach is equivalent to appending the relevant terms after the original query, which is what standard query expansion (without term reweighting) does.",
                "If we set μ > 1, we are putting more emphasis on the query terms than the checked ones.",
                "Note that the result model will be more biased toward θq if the original query is long or the user feedback is weak, which makes sense, as we can trust more on the original query in either case.",
                "Figure 1: Filled clarification form for Topic 363 363 transportation tunnel disasters Please select all terms that are relevant to the topic. traffic railway harbor rail bridge kilometer construct swiss cross link kong hong river project meter shanghai fire truck french smoke car italian firefights blaze blanc mont victim franc rescue driver chamonix emerge toll amtrak train airport turnpike lui jersey pass rome z center electron road boston speed bu submit 5.2 CFB (Cluster Feedback) Here we exploit the cluster structure that played an important role when we selected the presentation terms.",
                "The clusters represent different aspects of the query topic, each of which may or may not be relevant.",
                "If we are able to identify the relevant clusters, we can combine them to generate a query model that is good at discovering documents belonging to these clusters (instead of the irrelevant ones).",
                "We could ask the user to directly judge the relevance of a cluster after viewing representative terms in that cluster, but this would sometimes be a difficult task for the user, who has to guess the semantics of a cluster via its set of terms, which may not be well connected to one another due to a lack of context.",
                "Therefore, we propose to learn cluster feedback indirectly, inferring the relevance of a cluster through the relevance of its feedback terms.",
                "Because each cluster has an equal number of terms presented to the user, the simplest measure of a clusters relevance is the number of terms that are judged relevant in it.",
                "Intuitively, the more terms are marked relevant in a cluster, the closer the cluster is to the query topic, and the more the cluster should participate in query modification.",
                "If we combine the cluster models using weights determined this way and then interpolate with the original query model, we get the following formula for query updating, which we call CFB (Cluster FeedBack): p(w|θq ) = λp(w|θq) + (1 − λ) K i=1 L j=1 δti,j K k=1 L j=1 δtk,j p(w|θi) where L j=1 δti,j is the number of relevant terms in cluster Ci, and K k=1 L j=1 δtk,j is the total number of relevant terms.",
                "We note that when there is only one cluster (K = 1), the above formula degenerates to p(w|θq ) = λp(w|θq) + (1 − λ)p(w|θ1) which is merely pseudo-feedback of the form proposed in [25]. 5.3 TCFB (Term-cluster Feedback) TFB and CFB both have their drawbacks.",
                "TFB assigns non-zero probabilities to the presented terms that are marked relevant, but completely ignores (a lot more) others, which may be left unchecked due to the users ignorance, or simply not included in the presentation list, but we should be able to infer their relevance from the checked ones.",
                "For example, in Figure 1, since as many as 5 terms in the middle cluster (the third and fourth columns) are checked, we should have high confidence in the relevance of other terms in that cluster.",
                "CFB remedies TFBs problem by treating the terms in a cluster collectively, so that unchecked/unpresented terms receive weights when presented terms in their clusters are judged as relevant, but it does not distinguish which terms in a cluster are presented or judged.",
                "Intuitively, the judged relevant terms should receive larger weights because they are explicitly indicated as relevant by the user.",
                "Therefore, we try to combine the two methods, hoping to get the best out of both.",
                "We do this by interpolating the TFB model with the CFB model, and call it TCFB: p(w|θq ) = αp(w|θqT F B ) + (1 − α)p(w|θqCF B ) 6.",
                "EXPERIMENTS In this section, we describe our experiment results.",
                "We first describe our experiment setup and present an overview of various methods performance.",
                "Then we discuss the effects of varying the parameter setting in the algorithms, as well as the number of presentation terms.",
                "Next we analyze user term feedback behavior and its relation to retrieval performance.",
                "Finally we compare term feedback to relevance feedback and show that it has its particular advantage. 6.1 Experiment Setup and Basic Results We took the opportunity of TREC 2005 HARD Track[2] for the evaluation of our algorithms.",
                "The tracks used the AQUAINT collection, a 3GB corpus of English newswire text.",
                "The topics included 50 ones previously known to be hard, i.e. with low retrieval performance.",
                "It is for these hard topics that user feedback is most helpful, as it can provide information to disambiguate the queries; with easy topics the user may be unwilling to spend efforts for feedback if the automatic retrieval results are good enough.",
                "Participants of the track were able to submit custom-designed clarification forms (CF) to solicit feedback from human assessors provided by Table 2: Retrieval performance for different methods and CF types.",
                "The last row is the percentage of MAP improvement over the baseline.",
                "The parameter settings μ = 4, λ = 0.1, α = 0.3 are near optimal.",
                "Baseline TFB1C TFB3C TFB6C CFB1C CFB3C CFB6C TCFB1C TCFB3C TCFB6C MAP 0.219 0.288 0.288 0.278 0.254 0.305 0.301 0.274 0.309 0.304 Pr@30 0.393 0.467 0.475 0.457 0.399 0.480 0.473 0.431 0.491 0.473 RR 4339 4753 4762 4740 4600 4907 4872 4767 4947 4906 % 0% 31.5% 31.5% 26.9% 16.0% 39.3% 37.4% 25.1% 41.1% 38.8% Table 3: MAP variation with the number of presented terms. # terms TFB1C TFB3C TFB6C CFB3C CFB6C TCFB3C TCFB6C 6 0.245 0.240 0.227 0.279 0.279 0.281 0.274 12 0.261 0.261 0.242 0.299 0.286 0.297 0.281 18 0.275 0.274 0.256 0.301 0.282 0.300 0.286 24 0.276 0.281 0.265 0.303 0.292 0.305 0.292 30 0.280 0.285 0.270 0.304 0.296 0.307 0.296 36 0.282 0.288 0.272 0.307 0.297 0.309 0.297 42 0.283 0.288 0.275 0.306 0.298 0.309 0.300 48 0.288 0.288 0.278 0.305 0.301 0.309 0.303 NIST.",
                "We designed three sets of clarification forms for term feedback, differing in the choice of K, the number of clusters, and L, the number of presented terms from each cluster.",
                "They are: 1× 48, a big cluster with 48 terms, 3 × 16, 3 clusters with 16 terms each, and 6 × 8, 6 clusters with 8 terms each.",
                "The total number of presented terms (M) is fixed at 48, so by comparing the performance of different types of clarification forms we can know the effects of different degree of clustering.",
                "For each topic, an assessor would complete the forms ordered by 6 × 8, 1 × 48 and 3 × 16, spending up to three minutes on each form.",
                "The sample clarification form shown in Figure 1 is of type 3 × 16.",
                "It is a simple and compact interface in which the user can check relevant terms.",
                "The form is self-explanatory; there is no need for extra user training on how to use it.",
                "Our initinal queries are constructed only using the topic title descriptions, which are on average 2.7 words in length.",
                "As our baseline we use the KL divergence retrieval method implemented in the Lemur Toolkit1 with 5 pseudo-feedback documents.",
                "We stem the terms, choose Dirichlet smoothing with a prior of 2000, and truncate query language models to 50 terms (these settings are used throughout the experiments).",
                "For all other parameters we use Lemurs default settings.",
                "The baseline turns out to perform above average among the track participants.",
                "After an initial run using this baseline retrieval method, we take the top 60 documents for each topic and apply the theme discovery algorithm to output the clusters (1, 3, or 6 of them), based on which we generate clarification forms.",
                "After user feedback is received, we run the term feedback algorithms (TFB, CFB or TCFB) to estimate updated query models, which are then used for a second iteration of retrieval.",
                "We evaluate the different retrieval methods performance on their rankings of the top 1000 documents.",
                "The evaluation metrics we adopt include mean average (non-interpolated) precision (MAP), precision at top 30 (Pr@30) and total relevant retrieved (RR).",
                "Table 2 shows the performance of various methods and configurations of K × L. The suffixes (1C, 3C, 6C) after TFB,CFB,TCFB stand for the number of clusters (K).",
                "For example, TCFB3C means the TCFB method on the 3 × 16 clarification forms.",
                "From Table 2 we can make the following observations: 1 http://www.lemurproject.com 1.",
                "All methods perform considerably better than the pseudofeedback baseline, with TCFB3C achieving a highest 41.1% improvement in MAP, indicating significant contribution of term feedback for clarification of the users information need.",
                "In other words, term feedback is truly helpful for improving retrieval accuracy. 2.",
                "For TFB, the performance is almost equal on the 1 × 48 and 3 × 16 clarification forms in terms of MAP (although the latter is slightly better in Pr@30 and RR), and a little worse on the 6 × 8 ones. 3.",
                "Both CFB3C and CFB6C perform better than their TFB counterparts in all three metrics, suggesting that feedback on a secondary cluster structure is indeed beneficial.",
                "CFB1C is actually worse because it cannot adjust the weight of its (single) cluster from term feedback and it is merely pseudofeedback. 4.",
                "Although TCFB is just a simple mixture of TFB and CFB by interpolation, it is able to outperform both.",
                "This supports our speculation that TCFB overcomes the drawbacks of TFB (paying attention only to checked terms) and CFB (not distinguishing checked and unchecked terms in a cluster).",
                "Except for TCFB6C v.s.",
                "CFB6C, the performance advantage of TCFB over TFB/CFB is significant at p < 0.05 using the Wilcoxon signed rank test.",
                "This is not true in the case of TFB v.s.",
                "CFB, each of which is better than the other in nearly half of the topics. 6.2 Reduction of Presentation Terms In some situations we may have to reduce the number of presentation terms due to limits in display space or user feedback efforts.",
                "It is interesting to know whether our algorithms performance deteriorates when the user is presented with fewer terms.",
                "Because the presentation terms within each cluster are generated in decreasing order of their frequencies, the presentation list forms a subset of the original one if its size is reduced2 .",
                "Therefore, we can easily simulate what happens when the number of presentation terms decreases 2 There are complexities arising from terms appearing in top L of multiple clusters, but these are exceptions from M to M : we will keep all judgments of the top L = M /K terms in each cluster and discard those of others.",
                "Table 3 shows the performance of various algorithms as the number of presentation terms ranges from 6 to 48.",
                "We find that the performance of TFB is more susceptible to presentation term reduction than that of CFB or TCFB.",
                "For example, at 12 terms the MAP of TFB3C is 90.6% of that at 48 terms, while the numbers for CFB3C and TCFB3C are 98.0% and 96.1% respectively.",
                "We conjecture the reason to be that while TFBs performance heavily depends on how many good terms are chosen for query expansion, CFB only needs a rough estimate of cluster weights to work.",
                "Also, the 3 × 16 clarification forms seem to be more robust than the 6 × 8 ones: at 12 terms the MAP of TFB6C is 87.1% of that at 48 terms, lower than 90.6% for TFB3C.",
                "Similarly, for CFB it is 95.0% against 98.0%.",
                "This is natual, as for a large cluster number of 6, it is easier to get into the situation where each cluster gets too few presentation terms to make topic diversification useful.",
                "Overall, we are surprised to see that the algorithms are still able to perform reasonably well when the number of presentation terms is small.",
                "For example, at only 12 terms CFB3C (the clarification form is of size 3 × 4) can still improve 36.5% over the baseline, dropping slightly from 39.3% at 48 terms. 6.3 User Feedback Analysis In this part we study several aspects of users term feedback behavior, and whether they are connected to retrieval performance.",
                "Figure 2: Clarification form completion time distributions 0−30 30−60 60−90 90−120 120−150 150−180 0 5 10 15 20 25 30 35 completion time (seconds) #topics 1×48 3×16 6×8 Figure 2 shows the distribution of time needed to complete a clarification form3 .",
                "We see that the user is usually able to finish term feedback within a reasonably short amount of time: for more than half of the topics the clarification form is completed in just 1 minute, and only a small fraction of topics (less than 10% for 1 × 48 and 3 × 16) take more than 2 minutes.",
                "This suggests that term feedback is suitable for interactive ad-hoc retrieval, where a user usually does not want to spend too much time on providing feedback.",
                "We find that a user often makes mistakes when judging term relevance.",
                "Sometimes a relevant term may be left out because its connection to the query topic is not obvious to the user.",
                "Other times a dubious term may be included but turns out to be irrelevant.",
                "Take the topic in Figure 1 for example.",
                "There was a fire disaster in Mont 3 The maximal time is 180 seconds, as the NIST assessor would be forced to submit the form at that moment.",
                "Table 4: Term selection statistics (topic average) CF Type 1 × 48 3 × 16 6 × 8 # checked terms 14.8 13.3 11.2 # rel. terms 15.0 12.6 11.2 # rel. checked terms 7.9 6.9 5.9 precision 0.534 0.519 0.527 recall 0.526 0.548 0.527 Blanc Tunnel between France and Italy in 1999, but the user failed to select such keywords as mont, blanc, french and italian due to his/her ignorance of the event.",
                "Indeed, without proper context it would be hard to make perfect judgment.",
                "What is then, the extent to which the user is good at term feedback?",
                "Does it have serious impact on retrieval performance?",
                "To answer these questions, we need a measure of individual terms true relevance.",
                "We adopt the Simplified KL Divergence metric used in [24] to decide query expansion terms as our term relevance measure: σKLD(w) = p(w|R) log p(w|R) p(w|¬R) where p(w|R) is the probability that a relevant document contains term w, and p(w|¬R) is the probability that an irrelevant document contains w, both of which can be easily computed via maximum likelihood estimate given document-level relevance judgment.",
                "If σKLD(w) > 0, w is more likely to appear in relevant documents than irrelevant ones.",
                "We consider a term relevant if its Simplified KL Divergence value is greater than a certain threshold σ0.",
                "We can then define precision and recall of user term judgment accordingly: precision is the fraction of terms checked by the user that are relevant; recall is the fraction of presented relevant terms that are checked by the user.",
                "Table 4 shows the number of checked terms, relevant terms and relevant checked terms when σ0 is set to 1.0, as well as the precision/recall of user term judgment.",
                "Note that when the clarification forms contain more clusters, fewer terms are checked: 14.8 for 1 × 48, 13.3 for 3 × 16 and 11.2 for 6×8.",
                "Similar pattern holds for relevant terms and relevant checked terms.",
                "There seems to be a trade-off between increasing topic diversity by clustering and losing extra relevant terms: when there are more clusters, each of them gets fewer terms to present, which can hurt a major relevant cluster that contains many relevant terms.",
                "Therefore, it is not always helpful to have more clusters, e.g., TFB6C is actually worse than TFB1C.",
                "The major finding we can make from Table 4 is that the user is not particularly good at identifying relevant terms, which echoes the discovery in [18].",
                "In the case of 3 × 16 clarification forms, the average number of terms checked as relevant by the user is 13.3 per topic, and the average number of relevant terms whose σKLD value exceed 1.0 is 12.6.",
                "The user is able to recognize only 6.9 of these terms on average.",
                "Indeed, the precision and recall of user feedback terms (as defined previously) are far from perfect.",
                "On the other hand, If the user had correctly checked all such relevant terms, the performance of our algorithms would have increased a lot, as shown in Table 5.",
                "We see that TFB gets big improvement when there is an oracle who checks all relevant terms, while CFB meets a bottleneck around MAP of 0.325, since all it does is adjust cluster weights, and when the learned weights are close to being accurate, it cannot benefit more from term feedback.",
                "Also note that TCFB fails to outperform TFB, probably because TFB is sufficiently accurate.",
                "Table 5: Change of MAP when using all (and only) relevant terms (σKLD > 1.0) for feedback. original term feedback relevant term feedback TF1 0.288 0.354 TF3 0.288 0.354 TF6 0.278 0.346 CF3 0.305 0.325 CF6 0.301 0.326 TCF3 0.309 0.345 TCF6 0.304 0.341 6.4 Comparison with Relevance Feedback Now we compare term feedback with document-level relevance feedback, in which the user is presented with the top N documents from an initial retrieval and asked to judge their relevance.",
                "The feedback process is simulated using document relevance judgment from NIST.",
                "We use the mixture model based feedback method proposed in [25], with mixture noise set to 0.95 and feedback coefficient set to 0.9.",
                "Comparative evaluation of relevance feedback against other methods is complicated by the fact that some documents have already been viewed during feedback, so it makes no sense to include them in the retrieval results of the second run.",
                "However, this does not hold for term feedback.",
                "Thus, to make it fair w.r.t. users information gain, if the feedback documents are relevant, they should be kept in the top of the ranking; if they are irrelevant, they should be left out.",
                "Therefore, we use relevance feedback to produce a ranking of top 1000 retrieved documents but with every feedback document excluded, and then prepend the relevant feedback documents at the front.",
                "Table 6 shows the performance of relevance feedback for different values of N and compares it with TCFB3C.",
                "Table 6: Performance of relevance feedback for different number of feedback documents (N).",
                "N MAP Pr@30 RR 5 0.302 0.586 4779 10 0.345 0.670 4916 20 0.389 0.772 5004 TCFB3C 0.309 0.491 4947 We see that the performance of TCFB3C is comparable to that of relevance feedback using 5 documents.",
                "Although it is poorer than when there are 10 feedback documents in terms of MAP and Pr@30, it does retrieve more documents (4947) when going down the ranked list.",
                "We try to compare the quality of automatically inserted terms in relevance feedback with that of manually selected terms in term feedback.",
                "This is done by truncating the relevance feedback modified query model to a size equal to the number of checked terms for the same topic.",
                "We can then compare the terms in the truncated model with the checked terms.",
                "Figure 3 shows the distribution of the terms σKLD scores.",
                "We find that term feedback tends to produce expansion terms of higher quality(those with σKLD > 1) compared to relevance feedback (with 10 feedback documents).",
                "This does not contradict the fact that the latter yields higher retrieval performance.",
                "Actually, when we use the truncated query model instead of the intact one refined from relevance feedback, the MAP is only 0.304.",
                "The truth Figure 3: Comparison of expansion term quality between relevance feedback (with 10 feedback documents) and term feedback (with 3 × 16 CFs) −1−0 0−1 1−2 2−3 3−4 4−5 5−6 0 50 100 150 200 250 300 350 σKLD #terms relevance feedback term feedback is, although there are many unwanted terms in the expanded query model from feedback documents, there are also more relevant terms than what the user can possibly select from the list of presentation terms generated with pseudo-feedback documents, and the positive effects often outweights the negative ones.",
                "We are interested to know under what circumstances term feedback has advantage over relevance feedback.",
                "One such situation is when none of the top N feedback documents is relevant, rendering relevance feedback useless.",
                "This is not infrequent, as one might have thought: out of the 50 topics, there are 13 such cases when N = 5, 10 when N = 10, and still 3 when N = 20.",
                "When this happens, one can only back off to the original retrieval method; the power of relevance feedback is lost.",
                "Surprisingly, in 11 out of 13 such cases where relevance feedback seems impossible, the user is able to check at least 2 relevant terms from the 3 × 16 clarification form (we consider term t to be relevant if σKLD(t) > 1.0).",
                "Furthermore, in 10 out of them TCFB3C outperforms the pseudo-feedback baseline, increasing MAP from 0.076 to 0.146 on average (these are particularly hard topics).",
                "We think that there are two possible explanations for this phenomenon of term feedback being active even when relevance feedback does not work: First, even if none of the top N (suppose it is a small number) documents are relevant, we may still find relevant documents in top 60, which is more inclusive but usually unreachable when people are doing relevance feedback in interactive ad-hoc search, from which we can draw feedback terms.",
                "This is true for topic 367 piracy, where the top 10 feedback documents are all about software piracy, yet there are documents between 10-60 that are about piracy on the seas (which is about the real information need), contributing terms such as pirate, ship for selection in the clarification form.",
                "Second, for some topics, a document needs to meet some special condition in order to be relevant.",
                "The top N documents may be related to the topic, but nonetheless irrelevant.",
                "In this case, we may still extract useful terms from these documents, even if they do not qualify as relevant ones.",
                "For example, in topic 639 consumer online shopping, a document needs to mention what contributes to shopping growth to really match the specified information need, hence none of the top 10 feedback documents are regarded as relevant.",
                "But nevertheless, the feedback terms such as retail, commerce are good for query expansion. 7.",
                "CONCLUSIONS In this paper we studied the use of term feedback for interactive <br>information retrieval</br> in the language modeling approach.",
                "We proposed a cluster-based method for selecting presentation terms as well as algorithms to estimate refined query models from user term feedback.",
                "We saw significant improvement in retrieval accuracy brought by term feedback, in spite of the fact that a user often makes mistakes in relevance judgment that hurts its performance.",
                "We found the best-performing algorithm to be TCFB, which benefits from the combination of directly observed term evidence with TFB and indirectly learned cluster relevance with CFB.",
                "When we reduced the number of presentation terms, term feedback is still able to keep much of its performance gain over the baseline.",
                "Finally, we compared term feedback to document-level relevance feedback, and found that TCFB3Cs performance is on a par with the latter with 5 feedback documents.",
                "We regarded term feedback as a viable alternative to traditional relevance feedback, especially when there are no relevant documents in the top.",
                "We propose to extend our work in several ways.",
                "First, we want to study whether the use of various contexts can help the user to better identify term relevance, while not sacrificing the simplicity and compactness of term feedback.",
                "Second, currently all terms are presented to the user in a single batch.",
                "We could instead consider iterative term feedback, by presenting a small number of terms first, and show more terms after receiving user feedback or stop when the refined query is good enough.",
                "The presented terms should be selected dynamically to maximize learning benefits at any moment.",
                "Third, we have plans to incorporate term feedback into our UCAIR toolbar[20], an Internet Explorer plugin, to make it work for web search.",
                "We are also interested in studying how to combine term feedback with relevance feedback or implicit feedback.",
                "We could, for example, allow the user to dynamically modify terms in a language model learned from feedback documents. 8.",
                "ACKNOWLEDGMENT This work is supported in part by the National Science Foundation grants IIS-0347933 and IIS-0428472. 9.",
                "REFERENCES [1] J. Allan.",
                "Relevance feedback with too much data.",
                "In Proceedings of the 18th annual international ACM SIGIR conference on research and development in <br>information retrieval</br>, pages 337-343, 1995. [2] J. Allan.",
                "HARD track overview in TREC 2005 - High Accuracy Retrieval from Documents.",
                "In The Fourteenth Text REtrieval Conference, 2005. [3] P. Anick.",
                "Using terminological feedback for web search refinement: a log-based study.",
                "In Proceedings of the 26th annual international ACM SIGIR conference on research and development in informaion retrieval, pages 88-95, 2003. [4] P. G. Anick and S. Tipirneni.",
                "The paraphrase search assistant: terminological feedback for iterative information seeking.",
                "In Proceedings of the 22nd annual international ACM SIGIR conference on research and development in <br>information retrieval</br>, pages 153-159, 1999. [5] C. Buckley, G. Salton, J. Allan, and A. Singhal.",
                "Automatic query expansion using SMART.",
                "In Proceedings of the Third Text REtrieval Conference, 1994. [6] D. Harman.",
                "Towards interactive query expansion.",
                "In Proceedings of the 11th annual international ACM SIGIR conference on research and development in <br>information retrieval</br>, pages 321-331, 1988. [7] N. A. Jaleel, A. Corrada-Emmanuel, Q. Li, X. Liu, C. Wade, and J. Allan.",
                "UMass at TREC 2003: HARD and QA.",
                "In TREC, pages 715-725, 2003. [8] H. Joho, C. Coverson, M. Sanderson, and M. Beaulieu.",
                "Hierarchical presentation of expansion terms.",
                "In Proceedings of the 2002 ACM symposium on applied computing, pages 645-649, 2002. [9] K. S. Jones, S. Walker, and S. E. Robertson.",
                "A probabilistic model of <br>information retrieval</br>: development and status.",
                "Technical Report 446, Computer Laboratory, University of Cambridge, 1998. [10] D. Kelly, V. D. Dollu, and X. Fu.",
                "The loquacious user: a document-independent source of terms for query expansion.",
                "In Proceedings of the 28th annual international ACM SIGIR conference on research and development in <br>information retrieval</br>, pages 457-464, 2005. [11] D. Kelly and X. Fu.",
                "Elicitation of term relevance feedback: an investigation of term source and context.",
                "In Proceedings of the 29th annual international ACM SIGIR conference on research and development in <br>information retrieval</br>, 2006. [12] J. Koenemann and N. Belkin.",
                "A case for interaction: A study of interactive <br>information retrieval</br> behavior and effectiveness.",
                "In Proceedings of the SIGCHI conference on human factors in computing systems, pages 205-212, 1996. [13] V. Lavrenko and W. B. Croft.",
                "Relevance-based language models.",
                "In Research and Development in <br>information retrieval</br>, pages 120-127, 2001. [14] Y. Nemeth, B. Shapira, and M. Taeib-Maimon.",
                "Evaluation of the real and perceived value of automatic and interactive query expansion.",
                "In Proceedings of the 27th annual international ACM SIGIR conference on research and development in <br>information retrieval</br>, pages 526-527, 2004. [15] J. Ponte.",
                "A Language Modeling Approach to <br>information retrieval</br>.",
                "PhD thesis, University of Massachusetts at Amherst, 1998. [16] S. E. Robertson, S. Walker, S. Jones, M. Beaulieu, and M. Gatford.",
                "Okapi at TREC-3.",
                "In Proceedings of the Third Text REtrieval Conference, 1994. [17] J. Rocchio.",
                "Relevance feedback in <br>information retrieval</br>.",
                "In The SMART retrieval system, pages 313-323. 1971. [18] I. Ruthven.",
                "Re-examining the potential effectiveness of interactive query expansion.",
                "In Proceedings of the 26th annual international ACM SIGIR conference on research and development in informaion retrieval, pages 213-220, 2003. [19] G. Salton and C. Buckley.",
                "Improving retrieval performance by relevance feedback.",
                "Journal of the American Society for Information Science, 41:288-297, 1990. [20] X. Shen, B. Tan, and C. Zhai.",
                "Implicit user modeling for personalized search.",
                "In Proceedings of the 14th ACM international conference on information and knowledge management, pages 824-831, 2005. [21] X. Shen and C. Zhai.",
                "Active feedback in ad-hoc <br>information retrieval</br>.",
                "In Proceedings of the 28th annual international ACM SIGIR conference on research and development in <br>information retrieval</br>, pages 59-66, 2005. [22] A. Spink.",
                "Term relevance feedback and query expansion: relation to design.",
                "In Proceedings of the 17th annual international ACM SIGIR conference on research and development in <br>information retrieval</br>, pages 81-90, 1994. [23] J. Xu and W. B. Croft.",
                "Query expansion using local and global document analysis.",
                "In Proceedings of the 19th annual international ACM SIGIR conference on research and development in <br>information retrieval</br>, pages 4-11, 1996. [24] H. Zaragoza, N. Craswell, M. Taylor, S. Saria, and S. Robertson.",
                "Microsoft cambridge at TREC-13: Web and HARD tracks.",
                "In Proceedings of the 13th Text REtrieval Conference, 2004. [25] C. Zhai and J. Lafferty.",
                "Model-based feedback in the language modeling approach to <br>information retrieval</br>.",
                "In Proceedings of the tenth international conference on information and knowledge management, pages 403-410, 2001. [26] C. Zhai, A. Velivelli, and B. Yu.",
                "A cross-collection mixture model for comparative text mining.",
                "In Proceedings of the tenth ACM SIGKDD international conference on knowledge discovery and data mining, pages 743-748, 2004."
            ],
            "original_annotated_samples": [
                "Term Feedback for <br>information retrieval</br> with Language Models Bin Tan† , Atulya Velivelli‡ , Hui Fang† , ChengXiang Zhai† Dept.",
                "of Computer Science† , Dept. of Electrical and Computer Engineering‡ University of Illinois at Urbana-Champaign bintan@cs.uiuc.edu, velivell@ifp.uiuc.edu, hfang@cs.uiuc.edu, czhai@cs.uiuc.edu ABSTRACT In this paper we study term-based feedback for <br>information retrieval</br> in the language modeling approach.",
                "INTRODUCTION In the language modeling approach to <br>information retrieval</br>, feedback is often modeled as estimating an improved query model or relevance model based on a set of feedback documents [25, 13].",
                "During our participation in the TREC 2005 HARD Track and continued study afterward, we explored how to exploit term feedback from the user to construct improved query models for <br>information retrieval</br> in the language modeling approach.",
                "CONCLUSIONS In this paper we studied the use of term feedback for interactive <br>information retrieval</br> in the language modeling approach."
            ],
            "translated_annotated_samples": [
                "Comentarios sobre el término para la <br>recuperación de información</br> con modelos de lenguaje Bin Tan†, Atulya Velivelli‡, Hui Fang†, ChengXiang Zhai† Dept.",
                "En este documento estudiamos la retroalimentación basada en términos para la <br>recuperación de información</br> en el enfoque de modelado de lenguaje.",
                "En el enfoque de modelado del lenguaje para la <br>recuperación de información</br>, la retroalimentación a menudo se modela como la estimación de un modelo de consulta mejorado o un modelo de relevancia basado en un conjunto de documentos de retroalimentación [25, 13].",
                "Durante nuestra participación en la pista HARD de TREC 2005 y el estudio continuado posteriormente, exploramos cómo aprovechar la retroalimentación de términos por parte del usuario para construir modelos de consulta mejorados para la <br>recuperación de información</br> en el enfoque de modelado de lenguaje.",
                "CONCLUSIONES En este artículo estudiamos el uso de la retroalimentación de términos para la <br>recuperación interactiva de información</br> en el enfoque de modelado del lenguaje."
            ],
            "translated_text": "Comentarios sobre el término para la <br>recuperación de información</br> con modelos de lenguaje Bin Tan†, Atulya Velivelli‡, Hui Fang†, ChengXiang Zhai† Dept. En este documento estudiamos la retroalimentación basada en términos para la <br>recuperación de información</br> en el enfoque de modelado de lenguaje. Con la retroalimentación de términos, un usuario juzga directamente la relevancia de términos individuales sin interacción con documentos de retroalimentación, tomando el control total del proceso de expansión de la consulta. Proponemos un método basado en clusters para seleccionar términos para presentar al usuario para su evaluación, así como algoritmos efectivos para construir modelos de lenguaje de consulta refinados a partir de la retroalimentación de términos del usuario. Nuestros algoritmos han demostrado proporcionar una mejora significativa en la precisión de recuperación en comparación con una línea base sin retroalimentación, y logran un rendimiento comparable al de la retroalimentación de relevancia. Son útiles incluso cuando no hay documentos relevantes en la parte superior. Categorías y Descriptores de Asignaturas H.3.3 [Búsqueda y Recuperación de Información]: Modelos de recuperación Términos generales Algoritmos 1. En el enfoque de modelado del lenguaje para la <br>recuperación de información</br>, la retroalimentación a menudo se modela como la estimación de un modelo de consulta mejorado o un modelo de relevancia basado en un conjunto de documentos de retroalimentación [25, 13]. Esto está en línea con la forma tradicional de realizar retroalimentación de relevancia: presentar al usuario documentos/pasajes para su juicio de relevancia y luego extraer términos de los documentos o pasajes juzgados para expandir la consulta inicial. Es una forma indirecta de buscar la ayuda de los usuarios para la construcción del modelo de consulta, en el sentido de que el modelo de consulta refinado (basado en términos) se aprende a través de documentos/pasajes de retroalimentación, que son estructuras de alto nivel de términos. Tiene la desventaja de que los términos irrelevantes, que ocurren junto con los relevantes en el contenido evaluado, pueden ser utilizados erróneamente para la expansión de la consulta, causando efectos no deseados. Por ejemplo, para la consulta TREC del telescopio Hubble logros, cuando un documento relevante habla más sobre la reparación de los telescopios que sobre sus descubrimientos, términos irrelevantes como caminata espacial pueden ser agregados a la consulta modificada. Podemos considerar una forma más directa de involucrar a un usuario en la mejora del modelo de consulta, sin un paso intermedio de retroalimentación de documentos que pueda introducir ruido. La idea es presentar al usuario un número (razonable) de términos individuales y pedirle que juzgue la relevancia de cada término o especifique directamente sus probabilidades en el modelo de consulta. Esta estrategia ha sido discutida en [15], pero hasta donde sabemos, no ha sido estudiada seriamente en la literatura existente de modelado de lenguaje. En comparación con la retroalimentación de relevancia tradicional, este enfoque basado en términos para el refinamiento del modelo de consulta interactivo tiene varias ventajas. Primero, el usuario tiene un mejor control del modelo de consulta final a través de la manipulación directa de términos: él/ella puede dictar qué términos son relevantes, irrelevantes y posiblemente, en qué medida. Esto evita el riesgo de introducir términos no deseados en el modelo de consulta, aunque a veces el usuario introduce términos de baja calidad. Segundo, dado que un término tarda menos tiempo en ser evaluado que el texto completo o el resumen de un documento, y con tan solo alrededor de 20 términos presentados se puede lograr una mejora significativa en el rendimiento de recuperación (como demostraremos más adelante), el feedback de términos permite recopilar la retroalimentación del usuario de manera más rápida. Esto es especialmente útil para la búsqueda interactiva ad hoc. En tercer lugar, a veces no hay documentos relevantes en los primeros N resultados recuperados inicialmente si el tema es difícil. Esto suele ser cierto cuando N está limitado a ser pequeño, lo cual surge del hecho de que el usuario no está dispuesto a evaluar demasiados documentos. En este caso, la retroalimentación de relevancia es inútil, ya que no se puede aprovechar ningún documento relevante, pero la retroalimentación de términos sigue siendo útil, al permitir seleccionar términos relevantes de documentos irrelevantes. Durante nuestra participación en la pista HARD de TREC 2005 y el estudio continuado posteriormente, exploramos cómo aprovechar la retroalimentación de términos por parte del usuario para construir modelos de consulta mejorados para la <br>recuperación de información</br> en el enfoque de modelado de lenguaje. Identificamos dos sub tareas clave del feedback basado en términos, es decir, la selección de términos para la presentación previa al feedback y la construcción del modelo de consulta posterior al feedback, con algoritmos efectivos desarrollados para ambos. Impusimos una estructura de clúster secundaria en los términos y descubrimos que una vista de clúster arroja una mayor comprensión de la necesidad de información de los usuarios, y proporciona una buena forma de utilizar la retroalimentación de términos. A través de experimentos encontramos que el feedback a término mejora significativamente sobre la línea base sin feedback, a pesar de que el usuario a menudo comete errores en la evaluación de relevancia. Entre nuestros algoritmos, el que tiene el mejor rendimiento de recuperación es TCFB, la combinación de TFB, el algoritmo de retroalimentación de términos directos, y CFB, el algoritmo de retroalimentación basado en clústeres. También variamos el número de términos de retroalimentación y observamos una mejora razonable incluso con números bajos. Finalmente, al comparar la retroalimentación a nivel de términos con la retroalimentación a nivel de documentos, encontramos que es una alternativa viable a esta última con un rendimiento competitivo en la recuperación. El resto del documento está organizado de la siguiente manera. La sección 2 discute algunos trabajos relacionados. La sección 4 describe nuestro enfoque general para la retroalimentación de términos. Presentamos nuestro método para la selección de términos de presentación en la Sección 3 y los algoritmos para la construcción del modelo de consulta en la Sección 5. Los resultados del experimento se presentan en la Sección 6. La sección 7 concluye este documento. 2. TRABAJO RELACIONADO La retroalimentación de relevancia[17, 19] ha sido reconocida durante mucho tiempo como un método efectivo para mejorar el rendimiento de recuperación. Normalmente, los N documentos principales recuperados utilizando la consulta original se presentan al usuario para su evaluación, después de lo cual se extraen términos de los documentos relevantes evaluados, ponderados por su potencial de atraer más documentos relevantes, y se añaden al modelo de consulta. La consulta ampliada suele representar mejor la necesidad de información de los usuarios que la original, que a menudo es solo una consulta de palabras clave cortas. Una segunda iteración de recuperación utilizando esta consulta modificada generalmente produce un aumento significativo en la precisión de la recuperación. En los casos en los que no se dispone de una evaluación de relevancia real y se asume que todos los documentos principales N son relevantes, se denomina retroalimentación ciega o pseudo [5, 16] y generalmente sigue mejorando el rendimiento. Debido a que un documento es una unidad de texto grande, cuando se utiliza para retroalimentación de relevancia, muchos términos irrelevantes pueden ser introducidos en el proceso de retroalimentación. Para superar esto, se propone el feedback de pasajes y se muestra que mejora el rendimiento del feedback. Una solución más directa es pedir al usuario su juicio de relevancia sobre los términos de retroalimentación. Por ejemplo, en algunos sistemas de retroalimentación de relevancia como [12], hay un paso de interacción que permite al usuario agregar o eliminar términos de expansión después de que son extraídos automáticamente de los documentos relevantes. Esto se clasifica como expansión interactiva de consultas, donde la consulta original se amplía con términos proporcionados por el usuario, que pueden provenir de la entrada directa del usuario (texto libre o palabras clave) o de la selección del usuario de términos sugeridos por el sistema (utilizando tesauros o extraídos de documentos de retroalimentación). En muchos casos, se ha encontrado que la retroalimentación de relevancia de términos mejora de manera efectiva el rendimiento de recuperación [6, 22, 12, 4, 10]. Por ejemplo, el estudio en [12] muestra que el usuario prefiere tener un conocimiento explícito y control directo de qué términos se utilizan para la expansión de la consulta, y se demuestra que la interfaz penetrable que proporciona esta libertad tiene un mejor rendimiento que otras interfaces. Sin embargo, en algunos otros casos no hay un beneficio significativo[3, 14], incluso si al usuario le gusta interactuar con los términos de expansión. En un estudio simulado realizado en [18], el autor compara el rendimiento de recuperación de la expansión interactiva de consultas y la expansión automática de consultas con un estudio simulado, y sugiere que los posibles beneficios de la primera pueden ser difíciles de lograr. Se descubre que el usuario no es bueno identificando términos útiles para la expansión de la consulta, cuando una interfaz de presentación de términos simple no puede proporcionar suficiente contexto semántico de los términos de retroalimentación. Nuestro trabajo difiere de los anteriores en dos aspectos importantes. Primero, al elegir los términos para presentar al usuario para la evaluación de relevancia, no solo consideramos el valor de un solo término (por ejemplo, la frecuencia relativa de un término en los documentos principales, que puede medirse mediante métricas como el Valor de Selección de Robertson y la Distancia Kullback-Leibler Simplificada como se detalla en [24]), sino que también examinamos la estructura de los grupos de términos, con el fin de producir una cobertura equilibrada de los diferentes aspectos del tema. Segundo, con el marco de modelado del lenguaje, permitimos una construcción detallada del modelo de consulta actualizado, estableciendo diferentes probabilidades para diferentes términos según si es un término de consulta, su importancia en los documentos principales y su pertenencia a un clúster. Aunque existen técnicas para ajustar los pesos de los términos de consulta en modelos de espacio vectorial y modelos de relevancia probabilística, la mayoría de los trabajos mencionados no las utilizan, optando por simplemente añadir términos de retroalimentación a la consulta original (usando pesos iguales para ellos), lo que puede llevar a un rendimiento de recuperación más pobre. La combinación de los dos aspectos permite que nuestro método funcione mucho mejor que el punto de referencia. La forma habitual de presentar los términos de retroalimentación es simplemente mostrar los términos en una lista. Ha habido algunos trabajos sobre interfaces de usuario alternativas. [8] organiza los términos en una jerarquía, y [11] compara tres interfaces diferentes, incluyendo términos + casillas de verificación, términos + contexto (oraciones) + casillas de verificación, oraciones + cuadro de texto de entrada. En ambos estudios, sin embargo, no hay una diferencia significativa en el rendimiento. En nuestro trabajo adoptamos el enfoque más simple de términos + casillas de verificación. Nos enfocamos en la presentación de términos y la construcción de modelos de consulta a partir de términos de retroalimentación, y creemos que el uso de contextos para mejorar la calidad de los términos de retroalimentación debería ser ortogonal a nuestro método. 3. Enfoque general: Seguimos el enfoque de modelado del lenguaje y basamos nuestro método en el modelo de recuperación de divergencia KL propuesto en [25]. Con este modelo, la tarea de recuperación implica estimar un modelo de lenguaje de consulta θq a partir de una consulta dada, un modelo de lenguaje de documento θd de cada documento, y calcular su divergencia KL D(θq||θd), que luego se utiliza para puntuar los documentos. [25] trata la retroalimentación de relevancia como un problema de reestimación del modelo de consulta, es decir, calcular un modelo de consulta actualizado θq dado el texto de la consulta original y la evidencia adicional proporcionada por los documentos relevantes juzgados. Adoptamos esta perspectiva y planteamos nuestra tarea como la actualización del modelo de consulta a partir de la retroalimentación de términos de usuario. Aquí hay dos tareas clave: Primero, cómo elegir los mejores términos para presentar al usuario para su evaluación, con el fin de recopilar la máxima evidencia sobre la necesidad de información del usuario. Segundo, cómo calcular un modelo de consulta actualizado basado en esta retroalimentación de términos, de manera que capture la necesidad de información de los usuarios y se traduzca en un buen rendimiento de recuperación. 4. SELECCIÓN DE TÉRMINOS DE PRESENTACIÓN La selección adecuada de los términos que se presentarán al usuario para su evaluación es crucial para el éxito de la retroalimentación de términos. Si los términos están mal elegidos y hay pocos relevantes, el usuario tendrá dificultades para buscar términos útiles que ayuden a aclarar su necesidad de información. Si los términos relevantes son abundantes, pero todos se centran en un solo aspecto del tema de la consulta, entonces solo podremos obtener retroalimentación sobre ese aspecto y perderemos otros, lo que resultará en una pérdida de amplitud en los resultados recuperados. Por lo tanto, es importante seleccionar cuidadosamente los términos de presentación para maximizar la ganancia esperada de la retroalimentación del usuario, es decir, aquellos que pueden revelar potencialmente la mayor evidencia de la necesidad de información del usuario. Esto es similar al feedback activo[21], que sugiere que un sistema de recuperación debería sondear activamente la necesidad de información de los usuarios, y en el caso del feedback de relevancia, los documentos de feedback deberían ser elegidos para maximizar los beneficios de aprendizaje (por ejemplo, de manera diversa para aumentar la cobertura). En nuestro enfoque, los N documentos principales de una recuperación inicial utilizando la consulta original forman la fuente de términos de retroalimentación: todos los términos que aparecen en ellos se consideran candidatos para presentar al usuario. Estos documentos sirven como pseudo-retroalimentación, ya que proporcionan un contexto mucho más amplio que la consulta original (generalmente muy breve), sin que se le pida al usuario que juzgue su relevancia. Debido a esta última razón, es posible hacer que N sea bastante grande (por ejemplo, en nuestros experimentos establecimos N = 60) para aumentar su cobertura de diferentes aspectos en el tema. La forma más sencilla de seleccionar términos de retroalimentación es elegir los M términos más frecuentes de los N documentos. Este método, sin embargo, tiene dos inconvenientes. Primero, se seleccionarán muchos términos ruidosos comunes debido a sus altas frecuencias en la colección de documentos, a menos que se utilice una lista de palabras vacías para filtrar. En segundo lugar, la lista de presentación tiende a estar compuesta por términos de los aspectos principales del tema; es probable que se pasen por alto aquellos de un aspecto menor debido a sus frecuencias relativamente bajas. Resolvemos los problemas anteriores mediante dos medidas correspondientes. Primero, introducimos un modelo de fondo θB que se estima a partir de estadísticas de la colección y explica los términos comunes, de modo que es mucho menos probable que aparezcan en la lista de presentación. Segundo, los términos se seleccionan de múltiples grupos en los documentos de retroalimentación simulada, para garantizar una representación suficiente de diferentes aspectos del tema. Nos basamos en el modelo multinomial de mezcla, que se utiliza para el descubrimiento de temas en [26]. Específicamente, asumimos que los N documentos contienen K grupos {Ci| i = 1, 2, · · · K}, cada uno caracterizado por una distribución de palabras multinomial (también conocida como modelo de lenguaje unigrama) θi y correspondiente a un aspecto del tema. Los documentos se consideran como muestreados de una mezcla de K + 1 componentes, incluidos los K grupos y el modelo de fondo: p(w|d) = λBp(w|θB) + (1 − λB) K i=1 πd,ip(w|θi) donde w es una palabra, λB es el peso de la mezcla para el modelo de fondo θB, y πd,i es el peso de la mezcla específico del documento para el modelo de cluster i-ésimo θi. Luego estimamos los modelos de clúster maximizando la probabilidad de que los documentos de pseudo-retroalimentación sean generados a partir del modelo de mezcla multinomial: log p(D|Λ) = d∈D w∈V c(w; d) log p(w|d) donde D = {di| i = 1, 2, · · · N} es el conjunto de los N documentos, V es el vocabulario, c(w; d) es la frecuencia de w en d y Λ = {θi| i = 1, 2, · · · K} ∪ {πdij | i = 1, 2, · · · N, j = 1, 2, · · · K} es el conjunto de parámetros del modelo a estimar. Los modelos de conglomerados pueden ser estimados eficientemente utilizando el algoritmo de Expectation-Maximization (EM). Para más detalles, remitimos al lector a [26]. La Tabla 1 muestra los modelos de clúster para la consulta de TREC sobre desastres en túneles de transporte (K = 3). Ten en cuenta que solo el grupo central es relevante. Tabla 1: Modelos de clúster para el tema 363 Desastres en túneles de transporte Clúster 1 Clúster 2 Clúster 3 túnel 0.0768 túnel 0.0935 túnel 0.0454 transporte 0.0364 fuego 0.0295 transporte 0.0406 tráfico 0.0206 camión 0.0236 peaje 0.0166 ferrocarril 0.0186 francés 0.0220 Amtrak 0.0153 puerto 0.0146 humo 0.0157 tren 0.0129 riel 0.0140 coche 0.0154 aeropuerto 0.0122 puente 0.0139 italiano 0.0152 autopista 0.0105 kilómetro 0.0136 incendio 0.0144 lui 0.0095 camión 0.0133 llamas 0.0127 Jersey 0.0093 construcción 0.0131 blanco 0.0121 paso 0.0087 · · · · · · · · · De cada uno de los K clústeres estimados, elegimos los L = M/K términos con las probabilidades más altas para formar un total de M términos de presentación. Si un término resulta estar en el top L en múltiples grupos, lo asignamos al grupo donde tenga la probabilidad más alta y permitimos que los otros grupos tomen un término adicional como compensación. También filtramos los términos en el texto de la consulta original porque tienden a ser siempre relevantes cuando la consulta es corta. Los términos seleccionados son luego presentados al usuario para su evaluación. Se muestra un formulario de retroalimentación (completado) de muestra en la Figura 1. En este estudio solo tratamos con juicios binarios: un término presentado está por defecto sin marcar, y un usuario puede marcarlo para indicar relevancia. Tampoco explotamos explícitamente la retroalimentación negativa (es decir, penalizar términos irrelevantes), porque con la retroalimentación binaria un término no verificado no es necesariamente irrelevante (quizás el usuario no está seguro de su relevancia). Podríamos pedir al usuario un juicio más detallado (por ejemplo, eligiendo entre altamente relevante, algo relevante, no sé, algo irrelevante y altamente irrelevante), pero la retroalimentación binaria es más compacta, ocupando menos espacio para mostrar y requiriendo menos esfuerzo por parte del usuario para emitir un juicio. 5. ESTIMACIÓN DE MODELOS DE CONSULTA A PARTIR DE RETROALIMENTACIÓN DE TÉRMINOS En esta sección, presentamos varios algoritmos para aprovechar la retroalimentación de términos. Los algoritmos toman como entrada la consulta original q, los grupos {θi} generados por el algoritmo de descubrimiento de temas, el conjunto de términos de retroalimentación T y su juicio de relevancia R, y producen un modelo de lenguaje de consulta actualizado θq que hace el mejor uso de la evidencia de retroalimentación para capturar la necesidad de información de los usuarios. Primero describimos nuestras notaciones: • θq: El modelo de consulta original, derivado solo de los términos de la consulta: p(w|θq) = c(w; q) |q| donde c(w; q) es la cantidad de veces que w aparece en q, y |q| = w∈q c(w; q) es la longitud de la consulta. • θq: El modelo de consulta actualizado que necesitamos estimar a partir de la retroalimentación de términos. • θi (i = 1, 2, . . . K): El modelo de lenguaje unigrama del clúster Ci, estimado utilizando el algoritmo de descubrimiento de temas. • T = {ti,j} (i = 1 . . . K, j = 1 . . . \n\nK, j = 1 . . . El conjunto de términos presentados al usuario para su evaluación. ti,j es el j-ésimo término elegido del grupo Ci. • R = {δw|w ∈ T}: δw es una variable indicadora que es 1 si w es considerado relevante o 0 en caso contrario. 5.1 TFB (Retroalimentación Directa de Términos) Esta es una forma directa de retroalimentación de términos que no implica ninguna estructura secundaria. Asignamos un peso de 1 a los términos considerados relevantes por el usuario, un peso de μ a los términos de la consulta, un peso de cero a los demás términos, y luego aplicamos la normalización: p(w|θq ) = δw + μ c(w; q) w ∈T δw + μ|q| donde w ∈T δw es el número total de términos considerados relevantes. Llamamos a este método TFB (retroalimentación directa de términos). Si dejamos que μ = 1, este enfoque es equivalente a agregar los términos relevantes después de la consulta original, que es lo que hace la expansión estándar de consultas (sin reponderación de términos). Si establecemos μ > 1, estamos poniendo más énfasis en los términos de consulta que en los verificados. Ten en cuenta que el modelo de resultado será más sesgado hacia θq si la consulta original es larga o el feedback del usuario es débil, lo cual tiene sentido, ya que podemos confiar más en la consulta original en ambos casos. Figura 1: Formulario de aclaración completado para el Tema 363 363 desastres en túneles de transporte. Por favor, seleccione todos los términos relevantes para el tema. tráfico ferrocarril puerto puente ferroviario kilómetro construir suizo enlace cruzado hongkonés río proyecto metro camión de bomberos francés humo coche italiano bomberos incendio blanco montaña víctima francés rescate conductor chamonix emerger peaje tren amtrak aeropuerto turnpike jersey pass rome z centro electrón carretera boston velocidad bu enviar 5.2 CFB (Retroalimentación de Clúster) Aquí explotamos la estructura de clúster que jugó un papel importante cuando seleccionamos los términos de presentación. Los grupos representan diferentes aspectos del tema de la consulta, cada uno de los cuales puede ser relevante o no. Si somos capaces de identificar los grupos relevantes, podemos combinarlos para generar un modelo de consulta que sea bueno para descubrir documentos pertenecientes a estos grupos (en lugar de los irrelevantes). Podríamos pedir al usuario que juzgue directamente la relevancia de un grupo después de ver los términos representativos en ese grupo, pero a veces esto sería una tarea difícil para el usuario, quien tendría que adivinar la semántica de un grupo a través de su conjunto de términos, los cuales podrían no estar bien conectados entre sí debido a la falta de contexto. Por lo tanto, proponemos aprender retroalimentación de clúster de forma indirecta, inferir la relevancia de un clúster a través de la relevancia de sus términos de retroalimentación. Dado que cada grupo tiene un número igual de términos presentados al usuario, la medida más simple de la relevancia de un grupo es el número de términos que se consideran relevantes en él. Intuitivamente, cuantos más términos estén marcados como relevantes en un grupo, más cerca estará el grupo del tema de la consulta y más debería participar en la modificación de la consulta. Si combinamos los modelos de clúster utilizando pesos determinados de esta manera y luego interpolamos con el modelo de consulta original, obtenemos la siguiente fórmula para la actualización de la consulta, que llamamos CFB (Retroalimentación de Clúster): p(w|θq) = λp(w|θq) + (1 − λ) K i=1 L j=1 δti,j K k=1 L j=1 δtk,j p(w|θi) donde L j=1 δti,j es el número de términos relevantes en el clúster Ci, y K k=1 L j=1 δtk,j es el número total de términos relevantes. Observamos que cuando solo hay un clúster (K = 1), la fórmula anterior se degrada a p(w|θq ) = λp(w|θq) + (1 − λ)p(w|θ1) que es simplemente una pseudo-retroalimentación de la forma propuesta en [25]. TCFB (Retroalimentación Término-Clúster) TFB y CFB ambos tienen sus inconvenientes. TFB asigna probabilidades no nulas a los términos presentados que están marcados como relevantes, pero ignora por completo (muchos más) otros, que pueden quedar sin marcar debido a la ignorancia de los usuarios, o simplemente no estar incluidos en la lista de presentación, pero deberíamos ser capaces de inferir su relevancia a partir de los marcados. Por ejemplo, en la Figura 1, dado que se revisan hasta 5 términos en el grupo central (las tercera y cuarta columnas), deberíamos tener una alta confianza en la relevancia de otros términos en ese grupo. CFB resuelve el problema de TFB tratando los términos en un grupo de manera colectiva, de modo que los términos no verificados/no presentados reciban pesos cuando los términos presentados en sus grupos son considerados relevantes, pero no distingue qué términos en un grupo son presentados o considerados. Intuitivamente, los términos considerados relevantes deberían recibir pesos mayores porque son indicados explícitamente como relevantes por el usuario. Por lo tanto, intentamos combinar los dos métodos, con la esperanza de obtener lo mejor de ambos. Lo hacemos interpolando el modelo TFB con el modelo CFB, y lo llamamos TCFB: p(w|θq ) = αp(w|θqT F B ) + (1 − α)p(w|θqCF B ) 6. EXPERIMENTOS En esta sección, describimos los resultados de nuestro experimento. Primero describimos nuestra configuración experimental y presentamos una visión general del rendimiento de varios métodos. Luego discutimos los efectos de variar la configuración de los parámetros en los algoritmos, así como el número de términos de presentación. A continuación analizamos el comportamiento de retroalimentación de los usuarios y su relación con el rendimiento de recuperación. Finalmente comparamos la retroalimentación de términos con la retroalimentación de relevancia y mostramos que tiene su ventaja particular. 6.1 Configuración del Experimento y Resultados Básicos Aprovechamos la oportunidad de la pista HARD de TREC 2005[2] para la evaluación de nuestros algoritmos. Las pistas utilizaron la colección AQUAINT, un corpus de 3GB de texto de noticias en inglés. Los temas incluyeron 50 que se sabía que eran difíciles, es decir, con bajo rendimiento de recuperación. Es para estos temas difíciles que la retroalimentación del usuario es más útil, ya que puede proporcionar información para desambiguar las consultas; con temas fáciles, es posible que el usuario no esté dispuesto a esforzarse por dar retroalimentación si los resultados de recuperación automática son lo suficientemente buenos. Los participantes de la pista pudieron enviar formularios de aclaración (CF) diseñados a medida para solicitar retroalimentación de los evaluadores humanos proporcionados por la Tabla 2: Rendimiento de recuperación para diferentes métodos y tipos de CF. La última fila es el porcentaje de mejora del MAP sobre la línea base. Los ajustes de parámetros μ = 4, λ = 0.1, α = 0.3 son casi óptimos. Variación de MAP con el número de términos presentados. # términos TFB1C TFB3C TFB6C CFB3C CFB6C TCFB3C TCFB6C 6 0.245 0.240 0.227 0.279 0.279 0.281 0.274 12 0.261 0.261 0.242 0.299 0.286 0.297 0.281 18 0.275 0.274 0.256 0.301 0.282 0.300 0.286 24 0.276 0.281 0.265 0.303 0.292 0.305 0.292 30 0.280 0.285 0.270 0.304 0.296 0.307 0.296 36 0.282 0.288 0.272 0.307 0.297 0.309 0.297 42 0.283 0.288 0.275 0.306 0.298 0.309 0.300 48 0.288 0.288 0.278 0.305 0.301 0.309 0.303 NIST. Diseñamos tres conjuntos de formularios de aclaración para la retroalimentación de términos, diferenciados en la elección de K, el número de grupos, y L, el número de términos presentados de cada grupo. Son: 1 × 48, un gran grupo con 48 términos, 3 × 16, 3 grupos con 16 términos cada uno, y 6 × 8, 6 grupos con 8 términos cada uno. El número total de términos presentados (M) está fijo en 48, por lo que al comparar el rendimiento de diferentes tipos de formas de clarificación podemos conocer los efectos de diferentes grados de agrupamiento. Para cada tema, un evaluador completaría los formularios en el orden de 6 × 8, 1 × 48 y 3 × 16, dedicando hasta tres minutos en cada formulario. El formulario de aclaración de muestra mostrado en la Figura 1 es de tipo 3 × 16. Es una interfaz simple y compacta en la que el usuario puede consultar términos relevantes. El formulario es autoexplicativo; no es necesario brindar capacitación adicional al usuario sobre cómo utilizarlo. Nuestras consultas iniciales se construyen solo utilizando las descripciones de los títulos de los temas, que tienen en promedio 2.7 palabras de longitud. Como base utilizamos el método de recuperación de divergencia KL implementado en la herramienta Lemur con 5 documentos de retroalimentación pseudo. Aplicamos el suavizado de Dirichlet con una prior de 2000, y truncamos los modelos de lenguaje de consulta a 50 términos (estas configuraciones se utilizan en todos los experimentos). Para todos los demás parámetros, utilizamos la configuración predeterminada de Lemurs. El punto de referencia resulta tener un rendimiento por encima del promedio entre los participantes de la pista. Después de una ejecución inicial utilizando este método de recuperación de referencia, tomamos los 60 documentos principales para cada tema y aplicamos el algoritmo de descubrimiento de temas para producir los grupos (1, 3 o 6 de ellos), en base a los cuales generamos formularios de aclaración. Después de recibir la retroalimentación del usuario, ejecutamos los algoritmos de retroalimentación de términos (TFB, CFB o TCFB) para estimar modelos de consulta actualizados, los cuales luego se utilizan para una segunda iteración de recuperación. Evaluamos el rendimiento de los diferentes métodos de recuperación en sus clasificaciones de los 1000 documentos principales. Las métricas de evaluación que adoptamos incluyen la precisión media promedio (no interpolada) (MAP), precisión en los primeros 30 (Pr@30) y total relevante recuperado (RR). La Tabla 2 muestra el rendimiento de varios métodos y configuraciones de K × L. Los sufijos (1C, 3C, 6C) después de TFB, CFB, TCFB representan el número de grupos (K). Por ejemplo, TCFB3C significa el método TCFB en los formularios de aclaración de 3 × 16. De la Tabla 2 podemos hacer las siguientes observaciones: 1 http://www.lemurproject.com 1. Todos los métodos tienen un rendimiento considerablemente mejor que la línea base de pseudoretroalimentación, con TCFB3C logrando una mejora máxima del 41.1% en el MAP, lo que indica una contribución significativa de la retroalimentación de términos para la clarificación de la necesidad de información de los usuarios. En otras palabras, el feedback a término es realmente útil para mejorar la precisión de recuperación. 2. Para TFB, el rendimiento es casi igual en los formularios de aclaración de 1 × 48 y 3 × 16 en términos de MAP (aunque este último es ligeramente mejor en Pr@30 y RR), y un poco peor en los de 6 × 8. 3. Tanto CFB3C como CFB6C tienen un mejor rendimiento que sus contrapartes TFB en las tres métricas, lo que sugiere que el feedback sobre una estructura de clúster secundaria es realmente beneficioso. CFB1C es en realidad peor porque no puede ajustar el peso de su (único) grupo a partir de la retroalimentación de términos y es simplemente pseudoretroalimentación. 4. Aunque TCFB es solo una simple mezcla de TFB y CFB por interpolación, logra superar a ambos. Esto respalda nuestra especulación de que TCFB supera las desventajas de TFB (prestando atención solo a los términos marcados) y CFB (sin distinguir los términos marcados y no marcados en un grupo). Excepto por TCFB6C v.s. CFB6C, la ventaja de rendimiento de TCFB sobre TFB/CFB es significativa con p < 0.05 utilizando la prueba de rango con signo de Wilcoxon. Esto no es cierto en el caso de TFB v.s. CFB, cada uno de los cuales es mejor que el otro en casi la mitad de los temas. 6.2 Reducción de Términos de Presentación En algunas situaciones podemos tener que reducir el número de términos de presentación debido a limitaciones en el espacio de visualización o esfuerzos de retroalimentación del usuario. Es interesante saber si el rendimiento de nuestros algoritmos se deteriora cuando al usuario se le presentan menos términos. Dado que los términos de presentación dentro de cada grupo se generan en orden decreciente de sus frecuencias, la lista de presentación forma un subconjunto del original si su tamaño se reduce2. Por lo tanto, podemos simular fácilmente lo que sucede cuando el número de términos de presentación disminuye 2. Hay complejidades que surgen de los términos que aparecen en la parte superior L de múltiples grupos, pero estas son excepciones de M a M: mantendremos todos los juicios de los términos superiores L = M / K en cada grupo y descartaremos los de los demás. La Tabla 3 muestra el rendimiento de varios algoritmos a medida que el número de términos de presentación varía de 6 a 48. Observamos que el rendimiento de TFB es más susceptible a la reducción de términos de presentación que el de CFB o TCFB. Por ejemplo, en 12 términos el MAP de TFB3C es el 90.6% del que se obtiene en 48 términos, mientras que los números para CFB3C y TCFB3C son del 98.0% y 96.1% respectivamente. Conjeturamos que la razón es que mientras el rendimiento de TFB depende en gran medida de cuántos términos buenos se elijan para la expansión de la consulta, CFB solo necesita una estimación aproximada de los pesos de los clústeres para funcionar. Además, las formas de aclaración de 3 × 16 parecen ser más robustas que las de 6 × 8: con 12 términos, el MAP de TFB6C es el 87.1% del obtenido con 48 términos, por debajo del 90.6% de TFB3C. De manera similar, para CFB es del 95.0% en comparación con el 98.0%. Esto es natural, ya que para un gran número de 6 clusters, es más fácil llegar a la situación en la que cada cluster recibe muy pocos términos de presentación para que la diversificación de temas sea útil. En general, nos sorprende ver que los algoritmos aún pueden funcionar razonablemente bien cuando el número de términos de presentación es pequeño. Por ejemplo, con solo 12 términos, CFB3C (el formulario de aclaración es de tamaño 3 × 4) aún puede mejorar un 36.5% sobre el valor base, disminuyendo ligeramente desde el 39.3% con 48 términos. En el Análisis de Retroalimentación del Usuario, estudiamos varios aspectos del comportamiento de retroalimentación de los usuarios en términos, y si están conectados al rendimiento de recuperación. Figura 2: Distribuciones de tiempo de completación del formulario de aclaración 0−30 30−60 60−90 90−120 120−150 150−180 0 5 10 15 20 25 30 35 tiempo de completación (segundos) #temas 1×48 3×16 6×8 La Figura 2 muestra la distribución del tiempo necesario para completar un formulario de aclaración. Observamos que el usuario suele ser capaz de completar el feedback del término en un tiempo razonablemente corto: para más de la mitad de los temas, el formulario de aclaración se completa en solo 1 minuto, y solo una pequeña fracción de temas (menos del 10% para 1 × 48 y 3 × 16) tarda más de 2 minutos. Esto sugiere que la retroalimentación a corto plazo es adecuada para la recuperación interactiva ad-hoc, donde un usuario generalmente no desea dedicar demasiado tiempo a proporcionar retroalimentación. Observamos que un usuario a menudo comete errores al juzgar la relevancia de un término. A veces, un término relevante puede ser omitido porque su conexión con el tema de la consulta no es obvia para el usuario. En otras ocasiones, puede incluirse un término dudoso que resulta ser irrelevante. Toma como ejemplo el tema en la Figura 1. Hubo un desastre de incendio en Mont 3. El tiempo máximo es de 180 segundos, ya que el evaluador del NIST estaría obligado a enviar el formulario en ese momento. Tabla 4: Estadísticas de selección de términos (promedio del tema) CF Tipo 1 × 48 3 × 16 6 × 8 # términos revisados 14.8 13.3 11.2 # términos relevantes 15.0 12.6 11.2 # términos relevantes revisados 7.9 6.9 5.9 precisión 0.534 0.519 0.527 recuperación 0.526 0.548 0.527 El usuario no seleccionó palabras clave como mont, blanc, francés e italiano debido a su desconocimiento del evento del Túnel del Mont Blanc entre Francia e Italia en 1999. De hecho, sin un contexto adecuado sería difícil hacer un juicio perfecto. ¿Cuál es entonces, el nivel en el que el usuario es bueno dando retroalimentación? ¿Tiene un impacto serio en el rendimiento de recuperación? Para responder a estas preguntas, necesitamos una medida de la verdadera relevancia de los términos individuales. Adoptamos la métrica de Divergencia KL Simplificada utilizada en [24] para decidir los términos de expansión de la consulta como nuestra medida de relevancia de términos: σKLD(w) = p(w|R) log p(w|R) p(w|¬R) donde p(w|R) es la probabilidad de que un documento relevante contenga el término w, y p(w|¬R) es la probabilidad de que un documento irrelevante contenga w, ambas pueden ser fácilmente calculadas mediante una estimación de máxima verosimilitud dada la relevancia a nivel de documento. Si σKLD(w) > 0, w es más probable que aparezca en documentos relevantes que en irrelevantes. Consideramos un término relevante si su valor de Divergencia KL Simplificada es mayor que un umbral determinado σ0. Podemos entonces definir la precisión y la exhaustividad del juicio de términos del usuario de la siguiente manera: la precisión es la fracción de términos revisados por el usuario que son relevantes; la exhaustividad es la fracción de términos relevantes presentados que son revisados por el usuario. La Tabla 4 muestra el número de términos revisados, términos relevantes y términos relevantes revisados cuando σ0 se establece en 1.0, así como la precisión/recuperación del juicio de términos del usuario. Ten en cuenta que cuando los formularios de aclaración contienen más grupos, se revisan menos términos: 14.8 para 1 × 48, 13.3 para 3 × 16 y 11.2 para 6×8. El patrón similar se mantiene para los términos relevantes y los términos relevantes verificados. Parece haber un compromiso entre aumentar la diversidad de temas mediante el agrupamiento y perder términos adicionales relevantes: cuando hay más grupos, cada uno recibe menos términos para presentar, lo que puede perjudicar a un grupo relevante importante que contiene muchos términos relevantes. Por lo tanto, no siempre es útil tener más grupos, por ejemplo, TFB6C es en realidad peor que TFB1C. El hallazgo principal que podemos obtener de la Tabla 4 es que el usuario no es particularmente bueno identificando términos relevantes, lo cual coincide con el descubrimiento en [18]. En el caso de 3 formularios de aclaración de 16, el número promedio de términos marcados como relevantes por el usuario es de 13.3 por tema, y el número promedio de términos relevantes cuyo valor de σKLD excede 1.0 es de 12.6. El usuario es capaz de reconocer solo 6.9 de estos términos en promedio. De hecho, la precisión y la exhaustividad de los términos de retroalimentación de los usuarios (como se definió anteriormente) están lejos de ser perfectas. Por otro lado, si el usuario hubiera verificado correctamente todos esos términos relevantes, el rendimiento de nuestros algoritmos habría aumentado considerablemente, como se muestra en la Tabla 5. Vemos que TFB experimenta una gran mejora cuando hay un oráculo que verifica todos los términos relevantes, mientras que CFB se encuentra con un cuello de botella alrededor de un MAP de 0.325, ya que todo lo que hace es ajustar los pesos de los clústeres, y cuando los pesos aprendidos están cerca de ser precisos, no puede beneficiarse más del feedback de términos. También hay que tener en cuenta que TCFB no logra superar a TFB, probablemente porque TFB es lo suficientemente preciso. Tabla 5: Cambio de MAP al usar todos (y solo) los términos relevantes (σKLD > 1.0) para retroalimentación. retroalimentación de término original retroalimentación de término relevante TF1 0.288 0.354 TF3 0.288 0.354 TF6 0.278 0.346 CF3 0.305 0.325 CF6 0.301 0.326 TCF3 0.309 0.345 TCF6 0.304 0.341 6.4 Comparación con Retroalimentación de Relevancia Ahora comparamos la retroalimentación de términos con la retroalimentación de relevancia a nivel de documento, en la que al usuario se le presentan los N mejores documentos de una recuperación inicial y se le pide que juzgue su relevancia. El proceso de retroalimentación se simula utilizando la evaluación de relevancia de documentos de NIST. Utilizamos el método de retroalimentación basado en el modelo de mezcla propuesto en [25], con el ruido de mezcla establecido en 0.95 y el coeficiente de retroalimentación establecido en 0.9. La evaluación comparativa del feedback de relevancia frente a otros métodos se complica por el hecho de que algunos documentos ya han sido vistos durante el feedback, por lo que no tiene sentido incluirlos en los resultados de recuperación de la segunda ejecución. Sin embargo, esto no se aplica al término retroalimentación. Por lo tanto, para que sea justo en cuanto a la ganancia de información de los usuarios, si los documentos de retroalimentación son relevantes, deben mantenerse en la parte superior del ranking; si son irrelevantes, deben ser excluidos. Por lo tanto, utilizamos retroalimentación de relevancia para producir un ranking de los 1000 documentos recuperados, pero excluyendo cada documento de retroalimentación, y luego añadimos los documentos relevantes de retroalimentación al principio. La tabla 6 muestra el rendimiento de la retroalimentación de relevancia para diferentes valores de N y lo compara con TCFB3C. Tabla 6: Rendimiento de la retroalimentación de relevancia para diferentes números de documentos de retroalimentación (N). Vemos que el rendimiento de TCFB3C es comparable al de la retroalimentación de relevancia utilizando 5 documentos. Aunque es menos eficiente que cuando hay 10 documentos de retroalimentación en términos de MAP y Pr@30, sí recupera más documentos (4947) al descender por la lista clasificada. Intentamos comparar la calidad de los términos insertados automáticamente en la retroalimentación de relevancia con la de los términos seleccionados manualmente en la retroalimentación de términos. Esto se logra truncando el modelo de consulta modificado por retroalimentación de relevancia a un tamaño igual al número de términos verificados para el mismo tema. Podemos comparar los términos en el modelo truncado con los términos verificados. La Figura 3 muestra la distribución de los puntajes σKLD de los términos. Observamos que el término \"feedback\" tiende a producir términos de expansión de mayor calidad (aquellos con σKLD > 1) en comparación con el \"relevance feedback\" (con 10 documentos de feedback). Esto no contradice el hecho de que este último produce un rendimiento de recuperación más alto. De hecho, cuando usamos el modelo de consulta truncada en lugar del modelo íntegro refinado a partir de la retroalimentación de relevancia, el MAP es solo 0.304. La verdad Figura 3: Comparación de la calidad del término de expansión entre retroalimentación de relevancia (con 10 documentos de retroalimentación) y retroalimentación de términos (con 3 × 16 CFs) −1−0 0−1 1−2 2−3 3−4 4−5 5−6 0 50 100 150 200 250 300 350 σKLD #términos retroalimentación de relevancia retroalimentación de términos es que, aunque hay muchos términos no deseados en el modelo de consulta expandido a partir de los documentos de retroalimentación, también hay más términos relevantes de los que el usuario puede seleccionar de la lista de términos de presentación generados con documentos de pseudo-retroalimentación, y los efectos positivos a menudo superan a los negativos. Estamos interesados en saber en qué circunstancias la retroalimentación basada en términos tiene ventajas sobre la retroalimentación basada en relevancia. Una de esas situaciones es cuando ninguno de los documentos de retroalimentación principales es relevante, lo que hace que la retroalimentación de relevancia sea inútil. Esto no es infrecuente, como uno podría haber pensado: de los 50 temas, hay 13 casos así cuando N = 5, 10 cuando N = 10, y aún 3 cuando N = 20. Cuando esto sucede, solo se puede retroceder al método original de recuperación; se pierde el poder de la retroalimentación de relevancia. Sorprendentemente, en 11 de 13 casos en los que parece imposible el feedback de relevancia, el usuario puede marcar al menos 2 términos relevantes de los 3 × 16 del formulario de aclaración (consideramos que el término t es relevante si σKLD(t) > 1.0). Además, en 10 de ellos TCFB3C supera la línea base de pseudo-retroalimentación, aumentando el MAP de 0.076 a 0.146 en promedio (estos son temas particularmente difíciles). Creemos que hay dos posibles explicaciones para este fenómeno de que la retroalimentación de términos esté activa incluso cuando la retroalimentación de relevancia no funciona: Primero, incluso si ninguno de los primeros N (supongamos que es un número pequeño) documentos son relevantes, aún podemos encontrar documentos relevantes en los primeros 60, que son más inclusivos pero generalmente inalcanzables cuando las personas están realizando retroalimentación de relevancia en la búsqueda interactiva ad-hoc, de donde podemos extraer términos de retroalimentación. Esto es cierto para el tema 367 de piratería, donde los 10 documentos principales de retroalimentación son todos sobre piratería de software, sin embargo, hay documentos entre el 10 y el 60 que tratan sobre piratería en los mares (que es la necesidad de información real), contribuyendo términos como pirata, barco para la selección en el formulario de aclaración. Segundo, para algunos temas, un documento debe cumplir con ciertas condiciones especiales para ser relevante. Los N documentos principales pueden estar relacionados con el tema, pero no obstante ser irrelevantes. En este caso, aún podemos extraer términos útiles de estos documentos, incluso si no califican como relevantes. Por ejemplo, en el tema 639 de compras en línea de consumidores, un documento necesita mencionar qué contribuye al crecimiento de las compras para realmente coincidir con la necesidad de información especificada, por lo tanto, ninguno de los 10 documentos de retroalimentación principales se considera relevante. Sin embargo, los términos de retroalimentación como venta al por menor, comercio son buenos para la expansión de consultas. 7. CONCLUSIONES En este artículo estudiamos el uso de la retroalimentación de términos para la <br>recuperación interactiva de información</br> en el enfoque de modelado del lenguaje. ",
            "candidates": [],
            "error": [
                [
                    "recuperación de información",
                    "recuperación de información",
                    "recuperación de información",
                    "recuperación de información",
                    "recuperación interactiva de información"
                ]
            ]
        },
        "language modeling": {
            "translated_key": "",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Term Feedback for Information Retrieval with Language Models Bin Tan† , Atulya Velivelli‡ , Hui Fang† , ChengXiang Zhai† Dept.",
                "of Computer Science† , Dept. of Electrical and Computer Engineering‡ University of Illinois at Urbana-Champaign bintan@cs.uiuc.edu, velivell@ifp.uiuc.edu, hfang@cs.uiuc.edu, czhai@cs.uiuc.edu ABSTRACT In this paper we study term-based feedback for information retrieval in the <br>language modeling</br> approach.",
                "With term feedback a user directly judges the relevance of individual terms without interaction with feedback documents, taking full control of the query expansion process.",
                "We propose a cluster-based method for selecting terms to present to the user for judgment, as well as effective algorithms for constructing refined query language models from user term feedback.",
                "Our algorithms are shown to bring significant improvement in retrieval accuracy over a non-feedback baseline, and achieve comparable performance to relevance feedback.",
                "They are helpful even when there are no relevant documents in the top.",
                "Categories and Subject Descriptors H.3.3 [Information Search and Retrieval]: Retrieval models General Terms Algorithms 1.",
                "INTRODUCTION In the <br>language modeling</br> approach to information retrieval, feedback is often modeled as estimating an improved query model or relevance model based on a set of feedback documents [25, 13].",
                "This is in line with the traditional way of doing relevance feedback - presenting a user with documents/passages for relevance judgment and then extracting terms from the judged documents or passages to expand the initial query.",
                "It is an indirect way of seeking users assistance for query model construction, in the sense that the refined query model (based on terms) is learned through feedback documents/passages, which are high-level structures of terms.",
                "It has the disadvantage that irrelevant terms, which occur along with relevant ones in the judged content, may be erroneously used for query expansion, causing undesired effects.",
                "For example, for the TREC query Hubble telescope achievements, when a relevant document talks more about the telescopes repair than its discoveries, irrelevant terms such as spacewalk can be added into the modified query.",
                "We can consider a more direct way to involve a user in query model improvement, without an intermediary step of document feedback that can introduce noise.",
                "The idea is to present a (reasonable) number of individual terms to the user and ask him/her to judge the relevance of each term or directly specify their probabilities in the query model.",
                "This strategy has been discussed in [15], but to our knowledge, it has not been seriously studied in existing <br>language modeling</br> literature.",
                "Compared to traditional relevance feedback, this term-based approach to interactive query model refinement has several advantages.",
                "First, the user has better control of the final query model through direct manipulation of terms: he/she can dictate which terms are relevant, irrelevant, and possibly, to what degree.",
                "This avoids the risk of bringing unwanted terms into the query model, although sometimes the user introduces low-quality terms.",
                "Second, because a term takes less time to judge than a documents full text or summary, and as few as around 20 presented terms can bring significant improvement in retrieval performance (as we will show later), term feedback makes it faster to gather user feedback.",
                "This is especially helpful for interactive adhoc search.",
                "Third, sometimes there are no relevant documents in the top N of the initially retrieved results if the topic is hard.",
                "This is often true when N is constrained to be small, which arises from the fact that the user is unwilling to judge too many documents.",
                "In this case, relevance feedback is useless, as no relevant document can be leveraged on, but term feedback is still often helpful, by allowing relevant terms to be picked from irrelevant documents.",
                "During our participation in the TREC 2005 HARD Track and continued study afterward, we explored how to exploit term feedback from the user to construct improved query models for information retrieval in the <br>language modeling</br> approach.",
                "We identified two key subtasks of term-based feedback, i.e., pre-feedback presentation term selection and post-feedback query model construction, with effective algorithms developed for both.",
                "We imposed a secondary cluster structure on terms and found that a cluster view sheds additional insight into the users information need, and provides a good way of utilizing term feedback.",
                "Through experiments we found that term feedback improves significantly over the nonfeedback baseline, even though the user often makes mistakes in relevance judgment.",
                "Among our algorithms, the one with best retrieval performance is TCFB, the combination of TFB, the direct term feedback algorithm, and CFB, the cluster-based feedback algorithm.",
                "We also varied the number of feedback terms and observed reasonable improvement even at low numbers.",
                "Finally, by comparing term feedback with document-level feedback, we found it to be a viable alternative to the latter with competitive retrieval performance.",
                "The rest of the paper is organized as follows.",
                "Section 2 discusses some related work.",
                "Section 4 outlines our general approach to term feedback.",
                "We present our method for presentation term selection in Section 3 and algorithms for query model construction in Section 5.",
                "The experiment results are given in Section 6.",
                "Section 7 concludes this paper. 2.",
                "RELATED WORK Relevance feedback[17, 19] has long been recognized as an effective method for improving retrieval performance.",
                "Normally, the top N documents retrieved using the original query are presented to the user for judgment, after which terms are extracted from the judged relevant documents, weighted by their potential of attracting more relevant documents, and added into the query model.",
                "The expanded query usually represents the users information need better than the original one, which is often just a short keyword query.",
                "A second iteration of retrieval using this modified query usually produces significant increase in retrieval accuracy.",
                "In cases where true relevance judgment is unavailable and all top N documents are assumed to be relevant, it is called blind or pseudo feedback[5, 16] and usually still brings performance improvement.",
                "Because document is a large text unit, when it is used for relevance feedback many irrelevant terms can be introduced into the feedback process.",
                "To overcome this, passage feedback is proposed and shown to improve feedback performance[1, 23].",
                "A more direct solution is to ask the user for their relevance judgment of feedback terms.",
                "For example, in some relevance feedback systems such as [12], there is an interaction step that allows the user to add or remove expansion terms after they are automatically extracted from relevant documents.",
                "This is categorized as interactive query expansion, where the original query is augmented with user-provided terms, which can come from direct user input (free-form text or keywords)[22, 7, 10] or user selection of system-suggested terms (using thesauri[6, 22] or extracted from feedback documents[6, 22, 12, 4, 7]).",
                "In many cases term relevance feedback has been found to effectively improve retrieval performance[6, 22, 12, 4, 10].",
                "For example, the study in [12] shows that the user prefers to have explicit knowledge and direct control of which terms are used for query expansion, and the penetrable interface that provides this freedom is shown to perform better than other interfaces.",
                "However, in some other cases there is no significant benefit[3, 14], even if the user likes interacting with expansion terms.",
                "In a simulated study carried out in [18], the author compares the retrieval performance of interactive query expansion and automatic query expansion with a simulated study, and suggests that the potential benefits of the former can be hard to achieve.",
                "The user is found to be not good at identifying useful terms for query expansion, when a simple term presentation interface is unable to provide sufficient semantic context of the feedback terms.",
                "Our work differs from the previous ones in two important aspects.",
                "First, when we choose terms to present to the user for relevance judgment, we not only consider single-term value (e.g., the relative frequency of a term in the top documents, which can be measured by metrics such as Robertson Selection Value and Simplified Kullback-Leibler Distance as listed in [24]), but also examine the cluster structure of the terms, so as to produce a balanced coverage of the different topic aspects.",
                "Second, with the language modelling framework, we allow an elaborate construction of the updated query model, by setting different probabilities for different terms based on whether it is a query term, its significance in the top documents, and its cluster membership.",
                "Although techniques for adjusting query term weights exist for vector space models[17] and probablistic relevance models[9], most of the aforementioned works do not use them, choosing to just append feedback terms to the original query (thus using equal weights for them), which can lead to poorer retrieval performance.",
                "The combination of the two aspects allows our method to perform much better than the baseline.",
                "The usual way for feedback term presentation is just to display the terms in a list.",
                "There have been some works on alternative user interfaces. [8] arranges terms in a hierarchy, and [11] compares three different interfaces, including terms + checkboxes, terms + context (sentences) + checkboxes, sentences + input text box.",
                "In both studies, however, there is no significant performance difference.",
                "In our work we adopt the simplest approach of terms + checkboxes.",
                "We focus on term presentation and query model construction from feedback terms, and believe using contexts to improve feedback term quality should be orthogonal to our method. 3.",
                "GENERAL APPROACH We follow the <br>language modeling</br> approach, and base our method on the KL-divergence retrieval model proposed in [25].",
                "With this model, the retrieval task involves estimating a query language model θq from a given query, a document language model θd from each document, and calculating their KL-divergence D(θq||θd), which is then used to score the documents. [25] treats relevance feedback as a query model re-estimation problem, i.e., computing an updated query model θq given the original query text and the extra evidence carried by the judged relevant documents.",
                "We adopt this view, and cast our task as updating the query model from user term feedback.",
                "There are two key subtasks here: First, how to choose the best terms to present to the user for judgment, in order to gather maximal evidence about the users information need.",
                "Second, how to compute an updated query model based on this term feedback evidence, so that it captures the users information need and translates into good retrieval performance. 4.",
                "PRESENTATION TERM SELECTION Proper selection of terms to be presented to the user for judgment is crucial to the success of term feedback.",
                "If the terms are poorly chosen and there are few relevant ones, the user will have a hard time looking for useful terms to help clarify his/her information need.",
                "If the relevant terms are plentiful, but all concentrate on a single aspect of the query topic, then we will only be able to get feedback on that aspect and missing others, resulting in a breadth loss in retrieved results.",
                "Therefore, it is important to carefully select presentation terms to maximize expected gain from user feedback, i.e., those that can potentially reveal most evidence of the users information need.",
                "This is similar to active feedback[21], which suggests that a retrieval system should actively probe the users information need, and in the case of relevance feedback, the feedback documents should be chosen to maximize learning benefits (e.g. diversely so as to increase coverage).",
                "In our approach, the top N documents from an initial retrieval using the original query form the source of feedback terms: all terms that appear in them are considered candidates to present to the user.",
                "These documents serve as pseudo-feedback, since they provide a much richer context than the original query (usually very short), while the user is not asked to judge their relevance.",
                "Due to the latter reason, it is possible to make N quite large (e.g., in our experiments we set N = 60) to increase its coverage of different aspects in the topic.",
                "The simplest way of selecting feedback terms is to choose the most frequent M terms from the N documents.",
                "This method, however, has two drawbacks.",
                "First, a lot of common noisy terms will be selected due to their high frequencies in the document collection, unless a stop-word list is used for filtering.",
                "Second, the presentation list will tend to be filled by terms from major aspects of the topic; those from a minor aspect are likely to be missed due to their relatively low frequencies.",
                "We solve the above problems by two corresponding measures.",
                "First, we introduce a background model θB that is estimated from collection statistics and explains the common terms, so that they are much less likely to appear in the presentation list.",
                "Second, the terms are selected from multiple clusters in the pseudo-feedback documents, to ensure sufficient representation of different aspects of the topic.",
                "We rely on the mixture multinomial model, which is used for theme discovery in [26].",
                "Specifically, we assume the N documents contain K clusters {Ci| i = 1, 2, · · · K}, each characterized by a multinomial word distribution (also known as unigram language model) θi and corresponding to an aspect of the topic.",
                "The documents are regarded as sampled from a mixture of K + 1 components, including the K clusters and the background model: p(w|d) = λBp(w|θB) + (1 − λB) K i=1 πd,ip(w|θi) where w is a word, λB is the mixture weight for the background model θB, and πd,i is the document-specific mixture weight for the i-th cluster model θi.",
                "We then estimate the cluster models by maximizing the probability of the pseudo-feedback documents being generated from the multinomial mixture model: log p(D|Λ) = d∈D w∈V c(w; d) log p(w|d) where D = {di| i = 1, 2, · · · N} is the set of the N documents, V is the vocabulary, c(w; d) is ws frequency in d and Λ = {θi| i = 1, 2, · · · K} ∪ {πdij | i = 1, 2, · · · N, j = 1, 2, · · · K} is the set of model parameters to estimate.",
                "The cluster models can be efficiently estimated using the Expectation-Maximization (EM) algorithm.",
                "For its details, we refer the reader to [26].",
                "Table 1 shows the cluster models for TREC query Transportation tunnel disasters (K = 3).",
                "Note that only the middle cluster is relevant.",
                "Table 1: Cluster models for topic 363 Transportation tunnel disasters Cluster 1 Cluster 2 Cluster 3 tunnel 0.0768 tunnel 0.0935 tunnel 0.0454 transport 0.0364 fire 0.0295 transport 0.0406 traffic 0.0206 truck 0.0236 toll 0.0166 railwai 0.0186 french 0.0220 amtrak 0.0153 harbor 0.0146 smoke 0.0157 train 0.0129 rail 0.0140 car 0.0154 airport 0.0122 bridg 0.0139 italian 0.0152 turnpik 0.0105 kilomet 0.0136 firefight 0.0144 lui 0.0095 truck 0.0133 blaze 0.0127 jersei 0.0093 construct 0.0131 blanc 0.0121 pass 0.0087 · · · · · · · · · From each of the K estimated clusters, we choose the L = M/K terms with highest probabilities to form a total of M presentation terms.",
                "If a term happens to be in top L in multiple clusters, we assign it to the cluster where it has highest probability and let the other clusters take one more term as compensation.",
                "We also filter out terms in the original query text because they tend to always be relevant when the query is short.",
                "The selected terms are then presented to the user for judgment.",
                "A sample (completed) feedback form is shown in Figure 1.",
                "In this study we only deal with binary judgment: a presented term is by default unchecked, and a user may check it to indicate relevance.",
                "We also do not explicitly exploit negative feedback (i.e., penalizing irrelevant terms), because with binary feedback an unchecked term is not necessarily irrelevant (maybe the user is unsure about its relevance).",
                "We could ask the user for finer judgment (e.g., choosing from highly relevant, somewhat relevant, do not know, somewhat irrelevant and highly irrelevant), but binary feedback is more compact, taking less space to display and less user effort to make judgment. 5.",
                "ESTIMATING QUERY MODELS FROM TERM FEEDBACK In this section, we present several algorithms for exploiting term feedback.",
                "The algorithms take as input the original query q, the clusters {θi} as generated by the theme discovery algorithm, the set of feedback terms T and their relevance judgment R, and outputs an updated query language model θq that makes best use of the feedback evidence to capture the users information need.",
                "First we describe our notations: • θq: The original query model, derived from query terms only: p(w|θq) = c(w; q) |q| where c(w; q) is the count of w in q, and |q| = w∈q c(w; q) is the query length. • θq : The updated query model which we need to estimate from term feedback. • θi (i = 1, 2, . . .",
                "K): The unigram language model of cluster Ci, as estimated using the theme discovery algorithm. • T = {ti,j} (i = 1 . . .",
                "K, j = 1 . . .",
                "L): The set of terms presented to the user for judgment. ti,j is the j-th term chosen from cluster Ci. • R = {δw|w ∈ T}: δw is an indicator variable that is 1 if w is judged relevant or 0 otherwise. 5.1 TFB (Direct Term Feedback) This is a straight-forward form of term feedback that does not involve any secondary structure.",
                "We give a weight of 1 to terms judged relevant by the user, a weight of μ to query terms, zero weight to other terms, and then apply normalization: p(w|θq ) = δw + μ c(w; q) w ∈T δw + μ|q| where w ∈T δw is the total number of terms that are judged relevant.",
                "We call this method TFB (direct Term FeedBack).",
                "If we let μ = 1, this approach is equivalent to appending the relevant terms after the original query, which is what standard query expansion (without term reweighting) does.",
                "If we set μ > 1, we are putting more emphasis on the query terms than the checked ones.",
                "Note that the result model will be more biased toward θq if the original query is long or the user feedback is weak, which makes sense, as we can trust more on the original query in either case.",
                "Figure 1: Filled clarification form for Topic 363 363 transportation tunnel disasters Please select all terms that are relevant to the topic. traffic railway harbor rail bridge kilometer construct swiss cross link kong hong river project meter shanghai fire truck french smoke car italian firefights blaze blanc mont victim franc rescue driver chamonix emerge toll amtrak train airport turnpike lui jersey pass rome z center electron road boston speed bu submit 5.2 CFB (Cluster Feedback) Here we exploit the cluster structure that played an important role when we selected the presentation terms.",
                "The clusters represent different aspects of the query topic, each of which may or may not be relevant.",
                "If we are able to identify the relevant clusters, we can combine them to generate a query model that is good at discovering documents belonging to these clusters (instead of the irrelevant ones).",
                "We could ask the user to directly judge the relevance of a cluster after viewing representative terms in that cluster, but this would sometimes be a difficult task for the user, who has to guess the semantics of a cluster via its set of terms, which may not be well connected to one another due to a lack of context.",
                "Therefore, we propose to learn cluster feedback indirectly, inferring the relevance of a cluster through the relevance of its feedback terms.",
                "Because each cluster has an equal number of terms presented to the user, the simplest measure of a clusters relevance is the number of terms that are judged relevant in it.",
                "Intuitively, the more terms are marked relevant in a cluster, the closer the cluster is to the query topic, and the more the cluster should participate in query modification.",
                "If we combine the cluster models using weights determined this way and then interpolate with the original query model, we get the following formula for query updating, which we call CFB (Cluster FeedBack): p(w|θq ) = λp(w|θq) + (1 − λ) K i=1 L j=1 δti,j K k=1 L j=1 δtk,j p(w|θi) where L j=1 δti,j is the number of relevant terms in cluster Ci, and K k=1 L j=1 δtk,j is the total number of relevant terms.",
                "We note that when there is only one cluster (K = 1), the above formula degenerates to p(w|θq ) = λp(w|θq) + (1 − λ)p(w|θ1) which is merely pseudo-feedback of the form proposed in [25]. 5.3 TCFB (Term-cluster Feedback) TFB and CFB both have their drawbacks.",
                "TFB assigns non-zero probabilities to the presented terms that are marked relevant, but completely ignores (a lot more) others, which may be left unchecked due to the users ignorance, or simply not included in the presentation list, but we should be able to infer their relevance from the checked ones.",
                "For example, in Figure 1, since as many as 5 terms in the middle cluster (the third and fourth columns) are checked, we should have high confidence in the relevance of other terms in that cluster.",
                "CFB remedies TFBs problem by treating the terms in a cluster collectively, so that unchecked/unpresented terms receive weights when presented terms in their clusters are judged as relevant, but it does not distinguish which terms in a cluster are presented or judged.",
                "Intuitively, the judged relevant terms should receive larger weights because they are explicitly indicated as relevant by the user.",
                "Therefore, we try to combine the two methods, hoping to get the best out of both.",
                "We do this by interpolating the TFB model with the CFB model, and call it TCFB: p(w|θq ) = αp(w|θqT F B ) + (1 − α)p(w|θqCF B ) 6.",
                "EXPERIMENTS In this section, we describe our experiment results.",
                "We first describe our experiment setup and present an overview of various methods performance.",
                "Then we discuss the effects of varying the parameter setting in the algorithms, as well as the number of presentation terms.",
                "Next we analyze user term feedback behavior and its relation to retrieval performance.",
                "Finally we compare term feedback to relevance feedback and show that it has its particular advantage. 6.1 Experiment Setup and Basic Results We took the opportunity of TREC 2005 HARD Track[2] for the evaluation of our algorithms.",
                "The tracks used the AQUAINT collection, a 3GB corpus of English newswire text.",
                "The topics included 50 ones previously known to be hard, i.e. with low retrieval performance.",
                "It is for these hard topics that user feedback is most helpful, as it can provide information to disambiguate the queries; with easy topics the user may be unwilling to spend efforts for feedback if the automatic retrieval results are good enough.",
                "Participants of the track were able to submit custom-designed clarification forms (CF) to solicit feedback from human assessors provided by Table 2: Retrieval performance for different methods and CF types.",
                "The last row is the percentage of MAP improvement over the baseline.",
                "The parameter settings μ = 4, λ = 0.1, α = 0.3 are near optimal.",
                "Baseline TFB1C TFB3C TFB6C CFB1C CFB3C CFB6C TCFB1C TCFB3C TCFB6C MAP 0.219 0.288 0.288 0.278 0.254 0.305 0.301 0.274 0.309 0.304 Pr@30 0.393 0.467 0.475 0.457 0.399 0.480 0.473 0.431 0.491 0.473 RR 4339 4753 4762 4740 4600 4907 4872 4767 4947 4906 % 0% 31.5% 31.5% 26.9% 16.0% 39.3% 37.4% 25.1% 41.1% 38.8% Table 3: MAP variation with the number of presented terms. # terms TFB1C TFB3C TFB6C CFB3C CFB6C TCFB3C TCFB6C 6 0.245 0.240 0.227 0.279 0.279 0.281 0.274 12 0.261 0.261 0.242 0.299 0.286 0.297 0.281 18 0.275 0.274 0.256 0.301 0.282 0.300 0.286 24 0.276 0.281 0.265 0.303 0.292 0.305 0.292 30 0.280 0.285 0.270 0.304 0.296 0.307 0.296 36 0.282 0.288 0.272 0.307 0.297 0.309 0.297 42 0.283 0.288 0.275 0.306 0.298 0.309 0.300 48 0.288 0.288 0.278 0.305 0.301 0.309 0.303 NIST.",
                "We designed three sets of clarification forms for term feedback, differing in the choice of K, the number of clusters, and L, the number of presented terms from each cluster.",
                "They are: 1× 48, a big cluster with 48 terms, 3 × 16, 3 clusters with 16 terms each, and 6 × 8, 6 clusters with 8 terms each.",
                "The total number of presented terms (M) is fixed at 48, so by comparing the performance of different types of clarification forms we can know the effects of different degree of clustering.",
                "For each topic, an assessor would complete the forms ordered by 6 × 8, 1 × 48 and 3 × 16, spending up to three minutes on each form.",
                "The sample clarification form shown in Figure 1 is of type 3 × 16.",
                "It is a simple and compact interface in which the user can check relevant terms.",
                "The form is self-explanatory; there is no need for extra user training on how to use it.",
                "Our initinal queries are constructed only using the topic title descriptions, which are on average 2.7 words in length.",
                "As our baseline we use the KL divergence retrieval method implemented in the Lemur Toolkit1 with 5 pseudo-feedback documents.",
                "We stem the terms, choose Dirichlet smoothing with a prior of 2000, and truncate query language models to 50 terms (these settings are used throughout the experiments).",
                "For all other parameters we use Lemurs default settings.",
                "The baseline turns out to perform above average among the track participants.",
                "After an initial run using this baseline retrieval method, we take the top 60 documents for each topic and apply the theme discovery algorithm to output the clusters (1, 3, or 6 of them), based on which we generate clarification forms.",
                "After user feedback is received, we run the term feedback algorithms (TFB, CFB or TCFB) to estimate updated query models, which are then used for a second iteration of retrieval.",
                "We evaluate the different retrieval methods performance on their rankings of the top 1000 documents.",
                "The evaluation metrics we adopt include mean average (non-interpolated) precision (MAP), precision at top 30 (Pr@30) and total relevant retrieved (RR).",
                "Table 2 shows the performance of various methods and configurations of K × L. The suffixes (1C, 3C, 6C) after TFB,CFB,TCFB stand for the number of clusters (K).",
                "For example, TCFB3C means the TCFB method on the 3 × 16 clarification forms.",
                "From Table 2 we can make the following observations: 1 http://www.lemurproject.com 1.",
                "All methods perform considerably better than the pseudofeedback baseline, with TCFB3C achieving a highest 41.1% improvement in MAP, indicating significant contribution of term feedback for clarification of the users information need.",
                "In other words, term feedback is truly helpful for improving retrieval accuracy. 2.",
                "For TFB, the performance is almost equal on the 1 × 48 and 3 × 16 clarification forms in terms of MAP (although the latter is slightly better in Pr@30 and RR), and a little worse on the 6 × 8 ones. 3.",
                "Both CFB3C and CFB6C perform better than their TFB counterparts in all three metrics, suggesting that feedback on a secondary cluster structure is indeed beneficial.",
                "CFB1C is actually worse because it cannot adjust the weight of its (single) cluster from term feedback and it is merely pseudofeedback. 4.",
                "Although TCFB is just a simple mixture of TFB and CFB by interpolation, it is able to outperform both.",
                "This supports our speculation that TCFB overcomes the drawbacks of TFB (paying attention only to checked terms) and CFB (not distinguishing checked and unchecked terms in a cluster).",
                "Except for TCFB6C v.s.",
                "CFB6C, the performance advantage of TCFB over TFB/CFB is significant at p < 0.05 using the Wilcoxon signed rank test.",
                "This is not true in the case of TFB v.s.",
                "CFB, each of which is better than the other in nearly half of the topics. 6.2 Reduction of Presentation Terms In some situations we may have to reduce the number of presentation terms due to limits in display space or user feedback efforts.",
                "It is interesting to know whether our algorithms performance deteriorates when the user is presented with fewer terms.",
                "Because the presentation terms within each cluster are generated in decreasing order of their frequencies, the presentation list forms a subset of the original one if its size is reduced2 .",
                "Therefore, we can easily simulate what happens when the number of presentation terms decreases 2 There are complexities arising from terms appearing in top L of multiple clusters, but these are exceptions from M to M : we will keep all judgments of the top L = M /K terms in each cluster and discard those of others.",
                "Table 3 shows the performance of various algorithms as the number of presentation terms ranges from 6 to 48.",
                "We find that the performance of TFB is more susceptible to presentation term reduction than that of CFB or TCFB.",
                "For example, at 12 terms the MAP of TFB3C is 90.6% of that at 48 terms, while the numbers for CFB3C and TCFB3C are 98.0% and 96.1% respectively.",
                "We conjecture the reason to be that while TFBs performance heavily depends on how many good terms are chosen for query expansion, CFB only needs a rough estimate of cluster weights to work.",
                "Also, the 3 × 16 clarification forms seem to be more robust than the 6 × 8 ones: at 12 terms the MAP of TFB6C is 87.1% of that at 48 terms, lower than 90.6% for TFB3C.",
                "Similarly, for CFB it is 95.0% against 98.0%.",
                "This is natual, as for a large cluster number of 6, it is easier to get into the situation where each cluster gets too few presentation terms to make topic diversification useful.",
                "Overall, we are surprised to see that the algorithms are still able to perform reasonably well when the number of presentation terms is small.",
                "For example, at only 12 terms CFB3C (the clarification form is of size 3 × 4) can still improve 36.5% over the baseline, dropping slightly from 39.3% at 48 terms. 6.3 User Feedback Analysis In this part we study several aspects of users term feedback behavior, and whether they are connected to retrieval performance.",
                "Figure 2: Clarification form completion time distributions 0−30 30−60 60−90 90−120 120−150 150−180 0 5 10 15 20 25 30 35 completion time (seconds) #topics 1×48 3×16 6×8 Figure 2 shows the distribution of time needed to complete a clarification form3 .",
                "We see that the user is usually able to finish term feedback within a reasonably short amount of time: for more than half of the topics the clarification form is completed in just 1 minute, and only a small fraction of topics (less than 10% for 1 × 48 and 3 × 16) take more than 2 minutes.",
                "This suggests that term feedback is suitable for interactive ad-hoc retrieval, where a user usually does not want to spend too much time on providing feedback.",
                "We find that a user often makes mistakes when judging term relevance.",
                "Sometimes a relevant term may be left out because its connection to the query topic is not obvious to the user.",
                "Other times a dubious term may be included but turns out to be irrelevant.",
                "Take the topic in Figure 1 for example.",
                "There was a fire disaster in Mont 3 The maximal time is 180 seconds, as the NIST assessor would be forced to submit the form at that moment.",
                "Table 4: Term selection statistics (topic average) CF Type 1 × 48 3 × 16 6 × 8 # checked terms 14.8 13.3 11.2 # rel. terms 15.0 12.6 11.2 # rel. checked terms 7.9 6.9 5.9 precision 0.534 0.519 0.527 recall 0.526 0.548 0.527 Blanc Tunnel between France and Italy in 1999, but the user failed to select such keywords as mont, blanc, french and italian due to his/her ignorance of the event.",
                "Indeed, without proper context it would be hard to make perfect judgment.",
                "What is then, the extent to which the user is good at term feedback?",
                "Does it have serious impact on retrieval performance?",
                "To answer these questions, we need a measure of individual terms true relevance.",
                "We adopt the Simplified KL Divergence metric used in [24] to decide query expansion terms as our term relevance measure: σKLD(w) = p(w|R) log p(w|R) p(w|¬R) where p(w|R) is the probability that a relevant document contains term w, and p(w|¬R) is the probability that an irrelevant document contains w, both of which can be easily computed via maximum likelihood estimate given document-level relevance judgment.",
                "If σKLD(w) > 0, w is more likely to appear in relevant documents than irrelevant ones.",
                "We consider a term relevant if its Simplified KL Divergence value is greater than a certain threshold σ0.",
                "We can then define precision and recall of user term judgment accordingly: precision is the fraction of terms checked by the user that are relevant; recall is the fraction of presented relevant terms that are checked by the user.",
                "Table 4 shows the number of checked terms, relevant terms and relevant checked terms when σ0 is set to 1.0, as well as the precision/recall of user term judgment.",
                "Note that when the clarification forms contain more clusters, fewer terms are checked: 14.8 for 1 × 48, 13.3 for 3 × 16 and 11.2 for 6×8.",
                "Similar pattern holds for relevant terms and relevant checked terms.",
                "There seems to be a trade-off between increasing topic diversity by clustering and losing extra relevant terms: when there are more clusters, each of them gets fewer terms to present, which can hurt a major relevant cluster that contains many relevant terms.",
                "Therefore, it is not always helpful to have more clusters, e.g., TFB6C is actually worse than TFB1C.",
                "The major finding we can make from Table 4 is that the user is not particularly good at identifying relevant terms, which echoes the discovery in [18].",
                "In the case of 3 × 16 clarification forms, the average number of terms checked as relevant by the user is 13.3 per topic, and the average number of relevant terms whose σKLD value exceed 1.0 is 12.6.",
                "The user is able to recognize only 6.9 of these terms on average.",
                "Indeed, the precision and recall of user feedback terms (as defined previously) are far from perfect.",
                "On the other hand, If the user had correctly checked all such relevant terms, the performance of our algorithms would have increased a lot, as shown in Table 5.",
                "We see that TFB gets big improvement when there is an oracle who checks all relevant terms, while CFB meets a bottleneck around MAP of 0.325, since all it does is adjust cluster weights, and when the learned weights are close to being accurate, it cannot benefit more from term feedback.",
                "Also note that TCFB fails to outperform TFB, probably because TFB is sufficiently accurate.",
                "Table 5: Change of MAP when using all (and only) relevant terms (σKLD > 1.0) for feedback. original term feedback relevant term feedback TF1 0.288 0.354 TF3 0.288 0.354 TF6 0.278 0.346 CF3 0.305 0.325 CF6 0.301 0.326 TCF3 0.309 0.345 TCF6 0.304 0.341 6.4 Comparison with Relevance Feedback Now we compare term feedback with document-level relevance feedback, in which the user is presented with the top N documents from an initial retrieval and asked to judge their relevance.",
                "The feedback process is simulated using document relevance judgment from NIST.",
                "We use the mixture model based feedback method proposed in [25], with mixture noise set to 0.95 and feedback coefficient set to 0.9.",
                "Comparative evaluation of relevance feedback against other methods is complicated by the fact that some documents have already been viewed during feedback, so it makes no sense to include them in the retrieval results of the second run.",
                "However, this does not hold for term feedback.",
                "Thus, to make it fair w.r.t. users information gain, if the feedback documents are relevant, they should be kept in the top of the ranking; if they are irrelevant, they should be left out.",
                "Therefore, we use relevance feedback to produce a ranking of top 1000 retrieved documents but with every feedback document excluded, and then prepend the relevant feedback documents at the front.",
                "Table 6 shows the performance of relevance feedback for different values of N and compares it with TCFB3C.",
                "Table 6: Performance of relevance feedback for different number of feedback documents (N).",
                "N MAP Pr@30 RR 5 0.302 0.586 4779 10 0.345 0.670 4916 20 0.389 0.772 5004 TCFB3C 0.309 0.491 4947 We see that the performance of TCFB3C is comparable to that of relevance feedback using 5 documents.",
                "Although it is poorer than when there are 10 feedback documents in terms of MAP and Pr@30, it does retrieve more documents (4947) when going down the ranked list.",
                "We try to compare the quality of automatically inserted terms in relevance feedback with that of manually selected terms in term feedback.",
                "This is done by truncating the relevance feedback modified query model to a size equal to the number of checked terms for the same topic.",
                "We can then compare the terms in the truncated model with the checked terms.",
                "Figure 3 shows the distribution of the terms σKLD scores.",
                "We find that term feedback tends to produce expansion terms of higher quality(those with σKLD > 1) compared to relevance feedback (with 10 feedback documents).",
                "This does not contradict the fact that the latter yields higher retrieval performance.",
                "Actually, when we use the truncated query model instead of the intact one refined from relevance feedback, the MAP is only 0.304.",
                "The truth Figure 3: Comparison of expansion term quality between relevance feedback (with 10 feedback documents) and term feedback (with 3 × 16 CFs) −1−0 0−1 1−2 2−3 3−4 4−5 5−6 0 50 100 150 200 250 300 350 σKLD #terms relevance feedback term feedback is, although there are many unwanted terms in the expanded query model from feedback documents, there are also more relevant terms than what the user can possibly select from the list of presentation terms generated with pseudo-feedback documents, and the positive effects often outweights the negative ones.",
                "We are interested to know under what circumstances term feedback has advantage over relevance feedback.",
                "One such situation is when none of the top N feedback documents is relevant, rendering relevance feedback useless.",
                "This is not infrequent, as one might have thought: out of the 50 topics, there are 13 such cases when N = 5, 10 when N = 10, and still 3 when N = 20.",
                "When this happens, one can only back off to the original retrieval method; the power of relevance feedback is lost.",
                "Surprisingly, in 11 out of 13 such cases where relevance feedback seems impossible, the user is able to check at least 2 relevant terms from the 3 × 16 clarification form (we consider term t to be relevant if σKLD(t) > 1.0).",
                "Furthermore, in 10 out of them TCFB3C outperforms the pseudo-feedback baseline, increasing MAP from 0.076 to 0.146 on average (these are particularly hard topics).",
                "We think that there are two possible explanations for this phenomenon of term feedback being active even when relevance feedback does not work: First, even if none of the top N (suppose it is a small number) documents are relevant, we may still find relevant documents in top 60, which is more inclusive but usually unreachable when people are doing relevance feedback in interactive ad-hoc search, from which we can draw feedback terms.",
                "This is true for topic 367 piracy, where the top 10 feedback documents are all about software piracy, yet there are documents between 10-60 that are about piracy on the seas (which is about the real information need), contributing terms such as pirate, ship for selection in the clarification form.",
                "Second, for some topics, a document needs to meet some special condition in order to be relevant.",
                "The top N documents may be related to the topic, but nonetheless irrelevant.",
                "In this case, we may still extract useful terms from these documents, even if they do not qualify as relevant ones.",
                "For example, in topic 639 consumer online shopping, a document needs to mention what contributes to shopping growth to really match the specified information need, hence none of the top 10 feedback documents are regarded as relevant.",
                "But nevertheless, the feedback terms such as retail, commerce are good for query expansion. 7.",
                "CONCLUSIONS In this paper we studied the use of term feedback for interactive information retrieval in the <br>language modeling</br> approach.",
                "We proposed a cluster-based method for selecting presentation terms as well as algorithms to estimate refined query models from user term feedback.",
                "We saw significant improvement in retrieval accuracy brought by term feedback, in spite of the fact that a user often makes mistakes in relevance judgment that hurts its performance.",
                "We found the best-performing algorithm to be TCFB, which benefits from the combination of directly observed term evidence with TFB and indirectly learned cluster relevance with CFB.",
                "When we reduced the number of presentation terms, term feedback is still able to keep much of its performance gain over the baseline.",
                "Finally, we compared term feedback to document-level relevance feedback, and found that TCFB3Cs performance is on a par with the latter with 5 feedback documents.",
                "We regarded term feedback as a viable alternative to traditional relevance feedback, especially when there are no relevant documents in the top.",
                "We propose to extend our work in several ways.",
                "First, we want to study whether the use of various contexts can help the user to better identify term relevance, while not sacrificing the simplicity and compactness of term feedback.",
                "Second, currently all terms are presented to the user in a single batch.",
                "We could instead consider iterative term feedback, by presenting a small number of terms first, and show more terms after receiving user feedback or stop when the refined query is good enough.",
                "The presented terms should be selected dynamically to maximize learning benefits at any moment.",
                "Third, we have plans to incorporate term feedback into our UCAIR toolbar[20], an Internet Explorer plugin, to make it work for web search.",
                "We are also interested in studying how to combine term feedback with relevance feedback or implicit feedback.",
                "We could, for example, allow the user to dynamically modify terms in a language model learned from feedback documents. 8.",
                "ACKNOWLEDGMENT This work is supported in part by the National Science Foundation grants IIS-0347933 and IIS-0428472. 9.",
                "REFERENCES [1] J. Allan.",
                "Relevance feedback with too much data.",
                "In Proceedings of the 18th annual international ACM SIGIR conference on research and development in information retrieval, pages 337-343, 1995. [2] J. Allan.",
                "HARD track overview in TREC 2005 - High Accuracy Retrieval from Documents.",
                "In The Fourteenth Text REtrieval Conference, 2005. [3] P. Anick.",
                "Using terminological feedback for web search refinement: a log-based study.",
                "In Proceedings of the 26th annual international ACM SIGIR conference on research and development in informaion retrieval, pages 88-95, 2003. [4] P. G. Anick and S. Tipirneni.",
                "The paraphrase search assistant: terminological feedback for iterative information seeking.",
                "In Proceedings of the 22nd annual international ACM SIGIR conference on research and development in information retrieval, pages 153-159, 1999. [5] C. Buckley, G. Salton, J. Allan, and A. Singhal.",
                "Automatic query expansion using SMART.",
                "In Proceedings of the Third Text REtrieval Conference, 1994. [6] D. Harman.",
                "Towards interactive query expansion.",
                "In Proceedings of the 11th annual international ACM SIGIR conference on research and development in information retrieval, pages 321-331, 1988. [7] N. A. Jaleel, A. Corrada-Emmanuel, Q. Li, X. Liu, C. Wade, and J. Allan.",
                "UMass at TREC 2003: HARD and QA.",
                "In TREC, pages 715-725, 2003. [8] H. Joho, C. Coverson, M. Sanderson, and M. Beaulieu.",
                "Hierarchical presentation of expansion terms.",
                "In Proceedings of the 2002 ACM symposium on applied computing, pages 645-649, 2002. [9] K. S. Jones, S. Walker, and S. E. Robertson.",
                "A probabilistic model of information retrieval: development and status.",
                "Technical Report 446, Computer Laboratory, University of Cambridge, 1998. [10] D. Kelly, V. D. Dollu, and X. Fu.",
                "The loquacious user: a document-independent source of terms for query expansion.",
                "In Proceedings of the 28th annual international ACM SIGIR conference on research and development in information retrieval, pages 457-464, 2005. [11] D. Kelly and X. Fu.",
                "Elicitation of term relevance feedback: an investigation of term source and context.",
                "In Proceedings of the 29th annual international ACM SIGIR conference on research and development in information retrieval, 2006. [12] J. Koenemann and N. Belkin.",
                "A case for interaction: A study of interactive information retrieval behavior and effectiveness.",
                "In Proceedings of the SIGCHI conference on human factors in computing systems, pages 205-212, 1996. [13] V. Lavrenko and W. B. Croft.",
                "Relevance-based language models.",
                "In Research and Development in Information Retrieval, pages 120-127, 2001. [14] Y. Nemeth, B. Shapira, and M. Taeib-Maimon.",
                "Evaluation of the real and perceived value of automatic and interactive query expansion.",
                "In Proceedings of the 27th annual international ACM SIGIR conference on research and development in information retrieval, pages 526-527, 2004. [15] J. Ponte.",
                "A <br>language modeling</br> Approach to Information Retrieval.",
                "PhD thesis, University of Massachusetts at Amherst, 1998. [16] S. E. Robertson, S. Walker, S. Jones, M. Beaulieu, and M. Gatford.",
                "Okapi at TREC-3.",
                "In Proceedings of the Third Text REtrieval Conference, 1994. [17] J. Rocchio.",
                "Relevance feedback in information retrieval.",
                "In The SMART retrieval system, pages 313-323. 1971. [18] I. Ruthven.",
                "Re-examining the potential effectiveness of interactive query expansion.",
                "In Proceedings of the 26th annual international ACM SIGIR conference on research and development in informaion retrieval, pages 213-220, 2003. [19] G. Salton and C. Buckley.",
                "Improving retrieval performance by relevance feedback.",
                "Journal of the American Society for Information Science, 41:288-297, 1990. [20] X. Shen, B. Tan, and C. Zhai.",
                "Implicit user modeling for personalized search.",
                "In Proceedings of the 14th ACM international conference on information and knowledge management, pages 824-831, 2005. [21] X. Shen and C. Zhai.",
                "Active feedback in ad-hoc information retrieval.",
                "In Proceedings of the 28th annual international ACM SIGIR conference on research and development in information retrieval, pages 59-66, 2005. [22] A. Spink.",
                "Term relevance feedback and query expansion: relation to design.",
                "In Proceedings of the 17th annual international ACM SIGIR conference on research and development in information retrieval, pages 81-90, 1994. [23] J. Xu and W. B. Croft.",
                "Query expansion using local and global document analysis.",
                "In Proceedings of the 19th annual international ACM SIGIR conference on research and development in information retrieval, pages 4-11, 1996. [24] H. Zaragoza, N. Craswell, M. Taylor, S. Saria, and S. Robertson.",
                "Microsoft cambridge at TREC-13: Web and HARD tracks.",
                "In Proceedings of the 13th Text REtrieval Conference, 2004. [25] C. Zhai and J. Lafferty.",
                "Model-based feedback in the <br>language modeling</br> approach to information retrieval.",
                "In Proceedings of the tenth international conference on information and knowledge management, pages 403-410, 2001. [26] C. Zhai, A. Velivelli, and B. Yu.",
                "A cross-collection mixture model for comparative text mining.",
                "In Proceedings of the tenth ACM SIGKDD international conference on knowledge discovery and data mining, pages 743-748, 2004."
            ],
            "original_annotated_samples": [
                "of Computer Science† , Dept. of Electrical and Computer Engineering‡ University of Illinois at Urbana-Champaign bintan@cs.uiuc.edu, velivell@ifp.uiuc.edu, hfang@cs.uiuc.edu, czhai@cs.uiuc.edu ABSTRACT In this paper we study term-based feedback for information retrieval in the <br>language modeling</br> approach.",
                "INTRODUCTION In the <br>language modeling</br> approach to information retrieval, feedback is often modeled as estimating an improved query model or relevance model based on a set of feedback documents [25, 13].",
                "This strategy has been discussed in [15], but to our knowledge, it has not been seriously studied in existing <br>language modeling</br> literature.",
                "During our participation in the TREC 2005 HARD Track and continued study afterward, we explored how to exploit term feedback from the user to construct improved query models for information retrieval in the <br>language modeling</br> approach.",
                "GENERAL APPROACH We follow the <br>language modeling</br> approach, and base our method on the KL-divergence retrieval model proposed in [25]."
            ],
            "translated_annotated_samples": [
                "En este documento estudiamos la retroalimentación basada en términos para la recuperación de información en el enfoque de <br>modelado de lenguaje</br>.",
                "En el enfoque de <br>modelado del lenguaje</br> para la recuperación de información, la retroalimentación a menudo se modela como la estimación de un modelo de consulta mejorado o un modelo de relevancia basado en un conjunto de documentos de retroalimentación [25, 13].",
                "Esta estrategia ha sido discutida en [15], pero hasta donde sabemos, no ha sido estudiada seriamente en la literatura existente de <br>modelado de lenguaje</br>.",
                "Durante nuestra participación en la pista HARD de TREC 2005 y el estudio continuado posteriormente, exploramos cómo aprovechar la retroalimentación de términos por parte del usuario para construir modelos de consulta mejorados para la recuperación de información en el enfoque de <br>modelado de lenguaje</br>.",
                "Enfoque general: Seguimos el enfoque de <br>modelado del lenguaje</br> y basamos nuestro método en el modelo de recuperación de divergencia KL propuesto en [25]."
            ],
            "translated_text": "Comentarios sobre el término para la recuperación de información con modelos de lenguaje Bin Tan†, Atulya Velivelli‡, Hui Fang†, ChengXiang Zhai† Dept. En este documento estudiamos la retroalimentación basada en términos para la recuperación de información en el enfoque de <br>modelado de lenguaje</br>. Con la retroalimentación de términos, un usuario juzga directamente la relevancia de términos individuales sin interacción con documentos de retroalimentación, tomando el control total del proceso de expansión de la consulta. Proponemos un método basado en clusters para seleccionar términos para presentar al usuario para su evaluación, así como algoritmos efectivos para construir modelos de lenguaje de consulta refinados a partir de la retroalimentación de términos del usuario. Nuestros algoritmos han demostrado proporcionar una mejora significativa en la precisión de recuperación en comparación con una línea base sin retroalimentación, y logran un rendimiento comparable al de la retroalimentación de relevancia. Son útiles incluso cuando no hay documentos relevantes en la parte superior. Categorías y Descriptores de Asignaturas H.3.3 [Búsqueda y Recuperación de Información]: Modelos de recuperación Términos generales Algoritmos 1. En el enfoque de <br>modelado del lenguaje</br> para la recuperación de información, la retroalimentación a menudo se modela como la estimación de un modelo de consulta mejorado o un modelo de relevancia basado en un conjunto de documentos de retroalimentación [25, 13]. Esto está en línea con la forma tradicional de realizar retroalimentación de relevancia: presentar al usuario documentos/pasajes para su juicio de relevancia y luego extraer términos de los documentos o pasajes juzgados para expandir la consulta inicial. Es una forma indirecta de buscar la ayuda de los usuarios para la construcción del modelo de consulta, en el sentido de que el modelo de consulta refinado (basado en términos) se aprende a través de documentos/pasajes de retroalimentación, que son estructuras de alto nivel de términos. Tiene la desventaja de que los términos irrelevantes, que ocurren junto con los relevantes en el contenido evaluado, pueden ser utilizados erróneamente para la expansión de la consulta, causando efectos no deseados. Por ejemplo, para la consulta TREC del telescopio Hubble logros, cuando un documento relevante habla más sobre la reparación de los telescopios que sobre sus descubrimientos, términos irrelevantes como caminata espacial pueden ser agregados a la consulta modificada. Podemos considerar una forma más directa de involucrar a un usuario en la mejora del modelo de consulta, sin un paso intermedio de retroalimentación de documentos que pueda introducir ruido. La idea es presentar al usuario un número (razonable) de términos individuales y pedirle que juzgue la relevancia de cada término o especifique directamente sus probabilidades en el modelo de consulta. Esta estrategia ha sido discutida en [15], pero hasta donde sabemos, no ha sido estudiada seriamente en la literatura existente de <br>modelado de lenguaje</br>. En comparación con la retroalimentación de relevancia tradicional, este enfoque basado en términos para el refinamiento del modelo de consulta interactivo tiene varias ventajas. Primero, el usuario tiene un mejor control del modelo de consulta final a través de la manipulación directa de términos: él/ella puede dictar qué términos son relevantes, irrelevantes y posiblemente, en qué medida. Esto evita el riesgo de introducir términos no deseados en el modelo de consulta, aunque a veces el usuario introduce términos de baja calidad. Segundo, dado que un término tarda menos tiempo en ser evaluado que el texto completo o el resumen de un documento, y con tan solo alrededor de 20 términos presentados se puede lograr una mejora significativa en el rendimiento de recuperación (como demostraremos más adelante), el feedback de términos permite recopilar la retroalimentación del usuario de manera más rápida. Esto es especialmente útil para la búsqueda interactiva ad hoc. En tercer lugar, a veces no hay documentos relevantes en los primeros N resultados recuperados inicialmente si el tema es difícil. Esto suele ser cierto cuando N está limitado a ser pequeño, lo cual surge del hecho de que el usuario no está dispuesto a evaluar demasiados documentos. En este caso, la retroalimentación de relevancia es inútil, ya que no se puede aprovechar ningún documento relevante, pero la retroalimentación de términos sigue siendo útil, al permitir seleccionar términos relevantes de documentos irrelevantes. Durante nuestra participación en la pista HARD de TREC 2005 y el estudio continuado posteriormente, exploramos cómo aprovechar la retroalimentación de términos por parte del usuario para construir modelos de consulta mejorados para la recuperación de información en el enfoque de <br>modelado de lenguaje</br>. Identificamos dos sub tareas clave del feedback basado en términos, es decir, la selección de términos para la presentación previa al feedback y la construcción del modelo de consulta posterior al feedback, con algoritmos efectivos desarrollados para ambos. Impusimos una estructura de clúster secundaria en los términos y descubrimos que una vista de clúster arroja una mayor comprensión de la necesidad de información de los usuarios, y proporciona una buena forma de utilizar la retroalimentación de términos. A través de experimentos encontramos que el feedback a término mejora significativamente sobre la línea base sin feedback, a pesar de que el usuario a menudo comete errores en la evaluación de relevancia. Entre nuestros algoritmos, el que tiene el mejor rendimiento de recuperación es TCFB, la combinación de TFB, el algoritmo de retroalimentación de términos directos, y CFB, el algoritmo de retroalimentación basado en clústeres. También variamos el número de términos de retroalimentación y observamos una mejora razonable incluso con números bajos. Finalmente, al comparar la retroalimentación a nivel de términos con la retroalimentación a nivel de documentos, encontramos que es una alternativa viable a esta última con un rendimiento competitivo en la recuperación. El resto del documento está organizado de la siguiente manera. La sección 2 discute algunos trabajos relacionados. La sección 4 describe nuestro enfoque general para la retroalimentación de términos. Presentamos nuestro método para la selección de términos de presentación en la Sección 3 y los algoritmos para la construcción del modelo de consulta en la Sección 5. Los resultados del experimento se presentan en la Sección 6. La sección 7 concluye este documento. 2. TRABAJO RELACIONADO La retroalimentación de relevancia[17, 19] ha sido reconocida durante mucho tiempo como un método efectivo para mejorar el rendimiento de recuperación. Normalmente, los N documentos principales recuperados utilizando la consulta original se presentan al usuario para su evaluación, después de lo cual se extraen términos de los documentos relevantes evaluados, ponderados por su potencial de atraer más documentos relevantes, y se añaden al modelo de consulta. La consulta ampliada suele representar mejor la necesidad de información de los usuarios que la original, que a menudo es solo una consulta de palabras clave cortas. Una segunda iteración de recuperación utilizando esta consulta modificada generalmente produce un aumento significativo en la precisión de la recuperación. En los casos en los que no se dispone de una evaluación de relevancia real y se asume que todos los documentos principales N son relevantes, se denomina retroalimentación ciega o pseudo [5, 16] y generalmente sigue mejorando el rendimiento. Debido a que un documento es una unidad de texto grande, cuando se utiliza para retroalimentación de relevancia, muchos términos irrelevantes pueden ser introducidos en el proceso de retroalimentación. Para superar esto, se propone el feedback de pasajes y se muestra que mejora el rendimiento del feedback. Una solución más directa es pedir al usuario su juicio de relevancia sobre los términos de retroalimentación. Por ejemplo, en algunos sistemas de retroalimentación de relevancia como [12], hay un paso de interacción que permite al usuario agregar o eliminar términos de expansión después de que son extraídos automáticamente de los documentos relevantes. Esto se clasifica como expansión interactiva de consultas, donde la consulta original se amplía con términos proporcionados por el usuario, que pueden provenir de la entrada directa del usuario (texto libre o palabras clave) o de la selección del usuario de términos sugeridos por el sistema (utilizando tesauros o extraídos de documentos de retroalimentación). En muchos casos, se ha encontrado que la retroalimentación de relevancia de términos mejora de manera efectiva el rendimiento de recuperación [6, 22, 12, 4, 10]. Por ejemplo, el estudio en [12] muestra que el usuario prefiere tener un conocimiento explícito y control directo de qué términos se utilizan para la expansión de la consulta, y se demuestra que la interfaz penetrable que proporciona esta libertad tiene un mejor rendimiento que otras interfaces. Sin embargo, en algunos otros casos no hay un beneficio significativo[3, 14], incluso si al usuario le gusta interactuar con los términos de expansión. En un estudio simulado realizado en [18], el autor compara el rendimiento de recuperación de la expansión interactiva de consultas y la expansión automática de consultas con un estudio simulado, y sugiere que los posibles beneficios de la primera pueden ser difíciles de lograr. Se descubre que el usuario no es bueno identificando términos útiles para la expansión de la consulta, cuando una interfaz de presentación de términos simple no puede proporcionar suficiente contexto semántico de los términos de retroalimentación. Nuestro trabajo difiere de los anteriores en dos aspectos importantes. Primero, al elegir los términos para presentar al usuario para la evaluación de relevancia, no solo consideramos el valor de un solo término (por ejemplo, la frecuencia relativa de un término en los documentos principales, que puede medirse mediante métricas como el Valor de Selección de Robertson y la Distancia Kullback-Leibler Simplificada como se detalla en [24]), sino que también examinamos la estructura de los grupos de términos, con el fin de producir una cobertura equilibrada de los diferentes aspectos del tema. Segundo, con el marco de modelado del lenguaje, permitimos una construcción detallada del modelo de consulta actualizado, estableciendo diferentes probabilidades para diferentes términos según si es un término de consulta, su importancia en los documentos principales y su pertenencia a un clúster. Aunque existen técnicas para ajustar los pesos de los términos de consulta en modelos de espacio vectorial y modelos de relevancia probabilística, la mayoría de los trabajos mencionados no las utilizan, optando por simplemente añadir términos de retroalimentación a la consulta original (usando pesos iguales para ellos), lo que puede llevar a un rendimiento de recuperación más pobre. La combinación de los dos aspectos permite que nuestro método funcione mucho mejor que el punto de referencia. La forma habitual de presentar los términos de retroalimentación es simplemente mostrar los términos en una lista. Ha habido algunos trabajos sobre interfaces de usuario alternativas. [8] organiza los términos en una jerarquía, y [11] compara tres interfaces diferentes, incluyendo términos + casillas de verificación, términos + contexto (oraciones) + casillas de verificación, oraciones + cuadro de texto de entrada. En ambos estudios, sin embargo, no hay una diferencia significativa en el rendimiento. En nuestro trabajo adoptamos el enfoque más simple de términos + casillas de verificación. Nos enfocamos en la presentación de términos y la construcción de modelos de consulta a partir de términos de retroalimentación, y creemos que el uso de contextos para mejorar la calidad de los términos de retroalimentación debería ser ortogonal a nuestro método. 3. Enfoque general: Seguimos el enfoque de <br>modelado del lenguaje</br> y basamos nuestro método en el modelo de recuperación de divergencia KL propuesto en [25]. ",
            "candidates": [],
            "error": [
                [
                    "modelado de lenguaje",
                    "modelado del lenguaje",
                    "modelado de lenguaje",
                    "modelado de lenguaje",
                    "modelado del lenguaje"
                ]
            ]
        },
        "query expansion process": {
            "translated_key": "proceso de expansión de la consulta",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Term Feedback for Information Retrieval with Language Models Bin Tan† , Atulya Velivelli‡ , Hui Fang† , ChengXiang Zhai† Dept.",
                "of Computer Science† , Dept. of Electrical and Computer Engineering‡ University of Illinois at Urbana-Champaign bintan@cs.uiuc.edu, velivell@ifp.uiuc.edu, hfang@cs.uiuc.edu, czhai@cs.uiuc.edu ABSTRACT In this paper we study term-based feedback for information retrieval in the language modeling approach.",
                "With term feedback a user directly judges the relevance of individual terms without interaction with feedback documents, taking full control of the <br>query expansion process</br>.",
                "We propose a cluster-based method for selecting terms to present to the user for judgment, as well as effective algorithms for constructing refined query language models from user term feedback.",
                "Our algorithms are shown to bring significant improvement in retrieval accuracy over a non-feedback baseline, and achieve comparable performance to relevance feedback.",
                "They are helpful even when there are no relevant documents in the top.",
                "Categories and Subject Descriptors H.3.3 [Information Search and Retrieval]: Retrieval models General Terms Algorithms 1.",
                "INTRODUCTION In the language modeling approach to information retrieval, feedback is often modeled as estimating an improved query model or relevance model based on a set of feedback documents [25, 13].",
                "This is in line with the traditional way of doing relevance feedback - presenting a user with documents/passages for relevance judgment and then extracting terms from the judged documents or passages to expand the initial query.",
                "It is an indirect way of seeking users assistance for query model construction, in the sense that the refined query model (based on terms) is learned through feedback documents/passages, which are high-level structures of terms.",
                "It has the disadvantage that irrelevant terms, which occur along with relevant ones in the judged content, may be erroneously used for query expansion, causing undesired effects.",
                "For example, for the TREC query Hubble telescope achievements, when a relevant document talks more about the telescopes repair than its discoveries, irrelevant terms such as spacewalk can be added into the modified query.",
                "We can consider a more direct way to involve a user in query model improvement, without an intermediary step of document feedback that can introduce noise.",
                "The idea is to present a (reasonable) number of individual terms to the user and ask him/her to judge the relevance of each term or directly specify their probabilities in the query model.",
                "This strategy has been discussed in [15], but to our knowledge, it has not been seriously studied in existing language modeling literature.",
                "Compared to traditional relevance feedback, this term-based approach to interactive query model refinement has several advantages.",
                "First, the user has better control of the final query model through direct manipulation of terms: he/she can dictate which terms are relevant, irrelevant, and possibly, to what degree.",
                "This avoids the risk of bringing unwanted terms into the query model, although sometimes the user introduces low-quality terms.",
                "Second, because a term takes less time to judge than a documents full text or summary, and as few as around 20 presented terms can bring significant improvement in retrieval performance (as we will show later), term feedback makes it faster to gather user feedback.",
                "This is especially helpful for interactive adhoc search.",
                "Third, sometimes there are no relevant documents in the top N of the initially retrieved results if the topic is hard.",
                "This is often true when N is constrained to be small, which arises from the fact that the user is unwilling to judge too many documents.",
                "In this case, relevance feedback is useless, as no relevant document can be leveraged on, but term feedback is still often helpful, by allowing relevant terms to be picked from irrelevant documents.",
                "During our participation in the TREC 2005 HARD Track and continued study afterward, we explored how to exploit term feedback from the user to construct improved query models for information retrieval in the language modeling approach.",
                "We identified two key subtasks of term-based feedback, i.e., pre-feedback presentation term selection and post-feedback query model construction, with effective algorithms developed for both.",
                "We imposed a secondary cluster structure on terms and found that a cluster view sheds additional insight into the users information need, and provides a good way of utilizing term feedback.",
                "Through experiments we found that term feedback improves significantly over the nonfeedback baseline, even though the user often makes mistakes in relevance judgment.",
                "Among our algorithms, the one with best retrieval performance is TCFB, the combination of TFB, the direct term feedback algorithm, and CFB, the cluster-based feedback algorithm.",
                "We also varied the number of feedback terms and observed reasonable improvement even at low numbers.",
                "Finally, by comparing term feedback with document-level feedback, we found it to be a viable alternative to the latter with competitive retrieval performance.",
                "The rest of the paper is organized as follows.",
                "Section 2 discusses some related work.",
                "Section 4 outlines our general approach to term feedback.",
                "We present our method for presentation term selection in Section 3 and algorithms for query model construction in Section 5.",
                "The experiment results are given in Section 6.",
                "Section 7 concludes this paper. 2.",
                "RELATED WORK Relevance feedback[17, 19] has long been recognized as an effective method for improving retrieval performance.",
                "Normally, the top N documents retrieved using the original query are presented to the user for judgment, after which terms are extracted from the judged relevant documents, weighted by their potential of attracting more relevant documents, and added into the query model.",
                "The expanded query usually represents the users information need better than the original one, which is often just a short keyword query.",
                "A second iteration of retrieval using this modified query usually produces significant increase in retrieval accuracy.",
                "In cases where true relevance judgment is unavailable and all top N documents are assumed to be relevant, it is called blind or pseudo feedback[5, 16] and usually still brings performance improvement.",
                "Because document is a large text unit, when it is used for relevance feedback many irrelevant terms can be introduced into the feedback process.",
                "To overcome this, passage feedback is proposed and shown to improve feedback performance[1, 23].",
                "A more direct solution is to ask the user for their relevance judgment of feedback terms.",
                "For example, in some relevance feedback systems such as [12], there is an interaction step that allows the user to add or remove expansion terms after they are automatically extracted from relevant documents.",
                "This is categorized as interactive query expansion, where the original query is augmented with user-provided terms, which can come from direct user input (free-form text or keywords)[22, 7, 10] or user selection of system-suggested terms (using thesauri[6, 22] or extracted from feedback documents[6, 22, 12, 4, 7]).",
                "In many cases term relevance feedback has been found to effectively improve retrieval performance[6, 22, 12, 4, 10].",
                "For example, the study in [12] shows that the user prefers to have explicit knowledge and direct control of which terms are used for query expansion, and the penetrable interface that provides this freedom is shown to perform better than other interfaces.",
                "However, in some other cases there is no significant benefit[3, 14], even if the user likes interacting with expansion terms.",
                "In a simulated study carried out in [18], the author compares the retrieval performance of interactive query expansion and automatic query expansion with a simulated study, and suggests that the potential benefits of the former can be hard to achieve.",
                "The user is found to be not good at identifying useful terms for query expansion, when a simple term presentation interface is unable to provide sufficient semantic context of the feedback terms.",
                "Our work differs from the previous ones in two important aspects.",
                "First, when we choose terms to present to the user for relevance judgment, we not only consider single-term value (e.g., the relative frequency of a term in the top documents, which can be measured by metrics such as Robertson Selection Value and Simplified Kullback-Leibler Distance as listed in [24]), but also examine the cluster structure of the terms, so as to produce a balanced coverage of the different topic aspects.",
                "Second, with the language modelling framework, we allow an elaborate construction of the updated query model, by setting different probabilities for different terms based on whether it is a query term, its significance in the top documents, and its cluster membership.",
                "Although techniques for adjusting query term weights exist for vector space models[17] and probablistic relevance models[9], most of the aforementioned works do not use them, choosing to just append feedback terms to the original query (thus using equal weights for them), which can lead to poorer retrieval performance.",
                "The combination of the two aspects allows our method to perform much better than the baseline.",
                "The usual way for feedback term presentation is just to display the terms in a list.",
                "There have been some works on alternative user interfaces. [8] arranges terms in a hierarchy, and [11] compares three different interfaces, including terms + checkboxes, terms + context (sentences) + checkboxes, sentences + input text box.",
                "In both studies, however, there is no significant performance difference.",
                "In our work we adopt the simplest approach of terms + checkboxes.",
                "We focus on term presentation and query model construction from feedback terms, and believe using contexts to improve feedback term quality should be orthogonal to our method. 3.",
                "GENERAL APPROACH We follow the language modeling approach, and base our method on the KL-divergence retrieval model proposed in [25].",
                "With this model, the retrieval task involves estimating a query language model θq from a given query, a document language model θd from each document, and calculating their KL-divergence D(θq||θd), which is then used to score the documents. [25] treats relevance feedback as a query model re-estimation problem, i.e., computing an updated query model θq given the original query text and the extra evidence carried by the judged relevant documents.",
                "We adopt this view, and cast our task as updating the query model from user term feedback.",
                "There are two key subtasks here: First, how to choose the best terms to present to the user for judgment, in order to gather maximal evidence about the users information need.",
                "Second, how to compute an updated query model based on this term feedback evidence, so that it captures the users information need and translates into good retrieval performance. 4.",
                "PRESENTATION TERM SELECTION Proper selection of terms to be presented to the user for judgment is crucial to the success of term feedback.",
                "If the terms are poorly chosen and there are few relevant ones, the user will have a hard time looking for useful terms to help clarify his/her information need.",
                "If the relevant terms are plentiful, but all concentrate on a single aspect of the query topic, then we will only be able to get feedback on that aspect and missing others, resulting in a breadth loss in retrieved results.",
                "Therefore, it is important to carefully select presentation terms to maximize expected gain from user feedback, i.e., those that can potentially reveal most evidence of the users information need.",
                "This is similar to active feedback[21], which suggests that a retrieval system should actively probe the users information need, and in the case of relevance feedback, the feedback documents should be chosen to maximize learning benefits (e.g. diversely so as to increase coverage).",
                "In our approach, the top N documents from an initial retrieval using the original query form the source of feedback terms: all terms that appear in them are considered candidates to present to the user.",
                "These documents serve as pseudo-feedback, since they provide a much richer context than the original query (usually very short), while the user is not asked to judge their relevance.",
                "Due to the latter reason, it is possible to make N quite large (e.g., in our experiments we set N = 60) to increase its coverage of different aspects in the topic.",
                "The simplest way of selecting feedback terms is to choose the most frequent M terms from the N documents.",
                "This method, however, has two drawbacks.",
                "First, a lot of common noisy terms will be selected due to their high frequencies in the document collection, unless a stop-word list is used for filtering.",
                "Second, the presentation list will tend to be filled by terms from major aspects of the topic; those from a minor aspect are likely to be missed due to their relatively low frequencies.",
                "We solve the above problems by two corresponding measures.",
                "First, we introduce a background model θB that is estimated from collection statistics and explains the common terms, so that they are much less likely to appear in the presentation list.",
                "Second, the terms are selected from multiple clusters in the pseudo-feedback documents, to ensure sufficient representation of different aspects of the topic.",
                "We rely on the mixture multinomial model, which is used for theme discovery in [26].",
                "Specifically, we assume the N documents contain K clusters {Ci| i = 1, 2, · · · K}, each characterized by a multinomial word distribution (also known as unigram language model) θi and corresponding to an aspect of the topic.",
                "The documents are regarded as sampled from a mixture of K + 1 components, including the K clusters and the background model: p(w|d) = λBp(w|θB) + (1 − λB) K i=1 πd,ip(w|θi) where w is a word, λB is the mixture weight for the background model θB, and πd,i is the document-specific mixture weight for the i-th cluster model θi.",
                "We then estimate the cluster models by maximizing the probability of the pseudo-feedback documents being generated from the multinomial mixture model: log p(D|Λ) = d∈D w∈V c(w; d) log p(w|d) where D = {di| i = 1, 2, · · · N} is the set of the N documents, V is the vocabulary, c(w; d) is ws frequency in d and Λ = {θi| i = 1, 2, · · · K} ∪ {πdij | i = 1, 2, · · · N, j = 1, 2, · · · K} is the set of model parameters to estimate.",
                "The cluster models can be efficiently estimated using the Expectation-Maximization (EM) algorithm.",
                "For its details, we refer the reader to [26].",
                "Table 1 shows the cluster models for TREC query Transportation tunnel disasters (K = 3).",
                "Note that only the middle cluster is relevant.",
                "Table 1: Cluster models for topic 363 Transportation tunnel disasters Cluster 1 Cluster 2 Cluster 3 tunnel 0.0768 tunnel 0.0935 tunnel 0.0454 transport 0.0364 fire 0.0295 transport 0.0406 traffic 0.0206 truck 0.0236 toll 0.0166 railwai 0.0186 french 0.0220 amtrak 0.0153 harbor 0.0146 smoke 0.0157 train 0.0129 rail 0.0140 car 0.0154 airport 0.0122 bridg 0.0139 italian 0.0152 turnpik 0.0105 kilomet 0.0136 firefight 0.0144 lui 0.0095 truck 0.0133 blaze 0.0127 jersei 0.0093 construct 0.0131 blanc 0.0121 pass 0.0087 · · · · · · · · · From each of the K estimated clusters, we choose the L = M/K terms with highest probabilities to form a total of M presentation terms.",
                "If a term happens to be in top L in multiple clusters, we assign it to the cluster where it has highest probability and let the other clusters take one more term as compensation.",
                "We also filter out terms in the original query text because they tend to always be relevant when the query is short.",
                "The selected terms are then presented to the user for judgment.",
                "A sample (completed) feedback form is shown in Figure 1.",
                "In this study we only deal with binary judgment: a presented term is by default unchecked, and a user may check it to indicate relevance.",
                "We also do not explicitly exploit negative feedback (i.e., penalizing irrelevant terms), because with binary feedback an unchecked term is not necessarily irrelevant (maybe the user is unsure about its relevance).",
                "We could ask the user for finer judgment (e.g., choosing from highly relevant, somewhat relevant, do not know, somewhat irrelevant and highly irrelevant), but binary feedback is more compact, taking less space to display and less user effort to make judgment. 5.",
                "ESTIMATING QUERY MODELS FROM TERM FEEDBACK In this section, we present several algorithms for exploiting term feedback.",
                "The algorithms take as input the original query q, the clusters {θi} as generated by the theme discovery algorithm, the set of feedback terms T and their relevance judgment R, and outputs an updated query language model θq that makes best use of the feedback evidence to capture the users information need.",
                "First we describe our notations: • θq: The original query model, derived from query terms only: p(w|θq) = c(w; q) |q| where c(w; q) is the count of w in q, and |q| = w∈q c(w; q) is the query length. • θq : The updated query model which we need to estimate from term feedback. • θi (i = 1, 2, . . .",
                "K): The unigram language model of cluster Ci, as estimated using the theme discovery algorithm. • T = {ti,j} (i = 1 . . .",
                "K, j = 1 . . .",
                "L): The set of terms presented to the user for judgment. ti,j is the j-th term chosen from cluster Ci. • R = {δw|w ∈ T}: δw is an indicator variable that is 1 if w is judged relevant or 0 otherwise. 5.1 TFB (Direct Term Feedback) This is a straight-forward form of term feedback that does not involve any secondary structure.",
                "We give a weight of 1 to terms judged relevant by the user, a weight of μ to query terms, zero weight to other terms, and then apply normalization: p(w|θq ) = δw + μ c(w; q) w ∈T δw + μ|q| where w ∈T δw is the total number of terms that are judged relevant.",
                "We call this method TFB (direct Term FeedBack).",
                "If we let μ = 1, this approach is equivalent to appending the relevant terms after the original query, which is what standard query expansion (without term reweighting) does.",
                "If we set μ > 1, we are putting more emphasis on the query terms than the checked ones.",
                "Note that the result model will be more biased toward θq if the original query is long or the user feedback is weak, which makes sense, as we can trust more on the original query in either case.",
                "Figure 1: Filled clarification form for Topic 363 363 transportation tunnel disasters Please select all terms that are relevant to the topic. traffic railway harbor rail bridge kilometer construct swiss cross link kong hong river project meter shanghai fire truck french smoke car italian firefights blaze blanc mont victim franc rescue driver chamonix emerge toll amtrak train airport turnpike lui jersey pass rome z center electron road boston speed bu submit 5.2 CFB (Cluster Feedback) Here we exploit the cluster structure that played an important role when we selected the presentation terms.",
                "The clusters represent different aspects of the query topic, each of which may or may not be relevant.",
                "If we are able to identify the relevant clusters, we can combine them to generate a query model that is good at discovering documents belonging to these clusters (instead of the irrelevant ones).",
                "We could ask the user to directly judge the relevance of a cluster after viewing representative terms in that cluster, but this would sometimes be a difficult task for the user, who has to guess the semantics of a cluster via its set of terms, which may not be well connected to one another due to a lack of context.",
                "Therefore, we propose to learn cluster feedback indirectly, inferring the relevance of a cluster through the relevance of its feedback terms.",
                "Because each cluster has an equal number of terms presented to the user, the simplest measure of a clusters relevance is the number of terms that are judged relevant in it.",
                "Intuitively, the more terms are marked relevant in a cluster, the closer the cluster is to the query topic, and the more the cluster should participate in query modification.",
                "If we combine the cluster models using weights determined this way and then interpolate with the original query model, we get the following formula for query updating, which we call CFB (Cluster FeedBack): p(w|θq ) = λp(w|θq) + (1 − λ) K i=1 L j=1 δti,j K k=1 L j=1 δtk,j p(w|θi) where L j=1 δti,j is the number of relevant terms in cluster Ci, and K k=1 L j=1 δtk,j is the total number of relevant terms.",
                "We note that when there is only one cluster (K = 1), the above formula degenerates to p(w|θq ) = λp(w|θq) + (1 − λ)p(w|θ1) which is merely pseudo-feedback of the form proposed in [25]. 5.3 TCFB (Term-cluster Feedback) TFB and CFB both have their drawbacks.",
                "TFB assigns non-zero probabilities to the presented terms that are marked relevant, but completely ignores (a lot more) others, which may be left unchecked due to the users ignorance, or simply not included in the presentation list, but we should be able to infer their relevance from the checked ones.",
                "For example, in Figure 1, since as many as 5 terms in the middle cluster (the third and fourth columns) are checked, we should have high confidence in the relevance of other terms in that cluster.",
                "CFB remedies TFBs problem by treating the terms in a cluster collectively, so that unchecked/unpresented terms receive weights when presented terms in their clusters are judged as relevant, but it does not distinguish which terms in a cluster are presented or judged.",
                "Intuitively, the judged relevant terms should receive larger weights because they are explicitly indicated as relevant by the user.",
                "Therefore, we try to combine the two methods, hoping to get the best out of both.",
                "We do this by interpolating the TFB model with the CFB model, and call it TCFB: p(w|θq ) = αp(w|θqT F B ) + (1 − α)p(w|θqCF B ) 6.",
                "EXPERIMENTS In this section, we describe our experiment results.",
                "We first describe our experiment setup and present an overview of various methods performance.",
                "Then we discuss the effects of varying the parameter setting in the algorithms, as well as the number of presentation terms.",
                "Next we analyze user term feedback behavior and its relation to retrieval performance.",
                "Finally we compare term feedback to relevance feedback and show that it has its particular advantage. 6.1 Experiment Setup and Basic Results We took the opportunity of TREC 2005 HARD Track[2] for the evaluation of our algorithms.",
                "The tracks used the AQUAINT collection, a 3GB corpus of English newswire text.",
                "The topics included 50 ones previously known to be hard, i.e. with low retrieval performance.",
                "It is for these hard topics that user feedback is most helpful, as it can provide information to disambiguate the queries; with easy topics the user may be unwilling to spend efforts for feedback if the automatic retrieval results are good enough.",
                "Participants of the track were able to submit custom-designed clarification forms (CF) to solicit feedback from human assessors provided by Table 2: Retrieval performance for different methods and CF types.",
                "The last row is the percentage of MAP improvement over the baseline.",
                "The parameter settings μ = 4, λ = 0.1, α = 0.3 are near optimal.",
                "Baseline TFB1C TFB3C TFB6C CFB1C CFB3C CFB6C TCFB1C TCFB3C TCFB6C MAP 0.219 0.288 0.288 0.278 0.254 0.305 0.301 0.274 0.309 0.304 Pr@30 0.393 0.467 0.475 0.457 0.399 0.480 0.473 0.431 0.491 0.473 RR 4339 4753 4762 4740 4600 4907 4872 4767 4947 4906 % 0% 31.5% 31.5% 26.9% 16.0% 39.3% 37.4% 25.1% 41.1% 38.8% Table 3: MAP variation with the number of presented terms. # terms TFB1C TFB3C TFB6C CFB3C CFB6C TCFB3C TCFB6C 6 0.245 0.240 0.227 0.279 0.279 0.281 0.274 12 0.261 0.261 0.242 0.299 0.286 0.297 0.281 18 0.275 0.274 0.256 0.301 0.282 0.300 0.286 24 0.276 0.281 0.265 0.303 0.292 0.305 0.292 30 0.280 0.285 0.270 0.304 0.296 0.307 0.296 36 0.282 0.288 0.272 0.307 0.297 0.309 0.297 42 0.283 0.288 0.275 0.306 0.298 0.309 0.300 48 0.288 0.288 0.278 0.305 0.301 0.309 0.303 NIST.",
                "We designed three sets of clarification forms for term feedback, differing in the choice of K, the number of clusters, and L, the number of presented terms from each cluster.",
                "They are: 1× 48, a big cluster with 48 terms, 3 × 16, 3 clusters with 16 terms each, and 6 × 8, 6 clusters with 8 terms each.",
                "The total number of presented terms (M) is fixed at 48, so by comparing the performance of different types of clarification forms we can know the effects of different degree of clustering.",
                "For each topic, an assessor would complete the forms ordered by 6 × 8, 1 × 48 and 3 × 16, spending up to three minutes on each form.",
                "The sample clarification form shown in Figure 1 is of type 3 × 16.",
                "It is a simple and compact interface in which the user can check relevant terms.",
                "The form is self-explanatory; there is no need for extra user training on how to use it.",
                "Our initinal queries are constructed only using the topic title descriptions, which are on average 2.7 words in length.",
                "As our baseline we use the KL divergence retrieval method implemented in the Lemur Toolkit1 with 5 pseudo-feedback documents.",
                "We stem the terms, choose Dirichlet smoothing with a prior of 2000, and truncate query language models to 50 terms (these settings are used throughout the experiments).",
                "For all other parameters we use Lemurs default settings.",
                "The baseline turns out to perform above average among the track participants.",
                "After an initial run using this baseline retrieval method, we take the top 60 documents for each topic and apply the theme discovery algorithm to output the clusters (1, 3, or 6 of them), based on which we generate clarification forms.",
                "After user feedback is received, we run the term feedback algorithms (TFB, CFB or TCFB) to estimate updated query models, which are then used for a second iteration of retrieval.",
                "We evaluate the different retrieval methods performance on their rankings of the top 1000 documents.",
                "The evaluation metrics we adopt include mean average (non-interpolated) precision (MAP), precision at top 30 (Pr@30) and total relevant retrieved (RR).",
                "Table 2 shows the performance of various methods and configurations of K × L. The suffixes (1C, 3C, 6C) after TFB,CFB,TCFB stand for the number of clusters (K).",
                "For example, TCFB3C means the TCFB method on the 3 × 16 clarification forms.",
                "From Table 2 we can make the following observations: 1 http://www.lemurproject.com 1.",
                "All methods perform considerably better than the pseudofeedback baseline, with TCFB3C achieving a highest 41.1% improvement in MAP, indicating significant contribution of term feedback for clarification of the users information need.",
                "In other words, term feedback is truly helpful for improving retrieval accuracy. 2.",
                "For TFB, the performance is almost equal on the 1 × 48 and 3 × 16 clarification forms in terms of MAP (although the latter is slightly better in Pr@30 and RR), and a little worse on the 6 × 8 ones. 3.",
                "Both CFB3C and CFB6C perform better than their TFB counterparts in all three metrics, suggesting that feedback on a secondary cluster structure is indeed beneficial.",
                "CFB1C is actually worse because it cannot adjust the weight of its (single) cluster from term feedback and it is merely pseudofeedback. 4.",
                "Although TCFB is just a simple mixture of TFB and CFB by interpolation, it is able to outperform both.",
                "This supports our speculation that TCFB overcomes the drawbacks of TFB (paying attention only to checked terms) and CFB (not distinguishing checked and unchecked terms in a cluster).",
                "Except for TCFB6C v.s.",
                "CFB6C, the performance advantage of TCFB over TFB/CFB is significant at p < 0.05 using the Wilcoxon signed rank test.",
                "This is not true in the case of TFB v.s.",
                "CFB, each of which is better than the other in nearly half of the topics. 6.2 Reduction of Presentation Terms In some situations we may have to reduce the number of presentation terms due to limits in display space or user feedback efforts.",
                "It is interesting to know whether our algorithms performance deteriorates when the user is presented with fewer terms.",
                "Because the presentation terms within each cluster are generated in decreasing order of their frequencies, the presentation list forms a subset of the original one if its size is reduced2 .",
                "Therefore, we can easily simulate what happens when the number of presentation terms decreases 2 There are complexities arising from terms appearing in top L of multiple clusters, but these are exceptions from M to M : we will keep all judgments of the top L = M /K terms in each cluster and discard those of others.",
                "Table 3 shows the performance of various algorithms as the number of presentation terms ranges from 6 to 48.",
                "We find that the performance of TFB is more susceptible to presentation term reduction than that of CFB or TCFB.",
                "For example, at 12 terms the MAP of TFB3C is 90.6% of that at 48 terms, while the numbers for CFB3C and TCFB3C are 98.0% and 96.1% respectively.",
                "We conjecture the reason to be that while TFBs performance heavily depends on how many good terms are chosen for query expansion, CFB only needs a rough estimate of cluster weights to work.",
                "Also, the 3 × 16 clarification forms seem to be more robust than the 6 × 8 ones: at 12 terms the MAP of TFB6C is 87.1% of that at 48 terms, lower than 90.6% for TFB3C.",
                "Similarly, for CFB it is 95.0% against 98.0%.",
                "This is natual, as for a large cluster number of 6, it is easier to get into the situation where each cluster gets too few presentation terms to make topic diversification useful.",
                "Overall, we are surprised to see that the algorithms are still able to perform reasonably well when the number of presentation terms is small.",
                "For example, at only 12 terms CFB3C (the clarification form is of size 3 × 4) can still improve 36.5% over the baseline, dropping slightly from 39.3% at 48 terms. 6.3 User Feedback Analysis In this part we study several aspects of users term feedback behavior, and whether they are connected to retrieval performance.",
                "Figure 2: Clarification form completion time distributions 0−30 30−60 60−90 90−120 120−150 150−180 0 5 10 15 20 25 30 35 completion time (seconds) #topics 1×48 3×16 6×8 Figure 2 shows the distribution of time needed to complete a clarification form3 .",
                "We see that the user is usually able to finish term feedback within a reasonably short amount of time: for more than half of the topics the clarification form is completed in just 1 minute, and only a small fraction of topics (less than 10% for 1 × 48 and 3 × 16) take more than 2 minutes.",
                "This suggests that term feedback is suitable for interactive ad-hoc retrieval, where a user usually does not want to spend too much time on providing feedback.",
                "We find that a user often makes mistakes when judging term relevance.",
                "Sometimes a relevant term may be left out because its connection to the query topic is not obvious to the user.",
                "Other times a dubious term may be included but turns out to be irrelevant.",
                "Take the topic in Figure 1 for example.",
                "There was a fire disaster in Mont 3 The maximal time is 180 seconds, as the NIST assessor would be forced to submit the form at that moment.",
                "Table 4: Term selection statistics (topic average) CF Type 1 × 48 3 × 16 6 × 8 # checked terms 14.8 13.3 11.2 # rel. terms 15.0 12.6 11.2 # rel. checked terms 7.9 6.9 5.9 precision 0.534 0.519 0.527 recall 0.526 0.548 0.527 Blanc Tunnel between France and Italy in 1999, but the user failed to select such keywords as mont, blanc, french and italian due to his/her ignorance of the event.",
                "Indeed, without proper context it would be hard to make perfect judgment.",
                "What is then, the extent to which the user is good at term feedback?",
                "Does it have serious impact on retrieval performance?",
                "To answer these questions, we need a measure of individual terms true relevance.",
                "We adopt the Simplified KL Divergence metric used in [24] to decide query expansion terms as our term relevance measure: σKLD(w) = p(w|R) log p(w|R) p(w|¬R) where p(w|R) is the probability that a relevant document contains term w, and p(w|¬R) is the probability that an irrelevant document contains w, both of which can be easily computed via maximum likelihood estimate given document-level relevance judgment.",
                "If σKLD(w) > 0, w is more likely to appear in relevant documents than irrelevant ones.",
                "We consider a term relevant if its Simplified KL Divergence value is greater than a certain threshold σ0.",
                "We can then define precision and recall of user term judgment accordingly: precision is the fraction of terms checked by the user that are relevant; recall is the fraction of presented relevant terms that are checked by the user.",
                "Table 4 shows the number of checked terms, relevant terms and relevant checked terms when σ0 is set to 1.0, as well as the precision/recall of user term judgment.",
                "Note that when the clarification forms contain more clusters, fewer terms are checked: 14.8 for 1 × 48, 13.3 for 3 × 16 and 11.2 for 6×8.",
                "Similar pattern holds for relevant terms and relevant checked terms.",
                "There seems to be a trade-off between increasing topic diversity by clustering and losing extra relevant terms: when there are more clusters, each of them gets fewer terms to present, which can hurt a major relevant cluster that contains many relevant terms.",
                "Therefore, it is not always helpful to have more clusters, e.g., TFB6C is actually worse than TFB1C.",
                "The major finding we can make from Table 4 is that the user is not particularly good at identifying relevant terms, which echoes the discovery in [18].",
                "In the case of 3 × 16 clarification forms, the average number of terms checked as relevant by the user is 13.3 per topic, and the average number of relevant terms whose σKLD value exceed 1.0 is 12.6.",
                "The user is able to recognize only 6.9 of these terms on average.",
                "Indeed, the precision and recall of user feedback terms (as defined previously) are far from perfect.",
                "On the other hand, If the user had correctly checked all such relevant terms, the performance of our algorithms would have increased a lot, as shown in Table 5.",
                "We see that TFB gets big improvement when there is an oracle who checks all relevant terms, while CFB meets a bottleneck around MAP of 0.325, since all it does is adjust cluster weights, and when the learned weights are close to being accurate, it cannot benefit more from term feedback.",
                "Also note that TCFB fails to outperform TFB, probably because TFB is sufficiently accurate.",
                "Table 5: Change of MAP when using all (and only) relevant terms (σKLD > 1.0) for feedback. original term feedback relevant term feedback TF1 0.288 0.354 TF3 0.288 0.354 TF6 0.278 0.346 CF3 0.305 0.325 CF6 0.301 0.326 TCF3 0.309 0.345 TCF6 0.304 0.341 6.4 Comparison with Relevance Feedback Now we compare term feedback with document-level relevance feedback, in which the user is presented with the top N documents from an initial retrieval and asked to judge their relevance.",
                "The feedback process is simulated using document relevance judgment from NIST.",
                "We use the mixture model based feedback method proposed in [25], with mixture noise set to 0.95 and feedback coefficient set to 0.9.",
                "Comparative evaluation of relevance feedback against other methods is complicated by the fact that some documents have already been viewed during feedback, so it makes no sense to include them in the retrieval results of the second run.",
                "However, this does not hold for term feedback.",
                "Thus, to make it fair w.r.t. users information gain, if the feedback documents are relevant, they should be kept in the top of the ranking; if they are irrelevant, they should be left out.",
                "Therefore, we use relevance feedback to produce a ranking of top 1000 retrieved documents but with every feedback document excluded, and then prepend the relevant feedback documents at the front.",
                "Table 6 shows the performance of relevance feedback for different values of N and compares it with TCFB3C.",
                "Table 6: Performance of relevance feedback for different number of feedback documents (N).",
                "N MAP Pr@30 RR 5 0.302 0.586 4779 10 0.345 0.670 4916 20 0.389 0.772 5004 TCFB3C 0.309 0.491 4947 We see that the performance of TCFB3C is comparable to that of relevance feedback using 5 documents.",
                "Although it is poorer than when there are 10 feedback documents in terms of MAP and Pr@30, it does retrieve more documents (4947) when going down the ranked list.",
                "We try to compare the quality of automatically inserted terms in relevance feedback with that of manually selected terms in term feedback.",
                "This is done by truncating the relevance feedback modified query model to a size equal to the number of checked terms for the same topic.",
                "We can then compare the terms in the truncated model with the checked terms.",
                "Figure 3 shows the distribution of the terms σKLD scores.",
                "We find that term feedback tends to produce expansion terms of higher quality(those with σKLD > 1) compared to relevance feedback (with 10 feedback documents).",
                "This does not contradict the fact that the latter yields higher retrieval performance.",
                "Actually, when we use the truncated query model instead of the intact one refined from relevance feedback, the MAP is only 0.304.",
                "The truth Figure 3: Comparison of expansion term quality between relevance feedback (with 10 feedback documents) and term feedback (with 3 × 16 CFs) −1−0 0−1 1−2 2−3 3−4 4−5 5−6 0 50 100 150 200 250 300 350 σKLD #terms relevance feedback term feedback is, although there are many unwanted terms in the expanded query model from feedback documents, there are also more relevant terms than what the user can possibly select from the list of presentation terms generated with pseudo-feedback documents, and the positive effects often outweights the negative ones.",
                "We are interested to know under what circumstances term feedback has advantage over relevance feedback.",
                "One such situation is when none of the top N feedback documents is relevant, rendering relevance feedback useless.",
                "This is not infrequent, as one might have thought: out of the 50 topics, there are 13 such cases when N = 5, 10 when N = 10, and still 3 when N = 20.",
                "When this happens, one can only back off to the original retrieval method; the power of relevance feedback is lost.",
                "Surprisingly, in 11 out of 13 such cases where relevance feedback seems impossible, the user is able to check at least 2 relevant terms from the 3 × 16 clarification form (we consider term t to be relevant if σKLD(t) > 1.0).",
                "Furthermore, in 10 out of them TCFB3C outperforms the pseudo-feedback baseline, increasing MAP from 0.076 to 0.146 on average (these are particularly hard topics).",
                "We think that there are two possible explanations for this phenomenon of term feedback being active even when relevance feedback does not work: First, even if none of the top N (suppose it is a small number) documents are relevant, we may still find relevant documents in top 60, which is more inclusive but usually unreachable when people are doing relevance feedback in interactive ad-hoc search, from which we can draw feedback terms.",
                "This is true for topic 367 piracy, where the top 10 feedback documents are all about software piracy, yet there are documents between 10-60 that are about piracy on the seas (which is about the real information need), contributing terms such as pirate, ship for selection in the clarification form.",
                "Second, for some topics, a document needs to meet some special condition in order to be relevant.",
                "The top N documents may be related to the topic, but nonetheless irrelevant.",
                "In this case, we may still extract useful terms from these documents, even if they do not qualify as relevant ones.",
                "For example, in topic 639 consumer online shopping, a document needs to mention what contributes to shopping growth to really match the specified information need, hence none of the top 10 feedback documents are regarded as relevant.",
                "But nevertheless, the feedback terms such as retail, commerce are good for query expansion. 7.",
                "CONCLUSIONS In this paper we studied the use of term feedback for interactive information retrieval in the language modeling approach.",
                "We proposed a cluster-based method for selecting presentation terms as well as algorithms to estimate refined query models from user term feedback.",
                "We saw significant improvement in retrieval accuracy brought by term feedback, in spite of the fact that a user often makes mistakes in relevance judgment that hurts its performance.",
                "We found the best-performing algorithm to be TCFB, which benefits from the combination of directly observed term evidence with TFB and indirectly learned cluster relevance with CFB.",
                "When we reduced the number of presentation terms, term feedback is still able to keep much of its performance gain over the baseline.",
                "Finally, we compared term feedback to document-level relevance feedback, and found that TCFB3Cs performance is on a par with the latter with 5 feedback documents.",
                "We regarded term feedback as a viable alternative to traditional relevance feedback, especially when there are no relevant documents in the top.",
                "We propose to extend our work in several ways.",
                "First, we want to study whether the use of various contexts can help the user to better identify term relevance, while not sacrificing the simplicity and compactness of term feedback.",
                "Second, currently all terms are presented to the user in a single batch.",
                "We could instead consider iterative term feedback, by presenting a small number of terms first, and show more terms after receiving user feedback or stop when the refined query is good enough.",
                "The presented terms should be selected dynamically to maximize learning benefits at any moment.",
                "Third, we have plans to incorporate term feedback into our UCAIR toolbar[20], an Internet Explorer plugin, to make it work for web search.",
                "We are also interested in studying how to combine term feedback with relevance feedback or implicit feedback.",
                "We could, for example, allow the user to dynamically modify terms in a language model learned from feedback documents. 8.",
                "ACKNOWLEDGMENT This work is supported in part by the National Science Foundation grants IIS-0347933 and IIS-0428472. 9.",
                "REFERENCES [1] J. Allan.",
                "Relevance feedback with too much data.",
                "In Proceedings of the 18th annual international ACM SIGIR conference on research and development in information retrieval, pages 337-343, 1995. [2] J. Allan.",
                "HARD track overview in TREC 2005 - High Accuracy Retrieval from Documents.",
                "In The Fourteenth Text REtrieval Conference, 2005. [3] P. Anick.",
                "Using terminological feedback for web search refinement: a log-based study.",
                "In Proceedings of the 26th annual international ACM SIGIR conference on research and development in informaion retrieval, pages 88-95, 2003. [4] P. G. Anick and S. Tipirneni.",
                "The paraphrase search assistant: terminological feedback for iterative information seeking.",
                "In Proceedings of the 22nd annual international ACM SIGIR conference on research and development in information retrieval, pages 153-159, 1999. [5] C. Buckley, G. Salton, J. Allan, and A. Singhal.",
                "Automatic query expansion using SMART.",
                "In Proceedings of the Third Text REtrieval Conference, 1994. [6] D. Harman.",
                "Towards interactive query expansion.",
                "In Proceedings of the 11th annual international ACM SIGIR conference on research and development in information retrieval, pages 321-331, 1988. [7] N. A. Jaleel, A. Corrada-Emmanuel, Q. Li, X. Liu, C. Wade, and J. Allan.",
                "UMass at TREC 2003: HARD and QA.",
                "In TREC, pages 715-725, 2003. [8] H. Joho, C. Coverson, M. Sanderson, and M. Beaulieu.",
                "Hierarchical presentation of expansion terms.",
                "In Proceedings of the 2002 ACM symposium on applied computing, pages 645-649, 2002. [9] K. S. Jones, S. Walker, and S. E. Robertson.",
                "A probabilistic model of information retrieval: development and status.",
                "Technical Report 446, Computer Laboratory, University of Cambridge, 1998. [10] D. Kelly, V. D. Dollu, and X. Fu.",
                "The loquacious user: a document-independent source of terms for query expansion.",
                "In Proceedings of the 28th annual international ACM SIGIR conference on research and development in information retrieval, pages 457-464, 2005. [11] D. Kelly and X. Fu.",
                "Elicitation of term relevance feedback: an investigation of term source and context.",
                "In Proceedings of the 29th annual international ACM SIGIR conference on research and development in information retrieval, 2006. [12] J. Koenemann and N. Belkin.",
                "A case for interaction: A study of interactive information retrieval behavior and effectiveness.",
                "In Proceedings of the SIGCHI conference on human factors in computing systems, pages 205-212, 1996. [13] V. Lavrenko and W. B. Croft.",
                "Relevance-based language models.",
                "In Research and Development in Information Retrieval, pages 120-127, 2001. [14] Y. Nemeth, B. Shapira, and M. Taeib-Maimon.",
                "Evaluation of the real and perceived value of automatic and interactive query expansion.",
                "In Proceedings of the 27th annual international ACM SIGIR conference on research and development in information retrieval, pages 526-527, 2004. [15] J. Ponte.",
                "A Language Modeling Approach to Information Retrieval.",
                "PhD thesis, University of Massachusetts at Amherst, 1998. [16] S. E. Robertson, S. Walker, S. Jones, M. Beaulieu, and M. Gatford.",
                "Okapi at TREC-3.",
                "In Proceedings of the Third Text REtrieval Conference, 1994. [17] J. Rocchio.",
                "Relevance feedback in information retrieval.",
                "In The SMART retrieval system, pages 313-323. 1971. [18] I. Ruthven.",
                "Re-examining the potential effectiveness of interactive query expansion.",
                "In Proceedings of the 26th annual international ACM SIGIR conference on research and development in informaion retrieval, pages 213-220, 2003. [19] G. Salton and C. Buckley.",
                "Improving retrieval performance by relevance feedback.",
                "Journal of the American Society for Information Science, 41:288-297, 1990. [20] X. Shen, B. Tan, and C. Zhai.",
                "Implicit user modeling for personalized search.",
                "In Proceedings of the 14th ACM international conference on information and knowledge management, pages 824-831, 2005. [21] X. Shen and C. Zhai.",
                "Active feedback in ad-hoc information retrieval.",
                "In Proceedings of the 28th annual international ACM SIGIR conference on research and development in information retrieval, pages 59-66, 2005. [22] A. Spink.",
                "Term relevance feedback and query expansion: relation to design.",
                "In Proceedings of the 17th annual international ACM SIGIR conference on research and development in information retrieval, pages 81-90, 1994. [23] J. Xu and W. B. Croft.",
                "Query expansion using local and global document analysis.",
                "In Proceedings of the 19th annual international ACM SIGIR conference on research and development in information retrieval, pages 4-11, 1996. [24] H. Zaragoza, N. Craswell, M. Taylor, S. Saria, and S. Robertson.",
                "Microsoft cambridge at TREC-13: Web and HARD tracks.",
                "In Proceedings of the 13th Text REtrieval Conference, 2004. [25] C. Zhai and J. Lafferty.",
                "Model-based feedback in the language modeling approach to information retrieval.",
                "In Proceedings of the tenth international conference on information and knowledge management, pages 403-410, 2001. [26] C. Zhai, A. Velivelli, and B. Yu.",
                "A cross-collection mixture model for comparative text mining.",
                "In Proceedings of the tenth ACM SIGKDD international conference on knowledge discovery and data mining, pages 743-748, 2004."
            ],
            "original_annotated_samples": [
                "With term feedback a user directly judges the relevance of individual terms without interaction with feedback documents, taking full control of the <br>query expansion process</br>."
            ],
            "translated_annotated_samples": [
                "Con la retroalimentación de términos, un usuario juzga directamente la relevancia de términos individuales sin interacción con documentos de retroalimentación, tomando el control total del <br>proceso de expansión de la consulta</br>."
            ],
            "translated_text": "Comentarios sobre el término para la recuperación de información con modelos de lenguaje Bin Tan†, Atulya Velivelli‡, Hui Fang†, ChengXiang Zhai† Dept. En este documento estudiamos la retroalimentación basada en términos para la recuperación de información en el enfoque de modelado de lenguaje. Con la retroalimentación de términos, un usuario juzga directamente la relevancia de términos individuales sin interacción con documentos de retroalimentación, tomando el control total del <br>proceso de expansión de la consulta</br>. Proponemos un método basado en clusters para seleccionar términos para presentar al usuario para su evaluación, así como algoritmos efectivos para construir modelos de lenguaje de consulta refinados a partir de la retroalimentación de términos del usuario. Nuestros algoritmos han demostrado proporcionar una mejora significativa en la precisión de recuperación en comparación con una línea base sin retroalimentación, y logran un rendimiento comparable al de la retroalimentación de relevancia. Son útiles incluso cuando no hay documentos relevantes en la parte superior. Categorías y Descriptores de Asignaturas H.3.3 [Búsqueda y Recuperación de Información]: Modelos de recuperación Términos generales Algoritmos 1. En el enfoque de modelado del lenguaje para la recuperación de información, la retroalimentación a menudo se modela como la estimación de un modelo de consulta mejorado o un modelo de relevancia basado en un conjunto de documentos de retroalimentación [25, 13]. Esto está en línea con la forma tradicional de realizar retroalimentación de relevancia: presentar al usuario documentos/pasajes para su juicio de relevancia y luego extraer términos de los documentos o pasajes juzgados para expandir la consulta inicial. Es una forma indirecta de buscar la ayuda de los usuarios para la construcción del modelo de consulta, en el sentido de que el modelo de consulta refinado (basado en términos) se aprende a través de documentos/pasajes de retroalimentación, que son estructuras de alto nivel de términos. Tiene la desventaja de que los términos irrelevantes, que ocurren junto con los relevantes en el contenido evaluado, pueden ser utilizados erróneamente para la expansión de la consulta, causando efectos no deseados. Por ejemplo, para la consulta TREC del telescopio Hubble logros, cuando un documento relevante habla más sobre la reparación de los telescopios que sobre sus descubrimientos, términos irrelevantes como caminata espacial pueden ser agregados a la consulta modificada. Podemos considerar una forma más directa de involucrar a un usuario en la mejora del modelo de consulta, sin un paso intermedio de retroalimentación de documentos que pueda introducir ruido. La idea es presentar al usuario un número (razonable) de términos individuales y pedirle que juzgue la relevancia de cada término o especifique directamente sus probabilidades en el modelo de consulta. Esta estrategia ha sido discutida en [15], pero hasta donde sabemos, no ha sido estudiada seriamente en la literatura existente de modelado de lenguaje. En comparación con la retroalimentación de relevancia tradicional, este enfoque basado en términos para el refinamiento del modelo de consulta interactivo tiene varias ventajas. Primero, el usuario tiene un mejor control del modelo de consulta final a través de la manipulación directa de términos: él/ella puede dictar qué términos son relevantes, irrelevantes y posiblemente, en qué medida. Esto evita el riesgo de introducir términos no deseados en el modelo de consulta, aunque a veces el usuario introduce términos de baja calidad. Segundo, dado que un término tarda menos tiempo en ser evaluado que el texto completo o el resumen de un documento, y con tan solo alrededor de 20 términos presentados se puede lograr una mejora significativa en el rendimiento de recuperación (como demostraremos más adelante), el feedback de términos permite recopilar la retroalimentación del usuario de manera más rápida. Esto es especialmente útil para la búsqueda interactiva ad hoc. En tercer lugar, a veces no hay documentos relevantes en los primeros N resultados recuperados inicialmente si el tema es difícil. Esto suele ser cierto cuando N está limitado a ser pequeño, lo cual surge del hecho de que el usuario no está dispuesto a evaluar demasiados documentos. En este caso, la retroalimentación de relevancia es inútil, ya que no se puede aprovechar ningún documento relevante, pero la retroalimentación de términos sigue siendo útil, al permitir seleccionar términos relevantes de documentos irrelevantes. Durante nuestra participación en la pista HARD de TREC 2005 y el estudio continuado posteriormente, exploramos cómo aprovechar la retroalimentación de términos por parte del usuario para construir modelos de consulta mejorados para la recuperación de información en el enfoque de modelado de lenguaje. Identificamos dos sub tareas clave del feedback basado en términos, es decir, la selección de términos para la presentación previa al feedback y la construcción del modelo de consulta posterior al feedback, con algoritmos efectivos desarrollados para ambos. Impusimos una estructura de clúster secundaria en los términos y descubrimos que una vista de clúster arroja una mayor comprensión de la necesidad de información de los usuarios, y proporciona una buena forma de utilizar la retroalimentación de términos. A través de experimentos encontramos que el feedback a término mejora significativamente sobre la línea base sin feedback, a pesar de que el usuario a menudo comete errores en la evaluación de relevancia. Entre nuestros algoritmos, el que tiene el mejor rendimiento de recuperación es TCFB, la combinación de TFB, el algoritmo de retroalimentación de términos directos, y CFB, el algoritmo de retroalimentación basado en clústeres. También variamos el número de términos de retroalimentación y observamos una mejora razonable incluso con números bajos. Finalmente, al comparar la retroalimentación a nivel de términos con la retroalimentación a nivel de documentos, encontramos que es una alternativa viable a esta última con un rendimiento competitivo en la recuperación. El resto del documento está organizado de la siguiente manera. La sección 2 discute algunos trabajos relacionados. La sección 4 describe nuestro enfoque general para la retroalimentación de términos. Presentamos nuestro método para la selección de términos de presentación en la Sección 3 y los algoritmos para la construcción del modelo de consulta en la Sección 5. Los resultados del experimento se presentan en la Sección 6. La sección 7 concluye este documento. 2. TRABAJO RELACIONADO La retroalimentación de relevancia[17, 19] ha sido reconocida durante mucho tiempo como un método efectivo para mejorar el rendimiento de recuperación. Normalmente, los N documentos principales recuperados utilizando la consulta original se presentan al usuario para su evaluación, después de lo cual se extraen términos de los documentos relevantes evaluados, ponderados por su potencial de atraer más documentos relevantes, y se añaden al modelo de consulta. La consulta ampliada suele representar mejor la necesidad de información de los usuarios que la original, que a menudo es solo una consulta de palabras clave cortas. Una segunda iteración de recuperación utilizando esta consulta modificada generalmente produce un aumento significativo en la precisión de la recuperación. En los casos en los que no se dispone de una evaluación de relevancia real y se asume que todos los documentos principales N son relevantes, se denomina retroalimentación ciega o pseudo [5, 16] y generalmente sigue mejorando el rendimiento. Debido a que un documento es una unidad de texto grande, cuando se utiliza para retroalimentación de relevancia, muchos términos irrelevantes pueden ser introducidos en el proceso de retroalimentación. Para superar esto, se propone el feedback de pasajes y se muestra que mejora el rendimiento del feedback. Una solución más directa es pedir al usuario su juicio de relevancia sobre los términos de retroalimentación. Por ejemplo, en algunos sistemas de retroalimentación de relevancia como [12], hay un paso de interacción que permite al usuario agregar o eliminar términos de expansión después de que son extraídos automáticamente de los documentos relevantes. Esto se clasifica como expansión interactiva de consultas, donde la consulta original se amplía con términos proporcionados por el usuario, que pueden provenir de la entrada directa del usuario (texto libre o palabras clave) o de la selección del usuario de términos sugeridos por el sistema (utilizando tesauros o extraídos de documentos de retroalimentación). En muchos casos, se ha encontrado que la retroalimentación de relevancia de términos mejora de manera efectiva el rendimiento de recuperación [6, 22, 12, 4, 10]. Por ejemplo, el estudio en [12] muestra que el usuario prefiere tener un conocimiento explícito y control directo de qué términos se utilizan para la expansión de la consulta, y se demuestra que la interfaz penetrable que proporciona esta libertad tiene un mejor rendimiento que otras interfaces. Sin embargo, en algunos otros casos no hay un beneficio significativo[3, 14], incluso si al usuario le gusta interactuar con los términos de expansión. En un estudio simulado realizado en [18], el autor compara el rendimiento de recuperación de la expansión interactiva de consultas y la expansión automática de consultas con un estudio simulado, y sugiere que los posibles beneficios de la primera pueden ser difíciles de lograr. Se descubre que el usuario no es bueno identificando términos útiles para la expansión de la consulta, cuando una interfaz de presentación de términos simple no puede proporcionar suficiente contexto semántico de los términos de retroalimentación. Nuestro trabajo difiere de los anteriores en dos aspectos importantes. Primero, al elegir los términos para presentar al usuario para la evaluación de relevancia, no solo consideramos el valor de un solo término (por ejemplo, la frecuencia relativa de un término en los documentos principales, que puede medirse mediante métricas como el Valor de Selección de Robertson y la Distancia Kullback-Leibler Simplificada como se detalla en [24]), sino que también examinamos la estructura de los grupos de términos, con el fin de producir una cobertura equilibrada de los diferentes aspectos del tema. Segundo, con el marco de modelado del lenguaje, permitimos una construcción detallada del modelo de consulta actualizado, estableciendo diferentes probabilidades para diferentes términos según si es un término de consulta, su importancia en los documentos principales y su pertenencia a un clúster. Aunque existen técnicas para ajustar los pesos de los términos de consulta en modelos de espacio vectorial y modelos de relevancia probabilística, la mayoría de los trabajos mencionados no las utilizan, optando por simplemente añadir términos de retroalimentación a la consulta original (usando pesos iguales para ellos), lo que puede llevar a un rendimiento de recuperación más pobre. La combinación de los dos aspectos permite que nuestro método funcione mucho mejor que el punto de referencia. La forma habitual de presentar los términos de retroalimentación es simplemente mostrar los términos en una lista. Ha habido algunos trabajos sobre interfaces de usuario alternativas. [8] organiza los términos en una jerarquía, y [11] compara tres interfaces diferentes, incluyendo términos + casillas de verificación, términos + contexto (oraciones) + casillas de verificación, oraciones + cuadro de texto de entrada. En ambos estudios, sin embargo, no hay una diferencia significativa en el rendimiento. En nuestro trabajo adoptamos el enfoque más simple de términos + casillas de verificación. Nos enfocamos en la presentación de términos y la construcción de modelos de consulta a partir de términos de retroalimentación, y creemos que el uso de contextos para mejorar la calidad de los términos de retroalimentación debería ser ortogonal a nuestro método. 3. Enfoque general: Seguimos el enfoque de modelado del lenguaje y basamos nuestro método en el modelo de recuperación de divergencia KL propuesto en [25]. Con este modelo, la tarea de recuperación implica estimar un modelo de lenguaje de consulta θq a partir de una consulta dada, un modelo de lenguaje de documento θd de cada documento, y calcular su divergencia KL D(θq||θd), que luego se utiliza para puntuar los documentos. [25] trata la retroalimentación de relevancia como un problema de reestimación del modelo de consulta, es decir, calcular un modelo de consulta actualizado θq dado el texto de la consulta original y la evidencia adicional proporcionada por los documentos relevantes juzgados. Adoptamos esta perspectiva y planteamos nuestra tarea como la actualización del modelo de consulta a partir de la retroalimentación de términos de usuario. Aquí hay dos tareas clave: Primero, cómo elegir los mejores términos para presentar al usuario para su evaluación, con el fin de recopilar la máxima evidencia sobre la necesidad de información del usuario. Segundo, cómo calcular un modelo de consulta actualizado basado en esta retroalimentación de términos, de manera que capture la necesidad de información de los usuarios y se traduzca en un buen rendimiento de recuperación. 4. SELECCIÓN DE TÉRMINOS DE PRESENTACIÓN La selección adecuada de los términos que se presentarán al usuario para su evaluación es crucial para el éxito de la retroalimentación de términos. Si los términos están mal elegidos y hay pocos relevantes, el usuario tendrá dificultades para buscar términos útiles que ayuden a aclarar su necesidad de información. Si los términos relevantes son abundantes, pero todos se centran en un solo aspecto del tema de la consulta, entonces solo podremos obtener retroalimentación sobre ese aspecto y perderemos otros, lo que resultará en una pérdida de amplitud en los resultados recuperados. Por lo tanto, es importante seleccionar cuidadosamente los términos de presentación para maximizar la ganancia esperada de la retroalimentación del usuario, es decir, aquellos que pueden revelar potencialmente la mayor evidencia de la necesidad de información del usuario. Esto es similar al feedback activo[21], que sugiere que un sistema de recuperación debería sondear activamente la necesidad de información de los usuarios, y en el caso del feedback de relevancia, los documentos de feedback deberían ser elegidos para maximizar los beneficios de aprendizaje (por ejemplo, de manera diversa para aumentar la cobertura). En nuestro enfoque, los N documentos principales de una recuperación inicial utilizando la consulta original forman la fuente de términos de retroalimentación: todos los términos que aparecen en ellos se consideran candidatos para presentar al usuario. Estos documentos sirven como pseudo-retroalimentación, ya que proporcionan un contexto mucho más amplio que la consulta original (generalmente muy breve), sin que se le pida al usuario que juzgue su relevancia. Debido a esta última razón, es posible hacer que N sea bastante grande (por ejemplo, en nuestros experimentos establecimos N = 60) para aumentar su cobertura de diferentes aspectos en el tema. La forma más sencilla de seleccionar términos de retroalimentación es elegir los M términos más frecuentes de los N documentos. Este método, sin embargo, tiene dos inconvenientes. Primero, se seleccionarán muchos términos ruidosos comunes debido a sus altas frecuencias en la colección de documentos, a menos que se utilice una lista de palabras vacías para filtrar. En segundo lugar, la lista de presentación tiende a estar compuesta por términos de los aspectos principales del tema; es probable que se pasen por alto aquellos de un aspecto menor debido a sus frecuencias relativamente bajas. Resolvemos los problemas anteriores mediante dos medidas correspondientes. Primero, introducimos un modelo de fondo θB que se estima a partir de estadísticas de la colección y explica los términos comunes, de modo que es mucho menos probable que aparezcan en la lista de presentación. Segundo, los términos se seleccionan de múltiples grupos en los documentos de retroalimentación simulada, para garantizar una representación suficiente de diferentes aspectos del tema. Nos basamos en el modelo multinomial de mezcla, que se utiliza para el descubrimiento de temas en [26]. Específicamente, asumimos que los N documentos contienen K grupos {Ci| i = 1, 2, · · · K}, cada uno caracterizado por una distribución de palabras multinomial (también conocida como modelo de lenguaje unigrama) θi y correspondiente a un aspecto del tema. Los documentos se consideran como muestreados de una mezcla de K + 1 componentes, incluidos los K grupos y el modelo de fondo: p(w|d) = λBp(w|θB) + (1 − λB) K i=1 πd,ip(w|θi) donde w es una palabra, λB es el peso de la mezcla para el modelo de fondo θB, y πd,i es el peso de la mezcla específico del documento para el modelo de cluster i-ésimo θi. Luego estimamos los modelos de clúster maximizando la probabilidad de que los documentos de pseudo-retroalimentación sean generados a partir del modelo de mezcla multinomial: log p(D|Λ) = d∈D w∈V c(w; d) log p(w|d) donde D = {di| i = 1, 2, · · · N} es el conjunto de los N documentos, V es el vocabulario, c(w; d) es la frecuencia de w en d y Λ = {θi| i = 1, 2, · · · K} ∪ {πdij | i = 1, 2, · · · N, j = 1, 2, · · · K} es el conjunto de parámetros del modelo a estimar. Los modelos de conglomerados pueden ser estimados eficientemente utilizando el algoritmo de Expectation-Maximization (EM). Para más detalles, remitimos al lector a [26]. La Tabla 1 muestra los modelos de clúster para la consulta de TREC sobre desastres en túneles de transporte (K = 3). Ten en cuenta que solo el grupo central es relevante. Tabla 1: Modelos de clúster para el tema 363 Desastres en túneles de transporte Clúster 1 Clúster 2 Clúster 3 túnel 0.0768 túnel 0.0935 túnel 0.0454 transporte 0.0364 fuego 0.0295 transporte 0.0406 tráfico 0.0206 camión 0.0236 peaje 0.0166 ferrocarril 0.0186 francés 0.0220 Amtrak 0.0153 puerto 0.0146 humo 0.0157 tren 0.0129 riel 0.0140 coche 0.0154 aeropuerto 0.0122 puente 0.0139 italiano 0.0152 autopista 0.0105 kilómetro 0.0136 incendio 0.0144 lui 0.0095 camión 0.0133 llamas 0.0127 Jersey 0.0093 construcción 0.0131 blanco 0.0121 paso 0.0087 · · · · · · · · · De cada uno de los K clústeres estimados, elegimos los L = M/K términos con las probabilidades más altas para formar un total de M términos de presentación. Si un término resulta estar en el top L en múltiples grupos, lo asignamos al grupo donde tenga la probabilidad más alta y permitimos que los otros grupos tomen un término adicional como compensación. También filtramos los términos en el texto de la consulta original porque tienden a ser siempre relevantes cuando la consulta es corta. Los términos seleccionados son luego presentados al usuario para su evaluación. Se muestra un formulario de retroalimentación (completado) de muestra en la Figura 1. En este estudio solo tratamos con juicios binarios: un término presentado está por defecto sin marcar, y un usuario puede marcarlo para indicar relevancia. Tampoco explotamos explícitamente la retroalimentación negativa (es decir, penalizar términos irrelevantes), porque con la retroalimentación binaria un término no verificado no es necesariamente irrelevante (quizás el usuario no está seguro de su relevancia). Podríamos pedir al usuario un juicio más detallado (por ejemplo, eligiendo entre altamente relevante, algo relevante, no sé, algo irrelevante y altamente irrelevante), pero la retroalimentación binaria es más compacta, ocupando menos espacio para mostrar y requiriendo menos esfuerzo por parte del usuario para emitir un juicio. 5. ESTIMACIÓN DE MODELOS DE CONSULTA A PARTIR DE RETROALIMENTACIÓN DE TÉRMINOS En esta sección, presentamos varios algoritmos para aprovechar la retroalimentación de términos. Los algoritmos toman como entrada la consulta original q, los grupos {θi} generados por el algoritmo de descubrimiento de temas, el conjunto de términos de retroalimentación T y su juicio de relevancia R, y producen un modelo de lenguaje de consulta actualizado θq que hace el mejor uso de la evidencia de retroalimentación para capturar la necesidad de información de los usuarios. Primero describimos nuestras notaciones: • θq: El modelo de consulta original, derivado solo de los términos de la consulta: p(w|θq) = c(w; q) |q| donde c(w; q) es la cantidad de veces que w aparece en q, y |q| = w∈q c(w; q) es la longitud de la consulta. • θq: El modelo de consulta actualizado que necesitamos estimar a partir de la retroalimentación de términos. • θi (i = 1, 2, . . . K): El modelo de lenguaje unigrama del clúster Ci, estimado utilizando el algoritmo de descubrimiento de temas. • T = {ti,j} (i = 1 . . . K, j = 1 . . . \n\nK, j = 1 . . . El conjunto de términos presentados al usuario para su evaluación. ti,j es el j-ésimo término elegido del grupo Ci. • R = {δw|w ∈ T}: δw es una variable indicadora que es 1 si w es considerado relevante o 0 en caso contrario. 5.1 TFB (Retroalimentación Directa de Términos) Esta es una forma directa de retroalimentación de términos que no implica ninguna estructura secundaria. Asignamos un peso de 1 a los términos considerados relevantes por el usuario, un peso de μ a los términos de la consulta, un peso de cero a los demás términos, y luego aplicamos la normalización: p(w|θq ) = δw + μ c(w; q) w ∈T δw + μ|q| donde w ∈T δw es el número total de términos considerados relevantes. Llamamos a este método TFB (retroalimentación directa de términos). Si dejamos que μ = 1, este enfoque es equivalente a agregar los términos relevantes después de la consulta original, que es lo que hace la expansión estándar de consultas (sin reponderación de términos). Si establecemos μ > 1, estamos poniendo más énfasis en los términos de consulta que en los verificados. Ten en cuenta que el modelo de resultado será más sesgado hacia θq si la consulta original es larga o el feedback del usuario es débil, lo cual tiene sentido, ya que podemos confiar más en la consulta original en ambos casos. Figura 1: Formulario de aclaración completado para el Tema 363 363 desastres en túneles de transporte. Por favor, seleccione todos los términos relevantes para el tema. tráfico ferrocarril puerto puente ferroviario kilómetro construir suizo enlace cruzado hongkonés río proyecto metro camión de bomberos francés humo coche italiano bomberos incendio blanco montaña víctima francés rescate conductor chamonix emerger peaje tren amtrak aeropuerto turnpike jersey pass rome z centro electrón carretera boston velocidad bu enviar 5.2 CFB (Retroalimentación de Clúster) Aquí explotamos la estructura de clúster que jugó un papel importante cuando seleccionamos los términos de presentación. Los grupos representan diferentes aspectos del tema de la consulta, cada uno de los cuales puede ser relevante o no. Si somos capaces de identificar los grupos relevantes, podemos combinarlos para generar un modelo de consulta que sea bueno para descubrir documentos pertenecientes a estos grupos (en lugar de los irrelevantes). Podríamos pedir al usuario que juzgue directamente la relevancia de un grupo después de ver los términos representativos en ese grupo, pero a veces esto sería una tarea difícil para el usuario, quien tendría que adivinar la semántica de un grupo a través de su conjunto de términos, los cuales podrían no estar bien conectados entre sí debido a la falta de contexto. Por lo tanto, proponemos aprender retroalimentación de clúster de forma indirecta, inferir la relevancia de un clúster a través de la relevancia de sus términos de retroalimentación. Dado que cada grupo tiene un número igual de términos presentados al usuario, la medida más simple de la relevancia de un grupo es el número de términos que se consideran relevantes en él. Intuitivamente, cuantos más términos estén marcados como relevantes en un grupo, más cerca estará el grupo del tema de la consulta y más debería participar en la modificación de la consulta. Si combinamos los modelos de clúster utilizando pesos determinados de esta manera y luego interpolamos con el modelo de consulta original, obtenemos la siguiente fórmula para la actualización de la consulta, que llamamos CFB (Retroalimentación de Clúster): p(w|θq) = λp(w|θq) + (1 − λ) K i=1 L j=1 δti,j K k=1 L j=1 δtk,j p(w|θi) donde L j=1 δti,j es el número de términos relevantes en el clúster Ci, y K k=1 L j=1 δtk,j es el número total de términos relevantes. Observamos que cuando solo hay un clúster (K = 1), la fórmula anterior se degrada a p(w|θq ) = λp(w|θq) + (1 − λ)p(w|θ1) que es simplemente una pseudo-retroalimentación de la forma propuesta en [25]. TCFB (Retroalimentación Término-Clúster) TFB y CFB ambos tienen sus inconvenientes. TFB asigna probabilidades no nulas a los términos presentados que están marcados como relevantes, pero ignora por completo (muchos más) otros, que pueden quedar sin marcar debido a la ignorancia de los usuarios, o simplemente no estar incluidos en la lista de presentación, pero deberíamos ser capaces de inferir su relevancia a partir de los marcados. Por ejemplo, en la Figura 1, dado que se revisan hasta 5 términos en el grupo central (las tercera y cuarta columnas), deberíamos tener una alta confianza en la relevancia de otros términos en ese grupo. CFB resuelve el problema de TFB tratando los términos en un grupo de manera colectiva, de modo que los términos no verificados/no presentados reciban pesos cuando los términos presentados en sus grupos son considerados relevantes, pero no distingue qué términos en un grupo son presentados o considerados. Intuitivamente, los términos considerados relevantes deberían recibir pesos mayores porque son indicados explícitamente como relevantes por el usuario. Por lo tanto, intentamos combinar los dos métodos, con la esperanza de obtener lo mejor de ambos. Lo hacemos interpolando el modelo TFB con el modelo CFB, y lo llamamos TCFB: p(w|θq ) = αp(w|θqT F B ) + (1 − α)p(w|θqCF B ) 6. EXPERIMENTOS En esta sección, describimos los resultados de nuestro experimento. Primero describimos nuestra configuración experimental y presentamos una visión general del rendimiento de varios métodos. Luego discutimos los efectos de variar la configuración de los parámetros en los algoritmos, así como el número de términos de presentación. A continuación analizamos el comportamiento de retroalimentación de los usuarios y su relación con el rendimiento de recuperación. Finalmente comparamos la retroalimentación de términos con la retroalimentación de relevancia y mostramos que tiene su ventaja particular. 6.1 Configuración del Experimento y Resultados Básicos Aprovechamos la oportunidad de la pista HARD de TREC 2005[2] para la evaluación de nuestros algoritmos. Las pistas utilizaron la colección AQUAINT, un corpus de 3GB de texto de noticias en inglés. Los temas incluyeron 50 que se sabía que eran difíciles, es decir, con bajo rendimiento de recuperación. Es para estos temas difíciles que la retroalimentación del usuario es más útil, ya que puede proporcionar información para desambiguar las consultas; con temas fáciles, es posible que el usuario no esté dispuesto a esforzarse por dar retroalimentación si los resultados de recuperación automática son lo suficientemente buenos. Los participantes de la pista pudieron enviar formularios de aclaración (CF) diseñados a medida para solicitar retroalimentación de los evaluadores humanos proporcionados por la Tabla 2: Rendimiento de recuperación para diferentes métodos y tipos de CF. La última fila es el porcentaje de mejora del MAP sobre la línea base. Los ajustes de parámetros μ = 4, λ = 0.1, α = 0.3 son casi óptimos. Variación de MAP con el número de términos presentados. # términos TFB1C TFB3C TFB6C CFB3C CFB6C TCFB3C TCFB6C 6 0.245 0.240 0.227 0.279 0.279 0.281 0.274 12 0.261 0.261 0.242 0.299 0.286 0.297 0.281 18 0.275 0.274 0.256 0.301 0.282 0.300 0.286 24 0.276 0.281 0.265 0.303 0.292 0.305 0.292 30 0.280 0.285 0.270 0.304 0.296 0.307 0.296 36 0.282 0.288 0.272 0.307 0.297 0.309 0.297 42 0.283 0.288 0.275 0.306 0.298 0.309 0.300 48 0.288 0.288 0.278 0.305 0.301 0.309 0.303 NIST. Diseñamos tres conjuntos de formularios de aclaración para la retroalimentación de términos, diferenciados en la elección de K, el número de grupos, y L, el número de términos presentados de cada grupo. Son: 1 × 48, un gran grupo con 48 términos, 3 × 16, 3 grupos con 16 términos cada uno, y 6 × 8, 6 grupos con 8 términos cada uno. El número total de términos presentados (M) está fijo en 48, por lo que al comparar el rendimiento de diferentes tipos de formas de clarificación podemos conocer los efectos de diferentes grados de agrupamiento. Para cada tema, un evaluador completaría los formularios en el orden de 6 × 8, 1 × 48 y 3 × 16, dedicando hasta tres minutos en cada formulario. El formulario de aclaración de muestra mostrado en la Figura 1 es de tipo 3 × 16. Es una interfaz simple y compacta en la que el usuario puede consultar términos relevantes. El formulario es autoexplicativo; no es necesario brindar capacitación adicional al usuario sobre cómo utilizarlo. Nuestras consultas iniciales se construyen solo utilizando las descripciones de los títulos de los temas, que tienen en promedio 2.7 palabras de longitud. Como base utilizamos el método de recuperación de divergencia KL implementado en la herramienta Lemur con 5 documentos de retroalimentación pseudo. Aplicamos el suavizado de Dirichlet con una prior de 2000, y truncamos los modelos de lenguaje de consulta a 50 términos (estas configuraciones se utilizan en todos los experimentos). Para todos los demás parámetros, utilizamos la configuración predeterminada de Lemurs. El punto de referencia resulta tener un rendimiento por encima del promedio entre los participantes de la pista. Después de una ejecución inicial utilizando este método de recuperación de referencia, tomamos los 60 documentos principales para cada tema y aplicamos el algoritmo de descubrimiento de temas para producir los grupos (1, 3 o 6 de ellos), en base a los cuales generamos formularios de aclaración. Después de recibir la retroalimentación del usuario, ejecutamos los algoritmos de retroalimentación de términos (TFB, CFB o TCFB) para estimar modelos de consulta actualizados, los cuales luego se utilizan para una segunda iteración de recuperación. Evaluamos el rendimiento de los diferentes métodos de recuperación en sus clasificaciones de los 1000 documentos principales. Las métricas de evaluación que adoptamos incluyen la precisión media promedio (no interpolada) (MAP), precisión en los primeros 30 (Pr@30) y total relevante recuperado (RR). La Tabla 2 muestra el rendimiento de varios métodos y configuraciones de K × L. Los sufijos (1C, 3C, 6C) después de TFB, CFB, TCFB representan el número de grupos (K). Por ejemplo, TCFB3C significa el método TCFB en los formularios de aclaración de 3 × 16. De la Tabla 2 podemos hacer las siguientes observaciones: 1 http://www.lemurproject.com 1. Todos los métodos tienen un rendimiento considerablemente mejor que la línea base de pseudoretroalimentación, con TCFB3C logrando una mejora máxima del 41.1% en el MAP, lo que indica una contribución significativa de la retroalimentación de términos para la clarificación de la necesidad de información de los usuarios. En otras palabras, el feedback a término es realmente útil para mejorar la precisión de recuperación. 2. Para TFB, el rendimiento es casi igual en los formularios de aclaración de 1 × 48 y 3 × 16 en términos de MAP (aunque este último es ligeramente mejor en Pr@30 y RR), y un poco peor en los de 6 × 8. 3. Tanto CFB3C como CFB6C tienen un mejor rendimiento que sus contrapartes TFB en las tres métricas, lo que sugiere que el feedback sobre una estructura de clúster secundaria es realmente beneficioso. CFB1C es en realidad peor porque no puede ajustar el peso de su (único) grupo a partir de la retroalimentación de términos y es simplemente pseudoretroalimentación. 4. Aunque TCFB es solo una simple mezcla de TFB y CFB por interpolación, logra superar a ambos. Esto respalda nuestra especulación de que TCFB supera las desventajas de TFB (prestando atención solo a los términos marcados) y CFB (sin distinguir los términos marcados y no marcados en un grupo). Excepto por TCFB6C v.s. CFB6C, la ventaja de rendimiento de TCFB sobre TFB/CFB es significativa con p < 0.05 utilizando la prueba de rango con signo de Wilcoxon. Esto no es cierto en el caso de TFB v.s. CFB, cada uno de los cuales es mejor que el otro en casi la mitad de los temas. 6.2 Reducción de Términos de Presentación En algunas situaciones podemos tener que reducir el número de términos de presentación debido a limitaciones en el espacio de visualización o esfuerzos de retroalimentación del usuario. Es interesante saber si el rendimiento de nuestros algoritmos se deteriora cuando al usuario se le presentan menos términos. Dado que los términos de presentación dentro de cada grupo se generan en orden decreciente de sus frecuencias, la lista de presentación forma un subconjunto del original si su tamaño se reduce2. Por lo tanto, podemos simular fácilmente lo que sucede cuando el número de términos de presentación disminuye 2. Hay complejidades que surgen de los términos que aparecen en la parte superior L de múltiples grupos, pero estas son excepciones de M a M: mantendremos todos los juicios de los términos superiores L = M / K en cada grupo y descartaremos los de los demás. La Tabla 3 muestra el rendimiento de varios algoritmos a medida que el número de términos de presentación varía de 6 a 48. Observamos que el rendimiento de TFB es más susceptible a la reducción de términos de presentación que el de CFB o TCFB. Por ejemplo, en 12 términos el MAP de TFB3C es el 90.6% del que se obtiene en 48 términos, mientras que los números para CFB3C y TCFB3C son del 98.0% y 96.1% respectivamente. Conjeturamos que la razón es que mientras el rendimiento de TFB depende en gran medida de cuántos términos buenos se elijan para la expansión de la consulta, CFB solo necesita una estimación aproximada de los pesos de los clústeres para funcionar. Además, las formas de aclaración de 3 × 16 parecen ser más robustas que las de 6 × 8: con 12 términos, el MAP de TFB6C es el 87.1% del obtenido con 48 términos, por debajo del 90.6% de TFB3C. De manera similar, para CFB es del 95.0% en comparación con el 98.0%. Esto es natural, ya que para un gran número de 6 clusters, es más fácil llegar a la situación en la que cada cluster recibe muy pocos términos de presentación para que la diversificación de temas sea útil. En general, nos sorprende ver que los algoritmos aún pueden funcionar razonablemente bien cuando el número de términos de presentación es pequeño. Por ejemplo, con solo 12 términos, CFB3C (el formulario de aclaración es de tamaño 3 × 4) aún puede mejorar un 36.5% sobre el valor base, disminuyendo ligeramente desde el 39.3% con 48 términos. En el Análisis de Retroalimentación del Usuario, estudiamos varios aspectos del comportamiento de retroalimentación de los usuarios en términos, y si están conectados al rendimiento de recuperación. Figura 2: Distribuciones de tiempo de completación del formulario de aclaración 0−30 30−60 60−90 90−120 120−150 150−180 0 5 10 15 20 25 30 35 tiempo de completación (segundos) #temas 1×48 3×16 6×8 La Figura 2 muestra la distribución del tiempo necesario para completar un formulario de aclaración. Observamos que el usuario suele ser capaz de completar el feedback del término en un tiempo razonablemente corto: para más de la mitad de los temas, el formulario de aclaración se completa en solo 1 minuto, y solo una pequeña fracción de temas (menos del 10% para 1 × 48 y 3 × 16) tarda más de 2 minutos. Esto sugiere que la retroalimentación a corto plazo es adecuada para la recuperación interactiva ad-hoc, donde un usuario generalmente no desea dedicar demasiado tiempo a proporcionar retroalimentación. Observamos que un usuario a menudo comete errores al juzgar la relevancia de un término. A veces, un término relevante puede ser omitido porque su conexión con el tema de la consulta no es obvia para el usuario. En otras ocasiones, puede incluirse un término dudoso que resulta ser irrelevante. Toma como ejemplo el tema en la Figura 1. Hubo un desastre de incendio en Mont 3. El tiempo máximo es de 180 segundos, ya que el evaluador del NIST estaría obligado a enviar el formulario en ese momento. Tabla 4: Estadísticas de selección de términos (promedio del tema) CF Tipo 1 × 48 3 × 16 6 × 8 # términos revisados 14.8 13.3 11.2 # términos relevantes 15.0 12.6 11.2 # términos relevantes revisados 7.9 6.9 5.9 precisión 0.534 0.519 0.527 recuperación 0.526 0.548 0.527 El usuario no seleccionó palabras clave como mont, blanc, francés e italiano debido a su desconocimiento del evento del Túnel del Mont Blanc entre Francia e Italia en 1999. De hecho, sin un contexto adecuado sería difícil hacer un juicio perfecto. ¿Cuál es entonces, el nivel en el que el usuario es bueno dando retroalimentación? ¿Tiene un impacto serio en el rendimiento de recuperación? Para responder a estas preguntas, necesitamos una medida de la verdadera relevancia de los términos individuales. Adoptamos la métrica de Divergencia KL Simplificada utilizada en [24] para decidir los términos de expansión de la consulta como nuestra medida de relevancia de términos: σKLD(w) = p(w|R) log p(w|R) p(w|¬R) donde p(w|R) es la probabilidad de que un documento relevante contenga el término w, y p(w|¬R) es la probabilidad de que un documento irrelevante contenga w, ambas pueden ser fácilmente calculadas mediante una estimación de máxima verosimilitud dada la relevancia a nivel de documento. Si σKLD(w) > 0, w es más probable que aparezca en documentos relevantes que en irrelevantes. Consideramos un término relevante si su valor de Divergencia KL Simplificada es mayor que un umbral determinado σ0. Podemos entonces definir la precisión y la exhaustividad del juicio de términos del usuario de la siguiente manera: la precisión es la fracción de términos revisados por el usuario que son relevantes; la exhaustividad es la fracción de términos relevantes presentados que son revisados por el usuario. La Tabla 4 muestra el número de términos revisados, términos relevantes y términos relevantes revisados cuando σ0 se establece en 1.0, así como la precisión/recuperación del juicio de términos del usuario. Ten en cuenta que cuando los formularios de aclaración contienen más grupos, se revisan menos términos: 14.8 para 1 × 48, 13.3 para 3 × 16 y 11.2 para 6×8. El patrón similar se mantiene para los términos relevantes y los términos relevantes verificados. Parece haber un compromiso entre aumentar la diversidad de temas mediante el agrupamiento y perder términos adicionales relevantes: cuando hay más grupos, cada uno recibe menos términos para presentar, lo que puede perjudicar a un grupo relevante importante que contiene muchos términos relevantes. Por lo tanto, no siempre es útil tener más grupos, por ejemplo, TFB6C es en realidad peor que TFB1C. El hallazgo principal que podemos obtener de la Tabla 4 es que el usuario no es particularmente bueno identificando términos relevantes, lo cual coincide con el descubrimiento en [18]. En el caso de 3 formularios de aclaración de 16, el número promedio de términos marcados como relevantes por el usuario es de 13.3 por tema, y el número promedio de términos relevantes cuyo valor de σKLD excede 1.0 es de 12.6. El usuario es capaz de reconocer solo 6.9 de estos términos en promedio. De hecho, la precisión y la exhaustividad de los términos de retroalimentación de los usuarios (como se definió anteriormente) están lejos de ser perfectas. Por otro lado, si el usuario hubiera verificado correctamente todos esos términos relevantes, el rendimiento de nuestros algoritmos habría aumentado considerablemente, como se muestra en la Tabla 5. Vemos que TFB experimenta una gran mejora cuando hay un oráculo que verifica todos los términos relevantes, mientras que CFB se encuentra con un cuello de botella alrededor de un MAP de 0.325, ya que todo lo que hace es ajustar los pesos de los clústeres, y cuando los pesos aprendidos están cerca de ser precisos, no puede beneficiarse más del feedback de términos. También hay que tener en cuenta que TCFB no logra superar a TFB, probablemente porque TFB es lo suficientemente preciso. Tabla 5: Cambio de MAP al usar todos (y solo) los términos relevantes (σKLD > 1.0) para retroalimentación. retroalimentación de término original retroalimentación de término relevante TF1 0.288 0.354 TF3 0.288 0.354 TF6 0.278 0.346 CF3 0.305 0.325 CF6 0.301 0.326 TCF3 0.309 0.345 TCF6 0.304 0.341 6.4 Comparación con Retroalimentación de Relevancia Ahora comparamos la retroalimentación de términos con la retroalimentación de relevancia a nivel de documento, en la que al usuario se le presentan los N mejores documentos de una recuperación inicial y se le pide que juzgue su relevancia. El proceso de retroalimentación se simula utilizando la evaluación de relevancia de documentos de NIST. Utilizamos el método de retroalimentación basado en el modelo de mezcla propuesto en [25], con el ruido de mezcla establecido en 0.95 y el coeficiente de retroalimentación establecido en 0.9. La evaluación comparativa del feedback de relevancia frente a otros métodos se complica por el hecho de que algunos documentos ya han sido vistos durante el feedback, por lo que no tiene sentido incluirlos en los resultados de recuperación de la segunda ejecución. Sin embargo, esto no se aplica al término retroalimentación. Por lo tanto, para que sea justo en cuanto a la ganancia de información de los usuarios, si los documentos de retroalimentación son relevantes, deben mantenerse en la parte superior del ranking; si son irrelevantes, deben ser excluidos. Por lo tanto, utilizamos retroalimentación de relevancia para producir un ranking de los 1000 documentos recuperados, pero excluyendo cada documento de retroalimentación, y luego añadimos los documentos relevantes de retroalimentación al principio. La tabla 6 muestra el rendimiento de la retroalimentación de relevancia para diferentes valores de N y lo compara con TCFB3C. Tabla 6: Rendimiento de la retroalimentación de relevancia para diferentes números de documentos de retroalimentación (N). Vemos que el rendimiento de TCFB3C es comparable al de la retroalimentación de relevancia utilizando 5 documentos. Aunque es menos eficiente que cuando hay 10 documentos de retroalimentación en términos de MAP y Pr@30, sí recupera más documentos (4947) al descender por la lista clasificada. Intentamos comparar la calidad de los términos insertados automáticamente en la retroalimentación de relevancia con la de los términos seleccionados manualmente en la retroalimentación de términos. Esto se logra truncando el modelo de consulta modificado por retroalimentación de relevancia a un tamaño igual al número de términos verificados para el mismo tema. Podemos comparar los términos en el modelo truncado con los términos verificados. La Figura 3 muestra la distribución de los puntajes σKLD de los términos. Observamos que el término \"feedback\" tiende a producir términos de expansión de mayor calidad (aquellos con σKLD > 1) en comparación con el \"relevance feedback\" (con 10 documentos de feedback). Esto no contradice el hecho de que este último produce un rendimiento de recuperación más alto. De hecho, cuando usamos el modelo de consulta truncada en lugar del modelo íntegro refinado a partir de la retroalimentación de relevancia, el MAP es solo 0.304. La verdad Figura 3: Comparación de la calidad del término de expansión entre retroalimentación de relevancia (con 10 documentos de retroalimentación) y retroalimentación de términos (con 3 × 16 CFs) −1−0 0−1 1−2 2−3 3−4 4−5 5−6 0 50 100 150 200 250 300 350 σKLD #términos retroalimentación de relevancia retroalimentación de términos es que, aunque hay muchos términos no deseados en el modelo de consulta expandido a partir de los documentos de retroalimentación, también hay más términos relevantes de los que el usuario puede seleccionar de la lista de términos de presentación generados con documentos de pseudo-retroalimentación, y los efectos positivos a menudo superan a los negativos. Estamos interesados en saber en qué circunstancias la retroalimentación basada en términos tiene ventajas sobre la retroalimentación basada en relevancia. Una de esas situaciones es cuando ninguno de los documentos de retroalimentación principales es relevante, lo que hace que la retroalimentación de relevancia sea inútil. Esto no es infrecuente, como uno podría haber pensado: de los 50 temas, hay 13 casos así cuando N = 5, 10 cuando N = 10, y aún 3 cuando N = 20. Cuando esto sucede, solo se puede retroceder al método original de recuperación; se pierde el poder de la retroalimentación de relevancia. Sorprendentemente, en 11 de 13 casos en los que parece imposible el feedback de relevancia, el usuario puede marcar al menos 2 términos relevantes de los 3 × 16 del formulario de aclaración (consideramos que el término t es relevante si σKLD(t) > 1.0). Además, en 10 de ellos TCFB3C supera la línea base de pseudo-retroalimentación, aumentando el MAP de 0.076 a 0.146 en promedio (estos son temas particularmente difíciles). Creemos que hay dos posibles explicaciones para este fenómeno de que la retroalimentación de términos esté activa incluso cuando la retroalimentación de relevancia no funciona: Primero, incluso si ninguno de los primeros N (supongamos que es un número pequeño) documentos son relevantes, aún podemos encontrar documentos relevantes en los primeros 60, que son más inclusivos pero generalmente inalcanzables cuando las personas están realizando retroalimentación de relevancia en la búsqueda interactiva ad-hoc, de donde podemos extraer términos de retroalimentación. Esto es cierto para el tema 367 de piratería, donde los 10 documentos principales de retroalimentación son todos sobre piratería de software, sin embargo, hay documentos entre el 10 y el 60 que tratan sobre piratería en los mares (que es la necesidad de información real), contribuyendo términos como pirata, barco para la selección en el formulario de aclaración. Segundo, para algunos temas, un documento debe cumplir con ciertas condiciones especiales para ser relevante. Los N documentos principales pueden estar relacionados con el tema, pero no obstante ser irrelevantes. En este caso, aún podemos extraer términos útiles de estos documentos, incluso si no califican como relevantes. Por ejemplo, en el tema 639 de compras en línea de consumidores, un documento necesita mencionar qué contribuye al crecimiento de las compras para realmente coincidir con la necesidad de información especificada, por lo tanto, ninguno de los 10 documentos de retroalimentación principales se considera relevante. Sin embargo, los términos de retroalimentación como venta al por menor, comercio son buenos para la expansión de consultas. 7. CONCLUSIONES En este artículo estudiamos el uso de la retroalimentación de términos para la recuperación interactiva de información en el enfoque de modelado del lenguaje. Propusimos un método basado en clusters para seleccionar términos de presentación, así como algoritmos para estimar modelos de consulta refinados a partir de la retroalimentación de términos de usuario. Observamos una mejora significativa en la precisión de recuperación proporcionada por la retroalimentación de términos, a pesar de que un usuario a menudo comete errores en la evaluación de relevancia que perjudican su rendimiento. Encontramos que el algoritmo de mejor rendimiento es TCFB, el cual se beneficia de la combinación de la evidencia de términos observados directamente con TFB y la relevancia de clúster aprendida indirectamente con CFB. Cuando redujimos el número de términos de presentación, el feedback del término aún puede mantener gran parte de su mejora de rendimiento sobre el valor base. Finalmente, comparamos la retroalimentación de términos con la retroalimentación de relevancia a nivel de documento, y encontramos que el rendimiento de TCFB3C está a la par con este último con 5 documentos de retroalimentación. Consideramos el término \"feedback\" como una alternativa viable al feedback de relevancia tradicional, especialmente cuando no hay documentos relevantes en la parte superior. Proponemos ampliar nuestro trabajo de varias maneras. Primero, queremos estudiar si el uso de varios contextos puede ayudar al usuario a identificar mejor la relevancia de los términos, sin sacrificar la simplicidad y la concisión de la retroalimentación de los términos. Segundo, actualmente todos los términos se presentan al usuario en un solo lote. En su lugar, podríamos considerar la retroalimentación iterativa de términos, presentando primero un pequeño número de términos y mostrando más términos después de recibir la retroalimentación del usuario o detenernos cuando la consulta refinada sea lo suficientemente buena. Los términos presentados deben ser seleccionados dinámicamente para maximizar los beneficios de aprendizaje en cualquier momento. Tercero, tenemos planes de incorporar retroalimentación de términos en nuestra barra de herramientas UCAIR[20], un complemento de Internet Explorer, para que funcione en búsquedas web. También estamos interesados en estudiar cómo combinar la retroalimentación de términos con la retroalimentación de relevancia o la retroalimentación implícita. Por ejemplo, podríamos permitir al usuario modificar dinámicamente los términos en un modelo de lenguaje aprendido a partir de documentos de retroalimentación. 8. AGRADECIMIENTO Este trabajo cuenta con el apoyo parcial de las becas de la Fundación Nacional de Ciencias IIS-0347933 e IIS-0428472. 9. REFERENCIAS [1] J. Allan. Retroalimentación de relevancia con demasiados datos. En Actas de la 18ª conferencia internacional anual de ACM SIGIR sobre investigación y desarrollo en recuperación de información, páginas 337-343, 1995. [2] J. Allan. Resumen de la pista HARD en TREC 2005 - Recuperación de alta precisión de documentos. En la Decimocuarta Conferencia de Recuperación de Información de 2005. [3] P. Anick. Utilizando retroalimentación terminológica para refinar la búsqueda web: un estudio basado en registros. En Actas de la 26ª conferencia internacional anual de ACM SIGIR sobre investigación y desarrollo en recuperación de información, páginas 88-95, 2003. [4] P. G. Anick y S. Tipirneni. El asistente de búsqueda de paráfrasis: retroalimentación terminológica para la búsqueda iterativa de información. En Actas de la 22ª conferencia internacional anual de ACM SIGIR sobre investigación y desarrollo en recuperación de información, páginas 153-159, 1999. [5] C. Buckley, G. Salton, J. Allan y A. Singhal. Expansión automática de consultas utilizando SMART. En Actas de la Tercera Conferencia de Recuperación de Texto, 1994. [6] D. Harman. Hacia la expansión interactiva de consultas. En Actas de la 11ª conferencia internacional anual de ACM SIGIR sobre investigación y desarrollo en recuperación de información, páginas 321-331, 1988. [7] N. A. Jaleel, A. Corrada-Emmanuel, Q. Li, X. Liu, C. Wade y J. Allan. UMass en TREC 2003: HARD y QA. En TREC, páginas 715-725, 2003. [8] H. Joho, C. Coverson, M. Sanderson y M. Beaulieu. Presentación jerárquica de términos de expansión. En Actas del simposio de informática aplicada de ACM de 2002, páginas 645-649, 2002. [9] K. S. Jones, S. Walker y S. E. Robertson. Un modelo probabilístico de recuperación de información: desarrollo y estado actual. Informe técnico 446, Laboratorio de Computación, Universidad de Cambridge, 1998. [10] D. Kelly, V. D. Dollu y X. Fu. El usuario locuaz: una fuente de términos independiente del documento para la expansión de consultas. En Actas de la 28ª conferencia internacional anual de ACM SIGIR sobre investigación y desarrollo en recuperación de información, páginas 457-464, 2005. [11] D. Kelly y X. Fu. Obtención de retroalimentación de relevancia de términos: una investigación sobre la fuente y el contexto de los términos. En Actas de la 29ª conferencia internacional anual de ACM SIGIR sobre investigación y desarrollo en recuperación de información, 2006. [12] J. Koenemann y N. Belkin. Un caso para la interacción: Un estudio del comportamiento y la efectividad de la recuperación de información interactiva. En Actas de la conferencia SIGCHI sobre factores humanos en sistemas informáticos, páginas 205-212, 1996. [13] V. Lavrenko y W. B. Croft. Modelos de lenguaje basados en relevancia. En Investigación y Desarrollo en Recuperación de Información, páginas 120-127, 2001. [14] Y. Nemeth, B. Shapira y M. Taeib-Maimon. Evaluación del valor real y percibido de la expansión automática e interactiva de consultas. En Actas de la 27ª conferencia internacional anual de ACM SIGIR sobre investigación y desarrollo en recuperación de información, páginas 526-527, 2004. [15] J. Ponte. Un enfoque de modelado del lenguaje para la recuperación de información. Tesis doctoral, Universidad de Massachusetts en Amherst, 1998. [16] S. E. Robertson, S. Walker, S. Jones, M. Beaulieu y M. Gatford. Okapi en TREC-3. En Actas de la Tercera Conferencia de Recuperación de Texto, 1994. [17] J. Rocchio. Retroalimentación de relevancia en la recuperación de información. En el sistema de recuperación SMART, páginas 313-323. 1971. [18] I. Ruthven. Reexaminando la efectividad potencial de la expansión interactiva de consultas. En Actas de la 26ª conferencia internacional anual de ACM SIGIR sobre investigación y desarrollo en recuperación de información, páginas 213-220, 2003. [19] G. Salton y C. Buckley. Mejorando el rendimiento de recuperación mediante retroalimentación de relevancia. Revista de la Sociedad Americana de Ciencia de la Información, 41:288-297, 1990. [20] X. Shen, B. Tan y C. Zhai. Modelado implícito de usuario para búsqueda personalizada. En Actas de la 14ª conferencia internacional de ACM sobre información y gestión del conocimiento, páginas 824-831, 2005. [21] X. Shen y C. Zhai. Retroalimentación activa en la recuperación de información ad-hoc. En Actas de la 28ª conferencia internacional anual de ACM SIGIR sobre investigación y desarrollo en recuperación de información, páginas 59-66, 2005. [22] A. Spink. Retroalimentación de relevancia de términos y expansión de consultas: relación con el diseño. En Actas de la 17ª conferencia internacional anual de ACM SIGIR sobre investigación y desarrollo en recuperación de información, páginas 81-90, 1994. [23] J. Xu y W. B. Croft. Expansión de consulta utilizando análisis local y global de documentos. En Actas de la 19ª conferencia internacional anual de ACM SIGIR sobre investigación y desarrollo en recuperación de información, páginas 4-11, 1996. [24] H. Zaragoza, N. Craswell, M. Taylor, S. Saria y S. Robertson. Microsoft Cambridge en TREC-13: pistas Web y HARD. En Actas de la 13ª Conferencia de Recuperación de Información de Texto, 2004. [25] C. Zhai y J. Lafferty. Retroalimentación basada en modelos en el enfoque de modelado del lenguaje para la recuperación de información. En Actas de la décima conferencia internacional sobre gestión de la información y el conocimiento, páginas 403-410, 2001. [26] C. Zhai, A. Velivelli y B. Yu. Un modelo de mezcla de colecciones cruzadas para la minería de textos comparativa. En Actas de la décima conferencia internacional de ACM SIGKDD sobre descubrimiento de conocimiento y minería de datos, páginas 743-748, 2004. ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "query model": {
            "translated_key": "modelo de consulta",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Term Feedback for Information Retrieval with Language Models Bin Tan† , Atulya Velivelli‡ , Hui Fang† , ChengXiang Zhai† Dept.",
                "of Computer Science† , Dept. of Electrical and Computer Engineering‡ University of Illinois at Urbana-Champaign bintan@cs.uiuc.edu, velivell@ifp.uiuc.edu, hfang@cs.uiuc.edu, czhai@cs.uiuc.edu ABSTRACT In this paper we study term-based feedback for information retrieval in the language modeling approach.",
                "With term feedback a user directly judges the relevance of individual terms without interaction with feedback documents, taking full control of the query expansion process.",
                "We propose a cluster-based method for selecting terms to present to the user for judgment, as well as effective algorithms for constructing refined query language models from user term feedback.",
                "Our algorithms are shown to bring significant improvement in retrieval accuracy over a non-feedback baseline, and achieve comparable performance to relevance feedback.",
                "They are helpful even when there are no relevant documents in the top.",
                "Categories and Subject Descriptors H.3.3 [Information Search and Retrieval]: Retrieval models General Terms Algorithms 1.",
                "INTRODUCTION In the language modeling approach to information retrieval, feedback is often modeled as estimating an improved <br>query model</br> or relevance model based on a set of feedback documents [25, 13].",
                "This is in line with the traditional way of doing relevance feedback - presenting a user with documents/passages for relevance judgment and then extracting terms from the judged documents or passages to expand the initial query.",
                "It is an indirect way of seeking users assistance for <br>query model</br> construction, in the sense that the refined <br>query model</br> (based on terms) is learned through feedback documents/passages, which are high-level structures of terms.",
                "It has the disadvantage that irrelevant terms, which occur along with relevant ones in the judged content, may be erroneously used for query expansion, causing undesired effects.",
                "For example, for the TREC query Hubble telescope achievements, when a relevant document talks more about the telescopes repair than its discoveries, irrelevant terms such as spacewalk can be added into the modified query.",
                "We can consider a more direct way to involve a user in <br>query model</br> improvement, without an intermediary step of document feedback that can introduce noise.",
                "The idea is to present a (reasonable) number of individual terms to the user and ask him/her to judge the relevance of each term or directly specify their probabilities in the <br>query model</br>.",
                "This strategy has been discussed in [15], but to our knowledge, it has not been seriously studied in existing language modeling literature.",
                "Compared to traditional relevance feedback, this term-based approach to interactive <br>query model</br> refinement has several advantages.",
                "First, the user has better control of the final <br>query model</br> through direct manipulation of terms: he/she can dictate which terms are relevant, irrelevant, and possibly, to what degree.",
                "This avoids the risk of bringing unwanted terms into the <br>query model</br>, although sometimes the user introduces low-quality terms.",
                "Second, because a term takes less time to judge than a documents full text or summary, and as few as around 20 presented terms can bring significant improvement in retrieval performance (as we will show later), term feedback makes it faster to gather user feedback.",
                "This is especially helpful for interactive adhoc search.",
                "Third, sometimes there are no relevant documents in the top N of the initially retrieved results if the topic is hard.",
                "This is often true when N is constrained to be small, which arises from the fact that the user is unwilling to judge too many documents.",
                "In this case, relevance feedback is useless, as no relevant document can be leveraged on, but term feedback is still often helpful, by allowing relevant terms to be picked from irrelevant documents.",
                "During our participation in the TREC 2005 HARD Track and continued study afterward, we explored how to exploit term feedback from the user to construct improved query models for information retrieval in the language modeling approach.",
                "We identified two key subtasks of term-based feedback, i.e., pre-feedback presentation term selection and post-feedback <br>query model</br> construction, with effective algorithms developed for both.",
                "We imposed a secondary cluster structure on terms and found that a cluster view sheds additional insight into the users information need, and provides a good way of utilizing term feedback.",
                "Through experiments we found that term feedback improves significantly over the nonfeedback baseline, even though the user often makes mistakes in relevance judgment.",
                "Among our algorithms, the one with best retrieval performance is TCFB, the combination of TFB, the direct term feedback algorithm, and CFB, the cluster-based feedback algorithm.",
                "We also varied the number of feedback terms and observed reasonable improvement even at low numbers.",
                "Finally, by comparing term feedback with document-level feedback, we found it to be a viable alternative to the latter with competitive retrieval performance.",
                "The rest of the paper is organized as follows.",
                "Section 2 discusses some related work.",
                "Section 4 outlines our general approach to term feedback.",
                "We present our method for presentation term selection in Section 3 and algorithms for <br>query model</br> construction in Section 5.",
                "The experiment results are given in Section 6.",
                "Section 7 concludes this paper. 2.",
                "RELATED WORK Relevance feedback[17, 19] has long been recognized as an effective method for improving retrieval performance.",
                "Normally, the top N documents retrieved using the original query are presented to the user for judgment, after which terms are extracted from the judged relevant documents, weighted by their potential of attracting more relevant documents, and added into the <br>query model</br>.",
                "The expanded query usually represents the users information need better than the original one, which is often just a short keyword query.",
                "A second iteration of retrieval using this modified query usually produces significant increase in retrieval accuracy.",
                "In cases where true relevance judgment is unavailable and all top N documents are assumed to be relevant, it is called blind or pseudo feedback[5, 16] and usually still brings performance improvement.",
                "Because document is a large text unit, when it is used for relevance feedback many irrelevant terms can be introduced into the feedback process.",
                "To overcome this, passage feedback is proposed and shown to improve feedback performance[1, 23].",
                "A more direct solution is to ask the user for their relevance judgment of feedback terms.",
                "For example, in some relevance feedback systems such as [12], there is an interaction step that allows the user to add or remove expansion terms after they are automatically extracted from relevant documents.",
                "This is categorized as interactive query expansion, where the original query is augmented with user-provided terms, which can come from direct user input (free-form text or keywords)[22, 7, 10] or user selection of system-suggested terms (using thesauri[6, 22] or extracted from feedback documents[6, 22, 12, 4, 7]).",
                "In many cases term relevance feedback has been found to effectively improve retrieval performance[6, 22, 12, 4, 10].",
                "For example, the study in [12] shows that the user prefers to have explicit knowledge and direct control of which terms are used for query expansion, and the penetrable interface that provides this freedom is shown to perform better than other interfaces.",
                "However, in some other cases there is no significant benefit[3, 14], even if the user likes interacting with expansion terms.",
                "In a simulated study carried out in [18], the author compares the retrieval performance of interactive query expansion and automatic query expansion with a simulated study, and suggests that the potential benefits of the former can be hard to achieve.",
                "The user is found to be not good at identifying useful terms for query expansion, when a simple term presentation interface is unable to provide sufficient semantic context of the feedback terms.",
                "Our work differs from the previous ones in two important aspects.",
                "First, when we choose terms to present to the user for relevance judgment, we not only consider single-term value (e.g., the relative frequency of a term in the top documents, which can be measured by metrics such as Robertson Selection Value and Simplified Kullback-Leibler Distance as listed in [24]), but also examine the cluster structure of the terms, so as to produce a balanced coverage of the different topic aspects.",
                "Second, with the language modelling framework, we allow an elaborate construction of the updated <br>query model</br>, by setting different probabilities for different terms based on whether it is a query term, its significance in the top documents, and its cluster membership.",
                "Although techniques for adjusting query term weights exist for vector space models[17] and probablistic relevance models[9], most of the aforementioned works do not use them, choosing to just append feedback terms to the original query (thus using equal weights for them), which can lead to poorer retrieval performance.",
                "The combination of the two aspects allows our method to perform much better than the baseline.",
                "The usual way for feedback term presentation is just to display the terms in a list.",
                "There have been some works on alternative user interfaces. [8] arranges terms in a hierarchy, and [11] compares three different interfaces, including terms + checkboxes, terms + context (sentences) + checkboxes, sentences + input text box.",
                "In both studies, however, there is no significant performance difference.",
                "In our work we adopt the simplest approach of terms + checkboxes.",
                "We focus on term presentation and <br>query model</br> construction from feedback terms, and believe using contexts to improve feedback term quality should be orthogonal to our method. 3.",
                "GENERAL APPROACH We follow the language modeling approach, and base our method on the KL-divergence retrieval model proposed in [25].",
                "With this model, the retrieval task involves estimating a query language model θq from a given query, a document language model θd from each document, and calculating their KL-divergence D(θq||θd), which is then used to score the documents. [25] treats relevance feedback as a <br>query model</br> re-estimation problem, i.e., computing an updated <br>query model</br> θq given the original query text and the extra evidence carried by the judged relevant documents.",
                "We adopt this view, and cast our task as updating the <br>query model</br> from user term feedback.",
                "There are two key subtasks here: First, how to choose the best terms to present to the user for judgment, in order to gather maximal evidence about the users information need.",
                "Second, how to compute an updated <br>query model</br> based on this term feedback evidence, so that it captures the users information need and translates into good retrieval performance. 4.",
                "PRESENTATION TERM SELECTION Proper selection of terms to be presented to the user for judgment is crucial to the success of term feedback.",
                "If the terms are poorly chosen and there are few relevant ones, the user will have a hard time looking for useful terms to help clarify his/her information need.",
                "If the relevant terms are plentiful, but all concentrate on a single aspect of the query topic, then we will only be able to get feedback on that aspect and missing others, resulting in a breadth loss in retrieved results.",
                "Therefore, it is important to carefully select presentation terms to maximize expected gain from user feedback, i.e., those that can potentially reveal most evidence of the users information need.",
                "This is similar to active feedback[21], which suggests that a retrieval system should actively probe the users information need, and in the case of relevance feedback, the feedback documents should be chosen to maximize learning benefits (e.g. diversely so as to increase coverage).",
                "In our approach, the top N documents from an initial retrieval using the original query form the source of feedback terms: all terms that appear in them are considered candidates to present to the user.",
                "These documents serve as pseudo-feedback, since they provide a much richer context than the original query (usually very short), while the user is not asked to judge their relevance.",
                "Due to the latter reason, it is possible to make N quite large (e.g., in our experiments we set N = 60) to increase its coverage of different aspects in the topic.",
                "The simplest way of selecting feedback terms is to choose the most frequent M terms from the N documents.",
                "This method, however, has two drawbacks.",
                "First, a lot of common noisy terms will be selected due to their high frequencies in the document collection, unless a stop-word list is used for filtering.",
                "Second, the presentation list will tend to be filled by terms from major aspects of the topic; those from a minor aspect are likely to be missed due to their relatively low frequencies.",
                "We solve the above problems by two corresponding measures.",
                "First, we introduce a background model θB that is estimated from collection statistics and explains the common terms, so that they are much less likely to appear in the presentation list.",
                "Second, the terms are selected from multiple clusters in the pseudo-feedback documents, to ensure sufficient representation of different aspects of the topic.",
                "We rely on the mixture multinomial model, which is used for theme discovery in [26].",
                "Specifically, we assume the N documents contain K clusters {Ci| i = 1, 2, · · · K}, each characterized by a multinomial word distribution (also known as unigram language model) θi and corresponding to an aspect of the topic.",
                "The documents are regarded as sampled from a mixture of K + 1 components, including the K clusters and the background model: p(w|d) = λBp(w|θB) + (1 − λB) K i=1 πd,ip(w|θi) where w is a word, λB is the mixture weight for the background model θB, and πd,i is the document-specific mixture weight for the i-th cluster model θi.",
                "We then estimate the cluster models by maximizing the probability of the pseudo-feedback documents being generated from the multinomial mixture model: log p(D|Λ) = d∈D w∈V c(w; d) log p(w|d) where D = {di| i = 1, 2, · · · N} is the set of the N documents, V is the vocabulary, c(w; d) is ws frequency in d and Λ = {θi| i = 1, 2, · · · K} ∪ {πdij | i = 1, 2, · · · N, j = 1, 2, · · · K} is the set of model parameters to estimate.",
                "The cluster models can be efficiently estimated using the Expectation-Maximization (EM) algorithm.",
                "For its details, we refer the reader to [26].",
                "Table 1 shows the cluster models for TREC query Transportation tunnel disasters (K = 3).",
                "Note that only the middle cluster is relevant.",
                "Table 1: Cluster models for topic 363 Transportation tunnel disasters Cluster 1 Cluster 2 Cluster 3 tunnel 0.0768 tunnel 0.0935 tunnel 0.0454 transport 0.0364 fire 0.0295 transport 0.0406 traffic 0.0206 truck 0.0236 toll 0.0166 railwai 0.0186 french 0.0220 amtrak 0.0153 harbor 0.0146 smoke 0.0157 train 0.0129 rail 0.0140 car 0.0154 airport 0.0122 bridg 0.0139 italian 0.0152 turnpik 0.0105 kilomet 0.0136 firefight 0.0144 lui 0.0095 truck 0.0133 blaze 0.0127 jersei 0.0093 construct 0.0131 blanc 0.0121 pass 0.0087 · · · · · · · · · From each of the K estimated clusters, we choose the L = M/K terms with highest probabilities to form a total of M presentation terms.",
                "If a term happens to be in top L in multiple clusters, we assign it to the cluster where it has highest probability and let the other clusters take one more term as compensation.",
                "We also filter out terms in the original query text because they tend to always be relevant when the query is short.",
                "The selected terms are then presented to the user for judgment.",
                "A sample (completed) feedback form is shown in Figure 1.",
                "In this study we only deal with binary judgment: a presented term is by default unchecked, and a user may check it to indicate relevance.",
                "We also do not explicitly exploit negative feedback (i.e., penalizing irrelevant terms), because with binary feedback an unchecked term is not necessarily irrelevant (maybe the user is unsure about its relevance).",
                "We could ask the user for finer judgment (e.g., choosing from highly relevant, somewhat relevant, do not know, somewhat irrelevant and highly irrelevant), but binary feedback is more compact, taking less space to display and less user effort to make judgment. 5.",
                "ESTIMATING QUERY MODELS FROM TERM FEEDBACK In this section, we present several algorithms for exploiting term feedback.",
                "The algorithms take as input the original query q, the clusters {θi} as generated by the theme discovery algorithm, the set of feedback terms T and their relevance judgment R, and outputs an updated query language model θq that makes best use of the feedback evidence to capture the users information need.",
                "First we describe our notations: • θq: The original <br>query model</br>, derived from query terms only: p(w|θq) = c(w; q) |q| where c(w; q) is the count of w in q, and |q| = w∈q c(w; q) is the query length. • θq : The updated <br>query model</br> which we need to estimate from term feedback. • θi (i = 1, 2, . . .",
                "K): The unigram language model of cluster Ci, as estimated using the theme discovery algorithm. • T = {ti,j} (i = 1 . . .",
                "K, j = 1 . . .",
                "L): The set of terms presented to the user for judgment. ti,j is the j-th term chosen from cluster Ci. • R = {δw|w ∈ T}: δw is an indicator variable that is 1 if w is judged relevant or 0 otherwise. 5.1 TFB (Direct Term Feedback) This is a straight-forward form of term feedback that does not involve any secondary structure.",
                "We give a weight of 1 to terms judged relevant by the user, a weight of μ to query terms, zero weight to other terms, and then apply normalization: p(w|θq ) = δw + μ c(w; q) w ∈T δw + μ|q| where w ∈T δw is the total number of terms that are judged relevant.",
                "We call this method TFB (direct Term FeedBack).",
                "If we let μ = 1, this approach is equivalent to appending the relevant terms after the original query, which is what standard query expansion (without term reweighting) does.",
                "If we set μ > 1, we are putting more emphasis on the query terms than the checked ones.",
                "Note that the result model will be more biased toward θq if the original query is long or the user feedback is weak, which makes sense, as we can trust more on the original query in either case.",
                "Figure 1: Filled clarification form for Topic 363 363 transportation tunnel disasters Please select all terms that are relevant to the topic. traffic railway harbor rail bridge kilometer construct swiss cross link kong hong river project meter shanghai fire truck french smoke car italian firefights blaze blanc mont victim franc rescue driver chamonix emerge toll amtrak train airport turnpike lui jersey pass rome z center electron road boston speed bu submit 5.2 CFB (Cluster Feedback) Here we exploit the cluster structure that played an important role when we selected the presentation terms.",
                "The clusters represent different aspects of the query topic, each of which may or may not be relevant.",
                "If we are able to identify the relevant clusters, we can combine them to generate a <br>query model</br> that is good at discovering documents belonging to these clusters (instead of the irrelevant ones).",
                "We could ask the user to directly judge the relevance of a cluster after viewing representative terms in that cluster, but this would sometimes be a difficult task for the user, who has to guess the semantics of a cluster via its set of terms, which may not be well connected to one another due to a lack of context.",
                "Therefore, we propose to learn cluster feedback indirectly, inferring the relevance of a cluster through the relevance of its feedback terms.",
                "Because each cluster has an equal number of terms presented to the user, the simplest measure of a clusters relevance is the number of terms that are judged relevant in it.",
                "Intuitively, the more terms are marked relevant in a cluster, the closer the cluster is to the query topic, and the more the cluster should participate in query modification.",
                "If we combine the cluster models using weights determined this way and then interpolate with the original <br>query model</br>, we get the following formula for query updating, which we call CFB (Cluster FeedBack): p(w|θq ) = λp(w|θq) + (1 − λ) K i=1 L j=1 δti,j K k=1 L j=1 δtk,j p(w|θi) where L j=1 δti,j is the number of relevant terms in cluster Ci, and K k=1 L j=1 δtk,j is the total number of relevant terms.",
                "We note that when there is only one cluster (K = 1), the above formula degenerates to p(w|θq ) = λp(w|θq) + (1 − λ)p(w|θ1) which is merely pseudo-feedback of the form proposed in [25]. 5.3 TCFB (Term-cluster Feedback) TFB and CFB both have their drawbacks.",
                "TFB assigns non-zero probabilities to the presented terms that are marked relevant, but completely ignores (a lot more) others, which may be left unchecked due to the users ignorance, or simply not included in the presentation list, but we should be able to infer their relevance from the checked ones.",
                "For example, in Figure 1, since as many as 5 terms in the middle cluster (the third and fourth columns) are checked, we should have high confidence in the relevance of other terms in that cluster.",
                "CFB remedies TFBs problem by treating the terms in a cluster collectively, so that unchecked/unpresented terms receive weights when presented terms in their clusters are judged as relevant, but it does not distinguish which terms in a cluster are presented or judged.",
                "Intuitively, the judged relevant terms should receive larger weights because they are explicitly indicated as relevant by the user.",
                "Therefore, we try to combine the two methods, hoping to get the best out of both.",
                "We do this by interpolating the TFB model with the CFB model, and call it TCFB: p(w|θq ) = αp(w|θqT F B ) + (1 − α)p(w|θqCF B ) 6.",
                "EXPERIMENTS In this section, we describe our experiment results.",
                "We first describe our experiment setup and present an overview of various methods performance.",
                "Then we discuss the effects of varying the parameter setting in the algorithms, as well as the number of presentation terms.",
                "Next we analyze user term feedback behavior and its relation to retrieval performance.",
                "Finally we compare term feedback to relevance feedback and show that it has its particular advantage. 6.1 Experiment Setup and Basic Results We took the opportunity of TREC 2005 HARD Track[2] for the evaluation of our algorithms.",
                "The tracks used the AQUAINT collection, a 3GB corpus of English newswire text.",
                "The topics included 50 ones previously known to be hard, i.e. with low retrieval performance.",
                "It is for these hard topics that user feedback is most helpful, as it can provide information to disambiguate the queries; with easy topics the user may be unwilling to spend efforts for feedback if the automatic retrieval results are good enough.",
                "Participants of the track were able to submit custom-designed clarification forms (CF) to solicit feedback from human assessors provided by Table 2: Retrieval performance for different methods and CF types.",
                "The last row is the percentage of MAP improvement over the baseline.",
                "The parameter settings μ = 4, λ = 0.1, α = 0.3 are near optimal.",
                "Baseline TFB1C TFB3C TFB6C CFB1C CFB3C CFB6C TCFB1C TCFB3C TCFB6C MAP 0.219 0.288 0.288 0.278 0.254 0.305 0.301 0.274 0.309 0.304 Pr@30 0.393 0.467 0.475 0.457 0.399 0.480 0.473 0.431 0.491 0.473 RR 4339 4753 4762 4740 4600 4907 4872 4767 4947 4906 % 0% 31.5% 31.5% 26.9% 16.0% 39.3% 37.4% 25.1% 41.1% 38.8% Table 3: MAP variation with the number of presented terms. # terms TFB1C TFB3C TFB6C CFB3C CFB6C TCFB3C TCFB6C 6 0.245 0.240 0.227 0.279 0.279 0.281 0.274 12 0.261 0.261 0.242 0.299 0.286 0.297 0.281 18 0.275 0.274 0.256 0.301 0.282 0.300 0.286 24 0.276 0.281 0.265 0.303 0.292 0.305 0.292 30 0.280 0.285 0.270 0.304 0.296 0.307 0.296 36 0.282 0.288 0.272 0.307 0.297 0.309 0.297 42 0.283 0.288 0.275 0.306 0.298 0.309 0.300 48 0.288 0.288 0.278 0.305 0.301 0.309 0.303 NIST.",
                "We designed three sets of clarification forms for term feedback, differing in the choice of K, the number of clusters, and L, the number of presented terms from each cluster.",
                "They are: 1× 48, a big cluster with 48 terms, 3 × 16, 3 clusters with 16 terms each, and 6 × 8, 6 clusters with 8 terms each.",
                "The total number of presented terms (M) is fixed at 48, so by comparing the performance of different types of clarification forms we can know the effects of different degree of clustering.",
                "For each topic, an assessor would complete the forms ordered by 6 × 8, 1 × 48 and 3 × 16, spending up to three minutes on each form.",
                "The sample clarification form shown in Figure 1 is of type 3 × 16.",
                "It is a simple and compact interface in which the user can check relevant terms.",
                "The form is self-explanatory; there is no need for extra user training on how to use it.",
                "Our initinal queries are constructed only using the topic title descriptions, which are on average 2.7 words in length.",
                "As our baseline we use the KL divergence retrieval method implemented in the Lemur Toolkit1 with 5 pseudo-feedback documents.",
                "We stem the terms, choose Dirichlet smoothing with a prior of 2000, and truncate query language models to 50 terms (these settings are used throughout the experiments).",
                "For all other parameters we use Lemurs default settings.",
                "The baseline turns out to perform above average among the track participants.",
                "After an initial run using this baseline retrieval method, we take the top 60 documents for each topic and apply the theme discovery algorithm to output the clusters (1, 3, or 6 of them), based on which we generate clarification forms.",
                "After user feedback is received, we run the term feedback algorithms (TFB, CFB or TCFB) to estimate updated query models, which are then used for a second iteration of retrieval.",
                "We evaluate the different retrieval methods performance on their rankings of the top 1000 documents.",
                "The evaluation metrics we adopt include mean average (non-interpolated) precision (MAP), precision at top 30 (Pr@30) and total relevant retrieved (RR).",
                "Table 2 shows the performance of various methods and configurations of K × L. The suffixes (1C, 3C, 6C) after TFB,CFB,TCFB stand for the number of clusters (K).",
                "For example, TCFB3C means the TCFB method on the 3 × 16 clarification forms.",
                "From Table 2 we can make the following observations: 1 http://www.lemurproject.com 1.",
                "All methods perform considerably better than the pseudofeedback baseline, with TCFB3C achieving a highest 41.1% improvement in MAP, indicating significant contribution of term feedback for clarification of the users information need.",
                "In other words, term feedback is truly helpful for improving retrieval accuracy. 2.",
                "For TFB, the performance is almost equal on the 1 × 48 and 3 × 16 clarification forms in terms of MAP (although the latter is slightly better in Pr@30 and RR), and a little worse on the 6 × 8 ones. 3.",
                "Both CFB3C and CFB6C perform better than their TFB counterparts in all three metrics, suggesting that feedback on a secondary cluster structure is indeed beneficial.",
                "CFB1C is actually worse because it cannot adjust the weight of its (single) cluster from term feedback and it is merely pseudofeedback. 4.",
                "Although TCFB is just a simple mixture of TFB and CFB by interpolation, it is able to outperform both.",
                "This supports our speculation that TCFB overcomes the drawbacks of TFB (paying attention only to checked terms) and CFB (not distinguishing checked and unchecked terms in a cluster).",
                "Except for TCFB6C v.s.",
                "CFB6C, the performance advantage of TCFB over TFB/CFB is significant at p < 0.05 using the Wilcoxon signed rank test.",
                "This is not true in the case of TFB v.s.",
                "CFB, each of which is better than the other in nearly half of the topics. 6.2 Reduction of Presentation Terms In some situations we may have to reduce the number of presentation terms due to limits in display space or user feedback efforts.",
                "It is interesting to know whether our algorithms performance deteriorates when the user is presented with fewer terms.",
                "Because the presentation terms within each cluster are generated in decreasing order of their frequencies, the presentation list forms a subset of the original one if its size is reduced2 .",
                "Therefore, we can easily simulate what happens when the number of presentation terms decreases 2 There are complexities arising from terms appearing in top L of multiple clusters, but these are exceptions from M to M : we will keep all judgments of the top L = M /K terms in each cluster and discard those of others.",
                "Table 3 shows the performance of various algorithms as the number of presentation terms ranges from 6 to 48.",
                "We find that the performance of TFB is more susceptible to presentation term reduction than that of CFB or TCFB.",
                "For example, at 12 terms the MAP of TFB3C is 90.6% of that at 48 terms, while the numbers for CFB3C and TCFB3C are 98.0% and 96.1% respectively.",
                "We conjecture the reason to be that while TFBs performance heavily depends on how many good terms are chosen for query expansion, CFB only needs a rough estimate of cluster weights to work.",
                "Also, the 3 × 16 clarification forms seem to be more robust than the 6 × 8 ones: at 12 terms the MAP of TFB6C is 87.1% of that at 48 terms, lower than 90.6% for TFB3C.",
                "Similarly, for CFB it is 95.0% against 98.0%.",
                "This is natual, as for a large cluster number of 6, it is easier to get into the situation where each cluster gets too few presentation terms to make topic diversification useful.",
                "Overall, we are surprised to see that the algorithms are still able to perform reasonably well when the number of presentation terms is small.",
                "For example, at only 12 terms CFB3C (the clarification form is of size 3 × 4) can still improve 36.5% over the baseline, dropping slightly from 39.3% at 48 terms. 6.3 User Feedback Analysis In this part we study several aspects of users term feedback behavior, and whether they are connected to retrieval performance.",
                "Figure 2: Clarification form completion time distributions 0−30 30−60 60−90 90−120 120−150 150−180 0 5 10 15 20 25 30 35 completion time (seconds) #topics 1×48 3×16 6×8 Figure 2 shows the distribution of time needed to complete a clarification form3 .",
                "We see that the user is usually able to finish term feedback within a reasonably short amount of time: for more than half of the topics the clarification form is completed in just 1 minute, and only a small fraction of topics (less than 10% for 1 × 48 and 3 × 16) take more than 2 minutes.",
                "This suggests that term feedback is suitable for interactive ad-hoc retrieval, where a user usually does not want to spend too much time on providing feedback.",
                "We find that a user often makes mistakes when judging term relevance.",
                "Sometimes a relevant term may be left out because its connection to the query topic is not obvious to the user.",
                "Other times a dubious term may be included but turns out to be irrelevant.",
                "Take the topic in Figure 1 for example.",
                "There was a fire disaster in Mont 3 The maximal time is 180 seconds, as the NIST assessor would be forced to submit the form at that moment.",
                "Table 4: Term selection statistics (topic average) CF Type 1 × 48 3 × 16 6 × 8 # checked terms 14.8 13.3 11.2 # rel. terms 15.0 12.6 11.2 # rel. checked terms 7.9 6.9 5.9 precision 0.534 0.519 0.527 recall 0.526 0.548 0.527 Blanc Tunnel between France and Italy in 1999, but the user failed to select such keywords as mont, blanc, french and italian due to his/her ignorance of the event.",
                "Indeed, without proper context it would be hard to make perfect judgment.",
                "What is then, the extent to which the user is good at term feedback?",
                "Does it have serious impact on retrieval performance?",
                "To answer these questions, we need a measure of individual terms true relevance.",
                "We adopt the Simplified KL Divergence metric used in [24] to decide query expansion terms as our term relevance measure: σKLD(w) = p(w|R) log p(w|R) p(w|¬R) where p(w|R) is the probability that a relevant document contains term w, and p(w|¬R) is the probability that an irrelevant document contains w, both of which can be easily computed via maximum likelihood estimate given document-level relevance judgment.",
                "If σKLD(w) > 0, w is more likely to appear in relevant documents than irrelevant ones.",
                "We consider a term relevant if its Simplified KL Divergence value is greater than a certain threshold σ0.",
                "We can then define precision and recall of user term judgment accordingly: precision is the fraction of terms checked by the user that are relevant; recall is the fraction of presented relevant terms that are checked by the user.",
                "Table 4 shows the number of checked terms, relevant terms and relevant checked terms when σ0 is set to 1.0, as well as the precision/recall of user term judgment.",
                "Note that when the clarification forms contain more clusters, fewer terms are checked: 14.8 for 1 × 48, 13.3 for 3 × 16 and 11.2 for 6×8.",
                "Similar pattern holds for relevant terms and relevant checked terms.",
                "There seems to be a trade-off between increasing topic diversity by clustering and losing extra relevant terms: when there are more clusters, each of them gets fewer terms to present, which can hurt a major relevant cluster that contains many relevant terms.",
                "Therefore, it is not always helpful to have more clusters, e.g., TFB6C is actually worse than TFB1C.",
                "The major finding we can make from Table 4 is that the user is not particularly good at identifying relevant terms, which echoes the discovery in [18].",
                "In the case of 3 × 16 clarification forms, the average number of terms checked as relevant by the user is 13.3 per topic, and the average number of relevant terms whose σKLD value exceed 1.0 is 12.6.",
                "The user is able to recognize only 6.9 of these terms on average.",
                "Indeed, the precision and recall of user feedback terms (as defined previously) are far from perfect.",
                "On the other hand, If the user had correctly checked all such relevant terms, the performance of our algorithms would have increased a lot, as shown in Table 5.",
                "We see that TFB gets big improvement when there is an oracle who checks all relevant terms, while CFB meets a bottleneck around MAP of 0.325, since all it does is adjust cluster weights, and when the learned weights are close to being accurate, it cannot benefit more from term feedback.",
                "Also note that TCFB fails to outperform TFB, probably because TFB is sufficiently accurate.",
                "Table 5: Change of MAP when using all (and only) relevant terms (σKLD > 1.0) for feedback. original term feedback relevant term feedback TF1 0.288 0.354 TF3 0.288 0.354 TF6 0.278 0.346 CF3 0.305 0.325 CF6 0.301 0.326 TCF3 0.309 0.345 TCF6 0.304 0.341 6.4 Comparison with Relevance Feedback Now we compare term feedback with document-level relevance feedback, in which the user is presented with the top N documents from an initial retrieval and asked to judge their relevance.",
                "The feedback process is simulated using document relevance judgment from NIST.",
                "We use the mixture model based feedback method proposed in [25], with mixture noise set to 0.95 and feedback coefficient set to 0.9.",
                "Comparative evaluation of relevance feedback against other methods is complicated by the fact that some documents have already been viewed during feedback, so it makes no sense to include them in the retrieval results of the second run.",
                "However, this does not hold for term feedback.",
                "Thus, to make it fair w.r.t. users information gain, if the feedback documents are relevant, they should be kept in the top of the ranking; if they are irrelevant, they should be left out.",
                "Therefore, we use relevance feedback to produce a ranking of top 1000 retrieved documents but with every feedback document excluded, and then prepend the relevant feedback documents at the front.",
                "Table 6 shows the performance of relevance feedback for different values of N and compares it with TCFB3C.",
                "Table 6: Performance of relevance feedback for different number of feedback documents (N).",
                "N MAP Pr@30 RR 5 0.302 0.586 4779 10 0.345 0.670 4916 20 0.389 0.772 5004 TCFB3C 0.309 0.491 4947 We see that the performance of TCFB3C is comparable to that of relevance feedback using 5 documents.",
                "Although it is poorer than when there are 10 feedback documents in terms of MAP and Pr@30, it does retrieve more documents (4947) when going down the ranked list.",
                "We try to compare the quality of automatically inserted terms in relevance feedback with that of manually selected terms in term feedback.",
                "This is done by truncating the relevance feedback modified <br>query model</br> to a size equal to the number of checked terms for the same topic.",
                "We can then compare the terms in the truncated model with the checked terms.",
                "Figure 3 shows the distribution of the terms σKLD scores.",
                "We find that term feedback tends to produce expansion terms of higher quality(those with σKLD > 1) compared to relevance feedback (with 10 feedback documents).",
                "This does not contradict the fact that the latter yields higher retrieval performance.",
                "Actually, when we use the truncated <br>query model</br> instead of the intact one refined from relevance feedback, the MAP is only 0.304.",
                "The truth Figure 3: Comparison of expansion term quality between relevance feedback (with 10 feedback documents) and term feedback (with 3 × 16 CFs) −1−0 0−1 1−2 2−3 3−4 4−5 5−6 0 50 100 150 200 250 300 350 σKLD #terms relevance feedback term feedback is, although there are many unwanted terms in the expanded <br>query model</br> from feedback documents, there are also more relevant terms than what the user can possibly select from the list of presentation terms generated with pseudo-feedback documents, and the positive effects often outweights the negative ones.",
                "We are interested to know under what circumstances term feedback has advantage over relevance feedback.",
                "One such situation is when none of the top N feedback documents is relevant, rendering relevance feedback useless.",
                "This is not infrequent, as one might have thought: out of the 50 topics, there are 13 such cases when N = 5, 10 when N = 10, and still 3 when N = 20.",
                "When this happens, one can only back off to the original retrieval method; the power of relevance feedback is lost.",
                "Surprisingly, in 11 out of 13 such cases where relevance feedback seems impossible, the user is able to check at least 2 relevant terms from the 3 × 16 clarification form (we consider term t to be relevant if σKLD(t) > 1.0).",
                "Furthermore, in 10 out of them TCFB3C outperforms the pseudo-feedback baseline, increasing MAP from 0.076 to 0.146 on average (these are particularly hard topics).",
                "We think that there are two possible explanations for this phenomenon of term feedback being active even when relevance feedback does not work: First, even if none of the top N (suppose it is a small number) documents are relevant, we may still find relevant documents in top 60, which is more inclusive but usually unreachable when people are doing relevance feedback in interactive ad-hoc search, from which we can draw feedback terms.",
                "This is true for topic 367 piracy, where the top 10 feedback documents are all about software piracy, yet there are documents between 10-60 that are about piracy on the seas (which is about the real information need), contributing terms such as pirate, ship for selection in the clarification form.",
                "Second, for some topics, a document needs to meet some special condition in order to be relevant.",
                "The top N documents may be related to the topic, but nonetheless irrelevant.",
                "In this case, we may still extract useful terms from these documents, even if they do not qualify as relevant ones.",
                "For example, in topic 639 consumer online shopping, a document needs to mention what contributes to shopping growth to really match the specified information need, hence none of the top 10 feedback documents are regarded as relevant.",
                "But nevertheless, the feedback terms such as retail, commerce are good for query expansion. 7.",
                "CONCLUSIONS In this paper we studied the use of term feedback for interactive information retrieval in the language modeling approach.",
                "We proposed a cluster-based method for selecting presentation terms as well as algorithms to estimate refined query models from user term feedback.",
                "We saw significant improvement in retrieval accuracy brought by term feedback, in spite of the fact that a user often makes mistakes in relevance judgment that hurts its performance.",
                "We found the best-performing algorithm to be TCFB, which benefits from the combination of directly observed term evidence with TFB and indirectly learned cluster relevance with CFB.",
                "When we reduced the number of presentation terms, term feedback is still able to keep much of its performance gain over the baseline.",
                "Finally, we compared term feedback to document-level relevance feedback, and found that TCFB3Cs performance is on a par with the latter with 5 feedback documents.",
                "We regarded term feedback as a viable alternative to traditional relevance feedback, especially when there are no relevant documents in the top.",
                "We propose to extend our work in several ways.",
                "First, we want to study whether the use of various contexts can help the user to better identify term relevance, while not sacrificing the simplicity and compactness of term feedback.",
                "Second, currently all terms are presented to the user in a single batch.",
                "We could instead consider iterative term feedback, by presenting a small number of terms first, and show more terms after receiving user feedback or stop when the refined query is good enough.",
                "The presented terms should be selected dynamically to maximize learning benefits at any moment.",
                "Third, we have plans to incorporate term feedback into our UCAIR toolbar[20], an Internet Explorer plugin, to make it work for web search.",
                "We are also interested in studying how to combine term feedback with relevance feedback or implicit feedback.",
                "We could, for example, allow the user to dynamically modify terms in a language model learned from feedback documents. 8.",
                "ACKNOWLEDGMENT This work is supported in part by the National Science Foundation grants IIS-0347933 and IIS-0428472. 9.",
                "REFERENCES [1] J. Allan.",
                "Relevance feedback with too much data.",
                "In Proceedings of the 18th annual international ACM SIGIR conference on research and development in information retrieval, pages 337-343, 1995. [2] J. Allan.",
                "HARD track overview in TREC 2005 - High Accuracy Retrieval from Documents.",
                "In The Fourteenth Text REtrieval Conference, 2005. [3] P. Anick.",
                "Using terminological feedback for web search refinement: a log-based study.",
                "In Proceedings of the 26th annual international ACM SIGIR conference on research and development in informaion retrieval, pages 88-95, 2003. [4] P. G. Anick and S. Tipirneni.",
                "The paraphrase search assistant: terminological feedback for iterative information seeking.",
                "In Proceedings of the 22nd annual international ACM SIGIR conference on research and development in information retrieval, pages 153-159, 1999. [5] C. Buckley, G. Salton, J. Allan, and A. Singhal.",
                "Automatic query expansion using SMART.",
                "In Proceedings of the Third Text REtrieval Conference, 1994. [6] D. Harman.",
                "Towards interactive query expansion.",
                "In Proceedings of the 11th annual international ACM SIGIR conference on research and development in information retrieval, pages 321-331, 1988. [7] N. A. Jaleel, A. Corrada-Emmanuel, Q. Li, X. Liu, C. Wade, and J. Allan.",
                "UMass at TREC 2003: HARD and QA.",
                "In TREC, pages 715-725, 2003. [8] H. Joho, C. Coverson, M. Sanderson, and M. Beaulieu.",
                "Hierarchical presentation of expansion terms.",
                "In Proceedings of the 2002 ACM symposium on applied computing, pages 645-649, 2002. [9] K. S. Jones, S. Walker, and S. E. Robertson.",
                "A probabilistic model of information retrieval: development and status.",
                "Technical Report 446, Computer Laboratory, University of Cambridge, 1998. [10] D. Kelly, V. D. Dollu, and X. Fu.",
                "The loquacious user: a document-independent source of terms for query expansion.",
                "In Proceedings of the 28th annual international ACM SIGIR conference on research and development in information retrieval, pages 457-464, 2005. [11] D. Kelly and X. Fu.",
                "Elicitation of term relevance feedback: an investigation of term source and context.",
                "In Proceedings of the 29th annual international ACM SIGIR conference on research and development in information retrieval, 2006. [12] J. Koenemann and N. Belkin.",
                "A case for interaction: A study of interactive information retrieval behavior and effectiveness.",
                "In Proceedings of the SIGCHI conference on human factors in computing systems, pages 205-212, 1996. [13] V. Lavrenko and W. B. Croft.",
                "Relevance-based language models.",
                "In Research and Development in Information Retrieval, pages 120-127, 2001. [14] Y. Nemeth, B. Shapira, and M. Taeib-Maimon.",
                "Evaluation of the real and perceived value of automatic and interactive query expansion.",
                "In Proceedings of the 27th annual international ACM SIGIR conference on research and development in information retrieval, pages 526-527, 2004. [15] J. Ponte.",
                "A Language Modeling Approach to Information Retrieval.",
                "PhD thesis, University of Massachusetts at Amherst, 1998. [16] S. E. Robertson, S. Walker, S. Jones, M. Beaulieu, and M. Gatford.",
                "Okapi at TREC-3.",
                "In Proceedings of the Third Text REtrieval Conference, 1994. [17] J. Rocchio.",
                "Relevance feedback in information retrieval.",
                "In The SMART retrieval system, pages 313-323. 1971. [18] I. Ruthven.",
                "Re-examining the potential effectiveness of interactive query expansion.",
                "In Proceedings of the 26th annual international ACM SIGIR conference on research and development in informaion retrieval, pages 213-220, 2003. [19] G. Salton and C. Buckley.",
                "Improving retrieval performance by relevance feedback.",
                "Journal of the American Society for Information Science, 41:288-297, 1990. [20] X. Shen, B. Tan, and C. Zhai.",
                "Implicit user modeling for personalized search.",
                "In Proceedings of the 14th ACM international conference on information and knowledge management, pages 824-831, 2005. [21] X. Shen and C. Zhai.",
                "Active feedback in ad-hoc information retrieval.",
                "In Proceedings of the 28th annual international ACM SIGIR conference on research and development in information retrieval, pages 59-66, 2005. [22] A. Spink.",
                "Term relevance feedback and query expansion: relation to design.",
                "In Proceedings of the 17th annual international ACM SIGIR conference on research and development in information retrieval, pages 81-90, 1994. [23] J. Xu and W. B. Croft.",
                "Query expansion using local and global document analysis.",
                "In Proceedings of the 19th annual international ACM SIGIR conference on research and development in information retrieval, pages 4-11, 1996. [24] H. Zaragoza, N. Craswell, M. Taylor, S. Saria, and S. Robertson.",
                "Microsoft cambridge at TREC-13: Web and HARD tracks.",
                "In Proceedings of the 13th Text REtrieval Conference, 2004. [25] C. Zhai and J. Lafferty.",
                "Model-based feedback in the language modeling approach to information retrieval.",
                "In Proceedings of the tenth international conference on information and knowledge management, pages 403-410, 2001. [26] C. Zhai, A. Velivelli, and B. Yu.",
                "A cross-collection mixture model for comparative text mining.",
                "In Proceedings of the tenth ACM SIGKDD international conference on knowledge discovery and data mining, pages 743-748, 2004."
            ],
            "original_annotated_samples": [
                "INTRODUCTION In the language modeling approach to information retrieval, feedback is often modeled as estimating an improved <br>query model</br> or relevance model based on a set of feedback documents [25, 13].",
                "It is an indirect way of seeking users assistance for <br>query model</br> construction, in the sense that the refined <br>query model</br> (based on terms) is learned through feedback documents/passages, which are high-level structures of terms.",
                "We can consider a more direct way to involve a user in <br>query model</br> improvement, without an intermediary step of document feedback that can introduce noise.",
                "The idea is to present a (reasonable) number of individual terms to the user and ask him/her to judge the relevance of each term or directly specify their probabilities in the <br>query model</br>.",
                "Compared to traditional relevance feedback, this term-based approach to interactive <br>query model</br> refinement has several advantages."
            ],
            "translated_annotated_samples": [
                "En el enfoque de modelado del lenguaje para la recuperación de información, la retroalimentación a menudo se modela como la estimación de un <br>modelo de consulta</br> mejorado o un modelo de relevancia basado en un conjunto de documentos de retroalimentación [25, 13].",
                "Es una forma indirecta de buscar la ayuda de los usuarios para la construcción del <br>modelo de consulta</br>, en el sentido de que el <br>modelo de consulta</br> refinado (basado en términos) se aprende a través de documentos/pasajes de retroalimentación, que son estructuras de alto nivel de términos.",
                "Podemos considerar una forma más directa de involucrar a un usuario en la mejora del <br>modelo de consulta</br>, sin un paso intermedio de retroalimentación de documentos que pueda introducir ruido.",
                "La idea es presentar al usuario un número (razonable) de términos individuales y pedirle que juzgue la relevancia de cada término o especifique directamente sus probabilidades en el <br>modelo de consulta</br>.",
                "En comparación con la retroalimentación de relevancia tradicional, este enfoque basado en términos para el refinamiento del <br>modelo de consulta</br> interactivo tiene varias ventajas."
            ],
            "translated_text": "Comentarios sobre el término para la recuperación de información con modelos de lenguaje Bin Tan†, Atulya Velivelli‡, Hui Fang†, ChengXiang Zhai† Dept. En este documento estudiamos la retroalimentación basada en términos para la recuperación de información en el enfoque de modelado de lenguaje. Con la retroalimentación de términos, un usuario juzga directamente la relevancia de términos individuales sin interacción con documentos de retroalimentación, tomando el control total del proceso de expansión de la consulta. Proponemos un método basado en clusters para seleccionar términos para presentar al usuario para su evaluación, así como algoritmos efectivos para construir modelos de lenguaje de consulta refinados a partir de la retroalimentación de términos del usuario. Nuestros algoritmos han demostrado proporcionar una mejora significativa en la precisión de recuperación en comparación con una línea base sin retroalimentación, y logran un rendimiento comparable al de la retroalimentación de relevancia. Son útiles incluso cuando no hay documentos relevantes en la parte superior. Categorías y Descriptores de Asignaturas H.3.3 [Búsqueda y Recuperación de Información]: Modelos de recuperación Términos generales Algoritmos 1. En el enfoque de modelado del lenguaje para la recuperación de información, la retroalimentación a menudo se modela como la estimación de un <br>modelo de consulta</br> mejorado o un modelo de relevancia basado en un conjunto de documentos de retroalimentación [25, 13]. Esto está en línea con la forma tradicional de realizar retroalimentación de relevancia: presentar al usuario documentos/pasajes para su juicio de relevancia y luego extraer términos de los documentos o pasajes juzgados para expandir la consulta inicial. Es una forma indirecta de buscar la ayuda de los usuarios para la construcción del <br>modelo de consulta</br>, en el sentido de que el <br>modelo de consulta</br> refinado (basado en términos) se aprende a través de documentos/pasajes de retroalimentación, que son estructuras de alto nivel de términos. Tiene la desventaja de que los términos irrelevantes, que ocurren junto con los relevantes en el contenido evaluado, pueden ser utilizados erróneamente para la expansión de la consulta, causando efectos no deseados. Por ejemplo, para la consulta TREC del telescopio Hubble logros, cuando un documento relevante habla más sobre la reparación de los telescopios que sobre sus descubrimientos, términos irrelevantes como caminata espacial pueden ser agregados a la consulta modificada. Podemos considerar una forma más directa de involucrar a un usuario en la mejora del <br>modelo de consulta</br>, sin un paso intermedio de retroalimentación de documentos que pueda introducir ruido. La idea es presentar al usuario un número (razonable) de términos individuales y pedirle que juzgue la relevancia de cada término o especifique directamente sus probabilidades en el <br>modelo de consulta</br>. Esta estrategia ha sido discutida en [15], pero hasta donde sabemos, no ha sido estudiada seriamente en la literatura existente de modelado de lenguaje. En comparación con la retroalimentación de relevancia tradicional, este enfoque basado en términos para el refinamiento del <br>modelo de consulta</br> interactivo tiene varias ventajas. ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "interactive adhoc search": {
            "translated_key": "búsqueda interactiva ad hoc",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Term Feedback for Information Retrieval with Language Models Bin Tan† , Atulya Velivelli‡ , Hui Fang† , ChengXiang Zhai† Dept.",
                "of Computer Science† , Dept. of Electrical and Computer Engineering‡ University of Illinois at Urbana-Champaign bintan@cs.uiuc.edu, velivell@ifp.uiuc.edu, hfang@cs.uiuc.edu, czhai@cs.uiuc.edu ABSTRACT In this paper we study term-based feedback for information retrieval in the language modeling approach.",
                "With term feedback a user directly judges the relevance of individual terms without interaction with feedback documents, taking full control of the query expansion process.",
                "We propose a cluster-based method for selecting terms to present to the user for judgment, as well as effective algorithms for constructing refined query language models from user term feedback.",
                "Our algorithms are shown to bring significant improvement in retrieval accuracy over a non-feedback baseline, and achieve comparable performance to relevance feedback.",
                "They are helpful even when there are no relevant documents in the top.",
                "Categories and Subject Descriptors H.3.3 [Information Search and Retrieval]: Retrieval models General Terms Algorithms 1.",
                "INTRODUCTION In the language modeling approach to information retrieval, feedback is often modeled as estimating an improved query model or relevance model based on a set of feedback documents [25, 13].",
                "This is in line with the traditional way of doing relevance feedback - presenting a user with documents/passages for relevance judgment and then extracting terms from the judged documents or passages to expand the initial query.",
                "It is an indirect way of seeking users assistance for query model construction, in the sense that the refined query model (based on terms) is learned through feedback documents/passages, which are high-level structures of terms.",
                "It has the disadvantage that irrelevant terms, which occur along with relevant ones in the judged content, may be erroneously used for query expansion, causing undesired effects.",
                "For example, for the TREC query Hubble telescope achievements, when a relevant document talks more about the telescopes repair than its discoveries, irrelevant terms such as spacewalk can be added into the modified query.",
                "We can consider a more direct way to involve a user in query model improvement, without an intermediary step of document feedback that can introduce noise.",
                "The idea is to present a (reasonable) number of individual terms to the user and ask him/her to judge the relevance of each term or directly specify their probabilities in the query model.",
                "This strategy has been discussed in [15], but to our knowledge, it has not been seriously studied in existing language modeling literature.",
                "Compared to traditional relevance feedback, this term-based approach to interactive query model refinement has several advantages.",
                "First, the user has better control of the final query model through direct manipulation of terms: he/she can dictate which terms are relevant, irrelevant, and possibly, to what degree.",
                "This avoids the risk of bringing unwanted terms into the query model, although sometimes the user introduces low-quality terms.",
                "Second, because a term takes less time to judge than a documents full text or summary, and as few as around 20 presented terms can bring significant improvement in retrieval performance (as we will show later), term feedback makes it faster to gather user feedback.",
                "This is especially helpful for <br>interactive adhoc search</br>.",
                "Third, sometimes there are no relevant documents in the top N of the initially retrieved results if the topic is hard.",
                "This is often true when N is constrained to be small, which arises from the fact that the user is unwilling to judge too many documents.",
                "In this case, relevance feedback is useless, as no relevant document can be leveraged on, but term feedback is still often helpful, by allowing relevant terms to be picked from irrelevant documents.",
                "During our participation in the TREC 2005 HARD Track and continued study afterward, we explored how to exploit term feedback from the user to construct improved query models for information retrieval in the language modeling approach.",
                "We identified two key subtasks of term-based feedback, i.e., pre-feedback presentation term selection and post-feedback query model construction, with effective algorithms developed for both.",
                "We imposed a secondary cluster structure on terms and found that a cluster view sheds additional insight into the users information need, and provides a good way of utilizing term feedback.",
                "Through experiments we found that term feedback improves significantly over the nonfeedback baseline, even though the user often makes mistakes in relevance judgment.",
                "Among our algorithms, the one with best retrieval performance is TCFB, the combination of TFB, the direct term feedback algorithm, and CFB, the cluster-based feedback algorithm.",
                "We also varied the number of feedback terms and observed reasonable improvement even at low numbers.",
                "Finally, by comparing term feedback with document-level feedback, we found it to be a viable alternative to the latter with competitive retrieval performance.",
                "The rest of the paper is organized as follows.",
                "Section 2 discusses some related work.",
                "Section 4 outlines our general approach to term feedback.",
                "We present our method for presentation term selection in Section 3 and algorithms for query model construction in Section 5.",
                "The experiment results are given in Section 6.",
                "Section 7 concludes this paper. 2.",
                "RELATED WORK Relevance feedback[17, 19] has long been recognized as an effective method for improving retrieval performance.",
                "Normally, the top N documents retrieved using the original query are presented to the user for judgment, after which terms are extracted from the judged relevant documents, weighted by their potential of attracting more relevant documents, and added into the query model.",
                "The expanded query usually represents the users information need better than the original one, which is often just a short keyword query.",
                "A second iteration of retrieval using this modified query usually produces significant increase in retrieval accuracy.",
                "In cases where true relevance judgment is unavailable and all top N documents are assumed to be relevant, it is called blind or pseudo feedback[5, 16] and usually still brings performance improvement.",
                "Because document is a large text unit, when it is used for relevance feedback many irrelevant terms can be introduced into the feedback process.",
                "To overcome this, passage feedback is proposed and shown to improve feedback performance[1, 23].",
                "A more direct solution is to ask the user for their relevance judgment of feedback terms.",
                "For example, in some relevance feedback systems such as [12], there is an interaction step that allows the user to add or remove expansion terms after they are automatically extracted from relevant documents.",
                "This is categorized as interactive query expansion, where the original query is augmented with user-provided terms, which can come from direct user input (free-form text or keywords)[22, 7, 10] or user selection of system-suggested terms (using thesauri[6, 22] or extracted from feedback documents[6, 22, 12, 4, 7]).",
                "In many cases term relevance feedback has been found to effectively improve retrieval performance[6, 22, 12, 4, 10].",
                "For example, the study in [12] shows that the user prefers to have explicit knowledge and direct control of which terms are used for query expansion, and the penetrable interface that provides this freedom is shown to perform better than other interfaces.",
                "However, in some other cases there is no significant benefit[3, 14], even if the user likes interacting with expansion terms.",
                "In a simulated study carried out in [18], the author compares the retrieval performance of interactive query expansion and automatic query expansion with a simulated study, and suggests that the potential benefits of the former can be hard to achieve.",
                "The user is found to be not good at identifying useful terms for query expansion, when a simple term presentation interface is unable to provide sufficient semantic context of the feedback terms.",
                "Our work differs from the previous ones in two important aspects.",
                "First, when we choose terms to present to the user for relevance judgment, we not only consider single-term value (e.g., the relative frequency of a term in the top documents, which can be measured by metrics such as Robertson Selection Value and Simplified Kullback-Leibler Distance as listed in [24]), but also examine the cluster structure of the terms, so as to produce a balanced coverage of the different topic aspects.",
                "Second, with the language modelling framework, we allow an elaborate construction of the updated query model, by setting different probabilities for different terms based on whether it is a query term, its significance in the top documents, and its cluster membership.",
                "Although techniques for adjusting query term weights exist for vector space models[17] and probablistic relevance models[9], most of the aforementioned works do not use them, choosing to just append feedback terms to the original query (thus using equal weights for them), which can lead to poorer retrieval performance.",
                "The combination of the two aspects allows our method to perform much better than the baseline.",
                "The usual way for feedback term presentation is just to display the terms in a list.",
                "There have been some works on alternative user interfaces. [8] arranges terms in a hierarchy, and [11] compares three different interfaces, including terms + checkboxes, terms + context (sentences) + checkboxes, sentences + input text box.",
                "In both studies, however, there is no significant performance difference.",
                "In our work we adopt the simplest approach of terms + checkboxes.",
                "We focus on term presentation and query model construction from feedback terms, and believe using contexts to improve feedback term quality should be orthogonal to our method. 3.",
                "GENERAL APPROACH We follow the language modeling approach, and base our method on the KL-divergence retrieval model proposed in [25].",
                "With this model, the retrieval task involves estimating a query language model θq from a given query, a document language model θd from each document, and calculating their KL-divergence D(θq||θd), which is then used to score the documents. [25] treats relevance feedback as a query model re-estimation problem, i.e., computing an updated query model θq given the original query text and the extra evidence carried by the judged relevant documents.",
                "We adopt this view, and cast our task as updating the query model from user term feedback.",
                "There are two key subtasks here: First, how to choose the best terms to present to the user for judgment, in order to gather maximal evidence about the users information need.",
                "Second, how to compute an updated query model based on this term feedback evidence, so that it captures the users information need and translates into good retrieval performance. 4.",
                "PRESENTATION TERM SELECTION Proper selection of terms to be presented to the user for judgment is crucial to the success of term feedback.",
                "If the terms are poorly chosen and there are few relevant ones, the user will have a hard time looking for useful terms to help clarify his/her information need.",
                "If the relevant terms are plentiful, but all concentrate on a single aspect of the query topic, then we will only be able to get feedback on that aspect and missing others, resulting in a breadth loss in retrieved results.",
                "Therefore, it is important to carefully select presentation terms to maximize expected gain from user feedback, i.e., those that can potentially reveal most evidence of the users information need.",
                "This is similar to active feedback[21], which suggests that a retrieval system should actively probe the users information need, and in the case of relevance feedback, the feedback documents should be chosen to maximize learning benefits (e.g. diversely so as to increase coverage).",
                "In our approach, the top N documents from an initial retrieval using the original query form the source of feedback terms: all terms that appear in them are considered candidates to present to the user.",
                "These documents serve as pseudo-feedback, since they provide a much richer context than the original query (usually very short), while the user is not asked to judge their relevance.",
                "Due to the latter reason, it is possible to make N quite large (e.g., in our experiments we set N = 60) to increase its coverage of different aspects in the topic.",
                "The simplest way of selecting feedback terms is to choose the most frequent M terms from the N documents.",
                "This method, however, has two drawbacks.",
                "First, a lot of common noisy terms will be selected due to their high frequencies in the document collection, unless a stop-word list is used for filtering.",
                "Second, the presentation list will tend to be filled by terms from major aspects of the topic; those from a minor aspect are likely to be missed due to their relatively low frequencies.",
                "We solve the above problems by two corresponding measures.",
                "First, we introduce a background model θB that is estimated from collection statistics and explains the common terms, so that they are much less likely to appear in the presentation list.",
                "Second, the terms are selected from multiple clusters in the pseudo-feedback documents, to ensure sufficient representation of different aspects of the topic.",
                "We rely on the mixture multinomial model, which is used for theme discovery in [26].",
                "Specifically, we assume the N documents contain K clusters {Ci| i = 1, 2, · · · K}, each characterized by a multinomial word distribution (also known as unigram language model) θi and corresponding to an aspect of the topic.",
                "The documents are regarded as sampled from a mixture of K + 1 components, including the K clusters and the background model: p(w|d) = λBp(w|θB) + (1 − λB) K i=1 πd,ip(w|θi) where w is a word, λB is the mixture weight for the background model θB, and πd,i is the document-specific mixture weight for the i-th cluster model θi.",
                "We then estimate the cluster models by maximizing the probability of the pseudo-feedback documents being generated from the multinomial mixture model: log p(D|Λ) = d∈D w∈V c(w; d) log p(w|d) where D = {di| i = 1, 2, · · · N} is the set of the N documents, V is the vocabulary, c(w; d) is ws frequency in d and Λ = {θi| i = 1, 2, · · · K} ∪ {πdij | i = 1, 2, · · · N, j = 1, 2, · · · K} is the set of model parameters to estimate.",
                "The cluster models can be efficiently estimated using the Expectation-Maximization (EM) algorithm.",
                "For its details, we refer the reader to [26].",
                "Table 1 shows the cluster models for TREC query Transportation tunnel disasters (K = 3).",
                "Note that only the middle cluster is relevant.",
                "Table 1: Cluster models for topic 363 Transportation tunnel disasters Cluster 1 Cluster 2 Cluster 3 tunnel 0.0768 tunnel 0.0935 tunnel 0.0454 transport 0.0364 fire 0.0295 transport 0.0406 traffic 0.0206 truck 0.0236 toll 0.0166 railwai 0.0186 french 0.0220 amtrak 0.0153 harbor 0.0146 smoke 0.0157 train 0.0129 rail 0.0140 car 0.0154 airport 0.0122 bridg 0.0139 italian 0.0152 turnpik 0.0105 kilomet 0.0136 firefight 0.0144 lui 0.0095 truck 0.0133 blaze 0.0127 jersei 0.0093 construct 0.0131 blanc 0.0121 pass 0.0087 · · · · · · · · · From each of the K estimated clusters, we choose the L = M/K terms with highest probabilities to form a total of M presentation terms.",
                "If a term happens to be in top L in multiple clusters, we assign it to the cluster where it has highest probability and let the other clusters take one more term as compensation.",
                "We also filter out terms in the original query text because they tend to always be relevant when the query is short.",
                "The selected terms are then presented to the user for judgment.",
                "A sample (completed) feedback form is shown in Figure 1.",
                "In this study we only deal with binary judgment: a presented term is by default unchecked, and a user may check it to indicate relevance.",
                "We also do not explicitly exploit negative feedback (i.e., penalizing irrelevant terms), because with binary feedback an unchecked term is not necessarily irrelevant (maybe the user is unsure about its relevance).",
                "We could ask the user for finer judgment (e.g., choosing from highly relevant, somewhat relevant, do not know, somewhat irrelevant and highly irrelevant), but binary feedback is more compact, taking less space to display and less user effort to make judgment. 5.",
                "ESTIMATING QUERY MODELS FROM TERM FEEDBACK In this section, we present several algorithms for exploiting term feedback.",
                "The algorithms take as input the original query q, the clusters {θi} as generated by the theme discovery algorithm, the set of feedback terms T and their relevance judgment R, and outputs an updated query language model θq that makes best use of the feedback evidence to capture the users information need.",
                "First we describe our notations: • θq: The original query model, derived from query terms only: p(w|θq) = c(w; q) |q| where c(w; q) is the count of w in q, and |q| = w∈q c(w; q) is the query length. • θq : The updated query model which we need to estimate from term feedback. • θi (i = 1, 2, . . .",
                "K): The unigram language model of cluster Ci, as estimated using the theme discovery algorithm. • T = {ti,j} (i = 1 . . .",
                "K, j = 1 . . .",
                "L): The set of terms presented to the user for judgment. ti,j is the j-th term chosen from cluster Ci. • R = {δw|w ∈ T}: δw is an indicator variable that is 1 if w is judged relevant or 0 otherwise. 5.1 TFB (Direct Term Feedback) This is a straight-forward form of term feedback that does not involve any secondary structure.",
                "We give a weight of 1 to terms judged relevant by the user, a weight of μ to query terms, zero weight to other terms, and then apply normalization: p(w|θq ) = δw + μ c(w; q) w ∈T δw + μ|q| where w ∈T δw is the total number of terms that are judged relevant.",
                "We call this method TFB (direct Term FeedBack).",
                "If we let μ = 1, this approach is equivalent to appending the relevant terms after the original query, which is what standard query expansion (without term reweighting) does.",
                "If we set μ > 1, we are putting more emphasis on the query terms than the checked ones.",
                "Note that the result model will be more biased toward θq if the original query is long or the user feedback is weak, which makes sense, as we can trust more on the original query in either case.",
                "Figure 1: Filled clarification form for Topic 363 363 transportation tunnel disasters Please select all terms that are relevant to the topic. traffic railway harbor rail bridge kilometer construct swiss cross link kong hong river project meter shanghai fire truck french smoke car italian firefights blaze blanc mont victim franc rescue driver chamonix emerge toll amtrak train airport turnpike lui jersey pass rome z center electron road boston speed bu submit 5.2 CFB (Cluster Feedback) Here we exploit the cluster structure that played an important role when we selected the presentation terms.",
                "The clusters represent different aspects of the query topic, each of which may or may not be relevant.",
                "If we are able to identify the relevant clusters, we can combine them to generate a query model that is good at discovering documents belonging to these clusters (instead of the irrelevant ones).",
                "We could ask the user to directly judge the relevance of a cluster after viewing representative terms in that cluster, but this would sometimes be a difficult task for the user, who has to guess the semantics of a cluster via its set of terms, which may not be well connected to one another due to a lack of context.",
                "Therefore, we propose to learn cluster feedback indirectly, inferring the relevance of a cluster through the relevance of its feedback terms.",
                "Because each cluster has an equal number of terms presented to the user, the simplest measure of a clusters relevance is the number of terms that are judged relevant in it.",
                "Intuitively, the more terms are marked relevant in a cluster, the closer the cluster is to the query topic, and the more the cluster should participate in query modification.",
                "If we combine the cluster models using weights determined this way and then interpolate with the original query model, we get the following formula for query updating, which we call CFB (Cluster FeedBack): p(w|θq ) = λp(w|θq) + (1 − λ) K i=1 L j=1 δti,j K k=1 L j=1 δtk,j p(w|θi) where L j=1 δti,j is the number of relevant terms in cluster Ci, and K k=1 L j=1 δtk,j is the total number of relevant terms.",
                "We note that when there is only one cluster (K = 1), the above formula degenerates to p(w|θq ) = λp(w|θq) + (1 − λ)p(w|θ1) which is merely pseudo-feedback of the form proposed in [25]. 5.3 TCFB (Term-cluster Feedback) TFB and CFB both have their drawbacks.",
                "TFB assigns non-zero probabilities to the presented terms that are marked relevant, but completely ignores (a lot more) others, which may be left unchecked due to the users ignorance, or simply not included in the presentation list, but we should be able to infer their relevance from the checked ones.",
                "For example, in Figure 1, since as many as 5 terms in the middle cluster (the third and fourth columns) are checked, we should have high confidence in the relevance of other terms in that cluster.",
                "CFB remedies TFBs problem by treating the terms in a cluster collectively, so that unchecked/unpresented terms receive weights when presented terms in their clusters are judged as relevant, but it does not distinguish which terms in a cluster are presented or judged.",
                "Intuitively, the judged relevant terms should receive larger weights because they are explicitly indicated as relevant by the user.",
                "Therefore, we try to combine the two methods, hoping to get the best out of both.",
                "We do this by interpolating the TFB model with the CFB model, and call it TCFB: p(w|θq ) = αp(w|θqT F B ) + (1 − α)p(w|θqCF B ) 6.",
                "EXPERIMENTS In this section, we describe our experiment results.",
                "We first describe our experiment setup and present an overview of various methods performance.",
                "Then we discuss the effects of varying the parameter setting in the algorithms, as well as the number of presentation terms.",
                "Next we analyze user term feedback behavior and its relation to retrieval performance.",
                "Finally we compare term feedback to relevance feedback and show that it has its particular advantage. 6.1 Experiment Setup and Basic Results We took the opportunity of TREC 2005 HARD Track[2] for the evaluation of our algorithms.",
                "The tracks used the AQUAINT collection, a 3GB corpus of English newswire text.",
                "The topics included 50 ones previously known to be hard, i.e. with low retrieval performance.",
                "It is for these hard topics that user feedback is most helpful, as it can provide information to disambiguate the queries; with easy topics the user may be unwilling to spend efforts for feedback if the automatic retrieval results are good enough.",
                "Participants of the track were able to submit custom-designed clarification forms (CF) to solicit feedback from human assessors provided by Table 2: Retrieval performance for different methods and CF types.",
                "The last row is the percentage of MAP improvement over the baseline.",
                "The parameter settings μ = 4, λ = 0.1, α = 0.3 are near optimal.",
                "Baseline TFB1C TFB3C TFB6C CFB1C CFB3C CFB6C TCFB1C TCFB3C TCFB6C MAP 0.219 0.288 0.288 0.278 0.254 0.305 0.301 0.274 0.309 0.304 Pr@30 0.393 0.467 0.475 0.457 0.399 0.480 0.473 0.431 0.491 0.473 RR 4339 4753 4762 4740 4600 4907 4872 4767 4947 4906 % 0% 31.5% 31.5% 26.9% 16.0% 39.3% 37.4% 25.1% 41.1% 38.8% Table 3: MAP variation with the number of presented terms. # terms TFB1C TFB3C TFB6C CFB3C CFB6C TCFB3C TCFB6C 6 0.245 0.240 0.227 0.279 0.279 0.281 0.274 12 0.261 0.261 0.242 0.299 0.286 0.297 0.281 18 0.275 0.274 0.256 0.301 0.282 0.300 0.286 24 0.276 0.281 0.265 0.303 0.292 0.305 0.292 30 0.280 0.285 0.270 0.304 0.296 0.307 0.296 36 0.282 0.288 0.272 0.307 0.297 0.309 0.297 42 0.283 0.288 0.275 0.306 0.298 0.309 0.300 48 0.288 0.288 0.278 0.305 0.301 0.309 0.303 NIST.",
                "We designed three sets of clarification forms for term feedback, differing in the choice of K, the number of clusters, and L, the number of presented terms from each cluster.",
                "They are: 1× 48, a big cluster with 48 terms, 3 × 16, 3 clusters with 16 terms each, and 6 × 8, 6 clusters with 8 terms each.",
                "The total number of presented terms (M) is fixed at 48, so by comparing the performance of different types of clarification forms we can know the effects of different degree of clustering.",
                "For each topic, an assessor would complete the forms ordered by 6 × 8, 1 × 48 and 3 × 16, spending up to three minutes on each form.",
                "The sample clarification form shown in Figure 1 is of type 3 × 16.",
                "It is a simple and compact interface in which the user can check relevant terms.",
                "The form is self-explanatory; there is no need for extra user training on how to use it.",
                "Our initinal queries are constructed only using the topic title descriptions, which are on average 2.7 words in length.",
                "As our baseline we use the KL divergence retrieval method implemented in the Lemur Toolkit1 with 5 pseudo-feedback documents.",
                "We stem the terms, choose Dirichlet smoothing with a prior of 2000, and truncate query language models to 50 terms (these settings are used throughout the experiments).",
                "For all other parameters we use Lemurs default settings.",
                "The baseline turns out to perform above average among the track participants.",
                "After an initial run using this baseline retrieval method, we take the top 60 documents for each topic and apply the theme discovery algorithm to output the clusters (1, 3, or 6 of them), based on which we generate clarification forms.",
                "After user feedback is received, we run the term feedback algorithms (TFB, CFB or TCFB) to estimate updated query models, which are then used for a second iteration of retrieval.",
                "We evaluate the different retrieval methods performance on their rankings of the top 1000 documents.",
                "The evaluation metrics we adopt include mean average (non-interpolated) precision (MAP), precision at top 30 (Pr@30) and total relevant retrieved (RR).",
                "Table 2 shows the performance of various methods and configurations of K × L. The suffixes (1C, 3C, 6C) after TFB,CFB,TCFB stand for the number of clusters (K).",
                "For example, TCFB3C means the TCFB method on the 3 × 16 clarification forms.",
                "From Table 2 we can make the following observations: 1 http://www.lemurproject.com 1.",
                "All methods perform considerably better than the pseudofeedback baseline, with TCFB3C achieving a highest 41.1% improvement in MAP, indicating significant contribution of term feedback for clarification of the users information need.",
                "In other words, term feedback is truly helpful for improving retrieval accuracy. 2.",
                "For TFB, the performance is almost equal on the 1 × 48 and 3 × 16 clarification forms in terms of MAP (although the latter is slightly better in Pr@30 and RR), and a little worse on the 6 × 8 ones. 3.",
                "Both CFB3C and CFB6C perform better than their TFB counterparts in all three metrics, suggesting that feedback on a secondary cluster structure is indeed beneficial.",
                "CFB1C is actually worse because it cannot adjust the weight of its (single) cluster from term feedback and it is merely pseudofeedback. 4.",
                "Although TCFB is just a simple mixture of TFB and CFB by interpolation, it is able to outperform both.",
                "This supports our speculation that TCFB overcomes the drawbacks of TFB (paying attention only to checked terms) and CFB (not distinguishing checked and unchecked terms in a cluster).",
                "Except for TCFB6C v.s.",
                "CFB6C, the performance advantage of TCFB over TFB/CFB is significant at p < 0.05 using the Wilcoxon signed rank test.",
                "This is not true in the case of TFB v.s.",
                "CFB, each of which is better than the other in nearly half of the topics. 6.2 Reduction of Presentation Terms In some situations we may have to reduce the number of presentation terms due to limits in display space or user feedback efforts.",
                "It is interesting to know whether our algorithms performance deteriorates when the user is presented with fewer terms.",
                "Because the presentation terms within each cluster are generated in decreasing order of their frequencies, the presentation list forms a subset of the original one if its size is reduced2 .",
                "Therefore, we can easily simulate what happens when the number of presentation terms decreases 2 There are complexities arising from terms appearing in top L of multiple clusters, but these are exceptions from M to M : we will keep all judgments of the top L = M /K terms in each cluster and discard those of others.",
                "Table 3 shows the performance of various algorithms as the number of presentation terms ranges from 6 to 48.",
                "We find that the performance of TFB is more susceptible to presentation term reduction than that of CFB or TCFB.",
                "For example, at 12 terms the MAP of TFB3C is 90.6% of that at 48 terms, while the numbers for CFB3C and TCFB3C are 98.0% and 96.1% respectively.",
                "We conjecture the reason to be that while TFBs performance heavily depends on how many good terms are chosen for query expansion, CFB only needs a rough estimate of cluster weights to work.",
                "Also, the 3 × 16 clarification forms seem to be more robust than the 6 × 8 ones: at 12 terms the MAP of TFB6C is 87.1% of that at 48 terms, lower than 90.6% for TFB3C.",
                "Similarly, for CFB it is 95.0% against 98.0%.",
                "This is natual, as for a large cluster number of 6, it is easier to get into the situation where each cluster gets too few presentation terms to make topic diversification useful.",
                "Overall, we are surprised to see that the algorithms are still able to perform reasonably well when the number of presentation terms is small.",
                "For example, at only 12 terms CFB3C (the clarification form is of size 3 × 4) can still improve 36.5% over the baseline, dropping slightly from 39.3% at 48 terms. 6.3 User Feedback Analysis In this part we study several aspects of users term feedback behavior, and whether they are connected to retrieval performance.",
                "Figure 2: Clarification form completion time distributions 0−30 30−60 60−90 90−120 120−150 150−180 0 5 10 15 20 25 30 35 completion time (seconds) #topics 1×48 3×16 6×8 Figure 2 shows the distribution of time needed to complete a clarification form3 .",
                "We see that the user is usually able to finish term feedback within a reasonably short amount of time: for more than half of the topics the clarification form is completed in just 1 minute, and only a small fraction of topics (less than 10% for 1 × 48 and 3 × 16) take more than 2 minutes.",
                "This suggests that term feedback is suitable for interactive ad-hoc retrieval, where a user usually does not want to spend too much time on providing feedback.",
                "We find that a user often makes mistakes when judging term relevance.",
                "Sometimes a relevant term may be left out because its connection to the query topic is not obvious to the user.",
                "Other times a dubious term may be included but turns out to be irrelevant.",
                "Take the topic in Figure 1 for example.",
                "There was a fire disaster in Mont 3 The maximal time is 180 seconds, as the NIST assessor would be forced to submit the form at that moment.",
                "Table 4: Term selection statistics (topic average) CF Type 1 × 48 3 × 16 6 × 8 # checked terms 14.8 13.3 11.2 # rel. terms 15.0 12.6 11.2 # rel. checked terms 7.9 6.9 5.9 precision 0.534 0.519 0.527 recall 0.526 0.548 0.527 Blanc Tunnel between France and Italy in 1999, but the user failed to select such keywords as mont, blanc, french and italian due to his/her ignorance of the event.",
                "Indeed, without proper context it would be hard to make perfect judgment.",
                "What is then, the extent to which the user is good at term feedback?",
                "Does it have serious impact on retrieval performance?",
                "To answer these questions, we need a measure of individual terms true relevance.",
                "We adopt the Simplified KL Divergence metric used in [24] to decide query expansion terms as our term relevance measure: σKLD(w) = p(w|R) log p(w|R) p(w|¬R) where p(w|R) is the probability that a relevant document contains term w, and p(w|¬R) is the probability that an irrelevant document contains w, both of which can be easily computed via maximum likelihood estimate given document-level relevance judgment.",
                "If σKLD(w) > 0, w is more likely to appear in relevant documents than irrelevant ones.",
                "We consider a term relevant if its Simplified KL Divergence value is greater than a certain threshold σ0.",
                "We can then define precision and recall of user term judgment accordingly: precision is the fraction of terms checked by the user that are relevant; recall is the fraction of presented relevant terms that are checked by the user.",
                "Table 4 shows the number of checked terms, relevant terms and relevant checked terms when σ0 is set to 1.0, as well as the precision/recall of user term judgment.",
                "Note that when the clarification forms contain more clusters, fewer terms are checked: 14.8 for 1 × 48, 13.3 for 3 × 16 and 11.2 for 6×8.",
                "Similar pattern holds for relevant terms and relevant checked terms.",
                "There seems to be a trade-off between increasing topic diversity by clustering and losing extra relevant terms: when there are more clusters, each of them gets fewer terms to present, which can hurt a major relevant cluster that contains many relevant terms.",
                "Therefore, it is not always helpful to have more clusters, e.g., TFB6C is actually worse than TFB1C.",
                "The major finding we can make from Table 4 is that the user is not particularly good at identifying relevant terms, which echoes the discovery in [18].",
                "In the case of 3 × 16 clarification forms, the average number of terms checked as relevant by the user is 13.3 per topic, and the average number of relevant terms whose σKLD value exceed 1.0 is 12.6.",
                "The user is able to recognize only 6.9 of these terms on average.",
                "Indeed, the precision and recall of user feedback terms (as defined previously) are far from perfect.",
                "On the other hand, If the user had correctly checked all such relevant terms, the performance of our algorithms would have increased a lot, as shown in Table 5.",
                "We see that TFB gets big improvement when there is an oracle who checks all relevant terms, while CFB meets a bottleneck around MAP of 0.325, since all it does is adjust cluster weights, and when the learned weights are close to being accurate, it cannot benefit more from term feedback.",
                "Also note that TCFB fails to outperform TFB, probably because TFB is sufficiently accurate.",
                "Table 5: Change of MAP when using all (and only) relevant terms (σKLD > 1.0) for feedback. original term feedback relevant term feedback TF1 0.288 0.354 TF3 0.288 0.354 TF6 0.278 0.346 CF3 0.305 0.325 CF6 0.301 0.326 TCF3 0.309 0.345 TCF6 0.304 0.341 6.4 Comparison with Relevance Feedback Now we compare term feedback with document-level relevance feedback, in which the user is presented with the top N documents from an initial retrieval and asked to judge their relevance.",
                "The feedback process is simulated using document relevance judgment from NIST.",
                "We use the mixture model based feedback method proposed in [25], with mixture noise set to 0.95 and feedback coefficient set to 0.9.",
                "Comparative evaluation of relevance feedback against other methods is complicated by the fact that some documents have already been viewed during feedback, so it makes no sense to include them in the retrieval results of the second run.",
                "However, this does not hold for term feedback.",
                "Thus, to make it fair w.r.t. users information gain, if the feedback documents are relevant, they should be kept in the top of the ranking; if they are irrelevant, they should be left out.",
                "Therefore, we use relevance feedback to produce a ranking of top 1000 retrieved documents but with every feedback document excluded, and then prepend the relevant feedback documents at the front.",
                "Table 6 shows the performance of relevance feedback for different values of N and compares it with TCFB3C.",
                "Table 6: Performance of relevance feedback for different number of feedback documents (N).",
                "N MAP Pr@30 RR 5 0.302 0.586 4779 10 0.345 0.670 4916 20 0.389 0.772 5004 TCFB3C 0.309 0.491 4947 We see that the performance of TCFB3C is comparable to that of relevance feedback using 5 documents.",
                "Although it is poorer than when there are 10 feedback documents in terms of MAP and Pr@30, it does retrieve more documents (4947) when going down the ranked list.",
                "We try to compare the quality of automatically inserted terms in relevance feedback with that of manually selected terms in term feedback.",
                "This is done by truncating the relevance feedback modified query model to a size equal to the number of checked terms for the same topic.",
                "We can then compare the terms in the truncated model with the checked terms.",
                "Figure 3 shows the distribution of the terms σKLD scores.",
                "We find that term feedback tends to produce expansion terms of higher quality(those with σKLD > 1) compared to relevance feedback (with 10 feedback documents).",
                "This does not contradict the fact that the latter yields higher retrieval performance.",
                "Actually, when we use the truncated query model instead of the intact one refined from relevance feedback, the MAP is only 0.304.",
                "The truth Figure 3: Comparison of expansion term quality between relevance feedback (with 10 feedback documents) and term feedback (with 3 × 16 CFs) −1−0 0−1 1−2 2−3 3−4 4−5 5−6 0 50 100 150 200 250 300 350 σKLD #terms relevance feedback term feedback is, although there are many unwanted terms in the expanded query model from feedback documents, there are also more relevant terms than what the user can possibly select from the list of presentation terms generated with pseudo-feedback documents, and the positive effects often outweights the negative ones.",
                "We are interested to know under what circumstances term feedback has advantage over relevance feedback.",
                "One such situation is when none of the top N feedback documents is relevant, rendering relevance feedback useless.",
                "This is not infrequent, as one might have thought: out of the 50 topics, there are 13 such cases when N = 5, 10 when N = 10, and still 3 when N = 20.",
                "When this happens, one can only back off to the original retrieval method; the power of relevance feedback is lost.",
                "Surprisingly, in 11 out of 13 such cases where relevance feedback seems impossible, the user is able to check at least 2 relevant terms from the 3 × 16 clarification form (we consider term t to be relevant if σKLD(t) > 1.0).",
                "Furthermore, in 10 out of them TCFB3C outperforms the pseudo-feedback baseline, increasing MAP from 0.076 to 0.146 on average (these are particularly hard topics).",
                "We think that there are two possible explanations for this phenomenon of term feedback being active even when relevance feedback does not work: First, even if none of the top N (suppose it is a small number) documents are relevant, we may still find relevant documents in top 60, which is more inclusive but usually unreachable when people are doing relevance feedback in interactive ad-hoc search, from which we can draw feedback terms.",
                "This is true for topic 367 piracy, where the top 10 feedback documents are all about software piracy, yet there are documents between 10-60 that are about piracy on the seas (which is about the real information need), contributing terms such as pirate, ship for selection in the clarification form.",
                "Second, for some topics, a document needs to meet some special condition in order to be relevant.",
                "The top N documents may be related to the topic, but nonetheless irrelevant.",
                "In this case, we may still extract useful terms from these documents, even if they do not qualify as relevant ones.",
                "For example, in topic 639 consumer online shopping, a document needs to mention what contributes to shopping growth to really match the specified information need, hence none of the top 10 feedback documents are regarded as relevant.",
                "But nevertheless, the feedback terms such as retail, commerce are good for query expansion. 7.",
                "CONCLUSIONS In this paper we studied the use of term feedback for interactive information retrieval in the language modeling approach.",
                "We proposed a cluster-based method for selecting presentation terms as well as algorithms to estimate refined query models from user term feedback.",
                "We saw significant improvement in retrieval accuracy brought by term feedback, in spite of the fact that a user often makes mistakes in relevance judgment that hurts its performance.",
                "We found the best-performing algorithm to be TCFB, which benefits from the combination of directly observed term evidence with TFB and indirectly learned cluster relevance with CFB.",
                "When we reduced the number of presentation terms, term feedback is still able to keep much of its performance gain over the baseline.",
                "Finally, we compared term feedback to document-level relevance feedback, and found that TCFB3Cs performance is on a par with the latter with 5 feedback documents.",
                "We regarded term feedback as a viable alternative to traditional relevance feedback, especially when there are no relevant documents in the top.",
                "We propose to extend our work in several ways.",
                "First, we want to study whether the use of various contexts can help the user to better identify term relevance, while not sacrificing the simplicity and compactness of term feedback.",
                "Second, currently all terms are presented to the user in a single batch.",
                "We could instead consider iterative term feedback, by presenting a small number of terms first, and show more terms after receiving user feedback or stop when the refined query is good enough.",
                "The presented terms should be selected dynamically to maximize learning benefits at any moment.",
                "Third, we have plans to incorporate term feedback into our UCAIR toolbar[20], an Internet Explorer plugin, to make it work for web search.",
                "We are also interested in studying how to combine term feedback with relevance feedback or implicit feedback.",
                "We could, for example, allow the user to dynamically modify terms in a language model learned from feedback documents. 8.",
                "ACKNOWLEDGMENT This work is supported in part by the National Science Foundation grants IIS-0347933 and IIS-0428472. 9.",
                "REFERENCES [1] J. Allan.",
                "Relevance feedback with too much data.",
                "In Proceedings of the 18th annual international ACM SIGIR conference on research and development in information retrieval, pages 337-343, 1995. [2] J. Allan.",
                "HARD track overview in TREC 2005 - High Accuracy Retrieval from Documents.",
                "In The Fourteenth Text REtrieval Conference, 2005. [3] P. Anick.",
                "Using terminological feedback for web search refinement: a log-based study.",
                "In Proceedings of the 26th annual international ACM SIGIR conference on research and development in informaion retrieval, pages 88-95, 2003. [4] P. G. Anick and S. Tipirneni.",
                "The paraphrase search assistant: terminological feedback for iterative information seeking.",
                "In Proceedings of the 22nd annual international ACM SIGIR conference on research and development in information retrieval, pages 153-159, 1999. [5] C. Buckley, G. Salton, J. Allan, and A. Singhal.",
                "Automatic query expansion using SMART.",
                "In Proceedings of the Third Text REtrieval Conference, 1994. [6] D. Harman.",
                "Towards interactive query expansion.",
                "In Proceedings of the 11th annual international ACM SIGIR conference on research and development in information retrieval, pages 321-331, 1988. [7] N. A. Jaleel, A. Corrada-Emmanuel, Q. Li, X. Liu, C. Wade, and J. Allan.",
                "UMass at TREC 2003: HARD and QA.",
                "In TREC, pages 715-725, 2003. [8] H. Joho, C. Coverson, M. Sanderson, and M. Beaulieu.",
                "Hierarchical presentation of expansion terms.",
                "In Proceedings of the 2002 ACM symposium on applied computing, pages 645-649, 2002. [9] K. S. Jones, S. Walker, and S. E. Robertson.",
                "A probabilistic model of information retrieval: development and status.",
                "Technical Report 446, Computer Laboratory, University of Cambridge, 1998. [10] D. Kelly, V. D. Dollu, and X. Fu.",
                "The loquacious user: a document-independent source of terms for query expansion.",
                "In Proceedings of the 28th annual international ACM SIGIR conference on research and development in information retrieval, pages 457-464, 2005. [11] D. Kelly and X. Fu.",
                "Elicitation of term relevance feedback: an investigation of term source and context.",
                "In Proceedings of the 29th annual international ACM SIGIR conference on research and development in information retrieval, 2006. [12] J. Koenemann and N. Belkin.",
                "A case for interaction: A study of interactive information retrieval behavior and effectiveness.",
                "In Proceedings of the SIGCHI conference on human factors in computing systems, pages 205-212, 1996. [13] V. Lavrenko and W. B. Croft.",
                "Relevance-based language models.",
                "In Research and Development in Information Retrieval, pages 120-127, 2001. [14] Y. Nemeth, B. Shapira, and M. Taeib-Maimon.",
                "Evaluation of the real and perceived value of automatic and interactive query expansion.",
                "In Proceedings of the 27th annual international ACM SIGIR conference on research and development in information retrieval, pages 526-527, 2004. [15] J. Ponte.",
                "A Language Modeling Approach to Information Retrieval.",
                "PhD thesis, University of Massachusetts at Amherst, 1998. [16] S. E. Robertson, S. Walker, S. Jones, M. Beaulieu, and M. Gatford.",
                "Okapi at TREC-3.",
                "In Proceedings of the Third Text REtrieval Conference, 1994. [17] J. Rocchio.",
                "Relevance feedback in information retrieval.",
                "In The SMART retrieval system, pages 313-323. 1971. [18] I. Ruthven.",
                "Re-examining the potential effectiveness of interactive query expansion.",
                "In Proceedings of the 26th annual international ACM SIGIR conference on research and development in informaion retrieval, pages 213-220, 2003. [19] G. Salton and C. Buckley.",
                "Improving retrieval performance by relevance feedback.",
                "Journal of the American Society for Information Science, 41:288-297, 1990. [20] X. Shen, B. Tan, and C. Zhai.",
                "Implicit user modeling for personalized search.",
                "In Proceedings of the 14th ACM international conference on information and knowledge management, pages 824-831, 2005. [21] X. Shen and C. Zhai.",
                "Active feedback in ad-hoc information retrieval.",
                "In Proceedings of the 28th annual international ACM SIGIR conference on research and development in information retrieval, pages 59-66, 2005. [22] A. Spink.",
                "Term relevance feedback and query expansion: relation to design.",
                "In Proceedings of the 17th annual international ACM SIGIR conference on research and development in information retrieval, pages 81-90, 1994. [23] J. Xu and W. B. Croft.",
                "Query expansion using local and global document analysis.",
                "In Proceedings of the 19th annual international ACM SIGIR conference on research and development in information retrieval, pages 4-11, 1996. [24] H. Zaragoza, N. Craswell, M. Taylor, S. Saria, and S. Robertson.",
                "Microsoft cambridge at TREC-13: Web and HARD tracks.",
                "In Proceedings of the 13th Text REtrieval Conference, 2004. [25] C. Zhai and J. Lafferty.",
                "Model-based feedback in the language modeling approach to information retrieval.",
                "In Proceedings of the tenth international conference on information and knowledge management, pages 403-410, 2001. [26] C. Zhai, A. Velivelli, and B. Yu.",
                "A cross-collection mixture model for comparative text mining.",
                "In Proceedings of the tenth ACM SIGKDD international conference on knowledge discovery and data mining, pages 743-748, 2004."
            ],
            "original_annotated_samples": [
                "This is especially helpful for <br>interactive adhoc search</br>."
            ],
            "translated_annotated_samples": [
                "Esto es especialmente útil para la <br>búsqueda interactiva ad hoc</br>."
            ],
            "translated_text": "Comentarios sobre el término para la recuperación de información con modelos de lenguaje Bin Tan†, Atulya Velivelli‡, Hui Fang†, ChengXiang Zhai† Dept. En este documento estudiamos la retroalimentación basada en términos para la recuperación de información en el enfoque de modelado de lenguaje. Con la retroalimentación de términos, un usuario juzga directamente la relevancia de términos individuales sin interacción con documentos de retroalimentación, tomando el control total del proceso de expansión de la consulta. Proponemos un método basado en clusters para seleccionar términos para presentar al usuario para su evaluación, así como algoritmos efectivos para construir modelos de lenguaje de consulta refinados a partir de la retroalimentación de términos del usuario. Nuestros algoritmos han demostrado proporcionar una mejora significativa en la precisión de recuperación en comparación con una línea base sin retroalimentación, y logran un rendimiento comparable al de la retroalimentación de relevancia. Son útiles incluso cuando no hay documentos relevantes en la parte superior. Categorías y Descriptores de Asignaturas H.3.3 [Búsqueda y Recuperación de Información]: Modelos de recuperación Términos generales Algoritmos 1. En el enfoque de modelado del lenguaje para la recuperación de información, la retroalimentación a menudo se modela como la estimación de un modelo de consulta mejorado o un modelo de relevancia basado en un conjunto de documentos de retroalimentación [25, 13]. Esto está en línea con la forma tradicional de realizar retroalimentación de relevancia: presentar al usuario documentos/pasajes para su juicio de relevancia y luego extraer términos de los documentos o pasajes juzgados para expandir la consulta inicial. Es una forma indirecta de buscar la ayuda de los usuarios para la construcción del modelo de consulta, en el sentido de que el modelo de consulta refinado (basado en términos) se aprende a través de documentos/pasajes de retroalimentación, que son estructuras de alto nivel de términos. Tiene la desventaja de que los términos irrelevantes, que ocurren junto con los relevantes en el contenido evaluado, pueden ser utilizados erróneamente para la expansión de la consulta, causando efectos no deseados. Por ejemplo, para la consulta TREC del telescopio Hubble logros, cuando un documento relevante habla más sobre la reparación de los telescopios que sobre sus descubrimientos, términos irrelevantes como caminata espacial pueden ser agregados a la consulta modificada. Podemos considerar una forma más directa de involucrar a un usuario en la mejora del modelo de consulta, sin un paso intermedio de retroalimentación de documentos que pueda introducir ruido. La idea es presentar al usuario un número (razonable) de términos individuales y pedirle que juzgue la relevancia de cada término o especifique directamente sus probabilidades en el modelo de consulta. Esta estrategia ha sido discutida en [15], pero hasta donde sabemos, no ha sido estudiada seriamente en la literatura existente de modelado de lenguaje. En comparación con la retroalimentación de relevancia tradicional, este enfoque basado en términos para el refinamiento del modelo de consulta interactivo tiene varias ventajas. Primero, el usuario tiene un mejor control del modelo de consulta final a través de la manipulación directa de términos: él/ella puede dictar qué términos son relevantes, irrelevantes y posiblemente, en qué medida. Esto evita el riesgo de introducir términos no deseados en el modelo de consulta, aunque a veces el usuario introduce términos de baja calidad. Segundo, dado que un término tarda menos tiempo en ser evaluado que el texto completo o el resumen de un documento, y con tan solo alrededor de 20 términos presentados se puede lograr una mejora significativa en el rendimiento de recuperación (como demostraremos más adelante), el feedback de términos permite recopilar la retroalimentación del usuario de manera más rápida. Esto es especialmente útil para la <br>búsqueda interactiva ad hoc</br>. En tercer lugar, a veces no hay documentos relevantes en los primeros N resultados recuperados inicialmente si el tema es difícil. Esto suele ser cierto cuando N está limitado a ser pequeño, lo cual surge del hecho de que el usuario no está dispuesto a evaluar demasiados documentos. En este caso, la retroalimentación de relevancia es inútil, ya que no se puede aprovechar ningún documento relevante, pero la retroalimentación de términos sigue siendo útil, al permitir seleccionar términos relevantes de documentos irrelevantes. Durante nuestra participación en la pista HARD de TREC 2005 y el estudio continuado posteriormente, exploramos cómo aprovechar la retroalimentación de términos por parte del usuario para construir modelos de consulta mejorados para la recuperación de información en el enfoque de modelado de lenguaje. Identificamos dos sub tareas clave del feedback basado en términos, es decir, la selección de términos para la presentación previa al feedback y la construcción del modelo de consulta posterior al feedback, con algoritmos efectivos desarrollados para ambos. Impusimos una estructura de clúster secundaria en los términos y descubrimos que una vista de clúster arroja una mayor comprensión de la necesidad de información de los usuarios, y proporciona una buena forma de utilizar la retroalimentación de términos. A través de experimentos encontramos que el feedback a término mejora significativamente sobre la línea base sin feedback, a pesar de que el usuario a menudo comete errores en la evaluación de relevancia. Entre nuestros algoritmos, el que tiene el mejor rendimiento de recuperación es TCFB, la combinación de TFB, el algoritmo de retroalimentación de términos directos, y CFB, el algoritmo de retroalimentación basado en clústeres. También variamos el número de términos de retroalimentación y observamos una mejora razonable incluso con números bajos. Finalmente, al comparar la retroalimentación a nivel de términos con la retroalimentación a nivel de documentos, encontramos que es una alternativa viable a esta última con un rendimiento competitivo en la recuperación. El resto del documento está organizado de la siguiente manera. La sección 2 discute algunos trabajos relacionados. La sección 4 describe nuestro enfoque general para la retroalimentación de términos. Presentamos nuestro método para la selección de términos de presentación en la Sección 3 y los algoritmos para la construcción del modelo de consulta en la Sección 5. Los resultados del experimento se presentan en la Sección 6. La sección 7 concluye este documento. 2. TRABAJO RELACIONADO La retroalimentación de relevancia[17, 19] ha sido reconocida durante mucho tiempo como un método efectivo para mejorar el rendimiento de recuperación. Normalmente, los N documentos principales recuperados utilizando la consulta original se presentan al usuario para su evaluación, después de lo cual se extraen términos de los documentos relevantes evaluados, ponderados por su potencial de atraer más documentos relevantes, y se añaden al modelo de consulta. La consulta ampliada suele representar mejor la necesidad de información de los usuarios que la original, que a menudo es solo una consulta de palabras clave cortas. Una segunda iteración de recuperación utilizando esta consulta modificada generalmente produce un aumento significativo en la precisión de la recuperación. En los casos en los que no se dispone de una evaluación de relevancia real y se asume que todos los documentos principales N son relevantes, se denomina retroalimentación ciega o pseudo [5, 16] y generalmente sigue mejorando el rendimiento. Debido a que un documento es una unidad de texto grande, cuando se utiliza para retroalimentación de relevancia, muchos términos irrelevantes pueden ser introducidos en el proceso de retroalimentación. Para superar esto, se propone el feedback de pasajes y se muestra que mejora el rendimiento del feedback. Una solución más directa es pedir al usuario su juicio de relevancia sobre los términos de retroalimentación. Por ejemplo, en algunos sistemas de retroalimentación de relevancia como [12], hay un paso de interacción que permite al usuario agregar o eliminar términos de expansión después de que son extraídos automáticamente de los documentos relevantes. Esto se clasifica como expansión interactiva de consultas, donde la consulta original se amplía con términos proporcionados por el usuario, que pueden provenir de la entrada directa del usuario (texto libre o palabras clave) o de la selección del usuario de términos sugeridos por el sistema (utilizando tesauros o extraídos de documentos de retroalimentación). En muchos casos, se ha encontrado que la retroalimentación de relevancia de términos mejora de manera efectiva el rendimiento de recuperación [6, 22, 12, 4, 10]. Por ejemplo, el estudio en [12] muestra que el usuario prefiere tener un conocimiento explícito y control directo de qué términos se utilizan para la expansión de la consulta, y se demuestra que la interfaz penetrable que proporciona esta libertad tiene un mejor rendimiento que otras interfaces. Sin embargo, en algunos otros casos no hay un beneficio significativo[3, 14], incluso si al usuario le gusta interactuar con los términos de expansión. En un estudio simulado realizado en [18], el autor compara el rendimiento de recuperación de la expansión interactiva de consultas y la expansión automática de consultas con un estudio simulado, y sugiere que los posibles beneficios de la primera pueden ser difíciles de lograr. Se descubre que el usuario no es bueno identificando términos útiles para la expansión de la consulta, cuando una interfaz de presentación de términos simple no puede proporcionar suficiente contexto semántico de los términos de retroalimentación. Nuestro trabajo difiere de los anteriores en dos aspectos importantes. Primero, al elegir los términos para presentar al usuario para la evaluación de relevancia, no solo consideramos el valor de un solo término (por ejemplo, la frecuencia relativa de un término en los documentos principales, que puede medirse mediante métricas como el Valor de Selección de Robertson y la Distancia Kullback-Leibler Simplificada como se detalla en [24]), sino que también examinamos la estructura de los grupos de términos, con el fin de producir una cobertura equilibrada de los diferentes aspectos del tema. Segundo, con el marco de modelado del lenguaje, permitimos una construcción detallada del modelo de consulta actualizado, estableciendo diferentes probabilidades para diferentes términos según si es un término de consulta, su importancia en los documentos principales y su pertenencia a un clúster. Aunque existen técnicas para ajustar los pesos de los términos de consulta en modelos de espacio vectorial y modelos de relevancia probabilística, la mayoría de los trabajos mencionados no las utilizan, optando por simplemente añadir términos de retroalimentación a la consulta original (usando pesos iguales para ellos), lo que puede llevar a un rendimiento de recuperación más pobre. La combinación de los dos aspectos permite que nuestro método funcione mucho mejor que el punto de referencia. La forma habitual de presentar los términos de retroalimentación es simplemente mostrar los términos en una lista. Ha habido algunos trabajos sobre interfaces de usuario alternativas. [8] organiza los términos en una jerarquía, y [11] compara tres interfaces diferentes, incluyendo términos + casillas de verificación, términos + contexto (oraciones) + casillas de verificación, oraciones + cuadro de texto de entrada. En ambos estudios, sin embargo, no hay una diferencia significativa en el rendimiento. En nuestro trabajo adoptamos el enfoque más simple de términos + casillas de verificación. Nos enfocamos en la presentación de términos y la construcción de modelos de consulta a partir de términos de retroalimentación, y creemos que el uso de contextos para mejorar la calidad de los términos de retroalimentación debería ser ortogonal a nuestro método. 3. Enfoque general: Seguimos el enfoque de modelado del lenguaje y basamos nuestro método en el modelo de recuperación de divergencia KL propuesto en [25]. Con este modelo, la tarea de recuperación implica estimar un modelo de lenguaje de consulta θq a partir de una consulta dada, un modelo de lenguaje de documento θd de cada documento, y calcular su divergencia KL D(θq||θd), que luego se utiliza para puntuar los documentos. [25] trata la retroalimentación de relevancia como un problema de reestimación del modelo de consulta, es decir, calcular un modelo de consulta actualizado θq dado el texto de la consulta original y la evidencia adicional proporcionada por los documentos relevantes juzgados. Adoptamos esta perspectiva y planteamos nuestra tarea como la actualización del modelo de consulta a partir de la retroalimentación de términos de usuario. Aquí hay dos tareas clave: Primero, cómo elegir los mejores términos para presentar al usuario para su evaluación, con el fin de recopilar la máxima evidencia sobre la necesidad de información del usuario. Segundo, cómo calcular un modelo de consulta actualizado basado en esta retroalimentación de términos, de manera que capture la necesidad de información de los usuarios y se traduzca en un buen rendimiento de recuperación. 4. SELECCIÓN DE TÉRMINOS DE PRESENTACIÓN La selección adecuada de los términos que se presentarán al usuario para su evaluación es crucial para el éxito de la retroalimentación de términos. Si los términos están mal elegidos y hay pocos relevantes, el usuario tendrá dificultades para buscar términos útiles que ayuden a aclarar su necesidad de información. Si los términos relevantes son abundantes, pero todos se centran en un solo aspecto del tema de la consulta, entonces solo podremos obtener retroalimentación sobre ese aspecto y perderemos otros, lo que resultará en una pérdida de amplitud en los resultados recuperados. Por lo tanto, es importante seleccionar cuidadosamente los términos de presentación para maximizar la ganancia esperada de la retroalimentación del usuario, es decir, aquellos que pueden revelar potencialmente la mayor evidencia de la necesidad de información del usuario. Esto es similar al feedback activo[21], que sugiere que un sistema de recuperación debería sondear activamente la necesidad de información de los usuarios, y en el caso del feedback de relevancia, los documentos de feedback deberían ser elegidos para maximizar los beneficios de aprendizaje (por ejemplo, de manera diversa para aumentar la cobertura). En nuestro enfoque, los N documentos principales de una recuperación inicial utilizando la consulta original forman la fuente de términos de retroalimentación: todos los términos que aparecen en ellos se consideran candidatos para presentar al usuario. Estos documentos sirven como pseudo-retroalimentación, ya que proporcionan un contexto mucho más amplio que la consulta original (generalmente muy breve), sin que se le pida al usuario que juzgue su relevancia. Debido a esta última razón, es posible hacer que N sea bastante grande (por ejemplo, en nuestros experimentos establecimos N = 60) para aumentar su cobertura de diferentes aspectos en el tema. La forma más sencilla de seleccionar términos de retroalimentación es elegir los M términos más frecuentes de los N documentos. Este método, sin embargo, tiene dos inconvenientes. Primero, se seleccionarán muchos términos ruidosos comunes debido a sus altas frecuencias en la colección de documentos, a menos que se utilice una lista de palabras vacías para filtrar. En segundo lugar, la lista de presentación tiende a estar compuesta por términos de los aspectos principales del tema; es probable que se pasen por alto aquellos de un aspecto menor debido a sus frecuencias relativamente bajas. Resolvemos los problemas anteriores mediante dos medidas correspondientes. Primero, introducimos un modelo de fondo θB que se estima a partir de estadísticas de la colección y explica los términos comunes, de modo que es mucho menos probable que aparezcan en la lista de presentación. Segundo, los términos se seleccionan de múltiples grupos en los documentos de retroalimentación simulada, para garantizar una representación suficiente de diferentes aspectos del tema. Nos basamos en el modelo multinomial de mezcla, que se utiliza para el descubrimiento de temas en [26]. Específicamente, asumimos que los N documentos contienen K grupos {Ci| i = 1, 2, · · · K}, cada uno caracterizado por una distribución de palabras multinomial (también conocida como modelo de lenguaje unigrama) θi y correspondiente a un aspecto del tema. Los documentos se consideran como muestreados de una mezcla de K + 1 componentes, incluidos los K grupos y el modelo de fondo: p(w|d) = λBp(w|θB) + (1 − λB) K i=1 πd,ip(w|θi) donde w es una palabra, λB es el peso de la mezcla para el modelo de fondo θB, y πd,i es el peso de la mezcla específico del documento para el modelo de cluster i-ésimo θi. Luego estimamos los modelos de clúster maximizando la probabilidad de que los documentos de pseudo-retroalimentación sean generados a partir del modelo de mezcla multinomial: log p(D|Λ) = d∈D w∈V c(w; d) log p(w|d) donde D = {di| i = 1, 2, · · · N} es el conjunto de los N documentos, V es el vocabulario, c(w; d) es la frecuencia de w en d y Λ = {θi| i = 1, 2, · · · K} ∪ {πdij | i = 1, 2, · · · N, j = 1, 2, · · · K} es el conjunto de parámetros del modelo a estimar. Los modelos de conglomerados pueden ser estimados eficientemente utilizando el algoritmo de Expectation-Maximization (EM). Para más detalles, remitimos al lector a [26]. La Tabla 1 muestra los modelos de clúster para la consulta de TREC sobre desastres en túneles de transporte (K = 3). Ten en cuenta que solo el grupo central es relevante. Tabla 1: Modelos de clúster para el tema 363 Desastres en túneles de transporte Clúster 1 Clúster 2 Clúster 3 túnel 0.0768 túnel 0.0935 túnel 0.0454 transporte 0.0364 fuego 0.0295 transporte 0.0406 tráfico 0.0206 camión 0.0236 peaje 0.0166 ferrocarril 0.0186 francés 0.0220 Amtrak 0.0153 puerto 0.0146 humo 0.0157 tren 0.0129 riel 0.0140 coche 0.0154 aeropuerto 0.0122 puente 0.0139 italiano 0.0152 autopista 0.0105 kilómetro 0.0136 incendio 0.0144 lui 0.0095 camión 0.0133 llamas 0.0127 Jersey 0.0093 construcción 0.0131 blanco 0.0121 paso 0.0087 · · · · · · · · · De cada uno de los K clústeres estimados, elegimos los L = M/K términos con las probabilidades más altas para formar un total de M términos de presentación. Si un término resulta estar en el top L en múltiples grupos, lo asignamos al grupo donde tenga la probabilidad más alta y permitimos que los otros grupos tomen un término adicional como compensación. También filtramos los términos en el texto de la consulta original porque tienden a ser siempre relevantes cuando la consulta es corta. Los términos seleccionados son luego presentados al usuario para su evaluación. Se muestra un formulario de retroalimentación (completado) de muestra en la Figura 1. En este estudio solo tratamos con juicios binarios: un término presentado está por defecto sin marcar, y un usuario puede marcarlo para indicar relevancia. Tampoco explotamos explícitamente la retroalimentación negativa (es decir, penalizar términos irrelevantes), porque con la retroalimentación binaria un término no verificado no es necesariamente irrelevante (quizás el usuario no está seguro de su relevancia). Podríamos pedir al usuario un juicio más detallado (por ejemplo, eligiendo entre altamente relevante, algo relevante, no sé, algo irrelevante y altamente irrelevante), pero la retroalimentación binaria es más compacta, ocupando menos espacio para mostrar y requiriendo menos esfuerzo por parte del usuario para emitir un juicio. 5. ESTIMACIÓN DE MODELOS DE CONSULTA A PARTIR DE RETROALIMENTACIÓN DE TÉRMINOS En esta sección, presentamos varios algoritmos para aprovechar la retroalimentación de términos. Los algoritmos toman como entrada la consulta original q, los grupos {θi} generados por el algoritmo de descubrimiento de temas, el conjunto de términos de retroalimentación T y su juicio de relevancia R, y producen un modelo de lenguaje de consulta actualizado θq que hace el mejor uso de la evidencia de retroalimentación para capturar la necesidad de información de los usuarios. Primero describimos nuestras notaciones: • θq: El modelo de consulta original, derivado solo de los términos de la consulta: p(w|θq) = c(w; q) |q| donde c(w; q) es la cantidad de veces que w aparece en q, y |q| = w∈q c(w; q) es la longitud de la consulta. • θq: El modelo de consulta actualizado que necesitamos estimar a partir de la retroalimentación de términos. • θi (i = 1, 2, . . . K): El modelo de lenguaje unigrama del clúster Ci, estimado utilizando el algoritmo de descubrimiento de temas. • T = {ti,j} (i = 1 . . . K, j = 1 . . . \n\nK, j = 1 . . . El conjunto de términos presentados al usuario para su evaluación. ti,j es el j-ésimo término elegido del grupo Ci. • R = {δw|w ∈ T}: δw es una variable indicadora que es 1 si w es considerado relevante o 0 en caso contrario. 5.1 TFB (Retroalimentación Directa de Términos) Esta es una forma directa de retroalimentación de términos que no implica ninguna estructura secundaria. Asignamos un peso de 1 a los términos considerados relevantes por el usuario, un peso de μ a los términos de la consulta, un peso de cero a los demás términos, y luego aplicamos la normalización: p(w|θq ) = δw + μ c(w; q) w ∈T δw + μ|q| donde w ∈T δw es el número total de términos considerados relevantes. Llamamos a este método TFB (retroalimentación directa de términos). Si dejamos que μ = 1, este enfoque es equivalente a agregar los términos relevantes después de la consulta original, que es lo que hace la expansión estándar de consultas (sin reponderación de términos). Si establecemos μ > 1, estamos poniendo más énfasis en los términos de consulta que en los verificados. Ten en cuenta que el modelo de resultado será más sesgado hacia θq si la consulta original es larga o el feedback del usuario es débil, lo cual tiene sentido, ya que podemos confiar más en la consulta original en ambos casos. Figura 1: Formulario de aclaración completado para el Tema 363 363 desastres en túneles de transporte. Por favor, seleccione todos los términos relevantes para el tema. tráfico ferrocarril puerto puente ferroviario kilómetro construir suizo enlace cruzado hongkonés río proyecto metro camión de bomberos francés humo coche italiano bomberos incendio blanco montaña víctima francés rescate conductor chamonix emerger peaje tren amtrak aeropuerto turnpike jersey pass rome z centro electrón carretera boston velocidad bu enviar 5.2 CFB (Retroalimentación de Clúster) Aquí explotamos la estructura de clúster que jugó un papel importante cuando seleccionamos los términos de presentación. Los grupos representan diferentes aspectos del tema de la consulta, cada uno de los cuales puede ser relevante o no. Si somos capaces de identificar los grupos relevantes, podemos combinarlos para generar un modelo de consulta que sea bueno para descubrir documentos pertenecientes a estos grupos (en lugar de los irrelevantes). Podríamos pedir al usuario que juzgue directamente la relevancia de un grupo después de ver los términos representativos en ese grupo, pero a veces esto sería una tarea difícil para el usuario, quien tendría que adivinar la semántica de un grupo a través de su conjunto de términos, los cuales podrían no estar bien conectados entre sí debido a la falta de contexto. Por lo tanto, proponemos aprender retroalimentación de clúster de forma indirecta, inferir la relevancia de un clúster a través de la relevancia de sus términos de retroalimentación. Dado que cada grupo tiene un número igual de términos presentados al usuario, la medida más simple de la relevancia de un grupo es el número de términos que se consideran relevantes en él. Intuitivamente, cuantos más términos estén marcados como relevantes en un grupo, más cerca estará el grupo del tema de la consulta y más debería participar en la modificación de la consulta. Si combinamos los modelos de clúster utilizando pesos determinados de esta manera y luego interpolamos con el modelo de consulta original, obtenemos la siguiente fórmula para la actualización de la consulta, que llamamos CFB (Retroalimentación de Clúster): p(w|θq) = λp(w|θq) + (1 − λ) K i=1 L j=1 δti,j K k=1 L j=1 δtk,j p(w|θi) donde L j=1 δti,j es el número de términos relevantes en el clúster Ci, y K k=1 L j=1 δtk,j es el número total de términos relevantes. Observamos que cuando solo hay un clúster (K = 1), la fórmula anterior se degrada a p(w|θq ) = λp(w|θq) + (1 − λ)p(w|θ1) que es simplemente una pseudo-retroalimentación de la forma propuesta en [25]. TCFB (Retroalimentación Término-Clúster) TFB y CFB ambos tienen sus inconvenientes. TFB asigna probabilidades no nulas a los términos presentados que están marcados como relevantes, pero ignora por completo (muchos más) otros, que pueden quedar sin marcar debido a la ignorancia de los usuarios, o simplemente no estar incluidos en la lista de presentación, pero deberíamos ser capaces de inferir su relevancia a partir de los marcados. Por ejemplo, en la Figura 1, dado que se revisan hasta 5 términos en el grupo central (las tercera y cuarta columnas), deberíamos tener una alta confianza en la relevancia de otros términos en ese grupo. CFB resuelve el problema de TFB tratando los términos en un grupo de manera colectiva, de modo que los términos no verificados/no presentados reciban pesos cuando los términos presentados en sus grupos son considerados relevantes, pero no distingue qué términos en un grupo son presentados o considerados. Intuitivamente, los términos considerados relevantes deberían recibir pesos mayores porque son indicados explícitamente como relevantes por el usuario. Por lo tanto, intentamos combinar los dos métodos, con la esperanza de obtener lo mejor de ambos. Lo hacemos interpolando el modelo TFB con el modelo CFB, y lo llamamos TCFB: p(w|θq ) = αp(w|θqT F B ) + (1 − α)p(w|θqCF B ) 6. EXPERIMENTOS En esta sección, describimos los resultados de nuestro experimento. Primero describimos nuestra configuración experimental y presentamos una visión general del rendimiento de varios métodos. Luego discutimos los efectos de variar la configuración de los parámetros en los algoritmos, así como el número de términos de presentación. A continuación analizamos el comportamiento de retroalimentación de los usuarios y su relación con el rendimiento de recuperación. Finalmente comparamos la retroalimentación de términos con la retroalimentación de relevancia y mostramos que tiene su ventaja particular. 6.1 Configuración del Experimento y Resultados Básicos Aprovechamos la oportunidad de la pista HARD de TREC 2005[2] para la evaluación de nuestros algoritmos. Las pistas utilizaron la colección AQUAINT, un corpus de 3GB de texto de noticias en inglés. Los temas incluyeron 50 que se sabía que eran difíciles, es decir, con bajo rendimiento de recuperación. Es para estos temas difíciles que la retroalimentación del usuario es más útil, ya que puede proporcionar información para desambiguar las consultas; con temas fáciles, es posible que el usuario no esté dispuesto a esforzarse por dar retroalimentación si los resultados de recuperación automática son lo suficientemente buenos. Los participantes de la pista pudieron enviar formularios de aclaración (CF) diseñados a medida para solicitar retroalimentación de los evaluadores humanos proporcionados por la Tabla 2: Rendimiento de recuperación para diferentes métodos y tipos de CF. La última fila es el porcentaje de mejora del MAP sobre la línea base. Los ajustes de parámetros μ = 4, λ = 0.1, α = 0.3 son casi óptimos. Variación de MAP con el número de términos presentados. # términos TFB1C TFB3C TFB6C CFB3C CFB6C TCFB3C TCFB6C 6 0.245 0.240 0.227 0.279 0.279 0.281 0.274 12 0.261 0.261 0.242 0.299 0.286 0.297 0.281 18 0.275 0.274 0.256 0.301 0.282 0.300 0.286 24 0.276 0.281 0.265 0.303 0.292 0.305 0.292 30 0.280 0.285 0.270 0.304 0.296 0.307 0.296 36 0.282 0.288 0.272 0.307 0.297 0.309 0.297 42 0.283 0.288 0.275 0.306 0.298 0.309 0.300 48 0.288 0.288 0.278 0.305 0.301 0.309 0.303 NIST. Diseñamos tres conjuntos de formularios de aclaración para la retroalimentación de términos, diferenciados en la elección de K, el número de grupos, y L, el número de términos presentados de cada grupo. Son: 1 × 48, un gran grupo con 48 términos, 3 × 16, 3 grupos con 16 términos cada uno, y 6 × 8, 6 grupos con 8 términos cada uno. El número total de términos presentados (M) está fijo en 48, por lo que al comparar el rendimiento de diferentes tipos de formas de clarificación podemos conocer los efectos de diferentes grados de agrupamiento. Para cada tema, un evaluador completaría los formularios en el orden de 6 × 8, 1 × 48 y 3 × 16, dedicando hasta tres minutos en cada formulario. El formulario de aclaración de muestra mostrado en la Figura 1 es de tipo 3 × 16. Es una interfaz simple y compacta en la que el usuario puede consultar términos relevantes. El formulario es autoexplicativo; no es necesario brindar capacitación adicional al usuario sobre cómo utilizarlo. Nuestras consultas iniciales se construyen solo utilizando las descripciones de los títulos de los temas, que tienen en promedio 2.7 palabras de longitud. Como base utilizamos el método de recuperación de divergencia KL implementado en la herramienta Lemur con 5 documentos de retroalimentación pseudo. Aplicamos el suavizado de Dirichlet con una prior de 2000, y truncamos los modelos de lenguaje de consulta a 50 términos (estas configuraciones se utilizan en todos los experimentos). Para todos los demás parámetros, utilizamos la configuración predeterminada de Lemurs. El punto de referencia resulta tener un rendimiento por encima del promedio entre los participantes de la pista. Después de una ejecución inicial utilizando este método de recuperación de referencia, tomamos los 60 documentos principales para cada tema y aplicamos el algoritmo de descubrimiento de temas para producir los grupos (1, 3 o 6 de ellos), en base a los cuales generamos formularios de aclaración. Después de recibir la retroalimentación del usuario, ejecutamos los algoritmos de retroalimentación de términos (TFB, CFB o TCFB) para estimar modelos de consulta actualizados, los cuales luego se utilizan para una segunda iteración de recuperación. Evaluamos el rendimiento de los diferentes métodos de recuperación en sus clasificaciones de los 1000 documentos principales. Las métricas de evaluación que adoptamos incluyen la precisión media promedio (no interpolada) (MAP), precisión en los primeros 30 (Pr@30) y total relevante recuperado (RR). La Tabla 2 muestra el rendimiento de varios métodos y configuraciones de K × L. Los sufijos (1C, 3C, 6C) después de TFB, CFB, TCFB representan el número de grupos (K). Por ejemplo, TCFB3C significa el método TCFB en los formularios de aclaración de 3 × 16. De la Tabla 2 podemos hacer las siguientes observaciones: 1 http://www.lemurproject.com 1. Todos los métodos tienen un rendimiento considerablemente mejor que la línea base de pseudoretroalimentación, con TCFB3C logrando una mejora máxima del 41.1% en el MAP, lo que indica una contribución significativa de la retroalimentación de términos para la clarificación de la necesidad de información de los usuarios. En otras palabras, el feedback a término es realmente útil para mejorar la precisión de recuperación. 2. Para TFB, el rendimiento es casi igual en los formularios de aclaración de 1 × 48 y 3 × 16 en términos de MAP (aunque este último es ligeramente mejor en Pr@30 y RR), y un poco peor en los de 6 × 8. 3. Tanto CFB3C como CFB6C tienen un mejor rendimiento que sus contrapartes TFB en las tres métricas, lo que sugiere que el feedback sobre una estructura de clúster secundaria es realmente beneficioso. CFB1C es en realidad peor porque no puede ajustar el peso de su (único) grupo a partir de la retroalimentación de términos y es simplemente pseudoretroalimentación. 4. Aunque TCFB es solo una simple mezcla de TFB y CFB por interpolación, logra superar a ambos. Esto respalda nuestra especulación de que TCFB supera las desventajas de TFB (prestando atención solo a los términos marcados) y CFB (sin distinguir los términos marcados y no marcados en un grupo). Excepto por TCFB6C v.s. CFB6C, la ventaja de rendimiento de TCFB sobre TFB/CFB es significativa con p < 0.05 utilizando la prueba de rango con signo de Wilcoxon. Esto no es cierto en el caso de TFB v.s. CFB, cada uno de los cuales es mejor que el otro en casi la mitad de los temas. 6.2 Reducción de Términos de Presentación En algunas situaciones podemos tener que reducir el número de términos de presentación debido a limitaciones en el espacio de visualización o esfuerzos de retroalimentación del usuario. Es interesante saber si el rendimiento de nuestros algoritmos se deteriora cuando al usuario se le presentan menos términos. Dado que los términos de presentación dentro de cada grupo se generan en orden decreciente de sus frecuencias, la lista de presentación forma un subconjunto del original si su tamaño se reduce2. Por lo tanto, podemos simular fácilmente lo que sucede cuando el número de términos de presentación disminuye 2. Hay complejidades que surgen de los términos que aparecen en la parte superior L de múltiples grupos, pero estas son excepciones de M a M: mantendremos todos los juicios de los términos superiores L = M / K en cada grupo y descartaremos los de los demás. La Tabla 3 muestra el rendimiento de varios algoritmos a medida que el número de términos de presentación varía de 6 a 48. Observamos que el rendimiento de TFB es más susceptible a la reducción de términos de presentación que el de CFB o TCFB. Por ejemplo, en 12 términos el MAP de TFB3C es el 90.6% del que se obtiene en 48 términos, mientras que los números para CFB3C y TCFB3C son del 98.0% y 96.1% respectivamente. Conjeturamos que la razón es que mientras el rendimiento de TFB depende en gran medida de cuántos términos buenos se elijan para la expansión de la consulta, CFB solo necesita una estimación aproximada de los pesos de los clústeres para funcionar. Además, las formas de aclaración de 3 × 16 parecen ser más robustas que las de 6 × 8: con 12 términos, el MAP de TFB6C es el 87.1% del obtenido con 48 términos, por debajo del 90.6% de TFB3C. De manera similar, para CFB es del 95.0% en comparación con el 98.0%. Esto es natural, ya que para un gran número de 6 clusters, es más fácil llegar a la situación en la que cada cluster recibe muy pocos términos de presentación para que la diversificación de temas sea útil. En general, nos sorprende ver que los algoritmos aún pueden funcionar razonablemente bien cuando el número de términos de presentación es pequeño. Por ejemplo, con solo 12 términos, CFB3C (el formulario de aclaración es de tamaño 3 × 4) aún puede mejorar un 36.5% sobre el valor base, disminuyendo ligeramente desde el 39.3% con 48 términos. En el Análisis de Retroalimentación del Usuario, estudiamos varios aspectos del comportamiento de retroalimentación de los usuarios en términos, y si están conectados al rendimiento de recuperación. Figura 2: Distribuciones de tiempo de completación del formulario de aclaración 0−30 30−60 60−90 90−120 120−150 150−180 0 5 10 15 20 25 30 35 tiempo de completación (segundos) #temas 1×48 3×16 6×8 La Figura 2 muestra la distribución del tiempo necesario para completar un formulario de aclaración. Observamos que el usuario suele ser capaz de completar el feedback del término en un tiempo razonablemente corto: para más de la mitad de los temas, el formulario de aclaración se completa en solo 1 minuto, y solo una pequeña fracción de temas (menos del 10% para 1 × 48 y 3 × 16) tarda más de 2 minutos. Esto sugiere que la retroalimentación a corto plazo es adecuada para la recuperación interactiva ad-hoc, donde un usuario generalmente no desea dedicar demasiado tiempo a proporcionar retroalimentación. Observamos que un usuario a menudo comete errores al juzgar la relevancia de un término. A veces, un término relevante puede ser omitido porque su conexión con el tema de la consulta no es obvia para el usuario. En otras ocasiones, puede incluirse un término dudoso que resulta ser irrelevante. Toma como ejemplo el tema en la Figura 1. Hubo un desastre de incendio en Mont 3. El tiempo máximo es de 180 segundos, ya que el evaluador del NIST estaría obligado a enviar el formulario en ese momento. Tabla 4: Estadísticas de selección de términos (promedio del tema) CF Tipo 1 × 48 3 × 16 6 × 8 # términos revisados 14.8 13.3 11.2 # términos relevantes 15.0 12.6 11.2 # términos relevantes revisados 7.9 6.9 5.9 precisión 0.534 0.519 0.527 recuperación 0.526 0.548 0.527 El usuario no seleccionó palabras clave como mont, blanc, francés e italiano debido a su desconocimiento del evento del Túnel del Mont Blanc entre Francia e Italia en 1999. De hecho, sin un contexto adecuado sería difícil hacer un juicio perfecto. ¿Cuál es entonces, el nivel en el que el usuario es bueno dando retroalimentación? ¿Tiene un impacto serio en el rendimiento de recuperación? Para responder a estas preguntas, necesitamos una medida de la verdadera relevancia de los términos individuales. Adoptamos la métrica de Divergencia KL Simplificada utilizada en [24] para decidir los términos de expansión de la consulta como nuestra medida de relevancia de términos: σKLD(w) = p(w|R) log p(w|R) p(w|¬R) donde p(w|R) es la probabilidad de que un documento relevante contenga el término w, y p(w|¬R) es la probabilidad de que un documento irrelevante contenga w, ambas pueden ser fácilmente calculadas mediante una estimación de máxima verosimilitud dada la relevancia a nivel de documento. Si σKLD(w) > 0, w es más probable que aparezca en documentos relevantes que en irrelevantes. Consideramos un término relevante si su valor de Divergencia KL Simplificada es mayor que un umbral determinado σ0. Podemos entonces definir la precisión y la exhaustividad del juicio de términos del usuario de la siguiente manera: la precisión es la fracción de términos revisados por el usuario que son relevantes; la exhaustividad es la fracción de términos relevantes presentados que son revisados por el usuario. La Tabla 4 muestra el número de términos revisados, términos relevantes y términos relevantes revisados cuando σ0 se establece en 1.0, así como la precisión/recuperación del juicio de términos del usuario. Ten en cuenta que cuando los formularios de aclaración contienen más grupos, se revisan menos términos: 14.8 para 1 × 48, 13.3 para 3 × 16 y 11.2 para 6×8. El patrón similar se mantiene para los términos relevantes y los términos relevantes verificados. Parece haber un compromiso entre aumentar la diversidad de temas mediante el agrupamiento y perder términos adicionales relevantes: cuando hay más grupos, cada uno recibe menos términos para presentar, lo que puede perjudicar a un grupo relevante importante que contiene muchos términos relevantes. Por lo tanto, no siempre es útil tener más grupos, por ejemplo, TFB6C es en realidad peor que TFB1C. El hallazgo principal que podemos obtener de la Tabla 4 es que el usuario no es particularmente bueno identificando términos relevantes, lo cual coincide con el descubrimiento en [18]. En el caso de 3 formularios de aclaración de 16, el número promedio de términos marcados como relevantes por el usuario es de 13.3 por tema, y el número promedio de términos relevantes cuyo valor de σKLD excede 1.0 es de 12.6. El usuario es capaz de reconocer solo 6.9 de estos términos en promedio. De hecho, la precisión y la exhaustividad de los términos de retroalimentación de los usuarios (como se definió anteriormente) están lejos de ser perfectas. Por otro lado, si el usuario hubiera verificado correctamente todos esos términos relevantes, el rendimiento de nuestros algoritmos habría aumentado considerablemente, como se muestra en la Tabla 5. Vemos que TFB experimenta una gran mejora cuando hay un oráculo que verifica todos los términos relevantes, mientras que CFB se encuentra con un cuello de botella alrededor de un MAP de 0.325, ya que todo lo que hace es ajustar los pesos de los clústeres, y cuando los pesos aprendidos están cerca de ser precisos, no puede beneficiarse más del feedback de términos. También hay que tener en cuenta que TCFB no logra superar a TFB, probablemente porque TFB es lo suficientemente preciso. Tabla 5: Cambio de MAP al usar todos (y solo) los términos relevantes (σKLD > 1.0) para retroalimentación. retroalimentación de término original retroalimentación de término relevante TF1 0.288 0.354 TF3 0.288 0.354 TF6 0.278 0.346 CF3 0.305 0.325 CF6 0.301 0.326 TCF3 0.309 0.345 TCF6 0.304 0.341 6.4 Comparación con Retroalimentación de Relevancia Ahora comparamos la retroalimentación de términos con la retroalimentación de relevancia a nivel de documento, en la que al usuario se le presentan los N mejores documentos de una recuperación inicial y se le pide que juzgue su relevancia. El proceso de retroalimentación se simula utilizando la evaluación de relevancia de documentos de NIST. Utilizamos el método de retroalimentación basado en el modelo de mezcla propuesto en [25], con el ruido de mezcla establecido en 0.95 y el coeficiente de retroalimentación establecido en 0.9. La evaluación comparativa del feedback de relevancia frente a otros métodos se complica por el hecho de que algunos documentos ya han sido vistos durante el feedback, por lo que no tiene sentido incluirlos en los resultados de recuperación de la segunda ejecución. Sin embargo, esto no se aplica al término retroalimentación. Por lo tanto, para que sea justo en cuanto a la ganancia de información de los usuarios, si los documentos de retroalimentación son relevantes, deben mantenerse en la parte superior del ranking; si son irrelevantes, deben ser excluidos. Por lo tanto, utilizamos retroalimentación de relevancia para producir un ranking de los 1000 documentos recuperados, pero excluyendo cada documento de retroalimentación, y luego añadimos los documentos relevantes de retroalimentación al principio. La tabla 6 muestra el rendimiento de la retroalimentación de relevancia para diferentes valores de N y lo compara con TCFB3C. Tabla 6: Rendimiento de la retroalimentación de relevancia para diferentes números de documentos de retroalimentación (N). Vemos que el rendimiento de TCFB3C es comparable al de la retroalimentación de relevancia utilizando 5 documentos. Aunque es menos eficiente que cuando hay 10 documentos de retroalimentación en términos de MAP y Pr@30, sí recupera más documentos (4947) al descender por la lista clasificada. Intentamos comparar la calidad de los términos insertados automáticamente en la retroalimentación de relevancia con la de los términos seleccionados manualmente en la retroalimentación de términos. Esto se logra truncando el modelo de consulta modificado por retroalimentación de relevancia a un tamaño igual al número de términos verificados para el mismo tema. Podemos comparar los términos en el modelo truncado con los términos verificados. La Figura 3 muestra la distribución de los puntajes σKLD de los términos. Observamos que el término \"feedback\" tiende a producir términos de expansión de mayor calidad (aquellos con σKLD > 1) en comparación con el \"relevance feedback\" (con 10 documentos de feedback). Esto no contradice el hecho de que este último produce un rendimiento de recuperación más alto. De hecho, cuando usamos el modelo de consulta truncada en lugar del modelo íntegro refinado a partir de la retroalimentación de relevancia, el MAP es solo 0.304. La verdad Figura 3: Comparación de la calidad del término de expansión entre retroalimentación de relevancia (con 10 documentos de retroalimentación) y retroalimentación de términos (con 3 × 16 CFs) −1−0 0−1 1−2 2−3 3−4 4−5 5−6 0 50 100 150 200 250 300 350 σKLD #términos retroalimentación de relevancia retroalimentación de términos es que, aunque hay muchos términos no deseados en el modelo de consulta expandido a partir de los documentos de retroalimentación, también hay más términos relevantes de los que el usuario puede seleccionar de la lista de términos de presentación generados con documentos de pseudo-retroalimentación, y los efectos positivos a menudo superan a los negativos. Estamos interesados en saber en qué circunstancias la retroalimentación basada en términos tiene ventajas sobre la retroalimentación basada en relevancia. Una de esas situaciones es cuando ninguno de los documentos de retroalimentación principales es relevante, lo que hace que la retroalimentación de relevancia sea inútil. Esto no es infrecuente, como uno podría haber pensado: de los 50 temas, hay 13 casos así cuando N = 5, 10 cuando N = 10, y aún 3 cuando N = 20. Cuando esto sucede, solo se puede retroceder al método original de recuperación; se pierde el poder de la retroalimentación de relevancia. Sorprendentemente, en 11 de 13 casos en los que parece imposible el feedback de relevancia, el usuario puede marcar al menos 2 términos relevantes de los 3 × 16 del formulario de aclaración (consideramos que el término t es relevante si σKLD(t) > 1.0). Además, en 10 de ellos TCFB3C supera la línea base de pseudo-retroalimentación, aumentando el MAP de 0.076 a 0.146 en promedio (estos son temas particularmente difíciles). Creemos que hay dos posibles explicaciones para este fenómeno de que la retroalimentación de términos esté activa incluso cuando la retroalimentación de relevancia no funciona: Primero, incluso si ninguno de los primeros N (supongamos que es un número pequeño) documentos son relevantes, aún podemos encontrar documentos relevantes en los primeros 60, que son más inclusivos pero generalmente inalcanzables cuando las personas están realizando retroalimentación de relevancia en la búsqueda interactiva ad-hoc, de donde podemos extraer términos de retroalimentación. Esto es cierto para el tema 367 de piratería, donde los 10 documentos principales de retroalimentación son todos sobre piratería de software, sin embargo, hay documentos entre el 10 y el 60 que tratan sobre piratería en los mares (que es la necesidad de información real), contribuyendo términos como pirata, barco para la selección en el formulario de aclaración. Segundo, para algunos temas, un documento debe cumplir con ciertas condiciones especiales para ser relevante. Los N documentos principales pueden estar relacionados con el tema, pero no obstante ser irrelevantes. En este caso, aún podemos extraer términos útiles de estos documentos, incluso si no califican como relevantes. Por ejemplo, en el tema 639 de compras en línea de consumidores, un documento necesita mencionar qué contribuye al crecimiento de las compras para realmente coincidir con la necesidad de información especificada, por lo tanto, ninguno de los 10 documentos de retroalimentación principales se considera relevante. Sin embargo, los términos de retroalimentación como venta al por menor, comercio son buenos para la expansión de consultas. 7. CONCLUSIONES En este artículo estudiamos el uso de la retroalimentación de términos para la recuperación interactiva de información en el enfoque de modelado del lenguaje. Propusimos un método basado en clusters para seleccionar términos de presentación, así como algoritmos para estimar modelos de consulta refinados a partir de la retroalimentación de términos de usuario. Observamos una mejora significativa en la precisión de recuperación proporcionada por la retroalimentación de términos, a pesar de que un usuario a menudo comete errores en la evaluación de relevancia que perjudican su rendimiento. Encontramos que el algoritmo de mejor rendimiento es TCFB, el cual se beneficia de la combinación de la evidencia de términos observados directamente con TFB y la relevancia de clúster aprendida indirectamente con CFB. Cuando redujimos el número de términos de presentación, el feedback del término aún puede mantener gran parte de su mejora de rendimiento sobre el valor base. Finalmente, comparamos la retroalimentación de términos con la retroalimentación de relevancia a nivel de documento, y encontramos que el rendimiento de TCFB3C está a la par con este último con 5 documentos de retroalimentación. Consideramos el término \"feedback\" como una alternativa viable al feedback de relevancia tradicional, especialmente cuando no hay documentos relevantes en la parte superior. Proponemos ampliar nuestro trabajo de varias maneras. Primero, queremos estudiar si el uso de varios contextos puede ayudar al usuario a identificar mejor la relevancia de los términos, sin sacrificar la simplicidad y la concisión de la retroalimentación de los términos. Segundo, actualmente todos los términos se presentan al usuario en un solo lote. En su lugar, podríamos considerar la retroalimentación iterativa de términos, presentando primero un pequeño número de términos y mostrando más términos después de recibir la retroalimentación del usuario o detenernos cuando la consulta refinada sea lo suficientemente buena. Los términos presentados deben ser seleccionados dinámicamente para maximizar los beneficios de aprendizaje en cualquier momento. Tercero, tenemos planes de incorporar retroalimentación de términos en nuestra barra de herramientas UCAIR[20], un complemento de Internet Explorer, para que funcione en búsquedas web. También estamos interesados en estudiar cómo combinar la retroalimentación de términos con la retroalimentación de relevancia o la retroalimentación implícita. Por ejemplo, podríamos permitir al usuario modificar dinámicamente los términos en un modelo de lenguaje aprendido a partir de documentos de retroalimentación. 8. AGRADECIMIENTO Este trabajo cuenta con el apoyo parcial de las becas de la Fundación Nacional de Ciencias IIS-0347933 e IIS-0428472. 9. REFERENCIAS [1] J. Allan. Retroalimentación de relevancia con demasiados datos. En Actas de la 18ª conferencia internacional anual de ACM SIGIR sobre investigación y desarrollo en recuperación de información, páginas 337-343, 1995. [2] J. Allan. Resumen de la pista HARD en TREC 2005 - Recuperación de alta precisión de documentos. En la Decimocuarta Conferencia de Recuperación de Información de 2005. [3] P. Anick. Utilizando retroalimentación terminológica para refinar la búsqueda web: un estudio basado en registros. En Actas de la 26ª conferencia internacional anual de ACM SIGIR sobre investigación y desarrollo en recuperación de información, páginas 88-95, 2003. [4] P. G. Anick y S. Tipirneni. El asistente de búsqueda de paráfrasis: retroalimentación terminológica para la búsqueda iterativa de información. En Actas de la 22ª conferencia internacional anual de ACM SIGIR sobre investigación y desarrollo en recuperación de información, páginas 153-159, 1999. [5] C. Buckley, G. Salton, J. Allan y A. Singhal. Expansión automática de consultas utilizando SMART. En Actas de la Tercera Conferencia de Recuperación de Texto, 1994. [6] D. Harman. Hacia la expansión interactiva de consultas. En Actas de la 11ª conferencia internacional anual de ACM SIGIR sobre investigación y desarrollo en recuperación de información, páginas 321-331, 1988. [7] N. A. Jaleel, A. Corrada-Emmanuel, Q. Li, X. Liu, C. Wade y J. Allan. UMass en TREC 2003: HARD y QA. En TREC, páginas 715-725, 2003. [8] H. Joho, C. Coverson, M. Sanderson y M. Beaulieu. Presentación jerárquica de términos de expansión. En Actas del simposio de informática aplicada de ACM de 2002, páginas 645-649, 2002. [9] K. S. Jones, S. Walker y S. E. Robertson. Un modelo probabilístico de recuperación de información: desarrollo y estado actual. Informe técnico 446, Laboratorio de Computación, Universidad de Cambridge, 1998. [10] D. Kelly, V. D. Dollu y X. Fu. El usuario locuaz: una fuente de términos independiente del documento para la expansión de consultas. En Actas de la 28ª conferencia internacional anual de ACM SIGIR sobre investigación y desarrollo en recuperación de información, páginas 457-464, 2005. [11] D. Kelly y X. Fu. Obtención de retroalimentación de relevancia de términos: una investigación sobre la fuente y el contexto de los términos. En Actas de la 29ª conferencia internacional anual de ACM SIGIR sobre investigación y desarrollo en recuperación de información, 2006. [12] J. Koenemann y N. Belkin. Un caso para la interacción: Un estudio del comportamiento y la efectividad de la recuperación de información interactiva. En Actas de la conferencia SIGCHI sobre factores humanos en sistemas informáticos, páginas 205-212, 1996. [13] V. Lavrenko y W. B. Croft. Modelos de lenguaje basados en relevancia. En Investigación y Desarrollo en Recuperación de Información, páginas 120-127, 2001. [14] Y. Nemeth, B. Shapira y M. Taeib-Maimon. Evaluación del valor real y percibido de la expansión automática e interactiva de consultas. En Actas de la 27ª conferencia internacional anual de ACM SIGIR sobre investigación y desarrollo en recuperación de información, páginas 526-527, 2004. [15] J. Ponte. Un enfoque de modelado del lenguaje para la recuperación de información. Tesis doctoral, Universidad de Massachusetts en Amherst, 1998. [16] S. E. Robertson, S. Walker, S. Jones, M. Beaulieu y M. Gatford. Okapi en TREC-3. En Actas de la Tercera Conferencia de Recuperación de Texto, 1994. [17] J. Rocchio. Retroalimentación de relevancia en la recuperación de información. En el sistema de recuperación SMART, páginas 313-323. 1971. [18] I. Ruthven. Reexaminando la efectividad potencial de la expansión interactiva de consultas. En Actas de la 26ª conferencia internacional anual de ACM SIGIR sobre investigación y desarrollo en recuperación de información, páginas 213-220, 2003. [19] G. Salton y C. Buckley. Mejorando el rendimiento de recuperación mediante retroalimentación de relevancia. Revista de la Sociedad Americana de Ciencia de la Información, 41:288-297, 1990. [20] X. Shen, B. Tan y C. Zhai. Modelado implícito de usuario para búsqueda personalizada. En Actas de la 14ª conferencia internacional de ACM sobre información y gestión del conocimiento, páginas 824-831, 2005. [21] X. Shen y C. Zhai. Retroalimentación activa en la recuperación de información ad-hoc. En Actas de la 28ª conferencia internacional anual de ACM SIGIR sobre investigación y desarrollo en recuperación de información, páginas 59-66, 2005. [22] A. Spink. Retroalimentación de relevancia de términos y expansión de consultas: relación con el diseño. En Actas de la 17ª conferencia internacional anual de ACM SIGIR sobre investigación y desarrollo en recuperación de información, páginas 81-90, 1994. [23] J. Xu y W. B. Croft. Expansión de consulta utilizando análisis local y global de documentos. En Actas de la 19ª conferencia internacional anual de ACM SIGIR sobre investigación y desarrollo en recuperación de información, páginas 4-11, 1996. [24] H. Zaragoza, N. Craswell, M. Taylor, S. Saria y S. Robertson. Microsoft Cambridge en TREC-13: pistas Web y HARD. En Actas de la 13ª Conferencia de Recuperación de Información de Texto, 2004. [25] C. Zhai y J. Lafferty. Retroalimentación basada en modelos en el enfoque de modelado del lenguaje para la recuperación de información. En Actas de la décima conferencia internacional sobre gestión de la información y el conocimiento, páginas 403-410, 2001. [26] C. Zhai, A. Velivelli y B. Yu. Un modelo de mezcla de colecciones cruzadas para la minería de textos comparativa. En Actas de la décima conferencia internacional de ACM SIGKDD sobre descubrimiento de conocimiento y minería de datos, páginas 743-748, 2004. ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "retrieval performance": {
            "translated_key": "rendimiento de recuperación",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Term Feedback for Information Retrieval with Language Models Bin Tan† , Atulya Velivelli‡ , Hui Fang† , ChengXiang Zhai† Dept.",
                "of Computer Science† , Dept. of Electrical and Computer Engineering‡ University of Illinois at Urbana-Champaign bintan@cs.uiuc.edu, velivell@ifp.uiuc.edu, hfang@cs.uiuc.edu, czhai@cs.uiuc.edu ABSTRACT In this paper we study term-based feedback for information retrieval in the language modeling approach.",
                "With term feedback a user directly judges the relevance of individual terms without interaction with feedback documents, taking full control of the query expansion process.",
                "We propose a cluster-based method for selecting terms to present to the user for judgment, as well as effective algorithms for constructing refined query language models from user term feedback.",
                "Our algorithms are shown to bring significant improvement in retrieval accuracy over a non-feedback baseline, and achieve comparable performance to relevance feedback.",
                "They are helpful even when there are no relevant documents in the top.",
                "Categories and Subject Descriptors H.3.3 [Information Search and Retrieval]: Retrieval models General Terms Algorithms 1.",
                "INTRODUCTION In the language modeling approach to information retrieval, feedback is often modeled as estimating an improved query model or relevance model based on a set of feedback documents [25, 13].",
                "This is in line with the traditional way of doing relevance feedback - presenting a user with documents/passages for relevance judgment and then extracting terms from the judged documents or passages to expand the initial query.",
                "It is an indirect way of seeking users assistance for query model construction, in the sense that the refined query model (based on terms) is learned through feedback documents/passages, which are high-level structures of terms.",
                "It has the disadvantage that irrelevant terms, which occur along with relevant ones in the judged content, may be erroneously used for query expansion, causing undesired effects.",
                "For example, for the TREC query Hubble telescope achievements, when a relevant document talks more about the telescopes repair than its discoveries, irrelevant terms such as spacewalk can be added into the modified query.",
                "We can consider a more direct way to involve a user in query model improvement, without an intermediary step of document feedback that can introduce noise.",
                "The idea is to present a (reasonable) number of individual terms to the user and ask him/her to judge the relevance of each term or directly specify their probabilities in the query model.",
                "This strategy has been discussed in [15], but to our knowledge, it has not been seriously studied in existing language modeling literature.",
                "Compared to traditional relevance feedback, this term-based approach to interactive query model refinement has several advantages.",
                "First, the user has better control of the final query model through direct manipulation of terms: he/she can dictate which terms are relevant, irrelevant, and possibly, to what degree.",
                "This avoids the risk of bringing unwanted terms into the query model, although sometimes the user introduces low-quality terms.",
                "Second, because a term takes less time to judge than a documents full text or summary, and as few as around 20 presented terms can bring significant improvement in <br>retrieval performance</br> (as we will show later), term feedback makes it faster to gather user feedback.",
                "This is especially helpful for interactive adhoc search.",
                "Third, sometimes there are no relevant documents in the top N of the initially retrieved results if the topic is hard.",
                "This is often true when N is constrained to be small, which arises from the fact that the user is unwilling to judge too many documents.",
                "In this case, relevance feedback is useless, as no relevant document can be leveraged on, but term feedback is still often helpful, by allowing relevant terms to be picked from irrelevant documents.",
                "During our participation in the TREC 2005 HARD Track and continued study afterward, we explored how to exploit term feedback from the user to construct improved query models for information retrieval in the language modeling approach.",
                "We identified two key subtasks of term-based feedback, i.e., pre-feedback presentation term selection and post-feedback query model construction, with effective algorithms developed for both.",
                "We imposed a secondary cluster structure on terms and found that a cluster view sheds additional insight into the users information need, and provides a good way of utilizing term feedback.",
                "Through experiments we found that term feedback improves significantly over the nonfeedback baseline, even though the user often makes mistakes in relevance judgment.",
                "Among our algorithms, the one with best <br>retrieval performance</br> is TCFB, the combination of TFB, the direct term feedback algorithm, and CFB, the cluster-based feedback algorithm.",
                "We also varied the number of feedback terms and observed reasonable improvement even at low numbers.",
                "Finally, by comparing term feedback with document-level feedback, we found it to be a viable alternative to the latter with competitive <br>retrieval performance</br>.",
                "The rest of the paper is organized as follows.",
                "Section 2 discusses some related work.",
                "Section 4 outlines our general approach to term feedback.",
                "We present our method for presentation term selection in Section 3 and algorithms for query model construction in Section 5.",
                "The experiment results are given in Section 6.",
                "Section 7 concludes this paper. 2.",
                "RELATED WORK Relevance feedback[17, 19] has long been recognized as an effective method for improving <br>retrieval performance</br>.",
                "Normally, the top N documents retrieved using the original query are presented to the user for judgment, after which terms are extracted from the judged relevant documents, weighted by their potential of attracting more relevant documents, and added into the query model.",
                "The expanded query usually represents the users information need better than the original one, which is often just a short keyword query.",
                "A second iteration of retrieval using this modified query usually produces significant increase in retrieval accuracy.",
                "In cases where true relevance judgment is unavailable and all top N documents are assumed to be relevant, it is called blind or pseudo feedback[5, 16] and usually still brings performance improvement.",
                "Because document is a large text unit, when it is used for relevance feedback many irrelevant terms can be introduced into the feedback process.",
                "To overcome this, passage feedback is proposed and shown to improve feedback performance[1, 23].",
                "A more direct solution is to ask the user for their relevance judgment of feedback terms.",
                "For example, in some relevance feedback systems such as [12], there is an interaction step that allows the user to add or remove expansion terms after they are automatically extracted from relevant documents.",
                "This is categorized as interactive query expansion, where the original query is augmented with user-provided terms, which can come from direct user input (free-form text or keywords)[22, 7, 10] or user selection of system-suggested terms (using thesauri[6, 22] or extracted from feedback documents[6, 22, 12, 4, 7]).",
                "In many cases term relevance feedback has been found to effectively improve <br>retrieval performance</br>[6, 22, 12, 4, 10].",
                "For example, the study in [12] shows that the user prefers to have explicit knowledge and direct control of which terms are used for query expansion, and the penetrable interface that provides this freedom is shown to perform better than other interfaces.",
                "However, in some other cases there is no significant benefit[3, 14], even if the user likes interacting with expansion terms.",
                "In a simulated study carried out in [18], the author compares the <br>retrieval performance</br> of interactive query expansion and automatic query expansion with a simulated study, and suggests that the potential benefits of the former can be hard to achieve.",
                "The user is found to be not good at identifying useful terms for query expansion, when a simple term presentation interface is unable to provide sufficient semantic context of the feedback terms.",
                "Our work differs from the previous ones in two important aspects.",
                "First, when we choose terms to present to the user for relevance judgment, we not only consider single-term value (e.g., the relative frequency of a term in the top documents, which can be measured by metrics such as Robertson Selection Value and Simplified Kullback-Leibler Distance as listed in [24]), but also examine the cluster structure of the terms, so as to produce a balanced coverage of the different topic aspects.",
                "Second, with the language modelling framework, we allow an elaborate construction of the updated query model, by setting different probabilities for different terms based on whether it is a query term, its significance in the top documents, and its cluster membership.",
                "Although techniques for adjusting query term weights exist for vector space models[17] and probablistic relevance models[9], most of the aforementioned works do not use them, choosing to just append feedback terms to the original query (thus using equal weights for them), which can lead to poorer <br>retrieval performance</br>.",
                "The combination of the two aspects allows our method to perform much better than the baseline.",
                "The usual way for feedback term presentation is just to display the terms in a list.",
                "There have been some works on alternative user interfaces. [8] arranges terms in a hierarchy, and [11] compares three different interfaces, including terms + checkboxes, terms + context (sentences) + checkboxes, sentences + input text box.",
                "In both studies, however, there is no significant performance difference.",
                "In our work we adopt the simplest approach of terms + checkboxes.",
                "We focus on term presentation and query model construction from feedback terms, and believe using contexts to improve feedback term quality should be orthogonal to our method. 3.",
                "GENERAL APPROACH We follow the language modeling approach, and base our method on the KL-divergence retrieval model proposed in [25].",
                "With this model, the retrieval task involves estimating a query language model θq from a given query, a document language model θd from each document, and calculating their KL-divergence D(θq||θd), which is then used to score the documents. [25] treats relevance feedback as a query model re-estimation problem, i.e., computing an updated query model θq given the original query text and the extra evidence carried by the judged relevant documents.",
                "We adopt this view, and cast our task as updating the query model from user term feedback.",
                "There are two key subtasks here: First, how to choose the best terms to present to the user for judgment, in order to gather maximal evidence about the users information need.",
                "Second, how to compute an updated query model based on this term feedback evidence, so that it captures the users information need and translates into good <br>retrieval performance</br>. 4.",
                "PRESENTATION TERM SELECTION Proper selection of terms to be presented to the user for judgment is crucial to the success of term feedback.",
                "If the terms are poorly chosen and there are few relevant ones, the user will have a hard time looking for useful terms to help clarify his/her information need.",
                "If the relevant terms are plentiful, but all concentrate on a single aspect of the query topic, then we will only be able to get feedback on that aspect and missing others, resulting in a breadth loss in retrieved results.",
                "Therefore, it is important to carefully select presentation terms to maximize expected gain from user feedback, i.e., those that can potentially reveal most evidence of the users information need.",
                "This is similar to active feedback[21], which suggests that a retrieval system should actively probe the users information need, and in the case of relevance feedback, the feedback documents should be chosen to maximize learning benefits (e.g. diversely so as to increase coverage).",
                "In our approach, the top N documents from an initial retrieval using the original query form the source of feedback terms: all terms that appear in them are considered candidates to present to the user.",
                "These documents serve as pseudo-feedback, since they provide a much richer context than the original query (usually very short), while the user is not asked to judge their relevance.",
                "Due to the latter reason, it is possible to make N quite large (e.g., in our experiments we set N = 60) to increase its coverage of different aspects in the topic.",
                "The simplest way of selecting feedback terms is to choose the most frequent M terms from the N documents.",
                "This method, however, has two drawbacks.",
                "First, a lot of common noisy terms will be selected due to their high frequencies in the document collection, unless a stop-word list is used for filtering.",
                "Second, the presentation list will tend to be filled by terms from major aspects of the topic; those from a minor aspect are likely to be missed due to their relatively low frequencies.",
                "We solve the above problems by two corresponding measures.",
                "First, we introduce a background model θB that is estimated from collection statistics and explains the common terms, so that they are much less likely to appear in the presentation list.",
                "Second, the terms are selected from multiple clusters in the pseudo-feedback documents, to ensure sufficient representation of different aspects of the topic.",
                "We rely on the mixture multinomial model, which is used for theme discovery in [26].",
                "Specifically, we assume the N documents contain K clusters {Ci| i = 1, 2, · · · K}, each characterized by a multinomial word distribution (also known as unigram language model) θi and corresponding to an aspect of the topic.",
                "The documents are regarded as sampled from a mixture of K + 1 components, including the K clusters and the background model: p(w|d) = λBp(w|θB) + (1 − λB) K i=1 πd,ip(w|θi) where w is a word, λB is the mixture weight for the background model θB, and πd,i is the document-specific mixture weight for the i-th cluster model θi.",
                "We then estimate the cluster models by maximizing the probability of the pseudo-feedback documents being generated from the multinomial mixture model: log p(D|Λ) = d∈D w∈V c(w; d) log p(w|d) where D = {di| i = 1, 2, · · · N} is the set of the N documents, V is the vocabulary, c(w; d) is ws frequency in d and Λ = {θi| i = 1, 2, · · · K} ∪ {πdij | i = 1, 2, · · · N, j = 1, 2, · · · K} is the set of model parameters to estimate.",
                "The cluster models can be efficiently estimated using the Expectation-Maximization (EM) algorithm.",
                "For its details, we refer the reader to [26].",
                "Table 1 shows the cluster models for TREC query Transportation tunnel disasters (K = 3).",
                "Note that only the middle cluster is relevant.",
                "Table 1: Cluster models for topic 363 Transportation tunnel disasters Cluster 1 Cluster 2 Cluster 3 tunnel 0.0768 tunnel 0.0935 tunnel 0.0454 transport 0.0364 fire 0.0295 transport 0.0406 traffic 0.0206 truck 0.0236 toll 0.0166 railwai 0.0186 french 0.0220 amtrak 0.0153 harbor 0.0146 smoke 0.0157 train 0.0129 rail 0.0140 car 0.0154 airport 0.0122 bridg 0.0139 italian 0.0152 turnpik 0.0105 kilomet 0.0136 firefight 0.0144 lui 0.0095 truck 0.0133 blaze 0.0127 jersei 0.0093 construct 0.0131 blanc 0.0121 pass 0.0087 · · · · · · · · · From each of the K estimated clusters, we choose the L = M/K terms with highest probabilities to form a total of M presentation terms.",
                "If a term happens to be in top L in multiple clusters, we assign it to the cluster where it has highest probability and let the other clusters take one more term as compensation.",
                "We also filter out terms in the original query text because they tend to always be relevant when the query is short.",
                "The selected terms are then presented to the user for judgment.",
                "A sample (completed) feedback form is shown in Figure 1.",
                "In this study we only deal with binary judgment: a presented term is by default unchecked, and a user may check it to indicate relevance.",
                "We also do not explicitly exploit negative feedback (i.e., penalizing irrelevant terms), because with binary feedback an unchecked term is not necessarily irrelevant (maybe the user is unsure about its relevance).",
                "We could ask the user for finer judgment (e.g., choosing from highly relevant, somewhat relevant, do not know, somewhat irrelevant and highly irrelevant), but binary feedback is more compact, taking less space to display and less user effort to make judgment. 5.",
                "ESTIMATING QUERY MODELS FROM TERM FEEDBACK In this section, we present several algorithms for exploiting term feedback.",
                "The algorithms take as input the original query q, the clusters {θi} as generated by the theme discovery algorithm, the set of feedback terms T and their relevance judgment R, and outputs an updated query language model θq that makes best use of the feedback evidence to capture the users information need.",
                "First we describe our notations: • θq: The original query model, derived from query terms only: p(w|θq) = c(w; q) |q| where c(w; q) is the count of w in q, and |q| = w∈q c(w; q) is the query length. • θq : The updated query model which we need to estimate from term feedback. • θi (i = 1, 2, . . .",
                "K): The unigram language model of cluster Ci, as estimated using the theme discovery algorithm. • T = {ti,j} (i = 1 . . .",
                "K, j = 1 . . .",
                "L): The set of terms presented to the user for judgment. ti,j is the j-th term chosen from cluster Ci. • R = {δw|w ∈ T}: δw is an indicator variable that is 1 if w is judged relevant or 0 otherwise. 5.1 TFB (Direct Term Feedback) This is a straight-forward form of term feedback that does not involve any secondary structure.",
                "We give a weight of 1 to terms judged relevant by the user, a weight of μ to query terms, zero weight to other terms, and then apply normalization: p(w|θq ) = δw + μ c(w; q) w ∈T δw + μ|q| where w ∈T δw is the total number of terms that are judged relevant.",
                "We call this method TFB (direct Term FeedBack).",
                "If we let μ = 1, this approach is equivalent to appending the relevant terms after the original query, which is what standard query expansion (without term reweighting) does.",
                "If we set μ > 1, we are putting more emphasis on the query terms than the checked ones.",
                "Note that the result model will be more biased toward θq if the original query is long or the user feedback is weak, which makes sense, as we can trust more on the original query in either case.",
                "Figure 1: Filled clarification form for Topic 363 363 transportation tunnel disasters Please select all terms that are relevant to the topic. traffic railway harbor rail bridge kilometer construct swiss cross link kong hong river project meter shanghai fire truck french smoke car italian firefights blaze blanc mont victim franc rescue driver chamonix emerge toll amtrak train airport turnpike lui jersey pass rome z center electron road boston speed bu submit 5.2 CFB (Cluster Feedback) Here we exploit the cluster structure that played an important role when we selected the presentation terms.",
                "The clusters represent different aspects of the query topic, each of which may or may not be relevant.",
                "If we are able to identify the relevant clusters, we can combine them to generate a query model that is good at discovering documents belonging to these clusters (instead of the irrelevant ones).",
                "We could ask the user to directly judge the relevance of a cluster after viewing representative terms in that cluster, but this would sometimes be a difficult task for the user, who has to guess the semantics of a cluster via its set of terms, which may not be well connected to one another due to a lack of context.",
                "Therefore, we propose to learn cluster feedback indirectly, inferring the relevance of a cluster through the relevance of its feedback terms.",
                "Because each cluster has an equal number of terms presented to the user, the simplest measure of a clusters relevance is the number of terms that are judged relevant in it.",
                "Intuitively, the more terms are marked relevant in a cluster, the closer the cluster is to the query topic, and the more the cluster should participate in query modification.",
                "If we combine the cluster models using weights determined this way and then interpolate with the original query model, we get the following formula for query updating, which we call CFB (Cluster FeedBack): p(w|θq ) = λp(w|θq) + (1 − λ) K i=1 L j=1 δti,j K k=1 L j=1 δtk,j p(w|θi) where L j=1 δti,j is the number of relevant terms in cluster Ci, and K k=1 L j=1 δtk,j is the total number of relevant terms.",
                "We note that when there is only one cluster (K = 1), the above formula degenerates to p(w|θq ) = λp(w|θq) + (1 − λ)p(w|θ1) which is merely pseudo-feedback of the form proposed in [25]. 5.3 TCFB (Term-cluster Feedback) TFB and CFB both have their drawbacks.",
                "TFB assigns non-zero probabilities to the presented terms that are marked relevant, but completely ignores (a lot more) others, which may be left unchecked due to the users ignorance, or simply not included in the presentation list, but we should be able to infer their relevance from the checked ones.",
                "For example, in Figure 1, since as many as 5 terms in the middle cluster (the third and fourth columns) are checked, we should have high confidence in the relevance of other terms in that cluster.",
                "CFB remedies TFBs problem by treating the terms in a cluster collectively, so that unchecked/unpresented terms receive weights when presented terms in their clusters are judged as relevant, but it does not distinguish which terms in a cluster are presented or judged.",
                "Intuitively, the judged relevant terms should receive larger weights because they are explicitly indicated as relevant by the user.",
                "Therefore, we try to combine the two methods, hoping to get the best out of both.",
                "We do this by interpolating the TFB model with the CFB model, and call it TCFB: p(w|θq ) = αp(w|θqT F B ) + (1 − α)p(w|θqCF B ) 6.",
                "EXPERIMENTS In this section, we describe our experiment results.",
                "We first describe our experiment setup and present an overview of various methods performance.",
                "Then we discuss the effects of varying the parameter setting in the algorithms, as well as the number of presentation terms.",
                "Next we analyze user term feedback behavior and its relation to <br>retrieval performance</br>.",
                "Finally we compare term feedback to relevance feedback and show that it has its particular advantage. 6.1 Experiment Setup and Basic Results We took the opportunity of TREC 2005 HARD Track[2] for the evaluation of our algorithms.",
                "The tracks used the AQUAINT collection, a 3GB corpus of English newswire text.",
                "The topics included 50 ones previously known to be hard, i.e. with low <br>retrieval performance</br>.",
                "It is for these hard topics that user feedback is most helpful, as it can provide information to disambiguate the queries; with easy topics the user may be unwilling to spend efforts for feedback if the automatic retrieval results are good enough.",
                "Participants of the track were able to submit custom-designed clarification forms (CF) to solicit feedback from human assessors provided by Table 2: <br>retrieval performance</br> for different methods and CF types.",
                "The last row is the percentage of MAP improvement over the baseline.",
                "The parameter settings μ = 4, λ = 0.1, α = 0.3 are near optimal.",
                "Baseline TFB1C TFB3C TFB6C CFB1C CFB3C CFB6C TCFB1C TCFB3C TCFB6C MAP 0.219 0.288 0.288 0.278 0.254 0.305 0.301 0.274 0.309 0.304 Pr@30 0.393 0.467 0.475 0.457 0.399 0.480 0.473 0.431 0.491 0.473 RR 4339 4753 4762 4740 4600 4907 4872 4767 4947 4906 % 0% 31.5% 31.5% 26.9% 16.0% 39.3% 37.4% 25.1% 41.1% 38.8% Table 3: MAP variation with the number of presented terms. # terms TFB1C TFB3C TFB6C CFB3C CFB6C TCFB3C TCFB6C 6 0.245 0.240 0.227 0.279 0.279 0.281 0.274 12 0.261 0.261 0.242 0.299 0.286 0.297 0.281 18 0.275 0.274 0.256 0.301 0.282 0.300 0.286 24 0.276 0.281 0.265 0.303 0.292 0.305 0.292 30 0.280 0.285 0.270 0.304 0.296 0.307 0.296 36 0.282 0.288 0.272 0.307 0.297 0.309 0.297 42 0.283 0.288 0.275 0.306 0.298 0.309 0.300 48 0.288 0.288 0.278 0.305 0.301 0.309 0.303 NIST.",
                "We designed three sets of clarification forms for term feedback, differing in the choice of K, the number of clusters, and L, the number of presented terms from each cluster.",
                "They are: 1× 48, a big cluster with 48 terms, 3 × 16, 3 clusters with 16 terms each, and 6 × 8, 6 clusters with 8 terms each.",
                "The total number of presented terms (M) is fixed at 48, so by comparing the performance of different types of clarification forms we can know the effects of different degree of clustering.",
                "For each topic, an assessor would complete the forms ordered by 6 × 8, 1 × 48 and 3 × 16, spending up to three minutes on each form.",
                "The sample clarification form shown in Figure 1 is of type 3 × 16.",
                "It is a simple and compact interface in which the user can check relevant terms.",
                "The form is self-explanatory; there is no need for extra user training on how to use it.",
                "Our initinal queries are constructed only using the topic title descriptions, which are on average 2.7 words in length.",
                "As our baseline we use the KL divergence retrieval method implemented in the Lemur Toolkit1 with 5 pseudo-feedback documents.",
                "We stem the terms, choose Dirichlet smoothing with a prior of 2000, and truncate query language models to 50 terms (these settings are used throughout the experiments).",
                "For all other parameters we use Lemurs default settings.",
                "The baseline turns out to perform above average among the track participants.",
                "After an initial run using this baseline retrieval method, we take the top 60 documents for each topic and apply the theme discovery algorithm to output the clusters (1, 3, or 6 of them), based on which we generate clarification forms.",
                "After user feedback is received, we run the term feedback algorithms (TFB, CFB or TCFB) to estimate updated query models, which are then used for a second iteration of retrieval.",
                "We evaluate the different retrieval methods performance on their rankings of the top 1000 documents.",
                "The evaluation metrics we adopt include mean average (non-interpolated) precision (MAP), precision at top 30 (Pr@30) and total relevant retrieved (RR).",
                "Table 2 shows the performance of various methods and configurations of K × L. The suffixes (1C, 3C, 6C) after TFB,CFB,TCFB stand for the number of clusters (K).",
                "For example, TCFB3C means the TCFB method on the 3 × 16 clarification forms.",
                "From Table 2 we can make the following observations: 1 http://www.lemurproject.com 1.",
                "All methods perform considerably better than the pseudofeedback baseline, with TCFB3C achieving a highest 41.1% improvement in MAP, indicating significant contribution of term feedback for clarification of the users information need.",
                "In other words, term feedback is truly helpful for improving retrieval accuracy. 2.",
                "For TFB, the performance is almost equal on the 1 × 48 and 3 × 16 clarification forms in terms of MAP (although the latter is slightly better in Pr@30 and RR), and a little worse on the 6 × 8 ones. 3.",
                "Both CFB3C and CFB6C perform better than their TFB counterparts in all three metrics, suggesting that feedback on a secondary cluster structure is indeed beneficial.",
                "CFB1C is actually worse because it cannot adjust the weight of its (single) cluster from term feedback and it is merely pseudofeedback. 4.",
                "Although TCFB is just a simple mixture of TFB and CFB by interpolation, it is able to outperform both.",
                "This supports our speculation that TCFB overcomes the drawbacks of TFB (paying attention only to checked terms) and CFB (not distinguishing checked and unchecked terms in a cluster).",
                "Except for TCFB6C v.s.",
                "CFB6C, the performance advantage of TCFB over TFB/CFB is significant at p < 0.05 using the Wilcoxon signed rank test.",
                "This is not true in the case of TFB v.s.",
                "CFB, each of which is better than the other in nearly half of the topics. 6.2 Reduction of Presentation Terms In some situations we may have to reduce the number of presentation terms due to limits in display space or user feedback efforts.",
                "It is interesting to know whether our algorithms performance deteriorates when the user is presented with fewer terms.",
                "Because the presentation terms within each cluster are generated in decreasing order of their frequencies, the presentation list forms a subset of the original one if its size is reduced2 .",
                "Therefore, we can easily simulate what happens when the number of presentation terms decreases 2 There are complexities arising from terms appearing in top L of multiple clusters, but these are exceptions from M to M : we will keep all judgments of the top L = M /K terms in each cluster and discard those of others.",
                "Table 3 shows the performance of various algorithms as the number of presentation terms ranges from 6 to 48.",
                "We find that the performance of TFB is more susceptible to presentation term reduction than that of CFB or TCFB.",
                "For example, at 12 terms the MAP of TFB3C is 90.6% of that at 48 terms, while the numbers for CFB3C and TCFB3C are 98.0% and 96.1% respectively.",
                "We conjecture the reason to be that while TFBs performance heavily depends on how many good terms are chosen for query expansion, CFB only needs a rough estimate of cluster weights to work.",
                "Also, the 3 × 16 clarification forms seem to be more robust than the 6 × 8 ones: at 12 terms the MAP of TFB6C is 87.1% of that at 48 terms, lower than 90.6% for TFB3C.",
                "Similarly, for CFB it is 95.0% against 98.0%.",
                "This is natual, as for a large cluster number of 6, it is easier to get into the situation where each cluster gets too few presentation terms to make topic diversification useful.",
                "Overall, we are surprised to see that the algorithms are still able to perform reasonably well when the number of presentation terms is small.",
                "For example, at only 12 terms CFB3C (the clarification form is of size 3 × 4) can still improve 36.5% over the baseline, dropping slightly from 39.3% at 48 terms. 6.3 User Feedback Analysis In this part we study several aspects of users term feedback behavior, and whether they are connected to <br>retrieval performance</br>.",
                "Figure 2: Clarification form completion time distributions 0−30 30−60 60−90 90−120 120−150 150−180 0 5 10 15 20 25 30 35 completion time (seconds) #topics 1×48 3×16 6×8 Figure 2 shows the distribution of time needed to complete a clarification form3 .",
                "We see that the user is usually able to finish term feedback within a reasonably short amount of time: for more than half of the topics the clarification form is completed in just 1 minute, and only a small fraction of topics (less than 10% for 1 × 48 and 3 × 16) take more than 2 minutes.",
                "This suggests that term feedback is suitable for interactive ad-hoc retrieval, where a user usually does not want to spend too much time on providing feedback.",
                "We find that a user often makes mistakes when judging term relevance.",
                "Sometimes a relevant term may be left out because its connection to the query topic is not obvious to the user.",
                "Other times a dubious term may be included but turns out to be irrelevant.",
                "Take the topic in Figure 1 for example.",
                "There was a fire disaster in Mont 3 The maximal time is 180 seconds, as the NIST assessor would be forced to submit the form at that moment.",
                "Table 4: Term selection statistics (topic average) CF Type 1 × 48 3 × 16 6 × 8 # checked terms 14.8 13.3 11.2 # rel. terms 15.0 12.6 11.2 # rel. checked terms 7.9 6.9 5.9 precision 0.534 0.519 0.527 recall 0.526 0.548 0.527 Blanc Tunnel between France and Italy in 1999, but the user failed to select such keywords as mont, blanc, french and italian due to his/her ignorance of the event.",
                "Indeed, without proper context it would be hard to make perfect judgment.",
                "What is then, the extent to which the user is good at term feedback?",
                "Does it have serious impact on <br>retrieval performance</br>?",
                "To answer these questions, we need a measure of individual terms true relevance.",
                "We adopt the Simplified KL Divergence metric used in [24] to decide query expansion terms as our term relevance measure: σKLD(w) = p(w|R) log p(w|R) p(w|¬R) where p(w|R) is the probability that a relevant document contains term w, and p(w|¬R) is the probability that an irrelevant document contains w, both of which can be easily computed via maximum likelihood estimate given document-level relevance judgment.",
                "If σKLD(w) > 0, w is more likely to appear in relevant documents than irrelevant ones.",
                "We consider a term relevant if its Simplified KL Divergence value is greater than a certain threshold σ0.",
                "We can then define precision and recall of user term judgment accordingly: precision is the fraction of terms checked by the user that are relevant; recall is the fraction of presented relevant terms that are checked by the user.",
                "Table 4 shows the number of checked terms, relevant terms and relevant checked terms when σ0 is set to 1.0, as well as the precision/recall of user term judgment.",
                "Note that when the clarification forms contain more clusters, fewer terms are checked: 14.8 for 1 × 48, 13.3 for 3 × 16 and 11.2 for 6×8.",
                "Similar pattern holds for relevant terms and relevant checked terms.",
                "There seems to be a trade-off between increasing topic diversity by clustering and losing extra relevant terms: when there are more clusters, each of them gets fewer terms to present, which can hurt a major relevant cluster that contains many relevant terms.",
                "Therefore, it is not always helpful to have more clusters, e.g., TFB6C is actually worse than TFB1C.",
                "The major finding we can make from Table 4 is that the user is not particularly good at identifying relevant terms, which echoes the discovery in [18].",
                "In the case of 3 × 16 clarification forms, the average number of terms checked as relevant by the user is 13.3 per topic, and the average number of relevant terms whose σKLD value exceed 1.0 is 12.6.",
                "The user is able to recognize only 6.9 of these terms on average.",
                "Indeed, the precision and recall of user feedback terms (as defined previously) are far from perfect.",
                "On the other hand, If the user had correctly checked all such relevant terms, the performance of our algorithms would have increased a lot, as shown in Table 5.",
                "We see that TFB gets big improvement when there is an oracle who checks all relevant terms, while CFB meets a bottleneck around MAP of 0.325, since all it does is adjust cluster weights, and when the learned weights are close to being accurate, it cannot benefit more from term feedback.",
                "Also note that TCFB fails to outperform TFB, probably because TFB is sufficiently accurate.",
                "Table 5: Change of MAP when using all (and only) relevant terms (σKLD > 1.0) for feedback. original term feedback relevant term feedback TF1 0.288 0.354 TF3 0.288 0.354 TF6 0.278 0.346 CF3 0.305 0.325 CF6 0.301 0.326 TCF3 0.309 0.345 TCF6 0.304 0.341 6.4 Comparison with Relevance Feedback Now we compare term feedback with document-level relevance feedback, in which the user is presented with the top N documents from an initial retrieval and asked to judge their relevance.",
                "The feedback process is simulated using document relevance judgment from NIST.",
                "We use the mixture model based feedback method proposed in [25], with mixture noise set to 0.95 and feedback coefficient set to 0.9.",
                "Comparative evaluation of relevance feedback against other methods is complicated by the fact that some documents have already been viewed during feedback, so it makes no sense to include them in the retrieval results of the second run.",
                "However, this does not hold for term feedback.",
                "Thus, to make it fair w.r.t. users information gain, if the feedback documents are relevant, they should be kept in the top of the ranking; if they are irrelevant, they should be left out.",
                "Therefore, we use relevance feedback to produce a ranking of top 1000 retrieved documents but with every feedback document excluded, and then prepend the relevant feedback documents at the front.",
                "Table 6 shows the performance of relevance feedback for different values of N and compares it with TCFB3C.",
                "Table 6: Performance of relevance feedback for different number of feedback documents (N).",
                "N MAP Pr@30 RR 5 0.302 0.586 4779 10 0.345 0.670 4916 20 0.389 0.772 5004 TCFB3C 0.309 0.491 4947 We see that the performance of TCFB3C is comparable to that of relevance feedback using 5 documents.",
                "Although it is poorer than when there are 10 feedback documents in terms of MAP and Pr@30, it does retrieve more documents (4947) when going down the ranked list.",
                "We try to compare the quality of automatically inserted terms in relevance feedback with that of manually selected terms in term feedback.",
                "This is done by truncating the relevance feedback modified query model to a size equal to the number of checked terms for the same topic.",
                "We can then compare the terms in the truncated model with the checked terms.",
                "Figure 3 shows the distribution of the terms σKLD scores.",
                "We find that term feedback tends to produce expansion terms of higher quality(those with σKLD > 1) compared to relevance feedback (with 10 feedback documents).",
                "This does not contradict the fact that the latter yields higher <br>retrieval performance</br>.",
                "Actually, when we use the truncated query model instead of the intact one refined from relevance feedback, the MAP is only 0.304.",
                "The truth Figure 3: Comparison of expansion term quality between relevance feedback (with 10 feedback documents) and term feedback (with 3 × 16 CFs) −1−0 0−1 1−2 2−3 3−4 4−5 5−6 0 50 100 150 200 250 300 350 σKLD #terms relevance feedback term feedback is, although there are many unwanted terms in the expanded query model from feedback documents, there are also more relevant terms than what the user can possibly select from the list of presentation terms generated with pseudo-feedback documents, and the positive effects often outweights the negative ones.",
                "We are interested to know under what circumstances term feedback has advantage over relevance feedback.",
                "One such situation is when none of the top N feedback documents is relevant, rendering relevance feedback useless.",
                "This is not infrequent, as one might have thought: out of the 50 topics, there are 13 such cases when N = 5, 10 when N = 10, and still 3 when N = 20.",
                "When this happens, one can only back off to the original retrieval method; the power of relevance feedback is lost.",
                "Surprisingly, in 11 out of 13 such cases where relevance feedback seems impossible, the user is able to check at least 2 relevant terms from the 3 × 16 clarification form (we consider term t to be relevant if σKLD(t) > 1.0).",
                "Furthermore, in 10 out of them TCFB3C outperforms the pseudo-feedback baseline, increasing MAP from 0.076 to 0.146 on average (these are particularly hard topics).",
                "We think that there are two possible explanations for this phenomenon of term feedback being active even when relevance feedback does not work: First, even if none of the top N (suppose it is a small number) documents are relevant, we may still find relevant documents in top 60, which is more inclusive but usually unreachable when people are doing relevance feedback in interactive ad-hoc search, from which we can draw feedback terms.",
                "This is true for topic 367 piracy, where the top 10 feedback documents are all about software piracy, yet there are documents between 10-60 that are about piracy on the seas (which is about the real information need), contributing terms such as pirate, ship for selection in the clarification form.",
                "Second, for some topics, a document needs to meet some special condition in order to be relevant.",
                "The top N documents may be related to the topic, but nonetheless irrelevant.",
                "In this case, we may still extract useful terms from these documents, even if they do not qualify as relevant ones.",
                "For example, in topic 639 consumer online shopping, a document needs to mention what contributes to shopping growth to really match the specified information need, hence none of the top 10 feedback documents are regarded as relevant.",
                "But nevertheless, the feedback terms such as retail, commerce are good for query expansion. 7.",
                "CONCLUSIONS In this paper we studied the use of term feedback for interactive information retrieval in the language modeling approach.",
                "We proposed a cluster-based method for selecting presentation terms as well as algorithms to estimate refined query models from user term feedback.",
                "We saw significant improvement in retrieval accuracy brought by term feedback, in spite of the fact that a user often makes mistakes in relevance judgment that hurts its performance.",
                "We found the best-performing algorithm to be TCFB, which benefits from the combination of directly observed term evidence with TFB and indirectly learned cluster relevance with CFB.",
                "When we reduced the number of presentation terms, term feedback is still able to keep much of its performance gain over the baseline.",
                "Finally, we compared term feedback to document-level relevance feedback, and found that TCFB3Cs performance is on a par with the latter with 5 feedback documents.",
                "We regarded term feedback as a viable alternative to traditional relevance feedback, especially when there are no relevant documents in the top.",
                "We propose to extend our work in several ways.",
                "First, we want to study whether the use of various contexts can help the user to better identify term relevance, while not sacrificing the simplicity and compactness of term feedback.",
                "Second, currently all terms are presented to the user in a single batch.",
                "We could instead consider iterative term feedback, by presenting a small number of terms first, and show more terms after receiving user feedback or stop when the refined query is good enough.",
                "The presented terms should be selected dynamically to maximize learning benefits at any moment.",
                "Third, we have plans to incorporate term feedback into our UCAIR toolbar[20], an Internet Explorer plugin, to make it work for web search.",
                "We are also interested in studying how to combine term feedback with relevance feedback or implicit feedback.",
                "We could, for example, allow the user to dynamically modify terms in a language model learned from feedback documents. 8.",
                "ACKNOWLEDGMENT This work is supported in part by the National Science Foundation grants IIS-0347933 and IIS-0428472. 9.",
                "REFERENCES [1] J. Allan.",
                "Relevance feedback with too much data.",
                "In Proceedings of the 18th annual international ACM SIGIR conference on research and development in information retrieval, pages 337-343, 1995. [2] J. Allan.",
                "HARD track overview in TREC 2005 - High Accuracy Retrieval from Documents.",
                "In The Fourteenth Text REtrieval Conference, 2005. [3] P. Anick.",
                "Using terminological feedback for web search refinement: a log-based study.",
                "In Proceedings of the 26th annual international ACM SIGIR conference on research and development in informaion retrieval, pages 88-95, 2003. [4] P. G. Anick and S. Tipirneni.",
                "The paraphrase search assistant: terminological feedback for iterative information seeking.",
                "In Proceedings of the 22nd annual international ACM SIGIR conference on research and development in information retrieval, pages 153-159, 1999. [5] C. Buckley, G. Salton, J. Allan, and A. Singhal.",
                "Automatic query expansion using SMART.",
                "In Proceedings of the Third Text REtrieval Conference, 1994. [6] D. Harman.",
                "Towards interactive query expansion.",
                "In Proceedings of the 11th annual international ACM SIGIR conference on research and development in information retrieval, pages 321-331, 1988. [7] N. A. Jaleel, A. Corrada-Emmanuel, Q. Li, X. Liu, C. Wade, and J. Allan.",
                "UMass at TREC 2003: HARD and QA.",
                "In TREC, pages 715-725, 2003. [8] H. Joho, C. Coverson, M. Sanderson, and M. Beaulieu.",
                "Hierarchical presentation of expansion terms.",
                "In Proceedings of the 2002 ACM symposium on applied computing, pages 645-649, 2002. [9] K. S. Jones, S. Walker, and S. E. Robertson.",
                "A probabilistic model of information retrieval: development and status.",
                "Technical Report 446, Computer Laboratory, University of Cambridge, 1998. [10] D. Kelly, V. D. Dollu, and X. Fu.",
                "The loquacious user: a document-independent source of terms for query expansion.",
                "In Proceedings of the 28th annual international ACM SIGIR conference on research and development in information retrieval, pages 457-464, 2005. [11] D. Kelly and X. Fu.",
                "Elicitation of term relevance feedback: an investigation of term source and context.",
                "In Proceedings of the 29th annual international ACM SIGIR conference on research and development in information retrieval, 2006. [12] J. Koenemann and N. Belkin.",
                "A case for interaction: A study of interactive information retrieval behavior and effectiveness.",
                "In Proceedings of the SIGCHI conference on human factors in computing systems, pages 205-212, 1996. [13] V. Lavrenko and W. B. Croft.",
                "Relevance-based language models.",
                "In Research and Development in Information Retrieval, pages 120-127, 2001. [14] Y. Nemeth, B. Shapira, and M. Taeib-Maimon.",
                "Evaluation of the real and perceived value of automatic and interactive query expansion.",
                "In Proceedings of the 27th annual international ACM SIGIR conference on research and development in information retrieval, pages 526-527, 2004. [15] J. Ponte.",
                "A Language Modeling Approach to Information Retrieval.",
                "PhD thesis, University of Massachusetts at Amherst, 1998. [16] S. E. Robertson, S. Walker, S. Jones, M. Beaulieu, and M. Gatford.",
                "Okapi at TREC-3.",
                "In Proceedings of the Third Text REtrieval Conference, 1994. [17] J. Rocchio.",
                "Relevance feedback in information retrieval.",
                "In The SMART retrieval system, pages 313-323. 1971. [18] I. Ruthven.",
                "Re-examining the potential effectiveness of interactive query expansion.",
                "In Proceedings of the 26th annual international ACM SIGIR conference on research and development in informaion retrieval, pages 213-220, 2003. [19] G. Salton and C. Buckley.",
                "Improving <br>retrieval performance</br> by relevance feedback.",
                "Journal of the American Society for Information Science, 41:288-297, 1990. [20] X. Shen, B. Tan, and C. Zhai.",
                "Implicit user modeling for personalized search.",
                "In Proceedings of the 14th ACM international conference on information and knowledge management, pages 824-831, 2005. [21] X. Shen and C. Zhai.",
                "Active feedback in ad-hoc information retrieval.",
                "In Proceedings of the 28th annual international ACM SIGIR conference on research and development in information retrieval, pages 59-66, 2005. [22] A. Spink.",
                "Term relevance feedback and query expansion: relation to design.",
                "In Proceedings of the 17th annual international ACM SIGIR conference on research and development in information retrieval, pages 81-90, 1994. [23] J. Xu and W. B. Croft.",
                "Query expansion using local and global document analysis.",
                "In Proceedings of the 19th annual international ACM SIGIR conference on research and development in information retrieval, pages 4-11, 1996. [24] H. Zaragoza, N. Craswell, M. Taylor, S. Saria, and S. Robertson.",
                "Microsoft cambridge at TREC-13: Web and HARD tracks.",
                "In Proceedings of the 13th Text REtrieval Conference, 2004. [25] C. Zhai and J. Lafferty.",
                "Model-based feedback in the language modeling approach to information retrieval.",
                "In Proceedings of the tenth international conference on information and knowledge management, pages 403-410, 2001. [26] C. Zhai, A. Velivelli, and B. Yu.",
                "A cross-collection mixture model for comparative text mining.",
                "In Proceedings of the tenth ACM SIGKDD international conference on knowledge discovery and data mining, pages 743-748, 2004."
            ],
            "original_annotated_samples": [
                "Second, because a term takes less time to judge than a documents full text or summary, and as few as around 20 presented terms can bring significant improvement in <br>retrieval performance</br> (as we will show later), term feedback makes it faster to gather user feedback.",
                "Among our algorithms, the one with best <br>retrieval performance</br> is TCFB, the combination of TFB, the direct term feedback algorithm, and CFB, the cluster-based feedback algorithm.",
                "Finally, by comparing term feedback with document-level feedback, we found it to be a viable alternative to the latter with competitive <br>retrieval performance</br>.",
                "RELATED WORK Relevance feedback[17, 19] has long been recognized as an effective method for improving <br>retrieval performance</br>.",
                "In many cases term relevance feedback has been found to effectively improve <br>retrieval performance</br>[6, 22, 12, 4, 10]."
            ],
            "translated_annotated_samples": [
                "Segundo, dado que un término tarda menos tiempo en ser evaluado que el texto completo o el resumen de un documento, y con tan solo alrededor de 20 términos presentados se puede lograr una mejora significativa en el <br>rendimiento de recuperación</br> (como demostraremos más adelante), el feedback de términos permite recopilar la retroalimentación del usuario de manera más rápida.",
                "Entre nuestros algoritmos, el que tiene el mejor <br>rendimiento de recuperación</br> es TCFB, la combinación de TFB, el algoritmo de retroalimentación de términos directos, y CFB, el algoritmo de retroalimentación basado en clústeres.",
                "Finalmente, al comparar la retroalimentación a nivel de términos con la retroalimentación a nivel de documentos, encontramos que es una alternativa viable a esta última con un rendimiento competitivo en la recuperación.",
                "TRABAJO RELACIONADO La retroalimentación de relevancia[17, 19] ha sido reconocida durante mucho tiempo como un método efectivo para mejorar el <br>rendimiento de recuperación</br>.",
                "En muchos casos, se ha encontrado que la retroalimentación de relevancia de términos mejora de manera efectiva el <br>rendimiento de recuperación</br> [6, 22, 12, 4, 10]."
            ],
            "translated_text": "Comentarios sobre el término para la recuperación de información con modelos de lenguaje Bin Tan†, Atulya Velivelli‡, Hui Fang†, ChengXiang Zhai† Dept. En este documento estudiamos la retroalimentación basada en términos para la recuperación de información en el enfoque de modelado de lenguaje. Con la retroalimentación de términos, un usuario juzga directamente la relevancia de términos individuales sin interacción con documentos de retroalimentación, tomando el control total del proceso de expansión de la consulta. Proponemos un método basado en clusters para seleccionar términos para presentar al usuario para su evaluación, así como algoritmos efectivos para construir modelos de lenguaje de consulta refinados a partir de la retroalimentación de términos del usuario. Nuestros algoritmos han demostrado proporcionar una mejora significativa en la precisión de recuperación en comparación con una línea base sin retroalimentación, y logran un rendimiento comparable al de la retroalimentación de relevancia. Son útiles incluso cuando no hay documentos relevantes en la parte superior. Categorías y Descriptores de Asignaturas H.3.3 [Búsqueda y Recuperación de Información]: Modelos de recuperación Términos generales Algoritmos 1. En el enfoque de modelado del lenguaje para la recuperación de información, la retroalimentación a menudo se modela como la estimación de un modelo de consulta mejorado o un modelo de relevancia basado en un conjunto de documentos de retroalimentación [25, 13]. Esto está en línea con la forma tradicional de realizar retroalimentación de relevancia: presentar al usuario documentos/pasajes para su juicio de relevancia y luego extraer términos de los documentos o pasajes juzgados para expandir la consulta inicial. Es una forma indirecta de buscar la ayuda de los usuarios para la construcción del modelo de consulta, en el sentido de que el modelo de consulta refinado (basado en términos) se aprende a través de documentos/pasajes de retroalimentación, que son estructuras de alto nivel de términos. Tiene la desventaja de que los términos irrelevantes, que ocurren junto con los relevantes en el contenido evaluado, pueden ser utilizados erróneamente para la expansión de la consulta, causando efectos no deseados. Por ejemplo, para la consulta TREC del telescopio Hubble logros, cuando un documento relevante habla más sobre la reparación de los telescopios que sobre sus descubrimientos, términos irrelevantes como caminata espacial pueden ser agregados a la consulta modificada. Podemos considerar una forma más directa de involucrar a un usuario en la mejora del modelo de consulta, sin un paso intermedio de retroalimentación de documentos que pueda introducir ruido. La idea es presentar al usuario un número (razonable) de términos individuales y pedirle que juzgue la relevancia de cada término o especifique directamente sus probabilidades en el modelo de consulta. Esta estrategia ha sido discutida en [15], pero hasta donde sabemos, no ha sido estudiada seriamente en la literatura existente de modelado de lenguaje. En comparación con la retroalimentación de relevancia tradicional, este enfoque basado en términos para el refinamiento del modelo de consulta interactivo tiene varias ventajas. Primero, el usuario tiene un mejor control del modelo de consulta final a través de la manipulación directa de términos: él/ella puede dictar qué términos son relevantes, irrelevantes y posiblemente, en qué medida. Esto evita el riesgo de introducir términos no deseados en el modelo de consulta, aunque a veces el usuario introduce términos de baja calidad. Segundo, dado que un término tarda menos tiempo en ser evaluado que el texto completo o el resumen de un documento, y con tan solo alrededor de 20 términos presentados se puede lograr una mejora significativa en el <br>rendimiento de recuperación</br> (como demostraremos más adelante), el feedback de términos permite recopilar la retroalimentación del usuario de manera más rápida. Esto es especialmente útil para la búsqueda interactiva ad hoc. En tercer lugar, a veces no hay documentos relevantes en los primeros N resultados recuperados inicialmente si el tema es difícil. Esto suele ser cierto cuando N está limitado a ser pequeño, lo cual surge del hecho de que el usuario no está dispuesto a evaluar demasiados documentos. En este caso, la retroalimentación de relevancia es inútil, ya que no se puede aprovechar ningún documento relevante, pero la retroalimentación de términos sigue siendo útil, al permitir seleccionar términos relevantes de documentos irrelevantes. Durante nuestra participación en la pista HARD de TREC 2005 y el estudio continuado posteriormente, exploramos cómo aprovechar la retroalimentación de términos por parte del usuario para construir modelos de consulta mejorados para la recuperación de información en el enfoque de modelado de lenguaje. Identificamos dos sub tareas clave del feedback basado en términos, es decir, la selección de términos para la presentación previa al feedback y la construcción del modelo de consulta posterior al feedback, con algoritmos efectivos desarrollados para ambos. Impusimos una estructura de clúster secundaria en los términos y descubrimos que una vista de clúster arroja una mayor comprensión de la necesidad de información de los usuarios, y proporciona una buena forma de utilizar la retroalimentación de términos. A través de experimentos encontramos que el feedback a término mejora significativamente sobre la línea base sin feedback, a pesar de que el usuario a menudo comete errores en la evaluación de relevancia. Entre nuestros algoritmos, el que tiene el mejor <br>rendimiento de recuperación</br> es TCFB, la combinación de TFB, el algoritmo de retroalimentación de términos directos, y CFB, el algoritmo de retroalimentación basado en clústeres. También variamos el número de términos de retroalimentación y observamos una mejora razonable incluso con números bajos. Finalmente, al comparar la retroalimentación a nivel de términos con la retroalimentación a nivel de documentos, encontramos que es una alternativa viable a esta última con un rendimiento competitivo en la recuperación. El resto del documento está organizado de la siguiente manera. La sección 2 discute algunos trabajos relacionados. La sección 4 describe nuestro enfoque general para la retroalimentación de términos. Presentamos nuestro método para la selección de términos de presentación en la Sección 3 y los algoritmos para la construcción del modelo de consulta en la Sección 5. Los resultados del experimento se presentan en la Sección 6. La sección 7 concluye este documento. 2. TRABAJO RELACIONADO La retroalimentación de relevancia[17, 19] ha sido reconocida durante mucho tiempo como un método efectivo para mejorar el <br>rendimiento de recuperación</br>. Normalmente, los N documentos principales recuperados utilizando la consulta original se presentan al usuario para su evaluación, después de lo cual se extraen términos de los documentos relevantes evaluados, ponderados por su potencial de atraer más documentos relevantes, y se añaden al modelo de consulta. La consulta ampliada suele representar mejor la necesidad de información de los usuarios que la original, que a menudo es solo una consulta de palabras clave cortas. Una segunda iteración de recuperación utilizando esta consulta modificada generalmente produce un aumento significativo en la precisión de la recuperación. En los casos en los que no se dispone de una evaluación de relevancia real y se asume que todos los documentos principales N son relevantes, se denomina retroalimentación ciega o pseudo [5, 16] y generalmente sigue mejorando el rendimiento. Debido a que un documento es una unidad de texto grande, cuando se utiliza para retroalimentación de relevancia, muchos términos irrelevantes pueden ser introducidos en el proceso de retroalimentación. Para superar esto, se propone el feedback de pasajes y se muestra que mejora el rendimiento del feedback. Una solución más directa es pedir al usuario su juicio de relevancia sobre los términos de retroalimentación. Por ejemplo, en algunos sistemas de retroalimentación de relevancia como [12], hay un paso de interacción que permite al usuario agregar o eliminar términos de expansión después de que son extraídos automáticamente de los documentos relevantes. Esto se clasifica como expansión interactiva de consultas, donde la consulta original se amplía con términos proporcionados por el usuario, que pueden provenir de la entrada directa del usuario (texto libre o palabras clave) o de la selección del usuario de términos sugeridos por el sistema (utilizando tesauros o extraídos de documentos de retroalimentación). En muchos casos, se ha encontrado que la retroalimentación de relevancia de términos mejora de manera efectiva el <br>rendimiento de recuperación</br> [6, 22, 12, 4, 10]. ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "probability": {
            "translated_key": "probabilidad",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Term Feedback for Information Retrieval with Language Models Bin Tan† , Atulya Velivelli‡ , Hui Fang† , ChengXiang Zhai† Dept.",
                "of Computer Science† , Dept. of Electrical and Computer Engineering‡ University of Illinois at Urbana-Champaign bintan@cs.uiuc.edu, velivell@ifp.uiuc.edu, hfang@cs.uiuc.edu, czhai@cs.uiuc.edu ABSTRACT In this paper we study term-based feedback for information retrieval in the language modeling approach.",
                "With term feedback a user directly judges the relevance of individual terms without interaction with feedback documents, taking full control of the query expansion process.",
                "We propose a cluster-based method for selecting terms to present to the user for judgment, as well as effective algorithms for constructing refined query language models from user term feedback.",
                "Our algorithms are shown to bring significant improvement in retrieval accuracy over a non-feedback baseline, and achieve comparable performance to relevance feedback.",
                "They are helpful even when there are no relevant documents in the top.",
                "Categories and Subject Descriptors H.3.3 [Information Search and Retrieval]: Retrieval models General Terms Algorithms 1.",
                "INTRODUCTION In the language modeling approach to information retrieval, feedback is often modeled as estimating an improved query model or relevance model based on a set of feedback documents [25, 13].",
                "This is in line with the traditional way of doing relevance feedback - presenting a user with documents/passages for relevance judgment and then extracting terms from the judged documents or passages to expand the initial query.",
                "It is an indirect way of seeking users assistance for query model construction, in the sense that the refined query model (based on terms) is learned through feedback documents/passages, which are high-level structures of terms.",
                "It has the disadvantage that irrelevant terms, which occur along with relevant ones in the judged content, may be erroneously used for query expansion, causing undesired effects.",
                "For example, for the TREC query Hubble telescope achievements, when a relevant document talks more about the telescopes repair than its discoveries, irrelevant terms such as spacewalk can be added into the modified query.",
                "We can consider a more direct way to involve a user in query model improvement, without an intermediary step of document feedback that can introduce noise.",
                "The idea is to present a (reasonable) number of individual terms to the user and ask him/her to judge the relevance of each term or directly specify their probabilities in the query model.",
                "This strategy has been discussed in [15], but to our knowledge, it has not been seriously studied in existing language modeling literature.",
                "Compared to traditional relevance feedback, this term-based approach to interactive query model refinement has several advantages.",
                "First, the user has better control of the final query model through direct manipulation of terms: he/she can dictate which terms are relevant, irrelevant, and possibly, to what degree.",
                "This avoids the risk of bringing unwanted terms into the query model, although sometimes the user introduces low-quality terms.",
                "Second, because a term takes less time to judge than a documents full text or summary, and as few as around 20 presented terms can bring significant improvement in retrieval performance (as we will show later), term feedback makes it faster to gather user feedback.",
                "This is especially helpful for interactive adhoc search.",
                "Third, sometimes there are no relevant documents in the top N of the initially retrieved results if the topic is hard.",
                "This is often true when N is constrained to be small, which arises from the fact that the user is unwilling to judge too many documents.",
                "In this case, relevance feedback is useless, as no relevant document can be leveraged on, but term feedback is still often helpful, by allowing relevant terms to be picked from irrelevant documents.",
                "During our participation in the TREC 2005 HARD Track and continued study afterward, we explored how to exploit term feedback from the user to construct improved query models for information retrieval in the language modeling approach.",
                "We identified two key subtasks of term-based feedback, i.e., pre-feedback presentation term selection and post-feedback query model construction, with effective algorithms developed for both.",
                "We imposed a secondary cluster structure on terms and found that a cluster view sheds additional insight into the users information need, and provides a good way of utilizing term feedback.",
                "Through experiments we found that term feedback improves significantly over the nonfeedback baseline, even though the user often makes mistakes in relevance judgment.",
                "Among our algorithms, the one with best retrieval performance is TCFB, the combination of TFB, the direct term feedback algorithm, and CFB, the cluster-based feedback algorithm.",
                "We also varied the number of feedback terms and observed reasonable improvement even at low numbers.",
                "Finally, by comparing term feedback with document-level feedback, we found it to be a viable alternative to the latter with competitive retrieval performance.",
                "The rest of the paper is organized as follows.",
                "Section 2 discusses some related work.",
                "Section 4 outlines our general approach to term feedback.",
                "We present our method for presentation term selection in Section 3 and algorithms for query model construction in Section 5.",
                "The experiment results are given in Section 6.",
                "Section 7 concludes this paper. 2.",
                "RELATED WORK Relevance feedback[17, 19] has long been recognized as an effective method for improving retrieval performance.",
                "Normally, the top N documents retrieved using the original query are presented to the user for judgment, after which terms are extracted from the judged relevant documents, weighted by their potential of attracting more relevant documents, and added into the query model.",
                "The expanded query usually represents the users information need better than the original one, which is often just a short keyword query.",
                "A second iteration of retrieval using this modified query usually produces significant increase in retrieval accuracy.",
                "In cases where true relevance judgment is unavailable and all top N documents are assumed to be relevant, it is called blind or pseudo feedback[5, 16] and usually still brings performance improvement.",
                "Because document is a large text unit, when it is used for relevance feedback many irrelevant terms can be introduced into the feedback process.",
                "To overcome this, passage feedback is proposed and shown to improve feedback performance[1, 23].",
                "A more direct solution is to ask the user for their relevance judgment of feedback terms.",
                "For example, in some relevance feedback systems such as [12], there is an interaction step that allows the user to add or remove expansion terms after they are automatically extracted from relevant documents.",
                "This is categorized as interactive query expansion, where the original query is augmented with user-provided terms, which can come from direct user input (free-form text or keywords)[22, 7, 10] or user selection of system-suggested terms (using thesauri[6, 22] or extracted from feedback documents[6, 22, 12, 4, 7]).",
                "In many cases term relevance feedback has been found to effectively improve retrieval performance[6, 22, 12, 4, 10].",
                "For example, the study in [12] shows that the user prefers to have explicit knowledge and direct control of which terms are used for query expansion, and the penetrable interface that provides this freedom is shown to perform better than other interfaces.",
                "However, in some other cases there is no significant benefit[3, 14], even if the user likes interacting with expansion terms.",
                "In a simulated study carried out in [18], the author compares the retrieval performance of interactive query expansion and automatic query expansion with a simulated study, and suggests that the potential benefits of the former can be hard to achieve.",
                "The user is found to be not good at identifying useful terms for query expansion, when a simple term presentation interface is unable to provide sufficient semantic context of the feedback terms.",
                "Our work differs from the previous ones in two important aspects.",
                "First, when we choose terms to present to the user for relevance judgment, we not only consider single-term value (e.g., the relative frequency of a term in the top documents, which can be measured by metrics such as Robertson Selection Value and Simplified Kullback-Leibler Distance as listed in [24]), but also examine the cluster structure of the terms, so as to produce a balanced coverage of the different topic aspects.",
                "Second, with the language modelling framework, we allow an elaborate construction of the updated query model, by setting different probabilities for different terms based on whether it is a query term, its significance in the top documents, and its cluster membership.",
                "Although techniques for adjusting query term weights exist for vector space models[17] and probablistic relevance models[9], most of the aforementioned works do not use them, choosing to just append feedback terms to the original query (thus using equal weights for them), which can lead to poorer retrieval performance.",
                "The combination of the two aspects allows our method to perform much better than the baseline.",
                "The usual way for feedback term presentation is just to display the terms in a list.",
                "There have been some works on alternative user interfaces. [8] arranges terms in a hierarchy, and [11] compares three different interfaces, including terms + checkboxes, terms + context (sentences) + checkboxes, sentences + input text box.",
                "In both studies, however, there is no significant performance difference.",
                "In our work we adopt the simplest approach of terms + checkboxes.",
                "We focus on term presentation and query model construction from feedback terms, and believe using contexts to improve feedback term quality should be orthogonal to our method. 3.",
                "GENERAL APPROACH We follow the language modeling approach, and base our method on the KL-divergence retrieval model proposed in [25].",
                "With this model, the retrieval task involves estimating a query language model θq from a given query, a document language model θd from each document, and calculating their KL-divergence D(θq||θd), which is then used to score the documents. [25] treats relevance feedback as a query model re-estimation problem, i.e., computing an updated query model θq given the original query text and the extra evidence carried by the judged relevant documents.",
                "We adopt this view, and cast our task as updating the query model from user term feedback.",
                "There are two key subtasks here: First, how to choose the best terms to present to the user for judgment, in order to gather maximal evidence about the users information need.",
                "Second, how to compute an updated query model based on this term feedback evidence, so that it captures the users information need and translates into good retrieval performance. 4.",
                "PRESENTATION TERM SELECTION Proper selection of terms to be presented to the user for judgment is crucial to the success of term feedback.",
                "If the terms are poorly chosen and there are few relevant ones, the user will have a hard time looking for useful terms to help clarify his/her information need.",
                "If the relevant terms are plentiful, but all concentrate on a single aspect of the query topic, then we will only be able to get feedback on that aspect and missing others, resulting in a breadth loss in retrieved results.",
                "Therefore, it is important to carefully select presentation terms to maximize expected gain from user feedback, i.e., those that can potentially reveal most evidence of the users information need.",
                "This is similar to active feedback[21], which suggests that a retrieval system should actively probe the users information need, and in the case of relevance feedback, the feedback documents should be chosen to maximize learning benefits (e.g. diversely so as to increase coverage).",
                "In our approach, the top N documents from an initial retrieval using the original query form the source of feedback terms: all terms that appear in them are considered candidates to present to the user.",
                "These documents serve as pseudo-feedback, since they provide a much richer context than the original query (usually very short), while the user is not asked to judge their relevance.",
                "Due to the latter reason, it is possible to make N quite large (e.g., in our experiments we set N = 60) to increase its coverage of different aspects in the topic.",
                "The simplest way of selecting feedback terms is to choose the most frequent M terms from the N documents.",
                "This method, however, has two drawbacks.",
                "First, a lot of common noisy terms will be selected due to their high frequencies in the document collection, unless a stop-word list is used for filtering.",
                "Second, the presentation list will tend to be filled by terms from major aspects of the topic; those from a minor aspect are likely to be missed due to their relatively low frequencies.",
                "We solve the above problems by two corresponding measures.",
                "First, we introduce a background model θB that is estimated from collection statistics and explains the common terms, so that they are much less likely to appear in the presentation list.",
                "Second, the terms are selected from multiple clusters in the pseudo-feedback documents, to ensure sufficient representation of different aspects of the topic.",
                "We rely on the mixture multinomial model, which is used for theme discovery in [26].",
                "Specifically, we assume the N documents contain K clusters {Ci| i = 1, 2, · · · K}, each characterized by a multinomial word distribution (also known as unigram language model) θi and corresponding to an aspect of the topic.",
                "The documents are regarded as sampled from a mixture of K + 1 components, including the K clusters and the background model: p(w|d) = λBp(w|θB) + (1 − λB) K i=1 πd,ip(w|θi) where w is a word, λB is the mixture weight for the background model θB, and πd,i is the document-specific mixture weight for the i-th cluster model θi.",
                "We then estimate the cluster models by maximizing the <br>probability</br> of the pseudo-feedback documents being generated from the multinomial mixture model: log p(D|Λ) = d∈D w∈V c(w; d) log p(w|d) where D = {di| i = 1, 2, · · · N} is the set of the N documents, V is the vocabulary, c(w; d) is ws frequency in d and Λ = {θi| i = 1, 2, · · · K} ∪ {πdij | i = 1, 2, · · · N, j = 1, 2, · · · K} is the set of model parameters to estimate.",
                "The cluster models can be efficiently estimated using the Expectation-Maximization (EM) algorithm.",
                "For its details, we refer the reader to [26].",
                "Table 1 shows the cluster models for TREC query Transportation tunnel disasters (K = 3).",
                "Note that only the middle cluster is relevant.",
                "Table 1: Cluster models for topic 363 Transportation tunnel disasters Cluster 1 Cluster 2 Cluster 3 tunnel 0.0768 tunnel 0.0935 tunnel 0.0454 transport 0.0364 fire 0.0295 transport 0.0406 traffic 0.0206 truck 0.0236 toll 0.0166 railwai 0.0186 french 0.0220 amtrak 0.0153 harbor 0.0146 smoke 0.0157 train 0.0129 rail 0.0140 car 0.0154 airport 0.0122 bridg 0.0139 italian 0.0152 turnpik 0.0105 kilomet 0.0136 firefight 0.0144 lui 0.0095 truck 0.0133 blaze 0.0127 jersei 0.0093 construct 0.0131 blanc 0.0121 pass 0.0087 · · · · · · · · · From each of the K estimated clusters, we choose the L = M/K terms with highest probabilities to form a total of M presentation terms.",
                "If a term happens to be in top L in multiple clusters, we assign it to the cluster where it has highest <br>probability</br> and let the other clusters take one more term as compensation.",
                "We also filter out terms in the original query text because they tend to always be relevant when the query is short.",
                "The selected terms are then presented to the user for judgment.",
                "A sample (completed) feedback form is shown in Figure 1.",
                "In this study we only deal with binary judgment: a presented term is by default unchecked, and a user may check it to indicate relevance.",
                "We also do not explicitly exploit negative feedback (i.e., penalizing irrelevant terms), because with binary feedback an unchecked term is not necessarily irrelevant (maybe the user is unsure about its relevance).",
                "We could ask the user for finer judgment (e.g., choosing from highly relevant, somewhat relevant, do not know, somewhat irrelevant and highly irrelevant), but binary feedback is more compact, taking less space to display and less user effort to make judgment. 5.",
                "ESTIMATING QUERY MODELS FROM TERM FEEDBACK In this section, we present several algorithms for exploiting term feedback.",
                "The algorithms take as input the original query q, the clusters {θi} as generated by the theme discovery algorithm, the set of feedback terms T and their relevance judgment R, and outputs an updated query language model θq that makes best use of the feedback evidence to capture the users information need.",
                "First we describe our notations: • θq: The original query model, derived from query terms only: p(w|θq) = c(w; q) |q| where c(w; q) is the count of w in q, and |q| = w∈q c(w; q) is the query length. • θq : The updated query model which we need to estimate from term feedback. • θi (i = 1, 2, . . .",
                "K): The unigram language model of cluster Ci, as estimated using the theme discovery algorithm. • T = {ti,j} (i = 1 . . .",
                "K, j = 1 . . .",
                "L): The set of terms presented to the user for judgment. ti,j is the j-th term chosen from cluster Ci. • R = {δw|w ∈ T}: δw is an indicator variable that is 1 if w is judged relevant or 0 otherwise. 5.1 TFB (Direct Term Feedback) This is a straight-forward form of term feedback that does not involve any secondary structure.",
                "We give a weight of 1 to terms judged relevant by the user, a weight of μ to query terms, zero weight to other terms, and then apply normalization: p(w|θq ) = δw + μ c(w; q) w ∈T δw + μ|q| where w ∈T δw is the total number of terms that are judged relevant.",
                "We call this method TFB (direct Term FeedBack).",
                "If we let μ = 1, this approach is equivalent to appending the relevant terms after the original query, which is what standard query expansion (without term reweighting) does.",
                "If we set μ > 1, we are putting more emphasis on the query terms than the checked ones.",
                "Note that the result model will be more biased toward θq if the original query is long or the user feedback is weak, which makes sense, as we can trust more on the original query in either case.",
                "Figure 1: Filled clarification form for Topic 363 363 transportation tunnel disasters Please select all terms that are relevant to the topic. traffic railway harbor rail bridge kilometer construct swiss cross link kong hong river project meter shanghai fire truck french smoke car italian firefights blaze blanc mont victim franc rescue driver chamonix emerge toll amtrak train airport turnpike lui jersey pass rome z center electron road boston speed bu submit 5.2 CFB (Cluster Feedback) Here we exploit the cluster structure that played an important role when we selected the presentation terms.",
                "The clusters represent different aspects of the query topic, each of which may or may not be relevant.",
                "If we are able to identify the relevant clusters, we can combine them to generate a query model that is good at discovering documents belonging to these clusters (instead of the irrelevant ones).",
                "We could ask the user to directly judge the relevance of a cluster after viewing representative terms in that cluster, but this would sometimes be a difficult task for the user, who has to guess the semantics of a cluster via its set of terms, which may not be well connected to one another due to a lack of context.",
                "Therefore, we propose to learn cluster feedback indirectly, inferring the relevance of a cluster through the relevance of its feedback terms.",
                "Because each cluster has an equal number of terms presented to the user, the simplest measure of a clusters relevance is the number of terms that are judged relevant in it.",
                "Intuitively, the more terms are marked relevant in a cluster, the closer the cluster is to the query topic, and the more the cluster should participate in query modification.",
                "If we combine the cluster models using weights determined this way and then interpolate with the original query model, we get the following formula for query updating, which we call CFB (Cluster FeedBack): p(w|θq ) = λp(w|θq) + (1 − λ) K i=1 L j=1 δti,j K k=1 L j=1 δtk,j p(w|θi) where L j=1 δti,j is the number of relevant terms in cluster Ci, and K k=1 L j=1 δtk,j is the total number of relevant terms.",
                "We note that when there is only one cluster (K = 1), the above formula degenerates to p(w|θq ) = λp(w|θq) + (1 − λ)p(w|θ1) which is merely pseudo-feedback of the form proposed in [25]. 5.3 TCFB (Term-cluster Feedback) TFB and CFB both have their drawbacks.",
                "TFB assigns non-zero probabilities to the presented terms that are marked relevant, but completely ignores (a lot more) others, which may be left unchecked due to the users ignorance, or simply not included in the presentation list, but we should be able to infer their relevance from the checked ones.",
                "For example, in Figure 1, since as many as 5 terms in the middle cluster (the third and fourth columns) are checked, we should have high confidence in the relevance of other terms in that cluster.",
                "CFB remedies TFBs problem by treating the terms in a cluster collectively, so that unchecked/unpresented terms receive weights when presented terms in their clusters are judged as relevant, but it does not distinguish which terms in a cluster are presented or judged.",
                "Intuitively, the judged relevant terms should receive larger weights because they are explicitly indicated as relevant by the user.",
                "Therefore, we try to combine the two methods, hoping to get the best out of both.",
                "We do this by interpolating the TFB model with the CFB model, and call it TCFB: p(w|θq ) = αp(w|θqT F B ) + (1 − α)p(w|θqCF B ) 6.",
                "EXPERIMENTS In this section, we describe our experiment results.",
                "We first describe our experiment setup and present an overview of various methods performance.",
                "Then we discuss the effects of varying the parameter setting in the algorithms, as well as the number of presentation terms.",
                "Next we analyze user term feedback behavior and its relation to retrieval performance.",
                "Finally we compare term feedback to relevance feedback and show that it has its particular advantage. 6.1 Experiment Setup and Basic Results We took the opportunity of TREC 2005 HARD Track[2] for the evaluation of our algorithms.",
                "The tracks used the AQUAINT collection, a 3GB corpus of English newswire text.",
                "The topics included 50 ones previously known to be hard, i.e. with low retrieval performance.",
                "It is for these hard topics that user feedback is most helpful, as it can provide information to disambiguate the queries; with easy topics the user may be unwilling to spend efforts for feedback if the automatic retrieval results are good enough.",
                "Participants of the track were able to submit custom-designed clarification forms (CF) to solicit feedback from human assessors provided by Table 2: Retrieval performance for different methods and CF types.",
                "The last row is the percentage of MAP improvement over the baseline.",
                "The parameter settings μ = 4, λ = 0.1, α = 0.3 are near optimal.",
                "Baseline TFB1C TFB3C TFB6C CFB1C CFB3C CFB6C TCFB1C TCFB3C TCFB6C MAP 0.219 0.288 0.288 0.278 0.254 0.305 0.301 0.274 0.309 0.304 Pr@30 0.393 0.467 0.475 0.457 0.399 0.480 0.473 0.431 0.491 0.473 RR 4339 4753 4762 4740 4600 4907 4872 4767 4947 4906 % 0% 31.5% 31.5% 26.9% 16.0% 39.3% 37.4% 25.1% 41.1% 38.8% Table 3: MAP variation with the number of presented terms. # terms TFB1C TFB3C TFB6C CFB3C CFB6C TCFB3C TCFB6C 6 0.245 0.240 0.227 0.279 0.279 0.281 0.274 12 0.261 0.261 0.242 0.299 0.286 0.297 0.281 18 0.275 0.274 0.256 0.301 0.282 0.300 0.286 24 0.276 0.281 0.265 0.303 0.292 0.305 0.292 30 0.280 0.285 0.270 0.304 0.296 0.307 0.296 36 0.282 0.288 0.272 0.307 0.297 0.309 0.297 42 0.283 0.288 0.275 0.306 0.298 0.309 0.300 48 0.288 0.288 0.278 0.305 0.301 0.309 0.303 NIST.",
                "We designed three sets of clarification forms for term feedback, differing in the choice of K, the number of clusters, and L, the number of presented terms from each cluster.",
                "They are: 1× 48, a big cluster with 48 terms, 3 × 16, 3 clusters with 16 terms each, and 6 × 8, 6 clusters with 8 terms each.",
                "The total number of presented terms (M) is fixed at 48, so by comparing the performance of different types of clarification forms we can know the effects of different degree of clustering.",
                "For each topic, an assessor would complete the forms ordered by 6 × 8, 1 × 48 and 3 × 16, spending up to three minutes on each form.",
                "The sample clarification form shown in Figure 1 is of type 3 × 16.",
                "It is a simple and compact interface in which the user can check relevant terms.",
                "The form is self-explanatory; there is no need for extra user training on how to use it.",
                "Our initinal queries are constructed only using the topic title descriptions, which are on average 2.7 words in length.",
                "As our baseline we use the KL divergence retrieval method implemented in the Lemur Toolkit1 with 5 pseudo-feedback documents.",
                "We stem the terms, choose Dirichlet smoothing with a prior of 2000, and truncate query language models to 50 terms (these settings are used throughout the experiments).",
                "For all other parameters we use Lemurs default settings.",
                "The baseline turns out to perform above average among the track participants.",
                "After an initial run using this baseline retrieval method, we take the top 60 documents for each topic and apply the theme discovery algorithm to output the clusters (1, 3, or 6 of them), based on which we generate clarification forms.",
                "After user feedback is received, we run the term feedback algorithms (TFB, CFB or TCFB) to estimate updated query models, which are then used for a second iteration of retrieval.",
                "We evaluate the different retrieval methods performance on their rankings of the top 1000 documents.",
                "The evaluation metrics we adopt include mean average (non-interpolated) precision (MAP), precision at top 30 (Pr@30) and total relevant retrieved (RR).",
                "Table 2 shows the performance of various methods and configurations of K × L. The suffixes (1C, 3C, 6C) after TFB,CFB,TCFB stand for the number of clusters (K).",
                "For example, TCFB3C means the TCFB method on the 3 × 16 clarification forms.",
                "From Table 2 we can make the following observations: 1 http://www.lemurproject.com 1.",
                "All methods perform considerably better than the pseudofeedback baseline, with TCFB3C achieving a highest 41.1% improvement in MAP, indicating significant contribution of term feedback for clarification of the users information need.",
                "In other words, term feedback is truly helpful for improving retrieval accuracy. 2.",
                "For TFB, the performance is almost equal on the 1 × 48 and 3 × 16 clarification forms in terms of MAP (although the latter is slightly better in Pr@30 and RR), and a little worse on the 6 × 8 ones. 3.",
                "Both CFB3C and CFB6C perform better than their TFB counterparts in all three metrics, suggesting that feedback on a secondary cluster structure is indeed beneficial.",
                "CFB1C is actually worse because it cannot adjust the weight of its (single) cluster from term feedback and it is merely pseudofeedback. 4.",
                "Although TCFB is just a simple mixture of TFB and CFB by interpolation, it is able to outperform both.",
                "This supports our speculation that TCFB overcomes the drawbacks of TFB (paying attention only to checked terms) and CFB (not distinguishing checked and unchecked terms in a cluster).",
                "Except for TCFB6C v.s.",
                "CFB6C, the performance advantage of TCFB over TFB/CFB is significant at p < 0.05 using the Wilcoxon signed rank test.",
                "This is not true in the case of TFB v.s.",
                "CFB, each of which is better than the other in nearly half of the topics. 6.2 Reduction of Presentation Terms In some situations we may have to reduce the number of presentation terms due to limits in display space or user feedback efforts.",
                "It is interesting to know whether our algorithms performance deteriorates when the user is presented with fewer terms.",
                "Because the presentation terms within each cluster are generated in decreasing order of their frequencies, the presentation list forms a subset of the original one if its size is reduced2 .",
                "Therefore, we can easily simulate what happens when the number of presentation terms decreases 2 There are complexities arising from terms appearing in top L of multiple clusters, but these are exceptions from M to M : we will keep all judgments of the top L = M /K terms in each cluster and discard those of others.",
                "Table 3 shows the performance of various algorithms as the number of presentation terms ranges from 6 to 48.",
                "We find that the performance of TFB is more susceptible to presentation term reduction than that of CFB or TCFB.",
                "For example, at 12 terms the MAP of TFB3C is 90.6% of that at 48 terms, while the numbers for CFB3C and TCFB3C are 98.0% and 96.1% respectively.",
                "We conjecture the reason to be that while TFBs performance heavily depends on how many good terms are chosen for query expansion, CFB only needs a rough estimate of cluster weights to work.",
                "Also, the 3 × 16 clarification forms seem to be more robust than the 6 × 8 ones: at 12 terms the MAP of TFB6C is 87.1% of that at 48 terms, lower than 90.6% for TFB3C.",
                "Similarly, for CFB it is 95.0% against 98.0%.",
                "This is natual, as for a large cluster number of 6, it is easier to get into the situation where each cluster gets too few presentation terms to make topic diversification useful.",
                "Overall, we are surprised to see that the algorithms are still able to perform reasonably well when the number of presentation terms is small.",
                "For example, at only 12 terms CFB3C (the clarification form is of size 3 × 4) can still improve 36.5% over the baseline, dropping slightly from 39.3% at 48 terms. 6.3 User Feedback Analysis In this part we study several aspects of users term feedback behavior, and whether they are connected to retrieval performance.",
                "Figure 2: Clarification form completion time distributions 0−30 30−60 60−90 90−120 120−150 150−180 0 5 10 15 20 25 30 35 completion time (seconds) #topics 1×48 3×16 6×8 Figure 2 shows the distribution of time needed to complete a clarification form3 .",
                "We see that the user is usually able to finish term feedback within a reasonably short amount of time: for more than half of the topics the clarification form is completed in just 1 minute, and only a small fraction of topics (less than 10% for 1 × 48 and 3 × 16) take more than 2 minutes.",
                "This suggests that term feedback is suitable for interactive ad-hoc retrieval, where a user usually does not want to spend too much time on providing feedback.",
                "We find that a user often makes mistakes when judging term relevance.",
                "Sometimes a relevant term may be left out because its connection to the query topic is not obvious to the user.",
                "Other times a dubious term may be included but turns out to be irrelevant.",
                "Take the topic in Figure 1 for example.",
                "There was a fire disaster in Mont 3 The maximal time is 180 seconds, as the NIST assessor would be forced to submit the form at that moment.",
                "Table 4: Term selection statistics (topic average) CF Type 1 × 48 3 × 16 6 × 8 # checked terms 14.8 13.3 11.2 # rel. terms 15.0 12.6 11.2 # rel. checked terms 7.9 6.9 5.9 precision 0.534 0.519 0.527 recall 0.526 0.548 0.527 Blanc Tunnel between France and Italy in 1999, but the user failed to select such keywords as mont, blanc, french and italian due to his/her ignorance of the event.",
                "Indeed, without proper context it would be hard to make perfect judgment.",
                "What is then, the extent to which the user is good at term feedback?",
                "Does it have serious impact on retrieval performance?",
                "To answer these questions, we need a measure of individual terms true relevance.",
                "We adopt the Simplified KL Divergence metric used in [24] to decide query expansion terms as our term relevance measure: σKLD(w) = p(w|R) log p(w|R) p(w|¬R) where p(w|R) is the <br>probability</br> that a relevant document contains term w, and p(w|¬R) is the <br>probability</br> that an irrelevant document contains w, both of which can be easily computed via maximum likelihood estimate given document-level relevance judgment.",
                "If σKLD(w) > 0, w is more likely to appear in relevant documents than irrelevant ones.",
                "We consider a term relevant if its Simplified KL Divergence value is greater than a certain threshold σ0.",
                "We can then define precision and recall of user term judgment accordingly: precision is the fraction of terms checked by the user that are relevant; recall is the fraction of presented relevant terms that are checked by the user.",
                "Table 4 shows the number of checked terms, relevant terms and relevant checked terms when σ0 is set to 1.0, as well as the precision/recall of user term judgment.",
                "Note that when the clarification forms contain more clusters, fewer terms are checked: 14.8 for 1 × 48, 13.3 for 3 × 16 and 11.2 for 6×8.",
                "Similar pattern holds for relevant terms and relevant checked terms.",
                "There seems to be a trade-off between increasing topic diversity by clustering and losing extra relevant terms: when there are more clusters, each of them gets fewer terms to present, which can hurt a major relevant cluster that contains many relevant terms.",
                "Therefore, it is not always helpful to have more clusters, e.g., TFB6C is actually worse than TFB1C.",
                "The major finding we can make from Table 4 is that the user is not particularly good at identifying relevant terms, which echoes the discovery in [18].",
                "In the case of 3 × 16 clarification forms, the average number of terms checked as relevant by the user is 13.3 per topic, and the average number of relevant terms whose σKLD value exceed 1.0 is 12.6.",
                "The user is able to recognize only 6.9 of these terms on average.",
                "Indeed, the precision and recall of user feedback terms (as defined previously) are far from perfect.",
                "On the other hand, If the user had correctly checked all such relevant terms, the performance of our algorithms would have increased a lot, as shown in Table 5.",
                "We see that TFB gets big improvement when there is an oracle who checks all relevant terms, while CFB meets a bottleneck around MAP of 0.325, since all it does is adjust cluster weights, and when the learned weights are close to being accurate, it cannot benefit more from term feedback.",
                "Also note that TCFB fails to outperform TFB, probably because TFB is sufficiently accurate.",
                "Table 5: Change of MAP when using all (and only) relevant terms (σKLD > 1.0) for feedback. original term feedback relevant term feedback TF1 0.288 0.354 TF3 0.288 0.354 TF6 0.278 0.346 CF3 0.305 0.325 CF6 0.301 0.326 TCF3 0.309 0.345 TCF6 0.304 0.341 6.4 Comparison with Relevance Feedback Now we compare term feedback with document-level relevance feedback, in which the user is presented with the top N documents from an initial retrieval and asked to judge their relevance.",
                "The feedback process is simulated using document relevance judgment from NIST.",
                "We use the mixture model based feedback method proposed in [25], with mixture noise set to 0.95 and feedback coefficient set to 0.9.",
                "Comparative evaluation of relevance feedback against other methods is complicated by the fact that some documents have already been viewed during feedback, so it makes no sense to include them in the retrieval results of the second run.",
                "However, this does not hold for term feedback.",
                "Thus, to make it fair w.r.t. users information gain, if the feedback documents are relevant, they should be kept in the top of the ranking; if they are irrelevant, they should be left out.",
                "Therefore, we use relevance feedback to produce a ranking of top 1000 retrieved documents but with every feedback document excluded, and then prepend the relevant feedback documents at the front.",
                "Table 6 shows the performance of relevance feedback for different values of N and compares it with TCFB3C.",
                "Table 6: Performance of relevance feedback for different number of feedback documents (N).",
                "N MAP Pr@30 RR 5 0.302 0.586 4779 10 0.345 0.670 4916 20 0.389 0.772 5004 TCFB3C 0.309 0.491 4947 We see that the performance of TCFB3C is comparable to that of relevance feedback using 5 documents.",
                "Although it is poorer than when there are 10 feedback documents in terms of MAP and Pr@30, it does retrieve more documents (4947) when going down the ranked list.",
                "We try to compare the quality of automatically inserted terms in relevance feedback with that of manually selected terms in term feedback.",
                "This is done by truncating the relevance feedback modified query model to a size equal to the number of checked terms for the same topic.",
                "We can then compare the terms in the truncated model with the checked terms.",
                "Figure 3 shows the distribution of the terms σKLD scores.",
                "We find that term feedback tends to produce expansion terms of higher quality(those with σKLD > 1) compared to relevance feedback (with 10 feedback documents).",
                "This does not contradict the fact that the latter yields higher retrieval performance.",
                "Actually, when we use the truncated query model instead of the intact one refined from relevance feedback, the MAP is only 0.304.",
                "The truth Figure 3: Comparison of expansion term quality between relevance feedback (with 10 feedback documents) and term feedback (with 3 × 16 CFs) −1−0 0−1 1−2 2−3 3−4 4−5 5−6 0 50 100 150 200 250 300 350 σKLD #terms relevance feedback term feedback is, although there are many unwanted terms in the expanded query model from feedback documents, there are also more relevant terms than what the user can possibly select from the list of presentation terms generated with pseudo-feedback documents, and the positive effects often outweights the negative ones.",
                "We are interested to know under what circumstances term feedback has advantage over relevance feedback.",
                "One such situation is when none of the top N feedback documents is relevant, rendering relevance feedback useless.",
                "This is not infrequent, as one might have thought: out of the 50 topics, there are 13 such cases when N = 5, 10 when N = 10, and still 3 when N = 20.",
                "When this happens, one can only back off to the original retrieval method; the power of relevance feedback is lost.",
                "Surprisingly, in 11 out of 13 such cases where relevance feedback seems impossible, the user is able to check at least 2 relevant terms from the 3 × 16 clarification form (we consider term t to be relevant if σKLD(t) > 1.0).",
                "Furthermore, in 10 out of them TCFB3C outperforms the pseudo-feedback baseline, increasing MAP from 0.076 to 0.146 on average (these are particularly hard topics).",
                "We think that there are two possible explanations for this phenomenon of term feedback being active even when relevance feedback does not work: First, even if none of the top N (suppose it is a small number) documents are relevant, we may still find relevant documents in top 60, which is more inclusive but usually unreachable when people are doing relevance feedback in interactive ad-hoc search, from which we can draw feedback terms.",
                "This is true for topic 367 piracy, where the top 10 feedback documents are all about software piracy, yet there are documents between 10-60 that are about piracy on the seas (which is about the real information need), contributing terms such as pirate, ship for selection in the clarification form.",
                "Second, for some topics, a document needs to meet some special condition in order to be relevant.",
                "The top N documents may be related to the topic, but nonetheless irrelevant.",
                "In this case, we may still extract useful terms from these documents, even if they do not qualify as relevant ones.",
                "For example, in topic 639 consumer online shopping, a document needs to mention what contributes to shopping growth to really match the specified information need, hence none of the top 10 feedback documents are regarded as relevant.",
                "But nevertheless, the feedback terms such as retail, commerce are good for query expansion. 7.",
                "CONCLUSIONS In this paper we studied the use of term feedback for interactive information retrieval in the language modeling approach.",
                "We proposed a cluster-based method for selecting presentation terms as well as algorithms to estimate refined query models from user term feedback.",
                "We saw significant improvement in retrieval accuracy brought by term feedback, in spite of the fact that a user often makes mistakes in relevance judgment that hurts its performance.",
                "We found the best-performing algorithm to be TCFB, which benefits from the combination of directly observed term evidence with TFB and indirectly learned cluster relevance with CFB.",
                "When we reduced the number of presentation terms, term feedback is still able to keep much of its performance gain over the baseline.",
                "Finally, we compared term feedback to document-level relevance feedback, and found that TCFB3Cs performance is on a par with the latter with 5 feedback documents.",
                "We regarded term feedback as a viable alternative to traditional relevance feedback, especially when there are no relevant documents in the top.",
                "We propose to extend our work in several ways.",
                "First, we want to study whether the use of various contexts can help the user to better identify term relevance, while not sacrificing the simplicity and compactness of term feedback.",
                "Second, currently all terms are presented to the user in a single batch.",
                "We could instead consider iterative term feedback, by presenting a small number of terms first, and show more terms after receiving user feedback or stop when the refined query is good enough.",
                "The presented terms should be selected dynamically to maximize learning benefits at any moment.",
                "Third, we have plans to incorporate term feedback into our UCAIR toolbar[20], an Internet Explorer plugin, to make it work for web search.",
                "We are also interested in studying how to combine term feedback with relevance feedback or implicit feedback.",
                "We could, for example, allow the user to dynamically modify terms in a language model learned from feedback documents. 8.",
                "ACKNOWLEDGMENT This work is supported in part by the National Science Foundation grants IIS-0347933 and IIS-0428472. 9.",
                "REFERENCES [1] J. Allan.",
                "Relevance feedback with too much data.",
                "In Proceedings of the 18th annual international ACM SIGIR conference on research and development in information retrieval, pages 337-343, 1995. [2] J. Allan.",
                "HARD track overview in TREC 2005 - High Accuracy Retrieval from Documents.",
                "In The Fourteenth Text REtrieval Conference, 2005. [3] P. Anick.",
                "Using terminological feedback for web search refinement: a log-based study.",
                "In Proceedings of the 26th annual international ACM SIGIR conference on research and development in informaion retrieval, pages 88-95, 2003. [4] P. G. Anick and S. Tipirneni.",
                "The paraphrase search assistant: terminological feedback for iterative information seeking.",
                "In Proceedings of the 22nd annual international ACM SIGIR conference on research and development in information retrieval, pages 153-159, 1999. [5] C. Buckley, G. Salton, J. Allan, and A. Singhal.",
                "Automatic query expansion using SMART.",
                "In Proceedings of the Third Text REtrieval Conference, 1994. [6] D. Harman.",
                "Towards interactive query expansion.",
                "In Proceedings of the 11th annual international ACM SIGIR conference on research and development in information retrieval, pages 321-331, 1988. [7] N. A. Jaleel, A. Corrada-Emmanuel, Q. Li, X. Liu, C. Wade, and J. Allan.",
                "UMass at TREC 2003: HARD and QA.",
                "In TREC, pages 715-725, 2003. [8] H. Joho, C. Coverson, M. Sanderson, and M. Beaulieu.",
                "Hierarchical presentation of expansion terms.",
                "In Proceedings of the 2002 ACM symposium on applied computing, pages 645-649, 2002. [9] K. S. Jones, S. Walker, and S. E. Robertson.",
                "A probabilistic model of information retrieval: development and status.",
                "Technical Report 446, Computer Laboratory, University of Cambridge, 1998. [10] D. Kelly, V. D. Dollu, and X. Fu.",
                "The loquacious user: a document-independent source of terms for query expansion.",
                "In Proceedings of the 28th annual international ACM SIGIR conference on research and development in information retrieval, pages 457-464, 2005. [11] D. Kelly and X. Fu.",
                "Elicitation of term relevance feedback: an investigation of term source and context.",
                "In Proceedings of the 29th annual international ACM SIGIR conference on research and development in information retrieval, 2006. [12] J. Koenemann and N. Belkin.",
                "A case for interaction: A study of interactive information retrieval behavior and effectiveness.",
                "In Proceedings of the SIGCHI conference on human factors in computing systems, pages 205-212, 1996. [13] V. Lavrenko and W. B. Croft.",
                "Relevance-based language models.",
                "In Research and Development in Information Retrieval, pages 120-127, 2001. [14] Y. Nemeth, B. Shapira, and M. Taeib-Maimon.",
                "Evaluation of the real and perceived value of automatic and interactive query expansion.",
                "In Proceedings of the 27th annual international ACM SIGIR conference on research and development in information retrieval, pages 526-527, 2004. [15] J. Ponte.",
                "A Language Modeling Approach to Information Retrieval.",
                "PhD thesis, University of Massachusetts at Amherst, 1998. [16] S. E. Robertson, S. Walker, S. Jones, M. Beaulieu, and M. Gatford.",
                "Okapi at TREC-3.",
                "In Proceedings of the Third Text REtrieval Conference, 1994. [17] J. Rocchio.",
                "Relevance feedback in information retrieval.",
                "In The SMART retrieval system, pages 313-323. 1971. [18] I. Ruthven.",
                "Re-examining the potential effectiveness of interactive query expansion.",
                "In Proceedings of the 26th annual international ACM SIGIR conference on research and development in informaion retrieval, pages 213-220, 2003. [19] G. Salton and C. Buckley.",
                "Improving retrieval performance by relevance feedback.",
                "Journal of the American Society for Information Science, 41:288-297, 1990. [20] X. Shen, B. Tan, and C. Zhai.",
                "Implicit user modeling for personalized search.",
                "In Proceedings of the 14th ACM international conference on information and knowledge management, pages 824-831, 2005. [21] X. Shen and C. Zhai.",
                "Active feedback in ad-hoc information retrieval.",
                "In Proceedings of the 28th annual international ACM SIGIR conference on research and development in information retrieval, pages 59-66, 2005. [22] A. Spink.",
                "Term relevance feedback and query expansion: relation to design.",
                "In Proceedings of the 17th annual international ACM SIGIR conference on research and development in information retrieval, pages 81-90, 1994. [23] J. Xu and W. B. Croft.",
                "Query expansion using local and global document analysis.",
                "In Proceedings of the 19th annual international ACM SIGIR conference on research and development in information retrieval, pages 4-11, 1996. [24] H. Zaragoza, N. Craswell, M. Taylor, S. Saria, and S. Robertson.",
                "Microsoft cambridge at TREC-13: Web and HARD tracks.",
                "In Proceedings of the 13th Text REtrieval Conference, 2004. [25] C. Zhai and J. Lafferty.",
                "Model-based feedback in the language modeling approach to information retrieval.",
                "In Proceedings of the tenth international conference on information and knowledge management, pages 403-410, 2001. [26] C. Zhai, A. Velivelli, and B. Yu.",
                "A cross-collection mixture model for comparative text mining.",
                "In Proceedings of the tenth ACM SIGKDD international conference on knowledge discovery and data mining, pages 743-748, 2004."
            ],
            "original_annotated_samples": [
                "We then estimate the cluster models by maximizing the <br>probability</br> of the pseudo-feedback documents being generated from the multinomial mixture model: log p(D|Λ) = d∈D w∈V c(w; d) log p(w|d) where D = {di| i = 1, 2, · · · N} is the set of the N documents, V is the vocabulary, c(w; d) is ws frequency in d and Λ = {θi| i = 1, 2, · · · K} ∪ {πdij | i = 1, 2, · · · N, j = 1, 2, · · · K} is the set of model parameters to estimate.",
                "If a term happens to be in top L in multiple clusters, we assign it to the cluster where it has highest <br>probability</br> and let the other clusters take one more term as compensation.",
                "We adopt the Simplified KL Divergence metric used in [24] to decide query expansion terms as our term relevance measure: σKLD(w) = p(w|R) log p(w|R) p(w|¬R) where p(w|R) is the <br>probability</br> that a relevant document contains term w, and p(w|¬R) is the <br>probability</br> that an irrelevant document contains w, both of which can be easily computed via maximum likelihood estimate given document-level relevance judgment."
            ],
            "translated_annotated_samples": [
                "Luego estimamos los modelos de clúster maximizando la <br>probabilidad</br> de que los documentos de pseudo-retroalimentación sean generados a partir del modelo de mezcla multinomial: log p(D|Λ) = d∈D w∈V c(w; d) log p(w|d) donde D = {di| i = 1, 2, · · · N} es el conjunto de los N documentos, V es el vocabulario, c(w; d) es la frecuencia de w en d y Λ = {θi| i = 1, 2, · · · K} ∪ {πdij | i = 1, 2, · · · N, j = 1, 2, · · · K} es el conjunto de parámetros del modelo a estimar.",
                "Si un término resulta estar en el top L en múltiples grupos, lo asignamos al grupo donde tenga la <br>probabilidad</br> más alta y permitimos que los otros grupos tomen un término adicional como compensación.",
                "Adoptamos la métrica de Divergencia KL Simplificada utilizada en [24] para decidir los términos de expansión de la consulta como nuestra medida de relevancia de términos: σKLD(w) = p(w|R) log p(w|R) p(w|¬R) donde p(w|R) es la <br>probabilidad</br> de que un documento relevante contenga el término w, y p(w|¬R) es la <br>probabilidad</br> de que un documento irrelevante contenga w, ambas pueden ser fácilmente calculadas mediante una estimación de máxima verosimilitud dada la relevancia a nivel de documento."
            ],
            "translated_text": "Comentarios sobre el término para la recuperación de información con modelos de lenguaje Bin Tan†, Atulya Velivelli‡, Hui Fang†, ChengXiang Zhai† Dept. En este documento estudiamos la retroalimentación basada en términos para la recuperación de información en el enfoque de modelado de lenguaje. Con la retroalimentación de términos, un usuario juzga directamente la relevancia de términos individuales sin interacción con documentos de retroalimentación, tomando el control total del proceso de expansión de la consulta. Proponemos un método basado en clusters para seleccionar términos para presentar al usuario para su evaluación, así como algoritmos efectivos para construir modelos de lenguaje de consulta refinados a partir de la retroalimentación de términos del usuario. Nuestros algoritmos han demostrado proporcionar una mejora significativa en la precisión de recuperación en comparación con una línea base sin retroalimentación, y logran un rendimiento comparable al de la retroalimentación de relevancia. Son útiles incluso cuando no hay documentos relevantes en la parte superior. Categorías y Descriptores de Asignaturas H.3.3 [Búsqueda y Recuperación de Información]: Modelos de recuperación Términos generales Algoritmos 1. En el enfoque de modelado del lenguaje para la recuperación de información, la retroalimentación a menudo se modela como la estimación de un modelo de consulta mejorado o un modelo de relevancia basado en un conjunto de documentos de retroalimentación [25, 13]. Esto está en línea con la forma tradicional de realizar retroalimentación de relevancia: presentar al usuario documentos/pasajes para su juicio de relevancia y luego extraer términos de los documentos o pasajes juzgados para expandir la consulta inicial. Es una forma indirecta de buscar la ayuda de los usuarios para la construcción del modelo de consulta, en el sentido de que el modelo de consulta refinado (basado en términos) se aprende a través de documentos/pasajes de retroalimentación, que son estructuras de alto nivel de términos. Tiene la desventaja de que los términos irrelevantes, que ocurren junto con los relevantes en el contenido evaluado, pueden ser utilizados erróneamente para la expansión de la consulta, causando efectos no deseados. Por ejemplo, para la consulta TREC del telescopio Hubble logros, cuando un documento relevante habla más sobre la reparación de los telescopios que sobre sus descubrimientos, términos irrelevantes como caminata espacial pueden ser agregados a la consulta modificada. Podemos considerar una forma más directa de involucrar a un usuario en la mejora del modelo de consulta, sin un paso intermedio de retroalimentación de documentos que pueda introducir ruido. La idea es presentar al usuario un número (razonable) de términos individuales y pedirle que juzgue la relevancia de cada término o especifique directamente sus probabilidades en el modelo de consulta. Esta estrategia ha sido discutida en [15], pero hasta donde sabemos, no ha sido estudiada seriamente en la literatura existente de modelado de lenguaje. En comparación con la retroalimentación de relevancia tradicional, este enfoque basado en términos para el refinamiento del modelo de consulta interactivo tiene varias ventajas. Primero, el usuario tiene un mejor control del modelo de consulta final a través de la manipulación directa de términos: él/ella puede dictar qué términos son relevantes, irrelevantes y posiblemente, en qué medida. Esto evita el riesgo de introducir términos no deseados en el modelo de consulta, aunque a veces el usuario introduce términos de baja calidad. Segundo, dado que un término tarda menos tiempo en ser evaluado que el texto completo o el resumen de un documento, y con tan solo alrededor de 20 términos presentados se puede lograr una mejora significativa en el rendimiento de recuperación (como demostraremos más adelante), el feedback de términos permite recopilar la retroalimentación del usuario de manera más rápida. Esto es especialmente útil para la búsqueda interactiva ad hoc. En tercer lugar, a veces no hay documentos relevantes en los primeros N resultados recuperados inicialmente si el tema es difícil. Esto suele ser cierto cuando N está limitado a ser pequeño, lo cual surge del hecho de que el usuario no está dispuesto a evaluar demasiados documentos. En este caso, la retroalimentación de relevancia es inútil, ya que no se puede aprovechar ningún documento relevante, pero la retroalimentación de términos sigue siendo útil, al permitir seleccionar términos relevantes de documentos irrelevantes. Durante nuestra participación en la pista HARD de TREC 2005 y el estudio continuado posteriormente, exploramos cómo aprovechar la retroalimentación de términos por parte del usuario para construir modelos de consulta mejorados para la recuperación de información en el enfoque de modelado de lenguaje. Identificamos dos sub tareas clave del feedback basado en términos, es decir, la selección de términos para la presentación previa al feedback y la construcción del modelo de consulta posterior al feedback, con algoritmos efectivos desarrollados para ambos. Impusimos una estructura de clúster secundaria en los términos y descubrimos que una vista de clúster arroja una mayor comprensión de la necesidad de información de los usuarios, y proporciona una buena forma de utilizar la retroalimentación de términos. A través de experimentos encontramos que el feedback a término mejora significativamente sobre la línea base sin feedback, a pesar de que el usuario a menudo comete errores en la evaluación de relevancia. Entre nuestros algoritmos, el que tiene el mejor rendimiento de recuperación es TCFB, la combinación de TFB, el algoritmo de retroalimentación de términos directos, y CFB, el algoritmo de retroalimentación basado en clústeres. También variamos el número de términos de retroalimentación y observamos una mejora razonable incluso con números bajos. Finalmente, al comparar la retroalimentación a nivel de términos con la retroalimentación a nivel de documentos, encontramos que es una alternativa viable a esta última con un rendimiento competitivo en la recuperación. El resto del documento está organizado de la siguiente manera. La sección 2 discute algunos trabajos relacionados. La sección 4 describe nuestro enfoque general para la retroalimentación de términos. Presentamos nuestro método para la selección de términos de presentación en la Sección 3 y los algoritmos para la construcción del modelo de consulta en la Sección 5. Los resultados del experimento se presentan en la Sección 6. La sección 7 concluye este documento. 2. TRABAJO RELACIONADO La retroalimentación de relevancia[17, 19] ha sido reconocida durante mucho tiempo como un método efectivo para mejorar el rendimiento de recuperación. Normalmente, los N documentos principales recuperados utilizando la consulta original se presentan al usuario para su evaluación, después de lo cual se extraen términos de los documentos relevantes evaluados, ponderados por su potencial de atraer más documentos relevantes, y se añaden al modelo de consulta. La consulta ampliada suele representar mejor la necesidad de información de los usuarios que la original, que a menudo es solo una consulta de palabras clave cortas. Una segunda iteración de recuperación utilizando esta consulta modificada generalmente produce un aumento significativo en la precisión de la recuperación. En los casos en los que no se dispone de una evaluación de relevancia real y se asume que todos los documentos principales N son relevantes, se denomina retroalimentación ciega o pseudo [5, 16] y generalmente sigue mejorando el rendimiento. Debido a que un documento es una unidad de texto grande, cuando se utiliza para retroalimentación de relevancia, muchos términos irrelevantes pueden ser introducidos en el proceso de retroalimentación. Para superar esto, se propone el feedback de pasajes y se muestra que mejora el rendimiento del feedback. Una solución más directa es pedir al usuario su juicio de relevancia sobre los términos de retroalimentación. Por ejemplo, en algunos sistemas de retroalimentación de relevancia como [12], hay un paso de interacción que permite al usuario agregar o eliminar términos de expansión después de que son extraídos automáticamente de los documentos relevantes. Esto se clasifica como expansión interactiva de consultas, donde la consulta original se amplía con términos proporcionados por el usuario, que pueden provenir de la entrada directa del usuario (texto libre o palabras clave) o de la selección del usuario de términos sugeridos por el sistema (utilizando tesauros o extraídos de documentos de retroalimentación). En muchos casos, se ha encontrado que la retroalimentación de relevancia de términos mejora de manera efectiva el rendimiento de recuperación [6, 22, 12, 4, 10]. Por ejemplo, el estudio en [12] muestra que el usuario prefiere tener un conocimiento explícito y control directo de qué términos se utilizan para la expansión de la consulta, y se demuestra que la interfaz penetrable que proporciona esta libertad tiene un mejor rendimiento que otras interfaces. Sin embargo, en algunos otros casos no hay un beneficio significativo[3, 14], incluso si al usuario le gusta interactuar con los términos de expansión. En un estudio simulado realizado en [18], el autor compara el rendimiento de recuperación de la expansión interactiva de consultas y la expansión automática de consultas con un estudio simulado, y sugiere que los posibles beneficios de la primera pueden ser difíciles de lograr. Se descubre que el usuario no es bueno identificando términos útiles para la expansión de la consulta, cuando una interfaz de presentación de términos simple no puede proporcionar suficiente contexto semántico de los términos de retroalimentación. Nuestro trabajo difiere de los anteriores en dos aspectos importantes. Primero, al elegir los términos para presentar al usuario para la evaluación de relevancia, no solo consideramos el valor de un solo término (por ejemplo, la frecuencia relativa de un término en los documentos principales, que puede medirse mediante métricas como el Valor de Selección de Robertson y la Distancia Kullback-Leibler Simplificada como se detalla en [24]), sino que también examinamos la estructura de los grupos de términos, con el fin de producir una cobertura equilibrada de los diferentes aspectos del tema. Segundo, con el marco de modelado del lenguaje, permitimos una construcción detallada del modelo de consulta actualizado, estableciendo diferentes probabilidades para diferentes términos según si es un término de consulta, su importancia en los documentos principales y su pertenencia a un clúster. Aunque existen técnicas para ajustar los pesos de los términos de consulta en modelos de espacio vectorial y modelos de relevancia probabilística, la mayoría de los trabajos mencionados no las utilizan, optando por simplemente añadir términos de retroalimentación a la consulta original (usando pesos iguales para ellos), lo que puede llevar a un rendimiento de recuperación más pobre. La combinación de los dos aspectos permite que nuestro método funcione mucho mejor que el punto de referencia. La forma habitual de presentar los términos de retroalimentación es simplemente mostrar los términos en una lista. Ha habido algunos trabajos sobre interfaces de usuario alternativas. [8] organiza los términos en una jerarquía, y [11] compara tres interfaces diferentes, incluyendo términos + casillas de verificación, términos + contexto (oraciones) + casillas de verificación, oraciones + cuadro de texto de entrada. En ambos estudios, sin embargo, no hay una diferencia significativa en el rendimiento. En nuestro trabajo adoptamos el enfoque más simple de términos + casillas de verificación. Nos enfocamos en la presentación de términos y la construcción de modelos de consulta a partir de términos de retroalimentación, y creemos que el uso de contextos para mejorar la calidad de los términos de retroalimentación debería ser ortogonal a nuestro método. 3. Enfoque general: Seguimos el enfoque de modelado del lenguaje y basamos nuestro método en el modelo de recuperación de divergencia KL propuesto en [25]. Con este modelo, la tarea de recuperación implica estimar un modelo de lenguaje de consulta θq a partir de una consulta dada, un modelo de lenguaje de documento θd de cada documento, y calcular su divergencia KL D(θq||θd), que luego se utiliza para puntuar los documentos. [25] trata la retroalimentación de relevancia como un problema de reestimación del modelo de consulta, es decir, calcular un modelo de consulta actualizado θq dado el texto de la consulta original y la evidencia adicional proporcionada por los documentos relevantes juzgados. Adoptamos esta perspectiva y planteamos nuestra tarea como la actualización del modelo de consulta a partir de la retroalimentación de términos de usuario. Aquí hay dos tareas clave: Primero, cómo elegir los mejores términos para presentar al usuario para su evaluación, con el fin de recopilar la máxima evidencia sobre la necesidad de información del usuario. Segundo, cómo calcular un modelo de consulta actualizado basado en esta retroalimentación de términos, de manera que capture la necesidad de información de los usuarios y se traduzca en un buen rendimiento de recuperación. 4. SELECCIÓN DE TÉRMINOS DE PRESENTACIÓN La selección adecuada de los términos que se presentarán al usuario para su evaluación es crucial para el éxito de la retroalimentación de términos. Si los términos están mal elegidos y hay pocos relevantes, el usuario tendrá dificultades para buscar términos útiles que ayuden a aclarar su necesidad de información. Si los términos relevantes son abundantes, pero todos se centran en un solo aspecto del tema de la consulta, entonces solo podremos obtener retroalimentación sobre ese aspecto y perderemos otros, lo que resultará en una pérdida de amplitud en los resultados recuperados. Por lo tanto, es importante seleccionar cuidadosamente los términos de presentación para maximizar la ganancia esperada de la retroalimentación del usuario, es decir, aquellos que pueden revelar potencialmente la mayor evidencia de la necesidad de información del usuario. Esto es similar al feedback activo[21], que sugiere que un sistema de recuperación debería sondear activamente la necesidad de información de los usuarios, y en el caso del feedback de relevancia, los documentos de feedback deberían ser elegidos para maximizar los beneficios de aprendizaje (por ejemplo, de manera diversa para aumentar la cobertura). En nuestro enfoque, los N documentos principales de una recuperación inicial utilizando la consulta original forman la fuente de términos de retroalimentación: todos los términos que aparecen en ellos se consideran candidatos para presentar al usuario. Estos documentos sirven como pseudo-retroalimentación, ya que proporcionan un contexto mucho más amplio que la consulta original (generalmente muy breve), sin que se le pida al usuario que juzgue su relevancia. Debido a esta última razón, es posible hacer que N sea bastante grande (por ejemplo, en nuestros experimentos establecimos N = 60) para aumentar su cobertura de diferentes aspectos en el tema. La forma más sencilla de seleccionar términos de retroalimentación es elegir los M términos más frecuentes de los N documentos. Este método, sin embargo, tiene dos inconvenientes. Primero, se seleccionarán muchos términos ruidosos comunes debido a sus altas frecuencias en la colección de documentos, a menos que se utilice una lista de palabras vacías para filtrar. En segundo lugar, la lista de presentación tiende a estar compuesta por términos de los aspectos principales del tema; es probable que se pasen por alto aquellos de un aspecto menor debido a sus frecuencias relativamente bajas. Resolvemos los problemas anteriores mediante dos medidas correspondientes. Primero, introducimos un modelo de fondo θB que se estima a partir de estadísticas de la colección y explica los términos comunes, de modo que es mucho menos probable que aparezcan en la lista de presentación. Segundo, los términos se seleccionan de múltiples grupos en los documentos de retroalimentación simulada, para garantizar una representación suficiente de diferentes aspectos del tema. Nos basamos en el modelo multinomial de mezcla, que se utiliza para el descubrimiento de temas en [26]. Específicamente, asumimos que los N documentos contienen K grupos {Ci| i = 1, 2, · · · K}, cada uno caracterizado por una distribución de palabras multinomial (también conocida como modelo de lenguaje unigrama) θi y correspondiente a un aspecto del tema. Los documentos se consideran como muestreados de una mezcla de K + 1 componentes, incluidos los K grupos y el modelo de fondo: p(w|d) = λBp(w|θB) + (1 − λB) K i=1 πd,ip(w|θi) donde w es una palabra, λB es el peso de la mezcla para el modelo de fondo θB, y πd,i es el peso de la mezcla específico del documento para el modelo de cluster i-ésimo θi. Luego estimamos los modelos de clúster maximizando la <br>probabilidad</br> de que los documentos de pseudo-retroalimentación sean generados a partir del modelo de mezcla multinomial: log p(D|Λ) = d∈D w∈V c(w; d) log p(w|d) donde D = {di| i = 1, 2, · · · N} es el conjunto de los N documentos, V es el vocabulario, c(w; d) es la frecuencia de w en d y Λ = {θi| i = 1, 2, · · · K} ∪ {πdij | i = 1, 2, · · · N, j = 1, 2, · · · K} es el conjunto de parámetros del modelo a estimar. Los modelos de conglomerados pueden ser estimados eficientemente utilizando el algoritmo de Expectation-Maximization (EM). Para más detalles, remitimos al lector a [26]. La Tabla 1 muestra los modelos de clúster para la consulta de TREC sobre desastres en túneles de transporte (K = 3). Ten en cuenta que solo el grupo central es relevante. Tabla 1: Modelos de clúster para el tema 363 Desastres en túneles de transporte Clúster 1 Clúster 2 Clúster 3 túnel 0.0768 túnel 0.0935 túnel 0.0454 transporte 0.0364 fuego 0.0295 transporte 0.0406 tráfico 0.0206 camión 0.0236 peaje 0.0166 ferrocarril 0.0186 francés 0.0220 Amtrak 0.0153 puerto 0.0146 humo 0.0157 tren 0.0129 riel 0.0140 coche 0.0154 aeropuerto 0.0122 puente 0.0139 italiano 0.0152 autopista 0.0105 kilómetro 0.0136 incendio 0.0144 lui 0.0095 camión 0.0133 llamas 0.0127 Jersey 0.0093 construcción 0.0131 blanco 0.0121 paso 0.0087 · · · · · · · · · De cada uno de los K clústeres estimados, elegimos los L = M/K términos con las probabilidades más altas para formar un total de M términos de presentación. Si un término resulta estar en el top L en múltiples grupos, lo asignamos al grupo donde tenga la <br>probabilidad</br> más alta y permitimos que los otros grupos tomen un término adicional como compensación. También filtramos los términos en el texto de la consulta original porque tienden a ser siempre relevantes cuando la consulta es corta. Los términos seleccionados son luego presentados al usuario para su evaluación. Se muestra un formulario de retroalimentación (completado) de muestra en la Figura 1. En este estudio solo tratamos con juicios binarios: un término presentado está por defecto sin marcar, y un usuario puede marcarlo para indicar relevancia. Tampoco explotamos explícitamente la retroalimentación negativa (es decir, penalizar términos irrelevantes), porque con la retroalimentación binaria un término no verificado no es necesariamente irrelevante (quizás el usuario no está seguro de su relevancia). Podríamos pedir al usuario un juicio más detallado (por ejemplo, eligiendo entre altamente relevante, algo relevante, no sé, algo irrelevante y altamente irrelevante), pero la retroalimentación binaria es más compacta, ocupando menos espacio para mostrar y requiriendo menos esfuerzo por parte del usuario para emitir un juicio. 5. ESTIMACIÓN DE MODELOS DE CONSULTA A PARTIR DE RETROALIMENTACIÓN DE TÉRMINOS En esta sección, presentamos varios algoritmos para aprovechar la retroalimentación de términos. Los algoritmos toman como entrada la consulta original q, los grupos {θi} generados por el algoritmo de descubrimiento de temas, el conjunto de términos de retroalimentación T y su juicio de relevancia R, y producen un modelo de lenguaje de consulta actualizado θq que hace el mejor uso de la evidencia de retroalimentación para capturar la necesidad de información de los usuarios. Primero describimos nuestras notaciones: • θq: El modelo de consulta original, derivado solo de los términos de la consulta: p(w|θq) = c(w; q) |q| donde c(w; q) es la cantidad de veces que w aparece en q, y |q| = w∈q c(w; q) es la longitud de la consulta. • θq: El modelo de consulta actualizado que necesitamos estimar a partir de la retroalimentación de términos. • θi (i = 1, 2, . . . K): El modelo de lenguaje unigrama del clúster Ci, estimado utilizando el algoritmo de descubrimiento de temas. • T = {ti,j} (i = 1 . . . K, j = 1 . . . \n\nK, j = 1 . . . El conjunto de términos presentados al usuario para su evaluación. ti,j es el j-ésimo término elegido del grupo Ci. • R = {δw|w ∈ T}: δw es una variable indicadora que es 1 si w es considerado relevante o 0 en caso contrario. 5.1 TFB (Retroalimentación Directa de Términos) Esta es una forma directa de retroalimentación de términos que no implica ninguna estructura secundaria. Asignamos un peso de 1 a los términos considerados relevantes por el usuario, un peso de μ a los términos de la consulta, un peso de cero a los demás términos, y luego aplicamos la normalización: p(w|θq ) = δw + μ c(w; q) w ∈T δw + μ|q| donde w ∈T δw es el número total de términos considerados relevantes. Llamamos a este método TFB (retroalimentación directa de términos). Si dejamos que μ = 1, este enfoque es equivalente a agregar los términos relevantes después de la consulta original, que es lo que hace la expansión estándar de consultas (sin reponderación de términos). Si establecemos μ > 1, estamos poniendo más énfasis en los términos de consulta que en los verificados. Ten en cuenta que el modelo de resultado será más sesgado hacia θq si la consulta original es larga o el feedback del usuario es débil, lo cual tiene sentido, ya que podemos confiar más en la consulta original en ambos casos. Figura 1: Formulario de aclaración completado para el Tema 363 363 desastres en túneles de transporte. Por favor, seleccione todos los términos relevantes para el tema. tráfico ferrocarril puerto puente ferroviario kilómetro construir suizo enlace cruzado hongkonés río proyecto metro camión de bomberos francés humo coche italiano bomberos incendio blanco montaña víctima francés rescate conductor chamonix emerger peaje tren amtrak aeropuerto turnpike jersey pass rome z centro electrón carretera boston velocidad bu enviar 5.2 CFB (Retroalimentación de Clúster) Aquí explotamos la estructura de clúster que jugó un papel importante cuando seleccionamos los términos de presentación. Los grupos representan diferentes aspectos del tema de la consulta, cada uno de los cuales puede ser relevante o no. Si somos capaces de identificar los grupos relevantes, podemos combinarlos para generar un modelo de consulta que sea bueno para descubrir documentos pertenecientes a estos grupos (en lugar de los irrelevantes). Podríamos pedir al usuario que juzgue directamente la relevancia de un grupo después de ver los términos representativos en ese grupo, pero a veces esto sería una tarea difícil para el usuario, quien tendría que adivinar la semántica de un grupo a través de su conjunto de términos, los cuales podrían no estar bien conectados entre sí debido a la falta de contexto. Por lo tanto, proponemos aprender retroalimentación de clúster de forma indirecta, inferir la relevancia de un clúster a través de la relevancia de sus términos de retroalimentación. Dado que cada grupo tiene un número igual de términos presentados al usuario, la medida más simple de la relevancia de un grupo es el número de términos que se consideran relevantes en él. Intuitivamente, cuantos más términos estén marcados como relevantes en un grupo, más cerca estará el grupo del tema de la consulta y más debería participar en la modificación de la consulta. Si combinamos los modelos de clúster utilizando pesos determinados de esta manera y luego interpolamos con el modelo de consulta original, obtenemos la siguiente fórmula para la actualización de la consulta, que llamamos CFB (Retroalimentación de Clúster): p(w|θq) = λp(w|θq) + (1 − λ) K i=1 L j=1 δti,j K k=1 L j=1 δtk,j p(w|θi) donde L j=1 δti,j es el número de términos relevantes en el clúster Ci, y K k=1 L j=1 δtk,j es el número total de términos relevantes. Observamos que cuando solo hay un clúster (K = 1), la fórmula anterior se degrada a p(w|θq ) = λp(w|θq) + (1 − λ)p(w|θ1) que es simplemente una pseudo-retroalimentación de la forma propuesta en [25]. TCFB (Retroalimentación Término-Clúster) TFB y CFB ambos tienen sus inconvenientes. TFB asigna probabilidades no nulas a los términos presentados que están marcados como relevantes, pero ignora por completo (muchos más) otros, que pueden quedar sin marcar debido a la ignorancia de los usuarios, o simplemente no estar incluidos en la lista de presentación, pero deberíamos ser capaces de inferir su relevancia a partir de los marcados. Por ejemplo, en la Figura 1, dado que se revisan hasta 5 términos en el grupo central (las tercera y cuarta columnas), deberíamos tener una alta confianza en la relevancia de otros términos en ese grupo. CFB resuelve el problema de TFB tratando los términos en un grupo de manera colectiva, de modo que los términos no verificados/no presentados reciban pesos cuando los términos presentados en sus grupos son considerados relevantes, pero no distingue qué términos en un grupo son presentados o considerados. Intuitivamente, los términos considerados relevantes deberían recibir pesos mayores porque son indicados explícitamente como relevantes por el usuario. Por lo tanto, intentamos combinar los dos métodos, con la esperanza de obtener lo mejor de ambos. Lo hacemos interpolando el modelo TFB con el modelo CFB, y lo llamamos TCFB: p(w|θq ) = αp(w|θqT F B ) + (1 − α)p(w|θqCF B ) 6. EXPERIMENTOS En esta sección, describimos los resultados de nuestro experimento. Primero describimos nuestra configuración experimental y presentamos una visión general del rendimiento de varios métodos. Luego discutimos los efectos de variar la configuración de los parámetros en los algoritmos, así como el número de términos de presentación. A continuación analizamos el comportamiento de retroalimentación de los usuarios y su relación con el rendimiento de recuperación. Finalmente comparamos la retroalimentación de términos con la retroalimentación de relevancia y mostramos que tiene su ventaja particular. 6.1 Configuración del Experimento y Resultados Básicos Aprovechamos la oportunidad de la pista HARD de TREC 2005[2] para la evaluación de nuestros algoritmos. Las pistas utilizaron la colección AQUAINT, un corpus de 3GB de texto de noticias en inglés. Los temas incluyeron 50 que se sabía que eran difíciles, es decir, con bajo rendimiento de recuperación. Es para estos temas difíciles que la retroalimentación del usuario es más útil, ya que puede proporcionar información para desambiguar las consultas; con temas fáciles, es posible que el usuario no esté dispuesto a esforzarse por dar retroalimentación si los resultados de recuperación automática son lo suficientemente buenos. Los participantes de la pista pudieron enviar formularios de aclaración (CF) diseñados a medida para solicitar retroalimentación de los evaluadores humanos proporcionados por la Tabla 2: Rendimiento de recuperación para diferentes métodos y tipos de CF. La última fila es el porcentaje de mejora del MAP sobre la línea base. Los ajustes de parámetros μ = 4, λ = 0.1, α = 0.3 son casi óptimos. Variación de MAP con el número de términos presentados. # términos TFB1C TFB3C TFB6C CFB3C CFB6C TCFB3C TCFB6C 6 0.245 0.240 0.227 0.279 0.279 0.281 0.274 12 0.261 0.261 0.242 0.299 0.286 0.297 0.281 18 0.275 0.274 0.256 0.301 0.282 0.300 0.286 24 0.276 0.281 0.265 0.303 0.292 0.305 0.292 30 0.280 0.285 0.270 0.304 0.296 0.307 0.296 36 0.282 0.288 0.272 0.307 0.297 0.309 0.297 42 0.283 0.288 0.275 0.306 0.298 0.309 0.300 48 0.288 0.288 0.278 0.305 0.301 0.309 0.303 NIST. Diseñamos tres conjuntos de formularios de aclaración para la retroalimentación de términos, diferenciados en la elección de K, el número de grupos, y L, el número de términos presentados de cada grupo. Son: 1 × 48, un gran grupo con 48 términos, 3 × 16, 3 grupos con 16 términos cada uno, y 6 × 8, 6 grupos con 8 términos cada uno. El número total de términos presentados (M) está fijo en 48, por lo que al comparar el rendimiento de diferentes tipos de formas de clarificación podemos conocer los efectos de diferentes grados de agrupamiento. Para cada tema, un evaluador completaría los formularios en el orden de 6 × 8, 1 × 48 y 3 × 16, dedicando hasta tres minutos en cada formulario. El formulario de aclaración de muestra mostrado en la Figura 1 es de tipo 3 × 16. Es una interfaz simple y compacta en la que el usuario puede consultar términos relevantes. El formulario es autoexplicativo; no es necesario brindar capacitación adicional al usuario sobre cómo utilizarlo. Nuestras consultas iniciales se construyen solo utilizando las descripciones de los títulos de los temas, que tienen en promedio 2.7 palabras de longitud. Como base utilizamos el método de recuperación de divergencia KL implementado en la herramienta Lemur con 5 documentos de retroalimentación pseudo. Aplicamos el suavizado de Dirichlet con una prior de 2000, y truncamos los modelos de lenguaje de consulta a 50 términos (estas configuraciones se utilizan en todos los experimentos). Para todos los demás parámetros, utilizamos la configuración predeterminada de Lemurs. El punto de referencia resulta tener un rendimiento por encima del promedio entre los participantes de la pista. Después de una ejecución inicial utilizando este método de recuperación de referencia, tomamos los 60 documentos principales para cada tema y aplicamos el algoritmo de descubrimiento de temas para producir los grupos (1, 3 o 6 de ellos), en base a los cuales generamos formularios de aclaración. Después de recibir la retroalimentación del usuario, ejecutamos los algoritmos de retroalimentación de términos (TFB, CFB o TCFB) para estimar modelos de consulta actualizados, los cuales luego se utilizan para una segunda iteración de recuperación. Evaluamos el rendimiento de los diferentes métodos de recuperación en sus clasificaciones de los 1000 documentos principales. Las métricas de evaluación que adoptamos incluyen la precisión media promedio (no interpolada) (MAP), precisión en los primeros 30 (Pr@30) y total relevante recuperado (RR). La Tabla 2 muestra el rendimiento de varios métodos y configuraciones de K × L. Los sufijos (1C, 3C, 6C) después de TFB, CFB, TCFB representan el número de grupos (K). Por ejemplo, TCFB3C significa el método TCFB en los formularios de aclaración de 3 × 16. De la Tabla 2 podemos hacer las siguientes observaciones: 1 http://www.lemurproject.com 1. Todos los métodos tienen un rendimiento considerablemente mejor que la línea base de pseudoretroalimentación, con TCFB3C logrando una mejora máxima del 41.1% en el MAP, lo que indica una contribución significativa de la retroalimentación de términos para la clarificación de la necesidad de información de los usuarios. En otras palabras, el feedback a término es realmente útil para mejorar la precisión de recuperación. 2. Para TFB, el rendimiento es casi igual en los formularios de aclaración de 1 × 48 y 3 × 16 en términos de MAP (aunque este último es ligeramente mejor en Pr@30 y RR), y un poco peor en los de 6 × 8. 3. Tanto CFB3C como CFB6C tienen un mejor rendimiento que sus contrapartes TFB en las tres métricas, lo que sugiere que el feedback sobre una estructura de clúster secundaria es realmente beneficioso. CFB1C es en realidad peor porque no puede ajustar el peso de su (único) grupo a partir de la retroalimentación de términos y es simplemente pseudoretroalimentación. 4. Aunque TCFB es solo una simple mezcla de TFB y CFB por interpolación, logra superar a ambos. Esto respalda nuestra especulación de que TCFB supera las desventajas de TFB (prestando atención solo a los términos marcados) y CFB (sin distinguir los términos marcados y no marcados en un grupo). Excepto por TCFB6C v.s. CFB6C, la ventaja de rendimiento de TCFB sobre TFB/CFB es significativa con p < 0.05 utilizando la prueba de rango con signo de Wilcoxon. Esto no es cierto en el caso de TFB v.s. CFB, cada uno de los cuales es mejor que el otro en casi la mitad de los temas. 6.2 Reducción de Términos de Presentación En algunas situaciones podemos tener que reducir el número de términos de presentación debido a limitaciones en el espacio de visualización o esfuerzos de retroalimentación del usuario. Es interesante saber si el rendimiento de nuestros algoritmos se deteriora cuando al usuario se le presentan menos términos. Dado que los términos de presentación dentro de cada grupo se generan en orden decreciente de sus frecuencias, la lista de presentación forma un subconjunto del original si su tamaño se reduce2. Por lo tanto, podemos simular fácilmente lo que sucede cuando el número de términos de presentación disminuye 2. Hay complejidades que surgen de los términos que aparecen en la parte superior L de múltiples grupos, pero estas son excepciones de M a M: mantendremos todos los juicios de los términos superiores L = M / K en cada grupo y descartaremos los de los demás. La Tabla 3 muestra el rendimiento de varios algoritmos a medida que el número de términos de presentación varía de 6 a 48. Observamos que el rendimiento de TFB es más susceptible a la reducción de términos de presentación que el de CFB o TCFB. Por ejemplo, en 12 términos el MAP de TFB3C es el 90.6% del que se obtiene en 48 términos, mientras que los números para CFB3C y TCFB3C son del 98.0% y 96.1% respectivamente. Conjeturamos que la razón es que mientras el rendimiento de TFB depende en gran medida de cuántos términos buenos se elijan para la expansión de la consulta, CFB solo necesita una estimación aproximada de los pesos de los clústeres para funcionar. Además, las formas de aclaración de 3 × 16 parecen ser más robustas que las de 6 × 8: con 12 términos, el MAP de TFB6C es el 87.1% del obtenido con 48 términos, por debajo del 90.6% de TFB3C. De manera similar, para CFB es del 95.0% en comparación con el 98.0%. Esto es natural, ya que para un gran número de 6 clusters, es más fácil llegar a la situación en la que cada cluster recibe muy pocos términos de presentación para que la diversificación de temas sea útil. En general, nos sorprende ver que los algoritmos aún pueden funcionar razonablemente bien cuando el número de términos de presentación es pequeño. Por ejemplo, con solo 12 términos, CFB3C (el formulario de aclaración es de tamaño 3 × 4) aún puede mejorar un 36.5% sobre el valor base, disminuyendo ligeramente desde el 39.3% con 48 términos. En el Análisis de Retroalimentación del Usuario, estudiamos varios aspectos del comportamiento de retroalimentación de los usuarios en términos, y si están conectados al rendimiento de recuperación. Figura 2: Distribuciones de tiempo de completación del formulario de aclaración 0−30 30−60 60−90 90−120 120−150 150−180 0 5 10 15 20 25 30 35 tiempo de completación (segundos) #temas 1×48 3×16 6×8 La Figura 2 muestra la distribución del tiempo necesario para completar un formulario de aclaración. Observamos que el usuario suele ser capaz de completar el feedback del término en un tiempo razonablemente corto: para más de la mitad de los temas, el formulario de aclaración se completa en solo 1 minuto, y solo una pequeña fracción de temas (menos del 10% para 1 × 48 y 3 × 16) tarda más de 2 minutos. Esto sugiere que la retroalimentación a corto plazo es adecuada para la recuperación interactiva ad-hoc, donde un usuario generalmente no desea dedicar demasiado tiempo a proporcionar retroalimentación. Observamos que un usuario a menudo comete errores al juzgar la relevancia de un término. A veces, un término relevante puede ser omitido porque su conexión con el tema de la consulta no es obvia para el usuario. En otras ocasiones, puede incluirse un término dudoso que resulta ser irrelevante. Toma como ejemplo el tema en la Figura 1. Hubo un desastre de incendio en Mont 3. El tiempo máximo es de 180 segundos, ya que el evaluador del NIST estaría obligado a enviar el formulario en ese momento. Tabla 4: Estadísticas de selección de términos (promedio del tema) CF Tipo 1 × 48 3 × 16 6 × 8 # términos revisados 14.8 13.3 11.2 # términos relevantes 15.0 12.6 11.2 # términos relevantes revisados 7.9 6.9 5.9 precisión 0.534 0.519 0.527 recuperación 0.526 0.548 0.527 El usuario no seleccionó palabras clave como mont, blanc, francés e italiano debido a su desconocimiento del evento del Túnel del Mont Blanc entre Francia e Italia en 1999. De hecho, sin un contexto adecuado sería difícil hacer un juicio perfecto. ¿Cuál es entonces, el nivel en el que el usuario es bueno dando retroalimentación? ¿Tiene un impacto serio en el rendimiento de recuperación? Para responder a estas preguntas, necesitamos una medida de la verdadera relevancia de los términos individuales. Adoptamos la métrica de Divergencia KL Simplificada utilizada en [24] para decidir los términos de expansión de la consulta como nuestra medida de relevancia de términos: σKLD(w) = p(w|R) log p(w|R) p(w|¬R) donde p(w|R) es la <br>probabilidad</br> de que un documento relevante contenga el término w, y p(w|¬R) es la <br>probabilidad</br> de que un documento irrelevante contenga w, ambas pueden ser fácilmente calculadas mediante una estimación de máxima verosimilitud dada la relevancia a nivel de documento. Si σKLD(w) > 0, w es más probable que aparezca en documentos relevantes que en irrelevantes. Consideramos un término relevante si su valor de Divergencia KL Simplificada es mayor que un umbral determinado σ0. Podemos entonces definir la precisión y la exhaustividad del juicio de términos del usuario de la siguiente manera: la precisión es la fracción de términos revisados por el usuario que son relevantes; la exhaustividad es la fracción de términos relevantes presentados que son revisados por el usuario. La Tabla 4 muestra el número de términos revisados, términos relevantes y términos relevantes revisados cuando σ0 se establece en 1.0, así como la precisión/recuperación del juicio de términos del usuario. Ten en cuenta que cuando los formularios de aclaración contienen más grupos, se revisan menos términos: 14.8 para 1 × 48, 13.3 para 3 × 16 y 11.2 para 6×8. El patrón similar se mantiene para los términos relevantes y los términos relevantes verificados. Parece haber un compromiso entre aumentar la diversidad de temas mediante el agrupamiento y perder términos adicionales relevantes: cuando hay más grupos, cada uno recibe menos términos para presentar, lo que puede perjudicar a un grupo relevante importante que contiene muchos términos relevantes. Por lo tanto, no siempre es útil tener más grupos, por ejemplo, TFB6C es en realidad peor que TFB1C. El hallazgo principal que podemos obtener de la Tabla 4 es que el usuario no es particularmente bueno identificando términos relevantes, lo cual coincide con el descubrimiento en [18]. En el caso de 3 formularios de aclaración de 16, el número promedio de términos marcados como relevantes por el usuario es de 13.3 por tema, y el número promedio de términos relevantes cuyo valor de σKLD excede 1.0 es de 12.6. El usuario es capaz de reconocer solo 6.9 de estos términos en promedio. De hecho, la precisión y la exhaustividad de los términos de retroalimentación de los usuarios (como se definió anteriormente) están lejos de ser perfectas. Por otro lado, si el usuario hubiera verificado correctamente todos esos términos relevantes, el rendimiento de nuestros algoritmos habría aumentado considerablemente, como se muestra en la Tabla 5. Vemos que TFB experimenta una gran mejora cuando hay un oráculo que verifica todos los términos relevantes, mientras que CFB se encuentra con un cuello de botella alrededor de un MAP de 0.325, ya que todo lo que hace es ajustar los pesos de los clústeres, y cuando los pesos aprendidos están cerca de ser precisos, no puede beneficiarse más del feedback de términos. También hay que tener en cuenta que TCFB no logra superar a TFB, probablemente porque TFB es lo suficientemente preciso. Tabla 5: Cambio de MAP al usar todos (y solo) los términos relevantes (σKLD > 1.0) para retroalimentación. retroalimentación de término original retroalimentación de término relevante TF1 0.288 0.354 TF3 0.288 0.354 TF6 0.278 0.346 CF3 0.305 0.325 CF6 0.301 0.326 TCF3 0.309 0.345 TCF6 0.304 0.341 6.4 Comparación con Retroalimentación de Relevancia Ahora comparamos la retroalimentación de términos con la retroalimentación de relevancia a nivel de documento, en la que al usuario se le presentan los N mejores documentos de una recuperación inicial y se le pide que juzgue su relevancia. El proceso de retroalimentación se simula utilizando la evaluación de relevancia de documentos de NIST. Utilizamos el método de retroalimentación basado en el modelo de mezcla propuesto en [25], con el ruido de mezcla establecido en 0.95 y el coeficiente de retroalimentación establecido en 0.9. La evaluación comparativa del feedback de relevancia frente a otros métodos se complica por el hecho de que algunos documentos ya han sido vistos durante el feedback, por lo que no tiene sentido incluirlos en los resultados de recuperación de la segunda ejecución. Sin embargo, esto no se aplica al término retroalimentación. Por lo tanto, para que sea justo en cuanto a la ganancia de información de los usuarios, si los documentos de retroalimentación son relevantes, deben mantenerse en la parte superior del ranking; si son irrelevantes, deben ser excluidos. Por lo tanto, utilizamos retroalimentación de relevancia para producir un ranking de los 1000 documentos recuperados, pero excluyendo cada documento de retroalimentación, y luego añadimos los documentos relevantes de retroalimentación al principio. La tabla 6 muestra el rendimiento de la retroalimentación de relevancia para diferentes valores de N y lo compara con TCFB3C. Tabla 6: Rendimiento de la retroalimentación de relevancia para diferentes números de documentos de retroalimentación (N). Vemos que el rendimiento de TCFB3C es comparable al de la retroalimentación de relevancia utilizando 5 documentos. Aunque es menos eficiente que cuando hay 10 documentos de retroalimentación en términos de MAP y Pr@30, sí recupera más documentos (4947) al descender por la lista clasificada. Intentamos comparar la calidad de los términos insertados automáticamente en la retroalimentación de relevancia con la de los términos seleccionados manualmente en la retroalimentación de términos. Esto se logra truncando el modelo de consulta modificado por retroalimentación de relevancia a un tamaño igual al número de términos verificados para el mismo tema. Podemos comparar los términos en el modelo truncado con los términos verificados. La Figura 3 muestra la distribución de los puntajes σKLD de los términos. Observamos que el término \"feedback\" tiende a producir términos de expansión de mayor calidad (aquellos con σKLD > 1) en comparación con el \"relevance feedback\" (con 10 documentos de feedback). Esto no contradice el hecho de que este último produce un rendimiento de recuperación más alto. De hecho, cuando usamos el modelo de consulta truncada en lugar del modelo íntegro refinado a partir de la retroalimentación de relevancia, el MAP es solo 0.304. La verdad Figura 3: Comparación de la calidad del término de expansión entre retroalimentación de relevancia (con 10 documentos de retroalimentación) y retroalimentación de términos (con 3 × 16 CFs) −1−0 0−1 1−2 2−3 3−4 4−5 5−6 0 50 100 150 200 250 300 350 σKLD #términos retroalimentación de relevancia retroalimentación de términos es que, aunque hay muchos términos no deseados en el modelo de consulta expandido a partir de los documentos de retroalimentación, también hay más términos relevantes de los que el usuario puede seleccionar de la lista de términos de presentación generados con documentos de pseudo-retroalimentación, y los efectos positivos a menudo superan a los negativos. Estamos interesados en saber en qué circunstancias la retroalimentación basada en términos tiene ventajas sobre la retroalimentación basada en relevancia. Una de esas situaciones es cuando ninguno de los documentos de retroalimentación principales es relevante, lo que hace que la retroalimentación de relevancia sea inútil. Esto no es infrecuente, como uno podría haber pensado: de los 50 temas, hay 13 casos así cuando N = 5, 10 cuando N = 10, y aún 3 cuando N = 20. Cuando esto sucede, solo se puede retroceder al método original de recuperación; se pierde el poder de la retroalimentación de relevancia. Sorprendentemente, en 11 de 13 casos en los que parece imposible el feedback de relevancia, el usuario puede marcar al menos 2 términos relevantes de los 3 × 16 del formulario de aclaración (consideramos que el término t es relevante si σKLD(t) > 1.0). Además, en 10 de ellos TCFB3C supera la línea base de pseudo-retroalimentación, aumentando el MAP de 0.076 a 0.146 en promedio (estos son temas particularmente difíciles). Creemos que hay dos posibles explicaciones para este fenómeno de que la retroalimentación de términos esté activa incluso cuando la retroalimentación de relevancia no funciona: Primero, incluso si ninguno de los primeros N (supongamos que es un número pequeño) documentos son relevantes, aún podemos encontrar documentos relevantes en los primeros 60, que son más inclusivos pero generalmente inalcanzables cuando las personas están realizando retroalimentación de relevancia en la búsqueda interactiva ad-hoc, de donde podemos extraer términos de retroalimentación. Esto es cierto para el tema 367 de piratería, donde los 10 documentos principales de retroalimentación son todos sobre piratería de software, sin embargo, hay documentos entre el 10 y el 60 que tratan sobre piratería en los mares (que es la necesidad de información real), contribuyendo términos como pirata, barco para la selección en el formulario de aclaración. Segundo, para algunos temas, un documento debe cumplir con ciertas condiciones especiales para ser relevante. Los N documentos principales pueden estar relacionados con el tema, pero no obstante ser irrelevantes. En este caso, aún podemos extraer términos útiles de estos documentos, incluso si no califican como relevantes. Por ejemplo, en el tema 639 de compras en línea de consumidores, un documento necesita mencionar qué contribuye al crecimiento de las compras para realmente coincidir con la necesidad de información especificada, por lo tanto, ninguno de los 10 documentos de retroalimentación principales se considera relevante. Sin embargo, los términos de retroalimentación como venta al por menor, comercio son buenos para la expansión de consultas. 7. CONCLUSIONES En este artículo estudiamos el uso de la retroalimentación de términos para la recuperación interactiva de información en el enfoque de modelado del lenguaje. Propusimos un método basado en clusters para seleccionar términos de presentación, así como algoritmos para estimar modelos de consulta refinados a partir de la retroalimentación de términos de usuario. Observamos una mejora significativa en la precisión de recuperación proporcionada por la retroalimentación de términos, a pesar de que un usuario a menudo comete errores en la evaluación de relevancia que perjudican su rendimiento. Encontramos que el algoritmo de mejor rendimiento es TCFB, el cual se beneficia de la combinación de la evidencia de términos observados directamente con TFB y la relevancia de clúster aprendida indirectamente con CFB. Cuando redujimos el número de términos de presentación, el feedback del término aún puede mantener gran parte de su mejora de rendimiento sobre el valor base. Finalmente, comparamos la retroalimentación de términos con la retroalimentación de relevancia a nivel de documento, y encontramos que el rendimiento de TCFB3C está a la par con este último con 5 documentos de retroalimentación. Consideramos el término \"feedback\" como una alternativa viable al feedback de relevancia tradicional, especialmente cuando no hay documentos relevantes en la parte superior. Proponemos ampliar nuestro trabajo de varias maneras. Primero, queremos estudiar si el uso de varios contextos puede ayudar al usuario a identificar mejor la relevancia de los términos, sin sacrificar la simplicidad y la concisión de la retroalimentación de los términos. Segundo, actualmente todos los términos se presentan al usuario en un solo lote. En su lugar, podríamos considerar la retroalimentación iterativa de términos, presentando primero un pequeño número de términos y mostrando más términos después de recibir la retroalimentación del usuario o detenernos cuando la consulta refinada sea lo suficientemente buena. Los términos presentados deben ser seleccionados dinámicamente para maximizar los beneficios de aprendizaje en cualquier momento. Tercero, tenemos planes de incorporar retroalimentación de términos en nuestra barra de herramientas UCAIR[20], un complemento de Internet Explorer, para que funcione en búsquedas web. También estamos interesados en estudiar cómo combinar la retroalimentación de términos con la retroalimentación de relevancia o la retroalimentación implícita. Por ejemplo, podríamos permitir al usuario modificar dinámicamente los términos en un modelo de lenguaje aprendido a partir de documentos de retroalimentación. 8. AGRADECIMIENTO Este trabajo cuenta con el apoyo parcial de las becas de la Fundación Nacional de Ciencias IIS-0347933 e IIS-0428472. 9. REFERENCIAS [1] J. Allan. Retroalimentación de relevancia con demasiados datos. En Actas de la 18ª conferencia internacional anual de ACM SIGIR sobre investigación y desarrollo en recuperación de información, páginas 337-343, 1995. [2] J. Allan. Resumen de la pista HARD en TREC 2005 - Recuperación de alta precisión de documentos. En la Decimocuarta Conferencia de Recuperación de Información de 2005. [3] P. Anick. Utilizando retroalimentación terminológica para refinar la búsqueda web: un estudio basado en registros. En Actas de la 26ª conferencia internacional anual de ACM SIGIR sobre investigación y desarrollo en recuperación de información, páginas 88-95, 2003. [4] P. G. Anick y S. Tipirneni. El asistente de búsqueda de paráfrasis: retroalimentación terminológica para la búsqueda iterativa de información. En Actas de la 22ª conferencia internacional anual de ACM SIGIR sobre investigación y desarrollo en recuperación de información, páginas 153-159, 1999. [5] C. Buckley, G. Salton, J. Allan y A. Singhal. Expansión automática de consultas utilizando SMART. En Actas de la Tercera Conferencia de Recuperación de Texto, 1994. [6] D. Harman. Hacia la expansión interactiva de consultas. En Actas de la 11ª conferencia internacional anual de ACM SIGIR sobre investigación y desarrollo en recuperación de información, páginas 321-331, 1988. [7] N. A. Jaleel, A. Corrada-Emmanuel, Q. Li, X. Liu, C. Wade y J. Allan. UMass en TREC 2003: HARD y QA. En TREC, páginas 715-725, 2003. [8] H. Joho, C. Coverson, M. Sanderson y M. Beaulieu. Presentación jerárquica de términos de expansión. En Actas del simposio de informática aplicada de ACM de 2002, páginas 645-649, 2002. [9] K. S. Jones, S. Walker y S. E. Robertson. Un modelo probabilístico de recuperación de información: desarrollo y estado actual. Informe técnico 446, Laboratorio de Computación, Universidad de Cambridge, 1998. [10] D. Kelly, V. D. Dollu y X. Fu. El usuario locuaz: una fuente de términos independiente del documento para la expansión de consultas. En Actas de la 28ª conferencia internacional anual de ACM SIGIR sobre investigación y desarrollo en recuperación de información, páginas 457-464, 2005. [11] D. Kelly y X. Fu. Obtención de retroalimentación de relevancia de términos: una investigación sobre la fuente y el contexto de los términos. En Actas de la 29ª conferencia internacional anual de ACM SIGIR sobre investigación y desarrollo en recuperación de información, 2006. [12] J. Koenemann y N. Belkin. Un caso para la interacción: Un estudio del comportamiento y la efectividad de la recuperación de información interactiva. En Actas de la conferencia SIGCHI sobre factores humanos en sistemas informáticos, páginas 205-212, 1996. [13] V. Lavrenko y W. B. Croft. Modelos de lenguaje basados en relevancia. En Investigación y Desarrollo en Recuperación de Información, páginas 120-127, 2001. [14] Y. Nemeth, B. Shapira y M. Taeib-Maimon. Evaluación del valor real y percibido de la expansión automática e interactiva de consultas. En Actas de la 27ª conferencia internacional anual de ACM SIGIR sobre investigación y desarrollo en recuperación de información, páginas 526-527, 2004. [15] J. Ponte. Un enfoque de modelado del lenguaje para la recuperación de información. Tesis doctoral, Universidad de Massachusetts en Amherst, 1998. [16] S. E. Robertson, S. Walker, S. Jones, M. Beaulieu y M. Gatford. Okapi en TREC-3. En Actas de la Tercera Conferencia de Recuperación de Texto, 1994. [17] J. Rocchio. Retroalimentación de relevancia en la recuperación de información. En el sistema de recuperación SMART, páginas 313-323. 1971. [18] I. Ruthven. Reexaminando la efectividad potencial de la expansión interactiva de consultas. En Actas de la 26ª conferencia internacional anual de ACM SIGIR sobre investigación y desarrollo en recuperación de información, páginas 213-220, 2003. [19] G. Salton y C. Buckley. Mejorando el rendimiento de recuperación mediante retroalimentación de relevancia. Revista de la Sociedad Americana de Ciencia de la Información, 41:288-297, 1990. [20] X. Shen, B. Tan y C. Zhai. Modelado implícito de usuario para búsqueda personalizada. En Actas de la 14ª conferencia internacional de ACM sobre información y gestión del conocimiento, páginas 824-831, 2005. [21] X. Shen y C. Zhai. Retroalimentación activa en la recuperación de información ad-hoc. En Actas de la 28ª conferencia internacional anual de ACM SIGIR sobre investigación y desarrollo en recuperación de información, páginas 59-66, 2005. [22] A. Spink. Retroalimentación de relevancia de términos y expansión de consultas: relación con el diseño. En Actas de la 17ª conferencia internacional anual de ACM SIGIR sobre investigación y desarrollo en recuperación de información, páginas 81-90, 1994. [23] J. Xu y W. B. Croft. Expansión de consulta utilizando análisis local y global de documentos. En Actas de la 19ª conferencia internacional anual de ACM SIGIR sobre investigación y desarrollo en recuperación de información, páginas 4-11, 1996. [24] H. Zaragoza, N. Craswell, M. Taylor, S. Saria y S. Robertson. Microsoft Cambridge en TREC-13: pistas Web y HARD. En Actas de la 13ª Conferencia de Recuperación de Información de Texto, 2004. [25] C. Zhai y J. Lafferty. Retroalimentación basada en modelos en el enfoque de modelado del lenguaje para la recuperación de información. En Actas de la décima conferencia internacional sobre gestión de la información y el conocimiento, páginas 403-410, 2001. [26] C. Zhai, A. Velivelli y B. Yu. Un modelo de mezcla de colecciones cruzadas para la minería de textos comparativa. En Actas de la décima conferencia internacional de ACM SIGKDD sobre descubrimiento de conocimiento y minería de datos, páginas 743-748, 2004. ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "kl-divergence": {
            "translated_key": "divergencia KL",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Term Feedback for Information Retrieval with Language Models Bin Tan† , Atulya Velivelli‡ , Hui Fang† , ChengXiang Zhai† Dept.",
                "of Computer Science† , Dept. of Electrical and Computer Engineering‡ University of Illinois at Urbana-Champaign bintan@cs.uiuc.edu, velivell@ifp.uiuc.edu, hfang@cs.uiuc.edu, czhai@cs.uiuc.edu ABSTRACT In this paper we study term-based feedback for information retrieval in the language modeling approach.",
                "With term feedback a user directly judges the relevance of individual terms without interaction with feedback documents, taking full control of the query expansion process.",
                "We propose a cluster-based method for selecting terms to present to the user for judgment, as well as effective algorithms for constructing refined query language models from user term feedback.",
                "Our algorithms are shown to bring significant improvement in retrieval accuracy over a non-feedback baseline, and achieve comparable performance to relevance feedback.",
                "They are helpful even when there are no relevant documents in the top.",
                "Categories and Subject Descriptors H.3.3 [Information Search and Retrieval]: Retrieval models General Terms Algorithms 1.",
                "INTRODUCTION In the language modeling approach to information retrieval, feedback is often modeled as estimating an improved query model or relevance model based on a set of feedback documents [25, 13].",
                "This is in line with the traditional way of doing relevance feedback - presenting a user with documents/passages for relevance judgment and then extracting terms from the judged documents or passages to expand the initial query.",
                "It is an indirect way of seeking users assistance for query model construction, in the sense that the refined query model (based on terms) is learned through feedback documents/passages, which are high-level structures of terms.",
                "It has the disadvantage that irrelevant terms, which occur along with relevant ones in the judged content, may be erroneously used for query expansion, causing undesired effects.",
                "For example, for the TREC query Hubble telescope achievements, when a relevant document talks more about the telescopes repair than its discoveries, irrelevant terms such as spacewalk can be added into the modified query.",
                "We can consider a more direct way to involve a user in query model improvement, without an intermediary step of document feedback that can introduce noise.",
                "The idea is to present a (reasonable) number of individual terms to the user and ask him/her to judge the relevance of each term or directly specify their probabilities in the query model.",
                "This strategy has been discussed in [15], but to our knowledge, it has not been seriously studied in existing language modeling literature.",
                "Compared to traditional relevance feedback, this term-based approach to interactive query model refinement has several advantages.",
                "First, the user has better control of the final query model through direct manipulation of terms: he/she can dictate which terms are relevant, irrelevant, and possibly, to what degree.",
                "This avoids the risk of bringing unwanted terms into the query model, although sometimes the user introduces low-quality terms.",
                "Second, because a term takes less time to judge than a documents full text or summary, and as few as around 20 presented terms can bring significant improvement in retrieval performance (as we will show later), term feedback makes it faster to gather user feedback.",
                "This is especially helpful for interactive adhoc search.",
                "Third, sometimes there are no relevant documents in the top N of the initially retrieved results if the topic is hard.",
                "This is often true when N is constrained to be small, which arises from the fact that the user is unwilling to judge too many documents.",
                "In this case, relevance feedback is useless, as no relevant document can be leveraged on, but term feedback is still often helpful, by allowing relevant terms to be picked from irrelevant documents.",
                "During our participation in the TREC 2005 HARD Track and continued study afterward, we explored how to exploit term feedback from the user to construct improved query models for information retrieval in the language modeling approach.",
                "We identified two key subtasks of term-based feedback, i.e., pre-feedback presentation term selection and post-feedback query model construction, with effective algorithms developed for both.",
                "We imposed a secondary cluster structure on terms and found that a cluster view sheds additional insight into the users information need, and provides a good way of utilizing term feedback.",
                "Through experiments we found that term feedback improves significantly over the nonfeedback baseline, even though the user often makes mistakes in relevance judgment.",
                "Among our algorithms, the one with best retrieval performance is TCFB, the combination of TFB, the direct term feedback algorithm, and CFB, the cluster-based feedback algorithm.",
                "We also varied the number of feedback terms and observed reasonable improvement even at low numbers.",
                "Finally, by comparing term feedback with document-level feedback, we found it to be a viable alternative to the latter with competitive retrieval performance.",
                "The rest of the paper is organized as follows.",
                "Section 2 discusses some related work.",
                "Section 4 outlines our general approach to term feedback.",
                "We present our method for presentation term selection in Section 3 and algorithms for query model construction in Section 5.",
                "The experiment results are given in Section 6.",
                "Section 7 concludes this paper. 2.",
                "RELATED WORK Relevance feedback[17, 19] has long been recognized as an effective method for improving retrieval performance.",
                "Normally, the top N documents retrieved using the original query are presented to the user for judgment, after which terms are extracted from the judged relevant documents, weighted by their potential of attracting more relevant documents, and added into the query model.",
                "The expanded query usually represents the users information need better than the original one, which is often just a short keyword query.",
                "A second iteration of retrieval using this modified query usually produces significant increase in retrieval accuracy.",
                "In cases where true relevance judgment is unavailable and all top N documents are assumed to be relevant, it is called blind or pseudo feedback[5, 16] and usually still brings performance improvement.",
                "Because document is a large text unit, when it is used for relevance feedback many irrelevant terms can be introduced into the feedback process.",
                "To overcome this, passage feedback is proposed and shown to improve feedback performance[1, 23].",
                "A more direct solution is to ask the user for their relevance judgment of feedback terms.",
                "For example, in some relevance feedback systems such as [12], there is an interaction step that allows the user to add or remove expansion terms after they are automatically extracted from relevant documents.",
                "This is categorized as interactive query expansion, where the original query is augmented with user-provided terms, which can come from direct user input (free-form text or keywords)[22, 7, 10] or user selection of system-suggested terms (using thesauri[6, 22] or extracted from feedback documents[6, 22, 12, 4, 7]).",
                "In many cases term relevance feedback has been found to effectively improve retrieval performance[6, 22, 12, 4, 10].",
                "For example, the study in [12] shows that the user prefers to have explicit knowledge and direct control of which terms are used for query expansion, and the penetrable interface that provides this freedom is shown to perform better than other interfaces.",
                "However, in some other cases there is no significant benefit[3, 14], even if the user likes interacting with expansion terms.",
                "In a simulated study carried out in [18], the author compares the retrieval performance of interactive query expansion and automatic query expansion with a simulated study, and suggests that the potential benefits of the former can be hard to achieve.",
                "The user is found to be not good at identifying useful terms for query expansion, when a simple term presentation interface is unable to provide sufficient semantic context of the feedback terms.",
                "Our work differs from the previous ones in two important aspects.",
                "First, when we choose terms to present to the user for relevance judgment, we not only consider single-term value (e.g., the relative frequency of a term in the top documents, which can be measured by metrics such as Robertson Selection Value and Simplified Kullback-Leibler Distance as listed in [24]), but also examine the cluster structure of the terms, so as to produce a balanced coverage of the different topic aspects.",
                "Second, with the language modelling framework, we allow an elaborate construction of the updated query model, by setting different probabilities for different terms based on whether it is a query term, its significance in the top documents, and its cluster membership.",
                "Although techniques for adjusting query term weights exist for vector space models[17] and probablistic relevance models[9], most of the aforementioned works do not use them, choosing to just append feedback terms to the original query (thus using equal weights for them), which can lead to poorer retrieval performance.",
                "The combination of the two aspects allows our method to perform much better than the baseline.",
                "The usual way for feedback term presentation is just to display the terms in a list.",
                "There have been some works on alternative user interfaces. [8] arranges terms in a hierarchy, and [11] compares three different interfaces, including terms + checkboxes, terms + context (sentences) + checkboxes, sentences + input text box.",
                "In both studies, however, there is no significant performance difference.",
                "In our work we adopt the simplest approach of terms + checkboxes.",
                "We focus on term presentation and query model construction from feedback terms, and believe using contexts to improve feedback term quality should be orthogonal to our method. 3.",
                "GENERAL APPROACH We follow the language modeling approach, and base our method on the <br>kl-divergence</br> retrieval model proposed in [25].",
                "With this model, the retrieval task involves estimating a query language model θq from a given query, a document language model θd from each document, and calculating their <br>kl-divergence</br> D(θq||θd), which is then used to score the documents. [25] treats relevance feedback as a query model re-estimation problem, i.e., computing an updated query model θq given the original query text and the extra evidence carried by the judged relevant documents.",
                "We adopt this view, and cast our task as updating the query model from user term feedback.",
                "There are two key subtasks here: First, how to choose the best terms to present to the user for judgment, in order to gather maximal evidence about the users information need.",
                "Second, how to compute an updated query model based on this term feedback evidence, so that it captures the users information need and translates into good retrieval performance. 4.",
                "PRESENTATION TERM SELECTION Proper selection of terms to be presented to the user for judgment is crucial to the success of term feedback.",
                "If the terms are poorly chosen and there are few relevant ones, the user will have a hard time looking for useful terms to help clarify his/her information need.",
                "If the relevant terms are plentiful, but all concentrate on a single aspect of the query topic, then we will only be able to get feedback on that aspect and missing others, resulting in a breadth loss in retrieved results.",
                "Therefore, it is important to carefully select presentation terms to maximize expected gain from user feedback, i.e., those that can potentially reveal most evidence of the users information need.",
                "This is similar to active feedback[21], which suggests that a retrieval system should actively probe the users information need, and in the case of relevance feedback, the feedback documents should be chosen to maximize learning benefits (e.g. diversely so as to increase coverage).",
                "In our approach, the top N documents from an initial retrieval using the original query form the source of feedback terms: all terms that appear in them are considered candidates to present to the user.",
                "These documents serve as pseudo-feedback, since they provide a much richer context than the original query (usually very short), while the user is not asked to judge their relevance.",
                "Due to the latter reason, it is possible to make N quite large (e.g., in our experiments we set N = 60) to increase its coverage of different aspects in the topic.",
                "The simplest way of selecting feedback terms is to choose the most frequent M terms from the N documents.",
                "This method, however, has two drawbacks.",
                "First, a lot of common noisy terms will be selected due to their high frequencies in the document collection, unless a stop-word list is used for filtering.",
                "Second, the presentation list will tend to be filled by terms from major aspects of the topic; those from a minor aspect are likely to be missed due to their relatively low frequencies.",
                "We solve the above problems by two corresponding measures.",
                "First, we introduce a background model θB that is estimated from collection statistics and explains the common terms, so that they are much less likely to appear in the presentation list.",
                "Second, the terms are selected from multiple clusters in the pseudo-feedback documents, to ensure sufficient representation of different aspects of the topic.",
                "We rely on the mixture multinomial model, which is used for theme discovery in [26].",
                "Specifically, we assume the N documents contain K clusters {Ci| i = 1, 2, · · · K}, each characterized by a multinomial word distribution (also known as unigram language model) θi and corresponding to an aspect of the topic.",
                "The documents are regarded as sampled from a mixture of K + 1 components, including the K clusters and the background model: p(w|d) = λBp(w|θB) + (1 − λB) K i=1 πd,ip(w|θi) where w is a word, λB is the mixture weight for the background model θB, and πd,i is the document-specific mixture weight for the i-th cluster model θi.",
                "We then estimate the cluster models by maximizing the probability of the pseudo-feedback documents being generated from the multinomial mixture model: log p(D|Λ) = d∈D w∈V c(w; d) log p(w|d) where D = {di| i = 1, 2, · · · N} is the set of the N documents, V is the vocabulary, c(w; d) is ws frequency in d and Λ = {θi| i = 1, 2, · · · K} ∪ {πdij | i = 1, 2, · · · N, j = 1, 2, · · · K} is the set of model parameters to estimate.",
                "The cluster models can be efficiently estimated using the Expectation-Maximization (EM) algorithm.",
                "For its details, we refer the reader to [26].",
                "Table 1 shows the cluster models for TREC query Transportation tunnel disasters (K = 3).",
                "Note that only the middle cluster is relevant.",
                "Table 1: Cluster models for topic 363 Transportation tunnel disasters Cluster 1 Cluster 2 Cluster 3 tunnel 0.0768 tunnel 0.0935 tunnel 0.0454 transport 0.0364 fire 0.0295 transport 0.0406 traffic 0.0206 truck 0.0236 toll 0.0166 railwai 0.0186 french 0.0220 amtrak 0.0153 harbor 0.0146 smoke 0.0157 train 0.0129 rail 0.0140 car 0.0154 airport 0.0122 bridg 0.0139 italian 0.0152 turnpik 0.0105 kilomet 0.0136 firefight 0.0144 lui 0.0095 truck 0.0133 blaze 0.0127 jersei 0.0093 construct 0.0131 blanc 0.0121 pass 0.0087 · · · · · · · · · From each of the K estimated clusters, we choose the L = M/K terms with highest probabilities to form a total of M presentation terms.",
                "If a term happens to be in top L in multiple clusters, we assign it to the cluster where it has highest probability and let the other clusters take one more term as compensation.",
                "We also filter out terms in the original query text because they tend to always be relevant when the query is short.",
                "The selected terms are then presented to the user for judgment.",
                "A sample (completed) feedback form is shown in Figure 1.",
                "In this study we only deal with binary judgment: a presented term is by default unchecked, and a user may check it to indicate relevance.",
                "We also do not explicitly exploit negative feedback (i.e., penalizing irrelevant terms), because with binary feedback an unchecked term is not necessarily irrelevant (maybe the user is unsure about its relevance).",
                "We could ask the user for finer judgment (e.g., choosing from highly relevant, somewhat relevant, do not know, somewhat irrelevant and highly irrelevant), but binary feedback is more compact, taking less space to display and less user effort to make judgment. 5.",
                "ESTIMATING QUERY MODELS FROM TERM FEEDBACK In this section, we present several algorithms for exploiting term feedback.",
                "The algorithms take as input the original query q, the clusters {θi} as generated by the theme discovery algorithm, the set of feedback terms T and their relevance judgment R, and outputs an updated query language model θq that makes best use of the feedback evidence to capture the users information need.",
                "First we describe our notations: • θq: The original query model, derived from query terms only: p(w|θq) = c(w; q) |q| where c(w; q) is the count of w in q, and |q| = w∈q c(w; q) is the query length. • θq : The updated query model which we need to estimate from term feedback. • θi (i = 1, 2, . . .",
                "K): The unigram language model of cluster Ci, as estimated using the theme discovery algorithm. • T = {ti,j} (i = 1 . . .",
                "K, j = 1 . . .",
                "L): The set of terms presented to the user for judgment. ti,j is the j-th term chosen from cluster Ci. • R = {δw|w ∈ T}: δw is an indicator variable that is 1 if w is judged relevant or 0 otherwise. 5.1 TFB (Direct Term Feedback) This is a straight-forward form of term feedback that does not involve any secondary structure.",
                "We give a weight of 1 to terms judged relevant by the user, a weight of μ to query terms, zero weight to other terms, and then apply normalization: p(w|θq ) = δw + μ c(w; q) w ∈T δw + μ|q| where w ∈T δw is the total number of terms that are judged relevant.",
                "We call this method TFB (direct Term FeedBack).",
                "If we let μ = 1, this approach is equivalent to appending the relevant terms after the original query, which is what standard query expansion (without term reweighting) does.",
                "If we set μ > 1, we are putting more emphasis on the query terms than the checked ones.",
                "Note that the result model will be more biased toward θq if the original query is long or the user feedback is weak, which makes sense, as we can trust more on the original query in either case.",
                "Figure 1: Filled clarification form for Topic 363 363 transportation tunnel disasters Please select all terms that are relevant to the topic. traffic railway harbor rail bridge kilometer construct swiss cross link kong hong river project meter shanghai fire truck french smoke car italian firefights blaze blanc mont victim franc rescue driver chamonix emerge toll amtrak train airport turnpike lui jersey pass rome z center electron road boston speed bu submit 5.2 CFB (Cluster Feedback) Here we exploit the cluster structure that played an important role when we selected the presentation terms.",
                "The clusters represent different aspects of the query topic, each of which may or may not be relevant.",
                "If we are able to identify the relevant clusters, we can combine them to generate a query model that is good at discovering documents belonging to these clusters (instead of the irrelevant ones).",
                "We could ask the user to directly judge the relevance of a cluster after viewing representative terms in that cluster, but this would sometimes be a difficult task for the user, who has to guess the semantics of a cluster via its set of terms, which may not be well connected to one another due to a lack of context.",
                "Therefore, we propose to learn cluster feedback indirectly, inferring the relevance of a cluster through the relevance of its feedback terms.",
                "Because each cluster has an equal number of terms presented to the user, the simplest measure of a clusters relevance is the number of terms that are judged relevant in it.",
                "Intuitively, the more terms are marked relevant in a cluster, the closer the cluster is to the query topic, and the more the cluster should participate in query modification.",
                "If we combine the cluster models using weights determined this way and then interpolate with the original query model, we get the following formula for query updating, which we call CFB (Cluster FeedBack): p(w|θq ) = λp(w|θq) + (1 − λ) K i=1 L j=1 δti,j K k=1 L j=1 δtk,j p(w|θi) where L j=1 δti,j is the number of relevant terms in cluster Ci, and K k=1 L j=1 δtk,j is the total number of relevant terms.",
                "We note that when there is only one cluster (K = 1), the above formula degenerates to p(w|θq ) = λp(w|θq) + (1 − λ)p(w|θ1) which is merely pseudo-feedback of the form proposed in [25]. 5.3 TCFB (Term-cluster Feedback) TFB and CFB both have their drawbacks.",
                "TFB assigns non-zero probabilities to the presented terms that are marked relevant, but completely ignores (a lot more) others, which may be left unchecked due to the users ignorance, or simply not included in the presentation list, but we should be able to infer their relevance from the checked ones.",
                "For example, in Figure 1, since as many as 5 terms in the middle cluster (the third and fourth columns) are checked, we should have high confidence in the relevance of other terms in that cluster.",
                "CFB remedies TFBs problem by treating the terms in a cluster collectively, so that unchecked/unpresented terms receive weights when presented terms in their clusters are judged as relevant, but it does not distinguish which terms in a cluster are presented or judged.",
                "Intuitively, the judged relevant terms should receive larger weights because they are explicitly indicated as relevant by the user.",
                "Therefore, we try to combine the two methods, hoping to get the best out of both.",
                "We do this by interpolating the TFB model with the CFB model, and call it TCFB: p(w|θq ) = αp(w|θqT F B ) + (1 − α)p(w|θqCF B ) 6.",
                "EXPERIMENTS In this section, we describe our experiment results.",
                "We first describe our experiment setup and present an overview of various methods performance.",
                "Then we discuss the effects of varying the parameter setting in the algorithms, as well as the number of presentation terms.",
                "Next we analyze user term feedback behavior and its relation to retrieval performance.",
                "Finally we compare term feedback to relevance feedback and show that it has its particular advantage. 6.1 Experiment Setup and Basic Results We took the opportunity of TREC 2005 HARD Track[2] for the evaluation of our algorithms.",
                "The tracks used the AQUAINT collection, a 3GB corpus of English newswire text.",
                "The topics included 50 ones previously known to be hard, i.e. with low retrieval performance.",
                "It is for these hard topics that user feedback is most helpful, as it can provide information to disambiguate the queries; with easy topics the user may be unwilling to spend efforts for feedback if the automatic retrieval results are good enough.",
                "Participants of the track were able to submit custom-designed clarification forms (CF) to solicit feedback from human assessors provided by Table 2: Retrieval performance for different methods and CF types.",
                "The last row is the percentage of MAP improvement over the baseline.",
                "The parameter settings μ = 4, λ = 0.1, α = 0.3 are near optimal.",
                "Baseline TFB1C TFB3C TFB6C CFB1C CFB3C CFB6C TCFB1C TCFB3C TCFB6C MAP 0.219 0.288 0.288 0.278 0.254 0.305 0.301 0.274 0.309 0.304 Pr@30 0.393 0.467 0.475 0.457 0.399 0.480 0.473 0.431 0.491 0.473 RR 4339 4753 4762 4740 4600 4907 4872 4767 4947 4906 % 0% 31.5% 31.5% 26.9% 16.0% 39.3% 37.4% 25.1% 41.1% 38.8% Table 3: MAP variation with the number of presented terms. # terms TFB1C TFB3C TFB6C CFB3C CFB6C TCFB3C TCFB6C 6 0.245 0.240 0.227 0.279 0.279 0.281 0.274 12 0.261 0.261 0.242 0.299 0.286 0.297 0.281 18 0.275 0.274 0.256 0.301 0.282 0.300 0.286 24 0.276 0.281 0.265 0.303 0.292 0.305 0.292 30 0.280 0.285 0.270 0.304 0.296 0.307 0.296 36 0.282 0.288 0.272 0.307 0.297 0.309 0.297 42 0.283 0.288 0.275 0.306 0.298 0.309 0.300 48 0.288 0.288 0.278 0.305 0.301 0.309 0.303 NIST.",
                "We designed three sets of clarification forms for term feedback, differing in the choice of K, the number of clusters, and L, the number of presented terms from each cluster.",
                "They are: 1× 48, a big cluster with 48 terms, 3 × 16, 3 clusters with 16 terms each, and 6 × 8, 6 clusters with 8 terms each.",
                "The total number of presented terms (M) is fixed at 48, so by comparing the performance of different types of clarification forms we can know the effects of different degree of clustering.",
                "For each topic, an assessor would complete the forms ordered by 6 × 8, 1 × 48 and 3 × 16, spending up to three minutes on each form.",
                "The sample clarification form shown in Figure 1 is of type 3 × 16.",
                "It is a simple and compact interface in which the user can check relevant terms.",
                "The form is self-explanatory; there is no need for extra user training on how to use it.",
                "Our initinal queries are constructed only using the topic title descriptions, which are on average 2.7 words in length.",
                "As our baseline we use the KL divergence retrieval method implemented in the Lemur Toolkit1 with 5 pseudo-feedback documents.",
                "We stem the terms, choose Dirichlet smoothing with a prior of 2000, and truncate query language models to 50 terms (these settings are used throughout the experiments).",
                "For all other parameters we use Lemurs default settings.",
                "The baseline turns out to perform above average among the track participants.",
                "After an initial run using this baseline retrieval method, we take the top 60 documents for each topic and apply the theme discovery algorithm to output the clusters (1, 3, or 6 of them), based on which we generate clarification forms.",
                "After user feedback is received, we run the term feedback algorithms (TFB, CFB or TCFB) to estimate updated query models, which are then used for a second iteration of retrieval.",
                "We evaluate the different retrieval methods performance on their rankings of the top 1000 documents.",
                "The evaluation metrics we adopt include mean average (non-interpolated) precision (MAP), precision at top 30 (Pr@30) and total relevant retrieved (RR).",
                "Table 2 shows the performance of various methods and configurations of K × L. The suffixes (1C, 3C, 6C) after TFB,CFB,TCFB stand for the number of clusters (K).",
                "For example, TCFB3C means the TCFB method on the 3 × 16 clarification forms.",
                "From Table 2 we can make the following observations: 1 http://www.lemurproject.com 1.",
                "All methods perform considerably better than the pseudofeedback baseline, with TCFB3C achieving a highest 41.1% improvement in MAP, indicating significant contribution of term feedback for clarification of the users information need.",
                "In other words, term feedback is truly helpful for improving retrieval accuracy. 2.",
                "For TFB, the performance is almost equal on the 1 × 48 and 3 × 16 clarification forms in terms of MAP (although the latter is slightly better in Pr@30 and RR), and a little worse on the 6 × 8 ones. 3.",
                "Both CFB3C and CFB6C perform better than their TFB counterparts in all three metrics, suggesting that feedback on a secondary cluster structure is indeed beneficial.",
                "CFB1C is actually worse because it cannot adjust the weight of its (single) cluster from term feedback and it is merely pseudofeedback. 4.",
                "Although TCFB is just a simple mixture of TFB and CFB by interpolation, it is able to outperform both.",
                "This supports our speculation that TCFB overcomes the drawbacks of TFB (paying attention only to checked terms) and CFB (not distinguishing checked and unchecked terms in a cluster).",
                "Except for TCFB6C v.s.",
                "CFB6C, the performance advantage of TCFB over TFB/CFB is significant at p < 0.05 using the Wilcoxon signed rank test.",
                "This is not true in the case of TFB v.s.",
                "CFB, each of which is better than the other in nearly half of the topics. 6.2 Reduction of Presentation Terms In some situations we may have to reduce the number of presentation terms due to limits in display space or user feedback efforts.",
                "It is interesting to know whether our algorithms performance deteriorates when the user is presented with fewer terms.",
                "Because the presentation terms within each cluster are generated in decreasing order of their frequencies, the presentation list forms a subset of the original one if its size is reduced2 .",
                "Therefore, we can easily simulate what happens when the number of presentation terms decreases 2 There are complexities arising from terms appearing in top L of multiple clusters, but these are exceptions from M to M : we will keep all judgments of the top L = M /K terms in each cluster and discard those of others.",
                "Table 3 shows the performance of various algorithms as the number of presentation terms ranges from 6 to 48.",
                "We find that the performance of TFB is more susceptible to presentation term reduction than that of CFB or TCFB.",
                "For example, at 12 terms the MAP of TFB3C is 90.6% of that at 48 terms, while the numbers for CFB3C and TCFB3C are 98.0% and 96.1% respectively.",
                "We conjecture the reason to be that while TFBs performance heavily depends on how many good terms are chosen for query expansion, CFB only needs a rough estimate of cluster weights to work.",
                "Also, the 3 × 16 clarification forms seem to be more robust than the 6 × 8 ones: at 12 terms the MAP of TFB6C is 87.1% of that at 48 terms, lower than 90.6% for TFB3C.",
                "Similarly, for CFB it is 95.0% against 98.0%.",
                "This is natual, as for a large cluster number of 6, it is easier to get into the situation where each cluster gets too few presentation terms to make topic diversification useful.",
                "Overall, we are surprised to see that the algorithms are still able to perform reasonably well when the number of presentation terms is small.",
                "For example, at only 12 terms CFB3C (the clarification form is of size 3 × 4) can still improve 36.5% over the baseline, dropping slightly from 39.3% at 48 terms. 6.3 User Feedback Analysis In this part we study several aspects of users term feedback behavior, and whether they are connected to retrieval performance.",
                "Figure 2: Clarification form completion time distributions 0−30 30−60 60−90 90−120 120−150 150−180 0 5 10 15 20 25 30 35 completion time (seconds) #topics 1×48 3×16 6×8 Figure 2 shows the distribution of time needed to complete a clarification form3 .",
                "We see that the user is usually able to finish term feedback within a reasonably short amount of time: for more than half of the topics the clarification form is completed in just 1 minute, and only a small fraction of topics (less than 10% for 1 × 48 and 3 × 16) take more than 2 minutes.",
                "This suggests that term feedback is suitable for interactive ad-hoc retrieval, where a user usually does not want to spend too much time on providing feedback.",
                "We find that a user often makes mistakes when judging term relevance.",
                "Sometimes a relevant term may be left out because its connection to the query topic is not obvious to the user.",
                "Other times a dubious term may be included but turns out to be irrelevant.",
                "Take the topic in Figure 1 for example.",
                "There was a fire disaster in Mont 3 The maximal time is 180 seconds, as the NIST assessor would be forced to submit the form at that moment.",
                "Table 4: Term selection statistics (topic average) CF Type 1 × 48 3 × 16 6 × 8 # checked terms 14.8 13.3 11.2 # rel. terms 15.0 12.6 11.2 # rel. checked terms 7.9 6.9 5.9 precision 0.534 0.519 0.527 recall 0.526 0.548 0.527 Blanc Tunnel between France and Italy in 1999, but the user failed to select such keywords as mont, blanc, french and italian due to his/her ignorance of the event.",
                "Indeed, without proper context it would be hard to make perfect judgment.",
                "What is then, the extent to which the user is good at term feedback?",
                "Does it have serious impact on retrieval performance?",
                "To answer these questions, we need a measure of individual terms true relevance.",
                "We adopt the Simplified KL Divergence metric used in [24] to decide query expansion terms as our term relevance measure: σKLD(w) = p(w|R) log p(w|R) p(w|¬R) where p(w|R) is the probability that a relevant document contains term w, and p(w|¬R) is the probability that an irrelevant document contains w, both of which can be easily computed via maximum likelihood estimate given document-level relevance judgment.",
                "If σKLD(w) > 0, w is more likely to appear in relevant documents than irrelevant ones.",
                "We consider a term relevant if its Simplified KL Divergence value is greater than a certain threshold σ0.",
                "We can then define precision and recall of user term judgment accordingly: precision is the fraction of terms checked by the user that are relevant; recall is the fraction of presented relevant terms that are checked by the user.",
                "Table 4 shows the number of checked terms, relevant terms and relevant checked terms when σ0 is set to 1.0, as well as the precision/recall of user term judgment.",
                "Note that when the clarification forms contain more clusters, fewer terms are checked: 14.8 for 1 × 48, 13.3 for 3 × 16 and 11.2 for 6×8.",
                "Similar pattern holds for relevant terms and relevant checked terms.",
                "There seems to be a trade-off between increasing topic diversity by clustering and losing extra relevant terms: when there are more clusters, each of them gets fewer terms to present, which can hurt a major relevant cluster that contains many relevant terms.",
                "Therefore, it is not always helpful to have more clusters, e.g., TFB6C is actually worse than TFB1C.",
                "The major finding we can make from Table 4 is that the user is not particularly good at identifying relevant terms, which echoes the discovery in [18].",
                "In the case of 3 × 16 clarification forms, the average number of terms checked as relevant by the user is 13.3 per topic, and the average number of relevant terms whose σKLD value exceed 1.0 is 12.6.",
                "The user is able to recognize only 6.9 of these terms on average.",
                "Indeed, the precision and recall of user feedback terms (as defined previously) are far from perfect.",
                "On the other hand, If the user had correctly checked all such relevant terms, the performance of our algorithms would have increased a lot, as shown in Table 5.",
                "We see that TFB gets big improvement when there is an oracle who checks all relevant terms, while CFB meets a bottleneck around MAP of 0.325, since all it does is adjust cluster weights, and when the learned weights are close to being accurate, it cannot benefit more from term feedback.",
                "Also note that TCFB fails to outperform TFB, probably because TFB is sufficiently accurate.",
                "Table 5: Change of MAP when using all (and only) relevant terms (σKLD > 1.0) for feedback. original term feedback relevant term feedback TF1 0.288 0.354 TF3 0.288 0.354 TF6 0.278 0.346 CF3 0.305 0.325 CF6 0.301 0.326 TCF3 0.309 0.345 TCF6 0.304 0.341 6.4 Comparison with Relevance Feedback Now we compare term feedback with document-level relevance feedback, in which the user is presented with the top N documents from an initial retrieval and asked to judge their relevance.",
                "The feedback process is simulated using document relevance judgment from NIST.",
                "We use the mixture model based feedback method proposed in [25], with mixture noise set to 0.95 and feedback coefficient set to 0.9.",
                "Comparative evaluation of relevance feedback against other methods is complicated by the fact that some documents have already been viewed during feedback, so it makes no sense to include them in the retrieval results of the second run.",
                "However, this does not hold for term feedback.",
                "Thus, to make it fair w.r.t. users information gain, if the feedback documents are relevant, they should be kept in the top of the ranking; if they are irrelevant, they should be left out.",
                "Therefore, we use relevance feedback to produce a ranking of top 1000 retrieved documents but with every feedback document excluded, and then prepend the relevant feedback documents at the front.",
                "Table 6 shows the performance of relevance feedback for different values of N and compares it with TCFB3C.",
                "Table 6: Performance of relevance feedback for different number of feedback documents (N).",
                "N MAP Pr@30 RR 5 0.302 0.586 4779 10 0.345 0.670 4916 20 0.389 0.772 5004 TCFB3C 0.309 0.491 4947 We see that the performance of TCFB3C is comparable to that of relevance feedback using 5 documents.",
                "Although it is poorer than when there are 10 feedback documents in terms of MAP and Pr@30, it does retrieve more documents (4947) when going down the ranked list.",
                "We try to compare the quality of automatically inserted terms in relevance feedback with that of manually selected terms in term feedback.",
                "This is done by truncating the relevance feedback modified query model to a size equal to the number of checked terms for the same topic.",
                "We can then compare the terms in the truncated model with the checked terms.",
                "Figure 3 shows the distribution of the terms σKLD scores.",
                "We find that term feedback tends to produce expansion terms of higher quality(those with σKLD > 1) compared to relevance feedback (with 10 feedback documents).",
                "This does not contradict the fact that the latter yields higher retrieval performance.",
                "Actually, when we use the truncated query model instead of the intact one refined from relevance feedback, the MAP is only 0.304.",
                "The truth Figure 3: Comparison of expansion term quality between relevance feedback (with 10 feedback documents) and term feedback (with 3 × 16 CFs) −1−0 0−1 1−2 2−3 3−4 4−5 5−6 0 50 100 150 200 250 300 350 σKLD #terms relevance feedback term feedback is, although there are many unwanted terms in the expanded query model from feedback documents, there are also more relevant terms than what the user can possibly select from the list of presentation terms generated with pseudo-feedback documents, and the positive effects often outweights the negative ones.",
                "We are interested to know under what circumstances term feedback has advantage over relevance feedback.",
                "One such situation is when none of the top N feedback documents is relevant, rendering relevance feedback useless.",
                "This is not infrequent, as one might have thought: out of the 50 topics, there are 13 such cases when N = 5, 10 when N = 10, and still 3 when N = 20.",
                "When this happens, one can only back off to the original retrieval method; the power of relevance feedback is lost.",
                "Surprisingly, in 11 out of 13 such cases where relevance feedback seems impossible, the user is able to check at least 2 relevant terms from the 3 × 16 clarification form (we consider term t to be relevant if σKLD(t) > 1.0).",
                "Furthermore, in 10 out of them TCFB3C outperforms the pseudo-feedback baseline, increasing MAP from 0.076 to 0.146 on average (these are particularly hard topics).",
                "We think that there are two possible explanations for this phenomenon of term feedback being active even when relevance feedback does not work: First, even if none of the top N (suppose it is a small number) documents are relevant, we may still find relevant documents in top 60, which is more inclusive but usually unreachable when people are doing relevance feedback in interactive ad-hoc search, from which we can draw feedback terms.",
                "This is true for topic 367 piracy, where the top 10 feedback documents are all about software piracy, yet there are documents between 10-60 that are about piracy on the seas (which is about the real information need), contributing terms such as pirate, ship for selection in the clarification form.",
                "Second, for some topics, a document needs to meet some special condition in order to be relevant.",
                "The top N documents may be related to the topic, but nonetheless irrelevant.",
                "In this case, we may still extract useful terms from these documents, even if they do not qualify as relevant ones.",
                "For example, in topic 639 consumer online shopping, a document needs to mention what contributes to shopping growth to really match the specified information need, hence none of the top 10 feedback documents are regarded as relevant.",
                "But nevertheless, the feedback terms such as retail, commerce are good for query expansion. 7.",
                "CONCLUSIONS In this paper we studied the use of term feedback for interactive information retrieval in the language modeling approach.",
                "We proposed a cluster-based method for selecting presentation terms as well as algorithms to estimate refined query models from user term feedback.",
                "We saw significant improvement in retrieval accuracy brought by term feedback, in spite of the fact that a user often makes mistakes in relevance judgment that hurts its performance.",
                "We found the best-performing algorithm to be TCFB, which benefits from the combination of directly observed term evidence with TFB and indirectly learned cluster relevance with CFB.",
                "When we reduced the number of presentation terms, term feedback is still able to keep much of its performance gain over the baseline.",
                "Finally, we compared term feedback to document-level relevance feedback, and found that TCFB3Cs performance is on a par with the latter with 5 feedback documents.",
                "We regarded term feedback as a viable alternative to traditional relevance feedback, especially when there are no relevant documents in the top.",
                "We propose to extend our work in several ways.",
                "First, we want to study whether the use of various contexts can help the user to better identify term relevance, while not sacrificing the simplicity and compactness of term feedback.",
                "Second, currently all terms are presented to the user in a single batch.",
                "We could instead consider iterative term feedback, by presenting a small number of terms first, and show more terms after receiving user feedback or stop when the refined query is good enough.",
                "The presented terms should be selected dynamically to maximize learning benefits at any moment.",
                "Third, we have plans to incorporate term feedback into our UCAIR toolbar[20], an Internet Explorer plugin, to make it work for web search.",
                "We are also interested in studying how to combine term feedback with relevance feedback or implicit feedback.",
                "We could, for example, allow the user to dynamically modify terms in a language model learned from feedback documents. 8.",
                "ACKNOWLEDGMENT This work is supported in part by the National Science Foundation grants IIS-0347933 and IIS-0428472. 9.",
                "REFERENCES [1] J. Allan.",
                "Relevance feedback with too much data.",
                "In Proceedings of the 18th annual international ACM SIGIR conference on research and development in information retrieval, pages 337-343, 1995. [2] J. Allan.",
                "HARD track overview in TREC 2005 - High Accuracy Retrieval from Documents.",
                "In The Fourteenth Text REtrieval Conference, 2005. [3] P. Anick.",
                "Using terminological feedback for web search refinement: a log-based study.",
                "In Proceedings of the 26th annual international ACM SIGIR conference on research and development in informaion retrieval, pages 88-95, 2003. [4] P. G. Anick and S. Tipirneni.",
                "The paraphrase search assistant: terminological feedback for iterative information seeking.",
                "In Proceedings of the 22nd annual international ACM SIGIR conference on research and development in information retrieval, pages 153-159, 1999. [5] C. Buckley, G. Salton, J. Allan, and A. Singhal.",
                "Automatic query expansion using SMART.",
                "In Proceedings of the Third Text REtrieval Conference, 1994. [6] D. Harman.",
                "Towards interactive query expansion.",
                "In Proceedings of the 11th annual international ACM SIGIR conference on research and development in information retrieval, pages 321-331, 1988. [7] N. A. Jaleel, A. Corrada-Emmanuel, Q. Li, X. Liu, C. Wade, and J. Allan.",
                "UMass at TREC 2003: HARD and QA.",
                "In TREC, pages 715-725, 2003. [8] H. Joho, C. Coverson, M. Sanderson, and M. Beaulieu.",
                "Hierarchical presentation of expansion terms.",
                "In Proceedings of the 2002 ACM symposium on applied computing, pages 645-649, 2002. [9] K. S. Jones, S. Walker, and S. E. Robertson.",
                "A probabilistic model of information retrieval: development and status.",
                "Technical Report 446, Computer Laboratory, University of Cambridge, 1998. [10] D. Kelly, V. D. Dollu, and X. Fu.",
                "The loquacious user: a document-independent source of terms for query expansion.",
                "In Proceedings of the 28th annual international ACM SIGIR conference on research and development in information retrieval, pages 457-464, 2005. [11] D. Kelly and X. Fu.",
                "Elicitation of term relevance feedback: an investigation of term source and context.",
                "In Proceedings of the 29th annual international ACM SIGIR conference on research and development in information retrieval, 2006. [12] J. Koenemann and N. Belkin.",
                "A case for interaction: A study of interactive information retrieval behavior and effectiveness.",
                "In Proceedings of the SIGCHI conference on human factors in computing systems, pages 205-212, 1996. [13] V. Lavrenko and W. B. Croft.",
                "Relevance-based language models.",
                "In Research and Development in Information Retrieval, pages 120-127, 2001. [14] Y. Nemeth, B. Shapira, and M. Taeib-Maimon.",
                "Evaluation of the real and perceived value of automatic and interactive query expansion.",
                "In Proceedings of the 27th annual international ACM SIGIR conference on research and development in information retrieval, pages 526-527, 2004. [15] J. Ponte.",
                "A Language Modeling Approach to Information Retrieval.",
                "PhD thesis, University of Massachusetts at Amherst, 1998. [16] S. E. Robertson, S. Walker, S. Jones, M. Beaulieu, and M. Gatford.",
                "Okapi at TREC-3.",
                "In Proceedings of the Third Text REtrieval Conference, 1994. [17] J. Rocchio.",
                "Relevance feedback in information retrieval.",
                "In The SMART retrieval system, pages 313-323. 1971. [18] I. Ruthven.",
                "Re-examining the potential effectiveness of interactive query expansion.",
                "In Proceedings of the 26th annual international ACM SIGIR conference on research and development in informaion retrieval, pages 213-220, 2003. [19] G. Salton and C. Buckley.",
                "Improving retrieval performance by relevance feedback.",
                "Journal of the American Society for Information Science, 41:288-297, 1990. [20] X. Shen, B. Tan, and C. Zhai.",
                "Implicit user modeling for personalized search.",
                "In Proceedings of the 14th ACM international conference on information and knowledge management, pages 824-831, 2005. [21] X. Shen and C. Zhai.",
                "Active feedback in ad-hoc information retrieval.",
                "In Proceedings of the 28th annual international ACM SIGIR conference on research and development in information retrieval, pages 59-66, 2005. [22] A. Spink.",
                "Term relevance feedback and query expansion: relation to design.",
                "In Proceedings of the 17th annual international ACM SIGIR conference on research and development in information retrieval, pages 81-90, 1994. [23] J. Xu and W. B. Croft.",
                "Query expansion using local and global document analysis.",
                "In Proceedings of the 19th annual international ACM SIGIR conference on research and development in information retrieval, pages 4-11, 1996. [24] H. Zaragoza, N. Craswell, M. Taylor, S. Saria, and S. Robertson.",
                "Microsoft cambridge at TREC-13: Web and HARD tracks.",
                "In Proceedings of the 13th Text REtrieval Conference, 2004. [25] C. Zhai and J. Lafferty.",
                "Model-based feedback in the language modeling approach to information retrieval.",
                "In Proceedings of the tenth international conference on information and knowledge management, pages 403-410, 2001. [26] C. Zhai, A. Velivelli, and B. Yu.",
                "A cross-collection mixture model for comparative text mining.",
                "In Proceedings of the tenth ACM SIGKDD international conference on knowledge discovery and data mining, pages 743-748, 2004."
            ],
            "original_annotated_samples": [
                "GENERAL APPROACH We follow the language modeling approach, and base our method on the <br>kl-divergence</br> retrieval model proposed in [25].",
                "With this model, the retrieval task involves estimating a query language model θq from a given query, a document language model θd from each document, and calculating their <br>kl-divergence</br> D(θq||θd), which is then used to score the documents. [25] treats relevance feedback as a query model re-estimation problem, i.e., computing an updated query model θq given the original query text and the extra evidence carried by the judged relevant documents."
            ],
            "translated_annotated_samples": [
                "Enfoque general: Seguimos el enfoque de modelado del lenguaje y basamos nuestro método en el modelo de recuperación de <br>divergencia KL</br> propuesto en [25].",
                "Con este modelo, la tarea de recuperación implica estimar un modelo de lenguaje de consulta θq a partir de una consulta dada, un modelo de lenguaje de documento θd de cada documento, y calcular su divergencia KL D(θq||θd), que luego se utiliza para puntuar los documentos. [25] trata la retroalimentación de relevancia como un problema de reestimación del modelo de consulta, es decir, calcular un modelo de consulta actualizado θq dado el texto de la consulta original y la evidencia adicional proporcionada por los documentos relevantes juzgados."
            ],
            "translated_text": "Comentarios sobre el término para la recuperación de información con modelos de lenguaje Bin Tan†, Atulya Velivelli‡, Hui Fang†, ChengXiang Zhai† Dept. En este documento estudiamos la retroalimentación basada en términos para la recuperación de información en el enfoque de modelado de lenguaje. Con la retroalimentación de términos, un usuario juzga directamente la relevancia de términos individuales sin interacción con documentos de retroalimentación, tomando el control total del proceso de expansión de la consulta. Proponemos un método basado en clusters para seleccionar términos para presentar al usuario para su evaluación, así como algoritmos efectivos para construir modelos de lenguaje de consulta refinados a partir de la retroalimentación de términos del usuario. Nuestros algoritmos han demostrado proporcionar una mejora significativa en la precisión de recuperación en comparación con una línea base sin retroalimentación, y logran un rendimiento comparable al de la retroalimentación de relevancia. Son útiles incluso cuando no hay documentos relevantes en la parte superior. Categorías y Descriptores de Asignaturas H.3.3 [Búsqueda y Recuperación de Información]: Modelos de recuperación Términos generales Algoritmos 1. En el enfoque de modelado del lenguaje para la recuperación de información, la retroalimentación a menudo se modela como la estimación de un modelo de consulta mejorado o un modelo de relevancia basado en un conjunto de documentos de retroalimentación [25, 13]. Esto está en línea con la forma tradicional de realizar retroalimentación de relevancia: presentar al usuario documentos/pasajes para su juicio de relevancia y luego extraer términos de los documentos o pasajes juzgados para expandir la consulta inicial. Es una forma indirecta de buscar la ayuda de los usuarios para la construcción del modelo de consulta, en el sentido de que el modelo de consulta refinado (basado en términos) se aprende a través de documentos/pasajes de retroalimentación, que son estructuras de alto nivel de términos. Tiene la desventaja de que los términos irrelevantes, que ocurren junto con los relevantes en el contenido evaluado, pueden ser utilizados erróneamente para la expansión de la consulta, causando efectos no deseados. Por ejemplo, para la consulta TREC del telescopio Hubble logros, cuando un documento relevante habla más sobre la reparación de los telescopios que sobre sus descubrimientos, términos irrelevantes como caminata espacial pueden ser agregados a la consulta modificada. Podemos considerar una forma más directa de involucrar a un usuario en la mejora del modelo de consulta, sin un paso intermedio de retroalimentación de documentos que pueda introducir ruido. La idea es presentar al usuario un número (razonable) de términos individuales y pedirle que juzgue la relevancia de cada término o especifique directamente sus probabilidades en el modelo de consulta. Esta estrategia ha sido discutida en [15], pero hasta donde sabemos, no ha sido estudiada seriamente en la literatura existente de modelado de lenguaje. En comparación con la retroalimentación de relevancia tradicional, este enfoque basado en términos para el refinamiento del modelo de consulta interactivo tiene varias ventajas. Primero, el usuario tiene un mejor control del modelo de consulta final a través de la manipulación directa de términos: él/ella puede dictar qué términos son relevantes, irrelevantes y posiblemente, en qué medida. Esto evita el riesgo de introducir términos no deseados en el modelo de consulta, aunque a veces el usuario introduce términos de baja calidad. Segundo, dado que un término tarda menos tiempo en ser evaluado que el texto completo o el resumen de un documento, y con tan solo alrededor de 20 términos presentados se puede lograr una mejora significativa en el rendimiento de recuperación (como demostraremos más adelante), el feedback de términos permite recopilar la retroalimentación del usuario de manera más rápida. Esto es especialmente útil para la búsqueda interactiva ad hoc. En tercer lugar, a veces no hay documentos relevantes en los primeros N resultados recuperados inicialmente si el tema es difícil. Esto suele ser cierto cuando N está limitado a ser pequeño, lo cual surge del hecho de que el usuario no está dispuesto a evaluar demasiados documentos. En este caso, la retroalimentación de relevancia es inútil, ya que no se puede aprovechar ningún documento relevante, pero la retroalimentación de términos sigue siendo útil, al permitir seleccionar términos relevantes de documentos irrelevantes. Durante nuestra participación en la pista HARD de TREC 2005 y el estudio continuado posteriormente, exploramos cómo aprovechar la retroalimentación de términos por parte del usuario para construir modelos de consulta mejorados para la recuperación de información en el enfoque de modelado de lenguaje. Identificamos dos sub tareas clave del feedback basado en términos, es decir, la selección de términos para la presentación previa al feedback y la construcción del modelo de consulta posterior al feedback, con algoritmos efectivos desarrollados para ambos. Impusimos una estructura de clúster secundaria en los términos y descubrimos que una vista de clúster arroja una mayor comprensión de la necesidad de información de los usuarios, y proporciona una buena forma de utilizar la retroalimentación de términos. A través de experimentos encontramos que el feedback a término mejora significativamente sobre la línea base sin feedback, a pesar de que el usuario a menudo comete errores en la evaluación de relevancia. Entre nuestros algoritmos, el que tiene el mejor rendimiento de recuperación es TCFB, la combinación de TFB, el algoritmo de retroalimentación de términos directos, y CFB, el algoritmo de retroalimentación basado en clústeres. También variamos el número de términos de retroalimentación y observamos una mejora razonable incluso con números bajos. Finalmente, al comparar la retroalimentación a nivel de términos con la retroalimentación a nivel de documentos, encontramos que es una alternativa viable a esta última con un rendimiento competitivo en la recuperación. El resto del documento está organizado de la siguiente manera. La sección 2 discute algunos trabajos relacionados. La sección 4 describe nuestro enfoque general para la retroalimentación de términos. Presentamos nuestro método para la selección de términos de presentación en la Sección 3 y los algoritmos para la construcción del modelo de consulta en la Sección 5. Los resultados del experimento se presentan en la Sección 6. La sección 7 concluye este documento. 2. TRABAJO RELACIONADO La retroalimentación de relevancia[17, 19] ha sido reconocida durante mucho tiempo como un método efectivo para mejorar el rendimiento de recuperación. Normalmente, los N documentos principales recuperados utilizando la consulta original se presentan al usuario para su evaluación, después de lo cual se extraen términos de los documentos relevantes evaluados, ponderados por su potencial de atraer más documentos relevantes, y se añaden al modelo de consulta. La consulta ampliada suele representar mejor la necesidad de información de los usuarios que la original, que a menudo es solo una consulta de palabras clave cortas. Una segunda iteración de recuperación utilizando esta consulta modificada generalmente produce un aumento significativo en la precisión de la recuperación. En los casos en los que no se dispone de una evaluación de relevancia real y se asume que todos los documentos principales N son relevantes, se denomina retroalimentación ciega o pseudo [5, 16] y generalmente sigue mejorando el rendimiento. Debido a que un documento es una unidad de texto grande, cuando se utiliza para retroalimentación de relevancia, muchos términos irrelevantes pueden ser introducidos en el proceso de retroalimentación. Para superar esto, se propone el feedback de pasajes y se muestra que mejora el rendimiento del feedback. Una solución más directa es pedir al usuario su juicio de relevancia sobre los términos de retroalimentación. Por ejemplo, en algunos sistemas de retroalimentación de relevancia como [12], hay un paso de interacción que permite al usuario agregar o eliminar términos de expansión después de que son extraídos automáticamente de los documentos relevantes. Esto se clasifica como expansión interactiva de consultas, donde la consulta original se amplía con términos proporcionados por el usuario, que pueden provenir de la entrada directa del usuario (texto libre o palabras clave) o de la selección del usuario de términos sugeridos por el sistema (utilizando tesauros o extraídos de documentos de retroalimentación). En muchos casos, se ha encontrado que la retroalimentación de relevancia de términos mejora de manera efectiva el rendimiento de recuperación [6, 22, 12, 4, 10]. Por ejemplo, el estudio en [12] muestra que el usuario prefiere tener un conocimiento explícito y control directo de qué términos se utilizan para la expansión de la consulta, y se demuestra que la interfaz penetrable que proporciona esta libertad tiene un mejor rendimiento que otras interfaces. Sin embargo, en algunos otros casos no hay un beneficio significativo[3, 14], incluso si al usuario le gusta interactuar con los términos de expansión. En un estudio simulado realizado en [18], el autor compara el rendimiento de recuperación de la expansión interactiva de consultas y la expansión automática de consultas con un estudio simulado, y sugiere que los posibles beneficios de la primera pueden ser difíciles de lograr. Se descubre que el usuario no es bueno identificando términos útiles para la expansión de la consulta, cuando una interfaz de presentación de términos simple no puede proporcionar suficiente contexto semántico de los términos de retroalimentación. Nuestro trabajo difiere de los anteriores en dos aspectos importantes. Primero, al elegir los términos para presentar al usuario para la evaluación de relevancia, no solo consideramos el valor de un solo término (por ejemplo, la frecuencia relativa de un término en los documentos principales, que puede medirse mediante métricas como el Valor de Selección de Robertson y la Distancia Kullback-Leibler Simplificada como se detalla en [24]), sino que también examinamos la estructura de los grupos de términos, con el fin de producir una cobertura equilibrada de los diferentes aspectos del tema. Segundo, con el marco de modelado del lenguaje, permitimos una construcción detallada del modelo de consulta actualizado, estableciendo diferentes probabilidades para diferentes términos según si es un término de consulta, su importancia en los documentos principales y su pertenencia a un clúster. Aunque existen técnicas para ajustar los pesos de los términos de consulta en modelos de espacio vectorial y modelos de relevancia probabilística, la mayoría de los trabajos mencionados no las utilizan, optando por simplemente añadir términos de retroalimentación a la consulta original (usando pesos iguales para ellos), lo que puede llevar a un rendimiento de recuperación más pobre. La combinación de los dos aspectos permite que nuestro método funcione mucho mejor que el punto de referencia. La forma habitual de presentar los términos de retroalimentación es simplemente mostrar los términos en una lista. Ha habido algunos trabajos sobre interfaces de usuario alternativas. [8] organiza los términos en una jerarquía, y [11] compara tres interfaces diferentes, incluyendo términos + casillas de verificación, términos + contexto (oraciones) + casillas de verificación, oraciones + cuadro de texto de entrada. En ambos estudios, sin embargo, no hay una diferencia significativa en el rendimiento. En nuestro trabajo adoptamos el enfoque más simple de términos + casillas de verificación. Nos enfocamos en la presentación de términos y la construcción de modelos de consulta a partir de términos de retroalimentación, y creemos que el uso de contextos para mejorar la calidad de los términos de retroalimentación debería ser ortogonal a nuestro método. 3. Enfoque general: Seguimos el enfoque de modelado del lenguaje y basamos nuestro método en el modelo de recuperación de <br>divergencia KL</br> propuesto en [25]. Con este modelo, la tarea de recuperación implica estimar un modelo de lenguaje de consulta θq a partir de una consulta dada, un modelo de lenguaje de documento θd de cada documento, y calcular su divergencia KL D(θq||θd), que luego se utiliza para puntuar los documentos. [25] trata la retroalimentación de relevancia como un problema de reestimación del modelo de consulta, es decir, calcular un modelo de consulta actualizado θq dado el texto de la consulta original y la evidencia adicional proporcionada por los documentos relevantes juzgados. Adoptamos esta perspectiva y planteamos nuestra tarea como la actualización del modelo de consulta a partir de la retroalimentación de términos de usuario. Aquí hay dos tareas clave: Primero, cómo elegir los mejores términos para presentar al usuario para su evaluación, con el fin de recopilar la máxima evidencia sobre la necesidad de información del usuario. Segundo, cómo calcular un modelo de consulta actualizado basado en esta retroalimentación de términos, de manera que capture la necesidad de información de los usuarios y se traduzca en un buen rendimiento de recuperación. 4. SELECCIÓN DE TÉRMINOS DE PRESENTACIÓN La selección adecuada de los términos que se presentarán al usuario para su evaluación es crucial para el éxito de la retroalimentación de términos. Si los términos están mal elegidos y hay pocos relevantes, el usuario tendrá dificultades para buscar términos útiles que ayuden a aclarar su necesidad de información. Si los términos relevantes son abundantes, pero todos se centran en un solo aspecto del tema de la consulta, entonces solo podremos obtener retroalimentación sobre ese aspecto y perderemos otros, lo que resultará en una pérdida de amplitud en los resultados recuperados. Por lo tanto, es importante seleccionar cuidadosamente los términos de presentación para maximizar la ganancia esperada de la retroalimentación del usuario, es decir, aquellos que pueden revelar potencialmente la mayor evidencia de la necesidad de información del usuario. Esto es similar al feedback activo[21], que sugiere que un sistema de recuperación debería sondear activamente la necesidad de información de los usuarios, y en el caso del feedback de relevancia, los documentos de feedback deberían ser elegidos para maximizar los beneficios de aprendizaje (por ejemplo, de manera diversa para aumentar la cobertura). En nuestro enfoque, los N documentos principales de una recuperación inicial utilizando la consulta original forman la fuente de términos de retroalimentación: todos los términos que aparecen en ellos se consideran candidatos para presentar al usuario. Estos documentos sirven como pseudo-retroalimentación, ya que proporcionan un contexto mucho más amplio que la consulta original (generalmente muy breve), sin que se le pida al usuario que juzgue su relevancia. Debido a esta última razón, es posible hacer que N sea bastante grande (por ejemplo, en nuestros experimentos establecimos N = 60) para aumentar su cobertura de diferentes aspectos en el tema. La forma más sencilla de seleccionar términos de retroalimentación es elegir los M términos más frecuentes de los N documentos. Este método, sin embargo, tiene dos inconvenientes. Primero, se seleccionarán muchos términos ruidosos comunes debido a sus altas frecuencias en la colección de documentos, a menos que se utilice una lista de palabras vacías para filtrar. En segundo lugar, la lista de presentación tiende a estar compuesta por términos de los aspectos principales del tema; es probable que se pasen por alto aquellos de un aspecto menor debido a sus frecuencias relativamente bajas. Resolvemos los problemas anteriores mediante dos medidas correspondientes. Primero, introducimos un modelo de fondo θB que se estima a partir de estadísticas de la colección y explica los términos comunes, de modo que es mucho menos probable que aparezcan en la lista de presentación. Segundo, los términos se seleccionan de múltiples grupos en los documentos de retroalimentación simulada, para garantizar una representación suficiente de diferentes aspectos del tema. Nos basamos en el modelo multinomial de mezcla, que se utiliza para el descubrimiento de temas en [26]. Específicamente, asumimos que los N documentos contienen K grupos {Ci| i = 1, 2, · · · K}, cada uno caracterizado por una distribución de palabras multinomial (también conocida como modelo de lenguaje unigrama) θi y correspondiente a un aspecto del tema. Los documentos se consideran como muestreados de una mezcla de K + 1 componentes, incluidos los K grupos y el modelo de fondo: p(w|d) = λBp(w|θB) + (1 − λB) K i=1 πd,ip(w|θi) donde w es una palabra, λB es el peso de la mezcla para el modelo de fondo θB, y πd,i es el peso de la mezcla específico del documento para el modelo de cluster i-ésimo θi. Luego estimamos los modelos de clúster maximizando la probabilidad de que los documentos de pseudo-retroalimentación sean generados a partir del modelo de mezcla multinomial: log p(D|Λ) = d∈D w∈V c(w; d) log p(w|d) donde D = {di| i = 1, 2, · · · N} es el conjunto de los N documentos, V es el vocabulario, c(w; d) es la frecuencia de w en d y Λ = {θi| i = 1, 2, · · · K} ∪ {πdij | i = 1, 2, · · · N, j = 1, 2, · · · K} es el conjunto de parámetros del modelo a estimar. Los modelos de conglomerados pueden ser estimados eficientemente utilizando el algoritmo de Expectation-Maximization (EM). Para más detalles, remitimos al lector a [26]. La Tabla 1 muestra los modelos de clúster para la consulta de TREC sobre desastres en túneles de transporte (K = 3). Ten en cuenta que solo el grupo central es relevante. Tabla 1: Modelos de clúster para el tema 363 Desastres en túneles de transporte Clúster 1 Clúster 2 Clúster 3 túnel 0.0768 túnel 0.0935 túnel 0.0454 transporte 0.0364 fuego 0.0295 transporte 0.0406 tráfico 0.0206 camión 0.0236 peaje 0.0166 ferrocarril 0.0186 francés 0.0220 Amtrak 0.0153 puerto 0.0146 humo 0.0157 tren 0.0129 riel 0.0140 coche 0.0154 aeropuerto 0.0122 puente 0.0139 italiano 0.0152 autopista 0.0105 kilómetro 0.0136 incendio 0.0144 lui 0.0095 camión 0.0133 llamas 0.0127 Jersey 0.0093 construcción 0.0131 blanco 0.0121 paso 0.0087 · · · · · · · · · De cada uno de los K clústeres estimados, elegimos los L = M/K términos con las probabilidades más altas para formar un total de M términos de presentación. Si un término resulta estar en el top L en múltiples grupos, lo asignamos al grupo donde tenga la probabilidad más alta y permitimos que los otros grupos tomen un término adicional como compensación. También filtramos los términos en el texto de la consulta original porque tienden a ser siempre relevantes cuando la consulta es corta. Los términos seleccionados son luego presentados al usuario para su evaluación. Se muestra un formulario de retroalimentación (completado) de muestra en la Figura 1. En este estudio solo tratamos con juicios binarios: un término presentado está por defecto sin marcar, y un usuario puede marcarlo para indicar relevancia. Tampoco explotamos explícitamente la retroalimentación negativa (es decir, penalizar términos irrelevantes), porque con la retroalimentación binaria un término no verificado no es necesariamente irrelevante (quizás el usuario no está seguro de su relevancia). Podríamos pedir al usuario un juicio más detallado (por ejemplo, eligiendo entre altamente relevante, algo relevante, no sé, algo irrelevante y altamente irrelevante), pero la retroalimentación binaria es más compacta, ocupando menos espacio para mostrar y requiriendo menos esfuerzo por parte del usuario para emitir un juicio. 5. ESTIMACIÓN DE MODELOS DE CONSULTA A PARTIR DE RETROALIMENTACIÓN DE TÉRMINOS En esta sección, presentamos varios algoritmos para aprovechar la retroalimentación de términos. Los algoritmos toman como entrada la consulta original q, los grupos {θi} generados por el algoritmo de descubrimiento de temas, el conjunto de términos de retroalimentación T y su juicio de relevancia R, y producen un modelo de lenguaje de consulta actualizado θq que hace el mejor uso de la evidencia de retroalimentación para capturar la necesidad de información de los usuarios. Primero describimos nuestras notaciones: • θq: El modelo de consulta original, derivado solo de los términos de la consulta: p(w|θq) = c(w; q) |q| donde c(w; q) es la cantidad de veces que w aparece en q, y |q| = w∈q c(w; q) es la longitud de la consulta. • θq: El modelo de consulta actualizado que necesitamos estimar a partir de la retroalimentación de términos. • θi (i = 1, 2, . . . K): El modelo de lenguaje unigrama del clúster Ci, estimado utilizando el algoritmo de descubrimiento de temas. • T = {ti,j} (i = 1 . . . K, j = 1 . . . \n\nK, j = 1 . . . El conjunto de términos presentados al usuario para su evaluación. ti,j es el j-ésimo término elegido del grupo Ci. • R = {δw|w ∈ T}: δw es una variable indicadora que es 1 si w es considerado relevante o 0 en caso contrario. 5.1 TFB (Retroalimentación Directa de Términos) Esta es una forma directa de retroalimentación de términos que no implica ninguna estructura secundaria. Asignamos un peso de 1 a los términos considerados relevantes por el usuario, un peso de μ a los términos de la consulta, un peso de cero a los demás términos, y luego aplicamos la normalización: p(w|θq ) = δw + μ c(w; q) w ∈T δw + μ|q| donde w ∈T δw es el número total de términos considerados relevantes. Llamamos a este método TFB (retroalimentación directa de términos). Si dejamos que μ = 1, este enfoque es equivalente a agregar los términos relevantes después de la consulta original, que es lo que hace la expansión estándar de consultas (sin reponderación de términos). Si establecemos μ > 1, estamos poniendo más énfasis en los términos de consulta que en los verificados. Ten en cuenta que el modelo de resultado será más sesgado hacia θq si la consulta original es larga o el feedback del usuario es débil, lo cual tiene sentido, ya que podemos confiar más en la consulta original en ambos casos. Figura 1: Formulario de aclaración completado para el Tema 363 363 desastres en túneles de transporte. Por favor, seleccione todos los términos relevantes para el tema. tráfico ferrocarril puerto puente ferroviario kilómetro construir suizo enlace cruzado hongkonés río proyecto metro camión de bomberos francés humo coche italiano bomberos incendio blanco montaña víctima francés rescate conductor chamonix emerger peaje tren amtrak aeropuerto turnpike jersey pass rome z centro electrón carretera boston velocidad bu enviar 5.2 CFB (Retroalimentación de Clúster) Aquí explotamos la estructura de clúster que jugó un papel importante cuando seleccionamos los términos de presentación. Los grupos representan diferentes aspectos del tema de la consulta, cada uno de los cuales puede ser relevante o no. Si somos capaces de identificar los grupos relevantes, podemos combinarlos para generar un modelo de consulta que sea bueno para descubrir documentos pertenecientes a estos grupos (en lugar de los irrelevantes). Podríamos pedir al usuario que juzgue directamente la relevancia de un grupo después de ver los términos representativos en ese grupo, pero a veces esto sería una tarea difícil para el usuario, quien tendría que adivinar la semántica de un grupo a través de su conjunto de términos, los cuales podrían no estar bien conectados entre sí debido a la falta de contexto. Por lo tanto, proponemos aprender retroalimentación de clúster de forma indirecta, inferir la relevancia de un clúster a través de la relevancia de sus términos de retroalimentación. Dado que cada grupo tiene un número igual de términos presentados al usuario, la medida más simple de la relevancia de un grupo es el número de términos que se consideran relevantes en él. Intuitivamente, cuantos más términos estén marcados como relevantes en un grupo, más cerca estará el grupo del tema de la consulta y más debería participar en la modificación de la consulta. Si combinamos los modelos de clúster utilizando pesos determinados de esta manera y luego interpolamos con el modelo de consulta original, obtenemos la siguiente fórmula para la actualización de la consulta, que llamamos CFB (Retroalimentación de Clúster): p(w|θq) = λp(w|θq) + (1 − λ) K i=1 L j=1 δti,j K k=1 L j=1 δtk,j p(w|θi) donde L j=1 δti,j es el número de términos relevantes en el clúster Ci, y K k=1 L j=1 δtk,j es el número total de términos relevantes. Observamos que cuando solo hay un clúster (K = 1), la fórmula anterior se degrada a p(w|θq ) = λp(w|θq) + (1 − λ)p(w|θ1) que es simplemente una pseudo-retroalimentación de la forma propuesta en [25]. TCFB (Retroalimentación Término-Clúster) TFB y CFB ambos tienen sus inconvenientes. TFB asigna probabilidades no nulas a los términos presentados que están marcados como relevantes, pero ignora por completo (muchos más) otros, que pueden quedar sin marcar debido a la ignorancia de los usuarios, o simplemente no estar incluidos en la lista de presentación, pero deberíamos ser capaces de inferir su relevancia a partir de los marcados. Por ejemplo, en la Figura 1, dado que se revisan hasta 5 términos en el grupo central (las tercera y cuarta columnas), deberíamos tener una alta confianza en la relevancia de otros términos en ese grupo. CFB resuelve el problema de TFB tratando los términos en un grupo de manera colectiva, de modo que los términos no verificados/no presentados reciban pesos cuando los términos presentados en sus grupos son considerados relevantes, pero no distingue qué términos en un grupo son presentados o considerados. Intuitivamente, los términos considerados relevantes deberían recibir pesos mayores porque son indicados explícitamente como relevantes por el usuario. Por lo tanto, intentamos combinar los dos métodos, con la esperanza de obtener lo mejor de ambos. Lo hacemos interpolando el modelo TFB con el modelo CFB, y lo llamamos TCFB: p(w|θq ) = αp(w|θqT F B ) + (1 − α)p(w|θqCF B ) 6. EXPERIMENTOS En esta sección, describimos los resultados de nuestro experimento. Primero describimos nuestra configuración experimental y presentamos una visión general del rendimiento de varios métodos. Luego discutimos los efectos de variar la configuración de los parámetros en los algoritmos, así como el número de términos de presentación. A continuación analizamos el comportamiento de retroalimentación de los usuarios y su relación con el rendimiento de recuperación. Finalmente comparamos la retroalimentación de términos con la retroalimentación de relevancia y mostramos que tiene su ventaja particular. 6.1 Configuración del Experimento y Resultados Básicos Aprovechamos la oportunidad de la pista HARD de TREC 2005[2] para la evaluación de nuestros algoritmos. Las pistas utilizaron la colección AQUAINT, un corpus de 3GB de texto de noticias en inglés. Los temas incluyeron 50 que se sabía que eran difíciles, es decir, con bajo rendimiento de recuperación. Es para estos temas difíciles que la retroalimentación del usuario es más útil, ya que puede proporcionar información para desambiguar las consultas; con temas fáciles, es posible que el usuario no esté dispuesto a esforzarse por dar retroalimentación si los resultados de recuperación automática son lo suficientemente buenos. Los participantes de la pista pudieron enviar formularios de aclaración (CF) diseñados a medida para solicitar retroalimentación de los evaluadores humanos proporcionados por la Tabla 2: Rendimiento de recuperación para diferentes métodos y tipos de CF. La última fila es el porcentaje de mejora del MAP sobre la línea base. Los ajustes de parámetros μ = 4, λ = 0.1, α = 0.3 son casi óptimos. Variación de MAP con el número de términos presentados. # términos TFB1C TFB3C TFB6C CFB3C CFB6C TCFB3C TCFB6C 6 0.245 0.240 0.227 0.279 0.279 0.281 0.274 12 0.261 0.261 0.242 0.299 0.286 0.297 0.281 18 0.275 0.274 0.256 0.301 0.282 0.300 0.286 24 0.276 0.281 0.265 0.303 0.292 0.305 0.292 30 0.280 0.285 0.270 0.304 0.296 0.307 0.296 36 0.282 0.288 0.272 0.307 0.297 0.309 0.297 42 0.283 0.288 0.275 0.306 0.298 0.309 0.300 48 0.288 0.288 0.278 0.305 0.301 0.309 0.303 NIST. Diseñamos tres conjuntos de formularios de aclaración para la retroalimentación de términos, diferenciados en la elección de K, el número de grupos, y L, el número de términos presentados de cada grupo. Son: 1 × 48, un gran grupo con 48 términos, 3 × 16, 3 grupos con 16 términos cada uno, y 6 × 8, 6 grupos con 8 términos cada uno. El número total de términos presentados (M) está fijo en 48, por lo que al comparar el rendimiento de diferentes tipos de formas de clarificación podemos conocer los efectos de diferentes grados de agrupamiento. Para cada tema, un evaluador completaría los formularios en el orden de 6 × 8, 1 × 48 y 3 × 16, dedicando hasta tres minutos en cada formulario. El formulario de aclaración de muestra mostrado en la Figura 1 es de tipo 3 × 16. Es una interfaz simple y compacta en la que el usuario puede consultar términos relevantes. El formulario es autoexplicativo; no es necesario brindar capacitación adicional al usuario sobre cómo utilizarlo. Nuestras consultas iniciales se construyen solo utilizando las descripciones de los títulos de los temas, que tienen en promedio 2.7 palabras de longitud. Como base utilizamos el método de recuperación de divergencia KL implementado en la herramienta Lemur con 5 documentos de retroalimentación pseudo. Aplicamos el suavizado de Dirichlet con una prior de 2000, y truncamos los modelos de lenguaje de consulta a 50 términos (estas configuraciones se utilizan en todos los experimentos). Para todos los demás parámetros, utilizamos la configuración predeterminada de Lemurs. El punto de referencia resulta tener un rendimiento por encima del promedio entre los participantes de la pista. Después de una ejecución inicial utilizando este método de recuperación de referencia, tomamos los 60 documentos principales para cada tema y aplicamos el algoritmo de descubrimiento de temas para producir los grupos (1, 3 o 6 de ellos), en base a los cuales generamos formularios de aclaración. Después de recibir la retroalimentación del usuario, ejecutamos los algoritmos de retroalimentación de términos (TFB, CFB o TCFB) para estimar modelos de consulta actualizados, los cuales luego se utilizan para una segunda iteración de recuperación. Evaluamos el rendimiento de los diferentes métodos de recuperación en sus clasificaciones de los 1000 documentos principales. Las métricas de evaluación que adoptamos incluyen la precisión media promedio (no interpolada) (MAP), precisión en los primeros 30 (Pr@30) y total relevante recuperado (RR). La Tabla 2 muestra el rendimiento de varios métodos y configuraciones de K × L. Los sufijos (1C, 3C, 6C) después de TFB, CFB, TCFB representan el número de grupos (K). Por ejemplo, TCFB3C significa el método TCFB en los formularios de aclaración de 3 × 16. De la Tabla 2 podemos hacer las siguientes observaciones: 1 http://www.lemurproject.com 1. Todos los métodos tienen un rendimiento considerablemente mejor que la línea base de pseudoretroalimentación, con TCFB3C logrando una mejora máxima del 41.1% en el MAP, lo que indica una contribución significativa de la retroalimentación de términos para la clarificación de la necesidad de información de los usuarios. En otras palabras, el feedback a término es realmente útil para mejorar la precisión de recuperación. 2. Para TFB, el rendimiento es casi igual en los formularios de aclaración de 1 × 48 y 3 × 16 en términos de MAP (aunque este último es ligeramente mejor en Pr@30 y RR), y un poco peor en los de 6 × 8. 3. Tanto CFB3C como CFB6C tienen un mejor rendimiento que sus contrapartes TFB en las tres métricas, lo que sugiere que el feedback sobre una estructura de clúster secundaria es realmente beneficioso. CFB1C es en realidad peor porque no puede ajustar el peso de su (único) grupo a partir de la retroalimentación de términos y es simplemente pseudoretroalimentación. 4. Aunque TCFB es solo una simple mezcla de TFB y CFB por interpolación, logra superar a ambos. Esto respalda nuestra especulación de que TCFB supera las desventajas de TFB (prestando atención solo a los términos marcados) y CFB (sin distinguir los términos marcados y no marcados en un grupo). Excepto por TCFB6C v.s. CFB6C, la ventaja de rendimiento de TCFB sobre TFB/CFB es significativa con p < 0.05 utilizando la prueba de rango con signo de Wilcoxon. Esto no es cierto en el caso de TFB v.s. CFB, cada uno de los cuales es mejor que el otro en casi la mitad de los temas. 6.2 Reducción de Términos de Presentación En algunas situaciones podemos tener que reducir el número de términos de presentación debido a limitaciones en el espacio de visualización o esfuerzos de retroalimentación del usuario. Es interesante saber si el rendimiento de nuestros algoritmos se deteriora cuando al usuario se le presentan menos términos. Dado que los términos de presentación dentro de cada grupo se generan en orden decreciente de sus frecuencias, la lista de presentación forma un subconjunto del original si su tamaño se reduce2. Por lo tanto, podemos simular fácilmente lo que sucede cuando el número de términos de presentación disminuye 2. Hay complejidades que surgen de los términos que aparecen en la parte superior L de múltiples grupos, pero estas son excepciones de M a M: mantendremos todos los juicios de los términos superiores L = M / K en cada grupo y descartaremos los de los demás. La Tabla 3 muestra el rendimiento de varios algoritmos a medida que el número de términos de presentación varía de 6 a 48. Observamos que el rendimiento de TFB es más susceptible a la reducción de términos de presentación que el de CFB o TCFB. Por ejemplo, en 12 términos el MAP de TFB3C es el 90.6% del que se obtiene en 48 términos, mientras que los números para CFB3C y TCFB3C son del 98.0% y 96.1% respectivamente. Conjeturamos que la razón es que mientras el rendimiento de TFB depende en gran medida de cuántos términos buenos se elijan para la expansión de la consulta, CFB solo necesita una estimación aproximada de los pesos de los clústeres para funcionar. Además, las formas de aclaración de 3 × 16 parecen ser más robustas que las de 6 × 8: con 12 términos, el MAP de TFB6C es el 87.1% del obtenido con 48 términos, por debajo del 90.6% de TFB3C. De manera similar, para CFB es del 95.0% en comparación con el 98.0%. Esto es natural, ya que para un gran número de 6 clusters, es más fácil llegar a la situación en la que cada cluster recibe muy pocos términos de presentación para que la diversificación de temas sea útil. En general, nos sorprende ver que los algoritmos aún pueden funcionar razonablemente bien cuando el número de términos de presentación es pequeño. Por ejemplo, con solo 12 términos, CFB3C (el formulario de aclaración es de tamaño 3 × 4) aún puede mejorar un 36.5% sobre el valor base, disminuyendo ligeramente desde el 39.3% con 48 términos. En el Análisis de Retroalimentación del Usuario, estudiamos varios aspectos del comportamiento de retroalimentación de los usuarios en términos, y si están conectados al rendimiento de recuperación. Figura 2: Distribuciones de tiempo de completación del formulario de aclaración 0−30 30−60 60−90 90−120 120−150 150−180 0 5 10 15 20 25 30 35 tiempo de completación (segundos) #temas 1×48 3×16 6×8 La Figura 2 muestra la distribución del tiempo necesario para completar un formulario de aclaración. Observamos que el usuario suele ser capaz de completar el feedback del término en un tiempo razonablemente corto: para más de la mitad de los temas, el formulario de aclaración se completa en solo 1 minuto, y solo una pequeña fracción de temas (menos del 10% para 1 × 48 y 3 × 16) tarda más de 2 minutos. Esto sugiere que la retroalimentación a corto plazo es adecuada para la recuperación interactiva ad-hoc, donde un usuario generalmente no desea dedicar demasiado tiempo a proporcionar retroalimentación. Observamos que un usuario a menudo comete errores al juzgar la relevancia de un término. A veces, un término relevante puede ser omitido porque su conexión con el tema de la consulta no es obvia para el usuario. En otras ocasiones, puede incluirse un término dudoso que resulta ser irrelevante. Toma como ejemplo el tema en la Figura 1. Hubo un desastre de incendio en Mont 3. El tiempo máximo es de 180 segundos, ya que el evaluador del NIST estaría obligado a enviar el formulario en ese momento. Tabla 4: Estadísticas de selección de términos (promedio del tema) CF Tipo 1 × 48 3 × 16 6 × 8 # términos revisados 14.8 13.3 11.2 # términos relevantes 15.0 12.6 11.2 # términos relevantes revisados 7.9 6.9 5.9 precisión 0.534 0.519 0.527 recuperación 0.526 0.548 0.527 El usuario no seleccionó palabras clave como mont, blanc, francés e italiano debido a su desconocimiento del evento del Túnel del Mont Blanc entre Francia e Italia en 1999. De hecho, sin un contexto adecuado sería difícil hacer un juicio perfecto. ¿Cuál es entonces, el nivel en el que el usuario es bueno dando retroalimentación? ¿Tiene un impacto serio en el rendimiento de recuperación? Para responder a estas preguntas, necesitamos una medida de la verdadera relevancia de los términos individuales. Adoptamos la métrica de Divergencia KL Simplificada utilizada en [24] para decidir los términos de expansión de la consulta como nuestra medida de relevancia de términos: σKLD(w) = p(w|R) log p(w|R) p(w|¬R) donde p(w|R) es la probabilidad de que un documento relevante contenga el término w, y p(w|¬R) es la probabilidad de que un documento irrelevante contenga w, ambas pueden ser fácilmente calculadas mediante una estimación de máxima verosimilitud dada la relevancia a nivel de documento. Si σKLD(w) > 0, w es más probable que aparezca en documentos relevantes que en irrelevantes. Consideramos un término relevante si su valor de Divergencia KL Simplificada es mayor que un umbral determinado σ0. Podemos entonces definir la precisión y la exhaustividad del juicio de términos del usuario de la siguiente manera: la precisión es la fracción de términos revisados por el usuario que son relevantes; la exhaustividad es la fracción de términos relevantes presentados que son revisados por el usuario. La Tabla 4 muestra el número de términos revisados, términos relevantes y términos relevantes revisados cuando σ0 se establece en 1.0, así como la precisión/recuperación del juicio de términos del usuario. Ten en cuenta que cuando los formularios de aclaración contienen más grupos, se revisan menos términos: 14.8 para 1 × 48, 13.3 para 3 × 16 y 11.2 para 6×8. El patrón similar se mantiene para los términos relevantes y los términos relevantes verificados. Parece haber un compromiso entre aumentar la diversidad de temas mediante el agrupamiento y perder términos adicionales relevantes: cuando hay más grupos, cada uno recibe menos términos para presentar, lo que puede perjudicar a un grupo relevante importante que contiene muchos términos relevantes. Por lo tanto, no siempre es útil tener más grupos, por ejemplo, TFB6C es en realidad peor que TFB1C. El hallazgo principal que podemos obtener de la Tabla 4 es que el usuario no es particularmente bueno identificando términos relevantes, lo cual coincide con el descubrimiento en [18]. En el caso de 3 formularios de aclaración de 16, el número promedio de términos marcados como relevantes por el usuario es de 13.3 por tema, y el número promedio de términos relevantes cuyo valor de σKLD excede 1.0 es de 12.6. El usuario es capaz de reconocer solo 6.9 de estos términos en promedio. De hecho, la precisión y la exhaustividad de los términos de retroalimentación de los usuarios (como se definió anteriormente) están lejos de ser perfectas. Por otro lado, si el usuario hubiera verificado correctamente todos esos términos relevantes, el rendimiento de nuestros algoritmos habría aumentado considerablemente, como se muestra en la Tabla 5. Vemos que TFB experimenta una gran mejora cuando hay un oráculo que verifica todos los términos relevantes, mientras que CFB se encuentra con un cuello de botella alrededor de un MAP de 0.325, ya que todo lo que hace es ajustar los pesos de los clústeres, y cuando los pesos aprendidos están cerca de ser precisos, no puede beneficiarse más del feedback de términos. También hay que tener en cuenta que TCFB no logra superar a TFB, probablemente porque TFB es lo suficientemente preciso. Tabla 5: Cambio de MAP al usar todos (y solo) los términos relevantes (σKLD > 1.0) para retroalimentación. retroalimentación de término original retroalimentación de término relevante TF1 0.288 0.354 TF3 0.288 0.354 TF6 0.278 0.346 CF3 0.305 0.325 CF6 0.301 0.326 TCF3 0.309 0.345 TCF6 0.304 0.341 6.4 Comparación con Retroalimentación de Relevancia Ahora comparamos la retroalimentación de términos con la retroalimentación de relevancia a nivel de documento, en la que al usuario se le presentan los N mejores documentos de una recuperación inicial y se le pide que juzgue su relevancia. El proceso de retroalimentación se simula utilizando la evaluación de relevancia de documentos de NIST. Utilizamos el método de retroalimentación basado en el modelo de mezcla propuesto en [25], con el ruido de mezcla establecido en 0.95 y el coeficiente de retroalimentación establecido en 0.9. La evaluación comparativa del feedback de relevancia frente a otros métodos se complica por el hecho de que algunos documentos ya han sido vistos durante el feedback, por lo que no tiene sentido incluirlos en los resultados de recuperación de la segunda ejecución. Sin embargo, esto no se aplica al término retroalimentación. Por lo tanto, para que sea justo en cuanto a la ganancia de información de los usuarios, si los documentos de retroalimentación son relevantes, deben mantenerse en la parte superior del ranking; si son irrelevantes, deben ser excluidos. Por lo tanto, utilizamos retroalimentación de relevancia para producir un ranking de los 1000 documentos recuperados, pero excluyendo cada documento de retroalimentación, y luego añadimos los documentos relevantes de retroalimentación al principio. La tabla 6 muestra el rendimiento de la retroalimentación de relevancia para diferentes valores de N y lo compara con TCFB3C. Tabla 6: Rendimiento de la retroalimentación de relevancia para diferentes números de documentos de retroalimentación (N). Vemos que el rendimiento de TCFB3C es comparable al de la retroalimentación de relevancia utilizando 5 documentos. Aunque es menos eficiente que cuando hay 10 documentos de retroalimentación en términos de MAP y Pr@30, sí recupera más documentos (4947) al descender por la lista clasificada. Intentamos comparar la calidad de los términos insertados automáticamente en la retroalimentación de relevancia con la de los términos seleccionados manualmente en la retroalimentación de términos. Esto se logra truncando el modelo de consulta modificado por retroalimentación de relevancia a un tamaño igual al número de términos verificados para el mismo tema. Podemos comparar los términos en el modelo truncado con los términos verificados. La Figura 3 muestra la distribución de los puntajes σKLD de los términos. Observamos que el término \"feedback\" tiende a producir términos de expansión de mayor calidad (aquellos con σKLD > 1) en comparación con el \"relevance feedback\" (con 10 documentos de feedback). Esto no contradice el hecho de que este último produce un rendimiento de recuperación más alto. De hecho, cuando usamos el modelo de consulta truncada en lugar del modelo íntegro refinado a partir de la retroalimentación de relevancia, el MAP es solo 0.304. La verdad Figura 3: Comparación de la calidad del término de expansión entre retroalimentación de relevancia (con 10 documentos de retroalimentación) y retroalimentación de términos (con 3 × 16 CFs) −1−0 0−1 1−2 2−3 3−4 4−5 5−6 0 50 100 150 200 250 300 350 σKLD #términos retroalimentación de relevancia retroalimentación de términos es que, aunque hay muchos términos no deseados en el modelo de consulta expandido a partir de los documentos de retroalimentación, también hay más términos relevantes de los que el usuario puede seleccionar de la lista de términos de presentación generados con documentos de pseudo-retroalimentación, y los efectos positivos a menudo superan a los negativos. Estamos interesados en saber en qué circunstancias la retroalimentación basada en términos tiene ventajas sobre la retroalimentación basada en relevancia. Una de esas situaciones es cuando ninguno de los documentos de retroalimentación principales es relevante, lo que hace que la retroalimentación de relevancia sea inútil. Esto no es infrecuente, como uno podría haber pensado: de los 50 temas, hay 13 casos así cuando N = 5, 10 cuando N = 10, y aún 3 cuando N = 20. Cuando esto sucede, solo se puede retroceder al método original de recuperación; se pierde el poder de la retroalimentación de relevancia. Sorprendentemente, en 11 de 13 casos en los que parece imposible el feedback de relevancia, el usuario puede marcar al menos 2 términos relevantes de los 3 × 16 del formulario de aclaración (consideramos que el término t es relevante si σKLD(t) > 1.0). Además, en 10 de ellos TCFB3C supera la línea base de pseudo-retroalimentación, aumentando el MAP de 0.076 a 0.146 en promedio (estos son temas particularmente difíciles). Creemos que hay dos posibles explicaciones para este fenómeno de que la retroalimentación de términos esté activa incluso cuando la retroalimentación de relevancia no funciona: Primero, incluso si ninguno de los primeros N (supongamos que es un número pequeño) documentos son relevantes, aún podemos encontrar documentos relevantes en los primeros 60, que son más inclusivos pero generalmente inalcanzables cuando las personas están realizando retroalimentación de relevancia en la búsqueda interactiva ad-hoc, de donde podemos extraer términos de retroalimentación. Esto es cierto para el tema 367 de piratería, donde los 10 documentos principales de retroalimentación son todos sobre piratería de software, sin embargo, hay documentos entre el 10 y el 60 que tratan sobre piratería en los mares (que es la necesidad de información real), contribuyendo términos como pirata, barco para la selección en el formulario de aclaración. Segundo, para algunos temas, un documento debe cumplir con ciertas condiciones especiales para ser relevante. Los N documentos principales pueden estar relacionados con el tema, pero no obstante ser irrelevantes. En este caso, aún podemos extraer términos útiles de estos documentos, incluso si no califican como relevantes. Por ejemplo, en el tema 639 de compras en línea de consumidores, un documento necesita mencionar qué contribuye al crecimiento de las compras para realmente coincidir con la necesidad de información especificada, por lo tanto, ninguno de los 10 documentos de retroalimentación principales se considera relevante. Sin embargo, los términos de retroalimentación como venta al por menor, comercio son buenos para la expansión de consultas. 7. CONCLUSIONES En este artículo estudiamos el uso de la retroalimentación de términos para la recuperación interactiva de información en el enfoque de modelado del lenguaje. Propusimos un método basado en clusters para seleccionar términos de presentación, así como algoritmos para estimar modelos de consulta refinados a partir de la retroalimentación de términos de usuario. Observamos una mejora significativa en la precisión de recuperación proporcionada por la retroalimentación de términos, a pesar de que un usuario a menudo comete errores en la evaluación de relevancia que perjudican su rendimiento. Encontramos que el algoritmo de mejor rendimiento es TCFB, el cual se beneficia de la combinación de la evidencia de términos observados directamente con TFB y la relevancia de clúster aprendida indirectamente con CFB. Cuando redujimos el número de términos de presentación, el feedback del término aún puede mantener gran parte de su mejora de rendimiento sobre el valor base. Finalmente, comparamos la retroalimentación de términos con la retroalimentación de relevancia a nivel de documento, y encontramos que el rendimiento de TCFB3C está a la par con este último con 5 documentos de retroalimentación. Consideramos el término \"feedback\" como una alternativa viable al feedback de relevancia tradicional, especialmente cuando no hay documentos relevantes en la parte superior. Proponemos ampliar nuestro trabajo de varias maneras. Primero, queremos estudiar si el uso de varios contextos puede ayudar al usuario a identificar mejor la relevancia de los términos, sin sacrificar la simplicidad y la concisión de la retroalimentación de los términos. Segundo, actualmente todos los términos se presentan al usuario en un solo lote. En su lugar, podríamos considerar la retroalimentación iterativa de términos, presentando primero un pequeño número de términos y mostrando más términos después de recibir la retroalimentación del usuario o detenernos cuando la consulta refinada sea lo suficientemente buena. Los términos presentados deben ser seleccionados dinámicamente para maximizar los beneficios de aprendizaje en cualquier momento. Tercero, tenemos planes de incorporar retroalimentación de términos en nuestra barra de herramientas UCAIR[20], un complemento de Internet Explorer, para que funcione en búsquedas web. También estamos interesados en estudiar cómo combinar la retroalimentación de términos con la retroalimentación de relevancia o la retroalimentación implícita. Por ejemplo, podríamos permitir al usuario modificar dinámicamente los términos en un modelo de lenguaje aprendido a partir de documentos de retroalimentación. 8. AGRADECIMIENTO Este trabajo cuenta con el apoyo parcial de las becas de la Fundación Nacional de Ciencias IIS-0347933 e IIS-0428472. 9. REFERENCIAS [1] J. Allan. Retroalimentación de relevancia con demasiados datos. En Actas de la 18ª conferencia internacional anual de ACM SIGIR sobre investigación y desarrollo en recuperación de información, páginas 337-343, 1995. [2] J. Allan. Resumen de la pista HARD en TREC 2005 - Recuperación de alta precisión de documentos. En la Decimocuarta Conferencia de Recuperación de Información de 2005. [3] P. Anick. Utilizando retroalimentación terminológica para refinar la búsqueda web: un estudio basado en registros. En Actas de la 26ª conferencia internacional anual de ACM SIGIR sobre investigación y desarrollo en recuperación de información, páginas 88-95, 2003. [4] P. G. Anick y S. Tipirneni. El asistente de búsqueda de paráfrasis: retroalimentación terminológica para la búsqueda iterativa de información. En Actas de la 22ª conferencia internacional anual de ACM SIGIR sobre investigación y desarrollo en recuperación de información, páginas 153-159, 1999. [5] C. Buckley, G. Salton, J. Allan y A. Singhal. Expansión automática de consultas utilizando SMART. En Actas de la Tercera Conferencia de Recuperación de Texto, 1994. [6] D. Harman. Hacia la expansión interactiva de consultas. En Actas de la 11ª conferencia internacional anual de ACM SIGIR sobre investigación y desarrollo en recuperación de información, páginas 321-331, 1988. [7] N. A. Jaleel, A. Corrada-Emmanuel, Q. Li, X. Liu, C. Wade y J. Allan. UMass en TREC 2003: HARD y QA. En TREC, páginas 715-725, 2003. [8] H. Joho, C. Coverson, M. Sanderson y M. Beaulieu. Presentación jerárquica de términos de expansión. En Actas del simposio de informática aplicada de ACM de 2002, páginas 645-649, 2002. [9] K. S. Jones, S. Walker y S. E. Robertson. Un modelo probabilístico de recuperación de información: desarrollo y estado actual. Informe técnico 446, Laboratorio de Computación, Universidad de Cambridge, 1998. [10] D. Kelly, V. D. Dollu y X. Fu. El usuario locuaz: una fuente de términos independiente del documento para la expansión de consultas. En Actas de la 28ª conferencia internacional anual de ACM SIGIR sobre investigación y desarrollo en recuperación de información, páginas 457-464, 2005. [11] D. Kelly y X. Fu. Obtención de retroalimentación de relevancia de términos: una investigación sobre la fuente y el contexto de los términos. En Actas de la 29ª conferencia internacional anual de ACM SIGIR sobre investigación y desarrollo en recuperación de información, 2006. [12] J. Koenemann y N. Belkin. Un caso para la interacción: Un estudio del comportamiento y la efectividad de la recuperación de información interactiva. En Actas de la conferencia SIGCHI sobre factores humanos en sistemas informáticos, páginas 205-212, 1996. [13] V. Lavrenko y W. B. Croft. Modelos de lenguaje basados en relevancia. En Investigación y Desarrollo en Recuperación de Información, páginas 120-127, 2001. [14] Y. Nemeth, B. Shapira y M. Taeib-Maimon. Evaluación del valor real y percibido de la expansión automática e interactiva de consultas. En Actas de la 27ª conferencia internacional anual de ACM SIGIR sobre investigación y desarrollo en recuperación de información, páginas 526-527, 2004. [15] J. Ponte. Un enfoque de modelado del lenguaje para la recuperación de información. Tesis doctoral, Universidad de Massachusetts en Amherst, 1998. [16] S. E. Robertson, S. Walker, S. Jones, M. Beaulieu y M. Gatford. Okapi en TREC-3. En Actas de la Tercera Conferencia de Recuperación de Texto, 1994. [17] J. Rocchio. Retroalimentación de relevancia en la recuperación de información. En el sistema de recuperación SMART, páginas 313-323. 1971. [18] I. Ruthven. Reexaminando la efectividad potencial de la expansión interactiva de consultas. En Actas de la 26ª conferencia internacional anual de ACM SIGIR sobre investigación y desarrollo en recuperación de información, páginas 213-220, 2003. [19] G. Salton y C. Buckley. Mejorando el rendimiento de recuperación mediante retroalimentación de relevancia. Revista de la Sociedad Americana de Ciencia de la Información, 41:288-297, 1990. [20] X. Shen, B. Tan y C. Zhai. Modelado implícito de usuario para búsqueda personalizada. En Actas de la 14ª conferencia internacional de ACM sobre información y gestión del conocimiento, páginas 824-831, 2005. [21] X. Shen y C. Zhai. Retroalimentación activa en la recuperación de información ad-hoc. En Actas de la 28ª conferencia internacional anual de ACM SIGIR sobre investigación y desarrollo en recuperación de información, páginas 59-66, 2005. [22] A. Spink. Retroalimentación de relevancia de términos y expansión de consultas: relación con el diseño. En Actas de la 17ª conferencia internacional anual de ACM SIGIR sobre investigación y desarrollo en recuperación de información, páginas 81-90, 1994. [23] J. Xu y W. B. Croft. Expansión de consulta utilizando análisis local y global de documentos. En Actas de la 19ª conferencia internacional anual de ACM SIGIR sobre investigación y desarrollo en recuperación de información, páginas 4-11, 1996. [24] H. Zaragoza, N. Craswell, M. Taylor, S. Saria y S. Robertson. Microsoft Cambridge en TREC-13: pistas Web y HARD. En Actas de la 13ª Conferencia de Recuperación de Información de Texto, 2004. [25] C. Zhai y J. Lafferty. Retroalimentación basada en modelos en el enfoque de modelado del lenguaje para la recuperación de información. En Actas de la décima conferencia internacional sobre gestión de la información y el conocimiento, páginas 403-410, 2001. [26] C. Zhai, A. Velivelli y B. Yu. Un modelo de mezcla de colecciones cruzadas para la minería de textos comparativa. En Actas de la décima conferencia internacional de ACM SIGKDD sobre descubrimiento de conocimiento y minería de datos, páginas 743-748, 2004. ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "presentation term": {
            "translated_key": "",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Term Feedback for Information Retrieval with Language Models Bin Tan† , Atulya Velivelli‡ , Hui Fang† , ChengXiang Zhai† Dept.",
                "of Computer Science† , Dept. of Electrical and Computer Engineering‡ University of Illinois at Urbana-Champaign bintan@cs.uiuc.edu, velivell@ifp.uiuc.edu, hfang@cs.uiuc.edu, czhai@cs.uiuc.edu ABSTRACT In this paper we study term-based feedback for information retrieval in the language modeling approach.",
                "With term feedback a user directly judges the relevance of individual terms without interaction with feedback documents, taking full control of the query expansion process.",
                "We propose a cluster-based method for selecting terms to present to the user for judgment, as well as effective algorithms for constructing refined query language models from user term feedback.",
                "Our algorithms are shown to bring significant improvement in retrieval accuracy over a non-feedback baseline, and achieve comparable performance to relevance feedback.",
                "They are helpful even when there are no relevant documents in the top.",
                "Categories and Subject Descriptors H.3.3 [Information Search and Retrieval]: Retrieval models General Terms Algorithms 1.",
                "INTRODUCTION In the language modeling approach to information retrieval, feedback is often modeled as estimating an improved query model or relevance model based on a set of feedback documents [25, 13].",
                "This is in line with the traditional way of doing relevance feedback - presenting a user with documents/passages for relevance judgment and then extracting terms from the judged documents or passages to expand the initial query.",
                "It is an indirect way of seeking users assistance for query model construction, in the sense that the refined query model (based on terms) is learned through feedback documents/passages, which are high-level structures of terms.",
                "It has the disadvantage that irrelevant terms, which occur along with relevant ones in the judged content, may be erroneously used for query expansion, causing undesired effects.",
                "For example, for the TREC query Hubble telescope achievements, when a relevant document talks more about the telescopes repair than its discoveries, irrelevant terms such as spacewalk can be added into the modified query.",
                "We can consider a more direct way to involve a user in query model improvement, without an intermediary step of document feedback that can introduce noise.",
                "The idea is to present a (reasonable) number of individual terms to the user and ask him/her to judge the relevance of each term or directly specify their probabilities in the query model.",
                "This strategy has been discussed in [15], but to our knowledge, it has not been seriously studied in existing language modeling literature.",
                "Compared to traditional relevance feedback, this term-based approach to interactive query model refinement has several advantages.",
                "First, the user has better control of the final query model through direct manipulation of terms: he/she can dictate which terms are relevant, irrelevant, and possibly, to what degree.",
                "This avoids the risk of bringing unwanted terms into the query model, although sometimes the user introduces low-quality terms.",
                "Second, because a term takes less time to judge than a documents full text or summary, and as few as around 20 presented terms can bring significant improvement in retrieval performance (as we will show later), term feedback makes it faster to gather user feedback.",
                "This is especially helpful for interactive adhoc search.",
                "Third, sometimes there are no relevant documents in the top N of the initially retrieved results if the topic is hard.",
                "This is often true when N is constrained to be small, which arises from the fact that the user is unwilling to judge too many documents.",
                "In this case, relevance feedback is useless, as no relevant document can be leveraged on, but term feedback is still often helpful, by allowing relevant terms to be picked from irrelevant documents.",
                "During our participation in the TREC 2005 HARD Track and continued study afterward, we explored how to exploit term feedback from the user to construct improved query models for information retrieval in the language modeling approach.",
                "We identified two key subtasks of term-based feedback, i.e., pre-feedback <br>presentation term</br> selection and post-feedback query model construction, with effective algorithms developed for both.",
                "We imposed a secondary cluster structure on terms and found that a cluster view sheds additional insight into the users information need, and provides a good way of utilizing term feedback.",
                "Through experiments we found that term feedback improves significantly over the nonfeedback baseline, even though the user often makes mistakes in relevance judgment.",
                "Among our algorithms, the one with best retrieval performance is TCFB, the combination of TFB, the direct term feedback algorithm, and CFB, the cluster-based feedback algorithm.",
                "We also varied the number of feedback terms and observed reasonable improvement even at low numbers.",
                "Finally, by comparing term feedback with document-level feedback, we found it to be a viable alternative to the latter with competitive retrieval performance.",
                "The rest of the paper is organized as follows.",
                "Section 2 discusses some related work.",
                "Section 4 outlines our general approach to term feedback.",
                "We present our method for <br>presentation term</br> selection in Section 3 and algorithms for query model construction in Section 5.",
                "The experiment results are given in Section 6.",
                "Section 7 concludes this paper. 2.",
                "RELATED WORK Relevance feedback[17, 19] has long been recognized as an effective method for improving retrieval performance.",
                "Normally, the top N documents retrieved using the original query are presented to the user for judgment, after which terms are extracted from the judged relevant documents, weighted by their potential of attracting more relevant documents, and added into the query model.",
                "The expanded query usually represents the users information need better than the original one, which is often just a short keyword query.",
                "A second iteration of retrieval using this modified query usually produces significant increase in retrieval accuracy.",
                "In cases where true relevance judgment is unavailable and all top N documents are assumed to be relevant, it is called blind or pseudo feedback[5, 16] and usually still brings performance improvement.",
                "Because document is a large text unit, when it is used for relevance feedback many irrelevant terms can be introduced into the feedback process.",
                "To overcome this, passage feedback is proposed and shown to improve feedback performance[1, 23].",
                "A more direct solution is to ask the user for their relevance judgment of feedback terms.",
                "For example, in some relevance feedback systems such as [12], there is an interaction step that allows the user to add or remove expansion terms after they are automatically extracted from relevant documents.",
                "This is categorized as interactive query expansion, where the original query is augmented with user-provided terms, which can come from direct user input (free-form text or keywords)[22, 7, 10] or user selection of system-suggested terms (using thesauri[6, 22] or extracted from feedback documents[6, 22, 12, 4, 7]).",
                "In many cases term relevance feedback has been found to effectively improve retrieval performance[6, 22, 12, 4, 10].",
                "For example, the study in [12] shows that the user prefers to have explicit knowledge and direct control of which terms are used for query expansion, and the penetrable interface that provides this freedom is shown to perform better than other interfaces.",
                "However, in some other cases there is no significant benefit[3, 14], even if the user likes interacting with expansion terms.",
                "In a simulated study carried out in [18], the author compares the retrieval performance of interactive query expansion and automatic query expansion with a simulated study, and suggests that the potential benefits of the former can be hard to achieve.",
                "The user is found to be not good at identifying useful terms for query expansion, when a simple term presentation interface is unable to provide sufficient semantic context of the feedback terms.",
                "Our work differs from the previous ones in two important aspects.",
                "First, when we choose terms to present to the user for relevance judgment, we not only consider single-term value (e.g., the relative frequency of a term in the top documents, which can be measured by metrics such as Robertson Selection Value and Simplified Kullback-Leibler Distance as listed in [24]), but also examine the cluster structure of the terms, so as to produce a balanced coverage of the different topic aspects.",
                "Second, with the language modelling framework, we allow an elaborate construction of the updated query model, by setting different probabilities for different terms based on whether it is a query term, its significance in the top documents, and its cluster membership.",
                "Although techniques for adjusting query term weights exist for vector space models[17] and probablistic relevance models[9], most of the aforementioned works do not use them, choosing to just append feedback terms to the original query (thus using equal weights for them), which can lead to poorer retrieval performance.",
                "The combination of the two aspects allows our method to perform much better than the baseline.",
                "The usual way for feedback term presentation is just to display the terms in a list.",
                "There have been some works on alternative user interfaces. [8] arranges terms in a hierarchy, and [11] compares three different interfaces, including terms + checkboxes, terms + context (sentences) + checkboxes, sentences + input text box.",
                "In both studies, however, there is no significant performance difference.",
                "In our work we adopt the simplest approach of terms + checkboxes.",
                "We focus on term presentation and query model construction from feedback terms, and believe using contexts to improve feedback term quality should be orthogonal to our method. 3.",
                "GENERAL APPROACH We follow the language modeling approach, and base our method on the KL-divergence retrieval model proposed in [25].",
                "With this model, the retrieval task involves estimating a query language model θq from a given query, a document language model θd from each document, and calculating their KL-divergence D(θq||θd), which is then used to score the documents. [25] treats relevance feedback as a query model re-estimation problem, i.e., computing an updated query model θq given the original query text and the extra evidence carried by the judged relevant documents.",
                "We adopt this view, and cast our task as updating the query model from user term feedback.",
                "There are two key subtasks here: First, how to choose the best terms to present to the user for judgment, in order to gather maximal evidence about the users information need.",
                "Second, how to compute an updated query model based on this term feedback evidence, so that it captures the users information need and translates into good retrieval performance. 4.",
                "<br>presentation term</br> SELECTION Proper selection of terms to be presented to the user for judgment is crucial to the success of term feedback.",
                "If the terms are poorly chosen and there are few relevant ones, the user will have a hard time looking for useful terms to help clarify his/her information need.",
                "If the relevant terms are plentiful, but all concentrate on a single aspect of the query topic, then we will only be able to get feedback on that aspect and missing others, resulting in a breadth loss in retrieved results.",
                "Therefore, it is important to carefully select presentation terms to maximize expected gain from user feedback, i.e., those that can potentially reveal most evidence of the users information need.",
                "This is similar to active feedback[21], which suggests that a retrieval system should actively probe the users information need, and in the case of relevance feedback, the feedback documents should be chosen to maximize learning benefits (e.g. diversely so as to increase coverage).",
                "In our approach, the top N documents from an initial retrieval using the original query form the source of feedback terms: all terms that appear in them are considered candidates to present to the user.",
                "These documents serve as pseudo-feedback, since they provide a much richer context than the original query (usually very short), while the user is not asked to judge their relevance.",
                "Due to the latter reason, it is possible to make N quite large (e.g., in our experiments we set N = 60) to increase its coverage of different aspects in the topic.",
                "The simplest way of selecting feedback terms is to choose the most frequent M terms from the N documents.",
                "This method, however, has two drawbacks.",
                "First, a lot of common noisy terms will be selected due to their high frequencies in the document collection, unless a stop-word list is used for filtering.",
                "Second, the presentation list will tend to be filled by terms from major aspects of the topic; those from a minor aspect are likely to be missed due to their relatively low frequencies.",
                "We solve the above problems by two corresponding measures.",
                "First, we introduce a background model θB that is estimated from collection statistics and explains the common terms, so that they are much less likely to appear in the presentation list.",
                "Second, the terms are selected from multiple clusters in the pseudo-feedback documents, to ensure sufficient representation of different aspects of the topic.",
                "We rely on the mixture multinomial model, which is used for theme discovery in [26].",
                "Specifically, we assume the N documents contain K clusters {Ci| i = 1, 2, · · · K}, each characterized by a multinomial word distribution (also known as unigram language model) θi and corresponding to an aspect of the topic.",
                "The documents are regarded as sampled from a mixture of K + 1 components, including the K clusters and the background model: p(w|d) = λBp(w|θB) + (1 − λB) K i=1 πd,ip(w|θi) where w is a word, λB is the mixture weight for the background model θB, and πd,i is the document-specific mixture weight for the i-th cluster model θi.",
                "We then estimate the cluster models by maximizing the probability of the pseudo-feedback documents being generated from the multinomial mixture model: log p(D|Λ) = d∈D w∈V c(w; d) log p(w|d) where D = {di| i = 1, 2, · · · N} is the set of the N documents, V is the vocabulary, c(w; d) is ws frequency in d and Λ = {θi| i = 1, 2, · · · K} ∪ {πdij | i = 1, 2, · · · N, j = 1, 2, · · · K} is the set of model parameters to estimate.",
                "The cluster models can be efficiently estimated using the Expectation-Maximization (EM) algorithm.",
                "For its details, we refer the reader to [26].",
                "Table 1 shows the cluster models for TREC query Transportation tunnel disasters (K = 3).",
                "Note that only the middle cluster is relevant.",
                "Table 1: Cluster models for topic 363 Transportation tunnel disasters Cluster 1 Cluster 2 Cluster 3 tunnel 0.0768 tunnel 0.0935 tunnel 0.0454 transport 0.0364 fire 0.0295 transport 0.0406 traffic 0.0206 truck 0.0236 toll 0.0166 railwai 0.0186 french 0.0220 amtrak 0.0153 harbor 0.0146 smoke 0.0157 train 0.0129 rail 0.0140 car 0.0154 airport 0.0122 bridg 0.0139 italian 0.0152 turnpik 0.0105 kilomet 0.0136 firefight 0.0144 lui 0.0095 truck 0.0133 blaze 0.0127 jersei 0.0093 construct 0.0131 blanc 0.0121 pass 0.0087 · · · · · · · · · From each of the K estimated clusters, we choose the L = M/K terms with highest probabilities to form a total of M presentation terms.",
                "If a term happens to be in top L in multiple clusters, we assign it to the cluster where it has highest probability and let the other clusters take one more term as compensation.",
                "We also filter out terms in the original query text because they tend to always be relevant when the query is short.",
                "The selected terms are then presented to the user for judgment.",
                "A sample (completed) feedback form is shown in Figure 1.",
                "In this study we only deal with binary judgment: a presented term is by default unchecked, and a user may check it to indicate relevance.",
                "We also do not explicitly exploit negative feedback (i.e., penalizing irrelevant terms), because with binary feedback an unchecked term is not necessarily irrelevant (maybe the user is unsure about its relevance).",
                "We could ask the user for finer judgment (e.g., choosing from highly relevant, somewhat relevant, do not know, somewhat irrelevant and highly irrelevant), but binary feedback is more compact, taking less space to display and less user effort to make judgment. 5.",
                "ESTIMATING QUERY MODELS FROM TERM FEEDBACK In this section, we present several algorithms for exploiting term feedback.",
                "The algorithms take as input the original query q, the clusters {θi} as generated by the theme discovery algorithm, the set of feedback terms T and their relevance judgment R, and outputs an updated query language model θq that makes best use of the feedback evidence to capture the users information need.",
                "First we describe our notations: • θq: The original query model, derived from query terms only: p(w|θq) = c(w; q) |q| where c(w; q) is the count of w in q, and |q| = w∈q c(w; q) is the query length. • θq : The updated query model which we need to estimate from term feedback. • θi (i = 1, 2, . . .",
                "K): The unigram language model of cluster Ci, as estimated using the theme discovery algorithm. • T = {ti,j} (i = 1 . . .",
                "K, j = 1 . . .",
                "L): The set of terms presented to the user for judgment. ti,j is the j-th term chosen from cluster Ci. • R = {δw|w ∈ T}: δw is an indicator variable that is 1 if w is judged relevant or 0 otherwise. 5.1 TFB (Direct Term Feedback) This is a straight-forward form of term feedback that does not involve any secondary structure.",
                "We give a weight of 1 to terms judged relevant by the user, a weight of μ to query terms, zero weight to other terms, and then apply normalization: p(w|θq ) = δw + μ c(w; q) w ∈T δw + μ|q| where w ∈T δw is the total number of terms that are judged relevant.",
                "We call this method TFB (direct Term FeedBack).",
                "If we let μ = 1, this approach is equivalent to appending the relevant terms after the original query, which is what standard query expansion (without term reweighting) does.",
                "If we set μ > 1, we are putting more emphasis on the query terms than the checked ones.",
                "Note that the result model will be more biased toward θq if the original query is long or the user feedback is weak, which makes sense, as we can trust more on the original query in either case.",
                "Figure 1: Filled clarification form for Topic 363 363 transportation tunnel disasters Please select all terms that are relevant to the topic. traffic railway harbor rail bridge kilometer construct swiss cross link kong hong river project meter shanghai fire truck french smoke car italian firefights blaze blanc mont victim franc rescue driver chamonix emerge toll amtrak train airport turnpike lui jersey pass rome z center electron road boston speed bu submit 5.2 CFB (Cluster Feedback) Here we exploit the cluster structure that played an important role when we selected the presentation terms.",
                "The clusters represent different aspects of the query topic, each of which may or may not be relevant.",
                "If we are able to identify the relevant clusters, we can combine them to generate a query model that is good at discovering documents belonging to these clusters (instead of the irrelevant ones).",
                "We could ask the user to directly judge the relevance of a cluster after viewing representative terms in that cluster, but this would sometimes be a difficult task for the user, who has to guess the semantics of a cluster via its set of terms, which may not be well connected to one another due to a lack of context.",
                "Therefore, we propose to learn cluster feedback indirectly, inferring the relevance of a cluster through the relevance of its feedback terms.",
                "Because each cluster has an equal number of terms presented to the user, the simplest measure of a clusters relevance is the number of terms that are judged relevant in it.",
                "Intuitively, the more terms are marked relevant in a cluster, the closer the cluster is to the query topic, and the more the cluster should participate in query modification.",
                "If we combine the cluster models using weights determined this way and then interpolate with the original query model, we get the following formula for query updating, which we call CFB (Cluster FeedBack): p(w|θq ) = λp(w|θq) + (1 − λ) K i=1 L j=1 δti,j K k=1 L j=1 δtk,j p(w|θi) where L j=1 δti,j is the number of relevant terms in cluster Ci, and K k=1 L j=1 δtk,j is the total number of relevant terms.",
                "We note that when there is only one cluster (K = 1), the above formula degenerates to p(w|θq ) = λp(w|θq) + (1 − λ)p(w|θ1) which is merely pseudo-feedback of the form proposed in [25]. 5.3 TCFB (Term-cluster Feedback) TFB and CFB both have their drawbacks.",
                "TFB assigns non-zero probabilities to the presented terms that are marked relevant, but completely ignores (a lot more) others, which may be left unchecked due to the users ignorance, or simply not included in the presentation list, but we should be able to infer their relevance from the checked ones.",
                "For example, in Figure 1, since as many as 5 terms in the middle cluster (the third and fourth columns) are checked, we should have high confidence in the relevance of other terms in that cluster.",
                "CFB remedies TFBs problem by treating the terms in a cluster collectively, so that unchecked/unpresented terms receive weights when presented terms in their clusters are judged as relevant, but it does not distinguish which terms in a cluster are presented or judged.",
                "Intuitively, the judged relevant terms should receive larger weights because they are explicitly indicated as relevant by the user.",
                "Therefore, we try to combine the two methods, hoping to get the best out of both.",
                "We do this by interpolating the TFB model with the CFB model, and call it TCFB: p(w|θq ) = αp(w|θqT F B ) + (1 − α)p(w|θqCF B ) 6.",
                "EXPERIMENTS In this section, we describe our experiment results.",
                "We first describe our experiment setup and present an overview of various methods performance.",
                "Then we discuss the effects of varying the parameter setting in the algorithms, as well as the number of presentation terms.",
                "Next we analyze user term feedback behavior and its relation to retrieval performance.",
                "Finally we compare term feedback to relevance feedback and show that it has its particular advantage. 6.1 Experiment Setup and Basic Results We took the opportunity of TREC 2005 HARD Track[2] for the evaluation of our algorithms.",
                "The tracks used the AQUAINT collection, a 3GB corpus of English newswire text.",
                "The topics included 50 ones previously known to be hard, i.e. with low retrieval performance.",
                "It is for these hard topics that user feedback is most helpful, as it can provide information to disambiguate the queries; with easy topics the user may be unwilling to spend efforts for feedback if the automatic retrieval results are good enough.",
                "Participants of the track were able to submit custom-designed clarification forms (CF) to solicit feedback from human assessors provided by Table 2: Retrieval performance for different methods and CF types.",
                "The last row is the percentage of MAP improvement over the baseline.",
                "The parameter settings μ = 4, λ = 0.1, α = 0.3 are near optimal.",
                "Baseline TFB1C TFB3C TFB6C CFB1C CFB3C CFB6C TCFB1C TCFB3C TCFB6C MAP 0.219 0.288 0.288 0.278 0.254 0.305 0.301 0.274 0.309 0.304 Pr@30 0.393 0.467 0.475 0.457 0.399 0.480 0.473 0.431 0.491 0.473 RR 4339 4753 4762 4740 4600 4907 4872 4767 4947 4906 % 0% 31.5% 31.5% 26.9% 16.0% 39.3% 37.4% 25.1% 41.1% 38.8% Table 3: MAP variation with the number of presented terms. # terms TFB1C TFB3C TFB6C CFB3C CFB6C TCFB3C TCFB6C 6 0.245 0.240 0.227 0.279 0.279 0.281 0.274 12 0.261 0.261 0.242 0.299 0.286 0.297 0.281 18 0.275 0.274 0.256 0.301 0.282 0.300 0.286 24 0.276 0.281 0.265 0.303 0.292 0.305 0.292 30 0.280 0.285 0.270 0.304 0.296 0.307 0.296 36 0.282 0.288 0.272 0.307 0.297 0.309 0.297 42 0.283 0.288 0.275 0.306 0.298 0.309 0.300 48 0.288 0.288 0.278 0.305 0.301 0.309 0.303 NIST.",
                "We designed three sets of clarification forms for term feedback, differing in the choice of K, the number of clusters, and L, the number of presented terms from each cluster.",
                "They are: 1× 48, a big cluster with 48 terms, 3 × 16, 3 clusters with 16 terms each, and 6 × 8, 6 clusters with 8 terms each.",
                "The total number of presented terms (M) is fixed at 48, so by comparing the performance of different types of clarification forms we can know the effects of different degree of clustering.",
                "For each topic, an assessor would complete the forms ordered by 6 × 8, 1 × 48 and 3 × 16, spending up to three minutes on each form.",
                "The sample clarification form shown in Figure 1 is of type 3 × 16.",
                "It is a simple and compact interface in which the user can check relevant terms.",
                "The form is self-explanatory; there is no need for extra user training on how to use it.",
                "Our initinal queries are constructed only using the topic title descriptions, which are on average 2.7 words in length.",
                "As our baseline we use the KL divergence retrieval method implemented in the Lemur Toolkit1 with 5 pseudo-feedback documents.",
                "We stem the terms, choose Dirichlet smoothing with a prior of 2000, and truncate query language models to 50 terms (these settings are used throughout the experiments).",
                "For all other parameters we use Lemurs default settings.",
                "The baseline turns out to perform above average among the track participants.",
                "After an initial run using this baseline retrieval method, we take the top 60 documents for each topic and apply the theme discovery algorithm to output the clusters (1, 3, or 6 of them), based on which we generate clarification forms.",
                "After user feedback is received, we run the term feedback algorithms (TFB, CFB or TCFB) to estimate updated query models, which are then used for a second iteration of retrieval.",
                "We evaluate the different retrieval methods performance on their rankings of the top 1000 documents.",
                "The evaluation metrics we adopt include mean average (non-interpolated) precision (MAP), precision at top 30 (Pr@30) and total relevant retrieved (RR).",
                "Table 2 shows the performance of various methods and configurations of K × L. The suffixes (1C, 3C, 6C) after TFB,CFB,TCFB stand for the number of clusters (K).",
                "For example, TCFB3C means the TCFB method on the 3 × 16 clarification forms.",
                "From Table 2 we can make the following observations: 1 http://www.lemurproject.com 1.",
                "All methods perform considerably better than the pseudofeedback baseline, with TCFB3C achieving a highest 41.1% improvement in MAP, indicating significant contribution of term feedback for clarification of the users information need.",
                "In other words, term feedback is truly helpful for improving retrieval accuracy. 2.",
                "For TFB, the performance is almost equal on the 1 × 48 and 3 × 16 clarification forms in terms of MAP (although the latter is slightly better in Pr@30 and RR), and a little worse on the 6 × 8 ones. 3.",
                "Both CFB3C and CFB6C perform better than their TFB counterparts in all three metrics, suggesting that feedback on a secondary cluster structure is indeed beneficial.",
                "CFB1C is actually worse because it cannot adjust the weight of its (single) cluster from term feedback and it is merely pseudofeedback. 4.",
                "Although TCFB is just a simple mixture of TFB and CFB by interpolation, it is able to outperform both.",
                "This supports our speculation that TCFB overcomes the drawbacks of TFB (paying attention only to checked terms) and CFB (not distinguishing checked and unchecked terms in a cluster).",
                "Except for TCFB6C v.s.",
                "CFB6C, the performance advantage of TCFB over TFB/CFB is significant at p < 0.05 using the Wilcoxon signed rank test.",
                "This is not true in the case of TFB v.s.",
                "CFB, each of which is better than the other in nearly half of the topics. 6.2 Reduction of Presentation Terms In some situations we may have to reduce the number of presentation terms due to limits in display space or user feedback efforts.",
                "It is interesting to know whether our algorithms performance deteriorates when the user is presented with fewer terms.",
                "Because the presentation terms within each cluster are generated in decreasing order of their frequencies, the presentation list forms a subset of the original one if its size is reduced2 .",
                "Therefore, we can easily simulate what happens when the number of presentation terms decreases 2 There are complexities arising from terms appearing in top L of multiple clusters, but these are exceptions from M to M : we will keep all judgments of the top L = M /K terms in each cluster and discard those of others.",
                "Table 3 shows the performance of various algorithms as the number of presentation terms ranges from 6 to 48.",
                "We find that the performance of TFB is more susceptible to <br>presentation term</br> reduction than that of CFB or TCFB.",
                "For example, at 12 terms the MAP of TFB3C is 90.6% of that at 48 terms, while the numbers for CFB3C and TCFB3C are 98.0% and 96.1% respectively.",
                "We conjecture the reason to be that while TFBs performance heavily depends on how many good terms are chosen for query expansion, CFB only needs a rough estimate of cluster weights to work.",
                "Also, the 3 × 16 clarification forms seem to be more robust than the 6 × 8 ones: at 12 terms the MAP of TFB6C is 87.1% of that at 48 terms, lower than 90.6% for TFB3C.",
                "Similarly, for CFB it is 95.0% against 98.0%.",
                "This is natual, as for a large cluster number of 6, it is easier to get into the situation where each cluster gets too few presentation terms to make topic diversification useful.",
                "Overall, we are surprised to see that the algorithms are still able to perform reasonably well when the number of presentation terms is small.",
                "For example, at only 12 terms CFB3C (the clarification form is of size 3 × 4) can still improve 36.5% over the baseline, dropping slightly from 39.3% at 48 terms. 6.3 User Feedback Analysis In this part we study several aspects of users term feedback behavior, and whether they are connected to retrieval performance.",
                "Figure 2: Clarification form completion time distributions 0−30 30−60 60−90 90−120 120−150 150−180 0 5 10 15 20 25 30 35 completion time (seconds) #topics 1×48 3×16 6×8 Figure 2 shows the distribution of time needed to complete a clarification form3 .",
                "We see that the user is usually able to finish term feedback within a reasonably short amount of time: for more than half of the topics the clarification form is completed in just 1 minute, and only a small fraction of topics (less than 10% for 1 × 48 and 3 × 16) take more than 2 minutes.",
                "This suggests that term feedback is suitable for interactive ad-hoc retrieval, where a user usually does not want to spend too much time on providing feedback.",
                "We find that a user often makes mistakes when judging term relevance.",
                "Sometimes a relevant term may be left out because its connection to the query topic is not obvious to the user.",
                "Other times a dubious term may be included but turns out to be irrelevant.",
                "Take the topic in Figure 1 for example.",
                "There was a fire disaster in Mont 3 The maximal time is 180 seconds, as the NIST assessor would be forced to submit the form at that moment.",
                "Table 4: Term selection statistics (topic average) CF Type 1 × 48 3 × 16 6 × 8 # checked terms 14.8 13.3 11.2 # rel. terms 15.0 12.6 11.2 # rel. checked terms 7.9 6.9 5.9 precision 0.534 0.519 0.527 recall 0.526 0.548 0.527 Blanc Tunnel between France and Italy in 1999, but the user failed to select such keywords as mont, blanc, french and italian due to his/her ignorance of the event.",
                "Indeed, without proper context it would be hard to make perfect judgment.",
                "What is then, the extent to which the user is good at term feedback?",
                "Does it have serious impact on retrieval performance?",
                "To answer these questions, we need a measure of individual terms true relevance.",
                "We adopt the Simplified KL Divergence metric used in [24] to decide query expansion terms as our term relevance measure: σKLD(w) = p(w|R) log p(w|R) p(w|¬R) where p(w|R) is the probability that a relevant document contains term w, and p(w|¬R) is the probability that an irrelevant document contains w, both of which can be easily computed via maximum likelihood estimate given document-level relevance judgment.",
                "If σKLD(w) > 0, w is more likely to appear in relevant documents than irrelevant ones.",
                "We consider a term relevant if its Simplified KL Divergence value is greater than a certain threshold σ0.",
                "We can then define precision and recall of user term judgment accordingly: precision is the fraction of terms checked by the user that are relevant; recall is the fraction of presented relevant terms that are checked by the user.",
                "Table 4 shows the number of checked terms, relevant terms and relevant checked terms when σ0 is set to 1.0, as well as the precision/recall of user term judgment.",
                "Note that when the clarification forms contain more clusters, fewer terms are checked: 14.8 for 1 × 48, 13.3 for 3 × 16 and 11.2 for 6×8.",
                "Similar pattern holds for relevant terms and relevant checked terms.",
                "There seems to be a trade-off between increasing topic diversity by clustering and losing extra relevant terms: when there are more clusters, each of them gets fewer terms to present, which can hurt a major relevant cluster that contains many relevant terms.",
                "Therefore, it is not always helpful to have more clusters, e.g., TFB6C is actually worse than TFB1C.",
                "The major finding we can make from Table 4 is that the user is not particularly good at identifying relevant terms, which echoes the discovery in [18].",
                "In the case of 3 × 16 clarification forms, the average number of terms checked as relevant by the user is 13.3 per topic, and the average number of relevant terms whose σKLD value exceed 1.0 is 12.6.",
                "The user is able to recognize only 6.9 of these terms on average.",
                "Indeed, the precision and recall of user feedback terms (as defined previously) are far from perfect.",
                "On the other hand, If the user had correctly checked all such relevant terms, the performance of our algorithms would have increased a lot, as shown in Table 5.",
                "We see that TFB gets big improvement when there is an oracle who checks all relevant terms, while CFB meets a bottleneck around MAP of 0.325, since all it does is adjust cluster weights, and when the learned weights are close to being accurate, it cannot benefit more from term feedback.",
                "Also note that TCFB fails to outperform TFB, probably because TFB is sufficiently accurate.",
                "Table 5: Change of MAP when using all (and only) relevant terms (σKLD > 1.0) for feedback. original term feedback relevant term feedback TF1 0.288 0.354 TF3 0.288 0.354 TF6 0.278 0.346 CF3 0.305 0.325 CF6 0.301 0.326 TCF3 0.309 0.345 TCF6 0.304 0.341 6.4 Comparison with Relevance Feedback Now we compare term feedback with document-level relevance feedback, in which the user is presented with the top N documents from an initial retrieval and asked to judge their relevance.",
                "The feedback process is simulated using document relevance judgment from NIST.",
                "We use the mixture model based feedback method proposed in [25], with mixture noise set to 0.95 and feedback coefficient set to 0.9.",
                "Comparative evaluation of relevance feedback against other methods is complicated by the fact that some documents have already been viewed during feedback, so it makes no sense to include them in the retrieval results of the second run.",
                "However, this does not hold for term feedback.",
                "Thus, to make it fair w.r.t. users information gain, if the feedback documents are relevant, they should be kept in the top of the ranking; if they are irrelevant, they should be left out.",
                "Therefore, we use relevance feedback to produce a ranking of top 1000 retrieved documents but with every feedback document excluded, and then prepend the relevant feedback documents at the front.",
                "Table 6 shows the performance of relevance feedback for different values of N and compares it with TCFB3C.",
                "Table 6: Performance of relevance feedback for different number of feedback documents (N).",
                "N MAP Pr@30 RR 5 0.302 0.586 4779 10 0.345 0.670 4916 20 0.389 0.772 5004 TCFB3C 0.309 0.491 4947 We see that the performance of TCFB3C is comparable to that of relevance feedback using 5 documents.",
                "Although it is poorer than when there are 10 feedback documents in terms of MAP and Pr@30, it does retrieve more documents (4947) when going down the ranked list.",
                "We try to compare the quality of automatically inserted terms in relevance feedback with that of manually selected terms in term feedback.",
                "This is done by truncating the relevance feedback modified query model to a size equal to the number of checked terms for the same topic.",
                "We can then compare the terms in the truncated model with the checked terms.",
                "Figure 3 shows the distribution of the terms σKLD scores.",
                "We find that term feedback tends to produce expansion terms of higher quality(those with σKLD > 1) compared to relevance feedback (with 10 feedback documents).",
                "This does not contradict the fact that the latter yields higher retrieval performance.",
                "Actually, when we use the truncated query model instead of the intact one refined from relevance feedback, the MAP is only 0.304.",
                "The truth Figure 3: Comparison of expansion term quality between relevance feedback (with 10 feedback documents) and term feedback (with 3 × 16 CFs) −1−0 0−1 1−2 2−3 3−4 4−5 5−6 0 50 100 150 200 250 300 350 σKLD #terms relevance feedback term feedback is, although there are many unwanted terms in the expanded query model from feedback documents, there are also more relevant terms than what the user can possibly select from the list of presentation terms generated with pseudo-feedback documents, and the positive effects often outweights the negative ones.",
                "We are interested to know under what circumstances term feedback has advantage over relevance feedback.",
                "One such situation is when none of the top N feedback documents is relevant, rendering relevance feedback useless.",
                "This is not infrequent, as one might have thought: out of the 50 topics, there are 13 such cases when N = 5, 10 when N = 10, and still 3 when N = 20.",
                "When this happens, one can only back off to the original retrieval method; the power of relevance feedback is lost.",
                "Surprisingly, in 11 out of 13 such cases where relevance feedback seems impossible, the user is able to check at least 2 relevant terms from the 3 × 16 clarification form (we consider term t to be relevant if σKLD(t) > 1.0).",
                "Furthermore, in 10 out of them TCFB3C outperforms the pseudo-feedback baseline, increasing MAP from 0.076 to 0.146 on average (these are particularly hard topics).",
                "We think that there are two possible explanations for this phenomenon of term feedback being active even when relevance feedback does not work: First, even if none of the top N (suppose it is a small number) documents are relevant, we may still find relevant documents in top 60, which is more inclusive but usually unreachable when people are doing relevance feedback in interactive ad-hoc search, from which we can draw feedback terms.",
                "This is true for topic 367 piracy, where the top 10 feedback documents are all about software piracy, yet there are documents between 10-60 that are about piracy on the seas (which is about the real information need), contributing terms such as pirate, ship for selection in the clarification form.",
                "Second, for some topics, a document needs to meet some special condition in order to be relevant.",
                "The top N documents may be related to the topic, but nonetheless irrelevant.",
                "In this case, we may still extract useful terms from these documents, even if they do not qualify as relevant ones.",
                "For example, in topic 639 consumer online shopping, a document needs to mention what contributes to shopping growth to really match the specified information need, hence none of the top 10 feedback documents are regarded as relevant.",
                "But nevertheless, the feedback terms such as retail, commerce are good for query expansion. 7.",
                "CONCLUSIONS In this paper we studied the use of term feedback for interactive information retrieval in the language modeling approach.",
                "We proposed a cluster-based method for selecting presentation terms as well as algorithms to estimate refined query models from user term feedback.",
                "We saw significant improvement in retrieval accuracy brought by term feedback, in spite of the fact that a user often makes mistakes in relevance judgment that hurts its performance.",
                "We found the best-performing algorithm to be TCFB, which benefits from the combination of directly observed term evidence with TFB and indirectly learned cluster relevance with CFB.",
                "When we reduced the number of presentation terms, term feedback is still able to keep much of its performance gain over the baseline.",
                "Finally, we compared term feedback to document-level relevance feedback, and found that TCFB3Cs performance is on a par with the latter with 5 feedback documents.",
                "We regarded term feedback as a viable alternative to traditional relevance feedback, especially when there are no relevant documents in the top.",
                "We propose to extend our work in several ways.",
                "First, we want to study whether the use of various contexts can help the user to better identify term relevance, while not sacrificing the simplicity and compactness of term feedback.",
                "Second, currently all terms are presented to the user in a single batch.",
                "We could instead consider iterative term feedback, by presenting a small number of terms first, and show more terms after receiving user feedback or stop when the refined query is good enough.",
                "The presented terms should be selected dynamically to maximize learning benefits at any moment.",
                "Third, we have plans to incorporate term feedback into our UCAIR toolbar[20], an Internet Explorer plugin, to make it work for web search.",
                "We are also interested in studying how to combine term feedback with relevance feedback or implicit feedback.",
                "We could, for example, allow the user to dynamically modify terms in a language model learned from feedback documents. 8.",
                "ACKNOWLEDGMENT This work is supported in part by the National Science Foundation grants IIS-0347933 and IIS-0428472. 9.",
                "REFERENCES [1] J. Allan.",
                "Relevance feedback with too much data.",
                "In Proceedings of the 18th annual international ACM SIGIR conference on research and development in information retrieval, pages 337-343, 1995. [2] J. Allan.",
                "HARD track overview in TREC 2005 - High Accuracy Retrieval from Documents.",
                "In The Fourteenth Text REtrieval Conference, 2005. [3] P. Anick.",
                "Using terminological feedback for web search refinement: a log-based study.",
                "In Proceedings of the 26th annual international ACM SIGIR conference on research and development in informaion retrieval, pages 88-95, 2003. [4] P. G. Anick and S. Tipirneni.",
                "The paraphrase search assistant: terminological feedback for iterative information seeking.",
                "In Proceedings of the 22nd annual international ACM SIGIR conference on research and development in information retrieval, pages 153-159, 1999. [5] C. Buckley, G. Salton, J. Allan, and A. Singhal.",
                "Automatic query expansion using SMART.",
                "In Proceedings of the Third Text REtrieval Conference, 1994. [6] D. Harman.",
                "Towards interactive query expansion.",
                "In Proceedings of the 11th annual international ACM SIGIR conference on research and development in information retrieval, pages 321-331, 1988. [7] N. A. Jaleel, A. Corrada-Emmanuel, Q. Li, X. Liu, C. Wade, and J. Allan.",
                "UMass at TREC 2003: HARD and QA.",
                "In TREC, pages 715-725, 2003. [8] H. Joho, C. Coverson, M. Sanderson, and M. Beaulieu.",
                "Hierarchical presentation of expansion terms.",
                "In Proceedings of the 2002 ACM symposium on applied computing, pages 645-649, 2002. [9] K. S. Jones, S. Walker, and S. E. Robertson.",
                "A probabilistic model of information retrieval: development and status.",
                "Technical Report 446, Computer Laboratory, University of Cambridge, 1998. [10] D. Kelly, V. D. Dollu, and X. Fu.",
                "The loquacious user: a document-independent source of terms for query expansion.",
                "In Proceedings of the 28th annual international ACM SIGIR conference on research and development in information retrieval, pages 457-464, 2005. [11] D. Kelly and X. Fu.",
                "Elicitation of term relevance feedback: an investigation of term source and context.",
                "In Proceedings of the 29th annual international ACM SIGIR conference on research and development in information retrieval, 2006. [12] J. Koenemann and N. Belkin.",
                "A case for interaction: A study of interactive information retrieval behavior and effectiveness.",
                "In Proceedings of the SIGCHI conference on human factors in computing systems, pages 205-212, 1996. [13] V. Lavrenko and W. B. Croft.",
                "Relevance-based language models.",
                "In Research and Development in Information Retrieval, pages 120-127, 2001. [14] Y. Nemeth, B. Shapira, and M. Taeib-Maimon.",
                "Evaluation of the real and perceived value of automatic and interactive query expansion.",
                "In Proceedings of the 27th annual international ACM SIGIR conference on research and development in information retrieval, pages 526-527, 2004. [15] J. Ponte.",
                "A Language Modeling Approach to Information Retrieval.",
                "PhD thesis, University of Massachusetts at Amherst, 1998. [16] S. E. Robertson, S. Walker, S. Jones, M. Beaulieu, and M. Gatford.",
                "Okapi at TREC-3.",
                "In Proceedings of the Third Text REtrieval Conference, 1994. [17] J. Rocchio.",
                "Relevance feedback in information retrieval.",
                "In The SMART retrieval system, pages 313-323. 1971. [18] I. Ruthven.",
                "Re-examining the potential effectiveness of interactive query expansion.",
                "In Proceedings of the 26th annual international ACM SIGIR conference on research and development in informaion retrieval, pages 213-220, 2003. [19] G. Salton and C. Buckley.",
                "Improving retrieval performance by relevance feedback.",
                "Journal of the American Society for Information Science, 41:288-297, 1990. [20] X. Shen, B. Tan, and C. Zhai.",
                "Implicit user modeling for personalized search.",
                "In Proceedings of the 14th ACM international conference on information and knowledge management, pages 824-831, 2005. [21] X. Shen and C. Zhai.",
                "Active feedback in ad-hoc information retrieval.",
                "In Proceedings of the 28th annual international ACM SIGIR conference on research and development in information retrieval, pages 59-66, 2005. [22] A. Spink.",
                "Term relevance feedback and query expansion: relation to design.",
                "In Proceedings of the 17th annual international ACM SIGIR conference on research and development in information retrieval, pages 81-90, 1994. [23] J. Xu and W. B. Croft.",
                "Query expansion using local and global document analysis.",
                "In Proceedings of the 19th annual international ACM SIGIR conference on research and development in information retrieval, pages 4-11, 1996. [24] H. Zaragoza, N. Craswell, M. Taylor, S. Saria, and S. Robertson.",
                "Microsoft cambridge at TREC-13: Web and HARD tracks.",
                "In Proceedings of the 13th Text REtrieval Conference, 2004. [25] C. Zhai and J. Lafferty.",
                "Model-based feedback in the language modeling approach to information retrieval.",
                "In Proceedings of the tenth international conference on information and knowledge management, pages 403-410, 2001. [26] C. Zhai, A. Velivelli, and B. Yu.",
                "A cross-collection mixture model for comparative text mining.",
                "In Proceedings of the tenth ACM SIGKDD international conference on knowledge discovery and data mining, pages 743-748, 2004."
            ],
            "original_annotated_samples": [
                "We identified two key subtasks of term-based feedback, i.e., pre-feedback <br>presentation term</br> selection and post-feedback query model construction, with effective algorithms developed for both.",
                "We present our method for <br>presentation term</br> selection in Section 3 and algorithms for query model construction in Section 5.",
                "<br>presentation term</br> SELECTION Proper selection of terms to be presented to the user for judgment is crucial to the success of term feedback.",
                "We find that the performance of TFB is more susceptible to <br>presentation term</br> reduction than that of CFB or TCFB."
            ],
            "translated_annotated_samples": [
                "Identificamos dos sub tareas clave del feedback basado en términos, es decir, la selección de <br>términos para la presentación previa</br> al feedback y la construcción del modelo de consulta posterior al feedback, con algoritmos efectivos desarrollados para ambos.",
                "Presentamos nuestro método para la selección de <br>términos de presentación</br> en la Sección 3 y los algoritmos para la construcción del modelo de consulta en la Sección 5.",
                "SELECCIÓN DE TÉRMINOS DE PRESENTACIÓN La selección adecuada de los términos que se presentarán al usuario para su evaluación es crucial para el éxito de la retroalimentación de términos.",
                "Observamos que el rendimiento de TFB es más susceptible a la reducción de <br>términos de presentación</br> que el de CFB o TCFB."
            ],
            "translated_text": "Comentarios sobre el término para la recuperación de información con modelos de lenguaje Bin Tan†, Atulya Velivelli‡, Hui Fang†, ChengXiang Zhai† Dept. En este documento estudiamos la retroalimentación basada en términos para la recuperación de información en el enfoque de modelado de lenguaje. Con la retroalimentación de términos, un usuario juzga directamente la relevancia de términos individuales sin interacción con documentos de retroalimentación, tomando el control total del proceso de expansión de la consulta. Proponemos un método basado en clusters para seleccionar términos para presentar al usuario para su evaluación, así como algoritmos efectivos para construir modelos de lenguaje de consulta refinados a partir de la retroalimentación de términos del usuario. Nuestros algoritmos han demostrado proporcionar una mejora significativa en la precisión de recuperación en comparación con una línea base sin retroalimentación, y logran un rendimiento comparable al de la retroalimentación de relevancia. Son útiles incluso cuando no hay documentos relevantes en la parte superior. Categorías y Descriptores de Asignaturas H.3.3 [Búsqueda y Recuperación de Información]: Modelos de recuperación Términos generales Algoritmos 1. En el enfoque de modelado del lenguaje para la recuperación de información, la retroalimentación a menudo se modela como la estimación de un modelo de consulta mejorado o un modelo de relevancia basado en un conjunto de documentos de retroalimentación [25, 13]. Esto está en línea con la forma tradicional de realizar retroalimentación de relevancia: presentar al usuario documentos/pasajes para su juicio de relevancia y luego extraer términos de los documentos o pasajes juzgados para expandir la consulta inicial. Es una forma indirecta de buscar la ayuda de los usuarios para la construcción del modelo de consulta, en el sentido de que el modelo de consulta refinado (basado en términos) se aprende a través de documentos/pasajes de retroalimentación, que son estructuras de alto nivel de términos. Tiene la desventaja de que los términos irrelevantes, que ocurren junto con los relevantes en el contenido evaluado, pueden ser utilizados erróneamente para la expansión de la consulta, causando efectos no deseados. Por ejemplo, para la consulta TREC del telescopio Hubble logros, cuando un documento relevante habla más sobre la reparación de los telescopios que sobre sus descubrimientos, términos irrelevantes como caminata espacial pueden ser agregados a la consulta modificada. Podemos considerar una forma más directa de involucrar a un usuario en la mejora del modelo de consulta, sin un paso intermedio de retroalimentación de documentos que pueda introducir ruido. La idea es presentar al usuario un número (razonable) de términos individuales y pedirle que juzgue la relevancia de cada término o especifique directamente sus probabilidades en el modelo de consulta. Esta estrategia ha sido discutida en [15], pero hasta donde sabemos, no ha sido estudiada seriamente en la literatura existente de modelado de lenguaje. En comparación con la retroalimentación de relevancia tradicional, este enfoque basado en términos para el refinamiento del modelo de consulta interactivo tiene varias ventajas. Primero, el usuario tiene un mejor control del modelo de consulta final a través de la manipulación directa de términos: él/ella puede dictar qué términos son relevantes, irrelevantes y posiblemente, en qué medida. Esto evita el riesgo de introducir términos no deseados en el modelo de consulta, aunque a veces el usuario introduce términos de baja calidad. Segundo, dado que un término tarda menos tiempo en ser evaluado que el texto completo o el resumen de un documento, y con tan solo alrededor de 20 términos presentados se puede lograr una mejora significativa en el rendimiento de recuperación (como demostraremos más adelante), el feedback de términos permite recopilar la retroalimentación del usuario de manera más rápida. Esto es especialmente útil para la búsqueda interactiva ad hoc. En tercer lugar, a veces no hay documentos relevantes en los primeros N resultados recuperados inicialmente si el tema es difícil. Esto suele ser cierto cuando N está limitado a ser pequeño, lo cual surge del hecho de que el usuario no está dispuesto a evaluar demasiados documentos. En este caso, la retroalimentación de relevancia es inútil, ya que no se puede aprovechar ningún documento relevante, pero la retroalimentación de términos sigue siendo útil, al permitir seleccionar términos relevantes de documentos irrelevantes. Durante nuestra participación en la pista HARD de TREC 2005 y el estudio continuado posteriormente, exploramos cómo aprovechar la retroalimentación de términos por parte del usuario para construir modelos de consulta mejorados para la recuperación de información en el enfoque de modelado de lenguaje. Identificamos dos sub tareas clave del feedback basado en términos, es decir, la selección de <br>términos para la presentación previa</br> al feedback y la construcción del modelo de consulta posterior al feedback, con algoritmos efectivos desarrollados para ambos. Impusimos una estructura de clúster secundaria en los términos y descubrimos que una vista de clúster arroja una mayor comprensión de la necesidad de información de los usuarios, y proporciona una buena forma de utilizar la retroalimentación de términos. A través de experimentos encontramos que el feedback a término mejora significativamente sobre la línea base sin feedback, a pesar de que el usuario a menudo comete errores en la evaluación de relevancia. Entre nuestros algoritmos, el que tiene el mejor rendimiento de recuperación es TCFB, la combinación de TFB, el algoritmo de retroalimentación de términos directos, y CFB, el algoritmo de retroalimentación basado en clústeres. También variamos el número de términos de retroalimentación y observamos una mejora razonable incluso con números bajos. Finalmente, al comparar la retroalimentación a nivel de términos con la retroalimentación a nivel de documentos, encontramos que es una alternativa viable a esta última con un rendimiento competitivo en la recuperación. El resto del documento está organizado de la siguiente manera. La sección 2 discute algunos trabajos relacionados. La sección 4 describe nuestro enfoque general para la retroalimentación de términos. Presentamos nuestro método para la selección de <br>términos de presentación</br> en la Sección 3 y los algoritmos para la construcción del modelo de consulta en la Sección 5. Los resultados del experimento se presentan en la Sección 6. La sección 7 concluye este documento. 2. TRABAJO RELACIONADO La retroalimentación de relevancia[17, 19] ha sido reconocida durante mucho tiempo como un método efectivo para mejorar el rendimiento de recuperación. Normalmente, los N documentos principales recuperados utilizando la consulta original se presentan al usuario para su evaluación, después de lo cual se extraen términos de los documentos relevantes evaluados, ponderados por su potencial de atraer más documentos relevantes, y se añaden al modelo de consulta. La consulta ampliada suele representar mejor la necesidad de información de los usuarios que la original, que a menudo es solo una consulta de palabras clave cortas. Una segunda iteración de recuperación utilizando esta consulta modificada generalmente produce un aumento significativo en la precisión de la recuperación. En los casos en los que no se dispone de una evaluación de relevancia real y se asume que todos los documentos principales N son relevantes, se denomina retroalimentación ciega o pseudo [5, 16] y generalmente sigue mejorando el rendimiento. Debido a que un documento es una unidad de texto grande, cuando se utiliza para retroalimentación de relevancia, muchos términos irrelevantes pueden ser introducidos en el proceso de retroalimentación. Para superar esto, se propone el feedback de pasajes y se muestra que mejora el rendimiento del feedback. Una solución más directa es pedir al usuario su juicio de relevancia sobre los términos de retroalimentación. Por ejemplo, en algunos sistemas de retroalimentación de relevancia como [12], hay un paso de interacción que permite al usuario agregar o eliminar términos de expansión después de que son extraídos automáticamente de los documentos relevantes. Esto se clasifica como expansión interactiva de consultas, donde la consulta original se amplía con términos proporcionados por el usuario, que pueden provenir de la entrada directa del usuario (texto libre o palabras clave) o de la selección del usuario de términos sugeridos por el sistema (utilizando tesauros o extraídos de documentos de retroalimentación). En muchos casos, se ha encontrado que la retroalimentación de relevancia de términos mejora de manera efectiva el rendimiento de recuperación [6, 22, 12, 4, 10]. Por ejemplo, el estudio en [12] muestra que el usuario prefiere tener un conocimiento explícito y control directo de qué términos se utilizan para la expansión de la consulta, y se demuestra que la interfaz penetrable que proporciona esta libertad tiene un mejor rendimiento que otras interfaces. Sin embargo, en algunos otros casos no hay un beneficio significativo[3, 14], incluso si al usuario le gusta interactuar con los términos de expansión. En un estudio simulado realizado en [18], el autor compara el rendimiento de recuperación de la expansión interactiva de consultas y la expansión automática de consultas con un estudio simulado, y sugiere que los posibles beneficios de la primera pueden ser difíciles de lograr. Se descubre que el usuario no es bueno identificando términos útiles para la expansión de la consulta, cuando una interfaz de presentación de términos simple no puede proporcionar suficiente contexto semántico de los términos de retroalimentación. Nuestro trabajo difiere de los anteriores en dos aspectos importantes. Primero, al elegir los términos para presentar al usuario para la evaluación de relevancia, no solo consideramos el valor de un solo término (por ejemplo, la frecuencia relativa de un término en los documentos principales, que puede medirse mediante métricas como el Valor de Selección de Robertson y la Distancia Kullback-Leibler Simplificada como se detalla en [24]), sino que también examinamos la estructura de los grupos de términos, con el fin de producir una cobertura equilibrada de los diferentes aspectos del tema. Segundo, con el marco de modelado del lenguaje, permitimos una construcción detallada del modelo de consulta actualizado, estableciendo diferentes probabilidades para diferentes términos según si es un término de consulta, su importancia en los documentos principales y su pertenencia a un clúster. Aunque existen técnicas para ajustar los pesos de los términos de consulta en modelos de espacio vectorial y modelos de relevancia probabilística, la mayoría de los trabajos mencionados no las utilizan, optando por simplemente añadir términos de retroalimentación a la consulta original (usando pesos iguales para ellos), lo que puede llevar a un rendimiento de recuperación más pobre. La combinación de los dos aspectos permite que nuestro método funcione mucho mejor que el punto de referencia. La forma habitual de presentar los términos de retroalimentación es simplemente mostrar los términos en una lista. Ha habido algunos trabajos sobre interfaces de usuario alternativas. [8] organiza los términos en una jerarquía, y [11] compara tres interfaces diferentes, incluyendo términos + casillas de verificación, términos + contexto (oraciones) + casillas de verificación, oraciones + cuadro de texto de entrada. En ambos estudios, sin embargo, no hay una diferencia significativa en el rendimiento. En nuestro trabajo adoptamos el enfoque más simple de términos + casillas de verificación. Nos enfocamos en la presentación de términos y la construcción de modelos de consulta a partir de términos de retroalimentación, y creemos que el uso de contextos para mejorar la calidad de los términos de retroalimentación debería ser ortogonal a nuestro método. 3. Enfoque general: Seguimos el enfoque de modelado del lenguaje y basamos nuestro método en el modelo de recuperación de divergencia KL propuesto en [25]. Con este modelo, la tarea de recuperación implica estimar un modelo de lenguaje de consulta θq a partir de una consulta dada, un modelo de lenguaje de documento θd de cada documento, y calcular su divergencia KL D(θq||θd), que luego se utiliza para puntuar los documentos. [25] trata la retroalimentación de relevancia como un problema de reestimación del modelo de consulta, es decir, calcular un modelo de consulta actualizado θq dado el texto de la consulta original y la evidencia adicional proporcionada por los documentos relevantes juzgados. Adoptamos esta perspectiva y planteamos nuestra tarea como la actualización del modelo de consulta a partir de la retroalimentación de términos de usuario. Aquí hay dos tareas clave: Primero, cómo elegir los mejores términos para presentar al usuario para su evaluación, con el fin de recopilar la máxima evidencia sobre la necesidad de información del usuario. Segundo, cómo calcular un modelo de consulta actualizado basado en esta retroalimentación de términos, de manera que capture la necesidad de información de los usuarios y se traduzca en un buen rendimiento de recuperación. 4. SELECCIÓN DE TÉRMINOS DE PRESENTACIÓN La selección adecuada de los términos que se presentarán al usuario para su evaluación es crucial para el éxito de la retroalimentación de términos. Si los términos están mal elegidos y hay pocos relevantes, el usuario tendrá dificultades para buscar términos útiles que ayuden a aclarar su necesidad de información. Si los términos relevantes son abundantes, pero todos se centran en un solo aspecto del tema de la consulta, entonces solo podremos obtener retroalimentación sobre ese aspecto y perderemos otros, lo que resultará en una pérdida de amplitud en los resultados recuperados. Por lo tanto, es importante seleccionar cuidadosamente los términos de presentación para maximizar la ganancia esperada de la retroalimentación del usuario, es decir, aquellos que pueden revelar potencialmente la mayor evidencia de la necesidad de información del usuario. Esto es similar al feedback activo[21], que sugiere que un sistema de recuperación debería sondear activamente la necesidad de información de los usuarios, y en el caso del feedback de relevancia, los documentos de feedback deberían ser elegidos para maximizar los beneficios de aprendizaje (por ejemplo, de manera diversa para aumentar la cobertura). En nuestro enfoque, los N documentos principales de una recuperación inicial utilizando la consulta original forman la fuente de términos de retroalimentación: todos los términos que aparecen en ellos se consideran candidatos para presentar al usuario. Estos documentos sirven como pseudo-retroalimentación, ya que proporcionan un contexto mucho más amplio que la consulta original (generalmente muy breve), sin que se le pida al usuario que juzgue su relevancia. Debido a esta última razón, es posible hacer que N sea bastante grande (por ejemplo, en nuestros experimentos establecimos N = 60) para aumentar su cobertura de diferentes aspectos en el tema. La forma más sencilla de seleccionar términos de retroalimentación es elegir los M términos más frecuentes de los N documentos. Este método, sin embargo, tiene dos inconvenientes. Primero, se seleccionarán muchos términos ruidosos comunes debido a sus altas frecuencias en la colección de documentos, a menos que se utilice una lista de palabras vacías para filtrar. En segundo lugar, la lista de presentación tiende a estar compuesta por términos de los aspectos principales del tema; es probable que se pasen por alto aquellos de un aspecto menor debido a sus frecuencias relativamente bajas. Resolvemos los problemas anteriores mediante dos medidas correspondientes. Primero, introducimos un modelo de fondo θB que se estima a partir de estadísticas de la colección y explica los términos comunes, de modo que es mucho menos probable que aparezcan en la lista de presentación. Segundo, los términos se seleccionan de múltiples grupos en los documentos de retroalimentación simulada, para garantizar una representación suficiente de diferentes aspectos del tema. Nos basamos en el modelo multinomial de mezcla, que se utiliza para el descubrimiento de temas en [26]. Específicamente, asumimos que los N documentos contienen K grupos {Ci| i = 1, 2, · · · K}, cada uno caracterizado por una distribución de palabras multinomial (también conocida como modelo de lenguaje unigrama) θi y correspondiente a un aspecto del tema. Los documentos se consideran como muestreados de una mezcla de K + 1 componentes, incluidos los K grupos y el modelo de fondo: p(w|d) = λBp(w|θB) + (1 − λB) K i=1 πd,ip(w|θi) donde w es una palabra, λB es el peso de la mezcla para el modelo de fondo θB, y πd,i es el peso de la mezcla específico del documento para el modelo de cluster i-ésimo θi. Luego estimamos los modelos de clúster maximizando la probabilidad de que los documentos de pseudo-retroalimentación sean generados a partir del modelo de mezcla multinomial: log p(D|Λ) = d∈D w∈V c(w; d) log p(w|d) donde D = {di| i = 1, 2, · · · N} es el conjunto de los N documentos, V es el vocabulario, c(w; d) es la frecuencia de w en d y Λ = {θi| i = 1, 2, · · · K} ∪ {πdij | i = 1, 2, · · · N, j = 1, 2, · · · K} es el conjunto de parámetros del modelo a estimar. Los modelos de conglomerados pueden ser estimados eficientemente utilizando el algoritmo de Expectation-Maximization (EM). Para más detalles, remitimos al lector a [26]. La Tabla 1 muestra los modelos de clúster para la consulta de TREC sobre desastres en túneles de transporte (K = 3). Ten en cuenta que solo el grupo central es relevante. Tabla 1: Modelos de clúster para el tema 363 Desastres en túneles de transporte Clúster 1 Clúster 2 Clúster 3 túnel 0.0768 túnel 0.0935 túnel 0.0454 transporte 0.0364 fuego 0.0295 transporte 0.0406 tráfico 0.0206 camión 0.0236 peaje 0.0166 ferrocarril 0.0186 francés 0.0220 Amtrak 0.0153 puerto 0.0146 humo 0.0157 tren 0.0129 riel 0.0140 coche 0.0154 aeropuerto 0.0122 puente 0.0139 italiano 0.0152 autopista 0.0105 kilómetro 0.0136 incendio 0.0144 lui 0.0095 camión 0.0133 llamas 0.0127 Jersey 0.0093 construcción 0.0131 blanco 0.0121 paso 0.0087 · · · · · · · · · De cada uno de los K clústeres estimados, elegimos los L = M/K términos con las probabilidades más altas para formar un total de M términos de presentación. Si un término resulta estar en el top L en múltiples grupos, lo asignamos al grupo donde tenga la probabilidad más alta y permitimos que los otros grupos tomen un término adicional como compensación. También filtramos los términos en el texto de la consulta original porque tienden a ser siempre relevantes cuando la consulta es corta. Los términos seleccionados son luego presentados al usuario para su evaluación. Se muestra un formulario de retroalimentación (completado) de muestra en la Figura 1. En este estudio solo tratamos con juicios binarios: un término presentado está por defecto sin marcar, y un usuario puede marcarlo para indicar relevancia. Tampoco explotamos explícitamente la retroalimentación negativa (es decir, penalizar términos irrelevantes), porque con la retroalimentación binaria un término no verificado no es necesariamente irrelevante (quizás el usuario no está seguro de su relevancia). Podríamos pedir al usuario un juicio más detallado (por ejemplo, eligiendo entre altamente relevante, algo relevante, no sé, algo irrelevante y altamente irrelevante), pero la retroalimentación binaria es más compacta, ocupando menos espacio para mostrar y requiriendo menos esfuerzo por parte del usuario para emitir un juicio. 5. ESTIMACIÓN DE MODELOS DE CONSULTA A PARTIR DE RETROALIMENTACIÓN DE TÉRMINOS En esta sección, presentamos varios algoritmos para aprovechar la retroalimentación de términos. Los algoritmos toman como entrada la consulta original q, los grupos {θi} generados por el algoritmo de descubrimiento de temas, el conjunto de términos de retroalimentación T y su juicio de relevancia R, y producen un modelo de lenguaje de consulta actualizado θq que hace el mejor uso de la evidencia de retroalimentación para capturar la necesidad de información de los usuarios. Primero describimos nuestras notaciones: • θq: El modelo de consulta original, derivado solo de los términos de la consulta: p(w|θq) = c(w; q) |q| donde c(w; q) es la cantidad de veces que w aparece en q, y |q| = w∈q c(w; q) es la longitud de la consulta. • θq: El modelo de consulta actualizado que necesitamos estimar a partir de la retroalimentación de términos. • θi (i = 1, 2, . . . K): El modelo de lenguaje unigrama del clúster Ci, estimado utilizando el algoritmo de descubrimiento de temas. • T = {ti,j} (i = 1 . . . K, j = 1 . . . \n\nK, j = 1 . . . El conjunto de términos presentados al usuario para su evaluación. ti,j es el j-ésimo término elegido del grupo Ci. • R = {δw|w ∈ T}: δw es una variable indicadora que es 1 si w es considerado relevante o 0 en caso contrario. 5.1 TFB (Retroalimentación Directa de Términos) Esta es una forma directa de retroalimentación de términos que no implica ninguna estructura secundaria. Asignamos un peso de 1 a los términos considerados relevantes por el usuario, un peso de μ a los términos de la consulta, un peso de cero a los demás términos, y luego aplicamos la normalización: p(w|θq ) = δw + μ c(w; q) w ∈T δw + μ|q| donde w ∈T δw es el número total de términos considerados relevantes. Llamamos a este método TFB (retroalimentación directa de términos). Si dejamos que μ = 1, este enfoque es equivalente a agregar los términos relevantes después de la consulta original, que es lo que hace la expansión estándar de consultas (sin reponderación de términos). Si establecemos μ > 1, estamos poniendo más énfasis en los términos de consulta que en los verificados. Ten en cuenta que el modelo de resultado será más sesgado hacia θq si la consulta original es larga o el feedback del usuario es débil, lo cual tiene sentido, ya que podemos confiar más en la consulta original en ambos casos. Figura 1: Formulario de aclaración completado para el Tema 363 363 desastres en túneles de transporte. Por favor, seleccione todos los términos relevantes para el tema. tráfico ferrocarril puerto puente ferroviario kilómetro construir suizo enlace cruzado hongkonés río proyecto metro camión de bomberos francés humo coche italiano bomberos incendio blanco montaña víctima francés rescate conductor chamonix emerger peaje tren amtrak aeropuerto turnpike jersey pass rome z centro electrón carretera boston velocidad bu enviar 5.2 CFB (Retroalimentación de Clúster) Aquí explotamos la estructura de clúster que jugó un papel importante cuando seleccionamos los términos de presentación. Los grupos representan diferentes aspectos del tema de la consulta, cada uno de los cuales puede ser relevante o no. Si somos capaces de identificar los grupos relevantes, podemos combinarlos para generar un modelo de consulta que sea bueno para descubrir documentos pertenecientes a estos grupos (en lugar de los irrelevantes). Podríamos pedir al usuario que juzgue directamente la relevancia de un grupo después de ver los términos representativos en ese grupo, pero a veces esto sería una tarea difícil para el usuario, quien tendría que adivinar la semántica de un grupo a través de su conjunto de términos, los cuales podrían no estar bien conectados entre sí debido a la falta de contexto. Por lo tanto, proponemos aprender retroalimentación de clúster de forma indirecta, inferir la relevancia de un clúster a través de la relevancia de sus términos de retroalimentación. Dado que cada grupo tiene un número igual de términos presentados al usuario, la medida más simple de la relevancia de un grupo es el número de términos que se consideran relevantes en él. Intuitivamente, cuantos más términos estén marcados como relevantes en un grupo, más cerca estará el grupo del tema de la consulta y más debería participar en la modificación de la consulta. Si combinamos los modelos de clúster utilizando pesos determinados de esta manera y luego interpolamos con el modelo de consulta original, obtenemos la siguiente fórmula para la actualización de la consulta, que llamamos CFB (Retroalimentación de Clúster): p(w|θq) = λp(w|θq) + (1 − λ) K i=1 L j=1 δti,j K k=1 L j=1 δtk,j p(w|θi) donde L j=1 δti,j es el número de términos relevantes en el clúster Ci, y K k=1 L j=1 δtk,j es el número total de términos relevantes. Observamos que cuando solo hay un clúster (K = 1), la fórmula anterior se degrada a p(w|θq ) = λp(w|θq) + (1 − λ)p(w|θ1) que es simplemente una pseudo-retroalimentación de la forma propuesta en [25]. TCFB (Retroalimentación Término-Clúster) TFB y CFB ambos tienen sus inconvenientes. TFB asigna probabilidades no nulas a los términos presentados que están marcados como relevantes, pero ignora por completo (muchos más) otros, que pueden quedar sin marcar debido a la ignorancia de los usuarios, o simplemente no estar incluidos en la lista de presentación, pero deberíamos ser capaces de inferir su relevancia a partir de los marcados. Por ejemplo, en la Figura 1, dado que se revisan hasta 5 términos en el grupo central (las tercera y cuarta columnas), deberíamos tener una alta confianza en la relevancia de otros términos en ese grupo. CFB resuelve el problema de TFB tratando los términos en un grupo de manera colectiva, de modo que los términos no verificados/no presentados reciban pesos cuando los términos presentados en sus grupos son considerados relevantes, pero no distingue qué términos en un grupo son presentados o considerados. Intuitivamente, los términos considerados relevantes deberían recibir pesos mayores porque son indicados explícitamente como relevantes por el usuario. Por lo tanto, intentamos combinar los dos métodos, con la esperanza de obtener lo mejor de ambos. Lo hacemos interpolando el modelo TFB con el modelo CFB, y lo llamamos TCFB: p(w|θq ) = αp(w|θqT F B ) + (1 − α)p(w|θqCF B ) 6. EXPERIMENTOS En esta sección, describimos los resultados de nuestro experimento. Primero describimos nuestra configuración experimental y presentamos una visión general del rendimiento de varios métodos. Luego discutimos los efectos de variar la configuración de los parámetros en los algoritmos, así como el número de términos de presentación. A continuación analizamos el comportamiento de retroalimentación de los usuarios y su relación con el rendimiento de recuperación. Finalmente comparamos la retroalimentación de términos con la retroalimentación de relevancia y mostramos que tiene su ventaja particular. 6.1 Configuración del Experimento y Resultados Básicos Aprovechamos la oportunidad de la pista HARD de TREC 2005[2] para la evaluación de nuestros algoritmos. Las pistas utilizaron la colección AQUAINT, un corpus de 3GB de texto de noticias en inglés. Los temas incluyeron 50 que se sabía que eran difíciles, es decir, con bajo rendimiento de recuperación. Es para estos temas difíciles que la retroalimentación del usuario es más útil, ya que puede proporcionar información para desambiguar las consultas; con temas fáciles, es posible que el usuario no esté dispuesto a esforzarse por dar retroalimentación si los resultados de recuperación automática son lo suficientemente buenos. Los participantes de la pista pudieron enviar formularios de aclaración (CF) diseñados a medida para solicitar retroalimentación de los evaluadores humanos proporcionados por la Tabla 2: Rendimiento de recuperación para diferentes métodos y tipos de CF. La última fila es el porcentaje de mejora del MAP sobre la línea base. Los ajustes de parámetros μ = 4, λ = 0.1, α = 0.3 son casi óptimos. Variación de MAP con el número de términos presentados. # términos TFB1C TFB3C TFB6C CFB3C CFB6C TCFB3C TCFB6C 6 0.245 0.240 0.227 0.279 0.279 0.281 0.274 12 0.261 0.261 0.242 0.299 0.286 0.297 0.281 18 0.275 0.274 0.256 0.301 0.282 0.300 0.286 24 0.276 0.281 0.265 0.303 0.292 0.305 0.292 30 0.280 0.285 0.270 0.304 0.296 0.307 0.296 36 0.282 0.288 0.272 0.307 0.297 0.309 0.297 42 0.283 0.288 0.275 0.306 0.298 0.309 0.300 48 0.288 0.288 0.278 0.305 0.301 0.309 0.303 NIST. Diseñamos tres conjuntos de formularios de aclaración para la retroalimentación de términos, diferenciados en la elección de K, el número de grupos, y L, el número de términos presentados de cada grupo. Son: 1 × 48, un gran grupo con 48 términos, 3 × 16, 3 grupos con 16 términos cada uno, y 6 × 8, 6 grupos con 8 términos cada uno. El número total de términos presentados (M) está fijo en 48, por lo que al comparar el rendimiento de diferentes tipos de formas de clarificación podemos conocer los efectos de diferentes grados de agrupamiento. Para cada tema, un evaluador completaría los formularios en el orden de 6 × 8, 1 × 48 y 3 × 16, dedicando hasta tres minutos en cada formulario. El formulario de aclaración de muestra mostrado en la Figura 1 es de tipo 3 × 16. Es una interfaz simple y compacta en la que el usuario puede consultar términos relevantes. El formulario es autoexplicativo; no es necesario brindar capacitación adicional al usuario sobre cómo utilizarlo. Nuestras consultas iniciales se construyen solo utilizando las descripciones de los títulos de los temas, que tienen en promedio 2.7 palabras de longitud. Como base utilizamos el método de recuperación de divergencia KL implementado en la herramienta Lemur con 5 documentos de retroalimentación pseudo. Aplicamos el suavizado de Dirichlet con una prior de 2000, y truncamos los modelos de lenguaje de consulta a 50 términos (estas configuraciones se utilizan en todos los experimentos). Para todos los demás parámetros, utilizamos la configuración predeterminada de Lemurs. El punto de referencia resulta tener un rendimiento por encima del promedio entre los participantes de la pista. Después de una ejecución inicial utilizando este método de recuperación de referencia, tomamos los 60 documentos principales para cada tema y aplicamos el algoritmo de descubrimiento de temas para producir los grupos (1, 3 o 6 de ellos), en base a los cuales generamos formularios de aclaración. Después de recibir la retroalimentación del usuario, ejecutamos los algoritmos de retroalimentación de términos (TFB, CFB o TCFB) para estimar modelos de consulta actualizados, los cuales luego se utilizan para una segunda iteración de recuperación. Evaluamos el rendimiento de los diferentes métodos de recuperación en sus clasificaciones de los 1000 documentos principales. Las métricas de evaluación que adoptamos incluyen la precisión media promedio (no interpolada) (MAP), precisión en los primeros 30 (Pr@30) y total relevante recuperado (RR). La Tabla 2 muestra el rendimiento de varios métodos y configuraciones de K × L. Los sufijos (1C, 3C, 6C) después de TFB, CFB, TCFB representan el número de grupos (K). Por ejemplo, TCFB3C significa el método TCFB en los formularios de aclaración de 3 × 16. De la Tabla 2 podemos hacer las siguientes observaciones: 1 http://www.lemurproject.com 1. Todos los métodos tienen un rendimiento considerablemente mejor que la línea base de pseudoretroalimentación, con TCFB3C logrando una mejora máxima del 41.1% en el MAP, lo que indica una contribución significativa de la retroalimentación de términos para la clarificación de la necesidad de información de los usuarios. En otras palabras, el feedback a término es realmente útil para mejorar la precisión de recuperación. 2. Para TFB, el rendimiento es casi igual en los formularios de aclaración de 1 × 48 y 3 × 16 en términos de MAP (aunque este último es ligeramente mejor en Pr@30 y RR), y un poco peor en los de 6 × 8. 3. Tanto CFB3C como CFB6C tienen un mejor rendimiento que sus contrapartes TFB en las tres métricas, lo que sugiere que el feedback sobre una estructura de clúster secundaria es realmente beneficioso. CFB1C es en realidad peor porque no puede ajustar el peso de su (único) grupo a partir de la retroalimentación de términos y es simplemente pseudoretroalimentación. 4. Aunque TCFB es solo una simple mezcla de TFB y CFB por interpolación, logra superar a ambos. Esto respalda nuestra especulación de que TCFB supera las desventajas de TFB (prestando atención solo a los términos marcados) y CFB (sin distinguir los términos marcados y no marcados en un grupo). Excepto por TCFB6C v.s. CFB6C, la ventaja de rendimiento de TCFB sobre TFB/CFB es significativa con p < 0.05 utilizando la prueba de rango con signo de Wilcoxon. Esto no es cierto en el caso de TFB v.s. CFB, cada uno de los cuales es mejor que el otro en casi la mitad de los temas. 6.2 Reducción de Términos de Presentación En algunas situaciones podemos tener que reducir el número de términos de presentación debido a limitaciones en el espacio de visualización o esfuerzos de retroalimentación del usuario. Es interesante saber si el rendimiento de nuestros algoritmos se deteriora cuando al usuario se le presentan menos términos. Dado que los términos de presentación dentro de cada grupo se generan en orden decreciente de sus frecuencias, la lista de presentación forma un subconjunto del original si su tamaño se reduce2. Por lo tanto, podemos simular fácilmente lo que sucede cuando el número de términos de presentación disminuye 2. Hay complejidades que surgen de los términos que aparecen en la parte superior L de múltiples grupos, pero estas son excepciones de M a M: mantendremos todos los juicios de los términos superiores L = M / K en cada grupo y descartaremos los de los demás. La Tabla 3 muestra el rendimiento de varios algoritmos a medida que el número de términos de presentación varía de 6 a 48. Observamos que el rendimiento de TFB es más susceptible a la reducción de <br>términos de presentación</br> que el de CFB o TCFB. Por ejemplo, en 12 términos el MAP de TFB3C es el 90.6% del que se obtiene en 48 términos, mientras que los números para CFB3C y TCFB3C son del 98.0% y 96.1% respectivamente. Conjeturamos que la razón es que mientras el rendimiento de TFB depende en gran medida de cuántos términos buenos se elijan para la expansión de la consulta, CFB solo necesita una estimación aproximada de los pesos de los clústeres para funcionar. Además, las formas de aclaración de 3 × 16 parecen ser más robustas que las de 6 × 8: con 12 términos, el MAP de TFB6C es el 87.1% del obtenido con 48 términos, por debajo del 90.6% de TFB3C. De manera similar, para CFB es del 95.0% en comparación con el 98.0%. Esto es natural, ya que para un gran número de 6 clusters, es más fácil llegar a la situación en la que cada cluster recibe muy pocos términos de presentación para que la diversificación de temas sea útil. En general, nos sorprende ver que los algoritmos aún pueden funcionar razonablemente bien cuando el número de términos de presentación es pequeño. Por ejemplo, con solo 12 términos, CFB3C (el formulario de aclaración es de tamaño 3 × 4) aún puede mejorar un 36.5% sobre el valor base, disminuyendo ligeramente desde el 39.3% con 48 términos. En el Análisis de Retroalimentación del Usuario, estudiamos varios aspectos del comportamiento de retroalimentación de los usuarios en términos, y si están conectados al rendimiento de recuperación. Figura 2: Distribuciones de tiempo de completación del formulario de aclaración 0−30 30−60 60−90 90−120 120−150 150−180 0 5 10 15 20 25 30 35 tiempo de completación (segundos) #temas 1×48 3×16 6×8 La Figura 2 muestra la distribución del tiempo necesario para completar un formulario de aclaración. Observamos que el usuario suele ser capaz de completar el feedback del término en un tiempo razonablemente corto: para más de la mitad de los temas, el formulario de aclaración se completa en solo 1 minuto, y solo una pequeña fracción de temas (menos del 10% para 1 × 48 y 3 × 16) tarda más de 2 minutos. Esto sugiere que la retroalimentación a corto plazo es adecuada para la recuperación interactiva ad-hoc, donde un usuario generalmente no desea dedicar demasiado tiempo a proporcionar retroalimentación. Observamos que un usuario a menudo comete errores al juzgar la relevancia de un término. A veces, un término relevante puede ser omitido porque su conexión con el tema de la consulta no es obvia para el usuario. En otras ocasiones, puede incluirse un término dudoso que resulta ser irrelevante. Toma como ejemplo el tema en la Figura 1. Hubo un desastre de incendio en Mont 3. El tiempo máximo es de 180 segundos, ya que el evaluador del NIST estaría obligado a enviar el formulario en ese momento. Tabla 4: Estadísticas de selección de términos (promedio del tema) CF Tipo 1 × 48 3 × 16 6 × 8 # términos revisados 14.8 13.3 11.2 # términos relevantes 15.0 12.6 11.2 # términos relevantes revisados 7.9 6.9 5.9 precisión 0.534 0.519 0.527 recuperación 0.526 0.548 0.527 El usuario no seleccionó palabras clave como mont, blanc, francés e italiano debido a su desconocimiento del evento del Túnel del Mont Blanc entre Francia e Italia en 1999. De hecho, sin un contexto adecuado sería difícil hacer un juicio perfecto. ¿Cuál es entonces, el nivel en el que el usuario es bueno dando retroalimentación? ¿Tiene un impacto serio en el rendimiento de recuperación? Para responder a estas preguntas, necesitamos una medida de la verdadera relevancia de los términos individuales. Adoptamos la métrica de Divergencia KL Simplificada utilizada en [24] para decidir los términos de expansión de la consulta como nuestra medida de relevancia de términos: σKLD(w) = p(w|R) log p(w|R) p(w|¬R) donde p(w|R) es la probabilidad de que un documento relevante contenga el término w, y p(w|¬R) es la probabilidad de que un documento irrelevante contenga w, ambas pueden ser fácilmente calculadas mediante una estimación de máxima verosimilitud dada la relevancia a nivel de documento. Si σKLD(w) > 0, w es más probable que aparezca en documentos relevantes que en irrelevantes. Consideramos un término relevante si su valor de Divergencia KL Simplificada es mayor que un umbral determinado σ0. Podemos entonces definir la precisión y la exhaustividad del juicio de términos del usuario de la siguiente manera: la precisión es la fracción de términos revisados por el usuario que son relevantes; la exhaustividad es la fracción de términos relevantes presentados que son revisados por el usuario. La Tabla 4 muestra el número de términos revisados, términos relevantes y términos relevantes revisados cuando σ0 se establece en 1.0, así como la precisión/recuperación del juicio de términos del usuario. Ten en cuenta que cuando los formularios de aclaración contienen más grupos, se revisan menos términos: 14.8 para 1 × 48, 13.3 para 3 × 16 y 11.2 para 6×8. El patrón similar se mantiene para los términos relevantes y los términos relevantes verificados. Parece haber un compromiso entre aumentar la diversidad de temas mediante el agrupamiento y perder términos adicionales relevantes: cuando hay más grupos, cada uno recibe menos términos para presentar, lo que puede perjudicar a un grupo relevante importante que contiene muchos términos relevantes. Por lo tanto, no siempre es útil tener más grupos, por ejemplo, TFB6C es en realidad peor que TFB1C. El hallazgo principal que podemos obtener de la Tabla 4 es que el usuario no es particularmente bueno identificando términos relevantes, lo cual coincide con el descubrimiento en [18]. En el caso de 3 formularios de aclaración de 16, el número promedio de términos marcados como relevantes por el usuario es de 13.3 por tema, y el número promedio de términos relevantes cuyo valor de σKLD excede 1.0 es de 12.6. El usuario es capaz de reconocer solo 6.9 de estos términos en promedio. De hecho, la precisión y la exhaustividad de los términos de retroalimentación de los usuarios (como se definió anteriormente) están lejos de ser perfectas. Por otro lado, si el usuario hubiera verificado correctamente todos esos términos relevantes, el rendimiento de nuestros algoritmos habría aumentado considerablemente, como se muestra en la Tabla 5. Vemos que TFB experimenta una gran mejora cuando hay un oráculo que verifica todos los términos relevantes, mientras que CFB se encuentra con un cuello de botella alrededor de un MAP de 0.325, ya que todo lo que hace es ajustar los pesos de los clústeres, y cuando los pesos aprendidos están cerca de ser precisos, no puede beneficiarse más del feedback de términos. También hay que tener en cuenta que TCFB no logra superar a TFB, probablemente porque TFB es lo suficientemente preciso. Tabla 5: Cambio de MAP al usar todos (y solo) los términos relevantes (σKLD > 1.0) para retroalimentación. retroalimentación de término original retroalimentación de término relevante TF1 0.288 0.354 TF3 0.288 0.354 TF6 0.278 0.346 CF3 0.305 0.325 CF6 0.301 0.326 TCF3 0.309 0.345 TCF6 0.304 0.341 6.4 Comparación con Retroalimentación de Relevancia Ahora comparamos la retroalimentación de términos con la retroalimentación de relevancia a nivel de documento, en la que al usuario se le presentan los N mejores documentos de una recuperación inicial y se le pide que juzgue su relevancia. El proceso de retroalimentación se simula utilizando la evaluación de relevancia de documentos de NIST. Utilizamos el método de retroalimentación basado en el modelo de mezcla propuesto en [25], con el ruido de mezcla establecido en 0.95 y el coeficiente de retroalimentación establecido en 0.9. La evaluación comparativa del feedback de relevancia frente a otros métodos se complica por el hecho de que algunos documentos ya han sido vistos durante el feedback, por lo que no tiene sentido incluirlos en los resultados de recuperación de la segunda ejecución. Sin embargo, esto no se aplica al término retroalimentación. Por lo tanto, para que sea justo en cuanto a la ganancia de información de los usuarios, si los documentos de retroalimentación son relevantes, deben mantenerse en la parte superior del ranking; si son irrelevantes, deben ser excluidos. Por lo tanto, utilizamos retroalimentación de relevancia para producir un ranking de los 1000 documentos recuperados, pero excluyendo cada documento de retroalimentación, y luego añadimos los documentos relevantes de retroalimentación al principio. La tabla 6 muestra el rendimiento de la retroalimentación de relevancia para diferentes valores de N y lo compara con TCFB3C. Tabla 6: Rendimiento de la retroalimentación de relevancia para diferentes números de documentos de retroalimentación (N). Vemos que el rendimiento de TCFB3C es comparable al de la retroalimentación de relevancia utilizando 5 documentos. Aunque es menos eficiente que cuando hay 10 documentos de retroalimentación en términos de MAP y Pr@30, sí recupera más documentos (4947) al descender por la lista clasificada. Intentamos comparar la calidad de los términos insertados automáticamente en la retroalimentación de relevancia con la de los términos seleccionados manualmente en la retroalimentación de términos. Esto se logra truncando el modelo de consulta modificado por retroalimentación de relevancia a un tamaño igual al número de términos verificados para el mismo tema. Podemos comparar los términos en el modelo truncado con los términos verificados. La Figura 3 muestra la distribución de los puntajes σKLD de los términos. Observamos que el término \"feedback\" tiende a producir términos de expansión de mayor calidad (aquellos con σKLD > 1) en comparación con el \"relevance feedback\" (con 10 documentos de feedback). Esto no contradice el hecho de que este último produce un rendimiento de recuperación más alto. De hecho, cuando usamos el modelo de consulta truncada en lugar del modelo íntegro refinado a partir de la retroalimentación de relevancia, el MAP es solo 0.304. La verdad Figura 3: Comparación de la calidad del término de expansión entre retroalimentación de relevancia (con 10 documentos de retroalimentación) y retroalimentación de términos (con 3 × 16 CFs) −1−0 0−1 1−2 2−3 3−4 4−5 5−6 0 50 100 150 200 250 300 350 σKLD #términos retroalimentación de relevancia retroalimentación de términos es que, aunque hay muchos términos no deseados en el modelo de consulta expandido a partir de los documentos de retroalimentación, también hay más términos relevantes de los que el usuario puede seleccionar de la lista de términos de presentación generados con documentos de pseudo-retroalimentación, y los efectos positivos a menudo superan a los negativos. Estamos interesados en saber en qué circunstancias la retroalimentación basada en términos tiene ventajas sobre la retroalimentación basada en relevancia. Una de esas situaciones es cuando ninguno de los documentos de retroalimentación principales es relevante, lo que hace que la retroalimentación de relevancia sea inútil. Esto no es infrecuente, como uno podría haber pensado: de los 50 temas, hay 13 casos así cuando N = 5, 10 cuando N = 10, y aún 3 cuando N = 20. Cuando esto sucede, solo se puede retroceder al método original de recuperación; se pierde el poder de la retroalimentación de relevancia. Sorprendentemente, en 11 de 13 casos en los que parece imposible el feedback de relevancia, el usuario puede marcar al menos 2 términos relevantes de los 3 × 16 del formulario de aclaración (consideramos que el término t es relevante si σKLD(t) > 1.0). Además, en 10 de ellos TCFB3C supera la línea base de pseudo-retroalimentación, aumentando el MAP de 0.076 a 0.146 en promedio (estos son temas particularmente difíciles). Creemos que hay dos posibles explicaciones para este fenómeno de que la retroalimentación de términos esté activa incluso cuando la retroalimentación de relevancia no funciona: Primero, incluso si ninguno de los primeros N (supongamos que es un número pequeño) documentos son relevantes, aún podemos encontrar documentos relevantes en los primeros 60, que son más inclusivos pero generalmente inalcanzables cuando las personas están realizando retroalimentación de relevancia en la búsqueda interactiva ad-hoc, de donde podemos extraer términos de retroalimentación. Esto es cierto para el tema 367 de piratería, donde los 10 documentos principales de retroalimentación son todos sobre piratería de software, sin embargo, hay documentos entre el 10 y el 60 que tratan sobre piratería en los mares (que es la necesidad de información real), contribuyendo términos como pirata, barco para la selección en el formulario de aclaración. Segundo, para algunos temas, un documento debe cumplir con ciertas condiciones especiales para ser relevante. Los N documentos principales pueden estar relacionados con el tema, pero no obstante ser irrelevantes. En este caso, aún podemos extraer términos útiles de estos documentos, incluso si no califican como relevantes. Por ejemplo, en el tema 639 de compras en línea de consumidores, un documento necesita mencionar qué contribuye al crecimiento de las compras para realmente coincidir con la necesidad de información especificada, por lo tanto, ninguno de los 10 documentos de retroalimentación principales se considera relevante. Sin embargo, los términos de retroalimentación como venta al por menor, comercio son buenos para la expansión de consultas. 7. CONCLUSIONES En este artículo estudiamos el uso de la retroalimentación de términos para la recuperación interactiva de información en el enfoque de modelado del lenguaje. Propusimos un método basado en clusters para seleccionar términos de presentación, así como algoritmos para estimar modelos de consulta refinados a partir de la retroalimentación de términos de usuario. Observamos una mejora significativa en la precisión de recuperación proporcionada por la retroalimentación de términos, a pesar de que un usuario a menudo comete errores en la evaluación de relevancia que perjudican su rendimiento. Encontramos que el algoritmo de mejor rendimiento es TCFB, el cual se beneficia de la combinación de la evidencia de términos observados directamente con TFB y la relevancia de clúster aprendida indirectamente con CFB. Cuando redujimos el número de términos de presentación, el feedback del término aún puede mantener gran parte de su mejora de rendimiento sobre el valor base. Finalmente, comparamos la retroalimentación de términos con la retroalimentación de relevancia a nivel de documento, y encontramos que el rendimiento de TCFB3C está a la par con este último con 5 documentos de retroalimentación. Consideramos el término \"feedback\" como una alternativa viable al feedback de relevancia tradicional, especialmente cuando no hay documentos relevantes en la parte superior. Proponemos ampliar nuestro trabajo de varias maneras. Primero, queremos estudiar si el uso de varios contextos puede ayudar al usuario a identificar mejor la relevancia de los términos, sin sacrificar la simplicidad y la concisión de la retroalimentación de los términos. Segundo, actualmente todos los términos se presentan al usuario en un solo lote. En su lugar, podríamos considerar la retroalimentación iterativa de términos, presentando primero un pequeño número de términos y mostrando más términos después de recibir la retroalimentación del usuario o detenernos cuando la consulta refinada sea lo suficientemente buena. Los términos presentados deben ser seleccionados dinámicamente para maximizar los beneficios de aprendizaje en cualquier momento. Tercero, tenemos planes de incorporar retroalimentación de términos en nuestra barra de herramientas UCAIR[20], un complemento de Internet Explorer, para que funcione en búsquedas web. También estamos interesados en estudiar cómo combinar la retroalimentación de términos con la retroalimentación de relevancia o la retroalimentación implícita. Por ejemplo, podríamos permitir al usuario modificar dinámicamente los términos en un modelo de lenguaje aprendido a partir de documentos de retroalimentación. 8. AGRADECIMIENTO Este trabajo cuenta con el apoyo parcial de las becas de la Fundación Nacional de Ciencias IIS-0347933 e IIS-0428472. 9. REFERENCIAS [1] J. Allan. Retroalimentación de relevancia con demasiados datos. En Actas de la 18ª conferencia internacional anual de ACM SIGIR sobre investigación y desarrollo en recuperación de información, páginas 337-343, 1995. [2] J. Allan. Resumen de la pista HARD en TREC 2005 - Recuperación de alta precisión de documentos. En la Decimocuarta Conferencia de Recuperación de Información de 2005. [3] P. Anick. Utilizando retroalimentación terminológica para refinar la búsqueda web: un estudio basado en registros. En Actas de la 26ª conferencia internacional anual de ACM SIGIR sobre investigación y desarrollo en recuperación de información, páginas 88-95, 2003. [4] P. G. Anick y S. Tipirneni. El asistente de búsqueda de paráfrasis: retroalimentación terminológica para la búsqueda iterativa de información. En Actas de la 22ª conferencia internacional anual de ACM SIGIR sobre investigación y desarrollo en recuperación de información, páginas 153-159, 1999. [5] C. Buckley, G. Salton, J. Allan y A. Singhal. Expansión automática de consultas utilizando SMART. En Actas de la Tercera Conferencia de Recuperación de Texto, 1994. [6] D. Harman. Hacia la expansión interactiva de consultas. En Actas de la 11ª conferencia internacional anual de ACM SIGIR sobre investigación y desarrollo en recuperación de información, páginas 321-331, 1988. [7] N. A. Jaleel, A. Corrada-Emmanuel, Q. Li, X. Liu, C. Wade y J. Allan. UMass en TREC 2003: HARD y QA. En TREC, páginas 715-725, 2003. [8] H. Joho, C. Coverson, M. Sanderson y M. Beaulieu. Presentación jerárquica de términos de expansión. En Actas del simposio de informática aplicada de ACM de 2002, páginas 645-649, 2002. [9] K. S. Jones, S. Walker y S. E. Robertson. Un modelo probabilístico de recuperación de información: desarrollo y estado actual. Informe técnico 446, Laboratorio de Computación, Universidad de Cambridge, 1998. [10] D. Kelly, V. D. Dollu y X. Fu. El usuario locuaz: una fuente de términos independiente del documento para la expansión de consultas. En Actas de la 28ª conferencia internacional anual de ACM SIGIR sobre investigación y desarrollo en recuperación de información, páginas 457-464, 2005. [11] D. Kelly y X. Fu. Obtención de retroalimentación de relevancia de términos: una investigación sobre la fuente y el contexto de los términos. En Actas de la 29ª conferencia internacional anual de ACM SIGIR sobre investigación y desarrollo en recuperación de información, 2006. [12] J. Koenemann y N. Belkin. Un caso para la interacción: Un estudio del comportamiento y la efectividad de la recuperación de información interactiva. En Actas de la conferencia SIGCHI sobre factores humanos en sistemas informáticos, páginas 205-212, 1996. [13] V. Lavrenko y W. B. Croft. Modelos de lenguaje basados en relevancia. En Investigación y Desarrollo en Recuperación de Información, páginas 120-127, 2001. [14] Y. Nemeth, B. Shapira y M. Taeib-Maimon. Evaluación del valor real y percibido de la expansión automática e interactiva de consultas. En Actas de la 27ª conferencia internacional anual de ACM SIGIR sobre investigación y desarrollo en recuperación de información, páginas 526-527, 2004. [15] J. Ponte. Un enfoque de modelado del lenguaje para la recuperación de información. Tesis doctoral, Universidad de Massachusetts en Amherst, 1998. [16] S. E. Robertson, S. Walker, S. Jones, M. Beaulieu y M. Gatford. Okapi en TREC-3. En Actas de la Tercera Conferencia de Recuperación de Texto, 1994. [17] J. Rocchio. Retroalimentación de relevancia en la recuperación de información. En el sistema de recuperación SMART, páginas 313-323. 1971. [18] I. Ruthven. Reexaminando la efectividad potencial de la expansión interactiva de consultas. En Actas de la 26ª conferencia internacional anual de ACM SIGIR sobre investigación y desarrollo en recuperación de información, páginas 213-220, 2003. [19] G. Salton y C. Buckley. Mejorando el rendimiento de recuperación mediante retroalimentación de relevancia. Revista de la Sociedad Americana de Ciencia de la Información, 41:288-297, 1990. [20] X. Shen, B. Tan y C. Zhai. Modelado implícito de usuario para búsqueda personalizada. En Actas de la 14ª conferencia internacional de ACM sobre información y gestión del conocimiento, páginas 824-831, 2005. [21] X. Shen y C. Zhai. Retroalimentación activa en la recuperación de información ad-hoc. En Actas de la 28ª conferencia internacional anual de ACM SIGIR sobre investigación y desarrollo en recuperación de información, páginas 59-66, 2005. [22] A. Spink. Retroalimentación de relevancia de términos y expansión de consultas: relación con el diseño. En Actas de la 17ª conferencia internacional anual de ACM SIGIR sobre investigación y desarrollo en recuperación de información, páginas 81-90, 1994. [23] J. Xu y W. B. Croft. Expansión de consulta utilizando análisis local y global de documentos. En Actas de la 19ª conferencia internacional anual de ACM SIGIR sobre investigación y desarrollo en recuperación de información, páginas 4-11, 1996. [24] H. Zaragoza, N. Craswell, M. Taylor, S. Saria y S. Robertson. Microsoft Cambridge en TREC-13: pistas Web y HARD. En Actas de la 13ª Conferencia de Recuperación de Información de Texto, 2004. [25] C. Zhai y J. Lafferty. Retroalimentación basada en modelos en el enfoque de modelado del lenguaje para la recuperación de información. En Actas de la décima conferencia internacional sobre gestión de la información y el conocimiento, páginas 403-410, 2001. [26] C. Zhai, A. Velivelli y B. Yu. Un modelo de mezcla de colecciones cruzadas para la minería de textos comparativa. En Actas de la décima conferencia internacional de ACM SIGKDD sobre descubrimiento de conocimiento y minería de datos, páginas 743-748, 2004. ",
            "candidates": [],
            "error": [
                [
                    "términos para la presentación previa",
                    "términos de presentación",
                    "términos de presentación"
                ]
            ]
        },
        "query expansion": {
            "translated_key": "",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Term Feedback for Information Retrieval with Language Models Bin Tan† , Atulya Velivelli‡ , Hui Fang† , ChengXiang Zhai† Dept.",
                "of Computer Science† , Dept. of Electrical and Computer Engineering‡ University of Illinois at Urbana-Champaign bintan@cs.uiuc.edu, velivell@ifp.uiuc.edu, hfang@cs.uiuc.edu, czhai@cs.uiuc.edu ABSTRACT In this paper we study term-based feedback for information retrieval in the language modeling approach.",
                "With term feedback a user directly judges the relevance of individual terms without interaction with feedback documents, taking full control of the <br>query expansion</br> process.",
                "We propose a cluster-based method for selecting terms to present to the user for judgment, as well as effective algorithms for constructing refined query language models from user term feedback.",
                "Our algorithms are shown to bring significant improvement in retrieval accuracy over a non-feedback baseline, and achieve comparable performance to relevance feedback.",
                "They are helpful even when there are no relevant documents in the top.",
                "Categories and Subject Descriptors H.3.3 [Information Search and Retrieval]: Retrieval models General Terms Algorithms 1.",
                "INTRODUCTION In the language modeling approach to information retrieval, feedback is often modeled as estimating an improved query model or relevance model based on a set of feedback documents [25, 13].",
                "This is in line with the traditional way of doing relevance feedback - presenting a user with documents/passages for relevance judgment and then extracting terms from the judged documents or passages to expand the initial query.",
                "It is an indirect way of seeking users assistance for query model construction, in the sense that the refined query model (based on terms) is learned through feedback documents/passages, which are high-level structures of terms.",
                "It has the disadvantage that irrelevant terms, which occur along with relevant ones in the judged content, may be erroneously used for <br>query expansion</br>, causing undesired effects.",
                "For example, for the TREC query Hubble telescope achievements, when a relevant document talks more about the telescopes repair than its discoveries, irrelevant terms such as spacewalk can be added into the modified query.",
                "We can consider a more direct way to involve a user in query model improvement, without an intermediary step of document feedback that can introduce noise.",
                "The idea is to present a (reasonable) number of individual terms to the user and ask him/her to judge the relevance of each term or directly specify their probabilities in the query model.",
                "This strategy has been discussed in [15], but to our knowledge, it has not been seriously studied in existing language modeling literature.",
                "Compared to traditional relevance feedback, this term-based approach to interactive query model refinement has several advantages.",
                "First, the user has better control of the final query model through direct manipulation of terms: he/she can dictate which terms are relevant, irrelevant, and possibly, to what degree.",
                "This avoids the risk of bringing unwanted terms into the query model, although sometimes the user introduces low-quality terms.",
                "Second, because a term takes less time to judge than a documents full text or summary, and as few as around 20 presented terms can bring significant improvement in retrieval performance (as we will show later), term feedback makes it faster to gather user feedback.",
                "This is especially helpful for interactive adhoc search.",
                "Third, sometimes there are no relevant documents in the top N of the initially retrieved results if the topic is hard.",
                "This is often true when N is constrained to be small, which arises from the fact that the user is unwilling to judge too many documents.",
                "In this case, relevance feedback is useless, as no relevant document can be leveraged on, but term feedback is still often helpful, by allowing relevant terms to be picked from irrelevant documents.",
                "During our participation in the TREC 2005 HARD Track and continued study afterward, we explored how to exploit term feedback from the user to construct improved query models for information retrieval in the language modeling approach.",
                "We identified two key subtasks of term-based feedback, i.e., pre-feedback presentation term selection and post-feedback query model construction, with effective algorithms developed for both.",
                "We imposed a secondary cluster structure on terms and found that a cluster view sheds additional insight into the users information need, and provides a good way of utilizing term feedback.",
                "Through experiments we found that term feedback improves significantly over the nonfeedback baseline, even though the user often makes mistakes in relevance judgment.",
                "Among our algorithms, the one with best retrieval performance is TCFB, the combination of TFB, the direct term feedback algorithm, and CFB, the cluster-based feedback algorithm.",
                "We also varied the number of feedback terms and observed reasonable improvement even at low numbers.",
                "Finally, by comparing term feedback with document-level feedback, we found it to be a viable alternative to the latter with competitive retrieval performance.",
                "The rest of the paper is organized as follows.",
                "Section 2 discusses some related work.",
                "Section 4 outlines our general approach to term feedback.",
                "We present our method for presentation term selection in Section 3 and algorithms for query model construction in Section 5.",
                "The experiment results are given in Section 6.",
                "Section 7 concludes this paper. 2.",
                "RELATED WORK Relevance feedback[17, 19] has long been recognized as an effective method for improving retrieval performance.",
                "Normally, the top N documents retrieved using the original query are presented to the user for judgment, after which terms are extracted from the judged relevant documents, weighted by their potential of attracting more relevant documents, and added into the query model.",
                "The expanded query usually represents the users information need better than the original one, which is often just a short keyword query.",
                "A second iteration of retrieval using this modified query usually produces significant increase in retrieval accuracy.",
                "In cases where true relevance judgment is unavailable and all top N documents are assumed to be relevant, it is called blind or pseudo feedback[5, 16] and usually still brings performance improvement.",
                "Because document is a large text unit, when it is used for relevance feedback many irrelevant terms can be introduced into the feedback process.",
                "To overcome this, passage feedback is proposed and shown to improve feedback performance[1, 23].",
                "A more direct solution is to ask the user for their relevance judgment of feedback terms.",
                "For example, in some relevance feedback systems such as [12], there is an interaction step that allows the user to add or remove expansion terms after they are automatically extracted from relevant documents.",
                "This is categorized as interactive <br>query expansion</br>, where the original query is augmented with user-provided terms, which can come from direct user input (free-form text or keywords)[22, 7, 10] or user selection of system-suggested terms (using thesauri[6, 22] or extracted from feedback documents[6, 22, 12, 4, 7]).",
                "In many cases term relevance feedback has been found to effectively improve retrieval performance[6, 22, 12, 4, 10].",
                "For example, the study in [12] shows that the user prefers to have explicit knowledge and direct control of which terms are used for <br>query expansion</br>, and the penetrable interface that provides this freedom is shown to perform better than other interfaces.",
                "However, in some other cases there is no significant benefit[3, 14], even if the user likes interacting with expansion terms.",
                "In a simulated study carried out in [18], the author compares the retrieval performance of interactive <br>query expansion</br> and automatic <br>query expansion</br> with a simulated study, and suggests that the potential benefits of the former can be hard to achieve.",
                "The user is found to be not good at identifying useful terms for <br>query expansion</br>, when a simple term presentation interface is unable to provide sufficient semantic context of the feedback terms.",
                "Our work differs from the previous ones in two important aspects.",
                "First, when we choose terms to present to the user for relevance judgment, we not only consider single-term value (e.g., the relative frequency of a term in the top documents, which can be measured by metrics such as Robertson Selection Value and Simplified Kullback-Leibler Distance as listed in [24]), but also examine the cluster structure of the terms, so as to produce a balanced coverage of the different topic aspects.",
                "Second, with the language modelling framework, we allow an elaborate construction of the updated query model, by setting different probabilities for different terms based on whether it is a query term, its significance in the top documents, and its cluster membership.",
                "Although techniques for adjusting query term weights exist for vector space models[17] and probablistic relevance models[9], most of the aforementioned works do not use them, choosing to just append feedback terms to the original query (thus using equal weights for them), which can lead to poorer retrieval performance.",
                "The combination of the two aspects allows our method to perform much better than the baseline.",
                "The usual way for feedback term presentation is just to display the terms in a list.",
                "There have been some works on alternative user interfaces. [8] arranges terms in a hierarchy, and [11] compares three different interfaces, including terms + checkboxes, terms + context (sentences) + checkboxes, sentences + input text box.",
                "In both studies, however, there is no significant performance difference.",
                "In our work we adopt the simplest approach of terms + checkboxes.",
                "We focus on term presentation and query model construction from feedback terms, and believe using contexts to improve feedback term quality should be orthogonal to our method. 3.",
                "GENERAL APPROACH We follow the language modeling approach, and base our method on the KL-divergence retrieval model proposed in [25].",
                "With this model, the retrieval task involves estimating a query language model θq from a given query, a document language model θd from each document, and calculating their KL-divergence D(θq||θd), which is then used to score the documents. [25] treats relevance feedback as a query model re-estimation problem, i.e., computing an updated query model θq given the original query text and the extra evidence carried by the judged relevant documents.",
                "We adopt this view, and cast our task as updating the query model from user term feedback.",
                "There are two key subtasks here: First, how to choose the best terms to present to the user for judgment, in order to gather maximal evidence about the users information need.",
                "Second, how to compute an updated query model based on this term feedback evidence, so that it captures the users information need and translates into good retrieval performance. 4.",
                "PRESENTATION TERM SELECTION Proper selection of terms to be presented to the user for judgment is crucial to the success of term feedback.",
                "If the terms are poorly chosen and there are few relevant ones, the user will have a hard time looking for useful terms to help clarify his/her information need.",
                "If the relevant terms are plentiful, but all concentrate on a single aspect of the query topic, then we will only be able to get feedback on that aspect and missing others, resulting in a breadth loss in retrieved results.",
                "Therefore, it is important to carefully select presentation terms to maximize expected gain from user feedback, i.e., those that can potentially reveal most evidence of the users information need.",
                "This is similar to active feedback[21], which suggests that a retrieval system should actively probe the users information need, and in the case of relevance feedback, the feedback documents should be chosen to maximize learning benefits (e.g. diversely so as to increase coverage).",
                "In our approach, the top N documents from an initial retrieval using the original query form the source of feedback terms: all terms that appear in them are considered candidates to present to the user.",
                "These documents serve as pseudo-feedback, since they provide a much richer context than the original query (usually very short), while the user is not asked to judge their relevance.",
                "Due to the latter reason, it is possible to make N quite large (e.g., in our experiments we set N = 60) to increase its coverage of different aspects in the topic.",
                "The simplest way of selecting feedback terms is to choose the most frequent M terms from the N documents.",
                "This method, however, has two drawbacks.",
                "First, a lot of common noisy terms will be selected due to their high frequencies in the document collection, unless a stop-word list is used for filtering.",
                "Second, the presentation list will tend to be filled by terms from major aspects of the topic; those from a minor aspect are likely to be missed due to their relatively low frequencies.",
                "We solve the above problems by two corresponding measures.",
                "First, we introduce a background model θB that is estimated from collection statistics and explains the common terms, so that they are much less likely to appear in the presentation list.",
                "Second, the terms are selected from multiple clusters in the pseudo-feedback documents, to ensure sufficient representation of different aspects of the topic.",
                "We rely on the mixture multinomial model, which is used for theme discovery in [26].",
                "Specifically, we assume the N documents contain K clusters {Ci| i = 1, 2, · · · K}, each characterized by a multinomial word distribution (also known as unigram language model) θi and corresponding to an aspect of the topic.",
                "The documents are regarded as sampled from a mixture of K + 1 components, including the K clusters and the background model: p(w|d) = λBp(w|θB) + (1 − λB) K i=1 πd,ip(w|θi) where w is a word, λB is the mixture weight for the background model θB, and πd,i is the document-specific mixture weight for the i-th cluster model θi.",
                "We then estimate the cluster models by maximizing the probability of the pseudo-feedback documents being generated from the multinomial mixture model: log p(D|Λ) = d∈D w∈V c(w; d) log p(w|d) where D = {di| i = 1, 2, · · · N} is the set of the N documents, V is the vocabulary, c(w; d) is ws frequency in d and Λ = {θi| i = 1, 2, · · · K} ∪ {πdij | i = 1, 2, · · · N, j = 1, 2, · · · K} is the set of model parameters to estimate.",
                "The cluster models can be efficiently estimated using the Expectation-Maximization (EM) algorithm.",
                "For its details, we refer the reader to [26].",
                "Table 1 shows the cluster models for TREC query Transportation tunnel disasters (K = 3).",
                "Note that only the middle cluster is relevant.",
                "Table 1: Cluster models for topic 363 Transportation tunnel disasters Cluster 1 Cluster 2 Cluster 3 tunnel 0.0768 tunnel 0.0935 tunnel 0.0454 transport 0.0364 fire 0.0295 transport 0.0406 traffic 0.0206 truck 0.0236 toll 0.0166 railwai 0.0186 french 0.0220 amtrak 0.0153 harbor 0.0146 smoke 0.0157 train 0.0129 rail 0.0140 car 0.0154 airport 0.0122 bridg 0.0139 italian 0.0152 turnpik 0.0105 kilomet 0.0136 firefight 0.0144 lui 0.0095 truck 0.0133 blaze 0.0127 jersei 0.0093 construct 0.0131 blanc 0.0121 pass 0.0087 · · · · · · · · · From each of the K estimated clusters, we choose the L = M/K terms with highest probabilities to form a total of M presentation terms.",
                "If a term happens to be in top L in multiple clusters, we assign it to the cluster where it has highest probability and let the other clusters take one more term as compensation.",
                "We also filter out terms in the original query text because they tend to always be relevant when the query is short.",
                "The selected terms are then presented to the user for judgment.",
                "A sample (completed) feedback form is shown in Figure 1.",
                "In this study we only deal with binary judgment: a presented term is by default unchecked, and a user may check it to indicate relevance.",
                "We also do not explicitly exploit negative feedback (i.e., penalizing irrelevant terms), because with binary feedback an unchecked term is not necessarily irrelevant (maybe the user is unsure about its relevance).",
                "We could ask the user for finer judgment (e.g., choosing from highly relevant, somewhat relevant, do not know, somewhat irrelevant and highly irrelevant), but binary feedback is more compact, taking less space to display and less user effort to make judgment. 5.",
                "ESTIMATING QUERY MODELS FROM TERM FEEDBACK In this section, we present several algorithms for exploiting term feedback.",
                "The algorithms take as input the original query q, the clusters {θi} as generated by the theme discovery algorithm, the set of feedback terms T and their relevance judgment R, and outputs an updated query language model θq that makes best use of the feedback evidence to capture the users information need.",
                "First we describe our notations: • θq: The original query model, derived from query terms only: p(w|θq) = c(w; q) |q| where c(w; q) is the count of w in q, and |q| = w∈q c(w; q) is the query length. • θq : The updated query model which we need to estimate from term feedback. • θi (i = 1, 2, . . .",
                "K): The unigram language model of cluster Ci, as estimated using the theme discovery algorithm. • T = {ti,j} (i = 1 . . .",
                "K, j = 1 . . .",
                "L): The set of terms presented to the user for judgment. ti,j is the j-th term chosen from cluster Ci. • R = {δw|w ∈ T}: δw is an indicator variable that is 1 if w is judged relevant or 0 otherwise. 5.1 TFB (Direct Term Feedback) This is a straight-forward form of term feedback that does not involve any secondary structure.",
                "We give a weight of 1 to terms judged relevant by the user, a weight of μ to query terms, zero weight to other terms, and then apply normalization: p(w|θq ) = δw + μ c(w; q) w ∈T δw + μ|q| where w ∈T δw is the total number of terms that are judged relevant.",
                "We call this method TFB (direct Term FeedBack).",
                "If we let μ = 1, this approach is equivalent to appending the relevant terms after the original query, which is what standard <br>query expansion</br> (without term reweighting) does.",
                "If we set μ > 1, we are putting more emphasis on the query terms than the checked ones.",
                "Note that the result model will be more biased toward θq if the original query is long or the user feedback is weak, which makes sense, as we can trust more on the original query in either case.",
                "Figure 1: Filled clarification form for Topic 363 363 transportation tunnel disasters Please select all terms that are relevant to the topic. traffic railway harbor rail bridge kilometer construct swiss cross link kong hong river project meter shanghai fire truck french smoke car italian firefights blaze blanc mont victim franc rescue driver chamonix emerge toll amtrak train airport turnpike lui jersey pass rome z center electron road boston speed bu submit 5.2 CFB (Cluster Feedback) Here we exploit the cluster structure that played an important role when we selected the presentation terms.",
                "The clusters represent different aspects of the query topic, each of which may or may not be relevant.",
                "If we are able to identify the relevant clusters, we can combine them to generate a query model that is good at discovering documents belonging to these clusters (instead of the irrelevant ones).",
                "We could ask the user to directly judge the relevance of a cluster after viewing representative terms in that cluster, but this would sometimes be a difficult task for the user, who has to guess the semantics of a cluster via its set of terms, which may not be well connected to one another due to a lack of context.",
                "Therefore, we propose to learn cluster feedback indirectly, inferring the relevance of a cluster through the relevance of its feedback terms.",
                "Because each cluster has an equal number of terms presented to the user, the simplest measure of a clusters relevance is the number of terms that are judged relevant in it.",
                "Intuitively, the more terms are marked relevant in a cluster, the closer the cluster is to the query topic, and the more the cluster should participate in query modification.",
                "If we combine the cluster models using weights determined this way and then interpolate with the original query model, we get the following formula for query updating, which we call CFB (Cluster FeedBack): p(w|θq ) = λp(w|θq) + (1 − λ) K i=1 L j=1 δti,j K k=1 L j=1 δtk,j p(w|θi) where L j=1 δti,j is the number of relevant terms in cluster Ci, and K k=1 L j=1 δtk,j is the total number of relevant terms.",
                "We note that when there is only one cluster (K = 1), the above formula degenerates to p(w|θq ) = λp(w|θq) + (1 − λ)p(w|θ1) which is merely pseudo-feedback of the form proposed in [25]. 5.3 TCFB (Term-cluster Feedback) TFB and CFB both have their drawbacks.",
                "TFB assigns non-zero probabilities to the presented terms that are marked relevant, but completely ignores (a lot more) others, which may be left unchecked due to the users ignorance, or simply not included in the presentation list, but we should be able to infer their relevance from the checked ones.",
                "For example, in Figure 1, since as many as 5 terms in the middle cluster (the third and fourth columns) are checked, we should have high confidence in the relevance of other terms in that cluster.",
                "CFB remedies TFBs problem by treating the terms in a cluster collectively, so that unchecked/unpresented terms receive weights when presented terms in their clusters are judged as relevant, but it does not distinguish which terms in a cluster are presented or judged.",
                "Intuitively, the judged relevant terms should receive larger weights because they are explicitly indicated as relevant by the user.",
                "Therefore, we try to combine the two methods, hoping to get the best out of both.",
                "We do this by interpolating the TFB model with the CFB model, and call it TCFB: p(w|θq ) = αp(w|θqT F B ) + (1 − α)p(w|θqCF B ) 6.",
                "EXPERIMENTS In this section, we describe our experiment results.",
                "We first describe our experiment setup and present an overview of various methods performance.",
                "Then we discuss the effects of varying the parameter setting in the algorithms, as well as the number of presentation terms.",
                "Next we analyze user term feedback behavior and its relation to retrieval performance.",
                "Finally we compare term feedback to relevance feedback and show that it has its particular advantage. 6.1 Experiment Setup and Basic Results We took the opportunity of TREC 2005 HARD Track[2] for the evaluation of our algorithms.",
                "The tracks used the AQUAINT collection, a 3GB corpus of English newswire text.",
                "The topics included 50 ones previously known to be hard, i.e. with low retrieval performance.",
                "It is for these hard topics that user feedback is most helpful, as it can provide information to disambiguate the queries; with easy topics the user may be unwilling to spend efforts for feedback if the automatic retrieval results are good enough.",
                "Participants of the track were able to submit custom-designed clarification forms (CF) to solicit feedback from human assessors provided by Table 2: Retrieval performance for different methods and CF types.",
                "The last row is the percentage of MAP improvement over the baseline.",
                "The parameter settings μ = 4, λ = 0.1, α = 0.3 are near optimal.",
                "Baseline TFB1C TFB3C TFB6C CFB1C CFB3C CFB6C TCFB1C TCFB3C TCFB6C MAP 0.219 0.288 0.288 0.278 0.254 0.305 0.301 0.274 0.309 0.304 Pr@30 0.393 0.467 0.475 0.457 0.399 0.480 0.473 0.431 0.491 0.473 RR 4339 4753 4762 4740 4600 4907 4872 4767 4947 4906 % 0% 31.5% 31.5% 26.9% 16.0% 39.3% 37.4% 25.1% 41.1% 38.8% Table 3: MAP variation with the number of presented terms. # terms TFB1C TFB3C TFB6C CFB3C CFB6C TCFB3C TCFB6C 6 0.245 0.240 0.227 0.279 0.279 0.281 0.274 12 0.261 0.261 0.242 0.299 0.286 0.297 0.281 18 0.275 0.274 0.256 0.301 0.282 0.300 0.286 24 0.276 0.281 0.265 0.303 0.292 0.305 0.292 30 0.280 0.285 0.270 0.304 0.296 0.307 0.296 36 0.282 0.288 0.272 0.307 0.297 0.309 0.297 42 0.283 0.288 0.275 0.306 0.298 0.309 0.300 48 0.288 0.288 0.278 0.305 0.301 0.309 0.303 NIST.",
                "We designed three sets of clarification forms for term feedback, differing in the choice of K, the number of clusters, and L, the number of presented terms from each cluster.",
                "They are: 1× 48, a big cluster with 48 terms, 3 × 16, 3 clusters with 16 terms each, and 6 × 8, 6 clusters with 8 terms each.",
                "The total number of presented terms (M) is fixed at 48, so by comparing the performance of different types of clarification forms we can know the effects of different degree of clustering.",
                "For each topic, an assessor would complete the forms ordered by 6 × 8, 1 × 48 and 3 × 16, spending up to three minutes on each form.",
                "The sample clarification form shown in Figure 1 is of type 3 × 16.",
                "It is a simple and compact interface in which the user can check relevant terms.",
                "The form is self-explanatory; there is no need for extra user training on how to use it.",
                "Our initinal queries are constructed only using the topic title descriptions, which are on average 2.7 words in length.",
                "As our baseline we use the KL divergence retrieval method implemented in the Lemur Toolkit1 with 5 pseudo-feedback documents.",
                "We stem the terms, choose Dirichlet smoothing with a prior of 2000, and truncate query language models to 50 terms (these settings are used throughout the experiments).",
                "For all other parameters we use Lemurs default settings.",
                "The baseline turns out to perform above average among the track participants.",
                "After an initial run using this baseline retrieval method, we take the top 60 documents for each topic and apply the theme discovery algorithm to output the clusters (1, 3, or 6 of them), based on which we generate clarification forms.",
                "After user feedback is received, we run the term feedback algorithms (TFB, CFB or TCFB) to estimate updated query models, which are then used for a second iteration of retrieval.",
                "We evaluate the different retrieval methods performance on their rankings of the top 1000 documents.",
                "The evaluation metrics we adopt include mean average (non-interpolated) precision (MAP), precision at top 30 (Pr@30) and total relevant retrieved (RR).",
                "Table 2 shows the performance of various methods and configurations of K × L. The suffixes (1C, 3C, 6C) after TFB,CFB,TCFB stand for the number of clusters (K).",
                "For example, TCFB3C means the TCFB method on the 3 × 16 clarification forms.",
                "From Table 2 we can make the following observations: 1 http://www.lemurproject.com 1.",
                "All methods perform considerably better than the pseudofeedback baseline, with TCFB3C achieving a highest 41.1% improvement in MAP, indicating significant contribution of term feedback for clarification of the users information need.",
                "In other words, term feedback is truly helpful for improving retrieval accuracy. 2.",
                "For TFB, the performance is almost equal on the 1 × 48 and 3 × 16 clarification forms in terms of MAP (although the latter is slightly better in Pr@30 and RR), and a little worse on the 6 × 8 ones. 3.",
                "Both CFB3C and CFB6C perform better than their TFB counterparts in all three metrics, suggesting that feedback on a secondary cluster structure is indeed beneficial.",
                "CFB1C is actually worse because it cannot adjust the weight of its (single) cluster from term feedback and it is merely pseudofeedback. 4.",
                "Although TCFB is just a simple mixture of TFB and CFB by interpolation, it is able to outperform both.",
                "This supports our speculation that TCFB overcomes the drawbacks of TFB (paying attention only to checked terms) and CFB (not distinguishing checked and unchecked terms in a cluster).",
                "Except for TCFB6C v.s.",
                "CFB6C, the performance advantage of TCFB over TFB/CFB is significant at p < 0.05 using the Wilcoxon signed rank test.",
                "This is not true in the case of TFB v.s.",
                "CFB, each of which is better than the other in nearly half of the topics. 6.2 Reduction of Presentation Terms In some situations we may have to reduce the number of presentation terms due to limits in display space or user feedback efforts.",
                "It is interesting to know whether our algorithms performance deteriorates when the user is presented with fewer terms.",
                "Because the presentation terms within each cluster are generated in decreasing order of their frequencies, the presentation list forms a subset of the original one if its size is reduced2 .",
                "Therefore, we can easily simulate what happens when the number of presentation terms decreases 2 There are complexities arising from terms appearing in top L of multiple clusters, but these are exceptions from M to M : we will keep all judgments of the top L = M /K terms in each cluster and discard those of others.",
                "Table 3 shows the performance of various algorithms as the number of presentation terms ranges from 6 to 48.",
                "We find that the performance of TFB is more susceptible to presentation term reduction than that of CFB or TCFB.",
                "For example, at 12 terms the MAP of TFB3C is 90.6% of that at 48 terms, while the numbers for CFB3C and TCFB3C are 98.0% and 96.1% respectively.",
                "We conjecture the reason to be that while TFBs performance heavily depends on how many good terms are chosen for <br>query expansion</br>, CFB only needs a rough estimate of cluster weights to work.",
                "Also, the 3 × 16 clarification forms seem to be more robust than the 6 × 8 ones: at 12 terms the MAP of TFB6C is 87.1% of that at 48 terms, lower than 90.6% for TFB3C.",
                "Similarly, for CFB it is 95.0% against 98.0%.",
                "This is natual, as for a large cluster number of 6, it is easier to get into the situation where each cluster gets too few presentation terms to make topic diversification useful.",
                "Overall, we are surprised to see that the algorithms are still able to perform reasonably well when the number of presentation terms is small.",
                "For example, at only 12 terms CFB3C (the clarification form is of size 3 × 4) can still improve 36.5% over the baseline, dropping slightly from 39.3% at 48 terms. 6.3 User Feedback Analysis In this part we study several aspects of users term feedback behavior, and whether they are connected to retrieval performance.",
                "Figure 2: Clarification form completion time distributions 0−30 30−60 60−90 90−120 120−150 150−180 0 5 10 15 20 25 30 35 completion time (seconds) #topics 1×48 3×16 6×8 Figure 2 shows the distribution of time needed to complete a clarification form3 .",
                "We see that the user is usually able to finish term feedback within a reasonably short amount of time: for more than half of the topics the clarification form is completed in just 1 minute, and only a small fraction of topics (less than 10% for 1 × 48 and 3 × 16) take more than 2 minutes.",
                "This suggests that term feedback is suitable for interactive ad-hoc retrieval, where a user usually does not want to spend too much time on providing feedback.",
                "We find that a user often makes mistakes when judging term relevance.",
                "Sometimes a relevant term may be left out because its connection to the query topic is not obvious to the user.",
                "Other times a dubious term may be included but turns out to be irrelevant.",
                "Take the topic in Figure 1 for example.",
                "There was a fire disaster in Mont 3 The maximal time is 180 seconds, as the NIST assessor would be forced to submit the form at that moment.",
                "Table 4: Term selection statistics (topic average) CF Type 1 × 48 3 × 16 6 × 8 # checked terms 14.8 13.3 11.2 # rel. terms 15.0 12.6 11.2 # rel. checked terms 7.9 6.9 5.9 precision 0.534 0.519 0.527 recall 0.526 0.548 0.527 Blanc Tunnel between France and Italy in 1999, but the user failed to select such keywords as mont, blanc, french and italian due to his/her ignorance of the event.",
                "Indeed, without proper context it would be hard to make perfect judgment.",
                "What is then, the extent to which the user is good at term feedback?",
                "Does it have serious impact on retrieval performance?",
                "To answer these questions, we need a measure of individual terms true relevance.",
                "We adopt the Simplified KL Divergence metric used in [24] to decide <br>query expansion</br> terms as our term relevance measure: σKLD(w) = p(w|R) log p(w|R) p(w|¬R) where p(w|R) is the probability that a relevant document contains term w, and p(w|¬R) is the probability that an irrelevant document contains w, both of which can be easily computed via maximum likelihood estimate given document-level relevance judgment.",
                "If σKLD(w) > 0, w is more likely to appear in relevant documents than irrelevant ones.",
                "We consider a term relevant if its Simplified KL Divergence value is greater than a certain threshold σ0.",
                "We can then define precision and recall of user term judgment accordingly: precision is the fraction of terms checked by the user that are relevant; recall is the fraction of presented relevant terms that are checked by the user.",
                "Table 4 shows the number of checked terms, relevant terms and relevant checked terms when σ0 is set to 1.0, as well as the precision/recall of user term judgment.",
                "Note that when the clarification forms contain more clusters, fewer terms are checked: 14.8 for 1 × 48, 13.3 for 3 × 16 and 11.2 for 6×8.",
                "Similar pattern holds for relevant terms and relevant checked terms.",
                "There seems to be a trade-off between increasing topic diversity by clustering and losing extra relevant terms: when there are more clusters, each of them gets fewer terms to present, which can hurt a major relevant cluster that contains many relevant terms.",
                "Therefore, it is not always helpful to have more clusters, e.g., TFB6C is actually worse than TFB1C.",
                "The major finding we can make from Table 4 is that the user is not particularly good at identifying relevant terms, which echoes the discovery in [18].",
                "In the case of 3 × 16 clarification forms, the average number of terms checked as relevant by the user is 13.3 per topic, and the average number of relevant terms whose σKLD value exceed 1.0 is 12.6.",
                "The user is able to recognize only 6.9 of these terms on average.",
                "Indeed, the precision and recall of user feedback terms (as defined previously) are far from perfect.",
                "On the other hand, If the user had correctly checked all such relevant terms, the performance of our algorithms would have increased a lot, as shown in Table 5.",
                "We see that TFB gets big improvement when there is an oracle who checks all relevant terms, while CFB meets a bottleneck around MAP of 0.325, since all it does is adjust cluster weights, and when the learned weights are close to being accurate, it cannot benefit more from term feedback.",
                "Also note that TCFB fails to outperform TFB, probably because TFB is sufficiently accurate.",
                "Table 5: Change of MAP when using all (and only) relevant terms (σKLD > 1.0) for feedback. original term feedback relevant term feedback TF1 0.288 0.354 TF3 0.288 0.354 TF6 0.278 0.346 CF3 0.305 0.325 CF6 0.301 0.326 TCF3 0.309 0.345 TCF6 0.304 0.341 6.4 Comparison with Relevance Feedback Now we compare term feedback with document-level relevance feedback, in which the user is presented with the top N documents from an initial retrieval and asked to judge their relevance.",
                "The feedback process is simulated using document relevance judgment from NIST.",
                "We use the mixture model based feedback method proposed in [25], with mixture noise set to 0.95 and feedback coefficient set to 0.9.",
                "Comparative evaluation of relevance feedback against other methods is complicated by the fact that some documents have already been viewed during feedback, so it makes no sense to include them in the retrieval results of the second run.",
                "However, this does not hold for term feedback.",
                "Thus, to make it fair w.r.t. users information gain, if the feedback documents are relevant, they should be kept in the top of the ranking; if they are irrelevant, they should be left out.",
                "Therefore, we use relevance feedback to produce a ranking of top 1000 retrieved documents but with every feedback document excluded, and then prepend the relevant feedback documents at the front.",
                "Table 6 shows the performance of relevance feedback for different values of N and compares it with TCFB3C.",
                "Table 6: Performance of relevance feedback for different number of feedback documents (N).",
                "N MAP Pr@30 RR 5 0.302 0.586 4779 10 0.345 0.670 4916 20 0.389 0.772 5004 TCFB3C 0.309 0.491 4947 We see that the performance of TCFB3C is comparable to that of relevance feedback using 5 documents.",
                "Although it is poorer than when there are 10 feedback documents in terms of MAP and Pr@30, it does retrieve more documents (4947) when going down the ranked list.",
                "We try to compare the quality of automatically inserted terms in relevance feedback with that of manually selected terms in term feedback.",
                "This is done by truncating the relevance feedback modified query model to a size equal to the number of checked terms for the same topic.",
                "We can then compare the terms in the truncated model with the checked terms.",
                "Figure 3 shows the distribution of the terms σKLD scores.",
                "We find that term feedback tends to produce expansion terms of higher quality(those with σKLD > 1) compared to relevance feedback (with 10 feedback documents).",
                "This does not contradict the fact that the latter yields higher retrieval performance.",
                "Actually, when we use the truncated query model instead of the intact one refined from relevance feedback, the MAP is only 0.304.",
                "The truth Figure 3: Comparison of expansion term quality between relevance feedback (with 10 feedback documents) and term feedback (with 3 × 16 CFs) −1−0 0−1 1−2 2−3 3−4 4−5 5−6 0 50 100 150 200 250 300 350 σKLD #terms relevance feedback term feedback is, although there are many unwanted terms in the expanded query model from feedback documents, there are also more relevant terms than what the user can possibly select from the list of presentation terms generated with pseudo-feedback documents, and the positive effects often outweights the negative ones.",
                "We are interested to know under what circumstances term feedback has advantage over relevance feedback.",
                "One such situation is when none of the top N feedback documents is relevant, rendering relevance feedback useless.",
                "This is not infrequent, as one might have thought: out of the 50 topics, there are 13 such cases when N = 5, 10 when N = 10, and still 3 when N = 20.",
                "When this happens, one can only back off to the original retrieval method; the power of relevance feedback is lost.",
                "Surprisingly, in 11 out of 13 such cases where relevance feedback seems impossible, the user is able to check at least 2 relevant terms from the 3 × 16 clarification form (we consider term t to be relevant if σKLD(t) > 1.0).",
                "Furthermore, in 10 out of them TCFB3C outperforms the pseudo-feedback baseline, increasing MAP from 0.076 to 0.146 on average (these are particularly hard topics).",
                "We think that there are two possible explanations for this phenomenon of term feedback being active even when relevance feedback does not work: First, even if none of the top N (suppose it is a small number) documents are relevant, we may still find relevant documents in top 60, which is more inclusive but usually unreachable when people are doing relevance feedback in interactive ad-hoc search, from which we can draw feedback terms.",
                "This is true for topic 367 piracy, where the top 10 feedback documents are all about software piracy, yet there are documents between 10-60 that are about piracy on the seas (which is about the real information need), contributing terms such as pirate, ship for selection in the clarification form.",
                "Second, for some topics, a document needs to meet some special condition in order to be relevant.",
                "The top N documents may be related to the topic, but nonetheless irrelevant.",
                "In this case, we may still extract useful terms from these documents, even if they do not qualify as relevant ones.",
                "For example, in topic 639 consumer online shopping, a document needs to mention what contributes to shopping growth to really match the specified information need, hence none of the top 10 feedback documents are regarded as relevant.",
                "But nevertheless, the feedback terms such as retail, commerce are good for <br>query expansion</br>. 7.",
                "CONCLUSIONS In this paper we studied the use of term feedback for interactive information retrieval in the language modeling approach.",
                "We proposed a cluster-based method for selecting presentation terms as well as algorithms to estimate refined query models from user term feedback.",
                "We saw significant improvement in retrieval accuracy brought by term feedback, in spite of the fact that a user often makes mistakes in relevance judgment that hurts its performance.",
                "We found the best-performing algorithm to be TCFB, which benefits from the combination of directly observed term evidence with TFB and indirectly learned cluster relevance with CFB.",
                "When we reduced the number of presentation terms, term feedback is still able to keep much of its performance gain over the baseline.",
                "Finally, we compared term feedback to document-level relevance feedback, and found that TCFB3Cs performance is on a par with the latter with 5 feedback documents.",
                "We regarded term feedback as a viable alternative to traditional relevance feedback, especially when there are no relevant documents in the top.",
                "We propose to extend our work in several ways.",
                "First, we want to study whether the use of various contexts can help the user to better identify term relevance, while not sacrificing the simplicity and compactness of term feedback.",
                "Second, currently all terms are presented to the user in a single batch.",
                "We could instead consider iterative term feedback, by presenting a small number of terms first, and show more terms after receiving user feedback or stop when the refined query is good enough.",
                "The presented terms should be selected dynamically to maximize learning benefits at any moment.",
                "Third, we have plans to incorporate term feedback into our UCAIR toolbar[20], an Internet Explorer plugin, to make it work for web search.",
                "We are also interested in studying how to combine term feedback with relevance feedback or implicit feedback.",
                "We could, for example, allow the user to dynamically modify terms in a language model learned from feedback documents. 8.",
                "ACKNOWLEDGMENT This work is supported in part by the National Science Foundation grants IIS-0347933 and IIS-0428472. 9.",
                "REFERENCES [1] J. Allan.",
                "Relevance feedback with too much data.",
                "In Proceedings of the 18th annual international ACM SIGIR conference on research and development in information retrieval, pages 337-343, 1995. [2] J. Allan.",
                "HARD track overview in TREC 2005 - High Accuracy Retrieval from Documents.",
                "In The Fourteenth Text REtrieval Conference, 2005. [3] P. Anick.",
                "Using terminological feedback for web search refinement: a log-based study.",
                "In Proceedings of the 26th annual international ACM SIGIR conference on research and development in informaion retrieval, pages 88-95, 2003. [4] P. G. Anick and S. Tipirneni.",
                "The paraphrase search assistant: terminological feedback for iterative information seeking.",
                "In Proceedings of the 22nd annual international ACM SIGIR conference on research and development in information retrieval, pages 153-159, 1999. [5] C. Buckley, G. Salton, J. Allan, and A. Singhal.",
                "Automatic <br>query expansion</br> using SMART.",
                "In Proceedings of the Third Text REtrieval Conference, 1994. [6] D. Harman.",
                "Towards interactive <br>query expansion</br>.",
                "In Proceedings of the 11th annual international ACM SIGIR conference on research and development in information retrieval, pages 321-331, 1988. [7] N. A. Jaleel, A. Corrada-Emmanuel, Q. Li, X. Liu, C. Wade, and J. Allan.",
                "UMass at TREC 2003: HARD and QA.",
                "In TREC, pages 715-725, 2003. [8] H. Joho, C. Coverson, M. Sanderson, and M. Beaulieu.",
                "Hierarchical presentation of expansion terms.",
                "In Proceedings of the 2002 ACM symposium on applied computing, pages 645-649, 2002. [9] K. S. Jones, S. Walker, and S. E. Robertson.",
                "A probabilistic model of information retrieval: development and status.",
                "Technical Report 446, Computer Laboratory, University of Cambridge, 1998. [10] D. Kelly, V. D. Dollu, and X. Fu.",
                "The loquacious user: a document-independent source of terms for <br>query expansion</br>.",
                "In Proceedings of the 28th annual international ACM SIGIR conference on research and development in information retrieval, pages 457-464, 2005. [11] D. Kelly and X. Fu.",
                "Elicitation of term relevance feedback: an investigation of term source and context.",
                "In Proceedings of the 29th annual international ACM SIGIR conference on research and development in information retrieval, 2006. [12] J. Koenemann and N. Belkin.",
                "A case for interaction: A study of interactive information retrieval behavior and effectiveness.",
                "In Proceedings of the SIGCHI conference on human factors in computing systems, pages 205-212, 1996. [13] V. Lavrenko and W. B. Croft.",
                "Relevance-based language models.",
                "In Research and Development in Information Retrieval, pages 120-127, 2001. [14] Y. Nemeth, B. Shapira, and M. Taeib-Maimon.",
                "Evaluation of the real and perceived value of automatic and interactive <br>query expansion</br>.",
                "In Proceedings of the 27th annual international ACM SIGIR conference on research and development in information retrieval, pages 526-527, 2004. [15] J. Ponte.",
                "A Language Modeling Approach to Information Retrieval.",
                "PhD thesis, University of Massachusetts at Amherst, 1998. [16] S. E. Robertson, S. Walker, S. Jones, M. Beaulieu, and M. Gatford.",
                "Okapi at TREC-3.",
                "In Proceedings of the Third Text REtrieval Conference, 1994. [17] J. Rocchio.",
                "Relevance feedback in information retrieval.",
                "In The SMART retrieval system, pages 313-323. 1971. [18] I. Ruthven.",
                "Re-examining the potential effectiveness of interactive <br>query expansion</br>.",
                "In Proceedings of the 26th annual international ACM SIGIR conference on research and development in informaion retrieval, pages 213-220, 2003. [19] G. Salton and C. Buckley.",
                "Improving retrieval performance by relevance feedback.",
                "Journal of the American Society for Information Science, 41:288-297, 1990. [20] X. Shen, B. Tan, and C. Zhai.",
                "Implicit user modeling for personalized search.",
                "In Proceedings of the 14th ACM international conference on information and knowledge management, pages 824-831, 2005. [21] X. Shen and C. Zhai.",
                "Active feedback in ad-hoc information retrieval.",
                "In Proceedings of the 28th annual international ACM SIGIR conference on research and development in information retrieval, pages 59-66, 2005. [22] A. Spink.",
                "Term relevance feedback and <br>query expansion</br>: relation to design.",
                "In Proceedings of the 17th annual international ACM SIGIR conference on research and development in information retrieval, pages 81-90, 1994. [23] J. Xu and W. B. Croft.",
                "<br>query expansion</br> using local and global document analysis.",
                "In Proceedings of the 19th annual international ACM SIGIR conference on research and development in information retrieval, pages 4-11, 1996. [24] H. Zaragoza, N. Craswell, M. Taylor, S. Saria, and S. Robertson.",
                "Microsoft cambridge at TREC-13: Web and HARD tracks.",
                "In Proceedings of the 13th Text REtrieval Conference, 2004. [25] C. Zhai and J. Lafferty.",
                "Model-based feedback in the language modeling approach to information retrieval.",
                "In Proceedings of the tenth international conference on information and knowledge management, pages 403-410, 2001. [26] C. Zhai, A. Velivelli, and B. Yu.",
                "A cross-collection mixture model for comparative text mining.",
                "In Proceedings of the tenth ACM SIGKDD international conference on knowledge discovery and data mining, pages 743-748, 2004."
            ],
            "original_annotated_samples": [
                "With term feedback a user directly judges the relevance of individual terms without interaction with feedback documents, taking full control of the <br>query expansion</br> process.",
                "It has the disadvantage that irrelevant terms, which occur along with relevant ones in the judged content, may be erroneously used for <br>query expansion</br>, causing undesired effects.",
                "This is categorized as interactive <br>query expansion</br>, where the original query is augmented with user-provided terms, which can come from direct user input (free-form text or keywords)[22, 7, 10] or user selection of system-suggested terms (using thesauri[6, 22] or extracted from feedback documents[6, 22, 12, 4, 7]).",
                "For example, the study in [12] shows that the user prefers to have explicit knowledge and direct control of which terms are used for <br>query expansion</br>, and the penetrable interface that provides this freedom is shown to perform better than other interfaces.",
                "In a simulated study carried out in [18], the author compares the retrieval performance of interactive <br>query expansion</br> and automatic <br>query expansion</br> with a simulated study, and suggests that the potential benefits of the former can be hard to achieve."
            ],
            "translated_annotated_samples": [
                "Con la retroalimentación de términos, un usuario juzga directamente la relevancia de términos individuales sin interacción con documentos de retroalimentación, tomando el control total del proceso de <br>expansión de la consulta</br>.",
                "Tiene la desventaja de que los términos irrelevantes, que ocurren junto con los relevantes en el contenido evaluado, pueden ser utilizados erróneamente para la <br>expansión de la consulta</br>, causando efectos no deseados.",
                "Esto se clasifica como <br>expansión interactiva de consultas</br>, donde la consulta original se amplía con términos proporcionados por el usuario, que pueden provenir de la entrada directa del usuario (texto libre o palabras clave) o de la selección del usuario de términos sugeridos por el sistema (utilizando tesauros o extraídos de documentos de retroalimentación).",
                "Por ejemplo, el estudio en [12] muestra que el usuario prefiere tener un conocimiento explícito y control directo de qué términos se utilizan para la <br>expansión de la consulta</br>, y se demuestra que la interfaz penetrable que proporciona esta libertad tiene un mejor rendimiento que otras interfaces.",
                "En un estudio simulado realizado en [18], el autor compara el rendimiento de recuperación de la expansión interactiva de consultas y la expansión automática de consultas con un estudio simulado, y sugiere que los posibles beneficios de la primera pueden ser difíciles de lograr."
            ],
            "translated_text": "Comentarios sobre el término para la recuperación de información con modelos de lenguaje Bin Tan†, Atulya Velivelli‡, Hui Fang†, ChengXiang Zhai† Dept. En este documento estudiamos la retroalimentación basada en términos para la recuperación de información en el enfoque de modelado de lenguaje. Con la retroalimentación de términos, un usuario juzga directamente la relevancia de términos individuales sin interacción con documentos de retroalimentación, tomando el control total del proceso de <br>expansión de la consulta</br>. Proponemos un método basado en clusters para seleccionar términos para presentar al usuario para su evaluación, así como algoritmos efectivos para construir modelos de lenguaje de consulta refinados a partir de la retroalimentación de términos del usuario. Nuestros algoritmos han demostrado proporcionar una mejora significativa en la precisión de recuperación en comparación con una línea base sin retroalimentación, y logran un rendimiento comparable al de la retroalimentación de relevancia. Son útiles incluso cuando no hay documentos relevantes en la parte superior. Categorías y Descriptores de Asignaturas H.3.3 [Búsqueda y Recuperación de Información]: Modelos de recuperación Términos generales Algoritmos 1. En el enfoque de modelado del lenguaje para la recuperación de información, la retroalimentación a menudo se modela como la estimación de un modelo de consulta mejorado o un modelo de relevancia basado en un conjunto de documentos de retroalimentación [25, 13]. Esto está en línea con la forma tradicional de realizar retroalimentación de relevancia: presentar al usuario documentos/pasajes para su juicio de relevancia y luego extraer términos de los documentos o pasajes juzgados para expandir la consulta inicial. Es una forma indirecta de buscar la ayuda de los usuarios para la construcción del modelo de consulta, en el sentido de que el modelo de consulta refinado (basado en términos) se aprende a través de documentos/pasajes de retroalimentación, que son estructuras de alto nivel de términos. Tiene la desventaja de que los términos irrelevantes, que ocurren junto con los relevantes en el contenido evaluado, pueden ser utilizados erróneamente para la <br>expansión de la consulta</br>, causando efectos no deseados. Por ejemplo, para la consulta TREC del telescopio Hubble logros, cuando un documento relevante habla más sobre la reparación de los telescopios que sobre sus descubrimientos, términos irrelevantes como caminata espacial pueden ser agregados a la consulta modificada. Podemos considerar una forma más directa de involucrar a un usuario en la mejora del modelo de consulta, sin un paso intermedio de retroalimentación de documentos que pueda introducir ruido. La idea es presentar al usuario un número (razonable) de términos individuales y pedirle que juzgue la relevancia de cada término o especifique directamente sus probabilidades en el modelo de consulta. Esta estrategia ha sido discutida en [15], pero hasta donde sabemos, no ha sido estudiada seriamente en la literatura existente de modelado de lenguaje. En comparación con la retroalimentación de relevancia tradicional, este enfoque basado en términos para el refinamiento del modelo de consulta interactivo tiene varias ventajas. Primero, el usuario tiene un mejor control del modelo de consulta final a través de la manipulación directa de términos: él/ella puede dictar qué términos son relevantes, irrelevantes y posiblemente, en qué medida. Esto evita el riesgo de introducir términos no deseados en el modelo de consulta, aunque a veces el usuario introduce términos de baja calidad. Segundo, dado que un término tarda menos tiempo en ser evaluado que el texto completo o el resumen de un documento, y con tan solo alrededor de 20 términos presentados se puede lograr una mejora significativa en el rendimiento de recuperación (como demostraremos más adelante), el feedback de términos permite recopilar la retroalimentación del usuario de manera más rápida. Esto es especialmente útil para la búsqueda interactiva ad hoc. En tercer lugar, a veces no hay documentos relevantes en los primeros N resultados recuperados inicialmente si el tema es difícil. Esto suele ser cierto cuando N está limitado a ser pequeño, lo cual surge del hecho de que el usuario no está dispuesto a evaluar demasiados documentos. En este caso, la retroalimentación de relevancia es inútil, ya que no se puede aprovechar ningún documento relevante, pero la retroalimentación de términos sigue siendo útil, al permitir seleccionar términos relevantes de documentos irrelevantes. Durante nuestra participación en la pista HARD de TREC 2005 y el estudio continuado posteriormente, exploramos cómo aprovechar la retroalimentación de términos por parte del usuario para construir modelos de consulta mejorados para la recuperación de información en el enfoque de modelado de lenguaje. Identificamos dos sub tareas clave del feedback basado en términos, es decir, la selección de términos para la presentación previa al feedback y la construcción del modelo de consulta posterior al feedback, con algoritmos efectivos desarrollados para ambos. Impusimos una estructura de clúster secundaria en los términos y descubrimos que una vista de clúster arroja una mayor comprensión de la necesidad de información de los usuarios, y proporciona una buena forma de utilizar la retroalimentación de términos. A través de experimentos encontramos que el feedback a término mejora significativamente sobre la línea base sin feedback, a pesar de que el usuario a menudo comete errores en la evaluación de relevancia. Entre nuestros algoritmos, el que tiene el mejor rendimiento de recuperación es TCFB, la combinación de TFB, el algoritmo de retroalimentación de términos directos, y CFB, el algoritmo de retroalimentación basado en clústeres. También variamos el número de términos de retroalimentación y observamos una mejora razonable incluso con números bajos. Finalmente, al comparar la retroalimentación a nivel de términos con la retroalimentación a nivel de documentos, encontramos que es una alternativa viable a esta última con un rendimiento competitivo en la recuperación. El resto del documento está organizado de la siguiente manera. La sección 2 discute algunos trabajos relacionados. La sección 4 describe nuestro enfoque general para la retroalimentación de términos. Presentamos nuestro método para la selección de términos de presentación en la Sección 3 y los algoritmos para la construcción del modelo de consulta en la Sección 5. Los resultados del experimento se presentan en la Sección 6. La sección 7 concluye este documento. 2. TRABAJO RELACIONADO La retroalimentación de relevancia[17, 19] ha sido reconocida durante mucho tiempo como un método efectivo para mejorar el rendimiento de recuperación. Normalmente, los N documentos principales recuperados utilizando la consulta original se presentan al usuario para su evaluación, después de lo cual se extraen términos de los documentos relevantes evaluados, ponderados por su potencial de atraer más documentos relevantes, y se añaden al modelo de consulta. La consulta ampliada suele representar mejor la necesidad de información de los usuarios que la original, que a menudo es solo una consulta de palabras clave cortas. Una segunda iteración de recuperación utilizando esta consulta modificada generalmente produce un aumento significativo en la precisión de la recuperación. En los casos en los que no se dispone de una evaluación de relevancia real y se asume que todos los documentos principales N son relevantes, se denomina retroalimentación ciega o pseudo [5, 16] y generalmente sigue mejorando el rendimiento. Debido a que un documento es una unidad de texto grande, cuando se utiliza para retroalimentación de relevancia, muchos términos irrelevantes pueden ser introducidos en el proceso de retroalimentación. Para superar esto, se propone el feedback de pasajes y se muestra que mejora el rendimiento del feedback. Una solución más directa es pedir al usuario su juicio de relevancia sobre los términos de retroalimentación. Por ejemplo, en algunos sistemas de retroalimentación de relevancia como [12], hay un paso de interacción que permite al usuario agregar o eliminar términos de expansión después de que son extraídos automáticamente de los documentos relevantes. Esto se clasifica como <br>expansión interactiva de consultas</br>, donde la consulta original se amplía con términos proporcionados por el usuario, que pueden provenir de la entrada directa del usuario (texto libre o palabras clave) o de la selección del usuario de términos sugeridos por el sistema (utilizando tesauros o extraídos de documentos de retroalimentación). En muchos casos, se ha encontrado que la retroalimentación de relevancia de términos mejora de manera efectiva el rendimiento de recuperación [6, 22, 12, 4, 10]. Por ejemplo, el estudio en [12] muestra que el usuario prefiere tener un conocimiento explícito y control directo de qué términos se utilizan para la <br>expansión de la consulta</br>, y se demuestra que la interfaz penetrable que proporciona esta libertad tiene un mejor rendimiento que otras interfaces. Sin embargo, en algunos otros casos no hay un beneficio significativo[3, 14], incluso si al usuario le gusta interactuar con los términos de expansión. En un estudio simulado realizado en [18], el autor compara el rendimiento de recuperación de la expansión interactiva de consultas y la expansión automática de consultas con un estudio simulado, y sugiere que los posibles beneficios de la primera pueden ser difíciles de lograr. ",
            "candidates": [],
            "error": [
                [
                    "expansión de la consulta",
                    "expansión de la consulta",
                    "expansión interactiva de consultas",
                    "expansión de la consulta"
                ]
            ]
        },
        "interactive retrieval": {
            "translated_key": "recuperación interactiva",
            "is_in_text": false,
            "original_annotated_sentences": [
                "Term Feedback for Information Retrieval with Language Models Bin Tan† , Atulya Velivelli‡ , Hui Fang† , ChengXiang Zhai† Dept.",
                "of Computer Science† , Dept. of Electrical and Computer Engineering‡ University of Illinois at Urbana-Champaign bintan@cs.uiuc.edu, velivell@ifp.uiuc.edu, hfang@cs.uiuc.edu, czhai@cs.uiuc.edu ABSTRACT In this paper we study term-based feedback for information retrieval in the language modeling approach.",
                "With term feedback a user directly judges the relevance of individual terms without interaction with feedback documents, taking full control of the query expansion process.",
                "We propose a cluster-based method for selecting terms to present to the user for judgment, as well as effective algorithms for constructing refined query language models from user term feedback.",
                "Our algorithms are shown to bring significant improvement in retrieval accuracy over a non-feedback baseline, and achieve comparable performance to relevance feedback.",
                "They are helpful even when there are no relevant documents in the top.",
                "Categories and Subject Descriptors H.3.3 [Information Search and Retrieval]: Retrieval models General Terms Algorithms 1.",
                "INTRODUCTION In the language modeling approach to information retrieval, feedback is often modeled as estimating an improved query model or relevance model based on a set of feedback documents [25, 13].",
                "This is in line with the traditional way of doing relevance feedback - presenting a user with documents/passages for relevance judgment and then extracting terms from the judged documents or passages to expand the initial query.",
                "It is an indirect way of seeking users assistance for query model construction, in the sense that the refined query model (based on terms) is learned through feedback documents/passages, which are high-level structures of terms.",
                "It has the disadvantage that irrelevant terms, which occur along with relevant ones in the judged content, may be erroneously used for query expansion, causing undesired effects.",
                "For example, for the TREC query Hubble telescope achievements, when a relevant document talks more about the telescopes repair than its discoveries, irrelevant terms such as spacewalk can be added into the modified query.",
                "We can consider a more direct way to involve a user in query model improvement, without an intermediary step of document feedback that can introduce noise.",
                "The idea is to present a (reasonable) number of individual terms to the user and ask him/her to judge the relevance of each term or directly specify their probabilities in the query model.",
                "This strategy has been discussed in [15], but to our knowledge, it has not been seriously studied in existing language modeling literature.",
                "Compared to traditional relevance feedback, this term-based approach to interactive query model refinement has several advantages.",
                "First, the user has better control of the final query model through direct manipulation of terms: he/she can dictate which terms are relevant, irrelevant, and possibly, to what degree.",
                "This avoids the risk of bringing unwanted terms into the query model, although sometimes the user introduces low-quality terms.",
                "Second, because a term takes less time to judge than a documents full text or summary, and as few as around 20 presented terms can bring significant improvement in retrieval performance (as we will show later), term feedback makes it faster to gather user feedback.",
                "This is especially helpful for interactive adhoc search.",
                "Third, sometimes there are no relevant documents in the top N of the initially retrieved results if the topic is hard.",
                "This is often true when N is constrained to be small, which arises from the fact that the user is unwilling to judge too many documents.",
                "In this case, relevance feedback is useless, as no relevant document can be leveraged on, but term feedback is still often helpful, by allowing relevant terms to be picked from irrelevant documents.",
                "During our participation in the TREC 2005 HARD Track and continued study afterward, we explored how to exploit term feedback from the user to construct improved query models for information retrieval in the language modeling approach.",
                "We identified two key subtasks of term-based feedback, i.e., pre-feedback presentation term selection and post-feedback query model construction, with effective algorithms developed for both.",
                "We imposed a secondary cluster structure on terms and found that a cluster view sheds additional insight into the users information need, and provides a good way of utilizing term feedback.",
                "Through experiments we found that term feedback improves significantly over the nonfeedback baseline, even though the user often makes mistakes in relevance judgment.",
                "Among our algorithms, the one with best retrieval performance is TCFB, the combination of TFB, the direct term feedback algorithm, and CFB, the cluster-based feedback algorithm.",
                "We also varied the number of feedback terms and observed reasonable improvement even at low numbers.",
                "Finally, by comparing term feedback with document-level feedback, we found it to be a viable alternative to the latter with competitive retrieval performance.",
                "The rest of the paper is organized as follows.",
                "Section 2 discusses some related work.",
                "Section 4 outlines our general approach to term feedback.",
                "We present our method for presentation term selection in Section 3 and algorithms for query model construction in Section 5.",
                "The experiment results are given in Section 6.",
                "Section 7 concludes this paper. 2.",
                "RELATED WORK Relevance feedback[17, 19] has long been recognized as an effective method for improving retrieval performance.",
                "Normally, the top N documents retrieved using the original query are presented to the user for judgment, after which terms are extracted from the judged relevant documents, weighted by their potential of attracting more relevant documents, and added into the query model.",
                "The expanded query usually represents the users information need better than the original one, which is often just a short keyword query.",
                "A second iteration of retrieval using this modified query usually produces significant increase in retrieval accuracy.",
                "In cases where true relevance judgment is unavailable and all top N documents are assumed to be relevant, it is called blind or pseudo feedback[5, 16] and usually still brings performance improvement.",
                "Because document is a large text unit, when it is used for relevance feedback many irrelevant terms can be introduced into the feedback process.",
                "To overcome this, passage feedback is proposed and shown to improve feedback performance[1, 23].",
                "A more direct solution is to ask the user for their relevance judgment of feedback terms.",
                "For example, in some relevance feedback systems such as [12], there is an interaction step that allows the user to add or remove expansion terms after they are automatically extracted from relevant documents.",
                "This is categorized as interactive query expansion, where the original query is augmented with user-provided terms, which can come from direct user input (free-form text or keywords)[22, 7, 10] or user selection of system-suggested terms (using thesauri[6, 22] or extracted from feedback documents[6, 22, 12, 4, 7]).",
                "In many cases term relevance feedback has been found to effectively improve retrieval performance[6, 22, 12, 4, 10].",
                "For example, the study in [12] shows that the user prefers to have explicit knowledge and direct control of which terms are used for query expansion, and the penetrable interface that provides this freedom is shown to perform better than other interfaces.",
                "However, in some other cases there is no significant benefit[3, 14], even if the user likes interacting with expansion terms.",
                "In a simulated study carried out in [18], the author compares the retrieval performance of interactive query expansion and automatic query expansion with a simulated study, and suggests that the potential benefits of the former can be hard to achieve.",
                "The user is found to be not good at identifying useful terms for query expansion, when a simple term presentation interface is unable to provide sufficient semantic context of the feedback terms.",
                "Our work differs from the previous ones in two important aspects.",
                "First, when we choose terms to present to the user for relevance judgment, we not only consider single-term value (e.g., the relative frequency of a term in the top documents, which can be measured by metrics such as Robertson Selection Value and Simplified Kullback-Leibler Distance as listed in [24]), but also examine the cluster structure of the terms, so as to produce a balanced coverage of the different topic aspects.",
                "Second, with the language modelling framework, we allow an elaborate construction of the updated query model, by setting different probabilities for different terms based on whether it is a query term, its significance in the top documents, and its cluster membership.",
                "Although techniques for adjusting query term weights exist for vector space models[17] and probablistic relevance models[9], most of the aforementioned works do not use them, choosing to just append feedback terms to the original query (thus using equal weights for them), which can lead to poorer retrieval performance.",
                "The combination of the two aspects allows our method to perform much better than the baseline.",
                "The usual way for feedback term presentation is just to display the terms in a list.",
                "There have been some works on alternative user interfaces. [8] arranges terms in a hierarchy, and [11] compares three different interfaces, including terms + checkboxes, terms + context (sentences) + checkboxes, sentences + input text box.",
                "In both studies, however, there is no significant performance difference.",
                "In our work we adopt the simplest approach of terms + checkboxes.",
                "We focus on term presentation and query model construction from feedback terms, and believe using contexts to improve feedback term quality should be orthogonal to our method. 3.",
                "GENERAL APPROACH We follow the language modeling approach, and base our method on the KL-divergence retrieval model proposed in [25].",
                "With this model, the retrieval task involves estimating a query language model θq from a given query, a document language model θd from each document, and calculating their KL-divergence D(θq||θd), which is then used to score the documents. [25] treats relevance feedback as a query model re-estimation problem, i.e., computing an updated query model θq given the original query text and the extra evidence carried by the judged relevant documents.",
                "We adopt this view, and cast our task as updating the query model from user term feedback.",
                "There are two key subtasks here: First, how to choose the best terms to present to the user for judgment, in order to gather maximal evidence about the users information need.",
                "Second, how to compute an updated query model based on this term feedback evidence, so that it captures the users information need and translates into good retrieval performance. 4.",
                "PRESENTATION TERM SELECTION Proper selection of terms to be presented to the user for judgment is crucial to the success of term feedback.",
                "If the terms are poorly chosen and there are few relevant ones, the user will have a hard time looking for useful terms to help clarify his/her information need.",
                "If the relevant terms are plentiful, but all concentrate on a single aspect of the query topic, then we will only be able to get feedback on that aspect and missing others, resulting in a breadth loss in retrieved results.",
                "Therefore, it is important to carefully select presentation terms to maximize expected gain from user feedback, i.e., those that can potentially reveal most evidence of the users information need.",
                "This is similar to active feedback[21], which suggests that a retrieval system should actively probe the users information need, and in the case of relevance feedback, the feedback documents should be chosen to maximize learning benefits (e.g. diversely so as to increase coverage).",
                "In our approach, the top N documents from an initial retrieval using the original query form the source of feedback terms: all terms that appear in them are considered candidates to present to the user.",
                "These documents serve as pseudo-feedback, since they provide a much richer context than the original query (usually very short), while the user is not asked to judge their relevance.",
                "Due to the latter reason, it is possible to make N quite large (e.g., in our experiments we set N = 60) to increase its coverage of different aspects in the topic.",
                "The simplest way of selecting feedback terms is to choose the most frequent M terms from the N documents.",
                "This method, however, has two drawbacks.",
                "First, a lot of common noisy terms will be selected due to their high frequencies in the document collection, unless a stop-word list is used for filtering.",
                "Second, the presentation list will tend to be filled by terms from major aspects of the topic; those from a minor aspect are likely to be missed due to their relatively low frequencies.",
                "We solve the above problems by two corresponding measures.",
                "First, we introduce a background model θB that is estimated from collection statistics and explains the common terms, so that they are much less likely to appear in the presentation list.",
                "Second, the terms are selected from multiple clusters in the pseudo-feedback documents, to ensure sufficient representation of different aspects of the topic.",
                "We rely on the mixture multinomial model, which is used for theme discovery in [26].",
                "Specifically, we assume the N documents contain K clusters {Ci| i = 1, 2, · · · K}, each characterized by a multinomial word distribution (also known as unigram language model) θi and corresponding to an aspect of the topic.",
                "The documents are regarded as sampled from a mixture of K + 1 components, including the K clusters and the background model: p(w|d) = λBp(w|θB) + (1 − λB) K i=1 πd,ip(w|θi) where w is a word, λB is the mixture weight for the background model θB, and πd,i is the document-specific mixture weight for the i-th cluster model θi.",
                "We then estimate the cluster models by maximizing the probability of the pseudo-feedback documents being generated from the multinomial mixture model: log p(D|Λ) = d∈D w∈V c(w; d) log p(w|d) where D = {di| i = 1, 2, · · · N} is the set of the N documents, V is the vocabulary, c(w; d) is ws frequency in d and Λ = {θi| i = 1, 2, · · · K} ∪ {πdij | i = 1, 2, · · · N, j = 1, 2, · · · K} is the set of model parameters to estimate.",
                "The cluster models can be efficiently estimated using the Expectation-Maximization (EM) algorithm.",
                "For its details, we refer the reader to [26].",
                "Table 1 shows the cluster models for TREC query Transportation tunnel disasters (K = 3).",
                "Note that only the middle cluster is relevant.",
                "Table 1: Cluster models for topic 363 Transportation tunnel disasters Cluster 1 Cluster 2 Cluster 3 tunnel 0.0768 tunnel 0.0935 tunnel 0.0454 transport 0.0364 fire 0.0295 transport 0.0406 traffic 0.0206 truck 0.0236 toll 0.0166 railwai 0.0186 french 0.0220 amtrak 0.0153 harbor 0.0146 smoke 0.0157 train 0.0129 rail 0.0140 car 0.0154 airport 0.0122 bridg 0.0139 italian 0.0152 turnpik 0.0105 kilomet 0.0136 firefight 0.0144 lui 0.0095 truck 0.0133 blaze 0.0127 jersei 0.0093 construct 0.0131 blanc 0.0121 pass 0.0087 · · · · · · · · · From each of the K estimated clusters, we choose the L = M/K terms with highest probabilities to form a total of M presentation terms.",
                "If a term happens to be in top L in multiple clusters, we assign it to the cluster where it has highest probability and let the other clusters take one more term as compensation.",
                "We also filter out terms in the original query text because they tend to always be relevant when the query is short.",
                "The selected terms are then presented to the user for judgment.",
                "A sample (completed) feedback form is shown in Figure 1.",
                "In this study we only deal with binary judgment: a presented term is by default unchecked, and a user may check it to indicate relevance.",
                "We also do not explicitly exploit negative feedback (i.e., penalizing irrelevant terms), because with binary feedback an unchecked term is not necessarily irrelevant (maybe the user is unsure about its relevance).",
                "We could ask the user for finer judgment (e.g., choosing from highly relevant, somewhat relevant, do not know, somewhat irrelevant and highly irrelevant), but binary feedback is more compact, taking less space to display and less user effort to make judgment. 5.",
                "ESTIMATING QUERY MODELS FROM TERM FEEDBACK In this section, we present several algorithms for exploiting term feedback.",
                "The algorithms take as input the original query q, the clusters {θi} as generated by the theme discovery algorithm, the set of feedback terms T and their relevance judgment R, and outputs an updated query language model θq that makes best use of the feedback evidence to capture the users information need.",
                "First we describe our notations: • θq: The original query model, derived from query terms only: p(w|θq) = c(w; q) |q| where c(w; q) is the count of w in q, and |q| = w∈q c(w; q) is the query length. • θq : The updated query model which we need to estimate from term feedback. • θi (i = 1, 2, . . .",
                "K): The unigram language model of cluster Ci, as estimated using the theme discovery algorithm. • T = {ti,j} (i = 1 . . .",
                "K, j = 1 . . .",
                "L): The set of terms presented to the user for judgment. ti,j is the j-th term chosen from cluster Ci. • R = {δw|w ∈ T}: δw is an indicator variable that is 1 if w is judged relevant or 0 otherwise. 5.1 TFB (Direct Term Feedback) This is a straight-forward form of term feedback that does not involve any secondary structure.",
                "We give a weight of 1 to terms judged relevant by the user, a weight of μ to query terms, zero weight to other terms, and then apply normalization: p(w|θq ) = δw + μ c(w; q) w ∈T δw + μ|q| where w ∈T δw is the total number of terms that are judged relevant.",
                "We call this method TFB (direct Term FeedBack).",
                "If we let μ = 1, this approach is equivalent to appending the relevant terms after the original query, which is what standard query expansion (without term reweighting) does.",
                "If we set μ > 1, we are putting more emphasis on the query terms than the checked ones.",
                "Note that the result model will be more biased toward θq if the original query is long or the user feedback is weak, which makes sense, as we can trust more on the original query in either case.",
                "Figure 1: Filled clarification form for Topic 363 363 transportation tunnel disasters Please select all terms that are relevant to the topic. traffic railway harbor rail bridge kilometer construct swiss cross link kong hong river project meter shanghai fire truck french smoke car italian firefights blaze blanc mont victim franc rescue driver chamonix emerge toll amtrak train airport turnpike lui jersey pass rome z center electron road boston speed bu submit 5.2 CFB (Cluster Feedback) Here we exploit the cluster structure that played an important role when we selected the presentation terms.",
                "The clusters represent different aspects of the query topic, each of which may or may not be relevant.",
                "If we are able to identify the relevant clusters, we can combine them to generate a query model that is good at discovering documents belonging to these clusters (instead of the irrelevant ones).",
                "We could ask the user to directly judge the relevance of a cluster after viewing representative terms in that cluster, but this would sometimes be a difficult task for the user, who has to guess the semantics of a cluster via its set of terms, which may not be well connected to one another due to a lack of context.",
                "Therefore, we propose to learn cluster feedback indirectly, inferring the relevance of a cluster through the relevance of its feedback terms.",
                "Because each cluster has an equal number of terms presented to the user, the simplest measure of a clusters relevance is the number of terms that are judged relevant in it.",
                "Intuitively, the more terms are marked relevant in a cluster, the closer the cluster is to the query topic, and the more the cluster should participate in query modification.",
                "If we combine the cluster models using weights determined this way and then interpolate with the original query model, we get the following formula for query updating, which we call CFB (Cluster FeedBack): p(w|θq ) = λp(w|θq) + (1 − λ) K i=1 L j=1 δti,j K k=1 L j=1 δtk,j p(w|θi) where L j=1 δti,j is the number of relevant terms in cluster Ci, and K k=1 L j=1 δtk,j is the total number of relevant terms.",
                "We note that when there is only one cluster (K = 1), the above formula degenerates to p(w|θq ) = λp(w|θq) + (1 − λ)p(w|θ1) which is merely pseudo-feedback of the form proposed in [25]. 5.3 TCFB (Term-cluster Feedback) TFB and CFB both have their drawbacks.",
                "TFB assigns non-zero probabilities to the presented terms that are marked relevant, but completely ignores (a lot more) others, which may be left unchecked due to the users ignorance, or simply not included in the presentation list, but we should be able to infer their relevance from the checked ones.",
                "For example, in Figure 1, since as many as 5 terms in the middle cluster (the third and fourth columns) are checked, we should have high confidence in the relevance of other terms in that cluster.",
                "CFB remedies TFBs problem by treating the terms in a cluster collectively, so that unchecked/unpresented terms receive weights when presented terms in their clusters are judged as relevant, but it does not distinguish which terms in a cluster are presented or judged.",
                "Intuitively, the judged relevant terms should receive larger weights because they are explicitly indicated as relevant by the user.",
                "Therefore, we try to combine the two methods, hoping to get the best out of both.",
                "We do this by interpolating the TFB model with the CFB model, and call it TCFB: p(w|θq ) = αp(w|θqT F B ) + (1 − α)p(w|θqCF B ) 6.",
                "EXPERIMENTS In this section, we describe our experiment results.",
                "We first describe our experiment setup and present an overview of various methods performance.",
                "Then we discuss the effects of varying the parameter setting in the algorithms, as well as the number of presentation terms.",
                "Next we analyze user term feedback behavior and its relation to retrieval performance.",
                "Finally we compare term feedback to relevance feedback and show that it has its particular advantage. 6.1 Experiment Setup and Basic Results We took the opportunity of TREC 2005 HARD Track[2] for the evaluation of our algorithms.",
                "The tracks used the AQUAINT collection, a 3GB corpus of English newswire text.",
                "The topics included 50 ones previously known to be hard, i.e. with low retrieval performance.",
                "It is for these hard topics that user feedback is most helpful, as it can provide information to disambiguate the queries; with easy topics the user may be unwilling to spend efforts for feedback if the automatic retrieval results are good enough.",
                "Participants of the track were able to submit custom-designed clarification forms (CF) to solicit feedback from human assessors provided by Table 2: Retrieval performance for different methods and CF types.",
                "The last row is the percentage of MAP improvement over the baseline.",
                "The parameter settings μ = 4, λ = 0.1, α = 0.3 are near optimal.",
                "Baseline TFB1C TFB3C TFB6C CFB1C CFB3C CFB6C TCFB1C TCFB3C TCFB6C MAP 0.219 0.288 0.288 0.278 0.254 0.305 0.301 0.274 0.309 0.304 Pr@30 0.393 0.467 0.475 0.457 0.399 0.480 0.473 0.431 0.491 0.473 RR 4339 4753 4762 4740 4600 4907 4872 4767 4947 4906 % 0% 31.5% 31.5% 26.9% 16.0% 39.3% 37.4% 25.1% 41.1% 38.8% Table 3: MAP variation with the number of presented terms. # terms TFB1C TFB3C TFB6C CFB3C CFB6C TCFB3C TCFB6C 6 0.245 0.240 0.227 0.279 0.279 0.281 0.274 12 0.261 0.261 0.242 0.299 0.286 0.297 0.281 18 0.275 0.274 0.256 0.301 0.282 0.300 0.286 24 0.276 0.281 0.265 0.303 0.292 0.305 0.292 30 0.280 0.285 0.270 0.304 0.296 0.307 0.296 36 0.282 0.288 0.272 0.307 0.297 0.309 0.297 42 0.283 0.288 0.275 0.306 0.298 0.309 0.300 48 0.288 0.288 0.278 0.305 0.301 0.309 0.303 NIST.",
                "We designed three sets of clarification forms for term feedback, differing in the choice of K, the number of clusters, and L, the number of presented terms from each cluster.",
                "They are: 1× 48, a big cluster with 48 terms, 3 × 16, 3 clusters with 16 terms each, and 6 × 8, 6 clusters with 8 terms each.",
                "The total number of presented terms (M) is fixed at 48, so by comparing the performance of different types of clarification forms we can know the effects of different degree of clustering.",
                "For each topic, an assessor would complete the forms ordered by 6 × 8, 1 × 48 and 3 × 16, spending up to three minutes on each form.",
                "The sample clarification form shown in Figure 1 is of type 3 × 16.",
                "It is a simple and compact interface in which the user can check relevant terms.",
                "The form is self-explanatory; there is no need for extra user training on how to use it.",
                "Our initinal queries are constructed only using the topic title descriptions, which are on average 2.7 words in length.",
                "As our baseline we use the KL divergence retrieval method implemented in the Lemur Toolkit1 with 5 pseudo-feedback documents.",
                "We stem the terms, choose Dirichlet smoothing with a prior of 2000, and truncate query language models to 50 terms (these settings are used throughout the experiments).",
                "For all other parameters we use Lemurs default settings.",
                "The baseline turns out to perform above average among the track participants.",
                "After an initial run using this baseline retrieval method, we take the top 60 documents for each topic and apply the theme discovery algorithm to output the clusters (1, 3, or 6 of them), based on which we generate clarification forms.",
                "After user feedback is received, we run the term feedback algorithms (TFB, CFB or TCFB) to estimate updated query models, which are then used for a second iteration of retrieval.",
                "We evaluate the different retrieval methods performance on their rankings of the top 1000 documents.",
                "The evaluation metrics we adopt include mean average (non-interpolated) precision (MAP), precision at top 30 (Pr@30) and total relevant retrieved (RR).",
                "Table 2 shows the performance of various methods and configurations of K × L. The suffixes (1C, 3C, 6C) after TFB,CFB,TCFB stand for the number of clusters (K).",
                "For example, TCFB3C means the TCFB method on the 3 × 16 clarification forms.",
                "From Table 2 we can make the following observations: 1 http://www.lemurproject.com 1.",
                "All methods perform considerably better than the pseudofeedback baseline, with TCFB3C achieving a highest 41.1% improvement in MAP, indicating significant contribution of term feedback for clarification of the users information need.",
                "In other words, term feedback is truly helpful for improving retrieval accuracy. 2.",
                "For TFB, the performance is almost equal on the 1 × 48 and 3 × 16 clarification forms in terms of MAP (although the latter is slightly better in Pr@30 and RR), and a little worse on the 6 × 8 ones. 3.",
                "Both CFB3C and CFB6C perform better than their TFB counterparts in all three metrics, suggesting that feedback on a secondary cluster structure is indeed beneficial.",
                "CFB1C is actually worse because it cannot adjust the weight of its (single) cluster from term feedback and it is merely pseudofeedback. 4.",
                "Although TCFB is just a simple mixture of TFB and CFB by interpolation, it is able to outperform both.",
                "This supports our speculation that TCFB overcomes the drawbacks of TFB (paying attention only to checked terms) and CFB (not distinguishing checked and unchecked terms in a cluster).",
                "Except for TCFB6C v.s.",
                "CFB6C, the performance advantage of TCFB over TFB/CFB is significant at p < 0.05 using the Wilcoxon signed rank test.",
                "This is not true in the case of TFB v.s.",
                "CFB, each of which is better than the other in nearly half of the topics. 6.2 Reduction of Presentation Terms In some situations we may have to reduce the number of presentation terms due to limits in display space or user feedback efforts.",
                "It is interesting to know whether our algorithms performance deteriorates when the user is presented with fewer terms.",
                "Because the presentation terms within each cluster are generated in decreasing order of their frequencies, the presentation list forms a subset of the original one if its size is reduced2 .",
                "Therefore, we can easily simulate what happens when the number of presentation terms decreases 2 There are complexities arising from terms appearing in top L of multiple clusters, but these are exceptions from M to M : we will keep all judgments of the top L = M /K terms in each cluster and discard those of others.",
                "Table 3 shows the performance of various algorithms as the number of presentation terms ranges from 6 to 48.",
                "We find that the performance of TFB is more susceptible to presentation term reduction than that of CFB or TCFB.",
                "For example, at 12 terms the MAP of TFB3C is 90.6% of that at 48 terms, while the numbers for CFB3C and TCFB3C are 98.0% and 96.1% respectively.",
                "We conjecture the reason to be that while TFBs performance heavily depends on how many good terms are chosen for query expansion, CFB only needs a rough estimate of cluster weights to work.",
                "Also, the 3 × 16 clarification forms seem to be more robust than the 6 × 8 ones: at 12 terms the MAP of TFB6C is 87.1% of that at 48 terms, lower than 90.6% for TFB3C.",
                "Similarly, for CFB it is 95.0% against 98.0%.",
                "This is natual, as for a large cluster number of 6, it is easier to get into the situation where each cluster gets too few presentation terms to make topic diversification useful.",
                "Overall, we are surprised to see that the algorithms are still able to perform reasonably well when the number of presentation terms is small.",
                "For example, at only 12 terms CFB3C (the clarification form is of size 3 × 4) can still improve 36.5% over the baseline, dropping slightly from 39.3% at 48 terms. 6.3 User Feedback Analysis In this part we study several aspects of users term feedback behavior, and whether they are connected to retrieval performance.",
                "Figure 2: Clarification form completion time distributions 0−30 30−60 60−90 90−120 120−150 150−180 0 5 10 15 20 25 30 35 completion time (seconds) #topics 1×48 3×16 6×8 Figure 2 shows the distribution of time needed to complete a clarification form3 .",
                "We see that the user is usually able to finish term feedback within a reasonably short amount of time: for more than half of the topics the clarification form is completed in just 1 minute, and only a small fraction of topics (less than 10% for 1 × 48 and 3 × 16) take more than 2 minutes.",
                "This suggests that term feedback is suitable for interactive ad-hoc retrieval, where a user usually does not want to spend too much time on providing feedback.",
                "We find that a user often makes mistakes when judging term relevance.",
                "Sometimes a relevant term may be left out because its connection to the query topic is not obvious to the user.",
                "Other times a dubious term may be included but turns out to be irrelevant.",
                "Take the topic in Figure 1 for example.",
                "There was a fire disaster in Mont 3 The maximal time is 180 seconds, as the NIST assessor would be forced to submit the form at that moment.",
                "Table 4: Term selection statistics (topic average) CF Type 1 × 48 3 × 16 6 × 8 # checked terms 14.8 13.3 11.2 # rel. terms 15.0 12.6 11.2 # rel. checked terms 7.9 6.9 5.9 precision 0.534 0.519 0.527 recall 0.526 0.548 0.527 Blanc Tunnel between France and Italy in 1999, but the user failed to select such keywords as mont, blanc, french and italian due to his/her ignorance of the event.",
                "Indeed, without proper context it would be hard to make perfect judgment.",
                "What is then, the extent to which the user is good at term feedback?",
                "Does it have serious impact on retrieval performance?",
                "To answer these questions, we need a measure of individual terms true relevance.",
                "We adopt the Simplified KL Divergence metric used in [24] to decide query expansion terms as our term relevance measure: σKLD(w) = p(w|R) log p(w|R) p(w|¬R) where p(w|R) is the probability that a relevant document contains term w, and p(w|¬R) is the probability that an irrelevant document contains w, both of which can be easily computed via maximum likelihood estimate given document-level relevance judgment.",
                "If σKLD(w) > 0, w is more likely to appear in relevant documents than irrelevant ones.",
                "We consider a term relevant if its Simplified KL Divergence value is greater than a certain threshold σ0.",
                "We can then define precision and recall of user term judgment accordingly: precision is the fraction of terms checked by the user that are relevant; recall is the fraction of presented relevant terms that are checked by the user.",
                "Table 4 shows the number of checked terms, relevant terms and relevant checked terms when σ0 is set to 1.0, as well as the precision/recall of user term judgment.",
                "Note that when the clarification forms contain more clusters, fewer terms are checked: 14.8 for 1 × 48, 13.3 for 3 × 16 and 11.2 for 6×8.",
                "Similar pattern holds for relevant terms and relevant checked terms.",
                "There seems to be a trade-off between increasing topic diversity by clustering and losing extra relevant terms: when there are more clusters, each of them gets fewer terms to present, which can hurt a major relevant cluster that contains many relevant terms.",
                "Therefore, it is not always helpful to have more clusters, e.g., TFB6C is actually worse than TFB1C.",
                "The major finding we can make from Table 4 is that the user is not particularly good at identifying relevant terms, which echoes the discovery in [18].",
                "In the case of 3 × 16 clarification forms, the average number of terms checked as relevant by the user is 13.3 per topic, and the average number of relevant terms whose σKLD value exceed 1.0 is 12.6.",
                "The user is able to recognize only 6.9 of these terms on average.",
                "Indeed, the precision and recall of user feedback terms (as defined previously) are far from perfect.",
                "On the other hand, If the user had correctly checked all such relevant terms, the performance of our algorithms would have increased a lot, as shown in Table 5.",
                "We see that TFB gets big improvement when there is an oracle who checks all relevant terms, while CFB meets a bottleneck around MAP of 0.325, since all it does is adjust cluster weights, and when the learned weights are close to being accurate, it cannot benefit more from term feedback.",
                "Also note that TCFB fails to outperform TFB, probably because TFB is sufficiently accurate.",
                "Table 5: Change of MAP when using all (and only) relevant terms (σKLD > 1.0) for feedback. original term feedback relevant term feedback TF1 0.288 0.354 TF3 0.288 0.354 TF6 0.278 0.346 CF3 0.305 0.325 CF6 0.301 0.326 TCF3 0.309 0.345 TCF6 0.304 0.341 6.4 Comparison with Relevance Feedback Now we compare term feedback with document-level relevance feedback, in which the user is presented with the top N documents from an initial retrieval and asked to judge their relevance.",
                "The feedback process is simulated using document relevance judgment from NIST.",
                "We use the mixture model based feedback method proposed in [25], with mixture noise set to 0.95 and feedback coefficient set to 0.9.",
                "Comparative evaluation of relevance feedback against other methods is complicated by the fact that some documents have already been viewed during feedback, so it makes no sense to include them in the retrieval results of the second run.",
                "However, this does not hold for term feedback.",
                "Thus, to make it fair w.r.t. users information gain, if the feedback documents are relevant, they should be kept in the top of the ranking; if they are irrelevant, they should be left out.",
                "Therefore, we use relevance feedback to produce a ranking of top 1000 retrieved documents but with every feedback document excluded, and then prepend the relevant feedback documents at the front.",
                "Table 6 shows the performance of relevance feedback for different values of N and compares it with TCFB3C.",
                "Table 6: Performance of relevance feedback for different number of feedback documents (N).",
                "N MAP Pr@30 RR 5 0.302 0.586 4779 10 0.345 0.670 4916 20 0.389 0.772 5004 TCFB3C 0.309 0.491 4947 We see that the performance of TCFB3C is comparable to that of relevance feedback using 5 documents.",
                "Although it is poorer than when there are 10 feedback documents in terms of MAP and Pr@30, it does retrieve more documents (4947) when going down the ranked list.",
                "We try to compare the quality of automatically inserted terms in relevance feedback with that of manually selected terms in term feedback.",
                "This is done by truncating the relevance feedback modified query model to a size equal to the number of checked terms for the same topic.",
                "We can then compare the terms in the truncated model with the checked terms.",
                "Figure 3 shows the distribution of the terms σKLD scores.",
                "We find that term feedback tends to produce expansion terms of higher quality(those with σKLD > 1) compared to relevance feedback (with 10 feedback documents).",
                "This does not contradict the fact that the latter yields higher retrieval performance.",
                "Actually, when we use the truncated query model instead of the intact one refined from relevance feedback, the MAP is only 0.304.",
                "The truth Figure 3: Comparison of expansion term quality between relevance feedback (with 10 feedback documents) and term feedback (with 3 × 16 CFs) −1−0 0−1 1−2 2−3 3−4 4−5 5−6 0 50 100 150 200 250 300 350 σKLD #terms relevance feedback term feedback is, although there are many unwanted terms in the expanded query model from feedback documents, there are also more relevant terms than what the user can possibly select from the list of presentation terms generated with pseudo-feedback documents, and the positive effects often outweights the negative ones.",
                "We are interested to know under what circumstances term feedback has advantage over relevance feedback.",
                "One such situation is when none of the top N feedback documents is relevant, rendering relevance feedback useless.",
                "This is not infrequent, as one might have thought: out of the 50 topics, there are 13 such cases when N = 5, 10 when N = 10, and still 3 when N = 20.",
                "When this happens, one can only back off to the original retrieval method; the power of relevance feedback is lost.",
                "Surprisingly, in 11 out of 13 such cases where relevance feedback seems impossible, the user is able to check at least 2 relevant terms from the 3 × 16 clarification form (we consider term t to be relevant if σKLD(t) > 1.0).",
                "Furthermore, in 10 out of them TCFB3C outperforms the pseudo-feedback baseline, increasing MAP from 0.076 to 0.146 on average (these are particularly hard topics).",
                "We think that there are two possible explanations for this phenomenon of term feedback being active even when relevance feedback does not work: First, even if none of the top N (suppose it is a small number) documents are relevant, we may still find relevant documents in top 60, which is more inclusive but usually unreachable when people are doing relevance feedback in interactive ad-hoc search, from which we can draw feedback terms.",
                "This is true for topic 367 piracy, where the top 10 feedback documents are all about software piracy, yet there are documents between 10-60 that are about piracy on the seas (which is about the real information need), contributing terms such as pirate, ship for selection in the clarification form.",
                "Second, for some topics, a document needs to meet some special condition in order to be relevant.",
                "The top N documents may be related to the topic, but nonetheless irrelevant.",
                "In this case, we may still extract useful terms from these documents, even if they do not qualify as relevant ones.",
                "For example, in topic 639 consumer online shopping, a document needs to mention what contributes to shopping growth to really match the specified information need, hence none of the top 10 feedback documents are regarded as relevant.",
                "But nevertheless, the feedback terms such as retail, commerce are good for query expansion. 7.",
                "CONCLUSIONS In this paper we studied the use of term feedback for interactive information retrieval in the language modeling approach.",
                "We proposed a cluster-based method for selecting presentation terms as well as algorithms to estimate refined query models from user term feedback.",
                "We saw significant improvement in retrieval accuracy brought by term feedback, in spite of the fact that a user often makes mistakes in relevance judgment that hurts its performance.",
                "We found the best-performing algorithm to be TCFB, which benefits from the combination of directly observed term evidence with TFB and indirectly learned cluster relevance with CFB.",
                "When we reduced the number of presentation terms, term feedback is still able to keep much of its performance gain over the baseline.",
                "Finally, we compared term feedback to document-level relevance feedback, and found that TCFB3Cs performance is on a par with the latter with 5 feedback documents.",
                "We regarded term feedback as a viable alternative to traditional relevance feedback, especially when there are no relevant documents in the top.",
                "We propose to extend our work in several ways.",
                "First, we want to study whether the use of various contexts can help the user to better identify term relevance, while not sacrificing the simplicity and compactness of term feedback.",
                "Second, currently all terms are presented to the user in a single batch.",
                "We could instead consider iterative term feedback, by presenting a small number of terms first, and show more terms after receiving user feedback or stop when the refined query is good enough.",
                "The presented terms should be selected dynamically to maximize learning benefits at any moment.",
                "Third, we have plans to incorporate term feedback into our UCAIR toolbar[20], an Internet Explorer plugin, to make it work for web search.",
                "We are also interested in studying how to combine term feedback with relevance feedback or implicit feedback.",
                "We could, for example, allow the user to dynamically modify terms in a language model learned from feedback documents. 8.",
                "ACKNOWLEDGMENT This work is supported in part by the National Science Foundation grants IIS-0347933 and IIS-0428472. 9.",
                "REFERENCES [1] J. Allan.",
                "Relevance feedback with too much data.",
                "In Proceedings of the 18th annual international ACM SIGIR conference on research and development in information retrieval, pages 337-343, 1995. [2] J. Allan.",
                "HARD track overview in TREC 2005 - High Accuracy Retrieval from Documents.",
                "In The Fourteenth Text REtrieval Conference, 2005. [3] P. Anick.",
                "Using terminological feedback for web search refinement: a log-based study.",
                "In Proceedings of the 26th annual international ACM SIGIR conference on research and development in informaion retrieval, pages 88-95, 2003. [4] P. G. Anick and S. Tipirneni.",
                "The paraphrase search assistant: terminological feedback for iterative information seeking.",
                "In Proceedings of the 22nd annual international ACM SIGIR conference on research and development in information retrieval, pages 153-159, 1999. [5] C. Buckley, G. Salton, J. Allan, and A. Singhal.",
                "Automatic query expansion using SMART.",
                "In Proceedings of the Third Text REtrieval Conference, 1994. [6] D. Harman.",
                "Towards interactive query expansion.",
                "In Proceedings of the 11th annual international ACM SIGIR conference on research and development in information retrieval, pages 321-331, 1988. [7] N. A. Jaleel, A. Corrada-Emmanuel, Q. Li, X. Liu, C. Wade, and J. Allan.",
                "UMass at TREC 2003: HARD and QA.",
                "In TREC, pages 715-725, 2003. [8] H. Joho, C. Coverson, M. Sanderson, and M. Beaulieu.",
                "Hierarchical presentation of expansion terms.",
                "In Proceedings of the 2002 ACM symposium on applied computing, pages 645-649, 2002. [9] K. S. Jones, S. Walker, and S. E. Robertson.",
                "A probabilistic model of information retrieval: development and status.",
                "Technical Report 446, Computer Laboratory, University of Cambridge, 1998. [10] D. Kelly, V. D. Dollu, and X. Fu.",
                "The loquacious user: a document-independent source of terms for query expansion.",
                "In Proceedings of the 28th annual international ACM SIGIR conference on research and development in information retrieval, pages 457-464, 2005. [11] D. Kelly and X. Fu.",
                "Elicitation of term relevance feedback: an investigation of term source and context.",
                "In Proceedings of the 29th annual international ACM SIGIR conference on research and development in information retrieval, 2006. [12] J. Koenemann and N. Belkin.",
                "A case for interaction: A study of interactive information retrieval behavior and effectiveness.",
                "In Proceedings of the SIGCHI conference on human factors in computing systems, pages 205-212, 1996. [13] V. Lavrenko and W. B. Croft.",
                "Relevance-based language models.",
                "In Research and Development in Information Retrieval, pages 120-127, 2001. [14] Y. Nemeth, B. Shapira, and M. Taeib-Maimon.",
                "Evaluation of the real and perceived value of automatic and interactive query expansion.",
                "In Proceedings of the 27th annual international ACM SIGIR conference on research and development in information retrieval, pages 526-527, 2004. [15] J. Ponte.",
                "A Language Modeling Approach to Information Retrieval.",
                "PhD thesis, University of Massachusetts at Amherst, 1998. [16] S. E. Robertson, S. Walker, S. Jones, M. Beaulieu, and M. Gatford.",
                "Okapi at TREC-3.",
                "In Proceedings of the Third Text REtrieval Conference, 1994. [17] J. Rocchio.",
                "Relevance feedback in information retrieval.",
                "In The SMART retrieval system, pages 313-323. 1971. [18] I. Ruthven.",
                "Re-examining the potential effectiveness of interactive query expansion.",
                "In Proceedings of the 26th annual international ACM SIGIR conference on research and development in informaion retrieval, pages 213-220, 2003. [19] G. Salton and C. Buckley.",
                "Improving retrieval performance by relevance feedback.",
                "Journal of the American Society for Information Science, 41:288-297, 1990. [20] X. Shen, B. Tan, and C. Zhai.",
                "Implicit user modeling for personalized search.",
                "In Proceedings of the 14th ACM international conference on information and knowledge management, pages 824-831, 2005. [21] X. Shen and C. Zhai.",
                "Active feedback in ad-hoc information retrieval.",
                "In Proceedings of the 28th annual international ACM SIGIR conference on research and development in information retrieval, pages 59-66, 2005. [22] A. Spink.",
                "Term relevance feedback and query expansion: relation to design.",
                "In Proceedings of the 17th annual international ACM SIGIR conference on research and development in information retrieval, pages 81-90, 1994. [23] J. Xu and W. B. Croft.",
                "Query expansion using local and global document analysis.",
                "In Proceedings of the 19th annual international ACM SIGIR conference on research and development in information retrieval, pages 4-11, 1996. [24] H. Zaragoza, N. Craswell, M. Taylor, S. Saria, and S. Robertson.",
                "Microsoft cambridge at TREC-13: Web and HARD tracks.",
                "In Proceedings of the 13th Text REtrieval Conference, 2004. [25] C. Zhai and J. Lafferty.",
                "Model-based feedback in the language modeling approach to information retrieval.",
                "In Proceedings of the tenth international conference on information and knowledge management, pages 403-410, 2001. [26] C. Zhai, A. Velivelli, and B. Yu.",
                "A cross-collection mixture model for comparative text mining.",
                "In Proceedings of the tenth ACM SIGKDD international conference on knowledge discovery and data mining, pages 743-748, 2004."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [],
            "translated_text": "",
            "candidates": [],
            "error": [
                []
            ]
        }
    }
}