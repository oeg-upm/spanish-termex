{
    "id": "H-46",
    "original_text": "Broad Expertise Retrieval in Sparse Data Environments Krisztian Balog ISLA, University of Amsterdam Kruislaan 403, 1098 SJ Amsterdam, The Netherlands kbalog@science.uva.nl Toine Bogers ILK, Tilburg University P.O. Box 90153, 5000 LE Tilburg, The Netherlands A.M.Bogers@uvt.nl Leif Azzopardi Dept. of Computing Science University of Glasgow, Glasgow, G12 8QQ leif@dcs.gla.ac.uk Maarten de Rijke ISLA, University of Amsterdam Kruislaan 403, 1098 SJ Amsterdam, The Netherlands mdr@science.uva.nl Antal van den Bosch ILK, Tilburg University P.O. Box 90153, 5000 LE Tilburg, The Netherlands Antal.vdnBosch@uvt.nl ABSTRACT Expertise retrieval has been largely unexplored on data other than the W3C collection. At the same time, many intranets of universities and other knowledge-intensive organisations offer examples of relatively small but clean multilingual expertise data, covering broad ranges of expertise areas. We first present two main expertise retrieval tasks, along with a set of baseline approaches based on generative language modeling, aimed at finding expertise relations between topics and people. For our experimental evaluation, we introduce (and release) a new test set based on a crawl of a university site. Using this test set, we conduct two series of experiments. The first is aimed at determining the effectiveness of baseline expertise retrieval methods applied to the new test set. The second is aimed at assessing refined models that exploit characteristic features of the new test set, such as the organizational structure of the university, and the hierarchical structure of the topics in the test set. Expertise retrieval models are shown to be robust with respect to environments smaller than the W3C collection, and current techniques appear to be generalizable to other settings. Categories and Subject Descriptors H.3 [Information Storage and Retrieval]: H.3.1 Content Analysis and Indexing; H.3.3 Information Search and Retrieval; H.3.4 Systems and Software; H.4 [Information Systems Applications]: H.4.2 Types of Systems; H.4.m Miscellaneous General Terms Algorithms, Measurement, Performance, Experimentation 1. INTRODUCTION An organizations intranet provides a means for exchanging information between employees and for facilitating employee collaborations. To efficiently and effectively achieve this, it is necessary to provide search facilities that enable employees not only to access documents, but also to identify expert colleagues. At the TREC Enterprise Track [22] the need to study and understand expertise retrieval has been recognized through the introduction of Expert Finding tasks. The goal of expert finding is to identify a list of people who are knowledgeable about a given topic. This task is usually addressed by uncovering associations between people and topics [10]; commonly, a co-occurrence of the name of a person with topics in the same context is assumed to be evidence of expertise. An alternative task, which using the same idea of people-topic associations, is expert profiling, where the task is to return a list of topics that a person is knowledgeable about [3]. The launch of the Expert Finding task at TREC has generated a lot of interest in expertise retrieval, with rapid progress being made in terms of modeling, algorithms, and evaluation aspects. However, nearly all of the expert finding or profiling work performed has been validated experimentally using the W3C collection [24] from the Enterprise Track. While this collection is currently the only publicly available test collection for expertise retrieval tasks, it only represents one type of intranet. With only one test collection it is not possible to generalize conclusions to other realistic settings. In this paper we focus on expertise retrieval in a realistic setting that differs from the W3C setting-one in which relatively small amounts of clean, multilingual data are available, that cover a broad range of expertise areas, as can be found on the intranets of universities and other knowledge-intensive organizations. Typically, this setting features several additional types of structure: topical structure (e.g., topic hierarchies as employed by the organization), organizational structure (faculty, department, ...), as well as multiple types of documents (research and course descriptions, publications, and academic homepages). This setting is quite different from the W3C setting in ways that might impact upon the performance of expertise retrieval tasks. We focus on a number of research questions in this paper: Does the relatively small amount of data available on an intranet affect the quality of the topic-person associations that lie at the heart of expertise retrieval algorithms? How do state-of-the-art algorithms developed on the W3C data set perform in the alternative scenario of the type described above? More generally, do the lessons from the Expert Finding task at TREC carry over to this setting? How does the inclusion or exclusion of different documents affect expertise retrieval tasks? In addition to, how can the topical and organizational structure be used for retrieval purposes? To answer our research questions, we first present a set of baseline approaches, based on generative language modeling, aimed at finding associations between topics and people. This allows us to formulate the expert finding and expert profiling tasks in a uniform way, and has the added benefit of allowing us to understand the relations between the two tasks. For our experimental evaluation, we introduce a new data set (the UvT Expert Collection) which is representative of the type of intranet that we described above. Our collection is based on publicly available data, crawled from the website of Tilburg University (UvT). This type of data is particularly interesting, since (1) it is clean, heterogeneous, structured, and focused, but comprises a limited number of documents; (2) contains information on the organizational hierarchy; (3) it is bilingual (English and Dutch); and (4) the list of expertise areas of an individual are provided by the employees themselves. Using the UvT Expert collection, we conduct two sets of experiments. The first is aimed at determining the effectiveness of baseline expertise finding and profiling methods in this new setting. A second group of experiments is aimed at extensions of the baseline methods that exploit characteristic features of the UvT Expert Collection; specifically, we propose and evaluate refined expert finding and profiling methods that incorporate topicality and organizational structure. Apart from the research questions and data set that we contribute, our main contributions are as follows. The baseline models developed for expertise finding perform well on the new data set. While on the W3C setting the expert finding task appears to be more difficult than profiling, for the UvT data the opposite is the case. We find that profiling on the UvT data set is considerably more difficult than on the W3C set, which we believe is due to the large (but realistic) number of topical areas that we used for profiling: about 1,500 for the UvT set, versus 50 in the W3C case. Taking the similarity between topics into account can significantly improve retrieval performance. The best performing similarity measures are content-based, therefore they can be applied on the W3C (and other) settings as well. Finally, we demonstrate that the organizational structure can be exploited in the form of a context model, improving MAP scores for certain models by up to 70%. The remainder of this paper is organized as follows. In the next section we review related work. Then, in Section 3 we provide detailed descriptions of the expertise retrieval tasks that we address in this paper: expert finding and expert profiling. In Section 4 we present our baseline models, of which the performance is then assessed in Section 6 using the UvT data set that we introduce in Section 5. Advanced models exploiting specific features of our data are presented in Section 7 and evaluated in Section 8. We formulate our conclusions in Section 9. 2. RELATED WORK Initial approaches to expertise finding often employed databases containing information on the skills and knowledge of each individual in the organization [11]. Most of these tools (usually called yellow pages or people-finding systems) rely on people to self-assess their skills against a predefined set of keywords. For updating profiles in these systems in an automatic fashion there is a need for intelligent technologies [5]. More recent approaches use specific document sets (such as email [6] or software [18]) to find expertise. In contrast with focusing on particular document types, there is also an increased interest in the development of systems that index and mine published intranet documents as sources of evidence for expertise. One such published approach is the P@noptic system [9], which builds a representation of each person by concatenating all documents associated with that person-this is similar to Model 1 of Balog et al. [4], who formalize and compare two methods. Balog et al.s Model 1 directly models the knowledge of an expert from associated documents, while their Model 2 first locates documents on the topic and then finds the associated experts. In the reported experiments the second method performs significantly better when there are sufficiently many associated documents per candidate. Most systems that took part in the 2005 and 2006 editions of the Expert Finding task at TREC implemented (variations on) one of these two models; see [10, 20]. Macdonald and Ounis [16] propose a different approach for ranking candidate expertise with respect to a topic based on data fusion techniques, without using collectionspecific heuristics; they find that applying field-based weighting models improves the ranking of candidates. Petkova and Croft [19] propose yet another approach, based on a combination of the above Model 1 and 2, explicitly modeling topics. Turning to other expert retrieval tasks that can also be addressed using topic-people associations, Balog and de Rijke [3] addressed the task of determining topical expert profiles. While their methods proved to be efficient on the W3C corpus, they require an amount of data that may not be available in the typical knowledge-intensive organization. Balog and de Rijke [2] study the related task of finding experts that are similar to a small set of experts given as input. As an aside, creating a textual summary of a person shows some similarities to biography finding, which has received a considerable amount of attention recently; see e.g., [13]. We use generative language modeling to find associations between topics and people. In our modeling of expert finding and profiling we collect evidence for expertise from multiple sources, in a heterogeneous collection, and integrate it with the co-occurrence of candidates names and query terms-the language modeling setting allows us to do this in a transparent manner. Our modeling proceeds in two steps. In the first step, we consider three baseline models, two taken from [4] (the Models 1 and 2 mentioned above), and one a refined version of a model introduced in [3] (which we refer to as Model 3 below); this third model is also similar to the model described by Petkova and Croft [19]. The models we consider in our second round of experiments are mixture models similar to contextual language models [1] and to the expanded documents of Tao et al. [21]; however, the features that we use for definining our expansions-including topical structure and organizational structure-have not been used in this way before. 3. TASKS In the expertise retrieval scenario that we envisage, users seeking expertise within an organization have access to an interface that combines a search box (where they can search for experts or topics) with navigational structures (of experts and of topics) that allows them to click their way to an expert page (providing the profile of a person) or a topic page (providing a list of experts on the topic). To feed the above interface, we face two expertise retrieval tasks, expert finding and expert profiling, that we first define and then formalize using generative language models. In order to model either task, the probability of the query topic being associated to a candidate expert plays a key role in the final estimates for searching and profiling. By using language models, both the candidates and the query are characterized by distributions of terms in the vocabulary (used in the documents made available by the organization whose expertise retrieval needs we are addressing). 3.1 Expert finding Expert finding involves the task of finding the right person with the appropriate skills and knowledge: Who are the experts on topic X?. E.g., an employee wants to ascertain who worked on a particular project to find out why particular decisions were made without having to trawl through documentation (if there is any). Or, they may be in need a trained specialist for consultancy on a specific problem. Within an organization there are usually many possible candidates who could be experts for given topic. We can state this problem as follows: What is the probability of a candidate ca being an expert given the query topic q? That is, we determine p(ca|q), and rank candidates ca according to this probability. The candidates with the highest probability given the query are deemed the most likely experts for that topic. The challenge is how to estimate this probability accurately. Since the query is likely to consist of only a few terms to describe the expertise required, we should be able to obtain a more accurate estimate by invoking Bayes Theorem, and estimating: p(ca|q) = p(q|ca)p(ca) p(q) , (1) where p(ca) is the probability of a candidate and p(q) is the probability of a query. Since p(q) is a constant, it can be ignored for ranking purposes. Thus, the probability of a candidate ca being an expert given the query q is proportional to the probability of a query given the candidate p(q|ca), weighted by the a priori belief p(ca) that candidate ca is an expert. p(ca|q) ∝ p(q|ca)p(ca) (2) In this paper our main focus is on estimating the probability of a query given the candidate p(q|ca), because this probability captures the extent to which the candidate knows about the query topic. Whereas the candidate priors are generally assumed to be uniformand thus will not influence the ranking-it has been demonstrated that a sensible choice of priors may improve the performance [20]. 3.2 Expert profiling While the task of expert searching was concerned with finding experts given a particular topic, the task of expert profiling seeks to answer a related question: What topics does a candidate know about? Essentially, this turns the questions of expert finding around. The profiling of an individual candidate involves the identification of areas of skills and knowledge that they have expertise about and an evaluation of the level of proficiency in each of these areas. This is the candidates topical profile. Generally, topical profiles within organizations consist of tabular structures which explicitly catalogue the skills and knowledge of each individual in the organization. However, such practice is limited by the resources available for defining, creating, maintaining, and updating these profiles over time. By focusing on automatic methods which draw upon the available evidence within the document repositories of an organization, our aim is to reduce the human effort associated with the maintenance of topical profiles1 . A topical profile of a candidate, then, is defined as a vector where each element i of the vector corresponds to the candidate cas expertise on a given topic ki, (i.e., s(ca, ki)). Each topic ki defines a particular knowledge area or skill that the organization uses to define the candidates topical profile. Thus, it is assumed that a list of topics, {k1, . . . , kn}, where n is the number of pre-defined topics, is given: profile(ca) = s(ca, k1), s(ca, k2), . . . , s(ca, kn) . (3) 1 Context and evidence are needed to help users of expertise finding systems to decide whom to contact when seeking expertise in a particular area. Examples of such context are: Who does she work with? What are her contact details? Is she well-connected, just in case she is not able to help us herself? What is her role in the organization? Who is her superior? Collaborators, and affiliations, etc. are all part of the candidates social profile, and can serve as a background against which the systems recommendations should be interpreted. In this paper we only address the problem of determining topical profiles, and leave social profiling to further work. We state the problem of quantifying the competence of a person on a certain knowledge area as follows: What is the probability of a knowledge area (ki) being part of the candidates (expertise) profile? where s(ca, ki) is defined by p(ki|ca). Our task, then, is to estimate p(ki|ca), which is equivalent to the problem of obtaining p(q|ca), where the topic ki is represented as a query topic q, i.e., a sequence of keywords representing the expertise required. Both the expert finding and profiling tasks rely on the accurate estimation of p(q|ca). The only difference derives from the prior probability that a person is an expert (p(ca)), which can be incorporated into the expert finding task. This prior does not apply to the profiling task since the candidate (individual) is fixed. 4. BASELINE MODELS In this section we describe our baseline models for estimating p(q|ca), i.e., associations between topics and people. Both expert finding and expert profiling boil down to this estimation. We employ three models for calculating this probability. 4.1 From topics to candidates Using Candidate Models: Model 1 Model 1 [4] defines the probability of a query given a candidate (p(q|ca)) using standard language modeling techniques, based on a multinomial unigram language model. For each candidate ca, a candidate language model θca is inferred such that the probability of a term given θca is nonzero for all terms, i.e., p(t|θca) > 0. From the candidate model the query is generated with the following probability: p(q|θca) = Y t∈q p(t|θca)n(t,q) , where each term t in the query q is sampled identically and independently, and n(t, q) is the number of times t occurs in q. The candidate language model is inferred as follows: (1) an empirical model p(t|ca) is computed; (2) it is smoothed with background probabilities. Using the associations between a candidate and a document, the probability p(t|ca) can be approximated by: p(t|ca) = X d p(t|d)p(d|ca), where p(d|ca) is the probability that candidate ca generates a supporting document d, and p(t|d) is the probability of a term t occurring in the document d. We use the maximum-likelihood estimate of a term, that is, the normalised frequency of the term t in document d. The strength of the association between document d and candidate ca expressed by p(d|ca) reflects the degree to which the candidates expertise is described using this document. The estimation of this probability is presented later, in Section 4.2. The candidate model is then constructed as a linear interpolation of p(t|ca) and the background model p(t) to ensure there are no zero probabilities, which results in the final estimation: p(q|θca) = (4) Y t∈q ( (1 − λ) X d p(t|d)p(d|ca) ! + λp(t) )n(t,q) . Model 1 amasses all the term information from all the documents associated with the candidate, and uses this to represent that candidate. This model is used to predict how likely a candidate would produce a query q. This can can be intuitively interpreted as the probability of this candidate talking about the query topic, where we assume that this is indicative of their expertise. Using Document Models: Model 2 Model 2 [4] takes a different approach. Here, the process is broken into two parts. Given a candidate ca, (1) a document that is associated with a candidate is selected with probability p(d|ca), and (2) from this document a query q is generated with probability p(q|d). Then the sum over all documents is taken to obtain p(q|ca), such that: p(q|ca) = X d p(q|d)p(d|ca). (5) The probability of a query given a document is estimated by inferring a document language model θd for each document d in a similar manner as the candidate model was inferred: p(t|θd) = (1 − λ)p(t|d) + λp(t), (6) where p(t|d) is the probability of the term in the document. The probability of a query given the document model is: p(q|θd) = Y t∈q p(t|θd)n(t,q) . The final estimate of p(q|ca) is obtained by substituting p(q|d) for p(q|θd) into Eq. 5 (see [4] for full details). Conceptually, Model 2 differs from Model 1 because the candidate is not directly modeled. Instead, the document acts like a hidden variable in the process which separates the query from the candidate. This process is akin to how a user may search for candidates with a standard search engine: initially by finding the documents which are relevant, and then seeing who is associated with that document. By examining a number of documents the user can obtain an idea of which candidates are more likely to discuss the topic q. Using Topic Models: Model 3 We introduce a third model, Model 3. Instead of attempting to model the query generation process via candidate or document models, we represent the query as a topic language model and directly estimate the probability of the candidate p(ca|q). This approach is similar to the model presented in [3, 19]. As with the previous models, a language model is inferred, but this time for the query. We adapt the work of Lavrenko and Croft [14] to estimate a topic model from the query. The procedure is as follows. Given a collection of documents and a query topic q, it is assumed that there exists an unknown topic model θk that assigns probabilities p(t|θk) to the term occurrences in the topic documents. Both the query and the documents are samples from θk (as opposed to the previous approaches, where a query is assumed to be sampled from a specific document or candidate model). The main task is to estimate p(t|θk), the probability of a term given the topic model. Since the query q is very sparse, and as there are no examples of documents on the topic, this distribution needs to be approximated. Lavrenko and Croft [14] suggest a reasonable way of obtaining such an approximation, by assuming that p(t|θk) can be approximated by the probability of term t given the query q. We can then estimate p(t|q) using the joint probability of observing the term t together with the query terms, q1, . . . , qm, and dividing by the joint probability of the query terms: p(t|θk) ≈ p(t|q) = p(t, q1, . . . , qm) p(q1, . . . , qm) = p(t, q1, . . . , qm) P t ∈T p(t , q1, . . . , qm) , where p(q1, . . . , qm) = P t ∈T p(t , q1, . . . , qm), and T is the entire vocabulary of terms. In order to estimate the joint probability p(t, q1, . . . , qm), we follow [14, 15] and assume t and q1, . . . , qm are mutually independent, once we pick a source distribution from the set of underlying source distributions U. If we choose U to be a set of document models. then to construct this set, the query q would be issued against the collection, and the top n returned are assumed to be relevant to the topic, and thus treated as samples from the topic model. (Note that candidate models could be used instead.) With the document models forming U, the joint probability of term and query becomes: p(t, q1, . . . , qm) = X d∈U p(d) ˘ p(t|θd) mY i=1 p(qi|θd) ¯ . (7) Here, p(d) denotes the prior distribution over the set U, which reflects the relevance of the document to the topic. We assume that p(d) is uniform across U. In order to rank candidates according to the topic model defined, we use the Kullback-Leibler divergence metric (KL, [8]) to measure the difference between the candidate models and the topic model: KL(θk||θca) = X t p(t|θk) log p(t|θk) p(t|θca) . (8) Candidates with a smaller divergence from the topic model are considered to be more likely experts on that topic. The candidate model θca is defined in Eq. 4. By using KL divergence instead of the probability of a candidate given the topic model p(ca|θk), we avoid normalization problems. 4.2 Document-candidate associations For our models we need to be able to estimate the probability p(d|ca), which expresses the extent to which a document d characterizes the candidate ca. In [4], two methods are presented for estimating this probability, based on the number of person names recognized in a document. However, in our (intranet) setting it is reasonable to assume that authors of documents can unambiguously be identified (e.g., as the author of an article, the teacher assigned to a course, the owner of a web page, etc.) Hence, we set p(d|ca) to be 1 if candidate ca is author of document d, otherwise the probability is 0. In Section 6 we describe how authorship can be determined on different types of documents within the collection. 5. THE UVT EXPERT COLLECTION The UvT Expert collection used in the experiments in this paper fits the scenario outlined in Section 3. The collection is based on the Webwijs (Webwise) system developed at Tilburg University (UvT) in the Netherlands. Webwijs (http://www.uvt.nl/ webwijs/) is a publicly accessible database of UvT employees who are involved in research or teaching; currently, Webwijs contains information about 1168 experts, each of whom has a page with contact information and, if made available by the expert, a research description and publications list. In addition, each expert can select expertise areas from a list of 1491 topics and is encouraged to suggest new topics that need to be approved by the Webwijs editor. Each topic has a separate page that shows all experts associated with that topic and, if available, a list of related topics. Webwijs is available in Dutch and English, and this bilinguality has been preserved in the collection. Every Dutch Webwijs page has an English translation. Not all Dutch topics have an English translation, but the reverse is true: the 981 English topics all have a Dutch equivalent. About 42% of the experts teach courses at Tilburg University; these courses were also crawled and included in the profile. In addition, about 27% of the experts link to their academic homepage from their Webwijs page. These home pages were crawled and added to the collection. (This means that if experts put the full-text versions of their publications on their academic homepage, these were also available for indexing.) We also obtained 1880 full-text versions of publications from the UvT institutional repository and Dutch English no. of experts 1168 1168 no. of experts with ≥ 1 topic 743 727 no. of topics 1491 981 no. of expert-topic pairs 4318 3251 avg. no. of topics/expert 5.8 5.9 max. no. of topics/expert (no. of experts) 60 (1) 35 (1) min. no. of topics/expert (no. of experts) 1 (74) 1 (106) avg. no. of experts/topic 2.9 3.3 max. no. of experts/topic (no. of topics) 30 (1) 30 (1) min. no. of experts/topic (no. of topics) 1 (615) 1 (346) no. of experts with HP 318 318 no. of experts with CD 318 318 avg. no. of CDs per teaching expert 3.5 3.5 no. of experts with RD 329 313 no. of experts with PUB 734 734 avg. no. of PUBs per expert 27.0 27.0 avg. no. of PUB citations per expert 25.2 25.2 avg. no. of full-text PUBs per expert 1.8 1.8 Table 2: Descriptive statistics of the Dutch and English versions of the UvT Expert collection. converted them to plain text. We ran the TextCat [23] language identifier to classify the language of the home pages and the fulltext publications. We restricted ourselves to pages where the classifier was confident about the language used on the page. This resulted in four document types: research descriptions (RD), course descriptions (CD), publications (PUB; full-text and citationonly versions), and academic homepages (HP). Everything was bundled into the UvT Expert collection which is available at http: //ilk.uvt.nl/uvt-expert-collection/. The UvT Expert collection was extracted from a different organizational setting than the W3C collection and differs from it in a number of ways. The UvT setting is one with relatively small amounts of multilingual data. Document-author associations are clear and the data is structured and clean. The collection covers a broad range of expertise areas, as one can typically find on intranets of universities and other knowledge-intensive institutes. Additionally, our university setting features several types of structure (topical and organizational), as well as multiple document types. Another important difference between the two data sets is that the expertise areas in the UvT Expert collection are self-selected instead of being based on group membership or assignments by others. Size is another dimension along which the W3C and UvT Expert collections differ: the latter is the smaller of the two. Also realistic are the large differences in the amount of information available for each expert. Utilizing Webwijs is voluntary; 425 Dutch experts did not select any topics at all. This leaves us with 743 Dutch and 727 English usable expert profiles. Table 2 provides descriptive statistics for the UvT Expert collection. Universities tend to have a hierarchical structure that goes from the faculty level, to departments, research groups, down to the individual researchers. In the UvT Expert collection we have information about the affiliations of researchers with faculties and institutes, providing us with a two-level organizational hierarchy. Tilburg University has 22 organizational units at the faculty level (including the university office and several research institutes) and 71 departments, which amounts to 3.2 departments per faculty. As to the topical hierarchy used by Webwijs, 131 of the 1491 topics are top nodes in the hierarchy. This hierarchy has an average topic chain length of 2.65 and a maximum length of 7 topics. 6. EVALUATION Below, we evaluate Section 4s models for expert finding and profiling onthe UvT Expert collection. We detail our research questions and experimental setup, and then present our results. 6.1 Research Questions We address the following research questions. Both expert finding and profiling rely on the estimations of p(q|ca). The question is how the models compare on the different tasks, and in the setting of the UvT Expert collection. In [4], Model 2 outperformed Model 1 on the W3C collection. How do they compare on our data set? And how does Model 3 compare to Model 1? What about performance differences between the two languages in our test collection? 6.2 Experimental Setup The output of our models was evaluated against the self-assigned topic labels, which were treated as relevance judgements. Results were evaluated separately for English and Dutch. For English we only used topics for which the Dutch translation was available; for Dutch all topics were considered. The results were averaged for the queries in the intersection of relevance judgements and results; missing queries do not contribute a value of 0 to the scores. We use standard information retrieval measures, such as Mean Average Precision (MAP) and Mean Reciprocal Rank (MRR). We also report the percentage of topics (%q) and candidates (%ca) covered, for the expert finding and profiling tasks, respectively. 6.3 Results Table 1 shows the performance of Model 1, 2, and 3 on the expert finding and profiling tasks. The rows of the table correspond to the various document types (RD, CD, PUB, and HP) and to their combinations. RD+CD+PUB+HP is equivalent to the full collection and will be referred as the BASELINE of our experiments. Looking at Table 1 we see that Model 2 performs the best across the board. However, when the data is clean and very focused (RD), Model 3 outperforms it in a number of cases. Model 1 has the best coverage of candidates (%ca) and topics (%q). The various document types differ in their characteristics and how they improve the finding and profiling tasks. Expert profiling benefits much from the clean data present in the RD and CD document types, while the publications contribute the most to the expert finding task. Adding the homepages does not prove to be particularly useful. When we compare the results across languages, we find that the coverage of English topics (%q) is higher than of the Dutch ones for expert finding. Apart from that, the scores fall in the same range for both languages. For the profiling task the coverage of the candidates (%ca) is very similar for both languages. However, the performance is substantially better for the English topics. While it is hard to compare scores across collections, we conclude with a brief comparison of the absolute scores in Table 1 to those reported in [3, 4] on the W3C test set (2005 edition). For expert finding the MAP scores for Model 2 reported here are about 50% higher than the corresponding figures in [4], while our MRR scores are slightly below those in [4]. For expert profiling, the differences are far more dramatic: the MAP scores for Model 2 reported here are around 50% below the scores in [3], while the (best) MRR scores are about the same as those in [3]. The cause for the latter differences seems to reside in the number of knowledge areas considered here-approx. 30 times more than in the W3C setting. 7. ADVANCED MODELS Now that we have developed and assessed basic language modeling techniques for expertise retrieval, we turn to refined models that exploit special features of our test collection. 7.1 Exploiting knowledge area similarity One way to improve the scoring of a query given a candidate is to consider what other requests the candidate would satisfy and use them as further evidence to support the original query, proportional Expert finding Expert profiling Document types Model 1 Model 2 Model 3 Model 1 Model 2 Model 3 %q MAP MRR %q MAP MRR %q MAP MRR %ca MAP MRR %ca MAP MRR %ca MAP MRR English RD 97.8 0.126 0.269 83.5 0.144 0.311 83.3 0.129 0.271 100 0.089 0.189 39.3 0.232 0.465 41.1 0.166 0.337 CD 97.8 0.118 0.227 91.7 0.123 0.248 91.7 0.118 0.226 32.8 0.188 0.381 32.4 0.195 0.385 32.7 0.203 0.370 PUB 97.8 0.200 0.330 98.0 0.216 0.372 98.0 0.145 0.257 78.9 0.167 0.364 74.5 0.212 0.442 78.9 0.135 0.299 HP 97.8 0.081 0.186 97.4 0.071 0.168 97.2 0.062 0.149 31.2 0.150 0.299 28.8 0.185 0.335 30.1 0.136 0.287 RD+CD 97.8 0.188 0.352 92.9 0.193 0.360 92.9 0.150 0.273 100 0.145 0.286 61.3 0.251 0.477 63.2 0.217 0.416 RD+CD+PUB 97.8 0.235 0.373 98.1 0.277 0.439 98.1 0.178 0.305 100 0.196 0.380 87.2 0.280 0.533 89.5 0.170 0.344 RD+CD+PUB+HP 97.8 0.237 0.372 98.6 0.280 0.441 98.5 0.166 0.293 100 0.199 0.387 88.7 0.281 0.525 90.9 0.169 0.329 Dutch RD 61.3 0.094 0.229 38.4 0.137 0.336 38.3 0.127 0.295 38.0 0.127 0.386 34.1 0.138 0.420 38.0 0.105 0.327 CD 61.3 0.107 0.212 49.7 0.128 0.256 49.7 0.136 0.261 32.5 0.151 0.389 31.8 0.158 0.396 32.5 0.170 0.380 PUB 61.3 0.193 0.319 59.5 0.218 0.368 59.4 0.173 0.291 78.8 0.126 0.364 76.0 0.150 0.424 78.8 0.103 0.294 HP 61.3 0.063 0.169 56.6 0.064 0.175 56.4 0.062 0.163 29.8 0.108 0.308 27.8 0.125 0.338 29.8 0.098 0.255 RD+CD 61.3 0.159 0.314 51.9 0.184 0.360 51.9 0.169 0.324 60.5 0.151 0.410 57.2 0.166 0.431 60.4 0.159 0.384 RD+CD+PUB 61.3 0.244 0.398 61.5 0.260 0.424 61.4 0.210 0.350 90.3 0.165 0.445 88.2 0.189 0.479 90.3 0.126 0.339 RD+CD+PUB+HP 61.3 0.249 0.401 62.6 0.265 0.436 62.6 0.195 0.344 91.9 0.164 0.426 90.1 0.195 0.488 91.9 0.125 0.328 Table 1: Performance of the models on the expert finding and profiling tasks, using different document types and their combinations. %q is the number of topics covered (applies to the expert finding task), %ca is the number of candidates covered (applies to the expert profiling task). The top and bottom blocks correspond to English and Dutch respectively. The best scores are in boldface. to how related the other requests are to the original query. This can be modeled by interpolating between the p(q|ca) and the further supporting evidence from all similar requests q , as follows: p (q|ca) = λp(q|ca) + (1 − λ) X q p(q|q )p(q |ca), (9) where p(q|q ) represents the similarity between the two topics q and q . To be able to work with similarity methods that are not necessarily probabilities, we set p(q|q ) = w(q,q ) γ , where γ is a normalizing constant, such that γ = P q w(q , q ). We consider four methods for calculating the similarity score between two topics. Three approaches are strictly content-based, and establish similarity by examining co-occurrence patterns of topics within the collection, while the last approach exploits the hierarchical structure of topical areas that may be present within an organization (see [7] for further examples of integrating word relationships into language models). The Kullback-Leibler (KL) divergence metric defined in Eq. 8 provides a measure of how different or similar two probability distributions are. A topic model is inferred for q and q using the method presented in Section 4.1 to describe the query across the entire vocabulary. Since a lower KL score means the queries are more similar, we let w(q, q ) = max(KL(θq||·) − KL(θq||θq )). Pointwise Mutual Information (PMI, [17]) is a measure of association used in information theory to determine the extent of independence between variables. The dependence between two queries is reflected by the SI(q, q ) score, where scores greater than zero indicate that it is likely that there is a dependence, which we take to mean that the queries are likely to be similar: SI(q, q ) = log p(q, q ) p(q)p(q ) (10) We estimate the probability of a topic p(q) using the number of documents relevant to query q within the collection. The joint probability p(q, q ) is estimated similarly, by using the concatenation of q and q as a query. To obtain p(q|q ), we then set w(q, q ) = SI(q, q ) when SI(q, q ) > 0 otherwise w(q, q ) = 0, because we are only interested in including queries that are similar. The log-likelihood statistic provides another measure of dependence, which is more reliable than the pointwise mutual information measure [17]. Let k1 be the number of co-occurrences of q and q , k2 the number of occurrences of q not co-occurring with q , n1 the total number of occurrences of q , and n2 the total number of topic tokens minus the number of occurrences of q . Then, let p1 = k1/n1, p2 = k2/n2, and p = (k1 + k2)/(n1 + n2), (q, q ) = 2( (p1, k1, n1) + (p2, k2, n2) − (p, k1, n1) − (p, k2, n2)), where (p, n, k) = k log p + (n − k) log(1 − p). The higher score indicate that queries are also likely to be similar, thus we set w(q, q ) = (q, q ). Finally, we also estimate the similarity of two topics based on their distance within the topic hierarchy. The topic hierarchy is viewed as a directed graph, and for all topic-pairs the shortest path SP(q, q ) is calculated. We set the similarity score to be the reciprocal of the shortest path: w(q, q ) = 1/SP(q, q ). 7.2 Contextual information Given the hierarchy of an organization, the units to which a person belong are regarded as a context so as to compensate for data sparseness. We model it as follows: p (q|ca) = 1 − P ou∈OU(ca) λou · p(q|ca) + P ou∈OU(ca) λou · p(q|ou), where OU(ca) is the set of organizational units of which candidate ca is a member of, and p(q|o) expresses the strength of the association between query q and the unit ou. The latter probability can be estimated using either of the three basic models, by simply replacing ca with ou in the corresponding equations. An organizational unit is associated with all the documents that its members have authored. That is, p(d|ou) = maxca∈ou p(d|ca). 7.3 A simple multilingual model For knowledge institutes in Europe, academic or otherwise, a multilingual (or at least bilingual) setting is typical. The following model builds on a kind of independence assumption: there is no spill-over of expertise/profiles across language boundaries. While a simplification, this is a sensible first approach. That is: p (q|ca) =P l∈L λl · p(ql|ca), where L is the set of languages used in the collection, ql is the translation of the query q to language l, and λl is a language specific smoothing parameter, such that P l∈L λl = 1. 8. ADVANCED MODELS: EVALUATION In this section we present an experimental evaluation of our advanced models. Expert finding Expert profiling Language Model 1 Model 2 Model 3 Model 1 Model 2 Model 3 %q MAP MRR %q MAP MRR %q MAP MRR %ca MAP MRR %ca MAP MRR %ca MAP MRR English only 97.8 0.237 0.372 98.6 0.280 0.441 98.5 0.166 0.293 100 0.199 0.387 88.7 0.281 0.525 90.9 0.169 0.329 Dutch only 61.3 0.249 0.401 62.6 0.265 0.436 62.6 0.195 0.344 91.9 0.164 0.426 90.1 0.195 0.488 91.9 0.125 0.328 Combination 99.4 0.297 0.444 99.7 0.324 0.491 99.7 0.223 0.388 100 0.241 0.445 92.1 0.313 0.564 93.2 0.224 0.411 Table 3: Performance of the combination of languages on the expert finding and profiling tasks (on candidates). Best scores for each model are in italic, absolute best scores for the expert finding and profiling tasks are in boldface. Method Model 1 Model 2 Model 3 MAP MRR MAP MRR MAP MRR English BASELINE 0.296 0.454 0.339 0.509 0.221 0.333 KLDIV 0.291 0.453 0.327 0.503 0.219 0.330 PMI 0.291 0.453 0.337 0.509 0.219 0.331 LL 0.319 0.490 0.360 0.524 0.233 0.368 HDIST 0.299 0.465 0.346 0.537 0.219 0.332 Dutch BASELINE 0.240 0.350 0.271 0.403 0.227 0.389 KLDIV 0.239 0.347 0.253 0.386 0.224 0.385 PMI 0.239 0.350 0.260 0.392 0.227 0.389 LL 0.255 0.372 0.281 0.425 0.231 0.389 HDIST 0.253 0.365 0.271 0.407 0.236 0.402 Method Model 1 Model 2 Model 3 MAP MRR MAP MRR MAP MRR English BASELINE 0.485 0.546 0.499 0.548 0.381 0.416 KLDIV 0.510 0.564 0.513 0.558 0.381 0.416 PMI 0.486 0.546 0.495 0.542 0.407 0.451 LL 0.558 0.589 0.586 0.617 0.408 0.453 HDIST 0.507 0.567 0.512 0.563 0.386 0.420 Dutch BASELINE 0.263 0.313 0.294 0.358 0.262 0.315 KLDIV 0.284 0.336 0.271 0.321 0.261 0.314 PMI 0.265 0.317 0.265 0.316 0.273 0.330 LL 0.312 0.351 0.330 0.377 0.284 0.331 HDIST 0.280 0.327 0.288 0.341 0.266 0.321 Table 4: Performance on the expert finding (top) and profiling (bottom) tasks, using knowledge area similarities. Runs were evaluated on the main topics set. Best scores are in boldface. 8.1 Research Questions Our questions follow the refinements presented in the preceding section: Does exploiting the knowledge area similarity improve effectiveness? Which of the various methods for capturing word relationships is most effective? Furthermore, is our way of bringing in contextual information useful? For which tasks? And finally, is our simple way of combining the monolingual scores sufficient for obtaining significant improvements? 8.2 Experimental setup Given that the self-assessments are also sparse in our collection, in order to be able to measure differences between the various models, we selected a subset of topics, and evaluated (some of the) runs only on this subset. This set is referred as main topics, and consists of topics that are located at the top level of the topical hierarchy. (A main topic has subtopics, but is not a subtopic of any other topic.) This main set consists of 132 Dutch and 119 English topics. The relevance judgements were restricted to the main topic set, but were not expanded with subtopics. 8.3 Exploiting knowledge area similarity Table 4 presents the results. The four methods used for estimating knowledge-area similarity are KL divergence (KLDIV), PointLang. Topics Model 1 Model 2 Model 3 MAP MRR MAP MRR MAP MRR Expert finding UK ALL 0.423 0.545 0.654 0.799 0.494 0.629 UK MAIN 0.500 0.621 0.704 0.834 0.587 0.699 NL ALL 0.439 0.560 0.672 0.826 0.480 0.630 NL MAIN 0.440 0.584 0.645 0.816 0.515 0.655 Expert profiling UK ALL 0.240 0.640 0.306 0.778 0.223 0.616 UK MAIN 0.523 0.677 0.519 0.648 0.461 0.587 NL ALL 0.203 0.716 0.254 0.770 0.183 0.627 NL MAIN 0.332 0.576 0.380 0.624 0.332 0.549 Table 5: Evaluating the context models on organizational units. wise mutual information (PMI), log-likelihood (LL), and distance within topic hierarchy (HDIST). We managed to improve upon the baseline in all cases, but the improvement is more noticeable for the profiling task. For both tasks, the LL method performed best. The content-based approaches performed consistently better than HDIST. 8.4 Contextual information A two level hierarchy of organizational units (faculties and institutes) is available in the UvT Expert collection. The unit a person belongs to is used as a context for that person. First, we evaluated the models of the organizational units, using all topics (ALL) and only the main topics (MAIN). An organizational unit is considered to be relevant for a given topic (or vice versa) if at least one member of the unit selected the given topic as an expertise area. Table 5 reports on the results. As far as expert finding goes, given a topic, the corresponding organizational unit can be identified with high precision. However, the expert profiling task shows a different picture: the scores are low, and the task seems hard. The explanation may be that general concepts (i.e., our main topics) may belong to several organizational units. Second, we performed another evaluation, where we combined the contextual models with the candidate models (to score candidates again). Table 6 reports on the results. We find a positive impact of the context models only for expert finding. Noticably, for expert finding (and Model 1), it improves over 50% (for English) and over 70% (for Dutch) on MAP. The poor performance on expert profiling may be due to the fact that context models alone did not perform very well on the profiling task to begin with. 8.5 Multilingual models In this subsection we evaluate the method for combining results across multiple languages that we described in Section 7.3. In our setting the set of languages consists of English and Dutch: L = {UK, NL}. The weights on these languages were set to be identical (λUK = λNL = 0.5). We performed experiments with various λ settings, but did not observe significant differences in performance. Table 3 reports on the multilingual results, where performance is evaluated on the full topic set. All three models significantly imLang. Method Model 1 Model 2 Model 3 MAP MRR MAP MRR MAP MRR Expert finding UK BL 0.296 0.454 0.339 0.509 0.221 0.333 UK CT 0.330 0.491 0.342 0.500 0.228 0.342 NL BL 0.240 0.350 0.271 0.403 0.227 0.389 NL CT 0.251 0.382 0.267 0.410 0.246 0.404 Expert profiling UK BL 0.485 0.546 0.499 0.548 0.381 0.416 UK CT 0.562 0.620 0.508 0.558 0.440 0.486 NL BL 0.263 0.313 0.294 0.358 0.262 0.315 NL CT 0.330 0.384 0.317 0.387 0.294 0.345 Table 6: Performance of the context models (CT) compared to the baseline (BL). Best scores are in boldface. proved over all measures for both tasks. The coverage of topics and candidates for the expert finding and profiling tasks, respectively, is close to 100% in all cases. The relative improvement of the precision scores ranges from 10% to 80%. These scores demonstrate that despite its simplicity, our method for combining results over multiple languages achieves substantial improvements over the baseline. 9. CONCLUSIONS In this paper we focused on expertise retrieval (expert finding and profiling) in a new setting of a typical knowledge-intensive organization in which the available data is of high quality, multilingual, and covering a broad range of expertise area. Typically, the amount of available data in such an organization (e.g., a university, a research institute, or a research lab) is limited when compared to the W3C collection that has mostly been used for the experimental evaluation of expertise retrieval so far. To examine expertise retrieval in this setting, we introduced (and released) the UvT Expert collection as a representative case of such knowledge intensive organizations. The new collection reflects the typical properties of knowledge-intensive institutes noted above and also includes several features which may are potentially useful for expertise retrieval, such as topical and organizational structure. We evaluated how current state-of-the-art models for expert finding and profiling performed in this new setting and then refined these models in order to try and exploit the different characteristics within the data environment (language, topicality, and organizational structure). We found that current models of expertise retrieval generalize well to this new environment; in addition we found that refining the models to account for the differences results in significant improvements, thus making up for problems caused by data sparseness issues. Future work includes setting up manual assessments of automatically generated profiles by the employees themselves, especially in cases where the employees have not provided a profile themselves. 10. ACKNOWLEDGMENTS Krisztian Balog was supported by the Netherlands Organisation for Scientific Research (NWO) under project number 220-80-001. Maarten de Rijke was also supported by NWO under project numbers 017.001.190, 220-80-001, 264-70-050, 354-20-005, 600.065.120, 612-13-001, 612.000.106, 612.066.302, 612.069.006, 640.001.501, 640.002.501, and by the E.U. IST programme of the 6th FP for RTD under project MultiMATCH contract IST-033104. The work of Toine Bogers and Antal van den Bosch was funded by the IOP-MMI-program of SenterNovem / The Dutch Ministry of Economic Affairs, as part of the `A Propos project. 11. REFERENCES [1] L. Azzopardi. Incorporating Context in the Language Modeling Framework for ad-hoc Information Retrieval. PhD thesis, University of Paisley, 2005. [2] K. Balog and M. de Rijke. Finding similar experts. In This volume, 2007. [3] K. Balog and M. de Rijke. Determining expert profiles (with an application to expert finding). In IJCAI 07: Proc. 20th Intern. Joint Conf. on Artificial Intelligence, pages 2657-2662, 2007. [4] K. Balog, L. Azzopardi, and M. de Rijke. Formal models for expert finding in enterprise corpora. In SIGIR 06: Proc. 29th annual intern. ACM SIGIR conf. on Research and development in information retrieval, pages 43-50, 2006. [5] I. Becerra-Fernandez. The role of artificial intelligence technologies in the implementation of people-finder knowledge management systems. In AAAI Workshop on Bringing Knowledge to Business Processes, March 2000. [6] C. S. Campbell, P. P. Maglio, A. Cozzi, and B. Dom. Expertise identification using email communications. In CIKM 03: Proc. twelfth intern. conf. on Information and knowledge management, pages 528531, 2003. [7] G. Cao, J.-Y. Nie, and J. Bai. Integrating word relationships into language models. In SIGIR 05: Proc. 28th annual intern. ACM SIGIR conf. on Research and development in information retrieval, pages 298-305, 2005. [8] T. M. Cover and J. A. Thomas. Elements of Information Theory. Wiley-Interscience, 1991. [9] N. Craswell, D. Hawking, A. M. Vercoustre, and P. Wilkins. P@noptic expert: Searching for experts not just for documents. In Ausweb, 2001. [10] N. Craswell, A. de Vries, and I. Soboroff. Overview of the TREC2005 Enterprise Track. In The Fourteenth Text REtrieval Conf. Proc. (TREC 2005), 2006. [11] T. H. Davenport and L. Prusak. Working Knowledge: How Organizations Manage What They Know. Harvard Business School Press, Boston, MA, 1998. [12] T. Dunning. Accurate methods for the statistics of surprise and coincidence. Computational Linguistics, 19(1):61-74, 1993. [13] E. Filatova and J. Prager. Tell me what you do and Ill tell you what you are: Learning occupation-related activities for biographies. In HLT/EMNLP, 2005. [14] V. Lavrenko and W. B. Croft. Relevance based language models. In SIGIR 01: Proc. 24th annual intern. ACM SIGIR conf. on Research and development in information retrieval, pages 120-127, 2001. [15] V. Lavrenko, M. Choquette, and W. B. Croft. Cross-lingual relevance models. In SIGIR 02: Proc. 25th annual intern. ACM SIGIR conf. on Research and development in information retrieval, pages 175-182, 2002. [16] C. Macdonald and I. Ounis. Voting for candidates: adapting data fusion techniques for an expert search task. In CIKM 06: Proc. 15th ACM intern. conf. on Information and knowledge management, pages 387-396, 2006. [17] C. Manning and H. Sch¨utze. Foundations of Statistical Natural Language Processing. The MIT Press, 1999. [18] A. Mockus and J. D. Herbsleb. Expertise browser: a quantitative approach to identifying expertise. In ICSE 02: Proc. 24th Intern. Conf. on Software Engineering, pages 503-512, 2002. [19] D. Petkova and W. B. Croft. Hierarchical language models for expert finding in enterprise corpora. In Proc. ICTAI 2006, pages 599-608, 2006. [20] I. Soboroff, A. de Vries, and N. Craswell. Overview of the TREC 2006 Enterprise Track. In TREC 2006 Working Notes, 2006. [21] T. Tao, X. Wang, Q. Mei, and C. Zhai. Language model information retrieval with document expansion. In HLT-NAACL 2006, 2006. [22] TREC. Enterprise track, 2005. URL: http://www.ins.cwi. nl/projects/trec-ent/wiki/. [23] G. van Noord. TextCat Language Guesser. URL: http://www. let.rug.nl/˜vannoord/TextCat/. [24] W3C. The W3C test collection, 2005. URL: http://research. microsoft.com/users/nickcr/w3c-summary.html.",
    "original_translation": "Recuperación de amplia experiencia en entornos de datos dispersos Krisztian Balog ISLA, Universidad de Ámsterdam Kruislaan 403, 1098 SJ Ámsterdam, Países Bajos kbalog@science.uva.nl Toine Bogers ILK, Universidad de Tilburg P.O. Caja 90153, 5000 LE Tilburg, Países Bajos A.M.Bogers@uvt.nl Leif Azzopardi Dept. de Ciencias de la Computación Universidad de Glasgow, Glasgow, G12 8QQ leif@dcs.gla.ac.uk Maarten de Rijke ISLA, Universidad de Ámsterdam Kruislaan 403, 1098 SJ Ámsterdam, Países Bajos mdr@science.uva.nl Antal van den Bosch ILK, Universidad de Tilburg P.O. La recuperación de la experiencia ha sido ampliamente inexplorada en datos distintos a la colección de la W3C. Al mismo tiempo, muchas intranets de universidades y otras organizaciones intensivas en conocimiento ofrecen ejemplos de datos de expertos multilingües relativamente pequeños pero precisos, que abarcan amplias áreas de especialización. Primero presentamos dos tareas principales de recuperación de expertos, junto con un conjunto de enfoques de referencia basados en modelado del lenguaje generativo, con el objetivo de encontrar relaciones de experticia entre temas y personas. Para nuestra evaluación experimental, introducimos (y liberamos) un nuevo conjunto de pruebas basado en un rastreo de un sitio universitario. Usando este conjunto de pruebas, llevamos a cabo dos series de experimentos. El primero tiene como objetivo determinar la efectividad de los métodos de recuperación de conocimientos básicos aplicados al nuevo conjunto de pruebas. El segundo tiene como objetivo evaluar modelos refinados que exploten las características distintivas del nuevo conjunto de pruebas, como la estructura organizativa de la universidad y la estructura jerárquica de los temas en el conjunto de pruebas. Los modelos de recuperación de experiencia se muestran robustos con respecto a entornos más pequeños que la colección de la W3C, y las técnicas actuales parecen ser generalizables a otros contextos. Categorías y Descriptores de Asignaturas H.3 [Almacenamiento y Recuperación de Información]: H.3.1 Análisis de Contenido e Indexación; H.3.3 Búsqueda y Recuperación de Información; H.3.4 Sistemas y Software; H.4 [Aplicaciones de Sistemas de Información]: H.4.2 Tipos de Sistemas; H.4.m Términos Generales Misceláneos Algoritmos, Medición, Rendimiento, Experimentación 1. INTRODUCCIÓN La intranet de una organización proporciona un medio para intercambiar información entre empleados y facilitar colaboraciones entre ellos. Para lograr esto de manera eficiente y efectiva, es necesario proporcionar instalaciones de búsqueda que permitan a los empleados no solo acceder a documentos, sino también identificar a colegas expertos. En la pista de Empresas de TREC [22] se ha reconocido la necesidad de estudiar y comprender la recuperación de expertos a través de la introducción de tareas de Búsqueda de Expertos. El objetivo de la búsqueda de expertos es identificar una lista de personas que tienen conocimientos sobre un tema específico. Esta tarea suele abordarse descubriendo asociaciones entre personas y temas [10]; comúnmente, se asume que la co-ocurrencia del nombre de una persona con temas en el mismo contexto es evidencia de experiencia. Una tarea alternativa, que utiliza la misma idea de asociaciones entre personas y temas, es el perfilado de expertos, donde la tarea consiste en devolver una lista de temas sobre los que una persona tiene conocimiento [3]. El lanzamiento de la tarea de Búsqueda de Expertos en TREC ha generado mucho interés en la recuperación de expertos, con un rápido progreso en términos de modelado, algoritmos y aspectos de evaluación. Sin embargo, casi todo el trabajo de búsqueda o perfilado de expertos realizado ha sido validado experimentalmente utilizando la colección de W3C [24] de la pista empresarial. Si bien esta colección es actualmente la única colección de pruebas disponible públicamente para tareas de recuperación de expertos, solo representa un tipo de intranet. Con solo una colección de pruebas no es posible generalizar conclusiones a otros escenarios realistas. En este artículo nos enfocamos en la recuperación de expertos en un entorno realista que difiere del entorno de la W3C, uno en el que hay cantidades relativamente pequeñas de datos limpios y multilingües disponibles, que abarcan una amplia gama de áreas de expertise, como se puede encontrar en las intranets de universidades y otras organizaciones intensivas en conocimiento. Normalmente, esta configuración incluye varios tipos adicionales de estructuras: estructura temática (por ejemplo, jerarquías de temas utilizadas por la organización), estructura organizativa (facultad, departamento, ...), así como múltiples tipos de documentos (descripciones de investigación y cursos, publicaciones y páginas web académicas). Este entorno es bastante diferente del entorno del W3C de maneras que podrían afectar el rendimiento de las tareas de recuperación de conocimientos especializados. Nos enfocamos en varias preguntas de investigación en este artículo: ¿La cantidad relativamente pequeña de datos disponibles en una intranet afecta la calidad de las asociaciones tema-persona que son fundamentales en los algoritmos de recuperación de expertos? ¿Cómo se desempeñan los algoritmos de vanguardia desarrollados en el conjunto de datos del W3C en el escenario alternativo del tipo descrito anteriormente? Más en general, ¿se pueden aplicar las lecciones de la tarea de Búsqueda de Expertos en TREC a este contexto? ¿Cómo afecta la inclusión o exclusión de diferentes documentos a las tareas de recuperación de expertos? Además, ¿cómo se puede utilizar la estructura temática y organizativa con fines de recuperación? Para responder a nuestras preguntas de investigación, primero presentamos un conjunto de enfoques de línea base, basados en modelado de lenguaje generativo, con el objetivo de encontrar asociaciones entre temas y personas. Esto nos permite formular las tareas de búsqueda de expertos y perfilado de expertos de manera uniforme, y tiene el beneficio adicional de permitirnos entender las relaciones entre las dos tareas. Para nuestra evaluación experimental, introducimos un nuevo conjunto de datos (la Colección de Expertos de UvT) que es representativo del tipo de intranet que describimos anteriormente. Nuestra colección se basa en datos de acceso público, extraídos del sitio web de la Universidad de Tilburg (UvT). Este tipo de datos es particularmente interesante, ya que (1) es limpio, heterogéneo, estructurado y enfocado, pero comprende un número limitado de documentos; (2) contiene información sobre la jerarquía organizativa; (3) es bilingüe (inglés y holandés); y (4) la lista de áreas de especialización de un individuo es proporcionada por los propios empleados. Usando la colección de expertos de UvT, llevamos a cabo dos conjuntos de experimentos. El primero tiene como objetivo determinar la efectividad de los métodos de búsqueda y perfilado de experiencia inicial en este nuevo entorno. Un segundo grupo de experimentos está dirigido a extensiones de los métodos base que aprovechan las características distintivas de la Colección de Expertos de UvT; específicamente, proponemos y evaluamos métodos refinados de búsqueda y perfilado de expertos que incorporan la topicalidad y la estructura organizativa. Además de las preguntas de investigación y el conjunto de datos que aportamos, nuestras principales contribuciones son las siguientes. Los modelos base desarrollados para encontrar expertos funcionan bien en el nuevo conjunto de datos. Mientras que en el entorno del W3C la tarea de encontrar expertos parece ser más difícil que el perfilado, para los datos de UvT ocurre lo contrario. Encontramos que el perfilado en el conjunto de datos UvT es considerablemente más difícil que en el conjunto de datos W3C, lo cual creemos que se debe al gran (pero realista) número de áreas temáticas que utilizamos para el perfilado: alrededor de 1,500 para el conjunto de datos UvT, frente a 50 en el caso de W3C. Teniendo en cuenta la similitud entre los temas puede mejorar significativamente el rendimiento de recuperación. Las medidas de similitud de mejor rendimiento son basadas en contenido, por lo tanto también se pueden aplicar en entornos de la W3C (y otros). Finalmente, demostramos que la estructura organizativa puede ser explotada en forma de un modelo de contexto, mejorando los puntajes de MAP para ciertos modelos hasta en un 70%. El resto de este documento está organizado de la siguiente manera. En la siguiente sección revisamos el trabajo relacionado. Luego, en la Sección 3 proporcionamos descripciones detalladas de las tareas de recuperación de expertos que abordamos en este artículo: la búsqueda de expertos y la creación de perfiles de expertos. En la Sección 4 presentamos nuestros modelos base, cuyo rendimiento luego es evaluado en la Sección 6 utilizando el conjunto de datos UvT que presentamos en la Sección 5. Los modelos avanzados que explotan características específicas de nuestros datos se presentan en la Sección 7 y se evalúan en la Sección 8. Formulamos nuestras conclusiones en la Sección 9.2. Trabajos relacionados Los enfoques iniciales para encontrar expertos a menudo empleaban bases de datos que contenían información sobre las habilidades y conocimientos de cada individuo en la organización [11]. La mayoría de estas herramientas (generalmente llamadas páginas amarillas o sistemas de búsqueda de personas) dependen de que las personas evalúen sus habilidades frente a un conjunto predefinido de palabras clave. Para actualizar perfiles en estos sistemas de forma automática, se necesita de tecnologías inteligentes [5]. Enfoques más recientes utilizan conjuntos de documentos específicos (como correos electrónicos [6] o software [18]) para encontrar experiencia. En contraste con centrarse en tipos de documentos particulares, también hay un interés creciente en el desarrollo de sistemas que indexan y extraen información de documentos publicados en intranets como fuentes de evidencia de experiencia. Un enfoque publicado es el sistema P@noptic [9], que construye una representación de cada persona concatenando todos los documentos asociados con esa persona, similar al Modelo 1 de Balog et al. [4], quienes formalizan y comparan dos métodos. El Modelo 1 de Balog et al. modela directamente el conocimiento de un experto a partir de documentos asociados, mientras que su Modelo 2 primero localiza documentos sobre el tema y luego encuentra a los expertos asociados. En los experimentos reportados, el segundo método tiene un rendimiento significativamente mejor cuando hay suficientes documentos asociados por candidato. La mayoría de los sistemas que participaron en las ediciones de 2005 y 2006 de la tarea de Búsqueda de Expertos en TREC implementaron (variaciones de) uno de estos dos modelos; ver [10, 20]. Macdonald y Ounis [16] proponen un enfoque diferente para clasificar la experiencia de los candidatos con respecto a un tema basado en técnicas de fusión de datos, sin utilizar heurísticas específicas de la colección; encuentran que la aplicación de modelos de ponderación basados en campos mejora la clasificación de los candidatos. Petkova y Croft [19] proponen otro enfoque, basado en una combinación de los Modelos 1 y 2 anteriores, modelando explícitamente los temas. Al abordar otras tareas de recuperación de expertos que también pueden abordarse utilizando asociaciones entre temas y personas, Balog y de Rijke [3] abordaron la tarea de determinar perfiles de expertos temáticos. Si bien sus métodos resultaron ser eficientes en el corpus de la W3C, requieren una cantidad de datos que puede que no esté disponible en la típica organización intensiva en conocimiento. Balog y de Rijke [2] estudian la tarea relacionada de encontrar expertos que sean similares a un pequeño conjunto de expertos dados como entrada. Por cierto, crear un resumen textual de una persona muestra algunas similitudes con la búsqueda de biografías, la cual ha recibido una considerable cantidad de atención recientemente; ver por ejemplo, [13]. Utilizamos modelado de lenguaje generativo para encontrar asociaciones entre temas y personas. En nuestro modelado de búsqueda y perfilado de expertos recopilamos evidencia de experiencia de múltiples fuentes, en una colección heterogénea, e integramos esta información con la co-ocurrencia de nombres de candidatos y términos de consulta: el entorno de modelado de lenguaje nos permite hacer esto de manera transparente. Nuestro modelado avanza en dos pasos. En el primer paso, consideramos tres modelos base, dos tomados de [4] (los Modelos 1 y 2 mencionados anteriormente), y uno una versión refinada de un modelo introducido en [3] (al que nos referimos como Modelo 3 a continuación); este tercer modelo también es similar al modelo descrito por Petkova y Croft [19]. Los modelos que consideramos en nuestra segunda ronda de experimentos son modelos de mezcla similares a los modelos de lenguaje contextual [1] y a los documentos ampliados de Tao et al. [21]; sin embargo, las características que utilizamos para definir nuestras expansiones, incluida la estructura temática y la estructura organizativa, no se han utilizado de esta manera antes. 3. En el escenario de recuperación de expertos que imaginamos, los usuarios que buscan expertos dentro de una organización tienen acceso a una interfaz que combina un cuadro de búsqueda (donde pueden buscar expertos o temas) con estructuras de navegación (de expertos y de temas) que les permite hacer clic hasta llegar a una página de experto (que proporciona el perfil de una persona) o a una página de tema (que proporciona una lista de expertos en el tema). Para alimentar la interfaz anterior, nos enfrentamos a dos tareas de recuperación de expertos, la búsqueda de expertos y el perfilado de expertos, que primero definimos y luego formalizamos utilizando modelos de lenguaje generativos. Para modelar cualquiera de las tareas, la probabilidad de que el tema de la consulta esté asociado a un experto candidato juega un papel clave en las estimaciones finales para la búsqueda y el perfilado. Al utilizar modelos de lenguaje, tanto los candidatos como la consulta se caracterizan por distribuciones de términos en el vocabulario (utilizado en los documentos puestos a disposición por la organización cuyas necesidades de recuperación de experiencia estamos abordando). 3.1 Búsqueda de expertos La búsqueda de expertos implica la tarea de encontrar a la persona adecuada con las habilidades y conocimientos apropiados: ¿Quiénes son los expertos en el tema X?. Por ejemplo, un empleado quiere averiguar quién trabajó en un proyecto en particular para descubrir por qué se tomaron decisiones específicas sin tener que revisar minuciosamente la documentación (si es que la hay). O pueden necesitar un especialista capacitado para consultoría sobre un problema específico. Dentro de una organización, generalmente hay muchos posibles candidatos que podrían ser expertos en un tema dado. Podemos plantear este problema de la siguiente manera: ¿Cuál es la probabilidad de que un candidato ca sea un experto dado el tema de consulta q? Es decir, determinamos p(ca|q) y clasificamos a los candidatos ca de acuerdo con esta probabilidad. Los candidatos con la probabilidad más alta dada la consulta son considerados los expertos más probables en ese tema. El desafío es cómo estimar esta probabilidad con precisión. Dado que es probable que la consulta consista solo en unos pocos términos para describir la experiencia requerida, deberíamos poder obtener una estimación más precisa al invocar el Teorema de Bayes, y estimar: p(ca|q) = p(q|ca)p(ca) p(q) , (1) donde p(ca) es la probabilidad de un candidato y p(q) es la probabilidad de una consulta. Dado que p(q) es una constante, se puede ignorar para fines de clasificación. Por lo tanto, la probabilidad de que un candidato ca sea un experto dado la consulta q es proporcional a la probabilidad de una consulta dada el candidato p(q|ca), ponderada por la creencia a priori p(ca) de que el candidato ca es un experto. p(ca|q) ∝ p(q|ca)p(ca) (2) En este artículo, nuestro enfoque principal se centra en estimar la probabilidad de una consulta dada el candidato p(q|ca), ya que esta probabilidad captura en qué medida el candidato conoce sobre el tema de la consulta. Si bien se asume generalmente que los priors del candidato son uniformes y, por lo tanto, no influirán en la clasificación, se ha demostrado que una elección sensata de priors puede mejorar el rendimiento [20]. Perfilado de expertos. Mientras que la tarea de búsqueda de expertos se centraba en encontrar expertos en un tema específico, la tarea de perfilado de expertos busca responder a una pregunta relacionada: ¿Sobre qué temas tiene conocimiento un candidato? Básicamente, esto da un giro a las preguntas sobre la búsqueda de expertos. El perfilado de un candidato individual implica la identificación de áreas de habilidades y conocimientos en las que tienen experiencia, así como una evaluación del nivel de competencia en cada una de estas áreas. Este es el perfil temático de los candidatos. Generalmente, los perfiles temáticos dentro de las organizaciones consisten en estructuras tabulares que catalogan explícitamente las habilidades y conocimientos de cada individuo en la organización. Sin embargo, esta práctica está limitada por los recursos disponibles para definir, crear, mantener y actualizar estos perfiles con el tiempo. Al centrarnos en métodos automáticos que se basan en la evidencia disponible dentro de los repositorios de documentos de una organización, nuestro objetivo es reducir el esfuerzo humano asociado con el mantenimiento de perfiles temáticos. Un perfil temático de un candidato, entonces, se define como un vector donde cada elemento i del vector corresponde a la experiencia del candidato en un tema dado ki, (es decir, s(ca, ki)). Cada tema ki define un área de conocimiento o habilidad particular que la organización utiliza para definir el perfil temático de los candidatos. Por lo tanto, se asume que se proporciona una lista de temas, {k1, . . . , kn}, donde n es el número de temas predefinidos: perfil(ca) = s(ca, k1), s(ca, k2), . . . , s(ca, kn) . Se necesita contexto y evidencia para ayudar a los usuarios de sistemas de búsqueda de expertos a decidir a quién contactar al buscar experiencia en un área particular. Ejemplos de dicho contexto son: ¿Con quién trabaja ella? ¿Cuáles son sus datos de contacto? ¿Está bien conectada, por si acaso no puede ayudarnos ella misma? ¿Cuál es su rol en la organización? ¿Quién es su superior? Los colaboradores, afiliaciones, etc. son parte del perfil social de los candidatos y pueden servir como antecedentes para interpretar las recomendaciones del sistema. En este documento solo abordamos el problema de determinar perfiles temáticos, dejando el perfilado social para trabajos futuros. Planteamos el problema de cuantificar la competencia de una persona en un área de conocimiento específica de la siguiente manera: ¿Cuál es la probabilidad de que un área de conocimiento (ki) forme parte del perfil de candidatos (expertise)? donde s(ca, ki) está definido por p(ki|ca). Nuestra tarea, entonces, es estimar p(ki|ca), lo cual es equivalente al problema de obtener p(q|ca), donde el tema ki se representa como un tema de consulta q, es decir, una secuencia de palabras clave que representan la experiencia requerida. Tanto la tarea de encontrar expertos como la de perfilar expertos dependen de la estimación precisa de p(q|ca). La única diferencia proviene de la probabilidad previa de que una persona sea experta (p(ca)), la cual puede ser incorporada en la tarea de encontrar expertos. Este antecedente no se aplica a la tarea de perfilado ya que el candidato (individuo) está fijo. 4. MODELOS DE REFERENCIA En esta sección describimos nuestros modelos de referencia para estimar p(q|ca), es decir, las asociaciones entre temas y personas. Tanto la búsqueda de expertos como el perfilado de expertos se reducen a esta estimación. Empleamos tres modelos para calcular esta probabilidad. 4.1 De temas a candidatos Utilizando Modelos de Candidatos: El Modelo 1 [4] define la probabilidad de una consulta dada un candidato (p(q|ca)) utilizando técnicas estándar de modelado de lenguaje, basadas en un modelo de lenguaje multinomial de unigrama. Para cada candidato ca, se infiere un modelo de lenguaje candidato θca de tal manera que la probabilidad de un término dado θca no sea cero para todos los términos, es decir, p(t|θca) > 0. A partir del modelo de candidato, la consulta se genera con la siguiente probabilidad: p(q|θca) = Y t∈q p(t|θca)n(t,q), donde cada término t en la consulta q se muestrea de manera idéntica e independiente, y n(t, q) es el número de veces que t ocurre en q. El modelo de lenguaje candidato se infiere de la siguiente manera: (1) se calcula un modelo empírico p(t|ca); (2) se suaviza con probabilidades de fondo. Usando las asociaciones entre un candidato y un documento, la probabilidad p(t|ca) puede aproximarse por: p(t|ca) = X d p(t|d)p(d|ca), donde p(d|ca) es la probabilidad de que el candidato ca genere un documento de apoyo d, y p(t|d) es la probabilidad de que un término t ocurra en el documento d. Utilizamos la estimación de máxima verosimilitud de un término, es decir, la frecuencia normalizada del término t en el documento d. La fuerza de la asociación entre el documento d y el candidato ca expresada por p(d|ca) refleja el grado en que la experiencia de los candidatos se describe utilizando este documento. La estimación de esta probabilidad se presenta más adelante, en la Sección 4.2. El modelo candidato se construye entonces como una interpolación lineal de p(t|ca) y el modelo de fondo p(t) para asegurar que no haya probabilidades nulas, lo que resulta en la estimación final: p(q|θca) = (4) Y t∈q ( (1 − λ) X d p(t|d)p(d|ca) ! + λp(t) )n(t,q) . El Modelo 1 recopila toda la información de términos de todos los documentos asociados con el candidato, y la utiliza para representar a ese candidato. Este modelo se utiliza para predecir qué tan probable es que un candidato produzca una consulta q. Esto puede interpretarse intuitivamente como la probabilidad de que este candidato hable sobre el tema de la consulta, donde asumimos que esto es indicativo de su experiencia. Usando Modelos de Documentos: El Modelo 2 [4] toma un enfoque diferente. Aquí, el proceso se divide en dos partes. Dado un candidato ca, (1) se selecciona un documento asociado con un candidato con probabilidad p(d|ca), y (2) a partir de este documento se genera una consulta q con probabilidad p(q|d). Entonces se toma la suma sobre todos los documentos para obtener p(q|ca), de modo que: p(q|ca) = Σ d p(q|d)p(d|ca). (5) La probabilidad de una consulta dada un documento se estima inferiendo un modelo de lenguaje del documento θd para cada documento d de manera similar a como se infería el modelo candidato: p(t|θd) = (1 − λ)p(t|d) + λp(t), (6) donde p(t|d) es la probabilidad del término en el documento. La probabilidad de una consulta dada el modelo de documento es: p(q|θd) = Y t∈q p(t|θd)n(t,q) . La estimación final de p(q|ca) se obtiene sustituyendo p(q|d) por p(q|θd) en la Ecuación 5 (ver [4] para más detalles). Conceptualmente, el Modelo 2 difiere del Modelo 1 porque el candidato no está modelado directamente. En cambio, el documento actúa como una variable oculta en el proceso que separa la consulta del candidato. Este proceso es similar a cómo un usuario puede buscar candidatos con un motor de búsqueda estándar: inicialmente encontrando los documentos relevantes y luego viendo quién está asociado con ese documento. Al examinar una serie de documentos, el usuario puede obtener una idea de qué candidatos son más propensos a discutir el tema q. Usando Modelos de Temas: Modelo 3 Introducimos un tercer modelo, Modelo 3. En lugar de intentar modelar el proceso de generación de consultas a través de modelos de candidatos o documentos, representamos la consulta como un modelo de lenguaje de temas y estimamos directamente la probabilidad del candidato p(ca|q). Este enfoque es similar al modelo presentado en [3, 19]. Como en los modelos anteriores, se infiere un modelo de lenguaje, pero esta vez para la consulta. Adaptamos el trabajo de Lavrenko y Croft [14] para estimar un modelo de tema a partir de la consulta. El procedimiento es el siguiente. Dada una colección de documentos y un tema de consulta q, se asume que existe un modelo de tema desconocido θk que asigna probabilidades p(t|θk) a las ocurrencias de términos en los documentos del tema. Tanto la consulta como los documentos son muestras de θk (a diferencia de los enfoques anteriores, donde se asume que una consulta se extrae de un documento específico o un modelo candidato). La tarea principal es estimar p(t|θk), la probabilidad de un término dado el modelo de tema. Dado que la consulta q es muy dispersa y no hay ejemplos de documentos sobre el tema, es necesario aproximar esta distribución. Lavrenko y Croft [14] sugieren una forma razonable de obtener dicha aproximación, asumiendo que p(t|θk) puede aproximarse por la probabilidad del término t dado la consulta q. Entonces podemos estimar p(t|q) utilizando la probabilidad conjunta de observar el término t junto con los términos de la consulta, q1, . . . , qm, y dividiendo por la probabilidad conjunta de los términos de la consulta: p(t|θk) ≈ p(t|q) = p(t, q1, . . . , qm) p(q1, . . . , qm) = p(t, q1, . . . , qm) P t ∈T p(t , q1, . . . , qm) , donde p(q1, . . . , qm) = P t ∈T p(t , q1, . . . , qm), y T es el vocabulario completo de términos. Para estimar la probabilidad conjunta p(t, q1, . . . , qm), seguimos [14, 15] y asumimos que t y q1, . . . , qm son mutuamente independientes, una vez que elegimos una distribución fuente del conjunto de distribuciones fuente subyacentes U. Si elegimos U como un conjunto de modelos de documentos, entonces para construir este conjunto, la consulta q se emitiría contra la colección, y se asume que los primeros n devueltos son relevantes para el tema, y por lo tanto se tratan como muestras del modelo de tema. (Tenga en cuenta que en su lugar podrían usarse modelos candidatos). Con los modelos de documentos formando U, la probabilidad conjunta del término y la consulta se convierte en: p(t, q1, . . . , qm) = X d∈U p(d) ˘ p(t|θd) mY i=1 p(qi|θd) ¯ . (7) Aquí, p(d) denota la distribución previa sobre el conjunto U, que refleja la relevancia del documento para el tema. Suponemos que p(d) es uniforme en todo U. Para clasificar a los candidatos según el modelo de tema definido, utilizamos la métrica de divergencia de Kullback-Leibler (KL, [8]) para medir la diferencia entre los modelos de los candidatos y el modelo de tema: KL(θk||θca) = X t p(t|θk) log p(t|θk) p(t|θca) . (8) Se considera que los candidatos con una divergencia menor respecto al modelo de tema son más propensos a ser expertos en ese tema. El modelo candidato θca está definido en la Ecuación 4. Al utilizar la divergencia de Kullback-Leibler en lugar de la probabilidad de un candidato dado el modelo de tema p(ca|θk), evitamos problemas de normalización. 4.2 Asociaciones documento-candidato Para nuestros modelos, necesitamos poder estimar la probabilidad p(d|ca), que expresa en qué medida un documento d caracteriza al candidato ca. En [4], se presentan dos métodos para estimar esta probabilidad, basados en el número de nombres de personas reconocidos en un documento. Sin embargo, en nuestro entorno de intranet es razonable asumir que los autores de los documentos pueden ser identificados de manera inequívoca (por ejemplo, como el autor de un artículo, el profesor asignado a un curso, el propietario de una página web, etc.). Por lo tanto, establecemos p(d|ca) en 1 si el candidato ca es el autor del documento d, de lo contrario la probabilidad es 0. En la Sección 6 describimos cómo se puede determinar la autoría en diferentes tipos de documentos dentro de la colección. 5. La colección de expertos de UvT utilizada en los experimentos de este artículo se ajusta al escenario descrito en la Sección 3. La colección se basa en el sistema Webwijs (Webwise) desarrollado en la Universidad de Tilburg (UvT) en los Países Bajos. Webwijs (http://www.uvt.nl/webwijs/) es una base de datos de acceso público de empleados de la UvT que están involucrados en investigación o enseñanza; actualmente, Webwijs contiene información sobre 1168 expertos, cada uno de los cuales tiene una página con información de contacto y, si está disponible por parte del experto, una descripción de investigación y lista de publicaciones. Además, cada experto puede seleccionar áreas de especialización de una lista de 1491 temas y se le anima a sugerir nuevos temas que necesitan ser aprobados por el editor de Webwijs. Cada tema tiene una página separada que muestra a todos los expertos asociados con ese tema y, si está disponible, una lista de temas relacionados. Webwijs está disponible en holandés e inglés, y esta bilingüidad se ha preservado en la colección. Cada página de Webwijs en holandés tiene una traducción al inglés. No todos los temas en holandés tienen una traducción al inglés, pero lo contrario es cierto: los 981 temas en inglés tienen un equivalente en holandés. Aproximadamente el 42% de los expertos imparten cursos en la Universidad de Tilburg; estos cursos también fueron rastreados e incluidos en el perfil. Además, aproximadamente el 27% de los expertos enlazan su página académica desde su página de Webwijs. Estas páginas de inicio fueron rastreadas y añadidas a la colección. (Esto significa que si los expertos colocaron las versiones completas de sus publicaciones en sus páginas de inicio académicas, estas también estaban disponibles para su indexación). También obtuvimos 1880 versiones completas de publicaciones del repositorio institucional de UvT y el número de expertos holandeses en inglés es de 1168, con 743 expertos con ≥ 1 tema. El número de temas es de 1491 y 981, y el número de pares experto-tema es de 4318 y 3251, con un promedio de temas por experto de 5.8 y 5.9 respectivamente. El máximo número de temas por experto es de 60 (1) y 35 (1), y el mínimo es de 1 (74) y 1 (106) respectivamente. El promedio de expertos por tema es de 2.9 y 3.3, con un máximo de 30 (1) y un mínimo de 1 (615) y 1 (346) respectivamente. Hay 318 expertos con HP y CD, con un promedio de 3.5 CDs por experto docente. Además, hay 329 expertos con RD y 734 con PUB, con un promedio de 27.0 publicaciones por experto, 25.2 citas por experto y 1.8 publicaciones completas por experto. Tabla 2: Estadísticas descriptivas de las versiones holandesas e inglesas de la colección de expertos de UvT. Ejecutamos el identificador de idioma TextCat [23] para clasificar el idioma de las páginas de inicio y las publicaciones de texto completo. Nos limitamos a las páginas en las que el clasificador tenía confianza en el idioma utilizado en la página. Esto resultó en cuatro tipos de documentos: descripciones de investigación (RD), descripciones de cursos (CD), publicaciones (PUB; versiones de texto completo y solo de cita) y páginas web académicas (HP). Todo fue agrupado en la colección UvT Expert que está disponible en http://ilk.uvt.nl/uvt-expert-collection/. La colección UvT Expert fue extraída de un entorno organizativo diferente a la colección de la W3C y difiere de ella en varios aspectos. El entorno de UvT es uno con cantidades relativamente pequeñas de datos multilingües. Las asociaciones entre los documentos y los autores son claras y los datos están estructurados y limpios. La colección abarca una amplia gama de áreas de especialización, como suele encontrarse en intranets de universidades y otros institutos intensivos en conocimiento. Además, nuestro entorno universitario cuenta con varios tipos de estructura (temática y organizativa), así como múltiples tipos de documentos. Otra diferencia importante entre los dos conjuntos de datos es que las áreas de especialización en la colección de Expertos de UvT son autoseleccionadas en lugar de basarse en la membresía de un grupo o asignaciones de otros. El tamaño es otra dimensión en la que difieren las colecciones de expertos de W3C y UvT: esta última es la más pequeña de las dos. También son realistas las grandes diferencias en la cantidad de información disponible para cada experto. Utilizar Webwijs es voluntario; 425 expertos holandeses no seleccionaron ningún tema en absoluto. Esto nos deja con 743 perfiles de expertos holandeses y 727 perfiles de expertos en inglés utilizables. La Tabla 2 proporciona estadísticas descriptivas para la colección de Expertos de UvT. Las universidades tienden a tener una estructura jerárquica que va desde el nivel de la facultad, pasando por los departamentos, grupos de investigación, hasta llegar a los investigadores individuales. En la colección de Expertos de UvT tenemos información sobre las afiliaciones de los investigadores con facultades e institutos, lo que nos proporciona una jerarquía organizativa de dos niveles. La Universidad de Tilburg tiene 22 unidades organizativas a nivel de facultad (incluida la oficina universitaria y varios institutos de investigación) y 71 departamentos, lo que equivale a 3.2 departamentos por facultad. En cuanto a la jerarquía temática utilizada por Webwijs, 131 de los 1491 temas son nodos principales en la jerarquía. Esta jerarquía tiene una longitud promedio de cadena de temas de 2.65 y una longitud máxima de 7 temas. 6. EVALUACIÓN A continuación, evaluamos los modelos de la Sección 4 para la búsqueda y perfilado de expertos en la colección de Expertos de UvT. Detallamos nuestras preguntas de investigación y configuración experimental, y luego presentamos nuestros resultados. 6.1 Preguntas de investigación Abordamos las siguientes preguntas de investigación. Tanto la búsqueda de expertos como el perfilado dependen de las estimaciones de p(q|ca). La pregunta es cómo se comparan los modelos en las diferentes tareas, y en el contexto de la colección de expertos de UvT. En [4], el Modelo 2 superó al Modelo 1 en la colección de la W3C. ¿Cómo se comparan en nuestro conjunto de datos? ¿Y cómo se compara el Modelo 3 con el Modelo 1? ¿Qué hay de las diferencias de rendimiento entre los dos idiomas en nuestra colección de pruebas? 6.2 Configuración Experimental La salida de nuestros modelos fue evaluada frente a las etiquetas de temas autoasignadas, que fueron tratadas como juicios de relevancia. Los resultados fueron evaluados por separado para inglés y holandés. Para inglés solo usamos temas para los cuales había traducción al neerlandés; para neerlandés se consideraron todos los temas. Los resultados se promediaron para las consultas en la intersección de las evaluaciones de relevancia y los resultados; las consultas faltantes no contribuyen con un valor de 0 a las puntuaciones. Utilizamos medidas estándar de recuperación de información, como la Precisión Media Promedio (MAP) y la Reciprocidad Media Promedio (MRR). También informamos el porcentaje de temas (%q) y candidatos (%ca) cubiertos, respectivamente, para las tareas de búsqueda y perfilado de expertos. 6.3 Resultados La Tabla 1 muestra el rendimiento de los Modelos 1, 2 y 3 en las tareas de búsqueda y perfilado de expertos. Las filas de la tabla corresponden a los diversos tipos de documentos (RD, CD, PUB y HP) y a sus combinaciones. RD+CD+PUB+HP es equivalente a la colección completa y será referida como la LÍNEA BASE de nuestros experimentos. Al observar la Tabla 1, vemos que el Modelo 2 tiene el mejor rendimiento en general. Sin embargo, cuando los datos están limpios y muy enfocados (RD), el Modelo 3 lo supera en varios casos. El modelo 1 tiene la mejor cobertura de candidatos (%ca) y temas (%q). Los diversos tipos de documentos difieren en sus características y en cómo mejoran las tareas de búsqueda y perfilado. El perfilado de expertos se beneficia mucho de los datos limpios presentes en los tipos de documentos RD y CD, mientras que las publicaciones contribuyen principalmente a la tarea de encontrar expertos. Agregar las páginas de inicio no resulta ser particularmente útil. Cuando comparamos los resultados entre idiomas, encontramos que la cobertura de los temas en inglés (%q) es mayor que la de los temas en neerlandés para la búsqueda de expertos. Además de eso, las puntuaciones caen en el mismo rango para ambos idiomas. Para la tarea de perfilado, la cobertura de los candidatos (%ca) es muy similar para ambos idiomas. Sin embargo, el rendimiento es considerablemente mejor para los temas en inglés. Si bien es difícil comparar las puntuaciones entre colecciones, concluimos con una breve comparación de las puntuaciones absolutas en la Tabla 1 con las reportadas en [3, 4] en el conjunto de pruebas de W3C (edición de 2005). Para el experto que busca, los puntajes MAP para el Modelo 2 reportados aquí son aproximadamente un 50% más altos que las cifras correspondientes en [4], mientras que nuestros puntajes MRR están ligeramente por debajo de los de [4]. Para el perfilado de expertos, las diferencias son mucho más dramáticas: las puntuaciones MAP para el Modelo 2 reportadas aquí son aproximadamente un 50% más bajas que las puntuaciones en [3], mientras que las puntuaciones MRR (mejores) son aproximadamente iguales a las de [3]. La causa de estas diferencias parece residir en el número de áreas de conocimiento consideradas aquí, aproximadamente 30 veces más que en el entorno del W3C. MODELOS AVANZADOS Ahora que hemos desarrollado y evaluado técnicas básicas de modelado de lenguaje para la recuperación de expertos, nos enfocamos en modelos refinados que explotan características especiales de nuestra colección de pruebas. 7.1 Explotando la similitud en el área de conocimiento Una forma de mejorar la puntuación de una consulta dada un candidato es considerar qué otras solicitudes satisfaría el candidato y usarlas como evidencia adicional para respaldar la consulta original, proporcionalmente. Búsqueda de expertos Perfilado de expertos Tipos de documentos Modelo 1 Modelo 2 Modelo 3 Modelo 1 Modelo 2 Modelo 3 %q MAP MRR %q MAP MRR %q MAP MRR %ca MAP MRR %ca MAP MRR %ca MAP MRR Inglés RD 97.8 0.126 0.269 83.5 0.144 0.311 83.3 0.129 0.271 100 0.089 0.189 39.3 0.232 0.465 41.1 0.166 0.337 CD 97.8 0.118 0.227 91.7 0.123 0.248 91.7 0.118 0.226 32.8 0.188 0.381 32.4 0.195 0.385 32.7 0.203 0.370 PUB 97.8 0.200 0.330 98.0 0.216 0.372 98.0 0.145 0.257 78.9 0.167 0.364 74.5 0.212 0.442 78.9 0.135 0.299 HP 97.8 0.081 0.186 97.4 0.071 0.168 97.2 0.062 0.149 31.2 0.150 0.299 28.8 0.185 0.335 30.1 0.136 0.287 RD+CD 97.8 0.188 0.352 92.9 0.193 0.360 92.9 0.150 0.273 100 0.145 0.286 61.3 0.251 0.477 63.2 0.217 0.416 RD+CD+PUB 97.8 0.235 0.373 98.1 0.277 0.439 98.1 0.178 0.305 100 0.196 0.380 87.2 0.280 0.533 89.5 0.170 0.344 RD+CD+PUB+HP 97.8 0.237 0.372 98.6 0.280 0.441 98.5 0.166 0.293 100 0.199 0.387 88.7 0.281 0.525 90.9 0.169 0.329 Holandés RD 61.3 0.094 0.229 38.4 0.137 0.336 38.3 0.127 0.295 38.0 0.127 0.386 34.1 0.138 0.420 38.0 0.105 0.327 CD 61.3 0.107 0.212 49.7 0.128 0.256 49.7 0.136 0.261 32.5 0.151 0.389 31.8 0.158 0.396 32.5 0.170 0.380 PUB 61.3 0.193 0.319 59.5 0.218 0.368 59.4 0.173 0.291 78.8 0.126 0.364 76.0 0.150 0.424 78.8 0.103 0.294 HP 61.3 0.063 0.169 56.6 0.064 0.175 56.4 0.062 0.163 29.8 0.108 0.308 27.8 0.125 0.338 29.8 0.098 0.255 RD+CD 61.3 0.159 0.314 51.9 0.184 0.360 51.9 0.169 0.324 60.5 0.151 0.410 57.2 0.166 0.431 60.4 0.159 0.384 RD+CD+PUB 61.3 0.244 0.398 61.5 0.260 0.424 61.4 0.210 0.350 90.3 0.165 0.445 88.2 0.189 0.479 90.3 0.126 0.339 RD+CD+PUB+HP 61.3 0.249 0.401 62.6 0.265 0.436 62.6 0.195 0.344 91.9 0.164 0.426 90.1 0.195 0.488 91.9 0.125 0.328 Tabla 1: Rendimiento de los modelos en las tareas de búsqueda y perfilado de expertos, utilizando diferentes tipos de documentos y sus combinaciones. %q es el número de temas cubiertos (se aplica a la tarea de búsqueda de expertos), %ca es el número de candidatos cubiertos (se aplica a la tarea de perfilado de expertos). Los bloques superior e inferior corresponden al inglés y al holandés respectivamente. Las mejores puntuaciones están en negrita. Esto se puede modelar interpolando entre el p(q|ca) y la evidencia adicional de todos los pedidos similares q, de la siguiente manera: p(q|ca) = λp(q|ca) + (1 − λ) X q p(q|q )p(q |ca), (9) donde p(q|q ) representa la similitud entre los dos temas q y q. Para poder trabajar con métodos de similitud que no son necesariamente probabilidades, establecemos p(q|q ) = w(q,q ) γ , donde γ es una constante de normalización, de modo que γ = P q w(q , q ). Consideramos cuatro métodos para calcular la puntuación de similitud entre dos temas. Tres enfoques se basan estrictamente en el contenido y establecen similitudes examinando patrones de co-ocurrencia de temas dentro de la colección, mientras que el último enfoque explota la estructura jerárquica de áreas temáticas que pueden estar presentes dentro de una organización (ver [7] para más ejemplos de integrar relaciones entre palabras en modelos de lenguaje). La métrica de divergencia Kullback-Leibler (KL) definida en la ecuación 8 proporciona una medida de cuán diferentes o similares son dos distribuciones de probabilidad. Se infiere un modelo de tema para q y q utilizando el método presentado en la Sección 4.1 para describir la consulta en todo el vocabulario. Dado que un puntaje KL más bajo significa que las consultas son más similares, permitimos que w(q, q ) = max(KL(θq||·) − KL(θq||θq )). El Información Mutua Puntual (PMI, [17]) es una medida de asociación utilizada en teoría de la información para determinar el grado de independencia entre variables. La dependencia entre dos consultas se refleja en la puntuación SI(q, q), donde las puntuaciones mayores que cero indican que es probable que exista una dependencia, lo que interpretamos como que las consultas son probablemente similares: SI(q, q) = log p(q, q) / p(q)p(q). Estimamos la probabilidad de un tema p(q) utilizando el número de documentos relevantes para la consulta q dentro de la colección. La probabilidad conjunta p(q, q) se estima de manera similar, utilizando la concatenación de q y q como una consulta. Para obtener p(q|q), luego establecemos w(q, q) = SI(q, q) cuando SI(q, q) > 0, de lo contrario w(q, q) = 0, ya que solo estamos interesados en incluir consultas que sean similares. La estadística de log-verosimilitud proporciona otra medida de dependencia, que es más confiable que la medida de información mutua puntual [17]. Sea k1 el número de co-ocurrencias de q y q, k2 el número de ocurrencias de q que no co-ocurren con q, n1 el número total de ocurrencias de q, y n2 el número total de tokens de tema menos el número de ocurrencias de q. Entonces, sea p1 = k1/n1, p2 = k2/n2 y p = (k1 + k2)/(n1 + n2), (q, q ) = 2( (p1, k1, n1) + (p2, k2, n2) − (p, k1, n1) − (p, k2, n2)), donde (p, n, k) = k log p + (n − k) log(1 − p). El puntaje más alto indica que las consultas también son probablemente similares, por lo tanto, establecemos w(q, q ) = (q, q ). Finalmente, también estimamos la similitud de dos temas basándonos en su distancia dentro de la jerarquía de temas. La jerarquía de temas se visualiza como un grafo dirigido, y para todos los pares de temas se calcula el camino más corto SP(q, q). Establecimos el puntaje de similitud como el recíproco del camino más corto: w(q, q ) = 1/SP(q, q ). 7.2 Información contextual Dada la jerarquía de una organización, las unidades a las que pertenece una persona se consideran como un contexto para compensar la escasez de datos. Lo modelamos de la siguiente manera: p (q|ca) = 1 − P ou∈OU(ca) λou · p(q|ca) + P ou∈OU(ca) λou · p(q|ou), donde OU(ca) es el conjunto de unidades organizativas de las cuales el candidato ca es miembro, y p(q|o) expresa la fuerza de la asociación entre la consulta q y la unidad ou. La última probabilidad se puede estimar utilizando cualquiera de los tres modelos básicos, simplemente reemplazando ca con ou en las ecuaciones correspondientes. Una unidad organizativa está asociada con todos los documentos que sus miembros han escrito. Es decir, p(d|ou) = maxca∈ou p(d|ca). 7.3 Un modelo multilingüe simple Para institutos de conocimiento en Europa, ya sea académicos u otros, un entorno multilingüe (o al menos bilingüe) es típico. El siguiente modelo se basa en un tipo de suposición de independencia: no hay transferencia de conocimientos/perfiles a través de las barreras del idioma. Si bien es una simplificación, esta es una aproximación sensata en primer lugar. Eso es: p (q|ca) =P l∈L λl · p(ql|ca), donde L es el conjunto de idiomas utilizados en la colección, ql es la traducción de la consulta q al idioma l, y λl es un parámetro de suavizado específico del idioma, tal que P l∈L λl = 1. 8. MODELOS AVANZADOS: EVALUACIÓN En esta sección presentamos una evaluación experimental de nuestros modelos avanzados. Búsqueda de expertos Perfilado de expertos Modelo de lenguaje 1 Modelo 2 Modelo 3 Modelo 1 Modelo 2 Modelo 3 %q MAP MRR %q MAP MRR %q MAP MRR %ca MAP MRR %ca MAP MRR %ca MAP MRR Solo inglés 97.8 0.237 0.372 98.6 0.280 0.441 98.5 0.166 0.293 100 0.199 0.387 88.7 0.281 0.525 90.9 0.169 0.329 Solo holandés 61.3 0.249 0.401 62.6 0.265 0.436 62.6 0.195 0.344 91.9 0.164 0.426 90.1 0.195 0.488 91.9 0.125 0.328 Combinación 99.4 0.297 0.444 99.7 0.324 0.491 99.7 0.223 0.388 100 0.241 0.445 92.1 0.313 0.564 93.2 0.224 0.411 Tabla 3: Rendimiento de la combinación de idiomas en las tareas de búsqueda y perfilado de expertos (en candidatos). Los mejores puntajes para cada modelo están en cursiva, los mejores puntajes absolutos para las tareas de búsqueda y perfilado de expertos están en negrita. Tabla 4: Rendimiento en las tareas de búsqueda de expertos (arriba) y perfilado (abajo), utilizando similitudes de áreas de conocimiento. Las carreras fueron evaluadas en los temas principales establecidos. Las mejores puntuaciones están en negrita. 8.1 Preguntas de investigación Nuestras preguntas siguen los refinamientos presentados en la sección anterior: ¿Explotar la similitud en el área de conocimiento mejora la efectividad? ¿Cuál de los diversos métodos para capturar las relaciones entre palabras es más efectivo? Además, ¿es útil nuestra forma de incorporar información contextual? ¿Para qué tareas? Y finalmente, ¿es nuestra forma simple de combinar las puntuaciones monolingües suficiente para obtener mejoras significativas? 8.2 Configuración experimental Dado que las autoevaluaciones también son escasas en nuestra colección, para poder medir las diferencias entre los diversos modelos, seleccionamos un subconjunto de temas y evaluamos (algunas de las) ejecuciones solo en este subconjunto. Este conjunto se denomina como temas principales, y consiste en temas que se encuentran en el nivel superior de la jerarquía temática. (Un tema principal tiene subtemas, pero no es un subtema de ningún otro tema). Este conjunto principal consiste en 132 temas en holandés y 119 en inglés. Los juicios de relevancia se limitaron al conjunto de temas principales, pero no se ampliaron con subtemas. 8.3 Explotando la similitud de áreas de conocimiento. La Tabla 4 presenta los resultados. Los cuatro métodos utilizados para estimar la similitud de áreas de conocimiento son la divergencia de KL (KLDIV) y PointLang. Modelo 1 Modelo 2 Modelo 3 MAP MRR MAP MRR MAP MRR Búsqueda de expertos UK TODOS 0.423 0.545 0.654 0.799 0.494 0.629 UK PRINCIPAL 0.500 0.621 0.704 0.834 0.587 0.699 NL TODOS 0.439 0.560 0.672 0.826 0.480 0.630 NL PRINCIPAL 0.440 0.584 0.645 0.816 0.515 0.655 Perfilado de expertos UK TODOS 0.240 0.640 0.306 0.778 0.223 0.616 UK PRINCIPAL 0.523 0.677 0.519 0.648 0.461 0.587 NL TODOS 0.203 0.716 0.254 0.770 0.183 0.627 NL PRINCIPAL 0.332 0.576 0.380 0.624 0.332 0.549 Tabla 5: Evaluación de los modelos de contexto en unidades organizativas. información mutua ponderada (PMI), log-verosimilitud (LL) y distancia dentro de la jerarquía de temas (HDIST). Logramos mejorar el resultado base en todos los casos, pero la mejora es más notable para la tarea de perfilado. Para ambas tareas, el método LL tuvo el mejor rendimiento. Los enfoques basados en el contenido tuvieron un rendimiento consistentemente mejor que HDIST. 8.4 Información contextual Una jerarquía de dos niveles de unidades organizativas (facultades e institutos) está disponible en la colección de Expertos de UvT. La unidad a la que pertenece una persona se utiliza como contexto para esa persona. Primero, evaluamos los modelos de las unidades organizativas, utilizando todos los temas (TODOS) y solo los temas principales (PRINCIPALES). Una unidad organizativa se considera relevante para un tema dado (o viceversa) si al menos un miembro de la unidad seleccionó el tema dado como área de especialización. La Tabla 5 informa sobre los resultados. En lo que respecta a la búsqueda de expertos, dado un tema, la unidad organizativa correspondiente puede ser identificada con alta precisión. Sin embargo, la tarea de perfilado de expertos muestra una imagen diferente: las puntuaciones son bajas y la tarea parece difícil. La explicación puede ser que los conceptos generales (es decir, nuestros temas principales) pueden pertenecer a varias unidades organizativas. Segundo, realizamos otra evaluación, donde combinamos los modelos contextuales con los modelos candidatos (para puntuar a los candidatos nuevamente). La tabla 6 informa sobre los resultados. Encontramos un impacto positivo de los modelos de contexto solo para la búsqueda de expertos. Notablemente, para la búsqueda de expertos (y Modelo 1), mejora más del 50% (para inglés) y más del 70% (para holandés) en el MAP. El bajo rendimiento en la creación de perfiles de expertos puede deberse al hecho de que los modelos de contexto por sí solos no tuvieron un buen desempeño en la tarea de creación de perfiles desde el principio. 8.5 Modelos multilingües En esta subsección evaluamos el método para combinar resultados en múltiples idiomas que describimos en la Sección 7.3. En nuestro entorno, el conjunto de idiomas consiste en inglés y holandés: L = {UK, NL}. Los pesos de estos idiomas se establecieron para ser idénticos (λUK = λNL = 0.5). Realizamos experimentos con diferentes configuraciones de λ, pero no observamos diferencias significativas en el rendimiento. La Tabla 3 informa sobre los resultados multilingües, donde el rendimiento se evalúa en el conjunto completo de temas. Los tres modelos tienen una diferencia significativa. Tabla 6: Rendimiento de los modelos de contexto (CT) en comparación con la línea base (BL). Las mejores puntuaciones están en negrita, demostradas en todas las medidas para ambas tareas. La cobertura de temas y candidatos para las tareas de búsqueda y perfilado de expertos, respectivamente, es cercana al 100% en todos los casos. La mejora relativa de las puntuaciones de precisión varía del 10% al 80%. Estas puntuaciones demuestran que, a pesar de su simplicidad, nuestro método para combinar resultados en varios idiomas logra mejoras sustanciales sobre el punto de referencia. CONCLUSIONES En este artículo nos enfocamos en la recuperación de expertos (búsqueda y perfilado de expertos) en un nuevo entorno de una organización típica intensiva en conocimiento en la que los datos disponibles son de alta calidad, multilingües y abarcan una amplia gama de áreas de expertise. Normalmente, la cantidad de datos disponibles en una organización como una universidad, un instituto de investigación o un laboratorio de investigación es limitada en comparación con la colección de la W3C que ha sido utilizada principalmente para la evaluación experimental de la recuperación de la experiencia hasta ahora. Para examinar la recuperación de la experiencia en este entorno, presentamos (y lanzamos) la colección de Expertos de UvT como un caso representativo de organizaciones intensivas en conocimiento. La nueva colección refleja las propiedades típicas de los institutos intensivos en conocimiento mencionados anteriormente e incluye también varias características que pueden ser potencialmente útiles para la recuperación de expertos, como la estructura temática y organizativa. Evaluamos cómo los modelos de vanguardia para la búsqueda y perfilado de expertos se desempeñaron en este nuevo entorno y luego refinamos estos modelos para intentar explotar las diferentes características dentro del entorno de datos (idioma, actualidad y estructura organizativa). Encontramos que los modelos actuales de recuperación de experiencia se generalizan bien a este nuevo entorno; además, descubrimos que refinar los modelos para tener en cuenta las diferencias resulta en mejoras significativas, compensando así los problemas causados por la escasez de datos. El trabajo futuro incluye establecer evaluaciones manuales de perfiles generados automáticamente por los propios empleados, especialmente en casos en los que los empleados no han proporcionado un perfil ellos mismos. 10. AGRADECIMIENTOS Krisztian Balog fue apoyado por la Organización Neerlandesa para la Investigación Científica (NWO) bajo el número de proyecto 220-80-001. Maarten de Rijke también recibió apoyo de NWO bajo los números de proyecto 017.001.190, 220-80-001, 264-70-050, 354-20-005, 600.065.120, 612-13-001, 612.000.106, 612.066.302, 612.069.006, 640.001.501, 640.002.501, y de la Unión Europea. Programa IST del 6º Programa Marco para la I+D en virtud del contrato del proyecto MultiMATCH IST-033104. El trabajo de Toine Bogers y Antal van den Bosch fue financiado por el programa IOP-MMI de SenterNovem / Ministerio de Asuntos Económicos de los Países Bajos, como parte del proyecto 'A Propos'. 11. REFERENCIAS [1] L. Azzopardi. Incorporando contexto en el marco de modelado del lenguaje para la recuperación de información ad-hoc. Tesis doctoral, Universidad de Paisley, 2005. [2] K. Balog y M. de Rijke. Encontrando expertos similares. En este volumen, 2007. [3] K. Balog y M. de Rijke. Determinación de perfiles de expertos (con una aplicación a la búsqueda de expertos). En IJCAI 07: Proc. 20th Intern. Conferencia Conjunta sobre Inteligencia Artificial, páginas 2657-2662, 2007. [4] K. Balog, L. Azzopardi y M. de Rijke. Modelos formales para la búsqueda de expertos en corporaciones empresariales. En SIGIR 06: Actas de la 29ª conferencia internacional anual. Conferencia ACM SIGIR sobre Investigación y desarrollo en recuperación de información, páginas 43-50, 2006. [5] I. Becerra-Fernández. El papel de las tecnologías de inteligencia artificial en la implementación de sistemas de gestión del conocimiento de localización de personas. En el taller de AAAI sobre la incorporación de conocimiento en los procesos de negocio, marzo de 2000. [6] C. S. Campbell, P. P. Maglio, A. Cozzi y B. Dom. Identificación de la experiencia utilizando comunicaciones por correo electrónico. En CIKM 03: Actas de la duodécima conferencia internacional sobre gestión de la información y el conocimiento, páginas 528-531, 2003. [7] G. Cao, J.-Y. Nie, y J. Bai. Integrando relaciones entre palabras en modelos de lenguaje. En SIGIR 05: Actas de la 28ª conferencia internacional anual. Conferencia ACM SIGIR sobre Investigación y desarrollo en recuperación de información, páginas 298-305, 2005. [8] T. M. Cover y J. A. Thomas. Elementos de la teoría de la información. Wiley-Interscience, 1991. [9] N. Craswell, D. Hawking, A. M. Vercoustre y P. Wilkins. Experto en P@noptic: Buscando expertos no solo documentos. En Ausweb, 2001. [10] N. Craswell, A. de Vries e I. Soboroff. Resumen de la pista empresarial TREC2005. En la Decimocuarta Conferencia de Recuperación de Información. Proc. (TREC 2005), 2006. [11] T. H. Davenport y L. Prusak. Conocimiento en Acción: Cómo las Organizaciones Gestionan lo que Saben. Harvard Business School Press, Boston, MA, 1998. [12] T. Dunning.\nHarvard Business School Press, Boston, MA, 1998. [12] T. Dunning. Métodos precisos para la estadística de sorpresa y coincidencia. Lingüística Computacional, 19(1):61-74, 1993. [13] E. Filatova y J. Prager. Dime qué haces y te diré quién eres: Aprendiendo actividades relacionadas con la ocupación para biografías. En HLT/EMNLP, 2005. [14] V. Lavrenko y W. B. Croft. Modelos de lenguaje basados en relevancia. En SIGIR 01: Actas de la 24ª conferencia internacional anual. Conferencia ACM SIGIR sobre Investigación y Desarrollo en Recuperación de Información, páginas 120-127, 2001. [15] V. Lavrenko, M. Choquette y W. B. Croft. Modelos de relevancia multilingües. En SIGIR 02: Actas de la 25ª conferencia internacional anual. Conferencia ACM SIGIR sobre Investigación y Desarrollo en Recuperación de Información, páginas 175-182, 2002. [16] C. Macdonald e I. Ounis. Votación de candidatos: adaptando técnicas de fusión de datos para una tarea de búsqueda de expertos. En CIKM 06: Actas de la 15ª conferencia internacional de ACM sobre gestión de la información y el conocimiento, páginas 387-396, 2006. [17] C. Manning y H. Schütze. Fundamentos del Procesamiento del Lenguaje Natural Estadístico. El MIT Press, 1999. [18] A. Mockus y J. D. Herbsleb. Explorador de experticia: un enfoque cuantitativo para identificar la experticia. En ICSE 02: Proc. 24th Intern. Conf. en Ingeniería de Software, páginas 503-512, 2002. [19] D. Petkova y W. B. Croft. Modelos de lenguaje jerárquicos para la búsqueda de expertos en corporaciones empresariales. En Proc. ICTAI 2006, páginas 599-608, 2006. [20] I. Soboroff, A. de Vries y N. Craswell. Resumen de la pista empresarial TREC 2006. En las Notas de Trabajo de TREC 2006, 2006. [21] T. Tao, X. Wang, Q. Mei y C. Zhai. Recuperación de información del modelo de lenguaje con expansión de documentos. En HLT-NAACL 2006, 2006. [22] TREC. Ruta empresarial, 2005. URL: http://www.ins.cwi.nl/projects/trec-ent/wiki/. [23] G. van Noord.\nURL: http://www.ins.cwi.nl/projects/trec-ent/wiki/. [23] G. van Noord. Adivinador de idioma TextCat. URL: http://www.let.rug.nl/˜vannoord/TextCat/. [24] W3C. La colección de pruebas del W3C, 2005. URL: http://research.microsoft.com/users/nickcr/w3c-summary.html.",
    "original_sentences": [
        "Broad Expertise Retrieval in Sparse Data Environments Krisztian Balog ISLA, University of Amsterdam Kruislaan 403, 1098 SJ Amsterdam, The Netherlands kbalog@science.uva.nl Toine Bogers ILK, Tilburg University P.O.",
        "Box 90153, 5000 LE Tilburg, The Netherlands A.M.Bogers@uvt.nl Leif Azzopardi Dept. of Computing Science University of Glasgow, Glasgow, G12 8QQ leif@dcs.gla.ac.uk Maarten de Rijke ISLA, University of Amsterdam Kruislaan 403, 1098 SJ Amsterdam, The Netherlands mdr@science.uva.nl Antal van den Bosch ILK, Tilburg University P.O.",
        "Box 90153, 5000 LE Tilburg, The Netherlands Antal.vdnBosch@uvt.nl ABSTRACT Expertise retrieval has been largely unexplored on data other than the W3C collection.",
        "At the same time, many intranets of universities and other knowledge-intensive organisations offer examples of relatively small but clean multilingual expertise data, covering broad ranges of expertise areas.",
        "We first present two main expertise retrieval tasks, along with a set of baseline approaches based on generative language modeling, aimed at finding expertise relations between topics and people.",
        "For our experimental evaluation, we introduce (and release) a new test set based on a crawl of a university site.",
        "Using this test set, we conduct two series of experiments.",
        "The first is aimed at determining the effectiveness of baseline expertise retrieval methods applied to the new test set.",
        "The second is aimed at assessing refined models that exploit characteristic features of the new test set, such as the organizational structure of the university, and the hierarchical structure of the topics in the test set.",
        "Expertise retrieval models are shown to be robust with respect to environments smaller than the W3C collection, and current techniques appear to be generalizable to other settings.",
        "Categories and Subject Descriptors H.3 [Information Storage and Retrieval]: H.3.1 Content Analysis and Indexing; H.3.3 Information Search and Retrieval; H.3.4 Systems and Software; H.4 [Information Systems Applications]: H.4.2 Types of Systems; H.4.m Miscellaneous General Terms Algorithms, Measurement, Performance, Experimentation 1.",
        "INTRODUCTION An organizations intranet provides a means for exchanging information between employees and for facilitating employee collaborations.",
        "To efficiently and effectively achieve this, it is necessary to provide search facilities that enable employees not only to access documents, but also to identify expert colleagues.",
        "At the TREC Enterprise Track [22] the need to study and understand expertise retrieval has been recognized through the introduction of Expert Finding tasks.",
        "The goal of expert finding is to identify a list of people who are knowledgeable about a given topic.",
        "This task is usually addressed by uncovering associations between people and topics [10]; commonly, a co-occurrence of the name of a person with topics in the same context is assumed to be evidence of expertise.",
        "An alternative task, which using the same idea of people-topic associations, is expert profiling, where the task is to return a list of topics that a person is knowledgeable about [3].",
        "The launch of the Expert Finding task at TREC has generated a lot of interest in expertise retrieval, with rapid progress being made in terms of modeling, algorithms, and evaluation aspects.",
        "However, nearly all of the expert finding or profiling work performed has been validated experimentally using the W3C collection [24] from the Enterprise Track.",
        "While this collection is currently the only publicly available test collection for expertise retrieval tasks, it only represents one type of intranet.",
        "With only one test collection it is not possible to generalize conclusions to other realistic settings.",
        "In this paper we focus on expertise retrieval in a realistic setting that differs from the W3C setting-one in which relatively small amounts of clean, multilingual data are available, that cover a broad range of expertise areas, as can be found on the intranets of universities and other knowledge-intensive organizations.",
        "Typically, this setting features several additional types of structure: topical structure (e.g., topic hierarchies as employed by the organization), organizational structure (faculty, department, ...), as well as multiple types of documents (research and course descriptions, publications, and academic homepages).",
        "This setting is quite different from the W3C setting in ways that might impact upon the performance of expertise retrieval tasks.",
        "We focus on a number of research questions in this paper: Does the relatively small amount of data available on an intranet affect the quality of the topic-person associations that lie at the heart of expertise retrieval algorithms?",
        "How do state-of-the-art algorithms developed on the W3C data set perform in the alternative scenario of the type described above?",
        "More generally, do the lessons from the Expert Finding task at TREC carry over to this setting?",
        "How does the inclusion or exclusion of different documents affect expertise retrieval tasks?",
        "In addition to, how can the topical and organizational structure be used for retrieval purposes?",
        "To answer our research questions, we first present a set of baseline approaches, based on generative language modeling, aimed at finding associations between topics and people.",
        "This allows us to formulate the expert finding and expert profiling tasks in a uniform way, and has the added benefit of allowing us to understand the relations between the two tasks.",
        "For our experimental evaluation, we introduce a new data set (the UvT Expert Collection) which is representative of the type of intranet that we described above.",
        "Our collection is based on publicly available data, crawled from the website of Tilburg University (UvT).",
        "This type of data is particularly interesting, since (1) it is clean, heterogeneous, structured, and focused, but comprises a limited number of documents; (2) contains information on the organizational hierarchy; (3) it is bilingual (English and Dutch); and (4) the list of expertise areas of an individual are provided by the employees themselves.",
        "Using the UvT Expert collection, we conduct two sets of experiments.",
        "The first is aimed at determining the effectiveness of baseline expertise finding and profiling methods in this new setting.",
        "A second group of experiments is aimed at extensions of the baseline methods that exploit characteristic features of the UvT Expert Collection; specifically, we propose and evaluate refined expert finding and profiling methods that incorporate topicality and organizational structure.",
        "Apart from the research questions and data set that we contribute, our main contributions are as follows.",
        "The baseline models developed for expertise finding perform well on the new data set.",
        "While on the W3C setting the expert finding task appears to be more difficult than profiling, for the UvT data the opposite is the case.",
        "We find that profiling on the UvT data set is considerably more difficult than on the W3C set, which we believe is due to the large (but realistic) number of topical areas that we used for profiling: about 1,500 for the UvT set, versus 50 in the W3C case.",
        "Taking the similarity between topics into account can significantly improve retrieval performance.",
        "The best performing similarity measures are content-based, therefore they can be applied on the W3C (and other) settings as well.",
        "Finally, we demonstrate that the organizational structure can be exploited in the form of a context model, improving MAP scores for certain models by up to 70%.",
        "The remainder of this paper is organized as follows.",
        "In the next section we review related work.",
        "Then, in Section 3 we provide detailed descriptions of the expertise retrieval tasks that we address in this paper: expert finding and expert profiling.",
        "In Section 4 we present our baseline models, of which the performance is then assessed in Section 6 using the UvT data set that we introduce in Section 5.",
        "Advanced models exploiting specific features of our data are presented in Section 7 and evaluated in Section 8.",
        "We formulate our conclusions in Section 9. 2.",
        "RELATED WORK Initial approaches to expertise finding often employed databases containing information on the skills and knowledge of each individual in the organization [11].",
        "Most of these tools (usually called yellow pages or people-finding systems) rely on people to self-assess their skills against a predefined set of keywords.",
        "For updating profiles in these systems in an automatic fashion there is a need for intelligent technologies [5].",
        "More recent approaches use specific document sets (such as email [6] or software [18]) to find expertise.",
        "In contrast with focusing on particular document types, there is also an increased interest in the development of systems that index and mine published intranet documents as sources of evidence for expertise.",
        "One such published approach is the P@noptic system [9], which builds a representation of each person by concatenating all documents associated with that person-this is similar to Model 1 of Balog et al. [4], who formalize and compare two methods.",
        "Balog et al.s Model 1 directly models the knowledge of an expert from associated documents, while their Model 2 first locates documents on the topic and then finds the associated experts.",
        "In the reported experiments the second method performs significantly better when there are sufficiently many associated documents per candidate.",
        "Most systems that took part in the 2005 and 2006 editions of the Expert Finding task at TREC implemented (variations on) one of these two models; see [10, 20].",
        "Macdonald and Ounis [16] propose a different approach for ranking candidate expertise with respect to a topic based on data fusion techniques, without using collectionspecific heuristics; they find that applying field-based weighting models improves the ranking of candidates.",
        "Petkova and Croft [19] propose yet another approach, based on a combination of the above Model 1 and 2, explicitly modeling topics.",
        "Turning to other expert retrieval tasks that can also be addressed using topic-people associations, Balog and de Rijke [3] addressed the task of determining topical expert profiles.",
        "While their methods proved to be efficient on the W3C corpus, they require an amount of data that may not be available in the typical knowledge-intensive organization.",
        "Balog and de Rijke [2] study the related task of finding experts that are similar to a small set of experts given as input.",
        "As an aside, creating a textual summary of a person shows some similarities to biography finding, which has received a considerable amount of attention recently; see e.g., [13].",
        "We use generative language modeling to find associations between topics and people.",
        "In our modeling of expert finding and profiling we collect evidence for expertise from multiple sources, in a heterogeneous collection, and integrate it with the co-occurrence of candidates names and query terms-the language modeling setting allows us to do this in a transparent manner.",
        "Our modeling proceeds in two steps.",
        "In the first step, we consider three baseline models, two taken from [4] (the Models 1 and 2 mentioned above), and one a refined version of a model introduced in [3] (which we refer to as Model 3 below); this third model is also similar to the model described by Petkova and Croft [19].",
        "The models we consider in our second round of experiments are mixture models similar to contextual language models [1] and to the expanded documents of Tao et al. [21]; however, the features that we use for definining our expansions-including topical structure and organizational structure-have not been used in this way before. 3.",
        "TASKS In the expertise retrieval scenario that we envisage, users seeking expertise within an organization have access to an interface that combines a search box (where they can search for experts or topics) with navigational structures (of experts and of topics) that allows them to click their way to an expert page (providing the profile of a person) or a topic page (providing a list of experts on the topic).",
        "To feed the above interface, we face two expertise retrieval tasks, expert finding and expert profiling, that we first define and then formalize using generative language models.",
        "In order to model either task, the probability of the query topic being associated to a candidate expert plays a key role in the final estimates for searching and profiling.",
        "By using language models, both the candidates and the query are characterized by distributions of terms in the vocabulary (used in the documents made available by the organization whose expertise retrieval needs we are addressing). 3.1 Expert finding Expert finding involves the task of finding the right person with the appropriate skills and knowledge: Who are the experts on topic X?.",
        "E.g., an employee wants to ascertain who worked on a particular project to find out why particular decisions were made without having to trawl through documentation (if there is any).",
        "Or, they may be in need a trained specialist for consultancy on a specific problem.",
        "Within an organization there are usually many possible candidates who could be experts for given topic.",
        "We can state this problem as follows: What is the probability of a candidate ca being an expert given the query topic q?",
        "That is, we determine p(ca|q), and rank candidates ca according to this probability.",
        "The candidates with the highest probability given the query are deemed the most likely experts for that topic.",
        "The challenge is how to estimate this probability accurately.",
        "Since the query is likely to consist of only a few terms to describe the expertise required, we should be able to obtain a more accurate estimate by invoking Bayes Theorem, and estimating: p(ca|q) = p(q|ca)p(ca) p(q) , (1) where p(ca) is the probability of a candidate and p(q) is the probability of a query.",
        "Since p(q) is a constant, it can be ignored for ranking purposes.",
        "Thus, the probability of a candidate ca being an expert given the query q is proportional to the probability of a query given the candidate p(q|ca), weighted by the a priori belief p(ca) that candidate ca is an expert. p(ca|q) ∝ p(q|ca)p(ca) (2) In this paper our main focus is on estimating the probability of a query given the candidate p(q|ca), because this probability captures the extent to which the candidate knows about the query topic.",
        "Whereas the candidate priors are generally assumed to be uniformand thus will not influence the ranking-it has been demonstrated that a sensible choice of priors may improve the performance [20]. 3.2 Expert profiling While the task of expert searching was concerned with finding experts given a particular topic, the task of expert profiling seeks to answer a related question: What topics does a candidate know about?",
        "Essentially, this turns the questions of expert finding around.",
        "The profiling of an individual candidate involves the identification of areas of skills and knowledge that they have expertise about and an evaluation of the level of proficiency in each of these areas.",
        "This is the candidates topical profile.",
        "Generally, topical profiles within organizations consist of tabular structures which explicitly catalogue the skills and knowledge of each individual in the organization.",
        "However, such practice is limited by the resources available for defining, creating, maintaining, and updating these profiles over time.",
        "By focusing on automatic methods which draw upon the available evidence within the document repositories of an organization, our aim is to reduce the human effort associated with the maintenance of topical profiles1 .",
        "A topical profile of a candidate, then, is defined as a vector where each element i of the vector corresponds to the candidate cas expertise on a given topic ki, (i.e., s(ca, ki)).",
        "Each topic ki defines a particular knowledge area or skill that the organization uses to define the candidates topical profile.",
        "Thus, it is assumed that a list of topics, {k1, . . . , kn}, where n is the number of pre-defined topics, is given: profile(ca) = s(ca, k1), s(ca, k2), . . . , s(ca, kn) . (3) 1 Context and evidence are needed to help users of expertise finding systems to decide whom to contact when seeking expertise in a particular area.",
        "Examples of such context are: Who does she work with?",
        "What are her contact details?",
        "Is she well-connected, just in case she is not able to help us herself?",
        "What is her role in the organization?",
        "Who is her superior?",
        "Collaborators, and affiliations, etc. are all part of the candidates social profile, and can serve as a background against which the systems recommendations should be interpreted.",
        "In this paper we only address the problem of determining topical profiles, and leave social profiling to further work.",
        "We state the problem of quantifying the competence of a person on a certain knowledge area as follows: What is the probability of a knowledge area (ki) being part of the candidates (expertise) profile? where s(ca, ki) is defined by p(ki|ca).",
        "Our task, then, is to estimate p(ki|ca), which is equivalent to the problem of obtaining p(q|ca), where the topic ki is represented as a query topic q, i.e., a sequence of keywords representing the expertise required.",
        "Both the expert finding and profiling tasks rely on the accurate estimation of p(q|ca).",
        "The only difference derives from the prior probability that a person is an expert (p(ca)), which can be incorporated into the expert finding task.",
        "This prior does not apply to the profiling task since the candidate (individual) is fixed. 4.",
        "BASELINE MODELS In this section we describe our baseline models for estimating p(q|ca), i.e., associations between topics and people.",
        "Both expert finding and expert profiling boil down to this estimation.",
        "We employ three models for calculating this probability. 4.1 From topics to candidates Using Candidate Models: Model 1 Model 1 [4] defines the probability of a query given a candidate (p(q|ca)) using standard language modeling techniques, based on a multinomial unigram language model.",
        "For each candidate ca, a candidate language model θca is inferred such that the probability of a term given θca is nonzero for all terms, i.e., p(t|θca) > 0.",
        "From the candidate model the query is generated with the following probability: p(q|θca) = Y t∈q p(t|θca)n(t,q) , where each term t in the query q is sampled identically and independently, and n(t, q) is the number of times t occurs in q.",
        "The candidate language model is inferred as follows: (1) an empirical model p(t|ca) is computed; (2) it is smoothed with background probabilities.",
        "Using the associations between a candidate and a document, the probability p(t|ca) can be approximated by: p(t|ca) = X d p(t|d)p(d|ca), where p(d|ca) is the probability that candidate ca generates a supporting document d, and p(t|d) is the probability of a term t occurring in the document d. We use the maximum-likelihood estimate of a term, that is, the normalised frequency of the term t in document d. The strength of the association between document d and candidate ca expressed by p(d|ca) reflects the degree to which the candidates expertise is described using this document.",
        "The estimation of this probability is presented later, in Section 4.2.",
        "The candidate model is then constructed as a linear interpolation of p(t|ca) and the background model p(t) to ensure there are no zero probabilities, which results in the final estimation: p(q|θca) = (4) Y t∈q ( (1 − λ) X d p(t|d)p(d|ca) ! + λp(t) )n(t,q) .",
        "Model 1 amasses all the term information from all the documents associated with the candidate, and uses this to represent that candidate.",
        "This model is used to predict how likely a candidate would produce a query q.",
        "This can can be intuitively interpreted as the probability of this candidate talking about the query topic, where we assume that this is indicative of their expertise.",
        "Using Document Models: Model 2 Model 2 [4] takes a different approach.",
        "Here, the process is broken into two parts.",
        "Given a candidate ca, (1) a document that is associated with a candidate is selected with probability p(d|ca), and (2) from this document a query q is generated with probability p(q|d).",
        "Then the sum over all documents is taken to obtain p(q|ca), such that: p(q|ca) = X d p(q|d)p(d|ca). (5) The probability of a query given a document is estimated by inferring a document language model θd for each document d in a similar manner as the candidate model was inferred: p(t|θd) = (1 − λ)p(t|d) + λp(t), (6) where p(t|d) is the probability of the term in the document.",
        "The probability of a query given the document model is: p(q|θd) = Y t∈q p(t|θd)n(t,q) .",
        "The final estimate of p(q|ca) is obtained by substituting p(q|d) for p(q|θd) into Eq. 5 (see [4] for full details).",
        "Conceptually, Model 2 differs from Model 1 because the candidate is not directly modeled.",
        "Instead, the document acts like a hidden variable in the process which separates the query from the candidate.",
        "This process is akin to how a user may search for candidates with a standard search engine: initially by finding the documents which are relevant, and then seeing who is associated with that document.",
        "By examining a number of documents the user can obtain an idea of which candidates are more likely to discuss the topic q.",
        "Using Topic Models: Model 3 We introduce a third model, Model 3.",
        "Instead of attempting to model the query generation process via candidate or document models, we represent the query as a topic language model and directly estimate the probability of the candidate p(ca|q).",
        "This approach is similar to the model presented in [3, 19].",
        "As with the previous models, a language model is inferred, but this time for the query.",
        "We adapt the work of Lavrenko and Croft [14] to estimate a topic model from the query.",
        "The procedure is as follows.",
        "Given a collection of documents and a query topic q, it is assumed that there exists an unknown topic model θk that assigns probabilities p(t|θk) to the term occurrences in the topic documents.",
        "Both the query and the documents are samples from θk (as opposed to the previous approaches, where a query is assumed to be sampled from a specific document or candidate model).",
        "The main task is to estimate p(t|θk), the probability of a term given the topic model.",
        "Since the query q is very sparse, and as there are no examples of documents on the topic, this distribution needs to be approximated.",
        "Lavrenko and Croft [14] suggest a reasonable way of obtaining such an approximation, by assuming that p(t|θk) can be approximated by the probability of term t given the query q.",
        "We can then estimate p(t|q) using the joint probability of observing the term t together with the query terms, q1, . . . , qm, and dividing by the joint probability of the query terms: p(t|θk) ≈ p(t|q) = p(t, q1, . . . , qm) p(q1, . . . , qm) = p(t, q1, . . . , qm) P t ∈T p(t , q1, . . . , qm) , where p(q1, . . . , qm) = P t ∈T p(t , q1, . . . , qm), and T is the entire vocabulary of terms.",
        "In order to estimate the joint probability p(t, q1, . . . , qm), we follow [14, 15] and assume t and q1, . . . , qm are mutually independent, once we pick a source distribution from the set of underlying source distributions U.",
        "If we choose U to be a set of document models. then to construct this set, the query q would be issued against the collection, and the top n returned are assumed to be relevant to the topic, and thus treated as samples from the topic model. (Note that candidate models could be used instead.)",
        "With the document models forming U, the joint probability of term and query becomes: p(t, q1, . . . , qm) = X d∈U p(d) ˘ p(t|θd) mY i=1 p(qi|θd) ¯ . (7) Here, p(d) denotes the prior distribution over the set U, which reflects the relevance of the document to the topic.",
        "We assume that p(d) is uniform across U.",
        "In order to rank candidates according to the topic model defined, we use the Kullback-Leibler divergence metric (KL, [8]) to measure the difference between the candidate models and the topic model: KL(θk||θca) = X t p(t|θk) log p(t|θk) p(t|θca) . (8) Candidates with a smaller divergence from the topic model are considered to be more likely experts on that topic.",
        "The candidate model θca is defined in Eq. 4.",
        "By using KL divergence instead of the probability of a candidate given the topic model p(ca|θk), we avoid normalization problems. 4.2 Document-candidate associations For our models we need to be able to estimate the probability p(d|ca), which expresses the extent to which a document d characterizes the candidate ca.",
        "In [4], two methods are presented for estimating this probability, based on the number of person names recognized in a document.",
        "However, in our (intranet) setting it is reasonable to assume that authors of documents can unambiguously be identified (e.g., as the author of an article, the teacher assigned to a course, the owner of a web page, etc.)",
        "Hence, we set p(d|ca) to be 1 if candidate ca is author of document d, otherwise the probability is 0.",
        "In Section 6 we describe how authorship can be determined on different types of documents within the collection. 5.",
        "THE UVT EXPERT COLLECTION The UvT Expert collection used in the experiments in this paper fits the scenario outlined in Section 3.",
        "The collection is based on the Webwijs (Webwise) system developed at Tilburg University (UvT) in the Netherlands.",
        "Webwijs (http://www.uvt.nl/ webwijs/) is a publicly accessible database of UvT employees who are involved in research or teaching; currently, Webwijs contains information about 1168 experts, each of whom has a page with contact information and, if made available by the expert, a research description and publications list.",
        "In addition, each expert can select expertise areas from a list of 1491 topics and is encouraged to suggest new topics that need to be approved by the Webwijs editor.",
        "Each topic has a separate page that shows all experts associated with that topic and, if available, a list of related topics.",
        "Webwijs is available in Dutch and English, and this bilinguality has been preserved in the collection.",
        "Every Dutch Webwijs page has an English translation.",
        "Not all Dutch topics have an English translation, but the reverse is true: the 981 English topics all have a Dutch equivalent.",
        "About 42% of the experts teach courses at Tilburg University; these courses were also crawled and included in the profile.",
        "In addition, about 27% of the experts link to their academic homepage from their Webwijs page.",
        "These home pages were crawled and added to the collection. (This means that if experts put the full-text versions of their publications on their academic homepage, these were also available for indexing.)",
        "We also obtained 1880 full-text versions of publications from the UvT institutional repository and Dutch English no. of experts 1168 1168 no. of experts with ≥ 1 topic 743 727 no. of topics 1491 981 no. of expert-topic pairs 4318 3251 avg. no. of topics/expert 5.8 5.9 max. no. of topics/expert (no. of experts) 60 (1) 35 (1) min. no. of topics/expert (no. of experts) 1 (74) 1 (106) avg. no. of experts/topic 2.9 3.3 max. no. of experts/topic (no. of topics) 30 (1) 30 (1) min. no. of experts/topic (no. of topics) 1 (615) 1 (346) no. of experts with HP 318 318 no. of experts with CD 318 318 avg. no. of CDs per teaching expert 3.5 3.5 no. of experts with RD 329 313 no. of experts with PUB 734 734 avg. no. of PUBs per expert 27.0 27.0 avg. no. of PUB citations per expert 25.2 25.2 avg. no. of full-text PUBs per expert 1.8 1.8 Table 2: Descriptive statistics of the Dutch and English versions of the UvT Expert collection. converted them to plain text.",
        "We ran the TextCat [23] language identifier to classify the language of the home pages and the fulltext publications.",
        "We restricted ourselves to pages where the classifier was confident about the language used on the page.",
        "This resulted in four document types: research descriptions (RD), course descriptions (CD), publications (PUB; full-text and citationonly versions), and academic homepages (HP).",
        "Everything was bundled into the UvT Expert collection which is available at http: //ilk.uvt.nl/uvt-expert-collection/.",
        "The UvT Expert collection was extracted from a different organizational setting than the W3C collection and differs from it in a number of ways.",
        "The UvT setting is one with relatively small amounts of multilingual data.",
        "Document-author associations are clear and the data is structured and clean.",
        "The collection covers a broad range of expertise areas, as one can typically find on intranets of universities and other knowledge-intensive institutes.",
        "Additionally, our university setting features several types of structure (topical and organizational), as well as multiple document types.",
        "Another important difference between the two data sets is that the expertise areas in the UvT Expert collection are self-selected instead of being based on group membership or assignments by others.",
        "Size is another dimension along which the W3C and UvT Expert collections differ: the latter is the smaller of the two.",
        "Also realistic are the large differences in the amount of information available for each expert.",
        "Utilizing Webwijs is voluntary; 425 Dutch experts did not select any topics at all.",
        "This leaves us with 743 Dutch and 727 English usable expert profiles.",
        "Table 2 provides descriptive statistics for the UvT Expert collection.",
        "Universities tend to have a hierarchical structure that goes from the faculty level, to departments, research groups, down to the individual researchers.",
        "In the UvT Expert collection we have information about the affiliations of researchers with faculties and institutes, providing us with a two-level organizational hierarchy.",
        "Tilburg University has 22 organizational units at the faculty level (including the university office and several research institutes) and 71 departments, which amounts to 3.2 departments per faculty.",
        "As to the topical hierarchy used by Webwijs, 131 of the 1491 topics are top nodes in the hierarchy.",
        "This hierarchy has an average topic chain length of 2.65 and a maximum length of 7 topics. 6.",
        "EVALUATION Below, we evaluate Section 4s models for expert finding and profiling onthe UvT Expert collection.",
        "We detail our research questions and experimental setup, and then present our results. 6.1 Research Questions We address the following research questions.",
        "Both expert finding and profiling rely on the estimations of p(q|ca).",
        "The question is how the models compare on the different tasks, and in the setting of the UvT Expert collection.",
        "In [4], Model 2 outperformed Model 1 on the W3C collection.",
        "How do they compare on our data set?",
        "And how does Model 3 compare to Model 1?",
        "What about performance differences between the two languages in our test collection? 6.2 Experimental Setup The output of our models was evaluated against the self-assigned topic labels, which were treated as relevance judgements.",
        "Results were evaluated separately for English and Dutch.",
        "For English we only used topics for which the Dutch translation was available; for Dutch all topics were considered.",
        "The results were averaged for the queries in the intersection of relevance judgements and results; missing queries do not contribute a value of 0 to the scores.",
        "We use standard information retrieval measures, such as Mean Average Precision (MAP) and Mean Reciprocal Rank (MRR).",
        "We also report the percentage of topics (%q) and candidates (%ca) covered, for the expert finding and profiling tasks, respectively. 6.3 Results Table 1 shows the performance of Model 1, 2, and 3 on the expert finding and profiling tasks.",
        "The rows of the table correspond to the various document types (RD, CD, PUB, and HP) and to their combinations.",
        "RD+CD+PUB+HP is equivalent to the full collection and will be referred as the BASELINE of our experiments.",
        "Looking at Table 1 we see that Model 2 performs the best across the board.",
        "However, when the data is clean and very focused (RD), Model 3 outperforms it in a number of cases.",
        "Model 1 has the best coverage of candidates (%ca) and topics (%q).",
        "The various document types differ in their characteristics and how they improve the finding and profiling tasks.",
        "Expert profiling benefits much from the clean data present in the RD and CD document types, while the publications contribute the most to the expert finding task.",
        "Adding the homepages does not prove to be particularly useful.",
        "When we compare the results across languages, we find that the coverage of English topics (%q) is higher than of the Dutch ones for expert finding.",
        "Apart from that, the scores fall in the same range for both languages.",
        "For the profiling task the coverage of the candidates (%ca) is very similar for both languages.",
        "However, the performance is substantially better for the English topics.",
        "While it is hard to compare scores across collections, we conclude with a brief comparison of the absolute scores in Table 1 to those reported in [3, 4] on the W3C test set (2005 edition).",
        "For expert finding the MAP scores for Model 2 reported here are about 50% higher than the corresponding figures in [4], while our MRR scores are slightly below those in [4].",
        "For expert profiling, the differences are far more dramatic: the MAP scores for Model 2 reported here are around 50% below the scores in [3], while the (best) MRR scores are about the same as those in [3].",
        "The cause for the latter differences seems to reside in the number of knowledge areas considered here-approx. 30 times more than in the W3C setting. 7.",
        "ADVANCED MODELS Now that we have developed and assessed basic language modeling techniques for expertise retrieval, we turn to refined models that exploit special features of our test collection. 7.1 Exploiting knowledge area similarity One way to improve the scoring of a query given a candidate is to consider what other requests the candidate would satisfy and use them as further evidence to support the original query, proportional Expert finding Expert profiling Document types Model 1 Model 2 Model 3 Model 1 Model 2 Model 3 %q MAP MRR %q MAP MRR %q MAP MRR %ca MAP MRR %ca MAP MRR %ca MAP MRR English RD 97.8 0.126 0.269 83.5 0.144 0.311 83.3 0.129 0.271 100 0.089 0.189 39.3 0.232 0.465 41.1 0.166 0.337 CD 97.8 0.118 0.227 91.7 0.123 0.248 91.7 0.118 0.226 32.8 0.188 0.381 32.4 0.195 0.385 32.7 0.203 0.370 PUB 97.8 0.200 0.330 98.0 0.216 0.372 98.0 0.145 0.257 78.9 0.167 0.364 74.5 0.212 0.442 78.9 0.135 0.299 HP 97.8 0.081 0.186 97.4 0.071 0.168 97.2 0.062 0.149 31.2 0.150 0.299 28.8 0.185 0.335 30.1 0.136 0.287 RD+CD 97.8 0.188 0.352 92.9 0.193 0.360 92.9 0.150 0.273 100 0.145 0.286 61.3 0.251 0.477 63.2 0.217 0.416 RD+CD+PUB 97.8 0.235 0.373 98.1 0.277 0.439 98.1 0.178 0.305 100 0.196 0.380 87.2 0.280 0.533 89.5 0.170 0.344 RD+CD+PUB+HP 97.8 0.237 0.372 98.6 0.280 0.441 98.5 0.166 0.293 100 0.199 0.387 88.7 0.281 0.525 90.9 0.169 0.329 Dutch RD 61.3 0.094 0.229 38.4 0.137 0.336 38.3 0.127 0.295 38.0 0.127 0.386 34.1 0.138 0.420 38.0 0.105 0.327 CD 61.3 0.107 0.212 49.7 0.128 0.256 49.7 0.136 0.261 32.5 0.151 0.389 31.8 0.158 0.396 32.5 0.170 0.380 PUB 61.3 0.193 0.319 59.5 0.218 0.368 59.4 0.173 0.291 78.8 0.126 0.364 76.0 0.150 0.424 78.8 0.103 0.294 HP 61.3 0.063 0.169 56.6 0.064 0.175 56.4 0.062 0.163 29.8 0.108 0.308 27.8 0.125 0.338 29.8 0.098 0.255 RD+CD 61.3 0.159 0.314 51.9 0.184 0.360 51.9 0.169 0.324 60.5 0.151 0.410 57.2 0.166 0.431 60.4 0.159 0.384 RD+CD+PUB 61.3 0.244 0.398 61.5 0.260 0.424 61.4 0.210 0.350 90.3 0.165 0.445 88.2 0.189 0.479 90.3 0.126 0.339 RD+CD+PUB+HP 61.3 0.249 0.401 62.6 0.265 0.436 62.6 0.195 0.344 91.9 0.164 0.426 90.1 0.195 0.488 91.9 0.125 0.328 Table 1: Performance of the models on the expert finding and profiling tasks, using different document types and their combinations. %q is the number of topics covered (applies to the expert finding task), %ca is the number of candidates covered (applies to the expert profiling task).",
        "The top and bottom blocks correspond to English and Dutch respectively.",
        "The best scores are in boldface. to how related the other requests are to the original query.",
        "This can be modeled by interpolating between the p(q|ca) and the further supporting evidence from all similar requests q , as follows: p (q|ca) = λp(q|ca) + (1 − λ) X q p(q|q )p(q |ca), (9) where p(q|q ) represents the similarity between the two topics q and q .",
        "To be able to work with similarity methods that are not necessarily probabilities, we set p(q|q ) = w(q,q ) γ , where γ is a normalizing constant, such that γ = P q w(q , q ).",
        "We consider four methods for calculating the similarity score between two topics.",
        "Three approaches are strictly content-based, and establish similarity by examining co-occurrence patterns of topics within the collection, while the last approach exploits the hierarchical structure of topical areas that may be present within an organization (see [7] for further examples of integrating word relationships into language models).",
        "The Kullback-Leibler (KL) divergence metric defined in Eq. 8 provides a measure of how different or similar two probability distributions are.",
        "A topic model is inferred for q and q using the method presented in Section 4.1 to describe the query across the entire vocabulary.",
        "Since a lower KL score means the queries are more similar, we let w(q, q ) = max(KL(θq||·) − KL(θq||θq )).",
        "Pointwise Mutual Information (PMI, [17]) is a measure of association used in information theory to determine the extent of independence between variables.",
        "The dependence between two queries is reflected by the SI(q, q ) score, where scores greater than zero indicate that it is likely that there is a dependence, which we take to mean that the queries are likely to be similar: SI(q, q ) = log p(q, q ) p(q)p(q ) (10) We estimate the probability of a topic p(q) using the number of documents relevant to query q within the collection.",
        "The joint probability p(q, q ) is estimated similarly, by using the concatenation of q and q as a query.",
        "To obtain p(q|q ), we then set w(q, q ) = SI(q, q ) when SI(q, q ) > 0 otherwise w(q, q ) = 0, because we are only interested in including queries that are similar.",
        "The log-likelihood statistic provides another measure of dependence, which is more reliable than the pointwise mutual information measure [17].",
        "Let k1 be the number of co-occurrences of q and q , k2 the number of occurrences of q not co-occurring with q , n1 the total number of occurrences of q , and n2 the total number of topic tokens minus the number of occurrences of q .",
        "Then, let p1 = k1/n1, p2 = k2/n2, and p = (k1 + k2)/(n1 + n2), (q, q ) = 2( (p1, k1, n1) + (p2, k2, n2) − (p, k1, n1) − (p, k2, n2)), where (p, n, k) = k log p + (n − k) log(1 − p).",
        "The higher score indicate that queries are also likely to be similar, thus we set w(q, q ) = (q, q ).",
        "Finally, we also estimate the similarity of two topics based on their distance within the topic hierarchy.",
        "The topic hierarchy is viewed as a directed graph, and for all topic-pairs the shortest path SP(q, q ) is calculated.",
        "We set the similarity score to be the reciprocal of the shortest path: w(q, q ) = 1/SP(q, q ). 7.2 Contextual information Given the hierarchy of an organization, the units to which a person belong are regarded as a context so as to compensate for data sparseness.",
        "We model it as follows: p (q|ca) = 1 − P ou∈OU(ca) λou · p(q|ca) + P ou∈OU(ca) λou · p(q|ou), where OU(ca) is the set of organizational units of which candidate ca is a member of, and p(q|o) expresses the strength of the association between query q and the unit ou.",
        "The latter probability can be estimated using either of the three basic models, by simply replacing ca with ou in the corresponding equations.",
        "An organizational unit is associated with all the documents that its members have authored.",
        "That is, p(d|ou) = maxca∈ou p(d|ca). 7.3 A simple multilingual model For knowledge institutes in Europe, academic or otherwise, a multilingual (or at least bilingual) setting is typical.",
        "The following model builds on a kind of independence assumption: there is no spill-over of expertise/profiles across language boundaries.",
        "While a simplification, this is a sensible first approach.",
        "That is: p (q|ca) =P l∈L λl · p(ql|ca), where L is the set of languages used in the collection, ql is the translation of the query q to language l, and λl is a language specific smoothing parameter, such that P l∈L λl = 1. 8.",
        "ADVANCED MODELS: EVALUATION In this section we present an experimental evaluation of our advanced models.",
        "Expert finding Expert profiling Language Model 1 Model 2 Model 3 Model 1 Model 2 Model 3 %q MAP MRR %q MAP MRR %q MAP MRR %ca MAP MRR %ca MAP MRR %ca MAP MRR English only 97.8 0.237 0.372 98.6 0.280 0.441 98.5 0.166 0.293 100 0.199 0.387 88.7 0.281 0.525 90.9 0.169 0.329 Dutch only 61.3 0.249 0.401 62.6 0.265 0.436 62.6 0.195 0.344 91.9 0.164 0.426 90.1 0.195 0.488 91.9 0.125 0.328 Combination 99.4 0.297 0.444 99.7 0.324 0.491 99.7 0.223 0.388 100 0.241 0.445 92.1 0.313 0.564 93.2 0.224 0.411 Table 3: Performance of the combination of languages on the expert finding and profiling tasks (on candidates).",
        "Best scores for each model are in italic, absolute best scores for the expert finding and profiling tasks are in boldface.",
        "Method Model 1 Model 2 Model 3 MAP MRR MAP MRR MAP MRR English BASELINE 0.296 0.454 0.339 0.509 0.221 0.333 KLDIV 0.291 0.453 0.327 0.503 0.219 0.330 PMI 0.291 0.453 0.337 0.509 0.219 0.331 LL 0.319 0.490 0.360 0.524 0.233 0.368 HDIST 0.299 0.465 0.346 0.537 0.219 0.332 Dutch BASELINE 0.240 0.350 0.271 0.403 0.227 0.389 KLDIV 0.239 0.347 0.253 0.386 0.224 0.385 PMI 0.239 0.350 0.260 0.392 0.227 0.389 LL 0.255 0.372 0.281 0.425 0.231 0.389 HDIST 0.253 0.365 0.271 0.407 0.236 0.402 Method Model 1 Model 2 Model 3 MAP MRR MAP MRR MAP MRR English BASELINE 0.485 0.546 0.499 0.548 0.381 0.416 KLDIV 0.510 0.564 0.513 0.558 0.381 0.416 PMI 0.486 0.546 0.495 0.542 0.407 0.451 LL 0.558 0.589 0.586 0.617 0.408 0.453 HDIST 0.507 0.567 0.512 0.563 0.386 0.420 Dutch BASELINE 0.263 0.313 0.294 0.358 0.262 0.315 KLDIV 0.284 0.336 0.271 0.321 0.261 0.314 PMI 0.265 0.317 0.265 0.316 0.273 0.330 LL 0.312 0.351 0.330 0.377 0.284 0.331 HDIST 0.280 0.327 0.288 0.341 0.266 0.321 Table 4: Performance on the expert finding (top) and profiling (bottom) tasks, using knowledge area similarities.",
        "Runs were evaluated on the main topics set.",
        "Best scores are in boldface. 8.1 Research Questions Our questions follow the refinements presented in the preceding section: Does exploiting the knowledge area similarity improve effectiveness?",
        "Which of the various methods for capturing word relationships is most effective?",
        "Furthermore, is our way of bringing in contextual information useful?",
        "For which tasks?",
        "And finally, is our simple way of combining the monolingual scores sufficient for obtaining significant improvements? 8.2 Experimental setup Given that the self-assessments are also sparse in our collection, in order to be able to measure differences between the various models, we selected a subset of topics, and evaluated (some of the) runs only on this subset.",
        "This set is referred as main topics, and consists of topics that are located at the top level of the topical hierarchy. (A main topic has subtopics, but is not a subtopic of any other topic.)",
        "This main set consists of 132 Dutch and 119 English topics.",
        "The relevance judgements were restricted to the main topic set, but were not expanded with subtopics. 8.3 Exploiting knowledge area similarity Table 4 presents the results.",
        "The four methods used for estimating knowledge-area similarity are KL divergence (KLDIV), PointLang.",
        "Topics Model 1 Model 2 Model 3 MAP MRR MAP MRR MAP MRR Expert finding UK ALL 0.423 0.545 0.654 0.799 0.494 0.629 UK MAIN 0.500 0.621 0.704 0.834 0.587 0.699 NL ALL 0.439 0.560 0.672 0.826 0.480 0.630 NL MAIN 0.440 0.584 0.645 0.816 0.515 0.655 Expert profiling UK ALL 0.240 0.640 0.306 0.778 0.223 0.616 UK MAIN 0.523 0.677 0.519 0.648 0.461 0.587 NL ALL 0.203 0.716 0.254 0.770 0.183 0.627 NL MAIN 0.332 0.576 0.380 0.624 0.332 0.549 Table 5: Evaluating the context models on organizational units. wise mutual information (PMI), log-likelihood (LL), and distance within topic hierarchy (HDIST).",
        "We managed to improve upon the baseline in all cases, but the improvement is more noticeable for the profiling task.",
        "For both tasks, the LL method performed best.",
        "The content-based approaches performed consistently better than HDIST. 8.4 Contextual information A two level hierarchy of organizational units (faculties and institutes) is available in the UvT Expert collection.",
        "The unit a person belongs to is used as a context for that person.",
        "First, we evaluated the models of the organizational units, using all topics (ALL) and only the main topics (MAIN).",
        "An organizational unit is considered to be relevant for a given topic (or vice versa) if at least one member of the unit selected the given topic as an expertise area.",
        "Table 5 reports on the results.",
        "As far as expert finding goes, given a topic, the corresponding organizational unit can be identified with high precision.",
        "However, the expert profiling task shows a different picture: the scores are low, and the task seems hard.",
        "The explanation may be that general concepts (i.e., our main topics) may belong to several organizational units.",
        "Second, we performed another evaluation, where we combined the contextual models with the candidate models (to score candidates again).",
        "Table 6 reports on the results.",
        "We find a positive impact of the context models only for expert finding.",
        "Noticably, for expert finding (and Model 1), it improves over 50% (for English) and over 70% (for Dutch) on MAP.",
        "The poor performance on expert profiling may be due to the fact that context models alone did not perform very well on the profiling task to begin with. 8.5 Multilingual models In this subsection we evaluate the method for combining results across multiple languages that we described in Section 7.3.",
        "In our setting the set of languages consists of English and Dutch: L = {UK, NL}.",
        "The weights on these languages were set to be identical (λUK = λNL = 0.5).",
        "We performed experiments with various λ settings, but did not observe significant differences in performance.",
        "Table 3 reports on the multilingual results, where performance is evaluated on the full topic set.",
        "All three models significantly imLang.",
        "Method Model 1 Model 2 Model 3 MAP MRR MAP MRR MAP MRR Expert finding UK BL 0.296 0.454 0.339 0.509 0.221 0.333 UK CT 0.330 0.491 0.342 0.500 0.228 0.342 NL BL 0.240 0.350 0.271 0.403 0.227 0.389 NL CT 0.251 0.382 0.267 0.410 0.246 0.404 Expert profiling UK BL 0.485 0.546 0.499 0.548 0.381 0.416 UK CT 0.562 0.620 0.508 0.558 0.440 0.486 NL BL 0.263 0.313 0.294 0.358 0.262 0.315 NL CT 0.330 0.384 0.317 0.387 0.294 0.345 Table 6: Performance of the context models (CT) compared to the baseline (BL).",
        "Best scores are in boldface. proved over all measures for both tasks.",
        "The coverage of topics and candidates for the expert finding and profiling tasks, respectively, is close to 100% in all cases.",
        "The relative improvement of the precision scores ranges from 10% to 80%.",
        "These scores demonstrate that despite its simplicity, our method for combining results over multiple languages achieves substantial improvements over the baseline. 9.",
        "CONCLUSIONS In this paper we focused on expertise retrieval (expert finding and profiling) in a new setting of a typical knowledge-intensive organization in which the available data is of high quality, multilingual, and covering a broad range of expertise area.",
        "Typically, the amount of available data in such an organization (e.g., a university, a research institute, or a research lab) is limited when compared to the W3C collection that has mostly been used for the experimental evaluation of expertise retrieval so far.",
        "To examine expertise retrieval in this setting, we introduced (and released) the UvT Expert collection as a representative case of such knowledge intensive organizations.",
        "The new collection reflects the typical properties of knowledge-intensive institutes noted above and also includes several features which may are potentially useful for expertise retrieval, such as topical and organizational structure.",
        "We evaluated how current state-of-the-art models for expert finding and profiling performed in this new setting and then refined these models in order to try and exploit the different characteristics within the data environment (language, topicality, and organizational structure).",
        "We found that current models of expertise retrieval generalize well to this new environment; in addition we found that refining the models to account for the differences results in significant improvements, thus making up for problems caused by data sparseness issues.",
        "Future work includes setting up manual assessments of automatically generated profiles by the employees themselves, especially in cases where the employees have not provided a profile themselves. 10.",
        "ACKNOWLEDGMENTS Krisztian Balog was supported by the Netherlands Organisation for Scientific Research (NWO) under project number 220-80-001.",
        "Maarten de Rijke was also supported by NWO under project numbers 017.001.190, 220-80-001, 264-70-050, 354-20-005, 600.065.120, 612-13-001, 612.000.106, 612.066.302, 612.069.006, 640.001.501, 640.002.501, and by the E.U.",
        "IST programme of the 6th FP for RTD under project MultiMATCH contract IST-033104.",
        "The work of Toine Bogers and Antal van den Bosch was funded by the IOP-MMI-program of SenterNovem / The Dutch Ministry of Economic Affairs, as part of the `A Propos project. 11.",
        "REFERENCES [1] L. Azzopardi.",
        "Incorporating Context in the Language Modeling Framework for ad-hoc Information Retrieval.",
        "PhD thesis, University of Paisley, 2005. [2] K. Balog and M. de Rijke.",
        "Finding similar experts.",
        "In This volume, 2007. [3] K. Balog and M. de Rijke.",
        "Determining expert profiles (with an application to expert finding).",
        "In IJCAI 07: Proc. 20th Intern.",
        "Joint Conf. on Artificial Intelligence, pages 2657-2662, 2007. [4] K. Balog, L. Azzopardi, and M. de Rijke.",
        "Formal models for expert finding in enterprise corpora.",
        "In SIGIR 06: Proc. 29th annual intern.",
        "ACM SIGIR conf. on Research and development in information retrieval, pages 43-50, 2006. [5] I. Becerra-Fernandez.",
        "The role of artificial intelligence technologies in the implementation of people-finder knowledge management systems.",
        "In AAAI Workshop on Bringing Knowledge to Business Processes, March 2000. [6] C. S. Campbell, P. P. Maglio, A. Cozzi, and B. Dom.",
        "Expertise identification using email communications.",
        "In CIKM 03: Proc. twelfth intern. conf. on Information and knowledge management, pages 528531, 2003. [7] G. Cao, J.-Y.",
        "Nie, and J. Bai.",
        "Integrating word relationships into language models.",
        "In SIGIR 05: Proc. 28th annual intern.",
        "ACM SIGIR conf. on Research and development in information retrieval, pages 298-305, 2005. [8] T. M. Cover and J.",
        "A. Thomas.",
        "Elements of Information Theory.",
        "Wiley-Interscience, 1991. [9] N. Craswell, D. Hawking, A. M. Vercoustre, and P. Wilkins.",
        "P@noptic expert: Searching for experts not just for documents.",
        "In Ausweb, 2001. [10] N. Craswell, A. de Vries, and I. Soboroff.",
        "Overview of the TREC2005 Enterprise Track.",
        "In The Fourteenth Text REtrieval Conf.",
        "Proc. (TREC 2005), 2006. [11] T. H. Davenport and L. Prusak.",
        "Working Knowledge: How Organizations Manage What They Know.",
        "Harvard Business School Press, Boston, MA, 1998. [12] T. Dunning.",
        "Accurate methods for the statistics of surprise and coincidence.",
        "Computational Linguistics, 19(1):61-74, 1993. [13] E. Filatova and J. Prager.",
        "Tell me what you do and Ill tell you what you are: Learning occupation-related activities for biographies.",
        "In HLT/EMNLP, 2005. [14] V. Lavrenko and W. B. Croft.",
        "Relevance based language models.",
        "In SIGIR 01: Proc. 24th annual intern.",
        "ACM SIGIR conf. on Research and development in information retrieval, pages 120-127, 2001. [15] V. Lavrenko, M. Choquette, and W. B. Croft.",
        "Cross-lingual relevance models.",
        "In SIGIR 02: Proc. 25th annual intern.",
        "ACM SIGIR conf. on Research and development in information retrieval, pages 175-182, 2002. [16] C. Macdonald and I. Ounis.",
        "Voting for candidates: adapting data fusion techniques for an expert search task.",
        "In CIKM 06: Proc. 15th ACM intern. conf. on Information and knowledge management, pages 387-396, 2006. [17] C. Manning and H. Sch¨utze.",
        "Foundations of Statistical Natural Language Processing.",
        "The MIT Press, 1999. [18] A. Mockus and J. D. Herbsleb.",
        "Expertise browser: a quantitative approach to identifying expertise.",
        "In ICSE 02: Proc. 24th Intern.",
        "Conf. on Software Engineering, pages 503-512, 2002. [19] D. Petkova and W. B. Croft.",
        "Hierarchical language models for expert finding in enterprise corpora.",
        "In Proc.",
        "ICTAI 2006, pages 599-608, 2006. [20] I. Soboroff, A. de Vries, and N. Craswell.",
        "Overview of the TREC 2006 Enterprise Track.",
        "In TREC 2006 Working Notes, 2006. [21] T. Tao, X. Wang, Q. Mei, and C. Zhai.",
        "Language model information retrieval with document expansion.",
        "In HLT-NAACL 2006, 2006. [22] TREC.",
        "Enterprise track, 2005.",
        "URL: http://www.ins.cwi. nl/projects/trec-ent/wiki/. [23] G. van Noord.",
        "TextCat Language Guesser.",
        "URL: http://www. let.rug.nl/˜vannoord/TextCat/. [24] W3C.",
        "The W3C test collection, 2005.",
        "URL: http://research. microsoft.com/users/nickcr/w3c-summary.html."
    ],
    "translated_text_sentences": [
        "Recuperación de amplia experiencia en entornos de datos dispersos Krisztian Balog ISLA, Universidad de Ámsterdam Kruislaan 403, 1098 SJ Ámsterdam, Países Bajos kbalog@science.uva.nl Toine Bogers ILK, Universidad de Tilburg P.O.",
        "Caja 90153, 5000 LE Tilburg, Países Bajos A.M.Bogers@uvt.nl Leif Azzopardi Dept. de Ciencias de la Computación Universidad de Glasgow, Glasgow, G12 8QQ leif@dcs.gla.ac.uk Maarten de Rijke ISLA, Universidad de Ámsterdam Kruislaan 403, 1098 SJ Ámsterdam, Países Bajos mdr@science.uva.nl Antal van den Bosch ILK, Universidad de Tilburg P.O.",
        "La recuperación de la experiencia ha sido ampliamente inexplorada en datos distintos a la colección de la W3C.",
        "Al mismo tiempo, muchas intranets de universidades y otras organizaciones intensivas en conocimiento ofrecen ejemplos de datos de expertos multilingües relativamente pequeños pero precisos, que abarcan amplias áreas de especialización.",
        "Primero presentamos dos tareas principales de recuperación de expertos, junto con un conjunto de enfoques de referencia basados en modelado del lenguaje generativo, con el objetivo de encontrar relaciones de experticia entre temas y personas.",
        "Para nuestra evaluación experimental, introducimos (y liberamos) un nuevo conjunto de pruebas basado en un rastreo de un sitio universitario.",
        "Usando este conjunto de pruebas, llevamos a cabo dos series de experimentos.",
        "El primero tiene como objetivo determinar la efectividad de los métodos de recuperación de conocimientos básicos aplicados al nuevo conjunto de pruebas.",
        "El segundo tiene como objetivo evaluar modelos refinados que exploten las características distintivas del nuevo conjunto de pruebas, como la estructura organizativa de la universidad y la estructura jerárquica de los temas en el conjunto de pruebas.",
        "Los modelos de recuperación de experiencia se muestran robustos con respecto a entornos más pequeños que la colección de la W3C, y las técnicas actuales parecen ser generalizables a otros contextos.",
        "Categorías y Descriptores de Asignaturas H.3 [Almacenamiento y Recuperación de Información]: H.3.1 Análisis de Contenido e Indexación; H.3.3 Búsqueda y Recuperación de Información; H.3.4 Sistemas y Software; H.4 [Aplicaciones de Sistemas de Información]: H.4.2 Tipos de Sistemas; H.4.m Términos Generales Misceláneos Algoritmos, Medición, Rendimiento, Experimentación 1.",
        "INTRODUCCIÓN La intranet de una organización proporciona un medio para intercambiar información entre empleados y facilitar colaboraciones entre ellos.",
        "Para lograr esto de manera eficiente y efectiva, es necesario proporcionar instalaciones de búsqueda que permitan a los empleados no solo acceder a documentos, sino también identificar a colegas expertos.",
        "En la pista de Empresas de TREC [22] se ha reconocido la necesidad de estudiar y comprender la recuperación de expertos a través de la introducción de tareas de Búsqueda de Expertos.",
        "El objetivo de la búsqueda de expertos es identificar una lista de personas que tienen conocimientos sobre un tema específico.",
        "Esta tarea suele abordarse descubriendo asociaciones entre personas y temas [10]; comúnmente, se asume que la co-ocurrencia del nombre de una persona con temas en el mismo contexto es evidencia de experiencia.",
        "Una tarea alternativa, que utiliza la misma idea de asociaciones entre personas y temas, es el perfilado de expertos, donde la tarea consiste en devolver una lista de temas sobre los que una persona tiene conocimiento [3].",
        "El lanzamiento de la tarea de Búsqueda de Expertos en TREC ha generado mucho interés en la recuperación de expertos, con un rápido progreso en términos de modelado, algoritmos y aspectos de evaluación.",
        "Sin embargo, casi todo el trabajo de búsqueda o perfilado de expertos realizado ha sido validado experimentalmente utilizando la colección de W3C [24] de la pista empresarial.",
        "Si bien esta colección es actualmente la única colección de pruebas disponible públicamente para tareas de recuperación de expertos, solo representa un tipo de intranet.",
        "Con solo una colección de pruebas no es posible generalizar conclusiones a otros escenarios realistas.",
        "En este artículo nos enfocamos en la recuperación de expertos en un entorno realista que difiere del entorno de la W3C, uno en el que hay cantidades relativamente pequeñas de datos limpios y multilingües disponibles, que abarcan una amplia gama de áreas de expertise, como se puede encontrar en las intranets de universidades y otras organizaciones intensivas en conocimiento.",
        "Normalmente, esta configuración incluye varios tipos adicionales de estructuras: estructura temática (por ejemplo, jerarquías de temas utilizadas por la organización), estructura organizativa (facultad, departamento, ...), así como múltiples tipos de documentos (descripciones de investigación y cursos, publicaciones y páginas web académicas).",
        "Este entorno es bastante diferente del entorno del W3C de maneras que podrían afectar el rendimiento de las tareas de recuperación de conocimientos especializados.",
        "Nos enfocamos en varias preguntas de investigación en este artículo: ¿La cantidad relativamente pequeña de datos disponibles en una intranet afecta la calidad de las asociaciones tema-persona que son fundamentales en los algoritmos de recuperación de expertos?",
        "¿Cómo se desempeñan los algoritmos de vanguardia desarrollados en el conjunto de datos del W3C en el escenario alternativo del tipo descrito anteriormente?",
        "Más en general, ¿se pueden aplicar las lecciones de la tarea de Búsqueda de Expertos en TREC a este contexto?",
        "¿Cómo afecta la inclusión o exclusión de diferentes documentos a las tareas de recuperación de expertos?",
        "Además, ¿cómo se puede utilizar la estructura temática y organizativa con fines de recuperación?",
        "Para responder a nuestras preguntas de investigación, primero presentamos un conjunto de enfoques de línea base, basados en modelado de lenguaje generativo, con el objetivo de encontrar asociaciones entre temas y personas.",
        "Esto nos permite formular las tareas de búsqueda de expertos y perfilado de expertos de manera uniforme, y tiene el beneficio adicional de permitirnos entender las relaciones entre las dos tareas.",
        "Para nuestra evaluación experimental, introducimos un nuevo conjunto de datos (la Colección de Expertos de UvT) que es representativo del tipo de intranet que describimos anteriormente.",
        "Nuestra colección se basa en datos de acceso público, extraídos del sitio web de la Universidad de Tilburg (UvT).",
        "Este tipo de datos es particularmente interesante, ya que (1) es limpio, heterogéneo, estructurado y enfocado, pero comprende un número limitado de documentos; (2) contiene información sobre la jerarquía organizativa; (3) es bilingüe (inglés y holandés); y (4) la lista de áreas de especialización de un individuo es proporcionada por los propios empleados.",
        "Usando la colección de expertos de UvT, llevamos a cabo dos conjuntos de experimentos.",
        "El primero tiene como objetivo determinar la efectividad de los métodos de búsqueda y perfilado de experiencia inicial en este nuevo entorno.",
        "Un segundo grupo de experimentos está dirigido a extensiones de los métodos base que aprovechan las características distintivas de la Colección de Expertos de UvT; específicamente, proponemos y evaluamos métodos refinados de búsqueda y perfilado de expertos que incorporan la topicalidad y la estructura organizativa.",
        "Además de las preguntas de investigación y el conjunto de datos que aportamos, nuestras principales contribuciones son las siguientes.",
        "Los modelos base desarrollados para encontrar expertos funcionan bien en el nuevo conjunto de datos.",
        "Mientras que en el entorno del W3C la tarea de encontrar expertos parece ser más difícil que el perfilado, para los datos de UvT ocurre lo contrario.",
        "Encontramos que el perfilado en el conjunto de datos UvT es considerablemente más difícil que en el conjunto de datos W3C, lo cual creemos que se debe al gran (pero realista) número de áreas temáticas que utilizamos para el perfilado: alrededor de 1,500 para el conjunto de datos UvT, frente a 50 en el caso de W3C.",
        "Teniendo en cuenta la similitud entre los temas puede mejorar significativamente el rendimiento de recuperación.",
        "Las medidas de similitud de mejor rendimiento son basadas en contenido, por lo tanto también se pueden aplicar en entornos de la W3C (y otros).",
        "Finalmente, demostramos que la estructura organizativa puede ser explotada en forma de un modelo de contexto, mejorando los puntajes de MAP para ciertos modelos hasta en un 70%.",
        "El resto de este documento está organizado de la siguiente manera.",
        "En la siguiente sección revisamos el trabajo relacionado.",
        "Luego, en la Sección 3 proporcionamos descripciones detalladas de las tareas de recuperación de expertos que abordamos en este artículo: la búsqueda de expertos y la creación de perfiles de expertos.",
        "En la Sección 4 presentamos nuestros modelos base, cuyo rendimiento luego es evaluado en la Sección 6 utilizando el conjunto de datos UvT que presentamos en la Sección 5.",
        "Los modelos avanzados que explotan características específicas de nuestros datos se presentan en la Sección 7 y se evalúan en la Sección 8.",
        "Formulamos nuestras conclusiones en la Sección 9.2.",
        "Trabajos relacionados Los enfoques iniciales para encontrar expertos a menudo empleaban bases de datos que contenían información sobre las habilidades y conocimientos de cada individuo en la organización [11].",
        "La mayoría de estas herramientas (generalmente llamadas páginas amarillas o sistemas de búsqueda de personas) dependen de que las personas evalúen sus habilidades frente a un conjunto predefinido de palabras clave.",
        "Para actualizar perfiles en estos sistemas de forma automática, se necesita de tecnologías inteligentes [5].",
        "Enfoques más recientes utilizan conjuntos de documentos específicos (como correos electrónicos [6] o software [18]) para encontrar experiencia.",
        "En contraste con centrarse en tipos de documentos particulares, también hay un interés creciente en el desarrollo de sistemas que indexan y extraen información de documentos publicados en intranets como fuentes de evidencia de experiencia.",
        "Un enfoque publicado es el sistema P@noptic [9], que construye una representación de cada persona concatenando todos los documentos asociados con esa persona, similar al Modelo 1 de Balog et al. [4], quienes formalizan y comparan dos métodos.",
        "El Modelo 1 de Balog et al. modela directamente el conocimiento de un experto a partir de documentos asociados, mientras que su Modelo 2 primero localiza documentos sobre el tema y luego encuentra a los expertos asociados.",
        "En los experimentos reportados, el segundo método tiene un rendimiento significativamente mejor cuando hay suficientes documentos asociados por candidato.",
        "La mayoría de los sistemas que participaron en las ediciones de 2005 y 2006 de la tarea de Búsqueda de Expertos en TREC implementaron (variaciones de) uno de estos dos modelos; ver [10, 20].",
        "Macdonald y Ounis [16] proponen un enfoque diferente para clasificar la experiencia de los candidatos con respecto a un tema basado en técnicas de fusión de datos, sin utilizar heurísticas específicas de la colección; encuentran que la aplicación de modelos de ponderación basados en campos mejora la clasificación de los candidatos.",
        "Petkova y Croft [19] proponen otro enfoque, basado en una combinación de los Modelos 1 y 2 anteriores, modelando explícitamente los temas.",
        "Al abordar otras tareas de recuperación de expertos que también pueden abordarse utilizando asociaciones entre temas y personas, Balog y de Rijke [3] abordaron la tarea de determinar perfiles de expertos temáticos.",
        "Si bien sus métodos resultaron ser eficientes en el corpus de la W3C, requieren una cantidad de datos que puede que no esté disponible en la típica organización intensiva en conocimiento.",
        "Balog y de Rijke [2] estudian la tarea relacionada de encontrar expertos que sean similares a un pequeño conjunto de expertos dados como entrada.",
        "Por cierto, crear un resumen textual de una persona muestra algunas similitudes con la búsqueda de biografías, la cual ha recibido una considerable cantidad de atención recientemente; ver por ejemplo, [13].",
        "Utilizamos modelado de lenguaje generativo para encontrar asociaciones entre temas y personas.",
        "En nuestro modelado de búsqueda y perfilado de expertos recopilamos evidencia de experiencia de múltiples fuentes, en una colección heterogénea, e integramos esta información con la co-ocurrencia de nombres de candidatos y términos de consulta: el entorno de modelado de lenguaje nos permite hacer esto de manera transparente.",
        "Nuestro modelado avanza en dos pasos.",
        "En el primer paso, consideramos tres modelos base, dos tomados de [4] (los Modelos 1 y 2 mencionados anteriormente), y uno una versión refinada de un modelo introducido en [3] (al que nos referimos como Modelo 3 a continuación); este tercer modelo también es similar al modelo descrito por Petkova y Croft [19].",
        "Los modelos que consideramos en nuestra segunda ronda de experimentos son modelos de mezcla similares a los modelos de lenguaje contextual [1] y a los documentos ampliados de Tao et al. [21]; sin embargo, las características que utilizamos para definir nuestras expansiones, incluida la estructura temática y la estructura organizativa, no se han utilizado de esta manera antes. 3.",
        "En el escenario de recuperación de expertos que imaginamos, los usuarios que buscan expertos dentro de una organización tienen acceso a una interfaz que combina un cuadro de búsqueda (donde pueden buscar expertos o temas) con estructuras de navegación (de expertos y de temas) que les permite hacer clic hasta llegar a una página de experto (que proporciona el perfil de una persona) o a una página de tema (que proporciona una lista de expertos en el tema).",
        "Para alimentar la interfaz anterior, nos enfrentamos a dos tareas de recuperación de expertos, la búsqueda de expertos y el perfilado de expertos, que primero definimos y luego formalizamos utilizando modelos de lenguaje generativos.",
        "Para modelar cualquiera de las tareas, la probabilidad de que el tema de la consulta esté asociado a un experto candidato juega un papel clave en las estimaciones finales para la búsqueda y el perfilado.",
        "Al utilizar modelos de lenguaje, tanto los candidatos como la consulta se caracterizan por distribuciones de términos en el vocabulario (utilizado en los documentos puestos a disposición por la organización cuyas necesidades de recuperación de experiencia estamos abordando). 3.1 Búsqueda de expertos La búsqueda de expertos implica la tarea de encontrar a la persona adecuada con las habilidades y conocimientos apropiados: ¿Quiénes son los expertos en el tema X?.",
        "Por ejemplo, un empleado quiere averiguar quién trabajó en un proyecto en particular para descubrir por qué se tomaron decisiones específicas sin tener que revisar minuciosamente la documentación (si es que la hay).",
        "O pueden necesitar un especialista capacitado para consultoría sobre un problema específico.",
        "Dentro de una organización, generalmente hay muchos posibles candidatos que podrían ser expertos en un tema dado.",
        "Podemos plantear este problema de la siguiente manera: ¿Cuál es la probabilidad de que un candidato ca sea un experto dado el tema de consulta q?",
        "Es decir, determinamos p(ca|q) y clasificamos a los candidatos ca de acuerdo con esta probabilidad.",
        "Los candidatos con la probabilidad más alta dada la consulta son considerados los expertos más probables en ese tema.",
        "El desafío es cómo estimar esta probabilidad con precisión.",
        "Dado que es probable que la consulta consista solo en unos pocos términos para describir la experiencia requerida, deberíamos poder obtener una estimación más precisa al invocar el Teorema de Bayes, y estimar: p(ca|q) = p(q|ca)p(ca) p(q) , (1) donde p(ca) es la probabilidad de un candidato y p(q) es la probabilidad de una consulta.",
        "Dado que p(q) es una constante, se puede ignorar para fines de clasificación.",
        "Por lo tanto, la probabilidad de que un candidato ca sea un experto dado la consulta q es proporcional a la probabilidad de una consulta dada el candidato p(q|ca), ponderada por la creencia a priori p(ca) de que el candidato ca es un experto. p(ca|q) ∝ p(q|ca)p(ca) (2) En este artículo, nuestro enfoque principal se centra en estimar la probabilidad de una consulta dada el candidato p(q|ca), ya que esta probabilidad captura en qué medida el candidato conoce sobre el tema de la consulta.",
        "Si bien se asume generalmente que los priors del candidato son uniformes y, por lo tanto, no influirán en la clasificación, se ha demostrado que una elección sensata de priors puede mejorar el rendimiento [20]. Perfilado de expertos. Mientras que la tarea de búsqueda de expertos se centraba en encontrar expertos en un tema específico, la tarea de perfilado de expertos busca responder a una pregunta relacionada: ¿Sobre qué temas tiene conocimiento un candidato?",
        "Básicamente, esto da un giro a las preguntas sobre la búsqueda de expertos.",
        "El perfilado de un candidato individual implica la identificación de áreas de habilidades y conocimientos en las que tienen experiencia, así como una evaluación del nivel de competencia en cada una de estas áreas.",
        "Este es el perfil temático de los candidatos.",
        "Generalmente, los perfiles temáticos dentro de las organizaciones consisten en estructuras tabulares que catalogan explícitamente las habilidades y conocimientos de cada individuo en la organización.",
        "Sin embargo, esta práctica está limitada por los recursos disponibles para definir, crear, mantener y actualizar estos perfiles con el tiempo.",
        "Al centrarnos en métodos automáticos que se basan en la evidencia disponible dentro de los repositorios de documentos de una organización, nuestro objetivo es reducir el esfuerzo humano asociado con el mantenimiento de perfiles temáticos.",
        "Un perfil temático de un candidato, entonces, se define como un vector donde cada elemento i del vector corresponde a la experiencia del candidato en un tema dado ki, (es decir, s(ca, ki)).",
        "Cada tema ki define un área de conocimiento o habilidad particular que la organización utiliza para definir el perfil temático de los candidatos.",
        "Por lo tanto, se asume que se proporciona una lista de temas, {k1, . . . , kn}, donde n es el número de temas predefinidos: perfil(ca) = s(ca, k1), s(ca, k2), . . . , s(ca, kn) . Se necesita contexto y evidencia para ayudar a los usuarios de sistemas de búsqueda de expertos a decidir a quién contactar al buscar experiencia en un área particular.",
        "Ejemplos de dicho contexto son: ¿Con quién trabaja ella?",
        "¿Cuáles son sus datos de contacto?",
        "¿Está bien conectada, por si acaso no puede ayudarnos ella misma?",
        "¿Cuál es su rol en la organización?",
        "¿Quién es su superior?",
        "Los colaboradores, afiliaciones, etc. son parte del perfil social de los candidatos y pueden servir como antecedentes para interpretar las recomendaciones del sistema.",
        "En este documento solo abordamos el problema de determinar perfiles temáticos, dejando el perfilado social para trabajos futuros.",
        "Planteamos el problema de cuantificar la competencia de una persona en un área de conocimiento específica de la siguiente manera: ¿Cuál es la probabilidad de que un área de conocimiento (ki) forme parte del perfil de candidatos (expertise)? donde s(ca, ki) está definido por p(ki|ca).",
        "Nuestra tarea, entonces, es estimar p(ki|ca), lo cual es equivalente al problema de obtener p(q|ca), donde el tema ki se representa como un tema de consulta q, es decir, una secuencia de palabras clave que representan la experiencia requerida.",
        "Tanto la tarea de encontrar expertos como la de perfilar expertos dependen de la estimación precisa de p(q|ca).",
        "La única diferencia proviene de la probabilidad previa de que una persona sea experta (p(ca)), la cual puede ser incorporada en la tarea de encontrar expertos.",
        "Este antecedente no se aplica a la tarea de perfilado ya que el candidato (individuo) está fijo. 4.",
        "MODELOS DE REFERENCIA En esta sección describimos nuestros modelos de referencia para estimar p(q|ca), es decir, las asociaciones entre temas y personas.",
        "Tanto la búsqueda de expertos como el perfilado de expertos se reducen a esta estimación.",
        "Empleamos tres modelos para calcular esta probabilidad. 4.1 De temas a candidatos Utilizando Modelos de Candidatos: El Modelo 1 [4] define la probabilidad de una consulta dada un candidato (p(q|ca)) utilizando técnicas estándar de modelado de lenguaje, basadas en un modelo de lenguaje multinomial de unigrama.",
        "Para cada candidato ca, se infiere un modelo de lenguaje candidato θca de tal manera que la probabilidad de un término dado θca no sea cero para todos los términos, es decir, p(t|θca) > 0.",
        "A partir del modelo de candidato, la consulta se genera con la siguiente probabilidad: p(q|θca) = Y t∈q p(t|θca)n(t,q), donde cada término t en la consulta q se muestrea de manera idéntica e independiente, y n(t, q) es el número de veces que t ocurre en q.",
        "El modelo de lenguaje candidato se infiere de la siguiente manera: (1) se calcula un modelo empírico p(t|ca); (2) se suaviza con probabilidades de fondo.",
        "Usando las asociaciones entre un candidato y un documento, la probabilidad p(t|ca) puede aproximarse por: p(t|ca) = X d p(t|d)p(d|ca), donde p(d|ca) es la probabilidad de que el candidato ca genere un documento de apoyo d, y p(t|d) es la probabilidad de que un término t ocurra en el documento d. Utilizamos la estimación de máxima verosimilitud de un término, es decir, la frecuencia normalizada del término t en el documento d. La fuerza de la asociación entre el documento d y el candidato ca expresada por p(d|ca) refleja el grado en que la experiencia de los candidatos se describe utilizando este documento.",
        "La estimación de esta probabilidad se presenta más adelante, en la Sección 4.2.",
        "El modelo candidato se construye entonces como una interpolación lineal de p(t|ca) y el modelo de fondo p(t) para asegurar que no haya probabilidades nulas, lo que resulta en la estimación final: p(q|θca) = (4) Y t∈q ( (1 − λ) X d p(t|d)p(d|ca) ! + λp(t) )n(t,q) .",
        "El Modelo 1 recopila toda la información de términos de todos los documentos asociados con el candidato, y la utiliza para representar a ese candidato.",
        "Este modelo se utiliza para predecir qué tan probable es que un candidato produzca una consulta q.",
        "Esto puede interpretarse intuitivamente como la probabilidad de que este candidato hable sobre el tema de la consulta, donde asumimos que esto es indicativo de su experiencia.",
        "Usando Modelos de Documentos: El Modelo 2 [4] toma un enfoque diferente.",
        "Aquí, el proceso se divide en dos partes.",
        "Dado un candidato ca, (1) se selecciona un documento asociado con un candidato con probabilidad p(d|ca), y (2) a partir de este documento se genera una consulta q con probabilidad p(q|d).",
        "Entonces se toma la suma sobre todos los documentos para obtener p(q|ca), de modo que: p(q|ca) = Σ d p(q|d)p(d|ca). (5) La probabilidad de una consulta dada un documento se estima inferiendo un modelo de lenguaje del documento θd para cada documento d de manera similar a como se infería el modelo candidato: p(t|θd) = (1 − λ)p(t|d) + λp(t), (6) donde p(t|d) es la probabilidad del término en el documento.",
        "La probabilidad de una consulta dada el modelo de documento es: p(q|θd) = Y t∈q p(t|θd)n(t,q) .",
        "La estimación final de p(q|ca) se obtiene sustituyendo p(q|d) por p(q|θd) en la Ecuación 5 (ver [4] para más detalles).",
        "Conceptualmente, el Modelo 2 difiere del Modelo 1 porque el candidato no está modelado directamente.",
        "En cambio, el documento actúa como una variable oculta en el proceso que separa la consulta del candidato.",
        "Este proceso es similar a cómo un usuario puede buscar candidatos con un motor de búsqueda estándar: inicialmente encontrando los documentos relevantes y luego viendo quién está asociado con ese documento.",
        "Al examinar una serie de documentos, el usuario puede obtener una idea de qué candidatos son más propensos a discutir el tema q.",
        "Usando Modelos de Temas: Modelo 3 Introducimos un tercer modelo, Modelo 3.",
        "En lugar de intentar modelar el proceso de generación de consultas a través de modelos de candidatos o documentos, representamos la consulta como un modelo de lenguaje de temas y estimamos directamente la probabilidad del candidato p(ca|q).",
        "Este enfoque es similar al modelo presentado en [3, 19].",
        "Como en los modelos anteriores, se infiere un modelo de lenguaje, pero esta vez para la consulta.",
        "Adaptamos el trabajo de Lavrenko y Croft [14] para estimar un modelo de tema a partir de la consulta.",
        "El procedimiento es el siguiente.",
        "Dada una colección de documentos y un tema de consulta q, se asume que existe un modelo de tema desconocido θk que asigna probabilidades p(t|θk) a las ocurrencias de términos en los documentos del tema.",
        "Tanto la consulta como los documentos son muestras de θk (a diferencia de los enfoques anteriores, donde se asume que una consulta se extrae de un documento específico o un modelo candidato).",
        "La tarea principal es estimar p(t|θk), la probabilidad de un término dado el modelo de tema.",
        "Dado que la consulta q es muy dispersa y no hay ejemplos de documentos sobre el tema, es necesario aproximar esta distribución.",
        "Lavrenko y Croft [14] sugieren una forma razonable de obtener dicha aproximación, asumiendo que p(t|θk) puede aproximarse por la probabilidad del término t dado la consulta q.",
        "Entonces podemos estimar p(t|q) utilizando la probabilidad conjunta de observar el término t junto con los términos de la consulta, q1, . . . , qm, y dividiendo por la probabilidad conjunta de los términos de la consulta: p(t|θk) ≈ p(t|q) = p(t, q1, . . . , qm) p(q1, . . . , qm) = p(t, q1, . . . , qm) P t ∈T p(t , q1, . . . , qm) , donde p(q1, . . . , qm) = P t ∈T p(t , q1, . . . , qm), y T es el vocabulario completo de términos.",
        "Para estimar la probabilidad conjunta p(t, q1, . . . , qm), seguimos [14, 15] y asumimos que t y q1, . . . , qm son mutuamente independientes, una vez que elegimos una distribución fuente del conjunto de distribuciones fuente subyacentes U.",
        "Si elegimos U como un conjunto de modelos de documentos, entonces para construir este conjunto, la consulta q se emitiría contra la colección, y se asume que los primeros n devueltos son relevantes para el tema, y por lo tanto se tratan como muestras del modelo de tema. (Tenga en cuenta que en su lugar podrían usarse modelos candidatos).",
        "Con los modelos de documentos formando U, la probabilidad conjunta del término y la consulta se convierte en: p(t, q1, . . . , qm) = X d∈U p(d) ˘ p(t|θd) mY i=1 p(qi|θd) ¯ . (7) Aquí, p(d) denota la distribución previa sobre el conjunto U, que refleja la relevancia del documento para el tema.",
        "Suponemos que p(d) es uniforme en todo U.",
        "Para clasificar a los candidatos según el modelo de tema definido, utilizamos la métrica de divergencia de Kullback-Leibler (KL, [8]) para medir la diferencia entre los modelos de los candidatos y el modelo de tema: KL(θk||θca) = X t p(t|θk) log p(t|θk) p(t|θca) . (8) Se considera que los candidatos con una divergencia menor respecto al modelo de tema son más propensos a ser expertos en ese tema.",
        "El modelo candidato θca está definido en la Ecuación 4.",
        "Al utilizar la divergencia de Kullback-Leibler en lugar de la probabilidad de un candidato dado el modelo de tema p(ca|θk), evitamos problemas de normalización. 4.2 Asociaciones documento-candidato Para nuestros modelos, necesitamos poder estimar la probabilidad p(d|ca), que expresa en qué medida un documento d caracteriza al candidato ca.",
        "En [4], se presentan dos métodos para estimar esta probabilidad, basados en el número de nombres de personas reconocidos en un documento.",
        "Sin embargo, en nuestro entorno de intranet es razonable asumir que los autores de los documentos pueden ser identificados de manera inequívoca (por ejemplo, como el autor de un artículo, el profesor asignado a un curso, el propietario de una página web, etc.).",
        "Por lo tanto, establecemos p(d|ca) en 1 si el candidato ca es el autor del documento d, de lo contrario la probabilidad es 0.",
        "En la Sección 6 describimos cómo se puede determinar la autoría en diferentes tipos de documentos dentro de la colección. 5.",
        "La colección de expertos de UvT utilizada en los experimentos de este artículo se ajusta al escenario descrito en la Sección 3.",
        "La colección se basa en el sistema Webwijs (Webwise) desarrollado en la Universidad de Tilburg (UvT) en los Países Bajos.",
        "Webwijs (http://www.uvt.nl/webwijs/) es una base de datos de acceso público de empleados de la UvT que están involucrados en investigación o enseñanza; actualmente, Webwijs contiene información sobre 1168 expertos, cada uno de los cuales tiene una página con información de contacto y, si está disponible por parte del experto, una descripción de investigación y lista de publicaciones.",
        "Además, cada experto puede seleccionar áreas de especialización de una lista de 1491 temas y se le anima a sugerir nuevos temas que necesitan ser aprobados por el editor de Webwijs.",
        "Cada tema tiene una página separada que muestra a todos los expertos asociados con ese tema y, si está disponible, una lista de temas relacionados.",
        "Webwijs está disponible en holandés e inglés, y esta bilingüidad se ha preservado en la colección.",
        "Cada página de Webwijs en holandés tiene una traducción al inglés.",
        "No todos los temas en holandés tienen una traducción al inglés, pero lo contrario es cierto: los 981 temas en inglés tienen un equivalente en holandés.",
        "Aproximadamente el 42% de los expertos imparten cursos en la Universidad de Tilburg; estos cursos también fueron rastreados e incluidos en el perfil.",
        "Además, aproximadamente el 27% de los expertos enlazan su página académica desde su página de Webwijs.",
        "Estas páginas de inicio fueron rastreadas y añadidas a la colección. (Esto significa que si los expertos colocaron las versiones completas de sus publicaciones en sus páginas de inicio académicas, estas también estaban disponibles para su indexación).",
        "También obtuvimos 1880 versiones completas de publicaciones del repositorio institucional de UvT y el número de expertos holandeses en inglés es de 1168, con 743 expertos con ≥ 1 tema. El número de temas es de 1491 y 981, y el número de pares experto-tema es de 4318 y 3251, con un promedio de temas por experto de 5.8 y 5.9 respectivamente. El máximo número de temas por experto es de 60 (1) y 35 (1), y el mínimo es de 1 (74) y 1 (106) respectivamente. El promedio de expertos por tema es de 2.9 y 3.3, con un máximo de 30 (1) y un mínimo de 1 (615) y 1 (346) respectivamente. Hay 318 expertos con HP y CD, con un promedio de 3.5 CDs por experto docente. Además, hay 329 expertos con RD y 734 con PUB, con un promedio de 27.0 publicaciones por experto, 25.2 citas por experto y 1.8 publicaciones completas por experto. Tabla 2: Estadísticas descriptivas de las versiones holandesas e inglesas de la colección de expertos de UvT.",
        "Ejecutamos el identificador de idioma TextCat [23] para clasificar el idioma de las páginas de inicio y las publicaciones de texto completo.",
        "Nos limitamos a las páginas en las que el clasificador tenía confianza en el idioma utilizado en la página.",
        "Esto resultó en cuatro tipos de documentos: descripciones de investigación (RD), descripciones de cursos (CD), publicaciones (PUB; versiones de texto completo y solo de cita) y páginas web académicas (HP).",
        "Todo fue agrupado en la colección UvT Expert que está disponible en http://ilk.uvt.nl/uvt-expert-collection/.",
        "La colección UvT Expert fue extraída de un entorno organizativo diferente a la colección de la W3C y difiere de ella en varios aspectos.",
        "El entorno de UvT es uno con cantidades relativamente pequeñas de datos multilingües.",
        "Las asociaciones entre los documentos y los autores son claras y los datos están estructurados y limpios.",
        "La colección abarca una amplia gama de áreas de especialización, como suele encontrarse en intranets de universidades y otros institutos intensivos en conocimiento.",
        "Además, nuestro entorno universitario cuenta con varios tipos de estructura (temática y organizativa), así como múltiples tipos de documentos.",
        "Otra diferencia importante entre los dos conjuntos de datos es que las áreas de especialización en la colección de Expertos de UvT son autoseleccionadas en lugar de basarse en la membresía de un grupo o asignaciones de otros.",
        "El tamaño es otra dimensión en la que difieren las colecciones de expertos de W3C y UvT: esta última es la más pequeña de las dos.",
        "También son realistas las grandes diferencias en la cantidad de información disponible para cada experto.",
        "Utilizar Webwijs es voluntario; 425 expertos holandeses no seleccionaron ningún tema en absoluto.",
        "Esto nos deja con 743 perfiles de expertos holandeses y 727 perfiles de expertos en inglés utilizables.",
        "La Tabla 2 proporciona estadísticas descriptivas para la colección de Expertos de UvT.",
        "Las universidades tienden a tener una estructura jerárquica que va desde el nivel de la facultad, pasando por los departamentos, grupos de investigación, hasta llegar a los investigadores individuales.",
        "En la colección de Expertos de UvT tenemos información sobre las afiliaciones de los investigadores con facultades e institutos, lo que nos proporciona una jerarquía organizativa de dos niveles.",
        "La Universidad de Tilburg tiene 22 unidades organizativas a nivel de facultad (incluida la oficina universitaria y varios institutos de investigación) y 71 departamentos, lo que equivale a 3.2 departamentos por facultad.",
        "En cuanto a la jerarquía temática utilizada por Webwijs, 131 de los 1491 temas son nodos principales en la jerarquía.",
        "Esta jerarquía tiene una longitud promedio de cadena de temas de 2.65 y una longitud máxima de 7 temas. 6.",
        "EVALUACIÓN A continuación, evaluamos los modelos de la Sección 4 para la búsqueda y perfilado de expertos en la colección de Expertos de UvT.",
        "Detallamos nuestras preguntas de investigación y configuración experimental, y luego presentamos nuestros resultados. 6.1 Preguntas de investigación Abordamos las siguientes preguntas de investigación.",
        "Tanto la búsqueda de expertos como el perfilado dependen de las estimaciones de p(q|ca).",
        "La pregunta es cómo se comparan los modelos en las diferentes tareas, y en el contexto de la colección de expertos de UvT.",
        "En [4], el Modelo 2 superó al Modelo 1 en la colección de la W3C.",
        "¿Cómo se comparan en nuestro conjunto de datos?",
        "¿Y cómo se compara el Modelo 3 con el Modelo 1?",
        "¿Qué hay de las diferencias de rendimiento entre los dos idiomas en nuestra colección de pruebas? 6.2 Configuración Experimental La salida de nuestros modelos fue evaluada frente a las etiquetas de temas autoasignadas, que fueron tratadas como juicios de relevancia.",
        "Los resultados fueron evaluados por separado para inglés y holandés.",
        "Para inglés solo usamos temas para los cuales había traducción al neerlandés; para neerlandés se consideraron todos los temas.",
        "Los resultados se promediaron para las consultas en la intersección de las evaluaciones de relevancia y los resultados; las consultas faltantes no contribuyen con un valor de 0 a las puntuaciones.",
        "Utilizamos medidas estándar de recuperación de información, como la Precisión Media Promedio (MAP) y la Reciprocidad Media Promedio (MRR).",
        "También informamos el porcentaje de temas (%q) y candidatos (%ca) cubiertos, respectivamente, para las tareas de búsqueda y perfilado de expertos. 6.3 Resultados La Tabla 1 muestra el rendimiento de los Modelos 1, 2 y 3 en las tareas de búsqueda y perfilado de expertos.",
        "Las filas de la tabla corresponden a los diversos tipos de documentos (RD, CD, PUB y HP) y a sus combinaciones.",
        "RD+CD+PUB+HP es equivalente a la colección completa y será referida como la LÍNEA BASE de nuestros experimentos.",
        "Al observar la Tabla 1, vemos que el Modelo 2 tiene el mejor rendimiento en general.",
        "Sin embargo, cuando los datos están limpios y muy enfocados (RD), el Modelo 3 lo supera en varios casos.",
        "El modelo 1 tiene la mejor cobertura de candidatos (%ca) y temas (%q).",
        "Los diversos tipos de documentos difieren en sus características y en cómo mejoran las tareas de búsqueda y perfilado.",
        "El perfilado de expertos se beneficia mucho de los datos limpios presentes en los tipos de documentos RD y CD, mientras que las publicaciones contribuyen principalmente a la tarea de encontrar expertos.",
        "Agregar las páginas de inicio no resulta ser particularmente útil.",
        "Cuando comparamos los resultados entre idiomas, encontramos que la cobertura de los temas en inglés (%q) es mayor que la de los temas en neerlandés para la búsqueda de expertos.",
        "Además de eso, las puntuaciones caen en el mismo rango para ambos idiomas.",
        "Para la tarea de perfilado, la cobertura de los candidatos (%ca) es muy similar para ambos idiomas.",
        "Sin embargo, el rendimiento es considerablemente mejor para los temas en inglés.",
        "Si bien es difícil comparar las puntuaciones entre colecciones, concluimos con una breve comparación de las puntuaciones absolutas en la Tabla 1 con las reportadas en [3, 4] en el conjunto de pruebas de W3C (edición de 2005).",
        "Para el experto que busca, los puntajes MAP para el Modelo 2 reportados aquí son aproximadamente un 50% más altos que las cifras correspondientes en [4], mientras que nuestros puntajes MRR están ligeramente por debajo de los de [4].",
        "Para el perfilado de expertos, las diferencias son mucho más dramáticas: las puntuaciones MAP para el Modelo 2 reportadas aquí son aproximadamente un 50% más bajas que las puntuaciones en [3], mientras que las puntuaciones MRR (mejores) son aproximadamente iguales a las de [3].",
        "La causa de estas diferencias parece residir en el número de áreas de conocimiento consideradas aquí, aproximadamente 30 veces más que en el entorno del W3C.",
        "MODELOS AVANZADOS Ahora que hemos desarrollado y evaluado técnicas básicas de modelado de lenguaje para la recuperación de expertos, nos enfocamos en modelos refinados que explotan características especiales de nuestra colección de pruebas. 7.1 Explotando la similitud en el área de conocimiento Una forma de mejorar la puntuación de una consulta dada un candidato es considerar qué otras solicitudes satisfaría el candidato y usarlas como evidencia adicional para respaldar la consulta original, proporcionalmente. Búsqueda de expertos Perfilado de expertos Tipos de documentos Modelo 1 Modelo 2 Modelo 3 Modelo 1 Modelo 2 Modelo 3 %q MAP MRR %q MAP MRR %q MAP MRR %ca MAP MRR %ca MAP MRR %ca MAP MRR Inglés RD 97.8 0.126 0.269 83.5 0.144 0.311 83.3 0.129 0.271 100 0.089 0.189 39.3 0.232 0.465 41.1 0.166 0.337 CD 97.8 0.118 0.227 91.7 0.123 0.248 91.7 0.118 0.226 32.8 0.188 0.381 32.4 0.195 0.385 32.7 0.203 0.370 PUB 97.8 0.200 0.330 98.0 0.216 0.372 98.0 0.145 0.257 78.9 0.167 0.364 74.5 0.212 0.442 78.9 0.135 0.299 HP 97.8 0.081 0.186 97.4 0.071 0.168 97.2 0.062 0.149 31.2 0.150 0.299 28.8 0.185 0.335 30.1 0.136 0.287 RD+CD 97.8 0.188 0.352 92.9 0.193 0.360 92.9 0.150 0.273 100 0.145 0.286 61.3 0.251 0.477 63.2 0.217 0.416 RD+CD+PUB 97.8 0.235 0.373 98.1 0.277 0.439 98.1 0.178 0.305 100 0.196 0.380 87.2 0.280 0.533 89.5 0.170 0.344 RD+CD+PUB+HP 97.8 0.237 0.372 98.6 0.280 0.441 98.5 0.166 0.293 100 0.199 0.387 88.7 0.281 0.525 90.9 0.169 0.329 Holandés RD 61.3 0.094 0.229 38.4 0.137 0.336 38.3 0.127 0.295 38.0 0.127 0.386 34.1 0.138 0.420 38.0 0.105 0.327 CD 61.3 0.107 0.212 49.7 0.128 0.256 49.7 0.136 0.261 32.5 0.151 0.389 31.8 0.158 0.396 32.5 0.170 0.380 PUB 61.3 0.193 0.319 59.5 0.218 0.368 59.4 0.173 0.291 78.8 0.126 0.364 76.0 0.150 0.424 78.8 0.103 0.294 HP 61.3 0.063 0.169 56.6 0.064 0.175 56.4 0.062 0.163 29.8 0.108 0.308 27.8 0.125 0.338 29.8 0.098 0.255 RD+CD 61.3 0.159 0.314 51.9 0.184 0.360 51.9 0.169 0.324 60.5 0.151 0.410 57.2 0.166 0.431 60.4 0.159 0.384 RD+CD+PUB 61.3 0.244 0.398 61.5 0.260 0.424 61.4 0.210 0.350 90.3 0.165 0.445 88.2 0.189 0.479 90.3 0.126 0.339 RD+CD+PUB+HP 61.3 0.249 0.401 62.6 0.265 0.436 62.6 0.195 0.344 91.9 0.164 0.426 90.1 0.195 0.488 91.9 0.125 0.328 Tabla 1: Rendimiento de los modelos en las tareas de búsqueda y perfilado de expertos, utilizando diferentes tipos de documentos y sus combinaciones. %q es el número de temas cubiertos (se aplica a la tarea de búsqueda de expertos), %ca es el número de candidatos cubiertos (se aplica a la tarea de perfilado de expertos).",
        "Los bloques superior e inferior corresponden al inglés y al holandés respectivamente.",
        "Las mejores puntuaciones están en negrita.",
        "Esto se puede modelar interpolando entre el p(q|ca) y la evidencia adicional de todos los pedidos similares q, de la siguiente manera: p(q|ca) = λp(q|ca) + (1 − λ) X q p(q|q )p(q |ca), (9) donde p(q|q ) representa la similitud entre los dos temas q y q.",
        "Para poder trabajar con métodos de similitud que no son necesariamente probabilidades, establecemos p(q|q ) = w(q,q ) γ , donde γ es una constante de normalización, de modo que γ = P q w(q , q ).",
        "Consideramos cuatro métodos para calcular la puntuación de similitud entre dos temas.",
        "Tres enfoques se basan estrictamente en el contenido y establecen similitudes examinando patrones de co-ocurrencia de temas dentro de la colección, mientras que el último enfoque explota la estructura jerárquica de áreas temáticas que pueden estar presentes dentro de una organización (ver [7] para más ejemplos de integrar relaciones entre palabras en modelos de lenguaje).",
        "La métrica de divergencia Kullback-Leibler (KL) definida en la ecuación 8 proporciona una medida de cuán diferentes o similares son dos distribuciones de probabilidad.",
        "Se infiere un modelo de tema para q y q utilizando el método presentado en la Sección 4.1 para describir la consulta en todo el vocabulario.",
        "Dado que un puntaje KL más bajo significa que las consultas son más similares, permitimos que w(q, q ) = max(KL(θq||·) − KL(θq||θq )).",
        "El Información Mutua Puntual (PMI, [17]) es una medida de asociación utilizada en teoría de la información para determinar el grado de independencia entre variables.",
        "La dependencia entre dos consultas se refleja en la puntuación SI(q, q), donde las puntuaciones mayores que cero indican que es probable que exista una dependencia, lo que interpretamos como que las consultas son probablemente similares: SI(q, q) = log p(q, q) / p(q)p(q). Estimamos la probabilidad de un tema p(q) utilizando el número de documentos relevantes para la consulta q dentro de la colección.",
        "La probabilidad conjunta p(q, q) se estima de manera similar, utilizando la concatenación de q y q como una consulta.",
        "Para obtener p(q|q), luego establecemos w(q, q) = SI(q, q) cuando SI(q, q) > 0, de lo contrario w(q, q) = 0, ya que solo estamos interesados en incluir consultas que sean similares.",
        "La estadística de log-verosimilitud proporciona otra medida de dependencia, que es más confiable que la medida de información mutua puntual [17].",
        "Sea k1 el número de co-ocurrencias de q y q, k2 el número de ocurrencias de q que no co-ocurren con q, n1 el número total de ocurrencias de q, y n2 el número total de tokens de tema menos el número de ocurrencias de q.",
        "Entonces, sea p1 = k1/n1, p2 = k2/n2 y p = (k1 + k2)/(n1 + n2), (q, q ) = 2( (p1, k1, n1) + (p2, k2, n2) − (p, k1, n1) − (p, k2, n2)), donde (p, n, k) = k log p + (n − k) log(1 − p).",
        "El puntaje más alto indica que las consultas también son probablemente similares, por lo tanto, establecemos w(q, q ) = (q, q ).",
        "Finalmente, también estimamos la similitud de dos temas basándonos en su distancia dentro de la jerarquía de temas.",
        "La jerarquía de temas se visualiza como un grafo dirigido, y para todos los pares de temas se calcula el camino más corto SP(q, q).",
        "Establecimos el puntaje de similitud como el recíproco del camino más corto: w(q, q ) = 1/SP(q, q ). 7.2 Información contextual Dada la jerarquía de una organización, las unidades a las que pertenece una persona se consideran como un contexto para compensar la escasez de datos.",
        "Lo modelamos de la siguiente manera: p (q|ca) = 1 − P ou∈OU(ca) λou · p(q|ca) + P ou∈OU(ca) λou · p(q|ou), donde OU(ca) es el conjunto de unidades organizativas de las cuales el candidato ca es miembro, y p(q|o) expresa la fuerza de la asociación entre la consulta q y la unidad ou.",
        "La última probabilidad se puede estimar utilizando cualquiera de los tres modelos básicos, simplemente reemplazando ca con ou en las ecuaciones correspondientes.",
        "Una unidad organizativa está asociada con todos los documentos que sus miembros han escrito.",
        "Es decir, p(d|ou) = maxca∈ou p(d|ca). 7.3 Un modelo multilingüe simple Para institutos de conocimiento en Europa, ya sea académicos u otros, un entorno multilingüe (o al menos bilingüe) es típico.",
        "El siguiente modelo se basa en un tipo de suposición de independencia: no hay transferencia de conocimientos/perfiles a través de las barreras del idioma.",
        "Si bien es una simplificación, esta es una aproximación sensata en primer lugar.",
        "Eso es: p (q|ca) =P l∈L λl · p(ql|ca), donde L es el conjunto de idiomas utilizados en la colección, ql es la traducción de la consulta q al idioma l, y λl es un parámetro de suavizado específico del idioma, tal que P l∈L λl = 1. 8.",
        "MODELOS AVANZADOS: EVALUACIÓN En esta sección presentamos una evaluación experimental de nuestros modelos avanzados.",
        "Búsqueda de expertos Perfilado de expertos Modelo de lenguaje 1 Modelo 2 Modelo 3 Modelo 1 Modelo 2 Modelo 3 %q MAP MRR %q MAP MRR %q MAP MRR %ca MAP MRR %ca MAP MRR %ca MAP MRR Solo inglés 97.8 0.237 0.372 98.6 0.280 0.441 98.5 0.166 0.293 100 0.199 0.387 88.7 0.281 0.525 90.9 0.169 0.329 Solo holandés 61.3 0.249 0.401 62.6 0.265 0.436 62.6 0.195 0.344 91.9 0.164 0.426 90.1 0.195 0.488 91.9 0.125 0.328 Combinación 99.4 0.297 0.444 99.7 0.324 0.491 99.7 0.223 0.388 100 0.241 0.445 92.1 0.313 0.564 93.2 0.224 0.411 Tabla 3: Rendimiento de la combinación de idiomas en las tareas de búsqueda y perfilado de expertos (en candidatos).",
        "Los mejores puntajes para cada modelo están en cursiva, los mejores puntajes absolutos para las tareas de búsqueda y perfilado de expertos están en negrita.",
        "Tabla 4: Rendimiento en las tareas de búsqueda de expertos (arriba) y perfilado (abajo), utilizando similitudes de áreas de conocimiento.",
        "Las carreras fueron evaluadas en los temas principales establecidos.",
        "Las mejores puntuaciones están en negrita. 8.1 Preguntas de investigación Nuestras preguntas siguen los refinamientos presentados en la sección anterior: ¿Explotar la similitud en el área de conocimiento mejora la efectividad?",
        "¿Cuál de los diversos métodos para capturar las relaciones entre palabras es más efectivo?",
        "Además, ¿es útil nuestra forma de incorporar información contextual?",
        "¿Para qué tareas?",
        "Y finalmente, ¿es nuestra forma simple de combinar las puntuaciones monolingües suficiente para obtener mejoras significativas? 8.2 Configuración experimental Dado que las autoevaluaciones también son escasas en nuestra colección, para poder medir las diferencias entre los diversos modelos, seleccionamos un subconjunto de temas y evaluamos (algunas de las) ejecuciones solo en este subconjunto.",
        "Este conjunto se denomina como temas principales, y consiste en temas que se encuentran en el nivel superior de la jerarquía temática. (Un tema principal tiene subtemas, pero no es un subtema de ningún otro tema).",
        "Este conjunto principal consiste en 132 temas en holandés y 119 en inglés.",
        "Los juicios de relevancia se limitaron al conjunto de temas principales, pero no se ampliaron con subtemas. 8.3 Explotando la similitud de áreas de conocimiento. La Tabla 4 presenta los resultados.",
        "Los cuatro métodos utilizados para estimar la similitud de áreas de conocimiento son la divergencia de KL (KLDIV) y PointLang.",
        "Modelo 1 Modelo 2 Modelo 3 MAP MRR MAP MRR MAP MRR Búsqueda de expertos UK TODOS 0.423 0.545 0.654 0.799 0.494 0.629 UK PRINCIPAL 0.500 0.621 0.704 0.834 0.587 0.699 NL TODOS 0.439 0.560 0.672 0.826 0.480 0.630 NL PRINCIPAL 0.440 0.584 0.645 0.816 0.515 0.655 Perfilado de expertos UK TODOS 0.240 0.640 0.306 0.778 0.223 0.616 UK PRINCIPAL 0.523 0.677 0.519 0.648 0.461 0.587 NL TODOS 0.203 0.716 0.254 0.770 0.183 0.627 NL PRINCIPAL 0.332 0.576 0.380 0.624 0.332 0.549 Tabla 5: Evaluación de los modelos de contexto en unidades organizativas. información mutua ponderada (PMI), log-verosimilitud (LL) y distancia dentro de la jerarquía de temas (HDIST).",
        "Logramos mejorar el resultado base en todos los casos, pero la mejora es más notable para la tarea de perfilado.",
        "Para ambas tareas, el método LL tuvo el mejor rendimiento.",
        "Los enfoques basados en el contenido tuvieron un rendimiento consistentemente mejor que HDIST. 8.4 Información contextual Una jerarquía de dos niveles de unidades organizativas (facultades e institutos) está disponible en la colección de Expertos de UvT.",
        "La unidad a la que pertenece una persona se utiliza como contexto para esa persona.",
        "Primero, evaluamos los modelos de las unidades organizativas, utilizando todos los temas (TODOS) y solo los temas principales (PRINCIPALES).",
        "Una unidad organizativa se considera relevante para un tema dado (o viceversa) si al menos un miembro de la unidad seleccionó el tema dado como área de especialización.",
        "La Tabla 5 informa sobre los resultados.",
        "En lo que respecta a la búsqueda de expertos, dado un tema, la unidad organizativa correspondiente puede ser identificada con alta precisión.",
        "Sin embargo, la tarea de perfilado de expertos muestra una imagen diferente: las puntuaciones son bajas y la tarea parece difícil.",
        "La explicación puede ser que los conceptos generales (es decir, nuestros temas principales) pueden pertenecer a varias unidades organizativas.",
        "Segundo, realizamos otra evaluación, donde combinamos los modelos contextuales con los modelos candidatos (para puntuar a los candidatos nuevamente).",
        "La tabla 6 informa sobre los resultados.",
        "Encontramos un impacto positivo de los modelos de contexto solo para la búsqueda de expertos.",
        "Notablemente, para la búsqueda de expertos (y Modelo 1), mejora más del 50% (para inglés) y más del 70% (para holandés) en el MAP.",
        "El bajo rendimiento en la creación de perfiles de expertos puede deberse al hecho de que los modelos de contexto por sí solos no tuvieron un buen desempeño en la tarea de creación de perfiles desde el principio. 8.5 Modelos multilingües En esta subsección evaluamos el método para combinar resultados en múltiples idiomas que describimos en la Sección 7.3.",
        "En nuestro entorno, el conjunto de idiomas consiste en inglés y holandés: L = {UK, NL}.",
        "Los pesos de estos idiomas se establecieron para ser idénticos (λUK = λNL = 0.5).",
        "Realizamos experimentos con diferentes configuraciones de λ, pero no observamos diferencias significativas en el rendimiento.",
        "La Tabla 3 informa sobre los resultados multilingües, donde el rendimiento se evalúa en el conjunto completo de temas.",
        "Los tres modelos tienen una diferencia significativa.",
        "Tabla 6: Rendimiento de los modelos de contexto (CT) en comparación con la línea base (BL).",
        "Las mejores puntuaciones están en negrita, demostradas en todas las medidas para ambas tareas.",
        "La cobertura de temas y candidatos para las tareas de búsqueda y perfilado de expertos, respectivamente, es cercana al 100% en todos los casos.",
        "La mejora relativa de las puntuaciones de precisión varía del 10% al 80%.",
        "Estas puntuaciones demuestran que, a pesar de su simplicidad, nuestro método para combinar resultados en varios idiomas logra mejoras sustanciales sobre el punto de referencia.",
        "CONCLUSIONES En este artículo nos enfocamos en la recuperación de expertos (búsqueda y perfilado de expertos) en un nuevo entorno de una organización típica intensiva en conocimiento en la que los datos disponibles son de alta calidad, multilingües y abarcan una amplia gama de áreas de expertise.",
        "Normalmente, la cantidad de datos disponibles en una organización como una universidad, un instituto de investigación o un laboratorio de investigación es limitada en comparación con la colección de la W3C que ha sido utilizada principalmente para la evaluación experimental de la recuperación de la experiencia hasta ahora.",
        "Para examinar la recuperación de la experiencia en este entorno, presentamos (y lanzamos) la colección de Expertos de UvT como un caso representativo de organizaciones intensivas en conocimiento.",
        "La nueva colección refleja las propiedades típicas de los institutos intensivos en conocimiento mencionados anteriormente e incluye también varias características que pueden ser potencialmente útiles para la recuperación de expertos, como la estructura temática y organizativa.",
        "Evaluamos cómo los modelos de vanguardia para la búsqueda y perfilado de expertos se desempeñaron en este nuevo entorno y luego refinamos estos modelos para intentar explotar las diferentes características dentro del entorno de datos (idioma, actualidad y estructura organizativa).",
        "Encontramos que los modelos actuales de recuperación de experiencia se generalizan bien a este nuevo entorno; además, descubrimos que refinar los modelos para tener en cuenta las diferencias resulta en mejoras significativas, compensando así los problemas causados por la escasez de datos.",
        "El trabajo futuro incluye establecer evaluaciones manuales de perfiles generados automáticamente por los propios empleados, especialmente en casos en los que los empleados no han proporcionado un perfil ellos mismos. 10.",
        "AGRADECIMIENTOS Krisztian Balog fue apoyado por la Organización Neerlandesa para la Investigación Científica (NWO) bajo el número de proyecto 220-80-001.",
        "Maarten de Rijke también recibió apoyo de NWO bajo los números de proyecto 017.001.190, 220-80-001, 264-70-050, 354-20-005, 600.065.120, 612-13-001, 612.000.106, 612.066.302, 612.069.006, 640.001.501, 640.002.501, y de la Unión Europea.",
        "Programa IST del 6º Programa Marco para la I+D en virtud del contrato del proyecto MultiMATCH IST-033104.",
        "El trabajo de Toine Bogers y Antal van den Bosch fue financiado por el programa IOP-MMI de SenterNovem / Ministerio de Asuntos Económicos de los Países Bajos, como parte del proyecto 'A Propos'. 11.",
        "REFERENCIAS [1] L. Azzopardi.",
        "Incorporando contexto en el marco de modelado del lenguaje para la recuperación de información ad-hoc.",
        "Tesis doctoral, Universidad de Paisley, 2005. [2] K. Balog y M. de Rijke.",
        "Encontrando expertos similares.",
        "En este volumen, 2007. [3] K. Balog y M. de Rijke.",
        "Determinación de perfiles de expertos (con una aplicación a la búsqueda de expertos).",
        "En IJCAI 07: Proc. 20th Intern.",
        "Conferencia Conjunta sobre Inteligencia Artificial, páginas 2657-2662, 2007. [4] K. Balog, L. Azzopardi y M. de Rijke.",
        "Modelos formales para la búsqueda de expertos en corporaciones empresariales.",
        "En SIGIR 06: Actas de la 29ª conferencia internacional anual.",
        "Conferencia ACM SIGIR sobre Investigación y desarrollo en recuperación de información, páginas 43-50, 2006. [5] I. Becerra-Fernández.",
        "El papel de las tecnologías de inteligencia artificial en la implementación de sistemas de gestión del conocimiento de localización de personas.",
        "En el taller de AAAI sobre la incorporación de conocimiento en los procesos de negocio, marzo de 2000. [6] C. S. Campbell, P. P. Maglio, A. Cozzi y B. Dom.",
        "Identificación de la experiencia utilizando comunicaciones por correo electrónico.",
        "En CIKM 03: Actas de la duodécima conferencia internacional sobre gestión de la información y el conocimiento, páginas 528-531, 2003. [7] G. Cao, J.-Y.",
        "Nie, y J. Bai.",
        "Integrando relaciones entre palabras en modelos de lenguaje.",
        "En SIGIR 05: Actas de la 28ª conferencia internacional anual.",
        "Conferencia ACM SIGIR sobre Investigación y desarrollo en recuperación de información, páginas 298-305, 2005. [8] T. M. Cover y J.",
        "A. Thomas.",
        "Elementos de la teoría de la información.",
        "Wiley-Interscience, 1991. [9] N. Craswell, D. Hawking, A. M. Vercoustre y P. Wilkins.",
        "Experto en P@noptic: Buscando expertos no solo documentos.",
        "En Ausweb, 2001. [10] N. Craswell, A. de Vries e I. Soboroff.",
        "Resumen de la pista empresarial TREC2005.",
        "En la Decimocuarta Conferencia de Recuperación de Información.",
        "Proc. (TREC 2005), 2006. [11] T. H. Davenport y L. Prusak.",
        "Conocimiento en Acción: Cómo las Organizaciones Gestionan lo que Saben.",
        "Harvard Business School Press, Boston, MA, 1998. [12] T. Dunning.\nHarvard Business School Press, Boston, MA, 1998. [12] T. Dunning.",
        "Métodos precisos para la estadística de sorpresa y coincidencia.",
        "Lingüística Computacional, 19(1):61-74, 1993. [13] E. Filatova y J. Prager.",
        "Dime qué haces y te diré quién eres: Aprendiendo actividades relacionadas con la ocupación para biografías.",
        "En HLT/EMNLP, 2005. [14] V. Lavrenko y W. B. Croft.",
        "Modelos de lenguaje basados en relevancia.",
        "En SIGIR 01: Actas de la 24ª conferencia internacional anual.",
        "Conferencia ACM SIGIR sobre Investigación y Desarrollo en Recuperación de Información, páginas 120-127, 2001. [15] V. Lavrenko, M. Choquette y W. B. Croft.",
        "Modelos de relevancia multilingües.",
        "En SIGIR 02: Actas de la 25ª conferencia internacional anual.",
        "Conferencia ACM SIGIR sobre Investigación y Desarrollo en Recuperación de Información, páginas 175-182, 2002. [16] C. Macdonald e I. Ounis.",
        "Votación de candidatos: adaptando técnicas de fusión de datos para una tarea de búsqueda de expertos.",
        "En CIKM 06: Actas de la 15ª conferencia internacional de ACM sobre gestión de la información y el conocimiento, páginas 387-396, 2006. [17] C. Manning y H. Schütze.",
        "Fundamentos del Procesamiento del Lenguaje Natural Estadístico.",
        "El MIT Press, 1999. [18] A. Mockus y J. D. Herbsleb.",
        "Explorador de experticia: un enfoque cuantitativo para identificar la experticia.",
        "En ICSE 02: Proc. 24th Intern.",
        "Conf. en Ingeniería de Software, páginas 503-512, 2002. [19] D. Petkova y W. B. Croft.",
        "Modelos de lenguaje jerárquicos para la búsqueda de expertos en corporaciones empresariales.",
        "En Proc.",
        "ICTAI 2006, páginas 599-608, 2006. [20] I. Soboroff, A. de Vries y N. Craswell.",
        "Resumen de la pista empresarial TREC 2006.",
        "En las Notas de Trabajo de TREC 2006, 2006. [21] T. Tao, X. Wang, Q. Mei y C. Zhai.",
        "Recuperación de información del modelo de lenguaje con expansión de documentos.",
        "En HLT-NAACL 2006, 2006. [22] TREC.",
        "Ruta empresarial, 2005.",
        "URL: http://www.ins.cwi.nl/projects/trec-ent/wiki/. [23] G. van Noord.\nURL: http://www.ins.cwi.nl/projects/trec-ent/wiki/. [23] G. van Noord.",
        "Adivinador de idioma TextCat.",
        "URL: http://www.let.rug.nl/˜vannoord/TextCat/. [24] W3C.",
        "La colección de pruebas del W3C, 2005.",
        "URL: http://research.microsoft.com/users/nickcr/w3c-summary.html."
    ],
    "error_count": 9,
    "keys": {
        "broad expertise retrieval": {
            "translated_key": "",
            "is_in_text": true,
            "original_annotated_sentences": [
                "<br>broad expertise retrieval</br> in Sparse Data Environments Krisztian Balog ISLA, University of Amsterdam Kruislaan 403, 1098 SJ Amsterdam, The Netherlands kbalog@science.uva.nl Toine Bogers ILK, Tilburg University P.O.",
                "Box 90153, 5000 LE Tilburg, The Netherlands A.M.Bogers@uvt.nl Leif Azzopardi Dept. of Computing Science University of Glasgow, Glasgow, G12 8QQ leif@dcs.gla.ac.uk Maarten de Rijke ISLA, University of Amsterdam Kruislaan 403, 1098 SJ Amsterdam, The Netherlands mdr@science.uva.nl Antal van den Bosch ILK, Tilburg University P.O.",
                "Box 90153, 5000 LE Tilburg, The Netherlands Antal.vdnBosch@uvt.nl ABSTRACT Expertise retrieval has been largely unexplored on data other than the W3C collection.",
                "At the same time, many intranets of universities and other knowledge-intensive organisations offer examples of relatively small but clean multilingual expertise data, covering broad ranges of expertise areas.",
                "We first present two main expertise retrieval tasks, along with a set of baseline approaches based on generative language modeling, aimed at finding expertise relations between topics and people.",
                "For our experimental evaluation, we introduce (and release) a new test set based on a crawl of a university site.",
                "Using this test set, we conduct two series of experiments.",
                "The first is aimed at determining the effectiveness of baseline expertise retrieval methods applied to the new test set.",
                "The second is aimed at assessing refined models that exploit characteristic features of the new test set, such as the organizational structure of the university, and the hierarchical structure of the topics in the test set.",
                "Expertise retrieval models are shown to be robust with respect to environments smaller than the W3C collection, and current techniques appear to be generalizable to other settings.",
                "Categories and Subject Descriptors H.3 [Information Storage and Retrieval]: H.3.1 Content Analysis and Indexing; H.3.3 Information Search and Retrieval; H.3.4 Systems and Software; H.4 [Information Systems Applications]: H.4.2 Types of Systems; H.4.m Miscellaneous General Terms Algorithms, Measurement, Performance, Experimentation 1.",
                "INTRODUCTION An organizations intranet provides a means for exchanging information between employees and for facilitating employee collaborations.",
                "To efficiently and effectively achieve this, it is necessary to provide search facilities that enable employees not only to access documents, but also to identify expert colleagues.",
                "At the TREC Enterprise Track [22] the need to study and understand expertise retrieval has been recognized through the introduction of Expert Finding tasks.",
                "The goal of expert finding is to identify a list of people who are knowledgeable about a given topic.",
                "This task is usually addressed by uncovering associations between people and topics [10]; commonly, a co-occurrence of the name of a person with topics in the same context is assumed to be evidence of expertise.",
                "An alternative task, which using the same idea of people-topic associations, is expert profiling, where the task is to return a list of topics that a person is knowledgeable about [3].",
                "The launch of the Expert Finding task at TREC has generated a lot of interest in expertise retrieval, with rapid progress being made in terms of modeling, algorithms, and evaluation aspects.",
                "However, nearly all of the expert finding or profiling work performed has been validated experimentally using the W3C collection [24] from the Enterprise Track.",
                "While this collection is currently the only publicly available test collection for expertise retrieval tasks, it only represents one type of intranet.",
                "With only one test collection it is not possible to generalize conclusions to other realistic settings.",
                "In this paper we focus on expertise retrieval in a realistic setting that differs from the W3C setting-one in which relatively small amounts of clean, multilingual data are available, that cover a broad range of expertise areas, as can be found on the intranets of universities and other knowledge-intensive organizations.",
                "Typically, this setting features several additional types of structure: topical structure (e.g., topic hierarchies as employed by the organization), organizational structure (faculty, department, ...), as well as multiple types of documents (research and course descriptions, publications, and academic homepages).",
                "This setting is quite different from the W3C setting in ways that might impact upon the performance of expertise retrieval tasks.",
                "We focus on a number of research questions in this paper: Does the relatively small amount of data available on an intranet affect the quality of the topic-person associations that lie at the heart of expertise retrieval algorithms?",
                "How do state-of-the-art algorithms developed on the W3C data set perform in the alternative scenario of the type described above?",
                "More generally, do the lessons from the Expert Finding task at TREC carry over to this setting?",
                "How does the inclusion or exclusion of different documents affect expertise retrieval tasks?",
                "In addition to, how can the topical and organizational structure be used for retrieval purposes?",
                "To answer our research questions, we first present a set of baseline approaches, based on generative language modeling, aimed at finding associations between topics and people.",
                "This allows us to formulate the expert finding and expert profiling tasks in a uniform way, and has the added benefit of allowing us to understand the relations between the two tasks.",
                "For our experimental evaluation, we introduce a new data set (the UvT Expert Collection) which is representative of the type of intranet that we described above.",
                "Our collection is based on publicly available data, crawled from the website of Tilburg University (UvT).",
                "This type of data is particularly interesting, since (1) it is clean, heterogeneous, structured, and focused, but comprises a limited number of documents; (2) contains information on the organizational hierarchy; (3) it is bilingual (English and Dutch); and (4) the list of expertise areas of an individual are provided by the employees themselves.",
                "Using the UvT Expert collection, we conduct two sets of experiments.",
                "The first is aimed at determining the effectiveness of baseline expertise finding and profiling methods in this new setting.",
                "A second group of experiments is aimed at extensions of the baseline methods that exploit characteristic features of the UvT Expert Collection; specifically, we propose and evaluate refined expert finding and profiling methods that incorporate topicality and organizational structure.",
                "Apart from the research questions and data set that we contribute, our main contributions are as follows.",
                "The baseline models developed for expertise finding perform well on the new data set.",
                "While on the W3C setting the expert finding task appears to be more difficult than profiling, for the UvT data the opposite is the case.",
                "We find that profiling on the UvT data set is considerably more difficult than on the W3C set, which we believe is due to the large (but realistic) number of topical areas that we used for profiling: about 1,500 for the UvT set, versus 50 in the W3C case.",
                "Taking the similarity between topics into account can significantly improve retrieval performance.",
                "The best performing similarity measures are content-based, therefore they can be applied on the W3C (and other) settings as well.",
                "Finally, we demonstrate that the organizational structure can be exploited in the form of a context model, improving MAP scores for certain models by up to 70%.",
                "The remainder of this paper is organized as follows.",
                "In the next section we review related work.",
                "Then, in Section 3 we provide detailed descriptions of the expertise retrieval tasks that we address in this paper: expert finding and expert profiling.",
                "In Section 4 we present our baseline models, of which the performance is then assessed in Section 6 using the UvT data set that we introduce in Section 5.",
                "Advanced models exploiting specific features of our data are presented in Section 7 and evaluated in Section 8.",
                "We formulate our conclusions in Section 9. 2.",
                "RELATED WORK Initial approaches to expertise finding often employed databases containing information on the skills and knowledge of each individual in the organization [11].",
                "Most of these tools (usually called yellow pages or people-finding systems) rely on people to self-assess their skills against a predefined set of keywords.",
                "For updating profiles in these systems in an automatic fashion there is a need for intelligent technologies [5].",
                "More recent approaches use specific document sets (such as email [6] or software [18]) to find expertise.",
                "In contrast with focusing on particular document types, there is also an increased interest in the development of systems that index and mine published intranet documents as sources of evidence for expertise.",
                "One such published approach is the P@noptic system [9], which builds a representation of each person by concatenating all documents associated with that person-this is similar to Model 1 of Balog et al. [4], who formalize and compare two methods.",
                "Balog et al.s Model 1 directly models the knowledge of an expert from associated documents, while their Model 2 first locates documents on the topic and then finds the associated experts.",
                "In the reported experiments the second method performs significantly better when there are sufficiently many associated documents per candidate.",
                "Most systems that took part in the 2005 and 2006 editions of the Expert Finding task at TREC implemented (variations on) one of these two models; see [10, 20].",
                "Macdonald and Ounis [16] propose a different approach for ranking candidate expertise with respect to a topic based on data fusion techniques, without using collectionspecific heuristics; they find that applying field-based weighting models improves the ranking of candidates.",
                "Petkova and Croft [19] propose yet another approach, based on a combination of the above Model 1 and 2, explicitly modeling topics.",
                "Turning to other expert retrieval tasks that can also be addressed using topic-people associations, Balog and de Rijke [3] addressed the task of determining topical expert profiles.",
                "While their methods proved to be efficient on the W3C corpus, they require an amount of data that may not be available in the typical knowledge-intensive organization.",
                "Balog and de Rijke [2] study the related task of finding experts that are similar to a small set of experts given as input.",
                "As an aside, creating a textual summary of a person shows some similarities to biography finding, which has received a considerable amount of attention recently; see e.g., [13].",
                "We use generative language modeling to find associations between topics and people.",
                "In our modeling of expert finding and profiling we collect evidence for expertise from multiple sources, in a heterogeneous collection, and integrate it with the co-occurrence of candidates names and query terms-the language modeling setting allows us to do this in a transparent manner.",
                "Our modeling proceeds in two steps.",
                "In the first step, we consider three baseline models, two taken from [4] (the Models 1 and 2 mentioned above), and one a refined version of a model introduced in [3] (which we refer to as Model 3 below); this third model is also similar to the model described by Petkova and Croft [19].",
                "The models we consider in our second round of experiments are mixture models similar to contextual language models [1] and to the expanded documents of Tao et al. [21]; however, the features that we use for definining our expansions-including topical structure and organizational structure-have not been used in this way before. 3.",
                "TASKS In the expertise retrieval scenario that we envisage, users seeking expertise within an organization have access to an interface that combines a search box (where they can search for experts or topics) with navigational structures (of experts and of topics) that allows them to click their way to an expert page (providing the profile of a person) or a topic page (providing a list of experts on the topic).",
                "To feed the above interface, we face two expertise retrieval tasks, expert finding and expert profiling, that we first define and then formalize using generative language models.",
                "In order to model either task, the probability of the query topic being associated to a candidate expert plays a key role in the final estimates for searching and profiling.",
                "By using language models, both the candidates and the query are characterized by distributions of terms in the vocabulary (used in the documents made available by the organization whose expertise retrieval needs we are addressing). 3.1 Expert finding Expert finding involves the task of finding the right person with the appropriate skills and knowledge: Who are the experts on topic X?.",
                "E.g., an employee wants to ascertain who worked on a particular project to find out why particular decisions were made without having to trawl through documentation (if there is any).",
                "Or, they may be in need a trained specialist for consultancy on a specific problem.",
                "Within an organization there are usually many possible candidates who could be experts for given topic.",
                "We can state this problem as follows: What is the probability of a candidate ca being an expert given the query topic q?",
                "That is, we determine p(ca|q), and rank candidates ca according to this probability.",
                "The candidates with the highest probability given the query are deemed the most likely experts for that topic.",
                "The challenge is how to estimate this probability accurately.",
                "Since the query is likely to consist of only a few terms to describe the expertise required, we should be able to obtain a more accurate estimate by invoking Bayes Theorem, and estimating: p(ca|q) = p(q|ca)p(ca) p(q) , (1) where p(ca) is the probability of a candidate and p(q) is the probability of a query.",
                "Since p(q) is a constant, it can be ignored for ranking purposes.",
                "Thus, the probability of a candidate ca being an expert given the query q is proportional to the probability of a query given the candidate p(q|ca), weighted by the a priori belief p(ca) that candidate ca is an expert. p(ca|q) ∝ p(q|ca)p(ca) (2) In this paper our main focus is on estimating the probability of a query given the candidate p(q|ca), because this probability captures the extent to which the candidate knows about the query topic.",
                "Whereas the candidate priors are generally assumed to be uniformand thus will not influence the ranking-it has been demonstrated that a sensible choice of priors may improve the performance [20]. 3.2 Expert profiling While the task of expert searching was concerned with finding experts given a particular topic, the task of expert profiling seeks to answer a related question: What topics does a candidate know about?",
                "Essentially, this turns the questions of expert finding around.",
                "The profiling of an individual candidate involves the identification of areas of skills and knowledge that they have expertise about and an evaluation of the level of proficiency in each of these areas.",
                "This is the candidates topical profile.",
                "Generally, topical profiles within organizations consist of tabular structures which explicitly catalogue the skills and knowledge of each individual in the organization.",
                "However, such practice is limited by the resources available for defining, creating, maintaining, and updating these profiles over time.",
                "By focusing on automatic methods which draw upon the available evidence within the document repositories of an organization, our aim is to reduce the human effort associated with the maintenance of topical profiles1 .",
                "A topical profile of a candidate, then, is defined as a vector where each element i of the vector corresponds to the candidate cas expertise on a given topic ki, (i.e., s(ca, ki)).",
                "Each topic ki defines a particular knowledge area or skill that the organization uses to define the candidates topical profile.",
                "Thus, it is assumed that a list of topics, {k1, . . . , kn}, where n is the number of pre-defined topics, is given: profile(ca) = s(ca, k1), s(ca, k2), . . . , s(ca, kn) . (3) 1 Context and evidence are needed to help users of expertise finding systems to decide whom to contact when seeking expertise in a particular area.",
                "Examples of such context are: Who does she work with?",
                "What are her contact details?",
                "Is she well-connected, just in case she is not able to help us herself?",
                "What is her role in the organization?",
                "Who is her superior?",
                "Collaborators, and affiliations, etc. are all part of the candidates social profile, and can serve as a background against which the systems recommendations should be interpreted.",
                "In this paper we only address the problem of determining topical profiles, and leave social profiling to further work.",
                "We state the problem of quantifying the competence of a person on a certain knowledge area as follows: What is the probability of a knowledge area (ki) being part of the candidates (expertise) profile? where s(ca, ki) is defined by p(ki|ca).",
                "Our task, then, is to estimate p(ki|ca), which is equivalent to the problem of obtaining p(q|ca), where the topic ki is represented as a query topic q, i.e., a sequence of keywords representing the expertise required.",
                "Both the expert finding and profiling tasks rely on the accurate estimation of p(q|ca).",
                "The only difference derives from the prior probability that a person is an expert (p(ca)), which can be incorporated into the expert finding task.",
                "This prior does not apply to the profiling task since the candidate (individual) is fixed. 4.",
                "BASELINE MODELS In this section we describe our baseline models for estimating p(q|ca), i.e., associations between topics and people.",
                "Both expert finding and expert profiling boil down to this estimation.",
                "We employ three models for calculating this probability. 4.1 From topics to candidates Using Candidate Models: Model 1 Model 1 [4] defines the probability of a query given a candidate (p(q|ca)) using standard language modeling techniques, based on a multinomial unigram language model.",
                "For each candidate ca, a candidate language model θca is inferred such that the probability of a term given θca is nonzero for all terms, i.e., p(t|θca) > 0.",
                "From the candidate model the query is generated with the following probability: p(q|θca) = Y t∈q p(t|θca)n(t,q) , where each term t in the query q is sampled identically and independently, and n(t, q) is the number of times t occurs in q.",
                "The candidate language model is inferred as follows: (1) an empirical model p(t|ca) is computed; (2) it is smoothed with background probabilities.",
                "Using the associations between a candidate and a document, the probability p(t|ca) can be approximated by: p(t|ca) = X d p(t|d)p(d|ca), where p(d|ca) is the probability that candidate ca generates a supporting document d, and p(t|d) is the probability of a term t occurring in the document d. We use the maximum-likelihood estimate of a term, that is, the normalised frequency of the term t in document d. The strength of the association between document d and candidate ca expressed by p(d|ca) reflects the degree to which the candidates expertise is described using this document.",
                "The estimation of this probability is presented later, in Section 4.2.",
                "The candidate model is then constructed as a linear interpolation of p(t|ca) and the background model p(t) to ensure there are no zero probabilities, which results in the final estimation: p(q|θca) = (4) Y t∈q ( (1 − λ) X d p(t|d)p(d|ca) ! + λp(t) )n(t,q) .",
                "Model 1 amasses all the term information from all the documents associated with the candidate, and uses this to represent that candidate.",
                "This model is used to predict how likely a candidate would produce a query q.",
                "This can can be intuitively interpreted as the probability of this candidate talking about the query topic, where we assume that this is indicative of their expertise.",
                "Using Document Models: Model 2 Model 2 [4] takes a different approach.",
                "Here, the process is broken into two parts.",
                "Given a candidate ca, (1) a document that is associated with a candidate is selected with probability p(d|ca), and (2) from this document a query q is generated with probability p(q|d).",
                "Then the sum over all documents is taken to obtain p(q|ca), such that: p(q|ca) = X d p(q|d)p(d|ca). (5) The probability of a query given a document is estimated by inferring a document language model θd for each document d in a similar manner as the candidate model was inferred: p(t|θd) = (1 − λ)p(t|d) + λp(t), (6) where p(t|d) is the probability of the term in the document.",
                "The probability of a query given the document model is: p(q|θd) = Y t∈q p(t|θd)n(t,q) .",
                "The final estimate of p(q|ca) is obtained by substituting p(q|d) for p(q|θd) into Eq. 5 (see [4] for full details).",
                "Conceptually, Model 2 differs from Model 1 because the candidate is not directly modeled.",
                "Instead, the document acts like a hidden variable in the process which separates the query from the candidate.",
                "This process is akin to how a user may search for candidates with a standard search engine: initially by finding the documents which are relevant, and then seeing who is associated with that document.",
                "By examining a number of documents the user can obtain an idea of which candidates are more likely to discuss the topic q.",
                "Using Topic Models: Model 3 We introduce a third model, Model 3.",
                "Instead of attempting to model the query generation process via candidate or document models, we represent the query as a topic language model and directly estimate the probability of the candidate p(ca|q).",
                "This approach is similar to the model presented in [3, 19].",
                "As with the previous models, a language model is inferred, but this time for the query.",
                "We adapt the work of Lavrenko and Croft [14] to estimate a topic model from the query.",
                "The procedure is as follows.",
                "Given a collection of documents and a query topic q, it is assumed that there exists an unknown topic model θk that assigns probabilities p(t|θk) to the term occurrences in the topic documents.",
                "Both the query and the documents are samples from θk (as opposed to the previous approaches, where a query is assumed to be sampled from a specific document or candidate model).",
                "The main task is to estimate p(t|θk), the probability of a term given the topic model.",
                "Since the query q is very sparse, and as there are no examples of documents on the topic, this distribution needs to be approximated.",
                "Lavrenko and Croft [14] suggest a reasonable way of obtaining such an approximation, by assuming that p(t|θk) can be approximated by the probability of term t given the query q.",
                "We can then estimate p(t|q) using the joint probability of observing the term t together with the query terms, q1, . . . , qm, and dividing by the joint probability of the query terms: p(t|θk) ≈ p(t|q) = p(t, q1, . . . , qm) p(q1, . . . , qm) = p(t, q1, . . . , qm) P t ∈T p(t , q1, . . . , qm) , where p(q1, . . . , qm) = P t ∈T p(t , q1, . . . , qm), and T is the entire vocabulary of terms.",
                "In order to estimate the joint probability p(t, q1, . . . , qm), we follow [14, 15] and assume t and q1, . . . , qm are mutually independent, once we pick a source distribution from the set of underlying source distributions U.",
                "If we choose U to be a set of document models. then to construct this set, the query q would be issued against the collection, and the top n returned are assumed to be relevant to the topic, and thus treated as samples from the topic model. (Note that candidate models could be used instead.)",
                "With the document models forming U, the joint probability of term and query becomes: p(t, q1, . . . , qm) = X d∈U p(d) ˘ p(t|θd) mY i=1 p(qi|θd) ¯ . (7) Here, p(d) denotes the prior distribution over the set U, which reflects the relevance of the document to the topic.",
                "We assume that p(d) is uniform across U.",
                "In order to rank candidates according to the topic model defined, we use the Kullback-Leibler divergence metric (KL, [8]) to measure the difference between the candidate models and the topic model: KL(θk||θca) = X t p(t|θk) log p(t|θk) p(t|θca) . (8) Candidates with a smaller divergence from the topic model are considered to be more likely experts on that topic.",
                "The candidate model θca is defined in Eq. 4.",
                "By using KL divergence instead of the probability of a candidate given the topic model p(ca|θk), we avoid normalization problems. 4.2 Document-candidate associations For our models we need to be able to estimate the probability p(d|ca), which expresses the extent to which a document d characterizes the candidate ca.",
                "In [4], two methods are presented for estimating this probability, based on the number of person names recognized in a document.",
                "However, in our (intranet) setting it is reasonable to assume that authors of documents can unambiguously be identified (e.g., as the author of an article, the teacher assigned to a course, the owner of a web page, etc.)",
                "Hence, we set p(d|ca) to be 1 if candidate ca is author of document d, otherwise the probability is 0.",
                "In Section 6 we describe how authorship can be determined on different types of documents within the collection. 5.",
                "THE UVT EXPERT COLLECTION The UvT Expert collection used in the experiments in this paper fits the scenario outlined in Section 3.",
                "The collection is based on the Webwijs (Webwise) system developed at Tilburg University (UvT) in the Netherlands.",
                "Webwijs (http://www.uvt.nl/ webwijs/) is a publicly accessible database of UvT employees who are involved in research or teaching; currently, Webwijs contains information about 1168 experts, each of whom has a page with contact information and, if made available by the expert, a research description and publications list.",
                "In addition, each expert can select expertise areas from a list of 1491 topics and is encouraged to suggest new topics that need to be approved by the Webwijs editor.",
                "Each topic has a separate page that shows all experts associated with that topic and, if available, a list of related topics.",
                "Webwijs is available in Dutch and English, and this bilinguality has been preserved in the collection.",
                "Every Dutch Webwijs page has an English translation.",
                "Not all Dutch topics have an English translation, but the reverse is true: the 981 English topics all have a Dutch equivalent.",
                "About 42% of the experts teach courses at Tilburg University; these courses were also crawled and included in the profile.",
                "In addition, about 27% of the experts link to their academic homepage from their Webwijs page.",
                "These home pages were crawled and added to the collection. (This means that if experts put the full-text versions of their publications on their academic homepage, these were also available for indexing.)",
                "We also obtained 1880 full-text versions of publications from the UvT institutional repository and Dutch English no. of experts 1168 1168 no. of experts with ≥ 1 topic 743 727 no. of topics 1491 981 no. of expert-topic pairs 4318 3251 avg. no. of topics/expert 5.8 5.9 max. no. of topics/expert (no. of experts) 60 (1) 35 (1) min. no. of topics/expert (no. of experts) 1 (74) 1 (106) avg. no. of experts/topic 2.9 3.3 max. no. of experts/topic (no. of topics) 30 (1) 30 (1) min. no. of experts/topic (no. of topics) 1 (615) 1 (346) no. of experts with HP 318 318 no. of experts with CD 318 318 avg. no. of CDs per teaching expert 3.5 3.5 no. of experts with RD 329 313 no. of experts with PUB 734 734 avg. no. of PUBs per expert 27.0 27.0 avg. no. of PUB citations per expert 25.2 25.2 avg. no. of full-text PUBs per expert 1.8 1.8 Table 2: Descriptive statistics of the Dutch and English versions of the UvT Expert collection. converted them to plain text.",
                "We ran the TextCat [23] language identifier to classify the language of the home pages and the fulltext publications.",
                "We restricted ourselves to pages where the classifier was confident about the language used on the page.",
                "This resulted in four document types: research descriptions (RD), course descriptions (CD), publications (PUB; full-text and citationonly versions), and academic homepages (HP).",
                "Everything was bundled into the UvT Expert collection which is available at http: //ilk.uvt.nl/uvt-expert-collection/.",
                "The UvT Expert collection was extracted from a different organizational setting than the W3C collection and differs from it in a number of ways.",
                "The UvT setting is one with relatively small amounts of multilingual data.",
                "Document-author associations are clear and the data is structured and clean.",
                "The collection covers a broad range of expertise areas, as one can typically find on intranets of universities and other knowledge-intensive institutes.",
                "Additionally, our university setting features several types of structure (topical and organizational), as well as multiple document types.",
                "Another important difference between the two data sets is that the expertise areas in the UvT Expert collection are self-selected instead of being based on group membership or assignments by others.",
                "Size is another dimension along which the W3C and UvT Expert collections differ: the latter is the smaller of the two.",
                "Also realistic are the large differences in the amount of information available for each expert.",
                "Utilizing Webwijs is voluntary; 425 Dutch experts did not select any topics at all.",
                "This leaves us with 743 Dutch and 727 English usable expert profiles.",
                "Table 2 provides descriptive statistics for the UvT Expert collection.",
                "Universities tend to have a hierarchical structure that goes from the faculty level, to departments, research groups, down to the individual researchers.",
                "In the UvT Expert collection we have information about the affiliations of researchers with faculties and institutes, providing us with a two-level organizational hierarchy.",
                "Tilburg University has 22 organizational units at the faculty level (including the university office and several research institutes) and 71 departments, which amounts to 3.2 departments per faculty.",
                "As to the topical hierarchy used by Webwijs, 131 of the 1491 topics are top nodes in the hierarchy.",
                "This hierarchy has an average topic chain length of 2.65 and a maximum length of 7 topics. 6.",
                "EVALUATION Below, we evaluate Section 4s models for expert finding and profiling onthe UvT Expert collection.",
                "We detail our research questions and experimental setup, and then present our results. 6.1 Research Questions We address the following research questions.",
                "Both expert finding and profiling rely on the estimations of p(q|ca).",
                "The question is how the models compare on the different tasks, and in the setting of the UvT Expert collection.",
                "In [4], Model 2 outperformed Model 1 on the W3C collection.",
                "How do they compare on our data set?",
                "And how does Model 3 compare to Model 1?",
                "What about performance differences between the two languages in our test collection? 6.2 Experimental Setup The output of our models was evaluated against the self-assigned topic labels, which were treated as relevance judgements.",
                "Results were evaluated separately for English and Dutch.",
                "For English we only used topics for which the Dutch translation was available; for Dutch all topics were considered.",
                "The results were averaged for the queries in the intersection of relevance judgements and results; missing queries do not contribute a value of 0 to the scores.",
                "We use standard information retrieval measures, such as Mean Average Precision (MAP) and Mean Reciprocal Rank (MRR).",
                "We also report the percentage of topics (%q) and candidates (%ca) covered, for the expert finding and profiling tasks, respectively. 6.3 Results Table 1 shows the performance of Model 1, 2, and 3 on the expert finding and profiling tasks.",
                "The rows of the table correspond to the various document types (RD, CD, PUB, and HP) and to their combinations.",
                "RD+CD+PUB+HP is equivalent to the full collection and will be referred as the BASELINE of our experiments.",
                "Looking at Table 1 we see that Model 2 performs the best across the board.",
                "However, when the data is clean and very focused (RD), Model 3 outperforms it in a number of cases.",
                "Model 1 has the best coverage of candidates (%ca) and topics (%q).",
                "The various document types differ in their characteristics and how they improve the finding and profiling tasks.",
                "Expert profiling benefits much from the clean data present in the RD and CD document types, while the publications contribute the most to the expert finding task.",
                "Adding the homepages does not prove to be particularly useful.",
                "When we compare the results across languages, we find that the coverage of English topics (%q) is higher than of the Dutch ones for expert finding.",
                "Apart from that, the scores fall in the same range for both languages.",
                "For the profiling task the coverage of the candidates (%ca) is very similar for both languages.",
                "However, the performance is substantially better for the English topics.",
                "While it is hard to compare scores across collections, we conclude with a brief comparison of the absolute scores in Table 1 to those reported in [3, 4] on the W3C test set (2005 edition).",
                "For expert finding the MAP scores for Model 2 reported here are about 50% higher than the corresponding figures in [4], while our MRR scores are slightly below those in [4].",
                "For expert profiling, the differences are far more dramatic: the MAP scores for Model 2 reported here are around 50% below the scores in [3], while the (best) MRR scores are about the same as those in [3].",
                "The cause for the latter differences seems to reside in the number of knowledge areas considered here-approx. 30 times more than in the W3C setting. 7.",
                "ADVANCED MODELS Now that we have developed and assessed basic language modeling techniques for expertise retrieval, we turn to refined models that exploit special features of our test collection. 7.1 Exploiting knowledge area similarity One way to improve the scoring of a query given a candidate is to consider what other requests the candidate would satisfy and use them as further evidence to support the original query, proportional Expert finding Expert profiling Document types Model 1 Model 2 Model 3 Model 1 Model 2 Model 3 %q MAP MRR %q MAP MRR %q MAP MRR %ca MAP MRR %ca MAP MRR %ca MAP MRR English RD 97.8 0.126 0.269 83.5 0.144 0.311 83.3 0.129 0.271 100 0.089 0.189 39.3 0.232 0.465 41.1 0.166 0.337 CD 97.8 0.118 0.227 91.7 0.123 0.248 91.7 0.118 0.226 32.8 0.188 0.381 32.4 0.195 0.385 32.7 0.203 0.370 PUB 97.8 0.200 0.330 98.0 0.216 0.372 98.0 0.145 0.257 78.9 0.167 0.364 74.5 0.212 0.442 78.9 0.135 0.299 HP 97.8 0.081 0.186 97.4 0.071 0.168 97.2 0.062 0.149 31.2 0.150 0.299 28.8 0.185 0.335 30.1 0.136 0.287 RD+CD 97.8 0.188 0.352 92.9 0.193 0.360 92.9 0.150 0.273 100 0.145 0.286 61.3 0.251 0.477 63.2 0.217 0.416 RD+CD+PUB 97.8 0.235 0.373 98.1 0.277 0.439 98.1 0.178 0.305 100 0.196 0.380 87.2 0.280 0.533 89.5 0.170 0.344 RD+CD+PUB+HP 97.8 0.237 0.372 98.6 0.280 0.441 98.5 0.166 0.293 100 0.199 0.387 88.7 0.281 0.525 90.9 0.169 0.329 Dutch RD 61.3 0.094 0.229 38.4 0.137 0.336 38.3 0.127 0.295 38.0 0.127 0.386 34.1 0.138 0.420 38.0 0.105 0.327 CD 61.3 0.107 0.212 49.7 0.128 0.256 49.7 0.136 0.261 32.5 0.151 0.389 31.8 0.158 0.396 32.5 0.170 0.380 PUB 61.3 0.193 0.319 59.5 0.218 0.368 59.4 0.173 0.291 78.8 0.126 0.364 76.0 0.150 0.424 78.8 0.103 0.294 HP 61.3 0.063 0.169 56.6 0.064 0.175 56.4 0.062 0.163 29.8 0.108 0.308 27.8 0.125 0.338 29.8 0.098 0.255 RD+CD 61.3 0.159 0.314 51.9 0.184 0.360 51.9 0.169 0.324 60.5 0.151 0.410 57.2 0.166 0.431 60.4 0.159 0.384 RD+CD+PUB 61.3 0.244 0.398 61.5 0.260 0.424 61.4 0.210 0.350 90.3 0.165 0.445 88.2 0.189 0.479 90.3 0.126 0.339 RD+CD+PUB+HP 61.3 0.249 0.401 62.6 0.265 0.436 62.6 0.195 0.344 91.9 0.164 0.426 90.1 0.195 0.488 91.9 0.125 0.328 Table 1: Performance of the models on the expert finding and profiling tasks, using different document types and their combinations. %q is the number of topics covered (applies to the expert finding task), %ca is the number of candidates covered (applies to the expert profiling task).",
                "The top and bottom blocks correspond to English and Dutch respectively.",
                "The best scores are in boldface. to how related the other requests are to the original query.",
                "This can be modeled by interpolating between the p(q|ca) and the further supporting evidence from all similar requests q , as follows: p (q|ca) = λp(q|ca) + (1 − λ) X q p(q|q )p(q |ca), (9) where p(q|q ) represents the similarity between the two topics q and q .",
                "To be able to work with similarity methods that are not necessarily probabilities, we set p(q|q ) = w(q,q ) γ , where γ is a normalizing constant, such that γ = P q w(q , q ).",
                "We consider four methods for calculating the similarity score between two topics.",
                "Three approaches are strictly content-based, and establish similarity by examining co-occurrence patterns of topics within the collection, while the last approach exploits the hierarchical structure of topical areas that may be present within an organization (see [7] for further examples of integrating word relationships into language models).",
                "The Kullback-Leibler (KL) divergence metric defined in Eq. 8 provides a measure of how different or similar two probability distributions are.",
                "A topic model is inferred for q and q using the method presented in Section 4.1 to describe the query across the entire vocabulary.",
                "Since a lower KL score means the queries are more similar, we let w(q, q ) = max(KL(θq||·) − KL(θq||θq )).",
                "Pointwise Mutual Information (PMI, [17]) is a measure of association used in information theory to determine the extent of independence between variables.",
                "The dependence between two queries is reflected by the SI(q, q ) score, where scores greater than zero indicate that it is likely that there is a dependence, which we take to mean that the queries are likely to be similar: SI(q, q ) = log p(q, q ) p(q)p(q ) (10) We estimate the probability of a topic p(q) using the number of documents relevant to query q within the collection.",
                "The joint probability p(q, q ) is estimated similarly, by using the concatenation of q and q as a query.",
                "To obtain p(q|q ), we then set w(q, q ) = SI(q, q ) when SI(q, q ) > 0 otherwise w(q, q ) = 0, because we are only interested in including queries that are similar.",
                "The log-likelihood statistic provides another measure of dependence, which is more reliable than the pointwise mutual information measure [17].",
                "Let k1 be the number of co-occurrences of q and q , k2 the number of occurrences of q not co-occurring with q , n1 the total number of occurrences of q , and n2 the total number of topic tokens minus the number of occurrences of q .",
                "Then, let p1 = k1/n1, p2 = k2/n2, and p = (k1 + k2)/(n1 + n2), (q, q ) = 2( (p1, k1, n1) + (p2, k2, n2) − (p, k1, n1) − (p, k2, n2)), where (p, n, k) = k log p + (n − k) log(1 − p).",
                "The higher score indicate that queries are also likely to be similar, thus we set w(q, q ) = (q, q ).",
                "Finally, we also estimate the similarity of two topics based on their distance within the topic hierarchy.",
                "The topic hierarchy is viewed as a directed graph, and for all topic-pairs the shortest path SP(q, q ) is calculated.",
                "We set the similarity score to be the reciprocal of the shortest path: w(q, q ) = 1/SP(q, q ). 7.2 Contextual information Given the hierarchy of an organization, the units to which a person belong are regarded as a context so as to compensate for data sparseness.",
                "We model it as follows: p (q|ca) = 1 − P ou∈OU(ca) λou · p(q|ca) + P ou∈OU(ca) λou · p(q|ou), where OU(ca) is the set of organizational units of which candidate ca is a member of, and p(q|o) expresses the strength of the association between query q and the unit ou.",
                "The latter probability can be estimated using either of the three basic models, by simply replacing ca with ou in the corresponding equations.",
                "An organizational unit is associated with all the documents that its members have authored.",
                "That is, p(d|ou) = maxca∈ou p(d|ca). 7.3 A simple multilingual model For knowledge institutes in Europe, academic or otherwise, a multilingual (or at least bilingual) setting is typical.",
                "The following model builds on a kind of independence assumption: there is no spill-over of expertise/profiles across language boundaries.",
                "While a simplification, this is a sensible first approach.",
                "That is: p (q|ca) =P l∈L λl · p(ql|ca), where L is the set of languages used in the collection, ql is the translation of the query q to language l, and λl is a language specific smoothing parameter, such that P l∈L λl = 1. 8.",
                "ADVANCED MODELS: EVALUATION In this section we present an experimental evaluation of our advanced models.",
                "Expert finding Expert profiling Language Model 1 Model 2 Model 3 Model 1 Model 2 Model 3 %q MAP MRR %q MAP MRR %q MAP MRR %ca MAP MRR %ca MAP MRR %ca MAP MRR English only 97.8 0.237 0.372 98.6 0.280 0.441 98.5 0.166 0.293 100 0.199 0.387 88.7 0.281 0.525 90.9 0.169 0.329 Dutch only 61.3 0.249 0.401 62.6 0.265 0.436 62.6 0.195 0.344 91.9 0.164 0.426 90.1 0.195 0.488 91.9 0.125 0.328 Combination 99.4 0.297 0.444 99.7 0.324 0.491 99.7 0.223 0.388 100 0.241 0.445 92.1 0.313 0.564 93.2 0.224 0.411 Table 3: Performance of the combination of languages on the expert finding and profiling tasks (on candidates).",
                "Best scores for each model are in italic, absolute best scores for the expert finding and profiling tasks are in boldface.",
                "Method Model 1 Model 2 Model 3 MAP MRR MAP MRR MAP MRR English BASELINE 0.296 0.454 0.339 0.509 0.221 0.333 KLDIV 0.291 0.453 0.327 0.503 0.219 0.330 PMI 0.291 0.453 0.337 0.509 0.219 0.331 LL 0.319 0.490 0.360 0.524 0.233 0.368 HDIST 0.299 0.465 0.346 0.537 0.219 0.332 Dutch BASELINE 0.240 0.350 0.271 0.403 0.227 0.389 KLDIV 0.239 0.347 0.253 0.386 0.224 0.385 PMI 0.239 0.350 0.260 0.392 0.227 0.389 LL 0.255 0.372 0.281 0.425 0.231 0.389 HDIST 0.253 0.365 0.271 0.407 0.236 0.402 Method Model 1 Model 2 Model 3 MAP MRR MAP MRR MAP MRR English BASELINE 0.485 0.546 0.499 0.548 0.381 0.416 KLDIV 0.510 0.564 0.513 0.558 0.381 0.416 PMI 0.486 0.546 0.495 0.542 0.407 0.451 LL 0.558 0.589 0.586 0.617 0.408 0.453 HDIST 0.507 0.567 0.512 0.563 0.386 0.420 Dutch BASELINE 0.263 0.313 0.294 0.358 0.262 0.315 KLDIV 0.284 0.336 0.271 0.321 0.261 0.314 PMI 0.265 0.317 0.265 0.316 0.273 0.330 LL 0.312 0.351 0.330 0.377 0.284 0.331 HDIST 0.280 0.327 0.288 0.341 0.266 0.321 Table 4: Performance on the expert finding (top) and profiling (bottom) tasks, using knowledge area similarities.",
                "Runs were evaluated on the main topics set.",
                "Best scores are in boldface. 8.1 Research Questions Our questions follow the refinements presented in the preceding section: Does exploiting the knowledge area similarity improve effectiveness?",
                "Which of the various methods for capturing word relationships is most effective?",
                "Furthermore, is our way of bringing in contextual information useful?",
                "For which tasks?",
                "And finally, is our simple way of combining the monolingual scores sufficient for obtaining significant improvements? 8.2 Experimental setup Given that the self-assessments are also sparse in our collection, in order to be able to measure differences between the various models, we selected a subset of topics, and evaluated (some of the) runs only on this subset.",
                "This set is referred as main topics, and consists of topics that are located at the top level of the topical hierarchy. (A main topic has subtopics, but is not a subtopic of any other topic.)",
                "This main set consists of 132 Dutch and 119 English topics.",
                "The relevance judgements were restricted to the main topic set, but were not expanded with subtopics. 8.3 Exploiting knowledge area similarity Table 4 presents the results.",
                "The four methods used for estimating knowledge-area similarity are KL divergence (KLDIV), PointLang.",
                "Topics Model 1 Model 2 Model 3 MAP MRR MAP MRR MAP MRR Expert finding UK ALL 0.423 0.545 0.654 0.799 0.494 0.629 UK MAIN 0.500 0.621 0.704 0.834 0.587 0.699 NL ALL 0.439 0.560 0.672 0.826 0.480 0.630 NL MAIN 0.440 0.584 0.645 0.816 0.515 0.655 Expert profiling UK ALL 0.240 0.640 0.306 0.778 0.223 0.616 UK MAIN 0.523 0.677 0.519 0.648 0.461 0.587 NL ALL 0.203 0.716 0.254 0.770 0.183 0.627 NL MAIN 0.332 0.576 0.380 0.624 0.332 0.549 Table 5: Evaluating the context models on organizational units. wise mutual information (PMI), log-likelihood (LL), and distance within topic hierarchy (HDIST).",
                "We managed to improve upon the baseline in all cases, but the improvement is more noticeable for the profiling task.",
                "For both tasks, the LL method performed best.",
                "The content-based approaches performed consistently better than HDIST. 8.4 Contextual information A two level hierarchy of organizational units (faculties and institutes) is available in the UvT Expert collection.",
                "The unit a person belongs to is used as a context for that person.",
                "First, we evaluated the models of the organizational units, using all topics (ALL) and only the main topics (MAIN).",
                "An organizational unit is considered to be relevant for a given topic (or vice versa) if at least one member of the unit selected the given topic as an expertise area.",
                "Table 5 reports on the results.",
                "As far as expert finding goes, given a topic, the corresponding organizational unit can be identified with high precision.",
                "However, the expert profiling task shows a different picture: the scores are low, and the task seems hard.",
                "The explanation may be that general concepts (i.e., our main topics) may belong to several organizational units.",
                "Second, we performed another evaluation, where we combined the contextual models with the candidate models (to score candidates again).",
                "Table 6 reports on the results.",
                "We find a positive impact of the context models only for expert finding.",
                "Noticably, for expert finding (and Model 1), it improves over 50% (for English) and over 70% (for Dutch) on MAP.",
                "The poor performance on expert profiling may be due to the fact that context models alone did not perform very well on the profiling task to begin with. 8.5 Multilingual models In this subsection we evaluate the method for combining results across multiple languages that we described in Section 7.3.",
                "In our setting the set of languages consists of English and Dutch: L = {UK, NL}.",
                "The weights on these languages were set to be identical (λUK = λNL = 0.5).",
                "We performed experiments with various λ settings, but did not observe significant differences in performance.",
                "Table 3 reports on the multilingual results, where performance is evaluated on the full topic set.",
                "All three models significantly imLang.",
                "Method Model 1 Model 2 Model 3 MAP MRR MAP MRR MAP MRR Expert finding UK BL 0.296 0.454 0.339 0.509 0.221 0.333 UK CT 0.330 0.491 0.342 0.500 0.228 0.342 NL BL 0.240 0.350 0.271 0.403 0.227 0.389 NL CT 0.251 0.382 0.267 0.410 0.246 0.404 Expert profiling UK BL 0.485 0.546 0.499 0.548 0.381 0.416 UK CT 0.562 0.620 0.508 0.558 0.440 0.486 NL BL 0.263 0.313 0.294 0.358 0.262 0.315 NL CT 0.330 0.384 0.317 0.387 0.294 0.345 Table 6: Performance of the context models (CT) compared to the baseline (BL).",
                "Best scores are in boldface. proved over all measures for both tasks.",
                "The coverage of topics and candidates for the expert finding and profiling tasks, respectively, is close to 100% in all cases.",
                "The relative improvement of the precision scores ranges from 10% to 80%.",
                "These scores demonstrate that despite its simplicity, our method for combining results over multiple languages achieves substantial improvements over the baseline. 9.",
                "CONCLUSIONS In this paper we focused on expertise retrieval (expert finding and profiling) in a new setting of a typical knowledge-intensive organization in which the available data is of high quality, multilingual, and covering a broad range of expertise area.",
                "Typically, the amount of available data in such an organization (e.g., a university, a research institute, or a research lab) is limited when compared to the W3C collection that has mostly been used for the experimental evaluation of expertise retrieval so far.",
                "To examine expertise retrieval in this setting, we introduced (and released) the UvT Expert collection as a representative case of such knowledge intensive organizations.",
                "The new collection reflects the typical properties of knowledge-intensive institutes noted above and also includes several features which may are potentially useful for expertise retrieval, such as topical and organizational structure.",
                "We evaluated how current state-of-the-art models for expert finding and profiling performed in this new setting and then refined these models in order to try and exploit the different characteristics within the data environment (language, topicality, and organizational structure).",
                "We found that current models of expertise retrieval generalize well to this new environment; in addition we found that refining the models to account for the differences results in significant improvements, thus making up for problems caused by data sparseness issues.",
                "Future work includes setting up manual assessments of automatically generated profiles by the employees themselves, especially in cases where the employees have not provided a profile themselves. 10.",
                "ACKNOWLEDGMENTS Krisztian Balog was supported by the Netherlands Organisation for Scientific Research (NWO) under project number 220-80-001.",
                "Maarten de Rijke was also supported by NWO under project numbers 017.001.190, 220-80-001, 264-70-050, 354-20-005, 600.065.120, 612-13-001, 612.000.106, 612.066.302, 612.069.006, 640.001.501, 640.002.501, and by the E.U.",
                "IST programme of the 6th FP for RTD under project MultiMATCH contract IST-033104.",
                "The work of Toine Bogers and Antal van den Bosch was funded by the IOP-MMI-program of SenterNovem / The Dutch Ministry of Economic Affairs, as part of the `A Propos project. 11.",
                "REFERENCES [1] L. Azzopardi.",
                "Incorporating Context in the Language Modeling Framework for ad-hoc Information Retrieval.",
                "PhD thesis, University of Paisley, 2005. [2] K. Balog and M. de Rijke.",
                "Finding similar experts.",
                "In This volume, 2007. [3] K. Balog and M. de Rijke.",
                "Determining expert profiles (with an application to expert finding).",
                "In IJCAI 07: Proc. 20th Intern.",
                "Joint Conf. on Artificial Intelligence, pages 2657-2662, 2007. [4] K. Balog, L. Azzopardi, and M. de Rijke.",
                "Formal models for expert finding in enterprise corpora.",
                "In SIGIR 06: Proc. 29th annual intern.",
                "ACM SIGIR conf. on Research and development in information retrieval, pages 43-50, 2006. [5] I. Becerra-Fernandez.",
                "The role of artificial intelligence technologies in the implementation of people-finder knowledge management systems.",
                "In AAAI Workshop on Bringing Knowledge to Business Processes, March 2000. [6] C. S. Campbell, P. P. Maglio, A. Cozzi, and B. Dom.",
                "Expertise identification using email communications.",
                "In CIKM 03: Proc. twelfth intern. conf. on Information and knowledge management, pages 528531, 2003. [7] G. Cao, J.-Y.",
                "Nie, and J. Bai.",
                "Integrating word relationships into language models.",
                "In SIGIR 05: Proc. 28th annual intern.",
                "ACM SIGIR conf. on Research and development in information retrieval, pages 298-305, 2005. [8] T. M. Cover and J.",
                "A. Thomas.",
                "Elements of Information Theory.",
                "Wiley-Interscience, 1991. [9] N. Craswell, D. Hawking, A. M. Vercoustre, and P. Wilkins.",
                "P@noptic expert: Searching for experts not just for documents.",
                "In Ausweb, 2001. [10] N. Craswell, A. de Vries, and I. Soboroff.",
                "Overview of the TREC2005 Enterprise Track.",
                "In The Fourteenth Text REtrieval Conf.",
                "Proc. (TREC 2005), 2006. [11] T. H. Davenport and L. Prusak.",
                "Working Knowledge: How Organizations Manage What They Know.",
                "Harvard Business School Press, Boston, MA, 1998. [12] T. Dunning.",
                "Accurate methods for the statistics of surprise and coincidence.",
                "Computational Linguistics, 19(1):61-74, 1993. [13] E. Filatova and J. Prager.",
                "Tell me what you do and Ill tell you what you are: Learning occupation-related activities for biographies.",
                "In HLT/EMNLP, 2005. [14] V. Lavrenko and W. B. Croft.",
                "Relevance based language models.",
                "In SIGIR 01: Proc. 24th annual intern.",
                "ACM SIGIR conf. on Research and development in information retrieval, pages 120-127, 2001. [15] V. Lavrenko, M. Choquette, and W. B. Croft.",
                "Cross-lingual relevance models.",
                "In SIGIR 02: Proc. 25th annual intern.",
                "ACM SIGIR conf. on Research and development in information retrieval, pages 175-182, 2002. [16] C. Macdonald and I. Ounis.",
                "Voting for candidates: adapting data fusion techniques for an expert search task.",
                "In CIKM 06: Proc. 15th ACM intern. conf. on Information and knowledge management, pages 387-396, 2006. [17] C. Manning and H. Sch¨utze.",
                "Foundations of Statistical Natural Language Processing.",
                "The MIT Press, 1999. [18] A. Mockus and J. D. Herbsleb.",
                "Expertise browser: a quantitative approach to identifying expertise.",
                "In ICSE 02: Proc. 24th Intern.",
                "Conf. on Software Engineering, pages 503-512, 2002. [19] D. Petkova and W. B. Croft.",
                "Hierarchical language models for expert finding in enterprise corpora.",
                "In Proc.",
                "ICTAI 2006, pages 599-608, 2006. [20] I. Soboroff, A. de Vries, and N. Craswell.",
                "Overview of the TREC 2006 Enterprise Track.",
                "In TREC 2006 Working Notes, 2006. [21] T. Tao, X. Wang, Q. Mei, and C. Zhai.",
                "Language model information retrieval with document expansion.",
                "In HLT-NAACL 2006, 2006. [22] TREC.",
                "Enterprise track, 2005.",
                "URL: http://www.ins.cwi. nl/projects/trec-ent/wiki/. [23] G. van Noord.",
                "TextCat Language Guesser.",
                "URL: http://www. let.rug.nl/˜vannoord/TextCat/. [24] W3C.",
                "The W3C test collection, 2005.",
                "URL: http://research. microsoft.com/users/nickcr/w3c-summary.html."
            ],
            "original_annotated_samples": [
                "<br>broad expertise retrieval</br> in Sparse Data Environments Krisztian Balog ISLA, University of Amsterdam Kruislaan 403, 1098 SJ Amsterdam, The Netherlands kbalog@science.uva.nl Toine Bogers ILK, Tilburg University P.O."
            ],
            "translated_annotated_samples": [
                "Recuperación de amplia experiencia en entornos de datos dispersos Krisztian Balog ISLA, Universidad de Ámsterdam Kruislaan 403, 1098 SJ Ámsterdam, Países Bajos kbalog@science.uva.nl Toine Bogers ILK, Universidad de Tilburg P.O."
            ],
            "translated_text": "Recuperación de amplia experiencia en entornos de datos dispersos Krisztian Balog ISLA, Universidad de Ámsterdam Kruislaan 403, 1098 SJ Ámsterdam, Países Bajos kbalog@science.uva.nl Toine Bogers ILK, Universidad de Tilburg P.O. Caja 90153, 5000 LE Tilburg, Países Bajos A.M.Bogers@uvt.nl Leif Azzopardi Dept. de Ciencias de la Computación Universidad de Glasgow, Glasgow, G12 8QQ leif@dcs.gla.ac.uk Maarten de Rijke ISLA, Universidad de Ámsterdam Kruislaan 403, 1098 SJ Ámsterdam, Países Bajos mdr@science.uva.nl Antal van den Bosch ILK, Universidad de Tilburg P.O. La recuperación de la experiencia ha sido ampliamente inexplorada en datos distintos a la colección de la W3C. Al mismo tiempo, muchas intranets de universidades y otras organizaciones intensivas en conocimiento ofrecen ejemplos de datos de expertos multilingües relativamente pequeños pero precisos, que abarcan amplias áreas de especialización. Primero presentamos dos tareas principales de recuperación de expertos, junto con un conjunto de enfoques de referencia basados en modelado del lenguaje generativo, con el objetivo de encontrar relaciones de experticia entre temas y personas. Para nuestra evaluación experimental, introducimos (y liberamos) un nuevo conjunto de pruebas basado en un rastreo de un sitio universitario. Usando este conjunto de pruebas, llevamos a cabo dos series de experimentos. El primero tiene como objetivo determinar la efectividad de los métodos de recuperación de conocimientos básicos aplicados al nuevo conjunto de pruebas. El segundo tiene como objetivo evaluar modelos refinados que exploten las características distintivas del nuevo conjunto de pruebas, como la estructura organizativa de la universidad y la estructura jerárquica de los temas en el conjunto de pruebas. Los modelos de recuperación de experiencia se muestran robustos con respecto a entornos más pequeños que la colección de la W3C, y las técnicas actuales parecen ser generalizables a otros contextos. Categorías y Descriptores de Asignaturas H.3 [Almacenamiento y Recuperación de Información]: H.3.1 Análisis de Contenido e Indexación; H.3.3 Búsqueda y Recuperación de Información; H.3.4 Sistemas y Software; H.4 [Aplicaciones de Sistemas de Información]: H.4.2 Tipos de Sistemas; H.4.m Términos Generales Misceláneos Algoritmos, Medición, Rendimiento, Experimentación 1. INTRODUCCIÓN La intranet de una organización proporciona un medio para intercambiar información entre empleados y facilitar colaboraciones entre ellos. Para lograr esto de manera eficiente y efectiva, es necesario proporcionar instalaciones de búsqueda que permitan a los empleados no solo acceder a documentos, sino también identificar a colegas expertos. En la pista de Empresas de TREC [22] se ha reconocido la necesidad de estudiar y comprender la recuperación de expertos a través de la introducción de tareas de Búsqueda de Expertos. El objetivo de la búsqueda de expertos es identificar una lista de personas que tienen conocimientos sobre un tema específico. Esta tarea suele abordarse descubriendo asociaciones entre personas y temas [10]; comúnmente, se asume que la co-ocurrencia del nombre de una persona con temas en el mismo contexto es evidencia de experiencia. Una tarea alternativa, que utiliza la misma idea de asociaciones entre personas y temas, es el perfilado de expertos, donde la tarea consiste en devolver una lista de temas sobre los que una persona tiene conocimiento [3]. El lanzamiento de la tarea de Búsqueda de Expertos en TREC ha generado mucho interés en la recuperación de expertos, con un rápido progreso en términos de modelado, algoritmos y aspectos de evaluación. Sin embargo, casi todo el trabajo de búsqueda o perfilado de expertos realizado ha sido validado experimentalmente utilizando la colección de W3C [24] de la pista empresarial. Si bien esta colección es actualmente la única colección de pruebas disponible públicamente para tareas de recuperación de expertos, solo representa un tipo de intranet. Con solo una colección de pruebas no es posible generalizar conclusiones a otros escenarios realistas. En este artículo nos enfocamos en la recuperación de expertos en un entorno realista que difiere del entorno de la W3C, uno en el que hay cantidades relativamente pequeñas de datos limpios y multilingües disponibles, que abarcan una amplia gama de áreas de expertise, como se puede encontrar en las intranets de universidades y otras organizaciones intensivas en conocimiento. Normalmente, esta configuración incluye varios tipos adicionales de estructuras: estructura temática (por ejemplo, jerarquías de temas utilizadas por la organización), estructura organizativa (facultad, departamento, ...), así como múltiples tipos de documentos (descripciones de investigación y cursos, publicaciones y páginas web académicas). Este entorno es bastante diferente del entorno del W3C de maneras que podrían afectar el rendimiento de las tareas de recuperación de conocimientos especializados. Nos enfocamos en varias preguntas de investigación en este artículo: ¿La cantidad relativamente pequeña de datos disponibles en una intranet afecta la calidad de las asociaciones tema-persona que son fundamentales en los algoritmos de recuperación de expertos? ¿Cómo se desempeñan los algoritmos de vanguardia desarrollados en el conjunto de datos del W3C en el escenario alternativo del tipo descrito anteriormente? Más en general, ¿se pueden aplicar las lecciones de la tarea de Búsqueda de Expertos en TREC a este contexto? ¿Cómo afecta la inclusión o exclusión de diferentes documentos a las tareas de recuperación de expertos? Además, ¿cómo se puede utilizar la estructura temática y organizativa con fines de recuperación? Para responder a nuestras preguntas de investigación, primero presentamos un conjunto de enfoques de línea base, basados en modelado de lenguaje generativo, con el objetivo de encontrar asociaciones entre temas y personas. Esto nos permite formular las tareas de búsqueda de expertos y perfilado de expertos de manera uniforme, y tiene el beneficio adicional de permitirnos entender las relaciones entre las dos tareas. Para nuestra evaluación experimental, introducimos un nuevo conjunto de datos (la Colección de Expertos de UvT) que es representativo del tipo de intranet que describimos anteriormente. Nuestra colección se basa en datos de acceso público, extraídos del sitio web de la Universidad de Tilburg (UvT). Este tipo de datos es particularmente interesante, ya que (1) es limpio, heterogéneo, estructurado y enfocado, pero comprende un número limitado de documentos; (2) contiene información sobre la jerarquía organizativa; (3) es bilingüe (inglés y holandés); y (4) la lista de áreas de especialización de un individuo es proporcionada por los propios empleados. Usando la colección de expertos de UvT, llevamos a cabo dos conjuntos de experimentos. El primero tiene como objetivo determinar la efectividad de los métodos de búsqueda y perfilado de experiencia inicial en este nuevo entorno. Un segundo grupo de experimentos está dirigido a extensiones de los métodos base que aprovechan las características distintivas de la Colección de Expertos de UvT; específicamente, proponemos y evaluamos métodos refinados de búsqueda y perfilado de expertos que incorporan la topicalidad y la estructura organizativa. Además de las preguntas de investigación y el conjunto de datos que aportamos, nuestras principales contribuciones son las siguientes. Los modelos base desarrollados para encontrar expertos funcionan bien en el nuevo conjunto de datos. Mientras que en el entorno del W3C la tarea de encontrar expertos parece ser más difícil que el perfilado, para los datos de UvT ocurre lo contrario. Encontramos que el perfilado en el conjunto de datos UvT es considerablemente más difícil que en el conjunto de datos W3C, lo cual creemos que se debe al gran (pero realista) número de áreas temáticas que utilizamos para el perfilado: alrededor de 1,500 para el conjunto de datos UvT, frente a 50 en el caso de W3C. Teniendo en cuenta la similitud entre los temas puede mejorar significativamente el rendimiento de recuperación. Las medidas de similitud de mejor rendimiento son basadas en contenido, por lo tanto también se pueden aplicar en entornos de la W3C (y otros). Finalmente, demostramos que la estructura organizativa puede ser explotada en forma de un modelo de contexto, mejorando los puntajes de MAP para ciertos modelos hasta en un 70%. El resto de este documento está organizado de la siguiente manera. En la siguiente sección revisamos el trabajo relacionado. Luego, en la Sección 3 proporcionamos descripciones detalladas de las tareas de recuperación de expertos que abordamos en este artículo: la búsqueda de expertos y la creación de perfiles de expertos. En la Sección 4 presentamos nuestros modelos base, cuyo rendimiento luego es evaluado en la Sección 6 utilizando el conjunto de datos UvT que presentamos en la Sección 5. Los modelos avanzados que explotan características específicas de nuestros datos se presentan en la Sección 7 y se evalúan en la Sección 8. Formulamos nuestras conclusiones en la Sección 9.2. Trabajos relacionados Los enfoques iniciales para encontrar expertos a menudo empleaban bases de datos que contenían información sobre las habilidades y conocimientos de cada individuo en la organización [11]. La mayoría de estas herramientas (generalmente llamadas páginas amarillas o sistemas de búsqueda de personas) dependen de que las personas evalúen sus habilidades frente a un conjunto predefinido de palabras clave. Para actualizar perfiles en estos sistemas de forma automática, se necesita de tecnologías inteligentes [5]. Enfoques más recientes utilizan conjuntos de documentos específicos (como correos electrónicos [6] o software [18]) para encontrar experiencia. En contraste con centrarse en tipos de documentos particulares, también hay un interés creciente en el desarrollo de sistemas que indexan y extraen información de documentos publicados en intranets como fuentes de evidencia de experiencia. Un enfoque publicado es el sistema P@noptic [9], que construye una representación de cada persona concatenando todos los documentos asociados con esa persona, similar al Modelo 1 de Balog et al. [4], quienes formalizan y comparan dos métodos. El Modelo 1 de Balog et al. modela directamente el conocimiento de un experto a partir de documentos asociados, mientras que su Modelo 2 primero localiza documentos sobre el tema y luego encuentra a los expertos asociados. En los experimentos reportados, el segundo método tiene un rendimiento significativamente mejor cuando hay suficientes documentos asociados por candidato. La mayoría de los sistemas que participaron en las ediciones de 2005 y 2006 de la tarea de Búsqueda de Expertos en TREC implementaron (variaciones de) uno de estos dos modelos; ver [10, 20]. Macdonald y Ounis [16] proponen un enfoque diferente para clasificar la experiencia de los candidatos con respecto a un tema basado en técnicas de fusión de datos, sin utilizar heurísticas específicas de la colección; encuentran que la aplicación de modelos de ponderación basados en campos mejora la clasificación de los candidatos. Petkova y Croft [19] proponen otro enfoque, basado en una combinación de los Modelos 1 y 2 anteriores, modelando explícitamente los temas. Al abordar otras tareas de recuperación de expertos que también pueden abordarse utilizando asociaciones entre temas y personas, Balog y de Rijke [3] abordaron la tarea de determinar perfiles de expertos temáticos. Si bien sus métodos resultaron ser eficientes en el corpus de la W3C, requieren una cantidad de datos que puede que no esté disponible en la típica organización intensiva en conocimiento. Balog y de Rijke [2] estudian la tarea relacionada de encontrar expertos que sean similares a un pequeño conjunto de expertos dados como entrada. Por cierto, crear un resumen textual de una persona muestra algunas similitudes con la búsqueda de biografías, la cual ha recibido una considerable cantidad de atención recientemente; ver por ejemplo, [13]. Utilizamos modelado de lenguaje generativo para encontrar asociaciones entre temas y personas. En nuestro modelado de búsqueda y perfilado de expertos recopilamos evidencia de experiencia de múltiples fuentes, en una colección heterogénea, e integramos esta información con la co-ocurrencia de nombres de candidatos y términos de consulta: el entorno de modelado de lenguaje nos permite hacer esto de manera transparente. Nuestro modelado avanza en dos pasos. En el primer paso, consideramos tres modelos base, dos tomados de [4] (los Modelos 1 y 2 mencionados anteriormente), y uno una versión refinada de un modelo introducido en [3] (al que nos referimos como Modelo 3 a continuación); este tercer modelo también es similar al modelo descrito por Petkova y Croft [19]. Los modelos que consideramos en nuestra segunda ronda de experimentos son modelos de mezcla similares a los modelos de lenguaje contextual [1] y a los documentos ampliados de Tao et al. [21]; sin embargo, las características que utilizamos para definir nuestras expansiones, incluida la estructura temática y la estructura organizativa, no se han utilizado de esta manera antes. 3. En el escenario de recuperación de expertos que imaginamos, los usuarios que buscan expertos dentro de una organización tienen acceso a una interfaz que combina un cuadro de búsqueda (donde pueden buscar expertos o temas) con estructuras de navegación (de expertos y de temas) que les permite hacer clic hasta llegar a una página de experto (que proporciona el perfil de una persona) o a una página de tema (que proporciona una lista de expertos en el tema). Para alimentar la interfaz anterior, nos enfrentamos a dos tareas de recuperación de expertos, la búsqueda de expertos y el perfilado de expertos, que primero definimos y luego formalizamos utilizando modelos de lenguaje generativos. Para modelar cualquiera de las tareas, la probabilidad de que el tema de la consulta esté asociado a un experto candidato juega un papel clave en las estimaciones finales para la búsqueda y el perfilado. Al utilizar modelos de lenguaje, tanto los candidatos como la consulta se caracterizan por distribuciones de términos en el vocabulario (utilizado en los documentos puestos a disposición por la organización cuyas necesidades de recuperación de experiencia estamos abordando). 3.1 Búsqueda de expertos La búsqueda de expertos implica la tarea de encontrar a la persona adecuada con las habilidades y conocimientos apropiados: ¿Quiénes son los expertos en el tema X?. Por ejemplo, un empleado quiere averiguar quién trabajó en un proyecto en particular para descubrir por qué se tomaron decisiones específicas sin tener que revisar minuciosamente la documentación (si es que la hay). O pueden necesitar un especialista capacitado para consultoría sobre un problema específico. Dentro de una organización, generalmente hay muchos posibles candidatos que podrían ser expertos en un tema dado. Podemos plantear este problema de la siguiente manera: ¿Cuál es la probabilidad de que un candidato ca sea un experto dado el tema de consulta q? Es decir, determinamos p(ca|q) y clasificamos a los candidatos ca de acuerdo con esta probabilidad. Los candidatos con la probabilidad más alta dada la consulta son considerados los expertos más probables en ese tema. El desafío es cómo estimar esta probabilidad con precisión. Dado que es probable que la consulta consista solo en unos pocos términos para describir la experiencia requerida, deberíamos poder obtener una estimación más precisa al invocar el Teorema de Bayes, y estimar: p(ca|q) = p(q|ca)p(ca) p(q) , (1) donde p(ca) es la probabilidad de un candidato y p(q) es la probabilidad de una consulta. Dado que p(q) es una constante, se puede ignorar para fines de clasificación. Por lo tanto, la probabilidad de que un candidato ca sea un experto dado la consulta q es proporcional a la probabilidad de una consulta dada el candidato p(q|ca), ponderada por la creencia a priori p(ca) de que el candidato ca es un experto. p(ca|q) ∝ p(q|ca)p(ca) (2) En este artículo, nuestro enfoque principal se centra en estimar la probabilidad de una consulta dada el candidato p(q|ca), ya que esta probabilidad captura en qué medida el candidato conoce sobre el tema de la consulta. Si bien se asume generalmente que los priors del candidato son uniformes y, por lo tanto, no influirán en la clasificación, se ha demostrado que una elección sensata de priors puede mejorar el rendimiento [20]. Perfilado de expertos. Mientras que la tarea de búsqueda de expertos se centraba en encontrar expertos en un tema específico, la tarea de perfilado de expertos busca responder a una pregunta relacionada: ¿Sobre qué temas tiene conocimiento un candidato? Básicamente, esto da un giro a las preguntas sobre la búsqueda de expertos. El perfilado de un candidato individual implica la identificación de áreas de habilidades y conocimientos en las que tienen experiencia, así como una evaluación del nivel de competencia en cada una de estas áreas. Este es el perfil temático de los candidatos. Generalmente, los perfiles temáticos dentro de las organizaciones consisten en estructuras tabulares que catalogan explícitamente las habilidades y conocimientos de cada individuo en la organización. Sin embargo, esta práctica está limitada por los recursos disponibles para definir, crear, mantener y actualizar estos perfiles con el tiempo. Al centrarnos en métodos automáticos que se basan en la evidencia disponible dentro de los repositorios de documentos de una organización, nuestro objetivo es reducir el esfuerzo humano asociado con el mantenimiento de perfiles temáticos. Un perfil temático de un candidato, entonces, se define como un vector donde cada elemento i del vector corresponde a la experiencia del candidato en un tema dado ki, (es decir, s(ca, ki)). Cada tema ki define un área de conocimiento o habilidad particular que la organización utiliza para definir el perfil temático de los candidatos. Por lo tanto, se asume que se proporciona una lista de temas, {k1, . . . , kn}, donde n es el número de temas predefinidos: perfil(ca) = s(ca, k1), s(ca, k2), . . . , s(ca, kn) . Se necesita contexto y evidencia para ayudar a los usuarios de sistemas de búsqueda de expertos a decidir a quién contactar al buscar experiencia en un área particular. Ejemplos de dicho contexto son: ¿Con quién trabaja ella? ¿Cuáles son sus datos de contacto? ¿Está bien conectada, por si acaso no puede ayudarnos ella misma? ¿Cuál es su rol en la organización? ¿Quién es su superior? Los colaboradores, afiliaciones, etc. son parte del perfil social de los candidatos y pueden servir como antecedentes para interpretar las recomendaciones del sistema. En este documento solo abordamos el problema de determinar perfiles temáticos, dejando el perfilado social para trabajos futuros. Planteamos el problema de cuantificar la competencia de una persona en un área de conocimiento específica de la siguiente manera: ¿Cuál es la probabilidad de que un área de conocimiento (ki) forme parte del perfil de candidatos (expertise)? donde s(ca, ki) está definido por p(ki|ca). Nuestra tarea, entonces, es estimar p(ki|ca), lo cual es equivalente al problema de obtener p(q|ca), donde el tema ki se representa como un tema de consulta q, es decir, una secuencia de palabras clave que representan la experiencia requerida. Tanto la tarea de encontrar expertos como la de perfilar expertos dependen de la estimación precisa de p(q|ca). La única diferencia proviene de la probabilidad previa de que una persona sea experta (p(ca)), la cual puede ser incorporada en la tarea de encontrar expertos. Este antecedente no se aplica a la tarea de perfilado ya que el candidato (individuo) está fijo. 4. MODELOS DE REFERENCIA En esta sección describimos nuestros modelos de referencia para estimar p(q|ca), es decir, las asociaciones entre temas y personas. Tanto la búsqueda de expertos como el perfilado de expertos se reducen a esta estimación. Empleamos tres modelos para calcular esta probabilidad. 4.1 De temas a candidatos Utilizando Modelos de Candidatos: El Modelo 1 [4] define la probabilidad de una consulta dada un candidato (p(q|ca)) utilizando técnicas estándar de modelado de lenguaje, basadas en un modelo de lenguaje multinomial de unigrama. Para cada candidato ca, se infiere un modelo de lenguaje candidato θca de tal manera que la probabilidad de un término dado θca no sea cero para todos los términos, es decir, p(t|θca) > 0. A partir del modelo de candidato, la consulta se genera con la siguiente probabilidad: p(q|θca) = Y t∈q p(t|θca)n(t,q), donde cada término t en la consulta q se muestrea de manera idéntica e independiente, y n(t, q) es el número de veces que t ocurre en q. El modelo de lenguaje candidato se infiere de la siguiente manera: (1) se calcula un modelo empírico p(t|ca); (2) se suaviza con probabilidades de fondo. Usando las asociaciones entre un candidato y un documento, la probabilidad p(t|ca) puede aproximarse por: p(t|ca) = X d p(t|d)p(d|ca), donde p(d|ca) es la probabilidad de que el candidato ca genere un documento de apoyo d, y p(t|d) es la probabilidad de que un término t ocurra en el documento d. Utilizamos la estimación de máxima verosimilitud de un término, es decir, la frecuencia normalizada del término t en el documento d. La fuerza de la asociación entre el documento d y el candidato ca expresada por p(d|ca) refleja el grado en que la experiencia de los candidatos se describe utilizando este documento. La estimación de esta probabilidad se presenta más adelante, en la Sección 4.2. El modelo candidato se construye entonces como una interpolación lineal de p(t|ca) y el modelo de fondo p(t) para asegurar que no haya probabilidades nulas, lo que resulta en la estimación final: p(q|θca) = (4) Y t∈q ( (1 − λ) X d p(t|d)p(d|ca) ! + λp(t) )n(t,q) . El Modelo 1 recopila toda la información de términos de todos los documentos asociados con el candidato, y la utiliza para representar a ese candidato. Este modelo se utiliza para predecir qué tan probable es que un candidato produzca una consulta q. Esto puede interpretarse intuitivamente como la probabilidad de que este candidato hable sobre el tema de la consulta, donde asumimos que esto es indicativo de su experiencia. Usando Modelos de Documentos: El Modelo 2 [4] toma un enfoque diferente. Aquí, el proceso se divide en dos partes. Dado un candidato ca, (1) se selecciona un documento asociado con un candidato con probabilidad p(d|ca), y (2) a partir de este documento se genera una consulta q con probabilidad p(q|d). Entonces se toma la suma sobre todos los documentos para obtener p(q|ca), de modo que: p(q|ca) = Σ d p(q|d)p(d|ca). (5) La probabilidad de una consulta dada un documento se estima inferiendo un modelo de lenguaje del documento θd para cada documento d de manera similar a como se infería el modelo candidato: p(t|θd) = (1 − λ)p(t|d) + λp(t), (6) donde p(t|d) es la probabilidad del término en el documento. La probabilidad de una consulta dada el modelo de documento es: p(q|θd) = Y t∈q p(t|θd)n(t,q) . La estimación final de p(q|ca) se obtiene sustituyendo p(q|d) por p(q|θd) en la Ecuación 5 (ver [4] para más detalles). Conceptualmente, el Modelo 2 difiere del Modelo 1 porque el candidato no está modelado directamente. En cambio, el documento actúa como una variable oculta en el proceso que separa la consulta del candidato. Este proceso es similar a cómo un usuario puede buscar candidatos con un motor de búsqueda estándar: inicialmente encontrando los documentos relevantes y luego viendo quién está asociado con ese documento. Al examinar una serie de documentos, el usuario puede obtener una idea de qué candidatos son más propensos a discutir el tema q. Usando Modelos de Temas: Modelo 3 Introducimos un tercer modelo, Modelo 3. En lugar de intentar modelar el proceso de generación de consultas a través de modelos de candidatos o documentos, representamos la consulta como un modelo de lenguaje de temas y estimamos directamente la probabilidad del candidato p(ca|q). Este enfoque es similar al modelo presentado en [3, 19]. Como en los modelos anteriores, se infiere un modelo de lenguaje, pero esta vez para la consulta. Adaptamos el trabajo de Lavrenko y Croft [14] para estimar un modelo de tema a partir de la consulta. El procedimiento es el siguiente. Dada una colección de documentos y un tema de consulta q, se asume que existe un modelo de tema desconocido θk que asigna probabilidades p(t|θk) a las ocurrencias de términos en los documentos del tema. Tanto la consulta como los documentos son muestras de θk (a diferencia de los enfoques anteriores, donde se asume que una consulta se extrae de un documento específico o un modelo candidato). La tarea principal es estimar p(t|θk), la probabilidad de un término dado el modelo de tema. Dado que la consulta q es muy dispersa y no hay ejemplos de documentos sobre el tema, es necesario aproximar esta distribución. Lavrenko y Croft [14] sugieren una forma razonable de obtener dicha aproximación, asumiendo que p(t|θk) puede aproximarse por la probabilidad del término t dado la consulta q. Entonces podemos estimar p(t|q) utilizando la probabilidad conjunta de observar el término t junto con los términos de la consulta, q1, . . . , qm, y dividiendo por la probabilidad conjunta de los términos de la consulta: p(t|θk) ≈ p(t|q) = p(t, q1, . . . , qm) p(q1, . . . , qm) = p(t, q1, . . . , qm) P t ∈T p(t , q1, . . . , qm) , donde p(q1, . . . , qm) = P t ∈T p(t , q1, . . . , qm), y T es el vocabulario completo de términos. Para estimar la probabilidad conjunta p(t, q1, . . . , qm), seguimos [14, 15] y asumimos que t y q1, . . . , qm son mutuamente independientes, una vez que elegimos una distribución fuente del conjunto de distribuciones fuente subyacentes U. Si elegimos U como un conjunto de modelos de documentos, entonces para construir este conjunto, la consulta q se emitiría contra la colección, y se asume que los primeros n devueltos son relevantes para el tema, y por lo tanto se tratan como muestras del modelo de tema. (Tenga en cuenta que en su lugar podrían usarse modelos candidatos). Con los modelos de documentos formando U, la probabilidad conjunta del término y la consulta se convierte en: p(t, q1, . . . , qm) = X d∈U p(d) ˘ p(t|θd) mY i=1 p(qi|θd) ¯ . (7) Aquí, p(d) denota la distribución previa sobre el conjunto U, que refleja la relevancia del documento para el tema. Suponemos que p(d) es uniforme en todo U. Para clasificar a los candidatos según el modelo de tema definido, utilizamos la métrica de divergencia de Kullback-Leibler (KL, [8]) para medir la diferencia entre los modelos de los candidatos y el modelo de tema: KL(θk||θca) = X t p(t|θk) log p(t|θk) p(t|θca) . (8) Se considera que los candidatos con una divergencia menor respecto al modelo de tema son más propensos a ser expertos en ese tema. El modelo candidato θca está definido en la Ecuación 4. Al utilizar la divergencia de Kullback-Leibler en lugar de la probabilidad de un candidato dado el modelo de tema p(ca|θk), evitamos problemas de normalización. 4.2 Asociaciones documento-candidato Para nuestros modelos, necesitamos poder estimar la probabilidad p(d|ca), que expresa en qué medida un documento d caracteriza al candidato ca. En [4], se presentan dos métodos para estimar esta probabilidad, basados en el número de nombres de personas reconocidos en un documento. Sin embargo, en nuestro entorno de intranet es razonable asumir que los autores de los documentos pueden ser identificados de manera inequívoca (por ejemplo, como el autor de un artículo, el profesor asignado a un curso, el propietario de una página web, etc.). Por lo tanto, establecemos p(d|ca) en 1 si el candidato ca es el autor del documento d, de lo contrario la probabilidad es 0. En la Sección 6 describimos cómo se puede determinar la autoría en diferentes tipos de documentos dentro de la colección. 5. La colección de expertos de UvT utilizada en los experimentos de este artículo se ajusta al escenario descrito en la Sección 3. La colección se basa en el sistema Webwijs (Webwise) desarrollado en la Universidad de Tilburg (UvT) en los Países Bajos. Webwijs (http://www.uvt.nl/webwijs/) es una base de datos de acceso público de empleados de la UvT que están involucrados en investigación o enseñanza; actualmente, Webwijs contiene información sobre 1168 expertos, cada uno de los cuales tiene una página con información de contacto y, si está disponible por parte del experto, una descripción de investigación y lista de publicaciones. Además, cada experto puede seleccionar áreas de especialización de una lista de 1491 temas y se le anima a sugerir nuevos temas que necesitan ser aprobados por el editor de Webwijs. Cada tema tiene una página separada que muestra a todos los expertos asociados con ese tema y, si está disponible, una lista de temas relacionados. Webwijs está disponible en holandés e inglés, y esta bilingüidad se ha preservado en la colección. Cada página de Webwijs en holandés tiene una traducción al inglés. No todos los temas en holandés tienen una traducción al inglés, pero lo contrario es cierto: los 981 temas en inglés tienen un equivalente en holandés. Aproximadamente el 42% de los expertos imparten cursos en la Universidad de Tilburg; estos cursos también fueron rastreados e incluidos en el perfil. Además, aproximadamente el 27% de los expertos enlazan su página académica desde su página de Webwijs. Estas páginas de inicio fueron rastreadas y añadidas a la colección. (Esto significa que si los expertos colocaron las versiones completas de sus publicaciones en sus páginas de inicio académicas, estas también estaban disponibles para su indexación). También obtuvimos 1880 versiones completas de publicaciones del repositorio institucional de UvT y el número de expertos holandeses en inglés es de 1168, con 743 expertos con ≥ 1 tema. El número de temas es de 1491 y 981, y el número de pares experto-tema es de 4318 y 3251, con un promedio de temas por experto de 5.8 y 5.9 respectivamente. El máximo número de temas por experto es de 60 (1) y 35 (1), y el mínimo es de 1 (74) y 1 (106) respectivamente. El promedio de expertos por tema es de 2.9 y 3.3, con un máximo de 30 (1) y un mínimo de 1 (615) y 1 (346) respectivamente. Hay 318 expertos con HP y CD, con un promedio de 3.5 CDs por experto docente. Además, hay 329 expertos con RD y 734 con PUB, con un promedio de 27.0 publicaciones por experto, 25.2 citas por experto y 1.8 publicaciones completas por experto. Tabla 2: Estadísticas descriptivas de las versiones holandesas e inglesas de la colección de expertos de UvT. Ejecutamos el identificador de idioma TextCat [23] para clasificar el idioma de las páginas de inicio y las publicaciones de texto completo. Nos limitamos a las páginas en las que el clasificador tenía confianza en el idioma utilizado en la página. Esto resultó en cuatro tipos de documentos: descripciones de investigación (RD), descripciones de cursos (CD), publicaciones (PUB; versiones de texto completo y solo de cita) y páginas web académicas (HP). Todo fue agrupado en la colección UvT Expert que está disponible en http://ilk.uvt.nl/uvt-expert-collection/. La colección UvT Expert fue extraída de un entorno organizativo diferente a la colección de la W3C y difiere de ella en varios aspectos. El entorno de UvT es uno con cantidades relativamente pequeñas de datos multilingües. Las asociaciones entre los documentos y los autores son claras y los datos están estructurados y limpios. La colección abarca una amplia gama de áreas de especialización, como suele encontrarse en intranets de universidades y otros institutos intensivos en conocimiento. Además, nuestro entorno universitario cuenta con varios tipos de estructura (temática y organizativa), así como múltiples tipos de documentos. Otra diferencia importante entre los dos conjuntos de datos es que las áreas de especialización en la colección de Expertos de UvT son autoseleccionadas en lugar de basarse en la membresía de un grupo o asignaciones de otros. El tamaño es otra dimensión en la que difieren las colecciones de expertos de W3C y UvT: esta última es la más pequeña de las dos. También son realistas las grandes diferencias en la cantidad de información disponible para cada experto. Utilizar Webwijs es voluntario; 425 expertos holandeses no seleccionaron ningún tema en absoluto. Esto nos deja con 743 perfiles de expertos holandeses y 727 perfiles de expertos en inglés utilizables. La Tabla 2 proporciona estadísticas descriptivas para la colección de Expertos de UvT. Las universidades tienden a tener una estructura jerárquica que va desde el nivel de la facultad, pasando por los departamentos, grupos de investigación, hasta llegar a los investigadores individuales. En la colección de Expertos de UvT tenemos información sobre las afiliaciones de los investigadores con facultades e institutos, lo que nos proporciona una jerarquía organizativa de dos niveles. La Universidad de Tilburg tiene 22 unidades organizativas a nivel de facultad (incluida la oficina universitaria y varios institutos de investigación) y 71 departamentos, lo que equivale a 3.2 departamentos por facultad. En cuanto a la jerarquía temática utilizada por Webwijs, 131 de los 1491 temas son nodos principales en la jerarquía. Esta jerarquía tiene una longitud promedio de cadena de temas de 2.65 y una longitud máxima de 7 temas. 6. EVALUACIÓN A continuación, evaluamos los modelos de la Sección 4 para la búsqueda y perfilado de expertos en la colección de Expertos de UvT. Detallamos nuestras preguntas de investigación y configuración experimental, y luego presentamos nuestros resultados. 6.1 Preguntas de investigación Abordamos las siguientes preguntas de investigación. Tanto la búsqueda de expertos como el perfilado dependen de las estimaciones de p(q|ca). La pregunta es cómo se comparan los modelos en las diferentes tareas, y en el contexto de la colección de expertos de UvT. En [4], el Modelo 2 superó al Modelo 1 en la colección de la W3C. ¿Cómo se comparan en nuestro conjunto de datos? ¿Y cómo se compara el Modelo 3 con el Modelo 1? ¿Qué hay de las diferencias de rendimiento entre los dos idiomas en nuestra colección de pruebas? 6.2 Configuración Experimental La salida de nuestros modelos fue evaluada frente a las etiquetas de temas autoasignadas, que fueron tratadas como juicios de relevancia. Los resultados fueron evaluados por separado para inglés y holandés. Para inglés solo usamos temas para los cuales había traducción al neerlandés; para neerlandés se consideraron todos los temas. Los resultados se promediaron para las consultas en la intersección de las evaluaciones de relevancia y los resultados; las consultas faltantes no contribuyen con un valor de 0 a las puntuaciones. Utilizamos medidas estándar de recuperación de información, como la Precisión Media Promedio (MAP) y la Reciprocidad Media Promedio (MRR). También informamos el porcentaje de temas (%q) y candidatos (%ca) cubiertos, respectivamente, para las tareas de búsqueda y perfilado de expertos. 6.3 Resultados La Tabla 1 muestra el rendimiento de los Modelos 1, 2 y 3 en las tareas de búsqueda y perfilado de expertos. Las filas de la tabla corresponden a los diversos tipos de documentos (RD, CD, PUB y HP) y a sus combinaciones. RD+CD+PUB+HP es equivalente a la colección completa y será referida como la LÍNEA BASE de nuestros experimentos. Al observar la Tabla 1, vemos que el Modelo 2 tiene el mejor rendimiento en general. Sin embargo, cuando los datos están limpios y muy enfocados (RD), el Modelo 3 lo supera en varios casos. El modelo 1 tiene la mejor cobertura de candidatos (%ca) y temas (%q). Los diversos tipos de documentos difieren en sus características y en cómo mejoran las tareas de búsqueda y perfilado. El perfilado de expertos se beneficia mucho de los datos limpios presentes en los tipos de documentos RD y CD, mientras que las publicaciones contribuyen principalmente a la tarea de encontrar expertos. Agregar las páginas de inicio no resulta ser particularmente útil. Cuando comparamos los resultados entre idiomas, encontramos que la cobertura de los temas en inglés (%q) es mayor que la de los temas en neerlandés para la búsqueda de expertos. Además de eso, las puntuaciones caen en el mismo rango para ambos idiomas. Para la tarea de perfilado, la cobertura de los candidatos (%ca) es muy similar para ambos idiomas. Sin embargo, el rendimiento es considerablemente mejor para los temas en inglés. Si bien es difícil comparar las puntuaciones entre colecciones, concluimos con una breve comparación de las puntuaciones absolutas en la Tabla 1 con las reportadas en [3, 4] en el conjunto de pruebas de W3C (edición de 2005). Para el experto que busca, los puntajes MAP para el Modelo 2 reportados aquí son aproximadamente un 50% más altos que las cifras correspondientes en [4], mientras que nuestros puntajes MRR están ligeramente por debajo de los de [4]. Para el perfilado de expertos, las diferencias son mucho más dramáticas: las puntuaciones MAP para el Modelo 2 reportadas aquí son aproximadamente un 50% más bajas que las puntuaciones en [3], mientras que las puntuaciones MRR (mejores) son aproximadamente iguales a las de [3]. La causa de estas diferencias parece residir en el número de áreas de conocimiento consideradas aquí, aproximadamente 30 veces más que en el entorno del W3C. MODELOS AVANZADOS Ahora que hemos desarrollado y evaluado técnicas básicas de modelado de lenguaje para la recuperación de expertos, nos enfocamos en modelos refinados que explotan características especiales de nuestra colección de pruebas. 7.1 Explotando la similitud en el área de conocimiento Una forma de mejorar la puntuación de una consulta dada un candidato es considerar qué otras solicitudes satisfaría el candidato y usarlas como evidencia adicional para respaldar la consulta original, proporcionalmente. Búsqueda de expertos Perfilado de expertos Tipos de documentos Modelo 1 Modelo 2 Modelo 3 Modelo 1 Modelo 2 Modelo 3 %q MAP MRR %q MAP MRR %q MAP MRR %ca MAP MRR %ca MAP MRR %ca MAP MRR Inglés RD 97.8 0.126 0.269 83.5 0.144 0.311 83.3 0.129 0.271 100 0.089 0.189 39.3 0.232 0.465 41.1 0.166 0.337 CD 97.8 0.118 0.227 91.7 0.123 0.248 91.7 0.118 0.226 32.8 0.188 0.381 32.4 0.195 0.385 32.7 0.203 0.370 PUB 97.8 0.200 0.330 98.0 0.216 0.372 98.0 0.145 0.257 78.9 0.167 0.364 74.5 0.212 0.442 78.9 0.135 0.299 HP 97.8 0.081 0.186 97.4 0.071 0.168 97.2 0.062 0.149 31.2 0.150 0.299 28.8 0.185 0.335 30.1 0.136 0.287 RD+CD 97.8 0.188 0.352 92.9 0.193 0.360 92.9 0.150 0.273 100 0.145 0.286 61.3 0.251 0.477 63.2 0.217 0.416 RD+CD+PUB 97.8 0.235 0.373 98.1 0.277 0.439 98.1 0.178 0.305 100 0.196 0.380 87.2 0.280 0.533 89.5 0.170 0.344 RD+CD+PUB+HP 97.8 0.237 0.372 98.6 0.280 0.441 98.5 0.166 0.293 100 0.199 0.387 88.7 0.281 0.525 90.9 0.169 0.329 Holandés RD 61.3 0.094 0.229 38.4 0.137 0.336 38.3 0.127 0.295 38.0 0.127 0.386 34.1 0.138 0.420 38.0 0.105 0.327 CD 61.3 0.107 0.212 49.7 0.128 0.256 49.7 0.136 0.261 32.5 0.151 0.389 31.8 0.158 0.396 32.5 0.170 0.380 PUB 61.3 0.193 0.319 59.5 0.218 0.368 59.4 0.173 0.291 78.8 0.126 0.364 76.0 0.150 0.424 78.8 0.103 0.294 HP 61.3 0.063 0.169 56.6 0.064 0.175 56.4 0.062 0.163 29.8 0.108 0.308 27.8 0.125 0.338 29.8 0.098 0.255 RD+CD 61.3 0.159 0.314 51.9 0.184 0.360 51.9 0.169 0.324 60.5 0.151 0.410 57.2 0.166 0.431 60.4 0.159 0.384 RD+CD+PUB 61.3 0.244 0.398 61.5 0.260 0.424 61.4 0.210 0.350 90.3 0.165 0.445 88.2 0.189 0.479 90.3 0.126 0.339 RD+CD+PUB+HP 61.3 0.249 0.401 62.6 0.265 0.436 62.6 0.195 0.344 91.9 0.164 0.426 90.1 0.195 0.488 91.9 0.125 0.328 Tabla 1: Rendimiento de los modelos en las tareas de búsqueda y perfilado de expertos, utilizando diferentes tipos de documentos y sus combinaciones. %q es el número de temas cubiertos (se aplica a la tarea de búsqueda de expertos), %ca es el número de candidatos cubiertos (se aplica a la tarea de perfilado de expertos). Los bloques superior e inferior corresponden al inglés y al holandés respectivamente. Las mejores puntuaciones están en negrita. Esto se puede modelar interpolando entre el p(q|ca) y la evidencia adicional de todos los pedidos similares q, de la siguiente manera: p(q|ca) = λp(q|ca) + (1 − λ) X q p(q|q )p(q |ca), (9) donde p(q|q ) representa la similitud entre los dos temas q y q. Para poder trabajar con métodos de similitud que no son necesariamente probabilidades, establecemos p(q|q ) = w(q,q ) γ , donde γ es una constante de normalización, de modo que γ = P q w(q , q ). Consideramos cuatro métodos para calcular la puntuación de similitud entre dos temas. Tres enfoques se basan estrictamente en el contenido y establecen similitudes examinando patrones de co-ocurrencia de temas dentro de la colección, mientras que el último enfoque explota la estructura jerárquica de áreas temáticas que pueden estar presentes dentro de una organización (ver [7] para más ejemplos de integrar relaciones entre palabras en modelos de lenguaje). La métrica de divergencia Kullback-Leibler (KL) definida en la ecuación 8 proporciona una medida de cuán diferentes o similares son dos distribuciones de probabilidad. Se infiere un modelo de tema para q y q utilizando el método presentado en la Sección 4.1 para describir la consulta en todo el vocabulario. Dado que un puntaje KL más bajo significa que las consultas son más similares, permitimos que w(q, q ) = max(KL(θq||·) − KL(θq||θq )). El Información Mutua Puntual (PMI, [17]) es una medida de asociación utilizada en teoría de la información para determinar el grado de independencia entre variables. La dependencia entre dos consultas se refleja en la puntuación SI(q, q), donde las puntuaciones mayores que cero indican que es probable que exista una dependencia, lo que interpretamos como que las consultas son probablemente similares: SI(q, q) = log p(q, q) / p(q)p(q). Estimamos la probabilidad de un tema p(q) utilizando el número de documentos relevantes para la consulta q dentro de la colección. La probabilidad conjunta p(q, q) se estima de manera similar, utilizando la concatenación de q y q como una consulta. Para obtener p(q|q), luego establecemos w(q, q) = SI(q, q) cuando SI(q, q) > 0, de lo contrario w(q, q) = 0, ya que solo estamos interesados en incluir consultas que sean similares. La estadística de log-verosimilitud proporciona otra medida de dependencia, que es más confiable que la medida de información mutua puntual [17]. Sea k1 el número de co-ocurrencias de q y q, k2 el número de ocurrencias de q que no co-ocurren con q, n1 el número total de ocurrencias de q, y n2 el número total de tokens de tema menos el número de ocurrencias de q. Entonces, sea p1 = k1/n1, p2 = k2/n2 y p = (k1 + k2)/(n1 + n2), (q, q ) = 2( (p1, k1, n1) + (p2, k2, n2) − (p, k1, n1) − (p, k2, n2)), donde (p, n, k) = k log p + (n − k) log(1 − p). El puntaje más alto indica que las consultas también son probablemente similares, por lo tanto, establecemos w(q, q ) = (q, q ). Finalmente, también estimamos la similitud de dos temas basándonos en su distancia dentro de la jerarquía de temas. La jerarquía de temas se visualiza como un grafo dirigido, y para todos los pares de temas se calcula el camino más corto SP(q, q). Establecimos el puntaje de similitud como el recíproco del camino más corto: w(q, q ) = 1/SP(q, q ). 7.2 Información contextual Dada la jerarquía de una organización, las unidades a las que pertenece una persona se consideran como un contexto para compensar la escasez de datos. Lo modelamos de la siguiente manera: p (q|ca) = 1 − P ou∈OU(ca) λou · p(q|ca) + P ou∈OU(ca) λou · p(q|ou), donde OU(ca) es el conjunto de unidades organizativas de las cuales el candidato ca es miembro, y p(q|o) expresa la fuerza de la asociación entre la consulta q y la unidad ou. La última probabilidad se puede estimar utilizando cualquiera de los tres modelos básicos, simplemente reemplazando ca con ou en las ecuaciones correspondientes. Una unidad organizativa está asociada con todos los documentos que sus miembros han escrito. Es decir, p(d|ou) = maxca∈ou p(d|ca). 7.3 Un modelo multilingüe simple Para institutos de conocimiento en Europa, ya sea académicos u otros, un entorno multilingüe (o al menos bilingüe) es típico. El siguiente modelo se basa en un tipo de suposición de independencia: no hay transferencia de conocimientos/perfiles a través de las barreras del idioma. Si bien es una simplificación, esta es una aproximación sensata en primer lugar. Eso es: p (q|ca) =P l∈L λl · p(ql|ca), donde L es el conjunto de idiomas utilizados en la colección, ql es la traducción de la consulta q al idioma l, y λl es un parámetro de suavizado específico del idioma, tal que P l∈L λl = 1. 8. MODELOS AVANZADOS: EVALUACIÓN En esta sección presentamos una evaluación experimental de nuestros modelos avanzados. Búsqueda de expertos Perfilado de expertos Modelo de lenguaje 1 Modelo 2 Modelo 3 Modelo 1 Modelo 2 Modelo 3 %q MAP MRR %q MAP MRR %q MAP MRR %ca MAP MRR %ca MAP MRR %ca MAP MRR Solo inglés 97.8 0.237 0.372 98.6 0.280 0.441 98.5 0.166 0.293 100 0.199 0.387 88.7 0.281 0.525 90.9 0.169 0.329 Solo holandés 61.3 0.249 0.401 62.6 0.265 0.436 62.6 0.195 0.344 91.9 0.164 0.426 90.1 0.195 0.488 91.9 0.125 0.328 Combinación 99.4 0.297 0.444 99.7 0.324 0.491 99.7 0.223 0.388 100 0.241 0.445 92.1 0.313 0.564 93.2 0.224 0.411 Tabla 3: Rendimiento de la combinación de idiomas en las tareas de búsqueda y perfilado de expertos (en candidatos). Los mejores puntajes para cada modelo están en cursiva, los mejores puntajes absolutos para las tareas de búsqueda y perfilado de expertos están en negrita. Tabla 4: Rendimiento en las tareas de búsqueda de expertos (arriba) y perfilado (abajo), utilizando similitudes de áreas de conocimiento. Las carreras fueron evaluadas en los temas principales establecidos. Las mejores puntuaciones están en negrita. 8.1 Preguntas de investigación Nuestras preguntas siguen los refinamientos presentados en la sección anterior: ¿Explotar la similitud en el área de conocimiento mejora la efectividad? ¿Cuál de los diversos métodos para capturar las relaciones entre palabras es más efectivo? Además, ¿es útil nuestra forma de incorporar información contextual? ¿Para qué tareas? Y finalmente, ¿es nuestra forma simple de combinar las puntuaciones monolingües suficiente para obtener mejoras significativas? 8.2 Configuración experimental Dado que las autoevaluaciones también son escasas en nuestra colección, para poder medir las diferencias entre los diversos modelos, seleccionamos un subconjunto de temas y evaluamos (algunas de las) ejecuciones solo en este subconjunto. Este conjunto se denomina como temas principales, y consiste en temas que se encuentran en el nivel superior de la jerarquía temática. (Un tema principal tiene subtemas, pero no es un subtema de ningún otro tema). Este conjunto principal consiste en 132 temas en holandés y 119 en inglés. Los juicios de relevancia se limitaron al conjunto de temas principales, pero no se ampliaron con subtemas. 8.3 Explotando la similitud de áreas de conocimiento. La Tabla 4 presenta los resultados. Los cuatro métodos utilizados para estimar la similitud de áreas de conocimiento son la divergencia de KL (KLDIV) y PointLang. Modelo 1 Modelo 2 Modelo 3 MAP MRR MAP MRR MAP MRR Búsqueda de expertos UK TODOS 0.423 0.545 0.654 0.799 0.494 0.629 UK PRINCIPAL 0.500 0.621 0.704 0.834 0.587 0.699 NL TODOS 0.439 0.560 0.672 0.826 0.480 0.630 NL PRINCIPAL 0.440 0.584 0.645 0.816 0.515 0.655 Perfilado de expertos UK TODOS 0.240 0.640 0.306 0.778 0.223 0.616 UK PRINCIPAL 0.523 0.677 0.519 0.648 0.461 0.587 NL TODOS 0.203 0.716 0.254 0.770 0.183 0.627 NL PRINCIPAL 0.332 0.576 0.380 0.624 0.332 0.549 Tabla 5: Evaluación de los modelos de contexto en unidades organizativas. información mutua ponderada (PMI), log-verosimilitud (LL) y distancia dentro de la jerarquía de temas (HDIST). Logramos mejorar el resultado base en todos los casos, pero la mejora es más notable para la tarea de perfilado. Para ambas tareas, el método LL tuvo el mejor rendimiento. Los enfoques basados en el contenido tuvieron un rendimiento consistentemente mejor que HDIST. 8.4 Información contextual Una jerarquía de dos niveles de unidades organizativas (facultades e institutos) está disponible en la colección de Expertos de UvT. La unidad a la que pertenece una persona se utiliza como contexto para esa persona. Primero, evaluamos los modelos de las unidades organizativas, utilizando todos los temas (TODOS) y solo los temas principales (PRINCIPALES). Una unidad organizativa se considera relevante para un tema dado (o viceversa) si al menos un miembro de la unidad seleccionó el tema dado como área de especialización. La Tabla 5 informa sobre los resultados. En lo que respecta a la búsqueda de expertos, dado un tema, la unidad organizativa correspondiente puede ser identificada con alta precisión. Sin embargo, la tarea de perfilado de expertos muestra una imagen diferente: las puntuaciones son bajas y la tarea parece difícil. La explicación puede ser que los conceptos generales (es decir, nuestros temas principales) pueden pertenecer a varias unidades organizativas. Segundo, realizamos otra evaluación, donde combinamos los modelos contextuales con los modelos candidatos (para puntuar a los candidatos nuevamente). La tabla 6 informa sobre los resultados. Encontramos un impacto positivo de los modelos de contexto solo para la búsqueda de expertos. Notablemente, para la búsqueda de expertos (y Modelo 1), mejora más del 50% (para inglés) y más del 70% (para holandés) en el MAP. El bajo rendimiento en la creación de perfiles de expertos puede deberse al hecho de que los modelos de contexto por sí solos no tuvieron un buen desempeño en la tarea de creación de perfiles desde el principio. 8.5 Modelos multilingües En esta subsección evaluamos el método para combinar resultados en múltiples idiomas que describimos en la Sección 7.3. En nuestro entorno, el conjunto de idiomas consiste en inglés y holandés: L = {UK, NL}. Los pesos de estos idiomas se establecieron para ser idénticos (λUK = λNL = 0.5). Realizamos experimentos con diferentes configuraciones de λ, pero no observamos diferencias significativas en el rendimiento. La Tabla 3 informa sobre los resultados multilingües, donde el rendimiento se evalúa en el conjunto completo de temas. Los tres modelos tienen una diferencia significativa. Tabla 6: Rendimiento de los modelos de contexto (CT) en comparación con la línea base (BL). Las mejores puntuaciones están en negrita, demostradas en todas las medidas para ambas tareas. La cobertura de temas y candidatos para las tareas de búsqueda y perfilado de expertos, respectivamente, es cercana al 100% en todos los casos. La mejora relativa de las puntuaciones de precisión varía del 10% al 80%. Estas puntuaciones demuestran que, a pesar de su simplicidad, nuestro método para combinar resultados en varios idiomas logra mejoras sustanciales sobre el punto de referencia. CONCLUSIONES En este artículo nos enfocamos en la recuperación de expertos (búsqueda y perfilado de expertos) en un nuevo entorno de una organización típica intensiva en conocimiento en la que los datos disponibles son de alta calidad, multilingües y abarcan una amplia gama de áreas de expertise. Normalmente, la cantidad de datos disponibles en una organización como una universidad, un instituto de investigación o un laboratorio de investigación es limitada en comparación con la colección de la W3C que ha sido utilizada principalmente para la evaluación experimental de la recuperación de la experiencia hasta ahora. Para examinar la recuperación de la experiencia en este entorno, presentamos (y lanzamos) la colección de Expertos de UvT como un caso representativo de organizaciones intensivas en conocimiento. La nueva colección refleja las propiedades típicas de los institutos intensivos en conocimiento mencionados anteriormente e incluye también varias características que pueden ser potencialmente útiles para la recuperación de expertos, como la estructura temática y organizativa. Evaluamos cómo los modelos de vanguardia para la búsqueda y perfilado de expertos se desempeñaron en este nuevo entorno y luego refinamos estos modelos para intentar explotar las diferentes características dentro del entorno de datos (idioma, actualidad y estructura organizativa). Encontramos que los modelos actuales de recuperación de experiencia se generalizan bien a este nuevo entorno; además, descubrimos que refinar los modelos para tener en cuenta las diferencias resulta en mejoras significativas, compensando así los problemas causados por la escasez de datos. El trabajo futuro incluye establecer evaluaciones manuales de perfiles generados automáticamente por los propios empleados, especialmente en casos en los que los empleados no han proporcionado un perfil ellos mismos. 10. AGRADECIMIENTOS Krisztian Balog fue apoyado por la Organización Neerlandesa para la Investigación Científica (NWO) bajo el número de proyecto 220-80-001. Maarten de Rijke también recibió apoyo de NWO bajo los números de proyecto 017.001.190, 220-80-001, 264-70-050, 354-20-005, 600.065.120, 612-13-001, 612.000.106, 612.066.302, 612.069.006, 640.001.501, 640.002.501, y de la Unión Europea. Programa IST del 6º Programa Marco para la I+D en virtud del contrato del proyecto MultiMATCH IST-033104. El trabajo de Toine Bogers y Antal van den Bosch fue financiado por el programa IOP-MMI de SenterNovem / Ministerio de Asuntos Económicos de los Países Bajos, como parte del proyecto 'A Propos'. 11. REFERENCIAS [1] L. Azzopardi. Incorporando contexto en el marco de modelado del lenguaje para la recuperación de información ad-hoc. Tesis doctoral, Universidad de Paisley, 2005. [2] K. Balog y M. de Rijke. Encontrando expertos similares. En este volumen, 2007. [3] K. Balog y M. de Rijke. Determinación de perfiles de expertos (con una aplicación a la búsqueda de expertos). En IJCAI 07: Proc. 20th Intern. Conferencia Conjunta sobre Inteligencia Artificial, páginas 2657-2662, 2007. [4] K. Balog, L. Azzopardi y M. de Rijke. Modelos formales para la búsqueda de expertos en corporaciones empresariales. En SIGIR 06: Actas de la 29ª conferencia internacional anual. Conferencia ACM SIGIR sobre Investigación y desarrollo en recuperación de información, páginas 43-50, 2006. [5] I. Becerra-Fernández. El papel de las tecnologías de inteligencia artificial en la implementación de sistemas de gestión del conocimiento de localización de personas. En el taller de AAAI sobre la incorporación de conocimiento en los procesos de negocio, marzo de 2000. [6] C. S. Campbell, P. P. Maglio, A. Cozzi y B. Dom. Identificación de la experiencia utilizando comunicaciones por correo electrónico. En CIKM 03: Actas de la duodécima conferencia internacional sobre gestión de la información y el conocimiento, páginas 528-531, 2003. [7] G. Cao, J.-Y. Nie, y J. Bai. Integrando relaciones entre palabras en modelos de lenguaje. En SIGIR 05: Actas de la 28ª conferencia internacional anual. Conferencia ACM SIGIR sobre Investigación y desarrollo en recuperación de información, páginas 298-305, 2005. [8] T. M. Cover y J. A. Thomas. Elementos de la teoría de la información. Wiley-Interscience, 1991. [9] N. Craswell, D. Hawking, A. M. Vercoustre y P. Wilkins. Experto en P@noptic: Buscando expertos no solo documentos. En Ausweb, 2001. [10] N. Craswell, A. de Vries e I. Soboroff. Resumen de la pista empresarial TREC2005. En la Decimocuarta Conferencia de Recuperación de Información. Proc. (TREC 2005), 2006. [11] T. H. Davenport y L. Prusak. Conocimiento en Acción: Cómo las Organizaciones Gestionan lo que Saben. Harvard Business School Press, Boston, MA, 1998. [12] T. Dunning.\nHarvard Business School Press, Boston, MA, 1998. [12] T. Dunning. Métodos precisos para la estadística de sorpresa y coincidencia. Lingüística Computacional, 19(1):61-74, 1993. [13] E. Filatova y J. Prager. Dime qué haces y te diré quién eres: Aprendiendo actividades relacionadas con la ocupación para biografías. En HLT/EMNLP, 2005. [14] V. Lavrenko y W. B. Croft. Modelos de lenguaje basados en relevancia. En SIGIR 01: Actas de la 24ª conferencia internacional anual. Conferencia ACM SIGIR sobre Investigación y Desarrollo en Recuperación de Información, páginas 120-127, 2001. [15] V. Lavrenko, M. Choquette y W. B. Croft. Modelos de relevancia multilingües. En SIGIR 02: Actas de la 25ª conferencia internacional anual. Conferencia ACM SIGIR sobre Investigación y Desarrollo en Recuperación de Información, páginas 175-182, 2002. [16] C. Macdonald e I. Ounis. Votación de candidatos: adaptando técnicas de fusión de datos para una tarea de búsqueda de expertos. En CIKM 06: Actas de la 15ª conferencia internacional de ACM sobre gestión de la información y el conocimiento, páginas 387-396, 2006. [17] C. Manning y H. Schütze. Fundamentos del Procesamiento del Lenguaje Natural Estadístico. El MIT Press, 1999. [18] A. Mockus y J. D. Herbsleb. Explorador de experticia: un enfoque cuantitativo para identificar la experticia. En ICSE 02: Proc. 24th Intern. Conf. en Ingeniería de Software, páginas 503-512, 2002. [19] D. Petkova y W. B. Croft. Modelos de lenguaje jerárquicos para la búsqueda de expertos en corporaciones empresariales. En Proc. ICTAI 2006, páginas 599-608, 2006. [20] I. Soboroff, A. de Vries y N. Craswell. Resumen de la pista empresarial TREC 2006. En las Notas de Trabajo de TREC 2006, 2006. [21] T. Tao, X. Wang, Q. Mei y C. Zhai. Recuperación de información del modelo de lenguaje con expansión de documentos. En HLT-NAACL 2006, 2006. [22] TREC. Ruta empresarial, 2005. URL: http://www.ins.cwi.nl/projects/trec-ent/wiki/. [23] G. van Noord.\nURL: http://www.ins.cwi.nl/projects/trec-ent/wiki/. [23] G. van Noord. Adivinador de idioma TextCat. URL: http://www.let.rug.nl/˜vannoord/TextCat/. [24] W3C. La colección de pruebas del W3C, 2005. URL: http://research.microsoft.com/users/nickcr/w3c-summary.html. ",
            "candidates": [],
            "error": [
                []
            ]
        },
        "sparse datum environment": {
            "translated_key": "entorno de datos dispersos",
            "is_in_text": false,
            "original_annotated_sentences": [
                "Broad Expertise Retrieval in Sparse Data Environments Krisztian Balog ISLA, University of Amsterdam Kruislaan 403, 1098 SJ Amsterdam, The Netherlands kbalog@science.uva.nl Toine Bogers ILK, Tilburg University P.O.",
                "Box 90153, 5000 LE Tilburg, The Netherlands A.M.Bogers@uvt.nl Leif Azzopardi Dept. of Computing Science University of Glasgow, Glasgow, G12 8QQ leif@dcs.gla.ac.uk Maarten de Rijke ISLA, University of Amsterdam Kruislaan 403, 1098 SJ Amsterdam, The Netherlands mdr@science.uva.nl Antal van den Bosch ILK, Tilburg University P.O.",
                "Box 90153, 5000 LE Tilburg, The Netherlands Antal.vdnBosch@uvt.nl ABSTRACT Expertise retrieval has been largely unexplored on data other than the W3C collection.",
                "At the same time, many intranets of universities and other knowledge-intensive organisations offer examples of relatively small but clean multilingual expertise data, covering broad ranges of expertise areas.",
                "We first present two main expertise retrieval tasks, along with a set of baseline approaches based on generative language modeling, aimed at finding expertise relations between topics and people.",
                "For our experimental evaluation, we introduce (and release) a new test set based on a crawl of a university site.",
                "Using this test set, we conduct two series of experiments.",
                "The first is aimed at determining the effectiveness of baseline expertise retrieval methods applied to the new test set.",
                "The second is aimed at assessing refined models that exploit characteristic features of the new test set, such as the organizational structure of the university, and the hierarchical structure of the topics in the test set.",
                "Expertise retrieval models are shown to be robust with respect to environments smaller than the W3C collection, and current techniques appear to be generalizable to other settings.",
                "Categories and Subject Descriptors H.3 [Information Storage and Retrieval]: H.3.1 Content Analysis and Indexing; H.3.3 Information Search and Retrieval; H.3.4 Systems and Software; H.4 [Information Systems Applications]: H.4.2 Types of Systems; H.4.m Miscellaneous General Terms Algorithms, Measurement, Performance, Experimentation 1.",
                "INTRODUCTION An organizations intranet provides a means for exchanging information between employees and for facilitating employee collaborations.",
                "To efficiently and effectively achieve this, it is necessary to provide search facilities that enable employees not only to access documents, but also to identify expert colleagues.",
                "At the TREC Enterprise Track [22] the need to study and understand expertise retrieval has been recognized through the introduction of Expert Finding tasks.",
                "The goal of expert finding is to identify a list of people who are knowledgeable about a given topic.",
                "This task is usually addressed by uncovering associations between people and topics [10]; commonly, a co-occurrence of the name of a person with topics in the same context is assumed to be evidence of expertise.",
                "An alternative task, which using the same idea of people-topic associations, is expert profiling, where the task is to return a list of topics that a person is knowledgeable about [3].",
                "The launch of the Expert Finding task at TREC has generated a lot of interest in expertise retrieval, with rapid progress being made in terms of modeling, algorithms, and evaluation aspects.",
                "However, nearly all of the expert finding or profiling work performed has been validated experimentally using the W3C collection [24] from the Enterprise Track.",
                "While this collection is currently the only publicly available test collection for expertise retrieval tasks, it only represents one type of intranet.",
                "With only one test collection it is not possible to generalize conclusions to other realistic settings.",
                "In this paper we focus on expertise retrieval in a realistic setting that differs from the W3C setting-one in which relatively small amounts of clean, multilingual data are available, that cover a broad range of expertise areas, as can be found on the intranets of universities and other knowledge-intensive organizations.",
                "Typically, this setting features several additional types of structure: topical structure (e.g., topic hierarchies as employed by the organization), organizational structure (faculty, department, ...), as well as multiple types of documents (research and course descriptions, publications, and academic homepages).",
                "This setting is quite different from the W3C setting in ways that might impact upon the performance of expertise retrieval tasks.",
                "We focus on a number of research questions in this paper: Does the relatively small amount of data available on an intranet affect the quality of the topic-person associations that lie at the heart of expertise retrieval algorithms?",
                "How do state-of-the-art algorithms developed on the W3C data set perform in the alternative scenario of the type described above?",
                "More generally, do the lessons from the Expert Finding task at TREC carry over to this setting?",
                "How does the inclusion or exclusion of different documents affect expertise retrieval tasks?",
                "In addition to, how can the topical and organizational structure be used for retrieval purposes?",
                "To answer our research questions, we first present a set of baseline approaches, based on generative language modeling, aimed at finding associations between topics and people.",
                "This allows us to formulate the expert finding and expert profiling tasks in a uniform way, and has the added benefit of allowing us to understand the relations between the two tasks.",
                "For our experimental evaluation, we introduce a new data set (the UvT Expert Collection) which is representative of the type of intranet that we described above.",
                "Our collection is based on publicly available data, crawled from the website of Tilburg University (UvT).",
                "This type of data is particularly interesting, since (1) it is clean, heterogeneous, structured, and focused, but comprises a limited number of documents; (2) contains information on the organizational hierarchy; (3) it is bilingual (English and Dutch); and (4) the list of expertise areas of an individual are provided by the employees themselves.",
                "Using the UvT Expert collection, we conduct two sets of experiments.",
                "The first is aimed at determining the effectiveness of baseline expertise finding and profiling methods in this new setting.",
                "A second group of experiments is aimed at extensions of the baseline methods that exploit characteristic features of the UvT Expert Collection; specifically, we propose and evaluate refined expert finding and profiling methods that incorporate topicality and organizational structure.",
                "Apart from the research questions and data set that we contribute, our main contributions are as follows.",
                "The baseline models developed for expertise finding perform well on the new data set.",
                "While on the W3C setting the expert finding task appears to be more difficult than profiling, for the UvT data the opposite is the case.",
                "We find that profiling on the UvT data set is considerably more difficult than on the W3C set, which we believe is due to the large (but realistic) number of topical areas that we used for profiling: about 1,500 for the UvT set, versus 50 in the W3C case.",
                "Taking the similarity between topics into account can significantly improve retrieval performance.",
                "The best performing similarity measures are content-based, therefore they can be applied on the W3C (and other) settings as well.",
                "Finally, we demonstrate that the organizational structure can be exploited in the form of a context model, improving MAP scores for certain models by up to 70%.",
                "The remainder of this paper is organized as follows.",
                "In the next section we review related work.",
                "Then, in Section 3 we provide detailed descriptions of the expertise retrieval tasks that we address in this paper: expert finding and expert profiling.",
                "In Section 4 we present our baseline models, of which the performance is then assessed in Section 6 using the UvT data set that we introduce in Section 5.",
                "Advanced models exploiting specific features of our data are presented in Section 7 and evaluated in Section 8.",
                "We formulate our conclusions in Section 9. 2.",
                "RELATED WORK Initial approaches to expertise finding often employed databases containing information on the skills and knowledge of each individual in the organization [11].",
                "Most of these tools (usually called yellow pages or people-finding systems) rely on people to self-assess their skills against a predefined set of keywords.",
                "For updating profiles in these systems in an automatic fashion there is a need for intelligent technologies [5].",
                "More recent approaches use specific document sets (such as email [6] or software [18]) to find expertise.",
                "In contrast with focusing on particular document types, there is also an increased interest in the development of systems that index and mine published intranet documents as sources of evidence for expertise.",
                "One such published approach is the P@noptic system [9], which builds a representation of each person by concatenating all documents associated with that person-this is similar to Model 1 of Balog et al. [4], who formalize and compare two methods.",
                "Balog et al.s Model 1 directly models the knowledge of an expert from associated documents, while their Model 2 first locates documents on the topic and then finds the associated experts.",
                "In the reported experiments the second method performs significantly better when there are sufficiently many associated documents per candidate.",
                "Most systems that took part in the 2005 and 2006 editions of the Expert Finding task at TREC implemented (variations on) one of these two models; see [10, 20].",
                "Macdonald and Ounis [16] propose a different approach for ranking candidate expertise with respect to a topic based on data fusion techniques, without using collectionspecific heuristics; they find that applying field-based weighting models improves the ranking of candidates.",
                "Petkova and Croft [19] propose yet another approach, based on a combination of the above Model 1 and 2, explicitly modeling topics.",
                "Turning to other expert retrieval tasks that can also be addressed using topic-people associations, Balog and de Rijke [3] addressed the task of determining topical expert profiles.",
                "While their methods proved to be efficient on the W3C corpus, they require an amount of data that may not be available in the typical knowledge-intensive organization.",
                "Balog and de Rijke [2] study the related task of finding experts that are similar to a small set of experts given as input.",
                "As an aside, creating a textual summary of a person shows some similarities to biography finding, which has received a considerable amount of attention recently; see e.g., [13].",
                "We use generative language modeling to find associations between topics and people.",
                "In our modeling of expert finding and profiling we collect evidence for expertise from multiple sources, in a heterogeneous collection, and integrate it with the co-occurrence of candidates names and query terms-the language modeling setting allows us to do this in a transparent manner.",
                "Our modeling proceeds in two steps.",
                "In the first step, we consider three baseline models, two taken from [4] (the Models 1 and 2 mentioned above), and one a refined version of a model introduced in [3] (which we refer to as Model 3 below); this third model is also similar to the model described by Petkova and Croft [19].",
                "The models we consider in our second round of experiments are mixture models similar to contextual language models [1] and to the expanded documents of Tao et al. [21]; however, the features that we use for definining our expansions-including topical structure and organizational structure-have not been used in this way before. 3.",
                "TASKS In the expertise retrieval scenario that we envisage, users seeking expertise within an organization have access to an interface that combines a search box (where they can search for experts or topics) with navigational structures (of experts and of topics) that allows them to click their way to an expert page (providing the profile of a person) or a topic page (providing a list of experts on the topic).",
                "To feed the above interface, we face two expertise retrieval tasks, expert finding and expert profiling, that we first define and then formalize using generative language models.",
                "In order to model either task, the probability of the query topic being associated to a candidate expert plays a key role in the final estimates for searching and profiling.",
                "By using language models, both the candidates and the query are characterized by distributions of terms in the vocabulary (used in the documents made available by the organization whose expertise retrieval needs we are addressing). 3.1 Expert finding Expert finding involves the task of finding the right person with the appropriate skills and knowledge: Who are the experts on topic X?.",
                "E.g., an employee wants to ascertain who worked on a particular project to find out why particular decisions were made without having to trawl through documentation (if there is any).",
                "Or, they may be in need a trained specialist for consultancy on a specific problem.",
                "Within an organization there are usually many possible candidates who could be experts for given topic.",
                "We can state this problem as follows: What is the probability of a candidate ca being an expert given the query topic q?",
                "That is, we determine p(ca|q), and rank candidates ca according to this probability.",
                "The candidates with the highest probability given the query are deemed the most likely experts for that topic.",
                "The challenge is how to estimate this probability accurately.",
                "Since the query is likely to consist of only a few terms to describe the expertise required, we should be able to obtain a more accurate estimate by invoking Bayes Theorem, and estimating: p(ca|q) = p(q|ca)p(ca) p(q) , (1) where p(ca) is the probability of a candidate and p(q) is the probability of a query.",
                "Since p(q) is a constant, it can be ignored for ranking purposes.",
                "Thus, the probability of a candidate ca being an expert given the query q is proportional to the probability of a query given the candidate p(q|ca), weighted by the a priori belief p(ca) that candidate ca is an expert. p(ca|q) ∝ p(q|ca)p(ca) (2) In this paper our main focus is on estimating the probability of a query given the candidate p(q|ca), because this probability captures the extent to which the candidate knows about the query topic.",
                "Whereas the candidate priors are generally assumed to be uniformand thus will not influence the ranking-it has been demonstrated that a sensible choice of priors may improve the performance [20]. 3.2 Expert profiling While the task of expert searching was concerned with finding experts given a particular topic, the task of expert profiling seeks to answer a related question: What topics does a candidate know about?",
                "Essentially, this turns the questions of expert finding around.",
                "The profiling of an individual candidate involves the identification of areas of skills and knowledge that they have expertise about and an evaluation of the level of proficiency in each of these areas.",
                "This is the candidates topical profile.",
                "Generally, topical profiles within organizations consist of tabular structures which explicitly catalogue the skills and knowledge of each individual in the organization.",
                "However, such practice is limited by the resources available for defining, creating, maintaining, and updating these profiles over time.",
                "By focusing on automatic methods which draw upon the available evidence within the document repositories of an organization, our aim is to reduce the human effort associated with the maintenance of topical profiles1 .",
                "A topical profile of a candidate, then, is defined as a vector where each element i of the vector corresponds to the candidate cas expertise on a given topic ki, (i.e., s(ca, ki)).",
                "Each topic ki defines a particular knowledge area or skill that the organization uses to define the candidates topical profile.",
                "Thus, it is assumed that a list of topics, {k1, . . . , kn}, where n is the number of pre-defined topics, is given: profile(ca) = s(ca, k1), s(ca, k2), . . . , s(ca, kn) . (3) 1 Context and evidence are needed to help users of expertise finding systems to decide whom to contact when seeking expertise in a particular area.",
                "Examples of such context are: Who does she work with?",
                "What are her contact details?",
                "Is she well-connected, just in case she is not able to help us herself?",
                "What is her role in the organization?",
                "Who is her superior?",
                "Collaborators, and affiliations, etc. are all part of the candidates social profile, and can serve as a background against which the systems recommendations should be interpreted.",
                "In this paper we only address the problem of determining topical profiles, and leave social profiling to further work.",
                "We state the problem of quantifying the competence of a person on a certain knowledge area as follows: What is the probability of a knowledge area (ki) being part of the candidates (expertise) profile? where s(ca, ki) is defined by p(ki|ca).",
                "Our task, then, is to estimate p(ki|ca), which is equivalent to the problem of obtaining p(q|ca), where the topic ki is represented as a query topic q, i.e., a sequence of keywords representing the expertise required.",
                "Both the expert finding and profiling tasks rely on the accurate estimation of p(q|ca).",
                "The only difference derives from the prior probability that a person is an expert (p(ca)), which can be incorporated into the expert finding task.",
                "This prior does not apply to the profiling task since the candidate (individual) is fixed. 4.",
                "BASELINE MODELS In this section we describe our baseline models for estimating p(q|ca), i.e., associations between topics and people.",
                "Both expert finding and expert profiling boil down to this estimation.",
                "We employ three models for calculating this probability. 4.1 From topics to candidates Using Candidate Models: Model 1 Model 1 [4] defines the probability of a query given a candidate (p(q|ca)) using standard language modeling techniques, based on a multinomial unigram language model.",
                "For each candidate ca, a candidate language model θca is inferred such that the probability of a term given θca is nonzero for all terms, i.e., p(t|θca) > 0.",
                "From the candidate model the query is generated with the following probability: p(q|θca) = Y t∈q p(t|θca)n(t,q) , where each term t in the query q is sampled identically and independently, and n(t, q) is the number of times t occurs in q.",
                "The candidate language model is inferred as follows: (1) an empirical model p(t|ca) is computed; (2) it is smoothed with background probabilities.",
                "Using the associations between a candidate and a document, the probability p(t|ca) can be approximated by: p(t|ca) = X d p(t|d)p(d|ca), where p(d|ca) is the probability that candidate ca generates a supporting document d, and p(t|d) is the probability of a term t occurring in the document d. We use the maximum-likelihood estimate of a term, that is, the normalised frequency of the term t in document d. The strength of the association between document d and candidate ca expressed by p(d|ca) reflects the degree to which the candidates expertise is described using this document.",
                "The estimation of this probability is presented later, in Section 4.2.",
                "The candidate model is then constructed as a linear interpolation of p(t|ca) and the background model p(t) to ensure there are no zero probabilities, which results in the final estimation: p(q|θca) = (4) Y t∈q ( (1 − λ) X d p(t|d)p(d|ca) ! + λp(t) )n(t,q) .",
                "Model 1 amasses all the term information from all the documents associated with the candidate, and uses this to represent that candidate.",
                "This model is used to predict how likely a candidate would produce a query q.",
                "This can can be intuitively interpreted as the probability of this candidate talking about the query topic, where we assume that this is indicative of their expertise.",
                "Using Document Models: Model 2 Model 2 [4] takes a different approach.",
                "Here, the process is broken into two parts.",
                "Given a candidate ca, (1) a document that is associated with a candidate is selected with probability p(d|ca), and (2) from this document a query q is generated with probability p(q|d).",
                "Then the sum over all documents is taken to obtain p(q|ca), such that: p(q|ca) = X d p(q|d)p(d|ca). (5) The probability of a query given a document is estimated by inferring a document language model θd for each document d in a similar manner as the candidate model was inferred: p(t|θd) = (1 − λ)p(t|d) + λp(t), (6) where p(t|d) is the probability of the term in the document.",
                "The probability of a query given the document model is: p(q|θd) = Y t∈q p(t|θd)n(t,q) .",
                "The final estimate of p(q|ca) is obtained by substituting p(q|d) for p(q|θd) into Eq. 5 (see [4] for full details).",
                "Conceptually, Model 2 differs from Model 1 because the candidate is not directly modeled.",
                "Instead, the document acts like a hidden variable in the process which separates the query from the candidate.",
                "This process is akin to how a user may search for candidates with a standard search engine: initially by finding the documents which are relevant, and then seeing who is associated with that document.",
                "By examining a number of documents the user can obtain an idea of which candidates are more likely to discuss the topic q.",
                "Using Topic Models: Model 3 We introduce a third model, Model 3.",
                "Instead of attempting to model the query generation process via candidate or document models, we represent the query as a topic language model and directly estimate the probability of the candidate p(ca|q).",
                "This approach is similar to the model presented in [3, 19].",
                "As with the previous models, a language model is inferred, but this time for the query.",
                "We adapt the work of Lavrenko and Croft [14] to estimate a topic model from the query.",
                "The procedure is as follows.",
                "Given a collection of documents and a query topic q, it is assumed that there exists an unknown topic model θk that assigns probabilities p(t|θk) to the term occurrences in the topic documents.",
                "Both the query and the documents are samples from θk (as opposed to the previous approaches, where a query is assumed to be sampled from a specific document or candidate model).",
                "The main task is to estimate p(t|θk), the probability of a term given the topic model.",
                "Since the query q is very sparse, and as there are no examples of documents on the topic, this distribution needs to be approximated.",
                "Lavrenko and Croft [14] suggest a reasonable way of obtaining such an approximation, by assuming that p(t|θk) can be approximated by the probability of term t given the query q.",
                "We can then estimate p(t|q) using the joint probability of observing the term t together with the query terms, q1, . . . , qm, and dividing by the joint probability of the query terms: p(t|θk) ≈ p(t|q) = p(t, q1, . . . , qm) p(q1, . . . , qm) = p(t, q1, . . . , qm) P t ∈T p(t , q1, . . . , qm) , where p(q1, . . . , qm) = P t ∈T p(t , q1, . . . , qm), and T is the entire vocabulary of terms.",
                "In order to estimate the joint probability p(t, q1, . . . , qm), we follow [14, 15] and assume t and q1, . . . , qm are mutually independent, once we pick a source distribution from the set of underlying source distributions U.",
                "If we choose U to be a set of document models. then to construct this set, the query q would be issued against the collection, and the top n returned are assumed to be relevant to the topic, and thus treated as samples from the topic model. (Note that candidate models could be used instead.)",
                "With the document models forming U, the joint probability of term and query becomes: p(t, q1, . . . , qm) = X d∈U p(d) ˘ p(t|θd) mY i=1 p(qi|θd) ¯ . (7) Here, p(d) denotes the prior distribution over the set U, which reflects the relevance of the document to the topic.",
                "We assume that p(d) is uniform across U.",
                "In order to rank candidates according to the topic model defined, we use the Kullback-Leibler divergence metric (KL, [8]) to measure the difference between the candidate models and the topic model: KL(θk||θca) = X t p(t|θk) log p(t|θk) p(t|θca) . (8) Candidates with a smaller divergence from the topic model are considered to be more likely experts on that topic.",
                "The candidate model θca is defined in Eq. 4.",
                "By using KL divergence instead of the probability of a candidate given the topic model p(ca|θk), we avoid normalization problems. 4.2 Document-candidate associations For our models we need to be able to estimate the probability p(d|ca), which expresses the extent to which a document d characterizes the candidate ca.",
                "In [4], two methods are presented for estimating this probability, based on the number of person names recognized in a document.",
                "However, in our (intranet) setting it is reasonable to assume that authors of documents can unambiguously be identified (e.g., as the author of an article, the teacher assigned to a course, the owner of a web page, etc.)",
                "Hence, we set p(d|ca) to be 1 if candidate ca is author of document d, otherwise the probability is 0.",
                "In Section 6 we describe how authorship can be determined on different types of documents within the collection. 5.",
                "THE UVT EXPERT COLLECTION The UvT Expert collection used in the experiments in this paper fits the scenario outlined in Section 3.",
                "The collection is based on the Webwijs (Webwise) system developed at Tilburg University (UvT) in the Netherlands.",
                "Webwijs (http://www.uvt.nl/ webwijs/) is a publicly accessible database of UvT employees who are involved in research or teaching; currently, Webwijs contains information about 1168 experts, each of whom has a page with contact information and, if made available by the expert, a research description and publications list.",
                "In addition, each expert can select expertise areas from a list of 1491 topics and is encouraged to suggest new topics that need to be approved by the Webwijs editor.",
                "Each topic has a separate page that shows all experts associated with that topic and, if available, a list of related topics.",
                "Webwijs is available in Dutch and English, and this bilinguality has been preserved in the collection.",
                "Every Dutch Webwijs page has an English translation.",
                "Not all Dutch topics have an English translation, but the reverse is true: the 981 English topics all have a Dutch equivalent.",
                "About 42% of the experts teach courses at Tilburg University; these courses were also crawled and included in the profile.",
                "In addition, about 27% of the experts link to their academic homepage from their Webwijs page.",
                "These home pages were crawled and added to the collection. (This means that if experts put the full-text versions of their publications on their academic homepage, these were also available for indexing.)",
                "We also obtained 1880 full-text versions of publications from the UvT institutional repository and Dutch English no. of experts 1168 1168 no. of experts with ≥ 1 topic 743 727 no. of topics 1491 981 no. of expert-topic pairs 4318 3251 avg. no. of topics/expert 5.8 5.9 max. no. of topics/expert (no. of experts) 60 (1) 35 (1) min. no. of topics/expert (no. of experts) 1 (74) 1 (106) avg. no. of experts/topic 2.9 3.3 max. no. of experts/topic (no. of topics) 30 (1) 30 (1) min. no. of experts/topic (no. of topics) 1 (615) 1 (346) no. of experts with HP 318 318 no. of experts with CD 318 318 avg. no. of CDs per teaching expert 3.5 3.5 no. of experts with RD 329 313 no. of experts with PUB 734 734 avg. no. of PUBs per expert 27.0 27.0 avg. no. of PUB citations per expert 25.2 25.2 avg. no. of full-text PUBs per expert 1.8 1.8 Table 2: Descriptive statistics of the Dutch and English versions of the UvT Expert collection. converted them to plain text.",
                "We ran the TextCat [23] language identifier to classify the language of the home pages and the fulltext publications.",
                "We restricted ourselves to pages where the classifier was confident about the language used on the page.",
                "This resulted in four document types: research descriptions (RD), course descriptions (CD), publications (PUB; full-text and citationonly versions), and academic homepages (HP).",
                "Everything was bundled into the UvT Expert collection which is available at http: //ilk.uvt.nl/uvt-expert-collection/.",
                "The UvT Expert collection was extracted from a different organizational setting than the W3C collection and differs from it in a number of ways.",
                "The UvT setting is one with relatively small amounts of multilingual data.",
                "Document-author associations are clear and the data is structured and clean.",
                "The collection covers a broad range of expertise areas, as one can typically find on intranets of universities and other knowledge-intensive institutes.",
                "Additionally, our university setting features several types of structure (topical and organizational), as well as multiple document types.",
                "Another important difference between the two data sets is that the expertise areas in the UvT Expert collection are self-selected instead of being based on group membership or assignments by others.",
                "Size is another dimension along which the W3C and UvT Expert collections differ: the latter is the smaller of the two.",
                "Also realistic are the large differences in the amount of information available for each expert.",
                "Utilizing Webwijs is voluntary; 425 Dutch experts did not select any topics at all.",
                "This leaves us with 743 Dutch and 727 English usable expert profiles.",
                "Table 2 provides descriptive statistics for the UvT Expert collection.",
                "Universities tend to have a hierarchical structure that goes from the faculty level, to departments, research groups, down to the individual researchers.",
                "In the UvT Expert collection we have information about the affiliations of researchers with faculties and institutes, providing us with a two-level organizational hierarchy.",
                "Tilburg University has 22 organizational units at the faculty level (including the university office and several research institutes) and 71 departments, which amounts to 3.2 departments per faculty.",
                "As to the topical hierarchy used by Webwijs, 131 of the 1491 topics are top nodes in the hierarchy.",
                "This hierarchy has an average topic chain length of 2.65 and a maximum length of 7 topics. 6.",
                "EVALUATION Below, we evaluate Section 4s models for expert finding and profiling onthe UvT Expert collection.",
                "We detail our research questions and experimental setup, and then present our results. 6.1 Research Questions We address the following research questions.",
                "Both expert finding and profiling rely on the estimations of p(q|ca).",
                "The question is how the models compare on the different tasks, and in the setting of the UvT Expert collection.",
                "In [4], Model 2 outperformed Model 1 on the W3C collection.",
                "How do they compare on our data set?",
                "And how does Model 3 compare to Model 1?",
                "What about performance differences between the two languages in our test collection? 6.2 Experimental Setup The output of our models was evaluated against the self-assigned topic labels, which were treated as relevance judgements.",
                "Results were evaluated separately for English and Dutch.",
                "For English we only used topics for which the Dutch translation was available; for Dutch all topics were considered.",
                "The results were averaged for the queries in the intersection of relevance judgements and results; missing queries do not contribute a value of 0 to the scores.",
                "We use standard information retrieval measures, such as Mean Average Precision (MAP) and Mean Reciprocal Rank (MRR).",
                "We also report the percentage of topics (%q) and candidates (%ca) covered, for the expert finding and profiling tasks, respectively. 6.3 Results Table 1 shows the performance of Model 1, 2, and 3 on the expert finding and profiling tasks.",
                "The rows of the table correspond to the various document types (RD, CD, PUB, and HP) and to their combinations.",
                "RD+CD+PUB+HP is equivalent to the full collection and will be referred as the BASELINE of our experiments.",
                "Looking at Table 1 we see that Model 2 performs the best across the board.",
                "However, when the data is clean and very focused (RD), Model 3 outperforms it in a number of cases.",
                "Model 1 has the best coverage of candidates (%ca) and topics (%q).",
                "The various document types differ in their characteristics and how they improve the finding and profiling tasks.",
                "Expert profiling benefits much from the clean data present in the RD and CD document types, while the publications contribute the most to the expert finding task.",
                "Adding the homepages does not prove to be particularly useful.",
                "When we compare the results across languages, we find that the coverage of English topics (%q) is higher than of the Dutch ones for expert finding.",
                "Apart from that, the scores fall in the same range for both languages.",
                "For the profiling task the coverage of the candidates (%ca) is very similar for both languages.",
                "However, the performance is substantially better for the English topics.",
                "While it is hard to compare scores across collections, we conclude with a brief comparison of the absolute scores in Table 1 to those reported in [3, 4] on the W3C test set (2005 edition).",
                "For expert finding the MAP scores for Model 2 reported here are about 50% higher than the corresponding figures in [4], while our MRR scores are slightly below those in [4].",
                "For expert profiling, the differences are far more dramatic: the MAP scores for Model 2 reported here are around 50% below the scores in [3], while the (best) MRR scores are about the same as those in [3].",
                "The cause for the latter differences seems to reside in the number of knowledge areas considered here-approx. 30 times more than in the W3C setting. 7.",
                "ADVANCED MODELS Now that we have developed and assessed basic language modeling techniques for expertise retrieval, we turn to refined models that exploit special features of our test collection. 7.1 Exploiting knowledge area similarity One way to improve the scoring of a query given a candidate is to consider what other requests the candidate would satisfy and use them as further evidence to support the original query, proportional Expert finding Expert profiling Document types Model 1 Model 2 Model 3 Model 1 Model 2 Model 3 %q MAP MRR %q MAP MRR %q MAP MRR %ca MAP MRR %ca MAP MRR %ca MAP MRR English RD 97.8 0.126 0.269 83.5 0.144 0.311 83.3 0.129 0.271 100 0.089 0.189 39.3 0.232 0.465 41.1 0.166 0.337 CD 97.8 0.118 0.227 91.7 0.123 0.248 91.7 0.118 0.226 32.8 0.188 0.381 32.4 0.195 0.385 32.7 0.203 0.370 PUB 97.8 0.200 0.330 98.0 0.216 0.372 98.0 0.145 0.257 78.9 0.167 0.364 74.5 0.212 0.442 78.9 0.135 0.299 HP 97.8 0.081 0.186 97.4 0.071 0.168 97.2 0.062 0.149 31.2 0.150 0.299 28.8 0.185 0.335 30.1 0.136 0.287 RD+CD 97.8 0.188 0.352 92.9 0.193 0.360 92.9 0.150 0.273 100 0.145 0.286 61.3 0.251 0.477 63.2 0.217 0.416 RD+CD+PUB 97.8 0.235 0.373 98.1 0.277 0.439 98.1 0.178 0.305 100 0.196 0.380 87.2 0.280 0.533 89.5 0.170 0.344 RD+CD+PUB+HP 97.8 0.237 0.372 98.6 0.280 0.441 98.5 0.166 0.293 100 0.199 0.387 88.7 0.281 0.525 90.9 0.169 0.329 Dutch RD 61.3 0.094 0.229 38.4 0.137 0.336 38.3 0.127 0.295 38.0 0.127 0.386 34.1 0.138 0.420 38.0 0.105 0.327 CD 61.3 0.107 0.212 49.7 0.128 0.256 49.7 0.136 0.261 32.5 0.151 0.389 31.8 0.158 0.396 32.5 0.170 0.380 PUB 61.3 0.193 0.319 59.5 0.218 0.368 59.4 0.173 0.291 78.8 0.126 0.364 76.0 0.150 0.424 78.8 0.103 0.294 HP 61.3 0.063 0.169 56.6 0.064 0.175 56.4 0.062 0.163 29.8 0.108 0.308 27.8 0.125 0.338 29.8 0.098 0.255 RD+CD 61.3 0.159 0.314 51.9 0.184 0.360 51.9 0.169 0.324 60.5 0.151 0.410 57.2 0.166 0.431 60.4 0.159 0.384 RD+CD+PUB 61.3 0.244 0.398 61.5 0.260 0.424 61.4 0.210 0.350 90.3 0.165 0.445 88.2 0.189 0.479 90.3 0.126 0.339 RD+CD+PUB+HP 61.3 0.249 0.401 62.6 0.265 0.436 62.6 0.195 0.344 91.9 0.164 0.426 90.1 0.195 0.488 91.9 0.125 0.328 Table 1: Performance of the models on the expert finding and profiling tasks, using different document types and their combinations. %q is the number of topics covered (applies to the expert finding task), %ca is the number of candidates covered (applies to the expert profiling task).",
                "The top and bottom blocks correspond to English and Dutch respectively.",
                "The best scores are in boldface. to how related the other requests are to the original query.",
                "This can be modeled by interpolating between the p(q|ca) and the further supporting evidence from all similar requests q , as follows: p (q|ca) = λp(q|ca) + (1 − λ) X q p(q|q )p(q |ca), (9) where p(q|q ) represents the similarity between the two topics q and q .",
                "To be able to work with similarity methods that are not necessarily probabilities, we set p(q|q ) = w(q,q ) γ , where γ is a normalizing constant, such that γ = P q w(q , q ).",
                "We consider four methods for calculating the similarity score between two topics.",
                "Three approaches are strictly content-based, and establish similarity by examining co-occurrence patterns of topics within the collection, while the last approach exploits the hierarchical structure of topical areas that may be present within an organization (see [7] for further examples of integrating word relationships into language models).",
                "The Kullback-Leibler (KL) divergence metric defined in Eq. 8 provides a measure of how different or similar two probability distributions are.",
                "A topic model is inferred for q and q using the method presented in Section 4.1 to describe the query across the entire vocabulary.",
                "Since a lower KL score means the queries are more similar, we let w(q, q ) = max(KL(θq||·) − KL(θq||θq )).",
                "Pointwise Mutual Information (PMI, [17]) is a measure of association used in information theory to determine the extent of independence between variables.",
                "The dependence between two queries is reflected by the SI(q, q ) score, where scores greater than zero indicate that it is likely that there is a dependence, which we take to mean that the queries are likely to be similar: SI(q, q ) = log p(q, q ) p(q)p(q ) (10) We estimate the probability of a topic p(q) using the number of documents relevant to query q within the collection.",
                "The joint probability p(q, q ) is estimated similarly, by using the concatenation of q and q as a query.",
                "To obtain p(q|q ), we then set w(q, q ) = SI(q, q ) when SI(q, q ) > 0 otherwise w(q, q ) = 0, because we are only interested in including queries that are similar.",
                "The log-likelihood statistic provides another measure of dependence, which is more reliable than the pointwise mutual information measure [17].",
                "Let k1 be the number of co-occurrences of q and q , k2 the number of occurrences of q not co-occurring with q , n1 the total number of occurrences of q , and n2 the total number of topic tokens minus the number of occurrences of q .",
                "Then, let p1 = k1/n1, p2 = k2/n2, and p = (k1 + k2)/(n1 + n2), (q, q ) = 2( (p1, k1, n1) + (p2, k2, n2) − (p, k1, n1) − (p, k2, n2)), where (p, n, k) = k log p + (n − k) log(1 − p).",
                "The higher score indicate that queries are also likely to be similar, thus we set w(q, q ) = (q, q ).",
                "Finally, we also estimate the similarity of two topics based on their distance within the topic hierarchy.",
                "The topic hierarchy is viewed as a directed graph, and for all topic-pairs the shortest path SP(q, q ) is calculated.",
                "We set the similarity score to be the reciprocal of the shortest path: w(q, q ) = 1/SP(q, q ). 7.2 Contextual information Given the hierarchy of an organization, the units to which a person belong are regarded as a context so as to compensate for data sparseness.",
                "We model it as follows: p (q|ca) = 1 − P ou∈OU(ca) λou · p(q|ca) + P ou∈OU(ca) λou · p(q|ou), where OU(ca) is the set of organizational units of which candidate ca is a member of, and p(q|o) expresses the strength of the association between query q and the unit ou.",
                "The latter probability can be estimated using either of the three basic models, by simply replacing ca with ou in the corresponding equations.",
                "An organizational unit is associated with all the documents that its members have authored.",
                "That is, p(d|ou) = maxca∈ou p(d|ca). 7.3 A simple multilingual model For knowledge institutes in Europe, academic or otherwise, a multilingual (or at least bilingual) setting is typical.",
                "The following model builds on a kind of independence assumption: there is no spill-over of expertise/profiles across language boundaries.",
                "While a simplification, this is a sensible first approach.",
                "That is: p (q|ca) =P l∈L λl · p(ql|ca), where L is the set of languages used in the collection, ql is the translation of the query q to language l, and λl is a language specific smoothing parameter, such that P l∈L λl = 1. 8.",
                "ADVANCED MODELS: EVALUATION In this section we present an experimental evaluation of our advanced models.",
                "Expert finding Expert profiling Language Model 1 Model 2 Model 3 Model 1 Model 2 Model 3 %q MAP MRR %q MAP MRR %q MAP MRR %ca MAP MRR %ca MAP MRR %ca MAP MRR English only 97.8 0.237 0.372 98.6 0.280 0.441 98.5 0.166 0.293 100 0.199 0.387 88.7 0.281 0.525 90.9 0.169 0.329 Dutch only 61.3 0.249 0.401 62.6 0.265 0.436 62.6 0.195 0.344 91.9 0.164 0.426 90.1 0.195 0.488 91.9 0.125 0.328 Combination 99.4 0.297 0.444 99.7 0.324 0.491 99.7 0.223 0.388 100 0.241 0.445 92.1 0.313 0.564 93.2 0.224 0.411 Table 3: Performance of the combination of languages on the expert finding and profiling tasks (on candidates).",
                "Best scores for each model are in italic, absolute best scores for the expert finding and profiling tasks are in boldface.",
                "Method Model 1 Model 2 Model 3 MAP MRR MAP MRR MAP MRR English BASELINE 0.296 0.454 0.339 0.509 0.221 0.333 KLDIV 0.291 0.453 0.327 0.503 0.219 0.330 PMI 0.291 0.453 0.337 0.509 0.219 0.331 LL 0.319 0.490 0.360 0.524 0.233 0.368 HDIST 0.299 0.465 0.346 0.537 0.219 0.332 Dutch BASELINE 0.240 0.350 0.271 0.403 0.227 0.389 KLDIV 0.239 0.347 0.253 0.386 0.224 0.385 PMI 0.239 0.350 0.260 0.392 0.227 0.389 LL 0.255 0.372 0.281 0.425 0.231 0.389 HDIST 0.253 0.365 0.271 0.407 0.236 0.402 Method Model 1 Model 2 Model 3 MAP MRR MAP MRR MAP MRR English BASELINE 0.485 0.546 0.499 0.548 0.381 0.416 KLDIV 0.510 0.564 0.513 0.558 0.381 0.416 PMI 0.486 0.546 0.495 0.542 0.407 0.451 LL 0.558 0.589 0.586 0.617 0.408 0.453 HDIST 0.507 0.567 0.512 0.563 0.386 0.420 Dutch BASELINE 0.263 0.313 0.294 0.358 0.262 0.315 KLDIV 0.284 0.336 0.271 0.321 0.261 0.314 PMI 0.265 0.317 0.265 0.316 0.273 0.330 LL 0.312 0.351 0.330 0.377 0.284 0.331 HDIST 0.280 0.327 0.288 0.341 0.266 0.321 Table 4: Performance on the expert finding (top) and profiling (bottom) tasks, using knowledge area similarities.",
                "Runs were evaluated on the main topics set.",
                "Best scores are in boldface. 8.1 Research Questions Our questions follow the refinements presented in the preceding section: Does exploiting the knowledge area similarity improve effectiveness?",
                "Which of the various methods for capturing word relationships is most effective?",
                "Furthermore, is our way of bringing in contextual information useful?",
                "For which tasks?",
                "And finally, is our simple way of combining the monolingual scores sufficient for obtaining significant improvements? 8.2 Experimental setup Given that the self-assessments are also sparse in our collection, in order to be able to measure differences between the various models, we selected a subset of topics, and evaluated (some of the) runs only on this subset.",
                "This set is referred as main topics, and consists of topics that are located at the top level of the topical hierarchy. (A main topic has subtopics, but is not a subtopic of any other topic.)",
                "This main set consists of 132 Dutch and 119 English topics.",
                "The relevance judgements were restricted to the main topic set, but were not expanded with subtopics. 8.3 Exploiting knowledge area similarity Table 4 presents the results.",
                "The four methods used for estimating knowledge-area similarity are KL divergence (KLDIV), PointLang.",
                "Topics Model 1 Model 2 Model 3 MAP MRR MAP MRR MAP MRR Expert finding UK ALL 0.423 0.545 0.654 0.799 0.494 0.629 UK MAIN 0.500 0.621 0.704 0.834 0.587 0.699 NL ALL 0.439 0.560 0.672 0.826 0.480 0.630 NL MAIN 0.440 0.584 0.645 0.816 0.515 0.655 Expert profiling UK ALL 0.240 0.640 0.306 0.778 0.223 0.616 UK MAIN 0.523 0.677 0.519 0.648 0.461 0.587 NL ALL 0.203 0.716 0.254 0.770 0.183 0.627 NL MAIN 0.332 0.576 0.380 0.624 0.332 0.549 Table 5: Evaluating the context models on organizational units. wise mutual information (PMI), log-likelihood (LL), and distance within topic hierarchy (HDIST).",
                "We managed to improve upon the baseline in all cases, but the improvement is more noticeable for the profiling task.",
                "For both tasks, the LL method performed best.",
                "The content-based approaches performed consistently better than HDIST. 8.4 Contextual information A two level hierarchy of organizational units (faculties and institutes) is available in the UvT Expert collection.",
                "The unit a person belongs to is used as a context for that person.",
                "First, we evaluated the models of the organizational units, using all topics (ALL) and only the main topics (MAIN).",
                "An organizational unit is considered to be relevant for a given topic (or vice versa) if at least one member of the unit selected the given topic as an expertise area.",
                "Table 5 reports on the results.",
                "As far as expert finding goes, given a topic, the corresponding organizational unit can be identified with high precision.",
                "However, the expert profiling task shows a different picture: the scores are low, and the task seems hard.",
                "The explanation may be that general concepts (i.e., our main topics) may belong to several organizational units.",
                "Second, we performed another evaluation, where we combined the contextual models with the candidate models (to score candidates again).",
                "Table 6 reports on the results.",
                "We find a positive impact of the context models only for expert finding.",
                "Noticably, for expert finding (and Model 1), it improves over 50% (for English) and over 70% (for Dutch) on MAP.",
                "The poor performance on expert profiling may be due to the fact that context models alone did not perform very well on the profiling task to begin with. 8.5 Multilingual models In this subsection we evaluate the method for combining results across multiple languages that we described in Section 7.3.",
                "In our setting the set of languages consists of English and Dutch: L = {UK, NL}.",
                "The weights on these languages were set to be identical (λUK = λNL = 0.5).",
                "We performed experiments with various λ settings, but did not observe significant differences in performance.",
                "Table 3 reports on the multilingual results, where performance is evaluated on the full topic set.",
                "All three models significantly imLang.",
                "Method Model 1 Model 2 Model 3 MAP MRR MAP MRR MAP MRR Expert finding UK BL 0.296 0.454 0.339 0.509 0.221 0.333 UK CT 0.330 0.491 0.342 0.500 0.228 0.342 NL BL 0.240 0.350 0.271 0.403 0.227 0.389 NL CT 0.251 0.382 0.267 0.410 0.246 0.404 Expert profiling UK BL 0.485 0.546 0.499 0.548 0.381 0.416 UK CT 0.562 0.620 0.508 0.558 0.440 0.486 NL BL 0.263 0.313 0.294 0.358 0.262 0.315 NL CT 0.330 0.384 0.317 0.387 0.294 0.345 Table 6: Performance of the context models (CT) compared to the baseline (BL).",
                "Best scores are in boldface. proved over all measures for both tasks.",
                "The coverage of topics and candidates for the expert finding and profiling tasks, respectively, is close to 100% in all cases.",
                "The relative improvement of the precision scores ranges from 10% to 80%.",
                "These scores demonstrate that despite its simplicity, our method for combining results over multiple languages achieves substantial improvements over the baseline. 9.",
                "CONCLUSIONS In this paper we focused on expertise retrieval (expert finding and profiling) in a new setting of a typical knowledge-intensive organization in which the available data is of high quality, multilingual, and covering a broad range of expertise area.",
                "Typically, the amount of available data in such an organization (e.g., a university, a research institute, or a research lab) is limited when compared to the W3C collection that has mostly been used for the experimental evaluation of expertise retrieval so far.",
                "To examine expertise retrieval in this setting, we introduced (and released) the UvT Expert collection as a representative case of such knowledge intensive organizations.",
                "The new collection reflects the typical properties of knowledge-intensive institutes noted above and also includes several features which may are potentially useful for expertise retrieval, such as topical and organizational structure.",
                "We evaluated how current state-of-the-art models for expert finding and profiling performed in this new setting and then refined these models in order to try and exploit the different characteristics within the data environment (language, topicality, and organizational structure).",
                "We found that current models of expertise retrieval generalize well to this new environment; in addition we found that refining the models to account for the differences results in significant improvements, thus making up for problems caused by data sparseness issues.",
                "Future work includes setting up manual assessments of automatically generated profiles by the employees themselves, especially in cases where the employees have not provided a profile themselves. 10.",
                "ACKNOWLEDGMENTS Krisztian Balog was supported by the Netherlands Organisation for Scientific Research (NWO) under project number 220-80-001.",
                "Maarten de Rijke was also supported by NWO under project numbers 017.001.190, 220-80-001, 264-70-050, 354-20-005, 600.065.120, 612-13-001, 612.000.106, 612.066.302, 612.069.006, 640.001.501, 640.002.501, and by the E.U.",
                "IST programme of the 6th FP for RTD under project MultiMATCH contract IST-033104.",
                "The work of Toine Bogers and Antal van den Bosch was funded by the IOP-MMI-program of SenterNovem / The Dutch Ministry of Economic Affairs, as part of the `A Propos project. 11.",
                "REFERENCES [1] L. Azzopardi.",
                "Incorporating Context in the Language Modeling Framework for ad-hoc Information Retrieval.",
                "PhD thesis, University of Paisley, 2005. [2] K. Balog and M. de Rijke.",
                "Finding similar experts.",
                "In This volume, 2007. [3] K. Balog and M. de Rijke.",
                "Determining expert profiles (with an application to expert finding).",
                "In IJCAI 07: Proc. 20th Intern.",
                "Joint Conf. on Artificial Intelligence, pages 2657-2662, 2007. [4] K. Balog, L. Azzopardi, and M. de Rijke.",
                "Formal models for expert finding in enterprise corpora.",
                "In SIGIR 06: Proc. 29th annual intern.",
                "ACM SIGIR conf. on Research and development in information retrieval, pages 43-50, 2006. [5] I. Becerra-Fernandez.",
                "The role of artificial intelligence technologies in the implementation of people-finder knowledge management systems.",
                "In AAAI Workshop on Bringing Knowledge to Business Processes, March 2000. [6] C. S. Campbell, P. P. Maglio, A. Cozzi, and B. Dom.",
                "Expertise identification using email communications.",
                "In CIKM 03: Proc. twelfth intern. conf. on Information and knowledge management, pages 528531, 2003. [7] G. Cao, J.-Y.",
                "Nie, and J. Bai.",
                "Integrating word relationships into language models.",
                "In SIGIR 05: Proc. 28th annual intern.",
                "ACM SIGIR conf. on Research and development in information retrieval, pages 298-305, 2005. [8] T. M. Cover and J.",
                "A. Thomas.",
                "Elements of Information Theory.",
                "Wiley-Interscience, 1991. [9] N. Craswell, D. Hawking, A. M. Vercoustre, and P. Wilkins.",
                "P@noptic expert: Searching for experts not just for documents.",
                "In Ausweb, 2001. [10] N. Craswell, A. de Vries, and I. Soboroff.",
                "Overview of the TREC2005 Enterprise Track.",
                "In The Fourteenth Text REtrieval Conf.",
                "Proc. (TREC 2005), 2006. [11] T. H. Davenport and L. Prusak.",
                "Working Knowledge: How Organizations Manage What They Know.",
                "Harvard Business School Press, Boston, MA, 1998. [12] T. Dunning.",
                "Accurate methods for the statistics of surprise and coincidence.",
                "Computational Linguistics, 19(1):61-74, 1993. [13] E. Filatova and J. Prager.",
                "Tell me what you do and Ill tell you what you are: Learning occupation-related activities for biographies.",
                "In HLT/EMNLP, 2005. [14] V. Lavrenko and W. B. Croft.",
                "Relevance based language models.",
                "In SIGIR 01: Proc. 24th annual intern.",
                "ACM SIGIR conf. on Research and development in information retrieval, pages 120-127, 2001. [15] V. Lavrenko, M. Choquette, and W. B. Croft.",
                "Cross-lingual relevance models.",
                "In SIGIR 02: Proc. 25th annual intern.",
                "ACM SIGIR conf. on Research and development in information retrieval, pages 175-182, 2002. [16] C. Macdonald and I. Ounis.",
                "Voting for candidates: adapting data fusion techniques for an expert search task.",
                "In CIKM 06: Proc. 15th ACM intern. conf. on Information and knowledge management, pages 387-396, 2006. [17] C. Manning and H. Sch¨utze.",
                "Foundations of Statistical Natural Language Processing.",
                "The MIT Press, 1999. [18] A. Mockus and J. D. Herbsleb.",
                "Expertise browser: a quantitative approach to identifying expertise.",
                "In ICSE 02: Proc. 24th Intern.",
                "Conf. on Software Engineering, pages 503-512, 2002. [19] D. Petkova and W. B. Croft.",
                "Hierarchical language models for expert finding in enterprise corpora.",
                "In Proc.",
                "ICTAI 2006, pages 599-608, 2006. [20] I. Soboroff, A. de Vries, and N. Craswell.",
                "Overview of the TREC 2006 Enterprise Track.",
                "In TREC 2006 Working Notes, 2006. [21] T. Tao, X. Wang, Q. Mei, and C. Zhai.",
                "Language model information retrieval with document expansion.",
                "In HLT-NAACL 2006, 2006. [22] TREC.",
                "Enterprise track, 2005.",
                "URL: http://www.ins.cwi. nl/projects/trec-ent/wiki/. [23] G. van Noord.",
                "TextCat Language Guesser.",
                "URL: http://www. let.rug.nl/˜vannoord/TextCat/. [24] W3C.",
                "The W3C test collection, 2005.",
                "URL: http://research. microsoft.com/users/nickcr/w3c-summary.html."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [],
            "translated_text": "",
            "candidates": [],
            "error": [
                []
            ]
        },
        "generative language modeling": {
            "translated_key": "",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Broad Expertise Retrieval in Sparse Data Environments Krisztian Balog ISLA, University of Amsterdam Kruislaan 403, 1098 SJ Amsterdam, The Netherlands kbalog@science.uva.nl Toine Bogers ILK, Tilburg University P.O.",
                "Box 90153, 5000 LE Tilburg, The Netherlands A.M.Bogers@uvt.nl Leif Azzopardi Dept. of Computing Science University of Glasgow, Glasgow, G12 8QQ leif@dcs.gla.ac.uk Maarten de Rijke ISLA, University of Amsterdam Kruislaan 403, 1098 SJ Amsterdam, The Netherlands mdr@science.uva.nl Antal van den Bosch ILK, Tilburg University P.O.",
                "Box 90153, 5000 LE Tilburg, The Netherlands Antal.vdnBosch@uvt.nl ABSTRACT Expertise retrieval has been largely unexplored on data other than the W3C collection.",
                "At the same time, many intranets of universities and other knowledge-intensive organisations offer examples of relatively small but clean multilingual expertise data, covering broad ranges of expertise areas.",
                "We first present two main expertise retrieval tasks, along with a set of baseline approaches based on <br>generative language modeling</br>, aimed at finding expertise relations between topics and people.",
                "For our experimental evaluation, we introduce (and release) a new test set based on a crawl of a university site.",
                "Using this test set, we conduct two series of experiments.",
                "The first is aimed at determining the effectiveness of baseline expertise retrieval methods applied to the new test set.",
                "The second is aimed at assessing refined models that exploit characteristic features of the new test set, such as the organizational structure of the university, and the hierarchical structure of the topics in the test set.",
                "Expertise retrieval models are shown to be robust with respect to environments smaller than the W3C collection, and current techniques appear to be generalizable to other settings.",
                "Categories and Subject Descriptors H.3 [Information Storage and Retrieval]: H.3.1 Content Analysis and Indexing; H.3.3 Information Search and Retrieval; H.3.4 Systems and Software; H.4 [Information Systems Applications]: H.4.2 Types of Systems; H.4.m Miscellaneous General Terms Algorithms, Measurement, Performance, Experimentation 1.",
                "INTRODUCTION An organizations intranet provides a means for exchanging information between employees and for facilitating employee collaborations.",
                "To efficiently and effectively achieve this, it is necessary to provide search facilities that enable employees not only to access documents, but also to identify expert colleagues.",
                "At the TREC Enterprise Track [22] the need to study and understand expertise retrieval has been recognized through the introduction of Expert Finding tasks.",
                "The goal of expert finding is to identify a list of people who are knowledgeable about a given topic.",
                "This task is usually addressed by uncovering associations between people and topics [10]; commonly, a co-occurrence of the name of a person with topics in the same context is assumed to be evidence of expertise.",
                "An alternative task, which using the same idea of people-topic associations, is expert profiling, where the task is to return a list of topics that a person is knowledgeable about [3].",
                "The launch of the Expert Finding task at TREC has generated a lot of interest in expertise retrieval, with rapid progress being made in terms of modeling, algorithms, and evaluation aspects.",
                "However, nearly all of the expert finding or profiling work performed has been validated experimentally using the W3C collection [24] from the Enterprise Track.",
                "While this collection is currently the only publicly available test collection for expertise retrieval tasks, it only represents one type of intranet.",
                "With only one test collection it is not possible to generalize conclusions to other realistic settings.",
                "In this paper we focus on expertise retrieval in a realistic setting that differs from the W3C setting-one in which relatively small amounts of clean, multilingual data are available, that cover a broad range of expertise areas, as can be found on the intranets of universities and other knowledge-intensive organizations.",
                "Typically, this setting features several additional types of structure: topical structure (e.g., topic hierarchies as employed by the organization), organizational structure (faculty, department, ...), as well as multiple types of documents (research and course descriptions, publications, and academic homepages).",
                "This setting is quite different from the W3C setting in ways that might impact upon the performance of expertise retrieval tasks.",
                "We focus on a number of research questions in this paper: Does the relatively small amount of data available on an intranet affect the quality of the topic-person associations that lie at the heart of expertise retrieval algorithms?",
                "How do state-of-the-art algorithms developed on the W3C data set perform in the alternative scenario of the type described above?",
                "More generally, do the lessons from the Expert Finding task at TREC carry over to this setting?",
                "How does the inclusion or exclusion of different documents affect expertise retrieval tasks?",
                "In addition to, how can the topical and organizational structure be used for retrieval purposes?",
                "To answer our research questions, we first present a set of baseline approaches, based on <br>generative language modeling</br>, aimed at finding associations between topics and people.",
                "This allows us to formulate the expert finding and expert profiling tasks in a uniform way, and has the added benefit of allowing us to understand the relations between the two tasks.",
                "For our experimental evaluation, we introduce a new data set (the UvT Expert Collection) which is representative of the type of intranet that we described above.",
                "Our collection is based on publicly available data, crawled from the website of Tilburg University (UvT).",
                "This type of data is particularly interesting, since (1) it is clean, heterogeneous, structured, and focused, but comprises a limited number of documents; (2) contains information on the organizational hierarchy; (3) it is bilingual (English and Dutch); and (4) the list of expertise areas of an individual are provided by the employees themselves.",
                "Using the UvT Expert collection, we conduct two sets of experiments.",
                "The first is aimed at determining the effectiveness of baseline expertise finding and profiling methods in this new setting.",
                "A second group of experiments is aimed at extensions of the baseline methods that exploit characteristic features of the UvT Expert Collection; specifically, we propose and evaluate refined expert finding and profiling methods that incorporate topicality and organizational structure.",
                "Apart from the research questions and data set that we contribute, our main contributions are as follows.",
                "The baseline models developed for expertise finding perform well on the new data set.",
                "While on the W3C setting the expert finding task appears to be more difficult than profiling, for the UvT data the opposite is the case.",
                "We find that profiling on the UvT data set is considerably more difficult than on the W3C set, which we believe is due to the large (but realistic) number of topical areas that we used for profiling: about 1,500 for the UvT set, versus 50 in the W3C case.",
                "Taking the similarity between topics into account can significantly improve retrieval performance.",
                "The best performing similarity measures are content-based, therefore they can be applied on the W3C (and other) settings as well.",
                "Finally, we demonstrate that the organizational structure can be exploited in the form of a context model, improving MAP scores for certain models by up to 70%.",
                "The remainder of this paper is organized as follows.",
                "In the next section we review related work.",
                "Then, in Section 3 we provide detailed descriptions of the expertise retrieval tasks that we address in this paper: expert finding and expert profiling.",
                "In Section 4 we present our baseline models, of which the performance is then assessed in Section 6 using the UvT data set that we introduce in Section 5.",
                "Advanced models exploiting specific features of our data are presented in Section 7 and evaluated in Section 8.",
                "We formulate our conclusions in Section 9. 2.",
                "RELATED WORK Initial approaches to expertise finding often employed databases containing information on the skills and knowledge of each individual in the organization [11].",
                "Most of these tools (usually called yellow pages or people-finding systems) rely on people to self-assess their skills against a predefined set of keywords.",
                "For updating profiles in these systems in an automatic fashion there is a need for intelligent technologies [5].",
                "More recent approaches use specific document sets (such as email [6] or software [18]) to find expertise.",
                "In contrast with focusing on particular document types, there is also an increased interest in the development of systems that index and mine published intranet documents as sources of evidence for expertise.",
                "One such published approach is the P@noptic system [9], which builds a representation of each person by concatenating all documents associated with that person-this is similar to Model 1 of Balog et al. [4], who formalize and compare two methods.",
                "Balog et al.s Model 1 directly models the knowledge of an expert from associated documents, while their Model 2 first locates documents on the topic and then finds the associated experts.",
                "In the reported experiments the second method performs significantly better when there are sufficiently many associated documents per candidate.",
                "Most systems that took part in the 2005 and 2006 editions of the Expert Finding task at TREC implemented (variations on) one of these two models; see [10, 20].",
                "Macdonald and Ounis [16] propose a different approach for ranking candidate expertise with respect to a topic based on data fusion techniques, without using collectionspecific heuristics; they find that applying field-based weighting models improves the ranking of candidates.",
                "Petkova and Croft [19] propose yet another approach, based on a combination of the above Model 1 and 2, explicitly modeling topics.",
                "Turning to other expert retrieval tasks that can also be addressed using topic-people associations, Balog and de Rijke [3] addressed the task of determining topical expert profiles.",
                "While their methods proved to be efficient on the W3C corpus, they require an amount of data that may not be available in the typical knowledge-intensive organization.",
                "Balog and de Rijke [2] study the related task of finding experts that are similar to a small set of experts given as input.",
                "As an aside, creating a textual summary of a person shows some similarities to biography finding, which has received a considerable amount of attention recently; see e.g., [13].",
                "We use <br>generative language modeling</br> to find associations between topics and people.",
                "In our modeling of expert finding and profiling we collect evidence for expertise from multiple sources, in a heterogeneous collection, and integrate it with the co-occurrence of candidates names and query terms-the language modeling setting allows us to do this in a transparent manner.",
                "Our modeling proceeds in two steps.",
                "In the first step, we consider three baseline models, two taken from [4] (the Models 1 and 2 mentioned above), and one a refined version of a model introduced in [3] (which we refer to as Model 3 below); this third model is also similar to the model described by Petkova and Croft [19].",
                "The models we consider in our second round of experiments are mixture models similar to contextual language models [1] and to the expanded documents of Tao et al. [21]; however, the features that we use for definining our expansions-including topical structure and organizational structure-have not been used in this way before. 3.",
                "TASKS In the expertise retrieval scenario that we envisage, users seeking expertise within an organization have access to an interface that combines a search box (where they can search for experts or topics) with navigational structures (of experts and of topics) that allows them to click their way to an expert page (providing the profile of a person) or a topic page (providing a list of experts on the topic).",
                "To feed the above interface, we face two expertise retrieval tasks, expert finding and expert profiling, that we first define and then formalize using generative language models.",
                "In order to model either task, the probability of the query topic being associated to a candidate expert plays a key role in the final estimates for searching and profiling.",
                "By using language models, both the candidates and the query are characterized by distributions of terms in the vocabulary (used in the documents made available by the organization whose expertise retrieval needs we are addressing). 3.1 Expert finding Expert finding involves the task of finding the right person with the appropriate skills and knowledge: Who are the experts on topic X?.",
                "E.g., an employee wants to ascertain who worked on a particular project to find out why particular decisions were made without having to trawl through documentation (if there is any).",
                "Or, they may be in need a trained specialist for consultancy on a specific problem.",
                "Within an organization there are usually many possible candidates who could be experts for given topic.",
                "We can state this problem as follows: What is the probability of a candidate ca being an expert given the query topic q?",
                "That is, we determine p(ca|q), and rank candidates ca according to this probability.",
                "The candidates with the highest probability given the query are deemed the most likely experts for that topic.",
                "The challenge is how to estimate this probability accurately.",
                "Since the query is likely to consist of only a few terms to describe the expertise required, we should be able to obtain a more accurate estimate by invoking Bayes Theorem, and estimating: p(ca|q) = p(q|ca)p(ca) p(q) , (1) where p(ca) is the probability of a candidate and p(q) is the probability of a query.",
                "Since p(q) is a constant, it can be ignored for ranking purposes.",
                "Thus, the probability of a candidate ca being an expert given the query q is proportional to the probability of a query given the candidate p(q|ca), weighted by the a priori belief p(ca) that candidate ca is an expert. p(ca|q) ∝ p(q|ca)p(ca) (2) In this paper our main focus is on estimating the probability of a query given the candidate p(q|ca), because this probability captures the extent to which the candidate knows about the query topic.",
                "Whereas the candidate priors are generally assumed to be uniformand thus will not influence the ranking-it has been demonstrated that a sensible choice of priors may improve the performance [20]. 3.2 Expert profiling While the task of expert searching was concerned with finding experts given a particular topic, the task of expert profiling seeks to answer a related question: What topics does a candidate know about?",
                "Essentially, this turns the questions of expert finding around.",
                "The profiling of an individual candidate involves the identification of areas of skills and knowledge that they have expertise about and an evaluation of the level of proficiency in each of these areas.",
                "This is the candidates topical profile.",
                "Generally, topical profiles within organizations consist of tabular structures which explicitly catalogue the skills and knowledge of each individual in the organization.",
                "However, such practice is limited by the resources available for defining, creating, maintaining, and updating these profiles over time.",
                "By focusing on automatic methods which draw upon the available evidence within the document repositories of an organization, our aim is to reduce the human effort associated with the maintenance of topical profiles1 .",
                "A topical profile of a candidate, then, is defined as a vector where each element i of the vector corresponds to the candidate cas expertise on a given topic ki, (i.e., s(ca, ki)).",
                "Each topic ki defines a particular knowledge area or skill that the organization uses to define the candidates topical profile.",
                "Thus, it is assumed that a list of topics, {k1, . . . , kn}, where n is the number of pre-defined topics, is given: profile(ca) = s(ca, k1), s(ca, k2), . . . , s(ca, kn) . (3) 1 Context and evidence are needed to help users of expertise finding systems to decide whom to contact when seeking expertise in a particular area.",
                "Examples of such context are: Who does she work with?",
                "What are her contact details?",
                "Is she well-connected, just in case she is not able to help us herself?",
                "What is her role in the organization?",
                "Who is her superior?",
                "Collaborators, and affiliations, etc. are all part of the candidates social profile, and can serve as a background against which the systems recommendations should be interpreted.",
                "In this paper we only address the problem of determining topical profiles, and leave social profiling to further work.",
                "We state the problem of quantifying the competence of a person on a certain knowledge area as follows: What is the probability of a knowledge area (ki) being part of the candidates (expertise) profile? where s(ca, ki) is defined by p(ki|ca).",
                "Our task, then, is to estimate p(ki|ca), which is equivalent to the problem of obtaining p(q|ca), where the topic ki is represented as a query topic q, i.e., a sequence of keywords representing the expertise required.",
                "Both the expert finding and profiling tasks rely on the accurate estimation of p(q|ca).",
                "The only difference derives from the prior probability that a person is an expert (p(ca)), which can be incorporated into the expert finding task.",
                "This prior does not apply to the profiling task since the candidate (individual) is fixed. 4.",
                "BASELINE MODELS In this section we describe our baseline models for estimating p(q|ca), i.e., associations between topics and people.",
                "Both expert finding and expert profiling boil down to this estimation.",
                "We employ three models for calculating this probability. 4.1 From topics to candidates Using Candidate Models: Model 1 Model 1 [4] defines the probability of a query given a candidate (p(q|ca)) using standard language modeling techniques, based on a multinomial unigram language model.",
                "For each candidate ca, a candidate language model θca is inferred such that the probability of a term given θca is nonzero for all terms, i.e., p(t|θca) > 0.",
                "From the candidate model the query is generated with the following probability: p(q|θca) = Y t∈q p(t|θca)n(t,q) , where each term t in the query q is sampled identically and independently, and n(t, q) is the number of times t occurs in q.",
                "The candidate language model is inferred as follows: (1) an empirical model p(t|ca) is computed; (2) it is smoothed with background probabilities.",
                "Using the associations between a candidate and a document, the probability p(t|ca) can be approximated by: p(t|ca) = X d p(t|d)p(d|ca), where p(d|ca) is the probability that candidate ca generates a supporting document d, and p(t|d) is the probability of a term t occurring in the document d. We use the maximum-likelihood estimate of a term, that is, the normalised frequency of the term t in document d. The strength of the association between document d and candidate ca expressed by p(d|ca) reflects the degree to which the candidates expertise is described using this document.",
                "The estimation of this probability is presented later, in Section 4.2.",
                "The candidate model is then constructed as a linear interpolation of p(t|ca) and the background model p(t) to ensure there are no zero probabilities, which results in the final estimation: p(q|θca) = (4) Y t∈q ( (1 − λ) X d p(t|d)p(d|ca) ! + λp(t) )n(t,q) .",
                "Model 1 amasses all the term information from all the documents associated with the candidate, and uses this to represent that candidate.",
                "This model is used to predict how likely a candidate would produce a query q.",
                "This can can be intuitively interpreted as the probability of this candidate talking about the query topic, where we assume that this is indicative of their expertise.",
                "Using Document Models: Model 2 Model 2 [4] takes a different approach.",
                "Here, the process is broken into two parts.",
                "Given a candidate ca, (1) a document that is associated with a candidate is selected with probability p(d|ca), and (2) from this document a query q is generated with probability p(q|d).",
                "Then the sum over all documents is taken to obtain p(q|ca), such that: p(q|ca) = X d p(q|d)p(d|ca). (5) The probability of a query given a document is estimated by inferring a document language model θd for each document d in a similar manner as the candidate model was inferred: p(t|θd) = (1 − λ)p(t|d) + λp(t), (6) where p(t|d) is the probability of the term in the document.",
                "The probability of a query given the document model is: p(q|θd) = Y t∈q p(t|θd)n(t,q) .",
                "The final estimate of p(q|ca) is obtained by substituting p(q|d) for p(q|θd) into Eq. 5 (see [4] for full details).",
                "Conceptually, Model 2 differs from Model 1 because the candidate is not directly modeled.",
                "Instead, the document acts like a hidden variable in the process which separates the query from the candidate.",
                "This process is akin to how a user may search for candidates with a standard search engine: initially by finding the documents which are relevant, and then seeing who is associated with that document.",
                "By examining a number of documents the user can obtain an idea of which candidates are more likely to discuss the topic q.",
                "Using Topic Models: Model 3 We introduce a third model, Model 3.",
                "Instead of attempting to model the query generation process via candidate or document models, we represent the query as a topic language model and directly estimate the probability of the candidate p(ca|q).",
                "This approach is similar to the model presented in [3, 19].",
                "As with the previous models, a language model is inferred, but this time for the query.",
                "We adapt the work of Lavrenko and Croft [14] to estimate a topic model from the query.",
                "The procedure is as follows.",
                "Given a collection of documents and a query topic q, it is assumed that there exists an unknown topic model θk that assigns probabilities p(t|θk) to the term occurrences in the topic documents.",
                "Both the query and the documents are samples from θk (as opposed to the previous approaches, where a query is assumed to be sampled from a specific document or candidate model).",
                "The main task is to estimate p(t|θk), the probability of a term given the topic model.",
                "Since the query q is very sparse, and as there are no examples of documents on the topic, this distribution needs to be approximated.",
                "Lavrenko and Croft [14] suggest a reasonable way of obtaining such an approximation, by assuming that p(t|θk) can be approximated by the probability of term t given the query q.",
                "We can then estimate p(t|q) using the joint probability of observing the term t together with the query terms, q1, . . . , qm, and dividing by the joint probability of the query terms: p(t|θk) ≈ p(t|q) = p(t, q1, . . . , qm) p(q1, . . . , qm) = p(t, q1, . . . , qm) P t ∈T p(t , q1, . . . , qm) , where p(q1, . . . , qm) = P t ∈T p(t , q1, . . . , qm), and T is the entire vocabulary of terms.",
                "In order to estimate the joint probability p(t, q1, . . . , qm), we follow [14, 15] and assume t and q1, . . . , qm are mutually independent, once we pick a source distribution from the set of underlying source distributions U.",
                "If we choose U to be a set of document models. then to construct this set, the query q would be issued against the collection, and the top n returned are assumed to be relevant to the topic, and thus treated as samples from the topic model. (Note that candidate models could be used instead.)",
                "With the document models forming U, the joint probability of term and query becomes: p(t, q1, . . . , qm) = X d∈U p(d) ˘ p(t|θd) mY i=1 p(qi|θd) ¯ . (7) Here, p(d) denotes the prior distribution over the set U, which reflects the relevance of the document to the topic.",
                "We assume that p(d) is uniform across U.",
                "In order to rank candidates according to the topic model defined, we use the Kullback-Leibler divergence metric (KL, [8]) to measure the difference between the candidate models and the topic model: KL(θk||θca) = X t p(t|θk) log p(t|θk) p(t|θca) . (8) Candidates with a smaller divergence from the topic model are considered to be more likely experts on that topic.",
                "The candidate model θca is defined in Eq. 4.",
                "By using KL divergence instead of the probability of a candidate given the topic model p(ca|θk), we avoid normalization problems. 4.2 Document-candidate associations For our models we need to be able to estimate the probability p(d|ca), which expresses the extent to which a document d characterizes the candidate ca.",
                "In [4], two methods are presented for estimating this probability, based on the number of person names recognized in a document.",
                "However, in our (intranet) setting it is reasonable to assume that authors of documents can unambiguously be identified (e.g., as the author of an article, the teacher assigned to a course, the owner of a web page, etc.)",
                "Hence, we set p(d|ca) to be 1 if candidate ca is author of document d, otherwise the probability is 0.",
                "In Section 6 we describe how authorship can be determined on different types of documents within the collection. 5.",
                "THE UVT EXPERT COLLECTION The UvT Expert collection used in the experiments in this paper fits the scenario outlined in Section 3.",
                "The collection is based on the Webwijs (Webwise) system developed at Tilburg University (UvT) in the Netherlands.",
                "Webwijs (http://www.uvt.nl/ webwijs/) is a publicly accessible database of UvT employees who are involved in research or teaching; currently, Webwijs contains information about 1168 experts, each of whom has a page with contact information and, if made available by the expert, a research description and publications list.",
                "In addition, each expert can select expertise areas from a list of 1491 topics and is encouraged to suggest new topics that need to be approved by the Webwijs editor.",
                "Each topic has a separate page that shows all experts associated with that topic and, if available, a list of related topics.",
                "Webwijs is available in Dutch and English, and this bilinguality has been preserved in the collection.",
                "Every Dutch Webwijs page has an English translation.",
                "Not all Dutch topics have an English translation, but the reverse is true: the 981 English topics all have a Dutch equivalent.",
                "About 42% of the experts teach courses at Tilburg University; these courses were also crawled and included in the profile.",
                "In addition, about 27% of the experts link to their academic homepage from their Webwijs page.",
                "These home pages were crawled and added to the collection. (This means that if experts put the full-text versions of their publications on their academic homepage, these were also available for indexing.)",
                "We also obtained 1880 full-text versions of publications from the UvT institutional repository and Dutch English no. of experts 1168 1168 no. of experts with ≥ 1 topic 743 727 no. of topics 1491 981 no. of expert-topic pairs 4318 3251 avg. no. of topics/expert 5.8 5.9 max. no. of topics/expert (no. of experts) 60 (1) 35 (1) min. no. of topics/expert (no. of experts) 1 (74) 1 (106) avg. no. of experts/topic 2.9 3.3 max. no. of experts/topic (no. of topics) 30 (1) 30 (1) min. no. of experts/topic (no. of topics) 1 (615) 1 (346) no. of experts with HP 318 318 no. of experts with CD 318 318 avg. no. of CDs per teaching expert 3.5 3.5 no. of experts with RD 329 313 no. of experts with PUB 734 734 avg. no. of PUBs per expert 27.0 27.0 avg. no. of PUB citations per expert 25.2 25.2 avg. no. of full-text PUBs per expert 1.8 1.8 Table 2: Descriptive statistics of the Dutch and English versions of the UvT Expert collection. converted them to plain text.",
                "We ran the TextCat [23] language identifier to classify the language of the home pages and the fulltext publications.",
                "We restricted ourselves to pages where the classifier was confident about the language used on the page.",
                "This resulted in four document types: research descriptions (RD), course descriptions (CD), publications (PUB; full-text and citationonly versions), and academic homepages (HP).",
                "Everything was bundled into the UvT Expert collection which is available at http: //ilk.uvt.nl/uvt-expert-collection/.",
                "The UvT Expert collection was extracted from a different organizational setting than the W3C collection and differs from it in a number of ways.",
                "The UvT setting is one with relatively small amounts of multilingual data.",
                "Document-author associations are clear and the data is structured and clean.",
                "The collection covers a broad range of expertise areas, as one can typically find on intranets of universities and other knowledge-intensive institutes.",
                "Additionally, our university setting features several types of structure (topical and organizational), as well as multiple document types.",
                "Another important difference between the two data sets is that the expertise areas in the UvT Expert collection are self-selected instead of being based on group membership or assignments by others.",
                "Size is another dimension along which the W3C and UvT Expert collections differ: the latter is the smaller of the two.",
                "Also realistic are the large differences in the amount of information available for each expert.",
                "Utilizing Webwijs is voluntary; 425 Dutch experts did not select any topics at all.",
                "This leaves us with 743 Dutch and 727 English usable expert profiles.",
                "Table 2 provides descriptive statistics for the UvT Expert collection.",
                "Universities tend to have a hierarchical structure that goes from the faculty level, to departments, research groups, down to the individual researchers.",
                "In the UvT Expert collection we have information about the affiliations of researchers with faculties and institutes, providing us with a two-level organizational hierarchy.",
                "Tilburg University has 22 organizational units at the faculty level (including the university office and several research institutes) and 71 departments, which amounts to 3.2 departments per faculty.",
                "As to the topical hierarchy used by Webwijs, 131 of the 1491 topics are top nodes in the hierarchy.",
                "This hierarchy has an average topic chain length of 2.65 and a maximum length of 7 topics. 6.",
                "EVALUATION Below, we evaluate Section 4s models for expert finding and profiling onthe UvT Expert collection.",
                "We detail our research questions and experimental setup, and then present our results. 6.1 Research Questions We address the following research questions.",
                "Both expert finding and profiling rely on the estimations of p(q|ca).",
                "The question is how the models compare on the different tasks, and in the setting of the UvT Expert collection.",
                "In [4], Model 2 outperformed Model 1 on the W3C collection.",
                "How do they compare on our data set?",
                "And how does Model 3 compare to Model 1?",
                "What about performance differences between the two languages in our test collection? 6.2 Experimental Setup The output of our models was evaluated against the self-assigned topic labels, which were treated as relevance judgements.",
                "Results were evaluated separately for English and Dutch.",
                "For English we only used topics for which the Dutch translation was available; for Dutch all topics were considered.",
                "The results were averaged for the queries in the intersection of relevance judgements and results; missing queries do not contribute a value of 0 to the scores.",
                "We use standard information retrieval measures, such as Mean Average Precision (MAP) and Mean Reciprocal Rank (MRR).",
                "We also report the percentage of topics (%q) and candidates (%ca) covered, for the expert finding and profiling tasks, respectively. 6.3 Results Table 1 shows the performance of Model 1, 2, and 3 on the expert finding and profiling tasks.",
                "The rows of the table correspond to the various document types (RD, CD, PUB, and HP) and to their combinations.",
                "RD+CD+PUB+HP is equivalent to the full collection and will be referred as the BASELINE of our experiments.",
                "Looking at Table 1 we see that Model 2 performs the best across the board.",
                "However, when the data is clean and very focused (RD), Model 3 outperforms it in a number of cases.",
                "Model 1 has the best coverage of candidates (%ca) and topics (%q).",
                "The various document types differ in their characteristics and how they improve the finding and profiling tasks.",
                "Expert profiling benefits much from the clean data present in the RD and CD document types, while the publications contribute the most to the expert finding task.",
                "Adding the homepages does not prove to be particularly useful.",
                "When we compare the results across languages, we find that the coverage of English topics (%q) is higher than of the Dutch ones for expert finding.",
                "Apart from that, the scores fall in the same range for both languages.",
                "For the profiling task the coverage of the candidates (%ca) is very similar for both languages.",
                "However, the performance is substantially better for the English topics.",
                "While it is hard to compare scores across collections, we conclude with a brief comparison of the absolute scores in Table 1 to those reported in [3, 4] on the W3C test set (2005 edition).",
                "For expert finding the MAP scores for Model 2 reported here are about 50% higher than the corresponding figures in [4], while our MRR scores are slightly below those in [4].",
                "For expert profiling, the differences are far more dramatic: the MAP scores for Model 2 reported here are around 50% below the scores in [3], while the (best) MRR scores are about the same as those in [3].",
                "The cause for the latter differences seems to reside in the number of knowledge areas considered here-approx. 30 times more than in the W3C setting. 7.",
                "ADVANCED MODELS Now that we have developed and assessed basic language modeling techniques for expertise retrieval, we turn to refined models that exploit special features of our test collection. 7.1 Exploiting knowledge area similarity One way to improve the scoring of a query given a candidate is to consider what other requests the candidate would satisfy and use them as further evidence to support the original query, proportional Expert finding Expert profiling Document types Model 1 Model 2 Model 3 Model 1 Model 2 Model 3 %q MAP MRR %q MAP MRR %q MAP MRR %ca MAP MRR %ca MAP MRR %ca MAP MRR English RD 97.8 0.126 0.269 83.5 0.144 0.311 83.3 0.129 0.271 100 0.089 0.189 39.3 0.232 0.465 41.1 0.166 0.337 CD 97.8 0.118 0.227 91.7 0.123 0.248 91.7 0.118 0.226 32.8 0.188 0.381 32.4 0.195 0.385 32.7 0.203 0.370 PUB 97.8 0.200 0.330 98.0 0.216 0.372 98.0 0.145 0.257 78.9 0.167 0.364 74.5 0.212 0.442 78.9 0.135 0.299 HP 97.8 0.081 0.186 97.4 0.071 0.168 97.2 0.062 0.149 31.2 0.150 0.299 28.8 0.185 0.335 30.1 0.136 0.287 RD+CD 97.8 0.188 0.352 92.9 0.193 0.360 92.9 0.150 0.273 100 0.145 0.286 61.3 0.251 0.477 63.2 0.217 0.416 RD+CD+PUB 97.8 0.235 0.373 98.1 0.277 0.439 98.1 0.178 0.305 100 0.196 0.380 87.2 0.280 0.533 89.5 0.170 0.344 RD+CD+PUB+HP 97.8 0.237 0.372 98.6 0.280 0.441 98.5 0.166 0.293 100 0.199 0.387 88.7 0.281 0.525 90.9 0.169 0.329 Dutch RD 61.3 0.094 0.229 38.4 0.137 0.336 38.3 0.127 0.295 38.0 0.127 0.386 34.1 0.138 0.420 38.0 0.105 0.327 CD 61.3 0.107 0.212 49.7 0.128 0.256 49.7 0.136 0.261 32.5 0.151 0.389 31.8 0.158 0.396 32.5 0.170 0.380 PUB 61.3 0.193 0.319 59.5 0.218 0.368 59.4 0.173 0.291 78.8 0.126 0.364 76.0 0.150 0.424 78.8 0.103 0.294 HP 61.3 0.063 0.169 56.6 0.064 0.175 56.4 0.062 0.163 29.8 0.108 0.308 27.8 0.125 0.338 29.8 0.098 0.255 RD+CD 61.3 0.159 0.314 51.9 0.184 0.360 51.9 0.169 0.324 60.5 0.151 0.410 57.2 0.166 0.431 60.4 0.159 0.384 RD+CD+PUB 61.3 0.244 0.398 61.5 0.260 0.424 61.4 0.210 0.350 90.3 0.165 0.445 88.2 0.189 0.479 90.3 0.126 0.339 RD+CD+PUB+HP 61.3 0.249 0.401 62.6 0.265 0.436 62.6 0.195 0.344 91.9 0.164 0.426 90.1 0.195 0.488 91.9 0.125 0.328 Table 1: Performance of the models on the expert finding and profiling tasks, using different document types and their combinations. %q is the number of topics covered (applies to the expert finding task), %ca is the number of candidates covered (applies to the expert profiling task).",
                "The top and bottom blocks correspond to English and Dutch respectively.",
                "The best scores are in boldface. to how related the other requests are to the original query.",
                "This can be modeled by interpolating between the p(q|ca) and the further supporting evidence from all similar requests q , as follows: p (q|ca) = λp(q|ca) + (1 − λ) X q p(q|q )p(q |ca), (9) where p(q|q ) represents the similarity between the two topics q and q .",
                "To be able to work with similarity methods that are not necessarily probabilities, we set p(q|q ) = w(q,q ) γ , where γ is a normalizing constant, such that γ = P q w(q , q ).",
                "We consider four methods for calculating the similarity score between two topics.",
                "Three approaches are strictly content-based, and establish similarity by examining co-occurrence patterns of topics within the collection, while the last approach exploits the hierarchical structure of topical areas that may be present within an organization (see [7] for further examples of integrating word relationships into language models).",
                "The Kullback-Leibler (KL) divergence metric defined in Eq. 8 provides a measure of how different or similar two probability distributions are.",
                "A topic model is inferred for q and q using the method presented in Section 4.1 to describe the query across the entire vocabulary.",
                "Since a lower KL score means the queries are more similar, we let w(q, q ) = max(KL(θq||·) − KL(θq||θq )).",
                "Pointwise Mutual Information (PMI, [17]) is a measure of association used in information theory to determine the extent of independence between variables.",
                "The dependence between two queries is reflected by the SI(q, q ) score, where scores greater than zero indicate that it is likely that there is a dependence, which we take to mean that the queries are likely to be similar: SI(q, q ) = log p(q, q ) p(q)p(q ) (10) We estimate the probability of a topic p(q) using the number of documents relevant to query q within the collection.",
                "The joint probability p(q, q ) is estimated similarly, by using the concatenation of q and q as a query.",
                "To obtain p(q|q ), we then set w(q, q ) = SI(q, q ) when SI(q, q ) > 0 otherwise w(q, q ) = 0, because we are only interested in including queries that are similar.",
                "The log-likelihood statistic provides another measure of dependence, which is more reliable than the pointwise mutual information measure [17].",
                "Let k1 be the number of co-occurrences of q and q , k2 the number of occurrences of q not co-occurring with q , n1 the total number of occurrences of q , and n2 the total number of topic tokens minus the number of occurrences of q .",
                "Then, let p1 = k1/n1, p2 = k2/n2, and p = (k1 + k2)/(n1 + n2), (q, q ) = 2( (p1, k1, n1) + (p2, k2, n2) − (p, k1, n1) − (p, k2, n2)), where (p, n, k) = k log p + (n − k) log(1 − p).",
                "The higher score indicate that queries are also likely to be similar, thus we set w(q, q ) = (q, q ).",
                "Finally, we also estimate the similarity of two topics based on their distance within the topic hierarchy.",
                "The topic hierarchy is viewed as a directed graph, and for all topic-pairs the shortest path SP(q, q ) is calculated.",
                "We set the similarity score to be the reciprocal of the shortest path: w(q, q ) = 1/SP(q, q ). 7.2 Contextual information Given the hierarchy of an organization, the units to which a person belong are regarded as a context so as to compensate for data sparseness.",
                "We model it as follows: p (q|ca) = 1 − P ou∈OU(ca) λou · p(q|ca) + P ou∈OU(ca) λou · p(q|ou), where OU(ca) is the set of organizational units of which candidate ca is a member of, and p(q|o) expresses the strength of the association between query q and the unit ou.",
                "The latter probability can be estimated using either of the three basic models, by simply replacing ca with ou in the corresponding equations.",
                "An organizational unit is associated with all the documents that its members have authored.",
                "That is, p(d|ou) = maxca∈ou p(d|ca). 7.3 A simple multilingual model For knowledge institutes in Europe, academic or otherwise, a multilingual (or at least bilingual) setting is typical.",
                "The following model builds on a kind of independence assumption: there is no spill-over of expertise/profiles across language boundaries.",
                "While a simplification, this is a sensible first approach.",
                "That is: p (q|ca) =P l∈L λl · p(ql|ca), where L is the set of languages used in the collection, ql is the translation of the query q to language l, and λl is a language specific smoothing parameter, such that P l∈L λl = 1. 8.",
                "ADVANCED MODELS: EVALUATION In this section we present an experimental evaluation of our advanced models.",
                "Expert finding Expert profiling Language Model 1 Model 2 Model 3 Model 1 Model 2 Model 3 %q MAP MRR %q MAP MRR %q MAP MRR %ca MAP MRR %ca MAP MRR %ca MAP MRR English only 97.8 0.237 0.372 98.6 0.280 0.441 98.5 0.166 0.293 100 0.199 0.387 88.7 0.281 0.525 90.9 0.169 0.329 Dutch only 61.3 0.249 0.401 62.6 0.265 0.436 62.6 0.195 0.344 91.9 0.164 0.426 90.1 0.195 0.488 91.9 0.125 0.328 Combination 99.4 0.297 0.444 99.7 0.324 0.491 99.7 0.223 0.388 100 0.241 0.445 92.1 0.313 0.564 93.2 0.224 0.411 Table 3: Performance of the combination of languages on the expert finding and profiling tasks (on candidates).",
                "Best scores for each model are in italic, absolute best scores for the expert finding and profiling tasks are in boldface.",
                "Method Model 1 Model 2 Model 3 MAP MRR MAP MRR MAP MRR English BASELINE 0.296 0.454 0.339 0.509 0.221 0.333 KLDIV 0.291 0.453 0.327 0.503 0.219 0.330 PMI 0.291 0.453 0.337 0.509 0.219 0.331 LL 0.319 0.490 0.360 0.524 0.233 0.368 HDIST 0.299 0.465 0.346 0.537 0.219 0.332 Dutch BASELINE 0.240 0.350 0.271 0.403 0.227 0.389 KLDIV 0.239 0.347 0.253 0.386 0.224 0.385 PMI 0.239 0.350 0.260 0.392 0.227 0.389 LL 0.255 0.372 0.281 0.425 0.231 0.389 HDIST 0.253 0.365 0.271 0.407 0.236 0.402 Method Model 1 Model 2 Model 3 MAP MRR MAP MRR MAP MRR English BASELINE 0.485 0.546 0.499 0.548 0.381 0.416 KLDIV 0.510 0.564 0.513 0.558 0.381 0.416 PMI 0.486 0.546 0.495 0.542 0.407 0.451 LL 0.558 0.589 0.586 0.617 0.408 0.453 HDIST 0.507 0.567 0.512 0.563 0.386 0.420 Dutch BASELINE 0.263 0.313 0.294 0.358 0.262 0.315 KLDIV 0.284 0.336 0.271 0.321 0.261 0.314 PMI 0.265 0.317 0.265 0.316 0.273 0.330 LL 0.312 0.351 0.330 0.377 0.284 0.331 HDIST 0.280 0.327 0.288 0.341 0.266 0.321 Table 4: Performance on the expert finding (top) and profiling (bottom) tasks, using knowledge area similarities.",
                "Runs were evaluated on the main topics set.",
                "Best scores are in boldface. 8.1 Research Questions Our questions follow the refinements presented in the preceding section: Does exploiting the knowledge area similarity improve effectiveness?",
                "Which of the various methods for capturing word relationships is most effective?",
                "Furthermore, is our way of bringing in contextual information useful?",
                "For which tasks?",
                "And finally, is our simple way of combining the monolingual scores sufficient for obtaining significant improvements? 8.2 Experimental setup Given that the self-assessments are also sparse in our collection, in order to be able to measure differences between the various models, we selected a subset of topics, and evaluated (some of the) runs only on this subset.",
                "This set is referred as main topics, and consists of topics that are located at the top level of the topical hierarchy. (A main topic has subtopics, but is not a subtopic of any other topic.)",
                "This main set consists of 132 Dutch and 119 English topics.",
                "The relevance judgements were restricted to the main topic set, but were not expanded with subtopics. 8.3 Exploiting knowledge area similarity Table 4 presents the results.",
                "The four methods used for estimating knowledge-area similarity are KL divergence (KLDIV), PointLang.",
                "Topics Model 1 Model 2 Model 3 MAP MRR MAP MRR MAP MRR Expert finding UK ALL 0.423 0.545 0.654 0.799 0.494 0.629 UK MAIN 0.500 0.621 0.704 0.834 0.587 0.699 NL ALL 0.439 0.560 0.672 0.826 0.480 0.630 NL MAIN 0.440 0.584 0.645 0.816 0.515 0.655 Expert profiling UK ALL 0.240 0.640 0.306 0.778 0.223 0.616 UK MAIN 0.523 0.677 0.519 0.648 0.461 0.587 NL ALL 0.203 0.716 0.254 0.770 0.183 0.627 NL MAIN 0.332 0.576 0.380 0.624 0.332 0.549 Table 5: Evaluating the context models on organizational units. wise mutual information (PMI), log-likelihood (LL), and distance within topic hierarchy (HDIST).",
                "We managed to improve upon the baseline in all cases, but the improvement is more noticeable for the profiling task.",
                "For both tasks, the LL method performed best.",
                "The content-based approaches performed consistently better than HDIST. 8.4 Contextual information A two level hierarchy of organizational units (faculties and institutes) is available in the UvT Expert collection.",
                "The unit a person belongs to is used as a context for that person.",
                "First, we evaluated the models of the organizational units, using all topics (ALL) and only the main topics (MAIN).",
                "An organizational unit is considered to be relevant for a given topic (or vice versa) if at least one member of the unit selected the given topic as an expertise area.",
                "Table 5 reports on the results.",
                "As far as expert finding goes, given a topic, the corresponding organizational unit can be identified with high precision.",
                "However, the expert profiling task shows a different picture: the scores are low, and the task seems hard.",
                "The explanation may be that general concepts (i.e., our main topics) may belong to several organizational units.",
                "Second, we performed another evaluation, where we combined the contextual models with the candidate models (to score candidates again).",
                "Table 6 reports on the results.",
                "We find a positive impact of the context models only for expert finding.",
                "Noticably, for expert finding (and Model 1), it improves over 50% (for English) and over 70% (for Dutch) on MAP.",
                "The poor performance on expert profiling may be due to the fact that context models alone did not perform very well on the profiling task to begin with. 8.5 Multilingual models In this subsection we evaluate the method for combining results across multiple languages that we described in Section 7.3.",
                "In our setting the set of languages consists of English and Dutch: L = {UK, NL}.",
                "The weights on these languages were set to be identical (λUK = λNL = 0.5).",
                "We performed experiments with various λ settings, but did not observe significant differences in performance.",
                "Table 3 reports on the multilingual results, where performance is evaluated on the full topic set.",
                "All three models significantly imLang.",
                "Method Model 1 Model 2 Model 3 MAP MRR MAP MRR MAP MRR Expert finding UK BL 0.296 0.454 0.339 0.509 0.221 0.333 UK CT 0.330 0.491 0.342 0.500 0.228 0.342 NL BL 0.240 0.350 0.271 0.403 0.227 0.389 NL CT 0.251 0.382 0.267 0.410 0.246 0.404 Expert profiling UK BL 0.485 0.546 0.499 0.548 0.381 0.416 UK CT 0.562 0.620 0.508 0.558 0.440 0.486 NL BL 0.263 0.313 0.294 0.358 0.262 0.315 NL CT 0.330 0.384 0.317 0.387 0.294 0.345 Table 6: Performance of the context models (CT) compared to the baseline (BL).",
                "Best scores are in boldface. proved over all measures for both tasks.",
                "The coverage of topics and candidates for the expert finding and profiling tasks, respectively, is close to 100% in all cases.",
                "The relative improvement of the precision scores ranges from 10% to 80%.",
                "These scores demonstrate that despite its simplicity, our method for combining results over multiple languages achieves substantial improvements over the baseline. 9.",
                "CONCLUSIONS In this paper we focused on expertise retrieval (expert finding and profiling) in a new setting of a typical knowledge-intensive organization in which the available data is of high quality, multilingual, and covering a broad range of expertise area.",
                "Typically, the amount of available data in such an organization (e.g., a university, a research institute, or a research lab) is limited when compared to the W3C collection that has mostly been used for the experimental evaluation of expertise retrieval so far.",
                "To examine expertise retrieval in this setting, we introduced (and released) the UvT Expert collection as a representative case of such knowledge intensive organizations.",
                "The new collection reflects the typical properties of knowledge-intensive institutes noted above and also includes several features which may are potentially useful for expertise retrieval, such as topical and organizational structure.",
                "We evaluated how current state-of-the-art models for expert finding and profiling performed in this new setting and then refined these models in order to try and exploit the different characteristics within the data environment (language, topicality, and organizational structure).",
                "We found that current models of expertise retrieval generalize well to this new environment; in addition we found that refining the models to account for the differences results in significant improvements, thus making up for problems caused by data sparseness issues.",
                "Future work includes setting up manual assessments of automatically generated profiles by the employees themselves, especially in cases where the employees have not provided a profile themselves. 10.",
                "ACKNOWLEDGMENTS Krisztian Balog was supported by the Netherlands Organisation for Scientific Research (NWO) under project number 220-80-001.",
                "Maarten de Rijke was also supported by NWO under project numbers 017.001.190, 220-80-001, 264-70-050, 354-20-005, 600.065.120, 612-13-001, 612.000.106, 612.066.302, 612.069.006, 640.001.501, 640.002.501, and by the E.U.",
                "IST programme of the 6th FP for RTD under project MultiMATCH contract IST-033104.",
                "The work of Toine Bogers and Antal van den Bosch was funded by the IOP-MMI-program of SenterNovem / The Dutch Ministry of Economic Affairs, as part of the `A Propos project. 11.",
                "REFERENCES [1] L. Azzopardi.",
                "Incorporating Context in the Language Modeling Framework for ad-hoc Information Retrieval.",
                "PhD thesis, University of Paisley, 2005. [2] K. Balog and M. de Rijke.",
                "Finding similar experts.",
                "In This volume, 2007. [3] K. Balog and M. de Rijke.",
                "Determining expert profiles (with an application to expert finding).",
                "In IJCAI 07: Proc. 20th Intern.",
                "Joint Conf. on Artificial Intelligence, pages 2657-2662, 2007. [4] K. Balog, L. Azzopardi, and M. de Rijke.",
                "Formal models for expert finding in enterprise corpora.",
                "In SIGIR 06: Proc. 29th annual intern.",
                "ACM SIGIR conf. on Research and development in information retrieval, pages 43-50, 2006. [5] I. Becerra-Fernandez.",
                "The role of artificial intelligence technologies in the implementation of people-finder knowledge management systems.",
                "In AAAI Workshop on Bringing Knowledge to Business Processes, March 2000. [6] C. S. Campbell, P. P. Maglio, A. Cozzi, and B. Dom.",
                "Expertise identification using email communications.",
                "In CIKM 03: Proc. twelfth intern. conf. on Information and knowledge management, pages 528531, 2003. [7] G. Cao, J.-Y.",
                "Nie, and J. Bai.",
                "Integrating word relationships into language models.",
                "In SIGIR 05: Proc. 28th annual intern.",
                "ACM SIGIR conf. on Research and development in information retrieval, pages 298-305, 2005. [8] T. M. Cover and J.",
                "A. Thomas.",
                "Elements of Information Theory.",
                "Wiley-Interscience, 1991. [9] N. Craswell, D. Hawking, A. M. Vercoustre, and P. Wilkins.",
                "P@noptic expert: Searching for experts not just for documents.",
                "In Ausweb, 2001. [10] N. Craswell, A. de Vries, and I. Soboroff.",
                "Overview of the TREC2005 Enterprise Track.",
                "In The Fourteenth Text REtrieval Conf.",
                "Proc. (TREC 2005), 2006. [11] T. H. Davenport and L. Prusak.",
                "Working Knowledge: How Organizations Manage What They Know.",
                "Harvard Business School Press, Boston, MA, 1998. [12] T. Dunning.",
                "Accurate methods for the statistics of surprise and coincidence.",
                "Computational Linguistics, 19(1):61-74, 1993. [13] E. Filatova and J. Prager.",
                "Tell me what you do and Ill tell you what you are: Learning occupation-related activities for biographies.",
                "In HLT/EMNLP, 2005. [14] V. Lavrenko and W. B. Croft.",
                "Relevance based language models.",
                "In SIGIR 01: Proc. 24th annual intern.",
                "ACM SIGIR conf. on Research and development in information retrieval, pages 120-127, 2001. [15] V. Lavrenko, M. Choquette, and W. B. Croft.",
                "Cross-lingual relevance models.",
                "In SIGIR 02: Proc. 25th annual intern.",
                "ACM SIGIR conf. on Research and development in information retrieval, pages 175-182, 2002. [16] C. Macdonald and I. Ounis.",
                "Voting for candidates: adapting data fusion techniques for an expert search task.",
                "In CIKM 06: Proc. 15th ACM intern. conf. on Information and knowledge management, pages 387-396, 2006. [17] C. Manning and H. Sch¨utze.",
                "Foundations of Statistical Natural Language Processing.",
                "The MIT Press, 1999. [18] A. Mockus and J. D. Herbsleb.",
                "Expertise browser: a quantitative approach to identifying expertise.",
                "In ICSE 02: Proc. 24th Intern.",
                "Conf. on Software Engineering, pages 503-512, 2002. [19] D. Petkova and W. B. Croft.",
                "Hierarchical language models for expert finding in enterprise corpora.",
                "In Proc.",
                "ICTAI 2006, pages 599-608, 2006. [20] I. Soboroff, A. de Vries, and N. Craswell.",
                "Overview of the TREC 2006 Enterprise Track.",
                "In TREC 2006 Working Notes, 2006. [21] T. Tao, X. Wang, Q. Mei, and C. Zhai.",
                "Language model information retrieval with document expansion.",
                "In HLT-NAACL 2006, 2006. [22] TREC.",
                "Enterprise track, 2005.",
                "URL: http://www.ins.cwi. nl/projects/trec-ent/wiki/. [23] G. van Noord.",
                "TextCat Language Guesser.",
                "URL: http://www. let.rug.nl/˜vannoord/TextCat/. [24] W3C.",
                "The W3C test collection, 2005.",
                "URL: http://research. microsoft.com/users/nickcr/w3c-summary.html."
            ],
            "original_annotated_samples": [
                "We first present two main expertise retrieval tasks, along with a set of baseline approaches based on <br>generative language modeling</br>, aimed at finding expertise relations between topics and people.",
                "To answer our research questions, we first present a set of baseline approaches, based on <br>generative language modeling</br>, aimed at finding associations between topics and people.",
                "We use <br>generative language modeling</br> to find associations between topics and people."
            ],
            "translated_annotated_samples": [
                "Primero presentamos dos tareas principales de recuperación de expertos, junto con un conjunto de enfoques de referencia basados en <br>modelado del lenguaje generativo</br>, con el objetivo de encontrar relaciones de experticia entre temas y personas.",
                "Para responder a nuestras preguntas de investigación, primero presentamos un conjunto de enfoques de línea base, basados en <br>modelado de lenguaje generativo</br>, con el objetivo de encontrar asociaciones entre temas y personas.",
                "Utilizamos <br>modelado de lenguaje generativo</br> para encontrar asociaciones entre temas y personas."
            ],
            "translated_text": "Recuperación de amplia experiencia en entornos de datos dispersos Krisztian Balog ISLA, Universidad de Ámsterdam Kruislaan 403, 1098 SJ Ámsterdam, Países Bajos kbalog@science.uva.nl Toine Bogers ILK, Universidad de Tilburg P.O. Caja 90153, 5000 LE Tilburg, Países Bajos A.M.Bogers@uvt.nl Leif Azzopardi Dept. de Ciencias de la Computación Universidad de Glasgow, Glasgow, G12 8QQ leif@dcs.gla.ac.uk Maarten de Rijke ISLA, Universidad de Ámsterdam Kruislaan 403, 1098 SJ Ámsterdam, Países Bajos mdr@science.uva.nl Antal van den Bosch ILK, Universidad de Tilburg P.O. La recuperación de la experiencia ha sido ampliamente inexplorada en datos distintos a la colección de la W3C. Al mismo tiempo, muchas intranets de universidades y otras organizaciones intensivas en conocimiento ofrecen ejemplos de datos de expertos multilingües relativamente pequeños pero precisos, que abarcan amplias áreas de especialización. Primero presentamos dos tareas principales de recuperación de expertos, junto con un conjunto de enfoques de referencia basados en <br>modelado del lenguaje generativo</br>, con el objetivo de encontrar relaciones de experticia entre temas y personas. Para nuestra evaluación experimental, introducimos (y liberamos) un nuevo conjunto de pruebas basado en un rastreo de un sitio universitario. Usando este conjunto de pruebas, llevamos a cabo dos series de experimentos. El primero tiene como objetivo determinar la efectividad de los métodos de recuperación de conocimientos básicos aplicados al nuevo conjunto de pruebas. El segundo tiene como objetivo evaluar modelos refinados que exploten las características distintivas del nuevo conjunto de pruebas, como la estructura organizativa de la universidad y la estructura jerárquica de los temas en el conjunto de pruebas. Los modelos de recuperación de experiencia se muestran robustos con respecto a entornos más pequeños que la colección de la W3C, y las técnicas actuales parecen ser generalizables a otros contextos. Categorías y Descriptores de Asignaturas H.3 [Almacenamiento y Recuperación de Información]: H.3.1 Análisis de Contenido e Indexación; H.3.3 Búsqueda y Recuperación de Información; H.3.4 Sistemas y Software; H.4 [Aplicaciones de Sistemas de Información]: H.4.2 Tipos de Sistemas; H.4.m Términos Generales Misceláneos Algoritmos, Medición, Rendimiento, Experimentación 1. INTRODUCCIÓN La intranet de una organización proporciona un medio para intercambiar información entre empleados y facilitar colaboraciones entre ellos. Para lograr esto de manera eficiente y efectiva, es necesario proporcionar instalaciones de búsqueda que permitan a los empleados no solo acceder a documentos, sino también identificar a colegas expertos. En la pista de Empresas de TREC [22] se ha reconocido la necesidad de estudiar y comprender la recuperación de expertos a través de la introducción de tareas de Búsqueda de Expertos. El objetivo de la búsqueda de expertos es identificar una lista de personas que tienen conocimientos sobre un tema específico. Esta tarea suele abordarse descubriendo asociaciones entre personas y temas [10]; comúnmente, se asume que la co-ocurrencia del nombre de una persona con temas en el mismo contexto es evidencia de experiencia. Una tarea alternativa, que utiliza la misma idea de asociaciones entre personas y temas, es el perfilado de expertos, donde la tarea consiste en devolver una lista de temas sobre los que una persona tiene conocimiento [3]. El lanzamiento de la tarea de Búsqueda de Expertos en TREC ha generado mucho interés en la recuperación de expertos, con un rápido progreso en términos de modelado, algoritmos y aspectos de evaluación. Sin embargo, casi todo el trabajo de búsqueda o perfilado de expertos realizado ha sido validado experimentalmente utilizando la colección de W3C [24] de la pista empresarial. Si bien esta colección es actualmente la única colección de pruebas disponible públicamente para tareas de recuperación de expertos, solo representa un tipo de intranet. Con solo una colección de pruebas no es posible generalizar conclusiones a otros escenarios realistas. En este artículo nos enfocamos en la recuperación de expertos en un entorno realista que difiere del entorno de la W3C, uno en el que hay cantidades relativamente pequeñas de datos limpios y multilingües disponibles, que abarcan una amplia gama de áreas de expertise, como se puede encontrar en las intranets de universidades y otras organizaciones intensivas en conocimiento. Normalmente, esta configuración incluye varios tipos adicionales de estructuras: estructura temática (por ejemplo, jerarquías de temas utilizadas por la organización), estructura organizativa (facultad, departamento, ...), así como múltiples tipos de documentos (descripciones de investigación y cursos, publicaciones y páginas web académicas). Este entorno es bastante diferente del entorno del W3C de maneras que podrían afectar el rendimiento de las tareas de recuperación de conocimientos especializados. Nos enfocamos en varias preguntas de investigación en este artículo: ¿La cantidad relativamente pequeña de datos disponibles en una intranet afecta la calidad de las asociaciones tema-persona que son fundamentales en los algoritmos de recuperación de expertos? ¿Cómo se desempeñan los algoritmos de vanguardia desarrollados en el conjunto de datos del W3C en el escenario alternativo del tipo descrito anteriormente? Más en general, ¿se pueden aplicar las lecciones de la tarea de Búsqueda de Expertos en TREC a este contexto? ¿Cómo afecta la inclusión o exclusión de diferentes documentos a las tareas de recuperación de expertos? Además, ¿cómo se puede utilizar la estructura temática y organizativa con fines de recuperación? Para responder a nuestras preguntas de investigación, primero presentamos un conjunto de enfoques de línea base, basados en <br>modelado de lenguaje generativo</br>, con el objetivo de encontrar asociaciones entre temas y personas. Esto nos permite formular las tareas de búsqueda de expertos y perfilado de expertos de manera uniforme, y tiene el beneficio adicional de permitirnos entender las relaciones entre las dos tareas. Para nuestra evaluación experimental, introducimos un nuevo conjunto de datos (la Colección de Expertos de UvT) que es representativo del tipo de intranet que describimos anteriormente. Nuestra colección se basa en datos de acceso público, extraídos del sitio web de la Universidad de Tilburg (UvT). Este tipo de datos es particularmente interesante, ya que (1) es limpio, heterogéneo, estructurado y enfocado, pero comprende un número limitado de documentos; (2) contiene información sobre la jerarquía organizativa; (3) es bilingüe (inglés y holandés); y (4) la lista de áreas de especialización de un individuo es proporcionada por los propios empleados. Usando la colección de expertos de UvT, llevamos a cabo dos conjuntos de experimentos. El primero tiene como objetivo determinar la efectividad de los métodos de búsqueda y perfilado de experiencia inicial en este nuevo entorno. Un segundo grupo de experimentos está dirigido a extensiones de los métodos base que aprovechan las características distintivas de la Colección de Expertos de UvT; específicamente, proponemos y evaluamos métodos refinados de búsqueda y perfilado de expertos que incorporan la topicalidad y la estructura organizativa. Además de las preguntas de investigación y el conjunto de datos que aportamos, nuestras principales contribuciones son las siguientes. Los modelos base desarrollados para encontrar expertos funcionan bien en el nuevo conjunto de datos. Mientras que en el entorno del W3C la tarea de encontrar expertos parece ser más difícil que el perfilado, para los datos de UvT ocurre lo contrario. Encontramos que el perfilado en el conjunto de datos UvT es considerablemente más difícil que en el conjunto de datos W3C, lo cual creemos que se debe al gran (pero realista) número de áreas temáticas que utilizamos para el perfilado: alrededor de 1,500 para el conjunto de datos UvT, frente a 50 en el caso de W3C. Teniendo en cuenta la similitud entre los temas puede mejorar significativamente el rendimiento de recuperación. Las medidas de similitud de mejor rendimiento son basadas en contenido, por lo tanto también se pueden aplicar en entornos de la W3C (y otros). Finalmente, demostramos que la estructura organizativa puede ser explotada en forma de un modelo de contexto, mejorando los puntajes de MAP para ciertos modelos hasta en un 70%. El resto de este documento está organizado de la siguiente manera. En la siguiente sección revisamos el trabajo relacionado. Luego, en la Sección 3 proporcionamos descripciones detalladas de las tareas de recuperación de expertos que abordamos en este artículo: la búsqueda de expertos y la creación de perfiles de expertos. En la Sección 4 presentamos nuestros modelos base, cuyo rendimiento luego es evaluado en la Sección 6 utilizando el conjunto de datos UvT que presentamos en la Sección 5. Los modelos avanzados que explotan características específicas de nuestros datos se presentan en la Sección 7 y se evalúan en la Sección 8. Formulamos nuestras conclusiones en la Sección 9.2. Trabajos relacionados Los enfoques iniciales para encontrar expertos a menudo empleaban bases de datos que contenían información sobre las habilidades y conocimientos de cada individuo en la organización [11]. La mayoría de estas herramientas (generalmente llamadas páginas amarillas o sistemas de búsqueda de personas) dependen de que las personas evalúen sus habilidades frente a un conjunto predefinido de palabras clave. Para actualizar perfiles en estos sistemas de forma automática, se necesita de tecnologías inteligentes [5]. Enfoques más recientes utilizan conjuntos de documentos específicos (como correos electrónicos [6] o software [18]) para encontrar experiencia. En contraste con centrarse en tipos de documentos particulares, también hay un interés creciente en el desarrollo de sistemas que indexan y extraen información de documentos publicados en intranets como fuentes de evidencia de experiencia. Un enfoque publicado es el sistema P@noptic [9], que construye una representación de cada persona concatenando todos los documentos asociados con esa persona, similar al Modelo 1 de Balog et al. [4], quienes formalizan y comparan dos métodos. El Modelo 1 de Balog et al. modela directamente el conocimiento de un experto a partir de documentos asociados, mientras que su Modelo 2 primero localiza documentos sobre el tema y luego encuentra a los expertos asociados. En los experimentos reportados, el segundo método tiene un rendimiento significativamente mejor cuando hay suficientes documentos asociados por candidato. La mayoría de los sistemas que participaron en las ediciones de 2005 y 2006 de la tarea de Búsqueda de Expertos en TREC implementaron (variaciones de) uno de estos dos modelos; ver [10, 20]. Macdonald y Ounis [16] proponen un enfoque diferente para clasificar la experiencia de los candidatos con respecto a un tema basado en técnicas de fusión de datos, sin utilizar heurísticas específicas de la colección; encuentran que la aplicación de modelos de ponderación basados en campos mejora la clasificación de los candidatos. Petkova y Croft [19] proponen otro enfoque, basado en una combinación de los Modelos 1 y 2 anteriores, modelando explícitamente los temas. Al abordar otras tareas de recuperación de expertos que también pueden abordarse utilizando asociaciones entre temas y personas, Balog y de Rijke [3] abordaron la tarea de determinar perfiles de expertos temáticos. Si bien sus métodos resultaron ser eficientes en el corpus de la W3C, requieren una cantidad de datos que puede que no esté disponible en la típica organización intensiva en conocimiento. Balog y de Rijke [2] estudian la tarea relacionada de encontrar expertos que sean similares a un pequeño conjunto de expertos dados como entrada. Por cierto, crear un resumen textual de una persona muestra algunas similitudes con la búsqueda de biografías, la cual ha recibido una considerable cantidad de atención recientemente; ver por ejemplo, [13]. Utilizamos <br>modelado de lenguaje generativo</br> para encontrar asociaciones entre temas y personas. En nuestro modelado de búsqueda y perfilado de expertos recopilamos evidencia de experiencia de múltiples fuentes, en una colección heterogénea, e integramos esta información con la co-ocurrencia de nombres de candidatos y términos de consulta: el entorno de modelado de lenguaje nos permite hacer esto de manera transparente. Nuestro modelado avanza en dos pasos. En el primer paso, consideramos tres modelos base, dos tomados de [4] (los Modelos 1 y 2 mencionados anteriormente), y uno una versión refinada de un modelo introducido en [3] (al que nos referimos como Modelo 3 a continuación); este tercer modelo también es similar al modelo descrito por Petkova y Croft [19]. Los modelos que consideramos en nuestra segunda ronda de experimentos son modelos de mezcla similares a los modelos de lenguaje contextual [1] y a los documentos ampliados de Tao et al. [21]; sin embargo, las características que utilizamos para definir nuestras expansiones, incluida la estructura temática y la estructura organizativa, no se han utilizado de esta manera antes. 3. En el escenario de recuperación de expertos que imaginamos, los usuarios que buscan expertos dentro de una organización tienen acceso a una interfaz que combina un cuadro de búsqueda (donde pueden buscar expertos o temas) con estructuras de navegación (de expertos y de temas) que les permite hacer clic hasta llegar a una página de experto (que proporciona el perfil de una persona) o a una página de tema (que proporciona una lista de expertos en el tema). Para alimentar la interfaz anterior, nos enfrentamos a dos tareas de recuperación de expertos, la búsqueda de expertos y el perfilado de expertos, que primero definimos y luego formalizamos utilizando modelos de lenguaje generativos. Para modelar cualquiera de las tareas, la probabilidad de que el tema de la consulta esté asociado a un experto candidato juega un papel clave en las estimaciones finales para la búsqueda y el perfilado. Al utilizar modelos de lenguaje, tanto los candidatos como la consulta se caracterizan por distribuciones de términos en el vocabulario (utilizado en los documentos puestos a disposición por la organización cuyas necesidades de recuperación de experiencia estamos abordando). 3.1 Búsqueda de expertos La búsqueda de expertos implica la tarea de encontrar a la persona adecuada con las habilidades y conocimientos apropiados: ¿Quiénes son los expertos en el tema X?. Por ejemplo, un empleado quiere averiguar quién trabajó en un proyecto en particular para descubrir por qué se tomaron decisiones específicas sin tener que revisar minuciosamente la documentación (si es que la hay). O pueden necesitar un especialista capacitado para consultoría sobre un problema específico. Dentro de una organización, generalmente hay muchos posibles candidatos que podrían ser expertos en un tema dado. Podemos plantear este problema de la siguiente manera: ¿Cuál es la probabilidad de que un candidato ca sea un experto dado el tema de consulta q? Es decir, determinamos p(ca|q) y clasificamos a los candidatos ca de acuerdo con esta probabilidad. Los candidatos con la probabilidad más alta dada la consulta son considerados los expertos más probables en ese tema. El desafío es cómo estimar esta probabilidad con precisión. Dado que es probable que la consulta consista solo en unos pocos términos para describir la experiencia requerida, deberíamos poder obtener una estimación más precisa al invocar el Teorema de Bayes, y estimar: p(ca|q) = p(q|ca)p(ca) p(q) , (1) donde p(ca) es la probabilidad de un candidato y p(q) es la probabilidad de una consulta. Dado que p(q) es una constante, se puede ignorar para fines de clasificación. Por lo tanto, la probabilidad de que un candidato ca sea un experto dado la consulta q es proporcional a la probabilidad de una consulta dada el candidato p(q|ca), ponderada por la creencia a priori p(ca) de que el candidato ca es un experto. p(ca|q) ∝ p(q|ca)p(ca) (2) En este artículo, nuestro enfoque principal se centra en estimar la probabilidad de una consulta dada el candidato p(q|ca), ya que esta probabilidad captura en qué medida el candidato conoce sobre el tema de la consulta. Si bien se asume generalmente que los priors del candidato son uniformes y, por lo tanto, no influirán en la clasificación, se ha demostrado que una elección sensata de priors puede mejorar el rendimiento [20]. Perfilado de expertos. Mientras que la tarea de búsqueda de expertos se centraba en encontrar expertos en un tema específico, la tarea de perfilado de expertos busca responder a una pregunta relacionada: ¿Sobre qué temas tiene conocimiento un candidato? Básicamente, esto da un giro a las preguntas sobre la búsqueda de expertos. El perfilado de un candidato individual implica la identificación de áreas de habilidades y conocimientos en las que tienen experiencia, así como una evaluación del nivel de competencia en cada una de estas áreas. Este es el perfil temático de los candidatos. Generalmente, los perfiles temáticos dentro de las organizaciones consisten en estructuras tabulares que catalogan explícitamente las habilidades y conocimientos de cada individuo en la organización. Sin embargo, esta práctica está limitada por los recursos disponibles para definir, crear, mantener y actualizar estos perfiles con el tiempo. Al centrarnos en métodos automáticos que se basan en la evidencia disponible dentro de los repositorios de documentos de una organización, nuestro objetivo es reducir el esfuerzo humano asociado con el mantenimiento de perfiles temáticos. Un perfil temático de un candidato, entonces, se define como un vector donde cada elemento i del vector corresponde a la experiencia del candidato en un tema dado ki, (es decir, s(ca, ki)). Cada tema ki define un área de conocimiento o habilidad particular que la organización utiliza para definir el perfil temático de los candidatos. Por lo tanto, se asume que se proporciona una lista de temas, {k1, . . . , kn}, donde n es el número de temas predefinidos: perfil(ca) = s(ca, k1), s(ca, k2), . . . , s(ca, kn) . Se necesita contexto y evidencia para ayudar a los usuarios de sistemas de búsqueda de expertos a decidir a quién contactar al buscar experiencia en un área particular. Ejemplos de dicho contexto son: ¿Con quién trabaja ella? ¿Cuáles son sus datos de contacto? ¿Está bien conectada, por si acaso no puede ayudarnos ella misma? ¿Cuál es su rol en la organización? ¿Quién es su superior? Los colaboradores, afiliaciones, etc. son parte del perfil social de los candidatos y pueden servir como antecedentes para interpretar las recomendaciones del sistema. En este documento solo abordamos el problema de determinar perfiles temáticos, dejando el perfilado social para trabajos futuros. Planteamos el problema de cuantificar la competencia de una persona en un área de conocimiento específica de la siguiente manera: ¿Cuál es la probabilidad de que un área de conocimiento (ki) forme parte del perfil de candidatos (expertise)? donde s(ca, ki) está definido por p(ki|ca). Nuestra tarea, entonces, es estimar p(ki|ca), lo cual es equivalente al problema de obtener p(q|ca), donde el tema ki se representa como un tema de consulta q, es decir, una secuencia de palabras clave que representan la experiencia requerida. Tanto la tarea de encontrar expertos como la de perfilar expertos dependen de la estimación precisa de p(q|ca). La única diferencia proviene de la probabilidad previa de que una persona sea experta (p(ca)), la cual puede ser incorporada en la tarea de encontrar expertos. Este antecedente no se aplica a la tarea de perfilado ya que el candidato (individuo) está fijo. 4. MODELOS DE REFERENCIA En esta sección describimos nuestros modelos de referencia para estimar p(q|ca), es decir, las asociaciones entre temas y personas. Tanto la búsqueda de expertos como el perfilado de expertos se reducen a esta estimación. Empleamos tres modelos para calcular esta probabilidad. 4.1 De temas a candidatos Utilizando Modelos de Candidatos: El Modelo 1 [4] define la probabilidad de una consulta dada un candidato (p(q|ca)) utilizando técnicas estándar de modelado de lenguaje, basadas en un modelo de lenguaje multinomial de unigrama. Para cada candidato ca, se infiere un modelo de lenguaje candidato θca de tal manera que la probabilidad de un término dado θca no sea cero para todos los términos, es decir, p(t|θca) > 0. A partir del modelo de candidato, la consulta se genera con la siguiente probabilidad: p(q|θca) = Y t∈q p(t|θca)n(t,q), donde cada término t en la consulta q se muestrea de manera idéntica e independiente, y n(t, q) es el número de veces que t ocurre en q. El modelo de lenguaje candidato se infiere de la siguiente manera: (1) se calcula un modelo empírico p(t|ca); (2) se suaviza con probabilidades de fondo. Usando las asociaciones entre un candidato y un documento, la probabilidad p(t|ca) puede aproximarse por: p(t|ca) = X d p(t|d)p(d|ca), donde p(d|ca) es la probabilidad de que el candidato ca genere un documento de apoyo d, y p(t|d) es la probabilidad de que un término t ocurra en el documento d. Utilizamos la estimación de máxima verosimilitud de un término, es decir, la frecuencia normalizada del término t en el documento d. La fuerza de la asociación entre el documento d y el candidato ca expresada por p(d|ca) refleja el grado en que la experiencia de los candidatos se describe utilizando este documento. La estimación de esta probabilidad se presenta más adelante, en la Sección 4.2. El modelo candidato se construye entonces como una interpolación lineal de p(t|ca) y el modelo de fondo p(t) para asegurar que no haya probabilidades nulas, lo que resulta en la estimación final: p(q|θca) = (4) Y t∈q ( (1 − λ) X d p(t|d)p(d|ca) ! + λp(t) )n(t,q) . El Modelo 1 recopila toda la información de términos de todos los documentos asociados con el candidato, y la utiliza para representar a ese candidato. Este modelo se utiliza para predecir qué tan probable es que un candidato produzca una consulta q. Esto puede interpretarse intuitivamente como la probabilidad de que este candidato hable sobre el tema de la consulta, donde asumimos que esto es indicativo de su experiencia. Usando Modelos de Documentos: El Modelo 2 [4] toma un enfoque diferente. Aquí, el proceso se divide en dos partes. Dado un candidato ca, (1) se selecciona un documento asociado con un candidato con probabilidad p(d|ca), y (2) a partir de este documento se genera una consulta q con probabilidad p(q|d). Entonces se toma la suma sobre todos los documentos para obtener p(q|ca), de modo que: p(q|ca) = Σ d p(q|d)p(d|ca). (5) La probabilidad de una consulta dada un documento se estima inferiendo un modelo de lenguaje del documento θd para cada documento d de manera similar a como se infería el modelo candidato: p(t|θd) = (1 − λ)p(t|d) + λp(t), (6) donde p(t|d) es la probabilidad del término en el documento. La probabilidad de una consulta dada el modelo de documento es: p(q|θd) = Y t∈q p(t|θd)n(t,q) . La estimación final de p(q|ca) se obtiene sustituyendo p(q|d) por p(q|θd) en la Ecuación 5 (ver [4] para más detalles). Conceptualmente, el Modelo 2 difiere del Modelo 1 porque el candidato no está modelado directamente. En cambio, el documento actúa como una variable oculta en el proceso que separa la consulta del candidato. Este proceso es similar a cómo un usuario puede buscar candidatos con un motor de búsqueda estándar: inicialmente encontrando los documentos relevantes y luego viendo quién está asociado con ese documento. Al examinar una serie de documentos, el usuario puede obtener una idea de qué candidatos son más propensos a discutir el tema q. Usando Modelos de Temas: Modelo 3 Introducimos un tercer modelo, Modelo 3. En lugar de intentar modelar el proceso de generación de consultas a través de modelos de candidatos o documentos, representamos la consulta como un modelo de lenguaje de temas y estimamos directamente la probabilidad del candidato p(ca|q). Este enfoque es similar al modelo presentado en [3, 19]. Como en los modelos anteriores, se infiere un modelo de lenguaje, pero esta vez para la consulta. Adaptamos el trabajo de Lavrenko y Croft [14] para estimar un modelo de tema a partir de la consulta. El procedimiento es el siguiente. Dada una colección de documentos y un tema de consulta q, se asume que existe un modelo de tema desconocido θk que asigna probabilidades p(t|θk) a las ocurrencias de términos en los documentos del tema. Tanto la consulta como los documentos son muestras de θk (a diferencia de los enfoques anteriores, donde se asume que una consulta se extrae de un documento específico o un modelo candidato). La tarea principal es estimar p(t|θk), la probabilidad de un término dado el modelo de tema. Dado que la consulta q es muy dispersa y no hay ejemplos de documentos sobre el tema, es necesario aproximar esta distribución. Lavrenko y Croft [14] sugieren una forma razonable de obtener dicha aproximación, asumiendo que p(t|θk) puede aproximarse por la probabilidad del término t dado la consulta q. Entonces podemos estimar p(t|q) utilizando la probabilidad conjunta de observar el término t junto con los términos de la consulta, q1, . . . , qm, y dividiendo por la probabilidad conjunta de los términos de la consulta: p(t|θk) ≈ p(t|q) = p(t, q1, . . . , qm) p(q1, . . . , qm) = p(t, q1, . . . , qm) P t ∈T p(t , q1, . . . , qm) , donde p(q1, . . . , qm) = P t ∈T p(t , q1, . . . , qm), y T es el vocabulario completo de términos. Para estimar la probabilidad conjunta p(t, q1, . . . , qm), seguimos [14, 15] y asumimos que t y q1, . . . , qm son mutuamente independientes, una vez que elegimos una distribución fuente del conjunto de distribuciones fuente subyacentes U. Si elegimos U como un conjunto de modelos de documentos, entonces para construir este conjunto, la consulta q se emitiría contra la colección, y se asume que los primeros n devueltos son relevantes para el tema, y por lo tanto se tratan como muestras del modelo de tema. (Tenga en cuenta que en su lugar podrían usarse modelos candidatos). Con los modelos de documentos formando U, la probabilidad conjunta del término y la consulta se convierte en: p(t, q1, . . . , qm) = X d∈U p(d) ˘ p(t|θd) mY i=1 p(qi|θd) ¯ . (7) Aquí, p(d) denota la distribución previa sobre el conjunto U, que refleja la relevancia del documento para el tema. Suponemos que p(d) es uniforme en todo U. Para clasificar a los candidatos según el modelo de tema definido, utilizamos la métrica de divergencia de Kullback-Leibler (KL, [8]) para medir la diferencia entre los modelos de los candidatos y el modelo de tema: KL(θk||θca) = X t p(t|θk) log p(t|θk) p(t|θca) . (8) Se considera que los candidatos con una divergencia menor respecto al modelo de tema son más propensos a ser expertos en ese tema. El modelo candidato θca está definido en la Ecuación 4. Al utilizar la divergencia de Kullback-Leibler en lugar de la probabilidad de un candidato dado el modelo de tema p(ca|θk), evitamos problemas de normalización. 4.2 Asociaciones documento-candidato Para nuestros modelos, necesitamos poder estimar la probabilidad p(d|ca), que expresa en qué medida un documento d caracteriza al candidato ca. En [4], se presentan dos métodos para estimar esta probabilidad, basados en el número de nombres de personas reconocidos en un documento. Sin embargo, en nuestro entorno de intranet es razonable asumir que los autores de los documentos pueden ser identificados de manera inequívoca (por ejemplo, como el autor de un artículo, el profesor asignado a un curso, el propietario de una página web, etc.). Por lo tanto, establecemos p(d|ca) en 1 si el candidato ca es el autor del documento d, de lo contrario la probabilidad es 0. En la Sección 6 describimos cómo se puede determinar la autoría en diferentes tipos de documentos dentro de la colección. 5. La colección de expertos de UvT utilizada en los experimentos de este artículo se ajusta al escenario descrito en la Sección 3. La colección se basa en el sistema Webwijs (Webwise) desarrollado en la Universidad de Tilburg (UvT) en los Países Bajos. Webwijs (http://www.uvt.nl/webwijs/) es una base de datos de acceso público de empleados de la UvT que están involucrados en investigación o enseñanza; actualmente, Webwijs contiene información sobre 1168 expertos, cada uno de los cuales tiene una página con información de contacto y, si está disponible por parte del experto, una descripción de investigación y lista de publicaciones. Además, cada experto puede seleccionar áreas de especialización de una lista de 1491 temas y se le anima a sugerir nuevos temas que necesitan ser aprobados por el editor de Webwijs. Cada tema tiene una página separada que muestra a todos los expertos asociados con ese tema y, si está disponible, una lista de temas relacionados. Webwijs está disponible en holandés e inglés, y esta bilingüidad se ha preservado en la colección. Cada página de Webwijs en holandés tiene una traducción al inglés. No todos los temas en holandés tienen una traducción al inglés, pero lo contrario es cierto: los 981 temas en inglés tienen un equivalente en holandés. Aproximadamente el 42% de los expertos imparten cursos en la Universidad de Tilburg; estos cursos también fueron rastreados e incluidos en el perfil. Además, aproximadamente el 27% de los expertos enlazan su página académica desde su página de Webwijs. Estas páginas de inicio fueron rastreadas y añadidas a la colección. (Esto significa que si los expertos colocaron las versiones completas de sus publicaciones en sus páginas de inicio académicas, estas también estaban disponibles para su indexación). También obtuvimos 1880 versiones completas de publicaciones del repositorio institucional de UvT y el número de expertos holandeses en inglés es de 1168, con 743 expertos con ≥ 1 tema. El número de temas es de 1491 y 981, y el número de pares experto-tema es de 4318 y 3251, con un promedio de temas por experto de 5.8 y 5.9 respectivamente. El máximo número de temas por experto es de 60 (1) y 35 (1), y el mínimo es de 1 (74) y 1 (106) respectivamente. El promedio de expertos por tema es de 2.9 y 3.3, con un máximo de 30 (1) y un mínimo de 1 (615) y 1 (346) respectivamente. Hay 318 expertos con HP y CD, con un promedio de 3.5 CDs por experto docente. Además, hay 329 expertos con RD y 734 con PUB, con un promedio de 27.0 publicaciones por experto, 25.2 citas por experto y 1.8 publicaciones completas por experto. Tabla 2: Estadísticas descriptivas de las versiones holandesas e inglesas de la colección de expertos de UvT. Ejecutamos el identificador de idioma TextCat [23] para clasificar el idioma de las páginas de inicio y las publicaciones de texto completo. Nos limitamos a las páginas en las que el clasificador tenía confianza en el idioma utilizado en la página. Esto resultó en cuatro tipos de documentos: descripciones de investigación (RD), descripciones de cursos (CD), publicaciones (PUB; versiones de texto completo y solo de cita) y páginas web académicas (HP). Todo fue agrupado en la colección UvT Expert que está disponible en http://ilk.uvt.nl/uvt-expert-collection/. La colección UvT Expert fue extraída de un entorno organizativo diferente a la colección de la W3C y difiere de ella en varios aspectos. El entorno de UvT es uno con cantidades relativamente pequeñas de datos multilingües. Las asociaciones entre los documentos y los autores son claras y los datos están estructurados y limpios. La colección abarca una amplia gama de áreas de especialización, como suele encontrarse en intranets de universidades y otros institutos intensivos en conocimiento. Además, nuestro entorno universitario cuenta con varios tipos de estructura (temática y organizativa), así como múltiples tipos de documentos. Otra diferencia importante entre los dos conjuntos de datos es que las áreas de especialización en la colección de Expertos de UvT son autoseleccionadas en lugar de basarse en la membresía de un grupo o asignaciones de otros. El tamaño es otra dimensión en la que difieren las colecciones de expertos de W3C y UvT: esta última es la más pequeña de las dos. También son realistas las grandes diferencias en la cantidad de información disponible para cada experto. Utilizar Webwijs es voluntario; 425 expertos holandeses no seleccionaron ningún tema en absoluto. Esto nos deja con 743 perfiles de expertos holandeses y 727 perfiles de expertos en inglés utilizables. La Tabla 2 proporciona estadísticas descriptivas para la colección de Expertos de UvT. Las universidades tienden a tener una estructura jerárquica que va desde el nivel de la facultad, pasando por los departamentos, grupos de investigación, hasta llegar a los investigadores individuales. En la colección de Expertos de UvT tenemos información sobre las afiliaciones de los investigadores con facultades e institutos, lo que nos proporciona una jerarquía organizativa de dos niveles. La Universidad de Tilburg tiene 22 unidades organizativas a nivel de facultad (incluida la oficina universitaria y varios institutos de investigación) y 71 departamentos, lo que equivale a 3.2 departamentos por facultad. En cuanto a la jerarquía temática utilizada por Webwijs, 131 de los 1491 temas son nodos principales en la jerarquía. Esta jerarquía tiene una longitud promedio de cadena de temas de 2.65 y una longitud máxima de 7 temas. 6. EVALUACIÓN A continuación, evaluamos los modelos de la Sección 4 para la búsqueda y perfilado de expertos en la colección de Expertos de UvT. Detallamos nuestras preguntas de investigación y configuración experimental, y luego presentamos nuestros resultados. 6.1 Preguntas de investigación Abordamos las siguientes preguntas de investigación. Tanto la búsqueda de expertos como el perfilado dependen de las estimaciones de p(q|ca). La pregunta es cómo se comparan los modelos en las diferentes tareas, y en el contexto de la colección de expertos de UvT. En [4], el Modelo 2 superó al Modelo 1 en la colección de la W3C. ¿Cómo se comparan en nuestro conjunto de datos? ¿Y cómo se compara el Modelo 3 con el Modelo 1? ¿Qué hay de las diferencias de rendimiento entre los dos idiomas en nuestra colección de pruebas? 6.2 Configuración Experimental La salida de nuestros modelos fue evaluada frente a las etiquetas de temas autoasignadas, que fueron tratadas como juicios de relevancia. Los resultados fueron evaluados por separado para inglés y holandés. Para inglés solo usamos temas para los cuales había traducción al neerlandés; para neerlandés se consideraron todos los temas. Los resultados se promediaron para las consultas en la intersección de las evaluaciones de relevancia y los resultados; las consultas faltantes no contribuyen con un valor de 0 a las puntuaciones. Utilizamos medidas estándar de recuperación de información, como la Precisión Media Promedio (MAP) y la Reciprocidad Media Promedio (MRR). También informamos el porcentaje de temas (%q) y candidatos (%ca) cubiertos, respectivamente, para las tareas de búsqueda y perfilado de expertos. 6.3 Resultados La Tabla 1 muestra el rendimiento de los Modelos 1, 2 y 3 en las tareas de búsqueda y perfilado de expertos. Las filas de la tabla corresponden a los diversos tipos de documentos (RD, CD, PUB y HP) y a sus combinaciones. RD+CD+PUB+HP es equivalente a la colección completa y será referida como la LÍNEA BASE de nuestros experimentos. Al observar la Tabla 1, vemos que el Modelo 2 tiene el mejor rendimiento en general. Sin embargo, cuando los datos están limpios y muy enfocados (RD), el Modelo 3 lo supera en varios casos. El modelo 1 tiene la mejor cobertura de candidatos (%ca) y temas (%q). Los diversos tipos de documentos difieren en sus características y en cómo mejoran las tareas de búsqueda y perfilado. El perfilado de expertos se beneficia mucho de los datos limpios presentes en los tipos de documentos RD y CD, mientras que las publicaciones contribuyen principalmente a la tarea de encontrar expertos. Agregar las páginas de inicio no resulta ser particularmente útil. Cuando comparamos los resultados entre idiomas, encontramos que la cobertura de los temas en inglés (%q) es mayor que la de los temas en neerlandés para la búsqueda de expertos. Además de eso, las puntuaciones caen en el mismo rango para ambos idiomas. Para la tarea de perfilado, la cobertura de los candidatos (%ca) es muy similar para ambos idiomas. Sin embargo, el rendimiento es considerablemente mejor para los temas en inglés. Si bien es difícil comparar las puntuaciones entre colecciones, concluimos con una breve comparación de las puntuaciones absolutas en la Tabla 1 con las reportadas en [3, 4] en el conjunto de pruebas de W3C (edición de 2005). Para el experto que busca, los puntajes MAP para el Modelo 2 reportados aquí son aproximadamente un 50% más altos que las cifras correspondientes en [4], mientras que nuestros puntajes MRR están ligeramente por debajo de los de [4]. Para el perfilado de expertos, las diferencias son mucho más dramáticas: las puntuaciones MAP para el Modelo 2 reportadas aquí son aproximadamente un 50% más bajas que las puntuaciones en [3], mientras que las puntuaciones MRR (mejores) son aproximadamente iguales a las de [3]. La causa de estas diferencias parece residir en el número de áreas de conocimiento consideradas aquí, aproximadamente 30 veces más que en el entorno del W3C. MODELOS AVANZADOS Ahora que hemos desarrollado y evaluado técnicas básicas de modelado de lenguaje para la recuperación de expertos, nos enfocamos en modelos refinados que explotan características especiales de nuestra colección de pruebas. 7.1 Explotando la similitud en el área de conocimiento Una forma de mejorar la puntuación de una consulta dada un candidato es considerar qué otras solicitudes satisfaría el candidato y usarlas como evidencia adicional para respaldar la consulta original, proporcionalmente. Búsqueda de expertos Perfilado de expertos Tipos de documentos Modelo 1 Modelo 2 Modelo 3 Modelo 1 Modelo 2 Modelo 3 %q MAP MRR %q MAP MRR %q MAP MRR %ca MAP MRR %ca MAP MRR %ca MAP MRR Inglés RD 97.8 0.126 0.269 83.5 0.144 0.311 83.3 0.129 0.271 100 0.089 0.189 39.3 0.232 0.465 41.1 0.166 0.337 CD 97.8 0.118 0.227 91.7 0.123 0.248 91.7 0.118 0.226 32.8 0.188 0.381 32.4 0.195 0.385 32.7 0.203 0.370 PUB 97.8 0.200 0.330 98.0 0.216 0.372 98.0 0.145 0.257 78.9 0.167 0.364 74.5 0.212 0.442 78.9 0.135 0.299 HP 97.8 0.081 0.186 97.4 0.071 0.168 97.2 0.062 0.149 31.2 0.150 0.299 28.8 0.185 0.335 30.1 0.136 0.287 RD+CD 97.8 0.188 0.352 92.9 0.193 0.360 92.9 0.150 0.273 100 0.145 0.286 61.3 0.251 0.477 63.2 0.217 0.416 RD+CD+PUB 97.8 0.235 0.373 98.1 0.277 0.439 98.1 0.178 0.305 100 0.196 0.380 87.2 0.280 0.533 89.5 0.170 0.344 RD+CD+PUB+HP 97.8 0.237 0.372 98.6 0.280 0.441 98.5 0.166 0.293 100 0.199 0.387 88.7 0.281 0.525 90.9 0.169 0.329 Holandés RD 61.3 0.094 0.229 38.4 0.137 0.336 38.3 0.127 0.295 38.0 0.127 0.386 34.1 0.138 0.420 38.0 0.105 0.327 CD 61.3 0.107 0.212 49.7 0.128 0.256 49.7 0.136 0.261 32.5 0.151 0.389 31.8 0.158 0.396 32.5 0.170 0.380 PUB 61.3 0.193 0.319 59.5 0.218 0.368 59.4 0.173 0.291 78.8 0.126 0.364 76.0 0.150 0.424 78.8 0.103 0.294 HP 61.3 0.063 0.169 56.6 0.064 0.175 56.4 0.062 0.163 29.8 0.108 0.308 27.8 0.125 0.338 29.8 0.098 0.255 RD+CD 61.3 0.159 0.314 51.9 0.184 0.360 51.9 0.169 0.324 60.5 0.151 0.410 57.2 0.166 0.431 60.4 0.159 0.384 RD+CD+PUB 61.3 0.244 0.398 61.5 0.260 0.424 61.4 0.210 0.350 90.3 0.165 0.445 88.2 0.189 0.479 90.3 0.126 0.339 RD+CD+PUB+HP 61.3 0.249 0.401 62.6 0.265 0.436 62.6 0.195 0.344 91.9 0.164 0.426 90.1 0.195 0.488 91.9 0.125 0.328 Tabla 1: Rendimiento de los modelos en las tareas de búsqueda y perfilado de expertos, utilizando diferentes tipos de documentos y sus combinaciones. %q es el número de temas cubiertos (se aplica a la tarea de búsqueda de expertos), %ca es el número de candidatos cubiertos (se aplica a la tarea de perfilado de expertos). Los bloques superior e inferior corresponden al inglés y al holandés respectivamente. Las mejores puntuaciones están en negrita. Esto se puede modelar interpolando entre el p(q|ca) y la evidencia adicional de todos los pedidos similares q, de la siguiente manera: p(q|ca) = λp(q|ca) + (1 − λ) X q p(q|q )p(q |ca), (9) donde p(q|q ) representa la similitud entre los dos temas q y q. Para poder trabajar con métodos de similitud que no son necesariamente probabilidades, establecemos p(q|q ) = w(q,q ) γ , donde γ es una constante de normalización, de modo que γ = P q w(q , q ). Consideramos cuatro métodos para calcular la puntuación de similitud entre dos temas. Tres enfoques se basan estrictamente en el contenido y establecen similitudes examinando patrones de co-ocurrencia de temas dentro de la colección, mientras que el último enfoque explota la estructura jerárquica de áreas temáticas que pueden estar presentes dentro de una organización (ver [7] para más ejemplos de integrar relaciones entre palabras en modelos de lenguaje). La métrica de divergencia Kullback-Leibler (KL) definida en la ecuación 8 proporciona una medida de cuán diferentes o similares son dos distribuciones de probabilidad. Se infiere un modelo de tema para q y q utilizando el método presentado en la Sección 4.1 para describir la consulta en todo el vocabulario. Dado que un puntaje KL más bajo significa que las consultas son más similares, permitimos que w(q, q ) = max(KL(θq||·) − KL(θq||θq )). El Información Mutua Puntual (PMI, [17]) es una medida de asociación utilizada en teoría de la información para determinar el grado de independencia entre variables. La dependencia entre dos consultas se refleja en la puntuación SI(q, q), donde las puntuaciones mayores que cero indican que es probable que exista una dependencia, lo que interpretamos como que las consultas son probablemente similares: SI(q, q) = log p(q, q) / p(q)p(q). Estimamos la probabilidad de un tema p(q) utilizando el número de documentos relevantes para la consulta q dentro de la colección. La probabilidad conjunta p(q, q) se estima de manera similar, utilizando la concatenación de q y q como una consulta. Para obtener p(q|q), luego establecemos w(q, q) = SI(q, q) cuando SI(q, q) > 0, de lo contrario w(q, q) = 0, ya que solo estamos interesados en incluir consultas que sean similares. La estadística de log-verosimilitud proporciona otra medida de dependencia, que es más confiable que la medida de información mutua puntual [17]. Sea k1 el número de co-ocurrencias de q y q, k2 el número de ocurrencias de q que no co-ocurren con q, n1 el número total de ocurrencias de q, y n2 el número total de tokens de tema menos el número de ocurrencias de q. Entonces, sea p1 = k1/n1, p2 = k2/n2 y p = (k1 + k2)/(n1 + n2), (q, q ) = 2( (p1, k1, n1) + (p2, k2, n2) − (p, k1, n1) − (p, k2, n2)), donde (p, n, k) = k log p + (n − k) log(1 − p). El puntaje más alto indica que las consultas también son probablemente similares, por lo tanto, establecemos w(q, q ) = (q, q ). Finalmente, también estimamos la similitud de dos temas basándonos en su distancia dentro de la jerarquía de temas. La jerarquía de temas se visualiza como un grafo dirigido, y para todos los pares de temas se calcula el camino más corto SP(q, q). Establecimos el puntaje de similitud como el recíproco del camino más corto: w(q, q ) = 1/SP(q, q ). 7.2 Información contextual Dada la jerarquía de una organización, las unidades a las que pertenece una persona se consideran como un contexto para compensar la escasez de datos. Lo modelamos de la siguiente manera: p (q|ca) = 1 − P ou∈OU(ca) λou · p(q|ca) + P ou∈OU(ca) λou · p(q|ou), donde OU(ca) es el conjunto de unidades organizativas de las cuales el candidato ca es miembro, y p(q|o) expresa la fuerza de la asociación entre la consulta q y la unidad ou. La última probabilidad se puede estimar utilizando cualquiera de los tres modelos básicos, simplemente reemplazando ca con ou en las ecuaciones correspondientes. Una unidad organizativa está asociada con todos los documentos que sus miembros han escrito. Es decir, p(d|ou) = maxca∈ou p(d|ca). 7.3 Un modelo multilingüe simple Para institutos de conocimiento en Europa, ya sea académicos u otros, un entorno multilingüe (o al menos bilingüe) es típico. El siguiente modelo se basa en un tipo de suposición de independencia: no hay transferencia de conocimientos/perfiles a través de las barreras del idioma. Si bien es una simplificación, esta es una aproximación sensata en primer lugar. Eso es: p (q|ca) =P l∈L λl · p(ql|ca), donde L es el conjunto de idiomas utilizados en la colección, ql es la traducción de la consulta q al idioma l, y λl es un parámetro de suavizado específico del idioma, tal que P l∈L λl = 1. 8. MODELOS AVANZADOS: EVALUACIÓN En esta sección presentamos una evaluación experimental de nuestros modelos avanzados. Búsqueda de expertos Perfilado de expertos Modelo de lenguaje 1 Modelo 2 Modelo 3 Modelo 1 Modelo 2 Modelo 3 %q MAP MRR %q MAP MRR %q MAP MRR %ca MAP MRR %ca MAP MRR %ca MAP MRR Solo inglés 97.8 0.237 0.372 98.6 0.280 0.441 98.5 0.166 0.293 100 0.199 0.387 88.7 0.281 0.525 90.9 0.169 0.329 Solo holandés 61.3 0.249 0.401 62.6 0.265 0.436 62.6 0.195 0.344 91.9 0.164 0.426 90.1 0.195 0.488 91.9 0.125 0.328 Combinación 99.4 0.297 0.444 99.7 0.324 0.491 99.7 0.223 0.388 100 0.241 0.445 92.1 0.313 0.564 93.2 0.224 0.411 Tabla 3: Rendimiento de la combinación de idiomas en las tareas de búsqueda y perfilado de expertos (en candidatos). Los mejores puntajes para cada modelo están en cursiva, los mejores puntajes absolutos para las tareas de búsqueda y perfilado de expertos están en negrita. Tabla 4: Rendimiento en las tareas de búsqueda de expertos (arriba) y perfilado (abajo), utilizando similitudes de áreas de conocimiento. Las carreras fueron evaluadas en los temas principales establecidos. Las mejores puntuaciones están en negrita. 8.1 Preguntas de investigación Nuestras preguntas siguen los refinamientos presentados en la sección anterior: ¿Explotar la similitud en el área de conocimiento mejora la efectividad? ¿Cuál de los diversos métodos para capturar las relaciones entre palabras es más efectivo? Además, ¿es útil nuestra forma de incorporar información contextual? ¿Para qué tareas? Y finalmente, ¿es nuestra forma simple de combinar las puntuaciones monolingües suficiente para obtener mejoras significativas? 8.2 Configuración experimental Dado que las autoevaluaciones también son escasas en nuestra colección, para poder medir las diferencias entre los diversos modelos, seleccionamos un subconjunto de temas y evaluamos (algunas de las) ejecuciones solo en este subconjunto. Este conjunto se denomina como temas principales, y consiste en temas que se encuentran en el nivel superior de la jerarquía temática. (Un tema principal tiene subtemas, pero no es un subtema de ningún otro tema). Este conjunto principal consiste en 132 temas en holandés y 119 en inglés. Los juicios de relevancia se limitaron al conjunto de temas principales, pero no se ampliaron con subtemas. 8.3 Explotando la similitud de áreas de conocimiento. La Tabla 4 presenta los resultados. Los cuatro métodos utilizados para estimar la similitud de áreas de conocimiento son la divergencia de KL (KLDIV) y PointLang. Modelo 1 Modelo 2 Modelo 3 MAP MRR MAP MRR MAP MRR Búsqueda de expertos UK TODOS 0.423 0.545 0.654 0.799 0.494 0.629 UK PRINCIPAL 0.500 0.621 0.704 0.834 0.587 0.699 NL TODOS 0.439 0.560 0.672 0.826 0.480 0.630 NL PRINCIPAL 0.440 0.584 0.645 0.816 0.515 0.655 Perfilado de expertos UK TODOS 0.240 0.640 0.306 0.778 0.223 0.616 UK PRINCIPAL 0.523 0.677 0.519 0.648 0.461 0.587 NL TODOS 0.203 0.716 0.254 0.770 0.183 0.627 NL PRINCIPAL 0.332 0.576 0.380 0.624 0.332 0.549 Tabla 5: Evaluación de los modelos de contexto en unidades organizativas. información mutua ponderada (PMI), log-verosimilitud (LL) y distancia dentro de la jerarquía de temas (HDIST). Logramos mejorar el resultado base en todos los casos, pero la mejora es más notable para la tarea de perfilado. Para ambas tareas, el método LL tuvo el mejor rendimiento. Los enfoques basados en el contenido tuvieron un rendimiento consistentemente mejor que HDIST. 8.4 Información contextual Una jerarquía de dos niveles de unidades organizativas (facultades e institutos) está disponible en la colección de Expertos de UvT. La unidad a la que pertenece una persona se utiliza como contexto para esa persona. Primero, evaluamos los modelos de las unidades organizativas, utilizando todos los temas (TODOS) y solo los temas principales (PRINCIPALES). Una unidad organizativa se considera relevante para un tema dado (o viceversa) si al menos un miembro de la unidad seleccionó el tema dado como área de especialización. La Tabla 5 informa sobre los resultados. En lo que respecta a la búsqueda de expertos, dado un tema, la unidad organizativa correspondiente puede ser identificada con alta precisión. Sin embargo, la tarea de perfilado de expertos muestra una imagen diferente: las puntuaciones son bajas y la tarea parece difícil. La explicación puede ser que los conceptos generales (es decir, nuestros temas principales) pueden pertenecer a varias unidades organizativas. Segundo, realizamos otra evaluación, donde combinamos los modelos contextuales con los modelos candidatos (para puntuar a los candidatos nuevamente). La tabla 6 informa sobre los resultados. Encontramos un impacto positivo de los modelos de contexto solo para la búsqueda de expertos. Notablemente, para la búsqueda de expertos (y Modelo 1), mejora más del 50% (para inglés) y más del 70% (para holandés) en el MAP. El bajo rendimiento en la creación de perfiles de expertos puede deberse al hecho de que los modelos de contexto por sí solos no tuvieron un buen desempeño en la tarea de creación de perfiles desde el principio. 8.5 Modelos multilingües En esta subsección evaluamos el método para combinar resultados en múltiples idiomas que describimos en la Sección 7.3. En nuestro entorno, el conjunto de idiomas consiste en inglés y holandés: L = {UK, NL}. Los pesos de estos idiomas se establecieron para ser idénticos (λUK = λNL = 0.5). Realizamos experimentos con diferentes configuraciones de λ, pero no observamos diferencias significativas en el rendimiento. La Tabla 3 informa sobre los resultados multilingües, donde el rendimiento se evalúa en el conjunto completo de temas. Los tres modelos tienen una diferencia significativa. Tabla 6: Rendimiento de los modelos de contexto (CT) en comparación con la línea base (BL). Las mejores puntuaciones están en negrita, demostradas en todas las medidas para ambas tareas. La cobertura de temas y candidatos para las tareas de búsqueda y perfilado de expertos, respectivamente, es cercana al 100% en todos los casos. La mejora relativa de las puntuaciones de precisión varía del 10% al 80%. Estas puntuaciones demuestran que, a pesar de su simplicidad, nuestro método para combinar resultados en varios idiomas logra mejoras sustanciales sobre el punto de referencia. CONCLUSIONES En este artículo nos enfocamos en la recuperación de expertos (búsqueda y perfilado de expertos) en un nuevo entorno de una organización típica intensiva en conocimiento en la que los datos disponibles son de alta calidad, multilingües y abarcan una amplia gama de áreas de expertise. Normalmente, la cantidad de datos disponibles en una organización como una universidad, un instituto de investigación o un laboratorio de investigación es limitada en comparación con la colección de la W3C que ha sido utilizada principalmente para la evaluación experimental de la recuperación de la experiencia hasta ahora. Para examinar la recuperación de la experiencia en este entorno, presentamos (y lanzamos) la colección de Expertos de UvT como un caso representativo de organizaciones intensivas en conocimiento. La nueva colección refleja las propiedades típicas de los institutos intensivos en conocimiento mencionados anteriormente e incluye también varias características que pueden ser potencialmente útiles para la recuperación de expertos, como la estructura temática y organizativa. Evaluamos cómo los modelos de vanguardia para la búsqueda y perfilado de expertos se desempeñaron en este nuevo entorno y luego refinamos estos modelos para intentar explotar las diferentes características dentro del entorno de datos (idioma, actualidad y estructura organizativa). Encontramos que los modelos actuales de recuperación de experiencia se generalizan bien a este nuevo entorno; además, descubrimos que refinar los modelos para tener en cuenta las diferencias resulta en mejoras significativas, compensando así los problemas causados por la escasez de datos. El trabajo futuro incluye establecer evaluaciones manuales de perfiles generados automáticamente por los propios empleados, especialmente en casos en los que los empleados no han proporcionado un perfil ellos mismos. 10. AGRADECIMIENTOS Krisztian Balog fue apoyado por la Organización Neerlandesa para la Investigación Científica (NWO) bajo el número de proyecto 220-80-001. Maarten de Rijke también recibió apoyo de NWO bajo los números de proyecto 017.001.190, 220-80-001, 264-70-050, 354-20-005, 600.065.120, 612-13-001, 612.000.106, 612.066.302, 612.069.006, 640.001.501, 640.002.501, y de la Unión Europea. Programa IST del 6º Programa Marco para la I+D en virtud del contrato del proyecto MultiMATCH IST-033104. El trabajo de Toine Bogers y Antal van den Bosch fue financiado por el programa IOP-MMI de SenterNovem / Ministerio de Asuntos Económicos de los Países Bajos, como parte del proyecto 'A Propos'. 11. REFERENCIAS [1] L. Azzopardi. Incorporando contexto en el marco de modelado del lenguaje para la recuperación de información ad-hoc. Tesis doctoral, Universidad de Paisley, 2005. [2] K. Balog y M. de Rijke. Encontrando expertos similares. En este volumen, 2007. [3] K. Balog y M. de Rijke. Determinación de perfiles de expertos (con una aplicación a la búsqueda de expertos). En IJCAI 07: Proc. 20th Intern. Conferencia Conjunta sobre Inteligencia Artificial, páginas 2657-2662, 2007. [4] K. Balog, L. Azzopardi y M. de Rijke. Modelos formales para la búsqueda de expertos en corporaciones empresariales. En SIGIR 06: Actas de la 29ª conferencia internacional anual. Conferencia ACM SIGIR sobre Investigación y desarrollo en recuperación de información, páginas 43-50, 2006. [5] I. Becerra-Fernández. El papel de las tecnologías de inteligencia artificial en la implementación de sistemas de gestión del conocimiento de localización de personas. En el taller de AAAI sobre la incorporación de conocimiento en los procesos de negocio, marzo de 2000. [6] C. S. Campbell, P. P. Maglio, A. Cozzi y B. Dom. Identificación de la experiencia utilizando comunicaciones por correo electrónico. En CIKM 03: Actas de la duodécima conferencia internacional sobre gestión de la información y el conocimiento, páginas 528-531, 2003. [7] G. Cao, J.-Y. Nie, y J. Bai. Integrando relaciones entre palabras en modelos de lenguaje. En SIGIR 05: Actas de la 28ª conferencia internacional anual. Conferencia ACM SIGIR sobre Investigación y desarrollo en recuperación de información, páginas 298-305, 2005. [8] T. M. Cover y J. A. Thomas. Elementos de la teoría de la información. Wiley-Interscience, 1991. [9] N. Craswell, D. Hawking, A. M. Vercoustre y P. Wilkins. Experto en P@noptic: Buscando expertos no solo documentos. En Ausweb, 2001. [10] N. Craswell, A. de Vries e I. Soboroff. Resumen de la pista empresarial TREC2005. En la Decimocuarta Conferencia de Recuperación de Información. Proc. (TREC 2005), 2006. [11] T. H. Davenport y L. Prusak. Conocimiento en Acción: Cómo las Organizaciones Gestionan lo que Saben. Harvard Business School Press, Boston, MA, 1998. [12] T. Dunning.\nHarvard Business School Press, Boston, MA, 1998. [12] T. Dunning. Métodos precisos para la estadística de sorpresa y coincidencia. Lingüística Computacional, 19(1):61-74, 1993. [13] E. Filatova y J. Prager. Dime qué haces y te diré quién eres: Aprendiendo actividades relacionadas con la ocupación para biografías. En HLT/EMNLP, 2005. [14] V. Lavrenko y W. B. Croft. Modelos de lenguaje basados en relevancia. En SIGIR 01: Actas de la 24ª conferencia internacional anual. Conferencia ACM SIGIR sobre Investigación y Desarrollo en Recuperación de Información, páginas 120-127, 2001. [15] V. Lavrenko, M. Choquette y W. B. Croft. Modelos de relevancia multilingües. En SIGIR 02: Actas de la 25ª conferencia internacional anual. Conferencia ACM SIGIR sobre Investigación y Desarrollo en Recuperación de Información, páginas 175-182, 2002. [16] C. Macdonald e I. Ounis. Votación de candidatos: adaptando técnicas de fusión de datos para una tarea de búsqueda de expertos. En CIKM 06: Actas de la 15ª conferencia internacional de ACM sobre gestión de la información y el conocimiento, páginas 387-396, 2006. [17] C. Manning y H. Schütze. Fundamentos del Procesamiento del Lenguaje Natural Estadístico. El MIT Press, 1999. [18] A. Mockus y J. D. Herbsleb. Explorador de experticia: un enfoque cuantitativo para identificar la experticia. En ICSE 02: Proc. 24th Intern. Conf. en Ingeniería de Software, páginas 503-512, 2002. [19] D. Petkova y W. B. Croft. Modelos de lenguaje jerárquicos para la búsqueda de expertos en corporaciones empresariales. En Proc. ICTAI 2006, páginas 599-608, 2006. [20] I. Soboroff, A. de Vries y N. Craswell. Resumen de la pista empresarial TREC 2006. En las Notas de Trabajo de TREC 2006, 2006. [21] T. Tao, X. Wang, Q. Mei y C. Zhai. Recuperación de información del modelo de lenguaje con expansión de documentos. En HLT-NAACL 2006, 2006. [22] TREC. Ruta empresarial, 2005. URL: http://www.ins.cwi.nl/projects/trec-ent/wiki/. [23] G. van Noord.\nURL: http://www.ins.cwi.nl/projects/trec-ent/wiki/. [23] G. van Noord. Adivinador de idioma TextCat. URL: http://www.let.rug.nl/˜vannoord/TextCat/. [24] W3C. La colección de pruebas del W3C, 2005. URL: http://research.microsoft.com/users/nickcr/w3c-summary.html. ",
            "candidates": [],
            "error": [
                [
                    "modelado del lenguaje generativo",
                    "modelado de lenguaje generativo",
                    "modelado de lenguaje generativo"
                ]
            ]
        },
        "baseline expertise retrieval method": {
            "translated_key": "métodos de recuperación de conocimientos básicos",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Broad Expertise Retrieval in Sparse Data Environments Krisztian Balog ISLA, University of Amsterdam Kruislaan 403, 1098 SJ Amsterdam, The Netherlands kbalog@science.uva.nl Toine Bogers ILK, Tilburg University P.O.",
                "Box 90153, 5000 LE Tilburg, The Netherlands A.M.Bogers@uvt.nl Leif Azzopardi Dept. of Computing Science University of Glasgow, Glasgow, G12 8QQ leif@dcs.gla.ac.uk Maarten de Rijke ISLA, University of Amsterdam Kruislaan 403, 1098 SJ Amsterdam, The Netherlands mdr@science.uva.nl Antal van den Bosch ILK, Tilburg University P.O.",
                "Box 90153, 5000 LE Tilburg, The Netherlands Antal.vdnBosch@uvt.nl ABSTRACT Expertise retrieval has been largely unexplored on data other than the W3C collection.",
                "At the same time, many intranets of universities and other knowledge-intensive organisations offer examples of relatively small but clean multilingual expertise data, covering broad ranges of expertise areas.",
                "We first present two main expertise retrieval tasks, along with a set of baseline approaches based on generative language modeling, aimed at finding expertise relations between topics and people.",
                "For our experimental evaluation, we introduce (and release) a new test set based on a crawl of a university site.",
                "Using this test set, we conduct two series of experiments.",
                "The first is aimed at determining the effectiveness of <br>baseline expertise retrieval method</br>s applied to the new test set.",
                "The second is aimed at assessing refined models that exploit characteristic features of the new test set, such as the organizational structure of the university, and the hierarchical structure of the topics in the test set.",
                "Expertise retrieval models are shown to be robust with respect to environments smaller than the W3C collection, and current techniques appear to be generalizable to other settings.",
                "Categories and Subject Descriptors H.3 [Information Storage and Retrieval]: H.3.1 Content Analysis and Indexing; H.3.3 Information Search and Retrieval; H.3.4 Systems and Software; H.4 [Information Systems Applications]: H.4.2 Types of Systems; H.4.m Miscellaneous General Terms Algorithms, Measurement, Performance, Experimentation 1.",
                "INTRODUCTION An organizations intranet provides a means for exchanging information between employees and for facilitating employee collaborations.",
                "To efficiently and effectively achieve this, it is necessary to provide search facilities that enable employees not only to access documents, but also to identify expert colleagues.",
                "At the TREC Enterprise Track [22] the need to study and understand expertise retrieval has been recognized through the introduction of Expert Finding tasks.",
                "The goal of expert finding is to identify a list of people who are knowledgeable about a given topic.",
                "This task is usually addressed by uncovering associations between people and topics [10]; commonly, a co-occurrence of the name of a person with topics in the same context is assumed to be evidence of expertise.",
                "An alternative task, which using the same idea of people-topic associations, is expert profiling, where the task is to return a list of topics that a person is knowledgeable about [3].",
                "The launch of the Expert Finding task at TREC has generated a lot of interest in expertise retrieval, with rapid progress being made in terms of modeling, algorithms, and evaluation aspects.",
                "However, nearly all of the expert finding or profiling work performed has been validated experimentally using the W3C collection [24] from the Enterprise Track.",
                "While this collection is currently the only publicly available test collection for expertise retrieval tasks, it only represents one type of intranet.",
                "With only one test collection it is not possible to generalize conclusions to other realistic settings.",
                "In this paper we focus on expertise retrieval in a realistic setting that differs from the W3C setting-one in which relatively small amounts of clean, multilingual data are available, that cover a broad range of expertise areas, as can be found on the intranets of universities and other knowledge-intensive organizations.",
                "Typically, this setting features several additional types of structure: topical structure (e.g., topic hierarchies as employed by the organization), organizational structure (faculty, department, ...), as well as multiple types of documents (research and course descriptions, publications, and academic homepages).",
                "This setting is quite different from the W3C setting in ways that might impact upon the performance of expertise retrieval tasks.",
                "We focus on a number of research questions in this paper: Does the relatively small amount of data available on an intranet affect the quality of the topic-person associations that lie at the heart of expertise retrieval algorithms?",
                "How do state-of-the-art algorithms developed on the W3C data set perform in the alternative scenario of the type described above?",
                "More generally, do the lessons from the Expert Finding task at TREC carry over to this setting?",
                "How does the inclusion or exclusion of different documents affect expertise retrieval tasks?",
                "In addition to, how can the topical and organizational structure be used for retrieval purposes?",
                "To answer our research questions, we first present a set of baseline approaches, based on generative language modeling, aimed at finding associations between topics and people.",
                "This allows us to formulate the expert finding and expert profiling tasks in a uniform way, and has the added benefit of allowing us to understand the relations between the two tasks.",
                "For our experimental evaluation, we introduce a new data set (the UvT Expert Collection) which is representative of the type of intranet that we described above.",
                "Our collection is based on publicly available data, crawled from the website of Tilburg University (UvT).",
                "This type of data is particularly interesting, since (1) it is clean, heterogeneous, structured, and focused, but comprises a limited number of documents; (2) contains information on the organizational hierarchy; (3) it is bilingual (English and Dutch); and (4) the list of expertise areas of an individual are provided by the employees themselves.",
                "Using the UvT Expert collection, we conduct two sets of experiments.",
                "The first is aimed at determining the effectiveness of baseline expertise finding and profiling methods in this new setting.",
                "A second group of experiments is aimed at extensions of the baseline methods that exploit characteristic features of the UvT Expert Collection; specifically, we propose and evaluate refined expert finding and profiling methods that incorporate topicality and organizational structure.",
                "Apart from the research questions and data set that we contribute, our main contributions are as follows.",
                "The baseline models developed for expertise finding perform well on the new data set.",
                "While on the W3C setting the expert finding task appears to be more difficult than profiling, for the UvT data the opposite is the case.",
                "We find that profiling on the UvT data set is considerably more difficult than on the W3C set, which we believe is due to the large (but realistic) number of topical areas that we used for profiling: about 1,500 for the UvT set, versus 50 in the W3C case.",
                "Taking the similarity between topics into account can significantly improve retrieval performance.",
                "The best performing similarity measures are content-based, therefore they can be applied on the W3C (and other) settings as well.",
                "Finally, we demonstrate that the organizational structure can be exploited in the form of a context model, improving MAP scores for certain models by up to 70%.",
                "The remainder of this paper is organized as follows.",
                "In the next section we review related work.",
                "Then, in Section 3 we provide detailed descriptions of the expertise retrieval tasks that we address in this paper: expert finding and expert profiling.",
                "In Section 4 we present our baseline models, of which the performance is then assessed in Section 6 using the UvT data set that we introduce in Section 5.",
                "Advanced models exploiting specific features of our data are presented in Section 7 and evaluated in Section 8.",
                "We formulate our conclusions in Section 9. 2.",
                "RELATED WORK Initial approaches to expertise finding often employed databases containing information on the skills and knowledge of each individual in the organization [11].",
                "Most of these tools (usually called yellow pages or people-finding systems) rely on people to self-assess their skills against a predefined set of keywords.",
                "For updating profiles in these systems in an automatic fashion there is a need for intelligent technologies [5].",
                "More recent approaches use specific document sets (such as email [6] or software [18]) to find expertise.",
                "In contrast with focusing on particular document types, there is also an increased interest in the development of systems that index and mine published intranet documents as sources of evidence for expertise.",
                "One such published approach is the P@noptic system [9], which builds a representation of each person by concatenating all documents associated with that person-this is similar to Model 1 of Balog et al. [4], who formalize and compare two methods.",
                "Balog et al.s Model 1 directly models the knowledge of an expert from associated documents, while their Model 2 first locates documents on the topic and then finds the associated experts.",
                "In the reported experiments the second method performs significantly better when there are sufficiently many associated documents per candidate.",
                "Most systems that took part in the 2005 and 2006 editions of the Expert Finding task at TREC implemented (variations on) one of these two models; see [10, 20].",
                "Macdonald and Ounis [16] propose a different approach for ranking candidate expertise with respect to a topic based on data fusion techniques, without using collectionspecific heuristics; they find that applying field-based weighting models improves the ranking of candidates.",
                "Petkova and Croft [19] propose yet another approach, based on a combination of the above Model 1 and 2, explicitly modeling topics.",
                "Turning to other expert retrieval tasks that can also be addressed using topic-people associations, Balog and de Rijke [3] addressed the task of determining topical expert profiles.",
                "While their methods proved to be efficient on the W3C corpus, they require an amount of data that may not be available in the typical knowledge-intensive organization.",
                "Balog and de Rijke [2] study the related task of finding experts that are similar to a small set of experts given as input.",
                "As an aside, creating a textual summary of a person shows some similarities to biography finding, which has received a considerable amount of attention recently; see e.g., [13].",
                "We use generative language modeling to find associations between topics and people.",
                "In our modeling of expert finding and profiling we collect evidence for expertise from multiple sources, in a heterogeneous collection, and integrate it with the co-occurrence of candidates names and query terms-the language modeling setting allows us to do this in a transparent manner.",
                "Our modeling proceeds in two steps.",
                "In the first step, we consider three baseline models, two taken from [4] (the Models 1 and 2 mentioned above), and one a refined version of a model introduced in [3] (which we refer to as Model 3 below); this third model is also similar to the model described by Petkova and Croft [19].",
                "The models we consider in our second round of experiments are mixture models similar to contextual language models [1] and to the expanded documents of Tao et al. [21]; however, the features that we use for definining our expansions-including topical structure and organizational structure-have not been used in this way before. 3.",
                "TASKS In the expertise retrieval scenario that we envisage, users seeking expertise within an organization have access to an interface that combines a search box (where they can search for experts or topics) with navigational structures (of experts and of topics) that allows them to click their way to an expert page (providing the profile of a person) or a topic page (providing a list of experts on the topic).",
                "To feed the above interface, we face two expertise retrieval tasks, expert finding and expert profiling, that we first define and then formalize using generative language models.",
                "In order to model either task, the probability of the query topic being associated to a candidate expert plays a key role in the final estimates for searching and profiling.",
                "By using language models, both the candidates and the query are characterized by distributions of terms in the vocabulary (used in the documents made available by the organization whose expertise retrieval needs we are addressing). 3.1 Expert finding Expert finding involves the task of finding the right person with the appropriate skills and knowledge: Who are the experts on topic X?.",
                "E.g., an employee wants to ascertain who worked on a particular project to find out why particular decisions were made without having to trawl through documentation (if there is any).",
                "Or, they may be in need a trained specialist for consultancy on a specific problem.",
                "Within an organization there are usually many possible candidates who could be experts for given topic.",
                "We can state this problem as follows: What is the probability of a candidate ca being an expert given the query topic q?",
                "That is, we determine p(ca|q), and rank candidates ca according to this probability.",
                "The candidates with the highest probability given the query are deemed the most likely experts for that topic.",
                "The challenge is how to estimate this probability accurately.",
                "Since the query is likely to consist of only a few terms to describe the expertise required, we should be able to obtain a more accurate estimate by invoking Bayes Theorem, and estimating: p(ca|q) = p(q|ca)p(ca) p(q) , (1) where p(ca) is the probability of a candidate and p(q) is the probability of a query.",
                "Since p(q) is a constant, it can be ignored for ranking purposes.",
                "Thus, the probability of a candidate ca being an expert given the query q is proportional to the probability of a query given the candidate p(q|ca), weighted by the a priori belief p(ca) that candidate ca is an expert. p(ca|q) ∝ p(q|ca)p(ca) (2) In this paper our main focus is on estimating the probability of a query given the candidate p(q|ca), because this probability captures the extent to which the candidate knows about the query topic.",
                "Whereas the candidate priors are generally assumed to be uniformand thus will not influence the ranking-it has been demonstrated that a sensible choice of priors may improve the performance [20]. 3.2 Expert profiling While the task of expert searching was concerned with finding experts given a particular topic, the task of expert profiling seeks to answer a related question: What topics does a candidate know about?",
                "Essentially, this turns the questions of expert finding around.",
                "The profiling of an individual candidate involves the identification of areas of skills and knowledge that they have expertise about and an evaluation of the level of proficiency in each of these areas.",
                "This is the candidates topical profile.",
                "Generally, topical profiles within organizations consist of tabular structures which explicitly catalogue the skills and knowledge of each individual in the organization.",
                "However, such practice is limited by the resources available for defining, creating, maintaining, and updating these profiles over time.",
                "By focusing on automatic methods which draw upon the available evidence within the document repositories of an organization, our aim is to reduce the human effort associated with the maintenance of topical profiles1 .",
                "A topical profile of a candidate, then, is defined as a vector where each element i of the vector corresponds to the candidate cas expertise on a given topic ki, (i.e., s(ca, ki)).",
                "Each topic ki defines a particular knowledge area or skill that the organization uses to define the candidates topical profile.",
                "Thus, it is assumed that a list of topics, {k1, . . . , kn}, where n is the number of pre-defined topics, is given: profile(ca) = s(ca, k1), s(ca, k2), . . . , s(ca, kn) . (3) 1 Context and evidence are needed to help users of expertise finding systems to decide whom to contact when seeking expertise in a particular area.",
                "Examples of such context are: Who does she work with?",
                "What are her contact details?",
                "Is she well-connected, just in case she is not able to help us herself?",
                "What is her role in the organization?",
                "Who is her superior?",
                "Collaborators, and affiliations, etc. are all part of the candidates social profile, and can serve as a background against which the systems recommendations should be interpreted.",
                "In this paper we only address the problem of determining topical profiles, and leave social profiling to further work.",
                "We state the problem of quantifying the competence of a person on a certain knowledge area as follows: What is the probability of a knowledge area (ki) being part of the candidates (expertise) profile? where s(ca, ki) is defined by p(ki|ca).",
                "Our task, then, is to estimate p(ki|ca), which is equivalent to the problem of obtaining p(q|ca), where the topic ki is represented as a query topic q, i.e., a sequence of keywords representing the expertise required.",
                "Both the expert finding and profiling tasks rely on the accurate estimation of p(q|ca).",
                "The only difference derives from the prior probability that a person is an expert (p(ca)), which can be incorporated into the expert finding task.",
                "This prior does not apply to the profiling task since the candidate (individual) is fixed. 4.",
                "BASELINE MODELS In this section we describe our baseline models for estimating p(q|ca), i.e., associations between topics and people.",
                "Both expert finding and expert profiling boil down to this estimation.",
                "We employ three models for calculating this probability. 4.1 From topics to candidates Using Candidate Models: Model 1 Model 1 [4] defines the probability of a query given a candidate (p(q|ca)) using standard language modeling techniques, based on a multinomial unigram language model.",
                "For each candidate ca, a candidate language model θca is inferred such that the probability of a term given θca is nonzero for all terms, i.e., p(t|θca) > 0.",
                "From the candidate model the query is generated with the following probability: p(q|θca) = Y t∈q p(t|θca)n(t,q) , where each term t in the query q is sampled identically and independently, and n(t, q) is the number of times t occurs in q.",
                "The candidate language model is inferred as follows: (1) an empirical model p(t|ca) is computed; (2) it is smoothed with background probabilities.",
                "Using the associations between a candidate and a document, the probability p(t|ca) can be approximated by: p(t|ca) = X d p(t|d)p(d|ca), where p(d|ca) is the probability that candidate ca generates a supporting document d, and p(t|d) is the probability of a term t occurring in the document d. We use the maximum-likelihood estimate of a term, that is, the normalised frequency of the term t in document d. The strength of the association between document d and candidate ca expressed by p(d|ca) reflects the degree to which the candidates expertise is described using this document.",
                "The estimation of this probability is presented later, in Section 4.2.",
                "The candidate model is then constructed as a linear interpolation of p(t|ca) and the background model p(t) to ensure there are no zero probabilities, which results in the final estimation: p(q|θca) = (4) Y t∈q ( (1 − λ) X d p(t|d)p(d|ca) ! + λp(t) )n(t,q) .",
                "Model 1 amasses all the term information from all the documents associated with the candidate, and uses this to represent that candidate.",
                "This model is used to predict how likely a candidate would produce a query q.",
                "This can can be intuitively interpreted as the probability of this candidate talking about the query topic, where we assume that this is indicative of their expertise.",
                "Using Document Models: Model 2 Model 2 [4] takes a different approach.",
                "Here, the process is broken into two parts.",
                "Given a candidate ca, (1) a document that is associated with a candidate is selected with probability p(d|ca), and (2) from this document a query q is generated with probability p(q|d).",
                "Then the sum over all documents is taken to obtain p(q|ca), such that: p(q|ca) = X d p(q|d)p(d|ca). (5) The probability of a query given a document is estimated by inferring a document language model θd for each document d in a similar manner as the candidate model was inferred: p(t|θd) = (1 − λ)p(t|d) + λp(t), (6) where p(t|d) is the probability of the term in the document.",
                "The probability of a query given the document model is: p(q|θd) = Y t∈q p(t|θd)n(t,q) .",
                "The final estimate of p(q|ca) is obtained by substituting p(q|d) for p(q|θd) into Eq. 5 (see [4] for full details).",
                "Conceptually, Model 2 differs from Model 1 because the candidate is not directly modeled.",
                "Instead, the document acts like a hidden variable in the process which separates the query from the candidate.",
                "This process is akin to how a user may search for candidates with a standard search engine: initially by finding the documents which are relevant, and then seeing who is associated with that document.",
                "By examining a number of documents the user can obtain an idea of which candidates are more likely to discuss the topic q.",
                "Using Topic Models: Model 3 We introduce a third model, Model 3.",
                "Instead of attempting to model the query generation process via candidate or document models, we represent the query as a topic language model and directly estimate the probability of the candidate p(ca|q).",
                "This approach is similar to the model presented in [3, 19].",
                "As with the previous models, a language model is inferred, but this time for the query.",
                "We adapt the work of Lavrenko and Croft [14] to estimate a topic model from the query.",
                "The procedure is as follows.",
                "Given a collection of documents and a query topic q, it is assumed that there exists an unknown topic model θk that assigns probabilities p(t|θk) to the term occurrences in the topic documents.",
                "Both the query and the documents are samples from θk (as opposed to the previous approaches, where a query is assumed to be sampled from a specific document or candidate model).",
                "The main task is to estimate p(t|θk), the probability of a term given the topic model.",
                "Since the query q is very sparse, and as there are no examples of documents on the topic, this distribution needs to be approximated.",
                "Lavrenko and Croft [14] suggest a reasonable way of obtaining such an approximation, by assuming that p(t|θk) can be approximated by the probability of term t given the query q.",
                "We can then estimate p(t|q) using the joint probability of observing the term t together with the query terms, q1, . . . , qm, and dividing by the joint probability of the query terms: p(t|θk) ≈ p(t|q) = p(t, q1, . . . , qm) p(q1, . . . , qm) = p(t, q1, . . . , qm) P t ∈T p(t , q1, . . . , qm) , where p(q1, . . . , qm) = P t ∈T p(t , q1, . . . , qm), and T is the entire vocabulary of terms.",
                "In order to estimate the joint probability p(t, q1, . . . , qm), we follow [14, 15] and assume t and q1, . . . , qm are mutually independent, once we pick a source distribution from the set of underlying source distributions U.",
                "If we choose U to be a set of document models. then to construct this set, the query q would be issued against the collection, and the top n returned are assumed to be relevant to the topic, and thus treated as samples from the topic model. (Note that candidate models could be used instead.)",
                "With the document models forming U, the joint probability of term and query becomes: p(t, q1, . . . , qm) = X d∈U p(d) ˘ p(t|θd) mY i=1 p(qi|θd) ¯ . (7) Here, p(d) denotes the prior distribution over the set U, which reflects the relevance of the document to the topic.",
                "We assume that p(d) is uniform across U.",
                "In order to rank candidates according to the topic model defined, we use the Kullback-Leibler divergence metric (KL, [8]) to measure the difference between the candidate models and the topic model: KL(θk||θca) = X t p(t|θk) log p(t|θk) p(t|θca) . (8) Candidates with a smaller divergence from the topic model are considered to be more likely experts on that topic.",
                "The candidate model θca is defined in Eq. 4.",
                "By using KL divergence instead of the probability of a candidate given the topic model p(ca|θk), we avoid normalization problems. 4.2 Document-candidate associations For our models we need to be able to estimate the probability p(d|ca), which expresses the extent to which a document d characterizes the candidate ca.",
                "In [4], two methods are presented for estimating this probability, based on the number of person names recognized in a document.",
                "However, in our (intranet) setting it is reasonable to assume that authors of documents can unambiguously be identified (e.g., as the author of an article, the teacher assigned to a course, the owner of a web page, etc.)",
                "Hence, we set p(d|ca) to be 1 if candidate ca is author of document d, otherwise the probability is 0.",
                "In Section 6 we describe how authorship can be determined on different types of documents within the collection. 5.",
                "THE UVT EXPERT COLLECTION The UvT Expert collection used in the experiments in this paper fits the scenario outlined in Section 3.",
                "The collection is based on the Webwijs (Webwise) system developed at Tilburg University (UvT) in the Netherlands.",
                "Webwijs (http://www.uvt.nl/ webwijs/) is a publicly accessible database of UvT employees who are involved in research or teaching; currently, Webwijs contains information about 1168 experts, each of whom has a page with contact information and, if made available by the expert, a research description and publications list.",
                "In addition, each expert can select expertise areas from a list of 1491 topics and is encouraged to suggest new topics that need to be approved by the Webwijs editor.",
                "Each topic has a separate page that shows all experts associated with that topic and, if available, a list of related topics.",
                "Webwijs is available in Dutch and English, and this bilinguality has been preserved in the collection.",
                "Every Dutch Webwijs page has an English translation.",
                "Not all Dutch topics have an English translation, but the reverse is true: the 981 English topics all have a Dutch equivalent.",
                "About 42% of the experts teach courses at Tilburg University; these courses were also crawled and included in the profile.",
                "In addition, about 27% of the experts link to their academic homepage from their Webwijs page.",
                "These home pages were crawled and added to the collection. (This means that if experts put the full-text versions of their publications on their academic homepage, these were also available for indexing.)",
                "We also obtained 1880 full-text versions of publications from the UvT institutional repository and Dutch English no. of experts 1168 1168 no. of experts with ≥ 1 topic 743 727 no. of topics 1491 981 no. of expert-topic pairs 4318 3251 avg. no. of topics/expert 5.8 5.9 max. no. of topics/expert (no. of experts) 60 (1) 35 (1) min. no. of topics/expert (no. of experts) 1 (74) 1 (106) avg. no. of experts/topic 2.9 3.3 max. no. of experts/topic (no. of topics) 30 (1) 30 (1) min. no. of experts/topic (no. of topics) 1 (615) 1 (346) no. of experts with HP 318 318 no. of experts with CD 318 318 avg. no. of CDs per teaching expert 3.5 3.5 no. of experts with RD 329 313 no. of experts with PUB 734 734 avg. no. of PUBs per expert 27.0 27.0 avg. no. of PUB citations per expert 25.2 25.2 avg. no. of full-text PUBs per expert 1.8 1.8 Table 2: Descriptive statistics of the Dutch and English versions of the UvT Expert collection. converted them to plain text.",
                "We ran the TextCat [23] language identifier to classify the language of the home pages and the fulltext publications.",
                "We restricted ourselves to pages where the classifier was confident about the language used on the page.",
                "This resulted in four document types: research descriptions (RD), course descriptions (CD), publications (PUB; full-text and citationonly versions), and academic homepages (HP).",
                "Everything was bundled into the UvT Expert collection which is available at http: //ilk.uvt.nl/uvt-expert-collection/.",
                "The UvT Expert collection was extracted from a different organizational setting than the W3C collection and differs from it in a number of ways.",
                "The UvT setting is one with relatively small amounts of multilingual data.",
                "Document-author associations are clear and the data is structured and clean.",
                "The collection covers a broad range of expertise areas, as one can typically find on intranets of universities and other knowledge-intensive institutes.",
                "Additionally, our university setting features several types of structure (topical and organizational), as well as multiple document types.",
                "Another important difference between the two data sets is that the expertise areas in the UvT Expert collection are self-selected instead of being based on group membership or assignments by others.",
                "Size is another dimension along which the W3C and UvT Expert collections differ: the latter is the smaller of the two.",
                "Also realistic are the large differences in the amount of information available for each expert.",
                "Utilizing Webwijs is voluntary; 425 Dutch experts did not select any topics at all.",
                "This leaves us with 743 Dutch and 727 English usable expert profiles.",
                "Table 2 provides descriptive statistics for the UvT Expert collection.",
                "Universities tend to have a hierarchical structure that goes from the faculty level, to departments, research groups, down to the individual researchers.",
                "In the UvT Expert collection we have information about the affiliations of researchers with faculties and institutes, providing us with a two-level organizational hierarchy.",
                "Tilburg University has 22 organizational units at the faculty level (including the university office and several research institutes) and 71 departments, which amounts to 3.2 departments per faculty.",
                "As to the topical hierarchy used by Webwijs, 131 of the 1491 topics are top nodes in the hierarchy.",
                "This hierarchy has an average topic chain length of 2.65 and a maximum length of 7 topics. 6.",
                "EVALUATION Below, we evaluate Section 4s models for expert finding and profiling onthe UvT Expert collection.",
                "We detail our research questions and experimental setup, and then present our results. 6.1 Research Questions We address the following research questions.",
                "Both expert finding and profiling rely on the estimations of p(q|ca).",
                "The question is how the models compare on the different tasks, and in the setting of the UvT Expert collection.",
                "In [4], Model 2 outperformed Model 1 on the W3C collection.",
                "How do they compare on our data set?",
                "And how does Model 3 compare to Model 1?",
                "What about performance differences between the two languages in our test collection? 6.2 Experimental Setup The output of our models was evaluated against the self-assigned topic labels, which were treated as relevance judgements.",
                "Results were evaluated separately for English and Dutch.",
                "For English we only used topics for which the Dutch translation was available; for Dutch all topics were considered.",
                "The results were averaged for the queries in the intersection of relevance judgements and results; missing queries do not contribute a value of 0 to the scores.",
                "We use standard information retrieval measures, such as Mean Average Precision (MAP) and Mean Reciprocal Rank (MRR).",
                "We also report the percentage of topics (%q) and candidates (%ca) covered, for the expert finding and profiling tasks, respectively. 6.3 Results Table 1 shows the performance of Model 1, 2, and 3 on the expert finding and profiling tasks.",
                "The rows of the table correspond to the various document types (RD, CD, PUB, and HP) and to their combinations.",
                "RD+CD+PUB+HP is equivalent to the full collection and will be referred as the BASELINE of our experiments.",
                "Looking at Table 1 we see that Model 2 performs the best across the board.",
                "However, when the data is clean and very focused (RD), Model 3 outperforms it in a number of cases.",
                "Model 1 has the best coverage of candidates (%ca) and topics (%q).",
                "The various document types differ in their characteristics and how they improve the finding and profiling tasks.",
                "Expert profiling benefits much from the clean data present in the RD and CD document types, while the publications contribute the most to the expert finding task.",
                "Adding the homepages does not prove to be particularly useful.",
                "When we compare the results across languages, we find that the coverage of English topics (%q) is higher than of the Dutch ones for expert finding.",
                "Apart from that, the scores fall in the same range for both languages.",
                "For the profiling task the coverage of the candidates (%ca) is very similar for both languages.",
                "However, the performance is substantially better for the English topics.",
                "While it is hard to compare scores across collections, we conclude with a brief comparison of the absolute scores in Table 1 to those reported in [3, 4] on the W3C test set (2005 edition).",
                "For expert finding the MAP scores for Model 2 reported here are about 50% higher than the corresponding figures in [4], while our MRR scores are slightly below those in [4].",
                "For expert profiling, the differences are far more dramatic: the MAP scores for Model 2 reported here are around 50% below the scores in [3], while the (best) MRR scores are about the same as those in [3].",
                "The cause for the latter differences seems to reside in the number of knowledge areas considered here-approx. 30 times more than in the W3C setting. 7.",
                "ADVANCED MODELS Now that we have developed and assessed basic language modeling techniques for expertise retrieval, we turn to refined models that exploit special features of our test collection. 7.1 Exploiting knowledge area similarity One way to improve the scoring of a query given a candidate is to consider what other requests the candidate would satisfy and use them as further evidence to support the original query, proportional Expert finding Expert profiling Document types Model 1 Model 2 Model 3 Model 1 Model 2 Model 3 %q MAP MRR %q MAP MRR %q MAP MRR %ca MAP MRR %ca MAP MRR %ca MAP MRR English RD 97.8 0.126 0.269 83.5 0.144 0.311 83.3 0.129 0.271 100 0.089 0.189 39.3 0.232 0.465 41.1 0.166 0.337 CD 97.8 0.118 0.227 91.7 0.123 0.248 91.7 0.118 0.226 32.8 0.188 0.381 32.4 0.195 0.385 32.7 0.203 0.370 PUB 97.8 0.200 0.330 98.0 0.216 0.372 98.0 0.145 0.257 78.9 0.167 0.364 74.5 0.212 0.442 78.9 0.135 0.299 HP 97.8 0.081 0.186 97.4 0.071 0.168 97.2 0.062 0.149 31.2 0.150 0.299 28.8 0.185 0.335 30.1 0.136 0.287 RD+CD 97.8 0.188 0.352 92.9 0.193 0.360 92.9 0.150 0.273 100 0.145 0.286 61.3 0.251 0.477 63.2 0.217 0.416 RD+CD+PUB 97.8 0.235 0.373 98.1 0.277 0.439 98.1 0.178 0.305 100 0.196 0.380 87.2 0.280 0.533 89.5 0.170 0.344 RD+CD+PUB+HP 97.8 0.237 0.372 98.6 0.280 0.441 98.5 0.166 0.293 100 0.199 0.387 88.7 0.281 0.525 90.9 0.169 0.329 Dutch RD 61.3 0.094 0.229 38.4 0.137 0.336 38.3 0.127 0.295 38.0 0.127 0.386 34.1 0.138 0.420 38.0 0.105 0.327 CD 61.3 0.107 0.212 49.7 0.128 0.256 49.7 0.136 0.261 32.5 0.151 0.389 31.8 0.158 0.396 32.5 0.170 0.380 PUB 61.3 0.193 0.319 59.5 0.218 0.368 59.4 0.173 0.291 78.8 0.126 0.364 76.0 0.150 0.424 78.8 0.103 0.294 HP 61.3 0.063 0.169 56.6 0.064 0.175 56.4 0.062 0.163 29.8 0.108 0.308 27.8 0.125 0.338 29.8 0.098 0.255 RD+CD 61.3 0.159 0.314 51.9 0.184 0.360 51.9 0.169 0.324 60.5 0.151 0.410 57.2 0.166 0.431 60.4 0.159 0.384 RD+CD+PUB 61.3 0.244 0.398 61.5 0.260 0.424 61.4 0.210 0.350 90.3 0.165 0.445 88.2 0.189 0.479 90.3 0.126 0.339 RD+CD+PUB+HP 61.3 0.249 0.401 62.6 0.265 0.436 62.6 0.195 0.344 91.9 0.164 0.426 90.1 0.195 0.488 91.9 0.125 0.328 Table 1: Performance of the models on the expert finding and profiling tasks, using different document types and their combinations. %q is the number of topics covered (applies to the expert finding task), %ca is the number of candidates covered (applies to the expert profiling task).",
                "The top and bottom blocks correspond to English and Dutch respectively.",
                "The best scores are in boldface. to how related the other requests are to the original query.",
                "This can be modeled by interpolating between the p(q|ca) and the further supporting evidence from all similar requests q , as follows: p (q|ca) = λp(q|ca) + (1 − λ) X q p(q|q )p(q |ca), (9) where p(q|q ) represents the similarity between the two topics q and q .",
                "To be able to work with similarity methods that are not necessarily probabilities, we set p(q|q ) = w(q,q ) γ , where γ is a normalizing constant, such that γ = P q w(q , q ).",
                "We consider four methods for calculating the similarity score between two topics.",
                "Three approaches are strictly content-based, and establish similarity by examining co-occurrence patterns of topics within the collection, while the last approach exploits the hierarchical structure of topical areas that may be present within an organization (see [7] for further examples of integrating word relationships into language models).",
                "The Kullback-Leibler (KL) divergence metric defined in Eq. 8 provides a measure of how different or similar two probability distributions are.",
                "A topic model is inferred for q and q using the method presented in Section 4.1 to describe the query across the entire vocabulary.",
                "Since a lower KL score means the queries are more similar, we let w(q, q ) = max(KL(θq||·) − KL(θq||θq )).",
                "Pointwise Mutual Information (PMI, [17]) is a measure of association used in information theory to determine the extent of independence between variables.",
                "The dependence between two queries is reflected by the SI(q, q ) score, where scores greater than zero indicate that it is likely that there is a dependence, which we take to mean that the queries are likely to be similar: SI(q, q ) = log p(q, q ) p(q)p(q ) (10) We estimate the probability of a topic p(q) using the number of documents relevant to query q within the collection.",
                "The joint probability p(q, q ) is estimated similarly, by using the concatenation of q and q as a query.",
                "To obtain p(q|q ), we then set w(q, q ) = SI(q, q ) when SI(q, q ) > 0 otherwise w(q, q ) = 0, because we are only interested in including queries that are similar.",
                "The log-likelihood statistic provides another measure of dependence, which is more reliable than the pointwise mutual information measure [17].",
                "Let k1 be the number of co-occurrences of q and q , k2 the number of occurrences of q not co-occurring with q , n1 the total number of occurrences of q , and n2 the total number of topic tokens minus the number of occurrences of q .",
                "Then, let p1 = k1/n1, p2 = k2/n2, and p = (k1 + k2)/(n1 + n2), (q, q ) = 2( (p1, k1, n1) + (p2, k2, n2) − (p, k1, n1) − (p, k2, n2)), where (p, n, k) = k log p + (n − k) log(1 − p).",
                "The higher score indicate that queries are also likely to be similar, thus we set w(q, q ) = (q, q ).",
                "Finally, we also estimate the similarity of two topics based on their distance within the topic hierarchy.",
                "The topic hierarchy is viewed as a directed graph, and for all topic-pairs the shortest path SP(q, q ) is calculated.",
                "We set the similarity score to be the reciprocal of the shortest path: w(q, q ) = 1/SP(q, q ). 7.2 Contextual information Given the hierarchy of an organization, the units to which a person belong are regarded as a context so as to compensate for data sparseness.",
                "We model it as follows: p (q|ca) = 1 − P ou∈OU(ca) λou · p(q|ca) + P ou∈OU(ca) λou · p(q|ou), where OU(ca) is the set of organizational units of which candidate ca is a member of, and p(q|o) expresses the strength of the association between query q and the unit ou.",
                "The latter probability can be estimated using either of the three basic models, by simply replacing ca with ou in the corresponding equations.",
                "An organizational unit is associated with all the documents that its members have authored.",
                "That is, p(d|ou) = maxca∈ou p(d|ca). 7.3 A simple multilingual model For knowledge institutes in Europe, academic or otherwise, a multilingual (or at least bilingual) setting is typical.",
                "The following model builds on a kind of independence assumption: there is no spill-over of expertise/profiles across language boundaries.",
                "While a simplification, this is a sensible first approach.",
                "That is: p (q|ca) =P l∈L λl · p(ql|ca), where L is the set of languages used in the collection, ql is the translation of the query q to language l, and λl is a language specific smoothing parameter, such that P l∈L λl = 1. 8.",
                "ADVANCED MODELS: EVALUATION In this section we present an experimental evaluation of our advanced models.",
                "Expert finding Expert profiling Language Model 1 Model 2 Model 3 Model 1 Model 2 Model 3 %q MAP MRR %q MAP MRR %q MAP MRR %ca MAP MRR %ca MAP MRR %ca MAP MRR English only 97.8 0.237 0.372 98.6 0.280 0.441 98.5 0.166 0.293 100 0.199 0.387 88.7 0.281 0.525 90.9 0.169 0.329 Dutch only 61.3 0.249 0.401 62.6 0.265 0.436 62.6 0.195 0.344 91.9 0.164 0.426 90.1 0.195 0.488 91.9 0.125 0.328 Combination 99.4 0.297 0.444 99.7 0.324 0.491 99.7 0.223 0.388 100 0.241 0.445 92.1 0.313 0.564 93.2 0.224 0.411 Table 3: Performance of the combination of languages on the expert finding and profiling tasks (on candidates).",
                "Best scores for each model are in italic, absolute best scores for the expert finding and profiling tasks are in boldface.",
                "Method Model 1 Model 2 Model 3 MAP MRR MAP MRR MAP MRR English BASELINE 0.296 0.454 0.339 0.509 0.221 0.333 KLDIV 0.291 0.453 0.327 0.503 0.219 0.330 PMI 0.291 0.453 0.337 0.509 0.219 0.331 LL 0.319 0.490 0.360 0.524 0.233 0.368 HDIST 0.299 0.465 0.346 0.537 0.219 0.332 Dutch BASELINE 0.240 0.350 0.271 0.403 0.227 0.389 KLDIV 0.239 0.347 0.253 0.386 0.224 0.385 PMI 0.239 0.350 0.260 0.392 0.227 0.389 LL 0.255 0.372 0.281 0.425 0.231 0.389 HDIST 0.253 0.365 0.271 0.407 0.236 0.402 Method Model 1 Model 2 Model 3 MAP MRR MAP MRR MAP MRR English BASELINE 0.485 0.546 0.499 0.548 0.381 0.416 KLDIV 0.510 0.564 0.513 0.558 0.381 0.416 PMI 0.486 0.546 0.495 0.542 0.407 0.451 LL 0.558 0.589 0.586 0.617 0.408 0.453 HDIST 0.507 0.567 0.512 0.563 0.386 0.420 Dutch BASELINE 0.263 0.313 0.294 0.358 0.262 0.315 KLDIV 0.284 0.336 0.271 0.321 0.261 0.314 PMI 0.265 0.317 0.265 0.316 0.273 0.330 LL 0.312 0.351 0.330 0.377 0.284 0.331 HDIST 0.280 0.327 0.288 0.341 0.266 0.321 Table 4: Performance on the expert finding (top) and profiling (bottom) tasks, using knowledge area similarities.",
                "Runs were evaluated on the main topics set.",
                "Best scores are in boldface. 8.1 Research Questions Our questions follow the refinements presented in the preceding section: Does exploiting the knowledge area similarity improve effectiveness?",
                "Which of the various methods for capturing word relationships is most effective?",
                "Furthermore, is our way of bringing in contextual information useful?",
                "For which tasks?",
                "And finally, is our simple way of combining the monolingual scores sufficient for obtaining significant improvements? 8.2 Experimental setup Given that the self-assessments are also sparse in our collection, in order to be able to measure differences between the various models, we selected a subset of topics, and evaluated (some of the) runs only on this subset.",
                "This set is referred as main topics, and consists of topics that are located at the top level of the topical hierarchy. (A main topic has subtopics, but is not a subtopic of any other topic.)",
                "This main set consists of 132 Dutch and 119 English topics.",
                "The relevance judgements were restricted to the main topic set, but were not expanded with subtopics. 8.3 Exploiting knowledge area similarity Table 4 presents the results.",
                "The four methods used for estimating knowledge-area similarity are KL divergence (KLDIV), PointLang.",
                "Topics Model 1 Model 2 Model 3 MAP MRR MAP MRR MAP MRR Expert finding UK ALL 0.423 0.545 0.654 0.799 0.494 0.629 UK MAIN 0.500 0.621 0.704 0.834 0.587 0.699 NL ALL 0.439 0.560 0.672 0.826 0.480 0.630 NL MAIN 0.440 0.584 0.645 0.816 0.515 0.655 Expert profiling UK ALL 0.240 0.640 0.306 0.778 0.223 0.616 UK MAIN 0.523 0.677 0.519 0.648 0.461 0.587 NL ALL 0.203 0.716 0.254 0.770 0.183 0.627 NL MAIN 0.332 0.576 0.380 0.624 0.332 0.549 Table 5: Evaluating the context models on organizational units. wise mutual information (PMI), log-likelihood (LL), and distance within topic hierarchy (HDIST).",
                "We managed to improve upon the baseline in all cases, but the improvement is more noticeable for the profiling task.",
                "For both tasks, the LL method performed best.",
                "The content-based approaches performed consistently better than HDIST. 8.4 Contextual information A two level hierarchy of organizational units (faculties and institutes) is available in the UvT Expert collection.",
                "The unit a person belongs to is used as a context for that person.",
                "First, we evaluated the models of the organizational units, using all topics (ALL) and only the main topics (MAIN).",
                "An organizational unit is considered to be relevant for a given topic (or vice versa) if at least one member of the unit selected the given topic as an expertise area.",
                "Table 5 reports on the results.",
                "As far as expert finding goes, given a topic, the corresponding organizational unit can be identified with high precision.",
                "However, the expert profiling task shows a different picture: the scores are low, and the task seems hard.",
                "The explanation may be that general concepts (i.e., our main topics) may belong to several organizational units.",
                "Second, we performed another evaluation, where we combined the contextual models with the candidate models (to score candidates again).",
                "Table 6 reports on the results.",
                "We find a positive impact of the context models only for expert finding.",
                "Noticably, for expert finding (and Model 1), it improves over 50% (for English) and over 70% (for Dutch) on MAP.",
                "The poor performance on expert profiling may be due to the fact that context models alone did not perform very well on the profiling task to begin with. 8.5 Multilingual models In this subsection we evaluate the method for combining results across multiple languages that we described in Section 7.3.",
                "In our setting the set of languages consists of English and Dutch: L = {UK, NL}.",
                "The weights on these languages were set to be identical (λUK = λNL = 0.5).",
                "We performed experiments with various λ settings, but did not observe significant differences in performance.",
                "Table 3 reports on the multilingual results, where performance is evaluated on the full topic set.",
                "All three models significantly imLang.",
                "Method Model 1 Model 2 Model 3 MAP MRR MAP MRR MAP MRR Expert finding UK BL 0.296 0.454 0.339 0.509 0.221 0.333 UK CT 0.330 0.491 0.342 0.500 0.228 0.342 NL BL 0.240 0.350 0.271 0.403 0.227 0.389 NL CT 0.251 0.382 0.267 0.410 0.246 0.404 Expert profiling UK BL 0.485 0.546 0.499 0.548 0.381 0.416 UK CT 0.562 0.620 0.508 0.558 0.440 0.486 NL BL 0.263 0.313 0.294 0.358 0.262 0.315 NL CT 0.330 0.384 0.317 0.387 0.294 0.345 Table 6: Performance of the context models (CT) compared to the baseline (BL).",
                "Best scores are in boldface. proved over all measures for both tasks.",
                "The coverage of topics and candidates for the expert finding and profiling tasks, respectively, is close to 100% in all cases.",
                "The relative improvement of the precision scores ranges from 10% to 80%.",
                "These scores demonstrate that despite its simplicity, our method for combining results over multiple languages achieves substantial improvements over the baseline. 9.",
                "CONCLUSIONS In this paper we focused on expertise retrieval (expert finding and profiling) in a new setting of a typical knowledge-intensive organization in which the available data is of high quality, multilingual, and covering a broad range of expertise area.",
                "Typically, the amount of available data in such an organization (e.g., a university, a research institute, or a research lab) is limited when compared to the W3C collection that has mostly been used for the experimental evaluation of expertise retrieval so far.",
                "To examine expertise retrieval in this setting, we introduced (and released) the UvT Expert collection as a representative case of such knowledge intensive organizations.",
                "The new collection reflects the typical properties of knowledge-intensive institutes noted above and also includes several features which may are potentially useful for expertise retrieval, such as topical and organizational structure.",
                "We evaluated how current state-of-the-art models for expert finding and profiling performed in this new setting and then refined these models in order to try and exploit the different characteristics within the data environment (language, topicality, and organizational structure).",
                "We found that current models of expertise retrieval generalize well to this new environment; in addition we found that refining the models to account for the differences results in significant improvements, thus making up for problems caused by data sparseness issues.",
                "Future work includes setting up manual assessments of automatically generated profiles by the employees themselves, especially in cases where the employees have not provided a profile themselves. 10.",
                "ACKNOWLEDGMENTS Krisztian Balog was supported by the Netherlands Organisation for Scientific Research (NWO) under project number 220-80-001.",
                "Maarten de Rijke was also supported by NWO under project numbers 017.001.190, 220-80-001, 264-70-050, 354-20-005, 600.065.120, 612-13-001, 612.000.106, 612.066.302, 612.069.006, 640.001.501, 640.002.501, and by the E.U.",
                "IST programme of the 6th FP for RTD under project MultiMATCH contract IST-033104.",
                "The work of Toine Bogers and Antal van den Bosch was funded by the IOP-MMI-program of SenterNovem / The Dutch Ministry of Economic Affairs, as part of the `A Propos project. 11.",
                "REFERENCES [1] L. Azzopardi.",
                "Incorporating Context in the Language Modeling Framework for ad-hoc Information Retrieval.",
                "PhD thesis, University of Paisley, 2005. [2] K. Balog and M. de Rijke.",
                "Finding similar experts.",
                "In This volume, 2007. [3] K. Balog and M. de Rijke.",
                "Determining expert profiles (with an application to expert finding).",
                "In IJCAI 07: Proc. 20th Intern.",
                "Joint Conf. on Artificial Intelligence, pages 2657-2662, 2007. [4] K. Balog, L. Azzopardi, and M. de Rijke.",
                "Formal models for expert finding in enterprise corpora.",
                "In SIGIR 06: Proc. 29th annual intern.",
                "ACM SIGIR conf. on Research and development in information retrieval, pages 43-50, 2006. [5] I. Becerra-Fernandez.",
                "The role of artificial intelligence technologies in the implementation of people-finder knowledge management systems.",
                "In AAAI Workshop on Bringing Knowledge to Business Processes, March 2000. [6] C. S. Campbell, P. P. Maglio, A. Cozzi, and B. Dom.",
                "Expertise identification using email communications.",
                "In CIKM 03: Proc. twelfth intern. conf. on Information and knowledge management, pages 528531, 2003. [7] G. Cao, J.-Y.",
                "Nie, and J. Bai.",
                "Integrating word relationships into language models.",
                "In SIGIR 05: Proc. 28th annual intern.",
                "ACM SIGIR conf. on Research and development in information retrieval, pages 298-305, 2005. [8] T. M. Cover and J.",
                "A. Thomas.",
                "Elements of Information Theory.",
                "Wiley-Interscience, 1991. [9] N. Craswell, D. Hawking, A. M. Vercoustre, and P. Wilkins.",
                "P@noptic expert: Searching for experts not just for documents.",
                "In Ausweb, 2001. [10] N. Craswell, A. de Vries, and I. Soboroff.",
                "Overview of the TREC2005 Enterprise Track.",
                "In The Fourteenth Text REtrieval Conf.",
                "Proc. (TREC 2005), 2006. [11] T. H. Davenport and L. Prusak.",
                "Working Knowledge: How Organizations Manage What They Know.",
                "Harvard Business School Press, Boston, MA, 1998. [12] T. Dunning.",
                "Accurate methods for the statistics of surprise and coincidence.",
                "Computational Linguistics, 19(1):61-74, 1993. [13] E. Filatova and J. Prager.",
                "Tell me what you do and Ill tell you what you are: Learning occupation-related activities for biographies.",
                "In HLT/EMNLP, 2005. [14] V. Lavrenko and W. B. Croft.",
                "Relevance based language models.",
                "In SIGIR 01: Proc. 24th annual intern.",
                "ACM SIGIR conf. on Research and development in information retrieval, pages 120-127, 2001. [15] V. Lavrenko, M. Choquette, and W. B. Croft.",
                "Cross-lingual relevance models.",
                "In SIGIR 02: Proc. 25th annual intern.",
                "ACM SIGIR conf. on Research and development in information retrieval, pages 175-182, 2002. [16] C. Macdonald and I. Ounis.",
                "Voting for candidates: adapting data fusion techniques for an expert search task.",
                "In CIKM 06: Proc. 15th ACM intern. conf. on Information and knowledge management, pages 387-396, 2006. [17] C. Manning and H. Sch¨utze.",
                "Foundations of Statistical Natural Language Processing.",
                "The MIT Press, 1999. [18] A. Mockus and J. D. Herbsleb.",
                "Expertise browser: a quantitative approach to identifying expertise.",
                "In ICSE 02: Proc. 24th Intern.",
                "Conf. on Software Engineering, pages 503-512, 2002. [19] D. Petkova and W. B. Croft.",
                "Hierarchical language models for expert finding in enterprise corpora.",
                "In Proc.",
                "ICTAI 2006, pages 599-608, 2006. [20] I. Soboroff, A. de Vries, and N. Craswell.",
                "Overview of the TREC 2006 Enterprise Track.",
                "In TREC 2006 Working Notes, 2006. [21] T. Tao, X. Wang, Q. Mei, and C. Zhai.",
                "Language model information retrieval with document expansion.",
                "In HLT-NAACL 2006, 2006. [22] TREC.",
                "Enterprise track, 2005.",
                "URL: http://www.ins.cwi. nl/projects/trec-ent/wiki/. [23] G. van Noord.",
                "TextCat Language Guesser.",
                "URL: http://www. let.rug.nl/˜vannoord/TextCat/. [24] W3C.",
                "The W3C test collection, 2005.",
                "URL: http://research. microsoft.com/users/nickcr/w3c-summary.html."
            ],
            "original_annotated_samples": [
                "The first is aimed at determining the effectiveness of <br>baseline expertise retrieval method</br>s applied to the new test set."
            ],
            "translated_annotated_samples": [
                "El primero tiene como objetivo determinar la efectividad de los <br>métodos de recuperación de conocimientos básicos</br> aplicados al nuevo conjunto de pruebas."
            ],
            "translated_text": "Recuperación de amplia experiencia en entornos de datos dispersos Krisztian Balog ISLA, Universidad de Ámsterdam Kruislaan 403, 1098 SJ Ámsterdam, Países Bajos kbalog@science.uva.nl Toine Bogers ILK, Universidad de Tilburg P.O. Caja 90153, 5000 LE Tilburg, Países Bajos A.M.Bogers@uvt.nl Leif Azzopardi Dept. de Ciencias de la Computación Universidad de Glasgow, Glasgow, G12 8QQ leif@dcs.gla.ac.uk Maarten de Rijke ISLA, Universidad de Ámsterdam Kruislaan 403, 1098 SJ Ámsterdam, Países Bajos mdr@science.uva.nl Antal van den Bosch ILK, Universidad de Tilburg P.O. La recuperación de la experiencia ha sido ampliamente inexplorada en datos distintos a la colección de la W3C. Al mismo tiempo, muchas intranets de universidades y otras organizaciones intensivas en conocimiento ofrecen ejemplos de datos de expertos multilingües relativamente pequeños pero precisos, que abarcan amplias áreas de especialización. Primero presentamos dos tareas principales de recuperación de expertos, junto con un conjunto de enfoques de referencia basados en modelado del lenguaje generativo, con el objetivo de encontrar relaciones de experticia entre temas y personas. Para nuestra evaluación experimental, introducimos (y liberamos) un nuevo conjunto de pruebas basado en un rastreo de un sitio universitario. Usando este conjunto de pruebas, llevamos a cabo dos series de experimentos. El primero tiene como objetivo determinar la efectividad de los <br>métodos de recuperación de conocimientos básicos</br> aplicados al nuevo conjunto de pruebas. El segundo tiene como objetivo evaluar modelos refinados que exploten las características distintivas del nuevo conjunto de pruebas, como la estructura organizativa de la universidad y la estructura jerárquica de los temas en el conjunto de pruebas. Los modelos de recuperación de experiencia se muestran robustos con respecto a entornos más pequeños que la colección de la W3C, y las técnicas actuales parecen ser generalizables a otros contextos. Categorías y Descriptores de Asignaturas H.3 [Almacenamiento y Recuperación de Información]: H.3.1 Análisis de Contenido e Indexación; H.3.3 Búsqueda y Recuperación de Información; H.3.4 Sistemas y Software; H.4 [Aplicaciones de Sistemas de Información]: H.4.2 Tipos de Sistemas; H.4.m Términos Generales Misceláneos Algoritmos, Medición, Rendimiento, Experimentación 1. INTRODUCCIÓN La intranet de una organización proporciona un medio para intercambiar información entre empleados y facilitar colaboraciones entre ellos. Para lograr esto de manera eficiente y efectiva, es necesario proporcionar instalaciones de búsqueda que permitan a los empleados no solo acceder a documentos, sino también identificar a colegas expertos. En la pista de Empresas de TREC [22] se ha reconocido la necesidad de estudiar y comprender la recuperación de expertos a través de la introducción de tareas de Búsqueda de Expertos. El objetivo de la búsqueda de expertos es identificar una lista de personas que tienen conocimientos sobre un tema específico. Esta tarea suele abordarse descubriendo asociaciones entre personas y temas [10]; comúnmente, se asume que la co-ocurrencia del nombre de una persona con temas en el mismo contexto es evidencia de experiencia. Una tarea alternativa, que utiliza la misma idea de asociaciones entre personas y temas, es el perfilado de expertos, donde la tarea consiste en devolver una lista de temas sobre los que una persona tiene conocimiento [3]. El lanzamiento de la tarea de Búsqueda de Expertos en TREC ha generado mucho interés en la recuperación de expertos, con un rápido progreso en términos de modelado, algoritmos y aspectos de evaluación. Sin embargo, casi todo el trabajo de búsqueda o perfilado de expertos realizado ha sido validado experimentalmente utilizando la colección de W3C [24] de la pista empresarial. Si bien esta colección es actualmente la única colección de pruebas disponible públicamente para tareas de recuperación de expertos, solo representa un tipo de intranet. Con solo una colección de pruebas no es posible generalizar conclusiones a otros escenarios realistas. En este artículo nos enfocamos en la recuperación de expertos en un entorno realista que difiere del entorno de la W3C, uno en el que hay cantidades relativamente pequeñas de datos limpios y multilingües disponibles, que abarcan una amplia gama de áreas de expertise, como se puede encontrar en las intranets de universidades y otras organizaciones intensivas en conocimiento. Normalmente, esta configuración incluye varios tipos adicionales de estructuras: estructura temática (por ejemplo, jerarquías de temas utilizadas por la organización), estructura organizativa (facultad, departamento, ...), así como múltiples tipos de documentos (descripciones de investigación y cursos, publicaciones y páginas web académicas). Este entorno es bastante diferente del entorno del W3C de maneras que podrían afectar el rendimiento de las tareas de recuperación de conocimientos especializados. Nos enfocamos en varias preguntas de investigación en este artículo: ¿La cantidad relativamente pequeña de datos disponibles en una intranet afecta la calidad de las asociaciones tema-persona que son fundamentales en los algoritmos de recuperación de expertos? ¿Cómo se desempeñan los algoritmos de vanguardia desarrollados en el conjunto de datos del W3C en el escenario alternativo del tipo descrito anteriormente? Más en general, ¿se pueden aplicar las lecciones de la tarea de Búsqueda de Expertos en TREC a este contexto? ¿Cómo afecta la inclusión o exclusión de diferentes documentos a las tareas de recuperación de expertos? Además, ¿cómo se puede utilizar la estructura temática y organizativa con fines de recuperación? Para responder a nuestras preguntas de investigación, primero presentamos un conjunto de enfoques de línea base, basados en modelado de lenguaje generativo, con el objetivo de encontrar asociaciones entre temas y personas. Esto nos permite formular las tareas de búsqueda de expertos y perfilado de expertos de manera uniforme, y tiene el beneficio adicional de permitirnos entender las relaciones entre las dos tareas. Para nuestra evaluación experimental, introducimos un nuevo conjunto de datos (la Colección de Expertos de UvT) que es representativo del tipo de intranet que describimos anteriormente. Nuestra colección se basa en datos de acceso público, extraídos del sitio web de la Universidad de Tilburg (UvT). Este tipo de datos es particularmente interesante, ya que (1) es limpio, heterogéneo, estructurado y enfocado, pero comprende un número limitado de documentos; (2) contiene información sobre la jerarquía organizativa; (3) es bilingüe (inglés y holandés); y (4) la lista de áreas de especialización de un individuo es proporcionada por los propios empleados. Usando la colección de expertos de UvT, llevamos a cabo dos conjuntos de experimentos. El primero tiene como objetivo determinar la efectividad de los métodos de búsqueda y perfilado de experiencia inicial en este nuevo entorno. Un segundo grupo de experimentos está dirigido a extensiones de los métodos base que aprovechan las características distintivas de la Colección de Expertos de UvT; específicamente, proponemos y evaluamos métodos refinados de búsqueda y perfilado de expertos que incorporan la topicalidad y la estructura organizativa. Además de las preguntas de investigación y el conjunto de datos que aportamos, nuestras principales contribuciones son las siguientes. Los modelos base desarrollados para encontrar expertos funcionan bien en el nuevo conjunto de datos. Mientras que en el entorno del W3C la tarea de encontrar expertos parece ser más difícil que el perfilado, para los datos de UvT ocurre lo contrario. Encontramos que el perfilado en el conjunto de datos UvT es considerablemente más difícil que en el conjunto de datos W3C, lo cual creemos que se debe al gran (pero realista) número de áreas temáticas que utilizamos para el perfilado: alrededor de 1,500 para el conjunto de datos UvT, frente a 50 en el caso de W3C. Teniendo en cuenta la similitud entre los temas puede mejorar significativamente el rendimiento de recuperación. Las medidas de similitud de mejor rendimiento son basadas en contenido, por lo tanto también se pueden aplicar en entornos de la W3C (y otros). Finalmente, demostramos que la estructura organizativa puede ser explotada en forma de un modelo de contexto, mejorando los puntajes de MAP para ciertos modelos hasta en un 70%. El resto de este documento está organizado de la siguiente manera. En la siguiente sección revisamos el trabajo relacionado. Luego, en la Sección 3 proporcionamos descripciones detalladas de las tareas de recuperación de expertos que abordamos en este artículo: la búsqueda de expertos y la creación de perfiles de expertos. En la Sección 4 presentamos nuestros modelos base, cuyo rendimiento luego es evaluado en la Sección 6 utilizando el conjunto de datos UvT que presentamos en la Sección 5. Los modelos avanzados que explotan características específicas de nuestros datos se presentan en la Sección 7 y se evalúan en la Sección 8. Formulamos nuestras conclusiones en la Sección 9.2. Trabajos relacionados Los enfoques iniciales para encontrar expertos a menudo empleaban bases de datos que contenían información sobre las habilidades y conocimientos de cada individuo en la organización [11]. La mayoría de estas herramientas (generalmente llamadas páginas amarillas o sistemas de búsqueda de personas) dependen de que las personas evalúen sus habilidades frente a un conjunto predefinido de palabras clave. Para actualizar perfiles en estos sistemas de forma automática, se necesita de tecnologías inteligentes [5]. Enfoques más recientes utilizan conjuntos de documentos específicos (como correos electrónicos [6] o software [18]) para encontrar experiencia. En contraste con centrarse en tipos de documentos particulares, también hay un interés creciente en el desarrollo de sistemas que indexan y extraen información de documentos publicados en intranets como fuentes de evidencia de experiencia. Un enfoque publicado es el sistema P@noptic [9], que construye una representación de cada persona concatenando todos los documentos asociados con esa persona, similar al Modelo 1 de Balog et al. [4], quienes formalizan y comparan dos métodos. El Modelo 1 de Balog et al. modela directamente el conocimiento de un experto a partir de documentos asociados, mientras que su Modelo 2 primero localiza documentos sobre el tema y luego encuentra a los expertos asociados. En los experimentos reportados, el segundo método tiene un rendimiento significativamente mejor cuando hay suficientes documentos asociados por candidato. La mayoría de los sistemas que participaron en las ediciones de 2005 y 2006 de la tarea de Búsqueda de Expertos en TREC implementaron (variaciones de) uno de estos dos modelos; ver [10, 20]. Macdonald y Ounis [16] proponen un enfoque diferente para clasificar la experiencia de los candidatos con respecto a un tema basado en técnicas de fusión de datos, sin utilizar heurísticas específicas de la colección; encuentran que la aplicación de modelos de ponderación basados en campos mejora la clasificación de los candidatos. Petkova y Croft [19] proponen otro enfoque, basado en una combinación de los Modelos 1 y 2 anteriores, modelando explícitamente los temas. Al abordar otras tareas de recuperación de expertos que también pueden abordarse utilizando asociaciones entre temas y personas, Balog y de Rijke [3] abordaron la tarea de determinar perfiles de expertos temáticos. Si bien sus métodos resultaron ser eficientes en el corpus de la W3C, requieren una cantidad de datos que puede que no esté disponible en la típica organización intensiva en conocimiento. Balog y de Rijke [2] estudian la tarea relacionada de encontrar expertos que sean similares a un pequeño conjunto de expertos dados como entrada. Por cierto, crear un resumen textual de una persona muestra algunas similitudes con la búsqueda de biografías, la cual ha recibido una considerable cantidad de atención recientemente; ver por ejemplo, [13]. Utilizamos modelado de lenguaje generativo para encontrar asociaciones entre temas y personas. En nuestro modelado de búsqueda y perfilado de expertos recopilamos evidencia de experiencia de múltiples fuentes, en una colección heterogénea, e integramos esta información con la co-ocurrencia de nombres de candidatos y términos de consulta: el entorno de modelado de lenguaje nos permite hacer esto de manera transparente. Nuestro modelado avanza en dos pasos. En el primer paso, consideramos tres modelos base, dos tomados de [4] (los Modelos 1 y 2 mencionados anteriormente), y uno una versión refinada de un modelo introducido en [3] (al que nos referimos como Modelo 3 a continuación); este tercer modelo también es similar al modelo descrito por Petkova y Croft [19]. Los modelos que consideramos en nuestra segunda ronda de experimentos son modelos de mezcla similares a los modelos de lenguaje contextual [1] y a los documentos ampliados de Tao et al. [21]; sin embargo, las características que utilizamos para definir nuestras expansiones, incluida la estructura temática y la estructura organizativa, no se han utilizado de esta manera antes. 3. En el escenario de recuperación de expertos que imaginamos, los usuarios que buscan expertos dentro de una organización tienen acceso a una interfaz que combina un cuadro de búsqueda (donde pueden buscar expertos o temas) con estructuras de navegación (de expertos y de temas) que les permite hacer clic hasta llegar a una página de experto (que proporciona el perfil de una persona) o a una página de tema (que proporciona una lista de expertos en el tema). Para alimentar la interfaz anterior, nos enfrentamos a dos tareas de recuperación de expertos, la búsqueda de expertos y el perfilado de expertos, que primero definimos y luego formalizamos utilizando modelos de lenguaje generativos. Para modelar cualquiera de las tareas, la probabilidad de que el tema de la consulta esté asociado a un experto candidato juega un papel clave en las estimaciones finales para la búsqueda y el perfilado. Al utilizar modelos de lenguaje, tanto los candidatos como la consulta se caracterizan por distribuciones de términos en el vocabulario (utilizado en los documentos puestos a disposición por la organización cuyas necesidades de recuperación de experiencia estamos abordando). 3.1 Búsqueda de expertos La búsqueda de expertos implica la tarea de encontrar a la persona adecuada con las habilidades y conocimientos apropiados: ¿Quiénes son los expertos en el tema X?. Por ejemplo, un empleado quiere averiguar quién trabajó en un proyecto en particular para descubrir por qué se tomaron decisiones específicas sin tener que revisar minuciosamente la documentación (si es que la hay). O pueden necesitar un especialista capacitado para consultoría sobre un problema específico. Dentro de una organización, generalmente hay muchos posibles candidatos que podrían ser expertos en un tema dado. Podemos plantear este problema de la siguiente manera: ¿Cuál es la probabilidad de que un candidato ca sea un experto dado el tema de consulta q? Es decir, determinamos p(ca|q) y clasificamos a los candidatos ca de acuerdo con esta probabilidad. Los candidatos con la probabilidad más alta dada la consulta son considerados los expertos más probables en ese tema. El desafío es cómo estimar esta probabilidad con precisión. Dado que es probable que la consulta consista solo en unos pocos términos para describir la experiencia requerida, deberíamos poder obtener una estimación más precisa al invocar el Teorema de Bayes, y estimar: p(ca|q) = p(q|ca)p(ca) p(q) , (1) donde p(ca) es la probabilidad de un candidato y p(q) es la probabilidad de una consulta. Dado que p(q) es una constante, se puede ignorar para fines de clasificación. Por lo tanto, la probabilidad de que un candidato ca sea un experto dado la consulta q es proporcional a la probabilidad de una consulta dada el candidato p(q|ca), ponderada por la creencia a priori p(ca) de que el candidato ca es un experto. p(ca|q) ∝ p(q|ca)p(ca) (2) En este artículo, nuestro enfoque principal se centra en estimar la probabilidad de una consulta dada el candidato p(q|ca), ya que esta probabilidad captura en qué medida el candidato conoce sobre el tema de la consulta. Si bien se asume generalmente que los priors del candidato son uniformes y, por lo tanto, no influirán en la clasificación, se ha demostrado que una elección sensata de priors puede mejorar el rendimiento [20]. Perfilado de expertos. Mientras que la tarea de búsqueda de expertos se centraba en encontrar expertos en un tema específico, la tarea de perfilado de expertos busca responder a una pregunta relacionada: ¿Sobre qué temas tiene conocimiento un candidato? Básicamente, esto da un giro a las preguntas sobre la búsqueda de expertos. El perfilado de un candidato individual implica la identificación de áreas de habilidades y conocimientos en las que tienen experiencia, así como una evaluación del nivel de competencia en cada una de estas áreas. Este es el perfil temático de los candidatos. Generalmente, los perfiles temáticos dentro de las organizaciones consisten en estructuras tabulares que catalogan explícitamente las habilidades y conocimientos de cada individuo en la organización. Sin embargo, esta práctica está limitada por los recursos disponibles para definir, crear, mantener y actualizar estos perfiles con el tiempo. Al centrarnos en métodos automáticos que se basan en la evidencia disponible dentro de los repositorios de documentos de una organización, nuestro objetivo es reducir el esfuerzo humano asociado con el mantenimiento de perfiles temáticos. Un perfil temático de un candidato, entonces, se define como un vector donde cada elemento i del vector corresponde a la experiencia del candidato en un tema dado ki, (es decir, s(ca, ki)). Cada tema ki define un área de conocimiento o habilidad particular que la organización utiliza para definir el perfil temático de los candidatos. Por lo tanto, se asume que se proporciona una lista de temas, {k1, . . . , kn}, donde n es el número de temas predefinidos: perfil(ca) = s(ca, k1), s(ca, k2), . . . , s(ca, kn) . Se necesita contexto y evidencia para ayudar a los usuarios de sistemas de búsqueda de expertos a decidir a quién contactar al buscar experiencia en un área particular. Ejemplos de dicho contexto son: ¿Con quién trabaja ella? ¿Cuáles son sus datos de contacto? ¿Está bien conectada, por si acaso no puede ayudarnos ella misma? ¿Cuál es su rol en la organización? ¿Quién es su superior? Los colaboradores, afiliaciones, etc. son parte del perfil social de los candidatos y pueden servir como antecedentes para interpretar las recomendaciones del sistema. En este documento solo abordamos el problema de determinar perfiles temáticos, dejando el perfilado social para trabajos futuros. Planteamos el problema de cuantificar la competencia de una persona en un área de conocimiento específica de la siguiente manera: ¿Cuál es la probabilidad de que un área de conocimiento (ki) forme parte del perfil de candidatos (expertise)? donde s(ca, ki) está definido por p(ki|ca). Nuestra tarea, entonces, es estimar p(ki|ca), lo cual es equivalente al problema de obtener p(q|ca), donde el tema ki se representa como un tema de consulta q, es decir, una secuencia de palabras clave que representan la experiencia requerida. Tanto la tarea de encontrar expertos como la de perfilar expertos dependen de la estimación precisa de p(q|ca). La única diferencia proviene de la probabilidad previa de que una persona sea experta (p(ca)), la cual puede ser incorporada en la tarea de encontrar expertos. Este antecedente no se aplica a la tarea de perfilado ya que el candidato (individuo) está fijo. 4. MODELOS DE REFERENCIA En esta sección describimos nuestros modelos de referencia para estimar p(q|ca), es decir, las asociaciones entre temas y personas. Tanto la búsqueda de expertos como el perfilado de expertos se reducen a esta estimación. Empleamos tres modelos para calcular esta probabilidad. 4.1 De temas a candidatos Utilizando Modelos de Candidatos: El Modelo 1 [4] define la probabilidad de una consulta dada un candidato (p(q|ca)) utilizando técnicas estándar de modelado de lenguaje, basadas en un modelo de lenguaje multinomial de unigrama. Para cada candidato ca, se infiere un modelo de lenguaje candidato θca de tal manera que la probabilidad de un término dado θca no sea cero para todos los términos, es decir, p(t|θca) > 0. A partir del modelo de candidato, la consulta se genera con la siguiente probabilidad: p(q|θca) = Y t∈q p(t|θca)n(t,q), donde cada término t en la consulta q se muestrea de manera idéntica e independiente, y n(t, q) es el número de veces que t ocurre en q. El modelo de lenguaje candidato se infiere de la siguiente manera: (1) se calcula un modelo empírico p(t|ca); (2) se suaviza con probabilidades de fondo. Usando las asociaciones entre un candidato y un documento, la probabilidad p(t|ca) puede aproximarse por: p(t|ca) = X d p(t|d)p(d|ca), donde p(d|ca) es la probabilidad de que el candidato ca genere un documento de apoyo d, y p(t|d) es la probabilidad de que un término t ocurra en el documento d. Utilizamos la estimación de máxima verosimilitud de un término, es decir, la frecuencia normalizada del término t en el documento d. La fuerza de la asociación entre el documento d y el candidato ca expresada por p(d|ca) refleja el grado en que la experiencia de los candidatos se describe utilizando este documento. La estimación de esta probabilidad se presenta más adelante, en la Sección 4.2. El modelo candidato se construye entonces como una interpolación lineal de p(t|ca) y el modelo de fondo p(t) para asegurar que no haya probabilidades nulas, lo que resulta en la estimación final: p(q|θca) = (4) Y t∈q ( (1 − λ) X d p(t|d)p(d|ca) ! + λp(t) )n(t,q) . El Modelo 1 recopila toda la información de términos de todos los documentos asociados con el candidato, y la utiliza para representar a ese candidato. Este modelo se utiliza para predecir qué tan probable es que un candidato produzca una consulta q. Esto puede interpretarse intuitivamente como la probabilidad de que este candidato hable sobre el tema de la consulta, donde asumimos que esto es indicativo de su experiencia. Usando Modelos de Documentos: El Modelo 2 [4] toma un enfoque diferente. Aquí, el proceso se divide en dos partes. Dado un candidato ca, (1) se selecciona un documento asociado con un candidato con probabilidad p(d|ca), y (2) a partir de este documento se genera una consulta q con probabilidad p(q|d). Entonces se toma la suma sobre todos los documentos para obtener p(q|ca), de modo que: p(q|ca) = Σ d p(q|d)p(d|ca). (5) La probabilidad de una consulta dada un documento se estima inferiendo un modelo de lenguaje del documento θd para cada documento d de manera similar a como se infería el modelo candidato: p(t|θd) = (1 − λ)p(t|d) + λp(t), (6) donde p(t|d) es la probabilidad del término en el documento. La probabilidad de una consulta dada el modelo de documento es: p(q|θd) = Y t∈q p(t|θd)n(t,q) . La estimación final de p(q|ca) se obtiene sustituyendo p(q|d) por p(q|θd) en la Ecuación 5 (ver [4] para más detalles). Conceptualmente, el Modelo 2 difiere del Modelo 1 porque el candidato no está modelado directamente. En cambio, el documento actúa como una variable oculta en el proceso que separa la consulta del candidato. Este proceso es similar a cómo un usuario puede buscar candidatos con un motor de búsqueda estándar: inicialmente encontrando los documentos relevantes y luego viendo quién está asociado con ese documento. Al examinar una serie de documentos, el usuario puede obtener una idea de qué candidatos son más propensos a discutir el tema q. Usando Modelos de Temas: Modelo 3 Introducimos un tercer modelo, Modelo 3. En lugar de intentar modelar el proceso de generación de consultas a través de modelos de candidatos o documentos, representamos la consulta como un modelo de lenguaje de temas y estimamos directamente la probabilidad del candidato p(ca|q). Este enfoque es similar al modelo presentado en [3, 19]. Como en los modelos anteriores, se infiere un modelo de lenguaje, pero esta vez para la consulta. Adaptamos el trabajo de Lavrenko y Croft [14] para estimar un modelo de tema a partir de la consulta. El procedimiento es el siguiente. Dada una colección de documentos y un tema de consulta q, se asume que existe un modelo de tema desconocido θk que asigna probabilidades p(t|θk) a las ocurrencias de términos en los documentos del tema. Tanto la consulta como los documentos son muestras de θk (a diferencia de los enfoques anteriores, donde se asume que una consulta se extrae de un documento específico o un modelo candidato). La tarea principal es estimar p(t|θk), la probabilidad de un término dado el modelo de tema. Dado que la consulta q es muy dispersa y no hay ejemplos de documentos sobre el tema, es necesario aproximar esta distribución. Lavrenko y Croft [14] sugieren una forma razonable de obtener dicha aproximación, asumiendo que p(t|θk) puede aproximarse por la probabilidad del término t dado la consulta q. Entonces podemos estimar p(t|q) utilizando la probabilidad conjunta de observar el término t junto con los términos de la consulta, q1, . . . , qm, y dividiendo por la probabilidad conjunta de los términos de la consulta: p(t|θk) ≈ p(t|q) = p(t, q1, . . . , qm) p(q1, . . . , qm) = p(t, q1, . . . , qm) P t ∈T p(t , q1, . . . , qm) , donde p(q1, . . . , qm) = P t ∈T p(t , q1, . . . , qm), y T es el vocabulario completo de términos. Para estimar la probabilidad conjunta p(t, q1, . . . , qm), seguimos [14, 15] y asumimos que t y q1, . . . , qm son mutuamente independientes, una vez que elegimos una distribución fuente del conjunto de distribuciones fuente subyacentes U. Si elegimos U como un conjunto de modelos de documentos, entonces para construir este conjunto, la consulta q se emitiría contra la colección, y se asume que los primeros n devueltos son relevantes para el tema, y por lo tanto se tratan como muestras del modelo de tema. (Tenga en cuenta que en su lugar podrían usarse modelos candidatos). Con los modelos de documentos formando U, la probabilidad conjunta del término y la consulta se convierte en: p(t, q1, . . . , qm) = X d∈U p(d) ˘ p(t|θd) mY i=1 p(qi|θd) ¯ . (7) Aquí, p(d) denota la distribución previa sobre el conjunto U, que refleja la relevancia del documento para el tema. Suponemos que p(d) es uniforme en todo U. Para clasificar a los candidatos según el modelo de tema definido, utilizamos la métrica de divergencia de Kullback-Leibler (KL, [8]) para medir la diferencia entre los modelos de los candidatos y el modelo de tema: KL(θk||θca) = X t p(t|θk) log p(t|θk) p(t|θca) . (8) Se considera que los candidatos con una divergencia menor respecto al modelo de tema son más propensos a ser expertos en ese tema. El modelo candidato θca está definido en la Ecuación 4. Al utilizar la divergencia de Kullback-Leibler en lugar de la probabilidad de un candidato dado el modelo de tema p(ca|θk), evitamos problemas de normalización. 4.2 Asociaciones documento-candidato Para nuestros modelos, necesitamos poder estimar la probabilidad p(d|ca), que expresa en qué medida un documento d caracteriza al candidato ca. En [4], se presentan dos métodos para estimar esta probabilidad, basados en el número de nombres de personas reconocidos en un documento. Sin embargo, en nuestro entorno de intranet es razonable asumir que los autores de los documentos pueden ser identificados de manera inequívoca (por ejemplo, como el autor de un artículo, el profesor asignado a un curso, el propietario de una página web, etc.). Por lo tanto, establecemos p(d|ca) en 1 si el candidato ca es el autor del documento d, de lo contrario la probabilidad es 0. En la Sección 6 describimos cómo se puede determinar la autoría en diferentes tipos de documentos dentro de la colección. 5. La colección de expertos de UvT utilizada en los experimentos de este artículo se ajusta al escenario descrito en la Sección 3. La colección se basa en el sistema Webwijs (Webwise) desarrollado en la Universidad de Tilburg (UvT) en los Países Bajos. Webwijs (http://www.uvt.nl/webwijs/) es una base de datos de acceso público de empleados de la UvT que están involucrados en investigación o enseñanza; actualmente, Webwijs contiene información sobre 1168 expertos, cada uno de los cuales tiene una página con información de contacto y, si está disponible por parte del experto, una descripción de investigación y lista de publicaciones. Además, cada experto puede seleccionar áreas de especialización de una lista de 1491 temas y se le anima a sugerir nuevos temas que necesitan ser aprobados por el editor de Webwijs. Cada tema tiene una página separada que muestra a todos los expertos asociados con ese tema y, si está disponible, una lista de temas relacionados. Webwijs está disponible en holandés e inglés, y esta bilingüidad se ha preservado en la colección. Cada página de Webwijs en holandés tiene una traducción al inglés. No todos los temas en holandés tienen una traducción al inglés, pero lo contrario es cierto: los 981 temas en inglés tienen un equivalente en holandés. Aproximadamente el 42% de los expertos imparten cursos en la Universidad de Tilburg; estos cursos también fueron rastreados e incluidos en el perfil. Además, aproximadamente el 27% de los expertos enlazan su página académica desde su página de Webwijs. Estas páginas de inicio fueron rastreadas y añadidas a la colección. (Esto significa que si los expertos colocaron las versiones completas de sus publicaciones en sus páginas de inicio académicas, estas también estaban disponibles para su indexación). También obtuvimos 1880 versiones completas de publicaciones del repositorio institucional de UvT y el número de expertos holandeses en inglés es de 1168, con 743 expertos con ≥ 1 tema. El número de temas es de 1491 y 981, y el número de pares experto-tema es de 4318 y 3251, con un promedio de temas por experto de 5.8 y 5.9 respectivamente. El máximo número de temas por experto es de 60 (1) y 35 (1), y el mínimo es de 1 (74) y 1 (106) respectivamente. El promedio de expertos por tema es de 2.9 y 3.3, con un máximo de 30 (1) y un mínimo de 1 (615) y 1 (346) respectivamente. Hay 318 expertos con HP y CD, con un promedio de 3.5 CDs por experto docente. Además, hay 329 expertos con RD y 734 con PUB, con un promedio de 27.0 publicaciones por experto, 25.2 citas por experto y 1.8 publicaciones completas por experto. Tabla 2: Estadísticas descriptivas de las versiones holandesas e inglesas de la colección de expertos de UvT. Ejecutamos el identificador de idioma TextCat [23] para clasificar el idioma de las páginas de inicio y las publicaciones de texto completo. Nos limitamos a las páginas en las que el clasificador tenía confianza en el idioma utilizado en la página. Esto resultó en cuatro tipos de documentos: descripciones de investigación (RD), descripciones de cursos (CD), publicaciones (PUB; versiones de texto completo y solo de cita) y páginas web académicas (HP). Todo fue agrupado en la colección UvT Expert que está disponible en http://ilk.uvt.nl/uvt-expert-collection/. La colección UvT Expert fue extraída de un entorno organizativo diferente a la colección de la W3C y difiere de ella en varios aspectos. El entorno de UvT es uno con cantidades relativamente pequeñas de datos multilingües. Las asociaciones entre los documentos y los autores son claras y los datos están estructurados y limpios. La colección abarca una amplia gama de áreas de especialización, como suele encontrarse en intranets de universidades y otros institutos intensivos en conocimiento. Además, nuestro entorno universitario cuenta con varios tipos de estructura (temática y organizativa), así como múltiples tipos de documentos. Otra diferencia importante entre los dos conjuntos de datos es que las áreas de especialización en la colección de Expertos de UvT son autoseleccionadas en lugar de basarse en la membresía de un grupo o asignaciones de otros. El tamaño es otra dimensión en la que difieren las colecciones de expertos de W3C y UvT: esta última es la más pequeña de las dos. También son realistas las grandes diferencias en la cantidad de información disponible para cada experto. Utilizar Webwijs es voluntario; 425 expertos holandeses no seleccionaron ningún tema en absoluto. Esto nos deja con 743 perfiles de expertos holandeses y 727 perfiles de expertos en inglés utilizables. La Tabla 2 proporciona estadísticas descriptivas para la colección de Expertos de UvT. Las universidades tienden a tener una estructura jerárquica que va desde el nivel de la facultad, pasando por los departamentos, grupos de investigación, hasta llegar a los investigadores individuales. En la colección de Expertos de UvT tenemos información sobre las afiliaciones de los investigadores con facultades e institutos, lo que nos proporciona una jerarquía organizativa de dos niveles. La Universidad de Tilburg tiene 22 unidades organizativas a nivel de facultad (incluida la oficina universitaria y varios institutos de investigación) y 71 departamentos, lo que equivale a 3.2 departamentos por facultad. En cuanto a la jerarquía temática utilizada por Webwijs, 131 de los 1491 temas son nodos principales en la jerarquía. Esta jerarquía tiene una longitud promedio de cadena de temas de 2.65 y una longitud máxima de 7 temas. 6. EVALUACIÓN A continuación, evaluamos los modelos de la Sección 4 para la búsqueda y perfilado de expertos en la colección de Expertos de UvT. Detallamos nuestras preguntas de investigación y configuración experimental, y luego presentamos nuestros resultados. 6.1 Preguntas de investigación Abordamos las siguientes preguntas de investigación. Tanto la búsqueda de expertos como el perfilado dependen de las estimaciones de p(q|ca). La pregunta es cómo se comparan los modelos en las diferentes tareas, y en el contexto de la colección de expertos de UvT. En [4], el Modelo 2 superó al Modelo 1 en la colección de la W3C. ¿Cómo se comparan en nuestro conjunto de datos? ¿Y cómo se compara el Modelo 3 con el Modelo 1? ¿Qué hay de las diferencias de rendimiento entre los dos idiomas en nuestra colección de pruebas? 6.2 Configuración Experimental La salida de nuestros modelos fue evaluada frente a las etiquetas de temas autoasignadas, que fueron tratadas como juicios de relevancia. Los resultados fueron evaluados por separado para inglés y holandés. Para inglés solo usamos temas para los cuales había traducción al neerlandés; para neerlandés se consideraron todos los temas. Los resultados se promediaron para las consultas en la intersección de las evaluaciones de relevancia y los resultados; las consultas faltantes no contribuyen con un valor de 0 a las puntuaciones. Utilizamos medidas estándar de recuperación de información, como la Precisión Media Promedio (MAP) y la Reciprocidad Media Promedio (MRR). También informamos el porcentaje de temas (%q) y candidatos (%ca) cubiertos, respectivamente, para las tareas de búsqueda y perfilado de expertos. 6.3 Resultados La Tabla 1 muestra el rendimiento de los Modelos 1, 2 y 3 en las tareas de búsqueda y perfilado de expertos. Las filas de la tabla corresponden a los diversos tipos de documentos (RD, CD, PUB y HP) y a sus combinaciones. RD+CD+PUB+HP es equivalente a la colección completa y será referida como la LÍNEA BASE de nuestros experimentos. Al observar la Tabla 1, vemos que el Modelo 2 tiene el mejor rendimiento en general. Sin embargo, cuando los datos están limpios y muy enfocados (RD), el Modelo 3 lo supera en varios casos. El modelo 1 tiene la mejor cobertura de candidatos (%ca) y temas (%q). Los diversos tipos de documentos difieren en sus características y en cómo mejoran las tareas de búsqueda y perfilado. El perfilado de expertos se beneficia mucho de los datos limpios presentes en los tipos de documentos RD y CD, mientras que las publicaciones contribuyen principalmente a la tarea de encontrar expertos. Agregar las páginas de inicio no resulta ser particularmente útil. Cuando comparamos los resultados entre idiomas, encontramos que la cobertura de los temas en inglés (%q) es mayor que la de los temas en neerlandés para la búsqueda de expertos. Además de eso, las puntuaciones caen en el mismo rango para ambos idiomas. Para la tarea de perfilado, la cobertura de los candidatos (%ca) es muy similar para ambos idiomas. Sin embargo, el rendimiento es considerablemente mejor para los temas en inglés. Si bien es difícil comparar las puntuaciones entre colecciones, concluimos con una breve comparación de las puntuaciones absolutas en la Tabla 1 con las reportadas en [3, 4] en el conjunto de pruebas de W3C (edición de 2005). Para el experto que busca, los puntajes MAP para el Modelo 2 reportados aquí son aproximadamente un 50% más altos que las cifras correspondientes en [4], mientras que nuestros puntajes MRR están ligeramente por debajo de los de [4]. Para el perfilado de expertos, las diferencias son mucho más dramáticas: las puntuaciones MAP para el Modelo 2 reportadas aquí son aproximadamente un 50% más bajas que las puntuaciones en [3], mientras que las puntuaciones MRR (mejores) son aproximadamente iguales a las de [3]. La causa de estas diferencias parece residir en el número de áreas de conocimiento consideradas aquí, aproximadamente 30 veces más que en el entorno del W3C. MODELOS AVANZADOS Ahora que hemos desarrollado y evaluado técnicas básicas de modelado de lenguaje para la recuperación de expertos, nos enfocamos en modelos refinados que explotan características especiales de nuestra colección de pruebas. 7.1 Explotando la similitud en el área de conocimiento Una forma de mejorar la puntuación de una consulta dada un candidato es considerar qué otras solicitudes satisfaría el candidato y usarlas como evidencia adicional para respaldar la consulta original, proporcionalmente. Búsqueda de expertos Perfilado de expertos Tipos de documentos Modelo 1 Modelo 2 Modelo 3 Modelo 1 Modelo 2 Modelo 3 %q MAP MRR %q MAP MRR %q MAP MRR %ca MAP MRR %ca MAP MRR %ca MAP MRR Inglés RD 97.8 0.126 0.269 83.5 0.144 0.311 83.3 0.129 0.271 100 0.089 0.189 39.3 0.232 0.465 41.1 0.166 0.337 CD 97.8 0.118 0.227 91.7 0.123 0.248 91.7 0.118 0.226 32.8 0.188 0.381 32.4 0.195 0.385 32.7 0.203 0.370 PUB 97.8 0.200 0.330 98.0 0.216 0.372 98.0 0.145 0.257 78.9 0.167 0.364 74.5 0.212 0.442 78.9 0.135 0.299 HP 97.8 0.081 0.186 97.4 0.071 0.168 97.2 0.062 0.149 31.2 0.150 0.299 28.8 0.185 0.335 30.1 0.136 0.287 RD+CD 97.8 0.188 0.352 92.9 0.193 0.360 92.9 0.150 0.273 100 0.145 0.286 61.3 0.251 0.477 63.2 0.217 0.416 RD+CD+PUB 97.8 0.235 0.373 98.1 0.277 0.439 98.1 0.178 0.305 100 0.196 0.380 87.2 0.280 0.533 89.5 0.170 0.344 RD+CD+PUB+HP 97.8 0.237 0.372 98.6 0.280 0.441 98.5 0.166 0.293 100 0.199 0.387 88.7 0.281 0.525 90.9 0.169 0.329 Holandés RD 61.3 0.094 0.229 38.4 0.137 0.336 38.3 0.127 0.295 38.0 0.127 0.386 34.1 0.138 0.420 38.0 0.105 0.327 CD 61.3 0.107 0.212 49.7 0.128 0.256 49.7 0.136 0.261 32.5 0.151 0.389 31.8 0.158 0.396 32.5 0.170 0.380 PUB 61.3 0.193 0.319 59.5 0.218 0.368 59.4 0.173 0.291 78.8 0.126 0.364 76.0 0.150 0.424 78.8 0.103 0.294 HP 61.3 0.063 0.169 56.6 0.064 0.175 56.4 0.062 0.163 29.8 0.108 0.308 27.8 0.125 0.338 29.8 0.098 0.255 RD+CD 61.3 0.159 0.314 51.9 0.184 0.360 51.9 0.169 0.324 60.5 0.151 0.410 57.2 0.166 0.431 60.4 0.159 0.384 RD+CD+PUB 61.3 0.244 0.398 61.5 0.260 0.424 61.4 0.210 0.350 90.3 0.165 0.445 88.2 0.189 0.479 90.3 0.126 0.339 RD+CD+PUB+HP 61.3 0.249 0.401 62.6 0.265 0.436 62.6 0.195 0.344 91.9 0.164 0.426 90.1 0.195 0.488 91.9 0.125 0.328 Tabla 1: Rendimiento de los modelos en las tareas de búsqueda y perfilado de expertos, utilizando diferentes tipos de documentos y sus combinaciones. %q es el número de temas cubiertos (se aplica a la tarea de búsqueda de expertos), %ca es el número de candidatos cubiertos (se aplica a la tarea de perfilado de expertos). Los bloques superior e inferior corresponden al inglés y al holandés respectivamente. Las mejores puntuaciones están en negrita. Esto se puede modelar interpolando entre el p(q|ca) y la evidencia adicional de todos los pedidos similares q, de la siguiente manera: p(q|ca) = λp(q|ca) + (1 − λ) X q p(q|q )p(q |ca), (9) donde p(q|q ) representa la similitud entre los dos temas q y q. Para poder trabajar con métodos de similitud que no son necesariamente probabilidades, establecemos p(q|q ) = w(q,q ) γ , donde γ es una constante de normalización, de modo que γ = P q w(q , q ). Consideramos cuatro métodos para calcular la puntuación de similitud entre dos temas. Tres enfoques se basan estrictamente en el contenido y establecen similitudes examinando patrones de co-ocurrencia de temas dentro de la colección, mientras que el último enfoque explota la estructura jerárquica de áreas temáticas que pueden estar presentes dentro de una organización (ver [7] para más ejemplos de integrar relaciones entre palabras en modelos de lenguaje). La métrica de divergencia Kullback-Leibler (KL) definida en la ecuación 8 proporciona una medida de cuán diferentes o similares son dos distribuciones de probabilidad. Se infiere un modelo de tema para q y q utilizando el método presentado en la Sección 4.1 para describir la consulta en todo el vocabulario. Dado que un puntaje KL más bajo significa que las consultas son más similares, permitimos que w(q, q ) = max(KL(θq||·) − KL(θq||θq )). El Información Mutua Puntual (PMI, [17]) es una medida de asociación utilizada en teoría de la información para determinar el grado de independencia entre variables. La dependencia entre dos consultas se refleja en la puntuación SI(q, q), donde las puntuaciones mayores que cero indican que es probable que exista una dependencia, lo que interpretamos como que las consultas son probablemente similares: SI(q, q) = log p(q, q) / p(q)p(q). Estimamos la probabilidad de un tema p(q) utilizando el número de documentos relevantes para la consulta q dentro de la colección. La probabilidad conjunta p(q, q) se estima de manera similar, utilizando la concatenación de q y q como una consulta. Para obtener p(q|q), luego establecemos w(q, q) = SI(q, q) cuando SI(q, q) > 0, de lo contrario w(q, q) = 0, ya que solo estamos interesados en incluir consultas que sean similares. La estadística de log-verosimilitud proporciona otra medida de dependencia, que es más confiable que la medida de información mutua puntual [17]. Sea k1 el número de co-ocurrencias de q y q, k2 el número de ocurrencias de q que no co-ocurren con q, n1 el número total de ocurrencias de q, y n2 el número total de tokens de tema menos el número de ocurrencias de q. Entonces, sea p1 = k1/n1, p2 = k2/n2 y p = (k1 + k2)/(n1 + n2), (q, q ) = 2( (p1, k1, n1) + (p2, k2, n2) − (p, k1, n1) − (p, k2, n2)), donde (p, n, k) = k log p + (n − k) log(1 − p). El puntaje más alto indica que las consultas también son probablemente similares, por lo tanto, establecemos w(q, q ) = (q, q ). Finalmente, también estimamos la similitud de dos temas basándonos en su distancia dentro de la jerarquía de temas. La jerarquía de temas se visualiza como un grafo dirigido, y para todos los pares de temas se calcula el camino más corto SP(q, q). Establecimos el puntaje de similitud como el recíproco del camino más corto: w(q, q ) = 1/SP(q, q ). 7.2 Información contextual Dada la jerarquía de una organización, las unidades a las que pertenece una persona se consideran como un contexto para compensar la escasez de datos. Lo modelamos de la siguiente manera: p (q|ca) = 1 − P ou∈OU(ca) λou · p(q|ca) + P ou∈OU(ca) λou · p(q|ou), donde OU(ca) es el conjunto de unidades organizativas de las cuales el candidato ca es miembro, y p(q|o) expresa la fuerza de la asociación entre la consulta q y la unidad ou. La última probabilidad se puede estimar utilizando cualquiera de los tres modelos básicos, simplemente reemplazando ca con ou en las ecuaciones correspondientes. Una unidad organizativa está asociada con todos los documentos que sus miembros han escrito. Es decir, p(d|ou) = maxca∈ou p(d|ca). 7.3 Un modelo multilingüe simple Para institutos de conocimiento en Europa, ya sea académicos u otros, un entorno multilingüe (o al menos bilingüe) es típico. El siguiente modelo se basa en un tipo de suposición de independencia: no hay transferencia de conocimientos/perfiles a través de las barreras del idioma. Si bien es una simplificación, esta es una aproximación sensata en primer lugar. Eso es: p (q|ca) =P l∈L λl · p(ql|ca), donde L es el conjunto de idiomas utilizados en la colección, ql es la traducción de la consulta q al idioma l, y λl es un parámetro de suavizado específico del idioma, tal que P l∈L λl = 1. 8. MODELOS AVANZADOS: EVALUACIÓN En esta sección presentamos una evaluación experimental de nuestros modelos avanzados. Búsqueda de expertos Perfilado de expertos Modelo de lenguaje 1 Modelo 2 Modelo 3 Modelo 1 Modelo 2 Modelo 3 %q MAP MRR %q MAP MRR %q MAP MRR %ca MAP MRR %ca MAP MRR %ca MAP MRR Solo inglés 97.8 0.237 0.372 98.6 0.280 0.441 98.5 0.166 0.293 100 0.199 0.387 88.7 0.281 0.525 90.9 0.169 0.329 Solo holandés 61.3 0.249 0.401 62.6 0.265 0.436 62.6 0.195 0.344 91.9 0.164 0.426 90.1 0.195 0.488 91.9 0.125 0.328 Combinación 99.4 0.297 0.444 99.7 0.324 0.491 99.7 0.223 0.388 100 0.241 0.445 92.1 0.313 0.564 93.2 0.224 0.411 Tabla 3: Rendimiento de la combinación de idiomas en las tareas de búsqueda y perfilado de expertos (en candidatos). Los mejores puntajes para cada modelo están en cursiva, los mejores puntajes absolutos para las tareas de búsqueda y perfilado de expertos están en negrita. Tabla 4: Rendimiento en las tareas de búsqueda de expertos (arriba) y perfilado (abajo), utilizando similitudes de áreas de conocimiento. Las carreras fueron evaluadas en los temas principales establecidos. Las mejores puntuaciones están en negrita. 8.1 Preguntas de investigación Nuestras preguntas siguen los refinamientos presentados en la sección anterior: ¿Explotar la similitud en el área de conocimiento mejora la efectividad? ¿Cuál de los diversos métodos para capturar las relaciones entre palabras es más efectivo? Además, ¿es útil nuestra forma de incorporar información contextual? ¿Para qué tareas? Y finalmente, ¿es nuestra forma simple de combinar las puntuaciones monolingües suficiente para obtener mejoras significativas? 8.2 Configuración experimental Dado que las autoevaluaciones también son escasas en nuestra colección, para poder medir las diferencias entre los diversos modelos, seleccionamos un subconjunto de temas y evaluamos (algunas de las) ejecuciones solo en este subconjunto. Este conjunto se denomina como temas principales, y consiste en temas que se encuentran en el nivel superior de la jerarquía temática. (Un tema principal tiene subtemas, pero no es un subtema de ningún otro tema). Este conjunto principal consiste en 132 temas en holandés y 119 en inglés. Los juicios de relevancia se limitaron al conjunto de temas principales, pero no se ampliaron con subtemas. 8.3 Explotando la similitud de áreas de conocimiento. La Tabla 4 presenta los resultados. Los cuatro métodos utilizados para estimar la similitud de áreas de conocimiento son la divergencia de KL (KLDIV) y PointLang. Modelo 1 Modelo 2 Modelo 3 MAP MRR MAP MRR MAP MRR Búsqueda de expertos UK TODOS 0.423 0.545 0.654 0.799 0.494 0.629 UK PRINCIPAL 0.500 0.621 0.704 0.834 0.587 0.699 NL TODOS 0.439 0.560 0.672 0.826 0.480 0.630 NL PRINCIPAL 0.440 0.584 0.645 0.816 0.515 0.655 Perfilado de expertos UK TODOS 0.240 0.640 0.306 0.778 0.223 0.616 UK PRINCIPAL 0.523 0.677 0.519 0.648 0.461 0.587 NL TODOS 0.203 0.716 0.254 0.770 0.183 0.627 NL PRINCIPAL 0.332 0.576 0.380 0.624 0.332 0.549 Tabla 5: Evaluación de los modelos de contexto en unidades organizativas. información mutua ponderada (PMI), log-verosimilitud (LL) y distancia dentro de la jerarquía de temas (HDIST). Logramos mejorar el resultado base en todos los casos, pero la mejora es más notable para la tarea de perfilado. Para ambas tareas, el método LL tuvo el mejor rendimiento. Los enfoques basados en el contenido tuvieron un rendimiento consistentemente mejor que HDIST. 8.4 Información contextual Una jerarquía de dos niveles de unidades organizativas (facultades e institutos) está disponible en la colección de Expertos de UvT. La unidad a la que pertenece una persona se utiliza como contexto para esa persona. Primero, evaluamos los modelos de las unidades organizativas, utilizando todos los temas (TODOS) y solo los temas principales (PRINCIPALES). Una unidad organizativa se considera relevante para un tema dado (o viceversa) si al menos un miembro de la unidad seleccionó el tema dado como área de especialización. La Tabla 5 informa sobre los resultados. En lo que respecta a la búsqueda de expertos, dado un tema, la unidad organizativa correspondiente puede ser identificada con alta precisión. Sin embargo, la tarea de perfilado de expertos muestra una imagen diferente: las puntuaciones son bajas y la tarea parece difícil. La explicación puede ser que los conceptos generales (es decir, nuestros temas principales) pueden pertenecer a varias unidades organizativas. Segundo, realizamos otra evaluación, donde combinamos los modelos contextuales con los modelos candidatos (para puntuar a los candidatos nuevamente). La tabla 6 informa sobre los resultados. Encontramos un impacto positivo de los modelos de contexto solo para la búsqueda de expertos. Notablemente, para la búsqueda de expertos (y Modelo 1), mejora más del 50% (para inglés) y más del 70% (para holandés) en el MAP. El bajo rendimiento en la creación de perfiles de expertos puede deberse al hecho de que los modelos de contexto por sí solos no tuvieron un buen desempeño en la tarea de creación de perfiles desde el principio. 8.5 Modelos multilingües En esta subsección evaluamos el método para combinar resultados en múltiples idiomas que describimos en la Sección 7.3. En nuestro entorno, el conjunto de idiomas consiste en inglés y holandés: L = {UK, NL}. Los pesos de estos idiomas se establecieron para ser idénticos (λUK = λNL = 0.5). Realizamos experimentos con diferentes configuraciones de λ, pero no observamos diferencias significativas en el rendimiento. La Tabla 3 informa sobre los resultados multilingües, donde el rendimiento se evalúa en el conjunto completo de temas. Los tres modelos tienen una diferencia significativa. Tabla 6: Rendimiento de los modelos de contexto (CT) en comparación con la línea base (BL). Las mejores puntuaciones están en negrita, demostradas en todas las medidas para ambas tareas. La cobertura de temas y candidatos para las tareas de búsqueda y perfilado de expertos, respectivamente, es cercana al 100% en todos los casos. La mejora relativa de las puntuaciones de precisión varía del 10% al 80%. Estas puntuaciones demuestran que, a pesar de su simplicidad, nuestro método para combinar resultados en varios idiomas logra mejoras sustanciales sobre el punto de referencia. CONCLUSIONES En este artículo nos enfocamos en la recuperación de expertos (búsqueda y perfilado de expertos) en un nuevo entorno de una organización típica intensiva en conocimiento en la que los datos disponibles son de alta calidad, multilingües y abarcan una amplia gama de áreas de expertise. Normalmente, la cantidad de datos disponibles en una organización como una universidad, un instituto de investigación o un laboratorio de investigación es limitada en comparación con la colección de la W3C que ha sido utilizada principalmente para la evaluación experimental de la recuperación de la experiencia hasta ahora. Para examinar la recuperación de la experiencia en este entorno, presentamos (y lanzamos) la colección de Expertos de UvT como un caso representativo de organizaciones intensivas en conocimiento. La nueva colección refleja las propiedades típicas de los institutos intensivos en conocimiento mencionados anteriormente e incluye también varias características que pueden ser potencialmente útiles para la recuperación de expertos, como la estructura temática y organizativa. Evaluamos cómo los modelos de vanguardia para la búsqueda y perfilado de expertos se desempeñaron en este nuevo entorno y luego refinamos estos modelos para intentar explotar las diferentes características dentro del entorno de datos (idioma, actualidad y estructura organizativa). Encontramos que los modelos actuales de recuperación de experiencia se generalizan bien a este nuevo entorno; además, descubrimos que refinar los modelos para tener en cuenta las diferencias resulta en mejoras significativas, compensando así los problemas causados por la escasez de datos. El trabajo futuro incluye establecer evaluaciones manuales de perfiles generados automáticamente por los propios empleados, especialmente en casos en los que los empleados no han proporcionado un perfil ellos mismos. 10. AGRADECIMIENTOS Krisztian Balog fue apoyado por la Organización Neerlandesa para la Investigación Científica (NWO) bajo el número de proyecto 220-80-001. Maarten de Rijke también recibió apoyo de NWO bajo los números de proyecto 017.001.190, 220-80-001, 264-70-050, 354-20-005, 600.065.120, 612-13-001, 612.000.106, 612.066.302, 612.069.006, 640.001.501, 640.002.501, y de la Unión Europea. Programa IST del 6º Programa Marco para la I+D en virtud del contrato del proyecto MultiMATCH IST-033104. El trabajo de Toine Bogers y Antal van den Bosch fue financiado por el programa IOP-MMI de SenterNovem / Ministerio de Asuntos Económicos de los Países Bajos, como parte del proyecto 'A Propos'. 11. REFERENCIAS [1] L. Azzopardi. Incorporando contexto en el marco de modelado del lenguaje para la recuperación de información ad-hoc. Tesis doctoral, Universidad de Paisley, 2005. [2] K. Balog y M. de Rijke. Encontrando expertos similares. En este volumen, 2007. [3] K. Balog y M. de Rijke. Determinación de perfiles de expertos (con una aplicación a la búsqueda de expertos). En IJCAI 07: Proc. 20th Intern. Conferencia Conjunta sobre Inteligencia Artificial, páginas 2657-2662, 2007. [4] K. Balog, L. Azzopardi y M. de Rijke. Modelos formales para la búsqueda de expertos en corporaciones empresariales. En SIGIR 06: Actas de la 29ª conferencia internacional anual. Conferencia ACM SIGIR sobre Investigación y desarrollo en recuperación de información, páginas 43-50, 2006. [5] I. Becerra-Fernández. El papel de las tecnologías de inteligencia artificial en la implementación de sistemas de gestión del conocimiento de localización de personas. En el taller de AAAI sobre la incorporación de conocimiento en los procesos de negocio, marzo de 2000. [6] C. S. Campbell, P. P. Maglio, A. Cozzi y B. Dom. Identificación de la experiencia utilizando comunicaciones por correo electrónico. En CIKM 03: Actas de la duodécima conferencia internacional sobre gestión de la información y el conocimiento, páginas 528-531, 2003. [7] G. Cao, J.-Y. Nie, y J. Bai. Integrando relaciones entre palabras en modelos de lenguaje. En SIGIR 05: Actas de la 28ª conferencia internacional anual. Conferencia ACM SIGIR sobre Investigación y desarrollo en recuperación de información, páginas 298-305, 2005. [8] T. M. Cover y J. A. Thomas. Elementos de la teoría de la información. Wiley-Interscience, 1991. [9] N. Craswell, D. Hawking, A. M. Vercoustre y P. Wilkins. Experto en P@noptic: Buscando expertos no solo documentos. En Ausweb, 2001. [10] N. Craswell, A. de Vries e I. Soboroff. Resumen de la pista empresarial TREC2005. En la Decimocuarta Conferencia de Recuperación de Información. Proc. (TREC 2005), 2006. [11] T. H. Davenport y L. Prusak. Conocimiento en Acción: Cómo las Organizaciones Gestionan lo que Saben. Harvard Business School Press, Boston, MA, 1998. [12] T. Dunning.\nHarvard Business School Press, Boston, MA, 1998. [12] T. Dunning. Métodos precisos para la estadística de sorpresa y coincidencia. Lingüística Computacional, 19(1):61-74, 1993. [13] E. Filatova y J. Prager. Dime qué haces y te diré quién eres: Aprendiendo actividades relacionadas con la ocupación para biografías. En HLT/EMNLP, 2005. [14] V. Lavrenko y W. B. Croft. Modelos de lenguaje basados en relevancia. En SIGIR 01: Actas de la 24ª conferencia internacional anual. Conferencia ACM SIGIR sobre Investigación y Desarrollo en Recuperación de Información, páginas 120-127, 2001. [15] V. Lavrenko, M. Choquette y W. B. Croft. Modelos de relevancia multilingües. En SIGIR 02: Actas de la 25ª conferencia internacional anual. Conferencia ACM SIGIR sobre Investigación y Desarrollo en Recuperación de Información, páginas 175-182, 2002. [16] C. Macdonald e I. Ounis. Votación de candidatos: adaptando técnicas de fusión de datos para una tarea de búsqueda de expertos. En CIKM 06: Actas de la 15ª conferencia internacional de ACM sobre gestión de la información y el conocimiento, páginas 387-396, 2006. [17] C. Manning y H. Schütze. Fundamentos del Procesamiento del Lenguaje Natural Estadístico. El MIT Press, 1999. [18] A. Mockus y J. D. Herbsleb. Explorador de experticia: un enfoque cuantitativo para identificar la experticia. En ICSE 02: Proc. 24th Intern. Conf. en Ingeniería de Software, páginas 503-512, 2002. [19] D. Petkova y W. B. Croft. Modelos de lenguaje jerárquicos para la búsqueda de expertos en corporaciones empresariales. En Proc. ICTAI 2006, páginas 599-608, 2006. [20] I. Soboroff, A. de Vries y N. Craswell. Resumen de la pista empresarial TREC 2006. En las Notas de Trabajo de TREC 2006, 2006. [21] T. Tao, X. Wang, Q. Mei y C. Zhai. Recuperación de información del modelo de lenguaje con expansión de documentos. En HLT-NAACL 2006, 2006. [22] TREC. Ruta empresarial, 2005. URL: http://www.ins.cwi.nl/projects/trec-ent/wiki/. [23] G. van Noord.\nURL: http://www.ins.cwi.nl/projects/trec-ent/wiki/. [23] G. van Noord. Adivinador de idioma TextCat. URL: http://www.let.rug.nl/˜vannoord/TextCat/. [24] W3C. La colección de pruebas del W3C, 2005. URL: http://research.microsoft.com/users/nickcr/w3c-summary.html. ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "organizational structure": {
            "translated_key": "estructura organizativa",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Broad Expertise Retrieval in Sparse Data Environments Krisztian Balog ISLA, University of Amsterdam Kruislaan 403, 1098 SJ Amsterdam, The Netherlands kbalog@science.uva.nl Toine Bogers ILK, Tilburg University P.O.",
                "Box 90153, 5000 LE Tilburg, The Netherlands A.M.Bogers@uvt.nl Leif Azzopardi Dept. of Computing Science University of Glasgow, Glasgow, G12 8QQ leif@dcs.gla.ac.uk Maarten de Rijke ISLA, University of Amsterdam Kruislaan 403, 1098 SJ Amsterdam, The Netherlands mdr@science.uva.nl Antal van den Bosch ILK, Tilburg University P.O.",
                "Box 90153, 5000 LE Tilburg, The Netherlands Antal.vdnBosch@uvt.nl ABSTRACT Expertise retrieval has been largely unexplored on data other than the W3C collection.",
                "At the same time, many intranets of universities and other knowledge-intensive organisations offer examples of relatively small but clean multilingual expertise data, covering broad ranges of expertise areas.",
                "We first present two main expertise retrieval tasks, along with a set of baseline approaches based on generative language modeling, aimed at finding expertise relations between topics and people.",
                "For our experimental evaluation, we introduce (and release) a new test set based on a crawl of a university site.",
                "Using this test set, we conduct two series of experiments.",
                "The first is aimed at determining the effectiveness of baseline expertise retrieval methods applied to the new test set.",
                "The second is aimed at assessing refined models that exploit characteristic features of the new test set, such as the <br>organizational structure</br> of the university, and the hierarchical structure of the topics in the test set.",
                "Expertise retrieval models are shown to be robust with respect to environments smaller than the W3C collection, and current techniques appear to be generalizable to other settings.",
                "Categories and Subject Descriptors H.3 [Information Storage and Retrieval]: H.3.1 Content Analysis and Indexing; H.3.3 Information Search and Retrieval; H.3.4 Systems and Software; H.4 [Information Systems Applications]: H.4.2 Types of Systems; H.4.m Miscellaneous General Terms Algorithms, Measurement, Performance, Experimentation 1.",
                "INTRODUCTION An organizations intranet provides a means for exchanging information between employees and for facilitating employee collaborations.",
                "To efficiently and effectively achieve this, it is necessary to provide search facilities that enable employees not only to access documents, but also to identify expert colleagues.",
                "At the TREC Enterprise Track [22] the need to study and understand expertise retrieval has been recognized through the introduction of Expert Finding tasks.",
                "The goal of expert finding is to identify a list of people who are knowledgeable about a given topic.",
                "This task is usually addressed by uncovering associations between people and topics [10]; commonly, a co-occurrence of the name of a person with topics in the same context is assumed to be evidence of expertise.",
                "An alternative task, which using the same idea of people-topic associations, is expert profiling, where the task is to return a list of topics that a person is knowledgeable about [3].",
                "The launch of the Expert Finding task at TREC has generated a lot of interest in expertise retrieval, with rapid progress being made in terms of modeling, algorithms, and evaluation aspects.",
                "However, nearly all of the expert finding or profiling work performed has been validated experimentally using the W3C collection [24] from the Enterprise Track.",
                "While this collection is currently the only publicly available test collection for expertise retrieval tasks, it only represents one type of intranet.",
                "With only one test collection it is not possible to generalize conclusions to other realistic settings.",
                "In this paper we focus on expertise retrieval in a realistic setting that differs from the W3C setting-one in which relatively small amounts of clean, multilingual data are available, that cover a broad range of expertise areas, as can be found on the intranets of universities and other knowledge-intensive organizations.",
                "Typically, this setting features several additional types of structure: topical structure (e.g., topic hierarchies as employed by the organization), <br>organizational structure</br> (faculty, department, ...), as well as multiple types of documents (research and course descriptions, publications, and academic homepages).",
                "This setting is quite different from the W3C setting in ways that might impact upon the performance of expertise retrieval tasks.",
                "We focus on a number of research questions in this paper: Does the relatively small amount of data available on an intranet affect the quality of the topic-person associations that lie at the heart of expertise retrieval algorithms?",
                "How do state-of-the-art algorithms developed on the W3C data set perform in the alternative scenario of the type described above?",
                "More generally, do the lessons from the Expert Finding task at TREC carry over to this setting?",
                "How does the inclusion or exclusion of different documents affect expertise retrieval tasks?",
                "In addition to, how can the topical and <br>organizational structure</br> be used for retrieval purposes?",
                "To answer our research questions, we first present a set of baseline approaches, based on generative language modeling, aimed at finding associations between topics and people.",
                "This allows us to formulate the expert finding and expert profiling tasks in a uniform way, and has the added benefit of allowing us to understand the relations between the two tasks.",
                "For our experimental evaluation, we introduce a new data set (the UvT Expert Collection) which is representative of the type of intranet that we described above.",
                "Our collection is based on publicly available data, crawled from the website of Tilburg University (UvT).",
                "This type of data is particularly interesting, since (1) it is clean, heterogeneous, structured, and focused, but comprises a limited number of documents; (2) contains information on the organizational hierarchy; (3) it is bilingual (English and Dutch); and (4) the list of expertise areas of an individual are provided by the employees themselves.",
                "Using the UvT Expert collection, we conduct two sets of experiments.",
                "The first is aimed at determining the effectiveness of baseline expertise finding and profiling methods in this new setting.",
                "A second group of experiments is aimed at extensions of the baseline methods that exploit characteristic features of the UvT Expert Collection; specifically, we propose and evaluate refined expert finding and profiling methods that incorporate topicality and <br>organizational structure</br>.",
                "Apart from the research questions and data set that we contribute, our main contributions are as follows.",
                "The baseline models developed for expertise finding perform well on the new data set.",
                "While on the W3C setting the expert finding task appears to be more difficult than profiling, for the UvT data the opposite is the case.",
                "We find that profiling on the UvT data set is considerably more difficult than on the W3C set, which we believe is due to the large (but realistic) number of topical areas that we used for profiling: about 1,500 for the UvT set, versus 50 in the W3C case.",
                "Taking the similarity between topics into account can significantly improve retrieval performance.",
                "The best performing similarity measures are content-based, therefore they can be applied on the W3C (and other) settings as well.",
                "Finally, we demonstrate that the <br>organizational structure</br> can be exploited in the form of a context model, improving MAP scores for certain models by up to 70%.",
                "The remainder of this paper is organized as follows.",
                "In the next section we review related work.",
                "Then, in Section 3 we provide detailed descriptions of the expertise retrieval tasks that we address in this paper: expert finding and expert profiling.",
                "In Section 4 we present our baseline models, of which the performance is then assessed in Section 6 using the UvT data set that we introduce in Section 5.",
                "Advanced models exploiting specific features of our data are presented in Section 7 and evaluated in Section 8.",
                "We formulate our conclusions in Section 9. 2.",
                "RELATED WORK Initial approaches to expertise finding often employed databases containing information on the skills and knowledge of each individual in the organization [11].",
                "Most of these tools (usually called yellow pages or people-finding systems) rely on people to self-assess their skills against a predefined set of keywords.",
                "For updating profiles in these systems in an automatic fashion there is a need for intelligent technologies [5].",
                "More recent approaches use specific document sets (such as email [6] or software [18]) to find expertise.",
                "In contrast with focusing on particular document types, there is also an increased interest in the development of systems that index and mine published intranet documents as sources of evidence for expertise.",
                "One such published approach is the P@noptic system [9], which builds a representation of each person by concatenating all documents associated with that person-this is similar to Model 1 of Balog et al. [4], who formalize and compare two methods.",
                "Balog et al.s Model 1 directly models the knowledge of an expert from associated documents, while their Model 2 first locates documents on the topic and then finds the associated experts.",
                "In the reported experiments the second method performs significantly better when there are sufficiently many associated documents per candidate.",
                "Most systems that took part in the 2005 and 2006 editions of the Expert Finding task at TREC implemented (variations on) one of these two models; see [10, 20].",
                "Macdonald and Ounis [16] propose a different approach for ranking candidate expertise with respect to a topic based on data fusion techniques, without using collectionspecific heuristics; they find that applying field-based weighting models improves the ranking of candidates.",
                "Petkova and Croft [19] propose yet another approach, based on a combination of the above Model 1 and 2, explicitly modeling topics.",
                "Turning to other expert retrieval tasks that can also be addressed using topic-people associations, Balog and de Rijke [3] addressed the task of determining topical expert profiles.",
                "While their methods proved to be efficient on the W3C corpus, they require an amount of data that may not be available in the typical knowledge-intensive organization.",
                "Balog and de Rijke [2] study the related task of finding experts that are similar to a small set of experts given as input.",
                "As an aside, creating a textual summary of a person shows some similarities to biography finding, which has received a considerable amount of attention recently; see e.g., [13].",
                "We use generative language modeling to find associations between topics and people.",
                "In our modeling of expert finding and profiling we collect evidence for expertise from multiple sources, in a heterogeneous collection, and integrate it with the co-occurrence of candidates names and query terms-the language modeling setting allows us to do this in a transparent manner.",
                "Our modeling proceeds in two steps.",
                "In the first step, we consider three baseline models, two taken from [4] (the Models 1 and 2 mentioned above), and one a refined version of a model introduced in [3] (which we refer to as Model 3 below); this third model is also similar to the model described by Petkova and Croft [19].",
                "The models we consider in our second round of experiments are mixture models similar to contextual language models [1] and to the expanded documents of Tao et al. [21]; however, the features that we use for definining our expansions-including topical structure and <br>organizational structure</br>-have not been used in this way before. 3.",
                "TASKS In the expertise retrieval scenario that we envisage, users seeking expertise within an organization have access to an interface that combines a search box (where they can search for experts or topics) with navigational structures (of experts and of topics) that allows them to click their way to an expert page (providing the profile of a person) or a topic page (providing a list of experts on the topic).",
                "To feed the above interface, we face two expertise retrieval tasks, expert finding and expert profiling, that we first define and then formalize using generative language models.",
                "In order to model either task, the probability of the query topic being associated to a candidate expert plays a key role in the final estimates for searching and profiling.",
                "By using language models, both the candidates and the query are characterized by distributions of terms in the vocabulary (used in the documents made available by the organization whose expertise retrieval needs we are addressing). 3.1 Expert finding Expert finding involves the task of finding the right person with the appropriate skills and knowledge: Who are the experts on topic X?.",
                "E.g., an employee wants to ascertain who worked on a particular project to find out why particular decisions were made without having to trawl through documentation (if there is any).",
                "Or, they may be in need a trained specialist for consultancy on a specific problem.",
                "Within an organization there are usually many possible candidates who could be experts for given topic.",
                "We can state this problem as follows: What is the probability of a candidate ca being an expert given the query topic q?",
                "That is, we determine p(ca|q), and rank candidates ca according to this probability.",
                "The candidates with the highest probability given the query are deemed the most likely experts for that topic.",
                "The challenge is how to estimate this probability accurately.",
                "Since the query is likely to consist of only a few terms to describe the expertise required, we should be able to obtain a more accurate estimate by invoking Bayes Theorem, and estimating: p(ca|q) = p(q|ca)p(ca) p(q) , (1) where p(ca) is the probability of a candidate and p(q) is the probability of a query.",
                "Since p(q) is a constant, it can be ignored for ranking purposes.",
                "Thus, the probability of a candidate ca being an expert given the query q is proportional to the probability of a query given the candidate p(q|ca), weighted by the a priori belief p(ca) that candidate ca is an expert. p(ca|q) ∝ p(q|ca)p(ca) (2) In this paper our main focus is on estimating the probability of a query given the candidate p(q|ca), because this probability captures the extent to which the candidate knows about the query topic.",
                "Whereas the candidate priors are generally assumed to be uniformand thus will not influence the ranking-it has been demonstrated that a sensible choice of priors may improve the performance [20]. 3.2 Expert profiling While the task of expert searching was concerned with finding experts given a particular topic, the task of expert profiling seeks to answer a related question: What topics does a candidate know about?",
                "Essentially, this turns the questions of expert finding around.",
                "The profiling of an individual candidate involves the identification of areas of skills and knowledge that they have expertise about and an evaluation of the level of proficiency in each of these areas.",
                "This is the candidates topical profile.",
                "Generally, topical profiles within organizations consist of tabular structures which explicitly catalogue the skills and knowledge of each individual in the organization.",
                "However, such practice is limited by the resources available for defining, creating, maintaining, and updating these profiles over time.",
                "By focusing on automatic methods which draw upon the available evidence within the document repositories of an organization, our aim is to reduce the human effort associated with the maintenance of topical profiles1 .",
                "A topical profile of a candidate, then, is defined as a vector where each element i of the vector corresponds to the candidate cas expertise on a given topic ki, (i.e., s(ca, ki)).",
                "Each topic ki defines a particular knowledge area or skill that the organization uses to define the candidates topical profile.",
                "Thus, it is assumed that a list of topics, {k1, . . . , kn}, where n is the number of pre-defined topics, is given: profile(ca) = s(ca, k1), s(ca, k2), . . . , s(ca, kn) . (3) 1 Context and evidence are needed to help users of expertise finding systems to decide whom to contact when seeking expertise in a particular area.",
                "Examples of such context are: Who does she work with?",
                "What are her contact details?",
                "Is she well-connected, just in case she is not able to help us herself?",
                "What is her role in the organization?",
                "Who is her superior?",
                "Collaborators, and affiliations, etc. are all part of the candidates social profile, and can serve as a background against which the systems recommendations should be interpreted.",
                "In this paper we only address the problem of determining topical profiles, and leave social profiling to further work.",
                "We state the problem of quantifying the competence of a person on a certain knowledge area as follows: What is the probability of a knowledge area (ki) being part of the candidates (expertise) profile? where s(ca, ki) is defined by p(ki|ca).",
                "Our task, then, is to estimate p(ki|ca), which is equivalent to the problem of obtaining p(q|ca), where the topic ki is represented as a query topic q, i.e., a sequence of keywords representing the expertise required.",
                "Both the expert finding and profiling tasks rely on the accurate estimation of p(q|ca).",
                "The only difference derives from the prior probability that a person is an expert (p(ca)), which can be incorporated into the expert finding task.",
                "This prior does not apply to the profiling task since the candidate (individual) is fixed. 4.",
                "BASELINE MODELS In this section we describe our baseline models for estimating p(q|ca), i.e., associations between topics and people.",
                "Both expert finding and expert profiling boil down to this estimation.",
                "We employ three models for calculating this probability. 4.1 From topics to candidates Using Candidate Models: Model 1 Model 1 [4] defines the probability of a query given a candidate (p(q|ca)) using standard language modeling techniques, based on a multinomial unigram language model.",
                "For each candidate ca, a candidate language model θca is inferred such that the probability of a term given θca is nonzero for all terms, i.e., p(t|θca) > 0.",
                "From the candidate model the query is generated with the following probability: p(q|θca) = Y t∈q p(t|θca)n(t,q) , where each term t in the query q is sampled identically and independently, and n(t, q) is the number of times t occurs in q.",
                "The candidate language model is inferred as follows: (1) an empirical model p(t|ca) is computed; (2) it is smoothed with background probabilities.",
                "Using the associations between a candidate and a document, the probability p(t|ca) can be approximated by: p(t|ca) = X d p(t|d)p(d|ca), where p(d|ca) is the probability that candidate ca generates a supporting document d, and p(t|d) is the probability of a term t occurring in the document d. We use the maximum-likelihood estimate of a term, that is, the normalised frequency of the term t in document d. The strength of the association between document d and candidate ca expressed by p(d|ca) reflects the degree to which the candidates expertise is described using this document.",
                "The estimation of this probability is presented later, in Section 4.2.",
                "The candidate model is then constructed as a linear interpolation of p(t|ca) and the background model p(t) to ensure there are no zero probabilities, which results in the final estimation: p(q|θca) = (4) Y t∈q ( (1 − λ) X d p(t|d)p(d|ca) ! + λp(t) )n(t,q) .",
                "Model 1 amasses all the term information from all the documents associated with the candidate, and uses this to represent that candidate.",
                "This model is used to predict how likely a candidate would produce a query q.",
                "This can can be intuitively interpreted as the probability of this candidate talking about the query topic, where we assume that this is indicative of their expertise.",
                "Using Document Models: Model 2 Model 2 [4] takes a different approach.",
                "Here, the process is broken into two parts.",
                "Given a candidate ca, (1) a document that is associated with a candidate is selected with probability p(d|ca), and (2) from this document a query q is generated with probability p(q|d).",
                "Then the sum over all documents is taken to obtain p(q|ca), such that: p(q|ca) = X d p(q|d)p(d|ca). (5) The probability of a query given a document is estimated by inferring a document language model θd for each document d in a similar manner as the candidate model was inferred: p(t|θd) = (1 − λ)p(t|d) + λp(t), (6) where p(t|d) is the probability of the term in the document.",
                "The probability of a query given the document model is: p(q|θd) = Y t∈q p(t|θd)n(t,q) .",
                "The final estimate of p(q|ca) is obtained by substituting p(q|d) for p(q|θd) into Eq. 5 (see [4] for full details).",
                "Conceptually, Model 2 differs from Model 1 because the candidate is not directly modeled.",
                "Instead, the document acts like a hidden variable in the process which separates the query from the candidate.",
                "This process is akin to how a user may search for candidates with a standard search engine: initially by finding the documents which are relevant, and then seeing who is associated with that document.",
                "By examining a number of documents the user can obtain an idea of which candidates are more likely to discuss the topic q.",
                "Using Topic Models: Model 3 We introduce a third model, Model 3.",
                "Instead of attempting to model the query generation process via candidate or document models, we represent the query as a topic language model and directly estimate the probability of the candidate p(ca|q).",
                "This approach is similar to the model presented in [3, 19].",
                "As with the previous models, a language model is inferred, but this time for the query.",
                "We adapt the work of Lavrenko and Croft [14] to estimate a topic model from the query.",
                "The procedure is as follows.",
                "Given a collection of documents and a query topic q, it is assumed that there exists an unknown topic model θk that assigns probabilities p(t|θk) to the term occurrences in the topic documents.",
                "Both the query and the documents are samples from θk (as opposed to the previous approaches, where a query is assumed to be sampled from a specific document or candidate model).",
                "The main task is to estimate p(t|θk), the probability of a term given the topic model.",
                "Since the query q is very sparse, and as there are no examples of documents on the topic, this distribution needs to be approximated.",
                "Lavrenko and Croft [14] suggest a reasonable way of obtaining such an approximation, by assuming that p(t|θk) can be approximated by the probability of term t given the query q.",
                "We can then estimate p(t|q) using the joint probability of observing the term t together with the query terms, q1, . . . , qm, and dividing by the joint probability of the query terms: p(t|θk) ≈ p(t|q) = p(t, q1, . . . , qm) p(q1, . . . , qm) = p(t, q1, . . . , qm) P t ∈T p(t , q1, . . . , qm) , where p(q1, . . . , qm) = P t ∈T p(t , q1, . . . , qm), and T is the entire vocabulary of terms.",
                "In order to estimate the joint probability p(t, q1, . . . , qm), we follow [14, 15] and assume t and q1, . . . , qm are mutually independent, once we pick a source distribution from the set of underlying source distributions U.",
                "If we choose U to be a set of document models. then to construct this set, the query q would be issued against the collection, and the top n returned are assumed to be relevant to the topic, and thus treated as samples from the topic model. (Note that candidate models could be used instead.)",
                "With the document models forming U, the joint probability of term and query becomes: p(t, q1, . . . , qm) = X d∈U p(d) ˘ p(t|θd) mY i=1 p(qi|θd) ¯ . (7) Here, p(d) denotes the prior distribution over the set U, which reflects the relevance of the document to the topic.",
                "We assume that p(d) is uniform across U.",
                "In order to rank candidates according to the topic model defined, we use the Kullback-Leibler divergence metric (KL, [8]) to measure the difference between the candidate models and the topic model: KL(θk||θca) = X t p(t|θk) log p(t|θk) p(t|θca) . (8) Candidates with a smaller divergence from the topic model are considered to be more likely experts on that topic.",
                "The candidate model θca is defined in Eq. 4.",
                "By using KL divergence instead of the probability of a candidate given the topic model p(ca|θk), we avoid normalization problems. 4.2 Document-candidate associations For our models we need to be able to estimate the probability p(d|ca), which expresses the extent to which a document d characterizes the candidate ca.",
                "In [4], two methods are presented for estimating this probability, based on the number of person names recognized in a document.",
                "However, in our (intranet) setting it is reasonable to assume that authors of documents can unambiguously be identified (e.g., as the author of an article, the teacher assigned to a course, the owner of a web page, etc.)",
                "Hence, we set p(d|ca) to be 1 if candidate ca is author of document d, otherwise the probability is 0.",
                "In Section 6 we describe how authorship can be determined on different types of documents within the collection. 5.",
                "THE UVT EXPERT COLLECTION The UvT Expert collection used in the experiments in this paper fits the scenario outlined in Section 3.",
                "The collection is based on the Webwijs (Webwise) system developed at Tilburg University (UvT) in the Netherlands.",
                "Webwijs (http://www.uvt.nl/ webwijs/) is a publicly accessible database of UvT employees who are involved in research or teaching; currently, Webwijs contains information about 1168 experts, each of whom has a page with contact information and, if made available by the expert, a research description and publications list.",
                "In addition, each expert can select expertise areas from a list of 1491 topics and is encouraged to suggest new topics that need to be approved by the Webwijs editor.",
                "Each topic has a separate page that shows all experts associated with that topic and, if available, a list of related topics.",
                "Webwijs is available in Dutch and English, and this bilinguality has been preserved in the collection.",
                "Every Dutch Webwijs page has an English translation.",
                "Not all Dutch topics have an English translation, but the reverse is true: the 981 English topics all have a Dutch equivalent.",
                "About 42% of the experts teach courses at Tilburg University; these courses were also crawled and included in the profile.",
                "In addition, about 27% of the experts link to their academic homepage from their Webwijs page.",
                "These home pages were crawled and added to the collection. (This means that if experts put the full-text versions of their publications on their academic homepage, these were also available for indexing.)",
                "We also obtained 1880 full-text versions of publications from the UvT institutional repository and Dutch English no. of experts 1168 1168 no. of experts with ≥ 1 topic 743 727 no. of topics 1491 981 no. of expert-topic pairs 4318 3251 avg. no. of topics/expert 5.8 5.9 max. no. of topics/expert (no. of experts) 60 (1) 35 (1) min. no. of topics/expert (no. of experts) 1 (74) 1 (106) avg. no. of experts/topic 2.9 3.3 max. no. of experts/topic (no. of topics) 30 (1) 30 (1) min. no. of experts/topic (no. of topics) 1 (615) 1 (346) no. of experts with HP 318 318 no. of experts with CD 318 318 avg. no. of CDs per teaching expert 3.5 3.5 no. of experts with RD 329 313 no. of experts with PUB 734 734 avg. no. of PUBs per expert 27.0 27.0 avg. no. of PUB citations per expert 25.2 25.2 avg. no. of full-text PUBs per expert 1.8 1.8 Table 2: Descriptive statistics of the Dutch and English versions of the UvT Expert collection. converted them to plain text.",
                "We ran the TextCat [23] language identifier to classify the language of the home pages and the fulltext publications.",
                "We restricted ourselves to pages where the classifier was confident about the language used on the page.",
                "This resulted in four document types: research descriptions (RD), course descriptions (CD), publications (PUB; full-text and citationonly versions), and academic homepages (HP).",
                "Everything was bundled into the UvT Expert collection which is available at http: //ilk.uvt.nl/uvt-expert-collection/.",
                "The UvT Expert collection was extracted from a different organizational setting than the W3C collection and differs from it in a number of ways.",
                "The UvT setting is one with relatively small amounts of multilingual data.",
                "Document-author associations are clear and the data is structured and clean.",
                "The collection covers a broad range of expertise areas, as one can typically find on intranets of universities and other knowledge-intensive institutes.",
                "Additionally, our university setting features several types of structure (topical and organizational), as well as multiple document types.",
                "Another important difference between the two data sets is that the expertise areas in the UvT Expert collection are self-selected instead of being based on group membership or assignments by others.",
                "Size is another dimension along which the W3C and UvT Expert collections differ: the latter is the smaller of the two.",
                "Also realistic are the large differences in the amount of information available for each expert.",
                "Utilizing Webwijs is voluntary; 425 Dutch experts did not select any topics at all.",
                "This leaves us with 743 Dutch and 727 English usable expert profiles.",
                "Table 2 provides descriptive statistics for the UvT Expert collection.",
                "Universities tend to have a hierarchical structure that goes from the faculty level, to departments, research groups, down to the individual researchers.",
                "In the UvT Expert collection we have information about the affiliations of researchers with faculties and institutes, providing us with a two-level organizational hierarchy.",
                "Tilburg University has 22 organizational units at the faculty level (including the university office and several research institutes) and 71 departments, which amounts to 3.2 departments per faculty.",
                "As to the topical hierarchy used by Webwijs, 131 of the 1491 topics are top nodes in the hierarchy.",
                "This hierarchy has an average topic chain length of 2.65 and a maximum length of 7 topics. 6.",
                "EVALUATION Below, we evaluate Section 4s models for expert finding and profiling onthe UvT Expert collection.",
                "We detail our research questions and experimental setup, and then present our results. 6.1 Research Questions We address the following research questions.",
                "Both expert finding and profiling rely on the estimations of p(q|ca).",
                "The question is how the models compare on the different tasks, and in the setting of the UvT Expert collection.",
                "In [4], Model 2 outperformed Model 1 on the W3C collection.",
                "How do they compare on our data set?",
                "And how does Model 3 compare to Model 1?",
                "What about performance differences between the two languages in our test collection? 6.2 Experimental Setup The output of our models was evaluated against the self-assigned topic labels, which were treated as relevance judgements.",
                "Results were evaluated separately for English and Dutch.",
                "For English we only used topics for which the Dutch translation was available; for Dutch all topics were considered.",
                "The results were averaged for the queries in the intersection of relevance judgements and results; missing queries do not contribute a value of 0 to the scores.",
                "We use standard information retrieval measures, such as Mean Average Precision (MAP) and Mean Reciprocal Rank (MRR).",
                "We also report the percentage of topics (%q) and candidates (%ca) covered, for the expert finding and profiling tasks, respectively. 6.3 Results Table 1 shows the performance of Model 1, 2, and 3 on the expert finding and profiling tasks.",
                "The rows of the table correspond to the various document types (RD, CD, PUB, and HP) and to their combinations.",
                "RD+CD+PUB+HP is equivalent to the full collection and will be referred as the BASELINE of our experiments.",
                "Looking at Table 1 we see that Model 2 performs the best across the board.",
                "However, when the data is clean and very focused (RD), Model 3 outperforms it in a number of cases.",
                "Model 1 has the best coverage of candidates (%ca) and topics (%q).",
                "The various document types differ in their characteristics and how they improve the finding and profiling tasks.",
                "Expert profiling benefits much from the clean data present in the RD and CD document types, while the publications contribute the most to the expert finding task.",
                "Adding the homepages does not prove to be particularly useful.",
                "When we compare the results across languages, we find that the coverage of English topics (%q) is higher than of the Dutch ones for expert finding.",
                "Apart from that, the scores fall in the same range for both languages.",
                "For the profiling task the coverage of the candidates (%ca) is very similar for both languages.",
                "However, the performance is substantially better for the English topics.",
                "While it is hard to compare scores across collections, we conclude with a brief comparison of the absolute scores in Table 1 to those reported in [3, 4] on the W3C test set (2005 edition).",
                "For expert finding the MAP scores for Model 2 reported here are about 50% higher than the corresponding figures in [4], while our MRR scores are slightly below those in [4].",
                "For expert profiling, the differences are far more dramatic: the MAP scores for Model 2 reported here are around 50% below the scores in [3], while the (best) MRR scores are about the same as those in [3].",
                "The cause for the latter differences seems to reside in the number of knowledge areas considered here-approx. 30 times more than in the W3C setting. 7.",
                "ADVANCED MODELS Now that we have developed and assessed basic language modeling techniques for expertise retrieval, we turn to refined models that exploit special features of our test collection. 7.1 Exploiting knowledge area similarity One way to improve the scoring of a query given a candidate is to consider what other requests the candidate would satisfy and use them as further evidence to support the original query, proportional Expert finding Expert profiling Document types Model 1 Model 2 Model 3 Model 1 Model 2 Model 3 %q MAP MRR %q MAP MRR %q MAP MRR %ca MAP MRR %ca MAP MRR %ca MAP MRR English RD 97.8 0.126 0.269 83.5 0.144 0.311 83.3 0.129 0.271 100 0.089 0.189 39.3 0.232 0.465 41.1 0.166 0.337 CD 97.8 0.118 0.227 91.7 0.123 0.248 91.7 0.118 0.226 32.8 0.188 0.381 32.4 0.195 0.385 32.7 0.203 0.370 PUB 97.8 0.200 0.330 98.0 0.216 0.372 98.0 0.145 0.257 78.9 0.167 0.364 74.5 0.212 0.442 78.9 0.135 0.299 HP 97.8 0.081 0.186 97.4 0.071 0.168 97.2 0.062 0.149 31.2 0.150 0.299 28.8 0.185 0.335 30.1 0.136 0.287 RD+CD 97.8 0.188 0.352 92.9 0.193 0.360 92.9 0.150 0.273 100 0.145 0.286 61.3 0.251 0.477 63.2 0.217 0.416 RD+CD+PUB 97.8 0.235 0.373 98.1 0.277 0.439 98.1 0.178 0.305 100 0.196 0.380 87.2 0.280 0.533 89.5 0.170 0.344 RD+CD+PUB+HP 97.8 0.237 0.372 98.6 0.280 0.441 98.5 0.166 0.293 100 0.199 0.387 88.7 0.281 0.525 90.9 0.169 0.329 Dutch RD 61.3 0.094 0.229 38.4 0.137 0.336 38.3 0.127 0.295 38.0 0.127 0.386 34.1 0.138 0.420 38.0 0.105 0.327 CD 61.3 0.107 0.212 49.7 0.128 0.256 49.7 0.136 0.261 32.5 0.151 0.389 31.8 0.158 0.396 32.5 0.170 0.380 PUB 61.3 0.193 0.319 59.5 0.218 0.368 59.4 0.173 0.291 78.8 0.126 0.364 76.0 0.150 0.424 78.8 0.103 0.294 HP 61.3 0.063 0.169 56.6 0.064 0.175 56.4 0.062 0.163 29.8 0.108 0.308 27.8 0.125 0.338 29.8 0.098 0.255 RD+CD 61.3 0.159 0.314 51.9 0.184 0.360 51.9 0.169 0.324 60.5 0.151 0.410 57.2 0.166 0.431 60.4 0.159 0.384 RD+CD+PUB 61.3 0.244 0.398 61.5 0.260 0.424 61.4 0.210 0.350 90.3 0.165 0.445 88.2 0.189 0.479 90.3 0.126 0.339 RD+CD+PUB+HP 61.3 0.249 0.401 62.6 0.265 0.436 62.6 0.195 0.344 91.9 0.164 0.426 90.1 0.195 0.488 91.9 0.125 0.328 Table 1: Performance of the models on the expert finding and profiling tasks, using different document types and their combinations. %q is the number of topics covered (applies to the expert finding task), %ca is the number of candidates covered (applies to the expert profiling task).",
                "The top and bottom blocks correspond to English and Dutch respectively.",
                "The best scores are in boldface. to how related the other requests are to the original query.",
                "This can be modeled by interpolating between the p(q|ca) and the further supporting evidence from all similar requests q , as follows: p (q|ca) = λp(q|ca) + (1 − λ) X q p(q|q )p(q |ca), (9) where p(q|q ) represents the similarity between the two topics q and q .",
                "To be able to work with similarity methods that are not necessarily probabilities, we set p(q|q ) = w(q,q ) γ , where γ is a normalizing constant, such that γ = P q w(q , q ).",
                "We consider four methods for calculating the similarity score between two topics.",
                "Three approaches are strictly content-based, and establish similarity by examining co-occurrence patterns of topics within the collection, while the last approach exploits the hierarchical structure of topical areas that may be present within an organization (see [7] for further examples of integrating word relationships into language models).",
                "The Kullback-Leibler (KL) divergence metric defined in Eq. 8 provides a measure of how different or similar two probability distributions are.",
                "A topic model is inferred for q and q using the method presented in Section 4.1 to describe the query across the entire vocabulary.",
                "Since a lower KL score means the queries are more similar, we let w(q, q ) = max(KL(θq||·) − KL(θq||θq )).",
                "Pointwise Mutual Information (PMI, [17]) is a measure of association used in information theory to determine the extent of independence between variables.",
                "The dependence between two queries is reflected by the SI(q, q ) score, where scores greater than zero indicate that it is likely that there is a dependence, which we take to mean that the queries are likely to be similar: SI(q, q ) = log p(q, q ) p(q)p(q ) (10) We estimate the probability of a topic p(q) using the number of documents relevant to query q within the collection.",
                "The joint probability p(q, q ) is estimated similarly, by using the concatenation of q and q as a query.",
                "To obtain p(q|q ), we then set w(q, q ) = SI(q, q ) when SI(q, q ) > 0 otherwise w(q, q ) = 0, because we are only interested in including queries that are similar.",
                "The log-likelihood statistic provides another measure of dependence, which is more reliable than the pointwise mutual information measure [17].",
                "Let k1 be the number of co-occurrences of q and q , k2 the number of occurrences of q not co-occurring with q , n1 the total number of occurrences of q , and n2 the total number of topic tokens minus the number of occurrences of q .",
                "Then, let p1 = k1/n1, p2 = k2/n2, and p = (k1 + k2)/(n1 + n2), (q, q ) = 2( (p1, k1, n1) + (p2, k2, n2) − (p, k1, n1) − (p, k2, n2)), where (p, n, k) = k log p + (n − k) log(1 − p).",
                "The higher score indicate that queries are also likely to be similar, thus we set w(q, q ) = (q, q ).",
                "Finally, we also estimate the similarity of two topics based on their distance within the topic hierarchy.",
                "The topic hierarchy is viewed as a directed graph, and for all topic-pairs the shortest path SP(q, q ) is calculated.",
                "We set the similarity score to be the reciprocal of the shortest path: w(q, q ) = 1/SP(q, q ). 7.2 Contextual information Given the hierarchy of an organization, the units to which a person belong are regarded as a context so as to compensate for data sparseness.",
                "We model it as follows: p (q|ca) = 1 − P ou∈OU(ca) λou · p(q|ca) + P ou∈OU(ca) λou · p(q|ou), where OU(ca) is the set of organizational units of which candidate ca is a member of, and p(q|o) expresses the strength of the association between query q and the unit ou.",
                "The latter probability can be estimated using either of the three basic models, by simply replacing ca with ou in the corresponding equations.",
                "An organizational unit is associated with all the documents that its members have authored.",
                "That is, p(d|ou) = maxca∈ou p(d|ca). 7.3 A simple multilingual model For knowledge institutes in Europe, academic or otherwise, a multilingual (or at least bilingual) setting is typical.",
                "The following model builds on a kind of independence assumption: there is no spill-over of expertise/profiles across language boundaries.",
                "While a simplification, this is a sensible first approach.",
                "That is: p (q|ca) =P l∈L λl · p(ql|ca), where L is the set of languages used in the collection, ql is the translation of the query q to language l, and λl is a language specific smoothing parameter, such that P l∈L λl = 1. 8.",
                "ADVANCED MODELS: EVALUATION In this section we present an experimental evaluation of our advanced models.",
                "Expert finding Expert profiling Language Model 1 Model 2 Model 3 Model 1 Model 2 Model 3 %q MAP MRR %q MAP MRR %q MAP MRR %ca MAP MRR %ca MAP MRR %ca MAP MRR English only 97.8 0.237 0.372 98.6 0.280 0.441 98.5 0.166 0.293 100 0.199 0.387 88.7 0.281 0.525 90.9 0.169 0.329 Dutch only 61.3 0.249 0.401 62.6 0.265 0.436 62.6 0.195 0.344 91.9 0.164 0.426 90.1 0.195 0.488 91.9 0.125 0.328 Combination 99.4 0.297 0.444 99.7 0.324 0.491 99.7 0.223 0.388 100 0.241 0.445 92.1 0.313 0.564 93.2 0.224 0.411 Table 3: Performance of the combination of languages on the expert finding and profiling tasks (on candidates).",
                "Best scores for each model are in italic, absolute best scores for the expert finding and profiling tasks are in boldface.",
                "Method Model 1 Model 2 Model 3 MAP MRR MAP MRR MAP MRR English BASELINE 0.296 0.454 0.339 0.509 0.221 0.333 KLDIV 0.291 0.453 0.327 0.503 0.219 0.330 PMI 0.291 0.453 0.337 0.509 0.219 0.331 LL 0.319 0.490 0.360 0.524 0.233 0.368 HDIST 0.299 0.465 0.346 0.537 0.219 0.332 Dutch BASELINE 0.240 0.350 0.271 0.403 0.227 0.389 KLDIV 0.239 0.347 0.253 0.386 0.224 0.385 PMI 0.239 0.350 0.260 0.392 0.227 0.389 LL 0.255 0.372 0.281 0.425 0.231 0.389 HDIST 0.253 0.365 0.271 0.407 0.236 0.402 Method Model 1 Model 2 Model 3 MAP MRR MAP MRR MAP MRR English BASELINE 0.485 0.546 0.499 0.548 0.381 0.416 KLDIV 0.510 0.564 0.513 0.558 0.381 0.416 PMI 0.486 0.546 0.495 0.542 0.407 0.451 LL 0.558 0.589 0.586 0.617 0.408 0.453 HDIST 0.507 0.567 0.512 0.563 0.386 0.420 Dutch BASELINE 0.263 0.313 0.294 0.358 0.262 0.315 KLDIV 0.284 0.336 0.271 0.321 0.261 0.314 PMI 0.265 0.317 0.265 0.316 0.273 0.330 LL 0.312 0.351 0.330 0.377 0.284 0.331 HDIST 0.280 0.327 0.288 0.341 0.266 0.321 Table 4: Performance on the expert finding (top) and profiling (bottom) tasks, using knowledge area similarities.",
                "Runs were evaluated on the main topics set.",
                "Best scores are in boldface. 8.1 Research Questions Our questions follow the refinements presented in the preceding section: Does exploiting the knowledge area similarity improve effectiveness?",
                "Which of the various methods for capturing word relationships is most effective?",
                "Furthermore, is our way of bringing in contextual information useful?",
                "For which tasks?",
                "And finally, is our simple way of combining the monolingual scores sufficient for obtaining significant improvements? 8.2 Experimental setup Given that the self-assessments are also sparse in our collection, in order to be able to measure differences between the various models, we selected a subset of topics, and evaluated (some of the) runs only on this subset.",
                "This set is referred as main topics, and consists of topics that are located at the top level of the topical hierarchy. (A main topic has subtopics, but is not a subtopic of any other topic.)",
                "This main set consists of 132 Dutch and 119 English topics.",
                "The relevance judgements were restricted to the main topic set, but were not expanded with subtopics. 8.3 Exploiting knowledge area similarity Table 4 presents the results.",
                "The four methods used for estimating knowledge-area similarity are KL divergence (KLDIV), PointLang.",
                "Topics Model 1 Model 2 Model 3 MAP MRR MAP MRR MAP MRR Expert finding UK ALL 0.423 0.545 0.654 0.799 0.494 0.629 UK MAIN 0.500 0.621 0.704 0.834 0.587 0.699 NL ALL 0.439 0.560 0.672 0.826 0.480 0.630 NL MAIN 0.440 0.584 0.645 0.816 0.515 0.655 Expert profiling UK ALL 0.240 0.640 0.306 0.778 0.223 0.616 UK MAIN 0.523 0.677 0.519 0.648 0.461 0.587 NL ALL 0.203 0.716 0.254 0.770 0.183 0.627 NL MAIN 0.332 0.576 0.380 0.624 0.332 0.549 Table 5: Evaluating the context models on organizational units. wise mutual information (PMI), log-likelihood (LL), and distance within topic hierarchy (HDIST).",
                "We managed to improve upon the baseline in all cases, but the improvement is more noticeable for the profiling task.",
                "For both tasks, the LL method performed best.",
                "The content-based approaches performed consistently better than HDIST. 8.4 Contextual information A two level hierarchy of organizational units (faculties and institutes) is available in the UvT Expert collection.",
                "The unit a person belongs to is used as a context for that person.",
                "First, we evaluated the models of the organizational units, using all topics (ALL) and only the main topics (MAIN).",
                "An organizational unit is considered to be relevant for a given topic (or vice versa) if at least one member of the unit selected the given topic as an expertise area.",
                "Table 5 reports on the results.",
                "As far as expert finding goes, given a topic, the corresponding organizational unit can be identified with high precision.",
                "However, the expert profiling task shows a different picture: the scores are low, and the task seems hard.",
                "The explanation may be that general concepts (i.e., our main topics) may belong to several organizational units.",
                "Second, we performed another evaluation, where we combined the contextual models with the candidate models (to score candidates again).",
                "Table 6 reports on the results.",
                "We find a positive impact of the context models only for expert finding.",
                "Noticably, for expert finding (and Model 1), it improves over 50% (for English) and over 70% (for Dutch) on MAP.",
                "The poor performance on expert profiling may be due to the fact that context models alone did not perform very well on the profiling task to begin with. 8.5 Multilingual models In this subsection we evaluate the method for combining results across multiple languages that we described in Section 7.3.",
                "In our setting the set of languages consists of English and Dutch: L = {UK, NL}.",
                "The weights on these languages were set to be identical (λUK = λNL = 0.5).",
                "We performed experiments with various λ settings, but did not observe significant differences in performance.",
                "Table 3 reports on the multilingual results, where performance is evaluated on the full topic set.",
                "All three models significantly imLang.",
                "Method Model 1 Model 2 Model 3 MAP MRR MAP MRR MAP MRR Expert finding UK BL 0.296 0.454 0.339 0.509 0.221 0.333 UK CT 0.330 0.491 0.342 0.500 0.228 0.342 NL BL 0.240 0.350 0.271 0.403 0.227 0.389 NL CT 0.251 0.382 0.267 0.410 0.246 0.404 Expert profiling UK BL 0.485 0.546 0.499 0.548 0.381 0.416 UK CT 0.562 0.620 0.508 0.558 0.440 0.486 NL BL 0.263 0.313 0.294 0.358 0.262 0.315 NL CT 0.330 0.384 0.317 0.387 0.294 0.345 Table 6: Performance of the context models (CT) compared to the baseline (BL).",
                "Best scores are in boldface. proved over all measures for both tasks.",
                "The coverage of topics and candidates for the expert finding and profiling tasks, respectively, is close to 100% in all cases.",
                "The relative improvement of the precision scores ranges from 10% to 80%.",
                "These scores demonstrate that despite its simplicity, our method for combining results over multiple languages achieves substantial improvements over the baseline. 9.",
                "CONCLUSIONS In this paper we focused on expertise retrieval (expert finding and profiling) in a new setting of a typical knowledge-intensive organization in which the available data is of high quality, multilingual, and covering a broad range of expertise area.",
                "Typically, the amount of available data in such an organization (e.g., a university, a research institute, or a research lab) is limited when compared to the W3C collection that has mostly been used for the experimental evaluation of expertise retrieval so far.",
                "To examine expertise retrieval in this setting, we introduced (and released) the UvT Expert collection as a representative case of such knowledge intensive organizations.",
                "The new collection reflects the typical properties of knowledge-intensive institutes noted above and also includes several features which may are potentially useful for expertise retrieval, such as topical and <br>organizational structure</br>.",
                "We evaluated how current state-of-the-art models for expert finding and profiling performed in this new setting and then refined these models in order to try and exploit the different characteristics within the data environment (language, topicality, and <br>organizational structure</br>).",
                "We found that current models of expertise retrieval generalize well to this new environment; in addition we found that refining the models to account for the differences results in significant improvements, thus making up for problems caused by data sparseness issues.",
                "Future work includes setting up manual assessments of automatically generated profiles by the employees themselves, especially in cases where the employees have not provided a profile themselves. 10.",
                "ACKNOWLEDGMENTS Krisztian Balog was supported by the Netherlands Organisation for Scientific Research (NWO) under project number 220-80-001.",
                "Maarten de Rijke was also supported by NWO under project numbers 017.001.190, 220-80-001, 264-70-050, 354-20-005, 600.065.120, 612-13-001, 612.000.106, 612.066.302, 612.069.006, 640.001.501, 640.002.501, and by the E.U.",
                "IST programme of the 6th FP for RTD under project MultiMATCH contract IST-033104.",
                "The work of Toine Bogers and Antal van den Bosch was funded by the IOP-MMI-program of SenterNovem / The Dutch Ministry of Economic Affairs, as part of the `A Propos project. 11.",
                "REFERENCES [1] L. Azzopardi.",
                "Incorporating Context in the Language Modeling Framework for ad-hoc Information Retrieval.",
                "PhD thesis, University of Paisley, 2005. [2] K. Balog and M. de Rijke.",
                "Finding similar experts.",
                "In This volume, 2007. [3] K. Balog and M. de Rijke.",
                "Determining expert profiles (with an application to expert finding).",
                "In IJCAI 07: Proc. 20th Intern.",
                "Joint Conf. on Artificial Intelligence, pages 2657-2662, 2007. [4] K. Balog, L. Azzopardi, and M. de Rijke.",
                "Formal models for expert finding in enterprise corpora.",
                "In SIGIR 06: Proc. 29th annual intern.",
                "ACM SIGIR conf. on Research and development in information retrieval, pages 43-50, 2006. [5] I. Becerra-Fernandez.",
                "The role of artificial intelligence technologies in the implementation of people-finder knowledge management systems.",
                "In AAAI Workshop on Bringing Knowledge to Business Processes, March 2000. [6] C. S. Campbell, P. P. Maglio, A. Cozzi, and B. Dom.",
                "Expertise identification using email communications.",
                "In CIKM 03: Proc. twelfth intern. conf. on Information and knowledge management, pages 528531, 2003. [7] G. Cao, J.-Y.",
                "Nie, and J. Bai.",
                "Integrating word relationships into language models.",
                "In SIGIR 05: Proc. 28th annual intern.",
                "ACM SIGIR conf. on Research and development in information retrieval, pages 298-305, 2005. [8] T. M. Cover and J.",
                "A. Thomas.",
                "Elements of Information Theory.",
                "Wiley-Interscience, 1991. [9] N. Craswell, D. Hawking, A. M. Vercoustre, and P. Wilkins.",
                "P@noptic expert: Searching for experts not just for documents.",
                "In Ausweb, 2001. [10] N. Craswell, A. de Vries, and I. Soboroff.",
                "Overview of the TREC2005 Enterprise Track.",
                "In The Fourteenth Text REtrieval Conf.",
                "Proc. (TREC 2005), 2006. [11] T. H. Davenport and L. Prusak.",
                "Working Knowledge: How Organizations Manage What They Know.",
                "Harvard Business School Press, Boston, MA, 1998. [12] T. Dunning.",
                "Accurate methods for the statistics of surprise and coincidence.",
                "Computational Linguistics, 19(1):61-74, 1993. [13] E. Filatova and J. Prager.",
                "Tell me what you do and Ill tell you what you are: Learning occupation-related activities for biographies.",
                "In HLT/EMNLP, 2005. [14] V. Lavrenko and W. B. Croft.",
                "Relevance based language models.",
                "In SIGIR 01: Proc. 24th annual intern.",
                "ACM SIGIR conf. on Research and development in information retrieval, pages 120-127, 2001. [15] V. Lavrenko, M. Choquette, and W. B. Croft.",
                "Cross-lingual relevance models.",
                "In SIGIR 02: Proc. 25th annual intern.",
                "ACM SIGIR conf. on Research and development in information retrieval, pages 175-182, 2002. [16] C. Macdonald and I. Ounis.",
                "Voting for candidates: adapting data fusion techniques for an expert search task.",
                "In CIKM 06: Proc. 15th ACM intern. conf. on Information and knowledge management, pages 387-396, 2006. [17] C. Manning and H. Sch¨utze.",
                "Foundations of Statistical Natural Language Processing.",
                "The MIT Press, 1999. [18] A. Mockus and J. D. Herbsleb.",
                "Expertise browser: a quantitative approach to identifying expertise.",
                "In ICSE 02: Proc. 24th Intern.",
                "Conf. on Software Engineering, pages 503-512, 2002. [19] D. Petkova and W. B. Croft.",
                "Hierarchical language models for expert finding in enterprise corpora.",
                "In Proc.",
                "ICTAI 2006, pages 599-608, 2006. [20] I. Soboroff, A. de Vries, and N. Craswell.",
                "Overview of the TREC 2006 Enterprise Track.",
                "In TREC 2006 Working Notes, 2006. [21] T. Tao, X. Wang, Q. Mei, and C. Zhai.",
                "Language model information retrieval with document expansion.",
                "In HLT-NAACL 2006, 2006. [22] TREC.",
                "Enterprise track, 2005.",
                "URL: http://www.ins.cwi. nl/projects/trec-ent/wiki/. [23] G. van Noord.",
                "TextCat Language Guesser.",
                "URL: http://www. let.rug.nl/˜vannoord/TextCat/. [24] W3C.",
                "The W3C test collection, 2005.",
                "URL: http://research. microsoft.com/users/nickcr/w3c-summary.html."
            ],
            "original_annotated_samples": [
                "The second is aimed at assessing refined models that exploit characteristic features of the new test set, such as the <br>organizational structure</br> of the university, and the hierarchical structure of the topics in the test set.",
                "Typically, this setting features several additional types of structure: topical structure (e.g., topic hierarchies as employed by the organization), <br>organizational structure</br> (faculty, department, ...), as well as multiple types of documents (research and course descriptions, publications, and academic homepages).",
                "In addition to, how can the topical and <br>organizational structure</br> be used for retrieval purposes?",
                "A second group of experiments is aimed at extensions of the baseline methods that exploit characteristic features of the UvT Expert Collection; specifically, we propose and evaluate refined expert finding and profiling methods that incorporate topicality and <br>organizational structure</br>.",
                "Finally, we demonstrate that the <br>organizational structure</br> can be exploited in the form of a context model, improving MAP scores for certain models by up to 70%."
            ],
            "translated_annotated_samples": [
                "El segundo tiene como objetivo evaluar modelos refinados que exploten las características distintivas del nuevo conjunto de pruebas, como la <br>estructura organizativa</br> de la universidad y la estructura jerárquica de los temas en el conjunto de pruebas.",
                "Normalmente, esta configuración incluye varios tipos adicionales de estructuras: estructura temática (por ejemplo, jerarquías de temas utilizadas por la organización), <br>estructura organizativa</br> (facultad, departamento, ...), así como múltiples tipos de documentos (descripciones de investigación y cursos, publicaciones y páginas web académicas).",
                "Además, ¿cómo se puede utilizar la estructura temática y organizativa con fines de recuperación?",
                "Un segundo grupo de experimentos está dirigido a extensiones de los métodos base que aprovechan las características distintivas de la Colección de Expertos de UvT; específicamente, proponemos y evaluamos métodos refinados de búsqueda y perfilado de expertos que incorporan la topicalidad y la <br>estructura organizativa</br>.",
                "Finalmente, demostramos que la <br>estructura organizativa</br> puede ser explotada en forma de un modelo de contexto, mejorando los puntajes de MAP para ciertos modelos hasta en un 70%."
            ],
            "translated_text": "Recuperación de amplia experiencia en entornos de datos dispersos Krisztian Balog ISLA, Universidad de Ámsterdam Kruislaan 403, 1098 SJ Ámsterdam, Países Bajos kbalog@science.uva.nl Toine Bogers ILK, Universidad de Tilburg P.O. Caja 90153, 5000 LE Tilburg, Países Bajos A.M.Bogers@uvt.nl Leif Azzopardi Dept. de Ciencias de la Computación Universidad de Glasgow, Glasgow, G12 8QQ leif@dcs.gla.ac.uk Maarten de Rijke ISLA, Universidad de Ámsterdam Kruislaan 403, 1098 SJ Ámsterdam, Países Bajos mdr@science.uva.nl Antal van den Bosch ILK, Universidad de Tilburg P.O. La recuperación de la experiencia ha sido ampliamente inexplorada en datos distintos a la colección de la W3C. Al mismo tiempo, muchas intranets de universidades y otras organizaciones intensivas en conocimiento ofrecen ejemplos de datos de expertos multilingües relativamente pequeños pero precisos, que abarcan amplias áreas de especialización. Primero presentamos dos tareas principales de recuperación de expertos, junto con un conjunto de enfoques de referencia basados en modelado del lenguaje generativo, con el objetivo de encontrar relaciones de experticia entre temas y personas. Para nuestra evaluación experimental, introducimos (y liberamos) un nuevo conjunto de pruebas basado en un rastreo de un sitio universitario. Usando este conjunto de pruebas, llevamos a cabo dos series de experimentos. El primero tiene como objetivo determinar la efectividad de los métodos de recuperación de conocimientos básicos aplicados al nuevo conjunto de pruebas. El segundo tiene como objetivo evaluar modelos refinados que exploten las características distintivas del nuevo conjunto de pruebas, como la <br>estructura organizativa</br> de la universidad y la estructura jerárquica de los temas en el conjunto de pruebas. Los modelos de recuperación de experiencia se muestran robustos con respecto a entornos más pequeños que la colección de la W3C, y las técnicas actuales parecen ser generalizables a otros contextos. Categorías y Descriptores de Asignaturas H.3 [Almacenamiento y Recuperación de Información]: H.3.1 Análisis de Contenido e Indexación; H.3.3 Búsqueda y Recuperación de Información; H.3.4 Sistemas y Software; H.4 [Aplicaciones de Sistemas de Información]: H.4.2 Tipos de Sistemas; H.4.m Términos Generales Misceláneos Algoritmos, Medición, Rendimiento, Experimentación 1. INTRODUCCIÓN La intranet de una organización proporciona un medio para intercambiar información entre empleados y facilitar colaboraciones entre ellos. Para lograr esto de manera eficiente y efectiva, es necesario proporcionar instalaciones de búsqueda que permitan a los empleados no solo acceder a documentos, sino también identificar a colegas expertos. En la pista de Empresas de TREC [22] se ha reconocido la necesidad de estudiar y comprender la recuperación de expertos a través de la introducción de tareas de Búsqueda de Expertos. El objetivo de la búsqueda de expertos es identificar una lista de personas que tienen conocimientos sobre un tema específico. Esta tarea suele abordarse descubriendo asociaciones entre personas y temas [10]; comúnmente, se asume que la co-ocurrencia del nombre de una persona con temas en el mismo contexto es evidencia de experiencia. Una tarea alternativa, que utiliza la misma idea de asociaciones entre personas y temas, es el perfilado de expertos, donde la tarea consiste en devolver una lista de temas sobre los que una persona tiene conocimiento [3]. El lanzamiento de la tarea de Búsqueda de Expertos en TREC ha generado mucho interés en la recuperación de expertos, con un rápido progreso en términos de modelado, algoritmos y aspectos de evaluación. Sin embargo, casi todo el trabajo de búsqueda o perfilado de expertos realizado ha sido validado experimentalmente utilizando la colección de W3C [24] de la pista empresarial. Si bien esta colección es actualmente la única colección de pruebas disponible públicamente para tareas de recuperación de expertos, solo representa un tipo de intranet. Con solo una colección de pruebas no es posible generalizar conclusiones a otros escenarios realistas. En este artículo nos enfocamos en la recuperación de expertos en un entorno realista que difiere del entorno de la W3C, uno en el que hay cantidades relativamente pequeñas de datos limpios y multilingües disponibles, que abarcan una amplia gama de áreas de expertise, como se puede encontrar en las intranets de universidades y otras organizaciones intensivas en conocimiento. Normalmente, esta configuración incluye varios tipos adicionales de estructuras: estructura temática (por ejemplo, jerarquías de temas utilizadas por la organización), <br>estructura organizativa</br> (facultad, departamento, ...), así como múltiples tipos de documentos (descripciones de investigación y cursos, publicaciones y páginas web académicas). Este entorno es bastante diferente del entorno del W3C de maneras que podrían afectar el rendimiento de las tareas de recuperación de conocimientos especializados. Nos enfocamos en varias preguntas de investigación en este artículo: ¿La cantidad relativamente pequeña de datos disponibles en una intranet afecta la calidad de las asociaciones tema-persona que son fundamentales en los algoritmos de recuperación de expertos? ¿Cómo se desempeñan los algoritmos de vanguardia desarrollados en el conjunto de datos del W3C en el escenario alternativo del tipo descrito anteriormente? Más en general, ¿se pueden aplicar las lecciones de la tarea de Búsqueda de Expertos en TREC a este contexto? ¿Cómo afecta la inclusión o exclusión de diferentes documentos a las tareas de recuperación de expertos? Además, ¿cómo se puede utilizar la estructura temática y organizativa con fines de recuperación? Para responder a nuestras preguntas de investigación, primero presentamos un conjunto de enfoques de línea base, basados en modelado de lenguaje generativo, con el objetivo de encontrar asociaciones entre temas y personas. Esto nos permite formular las tareas de búsqueda de expertos y perfilado de expertos de manera uniforme, y tiene el beneficio adicional de permitirnos entender las relaciones entre las dos tareas. Para nuestra evaluación experimental, introducimos un nuevo conjunto de datos (la Colección de Expertos de UvT) que es representativo del tipo de intranet que describimos anteriormente. Nuestra colección se basa en datos de acceso público, extraídos del sitio web de la Universidad de Tilburg (UvT). Este tipo de datos es particularmente interesante, ya que (1) es limpio, heterogéneo, estructurado y enfocado, pero comprende un número limitado de documentos; (2) contiene información sobre la jerarquía organizativa; (3) es bilingüe (inglés y holandés); y (4) la lista de áreas de especialización de un individuo es proporcionada por los propios empleados. Usando la colección de expertos de UvT, llevamos a cabo dos conjuntos de experimentos. El primero tiene como objetivo determinar la efectividad de los métodos de búsqueda y perfilado de experiencia inicial en este nuevo entorno. Un segundo grupo de experimentos está dirigido a extensiones de los métodos base que aprovechan las características distintivas de la Colección de Expertos de UvT; específicamente, proponemos y evaluamos métodos refinados de búsqueda y perfilado de expertos que incorporan la topicalidad y la <br>estructura organizativa</br>. Además de las preguntas de investigación y el conjunto de datos que aportamos, nuestras principales contribuciones son las siguientes. Los modelos base desarrollados para encontrar expertos funcionan bien en el nuevo conjunto de datos. Mientras que en el entorno del W3C la tarea de encontrar expertos parece ser más difícil que el perfilado, para los datos de UvT ocurre lo contrario. Encontramos que el perfilado en el conjunto de datos UvT es considerablemente más difícil que en el conjunto de datos W3C, lo cual creemos que se debe al gran (pero realista) número de áreas temáticas que utilizamos para el perfilado: alrededor de 1,500 para el conjunto de datos UvT, frente a 50 en el caso de W3C. Teniendo en cuenta la similitud entre los temas puede mejorar significativamente el rendimiento de recuperación. Las medidas de similitud de mejor rendimiento son basadas en contenido, por lo tanto también se pueden aplicar en entornos de la W3C (y otros). Finalmente, demostramos que la <br>estructura organizativa</br> puede ser explotada en forma de un modelo de contexto, mejorando los puntajes de MAP para ciertos modelos hasta en un 70%. ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "intranet search": {
            "translated_key": "búsqueda en la intranet",
            "is_in_text": false,
            "original_annotated_sentences": [
                "Broad Expertise Retrieval in Sparse Data Environments Krisztian Balog ISLA, University of Amsterdam Kruislaan 403, 1098 SJ Amsterdam, The Netherlands kbalog@science.uva.nl Toine Bogers ILK, Tilburg University P.O.",
                "Box 90153, 5000 LE Tilburg, The Netherlands A.M.Bogers@uvt.nl Leif Azzopardi Dept. of Computing Science University of Glasgow, Glasgow, G12 8QQ leif@dcs.gla.ac.uk Maarten de Rijke ISLA, University of Amsterdam Kruislaan 403, 1098 SJ Amsterdam, The Netherlands mdr@science.uva.nl Antal van den Bosch ILK, Tilburg University P.O.",
                "Box 90153, 5000 LE Tilburg, The Netherlands Antal.vdnBosch@uvt.nl ABSTRACT Expertise retrieval has been largely unexplored on data other than the W3C collection.",
                "At the same time, many intranets of universities and other knowledge-intensive organisations offer examples of relatively small but clean multilingual expertise data, covering broad ranges of expertise areas.",
                "We first present two main expertise retrieval tasks, along with a set of baseline approaches based on generative language modeling, aimed at finding expertise relations between topics and people.",
                "For our experimental evaluation, we introduce (and release) a new test set based on a crawl of a university site.",
                "Using this test set, we conduct two series of experiments.",
                "The first is aimed at determining the effectiveness of baseline expertise retrieval methods applied to the new test set.",
                "The second is aimed at assessing refined models that exploit characteristic features of the new test set, such as the organizational structure of the university, and the hierarchical structure of the topics in the test set.",
                "Expertise retrieval models are shown to be robust with respect to environments smaller than the W3C collection, and current techniques appear to be generalizable to other settings.",
                "Categories and Subject Descriptors H.3 [Information Storage and Retrieval]: H.3.1 Content Analysis and Indexing; H.3.3 Information Search and Retrieval; H.3.4 Systems and Software; H.4 [Information Systems Applications]: H.4.2 Types of Systems; H.4.m Miscellaneous General Terms Algorithms, Measurement, Performance, Experimentation 1.",
                "INTRODUCTION An organizations intranet provides a means for exchanging information between employees and for facilitating employee collaborations.",
                "To efficiently and effectively achieve this, it is necessary to provide search facilities that enable employees not only to access documents, but also to identify expert colleagues.",
                "At the TREC Enterprise Track [22] the need to study and understand expertise retrieval has been recognized through the introduction of Expert Finding tasks.",
                "The goal of expert finding is to identify a list of people who are knowledgeable about a given topic.",
                "This task is usually addressed by uncovering associations between people and topics [10]; commonly, a co-occurrence of the name of a person with topics in the same context is assumed to be evidence of expertise.",
                "An alternative task, which using the same idea of people-topic associations, is expert profiling, where the task is to return a list of topics that a person is knowledgeable about [3].",
                "The launch of the Expert Finding task at TREC has generated a lot of interest in expertise retrieval, with rapid progress being made in terms of modeling, algorithms, and evaluation aspects.",
                "However, nearly all of the expert finding or profiling work performed has been validated experimentally using the W3C collection [24] from the Enterprise Track.",
                "While this collection is currently the only publicly available test collection for expertise retrieval tasks, it only represents one type of intranet.",
                "With only one test collection it is not possible to generalize conclusions to other realistic settings.",
                "In this paper we focus on expertise retrieval in a realistic setting that differs from the W3C setting-one in which relatively small amounts of clean, multilingual data are available, that cover a broad range of expertise areas, as can be found on the intranets of universities and other knowledge-intensive organizations.",
                "Typically, this setting features several additional types of structure: topical structure (e.g., topic hierarchies as employed by the organization), organizational structure (faculty, department, ...), as well as multiple types of documents (research and course descriptions, publications, and academic homepages).",
                "This setting is quite different from the W3C setting in ways that might impact upon the performance of expertise retrieval tasks.",
                "We focus on a number of research questions in this paper: Does the relatively small amount of data available on an intranet affect the quality of the topic-person associations that lie at the heart of expertise retrieval algorithms?",
                "How do state-of-the-art algorithms developed on the W3C data set perform in the alternative scenario of the type described above?",
                "More generally, do the lessons from the Expert Finding task at TREC carry over to this setting?",
                "How does the inclusion or exclusion of different documents affect expertise retrieval tasks?",
                "In addition to, how can the topical and organizational structure be used for retrieval purposes?",
                "To answer our research questions, we first present a set of baseline approaches, based on generative language modeling, aimed at finding associations between topics and people.",
                "This allows us to formulate the expert finding and expert profiling tasks in a uniform way, and has the added benefit of allowing us to understand the relations between the two tasks.",
                "For our experimental evaluation, we introduce a new data set (the UvT Expert Collection) which is representative of the type of intranet that we described above.",
                "Our collection is based on publicly available data, crawled from the website of Tilburg University (UvT).",
                "This type of data is particularly interesting, since (1) it is clean, heterogeneous, structured, and focused, but comprises a limited number of documents; (2) contains information on the organizational hierarchy; (3) it is bilingual (English and Dutch); and (4) the list of expertise areas of an individual are provided by the employees themselves.",
                "Using the UvT Expert collection, we conduct two sets of experiments.",
                "The first is aimed at determining the effectiveness of baseline expertise finding and profiling methods in this new setting.",
                "A second group of experiments is aimed at extensions of the baseline methods that exploit characteristic features of the UvT Expert Collection; specifically, we propose and evaluate refined expert finding and profiling methods that incorporate topicality and organizational structure.",
                "Apart from the research questions and data set that we contribute, our main contributions are as follows.",
                "The baseline models developed for expertise finding perform well on the new data set.",
                "While on the W3C setting the expert finding task appears to be more difficult than profiling, for the UvT data the opposite is the case.",
                "We find that profiling on the UvT data set is considerably more difficult than on the W3C set, which we believe is due to the large (but realistic) number of topical areas that we used for profiling: about 1,500 for the UvT set, versus 50 in the W3C case.",
                "Taking the similarity between topics into account can significantly improve retrieval performance.",
                "The best performing similarity measures are content-based, therefore they can be applied on the W3C (and other) settings as well.",
                "Finally, we demonstrate that the organizational structure can be exploited in the form of a context model, improving MAP scores for certain models by up to 70%.",
                "The remainder of this paper is organized as follows.",
                "In the next section we review related work.",
                "Then, in Section 3 we provide detailed descriptions of the expertise retrieval tasks that we address in this paper: expert finding and expert profiling.",
                "In Section 4 we present our baseline models, of which the performance is then assessed in Section 6 using the UvT data set that we introduce in Section 5.",
                "Advanced models exploiting specific features of our data are presented in Section 7 and evaluated in Section 8.",
                "We formulate our conclusions in Section 9. 2.",
                "RELATED WORK Initial approaches to expertise finding often employed databases containing information on the skills and knowledge of each individual in the organization [11].",
                "Most of these tools (usually called yellow pages or people-finding systems) rely on people to self-assess their skills against a predefined set of keywords.",
                "For updating profiles in these systems in an automatic fashion there is a need for intelligent technologies [5].",
                "More recent approaches use specific document sets (such as email [6] or software [18]) to find expertise.",
                "In contrast with focusing on particular document types, there is also an increased interest in the development of systems that index and mine published intranet documents as sources of evidence for expertise.",
                "One such published approach is the P@noptic system [9], which builds a representation of each person by concatenating all documents associated with that person-this is similar to Model 1 of Balog et al. [4], who formalize and compare two methods.",
                "Balog et al.s Model 1 directly models the knowledge of an expert from associated documents, while their Model 2 first locates documents on the topic and then finds the associated experts.",
                "In the reported experiments the second method performs significantly better when there are sufficiently many associated documents per candidate.",
                "Most systems that took part in the 2005 and 2006 editions of the Expert Finding task at TREC implemented (variations on) one of these two models; see [10, 20].",
                "Macdonald and Ounis [16] propose a different approach for ranking candidate expertise with respect to a topic based on data fusion techniques, without using collectionspecific heuristics; they find that applying field-based weighting models improves the ranking of candidates.",
                "Petkova and Croft [19] propose yet another approach, based on a combination of the above Model 1 and 2, explicitly modeling topics.",
                "Turning to other expert retrieval tasks that can also be addressed using topic-people associations, Balog and de Rijke [3] addressed the task of determining topical expert profiles.",
                "While their methods proved to be efficient on the W3C corpus, they require an amount of data that may not be available in the typical knowledge-intensive organization.",
                "Balog and de Rijke [2] study the related task of finding experts that are similar to a small set of experts given as input.",
                "As an aside, creating a textual summary of a person shows some similarities to biography finding, which has received a considerable amount of attention recently; see e.g., [13].",
                "We use generative language modeling to find associations between topics and people.",
                "In our modeling of expert finding and profiling we collect evidence for expertise from multiple sources, in a heterogeneous collection, and integrate it with the co-occurrence of candidates names and query terms-the language modeling setting allows us to do this in a transparent manner.",
                "Our modeling proceeds in two steps.",
                "In the first step, we consider three baseline models, two taken from [4] (the Models 1 and 2 mentioned above), and one a refined version of a model introduced in [3] (which we refer to as Model 3 below); this third model is also similar to the model described by Petkova and Croft [19].",
                "The models we consider in our second round of experiments are mixture models similar to contextual language models [1] and to the expanded documents of Tao et al. [21]; however, the features that we use for definining our expansions-including topical structure and organizational structure-have not been used in this way before. 3.",
                "TASKS In the expertise retrieval scenario that we envisage, users seeking expertise within an organization have access to an interface that combines a search box (where they can search for experts or topics) with navigational structures (of experts and of topics) that allows them to click their way to an expert page (providing the profile of a person) or a topic page (providing a list of experts on the topic).",
                "To feed the above interface, we face two expertise retrieval tasks, expert finding and expert profiling, that we first define and then formalize using generative language models.",
                "In order to model either task, the probability of the query topic being associated to a candidate expert plays a key role in the final estimates for searching and profiling.",
                "By using language models, both the candidates and the query are characterized by distributions of terms in the vocabulary (used in the documents made available by the organization whose expertise retrieval needs we are addressing). 3.1 Expert finding Expert finding involves the task of finding the right person with the appropriate skills and knowledge: Who are the experts on topic X?.",
                "E.g., an employee wants to ascertain who worked on a particular project to find out why particular decisions were made without having to trawl through documentation (if there is any).",
                "Or, they may be in need a trained specialist for consultancy on a specific problem.",
                "Within an organization there are usually many possible candidates who could be experts for given topic.",
                "We can state this problem as follows: What is the probability of a candidate ca being an expert given the query topic q?",
                "That is, we determine p(ca|q), and rank candidates ca according to this probability.",
                "The candidates with the highest probability given the query are deemed the most likely experts for that topic.",
                "The challenge is how to estimate this probability accurately.",
                "Since the query is likely to consist of only a few terms to describe the expertise required, we should be able to obtain a more accurate estimate by invoking Bayes Theorem, and estimating: p(ca|q) = p(q|ca)p(ca) p(q) , (1) where p(ca) is the probability of a candidate and p(q) is the probability of a query.",
                "Since p(q) is a constant, it can be ignored for ranking purposes.",
                "Thus, the probability of a candidate ca being an expert given the query q is proportional to the probability of a query given the candidate p(q|ca), weighted by the a priori belief p(ca) that candidate ca is an expert. p(ca|q) ∝ p(q|ca)p(ca) (2) In this paper our main focus is on estimating the probability of a query given the candidate p(q|ca), because this probability captures the extent to which the candidate knows about the query topic.",
                "Whereas the candidate priors are generally assumed to be uniformand thus will not influence the ranking-it has been demonstrated that a sensible choice of priors may improve the performance [20]. 3.2 Expert profiling While the task of expert searching was concerned with finding experts given a particular topic, the task of expert profiling seeks to answer a related question: What topics does a candidate know about?",
                "Essentially, this turns the questions of expert finding around.",
                "The profiling of an individual candidate involves the identification of areas of skills and knowledge that they have expertise about and an evaluation of the level of proficiency in each of these areas.",
                "This is the candidates topical profile.",
                "Generally, topical profiles within organizations consist of tabular structures which explicitly catalogue the skills and knowledge of each individual in the organization.",
                "However, such practice is limited by the resources available for defining, creating, maintaining, and updating these profiles over time.",
                "By focusing on automatic methods which draw upon the available evidence within the document repositories of an organization, our aim is to reduce the human effort associated with the maintenance of topical profiles1 .",
                "A topical profile of a candidate, then, is defined as a vector where each element i of the vector corresponds to the candidate cas expertise on a given topic ki, (i.e., s(ca, ki)).",
                "Each topic ki defines a particular knowledge area or skill that the organization uses to define the candidates topical profile.",
                "Thus, it is assumed that a list of topics, {k1, . . . , kn}, where n is the number of pre-defined topics, is given: profile(ca) = s(ca, k1), s(ca, k2), . . . , s(ca, kn) . (3) 1 Context and evidence are needed to help users of expertise finding systems to decide whom to contact when seeking expertise in a particular area.",
                "Examples of such context are: Who does she work with?",
                "What are her contact details?",
                "Is she well-connected, just in case she is not able to help us herself?",
                "What is her role in the organization?",
                "Who is her superior?",
                "Collaborators, and affiliations, etc. are all part of the candidates social profile, and can serve as a background against which the systems recommendations should be interpreted.",
                "In this paper we only address the problem of determining topical profiles, and leave social profiling to further work.",
                "We state the problem of quantifying the competence of a person on a certain knowledge area as follows: What is the probability of a knowledge area (ki) being part of the candidates (expertise) profile? where s(ca, ki) is defined by p(ki|ca).",
                "Our task, then, is to estimate p(ki|ca), which is equivalent to the problem of obtaining p(q|ca), where the topic ki is represented as a query topic q, i.e., a sequence of keywords representing the expertise required.",
                "Both the expert finding and profiling tasks rely on the accurate estimation of p(q|ca).",
                "The only difference derives from the prior probability that a person is an expert (p(ca)), which can be incorporated into the expert finding task.",
                "This prior does not apply to the profiling task since the candidate (individual) is fixed. 4.",
                "BASELINE MODELS In this section we describe our baseline models for estimating p(q|ca), i.e., associations between topics and people.",
                "Both expert finding and expert profiling boil down to this estimation.",
                "We employ three models for calculating this probability. 4.1 From topics to candidates Using Candidate Models: Model 1 Model 1 [4] defines the probability of a query given a candidate (p(q|ca)) using standard language modeling techniques, based on a multinomial unigram language model.",
                "For each candidate ca, a candidate language model θca is inferred such that the probability of a term given θca is nonzero for all terms, i.e., p(t|θca) > 0.",
                "From the candidate model the query is generated with the following probability: p(q|θca) = Y t∈q p(t|θca)n(t,q) , where each term t in the query q is sampled identically and independently, and n(t, q) is the number of times t occurs in q.",
                "The candidate language model is inferred as follows: (1) an empirical model p(t|ca) is computed; (2) it is smoothed with background probabilities.",
                "Using the associations between a candidate and a document, the probability p(t|ca) can be approximated by: p(t|ca) = X d p(t|d)p(d|ca), where p(d|ca) is the probability that candidate ca generates a supporting document d, and p(t|d) is the probability of a term t occurring in the document d. We use the maximum-likelihood estimate of a term, that is, the normalised frequency of the term t in document d. The strength of the association between document d and candidate ca expressed by p(d|ca) reflects the degree to which the candidates expertise is described using this document.",
                "The estimation of this probability is presented later, in Section 4.2.",
                "The candidate model is then constructed as a linear interpolation of p(t|ca) and the background model p(t) to ensure there are no zero probabilities, which results in the final estimation: p(q|θca) = (4) Y t∈q ( (1 − λ) X d p(t|d)p(d|ca) ! + λp(t) )n(t,q) .",
                "Model 1 amasses all the term information from all the documents associated with the candidate, and uses this to represent that candidate.",
                "This model is used to predict how likely a candidate would produce a query q.",
                "This can can be intuitively interpreted as the probability of this candidate talking about the query topic, where we assume that this is indicative of their expertise.",
                "Using Document Models: Model 2 Model 2 [4] takes a different approach.",
                "Here, the process is broken into two parts.",
                "Given a candidate ca, (1) a document that is associated with a candidate is selected with probability p(d|ca), and (2) from this document a query q is generated with probability p(q|d).",
                "Then the sum over all documents is taken to obtain p(q|ca), such that: p(q|ca) = X d p(q|d)p(d|ca). (5) The probability of a query given a document is estimated by inferring a document language model θd for each document d in a similar manner as the candidate model was inferred: p(t|θd) = (1 − λ)p(t|d) + λp(t), (6) where p(t|d) is the probability of the term in the document.",
                "The probability of a query given the document model is: p(q|θd) = Y t∈q p(t|θd)n(t,q) .",
                "The final estimate of p(q|ca) is obtained by substituting p(q|d) for p(q|θd) into Eq. 5 (see [4] for full details).",
                "Conceptually, Model 2 differs from Model 1 because the candidate is not directly modeled.",
                "Instead, the document acts like a hidden variable in the process which separates the query from the candidate.",
                "This process is akin to how a user may search for candidates with a standard search engine: initially by finding the documents which are relevant, and then seeing who is associated with that document.",
                "By examining a number of documents the user can obtain an idea of which candidates are more likely to discuss the topic q.",
                "Using Topic Models: Model 3 We introduce a third model, Model 3.",
                "Instead of attempting to model the query generation process via candidate or document models, we represent the query as a topic language model and directly estimate the probability of the candidate p(ca|q).",
                "This approach is similar to the model presented in [3, 19].",
                "As with the previous models, a language model is inferred, but this time for the query.",
                "We adapt the work of Lavrenko and Croft [14] to estimate a topic model from the query.",
                "The procedure is as follows.",
                "Given a collection of documents and a query topic q, it is assumed that there exists an unknown topic model θk that assigns probabilities p(t|θk) to the term occurrences in the topic documents.",
                "Both the query and the documents are samples from θk (as opposed to the previous approaches, where a query is assumed to be sampled from a specific document or candidate model).",
                "The main task is to estimate p(t|θk), the probability of a term given the topic model.",
                "Since the query q is very sparse, and as there are no examples of documents on the topic, this distribution needs to be approximated.",
                "Lavrenko and Croft [14] suggest a reasonable way of obtaining such an approximation, by assuming that p(t|θk) can be approximated by the probability of term t given the query q.",
                "We can then estimate p(t|q) using the joint probability of observing the term t together with the query terms, q1, . . . , qm, and dividing by the joint probability of the query terms: p(t|θk) ≈ p(t|q) = p(t, q1, . . . , qm) p(q1, . . . , qm) = p(t, q1, . . . , qm) P t ∈T p(t , q1, . . . , qm) , where p(q1, . . . , qm) = P t ∈T p(t , q1, . . . , qm), and T is the entire vocabulary of terms.",
                "In order to estimate the joint probability p(t, q1, . . . , qm), we follow [14, 15] and assume t and q1, . . . , qm are mutually independent, once we pick a source distribution from the set of underlying source distributions U.",
                "If we choose U to be a set of document models. then to construct this set, the query q would be issued against the collection, and the top n returned are assumed to be relevant to the topic, and thus treated as samples from the topic model. (Note that candidate models could be used instead.)",
                "With the document models forming U, the joint probability of term and query becomes: p(t, q1, . . . , qm) = X d∈U p(d) ˘ p(t|θd) mY i=1 p(qi|θd) ¯ . (7) Here, p(d) denotes the prior distribution over the set U, which reflects the relevance of the document to the topic.",
                "We assume that p(d) is uniform across U.",
                "In order to rank candidates according to the topic model defined, we use the Kullback-Leibler divergence metric (KL, [8]) to measure the difference between the candidate models and the topic model: KL(θk||θca) = X t p(t|θk) log p(t|θk) p(t|θca) . (8) Candidates with a smaller divergence from the topic model are considered to be more likely experts on that topic.",
                "The candidate model θca is defined in Eq. 4.",
                "By using KL divergence instead of the probability of a candidate given the topic model p(ca|θk), we avoid normalization problems. 4.2 Document-candidate associations For our models we need to be able to estimate the probability p(d|ca), which expresses the extent to which a document d characterizes the candidate ca.",
                "In [4], two methods are presented for estimating this probability, based on the number of person names recognized in a document.",
                "However, in our (intranet) setting it is reasonable to assume that authors of documents can unambiguously be identified (e.g., as the author of an article, the teacher assigned to a course, the owner of a web page, etc.)",
                "Hence, we set p(d|ca) to be 1 if candidate ca is author of document d, otherwise the probability is 0.",
                "In Section 6 we describe how authorship can be determined on different types of documents within the collection. 5.",
                "THE UVT EXPERT COLLECTION The UvT Expert collection used in the experiments in this paper fits the scenario outlined in Section 3.",
                "The collection is based on the Webwijs (Webwise) system developed at Tilburg University (UvT) in the Netherlands.",
                "Webwijs (http://www.uvt.nl/ webwijs/) is a publicly accessible database of UvT employees who are involved in research or teaching; currently, Webwijs contains information about 1168 experts, each of whom has a page with contact information and, if made available by the expert, a research description and publications list.",
                "In addition, each expert can select expertise areas from a list of 1491 topics and is encouraged to suggest new topics that need to be approved by the Webwijs editor.",
                "Each topic has a separate page that shows all experts associated with that topic and, if available, a list of related topics.",
                "Webwijs is available in Dutch and English, and this bilinguality has been preserved in the collection.",
                "Every Dutch Webwijs page has an English translation.",
                "Not all Dutch topics have an English translation, but the reverse is true: the 981 English topics all have a Dutch equivalent.",
                "About 42% of the experts teach courses at Tilburg University; these courses were also crawled and included in the profile.",
                "In addition, about 27% of the experts link to their academic homepage from their Webwijs page.",
                "These home pages were crawled and added to the collection. (This means that if experts put the full-text versions of their publications on their academic homepage, these were also available for indexing.)",
                "We also obtained 1880 full-text versions of publications from the UvT institutional repository and Dutch English no. of experts 1168 1168 no. of experts with ≥ 1 topic 743 727 no. of topics 1491 981 no. of expert-topic pairs 4318 3251 avg. no. of topics/expert 5.8 5.9 max. no. of topics/expert (no. of experts) 60 (1) 35 (1) min. no. of topics/expert (no. of experts) 1 (74) 1 (106) avg. no. of experts/topic 2.9 3.3 max. no. of experts/topic (no. of topics) 30 (1) 30 (1) min. no. of experts/topic (no. of topics) 1 (615) 1 (346) no. of experts with HP 318 318 no. of experts with CD 318 318 avg. no. of CDs per teaching expert 3.5 3.5 no. of experts with RD 329 313 no. of experts with PUB 734 734 avg. no. of PUBs per expert 27.0 27.0 avg. no. of PUB citations per expert 25.2 25.2 avg. no. of full-text PUBs per expert 1.8 1.8 Table 2: Descriptive statistics of the Dutch and English versions of the UvT Expert collection. converted them to plain text.",
                "We ran the TextCat [23] language identifier to classify the language of the home pages and the fulltext publications.",
                "We restricted ourselves to pages where the classifier was confident about the language used on the page.",
                "This resulted in four document types: research descriptions (RD), course descriptions (CD), publications (PUB; full-text and citationonly versions), and academic homepages (HP).",
                "Everything was bundled into the UvT Expert collection which is available at http: //ilk.uvt.nl/uvt-expert-collection/.",
                "The UvT Expert collection was extracted from a different organizational setting than the W3C collection and differs from it in a number of ways.",
                "The UvT setting is one with relatively small amounts of multilingual data.",
                "Document-author associations are clear and the data is structured and clean.",
                "The collection covers a broad range of expertise areas, as one can typically find on intranets of universities and other knowledge-intensive institutes.",
                "Additionally, our university setting features several types of structure (topical and organizational), as well as multiple document types.",
                "Another important difference between the two data sets is that the expertise areas in the UvT Expert collection are self-selected instead of being based on group membership or assignments by others.",
                "Size is another dimension along which the W3C and UvT Expert collections differ: the latter is the smaller of the two.",
                "Also realistic are the large differences in the amount of information available for each expert.",
                "Utilizing Webwijs is voluntary; 425 Dutch experts did not select any topics at all.",
                "This leaves us with 743 Dutch and 727 English usable expert profiles.",
                "Table 2 provides descriptive statistics for the UvT Expert collection.",
                "Universities tend to have a hierarchical structure that goes from the faculty level, to departments, research groups, down to the individual researchers.",
                "In the UvT Expert collection we have information about the affiliations of researchers with faculties and institutes, providing us with a two-level organizational hierarchy.",
                "Tilburg University has 22 organizational units at the faculty level (including the university office and several research institutes) and 71 departments, which amounts to 3.2 departments per faculty.",
                "As to the topical hierarchy used by Webwijs, 131 of the 1491 topics are top nodes in the hierarchy.",
                "This hierarchy has an average topic chain length of 2.65 and a maximum length of 7 topics. 6.",
                "EVALUATION Below, we evaluate Section 4s models for expert finding and profiling onthe UvT Expert collection.",
                "We detail our research questions and experimental setup, and then present our results. 6.1 Research Questions We address the following research questions.",
                "Both expert finding and profiling rely on the estimations of p(q|ca).",
                "The question is how the models compare on the different tasks, and in the setting of the UvT Expert collection.",
                "In [4], Model 2 outperformed Model 1 on the W3C collection.",
                "How do they compare on our data set?",
                "And how does Model 3 compare to Model 1?",
                "What about performance differences between the two languages in our test collection? 6.2 Experimental Setup The output of our models was evaluated against the self-assigned topic labels, which were treated as relevance judgements.",
                "Results were evaluated separately for English and Dutch.",
                "For English we only used topics for which the Dutch translation was available; for Dutch all topics were considered.",
                "The results were averaged for the queries in the intersection of relevance judgements and results; missing queries do not contribute a value of 0 to the scores.",
                "We use standard information retrieval measures, such as Mean Average Precision (MAP) and Mean Reciprocal Rank (MRR).",
                "We also report the percentage of topics (%q) and candidates (%ca) covered, for the expert finding and profiling tasks, respectively. 6.3 Results Table 1 shows the performance of Model 1, 2, and 3 on the expert finding and profiling tasks.",
                "The rows of the table correspond to the various document types (RD, CD, PUB, and HP) and to their combinations.",
                "RD+CD+PUB+HP is equivalent to the full collection and will be referred as the BASELINE of our experiments.",
                "Looking at Table 1 we see that Model 2 performs the best across the board.",
                "However, when the data is clean and very focused (RD), Model 3 outperforms it in a number of cases.",
                "Model 1 has the best coverage of candidates (%ca) and topics (%q).",
                "The various document types differ in their characteristics and how they improve the finding and profiling tasks.",
                "Expert profiling benefits much from the clean data present in the RD and CD document types, while the publications contribute the most to the expert finding task.",
                "Adding the homepages does not prove to be particularly useful.",
                "When we compare the results across languages, we find that the coverage of English topics (%q) is higher than of the Dutch ones for expert finding.",
                "Apart from that, the scores fall in the same range for both languages.",
                "For the profiling task the coverage of the candidates (%ca) is very similar for both languages.",
                "However, the performance is substantially better for the English topics.",
                "While it is hard to compare scores across collections, we conclude with a brief comparison of the absolute scores in Table 1 to those reported in [3, 4] on the W3C test set (2005 edition).",
                "For expert finding the MAP scores for Model 2 reported here are about 50% higher than the corresponding figures in [4], while our MRR scores are slightly below those in [4].",
                "For expert profiling, the differences are far more dramatic: the MAP scores for Model 2 reported here are around 50% below the scores in [3], while the (best) MRR scores are about the same as those in [3].",
                "The cause for the latter differences seems to reside in the number of knowledge areas considered here-approx. 30 times more than in the W3C setting. 7.",
                "ADVANCED MODELS Now that we have developed and assessed basic language modeling techniques for expertise retrieval, we turn to refined models that exploit special features of our test collection. 7.1 Exploiting knowledge area similarity One way to improve the scoring of a query given a candidate is to consider what other requests the candidate would satisfy and use them as further evidence to support the original query, proportional Expert finding Expert profiling Document types Model 1 Model 2 Model 3 Model 1 Model 2 Model 3 %q MAP MRR %q MAP MRR %q MAP MRR %ca MAP MRR %ca MAP MRR %ca MAP MRR English RD 97.8 0.126 0.269 83.5 0.144 0.311 83.3 0.129 0.271 100 0.089 0.189 39.3 0.232 0.465 41.1 0.166 0.337 CD 97.8 0.118 0.227 91.7 0.123 0.248 91.7 0.118 0.226 32.8 0.188 0.381 32.4 0.195 0.385 32.7 0.203 0.370 PUB 97.8 0.200 0.330 98.0 0.216 0.372 98.0 0.145 0.257 78.9 0.167 0.364 74.5 0.212 0.442 78.9 0.135 0.299 HP 97.8 0.081 0.186 97.4 0.071 0.168 97.2 0.062 0.149 31.2 0.150 0.299 28.8 0.185 0.335 30.1 0.136 0.287 RD+CD 97.8 0.188 0.352 92.9 0.193 0.360 92.9 0.150 0.273 100 0.145 0.286 61.3 0.251 0.477 63.2 0.217 0.416 RD+CD+PUB 97.8 0.235 0.373 98.1 0.277 0.439 98.1 0.178 0.305 100 0.196 0.380 87.2 0.280 0.533 89.5 0.170 0.344 RD+CD+PUB+HP 97.8 0.237 0.372 98.6 0.280 0.441 98.5 0.166 0.293 100 0.199 0.387 88.7 0.281 0.525 90.9 0.169 0.329 Dutch RD 61.3 0.094 0.229 38.4 0.137 0.336 38.3 0.127 0.295 38.0 0.127 0.386 34.1 0.138 0.420 38.0 0.105 0.327 CD 61.3 0.107 0.212 49.7 0.128 0.256 49.7 0.136 0.261 32.5 0.151 0.389 31.8 0.158 0.396 32.5 0.170 0.380 PUB 61.3 0.193 0.319 59.5 0.218 0.368 59.4 0.173 0.291 78.8 0.126 0.364 76.0 0.150 0.424 78.8 0.103 0.294 HP 61.3 0.063 0.169 56.6 0.064 0.175 56.4 0.062 0.163 29.8 0.108 0.308 27.8 0.125 0.338 29.8 0.098 0.255 RD+CD 61.3 0.159 0.314 51.9 0.184 0.360 51.9 0.169 0.324 60.5 0.151 0.410 57.2 0.166 0.431 60.4 0.159 0.384 RD+CD+PUB 61.3 0.244 0.398 61.5 0.260 0.424 61.4 0.210 0.350 90.3 0.165 0.445 88.2 0.189 0.479 90.3 0.126 0.339 RD+CD+PUB+HP 61.3 0.249 0.401 62.6 0.265 0.436 62.6 0.195 0.344 91.9 0.164 0.426 90.1 0.195 0.488 91.9 0.125 0.328 Table 1: Performance of the models on the expert finding and profiling tasks, using different document types and their combinations. %q is the number of topics covered (applies to the expert finding task), %ca is the number of candidates covered (applies to the expert profiling task).",
                "The top and bottom blocks correspond to English and Dutch respectively.",
                "The best scores are in boldface. to how related the other requests are to the original query.",
                "This can be modeled by interpolating between the p(q|ca) and the further supporting evidence from all similar requests q , as follows: p (q|ca) = λp(q|ca) + (1 − λ) X q p(q|q )p(q |ca), (9) where p(q|q ) represents the similarity between the two topics q and q .",
                "To be able to work with similarity methods that are not necessarily probabilities, we set p(q|q ) = w(q,q ) γ , where γ is a normalizing constant, such that γ = P q w(q , q ).",
                "We consider four methods for calculating the similarity score between two topics.",
                "Three approaches are strictly content-based, and establish similarity by examining co-occurrence patterns of topics within the collection, while the last approach exploits the hierarchical structure of topical areas that may be present within an organization (see [7] for further examples of integrating word relationships into language models).",
                "The Kullback-Leibler (KL) divergence metric defined in Eq. 8 provides a measure of how different or similar two probability distributions are.",
                "A topic model is inferred for q and q using the method presented in Section 4.1 to describe the query across the entire vocabulary.",
                "Since a lower KL score means the queries are more similar, we let w(q, q ) = max(KL(θq||·) − KL(θq||θq )).",
                "Pointwise Mutual Information (PMI, [17]) is a measure of association used in information theory to determine the extent of independence between variables.",
                "The dependence between two queries is reflected by the SI(q, q ) score, where scores greater than zero indicate that it is likely that there is a dependence, which we take to mean that the queries are likely to be similar: SI(q, q ) = log p(q, q ) p(q)p(q ) (10) We estimate the probability of a topic p(q) using the number of documents relevant to query q within the collection.",
                "The joint probability p(q, q ) is estimated similarly, by using the concatenation of q and q as a query.",
                "To obtain p(q|q ), we then set w(q, q ) = SI(q, q ) when SI(q, q ) > 0 otherwise w(q, q ) = 0, because we are only interested in including queries that are similar.",
                "The log-likelihood statistic provides another measure of dependence, which is more reliable than the pointwise mutual information measure [17].",
                "Let k1 be the number of co-occurrences of q and q , k2 the number of occurrences of q not co-occurring with q , n1 the total number of occurrences of q , and n2 the total number of topic tokens minus the number of occurrences of q .",
                "Then, let p1 = k1/n1, p2 = k2/n2, and p = (k1 + k2)/(n1 + n2), (q, q ) = 2( (p1, k1, n1) + (p2, k2, n2) − (p, k1, n1) − (p, k2, n2)), where (p, n, k) = k log p + (n − k) log(1 − p).",
                "The higher score indicate that queries are also likely to be similar, thus we set w(q, q ) = (q, q ).",
                "Finally, we also estimate the similarity of two topics based on their distance within the topic hierarchy.",
                "The topic hierarchy is viewed as a directed graph, and for all topic-pairs the shortest path SP(q, q ) is calculated.",
                "We set the similarity score to be the reciprocal of the shortest path: w(q, q ) = 1/SP(q, q ). 7.2 Contextual information Given the hierarchy of an organization, the units to which a person belong are regarded as a context so as to compensate for data sparseness.",
                "We model it as follows: p (q|ca) = 1 − P ou∈OU(ca) λou · p(q|ca) + P ou∈OU(ca) λou · p(q|ou), where OU(ca) is the set of organizational units of which candidate ca is a member of, and p(q|o) expresses the strength of the association between query q and the unit ou.",
                "The latter probability can be estimated using either of the three basic models, by simply replacing ca with ou in the corresponding equations.",
                "An organizational unit is associated with all the documents that its members have authored.",
                "That is, p(d|ou) = maxca∈ou p(d|ca). 7.3 A simple multilingual model For knowledge institutes in Europe, academic or otherwise, a multilingual (or at least bilingual) setting is typical.",
                "The following model builds on a kind of independence assumption: there is no spill-over of expertise/profiles across language boundaries.",
                "While a simplification, this is a sensible first approach.",
                "That is: p (q|ca) =P l∈L λl · p(ql|ca), where L is the set of languages used in the collection, ql is the translation of the query q to language l, and λl is a language specific smoothing parameter, such that P l∈L λl = 1. 8.",
                "ADVANCED MODELS: EVALUATION In this section we present an experimental evaluation of our advanced models.",
                "Expert finding Expert profiling Language Model 1 Model 2 Model 3 Model 1 Model 2 Model 3 %q MAP MRR %q MAP MRR %q MAP MRR %ca MAP MRR %ca MAP MRR %ca MAP MRR English only 97.8 0.237 0.372 98.6 0.280 0.441 98.5 0.166 0.293 100 0.199 0.387 88.7 0.281 0.525 90.9 0.169 0.329 Dutch only 61.3 0.249 0.401 62.6 0.265 0.436 62.6 0.195 0.344 91.9 0.164 0.426 90.1 0.195 0.488 91.9 0.125 0.328 Combination 99.4 0.297 0.444 99.7 0.324 0.491 99.7 0.223 0.388 100 0.241 0.445 92.1 0.313 0.564 93.2 0.224 0.411 Table 3: Performance of the combination of languages on the expert finding and profiling tasks (on candidates).",
                "Best scores for each model are in italic, absolute best scores for the expert finding and profiling tasks are in boldface.",
                "Method Model 1 Model 2 Model 3 MAP MRR MAP MRR MAP MRR English BASELINE 0.296 0.454 0.339 0.509 0.221 0.333 KLDIV 0.291 0.453 0.327 0.503 0.219 0.330 PMI 0.291 0.453 0.337 0.509 0.219 0.331 LL 0.319 0.490 0.360 0.524 0.233 0.368 HDIST 0.299 0.465 0.346 0.537 0.219 0.332 Dutch BASELINE 0.240 0.350 0.271 0.403 0.227 0.389 KLDIV 0.239 0.347 0.253 0.386 0.224 0.385 PMI 0.239 0.350 0.260 0.392 0.227 0.389 LL 0.255 0.372 0.281 0.425 0.231 0.389 HDIST 0.253 0.365 0.271 0.407 0.236 0.402 Method Model 1 Model 2 Model 3 MAP MRR MAP MRR MAP MRR English BASELINE 0.485 0.546 0.499 0.548 0.381 0.416 KLDIV 0.510 0.564 0.513 0.558 0.381 0.416 PMI 0.486 0.546 0.495 0.542 0.407 0.451 LL 0.558 0.589 0.586 0.617 0.408 0.453 HDIST 0.507 0.567 0.512 0.563 0.386 0.420 Dutch BASELINE 0.263 0.313 0.294 0.358 0.262 0.315 KLDIV 0.284 0.336 0.271 0.321 0.261 0.314 PMI 0.265 0.317 0.265 0.316 0.273 0.330 LL 0.312 0.351 0.330 0.377 0.284 0.331 HDIST 0.280 0.327 0.288 0.341 0.266 0.321 Table 4: Performance on the expert finding (top) and profiling (bottom) tasks, using knowledge area similarities.",
                "Runs were evaluated on the main topics set.",
                "Best scores are in boldface. 8.1 Research Questions Our questions follow the refinements presented in the preceding section: Does exploiting the knowledge area similarity improve effectiveness?",
                "Which of the various methods for capturing word relationships is most effective?",
                "Furthermore, is our way of bringing in contextual information useful?",
                "For which tasks?",
                "And finally, is our simple way of combining the monolingual scores sufficient for obtaining significant improvements? 8.2 Experimental setup Given that the self-assessments are also sparse in our collection, in order to be able to measure differences between the various models, we selected a subset of topics, and evaluated (some of the) runs only on this subset.",
                "This set is referred as main topics, and consists of topics that are located at the top level of the topical hierarchy. (A main topic has subtopics, but is not a subtopic of any other topic.)",
                "This main set consists of 132 Dutch and 119 English topics.",
                "The relevance judgements were restricted to the main topic set, but were not expanded with subtopics. 8.3 Exploiting knowledge area similarity Table 4 presents the results.",
                "The four methods used for estimating knowledge-area similarity are KL divergence (KLDIV), PointLang.",
                "Topics Model 1 Model 2 Model 3 MAP MRR MAP MRR MAP MRR Expert finding UK ALL 0.423 0.545 0.654 0.799 0.494 0.629 UK MAIN 0.500 0.621 0.704 0.834 0.587 0.699 NL ALL 0.439 0.560 0.672 0.826 0.480 0.630 NL MAIN 0.440 0.584 0.645 0.816 0.515 0.655 Expert profiling UK ALL 0.240 0.640 0.306 0.778 0.223 0.616 UK MAIN 0.523 0.677 0.519 0.648 0.461 0.587 NL ALL 0.203 0.716 0.254 0.770 0.183 0.627 NL MAIN 0.332 0.576 0.380 0.624 0.332 0.549 Table 5: Evaluating the context models on organizational units. wise mutual information (PMI), log-likelihood (LL), and distance within topic hierarchy (HDIST).",
                "We managed to improve upon the baseline in all cases, but the improvement is more noticeable for the profiling task.",
                "For both tasks, the LL method performed best.",
                "The content-based approaches performed consistently better than HDIST. 8.4 Contextual information A two level hierarchy of organizational units (faculties and institutes) is available in the UvT Expert collection.",
                "The unit a person belongs to is used as a context for that person.",
                "First, we evaluated the models of the organizational units, using all topics (ALL) and only the main topics (MAIN).",
                "An organizational unit is considered to be relevant for a given topic (or vice versa) if at least one member of the unit selected the given topic as an expertise area.",
                "Table 5 reports on the results.",
                "As far as expert finding goes, given a topic, the corresponding organizational unit can be identified with high precision.",
                "However, the expert profiling task shows a different picture: the scores are low, and the task seems hard.",
                "The explanation may be that general concepts (i.e., our main topics) may belong to several organizational units.",
                "Second, we performed another evaluation, where we combined the contextual models with the candidate models (to score candidates again).",
                "Table 6 reports on the results.",
                "We find a positive impact of the context models only for expert finding.",
                "Noticably, for expert finding (and Model 1), it improves over 50% (for English) and over 70% (for Dutch) on MAP.",
                "The poor performance on expert profiling may be due to the fact that context models alone did not perform very well on the profiling task to begin with. 8.5 Multilingual models In this subsection we evaluate the method for combining results across multiple languages that we described in Section 7.3.",
                "In our setting the set of languages consists of English and Dutch: L = {UK, NL}.",
                "The weights on these languages were set to be identical (λUK = λNL = 0.5).",
                "We performed experiments with various λ settings, but did not observe significant differences in performance.",
                "Table 3 reports on the multilingual results, where performance is evaluated on the full topic set.",
                "All three models significantly imLang.",
                "Method Model 1 Model 2 Model 3 MAP MRR MAP MRR MAP MRR Expert finding UK BL 0.296 0.454 0.339 0.509 0.221 0.333 UK CT 0.330 0.491 0.342 0.500 0.228 0.342 NL BL 0.240 0.350 0.271 0.403 0.227 0.389 NL CT 0.251 0.382 0.267 0.410 0.246 0.404 Expert profiling UK BL 0.485 0.546 0.499 0.548 0.381 0.416 UK CT 0.562 0.620 0.508 0.558 0.440 0.486 NL BL 0.263 0.313 0.294 0.358 0.262 0.315 NL CT 0.330 0.384 0.317 0.387 0.294 0.345 Table 6: Performance of the context models (CT) compared to the baseline (BL).",
                "Best scores are in boldface. proved over all measures for both tasks.",
                "The coverage of topics and candidates for the expert finding and profiling tasks, respectively, is close to 100% in all cases.",
                "The relative improvement of the precision scores ranges from 10% to 80%.",
                "These scores demonstrate that despite its simplicity, our method for combining results over multiple languages achieves substantial improvements over the baseline. 9.",
                "CONCLUSIONS In this paper we focused on expertise retrieval (expert finding and profiling) in a new setting of a typical knowledge-intensive organization in which the available data is of high quality, multilingual, and covering a broad range of expertise area.",
                "Typically, the amount of available data in such an organization (e.g., a university, a research institute, or a research lab) is limited when compared to the W3C collection that has mostly been used for the experimental evaluation of expertise retrieval so far.",
                "To examine expertise retrieval in this setting, we introduced (and released) the UvT Expert collection as a representative case of such knowledge intensive organizations.",
                "The new collection reflects the typical properties of knowledge-intensive institutes noted above and also includes several features which may are potentially useful for expertise retrieval, such as topical and organizational structure.",
                "We evaluated how current state-of-the-art models for expert finding and profiling performed in this new setting and then refined these models in order to try and exploit the different characteristics within the data environment (language, topicality, and organizational structure).",
                "We found that current models of expertise retrieval generalize well to this new environment; in addition we found that refining the models to account for the differences results in significant improvements, thus making up for problems caused by data sparseness issues.",
                "Future work includes setting up manual assessments of automatically generated profiles by the employees themselves, especially in cases where the employees have not provided a profile themselves. 10.",
                "ACKNOWLEDGMENTS Krisztian Balog was supported by the Netherlands Organisation for Scientific Research (NWO) under project number 220-80-001.",
                "Maarten de Rijke was also supported by NWO under project numbers 017.001.190, 220-80-001, 264-70-050, 354-20-005, 600.065.120, 612-13-001, 612.000.106, 612.066.302, 612.069.006, 640.001.501, 640.002.501, and by the E.U.",
                "IST programme of the 6th FP for RTD under project MultiMATCH contract IST-033104.",
                "The work of Toine Bogers and Antal van den Bosch was funded by the IOP-MMI-program of SenterNovem / The Dutch Ministry of Economic Affairs, as part of the `A Propos project. 11.",
                "REFERENCES [1] L. Azzopardi.",
                "Incorporating Context in the Language Modeling Framework for ad-hoc Information Retrieval.",
                "PhD thesis, University of Paisley, 2005. [2] K. Balog and M. de Rijke.",
                "Finding similar experts.",
                "In This volume, 2007. [3] K. Balog and M. de Rijke.",
                "Determining expert profiles (with an application to expert finding).",
                "In IJCAI 07: Proc. 20th Intern.",
                "Joint Conf. on Artificial Intelligence, pages 2657-2662, 2007. [4] K. Balog, L. Azzopardi, and M. de Rijke.",
                "Formal models for expert finding in enterprise corpora.",
                "In SIGIR 06: Proc. 29th annual intern.",
                "ACM SIGIR conf. on Research and development in information retrieval, pages 43-50, 2006. [5] I. Becerra-Fernandez.",
                "The role of artificial intelligence technologies in the implementation of people-finder knowledge management systems.",
                "In AAAI Workshop on Bringing Knowledge to Business Processes, March 2000. [6] C. S. Campbell, P. P. Maglio, A. Cozzi, and B. Dom.",
                "Expertise identification using email communications.",
                "In CIKM 03: Proc. twelfth intern. conf. on Information and knowledge management, pages 528531, 2003. [7] G. Cao, J.-Y.",
                "Nie, and J. Bai.",
                "Integrating word relationships into language models.",
                "In SIGIR 05: Proc. 28th annual intern.",
                "ACM SIGIR conf. on Research and development in information retrieval, pages 298-305, 2005. [8] T. M. Cover and J.",
                "A. Thomas.",
                "Elements of Information Theory.",
                "Wiley-Interscience, 1991. [9] N. Craswell, D. Hawking, A. M. Vercoustre, and P. Wilkins.",
                "P@noptic expert: Searching for experts not just for documents.",
                "In Ausweb, 2001. [10] N. Craswell, A. de Vries, and I. Soboroff.",
                "Overview of the TREC2005 Enterprise Track.",
                "In The Fourteenth Text REtrieval Conf.",
                "Proc. (TREC 2005), 2006. [11] T. H. Davenport and L. Prusak.",
                "Working Knowledge: How Organizations Manage What They Know.",
                "Harvard Business School Press, Boston, MA, 1998. [12] T. Dunning.",
                "Accurate methods for the statistics of surprise and coincidence.",
                "Computational Linguistics, 19(1):61-74, 1993. [13] E. Filatova and J. Prager.",
                "Tell me what you do and Ill tell you what you are: Learning occupation-related activities for biographies.",
                "In HLT/EMNLP, 2005. [14] V. Lavrenko and W. B. Croft.",
                "Relevance based language models.",
                "In SIGIR 01: Proc. 24th annual intern.",
                "ACM SIGIR conf. on Research and development in information retrieval, pages 120-127, 2001. [15] V. Lavrenko, M. Choquette, and W. B. Croft.",
                "Cross-lingual relevance models.",
                "In SIGIR 02: Proc. 25th annual intern.",
                "ACM SIGIR conf. on Research and development in information retrieval, pages 175-182, 2002. [16] C. Macdonald and I. Ounis.",
                "Voting for candidates: adapting data fusion techniques for an expert search task.",
                "In CIKM 06: Proc. 15th ACM intern. conf. on Information and knowledge management, pages 387-396, 2006. [17] C. Manning and H. Sch¨utze.",
                "Foundations of Statistical Natural Language Processing.",
                "The MIT Press, 1999. [18] A. Mockus and J. D. Herbsleb.",
                "Expertise browser: a quantitative approach to identifying expertise.",
                "In ICSE 02: Proc. 24th Intern.",
                "Conf. on Software Engineering, pages 503-512, 2002. [19] D. Petkova and W. B. Croft.",
                "Hierarchical language models for expert finding in enterprise corpora.",
                "In Proc.",
                "ICTAI 2006, pages 599-608, 2006. [20] I. Soboroff, A. de Vries, and N. Craswell.",
                "Overview of the TREC 2006 Enterprise Track.",
                "In TREC 2006 Working Notes, 2006. [21] T. Tao, X. Wang, Q. Mei, and C. Zhai.",
                "Language model information retrieval with document expansion.",
                "In HLT-NAACL 2006, 2006. [22] TREC.",
                "Enterprise track, 2005.",
                "URL: http://www.ins.cwi. nl/projects/trec-ent/wiki/. [23] G. van Noord.",
                "TextCat Language Guesser.",
                "URL: http://www. let.rug.nl/˜vannoord/TextCat/. [24] W3C.",
                "The W3C test collection, 2005.",
                "URL: http://research. microsoft.com/users/nickcr/w3c-summary.html."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [],
            "translated_text": "",
            "candidates": [],
            "error": [
                []
            ]
        },
        "language model": {
            "translated_key": "modelo de lenguaje",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Broad Expertise Retrieval in Sparse Data Environments Krisztian Balog ISLA, University of Amsterdam Kruislaan 403, 1098 SJ Amsterdam, The Netherlands kbalog@science.uva.nl Toine Bogers ILK, Tilburg University P.O.",
                "Box 90153, 5000 LE Tilburg, The Netherlands A.M.Bogers@uvt.nl Leif Azzopardi Dept. of Computing Science University of Glasgow, Glasgow, G12 8QQ leif@dcs.gla.ac.uk Maarten de Rijke ISLA, University of Amsterdam Kruislaan 403, 1098 SJ Amsterdam, The Netherlands mdr@science.uva.nl Antal van den Bosch ILK, Tilburg University P.O.",
                "Box 90153, 5000 LE Tilburg, The Netherlands Antal.vdnBosch@uvt.nl ABSTRACT Expertise retrieval has been largely unexplored on data other than the W3C collection.",
                "At the same time, many intranets of universities and other knowledge-intensive organisations offer examples of relatively small but clean multilingual expertise data, covering broad ranges of expertise areas.",
                "We first present two main expertise retrieval tasks, along with a set of baseline approaches based on generative language modeling, aimed at finding expertise relations between topics and people.",
                "For our experimental evaluation, we introduce (and release) a new test set based on a crawl of a university site.",
                "Using this test set, we conduct two series of experiments.",
                "The first is aimed at determining the effectiveness of baseline expertise retrieval methods applied to the new test set.",
                "The second is aimed at assessing refined models that exploit characteristic features of the new test set, such as the organizational structure of the university, and the hierarchical structure of the topics in the test set.",
                "Expertise retrieval models are shown to be robust with respect to environments smaller than the W3C collection, and current techniques appear to be generalizable to other settings.",
                "Categories and Subject Descriptors H.3 [Information Storage and Retrieval]: H.3.1 Content Analysis and Indexing; H.3.3 Information Search and Retrieval; H.3.4 Systems and Software; H.4 [Information Systems Applications]: H.4.2 Types of Systems; H.4.m Miscellaneous General Terms Algorithms, Measurement, Performance, Experimentation 1.",
                "INTRODUCTION An organizations intranet provides a means for exchanging information between employees and for facilitating employee collaborations.",
                "To efficiently and effectively achieve this, it is necessary to provide search facilities that enable employees not only to access documents, but also to identify expert colleagues.",
                "At the TREC Enterprise Track [22] the need to study and understand expertise retrieval has been recognized through the introduction of Expert Finding tasks.",
                "The goal of expert finding is to identify a list of people who are knowledgeable about a given topic.",
                "This task is usually addressed by uncovering associations between people and topics [10]; commonly, a co-occurrence of the name of a person with topics in the same context is assumed to be evidence of expertise.",
                "An alternative task, which using the same idea of people-topic associations, is expert profiling, where the task is to return a list of topics that a person is knowledgeable about [3].",
                "The launch of the Expert Finding task at TREC has generated a lot of interest in expertise retrieval, with rapid progress being made in terms of modeling, algorithms, and evaluation aspects.",
                "However, nearly all of the expert finding or profiling work performed has been validated experimentally using the W3C collection [24] from the Enterprise Track.",
                "While this collection is currently the only publicly available test collection for expertise retrieval tasks, it only represents one type of intranet.",
                "With only one test collection it is not possible to generalize conclusions to other realistic settings.",
                "In this paper we focus on expertise retrieval in a realistic setting that differs from the W3C setting-one in which relatively small amounts of clean, multilingual data are available, that cover a broad range of expertise areas, as can be found on the intranets of universities and other knowledge-intensive organizations.",
                "Typically, this setting features several additional types of structure: topical structure (e.g., topic hierarchies as employed by the organization), organizational structure (faculty, department, ...), as well as multiple types of documents (research and course descriptions, publications, and academic homepages).",
                "This setting is quite different from the W3C setting in ways that might impact upon the performance of expertise retrieval tasks.",
                "We focus on a number of research questions in this paper: Does the relatively small amount of data available on an intranet affect the quality of the topic-person associations that lie at the heart of expertise retrieval algorithms?",
                "How do state-of-the-art algorithms developed on the W3C data set perform in the alternative scenario of the type described above?",
                "More generally, do the lessons from the Expert Finding task at TREC carry over to this setting?",
                "How does the inclusion or exclusion of different documents affect expertise retrieval tasks?",
                "In addition to, how can the topical and organizational structure be used for retrieval purposes?",
                "To answer our research questions, we first present a set of baseline approaches, based on generative language modeling, aimed at finding associations between topics and people.",
                "This allows us to formulate the expert finding and expert profiling tasks in a uniform way, and has the added benefit of allowing us to understand the relations between the two tasks.",
                "For our experimental evaluation, we introduce a new data set (the UvT Expert Collection) which is representative of the type of intranet that we described above.",
                "Our collection is based on publicly available data, crawled from the website of Tilburg University (UvT).",
                "This type of data is particularly interesting, since (1) it is clean, heterogeneous, structured, and focused, but comprises a limited number of documents; (2) contains information on the organizational hierarchy; (3) it is bilingual (English and Dutch); and (4) the list of expertise areas of an individual are provided by the employees themselves.",
                "Using the UvT Expert collection, we conduct two sets of experiments.",
                "The first is aimed at determining the effectiveness of baseline expertise finding and profiling methods in this new setting.",
                "A second group of experiments is aimed at extensions of the baseline methods that exploit characteristic features of the UvT Expert Collection; specifically, we propose and evaluate refined expert finding and profiling methods that incorporate topicality and organizational structure.",
                "Apart from the research questions and data set that we contribute, our main contributions are as follows.",
                "The baseline models developed for expertise finding perform well on the new data set.",
                "While on the W3C setting the expert finding task appears to be more difficult than profiling, for the UvT data the opposite is the case.",
                "We find that profiling on the UvT data set is considerably more difficult than on the W3C set, which we believe is due to the large (but realistic) number of topical areas that we used for profiling: about 1,500 for the UvT set, versus 50 in the W3C case.",
                "Taking the similarity between topics into account can significantly improve retrieval performance.",
                "The best performing similarity measures are content-based, therefore they can be applied on the W3C (and other) settings as well.",
                "Finally, we demonstrate that the organizational structure can be exploited in the form of a context model, improving MAP scores for certain models by up to 70%.",
                "The remainder of this paper is organized as follows.",
                "In the next section we review related work.",
                "Then, in Section 3 we provide detailed descriptions of the expertise retrieval tasks that we address in this paper: expert finding and expert profiling.",
                "In Section 4 we present our baseline models, of which the performance is then assessed in Section 6 using the UvT data set that we introduce in Section 5.",
                "Advanced models exploiting specific features of our data are presented in Section 7 and evaluated in Section 8.",
                "We formulate our conclusions in Section 9. 2.",
                "RELATED WORK Initial approaches to expertise finding often employed databases containing information on the skills and knowledge of each individual in the organization [11].",
                "Most of these tools (usually called yellow pages or people-finding systems) rely on people to self-assess their skills against a predefined set of keywords.",
                "For updating profiles in these systems in an automatic fashion there is a need for intelligent technologies [5].",
                "More recent approaches use specific document sets (such as email [6] or software [18]) to find expertise.",
                "In contrast with focusing on particular document types, there is also an increased interest in the development of systems that index and mine published intranet documents as sources of evidence for expertise.",
                "One such published approach is the P@noptic system [9], which builds a representation of each person by concatenating all documents associated with that person-this is similar to Model 1 of Balog et al. [4], who formalize and compare two methods.",
                "Balog et al.s Model 1 directly models the knowledge of an expert from associated documents, while their Model 2 first locates documents on the topic and then finds the associated experts.",
                "In the reported experiments the second method performs significantly better when there are sufficiently many associated documents per candidate.",
                "Most systems that took part in the 2005 and 2006 editions of the Expert Finding task at TREC implemented (variations on) one of these two models; see [10, 20].",
                "Macdonald and Ounis [16] propose a different approach for ranking candidate expertise with respect to a topic based on data fusion techniques, without using collectionspecific heuristics; they find that applying field-based weighting models improves the ranking of candidates.",
                "Petkova and Croft [19] propose yet another approach, based on a combination of the above Model 1 and 2, explicitly modeling topics.",
                "Turning to other expert retrieval tasks that can also be addressed using topic-people associations, Balog and de Rijke [3] addressed the task of determining topical expert profiles.",
                "While their methods proved to be efficient on the W3C corpus, they require an amount of data that may not be available in the typical knowledge-intensive organization.",
                "Balog and de Rijke [2] study the related task of finding experts that are similar to a small set of experts given as input.",
                "As an aside, creating a textual summary of a person shows some similarities to biography finding, which has received a considerable amount of attention recently; see e.g., [13].",
                "We use generative language modeling to find associations between topics and people.",
                "In our modeling of expert finding and profiling we collect evidence for expertise from multiple sources, in a heterogeneous collection, and integrate it with the co-occurrence of candidates names and query terms-the language modeling setting allows us to do this in a transparent manner.",
                "Our modeling proceeds in two steps.",
                "In the first step, we consider three baseline models, two taken from [4] (the Models 1 and 2 mentioned above), and one a refined version of a model introduced in [3] (which we refer to as Model 3 below); this third model is also similar to the model described by Petkova and Croft [19].",
                "The models we consider in our second round of experiments are mixture models similar to contextual language models [1] and to the expanded documents of Tao et al. [21]; however, the features that we use for definining our expansions-including topical structure and organizational structure-have not been used in this way before. 3.",
                "TASKS In the expertise retrieval scenario that we envisage, users seeking expertise within an organization have access to an interface that combines a search box (where they can search for experts or topics) with navigational structures (of experts and of topics) that allows them to click their way to an expert page (providing the profile of a person) or a topic page (providing a list of experts on the topic).",
                "To feed the above interface, we face two expertise retrieval tasks, expert finding and expert profiling, that we first define and then formalize using generative language models.",
                "In order to model either task, the probability of the query topic being associated to a candidate expert plays a key role in the final estimates for searching and profiling.",
                "By using language models, both the candidates and the query are characterized by distributions of terms in the vocabulary (used in the documents made available by the organization whose expertise retrieval needs we are addressing). 3.1 Expert finding Expert finding involves the task of finding the right person with the appropriate skills and knowledge: Who are the experts on topic X?.",
                "E.g., an employee wants to ascertain who worked on a particular project to find out why particular decisions were made without having to trawl through documentation (if there is any).",
                "Or, they may be in need a trained specialist for consultancy on a specific problem.",
                "Within an organization there are usually many possible candidates who could be experts for given topic.",
                "We can state this problem as follows: What is the probability of a candidate ca being an expert given the query topic q?",
                "That is, we determine p(ca|q), and rank candidates ca according to this probability.",
                "The candidates with the highest probability given the query are deemed the most likely experts for that topic.",
                "The challenge is how to estimate this probability accurately.",
                "Since the query is likely to consist of only a few terms to describe the expertise required, we should be able to obtain a more accurate estimate by invoking Bayes Theorem, and estimating: p(ca|q) = p(q|ca)p(ca) p(q) , (1) where p(ca) is the probability of a candidate and p(q) is the probability of a query.",
                "Since p(q) is a constant, it can be ignored for ranking purposes.",
                "Thus, the probability of a candidate ca being an expert given the query q is proportional to the probability of a query given the candidate p(q|ca), weighted by the a priori belief p(ca) that candidate ca is an expert. p(ca|q) ∝ p(q|ca)p(ca) (2) In this paper our main focus is on estimating the probability of a query given the candidate p(q|ca), because this probability captures the extent to which the candidate knows about the query topic.",
                "Whereas the candidate priors are generally assumed to be uniformand thus will not influence the ranking-it has been demonstrated that a sensible choice of priors may improve the performance [20]. 3.2 Expert profiling While the task of expert searching was concerned with finding experts given a particular topic, the task of expert profiling seeks to answer a related question: What topics does a candidate know about?",
                "Essentially, this turns the questions of expert finding around.",
                "The profiling of an individual candidate involves the identification of areas of skills and knowledge that they have expertise about and an evaluation of the level of proficiency in each of these areas.",
                "This is the candidates topical profile.",
                "Generally, topical profiles within organizations consist of tabular structures which explicitly catalogue the skills and knowledge of each individual in the organization.",
                "However, such practice is limited by the resources available for defining, creating, maintaining, and updating these profiles over time.",
                "By focusing on automatic methods which draw upon the available evidence within the document repositories of an organization, our aim is to reduce the human effort associated with the maintenance of topical profiles1 .",
                "A topical profile of a candidate, then, is defined as a vector where each element i of the vector corresponds to the candidate cas expertise on a given topic ki, (i.e., s(ca, ki)).",
                "Each topic ki defines a particular knowledge area or skill that the organization uses to define the candidates topical profile.",
                "Thus, it is assumed that a list of topics, {k1, . . . , kn}, where n is the number of pre-defined topics, is given: profile(ca) = s(ca, k1), s(ca, k2), . . . , s(ca, kn) . (3) 1 Context and evidence are needed to help users of expertise finding systems to decide whom to contact when seeking expertise in a particular area.",
                "Examples of such context are: Who does she work with?",
                "What are her contact details?",
                "Is she well-connected, just in case she is not able to help us herself?",
                "What is her role in the organization?",
                "Who is her superior?",
                "Collaborators, and affiliations, etc. are all part of the candidates social profile, and can serve as a background against which the systems recommendations should be interpreted.",
                "In this paper we only address the problem of determining topical profiles, and leave social profiling to further work.",
                "We state the problem of quantifying the competence of a person on a certain knowledge area as follows: What is the probability of a knowledge area (ki) being part of the candidates (expertise) profile? where s(ca, ki) is defined by p(ki|ca).",
                "Our task, then, is to estimate p(ki|ca), which is equivalent to the problem of obtaining p(q|ca), where the topic ki is represented as a query topic q, i.e., a sequence of keywords representing the expertise required.",
                "Both the expert finding and profiling tasks rely on the accurate estimation of p(q|ca).",
                "The only difference derives from the prior probability that a person is an expert (p(ca)), which can be incorporated into the expert finding task.",
                "This prior does not apply to the profiling task since the candidate (individual) is fixed. 4.",
                "BASELINE MODELS In this section we describe our baseline models for estimating p(q|ca), i.e., associations between topics and people.",
                "Both expert finding and expert profiling boil down to this estimation.",
                "We employ three models for calculating this probability. 4.1 From topics to candidates Using Candidate Models: Model 1 Model 1 [4] defines the probability of a query given a candidate (p(q|ca)) using standard language modeling techniques, based on a multinomial unigram <br>language model</br>.",
                "For each candidate ca, a candidate <br>language model</br> θca is inferred such that the probability of a term given θca is nonzero for all terms, i.e., p(t|θca) > 0.",
                "From the candidate model the query is generated with the following probability: p(q|θca) = Y t∈q p(t|θca)n(t,q) , where each term t in the query q is sampled identically and independently, and n(t, q) is the number of times t occurs in q.",
                "The candidate <br>language model</br> is inferred as follows: (1) an empirical model p(t|ca) is computed; (2) it is smoothed with background probabilities.",
                "Using the associations between a candidate and a document, the probability p(t|ca) can be approximated by: p(t|ca) = X d p(t|d)p(d|ca), where p(d|ca) is the probability that candidate ca generates a supporting document d, and p(t|d) is the probability of a term t occurring in the document d. We use the maximum-likelihood estimate of a term, that is, the normalised frequency of the term t in document d. The strength of the association between document d and candidate ca expressed by p(d|ca) reflects the degree to which the candidates expertise is described using this document.",
                "The estimation of this probability is presented later, in Section 4.2.",
                "The candidate model is then constructed as a linear interpolation of p(t|ca) and the background model p(t) to ensure there are no zero probabilities, which results in the final estimation: p(q|θca) = (4) Y t∈q ( (1 − λ) X d p(t|d)p(d|ca) ! + λp(t) )n(t,q) .",
                "Model 1 amasses all the term information from all the documents associated with the candidate, and uses this to represent that candidate.",
                "This model is used to predict how likely a candidate would produce a query q.",
                "This can can be intuitively interpreted as the probability of this candidate talking about the query topic, where we assume that this is indicative of their expertise.",
                "Using Document Models: Model 2 Model 2 [4] takes a different approach.",
                "Here, the process is broken into two parts.",
                "Given a candidate ca, (1) a document that is associated with a candidate is selected with probability p(d|ca), and (2) from this document a query q is generated with probability p(q|d).",
                "Then the sum over all documents is taken to obtain p(q|ca), such that: p(q|ca) = X d p(q|d)p(d|ca). (5) The probability of a query given a document is estimated by inferring a document <br>language model</br> θd for each document d in a similar manner as the candidate model was inferred: p(t|θd) = (1 − λ)p(t|d) + λp(t), (6) where p(t|d) is the probability of the term in the document.",
                "The probability of a query given the document model is: p(q|θd) = Y t∈q p(t|θd)n(t,q) .",
                "The final estimate of p(q|ca) is obtained by substituting p(q|d) for p(q|θd) into Eq. 5 (see [4] for full details).",
                "Conceptually, Model 2 differs from Model 1 because the candidate is not directly modeled.",
                "Instead, the document acts like a hidden variable in the process which separates the query from the candidate.",
                "This process is akin to how a user may search for candidates with a standard search engine: initially by finding the documents which are relevant, and then seeing who is associated with that document.",
                "By examining a number of documents the user can obtain an idea of which candidates are more likely to discuss the topic q.",
                "Using Topic Models: Model 3 We introduce a third model, Model 3.",
                "Instead of attempting to model the query generation process via candidate or document models, we represent the query as a topic <br>language model</br> and directly estimate the probability of the candidate p(ca|q).",
                "This approach is similar to the model presented in [3, 19].",
                "As with the previous models, a <br>language model</br> is inferred, but this time for the query.",
                "We adapt the work of Lavrenko and Croft [14] to estimate a topic model from the query.",
                "The procedure is as follows.",
                "Given a collection of documents and a query topic q, it is assumed that there exists an unknown topic model θk that assigns probabilities p(t|θk) to the term occurrences in the topic documents.",
                "Both the query and the documents are samples from θk (as opposed to the previous approaches, where a query is assumed to be sampled from a specific document or candidate model).",
                "The main task is to estimate p(t|θk), the probability of a term given the topic model.",
                "Since the query q is very sparse, and as there are no examples of documents on the topic, this distribution needs to be approximated.",
                "Lavrenko and Croft [14] suggest a reasonable way of obtaining such an approximation, by assuming that p(t|θk) can be approximated by the probability of term t given the query q.",
                "We can then estimate p(t|q) using the joint probability of observing the term t together with the query terms, q1, . . . , qm, and dividing by the joint probability of the query terms: p(t|θk) ≈ p(t|q) = p(t, q1, . . . , qm) p(q1, . . . , qm) = p(t, q1, . . . , qm) P t ∈T p(t , q1, . . . , qm) , where p(q1, . . . , qm) = P t ∈T p(t , q1, . . . , qm), and T is the entire vocabulary of terms.",
                "In order to estimate the joint probability p(t, q1, . . . , qm), we follow [14, 15] and assume t and q1, . . . , qm are mutually independent, once we pick a source distribution from the set of underlying source distributions U.",
                "If we choose U to be a set of document models. then to construct this set, the query q would be issued against the collection, and the top n returned are assumed to be relevant to the topic, and thus treated as samples from the topic model. (Note that candidate models could be used instead.)",
                "With the document models forming U, the joint probability of term and query becomes: p(t, q1, . . . , qm) = X d∈U p(d) ˘ p(t|θd) mY i=1 p(qi|θd) ¯ . (7) Here, p(d) denotes the prior distribution over the set U, which reflects the relevance of the document to the topic.",
                "We assume that p(d) is uniform across U.",
                "In order to rank candidates according to the topic model defined, we use the Kullback-Leibler divergence metric (KL, [8]) to measure the difference between the candidate models and the topic model: KL(θk||θca) = X t p(t|θk) log p(t|θk) p(t|θca) . (8) Candidates with a smaller divergence from the topic model are considered to be more likely experts on that topic.",
                "The candidate model θca is defined in Eq. 4.",
                "By using KL divergence instead of the probability of a candidate given the topic model p(ca|θk), we avoid normalization problems. 4.2 Document-candidate associations For our models we need to be able to estimate the probability p(d|ca), which expresses the extent to which a document d characterizes the candidate ca.",
                "In [4], two methods are presented for estimating this probability, based on the number of person names recognized in a document.",
                "However, in our (intranet) setting it is reasonable to assume that authors of documents can unambiguously be identified (e.g., as the author of an article, the teacher assigned to a course, the owner of a web page, etc.)",
                "Hence, we set p(d|ca) to be 1 if candidate ca is author of document d, otherwise the probability is 0.",
                "In Section 6 we describe how authorship can be determined on different types of documents within the collection. 5.",
                "THE UVT EXPERT COLLECTION The UvT Expert collection used in the experiments in this paper fits the scenario outlined in Section 3.",
                "The collection is based on the Webwijs (Webwise) system developed at Tilburg University (UvT) in the Netherlands.",
                "Webwijs (http://www.uvt.nl/ webwijs/) is a publicly accessible database of UvT employees who are involved in research or teaching; currently, Webwijs contains information about 1168 experts, each of whom has a page with contact information and, if made available by the expert, a research description and publications list.",
                "In addition, each expert can select expertise areas from a list of 1491 topics and is encouraged to suggest new topics that need to be approved by the Webwijs editor.",
                "Each topic has a separate page that shows all experts associated with that topic and, if available, a list of related topics.",
                "Webwijs is available in Dutch and English, and this bilinguality has been preserved in the collection.",
                "Every Dutch Webwijs page has an English translation.",
                "Not all Dutch topics have an English translation, but the reverse is true: the 981 English topics all have a Dutch equivalent.",
                "About 42% of the experts teach courses at Tilburg University; these courses were also crawled and included in the profile.",
                "In addition, about 27% of the experts link to their academic homepage from their Webwijs page.",
                "These home pages were crawled and added to the collection. (This means that if experts put the full-text versions of their publications on their academic homepage, these were also available for indexing.)",
                "We also obtained 1880 full-text versions of publications from the UvT institutional repository and Dutch English no. of experts 1168 1168 no. of experts with ≥ 1 topic 743 727 no. of topics 1491 981 no. of expert-topic pairs 4318 3251 avg. no. of topics/expert 5.8 5.9 max. no. of topics/expert (no. of experts) 60 (1) 35 (1) min. no. of topics/expert (no. of experts) 1 (74) 1 (106) avg. no. of experts/topic 2.9 3.3 max. no. of experts/topic (no. of topics) 30 (1) 30 (1) min. no. of experts/topic (no. of topics) 1 (615) 1 (346) no. of experts with HP 318 318 no. of experts with CD 318 318 avg. no. of CDs per teaching expert 3.5 3.5 no. of experts with RD 329 313 no. of experts with PUB 734 734 avg. no. of PUBs per expert 27.0 27.0 avg. no. of PUB citations per expert 25.2 25.2 avg. no. of full-text PUBs per expert 1.8 1.8 Table 2: Descriptive statistics of the Dutch and English versions of the UvT Expert collection. converted them to plain text.",
                "We ran the TextCat [23] language identifier to classify the language of the home pages and the fulltext publications.",
                "We restricted ourselves to pages where the classifier was confident about the language used on the page.",
                "This resulted in four document types: research descriptions (RD), course descriptions (CD), publications (PUB; full-text and citationonly versions), and academic homepages (HP).",
                "Everything was bundled into the UvT Expert collection which is available at http: //ilk.uvt.nl/uvt-expert-collection/.",
                "The UvT Expert collection was extracted from a different organizational setting than the W3C collection and differs from it in a number of ways.",
                "The UvT setting is one with relatively small amounts of multilingual data.",
                "Document-author associations are clear and the data is structured and clean.",
                "The collection covers a broad range of expertise areas, as one can typically find on intranets of universities and other knowledge-intensive institutes.",
                "Additionally, our university setting features several types of structure (topical and organizational), as well as multiple document types.",
                "Another important difference between the two data sets is that the expertise areas in the UvT Expert collection are self-selected instead of being based on group membership or assignments by others.",
                "Size is another dimension along which the W3C and UvT Expert collections differ: the latter is the smaller of the two.",
                "Also realistic are the large differences in the amount of information available for each expert.",
                "Utilizing Webwijs is voluntary; 425 Dutch experts did not select any topics at all.",
                "This leaves us with 743 Dutch and 727 English usable expert profiles.",
                "Table 2 provides descriptive statistics for the UvT Expert collection.",
                "Universities tend to have a hierarchical structure that goes from the faculty level, to departments, research groups, down to the individual researchers.",
                "In the UvT Expert collection we have information about the affiliations of researchers with faculties and institutes, providing us with a two-level organizational hierarchy.",
                "Tilburg University has 22 organizational units at the faculty level (including the university office and several research institutes) and 71 departments, which amounts to 3.2 departments per faculty.",
                "As to the topical hierarchy used by Webwijs, 131 of the 1491 topics are top nodes in the hierarchy.",
                "This hierarchy has an average topic chain length of 2.65 and a maximum length of 7 topics. 6.",
                "EVALUATION Below, we evaluate Section 4s models for expert finding and profiling onthe UvT Expert collection.",
                "We detail our research questions and experimental setup, and then present our results. 6.1 Research Questions We address the following research questions.",
                "Both expert finding and profiling rely on the estimations of p(q|ca).",
                "The question is how the models compare on the different tasks, and in the setting of the UvT Expert collection.",
                "In [4], Model 2 outperformed Model 1 on the W3C collection.",
                "How do they compare on our data set?",
                "And how does Model 3 compare to Model 1?",
                "What about performance differences between the two languages in our test collection? 6.2 Experimental Setup The output of our models was evaluated against the self-assigned topic labels, which were treated as relevance judgements.",
                "Results were evaluated separately for English and Dutch.",
                "For English we only used topics for which the Dutch translation was available; for Dutch all topics were considered.",
                "The results were averaged for the queries in the intersection of relevance judgements and results; missing queries do not contribute a value of 0 to the scores.",
                "We use standard information retrieval measures, such as Mean Average Precision (MAP) and Mean Reciprocal Rank (MRR).",
                "We also report the percentage of topics (%q) and candidates (%ca) covered, for the expert finding and profiling tasks, respectively. 6.3 Results Table 1 shows the performance of Model 1, 2, and 3 on the expert finding and profiling tasks.",
                "The rows of the table correspond to the various document types (RD, CD, PUB, and HP) and to their combinations.",
                "RD+CD+PUB+HP is equivalent to the full collection and will be referred as the BASELINE of our experiments.",
                "Looking at Table 1 we see that Model 2 performs the best across the board.",
                "However, when the data is clean and very focused (RD), Model 3 outperforms it in a number of cases.",
                "Model 1 has the best coverage of candidates (%ca) and topics (%q).",
                "The various document types differ in their characteristics and how they improve the finding and profiling tasks.",
                "Expert profiling benefits much from the clean data present in the RD and CD document types, while the publications contribute the most to the expert finding task.",
                "Adding the homepages does not prove to be particularly useful.",
                "When we compare the results across languages, we find that the coverage of English topics (%q) is higher than of the Dutch ones for expert finding.",
                "Apart from that, the scores fall in the same range for both languages.",
                "For the profiling task the coverage of the candidates (%ca) is very similar for both languages.",
                "However, the performance is substantially better for the English topics.",
                "While it is hard to compare scores across collections, we conclude with a brief comparison of the absolute scores in Table 1 to those reported in [3, 4] on the W3C test set (2005 edition).",
                "For expert finding the MAP scores for Model 2 reported here are about 50% higher than the corresponding figures in [4], while our MRR scores are slightly below those in [4].",
                "For expert profiling, the differences are far more dramatic: the MAP scores for Model 2 reported here are around 50% below the scores in [3], while the (best) MRR scores are about the same as those in [3].",
                "The cause for the latter differences seems to reside in the number of knowledge areas considered here-approx. 30 times more than in the W3C setting. 7.",
                "ADVANCED MODELS Now that we have developed and assessed basic language modeling techniques for expertise retrieval, we turn to refined models that exploit special features of our test collection. 7.1 Exploiting knowledge area similarity One way to improve the scoring of a query given a candidate is to consider what other requests the candidate would satisfy and use them as further evidence to support the original query, proportional Expert finding Expert profiling Document types Model 1 Model 2 Model 3 Model 1 Model 2 Model 3 %q MAP MRR %q MAP MRR %q MAP MRR %ca MAP MRR %ca MAP MRR %ca MAP MRR English RD 97.8 0.126 0.269 83.5 0.144 0.311 83.3 0.129 0.271 100 0.089 0.189 39.3 0.232 0.465 41.1 0.166 0.337 CD 97.8 0.118 0.227 91.7 0.123 0.248 91.7 0.118 0.226 32.8 0.188 0.381 32.4 0.195 0.385 32.7 0.203 0.370 PUB 97.8 0.200 0.330 98.0 0.216 0.372 98.0 0.145 0.257 78.9 0.167 0.364 74.5 0.212 0.442 78.9 0.135 0.299 HP 97.8 0.081 0.186 97.4 0.071 0.168 97.2 0.062 0.149 31.2 0.150 0.299 28.8 0.185 0.335 30.1 0.136 0.287 RD+CD 97.8 0.188 0.352 92.9 0.193 0.360 92.9 0.150 0.273 100 0.145 0.286 61.3 0.251 0.477 63.2 0.217 0.416 RD+CD+PUB 97.8 0.235 0.373 98.1 0.277 0.439 98.1 0.178 0.305 100 0.196 0.380 87.2 0.280 0.533 89.5 0.170 0.344 RD+CD+PUB+HP 97.8 0.237 0.372 98.6 0.280 0.441 98.5 0.166 0.293 100 0.199 0.387 88.7 0.281 0.525 90.9 0.169 0.329 Dutch RD 61.3 0.094 0.229 38.4 0.137 0.336 38.3 0.127 0.295 38.0 0.127 0.386 34.1 0.138 0.420 38.0 0.105 0.327 CD 61.3 0.107 0.212 49.7 0.128 0.256 49.7 0.136 0.261 32.5 0.151 0.389 31.8 0.158 0.396 32.5 0.170 0.380 PUB 61.3 0.193 0.319 59.5 0.218 0.368 59.4 0.173 0.291 78.8 0.126 0.364 76.0 0.150 0.424 78.8 0.103 0.294 HP 61.3 0.063 0.169 56.6 0.064 0.175 56.4 0.062 0.163 29.8 0.108 0.308 27.8 0.125 0.338 29.8 0.098 0.255 RD+CD 61.3 0.159 0.314 51.9 0.184 0.360 51.9 0.169 0.324 60.5 0.151 0.410 57.2 0.166 0.431 60.4 0.159 0.384 RD+CD+PUB 61.3 0.244 0.398 61.5 0.260 0.424 61.4 0.210 0.350 90.3 0.165 0.445 88.2 0.189 0.479 90.3 0.126 0.339 RD+CD+PUB+HP 61.3 0.249 0.401 62.6 0.265 0.436 62.6 0.195 0.344 91.9 0.164 0.426 90.1 0.195 0.488 91.9 0.125 0.328 Table 1: Performance of the models on the expert finding and profiling tasks, using different document types and their combinations. %q is the number of topics covered (applies to the expert finding task), %ca is the number of candidates covered (applies to the expert profiling task).",
                "The top and bottom blocks correspond to English and Dutch respectively.",
                "The best scores are in boldface. to how related the other requests are to the original query.",
                "This can be modeled by interpolating between the p(q|ca) and the further supporting evidence from all similar requests q , as follows: p (q|ca) = λp(q|ca) + (1 − λ) X q p(q|q )p(q |ca), (9) where p(q|q ) represents the similarity between the two topics q and q .",
                "To be able to work with similarity methods that are not necessarily probabilities, we set p(q|q ) = w(q,q ) γ , where γ is a normalizing constant, such that γ = P q w(q , q ).",
                "We consider four methods for calculating the similarity score between two topics.",
                "Three approaches are strictly content-based, and establish similarity by examining co-occurrence patterns of topics within the collection, while the last approach exploits the hierarchical structure of topical areas that may be present within an organization (see [7] for further examples of integrating word relationships into language models).",
                "The Kullback-Leibler (KL) divergence metric defined in Eq. 8 provides a measure of how different or similar two probability distributions are.",
                "A topic model is inferred for q and q using the method presented in Section 4.1 to describe the query across the entire vocabulary.",
                "Since a lower KL score means the queries are more similar, we let w(q, q ) = max(KL(θq||·) − KL(θq||θq )).",
                "Pointwise Mutual Information (PMI, [17]) is a measure of association used in information theory to determine the extent of independence between variables.",
                "The dependence between two queries is reflected by the SI(q, q ) score, where scores greater than zero indicate that it is likely that there is a dependence, which we take to mean that the queries are likely to be similar: SI(q, q ) = log p(q, q ) p(q)p(q ) (10) We estimate the probability of a topic p(q) using the number of documents relevant to query q within the collection.",
                "The joint probability p(q, q ) is estimated similarly, by using the concatenation of q and q as a query.",
                "To obtain p(q|q ), we then set w(q, q ) = SI(q, q ) when SI(q, q ) > 0 otherwise w(q, q ) = 0, because we are only interested in including queries that are similar.",
                "The log-likelihood statistic provides another measure of dependence, which is more reliable than the pointwise mutual information measure [17].",
                "Let k1 be the number of co-occurrences of q and q , k2 the number of occurrences of q not co-occurring with q , n1 the total number of occurrences of q , and n2 the total number of topic tokens minus the number of occurrences of q .",
                "Then, let p1 = k1/n1, p2 = k2/n2, and p = (k1 + k2)/(n1 + n2), (q, q ) = 2( (p1, k1, n1) + (p2, k2, n2) − (p, k1, n1) − (p, k2, n2)), where (p, n, k) = k log p + (n − k) log(1 − p).",
                "The higher score indicate that queries are also likely to be similar, thus we set w(q, q ) = (q, q ).",
                "Finally, we also estimate the similarity of two topics based on their distance within the topic hierarchy.",
                "The topic hierarchy is viewed as a directed graph, and for all topic-pairs the shortest path SP(q, q ) is calculated.",
                "We set the similarity score to be the reciprocal of the shortest path: w(q, q ) = 1/SP(q, q ). 7.2 Contextual information Given the hierarchy of an organization, the units to which a person belong are regarded as a context so as to compensate for data sparseness.",
                "We model it as follows: p (q|ca) = 1 − P ou∈OU(ca) λou · p(q|ca) + P ou∈OU(ca) λou · p(q|ou), where OU(ca) is the set of organizational units of which candidate ca is a member of, and p(q|o) expresses the strength of the association between query q and the unit ou.",
                "The latter probability can be estimated using either of the three basic models, by simply replacing ca with ou in the corresponding equations.",
                "An organizational unit is associated with all the documents that its members have authored.",
                "That is, p(d|ou) = maxca∈ou p(d|ca). 7.3 A simple multilingual model For knowledge institutes in Europe, academic or otherwise, a multilingual (or at least bilingual) setting is typical.",
                "The following model builds on a kind of independence assumption: there is no spill-over of expertise/profiles across language boundaries.",
                "While a simplification, this is a sensible first approach.",
                "That is: p (q|ca) =P l∈L λl · p(ql|ca), where L is the set of languages used in the collection, ql is the translation of the query q to language l, and λl is a language specific smoothing parameter, such that P l∈L λl = 1. 8.",
                "ADVANCED MODELS: EVALUATION In this section we present an experimental evaluation of our advanced models.",
                "Expert finding Expert profiling <br>language model</br> 1 Model 2 Model 3 Model 1 Model 2 Model 3 %q MAP MRR %q MAP MRR %q MAP MRR %ca MAP MRR %ca MAP MRR %ca MAP MRR English only 97.8 0.237 0.372 98.6 0.280 0.441 98.5 0.166 0.293 100 0.199 0.387 88.7 0.281 0.525 90.9 0.169 0.329 Dutch only 61.3 0.249 0.401 62.6 0.265 0.436 62.6 0.195 0.344 91.9 0.164 0.426 90.1 0.195 0.488 91.9 0.125 0.328 Combination 99.4 0.297 0.444 99.7 0.324 0.491 99.7 0.223 0.388 100 0.241 0.445 92.1 0.313 0.564 93.2 0.224 0.411 Table 3: Performance of the combination of languages on the expert finding and profiling tasks (on candidates).",
                "Best scores for each model are in italic, absolute best scores for the expert finding and profiling tasks are in boldface.",
                "Method Model 1 Model 2 Model 3 MAP MRR MAP MRR MAP MRR English BASELINE 0.296 0.454 0.339 0.509 0.221 0.333 KLDIV 0.291 0.453 0.327 0.503 0.219 0.330 PMI 0.291 0.453 0.337 0.509 0.219 0.331 LL 0.319 0.490 0.360 0.524 0.233 0.368 HDIST 0.299 0.465 0.346 0.537 0.219 0.332 Dutch BASELINE 0.240 0.350 0.271 0.403 0.227 0.389 KLDIV 0.239 0.347 0.253 0.386 0.224 0.385 PMI 0.239 0.350 0.260 0.392 0.227 0.389 LL 0.255 0.372 0.281 0.425 0.231 0.389 HDIST 0.253 0.365 0.271 0.407 0.236 0.402 Method Model 1 Model 2 Model 3 MAP MRR MAP MRR MAP MRR English BASELINE 0.485 0.546 0.499 0.548 0.381 0.416 KLDIV 0.510 0.564 0.513 0.558 0.381 0.416 PMI 0.486 0.546 0.495 0.542 0.407 0.451 LL 0.558 0.589 0.586 0.617 0.408 0.453 HDIST 0.507 0.567 0.512 0.563 0.386 0.420 Dutch BASELINE 0.263 0.313 0.294 0.358 0.262 0.315 KLDIV 0.284 0.336 0.271 0.321 0.261 0.314 PMI 0.265 0.317 0.265 0.316 0.273 0.330 LL 0.312 0.351 0.330 0.377 0.284 0.331 HDIST 0.280 0.327 0.288 0.341 0.266 0.321 Table 4: Performance on the expert finding (top) and profiling (bottom) tasks, using knowledge area similarities.",
                "Runs were evaluated on the main topics set.",
                "Best scores are in boldface. 8.1 Research Questions Our questions follow the refinements presented in the preceding section: Does exploiting the knowledge area similarity improve effectiveness?",
                "Which of the various methods for capturing word relationships is most effective?",
                "Furthermore, is our way of bringing in contextual information useful?",
                "For which tasks?",
                "And finally, is our simple way of combining the monolingual scores sufficient for obtaining significant improvements? 8.2 Experimental setup Given that the self-assessments are also sparse in our collection, in order to be able to measure differences between the various models, we selected a subset of topics, and evaluated (some of the) runs only on this subset.",
                "This set is referred as main topics, and consists of topics that are located at the top level of the topical hierarchy. (A main topic has subtopics, but is not a subtopic of any other topic.)",
                "This main set consists of 132 Dutch and 119 English topics.",
                "The relevance judgements were restricted to the main topic set, but were not expanded with subtopics. 8.3 Exploiting knowledge area similarity Table 4 presents the results.",
                "The four methods used for estimating knowledge-area similarity are KL divergence (KLDIV), PointLang.",
                "Topics Model 1 Model 2 Model 3 MAP MRR MAP MRR MAP MRR Expert finding UK ALL 0.423 0.545 0.654 0.799 0.494 0.629 UK MAIN 0.500 0.621 0.704 0.834 0.587 0.699 NL ALL 0.439 0.560 0.672 0.826 0.480 0.630 NL MAIN 0.440 0.584 0.645 0.816 0.515 0.655 Expert profiling UK ALL 0.240 0.640 0.306 0.778 0.223 0.616 UK MAIN 0.523 0.677 0.519 0.648 0.461 0.587 NL ALL 0.203 0.716 0.254 0.770 0.183 0.627 NL MAIN 0.332 0.576 0.380 0.624 0.332 0.549 Table 5: Evaluating the context models on organizational units. wise mutual information (PMI), log-likelihood (LL), and distance within topic hierarchy (HDIST).",
                "We managed to improve upon the baseline in all cases, but the improvement is more noticeable for the profiling task.",
                "For both tasks, the LL method performed best.",
                "The content-based approaches performed consistently better than HDIST. 8.4 Contextual information A two level hierarchy of organizational units (faculties and institutes) is available in the UvT Expert collection.",
                "The unit a person belongs to is used as a context for that person.",
                "First, we evaluated the models of the organizational units, using all topics (ALL) and only the main topics (MAIN).",
                "An organizational unit is considered to be relevant for a given topic (or vice versa) if at least one member of the unit selected the given topic as an expertise area.",
                "Table 5 reports on the results.",
                "As far as expert finding goes, given a topic, the corresponding organizational unit can be identified with high precision.",
                "However, the expert profiling task shows a different picture: the scores are low, and the task seems hard.",
                "The explanation may be that general concepts (i.e., our main topics) may belong to several organizational units.",
                "Second, we performed another evaluation, where we combined the contextual models with the candidate models (to score candidates again).",
                "Table 6 reports on the results.",
                "We find a positive impact of the context models only for expert finding.",
                "Noticably, for expert finding (and Model 1), it improves over 50% (for English) and over 70% (for Dutch) on MAP.",
                "The poor performance on expert profiling may be due to the fact that context models alone did not perform very well on the profiling task to begin with. 8.5 Multilingual models In this subsection we evaluate the method for combining results across multiple languages that we described in Section 7.3.",
                "In our setting the set of languages consists of English and Dutch: L = {UK, NL}.",
                "The weights on these languages were set to be identical (λUK = λNL = 0.5).",
                "We performed experiments with various λ settings, but did not observe significant differences in performance.",
                "Table 3 reports on the multilingual results, where performance is evaluated on the full topic set.",
                "All three models significantly imLang.",
                "Method Model 1 Model 2 Model 3 MAP MRR MAP MRR MAP MRR Expert finding UK BL 0.296 0.454 0.339 0.509 0.221 0.333 UK CT 0.330 0.491 0.342 0.500 0.228 0.342 NL BL 0.240 0.350 0.271 0.403 0.227 0.389 NL CT 0.251 0.382 0.267 0.410 0.246 0.404 Expert profiling UK BL 0.485 0.546 0.499 0.548 0.381 0.416 UK CT 0.562 0.620 0.508 0.558 0.440 0.486 NL BL 0.263 0.313 0.294 0.358 0.262 0.315 NL CT 0.330 0.384 0.317 0.387 0.294 0.345 Table 6: Performance of the context models (CT) compared to the baseline (BL).",
                "Best scores are in boldface. proved over all measures for both tasks.",
                "The coverage of topics and candidates for the expert finding and profiling tasks, respectively, is close to 100% in all cases.",
                "The relative improvement of the precision scores ranges from 10% to 80%.",
                "These scores demonstrate that despite its simplicity, our method for combining results over multiple languages achieves substantial improvements over the baseline. 9.",
                "CONCLUSIONS In this paper we focused on expertise retrieval (expert finding and profiling) in a new setting of a typical knowledge-intensive organization in which the available data is of high quality, multilingual, and covering a broad range of expertise area.",
                "Typically, the amount of available data in such an organization (e.g., a university, a research institute, or a research lab) is limited when compared to the W3C collection that has mostly been used for the experimental evaluation of expertise retrieval so far.",
                "To examine expertise retrieval in this setting, we introduced (and released) the UvT Expert collection as a representative case of such knowledge intensive organizations.",
                "The new collection reflects the typical properties of knowledge-intensive institutes noted above and also includes several features which may are potentially useful for expertise retrieval, such as topical and organizational structure.",
                "We evaluated how current state-of-the-art models for expert finding and profiling performed in this new setting and then refined these models in order to try and exploit the different characteristics within the data environment (language, topicality, and organizational structure).",
                "We found that current models of expertise retrieval generalize well to this new environment; in addition we found that refining the models to account for the differences results in significant improvements, thus making up for problems caused by data sparseness issues.",
                "Future work includes setting up manual assessments of automatically generated profiles by the employees themselves, especially in cases where the employees have not provided a profile themselves. 10.",
                "ACKNOWLEDGMENTS Krisztian Balog was supported by the Netherlands Organisation for Scientific Research (NWO) under project number 220-80-001.",
                "Maarten de Rijke was also supported by NWO under project numbers 017.001.190, 220-80-001, 264-70-050, 354-20-005, 600.065.120, 612-13-001, 612.000.106, 612.066.302, 612.069.006, 640.001.501, 640.002.501, and by the E.U.",
                "IST programme of the 6th FP for RTD under project MultiMATCH contract IST-033104.",
                "The work of Toine Bogers and Antal van den Bosch was funded by the IOP-MMI-program of SenterNovem / The Dutch Ministry of Economic Affairs, as part of the `A Propos project. 11.",
                "REFERENCES [1] L. Azzopardi.",
                "Incorporating Context in the Language Modeling Framework for ad-hoc Information Retrieval.",
                "PhD thesis, University of Paisley, 2005. [2] K. Balog and M. de Rijke.",
                "Finding similar experts.",
                "In This volume, 2007. [3] K. Balog and M. de Rijke.",
                "Determining expert profiles (with an application to expert finding).",
                "In IJCAI 07: Proc. 20th Intern.",
                "Joint Conf. on Artificial Intelligence, pages 2657-2662, 2007. [4] K. Balog, L. Azzopardi, and M. de Rijke.",
                "Formal models for expert finding in enterprise corpora.",
                "In SIGIR 06: Proc. 29th annual intern.",
                "ACM SIGIR conf. on Research and development in information retrieval, pages 43-50, 2006. [5] I. Becerra-Fernandez.",
                "The role of artificial intelligence technologies in the implementation of people-finder knowledge management systems.",
                "In AAAI Workshop on Bringing Knowledge to Business Processes, March 2000. [6] C. S. Campbell, P. P. Maglio, A. Cozzi, and B. Dom.",
                "Expertise identification using email communications.",
                "In CIKM 03: Proc. twelfth intern. conf. on Information and knowledge management, pages 528531, 2003. [7] G. Cao, J.-Y.",
                "Nie, and J. Bai.",
                "Integrating word relationships into language models.",
                "In SIGIR 05: Proc. 28th annual intern.",
                "ACM SIGIR conf. on Research and development in information retrieval, pages 298-305, 2005. [8] T. M. Cover and J.",
                "A. Thomas.",
                "Elements of Information Theory.",
                "Wiley-Interscience, 1991. [9] N. Craswell, D. Hawking, A. M. Vercoustre, and P. Wilkins.",
                "P@noptic expert: Searching for experts not just for documents.",
                "In Ausweb, 2001. [10] N. Craswell, A. de Vries, and I. Soboroff.",
                "Overview of the TREC2005 Enterprise Track.",
                "In The Fourteenth Text REtrieval Conf.",
                "Proc. (TREC 2005), 2006. [11] T. H. Davenport and L. Prusak.",
                "Working Knowledge: How Organizations Manage What They Know.",
                "Harvard Business School Press, Boston, MA, 1998. [12] T. Dunning.",
                "Accurate methods for the statistics of surprise and coincidence.",
                "Computational Linguistics, 19(1):61-74, 1993. [13] E. Filatova and J. Prager.",
                "Tell me what you do and Ill tell you what you are: Learning occupation-related activities for biographies.",
                "In HLT/EMNLP, 2005. [14] V. Lavrenko and W. B. Croft.",
                "Relevance based language models.",
                "In SIGIR 01: Proc. 24th annual intern.",
                "ACM SIGIR conf. on Research and development in information retrieval, pages 120-127, 2001. [15] V. Lavrenko, M. Choquette, and W. B. Croft.",
                "Cross-lingual relevance models.",
                "In SIGIR 02: Proc. 25th annual intern.",
                "ACM SIGIR conf. on Research and development in information retrieval, pages 175-182, 2002. [16] C. Macdonald and I. Ounis.",
                "Voting for candidates: adapting data fusion techniques for an expert search task.",
                "In CIKM 06: Proc. 15th ACM intern. conf. on Information and knowledge management, pages 387-396, 2006. [17] C. Manning and H. Sch¨utze.",
                "Foundations of Statistical Natural Language Processing.",
                "The MIT Press, 1999. [18] A. Mockus and J. D. Herbsleb.",
                "Expertise browser: a quantitative approach to identifying expertise.",
                "In ICSE 02: Proc. 24th Intern.",
                "Conf. on Software Engineering, pages 503-512, 2002. [19] D. Petkova and W. B. Croft.",
                "Hierarchical language models for expert finding in enterprise corpora.",
                "In Proc.",
                "ICTAI 2006, pages 599-608, 2006. [20] I. Soboroff, A. de Vries, and N. Craswell.",
                "Overview of the TREC 2006 Enterprise Track.",
                "In TREC 2006 Working Notes, 2006. [21] T. Tao, X. Wang, Q. Mei, and C. Zhai.",
                "<br>language model</br> information retrieval with document expansion.",
                "In HLT-NAACL 2006, 2006. [22] TREC.",
                "Enterprise track, 2005.",
                "URL: http://www.ins.cwi. nl/projects/trec-ent/wiki/. [23] G. van Noord.",
                "TextCat Language Guesser.",
                "URL: http://www. let.rug.nl/˜vannoord/TextCat/. [24] W3C.",
                "The W3C test collection, 2005.",
                "URL: http://research. microsoft.com/users/nickcr/w3c-summary.html."
            ],
            "original_annotated_samples": [
                "We employ three models for calculating this probability. 4.1 From topics to candidates Using Candidate Models: Model 1 Model 1 [4] defines the probability of a query given a candidate (p(q|ca)) using standard language modeling techniques, based on a multinomial unigram <br>language model</br>.",
                "For each candidate ca, a candidate <br>language model</br> θca is inferred such that the probability of a term given θca is nonzero for all terms, i.e., p(t|θca) > 0.",
                "The candidate <br>language model</br> is inferred as follows: (1) an empirical model p(t|ca) is computed; (2) it is smoothed with background probabilities.",
                "Then the sum over all documents is taken to obtain p(q|ca), such that: p(q|ca) = X d p(q|d)p(d|ca). (5) The probability of a query given a document is estimated by inferring a document <br>language model</br> θd for each document d in a similar manner as the candidate model was inferred: p(t|θd) = (1 − λ)p(t|d) + λp(t), (6) where p(t|d) is the probability of the term in the document.",
                "Instead of attempting to model the query generation process via candidate or document models, we represent the query as a topic <br>language model</br> and directly estimate the probability of the candidate p(ca|q)."
            ],
            "translated_annotated_samples": [
                "Empleamos tres modelos para calcular esta probabilidad. 4.1 De temas a candidatos Utilizando Modelos de Candidatos: El Modelo 1 [4] define la probabilidad de una consulta dada un candidato (p(q|ca)) utilizando técnicas estándar de modelado de lenguaje, basadas en un <br>modelo de lenguaje</br> multinomial de unigrama.",
                "Para cada candidato ca, se infiere un <br>modelo de lenguaje</br> candidato θca de tal manera que la probabilidad de un término dado θca no sea cero para todos los términos, es decir, p(t|θca) > 0.",
                "El <br>modelo de lenguaje</br> candidato se infiere de la siguiente manera: (1) se calcula un modelo empírico p(t|ca); (2) se suaviza con probabilidades de fondo.",
                "Entonces se toma la suma sobre todos los documentos para obtener p(q|ca), de modo que: p(q|ca) = Σ d p(q|d)p(d|ca). (5) La probabilidad de una consulta dada un documento se estima inferiendo un <br>modelo de lenguaje</br> del documento θd para cada documento d de manera similar a como se infería el modelo candidato: p(t|θd) = (1 − λ)p(t|d) + λp(t), (6) donde p(t|d) es la probabilidad del término en el documento.",
                "En lugar de intentar modelar el proceso de generación de consultas a través de modelos de candidatos o documentos, representamos la consulta como un <br>modelo de lenguaje</br> de temas y estimamos directamente la probabilidad del candidato p(ca|q)."
            ],
            "translated_text": "Recuperación de amplia experiencia en entornos de datos dispersos Krisztian Balog ISLA, Universidad de Ámsterdam Kruislaan 403, 1098 SJ Ámsterdam, Países Bajos kbalog@science.uva.nl Toine Bogers ILK, Universidad de Tilburg P.O. Caja 90153, 5000 LE Tilburg, Países Bajos A.M.Bogers@uvt.nl Leif Azzopardi Dept. de Ciencias de la Computación Universidad de Glasgow, Glasgow, G12 8QQ leif@dcs.gla.ac.uk Maarten de Rijke ISLA, Universidad de Ámsterdam Kruislaan 403, 1098 SJ Ámsterdam, Países Bajos mdr@science.uva.nl Antal van den Bosch ILK, Universidad de Tilburg P.O. La recuperación de la experiencia ha sido ampliamente inexplorada en datos distintos a la colección de la W3C. Al mismo tiempo, muchas intranets de universidades y otras organizaciones intensivas en conocimiento ofrecen ejemplos de datos de expertos multilingües relativamente pequeños pero precisos, que abarcan amplias áreas de especialización. Primero presentamos dos tareas principales de recuperación de expertos, junto con un conjunto de enfoques de referencia basados en modelado del lenguaje generativo, con el objetivo de encontrar relaciones de experticia entre temas y personas. Para nuestra evaluación experimental, introducimos (y liberamos) un nuevo conjunto de pruebas basado en un rastreo de un sitio universitario. Usando este conjunto de pruebas, llevamos a cabo dos series de experimentos. El primero tiene como objetivo determinar la efectividad de los métodos de recuperación de conocimientos básicos aplicados al nuevo conjunto de pruebas. El segundo tiene como objetivo evaluar modelos refinados que exploten las características distintivas del nuevo conjunto de pruebas, como la estructura organizativa de la universidad y la estructura jerárquica de los temas en el conjunto de pruebas. Los modelos de recuperación de experiencia se muestran robustos con respecto a entornos más pequeños que la colección de la W3C, y las técnicas actuales parecen ser generalizables a otros contextos. Categorías y Descriptores de Asignaturas H.3 [Almacenamiento y Recuperación de Información]: H.3.1 Análisis de Contenido e Indexación; H.3.3 Búsqueda y Recuperación de Información; H.3.4 Sistemas y Software; H.4 [Aplicaciones de Sistemas de Información]: H.4.2 Tipos de Sistemas; H.4.m Términos Generales Misceláneos Algoritmos, Medición, Rendimiento, Experimentación 1. INTRODUCCIÓN La intranet de una organización proporciona un medio para intercambiar información entre empleados y facilitar colaboraciones entre ellos. Para lograr esto de manera eficiente y efectiva, es necesario proporcionar instalaciones de búsqueda que permitan a los empleados no solo acceder a documentos, sino también identificar a colegas expertos. En la pista de Empresas de TREC [22] se ha reconocido la necesidad de estudiar y comprender la recuperación de expertos a través de la introducción de tareas de Búsqueda de Expertos. El objetivo de la búsqueda de expertos es identificar una lista de personas que tienen conocimientos sobre un tema específico. Esta tarea suele abordarse descubriendo asociaciones entre personas y temas [10]; comúnmente, se asume que la co-ocurrencia del nombre de una persona con temas en el mismo contexto es evidencia de experiencia. Una tarea alternativa, que utiliza la misma idea de asociaciones entre personas y temas, es el perfilado de expertos, donde la tarea consiste en devolver una lista de temas sobre los que una persona tiene conocimiento [3]. El lanzamiento de la tarea de Búsqueda de Expertos en TREC ha generado mucho interés en la recuperación de expertos, con un rápido progreso en términos de modelado, algoritmos y aspectos de evaluación. Sin embargo, casi todo el trabajo de búsqueda o perfilado de expertos realizado ha sido validado experimentalmente utilizando la colección de W3C [24] de la pista empresarial. Si bien esta colección es actualmente la única colección de pruebas disponible públicamente para tareas de recuperación de expertos, solo representa un tipo de intranet. Con solo una colección de pruebas no es posible generalizar conclusiones a otros escenarios realistas. En este artículo nos enfocamos en la recuperación de expertos en un entorno realista que difiere del entorno de la W3C, uno en el que hay cantidades relativamente pequeñas de datos limpios y multilingües disponibles, que abarcan una amplia gama de áreas de expertise, como se puede encontrar en las intranets de universidades y otras organizaciones intensivas en conocimiento. Normalmente, esta configuración incluye varios tipos adicionales de estructuras: estructura temática (por ejemplo, jerarquías de temas utilizadas por la organización), estructura organizativa (facultad, departamento, ...), así como múltiples tipos de documentos (descripciones de investigación y cursos, publicaciones y páginas web académicas). Este entorno es bastante diferente del entorno del W3C de maneras que podrían afectar el rendimiento de las tareas de recuperación de conocimientos especializados. Nos enfocamos en varias preguntas de investigación en este artículo: ¿La cantidad relativamente pequeña de datos disponibles en una intranet afecta la calidad de las asociaciones tema-persona que son fundamentales en los algoritmos de recuperación de expertos? ¿Cómo se desempeñan los algoritmos de vanguardia desarrollados en el conjunto de datos del W3C en el escenario alternativo del tipo descrito anteriormente? Más en general, ¿se pueden aplicar las lecciones de la tarea de Búsqueda de Expertos en TREC a este contexto? ¿Cómo afecta la inclusión o exclusión de diferentes documentos a las tareas de recuperación de expertos? Además, ¿cómo se puede utilizar la estructura temática y organizativa con fines de recuperación? Para responder a nuestras preguntas de investigación, primero presentamos un conjunto de enfoques de línea base, basados en modelado de lenguaje generativo, con el objetivo de encontrar asociaciones entre temas y personas. Esto nos permite formular las tareas de búsqueda de expertos y perfilado de expertos de manera uniforme, y tiene el beneficio adicional de permitirnos entender las relaciones entre las dos tareas. Para nuestra evaluación experimental, introducimos un nuevo conjunto de datos (la Colección de Expertos de UvT) que es representativo del tipo de intranet que describimos anteriormente. Nuestra colección se basa en datos de acceso público, extraídos del sitio web de la Universidad de Tilburg (UvT). Este tipo de datos es particularmente interesante, ya que (1) es limpio, heterogéneo, estructurado y enfocado, pero comprende un número limitado de documentos; (2) contiene información sobre la jerarquía organizativa; (3) es bilingüe (inglés y holandés); y (4) la lista de áreas de especialización de un individuo es proporcionada por los propios empleados. Usando la colección de expertos de UvT, llevamos a cabo dos conjuntos de experimentos. El primero tiene como objetivo determinar la efectividad de los métodos de búsqueda y perfilado de experiencia inicial en este nuevo entorno. Un segundo grupo de experimentos está dirigido a extensiones de los métodos base que aprovechan las características distintivas de la Colección de Expertos de UvT; específicamente, proponemos y evaluamos métodos refinados de búsqueda y perfilado de expertos que incorporan la topicalidad y la estructura organizativa. Además de las preguntas de investigación y el conjunto de datos que aportamos, nuestras principales contribuciones son las siguientes. Los modelos base desarrollados para encontrar expertos funcionan bien en el nuevo conjunto de datos. Mientras que en el entorno del W3C la tarea de encontrar expertos parece ser más difícil que el perfilado, para los datos de UvT ocurre lo contrario. Encontramos que el perfilado en el conjunto de datos UvT es considerablemente más difícil que en el conjunto de datos W3C, lo cual creemos que se debe al gran (pero realista) número de áreas temáticas que utilizamos para el perfilado: alrededor de 1,500 para el conjunto de datos UvT, frente a 50 en el caso de W3C. Teniendo en cuenta la similitud entre los temas puede mejorar significativamente el rendimiento de recuperación. Las medidas de similitud de mejor rendimiento son basadas en contenido, por lo tanto también se pueden aplicar en entornos de la W3C (y otros). Finalmente, demostramos que la estructura organizativa puede ser explotada en forma de un modelo de contexto, mejorando los puntajes de MAP para ciertos modelos hasta en un 70%. El resto de este documento está organizado de la siguiente manera. En la siguiente sección revisamos el trabajo relacionado. Luego, en la Sección 3 proporcionamos descripciones detalladas de las tareas de recuperación de expertos que abordamos en este artículo: la búsqueda de expertos y la creación de perfiles de expertos. En la Sección 4 presentamos nuestros modelos base, cuyo rendimiento luego es evaluado en la Sección 6 utilizando el conjunto de datos UvT que presentamos en la Sección 5. Los modelos avanzados que explotan características específicas de nuestros datos se presentan en la Sección 7 y se evalúan en la Sección 8. Formulamos nuestras conclusiones en la Sección 9.2. Trabajos relacionados Los enfoques iniciales para encontrar expertos a menudo empleaban bases de datos que contenían información sobre las habilidades y conocimientos de cada individuo en la organización [11]. La mayoría de estas herramientas (generalmente llamadas páginas amarillas o sistemas de búsqueda de personas) dependen de que las personas evalúen sus habilidades frente a un conjunto predefinido de palabras clave. Para actualizar perfiles en estos sistemas de forma automática, se necesita de tecnologías inteligentes [5]. Enfoques más recientes utilizan conjuntos de documentos específicos (como correos electrónicos [6] o software [18]) para encontrar experiencia. En contraste con centrarse en tipos de documentos particulares, también hay un interés creciente en el desarrollo de sistemas que indexan y extraen información de documentos publicados en intranets como fuentes de evidencia de experiencia. Un enfoque publicado es el sistema P@noptic [9], que construye una representación de cada persona concatenando todos los documentos asociados con esa persona, similar al Modelo 1 de Balog et al. [4], quienes formalizan y comparan dos métodos. El Modelo 1 de Balog et al. modela directamente el conocimiento de un experto a partir de documentos asociados, mientras que su Modelo 2 primero localiza documentos sobre el tema y luego encuentra a los expertos asociados. En los experimentos reportados, el segundo método tiene un rendimiento significativamente mejor cuando hay suficientes documentos asociados por candidato. La mayoría de los sistemas que participaron en las ediciones de 2005 y 2006 de la tarea de Búsqueda de Expertos en TREC implementaron (variaciones de) uno de estos dos modelos; ver [10, 20]. Macdonald y Ounis [16] proponen un enfoque diferente para clasificar la experiencia de los candidatos con respecto a un tema basado en técnicas de fusión de datos, sin utilizar heurísticas específicas de la colección; encuentran que la aplicación de modelos de ponderación basados en campos mejora la clasificación de los candidatos. Petkova y Croft [19] proponen otro enfoque, basado en una combinación de los Modelos 1 y 2 anteriores, modelando explícitamente los temas. Al abordar otras tareas de recuperación de expertos que también pueden abordarse utilizando asociaciones entre temas y personas, Balog y de Rijke [3] abordaron la tarea de determinar perfiles de expertos temáticos. Si bien sus métodos resultaron ser eficientes en el corpus de la W3C, requieren una cantidad de datos que puede que no esté disponible en la típica organización intensiva en conocimiento. Balog y de Rijke [2] estudian la tarea relacionada de encontrar expertos que sean similares a un pequeño conjunto de expertos dados como entrada. Por cierto, crear un resumen textual de una persona muestra algunas similitudes con la búsqueda de biografías, la cual ha recibido una considerable cantidad de atención recientemente; ver por ejemplo, [13]. Utilizamos modelado de lenguaje generativo para encontrar asociaciones entre temas y personas. En nuestro modelado de búsqueda y perfilado de expertos recopilamos evidencia de experiencia de múltiples fuentes, en una colección heterogénea, e integramos esta información con la co-ocurrencia de nombres de candidatos y términos de consulta: el entorno de modelado de lenguaje nos permite hacer esto de manera transparente. Nuestro modelado avanza en dos pasos. En el primer paso, consideramos tres modelos base, dos tomados de [4] (los Modelos 1 y 2 mencionados anteriormente), y uno una versión refinada de un modelo introducido en [3] (al que nos referimos como Modelo 3 a continuación); este tercer modelo también es similar al modelo descrito por Petkova y Croft [19]. Los modelos que consideramos en nuestra segunda ronda de experimentos son modelos de mezcla similares a los modelos de lenguaje contextual [1] y a los documentos ampliados de Tao et al. [21]; sin embargo, las características que utilizamos para definir nuestras expansiones, incluida la estructura temática y la estructura organizativa, no se han utilizado de esta manera antes. 3. En el escenario de recuperación de expertos que imaginamos, los usuarios que buscan expertos dentro de una organización tienen acceso a una interfaz que combina un cuadro de búsqueda (donde pueden buscar expertos o temas) con estructuras de navegación (de expertos y de temas) que les permite hacer clic hasta llegar a una página de experto (que proporciona el perfil de una persona) o a una página de tema (que proporciona una lista de expertos en el tema). Para alimentar la interfaz anterior, nos enfrentamos a dos tareas de recuperación de expertos, la búsqueda de expertos y el perfilado de expertos, que primero definimos y luego formalizamos utilizando modelos de lenguaje generativos. Para modelar cualquiera de las tareas, la probabilidad de que el tema de la consulta esté asociado a un experto candidato juega un papel clave en las estimaciones finales para la búsqueda y el perfilado. Al utilizar modelos de lenguaje, tanto los candidatos como la consulta se caracterizan por distribuciones de términos en el vocabulario (utilizado en los documentos puestos a disposición por la organización cuyas necesidades de recuperación de experiencia estamos abordando). 3.1 Búsqueda de expertos La búsqueda de expertos implica la tarea de encontrar a la persona adecuada con las habilidades y conocimientos apropiados: ¿Quiénes son los expertos en el tema X?. Por ejemplo, un empleado quiere averiguar quién trabajó en un proyecto en particular para descubrir por qué se tomaron decisiones específicas sin tener que revisar minuciosamente la documentación (si es que la hay). O pueden necesitar un especialista capacitado para consultoría sobre un problema específico. Dentro de una organización, generalmente hay muchos posibles candidatos que podrían ser expertos en un tema dado. Podemos plantear este problema de la siguiente manera: ¿Cuál es la probabilidad de que un candidato ca sea un experto dado el tema de consulta q? Es decir, determinamos p(ca|q) y clasificamos a los candidatos ca de acuerdo con esta probabilidad. Los candidatos con la probabilidad más alta dada la consulta son considerados los expertos más probables en ese tema. El desafío es cómo estimar esta probabilidad con precisión. Dado que es probable que la consulta consista solo en unos pocos términos para describir la experiencia requerida, deberíamos poder obtener una estimación más precisa al invocar el Teorema de Bayes, y estimar: p(ca|q) = p(q|ca)p(ca) p(q) , (1) donde p(ca) es la probabilidad de un candidato y p(q) es la probabilidad de una consulta. Dado que p(q) es una constante, se puede ignorar para fines de clasificación. Por lo tanto, la probabilidad de que un candidato ca sea un experto dado la consulta q es proporcional a la probabilidad de una consulta dada el candidato p(q|ca), ponderada por la creencia a priori p(ca) de que el candidato ca es un experto. p(ca|q) ∝ p(q|ca)p(ca) (2) En este artículo, nuestro enfoque principal se centra en estimar la probabilidad de una consulta dada el candidato p(q|ca), ya que esta probabilidad captura en qué medida el candidato conoce sobre el tema de la consulta. Si bien se asume generalmente que los priors del candidato son uniformes y, por lo tanto, no influirán en la clasificación, se ha demostrado que una elección sensata de priors puede mejorar el rendimiento [20]. Perfilado de expertos. Mientras que la tarea de búsqueda de expertos se centraba en encontrar expertos en un tema específico, la tarea de perfilado de expertos busca responder a una pregunta relacionada: ¿Sobre qué temas tiene conocimiento un candidato? Básicamente, esto da un giro a las preguntas sobre la búsqueda de expertos. El perfilado de un candidato individual implica la identificación de áreas de habilidades y conocimientos en las que tienen experiencia, así como una evaluación del nivel de competencia en cada una de estas áreas. Este es el perfil temático de los candidatos. Generalmente, los perfiles temáticos dentro de las organizaciones consisten en estructuras tabulares que catalogan explícitamente las habilidades y conocimientos de cada individuo en la organización. Sin embargo, esta práctica está limitada por los recursos disponibles para definir, crear, mantener y actualizar estos perfiles con el tiempo. Al centrarnos en métodos automáticos que se basan en la evidencia disponible dentro de los repositorios de documentos de una organización, nuestro objetivo es reducir el esfuerzo humano asociado con el mantenimiento de perfiles temáticos. Un perfil temático de un candidato, entonces, se define como un vector donde cada elemento i del vector corresponde a la experiencia del candidato en un tema dado ki, (es decir, s(ca, ki)). Cada tema ki define un área de conocimiento o habilidad particular que la organización utiliza para definir el perfil temático de los candidatos. Por lo tanto, se asume que se proporciona una lista de temas, {k1, . . . , kn}, donde n es el número de temas predefinidos: perfil(ca) = s(ca, k1), s(ca, k2), . . . , s(ca, kn) . Se necesita contexto y evidencia para ayudar a los usuarios de sistemas de búsqueda de expertos a decidir a quién contactar al buscar experiencia en un área particular. Ejemplos de dicho contexto son: ¿Con quién trabaja ella? ¿Cuáles son sus datos de contacto? ¿Está bien conectada, por si acaso no puede ayudarnos ella misma? ¿Cuál es su rol en la organización? ¿Quién es su superior? Los colaboradores, afiliaciones, etc. son parte del perfil social de los candidatos y pueden servir como antecedentes para interpretar las recomendaciones del sistema. En este documento solo abordamos el problema de determinar perfiles temáticos, dejando el perfilado social para trabajos futuros. Planteamos el problema de cuantificar la competencia de una persona en un área de conocimiento específica de la siguiente manera: ¿Cuál es la probabilidad de que un área de conocimiento (ki) forme parte del perfil de candidatos (expertise)? donde s(ca, ki) está definido por p(ki|ca). Nuestra tarea, entonces, es estimar p(ki|ca), lo cual es equivalente al problema de obtener p(q|ca), donde el tema ki se representa como un tema de consulta q, es decir, una secuencia de palabras clave que representan la experiencia requerida. Tanto la tarea de encontrar expertos como la de perfilar expertos dependen de la estimación precisa de p(q|ca). La única diferencia proviene de la probabilidad previa de que una persona sea experta (p(ca)), la cual puede ser incorporada en la tarea de encontrar expertos. Este antecedente no se aplica a la tarea de perfilado ya que el candidato (individuo) está fijo. 4. MODELOS DE REFERENCIA En esta sección describimos nuestros modelos de referencia para estimar p(q|ca), es decir, las asociaciones entre temas y personas. Tanto la búsqueda de expertos como el perfilado de expertos se reducen a esta estimación. Empleamos tres modelos para calcular esta probabilidad. 4.1 De temas a candidatos Utilizando Modelos de Candidatos: El Modelo 1 [4] define la probabilidad de una consulta dada un candidato (p(q|ca)) utilizando técnicas estándar de modelado de lenguaje, basadas en un <br>modelo de lenguaje</br> multinomial de unigrama. Para cada candidato ca, se infiere un <br>modelo de lenguaje</br> candidato θca de tal manera que la probabilidad de un término dado θca no sea cero para todos los términos, es decir, p(t|θca) > 0. A partir del modelo de candidato, la consulta se genera con la siguiente probabilidad: p(q|θca) = Y t∈q p(t|θca)n(t,q), donde cada término t en la consulta q se muestrea de manera idéntica e independiente, y n(t, q) es el número de veces que t ocurre en q. El <br>modelo de lenguaje</br> candidato se infiere de la siguiente manera: (1) se calcula un modelo empírico p(t|ca); (2) se suaviza con probabilidades de fondo. Usando las asociaciones entre un candidato y un documento, la probabilidad p(t|ca) puede aproximarse por: p(t|ca) = X d p(t|d)p(d|ca), donde p(d|ca) es la probabilidad de que el candidato ca genere un documento de apoyo d, y p(t|d) es la probabilidad de que un término t ocurra en el documento d. Utilizamos la estimación de máxima verosimilitud de un término, es decir, la frecuencia normalizada del término t en el documento d. La fuerza de la asociación entre el documento d y el candidato ca expresada por p(d|ca) refleja el grado en que la experiencia de los candidatos se describe utilizando este documento. La estimación de esta probabilidad se presenta más adelante, en la Sección 4.2. El modelo candidato se construye entonces como una interpolación lineal de p(t|ca) y el modelo de fondo p(t) para asegurar que no haya probabilidades nulas, lo que resulta en la estimación final: p(q|θca) = (4) Y t∈q ( (1 − λ) X d p(t|d)p(d|ca) ! + λp(t) )n(t,q) . El Modelo 1 recopila toda la información de términos de todos los documentos asociados con el candidato, y la utiliza para representar a ese candidato. Este modelo se utiliza para predecir qué tan probable es que un candidato produzca una consulta q. Esto puede interpretarse intuitivamente como la probabilidad de que este candidato hable sobre el tema de la consulta, donde asumimos que esto es indicativo de su experiencia. Usando Modelos de Documentos: El Modelo 2 [4] toma un enfoque diferente. Aquí, el proceso se divide en dos partes. Dado un candidato ca, (1) se selecciona un documento asociado con un candidato con probabilidad p(d|ca), y (2) a partir de este documento se genera una consulta q con probabilidad p(q|d). Entonces se toma la suma sobre todos los documentos para obtener p(q|ca), de modo que: p(q|ca) = Σ d p(q|d)p(d|ca). (5) La probabilidad de una consulta dada un documento se estima inferiendo un <br>modelo de lenguaje</br> del documento θd para cada documento d de manera similar a como se infería el modelo candidato: p(t|θd) = (1 − λ)p(t|d) + λp(t), (6) donde p(t|d) es la probabilidad del término en el documento. La probabilidad de una consulta dada el modelo de documento es: p(q|θd) = Y t∈q p(t|θd)n(t,q) . La estimación final de p(q|ca) se obtiene sustituyendo p(q|d) por p(q|θd) en la Ecuación 5 (ver [4] para más detalles). Conceptualmente, el Modelo 2 difiere del Modelo 1 porque el candidato no está modelado directamente. En cambio, el documento actúa como una variable oculta en el proceso que separa la consulta del candidato. Este proceso es similar a cómo un usuario puede buscar candidatos con un motor de búsqueda estándar: inicialmente encontrando los documentos relevantes y luego viendo quién está asociado con ese documento. Al examinar una serie de documentos, el usuario puede obtener una idea de qué candidatos son más propensos a discutir el tema q. Usando Modelos de Temas: Modelo 3 Introducimos un tercer modelo, Modelo 3. En lugar de intentar modelar el proceso de generación de consultas a través de modelos de candidatos o documentos, representamos la consulta como un <br>modelo de lenguaje</br> de temas y estimamos directamente la probabilidad del candidato p(ca|q). ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "expert colleague": {
            "translated_key": "colegas expertos",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Broad Expertise Retrieval in Sparse Data Environments Krisztian Balog ISLA, University of Amsterdam Kruislaan 403, 1098 SJ Amsterdam, The Netherlands kbalog@science.uva.nl Toine Bogers ILK, Tilburg University P.O.",
                "Box 90153, 5000 LE Tilburg, The Netherlands A.M.Bogers@uvt.nl Leif Azzopardi Dept. of Computing Science University of Glasgow, Glasgow, G12 8QQ leif@dcs.gla.ac.uk Maarten de Rijke ISLA, University of Amsterdam Kruislaan 403, 1098 SJ Amsterdam, The Netherlands mdr@science.uva.nl Antal van den Bosch ILK, Tilburg University P.O.",
                "Box 90153, 5000 LE Tilburg, The Netherlands Antal.vdnBosch@uvt.nl ABSTRACT Expertise retrieval has been largely unexplored on data other than the W3C collection.",
                "At the same time, many intranets of universities and other knowledge-intensive organisations offer examples of relatively small but clean multilingual expertise data, covering broad ranges of expertise areas.",
                "We first present two main expertise retrieval tasks, along with a set of baseline approaches based on generative language modeling, aimed at finding expertise relations between topics and people.",
                "For our experimental evaluation, we introduce (and release) a new test set based on a crawl of a university site.",
                "Using this test set, we conduct two series of experiments.",
                "The first is aimed at determining the effectiveness of baseline expertise retrieval methods applied to the new test set.",
                "The second is aimed at assessing refined models that exploit characteristic features of the new test set, such as the organizational structure of the university, and the hierarchical structure of the topics in the test set.",
                "Expertise retrieval models are shown to be robust with respect to environments smaller than the W3C collection, and current techniques appear to be generalizable to other settings.",
                "Categories and Subject Descriptors H.3 [Information Storage and Retrieval]: H.3.1 Content Analysis and Indexing; H.3.3 Information Search and Retrieval; H.3.4 Systems and Software; H.4 [Information Systems Applications]: H.4.2 Types of Systems; H.4.m Miscellaneous General Terms Algorithms, Measurement, Performance, Experimentation 1.",
                "INTRODUCTION An organizations intranet provides a means for exchanging information between employees and for facilitating employee collaborations.",
                "To efficiently and effectively achieve this, it is necessary to provide search facilities that enable employees not only to access documents, but also to identify <br>expert colleague</br>s.",
                "At the TREC Enterprise Track [22] the need to study and understand expertise retrieval has been recognized through the introduction of Expert Finding tasks.",
                "The goal of expert finding is to identify a list of people who are knowledgeable about a given topic.",
                "This task is usually addressed by uncovering associations between people and topics [10]; commonly, a co-occurrence of the name of a person with topics in the same context is assumed to be evidence of expertise.",
                "An alternative task, which using the same idea of people-topic associations, is expert profiling, where the task is to return a list of topics that a person is knowledgeable about [3].",
                "The launch of the Expert Finding task at TREC has generated a lot of interest in expertise retrieval, with rapid progress being made in terms of modeling, algorithms, and evaluation aspects.",
                "However, nearly all of the expert finding or profiling work performed has been validated experimentally using the W3C collection [24] from the Enterprise Track.",
                "While this collection is currently the only publicly available test collection for expertise retrieval tasks, it only represents one type of intranet.",
                "With only one test collection it is not possible to generalize conclusions to other realistic settings.",
                "In this paper we focus on expertise retrieval in a realistic setting that differs from the W3C setting-one in which relatively small amounts of clean, multilingual data are available, that cover a broad range of expertise areas, as can be found on the intranets of universities and other knowledge-intensive organizations.",
                "Typically, this setting features several additional types of structure: topical structure (e.g., topic hierarchies as employed by the organization), organizational structure (faculty, department, ...), as well as multiple types of documents (research and course descriptions, publications, and academic homepages).",
                "This setting is quite different from the W3C setting in ways that might impact upon the performance of expertise retrieval tasks.",
                "We focus on a number of research questions in this paper: Does the relatively small amount of data available on an intranet affect the quality of the topic-person associations that lie at the heart of expertise retrieval algorithms?",
                "How do state-of-the-art algorithms developed on the W3C data set perform in the alternative scenario of the type described above?",
                "More generally, do the lessons from the Expert Finding task at TREC carry over to this setting?",
                "How does the inclusion or exclusion of different documents affect expertise retrieval tasks?",
                "In addition to, how can the topical and organizational structure be used for retrieval purposes?",
                "To answer our research questions, we first present a set of baseline approaches, based on generative language modeling, aimed at finding associations between topics and people.",
                "This allows us to formulate the expert finding and expert profiling tasks in a uniform way, and has the added benefit of allowing us to understand the relations between the two tasks.",
                "For our experimental evaluation, we introduce a new data set (the UvT Expert Collection) which is representative of the type of intranet that we described above.",
                "Our collection is based on publicly available data, crawled from the website of Tilburg University (UvT).",
                "This type of data is particularly interesting, since (1) it is clean, heterogeneous, structured, and focused, but comprises a limited number of documents; (2) contains information on the organizational hierarchy; (3) it is bilingual (English and Dutch); and (4) the list of expertise areas of an individual are provided by the employees themselves.",
                "Using the UvT Expert collection, we conduct two sets of experiments.",
                "The first is aimed at determining the effectiveness of baseline expertise finding and profiling methods in this new setting.",
                "A second group of experiments is aimed at extensions of the baseline methods that exploit characteristic features of the UvT Expert Collection; specifically, we propose and evaluate refined expert finding and profiling methods that incorporate topicality and organizational structure.",
                "Apart from the research questions and data set that we contribute, our main contributions are as follows.",
                "The baseline models developed for expertise finding perform well on the new data set.",
                "While on the W3C setting the expert finding task appears to be more difficult than profiling, for the UvT data the opposite is the case.",
                "We find that profiling on the UvT data set is considerably more difficult than on the W3C set, which we believe is due to the large (but realistic) number of topical areas that we used for profiling: about 1,500 for the UvT set, versus 50 in the W3C case.",
                "Taking the similarity between topics into account can significantly improve retrieval performance.",
                "The best performing similarity measures are content-based, therefore they can be applied on the W3C (and other) settings as well.",
                "Finally, we demonstrate that the organizational structure can be exploited in the form of a context model, improving MAP scores for certain models by up to 70%.",
                "The remainder of this paper is organized as follows.",
                "In the next section we review related work.",
                "Then, in Section 3 we provide detailed descriptions of the expertise retrieval tasks that we address in this paper: expert finding and expert profiling.",
                "In Section 4 we present our baseline models, of which the performance is then assessed in Section 6 using the UvT data set that we introduce in Section 5.",
                "Advanced models exploiting specific features of our data are presented in Section 7 and evaluated in Section 8.",
                "We formulate our conclusions in Section 9. 2.",
                "RELATED WORK Initial approaches to expertise finding often employed databases containing information on the skills and knowledge of each individual in the organization [11].",
                "Most of these tools (usually called yellow pages or people-finding systems) rely on people to self-assess their skills against a predefined set of keywords.",
                "For updating profiles in these systems in an automatic fashion there is a need for intelligent technologies [5].",
                "More recent approaches use specific document sets (such as email [6] or software [18]) to find expertise.",
                "In contrast with focusing on particular document types, there is also an increased interest in the development of systems that index and mine published intranet documents as sources of evidence for expertise.",
                "One such published approach is the P@noptic system [9], which builds a representation of each person by concatenating all documents associated with that person-this is similar to Model 1 of Balog et al. [4], who formalize and compare two methods.",
                "Balog et al.s Model 1 directly models the knowledge of an expert from associated documents, while their Model 2 first locates documents on the topic and then finds the associated experts.",
                "In the reported experiments the second method performs significantly better when there are sufficiently many associated documents per candidate.",
                "Most systems that took part in the 2005 and 2006 editions of the Expert Finding task at TREC implemented (variations on) one of these two models; see [10, 20].",
                "Macdonald and Ounis [16] propose a different approach for ranking candidate expertise with respect to a topic based on data fusion techniques, without using collectionspecific heuristics; they find that applying field-based weighting models improves the ranking of candidates.",
                "Petkova and Croft [19] propose yet another approach, based on a combination of the above Model 1 and 2, explicitly modeling topics.",
                "Turning to other expert retrieval tasks that can also be addressed using topic-people associations, Balog and de Rijke [3] addressed the task of determining topical expert profiles.",
                "While their methods proved to be efficient on the W3C corpus, they require an amount of data that may not be available in the typical knowledge-intensive organization.",
                "Balog and de Rijke [2] study the related task of finding experts that are similar to a small set of experts given as input.",
                "As an aside, creating a textual summary of a person shows some similarities to biography finding, which has received a considerable amount of attention recently; see e.g., [13].",
                "We use generative language modeling to find associations between topics and people.",
                "In our modeling of expert finding and profiling we collect evidence for expertise from multiple sources, in a heterogeneous collection, and integrate it with the co-occurrence of candidates names and query terms-the language modeling setting allows us to do this in a transparent manner.",
                "Our modeling proceeds in two steps.",
                "In the first step, we consider three baseline models, two taken from [4] (the Models 1 and 2 mentioned above), and one a refined version of a model introduced in [3] (which we refer to as Model 3 below); this third model is also similar to the model described by Petkova and Croft [19].",
                "The models we consider in our second round of experiments are mixture models similar to contextual language models [1] and to the expanded documents of Tao et al. [21]; however, the features that we use for definining our expansions-including topical structure and organizational structure-have not been used in this way before. 3.",
                "TASKS In the expertise retrieval scenario that we envisage, users seeking expertise within an organization have access to an interface that combines a search box (where they can search for experts or topics) with navigational structures (of experts and of topics) that allows them to click their way to an expert page (providing the profile of a person) or a topic page (providing a list of experts on the topic).",
                "To feed the above interface, we face two expertise retrieval tasks, expert finding and expert profiling, that we first define and then formalize using generative language models.",
                "In order to model either task, the probability of the query topic being associated to a candidate expert plays a key role in the final estimates for searching and profiling.",
                "By using language models, both the candidates and the query are characterized by distributions of terms in the vocabulary (used in the documents made available by the organization whose expertise retrieval needs we are addressing). 3.1 Expert finding Expert finding involves the task of finding the right person with the appropriate skills and knowledge: Who are the experts on topic X?.",
                "E.g., an employee wants to ascertain who worked on a particular project to find out why particular decisions were made without having to trawl through documentation (if there is any).",
                "Or, they may be in need a trained specialist for consultancy on a specific problem.",
                "Within an organization there are usually many possible candidates who could be experts for given topic.",
                "We can state this problem as follows: What is the probability of a candidate ca being an expert given the query topic q?",
                "That is, we determine p(ca|q), and rank candidates ca according to this probability.",
                "The candidates with the highest probability given the query are deemed the most likely experts for that topic.",
                "The challenge is how to estimate this probability accurately.",
                "Since the query is likely to consist of only a few terms to describe the expertise required, we should be able to obtain a more accurate estimate by invoking Bayes Theorem, and estimating: p(ca|q) = p(q|ca)p(ca) p(q) , (1) where p(ca) is the probability of a candidate and p(q) is the probability of a query.",
                "Since p(q) is a constant, it can be ignored for ranking purposes.",
                "Thus, the probability of a candidate ca being an expert given the query q is proportional to the probability of a query given the candidate p(q|ca), weighted by the a priori belief p(ca) that candidate ca is an expert. p(ca|q) ∝ p(q|ca)p(ca) (2) In this paper our main focus is on estimating the probability of a query given the candidate p(q|ca), because this probability captures the extent to which the candidate knows about the query topic.",
                "Whereas the candidate priors are generally assumed to be uniformand thus will not influence the ranking-it has been demonstrated that a sensible choice of priors may improve the performance [20]. 3.2 Expert profiling While the task of expert searching was concerned with finding experts given a particular topic, the task of expert profiling seeks to answer a related question: What topics does a candidate know about?",
                "Essentially, this turns the questions of expert finding around.",
                "The profiling of an individual candidate involves the identification of areas of skills and knowledge that they have expertise about and an evaluation of the level of proficiency in each of these areas.",
                "This is the candidates topical profile.",
                "Generally, topical profiles within organizations consist of tabular structures which explicitly catalogue the skills and knowledge of each individual in the organization.",
                "However, such practice is limited by the resources available for defining, creating, maintaining, and updating these profiles over time.",
                "By focusing on automatic methods which draw upon the available evidence within the document repositories of an organization, our aim is to reduce the human effort associated with the maintenance of topical profiles1 .",
                "A topical profile of a candidate, then, is defined as a vector where each element i of the vector corresponds to the candidate cas expertise on a given topic ki, (i.e., s(ca, ki)).",
                "Each topic ki defines a particular knowledge area or skill that the organization uses to define the candidates topical profile.",
                "Thus, it is assumed that a list of topics, {k1, . . . , kn}, where n is the number of pre-defined topics, is given: profile(ca) = s(ca, k1), s(ca, k2), . . . , s(ca, kn) . (3) 1 Context and evidence are needed to help users of expertise finding systems to decide whom to contact when seeking expertise in a particular area.",
                "Examples of such context are: Who does she work with?",
                "What are her contact details?",
                "Is she well-connected, just in case she is not able to help us herself?",
                "What is her role in the organization?",
                "Who is her superior?",
                "Collaborators, and affiliations, etc. are all part of the candidates social profile, and can serve as a background against which the systems recommendations should be interpreted.",
                "In this paper we only address the problem of determining topical profiles, and leave social profiling to further work.",
                "We state the problem of quantifying the competence of a person on a certain knowledge area as follows: What is the probability of a knowledge area (ki) being part of the candidates (expertise) profile? where s(ca, ki) is defined by p(ki|ca).",
                "Our task, then, is to estimate p(ki|ca), which is equivalent to the problem of obtaining p(q|ca), where the topic ki is represented as a query topic q, i.e., a sequence of keywords representing the expertise required.",
                "Both the expert finding and profiling tasks rely on the accurate estimation of p(q|ca).",
                "The only difference derives from the prior probability that a person is an expert (p(ca)), which can be incorporated into the expert finding task.",
                "This prior does not apply to the profiling task since the candidate (individual) is fixed. 4.",
                "BASELINE MODELS In this section we describe our baseline models for estimating p(q|ca), i.e., associations between topics and people.",
                "Both expert finding and expert profiling boil down to this estimation.",
                "We employ three models for calculating this probability. 4.1 From topics to candidates Using Candidate Models: Model 1 Model 1 [4] defines the probability of a query given a candidate (p(q|ca)) using standard language modeling techniques, based on a multinomial unigram language model.",
                "For each candidate ca, a candidate language model θca is inferred such that the probability of a term given θca is nonzero for all terms, i.e., p(t|θca) > 0.",
                "From the candidate model the query is generated with the following probability: p(q|θca) = Y t∈q p(t|θca)n(t,q) , where each term t in the query q is sampled identically and independently, and n(t, q) is the number of times t occurs in q.",
                "The candidate language model is inferred as follows: (1) an empirical model p(t|ca) is computed; (2) it is smoothed with background probabilities.",
                "Using the associations between a candidate and a document, the probability p(t|ca) can be approximated by: p(t|ca) = X d p(t|d)p(d|ca), where p(d|ca) is the probability that candidate ca generates a supporting document d, and p(t|d) is the probability of a term t occurring in the document d. We use the maximum-likelihood estimate of a term, that is, the normalised frequency of the term t in document d. The strength of the association between document d and candidate ca expressed by p(d|ca) reflects the degree to which the candidates expertise is described using this document.",
                "The estimation of this probability is presented later, in Section 4.2.",
                "The candidate model is then constructed as a linear interpolation of p(t|ca) and the background model p(t) to ensure there are no zero probabilities, which results in the final estimation: p(q|θca) = (4) Y t∈q ( (1 − λ) X d p(t|d)p(d|ca) ! + λp(t) )n(t,q) .",
                "Model 1 amasses all the term information from all the documents associated with the candidate, and uses this to represent that candidate.",
                "This model is used to predict how likely a candidate would produce a query q.",
                "This can can be intuitively interpreted as the probability of this candidate talking about the query topic, where we assume that this is indicative of their expertise.",
                "Using Document Models: Model 2 Model 2 [4] takes a different approach.",
                "Here, the process is broken into two parts.",
                "Given a candidate ca, (1) a document that is associated with a candidate is selected with probability p(d|ca), and (2) from this document a query q is generated with probability p(q|d).",
                "Then the sum over all documents is taken to obtain p(q|ca), such that: p(q|ca) = X d p(q|d)p(d|ca). (5) The probability of a query given a document is estimated by inferring a document language model θd for each document d in a similar manner as the candidate model was inferred: p(t|θd) = (1 − λ)p(t|d) + λp(t), (6) where p(t|d) is the probability of the term in the document.",
                "The probability of a query given the document model is: p(q|θd) = Y t∈q p(t|θd)n(t,q) .",
                "The final estimate of p(q|ca) is obtained by substituting p(q|d) for p(q|θd) into Eq. 5 (see [4] for full details).",
                "Conceptually, Model 2 differs from Model 1 because the candidate is not directly modeled.",
                "Instead, the document acts like a hidden variable in the process which separates the query from the candidate.",
                "This process is akin to how a user may search for candidates with a standard search engine: initially by finding the documents which are relevant, and then seeing who is associated with that document.",
                "By examining a number of documents the user can obtain an idea of which candidates are more likely to discuss the topic q.",
                "Using Topic Models: Model 3 We introduce a third model, Model 3.",
                "Instead of attempting to model the query generation process via candidate or document models, we represent the query as a topic language model and directly estimate the probability of the candidate p(ca|q).",
                "This approach is similar to the model presented in [3, 19].",
                "As with the previous models, a language model is inferred, but this time for the query.",
                "We adapt the work of Lavrenko and Croft [14] to estimate a topic model from the query.",
                "The procedure is as follows.",
                "Given a collection of documents and a query topic q, it is assumed that there exists an unknown topic model θk that assigns probabilities p(t|θk) to the term occurrences in the topic documents.",
                "Both the query and the documents are samples from θk (as opposed to the previous approaches, where a query is assumed to be sampled from a specific document or candidate model).",
                "The main task is to estimate p(t|θk), the probability of a term given the topic model.",
                "Since the query q is very sparse, and as there are no examples of documents on the topic, this distribution needs to be approximated.",
                "Lavrenko and Croft [14] suggest a reasonable way of obtaining such an approximation, by assuming that p(t|θk) can be approximated by the probability of term t given the query q.",
                "We can then estimate p(t|q) using the joint probability of observing the term t together with the query terms, q1, . . . , qm, and dividing by the joint probability of the query terms: p(t|θk) ≈ p(t|q) = p(t, q1, . . . , qm) p(q1, . . . , qm) = p(t, q1, . . . , qm) P t ∈T p(t , q1, . . . , qm) , where p(q1, . . . , qm) = P t ∈T p(t , q1, . . . , qm), and T is the entire vocabulary of terms.",
                "In order to estimate the joint probability p(t, q1, . . . , qm), we follow [14, 15] and assume t and q1, . . . , qm are mutually independent, once we pick a source distribution from the set of underlying source distributions U.",
                "If we choose U to be a set of document models. then to construct this set, the query q would be issued against the collection, and the top n returned are assumed to be relevant to the topic, and thus treated as samples from the topic model. (Note that candidate models could be used instead.)",
                "With the document models forming U, the joint probability of term and query becomes: p(t, q1, . . . , qm) = X d∈U p(d) ˘ p(t|θd) mY i=1 p(qi|θd) ¯ . (7) Here, p(d) denotes the prior distribution over the set U, which reflects the relevance of the document to the topic.",
                "We assume that p(d) is uniform across U.",
                "In order to rank candidates according to the topic model defined, we use the Kullback-Leibler divergence metric (KL, [8]) to measure the difference between the candidate models and the topic model: KL(θk||θca) = X t p(t|θk) log p(t|θk) p(t|θca) . (8) Candidates with a smaller divergence from the topic model are considered to be more likely experts on that topic.",
                "The candidate model θca is defined in Eq. 4.",
                "By using KL divergence instead of the probability of a candidate given the topic model p(ca|θk), we avoid normalization problems. 4.2 Document-candidate associations For our models we need to be able to estimate the probability p(d|ca), which expresses the extent to which a document d characterizes the candidate ca.",
                "In [4], two methods are presented for estimating this probability, based on the number of person names recognized in a document.",
                "However, in our (intranet) setting it is reasonable to assume that authors of documents can unambiguously be identified (e.g., as the author of an article, the teacher assigned to a course, the owner of a web page, etc.)",
                "Hence, we set p(d|ca) to be 1 if candidate ca is author of document d, otherwise the probability is 0.",
                "In Section 6 we describe how authorship can be determined on different types of documents within the collection. 5.",
                "THE UVT EXPERT COLLECTION The UvT Expert collection used in the experiments in this paper fits the scenario outlined in Section 3.",
                "The collection is based on the Webwijs (Webwise) system developed at Tilburg University (UvT) in the Netherlands.",
                "Webwijs (http://www.uvt.nl/ webwijs/) is a publicly accessible database of UvT employees who are involved in research or teaching; currently, Webwijs contains information about 1168 experts, each of whom has a page with contact information and, if made available by the expert, a research description and publications list.",
                "In addition, each expert can select expertise areas from a list of 1491 topics and is encouraged to suggest new topics that need to be approved by the Webwijs editor.",
                "Each topic has a separate page that shows all experts associated with that topic and, if available, a list of related topics.",
                "Webwijs is available in Dutch and English, and this bilinguality has been preserved in the collection.",
                "Every Dutch Webwijs page has an English translation.",
                "Not all Dutch topics have an English translation, but the reverse is true: the 981 English topics all have a Dutch equivalent.",
                "About 42% of the experts teach courses at Tilburg University; these courses were also crawled and included in the profile.",
                "In addition, about 27% of the experts link to their academic homepage from their Webwijs page.",
                "These home pages were crawled and added to the collection. (This means that if experts put the full-text versions of their publications on their academic homepage, these were also available for indexing.)",
                "We also obtained 1880 full-text versions of publications from the UvT institutional repository and Dutch English no. of experts 1168 1168 no. of experts with ≥ 1 topic 743 727 no. of topics 1491 981 no. of expert-topic pairs 4318 3251 avg. no. of topics/expert 5.8 5.9 max. no. of topics/expert (no. of experts) 60 (1) 35 (1) min. no. of topics/expert (no. of experts) 1 (74) 1 (106) avg. no. of experts/topic 2.9 3.3 max. no. of experts/topic (no. of topics) 30 (1) 30 (1) min. no. of experts/topic (no. of topics) 1 (615) 1 (346) no. of experts with HP 318 318 no. of experts with CD 318 318 avg. no. of CDs per teaching expert 3.5 3.5 no. of experts with RD 329 313 no. of experts with PUB 734 734 avg. no. of PUBs per expert 27.0 27.0 avg. no. of PUB citations per expert 25.2 25.2 avg. no. of full-text PUBs per expert 1.8 1.8 Table 2: Descriptive statistics of the Dutch and English versions of the UvT Expert collection. converted them to plain text.",
                "We ran the TextCat [23] language identifier to classify the language of the home pages and the fulltext publications.",
                "We restricted ourselves to pages where the classifier was confident about the language used on the page.",
                "This resulted in four document types: research descriptions (RD), course descriptions (CD), publications (PUB; full-text and citationonly versions), and academic homepages (HP).",
                "Everything was bundled into the UvT Expert collection which is available at http: //ilk.uvt.nl/uvt-expert-collection/.",
                "The UvT Expert collection was extracted from a different organizational setting than the W3C collection and differs from it in a number of ways.",
                "The UvT setting is one with relatively small amounts of multilingual data.",
                "Document-author associations are clear and the data is structured and clean.",
                "The collection covers a broad range of expertise areas, as one can typically find on intranets of universities and other knowledge-intensive institutes.",
                "Additionally, our university setting features several types of structure (topical and organizational), as well as multiple document types.",
                "Another important difference between the two data sets is that the expertise areas in the UvT Expert collection are self-selected instead of being based on group membership or assignments by others.",
                "Size is another dimension along which the W3C and UvT Expert collections differ: the latter is the smaller of the two.",
                "Also realistic are the large differences in the amount of information available for each expert.",
                "Utilizing Webwijs is voluntary; 425 Dutch experts did not select any topics at all.",
                "This leaves us with 743 Dutch and 727 English usable expert profiles.",
                "Table 2 provides descriptive statistics for the UvT Expert collection.",
                "Universities tend to have a hierarchical structure that goes from the faculty level, to departments, research groups, down to the individual researchers.",
                "In the UvT Expert collection we have information about the affiliations of researchers with faculties and institutes, providing us with a two-level organizational hierarchy.",
                "Tilburg University has 22 organizational units at the faculty level (including the university office and several research institutes) and 71 departments, which amounts to 3.2 departments per faculty.",
                "As to the topical hierarchy used by Webwijs, 131 of the 1491 topics are top nodes in the hierarchy.",
                "This hierarchy has an average topic chain length of 2.65 and a maximum length of 7 topics. 6.",
                "EVALUATION Below, we evaluate Section 4s models for expert finding and profiling onthe UvT Expert collection.",
                "We detail our research questions and experimental setup, and then present our results. 6.1 Research Questions We address the following research questions.",
                "Both expert finding and profiling rely on the estimations of p(q|ca).",
                "The question is how the models compare on the different tasks, and in the setting of the UvT Expert collection.",
                "In [4], Model 2 outperformed Model 1 on the W3C collection.",
                "How do they compare on our data set?",
                "And how does Model 3 compare to Model 1?",
                "What about performance differences between the two languages in our test collection? 6.2 Experimental Setup The output of our models was evaluated against the self-assigned topic labels, which were treated as relevance judgements.",
                "Results were evaluated separately for English and Dutch.",
                "For English we only used topics for which the Dutch translation was available; for Dutch all topics were considered.",
                "The results were averaged for the queries in the intersection of relevance judgements and results; missing queries do not contribute a value of 0 to the scores.",
                "We use standard information retrieval measures, such as Mean Average Precision (MAP) and Mean Reciprocal Rank (MRR).",
                "We also report the percentage of topics (%q) and candidates (%ca) covered, for the expert finding and profiling tasks, respectively. 6.3 Results Table 1 shows the performance of Model 1, 2, and 3 on the expert finding and profiling tasks.",
                "The rows of the table correspond to the various document types (RD, CD, PUB, and HP) and to their combinations.",
                "RD+CD+PUB+HP is equivalent to the full collection and will be referred as the BASELINE of our experiments.",
                "Looking at Table 1 we see that Model 2 performs the best across the board.",
                "However, when the data is clean and very focused (RD), Model 3 outperforms it in a number of cases.",
                "Model 1 has the best coverage of candidates (%ca) and topics (%q).",
                "The various document types differ in their characteristics and how they improve the finding and profiling tasks.",
                "Expert profiling benefits much from the clean data present in the RD and CD document types, while the publications contribute the most to the expert finding task.",
                "Adding the homepages does not prove to be particularly useful.",
                "When we compare the results across languages, we find that the coverage of English topics (%q) is higher than of the Dutch ones for expert finding.",
                "Apart from that, the scores fall in the same range for both languages.",
                "For the profiling task the coverage of the candidates (%ca) is very similar for both languages.",
                "However, the performance is substantially better for the English topics.",
                "While it is hard to compare scores across collections, we conclude with a brief comparison of the absolute scores in Table 1 to those reported in [3, 4] on the W3C test set (2005 edition).",
                "For expert finding the MAP scores for Model 2 reported here are about 50% higher than the corresponding figures in [4], while our MRR scores are slightly below those in [4].",
                "For expert profiling, the differences are far more dramatic: the MAP scores for Model 2 reported here are around 50% below the scores in [3], while the (best) MRR scores are about the same as those in [3].",
                "The cause for the latter differences seems to reside in the number of knowledge areas considered here-approx. 30 times more than in the W3C setting. 7.",
                "ADVANCED MODELS Now that we have developed and assessed basic language modeling techniques for expertise retrieval, we turn to refined models that exploit special features of our test collection. 7.1 Exploiting knowledge area similarity One way to improve the scoring of a query given a candidate is to consider what other requests the candidate would satisfy and use them as further evidence to support the original query, proportional Expert finding Expert profiling Document types Model 1 Model 2 Model 3 Model 1 Model 2 Model 3 %q MAP MRR %q MAP MRR %q MAP MRR %ca MAP MRR %ca MAP MRR %ca MAP MRR English RD 97.8 0.126 0.269 83.5 0.144 0.311 83.3 0.129 0.271 100 0.089 0.189 39.3 0.232 0.465 41.1 0.166 0.337 CD 97.8 0.118 0.227 91.7 0.123 0.248 91.7 0.118 0.226 32.8 0.188 0.381 32.4 0.195 0.385 32.7 0.203 0.370 PUB 97.8 0.200 0.330 98.0 0.216 0.372 98.0 0.145 0.257 78.9 0.167 0.364 74.5 0.212 0.442 78.9 0.135 0.299 HP 97.8 0.081 0.186 97.4 0.071 0.168 97.2 0.062 0.149 31.2 0.150 0.299 28.8 0.185 0.335 30.1 0.136 0.287 RD+CD 97.8 0.188 0.352 92.9 0.193 0.360 92.9 0.150 0.273 100 0.145 0.286 61.3 0.251 0.477 63.2 0.217 0.416 RD+CD+PUB 97.8 0.235 0.373 98.1 0.277 0.439 98.1 0.178 0.305 100 0.196 0.380 87.2 0.280 0.533 89.5 0.170 0.344 RD+CD+PUB+HP 97.8 0.237 0.372 98.6 0.280 0.441 98.5 0.166 0.293 100 0.199 0.387 88.7 0.281 0.525 90.9 0.169 0.329 Dutch RD 61.3 0.094 0.229 38.4 0.137 0.336 38.3 0.127 0.295 38.0 0.127 0.386 34.1 0.138 0.420 38.0 0.105 0.327 CD 61.3 0.107 0.212 49.7 0.128 0.256 49.7 0.136 0.261 32.5 0.151 0.389 31.8 0.158 0.396 32.5 0.170 0.380 PUB 61.3 0.193 0.319 59.5 0.218 0.368 59.4 0.173 0.291 78.8 0.126 0.364 76.0 0.150 0.424 78.8 0.103 0.294 HP 61.3 0.063 0.169 56.6 0.064 0.175 56.4 0.062 0.163 29.8 0.108 0.308 27.8 0.125 0.338 29.8 0.098 0.255 RD+CD 61.3 0.159 0.314 51.9 0.184 0.360 51.9 0.169 0.324 60.5 0.151 0.410 57.2 0.166 0.431 60.4 0.159 0.384 RD+CD+PUB 61.3 0.244 0.398 61.5 0.260 0.424 61.4 0.210 0.350 90.3 0.165 0.445 88.2 0.189 0.479 90.3 0.126 0.339 RD+CD+PUB+HP 61.3 0.249 0.401 62.6 0.265 0.436 62.6 0.195 0.344 91.9 0.164 0.426 90.1 0.195 0.488 91.9 0.125 0.328 Table 1: Performance of the models on the expert finding and profiling tasks, using different document types and their combinations. %q is the number of topics covered (applies to the expert finding task), %ca is the number of candidates covered (applies to the expert profiling task).",
                "The top and bottom blocks correspond to English and Dutch respectively.",
                "The best scores are in boldface. to how related the other requests are to the original query.",
                "This can be modeled by interpolating between the p(q|ca) and the further supporting evidence from all similar requests q , as follows: p (q|ca) = λp(q|ca) + (1 − λ) X q p(q|q )p(q |ca), (9) where p(q|q ) represents the similarity between the two topics q and q .",
                "To be able to work with similarity methods that are not necessarily probabilities, we set p(q|q ) = w(q,q ) γ , where γ is a normalizing constant, such that γ = P q w(q , q ).",
                "We consider four methods for calculating the similarity score between two topics.",
                "Three approaches are strictly content-based, and establish similarity by examining co-occurrence patterns of topics within the collection, while the last approach exploits the hierarchical structure of topical areas that may be present within an organization (see [7] for further examples of integrating word relationships into language models).",
                "The Kullback-Leibler (KL) divergence metric defined in Eq. 8 provides a measure of how different or similar two probability distributions are.",
                "A topic model is inferred for q and q using the method presented in Section 4.1 to describe the query across the entire vocabulary.",
                "Since a lower KL score means the queries are more similar, we let w(q, q ) = max(KL(θq||·) − KL(θq||θq )).",
                "Pointwise Mutual Information (PMI, [17]) is a measure of association used in information theory to determine the extent of independence between variables.",
                "The dependence between two queries is reflected by the SI(q, q ) score, where scores greater than zero indicate that it is likely that there is a dependence, which we take to mean that the queries are likely to be similar: SI(q, q ) = log p(q, q ) p(q)p(q ) (10) We estimate the probability of a topic p(q) using the number of documents relevant to query q within the collection.",
                "The joint probability p(q, q ) is estimated similarly, by using the concatenation of q and q as a query.",
                "To obtain p(q|q ), we then set w(q, q ) = SI(q, q ) when SI(q, q ) > 0 otherwise w(q, q ) = 0, because we are only interested in including queries that are similar.",
                "The log-likelihood statistic provides another measure of dependence, which is more reliable than the pointwise mutual information measure [17].",
                "Let k1 be the number of co-occurrences of q and q , k2 the number of occurrences of q not co-occurring with q , n1 the total number of occurrences of q , and n2 the total number of topic tokens minus the number of occurrences of q .",
                "Then, let p1 = k1/n1, p2 = k2/n2, and p = (k1 + k2)/(n1 + n2), (q, q ) = 2( (p1, k1, n1) + (p2, k2, n2) − (p, k1, n1) − (p, k2, n2)), where (p, n, k) = k log p + (n − k) log(1 − p).",
                "The higher score indicate that queries are also likely to be similar, thus we set w(q, q ) = (q, q ).",
                "Finally, we also estimate the similarity of two topics based on their distance within the topic hierarchy.",
                "The topic hierarchy is viewed as a directed graph, and for all topic-pairs the shortest path SP(q, q ) is calculated.",
                "We set the similarity score to be the reciprocal of the shortest path: w(q, q ) = 1/SP(q, q ). 7.2 Contextual information Given the hierarchy of an organization, the units to which a person belong are regarded as a context so as to compensate for data sparseness.",
                "We model it as follows: p (q|ca) = 1 − P ou∈OU(ca) λou · p(q|ca) + P ou∈OU(ca) λou · p(q|ou), where OU(ca) is the set of organizational units of which candidate ca is a member of, and p(q|o) expresses the strength of the association between query q and the unit ou.",
                "The latter probability can be estimated using either of the three basic models, by simply replacing ca with ou in the corresponding equations.",
                "An organizational unit is associated with all the documents that its members have authored.",
                "That is, p(d|ou) = maxca∈ou p(d|ca). 7.3 A simple multilingual model For knowledge institutes in Europe, academic or otherwise, a multilingual (or at least bilingual) setting is typical.",
                "The following model builds on a kind of independence assumption: there is no spill-over of expertise/profiles across language boundaries.",
                "While a simplification, this is a sensible first approach.",
                "That is: p (q|ca) =P l∈L λl · p(ql|ca), where L is the set of languages used in the collection, ql is the translation of the query q to language l, and λl is a language specific smoothing parameter, such that P l∈L λl = 1. 8.",
                "ADVANCED MODELS: EVALUATION In this section we present an experimental evaluation of our advanced models.",
                "Expert finding Expert profiling Language Model 1 Model 2 Model 3 Model 1 Model 2 Model 3 %q MAP MRR %q MAP MRR %q MAP MRR %ca MAP MRR %ca MAP MRR %ca MAP MRR English only 97.8 0.237 0.372 98.6 0.280 0.441 98.5 0.166 0.293 100 0.199 0.387 88.7 0.281 0.525 90.9 0.169 0.329 Dutch only 61.3 0.249 0.401 62.6 0.265 0.436 62.6 0.195 0.344 91.9 0.164 0.426 90.1 0.195 0.488 91.9 0.125 0.328 Combination 99.4 0.297 0.444 99.7 0.324 0.491 99.7 0.223 0.388 100 0.241 0.445 92.1 0.313 0.564 93.2 0.224 0.411 Table 3: Performance of the combination of languages on the expert finding and profiling tasks (on candidates).",
                "Best scores for each model are in italic, absolute best scores for the expert finding and profiling tasks are in boldface.",
                "Method Model 1 Model 2 Model 3 MAP MRR MAP MRR MAP MRR English BASELINE 0.296 0.454 0.339 0.509 0.221 0.333 KLDIV 0.291 0.453 0.327 0.503 0.219 0.330 PMI 0.291 0.453 0.337 0.509 0.219 0.331 LL 0.319 0.490 0.360 0.524 0.233 0.368 HDIST 0.299 0.465 0.346 0.537 0.219 0.332 Dutch BASELINE 0.240 0.350 0.271 0.403 0.227 0.389 KLDIV 0.239 0.347 0.253 0.386 0.224 0.385 PMI 0.239 0.350 0.260 0.392 0.227 0.389 LL 0.255 0.372 0.281 0.425 0.231 0.389 HDIST 0.253 0.365 0.271 0.407 0.236 0.402 Method Model 1 Model 2 Model 3 MAP MRR MAP MRR MAP MRR English BASELINE 0.485 0.546 0.499 0.548 0.381 0.416 KLDIV 0.510 0.564 0.513 0.558 0.381 0.416 PMI 0.486 0.546 0.495 0.542 0.407 0.451 LL 0.558 0.589 0.586 0.617 0.408 0.453 HDIST 0.507 0.567 0.512 0.563 0.386 0.420 Dutch BASELINE 0.263 0.313 0.294 0.358 0.262 0.315 KLDIV 0.284 0.336 0.271 0.321 0.261 0.314 PMI 0.265 0.317 0.265 0.316 0.273 0.330 LL 0.312 0.351 0.330 0.377 0.284 0.331 HDIST 0.280 0.327 0.288 0.341 0.266 0.321 Table 4: Performance on the expert finding (top) and profiling (bottom) tasks, using knowledge area similarities.",
                "Runs were evaluated on the main topics set.",
                "Best scores are in boldface. 8.1 Research Questions Our questions follow the refinements presented in the preceding section: Does exploiting the knowledge area similarity improve effectiveness?",
                "Which of the various methods for capturing word relationships is most effective?",
                "Furthermore, is our way of bringing in contextual information useful?",
                "For which tasks?",
                "And finally, is our simple way of combining the monolingual scores sufficient for obtaining significant improvements? 8.2 Experimental setup Given that the self-assessments are also sparse in our collection, in order to be able to measure differences between the various models, we selected a subset of topics, and evaluated (some of the) runs only on this subset.",
                "This set is referred as main topics, and consists of topics that are located at the top level of the topical hierarchy. (A main topic has subtopics, but is not a subtopic of any other topic.)",
                "This main set consists of 132 Dutch and 119 English topics.",
                "The relevance judgements were restricted to the main topic set, but were not expanded with subtopics. 8.3 Exploiting knowledge area similarity Table 4 presents the results.",
                "The four methods used for estimating knowledge-area similarity are KL divergence (KLDIV), PointLang.",
                "Topics Model 1 Model 2 Model 3 MAP MRR MAP MRR MAP MRR Expert finding UK ALL 0.423 0.545 0.654 0.799 0.494 0.629 UK MAIN 0.500 0.621 0.704 0.834 0.587 0.699 NL ALL 0.439 0.560 0.672 0.826 0.480 0.630 NL MAIN 0.440 0.584 0.645 0.816 0.515 0.655 Expert profiling UK ALL 0.240 0.640 0.306 0.778 0.223 0.616 UK MAIN 0.523 0.677 0.519 0.648 0.461 0.587 NL ALL 0.203 0.716 0.254 0.770 0.183 0.627 NL MAIN 0.332 0.576 0.380 0.624 0.332 0.549 Table 5: Evaluating the context models on organizational units. wise mutual information (PMI), log-likelihood (LL), and distance within topic hierarchy (HDIST).",
                "We managed to improve upon the baseline in all cases, but the improvement is more noticeable for the profiling task.",
                "For both tasks, the LL method performed best.",
                "The content-based approaches performed consistently better than HDIST. 8.4 Contextual information A two level hierarchy of organizational units (faculties and institutes) is available in the UvT Expert collection.",
                "The unit a person belongs to is used as a context for that person.",
                "First, we evaluated the models of the organizational units, using all topics (ALL) and only the main topics (MAIN).",
                "An organizational unit is considered to be relevant for a given topic (or vice versa) if at least one member of the unit selected the given topic as an expertise area.",
                "Table 5 reports on the results.",
                "As far as expert finding goes, given a topic, the corresponding organizational unit can be identified with high precision.",
                "However, the expert profiling task shows a different picture: the scores are low, and the task seems hard.",
                "The explanation may be that general concepts (i.e., our main topics) may belong to several organizational units.",
                "Second, we performed another evaluation, where we combined the contextual models with the candidate models (to score candidates again).",
                "Table 6 reports on the results.",
                "We find a positive impact of the context models only for expert finding.",
                "Noticably, for expert finding (and Model 1), it improves over 50% (for English) and over 70% (for Dutch) on MAP.",
                "The poor performance on expert profiling may be due to the fact that context models alone did not perform very well on the profiling task to begin with. 8.5 Multilingual models In this subsection we evaluate the method for combining results across multiple languages that we described in Section 7.3.",
                "In our setting the set of languages consists of English and Dutch: L = {UK, NL}.",
                "The weights on these languages were set to be identical (λUK = λNL = 0.5).",
                "We performed experiments with various λ settings, but did not observe significant differences in performance.",
                "Table 3 reports on the multilingual results, where performance is evaluated on the full topic set.",
                "All three models significantly imLang.",
                "Method Model 1 Model 2 Model 3 MAP MRR MAP MRR MAP MRR Expert finding UK BL 0.296 0.454 0.339 0.509 0.221 0.333 UK CT 0.330 0.491 0.342 0.500 0.228 0.342 NL BL 0.240 0.350 0.271 0.403 0.227 0.389 NL CT 0.251 0.382 0.267 0.410 0.246 0.404 Expert profiling UK BL 0.485 0.546 0.499 0.548 0.381 0.416 UK CT 0.562 0.620 0.508 0.558 0.440 0.486 NL BL 0.263 0.313 0.294 0.358 0.262 0.315 NL CT 0.330 0.384 0.317 0.387 0.294 0.345 Table 6: Performance of the context models (CT) compared to the baseline (BL).",
                "Best scores are in boldface. proved over all measures for both tasks.",
                "The coverage of topics and candidates for the expert finding and profiling tasks, respectively, is close to 100% in all cases.",
                "The relative improvement of the precision scores ranges from 10% to 80%.",
                "These scores demonstrate that despite its simplicity, our method for combining results over multiple languages achieves substantial improvements over the baseline. 9.",
                "CONCLUSIONS In this paper we focused on expertise retrieval (expert finding and profiling) in a new setting of a typical knowledge-intensive organization in which the available data is of high quality, multilingual, and covering a broad range of expertise area.",
                "Typically, the amount of available data in such an organization (e.g., a university, a research institute, or a research lab) is limited when compared to the W3C collection that has mostly been used for the experimental evaluation of expertise retrieval so far.",
                "To examine expertise retrieval in this setting, we introduced (and released) the UvT Expert collection as a representative case of such knowledge intensive organizations.",
                "The new collection reflects the typical properties of knowledge-intensive institutes noted above and also includes several features which may are potentially useful for expertise retrieval, such as topical and organizational structure.",
                "We evaluated how current state-of-the-art models for expert finding and profiling performed in this new setting and then refined these models in order to try and exploit the different characteristics within the data environment (language, topicality, and organizational structure).",
                "We found that current models of expertise retrieval generalize well to this new environment; in addition we found that refining the models to account for the differences results in significant improvements, thus making up for problems caused by data sparseness issues.",
                "Future work includes setting up manual assessments of automatically generated profiles by the employees themselves, especially in cases where the employees have not provided a profile themselves. 10.",
                "ACKNOWLEDGMENTS Krisztian Balog was supported by the Netherlands Organisation for Scientific Research (NWO) under project number 220-80-001.",
                "Maarten de Rijke was also supported by NWO under project numbers 017.001.190, 220-80-001, 264-70-050, 354-20-005, 600.065.120, 612-13-001, 612.000.106, 612.066.302, 612.069.006, 640.001.501, 640.002.501, and by the E.U.",
                "IST programme of the 6th FP for RTD under project MultiMATCH contract IST-033104.",
                "The work of Toine Bogers and Antal van den Bosch was funded by the IOP-MMI-program of SenterNovem / The Dutch Ministry of Economic Affairs, as part of the `A Propos project. 11.",
                "REFERENCES [1] L. Azzopardi.",
                "Incorporating Context in the Language Modeling Framework for ad-hoc Information Retrieval.",
                "PhD thesis, University of Paisley, 2005. [2] K. Balog and M. de Rijke.",
                "Finding similar experts.",
                "In This volume, 2007. [3] K. Balog and M. de Rijke.",
                "Determining expert profiles (with an application to expert finding).",
                "In IJCAI 07: Proc. 20th Intern.",
                "Joint Conf. on Artificial Intelligence, pages 2657-2662, 2007. [4] K. Balog, L. Azzopardi, and M. de Rijke.",
                "Formal models for expert finding in enterprise corpora.",
                "In SIGIR 06: Proc. 29th annual intern.",
                "ACM SIGIR conf. on Research and development in information retrieval, pages 43-50, 2006. [5] I. Becerra-Fernandez.",
                "The role of artificial intelligence technologies in the implementation of people-finder knowledge management systems.",
                "In AAAI Workshop on Bringing Knowledge to Business Processes, March 2000. [6] C. S. Campbell, P. P. Maglio, A. Cozzi, and B. Dom.",
                "Expertise identification using email communications.",
                "In CIKM 03: Proc. twelfth intern. conf. on Information and knowledge management, pages 528531, 2003. [7] G. Cao, J.-Y.",
                "Nie, and J. Bai.",
                "Integrating word relationships into language models.",
                "In SIGIR 05: Proc. 28th annual intern.",
                "ACM SIGIR conf. on Research and development in information retrieval, pages 298-305, 2005. [8] T. M. Cover and J.",
                "A. Thomas.",
                "Elements of Information Theory.",
                "Wiley-Interscience, 1991. [9] N. Craswell, D. Hawking, A. M. Vercoustre, and P. Wilkins.",
                "P@noptic expert: Searching for experts not just for documents.",
                "In Ausweb, 2001. [10] N. Craswell, A. de Vries, and I. Soboroff.",
                "Overview of the TREC2005 Enterprise Track.",
                "In The Fourteenth Text REtrieval Conf.",
                "Proc. (TREC 2005), 2006. [11] T. H. Davenport and L. Prusak.",
                "Working Knowledge: How Organizations Manage What They Know.",
                "Harvard Business School Press, Boston, MA, 1998. [12] T. Dunning.",
                "Accurate methods for the statistics of surprise and coincidence.",
                "Computational Linguistics, 19(1):61-74, 1993. [13] E. Filatova and J. Prager.",
                "Tell me what you do and Ill tell you what you are: Learning occupation-related activities for biographies.",
                "In HLT/EMNLP, 2005. [14] V. Lavrenko and W. B. Croft.",
                "Relevance based language models.",
                "In SIGIR 01: Proc. 24th annual intern.",
                "ACM SIGIR conf. on Research and development in information retrieval, pages 120-127, 2001. [15] V. Lavrenko, M. Choquette, and W. B. Croft.",
                "Cross-lingual relevance models.",
                "In SIGIR 02: Proc. 25th annual intern.",
                "ACM SIGIR conf. on Research and development in information retrieval, pages 175-182, 2002. [16] C. Macdonald and I. Ounis.",
                "Voting for candidates: adapting data fusion techniques for an expert search task.",
                "In CIKM 06: Proc. 15th ACM intern. conf. on Information and knowledge management, pages 387-396, 2006. [17] C. Manning and H. Sch¨utze.",
                "Foundations of Statistical Natural Language Processing.",
                "The MIT Press, 1999. [18] A. Mockus and J. D. Herbsleb.",
                "Expertise browser: a quantitative approach to identifying expertise.",
                "In ICSE 02: Proc. 24th Intern.",
                "Conf. on Software Engineering, pages 503-512, 2002. [19] D. Petkova and W. B. Croft.",
                "Hierarchical language models for expert finding in enterprise corpora.",
                "In Proc.",
                "ICTAI 2006, pages 599-608, 2006. [20] I. Soboroff, A. de Vries, and N. Craswell.",
                "Overview of the TREC 2006 Enterprise Track.",
                "In TREC 2006 Working Notes, 2006. [21] T. Tao, X. Wang, Q. Mei, and C. Zhai.",
                "Language model information retrieval with document expansion.",
                "In HLT-NAACL 2006, 2006. [22] TREC.",
                "Enterprise track, 2005.",
                "URL: http://www.ins.cwi. nl/projects/trec-ent/wiki/. [23] G. van Noord.",
                "TextCat Language Guesser.",
                "URL: http://www. let.rug.nl/˜vannoord/TextCat/. [24] W3C.",
                "The W3C test collection, 2005.",
                "URL: http://research. microsoft.com/users/nickcr/w3c-summary.html."
            ],
            "original_annotated_samples": [
                "To efficiently and effectively achieve this, it is necessary to provide search facilities that enable employees not only to access documents, but also to identify <br>expert colleague</br>s."
            ],
            "translated_annotated_samples": [
                "Para lograr esto de manera eficiente y efectiva, es necesario proporcionar instalaciones de búsqueda que permitan a los empleados no solo acceder a documentos, sino también identificar a <br>colegas expertos</br>."
            ],
            "translated_text": "Recuperación de amplia experiencia en entornos de datos dispersos Krisztian Balog ISLA, Universidad de Ámsterdam Kruislaan 403, 1098 SJ Ámsterdam, Países Bajos kbalog@science.uva.nl Toine Bogers ILK, Universidad de Tilburg P.O. Caja 90153, 5000 LE Tilburg, Países Bajos A.M.Bogers@uvt.nl Leif Azzopardi Dept. de Ciencias de la Computación Universidad de Glasgow, Glasgow, G12 8QQ leif@dcs.gla.ac.uk Maarten de Rijke ISLA, Universidad de Ámsterdam Kruislaan 403, 1098 SJ Ámsterdam, Países Bajos mdr@science.uva.nl Antal van den Bosch ILK, Universidad de Tilburg P.O. La recuperación de la experiencia ha sido ampliamente inexplorada en datos distintos a la colección de la W3C. Al mismo tiempo, muchas intranets de universidades y otras organizaciones intensivas en conocimiento ofrecen ejemplos de datos de expertos multilingües relativamente pequeños pero precisos, que abarcan amplias áreas de especialización. Primero presentamos dos tareas principales de recuperación de expertos, junto con un conjunto de enfoques de referencia basados en modelado del lenguaje generativo, con el objetivo de encontrar relaciones de experticia entre temas y personas. Para nuestra evaluación experimental, introducimos (y liberamos) un nuevo conjunto de pruebas basado en un rastreo de un sitio universitario. Usando este conjunto de pruebas, llevamos a cabo dos series de experimentos. El primero tiene como objetivo determinar la efectividad de los métodos de recuperación de conocimientos básicos aplicados al nuevo conjunto de pruebas. El segundo tiene como objetivo evaluar modelos refinados que exploten las características distintivas del nuevo conjunto de pruebas, como la estructura organizativa de la universidad y la estructura jerárquica de los temas en el conjunto de pruebas. Los modelos de recuperación de experiencia se muestran robustos con respecto a entornos más pequeños que la colección de la W3C, y las técnicas actuales parecen ser generalizables a otros contextos. Categorías y Descriptores de Asignaturas H.3 [Almacenamiento y Recuperación de Información]: H.3.1 Análisis de Contenido e Indexación; H.3.3 Búsqueda y Recuperación de Información; H.3.4 Sistemas y Software; H.4 [Aplicaciones de Sistemas de Información]: H.4.2 Tipos de Sistemas; H.4.m Términos Generales Misceláneos Algoritmos, Medición, Rendimiento, Experimentación 1. INTRODUCCIÓN La intranet de una organización proporciona un medio para intercambiar información entre empleados y facilitar colaboraciones entre ellos. Para lograr esto de manera eficiente y efectiva, es necesario proporcionar instalaciones de búsqueda que permitan a los empleados no solo acceder a documentos, sino también identificar a <br>colegas expertos</br>. En la pista de Empresas de TREC [22] se ha reconocido la necesidad de estudiar y comprender la recuperación de expertos a través de la introducción de tareas de Búsqueda de Expertos. El objetivo de la búsqueda de expertos es identificar una lista de personas que tienen conocimientos sobre un tema específico. Esta tarea suele abordarse descubriendo asociaciones entre personas y temas [10]; comúnmente, se asume que la co-ocurrencia del nombre de una persona con temas en el mismo contexto es evidencia de experiencia. Una tarea alternativa, que utiliza la misma idea de asociaciones entre personas y temas, es el perfilado de expertos, donde la tarea consiste en devolver una lista de temas sobre los que una persona tiene conocimiento [3]. El lanzamiento de la tarea de Búsqueda de Expertos en TREC ha generado mucho interés en la recuperación de expertos, con un rápido progreso en términos de modelado, algoritmos y aspectos de evaluación. Sin embargo, casi todo el trabajo de búsqueda o perfilado de expertos realizado ha sido validado experimentalmente utilizando la colección de W3C [24] de la pista empresarial. Si bien esta colección es actualmente la única colección de pruebas disponible públicamente para tareas de recuperación de expertos, solo representa un tipo de intranet. Con solo una colección de pruebas no es posible generalizar conclusiones a otros escenarios realistas. En este artículo nos enfocamos en la recuperación de expertos en un entorno realista que difiere del entorno de la W3C, uno en el que hay cantidades relativamente pequeñas de datos limpios y multilingües disponibles, que abarcan una amplia gama de áreas de expertise, como se puede encontrar en las intranets de universidades y otras organizaciones intensivas en conocimiento. Normalmente, esta configuración incluye varios tipos adicionales de estructuras: estructura temática (por ejemplo, jerarquías de temas utilizadas por la organización), estructura organizativa (facultad, departamento, ...), así como múltiples tipos de documentos (descripciones de investigación y cursos, publicaciones y páginas web académicas). Este entorno es bastante diferente del entorno del W3C de maneras que podrían afectar el rendimiento de las tareas de recuperación de conocimientos especializados. Nos enfocamos en varias preguntas de investigación en este artículo: ¿La cantidad relativamente pequeña de datos disponibles en una intranet afecta la calidad de las asociaciones tema-persona que son fundamentales en los algoritmos de recuperación de expertos? ¿Cómo se desempeñan los algoritmos de vanguardia desarrollados en el conjunto de datos del W3C en el escenario alternativo del tipo descrito anteriormente? Más en general, ¿se pueden aplicar las lecciones de la tarea de Búsqueda de Expertos en TREC a este contexto? ¿Cómo afecta la inclusión o exclusión de diferentes documentos a las tareas de recuperación de expertos? Además, ¿cómo se puede utilizar la estructura temática y organizativa con fines de recuperación? Para responder a nuestras preguntas de investigación, primero presentamos un conjunto de enfoques de línea base, basados en modelado de lenguaje generativo, con el objetivo de encontrar asociaciones entre temas y personas. Esto nos permite formular las tareas de búsqueda de expertos y perfilado de expertos de manera uniforme, y tiene el beneficio adicional de permitirnos entender las relaciones entre las dos tareas. Para nuestra evaluación experimental, introducimos un nuevo conjunto de datos (la Colección de Expertos de UvT) que es representativo del tipo de intranet que describimos anteriormente. Nuestra colección se basa en datos de acceso público, extraídos del sitio web de la Universidad de Tilburg (UvT). Este tipo de datos es particularmente interesante, ya que (1) es limpio, heterogéneo, estructurado y enfocado, pero comprende un número limitado de documentos; (2) contiene información sobre la jerarquía organizativa; (3) es bilingüe (inglés y holandés); y (4) la lista de áreas de especialización de un individuo es proporcionada por los propios empleados. Usando la colección de expertos de UvT, llevamos a cabo dos conjuntos de experimentos. El primero tiene como objetivo determinar la efectividad de los métodos de búsqueda y perfilado de experiencia inicial en este nuevo entorno. Un segundo grupo de experimentos está dirigido a extensiones de los métodos base que aprovechan las características distintivas de la Colección de Expertos de UvT; específicamente, proponemos y evaluamos métodos refinados de búsqueda y perfilado de expertos que incorporan la topicalidad y la estructura organizativa. Además de las preguntas de investigación y el conjunto de datos que aportamos, nuestras principales contribuciones son las siguientes. Los modelos base desarrollados para encontrar expertos funcionan bien en el nuevo conjunto de datos. Mientras que en el entorno del W3C la tarea de encontrar expertos parece ser más difícil que el perfilado, para los datos de UvT ocurre lo contrario. Encontramos que el perfilado en el conjunto de datos UvT es considerablemente más difícil que en el conjunto de datos W3C, lo cual creemos que se debe al gran (pero realista) número de áreas temáticas que utilizamos para el perfilado: alrededor de 1,500 para el conjunto de datos UvT, frente a 50 en el caso de W3C. Teniendo en cuenta la similitud entre los temas puede mejorar significativamente el rendimiento de recuperación. Las medidas de similitud de mejor rendimiento son basadas en contenido, por lo tanto también se pueden aplicar en entornos de la W3C (y otros). Finalmente, demostramos que la estructura organizativa puede ser explotada en forma de un modelo de contexto, mejorando los puntajes de MAP para ciertos modelos hasta en un 70%. El resto de este documento está organizado de la siguiente manera. En la siguiente sección revisamos el trabajo relacionado. Luego, en la Sección 3 proporcionamos descripciones detalladas de las tareas de recuperación de expertos que abordamos en este artículo: la búsqueda de expertos y la creación de perfiles de expertos. En la Sección 4 presentamos nuestros modelos base, cuyo rendimiento luego es evaluado en la Sección 6 utilizando el conjunto de datos UvT que presentamos en la Sección 5. Los modelos avanzados que explotan características específicas de nuestros datos se presentan en la Sección 7 y se evalúan en la Sección 8. Formulamos nuestras conclusiones en la Sección 9.2. Trabajos relacionados Los enfoques iniciales para encontrar expertos a menudo empleaban bases de datos que contenían información sobre las habilidades y conocimientos de cada individuo en la organización [11]. La mayoría de estas herramientas (generalmente llamadas páginas amarillas o sistemas de búsqueda de personas) dependen de que las personas evalúen sus habilidades frente a un conjunto predefinido de palabras clave. Para actualizar perfiles en estos sistemas de forma automática, se necesita de tecnologías inteligentes [5]. Enfoques más recientes utilizan conjuntos de documentos específicos (como correos electrónicos [6] o software [18]) para encontrar experiencia. En contraste con centrarse en tipos de documentos particulares, también hay un interés creciente en el desarrollo de sistemas que indexan y extraen información de documentos publicados en intranets como fuentes de evidencia de experiencia. Un enfoque publicado es el sistema P@noptic [9], que construye una representación de cada persona concatenando todos los documentos asociados con esa persona, similar al Modelo 1 de Balog et al. [4], quienes formalizan y comparan dos métodos. El Modelo 1 de Balog et al. modela directamente el conocimiento de un experto a partir de documentos asociados, mientras que su Modelo 2 primero localiza documentos sobre el tema y luego encuentra a los expertos asociados. En los experimentos reportados, el segundo método tiene un rendimiento significativamente mejor cuando hay suficientes documentos asociados por candidato. La mayoría de los sistemas que participaron en las ediciones de 2005 y 2006 de la tarea de Búsqueda de Expertos en TREC implementaron (variaciones de) uno de estos dos modelos; ver [10, 20]. Macdonald y Ounis [16] proponen un enfoque diferente para clasificar la experiencia de los candidatos con respecto a un tema basado en técnicas de fusión de datos, sin utilizar heurísticas específicas de la colección; encuentran que la aplicación de modelos de ponderación basados en campos mejora la clasificación de los candidatos. Petkova y Croft [19] proponen otro enfoque, basado en una combinación de los Modelos 1 y 2 anteriores, modelando explícitamente los temas. Al abordar otras tareas de recuperación de expertos que también pueden abordarse utilizando asociaciones entre temas y personas, Balog y de Rijke [3] abordaron la tarea de determinar perfiles de expertos temáticos. Si bien sus métodos resultaron ser eficientes en el corpus de la W3C, requieren una cantidad de datos que puede que no esté disponible en la típica organización intensiva en conocimiento. Balog y de Rijke [2] estudian la tarea relacionada de encontrar expertos que sean similares a un pequeño conjunto de expertos dados como entrada. Por cierto, crear un resumen textual de una persona muestra algunas similitudes con la búsqueda de biografías, la cual ha recibido una considerable cantidad de atención recientemente; ver por ejemplo, [13]. Utilizamos modelado de lenguaje generativo para encontrar asociaciones entre temas y personas. En nuestro modelado de búsqueda y perfilado de expertos recopilamos evidencia de experiencia de múltiples fuentes, en una colección heterogénea, e integramos esta información con la co-ocurrencia de nombres de candidatos y términos de consulta: el entorno de modelado de lenguaje nos permite hacer esto de manera transparente. Nuestro modelado avanza en dos pasos. En el primer paso, consideramos tres modelos base, dos tomados de [4] (los Modelos 1 y 2 mencionados anteriormente), y uno una versión refinada de un modelo introducido en [3] (al que nos referimos como Modelo 3 a continuación); este tercer modelo también es similar al modelo descrito por Petkova y Croft [19]. Los modelos que consideramos en nuestra segunda ronda de experimentos son modelos de mezcla similares a los modelos de lenguaje contextual [1] y a los documentos ampliados de Tao et al. [21]; sin embargo, las características que utilizamos para definir nuestras expansiones, incluida la estructura temática y la estructura organizativa, no se han utilizado de esta manera antes. 3. En el escenario de recuperación de expertos que imaginamos, los usuarios que buscan expertos dentro de una organización tienen acceso a una interfaz que combina un cuadro de búsqueda (donde pueden buscar expertos o temas) con estructuras de navegación (de expertos y de temas) que les permite hacer clic hasta llegar a una página de experto (que proporciona el perfil de una persona) o a una página de tema (que proporciona una lista de expertos en el tema). Para alimentar la interfaz anterior, nos enfrentamos a dos tareas de recuperación de expertos, la búsqueda de expertos y el perfilado de expertos, que primero definimos y luego formalizamos utilizando modelos de lenguaje generativos. Para modelar cualquiera de las tareas, la probabilidad de que el tema de la consulta esté asociado a un experto candidato juega un papel clave en las estimaciones finales para la búsqueda y el perfilado. Al utilizar modelos de lenguaje, tanto los candidatos como la consulta se caracterizan por distribuciones de términos en el vocabulario (utilizado en los documentos puestos a disposición por la organización cuyas necesidades de recuperación de experiencia estamos abordando). 3.1 Búsqueda de expertos La búsqueda de expertos implica la tarea de encontrar a la persona adecuada con las habilidades y conocimientos apropiados: ¿Quiénes son los expertos en el tema X?. Por ejemplo, un empleado quiere averiguar quién trabajó en un proyecto en particular para descubrir por qué se tomaron decisiones específicas sin tener que revisar minuciosamente la documentación (si es que la hay). O pueden necesitar un especialista capacitado para consultoría sobre un problema específico. Dentro de una organización, generalmente hay muchos posibles candidatos que podrían ser expertos en un tema dado. Podemos plantear este problema de la siguiente manera: ¿Cuál es la probabilidad de que un candidato ca sea un experto dado el tema de consulta q? Es decir, determinamos p(ca|q) y clasificamos a los candidatos ca de acuerdo con esta probabilidad. Los candidatos con la probabilidad más alta dada la consulta son considerados los expertos más probables en ese tema. El desafío es cómo estimar esta probabilidad con precisión. Dado que es probable que la consulta consista solo en unos pocos términos para describir la experiencia requerida, deberíamos poder obtener una estimación más precisa al invocar el Teorema de Bayes, y estimar: p(ca|q) = p(q|ca)p(ca) p(q) , (1) donde p(ca) es la probabilidad de un candidato y p(q) es la probabilidad de una consulta. Dado que p(q) es una constante, se puede ignorar para fines de clasificación. Por lo tanto, la probabilidad de que un candidato ca sea un experto dado la consulta q es proporcional a la probabilidad de una consulta dada el candidato p(q|ca), ponderada por la creencia a priori p(ca) de que el candidato ca es un experto. p(ca|q) ∝ p(q|ca)p(ca) (2) En este artículo, nuestro enfoque principal se centra en estimar la probabilidad de una consulta dada el candidato p(q|ca), ya que esta probabilidad captura en qué medida el candidato conoce sobre el tema de la consulta. Si bien se asume generalmente que los priors del candidato son uniformes y, por lo tanto, no influirán en la clasificación, se ha demostrado que una elección sensata de priors puede mejorar el rendimiento [20]. Perfilado de expertos. Mientras que la tarea de búsqueda de expertos se centraba en encontrar expertos en un tema específico, la tarea de perfilado de expertos busca responder a una pregunta relacionada: ¿Sobre qué temas tiene conocimiento un candidato? Básicamente, esto da un giro a las preguntas sobre la búsqueda de expertos. El perfilado de un candidato individual implica la identificación de áreas de habilidades y conocimientos en las que tienen experiencia, así como una evaluación del nivel de competencia en cada una de estas áreas. Este es el perfil temático de los candidatos. Generalmente, los perfiles temáticos dentro de las organizaciones consisten en estructuras tabulares que catalogan explícitamente las habilidades y conocimientos de cada individuo en la organización. Sin embargo, esta práctica está limitada por los recursos disponibles para definir, crear, mantener y actualizar estos perfiles con el tiempo. Al centrarnos en métodos automáticos que se basan en la evidencia disponible dentro de los repositorios de documentos de una organización, nuestro objetivo es reducir el esfuerzo humano asociado con el mantenimiento de perfiles temáticos. Un perfil temático de un candidato, entonces, se define como un vector donde cada elemento i del vector corresponde a la experiencia del candidato en un tema dado ki, (es decir, s(ca, ki)). Cada tema ki define un área de conocimiento o habilidad particular que la organización utiliza para definir el perfil temático de los candidatos. Por lo tanto, se asume que se proporciona una lista de temas, {k1, . . . , kn}, donde n es el número de temas predefinidos: perfil(ca) = s(ca, k1), s(ca, k2), . . . , s(ca, kn) . Se necesita contexto y evidencia para ayudar a los usuarios de sistemas de búsqueda de expertos a decidir a quién contactar al buscar experiencia en un área particular. Ejemplos de dicho contexto son: ¿Con quién trabaja ella? ¿Cuáles son sus datos de contacto? ¿Está bien conectada, por si acaso no puede ayudarnos ella misma? ¿Cuál es su rol en la organización? ¿Quién es su superior? Los colaboradores, afiliaciones, etc. son parte del perfil social de los candidatos y pueden servir como antecedentes para interpretar las recomendaciones del sistema. En este documento solo abordamos el problema de determinar perfiles temáticos, dejando el perfilado social para trabajos futuros. Planteamos el problema de cuantificar la competencia de una persona en un área de conocimiento específica de la siguiente manera: ¿Cuál es la probabilidad de que un área de conocimiento (ki) forme parte del perfil de candidatos (expertise)? donde s(ca, ki) está definido por p(ki|ca). Nuestra tarea, entonces, es estimar p(ki|ca), lo cual es equivalente al problema de obtener p(q|ca), donde el tema ki se representa como un tema de consulta q, es decir, una secuencia de palabras clave que representan la experiencia requerida. Tanto la tarea de encontrar expertos como la de perfilar expertos dependen de la estimación precisa de p(q|ca). La única diferencia proviene de la probabilidad previa de que una persona sea experta (p(ca)), la cual puede ser incorporada en la tarea de encontrar expertos. Este antecedente no se aplica a la tarea de perfilado ya que el candidato (individuo) está fijo. 4. MODELOS DE REFERENCIA En esta sección describimos nuestros modelos de referencia para estimar p(q|ca), es decir, las asociaciones entre temas y personas. Tanto la búsqueda de expertos como el perfilado de expertos se reducen a esta estimación. Empleamos tres modelos para calcular esta probabilidad. 4.1 De temas a candidatos Utilizando Modelos de Candidatos: El Modelo 1 [4] define la probabilidad de una consulta dada un candidato (p(q|ca)) utilizando técnicas estándar de modelado de lenguaje, basadas en un modelo de lenguaje multinomial de unigrama. Para cada candidato ca, se infiere un modelo de lenguaje candidato θca de tal manera que la probabilidad de un término dado θca no sea cero para todos los términos, es decir, p(t|θca) > 0. A partir del modelo de candidato, la consulta se genera con la siguiente probabilidad: p(q|θca) = Y t∈q p(t|θca)n(t,q), donde cada término t en la consulta q se muestrea de manera idéntica e independiente, y n(t, q) es el número de veces que t ocurre en q. El modelo de lenguaje candidato se infiere de la siguiente manera: (1) se calcula un modelo empírico p(t|ca); (2) se suaviza con probabilidades de fondo. Usando las asociaciones entre un candidato y un documento, la probabilidad p(t|ca) puede aproximarse por: p(t|ca) = X d p(t|d)p(d|ca), donde p(d|ca) es la probabilidad de que el candidato ca genere un documento de apoyo d, y p(t|d) es la probabilidad de que un término t ocurra en el documento d. Utilizamos la estimación de máxima verosimilitud de un término, es decir, la frecuencia normalizada del término t en el documento d. La fuerza de la asociación entre el documento d y el candidato ca expresada por p(d|ca) refleja el grado en que la experiencia de los candidatos se describe utilizando este documento. La estimación de esta probabilidad se presenta más adelante, en la Sección 4.2. El modelo candidato se construye entonces como una interpolación lineal de p(t|ca) y el modelo de fondo p(t) para asegurar que no haya probabilidades nulas, lo que resulta en la estimación final: p(q|θca) = (4) Y t∈q ( (1 − λ) X d p(t|d)p(d|ca) ! + λp(t) )n(t,q) . El Modelo 1 recopila toda la información de términos de todos los documentos asociados con el candidato, y la utiliza para representar a ese candidato. Este modelo se utiliza para predecir qué tan probable es que un candidato produzca una consulta q. Esto puede interpretarse intuitivamente como la probabilidad de que este candidato hable sobre el tema de la consulta, donde asumimos que esto es indicativo de su experiencia. Usando Modelos de Documentos: El Modelo 2 [4] toma un enfoque diferente. Aquí, el proceso se divide en dos partes. Dado un candidato ca, (1) se selecciona un documento asociado con un candidato con probabilidad p(d|ca), y (2) a partir de este documento se genera una consulta q con probabilidad p(q|d). Entonces se toma la suma sobre todos los documentos para obtener p(q|ca), de modo que: p(q|ca) = Σ d p(q|d)p(d|ca). (5) La probabilidad de una consulta dada un documento se estima inferiendo un modelo de lenguaje del documento θd para cada documento d de manera similar a como se infería el modelo candidato: p(t|θd) = (1 − λ)p(t|d) + λp(t), (6) donde p(t|d) es la probabilidad del término en el documento. La probabilidad de una consulta dada el modelo de documento es: p(q|θd) = Y t∈q p(t|θd)n(t,q) . La estimación final de p(q|ca) se obtiene sustituyendo p(q|d) por p(q|θd) en la Ecuación 5 (ver [4] para más detalles). Conceptualmente, el Modelo 2 difiere del Modelo 1 porque el candidato no está modelado directamente. En cambio, el documento actúa como una variable oculta en el proceso que separa la consulta del candidato. Este proceso es similar a cómo un usuario puede buscar candidatos con un motor de búsqueda estándar: inicialmente encontrando los documentos relevantes y luego viendo quién está asociado con ese documento. Al examinar una serie de documentos, el usuario puede obtener una idea de qué candidatos son más propensos a discutir el tema q. Usando Modelos de Temas: Modelo 3 Introducimos un tercer modelo, Modelo 3. En lugar de intentar modelar el proceso de generación de consultas a través de modelos de candidatos o documentos, representamos la consulta como un modelo de lenguaje de temas y estimamos directamente la probabilidad del candidato p(ca|q). Este enfoque es similar al modelo presentado en [3, 19]. Como en los modelos anteriores, se infiere un modelo de lenguaje, pero esta vez para la consulta. Adaptamos el trabajo de Lavrenko y Croft [14] para estimar un modelo de tema a partir de la consulta. El procedimiento es el siguiente. Dada una colección de documentos y un tema de consulta q, se asume que existe un modelo de tema desconocido θk que asigna probabilidades p(t|θk) a las ocurrencias de términos en los documentos del tema. Tanto la consulta como los documentos son muestras de θk (a diferencia de los enfoques anteriores, donde se asume que una consulta se extrae de un documento específico o un modelo candidato). La tarea principal es estimar p(t|θk), la probabilidad de un término dado el modelo de tema. Dado que la consulta q es muy dispersa y no hay ejemplos de documentos sobre el tema, es necesario aproximar esta distribución. Lavrenko y Croft [14] sugieren una forma razonable de obtener dicha aproximación, asumiendo que p(t|θk) puede aproximarse por la probabilidad del término t dado la consulta q. Entonces podemos estimar p(t|q) utilizando la probabilidad conjunta de observar el término t junto con los términos de la consulta, q1, . . . , qm, y dividiendo por la probabilidad conjunta de los términos de la consulta: p(t|θk) ≈ p(t|q) = p(t, q1, . . . , qm) p(q1, . . . , qm) = p(t, q1, . . . , qm) P t ∈T p(t , q1, . . . , qm) , donde p(q1, . . . , qm) = P t ∈T p(t , q1, . . . , qm), y T es el vocabulario completo de términos. Para estimar la probabilidad conjunta p(t, q1, . . . , qm), seguimos [14, 15] y asumimos que t y q1, . . . , qm son mutuamente independientes, una vez que elegimos una distribución fuente del conjunto de distribuciones fuente subyacentes U. Si elegimos U como un conjunto de modelos de documentos, entonces para construir este conjunto, la consulta q se emitiría contra la colección, y se asume que los primeros n devueltos son relevantes para el tema, y por lo tanto se tratan como muestras del modelo de tema. (Tenga en cuenta que en su lugar podrían usarse modelos candidatos). Con los modelos de documentos formando U, la probabilidad conjunta del término y la consulta se convierte en: p(t, q1, . . . , qm) = X d∈U p(d) ˘ p(t|θd) mY i=1 p(qi|θd) ¯ . (7) Aquí, p(d) denota la distribución previa sobre el conjunto U, que refleja la relevancia del documento para el tema. Suponemos que p(d) es uniforme en todo U. Para clasificar a los candidatos según el modelo de tema definido, utilizamos la métrica de divergencia de Kullback-Leibler (KL, [8]) para medir la diferencia entre los modelos de los candidatos y el modelo de tema: KL(θk||θca) = X t p(t|θk) log p(t|θk) p(t|θca) . (8) Se considera que los candidatos con una divergencia menor respecto al modelo de tema son más propensos a ser expertos en ese tema. El modelo candidato θca está definido en la Ecuación 4. Al utilizar la divergencia de Kullback-Leibler en lugar de la probabilidad de un candidato dado el modelo de tema p(ca|θk), evitamos problemas de normalización. 4.2 Asociaciones documento-candidato Para nuestros modelos, necesitamos poder estimar la probabilidad p(d|ca), que expresa en qué medida un documento d caracteriza al candidato ca. En [4], se presentan dos métodos para estimar esta probabilidad, basados en el número de nombres de personas reconocidos en un documento. Sin embargo, en nuestro entorno de intranet es razonable asumir que los autores de los documentos pueden ser identificados de manera inequívoca (por ejemplo, como el autor de un artículo, el profesor asignado a un curso, el propietario de una página web, etc.). Por lo tanto, establecemos p(d|ca) en 1 si el candidato ca es el autor del documento d, de lo contrario la probabilidad es 0. En la Sección 6 describimos cómo se puede determinar la autoría en diferentes tipos de documentos dentro de la colección. 5. La colección de expertos de UvT utilizada en los experimentos de este artículo se ajusta al escenario descrito en la Sección 3. La colección se basa en el sistema Webwijs (Webwise) desarrollado en la Universidad de Tilburg (UvT) en los Países Bajos. Webwijs (http://www.uvt.nl/webwijs/) es una base de datos de acceso público de empleados de la UvT que están involucrados en investigación o enseñanza; actualmente, Webwijs contiene información sobre 1168 expertos, cada uno de los cuales tiene una página con información de contacto y, si está disponible por parte del experto, una descripción de investigación y lista de publicaciones. Además, cada experto puede seleccionar áreas de especialización de una lista de 1491 temas y se le anima a sugerir nuevos temas que necesitan ser aprobados por el editor de Webwijs. Cada tema tiene una página separada que muestra a todos los expertos asociados con ese tema y, si está disponible, una lista de temas relacionados. Webwijs está disponible en holandés e inglés, y esta bilingüidad se ha preservado en la colección. Cada página de Webwijs en holandés tiene una traducción al inglés. No todos los temas en holandés tienen una traducción al inglés, pero lo contrario es cierto: los 981 temas en inglés tienen un equivalente en holandés. Aproximadamente el 42% de los expertos imparten cursos en la Universidad de Tilburg; estos cursos también fueron rastreados e incluidos en el perfil. Además, aproximadamente el 27% de los expertos enlazan su página académica desde su página de Webwijs. Estas páginas de inicio fueron rastreadas y añadidas a la colección. (Esto significa que si los expertos colocaron las versiones completas de sus publicaciones en sus páginas de inicio académicas, estas también estaban disponibles para su indexación). También obtuvimos 1880 versiones completas de publicaciones del repositorio institucional de UvT y el número de expertos holandeses en inglés es de 1168, con 743 expertos con ≥ 1 tema. El número de temas es de 1491 y 981, y el número de pares experto-tema es de 4318 y 3251, con un promedio de temas por experto de 5.8 y 5.9 respectivamente. El máximo número de temas por experto es de 60 (1) y 35 (1), y el mínimo es de 1 (74) y 1 (106) respectivamente. El promedio de expertos por tema es de 2.9 y 3.3, con un máximo de 30 (1) y un mínimo de 1 (615) y 1 (346) respectivamente. Hay 318 expertos con HP y CD, con un promedio de 3.5 CDs por experto docente. Además, hay 329 expertos con RD y 734 con PUB, con un promedio de 27.0 publicaciones por experto, 25.2 citas por experto y 1.8 publicaciones completas por experto. Tabla 2: Estadísticas descriptivas de las versiones holandesas e inglesas de la colección de expertos de UvT. Ejecutamos el identificador de idioma TextCat [23] para clasificar el idioma de las páginas de inicio y las publicaciones de texto completo. Nos limitamos a las páginas en las que el clasificador tenía confianza en el idioma utilizado en la página. Esto resultó en cuatro tipos de documentos: descripciones de investigación (RD), descripciones de cursos (CD), publicaciones (PUB; versiones de texto completo y solo de cita) y páginas web académicas (HP). Todo fue agrupado en la colección UvT Expert que está disponible en http://ilk.uvt.nl/uvt-expert-collection/. La colección UvT Expert fue extraída de un entorno organizativo diferente a la colección de la W3C y difiere de ella en varios aspectos. El entorno de UvT es uno con cantidades relativamente pequeñas de datos multilingües. Las asociaciones entre los documentos y los autores son claras y los datos están estructurados y limpios. La colección abarca una amplia gama de áreas de especialización, como suele encontrarse en intranets de universidades y otros institutos intensivos en conocimiento. Además, nuestro entorno universitario cuenta con varios tipos de estructura (temática y organizativa), así como múltiples tipos de documentos. Otra diferencia importante entre los dos conjuntos de datos es que las áreas de especialización en la colección de Expertos de UvT son autoseleccionadas en lugar de basarse en la membresía de un grupo o asignaciones de otros. El tamaño es otra dimensión en la que difieren las colecciones de expertos de W3C y UvT: esta última es la más pequeña de las dos. También son realistas las grandes diferencias en la cantidad de información disponible para cada experto. Utilizar Webwijs es voluntario; 425 expertos holandeses no seleccionaron ningún tema en absoluto. Esto nos deja con 743 perfiles de expertos holandeses y 727 perfiles de expertos en inglés utilizables. La Tabla 2 proporciona estadísticas descriptivas para la colección de Expertos de UvT. Las universidades tienden a tener una estructura jerárquica que va desde el nivel de la facultad, pasando por los departamentos, grupos de investigación, hasta llegar a los investigadores individuales. En la colección de Expertos de UvT tenemos información sobre las afiliaciones de los investigadores con facultades e institutos, lo que nos proporciona una jerarquía organizativa de dos niveles. La Universidad de Tilburg tiene 22 unidades organizativas a nivel de facultad (incluida la oficina universitaria y varios institutos de investigación) y 71 departamentos, lo que equivale a 3.2 departamentos por facultad. En cuanto a la jerarquía temática utilizada por Webwijs, 131 de los 1491 temas son nodos principales en la jerarquía. Esta jerarquía tiene una longitud promedio de cadena de temas de 2.65 y una longitud máxima de 7 temas. 6. EVALUACIÓN A continuación, evaluamos los modelos de la Sección 4 para la búsqueda y perfilado de expertos en la colección de Expertos de UvT. Detallamos nuestras preguntas de investigación y configuración experimental, y luego presentamos nuestros resultados. 6.1 Preguntas de investigación Abordamos las siguientes preguntas de investigación. Tanto la búsqueda de expertos como el perfilado dependen de las estimaciones de p(q|ca). La pregunta es cómo se comparan los modelos en las diferentes tareas, y en el contexto de la colección de expertos de UvT. En [4], el Modelo 2 superó al Modelo 1 en la colección de la W3C. ¿Cómo se comparan en nuestro conjunto de datos? ¿Y cómo se compara el Modelo 3 con el Modelo 1? ¿Qué hay de las diferencias de rendimiento entre los dos idiomas en nuestra colección de pruebas? 6.2 Configuración Experimental La salida de nuestros modelos fue evaluada frente a las etiquetas de temas autoasignadas, que fueron tratadas como juicios de relevancia. Los resultados fueron evaluados por separado para inglés y holandés. Para inglés solo usamos temas para los cuales había traducción al neerlandés; para neerlandés se consideraron todos los temas. Los resultados se promediaron para las consultas en la intersección de las evaluaciones de relevancia y los resultados; las consultas faltantes no contribuyen con un valor de 0 a las puntuaciones. Utilizamos medidas estándar de recuperación de información, como la Precisión Media Promedio (MAP) y la Reciprocidad Media Promedio (MRR). También informamos el porcentaje de temas (%q) y candidatos (%ca) cubiertos, respectivamente, para las tareas de búsqueda y perfilado de expertos. 6.3 Resultados La Tabla 1 muestra el rendimiento de los Modelos 1, 2 y 3 en las tareas de búsqueda y perfilado de expertos. Las filas de la tabla corresponden a los diversos tipos de documentos (RD, CD, PUB y HP) y a sus combinaciones. RD+CD+PUB+HP es equivalente a la colección completa y será referida como la LÍNEA BASE de nuestros experimentos. Al observar la Tabla 1, vemos que el Modelo 2 tiene el mejor rendimiento en general. Sin embargo, cuando los datos están limpios y muy enfocados (RD), el Modelo 3 lo supera en varios casos. El modelo 1 tiene la mejor cobertura de candidatos (%ca) y temas (%q). Los diversos tipos de documentos difieren en sus características y en cómo mejoran las tareas de búsqueda y perfilado. El perfilado de expertos se beneficia mucho de los datos limpios presentes en los tipos de documentos RD y CD, mientras que las publicaciones contribuyen principalmente a la tarea de encontrar expertos. Agregar las páginas de inicio no resulta ser particularmente útil. Cuando comparamos los resultados entre idiomas, encontramos que la cobertura de los temas en inglés (%q) es mayor que la de los temas en neerlandés para la búsqueda de expertos. Además de eso, las puntuaciones caen en el mismo rango para ambos idiomas. Para la tarea de perfilado, la cobertura de los candidatos (%ca) es muy similar para ambos idiomas. Sin embargo, el rendimiento es considerablemente mejor para los temas en inglés. Si bien es difícil comparar las puntuaciones entre colecciones, concluimos con una breve comparación de las puntuaciones absolutas en la Tabla 1 con las reportadas en [3, 4] en el conjunto de pruebas de W3C (edición de 2005). Para el experto que busca, los puntajes MAP para el Modelo 2 reportados aquí son aproximadamente un 50% más altos que las cifras correspondientes en [4], mientras que nuestros puntajes MRR están ligeramente por debajo de los de [4]. Para el perfilado de expertos, las diferencias son mucho más dramáticas: las puntuaciones MAP para el Modelo 2 reportadas aquí son aproximadamente un 50% más bajas que las puntuaciones en [3], mientras que las puntuaciones MRR (mejores) son aproximadamente iguales a las de [3]. La causa de estas diferencias parece residir en el número de áreas de conocimiento consideradas aquí, aproximadamente 30 veces más que en el entorno del W3C. MODELOS AVANZADOS Ahora que hemos desarrollado y evaluado técnicas básicas de modelado de lenguaje para la recuperación de expertos, nos enfocamos en modelos refinados que explotan características especiales de nuestra colección de pruebas. 7.1 Explotando la similitud en el área de conocimiento Una forma de mejorar la puntuación de una consulta dada un candidato es considerar qué otras solicitudes satisfaría el candidato y usarlas como evidencia adicional para respaldar la consulta original, proporcionalmente. Búsqueda de expertos Perfilado de expertos Tipos de documentos Modelo 1 Modelo 2 Modelo 3 Modelo 1 Modelo 2 Modelo 3 %q MAP MRR %q MAP MRR %q MAP MRR %ca MAP MRR %ca MAP MRR %ca MAP MRR Inglés RD 97.8 0.126 0.269 83.5 0.144 0.311 83.3 0.129 0.271 100 0.089 0.189 39.3 0.232 0.465 41.1 0.166 0.337 CD 97.8 0.118 0.227 91.7 0.123 0.248 91.7 0.118 0.226 32.8 0.188 0.381 32.4 0.195 0.385 32.7 0.203 0.370 PUB 97.8 0.200 0.330 98.0 0.216 0.372 98.0 0.145 0.257 78.9 0.167 0.364 74.5 0.212 0.442 78.9 0.135 0.299 HP 97.8 0.081 0.186 97.4 0.071 0.168 97.2 0.062 0.149 31.2 0.150 0.299 28.8 0.185 0.335 30.1 0.136 0.287 RD+CD 97.8 0.188 0.352 92.9 0.193 0.360 92.9 0.150 0.273 100 0.145 0.286 61.3 0.251 0.477 63.2 0.217 0.416 RD+CD+PUB 97.8 0.235 0.373 98.1 0.277 0.439 98.1 0.178 0.305 100 0.196 0.380 87.2 0.280 0.533 89.5 0.170 0.344 RD+CD+PUB+HP 97.8 0.237 0.372 98.6 0.280 0.441 98.5 0.166 0.293 100 0.199 0.387 88.7 0.281 0.525 90.9 0.169 0.329 Holandés RD 61.3 0.094 0.229 38.4 0.137 0.336 38.3 0.127 0.295 38.0 0.127 0.386 34.1 0.138 0.420 38.0 0.105 0.327 CD 61.3 0.107 0.212 49.7 0.128 0.256 49.7 0.136 0.261 32.5 0.151 0.389 31.8 0.158 0.396 32.5 0.170 0.380 PUB 61.3 0.193 0.319 59.5 0.218 0.368 59.4 0.173 0.291 78.8 0.126 0.364 76.0 0.150 0.424 78.8 0.103 0.294 HP 61.3 0.063 0.169 56.6 0.064 0.175 56.4 0.062 0.163 29.8 0.108 0.308 27.8 0.125 0.338 29.8 0.098 0.255 RD+CD 61.3 0.159 0.314 51.9 0.184 0.360 51.9 0.169 0.324 60.5 0.151 0.410 57.2 0.166 0.431 60.4 0.159 0.384 RD+CD+PUB 61.3 0.244 0.398 61.5 0.260 0.424 61.4 0.210 0.350 90.3 0.165 0.445 88.2 0.189 0.479 90.3 0.126 0.339 RD+CD+PUB+HP 61.3 0.249 0.401 62.6 0.265 0.436 62.6 0.195 0.344 91.9 0.164 0.426 90.1 0.195 0.488 91.9 0.125 0.328 Tabla 1: Rendimiento de los modelos en las tareas de búsqueda y perfilado de expertos, utilizando diferentes tipos de documentos y sus combinaciones. %q es el número de temas cubiertos (se aplica a la tarea de búsqueda de expertos), %ca es el número de candidatos cubiertos (se aplica a la tarea de perfilado de expertos). Los bloques superior e inferior corresponden al inglés y al holandés respectivamente. Las mejores puntuaciones están en negrita. Esto se puede modelar interpolando entre el p(q|ca) y la evidencia adicional de todos los pedidos similares q, de la siguiente manera: p(q|ca) = λp(q|ca) + (1 − λ) X q p(q|q )p(q |ca), (9) donde p(q|q ) representa la similitud entre los dos temas q y q. Para poder trabajar con métodos de similitud que no son necesariamente probabilidades, establecemos p(q|q ) = w(q,q ) γ , donde γ es una constante de normalización, de modo que γ = P q w(q , q ). Consideramos cuatro métodos para calcular la puntuación de similitud entre dos temas. Tres enfoques se basan estrictamente en el contenido y establecen similitudes examinando patrones de co-ocurrencia de temas dentro de la colección, mientras que el último enfoque explota la estructura jerárquica de áreas temáticas que pueden estar presentes dentro de una organización (ver [7] para más ejemplos de integrar relaciones entre palabras en modelos de lenguaje). La métrica de divergencia Kullback-Leibler (KL) definida en la ecuación 8 proporciona una medida de cuán diferentes o similares son dos distribuciones de probabilidad. Se infiere un modelo de tema para q y q utilizando el método presentado en la Sección 4.1 para describir la consulta en todo el vocabulario. Dado que un puntaje KL más bajo significa que las consultas son más similares, permitimos que w(q, q ) = max(KL(θq||·) − KL(θq||θq )). El Información Mutua Puntual (PMI, [17]) es una medida de asociación utilizada en teoría de la información para determinar el grado de independencia entre variables. La dependencia entre dos consultas se refleja en la puntuación SI(q, q), donde las puntuaciones mayores que cero indican que es probable que exista una dependencia, lo que interpretamos como que las consultas son probablemente similares: SI(q, q) = log p(q, q) / p(q)p(q). Estimamos la probabilidad de un tema p(q) utilizando el número de documentos relevantes para la consulta q dentro de la colección. La probabilidad conjunta p(q, q) se estima de manera similar, utilizando la concatenación de q y q como una consulta. Para obtener p(q|q), luego establecemos w(q, q) = SI(q, q) cuando SI(q, q) > 0, de lo contrario w(q, q) = 0, ya que solo estamos interesados en incluir consultas que sean similares. La estadística de log-verosimilitud proporciona otra medida de dependencia, que es más confiable que la medida de información mutua puntual [17]. Sea k1 el número de co-ocurrencias de q y q, k2 el número de ocurrencias de q que no co-ocurren con q, n1 el número total de ocurrencias de q, y n2 el número total de tokens de tema menos el número de ocurrencias de q. Entonces, sea p1 = k1/n1, p2 = k2/n2 y p = (k1 + k2)/(n1 + n2), (q, q ) = 2( (p1, k1, n1) + (p2, k2, n2) − (p, k1, n1) − (p, k2, n2)), donde (p, n, k) = k log p + (n − k) log(1 − p). El puntaje más alto indica que las consultas también son probablemente similares, por lo tanto, establecemos w(q, q ) = (q, q ). Finalmente, también estimamos la similitud de dos temas basándonos en su distancia dentro de la jerarquía de temas. La jerarquía de temas se visualiza como un grafo dirigido, y para todos los pares de temas se calcula el camino más corto SP(q, q). Establecimos el puntaje de similitud como el recíproco del camino más corto: w(q, q ) = 1/SP(q, q ). 7.2 Información contextual Dada la jerarquía de una organización, las unidades a las que pertenece una persona se consideran como un contexto para compensar la escasez de datos. Lo modelamos de la siguiente manera: p (q|ca) = 1 − P ou∈OU(ca) λou · p(q|ca) + P ou∈OU(ca) λou · p(q|ou), donde OU(ca) es el conjunto de unidades organizativas de las cuales el candidato ca es miembro, y p(q|o) expresa la fuerza de la asociación entre la consulta q y la unidad ou. La última probabilidad se puede estimar utilizando cualquiera de los tres modelos básicos, simplemente reemplazando ca con ou en las ecuaciones correspondientes. Una unidad organizativa está asociada con todos los documentos que sus miembros han escrito. Es decir, p(d|ou) = maxca∈ou p(d|ca). 7.3 Un modelo multilingüe simple Para institutos de conocimiento en Europa, ya sea académicos u otros, un entorno multilingüe (o al menos bilingüe) es típico. El siguiente modelo se basa en un tipo de suposición de independencia: no hay transferencia de conocimientos/perfiles a través de las barreras del idioma. Si bien es una simplificación, esta es una aproximación sensata en primer lugar. Eso es: p (q|ca) =P l∈L λl · p(ql|ca), donde L es el conjunto de idiomas utilizados en la colección, ql es la traducción de la consulta q al idioma l, y λl es un parámetro de suavizado específico del idioma, tal que P l∈L λl = 1. 8. MODELOS AVANZADOS: EVALUACIÓN En esta sección presentamos una evaluación experimental de nuestros modelos avanzados. Búsqueda de expertos Perfilado de expertos Modelo de lenguaje 1 Modelo 2 Modelo 3 Modelo 1 Modelo 2 Modelo 3 %q MAP MRR %q MAP MRR %q MAP MRR %ca MAP MRR %ca MAP MRR %ca MAP MRR Solo inglés 97.8 0.237 0.372 98.6 0.280 0.441 98.5 0.166 0.293 100 0.199 0.387 88.7 0.281 0.525 90.9 0.169 0.329 Solo holandés 61.3 0.249 0.401 62.6 0.265 0.436 62.6 0.195 0.344 91.9 0.164 0.426 90.1 0.195 0.488 91.9 0.125 0.328 Combinación 99.4 0.297 0.444 99.7 0.324 0.491 99.7 0.223 0.388 100 0.241 0.445 92.1 0.313 0.564 93.2 0.224 0.411 Tabla 3: Rendimiento de la combinación de idiomas en las tareas de búsqueda y perfilado de expertos (en candidatos). Los mejores puntajes para cada modelo están en cursiva, los mejores puntajes absolutos para las tareas de búsqueda y perfilado de expertos están en negrita. Tabla 4: Rendimiento en las tareas de búsqueda de expertos (arriba) y perfilado (abajo), utilizando similitudes de áreas de conocimiento. Las carreras fueron evaluadas en los temas principales establecidos. Las mejores puntuaciones están en negrita. 8.1 Preguntas de investigación Nuestras preguntas siguen los refinamientos presentados en la sección anterior: ¿Explotar la similitud en el área de conocimiento mejora la efectividad? ¿Cuál de los diversos métodos para capturar las relaciones entre palabras es más efectivo? Además, ¿es útil nuestra forma de incorporar información contextual? ¿Para qué tareas? Y finalmente, ¿es nuestra forma simple de combinar las puntuaciones monolingües suficiente para obtener mejoras significativas? 8.2 Configuración experimental Dado que las autoevaluaciones también son escasas en nuestra colección, para poder medir las diferencias entre los diversos modelos, seleccionamos un subconjunto de temas y evaluamos (algunas de las) ejecuciones solo en este subconjunto. Este conjunto se denomina como temas principales, y consiste en temas que se encuentran en el nivel superior de la jerarquía temática. (Un tema principal tiene subtemas, pero no es un subtema de ningún otro tema). Este conjunto principal consiste en 132 temas en holandés y 119 en inglés. Los juicios de relevancia se limitaron al conjunto de temas principales, pero no se ampliaron con subtemas. 8.3 Explotando la similitud de áreas de conocimiento. La Tabla 4 presenta los resultados. Los cuatro métodos utilizados para estimar la similitud de áreas de conocimiento son la divergencia de KL (KLDIV) y PointLang. Modelo 1 Modelo 2 Modelo 3 MAP MRR MAP MRR MAP MRR Búsqueda de expertos UK TODOS 0.423 0.545 0.654 0.799 0.494 0.629 UK PRINCIPAL 0.500 0.621 0.704 0.834 0.587 0.699 NL TODOS 0.439 0.560 0.672 0.826 0.480 0.630 NL PRINCIPAL 0.440 0.584 0.645 0.816 0.515 0.655 Perfilado de expertos UK TODOS 0.240 0.640 0.306 0.778 0.223 0.616 UK PRINCIPAL 0.523 0.677 0.519 0.648 0.461 0.587 NL TODOS 0.203 0.716 0.254 0.770 0.183 0.627 NL PRINCIPAL 0.332 0.576 0.380 0.624 0.332 0.549 Tabla 5: Evaluación de los modelos de contexto en unidades organizativas. información mutua ponderada (PMI), log-verosimilitud (LL) y distancia dentro de la jerarquía de temas (HDIST). Logramos mejorar el resultado base en todos los casos, pero la mejora es más notable para la tarea de perfilado. Para ambas tareas, el método LL tuvo el mejor rendimiento. Los enfoques basados en el contenido tuvieron un rendimiento consistentemente mejor que HDIST. 8.4 Información contextual Una jerarquía de dos niveles de unidades organizativas (facultades e institutos) está disponible en la colección de Expertos de UvT. La unidad a la que pertenece una persona se utiliza como contexto para esa persona. Primero, evaluamos los modelos de las unidades organizativas, utilizando todos los temas (TODOS) y solo los temas principales (PRINCIPALES). Una unidad organizativa se considera relevante para un tema dado (o viceversa) si al menos un miembro de la unidad seleccionó el tema dado como área de especialización. La Tabla 5 informa sobre los resultados. En lo que respecta a la búsqueda de expertos, dado un tema, la unidad organizativa correspondiente puede ser identificada con alta precisión. Sin embargo, la tarea de perfilado de expertos muestra una imagen diferente: las puntuaciones son bajas y la tarea parece difícil. La explicación puede ser que los conceptos generales (es decir, nuestros temas principales) pueden pertenecer a varias unidades organizativas. Segundo, realizamos otra evaluación, donde combinamos los modelos contextuales con los modelos candidatos (para puntuar a los candidatos nuevamente). La tabla 6 informa sobre los resultados. Encontramos un impacto positivo de los modelos de contexto solo para la búsqueda de expertos. Notablemente, para la búsqueda de expertos (y Modelo 1), mejora más del 50% (para inglés) y más del 70% (para holandés) en el MAP. El bajo rendimiento en la creación de perfiles de expertos puede deberse al hecho de que los modelos de contexto por sí solos no tuvieron un buen desempeño en la tarea de creación de perfiles desde el principio. 8.5 Modelos multilingües En esta subsección evaluamos el método para combinar resultados en múltiples idiomas que describimos en la Sección 7.3. En nuestro entorno, el conjunto de idiomas consiste en inglés y holandés: L = {UK, NL}. Los pesos de estos idiomas se establecieron para ser idénticos (λUK = λNL = 0.5). Realizamos experimentos con diferentes configuraciones de λ, pero no observamos diferencias significativas en el rendimiento. La Tabla 3 informa sobre los resultados multilingües, donde el rendimiento se evalúa en el conjunto completo de temas. Los tres modelos tienen una diferencia significativa. Tabla 6: Rendimiento de los modelos de contexto (CT) en comparación con la línea base (BL). Las mejores puntuaciones están en negrita, demostradas en todas las medidas para ambas tareas. La cobertura de temas y candidatos para las tareas de búsqueda y perfilado de expertos, respectivamente, es cercana al 100% en todos los casos. La mejora relativa de las puntuaciones de precisión varía del 10% al 80%. Estas puntuaciones demuestran que, a pesar de su simplicidad, nuestro método para combinar resultados en varios idiomas logra mejoras sustanciales sobre el punto de referencia. CONCLUSIONES En este artículo nos enfocamos en la recuperación de expertos (búsqueda y perfilado de expertos) en un nuevo entorno de una organización típica intensiva en conocimiento en la que los datos disponibles son de alta calidad, multilingües y abarcan una amplia gama de áreas de expertise. Normalmente, la cantidad de datos disponibles en una organización como una universidad, un instituto de investigación o un laboratorio de investigación es limitada en comparación con la colección de la W3C que ha sido utilizada principalmente para la evaluación experimental de la recuperación de la experiencia hasta ahora. Para examinar la recuperación de la experiencia en este entorno, presentamos (y lanzamos) la colección de Expertos de UvT como un caso representativo de organizaciones intensivas en conocimiento. La nueva colección refleja las propiedades típicas de los institutos intensivos en conocimiento mencionados anteriormente e incluye también varias características que pueden ser potencialmente útiles para la recuperación de expertos, como la estructura temática y organizativa. Evaluamos cómo los modelos de vanguardia para la búsqueda y perfilado de expertos se desempeñaron en este nuevo entorno y luego refinamos estos modelos para intentar explotar las diferentes características dentro del entorno de datos (idioma, actualidad y estructura organizativa). Encontramos que los modelos actuales de recuperación de experiencia se generalizan bien a este nuevo entorno; además, descubrimos que refinar los modelos para tener en cuenta las diferencias resulta en mejoras significativas, compensando así los problemas causados por la escasez de datos. El trabajo futuro incluye establecer evaluaciones manuales de perfiles generados automáticamente por los propios empleados, especialmente en casos en los que los empleados no han proporcionado un perfil ellos mismos. 10. AGRADECIMIENTOS Krisztian Balog fue apoyado por la Organización Neerlandesa para la Investigación Científica (NWO) bajo el número de proyecto 220-80-001. Maarten de Rijke también recibió apoyo de NWO bajo los números de proyecto 017.001.190, 220-80-001, 264-70-050, 354-20-005, 600.065.120, 612-13-001, 612.000.106, 612.066.302, 612.069.006, 640.001.501, 640.002.501, y de la Unión Europea. Programa IST del 6º Programa Marco para la I+D en virtud del contrato del proyecto MultiMATCH IST-033104. El trabajo de Toine Bogers y Antal van den Bosch fue financiado por el programa IOP-MMI de SenterNovem / Ministerio de Asuntos Económicos de los Países Bajos, como parte del proyecto 'A Propos'. 11. REFERENCIAS [1] L. Azzopardi. Incorporando contexto en el marco de modelado del lenguaje para la recuperación de información ad-hoc. Tesis doctoral, Universidad de Paisley, 2005. [2] K. Balog y M. de Rijke. Encontrando expertos similares. En este volumen, 2007. [3] K. Balog y M. de Rijke. Determinación de perfiles de expertos (con una aplicación a la búsqueda de expertos). En IJCAI 07: Proc. 20th Intern. Conferencia Conjunta sobre Inteligencia Artificial, páginas 2657-2662, 2007. [4] K. Balog, L. Azzopardi y M. de Rijke. Modelos formales para la búsqueda de expertos en corporaciones empresariales. En SIGIR 06: Actas de la 29ª conferencia internacional anual. Conferencia ACM SIGIR sobre Investigación y desarrollo en recuperación de información, páginas 43-50, 2006. [5] I. Becerra-Fernández. El papel de las tecnologías de inteligencia artificial en la implementación de sistemas de gestión del conocimiento de localización de personas. En el taller de AAAI sobre la incorporación de conocimiento en los procesos de negocio, marzo de 2000. [6] C. S. Campbell, P. P. Maglio, A. Cozzi y B. Dom. Identificación de la experiencia utilizando comunicaciones por correo electrónico. En CIKM 03: Actas de la duodécima conferencia internacional sobre gestión de la información y el conocimiento, páginas 528-531, 2003. [7] G. Cao, J.-Y. Nie, y J. Bai. Integrando relaciones entre palabras en modelos de lenguaje. En SIGIR 05: Actas de la 28ª conferencia internacional anual. Conferencia ACM SIGIR sobre Investigación y desarrollo en recuperación de información, páginas 298-305, 2005. [8] T. M. Cover y J. A. Thomas. Elementos de la teoría de la información. Wiley-Interscience, 1991. [9] N. Craswell, D. Hawking, A. M. Vercoustre y P. Wilkins. Experto en P@noptic: Buscando expertos no solo documentos. En Ausweb, 2001. [10] N. Craswell, A. de Vries e I. Soboroff. Resumen de la pista empresarial TREC2005. En la Decimocuarta Conferencia de Recuperación de Información. Proc. (TREC 2005), 2006. [11] T. H. Davenport y L. Prusak. Conocimiento en Acción: Cómo las Organizaciones Gestionan lo que Saben. Harvard Business School Press, Boston, MA, 1998. [12] T. Dunning.\nHarvard Business School Press, Boston, MA, 1998. [12] T. Dunning. Métodos precisos para la estadística de sorpresa y coincidencia. Lingüística Computacional, 19(1):61-74, 1993. [13] E. Filatova y J. Prager. Dime qué haces y te diré quién eres: Aprendiendo actividades relacionadas con la ocupación para biografías. En HLT/EMNLP, 2005. [14] V. Lavrenko y W. B. Croft. Modelos de lenguaje basados en relevancia. En SIGIR 01: Actas de la 24ª conferencia internacional anual. Conferencia ACM SIGIR sobre Investigación y Desarrollo en Recuperación de Información, páginas 120-127, 2001. [15] V. Lavrenko, M. Choquette y W. B. Croft. Modelos de relevancia multilingües. En SIGIR 02: Actas de la 25ª conferencia internacional anual. Conferencia ACM SIGIR sobre Investigación y Desarrollo en Recuperación de Información, páginas 175-182, 2002. [16] C. Macdonald e I. Ounis. Votación de candidatos: adaptando técnicas de fusión de datos para una tarea de búsqueda de expertos. En CIKM 06: Actas de la 15ª conferencia internacional de ACM sobre gestión de la información y el conocimiento, páginas 387-396, 2006. [17] C. Manning y H. Schütze. Fundamentos del Procesamiento del Lenguaje Natural Estadístico. El MIT Press, 1999. [18] A. Mockus y J. D. Herbsleb. Explorador de experticia: un enfoque cuantitativo para identificar la experticia. En ICSE 02: Proc. 24th Intern. Conf. en Ingeniería de Software, páginas 503-512, 2002. [19] D. Petkova y W. B. Croft. Modelos de lenguaje jerárquicos para la búsqueda de expertos en corporaciones empresariales. En Proc. ICTAI 2006, páginas 599-608, 2006. [20] I. Soboroff, A. de Vries y N. Craswell. Resumen de la pista empresarial TREC 2006. En las Notas de Trabajo de TREC 2006, 2006. [21] T. Tao, X. Wang, Q. Mei y C. Zhai. Recuperación de información del modelo de lenguaje con expansión de documentos. En HLT-NAACL 2006, 2006. [22] TREC. Ruta empresarial, 2005. URL: http://www.ins.cwi.nl/projects/trec-ent/wiki/. [23] G. van Noord.\nURL: http://www.ins.cwi.nl/projects/trec-ent/wiki/. [23] G. van Noord. Adivinador de idioma TextCat. URL: http://www.let.rug.nl/˜vannoord/TextCat/. [24] W3C. La colección de pruebas del W3C, 2005. URL: http://research.microsoft.com/users/nickcr/w3c-summary.html. ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "trec enterprise track": {
            "translated_key": "recuperación de expertos",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Broad Expertise Retrieval in Sparse Data Environments Krisztian Balog ISLA, University of Amsterdam Kruislaan 403, 1098 SJ Amsterdam, The Netherlands kbalog@science.uva.nl Toine Bogers ILK, Tilburg University P.O.",
                "Box 90153, 5000 LE Tilburg, The Netherlands A.M.Bogers@uvt.nl Leif Azzopardi Dept. of Computing Science University of Glasgow, Glasgow, G12 8QQ leif@dcs.gla.ac.uk Maarten de Rijke ISLA, University of Amsterdam Kruislaan 403, 1098 SJ Amsterdam, The Netherlands mdr@science.uva.nl Antal van den Bosch ILK, Tilburg University P.O.",
                "Box 90153, 5000 LE Tilburg, The Netherlands Antal.vdnBosch@uvt.nl ABSTRACT Expertise retrieval has been largely unexplored on data other than the W3C collection.",
                "At the same time, many intranets of universities and other knowledge-intensive organisations offer examples of relatively small but clean multilingual expertise data, covering broad ranges of expertise areas.",
                "We first present two main expertise retrieval tasks, along with a set of baseline approaches based on generative language modeling, aimed at finding expertise relations between topics and people.",
                "For our experimental evaluation, we introduce (and release) a new test set based on a crawl of a university site.",
                "Using this test set, we conduct two series of experiments.",
                "The first is aimed at determining the effectiveness of baseline expertise retrieval methods applied to the new test set.",
                "The second is aimed at assessing refined models that exploit characteristic features of the new test set, such as the organizational structure of the university, and the hierarchical structure of the topics in the test set.",
                "Expertise retrieval models are shown to be robust with respect to environments smaller than the W3C collection, and current techniques appear to be generalizable to other settings.",
                "Categories and Subject Descriptors H.3 [Information Storage and Retrieval]: H.3.1 Content Analysis and Indexing; H.3.3 Information Search and Retrieval; H.3.4 Systems and Software; H.4 [Information Systems Applications]: H.4.2 Types of Systems; H.4.m Miscellaneous General Terms Algorithms, Measurement, Performance, Experimentation 1.",
                "INTRODUCTION An organizations intranet provides a means for exchanging information between employees and for facilitating employee collaborations.",
                "To efficiently and effectively achieve this, it is necessary to provide search facilities that enable employees not only to access documents, but also to identify expert colleagues.",
                "At the <br>trec enterprise track</br> [22] the need to study and understand expertise retrieval has been recognized through the introduction of Expert Finding tasks.",
                "The goal of expert finding is to identify a list of people who are knowledgeable about a given topic.",
                "This task is usually addressed by uncovering associations between people and topics [10]; commonly, a co-occurrence of the name of a person with topics in the same context is assumed to be evidence of expertise.",
                "An alternative task, which using the same idea of people-topic associations, is expert profiling, where the task is to return a list of topics that a person is knowledgeable about [3].",
                "The launch of the Expert Finding task at TREC has generated a lot of interest in expertise retrieval, with rapid progress being made in terms of modeling, algorithms, and evaluation aspects.",
                "However, nearly all of the expert finding or profiling work performed has been validated experimentally using the W3C collection [24] from the Enterprise Track.",
                "While this collection is currently the only publicly available test collection for expertise retrieval tasks, it only represents one type of intranet.",
                "With only one test collection it is not possible to generalize conclusions to other realistic settings.",
                "In this paper we focus on expertise retrieval in a realistic setting that differs from the W3C setting-one in which relatively small amounts of clean, multilingual data are available, that cover a broad range of expertise areas, as can be found on the intranets of universities and other knowledge-intensive organizations.",
                "Typically, this setting features several additional types of structure: topical structure (e.g., topic hierarchies as employed by the organization), organizational structure (faculty, department, ...), as well as multiple types of documents (research and course descriptions, publications, and academic homepages).",
                "This setting is quite different from the W3C setting in ways that might impact upon the performance of expertise retrieval tasks.",
                "We focus on a number of research questions in this paper: Does the relatively small amount of data available on an intranet affect the quality of the topic-person associations that lie at the heart of expertise retrieval algorithms?",
                "How do state-of-the-art algorithms developed on the W3C data set perform in the alternative scenario of the type described above?",
                "More generally, do the lessons from the Expert Finding task at TREC carry over to this setting?",
                "How does the inclusion or exclusion of different documents affect expertise retrieval tasks?",
                "In addition to, how can the topical and organizational structure be used for retrieval purposes?",
                "To answer our research questions, we first present a set of baseline approaches, based on generative language modeling, aimed at finding associations between topics and people.",
                "This allows us to formulate the expert finding and expert profiling tasks in a uniform way, and has the added benefit of allowing us to understand the relations between the two tasks.",
                "For our experimental evaluation, we introduce a new data set (the UvT Expert Collection) which is representative of the type of intranet that we described above.",
                "Our collection is based on publicly available data, crawled from the website of Tilburg University (UvT).",
                "This type of data is particularly interesting, since (1) it is clean, heterogeneous, structured, and focused, but comprises a limited number of documents; (2) contains information on the organizational hierarchy; (3) it is bilingual (English and Dutch); and (4) the list of expertise areas of an individual are provided by the employees themselves.",
                "Using the UvT Expert collection, we conduct two sets of experiments.",
                "The first is aimed at determining the effectiveness of baseline expertise finding and profiling methods in this new setting.",
                "A second group of experiments is aimed at extensions of the baseline methods that exploit characteristic features of the UvT Expert Collection; specifically, we propose and evaluate refined expert finding and profiling methods that incorporate topicality and organizational structure.",
                "Apart from the research questions and data set that we contribute, our main contributions are as follows.",
                "The baseline models developed for expertise finding perform well on the new data set.",
                "While on the W3C setting the expert finding task appears to be more difficult than profiling, for the UvT data the opposite is the case.",
                "We find that profiling on the UvT data set is considerably more difficult than on the W3C set, which we believe is due to the large (but realistic) number of topical areas that we used for profiling: about 1,500 for the UvT set, versus 50 in the W3C case.",
                "Taking the similarity between topics into account can significantly improve retrieval performance.",
                "The best performing similarity measures are content-based, therefore they can be applied on the W3C (and other) settings as well.",
                "Finally, we demonstrate that the organizational structure can be exploited in the form of a context model, improving MAP scores for certain models by up to 70%.",
                "The remainder of this paper is organized as follows.",
                "In the next section we review related work.",
                "Then, in Section 3 we provide detailed descriptions of the expertise retrieval tasks that we address in this paper: expert finding and expert profiling.",
                "In Section 4 we present our baseline models, of which the performance is then assessed in Section 6 using the UvT data set that we introduce in Section 5.",
                "Advanced models exploiting specific features of our data are presented in Section 7 and evaluated in Section 8.",
                "We formulate our conclusions in Section 9. 2.",
                "RELATED WORK Initial approaches to expertise finding often employed databases containing information on the skills and knowledge of each individual in the organization [11].",
                "Most of these tools (usually called yellow pages or people-finding systems) rely on people to self-assess their skills against a predefined set of keywords.",
                "For updating profiles in these systems in an automatic fashion there is a need for intelligent technologies [5].",
                "More recent approaches use specific document sets (such as email [6] or software [18]) to find expertise.",
                "In contrast with focusing on particular document types, there is also an increased interest in the development of systems that index and mine published intranet documents as sources of evidence for expertise.",
                "One such published approach is the P@noptic system [9], which builds a representation of each person by concatenating all documents associated with that person-this is similar to Model 1 of Balog et al. [4], who formalize and compare two methods.",
                "Balog et al.s Model 1 directly models the knowledge of an expert from associated documents, while their Model 2 first locates documents on the topic and then finds the associated experts.",
                "In the reported experiments the second method performs significantly better when there are sufficiently many associated documents per candidate.",
                "Most systems that took part in the 2005 and 2006 editions of the Expert Finding task at TREC implemented (variations on) one of these two models; see [10, 20].",
                "Macdonald and Ounis [16] propose a different approach for ranking candidate expertise with respect to a topic based on data fusion techniques, without using collectionspecific heuristics; they find that applying field-based weighting models improves the ranking of candidates.",
                "Petkova and Croft [19] propose yet another approach, based on a combination of the above Model 1 and 2, explicitly modeling topics.",
                "Turning to other expert retrieval tasks that can also be addressed using topic-people associations, Balog and de Rijke [3] addressed the task of determining topical expert profiles.",
                "While their methods proved to be efficient on the W3C corpus, they require an amount of data that may not be available in the typical knowledge-intensive organization.",
                "Balog and de Rijke [2] study the related task of finding experts that are similar to a small set of experts given as input.",
                "As an aside, creating a textual summary of a person shows some similarities to biography finding, which has received a considerable amount of attention recently; see e.g., [13].",
                "We use generative language modeling to find associations between topics and people.",
                "In our modeling of expert finding and profiling we collect evidence for expertise from multiple sources, in a heterogeneous collection, and integrate it with the co-occurrence of candidates names and query terms-the language modeling setting allows us to do this in a transparent manner.",
                "Our modeling proceeds in two steps.",
                "In the first step, we consider three baseline models, two taken from [4] (the Models 1 and 2 mentioned above), and one a refined version of a model introduced in [3] (which we refer to as Model 3 below); this third model is also similar to the model described by Petkova and Croft [19].",
                "The models we consider in our second round of experiments are mixture models similar to contextual language models [1] and to the expanded documents of Tao et al. [21]; however, the features that we use for definining our expansions-including topical structure and organizational structure-have not been used in this way before. 3.",
                "TASKS In the expertise retrieval scenario that we envisage, users seeking expertise within an organization have access to an interface that combines a search box (where they can search for experts or topics) with navigational structures (of experts and of topics) that allows them to click their way to an expert page (providing the profile of a person) or a topic page (providing a list of experts on the topic).",
                "To feed the above interface, we face two expertise retrieval tasks, expert finding and expert profiling, that we first define and then formalize using generative language models.",
                "In order to model either task, the probability of the query topic being associated to a candidate expert plays a key role in the final estimates for searching and profiling.",
                "By using language models, both the candidates and the query are characterized by distributions of terms in the vocabulary (used in the documents made available by the organization whose expertise retrieval needs we are addressing). 3.1 Expert finding Expert finding involves the task of finding the right person with the appropriate skills and knowledge: Who are the experts on topic X?.",
                "E.g., an employee wants to ascertain who worked on a particular project to find out why particular decisions were made without having to trawl through documentation (if there is any).",
                "Or, they may be in need a trained specialist for consultancy on a specific problem.",
                "Within an organization there are usually many possible candidates who could be experts for given topic.",
                "We can state this problem as follows: What is the probability of a candidate ca being an expert given the query topic q?",
                "That is, we determine p(ca|q), and rank candidates ca according to this probability.",
                "The candidates with the highest probability given the query are deemed the most likely experts for that topic.",
                "The challenge is how to estimate this probability accurately.",
                "Since the query is likely to consist of only a few terms to describe the expertise required, we should be able to obtain a more accurate estimate by invoking Bayes Theorem, and estimating: p(ca|q) = p(q|ca)p(ca) p(q) , (1) where p(ca) is the probability of a candidate and p(q) is the probability of a query.",
                "Since p(q) is a constant, it can be ignored for ranking purposes.",
                "Thus, the probability of a candidate ca being an expert given the query q is proportional to the probability of a query given the candidate p(q|ca), weighted by the a priori belief p(ca) that candidate ca is an expert. p(ca|q) ∝ p(q|ca)p(ca) (2) In this paper our main focus is on estimating the probability of a query given the candidate p(q|ca), because this probability captures the extent to which the candidate knows about the query topic.",
                "Whereas the candidate priors are generally assumed to be uniformand thus will not influence the ranking-it has been demonstrated that a sensible choice of priors may improve the performance [20]. 3.2 Expert profiling While the task of expert searching was concerned with finding experts given a particular topic, the task of expert profiling seeks to answer a related question: What topics does a candidate know about?",
                "Essentially, this turns the questions of expert finding around.",
                "The profiling of an individual candidate involves the identification of areas of skills and knowledge that they have expertise about and an evaluation of the level of proficiency in each of these areas.",
                "This is the candidates topical profile.",
                "Generally, topical profiles within organizations consist of tabular structures which explicitly catalogue the skills and knowledge of each individual in the organization.",
                "However, such practice is limited by the resources available for defining, creating, maintaining, and updating these profiles over time.",
                "By focusing on automatic methods which draw upon the available evidence within the document repositories of an organization, our aim is to reduce the human effort associated with the maintenance of topical profiles1 .",
                "A topical profile of a candidate, then, is defined as a vector where each element i of the vector corresponds to the candidate cas expertise on a given topic ki, (i.e., s(ca, ki)).",
                "Each topic ki defines a particular knowledge area or skill that the organization uses to define the candidates topical profile.",
                "Thus, it is assumed that a list of topics, {k1, . . . , kn}, where n is the number of pre-defined topics, is given: profile(ca) = s(ca, k1), s(ca, k2), . . . , s(ca, kn) . (3) 1 Context and evidence are needed to help users of expertise finding systems to decide whom to contact when seeking expertise in a particular area.",
                "Examples of such context are: Who does she work with?",
                "What are her contact details?",
                "Is she well-connected, just in case she is not able to help us herself?",
                "What is her role in the organization?",
                "Who is her superior?",
                "Collaborators, and affiliations, etc. are all part of the candidates social profile, and can serve as a background against which the systems recommendations should be interpreted.",
                "In this paper we only address the problem of determining topical profiles, and leave social profiling to further work.",
                "We state the problem of quantifying the competence of a person on a certain knowledge area as follows: What is the probability of a knowledge area (ki) being part of the candidates (expertise) profile? where s(ca, ki) is defined by p(ki|ca).",
                "Our task, then, is to estimate p(ki|ca), which is equivalent to the problem of obtaining p(q|ca), where the topic ki is represented as a query topic q, i.e., a sequence of keywords representing the expertise required.",
                "Both the expert finding and profiling tasks rely on the accurate estimation of p(q|ca).",
                "The only difference derives from the prior probability that a person is an expert (p(ca)), which can be incorporated into the expert finding task.",
                "This prior does not apply to the profiling task since the candidate (individual) is fixed. 4.",
                "BASELINE MODELS In this section we describe our baseline models for estimating p(q|ca), i.e., associations between topics and people.",
                "Both expert finding and expert profiling boil down to this estimation.",
                "We employ three models for calculating this probability. 4.1 From topics to candidates Using Candidate Models: Model 1 Model 1 [4] defines the probability of a query given a candidate (p(q|ca)) using standard language modeling techniques, based on a multinomial unigram language model.",
                "For each candidate ca, a candidate language model θca is inferred such that the probability of a term given θca is nonzero for all terms, i.e., p(t|θca) > 0.",
                "From the candidate model the query is generated with the following probability: p(q|θca) = Y t∈q p(t|θca)n(t,q) , where each term t in the query q is sampled identically and independently, and n(t, q) is the number of times t occurs in q.",
                "The candidate language model is inferred as follows: (1) an empirical model p(t|ca) is computed; (2) it is smoothed with background probabilities.",
                "Using the associations between a candidate and a document, the probability p(t|ca) can be approximated by: p(t|ca) = X d p(t|d)p(d|ca), where p(d|ca) is the probability that candidate ca generates a supporting document d, and p(t|d) is the probability of a term t occurring in the document d. We use the maximum-likelihood estimate of a term, that is, the normalised frequency of the term t in document d. The strength of the association between document d and candidate ca expressed by p(d|ca) reflects the degree to which the candidates expertise is described using this document.",
                "The estimation of this probability is presented later, in Section 4.2.",
                "The candidate model is then constructed as a linear interpolation of p(t|ca) and the background model p(t) to ensure there are no zero probabilities, which results in the final estimation: p(q|θca) = (4) Y t∈q ( (1 − λ) X d p(t|d)p(d|ca) ! + λp(t) )n(t,q) .",
                "Model 1 amasses all the term information from all the documents associated with the candidate, and uses this to represent that candidate.",
                "This model is used to predict how likely a candidate would produce a query q.",
                "This can can be intuitively interpreted as the probability of this candidate talking about the query topic, where we assume that this is indicative of their expertise.",
                "Using Document Models: Model 2 Model 2 [4] takes a different approach.",
                "Here, the process is broken into two parts.",
                "Given a candidate ca, (1) a document that is associated with a candidate is selected with probability p(d|ca), and (2) from this document a query q is generated with probability p(q|d).",
                "Then the sum over all documents is taken to obtain p(q|ca), such that: p(q|ca) = X d p(q|d)p(d|ca). (5) The probability of a query given a document is estimated by inferring a document language model θd for each document d in a similar manner as the candidate model was inferred: p(t|θd) = (1 − λ)p(t|d) + λp(t), (6) where p(t|d) is the probability of the term in the document.",
                "The probability of a query given the document model is: p(q|θd) = Y t∈q p(t|θd)n(t,q) .",
                "The final estimate of p(q|ca) is obtained by substituting p(q|d) for p(q|θd) into Eq. 5 (see [4] for full details).",
                "Conceptually, Model 2 differs from Model 1 because the candidate is not directly modeled.",
                "Instead, the document acts like a hidden variable in the process which separates the query from the candidate.",
                "This process is akin to how a user may search for candidates with a standard search engine: initially by finding the documents which are relevant, and then seeing who is associated with that document.",
                "By examining a number of documents the user can obtain an idea of which candidates are more likely to discuss the topic q.",
                "Using Topic Models: Model 3 We introduce a third model, Model 3.",
                "Instead of attempting to model the query generation process via candidate or document models, we represent the query as a topic language model and directly estimate the probability of the candidate p(ca|q).",
                "This approach is similar to the model presented in [3, 19].",
                "As with the previous models, a language model is inferred, but this time for the query.",
                "We adapt the work of Lavrenko and Croft [14] to estimate a topic model from the query.",
                "The procedure is as follows.",
                "Given a collection of documents and a query topic q, it is assumed that there exists an unknown topic model θk that assigns probabilities p(t|θk) to the term occurrences in the topic documents.",
                "Both the query and the documents are samples from θk (as opposed to the previous approaches, where a query is assumed to be sampled from a specific document or candidate model).",
                "The main task is to estimate p(t|θk), the probability of a term given the topic model.",
                "Since the query q is very sparse, and as there are no examples of documents on the topic, this distribution needs to be approximated.",
                "Lavrenko and Croft [14] suggest a reasonable way of obtaining such an approximation, by assuming that p(t|θk) can be approximated by the probability of term t given the query q.",
                "We can then estimate p(t|q) using the joint probability of observing the term t together with the query terms, q1, . . . , qm, and dividing by the joint probability of the query terms: p(t|θk) ≈ p(t|q) = p(t, q1, . . . , qm) p(q1, . . . , qm) = p(t, q1, . . . , qm) P t ∈T p(t , q1, . . . , qm) , where p(q1, . . . , qm) = P t ∈T p(t , q1, . . . , qm), and T is the entire vocabulary of terms.",
                "In order to estimate the joint probability p(t, q1, . . . , qm), we follow [14, 15] and assume t and q1, . . . , qm are mutually independent, once we pick a source distribution from the set of underlying source distributions U.",
                "If we choose U to be a set of document models. then to construct this set, the query q would be issued against the collection, and the top n returned are assumed to be relevant to the topic, and thus treated as samples from the topic model. (Note that candidate models could be used instead.)",
                "With the document models forming U, the joint probability of term and query becomes: p(t, q1, . . . , qm) = X d∈U p(d) ˘ p(t|θd) mY i=1 p(qi|θd) ¯ . (7) Here, p(d) denotes the prior distribution over the set U, which reflects the relevance of the document to the topic.",
                "We assume that p(d) is uniform across U.",
                "In order to rank candidates according to the topic model defined, we use the Kullback-Leibler divergence metric (KL, [8]) to measure the difference between the candidate models and the topic model: KL(θk||θca) = X t p(t|θk) log p(t|θk) p(t|θca) . (8) Candidates with a smaller divergence from the topic model are considered to be more likely experts on that topic.",
                "The candidate model θca is defined in Eq. 4.",
                "By using KL divergence instead of the probability of a candidate given the topic model p(ca|θk), we avoid normalization problems. 4.2 Document-candidate associations For our models we need to be able to estimate the probability p(d|ca), which expresses the extent to which a document d characterizes the candidate ca.",
                "In [4], two methods are presented for estimating this probability, based on the number of person names recognized in a document.",
                "However, in our (intranet) setting it is reasonable to assume that authors of documents can unambiguously be identified (e.g., as the author of an article, the teacher assigned to a course, the owner of a web page, etc.)",
                "Hence, we set p(d|ca) to be 1 if candidate ca is author of document d, otherwise the probability is 0.",
                "In Section 6 we describe how authorship can be determined on different types of documents within the collection. 5.",
                "THE UVT EXPERT COLLECTION The UvT Expert collection used in the experiments in this paper fits the scenario outlined in Section 3.",
                "The collection is based on the Webwijs (Webwise) system developed at Tilburg University (UvT) in the Netherlands.",
                "Webwijs (http://www.uvt.nl/ webwijs/) is a publicly accessible database of UvT employees who are involved in research or teaching; currently, Webwijs contains information about 1168 experts, each of whom has a page with contact information and, if made available by the expert, a research description and publications list.",
                "In addition, each expert can select expertise areas from a list of 1491 topics and is encouraged to suggest new topics that need to be approved by the Webwijs editor.",
                "Each topic has a separate page that shows all experts associated with that topic and, if available, a list of related topics.",
                "Webwijs is available in Dutch and English, and this bilinguality has been preserved in the collection.",
                "Every Dutch Webwijs page has an English translation.",
                "Not all Dutch topics have an English translation, but the reverse is true: the 981 English topics all have a Dutch equivalent.",
                "About 42% of the experts teach courses at Tilburg University; these courses were also crawled and included in the profile.",
                "In addition, about 27% of the experts link to their academic homepage from their Webwijs page.",
                "These home pages were crawled and added to the collection. (This means that if experts put the full-text versions of their publications on their academic homepage, these were also available for indexing.)",
                "We also obtained 1880 full-text versions of publications from the UvT institutional repository and Dutch English no. of experts 1168 1168 no. of experts with ≥ 1 topic 743 727 no. of topics 1491 981 no. of expert-topic pairs 4318 3251 avg. no. of topics/expert 5.8 5.9 max. no. of topics/expert (no. of experts) 60 (1) 35 (1) min. no. of topics/expert (no. of experts) 1 (74) 1 (106) avg. no. of experts/topic 2.9 3.3 max. no. of experts/topic (no. of topics) 30 (1) 30 (1) min. no. of experts/topic (no. of topics) 1 (615) 1 (346) no. of experts with HP 318 318 no. of experts with CD 318 318 avg. no. of CDs per teaching expert 3.5 3.5 no. of experts with RD 329 313 no. of experts with PUB 734 734 avg. no. of PUBs per expert 27.0 27.0 avg. no. of PUB citations per expert 25.2 25.2 avg. no. of full-text PUBs per expert 1.8 1.8 Table 2: Descriptive statistics of the Dutch and English versions of the UvT Expert collection. converted them to plain text.",
                "We ran the TextCat [23] language identifier to classify the language of the home pages and the fulltext publications.",
                "We restricted ourselves to pages where the classifier was confident about the language used on the page.",
                "This resulted in four document types: research descriptions (RD), course descriptions (CD), publications (PUB; full-text and citationonly versions), and academic homepages (HP).",
                "Everything was bundled into the UvT Expert collection which is available at http: //ilk.uvt.nl/uvt-expert-collection/.",
                "The UvT Expert collection was extracted from a different organizational setting than the W3C collection and differs from it in a number of ways.",
                "The UvT setting is one with relatively small amounts of multilingual data.",
                "Document-author associations are clear and the data is structured and clean.",
                "The collection covers a broad range of expertise areas, as one can typically find on intranets of universities and other knowledge-intensive institutes.",
                "Additionally, our university setting features several types of structure (topical and organizational), as well as multiple document types.",
                "Another important difference between the two data sets is that the expertise areas in the UvT Expert collection are self-selected instead of being based on group membership or assignments by others.",
                "Size is another dimension along which the W3C and UvT Expert collections differ: the latter is the smaller of the two.",
                "Also realistic are the large differences in the amount of information available for each expert.",
                "Utilizing Webwijs is voluntary; 425 Dutch experts did not select any topics at all.",
                "This leaves us with 743 Dutch and 727 English usable expert profiles.",
                "Table 2 provides descriptive statistics for the UvT Expert collection.",
                "Universities tend to have a hierarchical structure that goes from the faculty level, to departments, research groups, down to the individual researchers.",
                "In the UvT Expert collection we have information about the affiliations of researchers with faculties and institutes, providing us with a two-level organizational hierarchy.",
                "Tilburg University has 22 organizational units at the faculty level (including the university office and several research institutes) and 71 departments, which amounts to 3.2 departments per faculty.",
                "As to the topical hierarchy used by Webwijs, 131 of the 1491 topics are top nodes in the hierarchy.",
                "This hierarchy has an average topic chain length of 2.65 and a maximum length of 7 topics. 6.",
                "EVALUATION Below, we evaluate Section 4s models for expert finding and profiling onthe UvT Expert collection.",
                "We detail our research questions and experimental setup, and then present our results. 6.1 Research Questions We address the following research questions.",
                "Both expert finding and profiling rely on the estimations of p(q|ca).",
                "The question is how the models compare on the different tasks, and in the setting of the UvT Expert collection.",
                "In [4], Model 2 outperformed Model 1 on the W3C collection.",
                "How do they compare on our data set?",
                "And how does Model 3 compare to Model 1?",
                "What about performance differences between the two languages in our test collection? 6.2 Experimental Setup The output of our models was evaluated against the self-assigned topic labels, which were treated as relevance judgements.",
                "Results were evaluated separately for English and Dutch.",
                "For English we only used topics for which the Dutch translation was available; for Dutch all topics were considered.",
                "The results were averaged for the queries in the intersection of relevance judgements and results; missing queries do not contribute a value of 0 to the scores.",
                "We use standard information retrieval measures, such as Mean Average Precision (MAP) and Mean Reciprocal Rank (MRR).",
                "We also report the percentage of topics (%q) and candidates (%ca) covered, for the expert finding and profiling tasks, respectively. 6.3 Results Table 1 shows the performance of Model 1, 2, and 3 on the expert finding and profiling tasks.",
                "The rows of the table correspond to the various document types (RD, CD, PUB, and HP) and to their combinations.",
                "RD+CD+PUB+HP is equivalent to the full collection and will be referred as the BASELINE of our experiments.",
                "Looking at Table 1 we see that Model 2 performs the best across the board.",
                "However, when the data is clean and very focused (RD), Model 3 outperforms it in a number of cases.",
                "Model 1 has the best coverage of candidates (%ca) and topics (%q).",
                "The various document types differ in their characteristics and how they improve the finding and profiling tasks.",
                "Expert profiling benefits much from the clean data present in the RD and CD document types, while the publications contribute the most to the expert finding task.",
                "Adding the homepages does not prove to be particularly useful.",
                "When we compare the results across languages, we find that the coverage of English topics (%q) is higher than of the Dutch ones for expert finding.",
                "Apart from that, the scores fall in the same range for both languages.",
                "For the profiling task the coverage of the candidates (%ca) is very similar for both languages.",
                "However, the performance is substantially better for the English topics.",
                "While it is hard to compare scores across collections, we conclude with a brief comparison of the absolute scores in Table 1 to those reported in [3, 4] on the W3C test set (2005 edition).",
                "For expert finding the MAP scores for Model 2 reported here are about 50% higher than the corresponding figures in [4], while our MRR scores are slightly below those in [4].",
                "For expert profiling, the differences are far more dramatic: the MAP scores for Model 2 reported here are around 50% below the scores in [3], while the (best) MRR scores are about the same as those in [3].",
                "The cause for the latter differences seems to reside in the number of knowledge areas considered here-approx. 30 times more than in the W3C setting. 7.",
                "ADVANCED MODELS Now that we have developed and assessed basic language modeling techniques for expertise retrieval, we turn to refined models that exploit special features of our test collection. 7.1 Exploiting knowledge area similarity One way to improve the scoring of a query given a candidate is to consider what other requests the candidate would satisfy and use them as further evidence to support the original query, proportional Expert finding Expert profiling Document types Model 1 Model 2 Model 3 Model 1 Model 2 Model 3 %q MAP MRR %q MAP MRR %q MAP MRR %ca MAP MRR %ca MAP MRR %ca MAP MRR English RD 97.8 0.126 0.269 83.5 0.144 0.311 83.3 0.129 0.271 100 0.089 0.189 39.3 0.232 0.465 41.1 0.166 0.337 CD 97.8 0.118 0.227 91.7 0.123 0.248 91.7 0.118 0.226 32.8 0.188 0.381 32.4 0.195 0.385 32.7 0.203 0.370 PUB 97.8 0.200 0.330 98.0 0.216 0.372 98.0 0.145 0.257 78.9 0.167 0.364 74.5 0.212 0.442 78.9 0.135 0.299 HP 97.8 0.081 0.186 97.4 0.071 0.168 97.2 0.062 0.149 31.2 0.150 0.299 28.8 0.185 0.335 30.1 0.136 0.287 RD+CD 97.8 0.188 0.352 92.9 0.193 0.360 92.9 0.150 0.273 100 0.145 0.286 61.3 0.251 0.477 63.2 0.217 0.416 RD+CD+PUB 97.8 0.235 0.373 98.1 0.277 0.439 98.1 0.178 0.305 100 0.196 0.380 87.2 0.280 0.533 89.5 0.170 0.344 RD+CD+PUB+HP 97.8 0.237 0.372 98.6 0.280 0.441 98.5 0.166 0.293 100 0.199 0.387 88.7 0.281 0.525 90.9 0.169 0.329 Dutch RD 61.3 0.094 0.229 38.4 0.137 0.336 38.3 0.127 0.295 38.0 0.127 0.386 34.1 0.138 0.420 38.0 0.105 0.327 CD 61.3 0.107 0.212 49.7 0.128 0.256 49.7 0.136 0.261 32.5 0.151 0.389 31.8 0.158 0.396 32.5 0.170 0.380 PUB 61.3 0.193 0.319 59.5 0.218 0.368 59.4 0.173 0.291 78.8 0.126 0.364 76.0 0.150 0.424 78.8 0.103 0.294 HP 61.3 0.063 0.169 56.6 0.064 0.175 56.4 0.062 0.163 29.8 0.108 0.308 27.8 0.125 0.338 29.8 0.098 0.255 RD+CD 61.3 0.159 0.314 51.9 0.184 0.360 51.9 0.169 0.324 60.5 0.151 0.410 57.2 0.166 0.431 60.4 0.159 0.384 RD+CD+PUB 61.3 0.244 0.398 61.5 0.260 0.424 61.4 0.210 0.350 90.3 0.165 0.445 88.2 0.189 0.479 90.3 0.126 0.339 RD+CD+PUB+HP 61.3 0.249 0.401 62.6 0.265 0.436 62.6 0.195 0.344 91.9 0.164 0.426 90.1 0.195 0.488 91.9 0.125 0.328 Table 1: Performance of the models on the expert finding and profiling tasks, using different document types and their combinations. %q is the number of topics covered (applies to the expert finding task), %ca is the number of candidates covered (applies to the expert profiling task).",
                "The top and bottom blocks correspond to English and Dutch respectively.",
                "The best scores are in boldface. to how related the other requests are to the original query.",
                "This can be modeled by interpolating between the p(q|ca) and the further supporting evidence from all similar requests q , as follows: p (q|ca) = λp(q|ca) + (1 − λ) X q p(q|q )p(q |ca), (9) where p(q|q ) represents the similarity between the two topics q and q .",
                "To be able to work with similarity methods that are not necessarily probabilities, we set p(q|q ) = w(q,q ) γ , where γ is a normalizing constant, such that γ = P q w(q , q ).",
                "We consider four methods for calculating the similarity score between two topics.",
                "Three approaches are strictly content-based, and establish similarity by examining co-occurrence patterns of topics within the collection, while the last approach exploits the hierarchical structure of topical areas that may be present within an organization (see [7] for further examples of integrating word relationships into language models).",
                "The Kullback-Leibler (KL) divergence metric defined in Eq. 8 provides a measure of how different or similar two probability distributions are.",
                "A topic model is inferred for q and q using the method presented in Section 4.1 to describe the query across the entire vocabulary.",
                "Since a lower KL score means the queries are more similar, we let w(q, q ) = max(KL(θq||·) − KL(θq||θq )).",
                "Pointwise Mutual Information (PMI, [17]) is a measure of association used in information theory to determine the extent of independence between variables.",
                "The dependence between two queries is reflected by the SI(q, q ) score, where scores greater than zero indicate that it is likely that there is a dependence, which we take to mean that the queries are likely to be similar: SI(q, q ) = log p(q, q ) p(q)p(q ) (10) We estimate the probability of a topic p(q) using the number of documents relevant to query q within the collection.",
                "The joint probability p(q, q ) is estimated similarly, by using the concatenation of q and q as a query.",
                "To obtain p(q|q ), we then set w(q, q ) = SI(q, q ) when SI(q, q ) > 0 otherwise w(q, q ) = 0, because we are only interested in including queries that are similar.",
                "The log-likelihood statistic provides another measure of dependence, which is more reliable than the pointwise mutual information measure [17].",
                "Let k1 be the number of co-occurrences of q and q , k2 the number of occurrences of q not co-occurring with q , n1 the total number of occurrences of q , and n2 the total number of topic tokens minus the number of occurrences of q .",
                "Then, let p1 = k1/n1, p2 = k2/n2, and p = (k1 + k2)/(n1 + n2), (q, q ) = 2( (p1, k1, n1) + (p2, k2, n2) − (p, k1, n1) − (p, k2, n2)), where (p, n, k) = k log p + (n − k) log(1 − p).",
                "The higher score indicate that queries are also likely to be similar, thus we set w(q, q ) = (q, q ).",
                "Finally, we also estimate the similarity of two topics based on their distance within the topic hierarchy.",
                "The topic hierarchy is viewed as a directed graph, and for all topic-pairs the shortest path SP(q, q ) is calculated.",
                "We set the similarity score to be the reciprocal of the shortest path: w(q, q ) = 1/SP(q, q ). 7.2 Contextual information Given the hierarchy of an organization, the units to which a person belong are regarded as a context so as to compensate for data sparseness.",
                "We model it as follows: p (q|ca) = 1 − P ou∈OU(ca) λou · p(q|ca) + P ou∈OU(ca) λou · p(q|ou), where OU(ca) is the set of organizational units of which candidate ca is a member of, and p(q|o) expresses the strength of the association between query q and the unit ou.",
                "The latter probability can be estimated using either of the three basic models, by simply replacing ca with ou in the corresponding equations.",
                "An organizational unit is associated with all the documents that its members have authored.",
                "That is, p(d|ou) = maxca∈ou p(d|ca). 7.3 A simple multilingual model For knowledge institutes in Europe, academic or otherwise, a multilingual (or at least bilingual) setting is typical.",
                "The following model builds on a kind of independence assumption: there is no spill-over of expertise/profiles across language boundaries.",
                "While a simplification, this is a sensible first approach.",
                "That is: p (q|ca) =P l∈L λl · p(ql|ca), where L is the set of languages used in the collection, ql is the translation of the query q to language l, and λl is a language specific smoothing parameter, such that P l∈L λl = 1. 8.",
                "ADVANCED MODELS: EVALUATION In this section we present an experimental evaluation of our advanced models.",
                "Expert finding Expert profiling Language Model 1 Model 2 Model 3 Model 1 Model 2 Model 3 %q MAP MRR %q MAP MRR %q MAP MRR %ca MAP MRR %ca MAP MRR %ca MAP MRR English only 97.8 0.237 0.372 98.6 0.280 0.441 98.5 0.166 0.293 100 0.199 0.387 88.7 0.281 0.525 90.9 0.169 0.329 Dutch only 61.3 0.249 0.401 62.6 0.265 0.436 62.6 0.195 0.344 91.9 0.164 0.426 90.1 0.195 0.488 91.9 0.125 0.328 Combination 99.4 0.297 0.444 99.7 0.324 0.491 99.7 0.223 0.388 100 0.241 0.445 92.1 0.313 0.564 93.2 0.224 0.411 Table 3: Performance of the combination of languages on the expert finding and profiling tasks (on candidates).",
                "Best scores for each model are in italic, absolute best scores for the expert finding and profiling tasks are in boldface.",
                "Method Model 1 Model 2 Model 3 MAP MRR MAP MRR MAP MRR English BASELINE 0.296 0.454 0.339 0.509 0.221 0.333 KLDIV 0.291 0.453 0.327 0.503 0.219 0.330 PMI 0.291 0.453 0.337 0.509 0.219 0.331 LL 0.319 0.490 0.360 0.524 0.233 0.368 HDIST 0.299 0.465 0.346 0.537 0.219 0.332 Dutch BASELINE 0.240 0.350 0.271 0.403 0.227 0.389 KLDIV 0.239 0.347 0.253 0.386 0.224 0.385 PMI 0.239 0.350 0.260 0.392 0.227 0.389 LL 0.255 0.372 0.281 0.425 0.231 0.389 HDIST 0.253 0.365 0.271 0.407 0.236 0.402 Method Model 1 Model 2 Model 3 MAP MRR MAP MRR MAP MRR English BASELINE 0.485 0.546 0.499 0.548 0.381 0.416 KLDIV 0.510 0.564 0.513 0.558 0.381 0.416 PMI 0.486 0.546 0.495 0.542 0.407 0.451 LL 0.558 0.589 0.586 0.617 0.408 0.453 HDIST 0.507 0.567 0.512 0.563 0.386 0.420 Dutch BASELINE 0.263 0.313 0.294 0.358 0.262 0.315 KLDIV 0.284 0.336 0.271 0.321 0.261 0.314 PMI 0.265 0.317 0.265 0.316 0.273 0.330 LL 0.312 0.351 0.330 0.377 0.284 0.331 HDIST 0.280 0.327 0.288 0.341 0.266 0.321 Table 4: Performance on the expert finding (top) and profiling (bottom) tasks, using knowledge area similarities.",
                "Runs were evaluated on the main topics set.",
                "Best scores are in boldface. 8.1 Research Questions Our questions follow the refinements presented in the preceding section: Does exploiting the knowledge area similarity improve effectiveness?",
                "Which of the various methods for capturing word relationships is most effective?",
                "Furthermore, is our way of bringing in contextual information useful?",
                "For which tasks?",
                "And finally, is our simple way of combining the monolingual scores sufficient for obtaining significant improvements? 8.2 Experimental setup Given that the self-assessments are also sparse in our collection, in order to be able to measure differences between the various models, we selected a subset of topics, and evaluated (some of the) runs only on this subset.",
                "This set is referred as main topics, and consists of topics that are located at the top level of the topical hierarchy. (A main topic has subtopics, but is not a subtopic of any other topic.)",
                "This main set consists of 132 Dutch and 119 English topics.",
                "The relevance judgements were restricted to the main topic set, but were not expanded with subtopics. 8.3 Exploiting knowledge area similarity Table 4 presents the results.",
                "The four methods used for estimating knowledge-area similarity are KL divergence (KLDIV), PointLang.",
                "Topics Model 1 Model 2 Model 3 MAP MRR MAP MRR MAP MRR Expert finding UK ALL 0.423 0.545 0.654 0.799 0.494 0.629 UK MAIN 0.500 0.621 0.704 0.834 0.587 0.699 NL ALL 0.439 0.560 0.672 0.826 0.480 0.630 NL MAIN 0.440 0.584 0.645 0.816 0.515 0.655 Expert profiling UK ALL 0.240 0.640 0.306 0.778 0.223 0.616 UK MAIN 0.523 0.677 0.519 0.648 0.461 0.587 NL ALL 0.203 0.716 0.254 0.770 0.183 0.627 NL MAIN 0.332 0.576 0.380 0.624 0.332 0.549 Table 5: Evaluating the context models on organizational units. wise mutual information (PMI), log-likelihood (LL), and distance within topic hierarchy (HDIST).",
                "We managed to improve upon the baseline in all cases, but the improvement is more noticeable for the profiling task.",
                "For both tasks, the LL method performed best.",
                "The content-based approaches performed consistently better than HDIST. 8.4 Contextual information A two level hierarchy of organizational units (faculties and institutes) is available in the UvT Expert collection.",
                "The unit a person belongs to is used as a context for that person.",
                "First, we evaluated the models of the organizational units, using all topics (ALL) and only the main topics (MAIN).",
                "An organizational unit is considered to be relevant for a given topic (or vice versa) if at least one member of the unit selected the given topic as an expertise area.",
                "Table 5 reports on the results.",
                "As far as expert finding goes, given a topic, the corresponding organizational unit can be identified with high precision.",
                "However, the expert profiling task shows a different picture: the scores are low, and the task seems hard.",
                "The explanation may be that general concepts (i.e., our main topics) may belong to several organizational units.",
                "Second, we performed another evaluation, where we combined the contextual models with the candidate models (to score candidates again).",
                "Table 6 reports on the results.",
                "We find a positive impact of the context models only for expert finding.",
                "Noticably, for expert finding (and Model 1), it improves over 50% (for English) and over 70% (for Dutch) on MAP.",
                "The poor performance on expert profiling may be due to the fact that context models alone did not perform very well on the profiling task to begin with. 8.5 Multilingual models In this subsection we evaluate the method for combining results across multiple languages that we described in Section 7.3.",
                "In our setting the set of languages consists of English and Dutch: L = {UK, NL}.",
                "The weights on these languages were set to be identical (λUK = λNL = 0.5).",
                "We performed experiments with various λ settings, but did not observe significant differences in performance.",
                "Table 3 reports on the multilingual results, where performance is evaluated on the full topic set.",
                "All three models significantly imLang.",
                "Method Model 1 Model 2 Model 3 MAP MRR MAP MRR MAP MRR Expert finding UK BL 0.296 0.454 0.339 0.509 0.221 0.333 UK CT 0.330 0.491 0.342 0.500 0.228 0.342 NL BL 0.240 0.350 0.271 0.403 0.227 0.389 NL CT 0.251 0.382 0.267 0.410 0.246 0.404 Expert profiling UK BL 0.485 0.546 0.499 0.548 0.381 0.416 UK CT 0.562 0.620 0.508 0.558 0.440 0.486 NL BL 0.263 0.313 0.294 0.358 0.262 0.315 NL CT 0.330 0.384 0.317 0.387 0.294 0.345 Table 6: Performance of the context models (CT) compared to the baseline (BL).",
                "Best scores are in boldface. proved over all measures for both tasks.",
                "The coverage of topics and candidates for the expert finding and profiling tasks, respectively, is close to 100% in all cases.",
                "The relative improvement of the precision scores ranges from 10% to 80%.",
                "These scores demonstrate that despite its simplicity, our method for combining results over multiple languages achieves substantial improvements over the baseline. 9.",
                "CONCLUSIONS In this paper we focused on expertise retrieval (expert finding and profiling) in a new setting of a typical knowledge-intensive organization in which the available data is of high quality, multilingual, and covering a broad range of expertise area.",
                "Typically, the amount of available data in such an organization (e.g., a university, a research institute, or a research lab) is limited when compared to the W3C collection that has mostly been used for the experimental evaluation of expertise retrieval so far.",
                "To examine expertise retrieval in this setting, we introduced (and released) the UvT Expert collection as a representative case of such knowledge intensive organizations.",
                "The new collection reflects the typical properties of knowledge-intensive institutes noted above and also includes several features which may are potentially useful for expertise retrieval, such as topical and organizational structure.",
                "We evaluated how current state-of-the-art models for expert finding and profiling performed in this new setting and then refined these models in order to try and exploit the different characteristics within the data environment (language, topicality, and organizational structure).",
                "We found that current models of expertise retrieval generalize well to this new environment; in addition we found that refining the models to account for the differences results in significant improvements, thus making up for problems caused by data sparseness issues.",
                "Future work includes setting up manual assessments of automatically generated profiles by the employees themselves, especially in cases where the employees have not provided a profile themselves. 10.",
                "ACKNOWLEDGMENTS Krisztian Balog was supported by the Netherlands Organisation for Scientific Research (NWO) under project number 220-80-001.",
                "Maarten de Rijke was also supported by NWO under project numbers 017.001.190, 220-80-001, 264-70-050, 354-20-005, 600.065.120, 612-13-001, 612.000.106, 612.066.302, 612.069.006, 640.001.501, 640.002.501, and by the E.U.",
                "IST programme of the 6th FP for RTD under project MultiMATCH contract IST-033104.",
                "The work of Toine Bogers and Antal van den Bosch was funded by the IOP-MMI-program of SenterNovem / The Dutch Ministry of Economic Affairs, as part of the `A Propos project. 11.",
                "REFERENCES [1] L. Azzopardi.",
                "Incorporating Context in the Language Modeling Framework for ad-hoc Information Retrieval.",
                "PhD thesis, University of Paisley, 2005. [2] K. Balog and M. de Rijke.",
                "Finding similar experts.",
                "In This volume, 2007. [3] K. Balog and M. de Rijke.",
                "Determining expert profiles (with an application to expert finding).",
                "In IJCAI 07: Proc. 20th Intern.",
                "Joint Conf. on Artificial Intelligence, pages 2657-2662, 2007. [4] K. Balog, L. Azzopardi, and M. de Rijke.",
                "Formal models for expert finding in enterprise corpora.",
                "In SIGIR 06: Proc. 29th annual intern.",
                "ACM SIGIR conf. on Research and development in information retrieval, pages 43-50, 2006. [5] I. Becerra-Fernandez.",
                "The role of artificial intelligence technologies in the implementation of people-finder knowledge management systems.",
                "In AAAI Workshop on Bringing Knowledge to Business Processes, March 2000. [6] C. S. Campbell, P. P. Maglio, A. Cozzi, and B. Dom.",
                "Expertise identification using email communications.",
                "In CIKM 03: Proc. twelfth intern. conf. on Information and knowledge management, pages 528531, 2003. [7] G. Cao, J.-Y.",
                "Nie, and J. Bai.",
                "Integrating word relationships into language models.",
                "In SIGIR 05: Proc. 28th annual intern.",
                "ACM SIGIR conf. on Research and development in information retrieval, pages 298-305, 2005. [8] T. M. Cover and J.",
                "A. Thomas.",
                "Elements of Information Theory.",
                "Wiley-Interscience, 1991. [9] N. Craswell, D. Hawking, A. M. Vercoustre, and P. Wilkins.",
                "P@noptic expert: Searching for experts not just for documents.",
                "In Ausweb, 2001. [10] N. Craswell, A. de Vries, and I. Soboroff.",
                "Overview of the TREC2005 Enterprise Track.",
                "In The Fourteenth Text REtrieval Conf.",
                "Proc. (TREC 2005), 2006. [11] T. H. Davenport and L. Prusak.",
                "Working Knowledge: How Organizations Manage What They Know.",
                "Harvard Business School Press, Boston, MA, 1998. [12] T. Dunning.",
                "Accurate methods for the statistics of surprise and coincidence.",
                "Computational Linguistics, 19(1):61-74, 1993. [13] E. Filatova and J. Prager.",
                "Tell me what you do and Ill tell you what you are: Learning occupation-related activities for biographies.",
                "In HLT/EMNLP, 2005. [14] V. Lavrenko and W. B. Croft.",
                "Relevance based language models.",
                "In SIGIR 01: Proc. 24th annual intern.",
                "ACM SIGIR conf. on Research and development in information retrieval, pages 120-127, 2001. [15] V. Lavrenko, M. Choquette, and W. B. Croft.",
                "Cross-lingual relevance models.",
                "In SIGIR 02: Proc. 25th annual intern.",
                "ACM SIGIR conf. on Research and development in information retrieval, pages 175-182, 2002. [16] C. Macdonald and I. Ounis.",
                "Voting for candidates: adapting data fusion techniques for an expert search task.",
                "In CIKM 06: Proc. 15th ACM intern. conf. on Information and knowledge management, pages 387-396, 2006. [17] C. Manning and H. Sch¨utze.",
                "Foundations of Statistical Natural Language Processing.",
                "The MIT Press, 1999. [18] A. Mockus and J. D. Herbsleb.",
                "Expertise browser: a quantitative approach to identifying expertise.",
                "In ICSE 02: Proc. 24th Intern.",
                "Conf. on Software Engineering, pages 503-512, 2002. [19] D. Petkova and W. B. Croft.",
                "Hierarchical language models for expert finding in enterprise corpora.",
                "In Proc.",
                "ICTAI 2006, pages 599-608, 2006. [20] I. Soboroff, A. de Vries, and N. Craswell.",
                "Overview of the TREC 2006 Enterprise Track.",
                "In TREC 2006 Working Notes, 2006. [21] T. Tao, X. Wang, Q. Mei, and C. Zhai.",
                "Language model information retrieval with document expansion.",
                "In HLT-NAACL 2006, 2006. [22] TREC.",
                "Enterprise track, 2005.",
                "URL: http://www.ins.cwi. nl/projects/trec-ent/wiki/. [23] G. van Noord.",
                "TextCat Language Guesser.",
                "URL: http://www. let.rug.nl/˜vannoord/TextCat/. [24] W3C.",
                "The W3C test collection, 2005.",
                "URL: http://research. microsoft.com/users/nickcr/w3c-summary.html."
            ],
            "original_annotated_samples": [
                "At the <br>trec enterprise track</br> [22] the need to study and understand expertise retrieval has been recognized through the introduction of Expert Finding tasks."
            ],
            "translated_annotated_samples": [
                "En la pista de Empresas de TREC [22] se ha reconocido la necesidad de estudiar y comprender la <br>recuperación de expertos</br> a través de la introducción de tareas de Búsqueda de Expertos."
            ],
            "translated_text": "Recuperación de amplia experiencia en entornos de datos dispersos Krisztian Balog ISLA, Universidad de Ámsterdam Kruislaan 403, 1098 SJ Ámsterdam, Países Bajos kbalog@science.uva.nl Toine Bogers ILK, Universidad de Tilburg P.O. Caja 90153, 5000 LE Tilburg, Países Bajos A.M.Bogers@uvt.nl Leif Azzopardi Dept. de Ciencias de la Computación Universidad de Glasgow, Glasgow, G12 8QQ leif@dcs.gla.ac.uk Maarten de Rijke ISLA, Universidad de Ámsterdam Kruislaan 403, 1098 SJ Ámsterdam, Países Bajos mdr@science.uva.nl Antal van den Bosch ILK, Universidad de Tilburg P.O. La recuperación de la experiencia ha sido ampliamente inexplorada en datos distintos a la colección de la W3C. Al mismo tiempo, muchas intranets de universidades y otras organizaciones intensivas en conocimiento ofrecen ejemplos de datos de expertos multilingües relativamente pequeños pero precisos, que abarcan amplias áreas de especialización. Primero presentamos dos tareas principales de recuperación de expertos, junto con un conjunto de enfoques de referencia basados en modelado del lenguaje generativo, con el objetivo de encontrar relaciones de experticia entre temas y personas. Para nuestra evaluación experimental, introducimos (y liberamos) un nuevo conjunto de pruebas basado en un rastreo de un sitio universitario. Usando este conjunto de pruebas, llevamos a cabo dos series de experimentos. El primero tiene como objetivo determinar la efectividad de los métodos de recuperación de conocimientos básicos aplicados al nuevo conjunto de pruebas. El segundo tiene como objetivo evaluar modelos refinados que exploten las características distintivas del nuevo conjunto de pruebas, como la estructura organizativa de la universidad y la estructura jerárquica de los temas en el conjunto de pruebas. Los modelos de recuperación de experiencia se muestran robustos con respecto a entornos más pequeños que la colección de la W3C, y las técnicas actuales parecen ser generalizables a otros contextos. Categorías y Descriptores de Asignaturas H.3 [Almacenamiento y Recuperación de Información]: H.3.1 Análisis de Contenido e Indexación; H.3.3 Búsqueda y Recuperación de Información; H.3.4 Sistemas y Software; H.4 [Aplicaciones de Sistemas de Información]: H.4.2 Tipos de Sistemas; H.4.m Términos Generales Misceláneos Algoritmos, Medición, Rendimiento, Experimentación 1. INTRODUCCIÓN La intranet de una organización proporciona un medio para intercambiar información entre empleados y facilitar colaboraciones entre ellos. Para lograr esto de manera eficiente y efectiva, es necesario proporcionar instalaciones de búsqueda que permitan a los empleados no solo acceder a documentos, sino también identificar a colegas expertos. En la pista de Empresas de TREC [22] se ha reconocido la necesidad de estudiar y comprender la <br>recuperación de expertos</br> a través de la introducción de tareas de Búsqueda de Expertos. El objetivo de la búsqueda de expertos es identificar una lista de personas que tienen conocimientos sobre un tema específico. Esta tarea suele abordarse descubriendo asociaciones entre personas y temas [10]; comúnmente, se asume que la co-ocurrencia del nombre de una persona con temas en el mismo contexto es evidencia de experiencia. Una tarea alternativa, que utiliza la misma idea de asociaciones entre personas y temas, es el perfilado de expertos, donde la tarea consiste en devolver una lista de temas sobre los que una persona tiene conocimiento [3]. El lanzamiento de la tarea de Búsqueda de Expertos en TREC ha generado mucho interés en la recuperación de expertos, con un rápido progreso en términos de modelado, algoritmos y aspectos de evaluación. Sin embargo, casi todo el trabajo de búsqueda o perfilado de expertos realizado ha sido validado experimentalmente utilizando la colección de W3C [24] de la pista empresarial. Si bien esta colección es actualmente la única colección de pruebas disponible públicamente para tareas de recuperación de expertos, solo representa un tipo de intranet. Con solo una colección de pruebas no es posible generalizar conclusiones a otros escenarios realistas. En este artículo nos enfocamos en la recuperación de expertos en un entorno realista que difiere del entorno de la W3C, uno en el que hay cantidades relativamente pequeñas de datos limpios y multilingües disponibles, que abarcan una amplia gama de áreas de expertise, como se puede encontrar en las intranets de universidades y otras organizaciones intensivas en conocimiento. Normalmente, esta configuración incluye varios tipos adicionales de estructuras: estructura temática (por ejemplo, jerarquías de temas utilizadas por la organización), estructura organizativa (facultad, departamento, ...), así como múltiples tipos de documentos (descripciones de investigación y cursos, publicaciones y páginas web académicas). Este entorno es bastante diferente del entorno del W3C de maneras que podrían afectar el rendimiento de las tareas de recuperación de conocimientos especializados. Nos enfocamos en varias preguntas de investigación en este artículo: ¿La cantidad relativamente pequeña de datos disponibles en una intranet afecta la calidad de las asociaciones tema-persona que son fundamentales en los algoritmos de recuperación de expertos? ¿Cómo se desempeñan los algoritmos de vanguardia desarrollados en el conjunto de datos del W3C en el escenario alternativo del tipo descrito anteriormente? Más en general, ¿se pueden aplicar las lecciones de la tarea de Búsqueda de Expertos en TREC a este contexto? ¿Cómo afecta la inclusión o exclusión de diferentes documentos a las tareas de recuperación de expertos? Además, ¿cómo se puede utilizar la estructura temática y organizativa con fines de recuperación? Para responder a nuestras preguntas de investigación, primero presentamos un conjunto de enfoques de línea base, basados en modelado de lenguaje generativo, con el objetivo de encontrar asociaciones entre temas y personas. Esto nos permite formular las tareas de búsqueda de expertos y perfilado de expertos de manera uniforme, y tiene el beneficio adicional de permitirnos entender las relaciones entre las dos tareas. Para nuestra evaluación experimental, introducimos un nuevo conjunto de datos (la Colección de Expertos de UvT) que es representativo del tipo de intranet que describimos anteriormente. Nuestra colección se basa en datos de acceso público, extraídos del sitio web de la Universidad de Tilburg (UvT). Este tipo de datos es particularmente interesante, ya que (1) es limpio, heterogéneo, estructurado y enfocado, pero comprende un número limitado de documentos; (2) contiene información sobre la jerarquía organizativa; (3) es bilingüe (inglés y holandés); y (4) la lista de áreas de especialización de un individuo es proporcionada por los propios empleados. Usando la colección de expertos de UvT, llevamos a cabo dos conjuntos de experimentos. El primero tiene como objetivo determinar la efectividad de los métodos de búsqueda y perfilado de experiencia inicial en este nuevo entorno. Un segundo grupo de experimentos está dirigido a extensiones de los métodos base que aprovechan las características distintivas de la Colección de Expertos de UvT; específicamente, proponemos y evaluamos métodos refinados de búsqueda y perfilado de expertos que incorporan la topicalidad y la estructura organizativa. Además de las preguntas de investigación y el conjunto de datos que aportamos, nuestras principales contribuciones son las siguientes. Los modelos base desarrollados para encontrar expertos funcionan bien en el nuevo conjunto de datos. Mientras que en el entorno del W3C la tarea de encontrar expertos parece ser más difícil que el perfilado, para los datos de UvT ocurre lo contrario. Encontramos que el perfilado en el conjunto de datos UvT es considerablemente más difícil que en el conjunto de datos W3C, lo cual creemos que se debe al gran (pero realista) número de áreas temáticas que utilizamos para el perfilado: alrededor de 1,500 para el conjunto de datos UvT, frente a 50 en el caso de W3C. Teniendo en cuenta la similitud entre los temas puede mejorar significativamente el rendimiento de recuperación. Las medidas de similitud de mejor rendimiento son basadas en contenido, por lo tanto también se pueden aplicar en entornos de la W3C (y otros). Finalmente, demostramos que la estructura organizativa puede ser explotada en forma de un modelo de contexto, mejorando los puntajes de MAP para ciertos modelos hasta en un 70%. El resto de este documento está organizado de la siguiente manera. En la siguiente sección revisamos el trabajo relacionado. Luego, en la Sección 3 proporcionamos descripciones detalladas de las tareas de recuperación de expertos que abordamos en este artículo: la búsqueda de expertos y la creación de perfiles de expertos. En la Sección 4 presentamos nuestros modelos base, cuyo rendimiento luego es evaluado en la Sección 6 utilizando el conjunto de datos UvT que presentamos en la Sección 5. Los modelos avanzados que explotan características específicas de nuestros datos se presentan en la Sección 7 y se evalúan en la Sección 8. Formulamos nuestras conclusiones en la Sección 9.2. Trabajos relacionados Los enfoques iniciales para encontrar expertos a menudo empleaban bases de datos que contenían información sobre las habilidades y conocimientos de cada individuo en la organización [11]. La mayoría de estas herramientas (generalmente llamadas páginas amarillas o sistemas de búsqueda de personas) dependen de que las personas evalúen sus habilidades frente a un conjunto predefinido de palabras clave. Para actualizar perfiles en estos sistemas de forma automática, se necesita de tecnologías inteligentes [5]. Enfoques más recientes utilizan conjuntos de documentos específicos (como correos electrónicos [6] o software [18]) para encontrar experiencia. En contraste con centrarse en tipos de documentos particulares, también hay un interés creciente en el desarrollo de sistemas que indexan y extraen información de documentos publicados en intranets como fuentes de evidencia de experiencia. Un enfoque publicado es el sistema P@noptic [9], que construye una representación de cada persona concatenando todos los documentos asociados con esa persona, similar al Modelo 1 de Balog et al. [4], quienes formalizan y comparan dos métodos. El Modelo 1 de Balog et al. modela directamente el conocimiento de un experto a partir de documentos asociados, mientras que su Modelo 2 primero localiza documentos sobre el tema y luego encuentra a los expertos asociados. En los experimentos reportados, el segundo método tiene un rendimiento significativamente mejor cuando hay suficientes documentos asociados por candidato. La mayoría de los sistemas que participaron en las ediciones de 2005 y 2006 de la tarea de Búsqueda de Expertos en TREC implementaron (variaciones de) uno de estos dos modelos; ver [10, 20]. Macdonald y Ounis [16] proponen un enfoque diferente para clasificar la experiencia de los candidatos con respecto a un tema basado en técnicas de fusión de datos, sin utilizar heurísticas específicas de la colección; encuentran que la aplicación de modelos de ponderación basados en campos mejora la clasificación de los candidatos. Petkova y Croft [19] proponen otro enfoque, basado en una combinación de los Modelos 1 y 2 anteriores, modelando explícitamente los temas. Al abordar otras tareas de recuperación de expertos que también pueden abordarse utilizando asociaciones entre temas y personas, Balog y de Rijke [3] abordaron la tarea de determinar perfiles de expertos temáticos. Si bien sus métodos resultaron ser eficientes en el corpus de la W3C, requieren una cantidad de datos que puede que no esté disponible en la típica organización intensiva en conocimiento. Balog y de Rijke [2] estudian la tarea relacionada de encontrar expertos que sean similares a un pequeño conjunto de expertos dados como entrada. Por cierto, crear un resumen textual de una persona muestra algunas similitudes con la búsqueda de biografías, la cual ha recibido una considerable cantidad de atención recientemente; ver por ejemplo, [13]. Utilizamos modelado de lenguaje generativo para encontrar asociaciones entre temas y personas. En nuestro modelado de búsqueda y perfilado de expertos recopilamos evidencia de experiencia de múltiples fuentes, en una colección heterogénea, e integramos esta información con la co-ocurrencia de nombres de candidatos y términos de consulta: el entorno de modelado de lenguaje nos permite hacer esto de manera transparente. Nuestro modelado avanza en dos pasos. En el primer paso, consideramos tres modelos base, dos tomados de [4] (los Modelos 1 y 2 mencionados anteriormente), y uno una versión refinada de un modelo introducido en [3] (al que nos referimos como Modelo 3 a continuación); este tercer modelo también es similar al modelo descrito por Petkova y Croft [19]. Los modelos que consideramos en nuestra segunda ronda de experimentos son modelos de mezcla similares a los modelos de lenguaje contextual [1] y a los documentos ampliados de Tao et al. [21]; sin embargo, las características que utilizamos para definir nuestras expansiones, incluida la estructura temática y la estructura organizativa, no se han utilizado de esta manera antes. 3. En el escenario de recuperación de expertos que imaginamos, los usuarios que buscan expertos dentro de una organización tienen acceso a una interfaz que combina un cuadro de búsqueda (donde pueden buscar expertos o temas) con estructuras de navegación (de expertos y de temas) que les permite hacer clic hasta llegar a una página de experto (que proporciona el perfil de una persona) o a una página de tema (que proporciona una lista de expertos en el tema). Para alimentar la interfaz anterior, nos enfrentamos a dos tareas de recuperación de expertos, la búsqueda de expertos y el perfilado de expertos, que primero definimos y luego formalizamos utilizando modelos de lenguaje generativos. Para modelar cualquiera de las tareas, la probabilidad de que el tema de la consulta esté asociado a un experto candidato juega un papel clave en las estimaciones finales para la búsqueda y el perfilado. Al utilizar modelos de lenguaje, tanto los candidatos como la consulta se caracterizan por distribuciones de términos en el vocabulario (utilizado en los documentos puestos a disposición por la organización cuyas necesidades de recuperación de experiencia estamos abordando). 3.1 Búsqueda de expertos La búsqueda de expertos implica la tarea de encontrar a la persona adecuada con las habilidades y conocimientos apropiados: ¿Quiénes son los expertos en el tema X?. Por ejemplo, un empleado quiere averiguar quién trabajó en un proyecto en particular para descubrir por qué se tomaron decisiones específicas sin tener que revisar minuciosamente la documentación (si es que la hay). O pueden necesitar un especialista capacitado para consultoría sobre un problema específico. Dentro de una organización, generalmente hay muchos posibles candidatos que podrían ser expertos en un tema dado. Podemos plantear este problema de la siguiente manera: ¿Cuál es la probabilidad de que un candidato ca sea un experto dado el tema de consulta q? Es decir, determinamos p(ca|q) y clasificamos a los candidatos ca de acuerdo con esta probabilidad. Los candidatos con la probabilidad más alta dada la consulta son considerados los expertos más probables en ese tema. El desafío es cómo estimar esta probabilidad con precisión. Dado que es probable que la consulta consista solo en unos pocos términos para describir la experiencia requerida, deberíamos poder obtener una estimación más precisa al invocar el Teorema de Bayes, y estimar: p(ca|q) = p(q|ca)p(ca) p(q) , (1) donde p(ca) es la probabilidad de un candidato y p(q) es la probabilidad de una consulta. Dado que p(q) es una constante, se puede ignorar para fines de clasificación. Por lo tanto, la probabilidad de que un candidato ca sea un experto dado la consulta q es proporcional a la probabilidad de una consulta dada el candidato p(q|ca), ponderada por la creencia a priori p(ca) de que el candidato ca es un experto. p(ca|q) ∝ p(q|ca)p(ca) (2) En este artículo, nuestro enfoque principal se centra en estimar la probabilidad de una consulta dada el candidato p(q|ca), ya que esta probabilidad captura en qué medida el candidato conoce sobre el tema de la consulta. Si bien se asume generalmente que los priors del candidato son uniformes y, por lo tanto, no influirán en la clasificación, se ha demostrado que una elección sensata de priors puede mejorar el rendimiento [20]. Perfilado de expertos. Mientras que la tarea de búsqueda de expertos se centraba en encontrar expertos en un tema específico, la tarea de perfilado de expertos busca responder a una pregunta relacionada: ¿Sobre qué temas tiene conocimiento un candidato? Básicamente, esto da un giro a las preguntas sobre la búsqueda de expertos. El perfilado de un candidato individual implica la identificación de áreas de habilidades y conocimientos en las que tienen experiencia, así como una evaluación del nivel de competencia en cada una de estas áreas. Este es el perfil temático de los candidatos. Generalmente, los perfiles temáticos dentro de las organizaciones consisten en estructuras tabulares que catalogan explícitamente las habilidades y conocimientos de cada individuo en la organización. Sin embargo, esta práctica está limitada por los recursos disponibles para definir, crear, mantener y actualizar estos perfiles con el tiempo. Al centrarnos en métodos automáticos que se basan en la evidencia disponible dentro de los repositorios de documentos de una organización, nuestro objetivo es reducir el esfuerzo humano asociado con el mantenimiento de perfiles temáticos. Un perfil temático de un candidato, entonces, se define como un vector donde cada elemento i del vector corresponde a la experiencia del candidato en un tema dado ki, (es decir, s(ca, ki)). Cada tema ki define un área de conocimiento o habilidad particular que la organización utiliza para definir el perfil temático de los candidatos. Por lo tanto, se asume que se proporciona una lista de temas, {k1, . . . , kn}, donde n es el número de temas predefinidos: perfil(ca) = s(ca, k1), s(ca, k2), . . . , s(ca, kn) . Se necesita contexto y evidencia para ayudar a los usuarios de sistemas de búsqueda de expertos a decidir a quién contactar al buscar experiencia en un área particular. Ejemplos de dicho contexto son: ¿Con quién trabaja ella? ¿Cuáles son sus datos de contacto? ¿Está bien conectada, por si acaso no puede ayudarnos ella misma? ¿Cuál es su rol en la organización? ¿Quién es su superior? Los colaboradores, afiliaciones, etc. son parte del perfil social de los candidatos y pueden servir como antecedentes para interpretar las recomendaciones del sistema. En este documento solo abordamos el problema de determinar perfiles temáticos, dejando el perfilado social para trabajos futuros. Planteamos el problema de cuantificar la competencia de una persona en un área de conocimiento específica de la siguiente manera: ¿Cuál es la probabilidad de que un área de conocimiento (ki) forme parte del perfil de candidatos (expertise)? donde s(ca, ki) está definido por p(ki|ca). Nuestra tarea, entonces, es estimar p(ki|ca), lo cual es equivalente al problema de obtener p(q|ca), donde el tema ki se representa como un tema de consulta q, es decir, una secuencia de palabras clave que representan la experiencia requerida. Tanto la tarea de encontrar expertos como la de perfilar expertos dependen de la estimación precisa de p(q|ca). La única diferencia proviene de la probabilidad previa de que una persona sea experta (p(ca)), la cual puede ser incorporada en la tarea de encontrar expertos. Este antecedente no se aplica a la tarea de perfilado ya que el candidato (individuo) está fijo. 4. MODELOS DE REFERENCIA En esta sección describimos nuestros modelos de referencia para estimar p(q|ca), es decir, las asociaciones entre temas y personas. Tanto la búsqueda de expertos como el perfilado de expertos se reducen a esta estimación. Empleamos tres modelos para calcular esta probabilidad. 4.1 De temas a candidatos Utilizando Modelos de Candidatos: El Modelo 1 [4] define la probabilidad de una consulta dada un candidato (p(q|ca)) utilizando técnicas estándar de modelado de lenguaje, basadas en un modelo de lenguaje multinomial de unigrama. Para cada candidato ca, se infiere un modelo de lenguaje candidato θca de tal manera que la probabilidad de un término dado θca no sea cero para todos los términos, es decir, p(t|θca) > 0. A partir del modelo de candidato, la consulta se genera con la siguiente probabilidad: p(q|θca) = Y t∈q p(t|θca)n(t,q), donde cada término t en la consulta q se muestrea de manera idéntica e independiente, y n(t, q) es el número de veces que t ocurre en q. El modelo de lenguaje candidato se infiere de la siguiente manera: (1) se calcula un modelo empírico p(t|ca); (2) se suaviza con probabilidades de fondo. Usando las asociaciones entre un candidato y un documento, la probabilidad p(t|ca) puede aproximarse por: p(t|ca) = X d p(t|d)p(d|ca), donde p(d|ca) es la probabilidad de que el candidato ca genere un documento de apoyo d, y p(t|d) es la probabilidad de que un término t ocurra en el documento d. Utilizamos la estimación de máxima verosimilitud de un término, es decir, la frecuencia normalizada del término t en el documento d. La fuerza de la asociación entre el documento d y el candidato ca expresada por p(d|ca) refleja el grado en que la experiencia de los candidatos se describe utilizando este documento. La estimación de esta probabilidad se presenta más adelante, en la Sección 4.2. El modelo candidato se construye entonces como una interpolación lineal de p(t|ca) y el modelo de fondo p(t) para asegurar que no haya probabilidades nulas, lo que resulta en la estimación final: p(q|θca) = (4) Y t∈q ( (1 − λ) X d p(t|d)p(d|ca) ! + λp(t) )n(t,q) . El Modelo 1 recopila toda la información de términos de todos los documentos asociados con el candidato, y la utiliza para representar a ese candidato. Este modelo se utiliza para predecir qué tan probable es que un candidato produzca una consulta q. Esto puede interpretarse intuitivamente como la probabilidad de que este candidato hable sobre el tema de la consulta, donde asumimos que esto es indicativo de su experiencia. Usando Modelos de Documentos: El Modelo 2 [4] toma un enfoque diferente. Aquí, el proceso se divide en dos partes. Dado un candidato ca, (1) se selecciona un documento asociado con un candidato con probabilidad p(d|ca), y (2) a partir de este documento se genera una consulta q con probabilidad p(q|d). Entonces se toma la suma sobre todos los documentos para obtener p(q|ca), de modo que: p(q|ca) = Σ d p(q|d)p(d|ca). (5) La probabilidad de una consulta dada un documento se estima inferiendo un modelo de lenguaje del documento θd para cada documento d de manera similar a como se infería el modelo candidato: p(t|θd) = (1 − λ)p(t|d) + λp(t), (6) donde p(t|d) es la probabilidad del término en el documento. La probabilidad de una consulta dada el modelo de documento es: p(q|θd) = Y t∈q p(t|θd)n(t,q) . La estimación final de p(q|ca) se obtiene sustituyendo p(q|d) por p(q|θd) en la Ecuación 5 (ver [4] para más detalles). Conceptualmente, el Modelo 2 difiere del Modelo 1 porque el candidato no está modelado directamente. En cambio, el documento actúa como una variable oculta en el proceso que separa la consulta del candidato. Este proceso es similar a cómo un usuario puede buscar candidatos con un motor de búsqueda estándar: inicialmente encontrando los documentos relevantes y luego viendo quién está asociado con ese documento. Al examinar una serie de documentos, el usuario puede obtener una idea de qué candidatos son más propensos a discutir el tema q. Usando Modelos de Temas: Modelo 3 Introducimos un tercer modelo, Modelo 3. En lugar de intentar modelar el proceso de generación de consultas a través de modelos de candidatos o documentos, representamos la consulta como un modelo de lenguaje de temas y estimamos directamente la probabilidad del candidato p(ca|q). Este enfoque es similar al modelo presentado en [3, 19]. Como en los modelos anteriores, se infiere un modelo de lenguaje, pero esta vez para la consulta. Adaptamos el trabajo de Lavrenko y Croft [14] para estimar un modelo de tema a partir de la consulta. El procedimiento es el siguiente. Dada una colección de documentos y un tema de consulta q, se asume que existe un modelo de tema desconocido θk que asigna probabilidades p(t|θk) a las ocurrencias de términos en los documentos del tema. Tanto la consulta como los documentos son muestras de θk (a diferencia de los enfoques anteriores, donde se asume que una consulta se extrae de un documento específico o un modelo candidato). La tarea principal es estimar p(t|θk), la probabilidad de un término dado el modelo de tema. Dado que la consulta q es muy dispersa y no hay ejemplos de documentos sobre el tema, es necesario aproximar esta distribución. Lavrenko y Croft [14] sugieren una forma razonable de obtener dicha aproximación, asumiendo que p(t|θk) puede aproximarse por la probabilidad del término t dado la consulta q. Entonces podemos estimar p(t|q) utilizando la probabilidad conjunta de observar el término t junto con los términos de la consulta, q1, . . . , qm, y dividiendo por la probabilidad conjunta de los términos de la consulta: p(t|θk) ≈ p(t|q) = p(t, q1, . . . , qm) p(q1, . . . , qm) = p(t, q1, . . . , qm) P t ∈T p(t , q1, . . . , qm) , donde p(q1, . . . , qm) = P t ∈T p(t , q1, . . . , qm), y T es el vocabulario completo de términos. Para estimar la probabilidad conjunta p(t, q1, . . . , qm), seguimos [14, 15] y asumimos que t y q1, . . . , qm son mutuamente independientes, una vez que elegimos una distribución fuente del conjunto de distribuciones fuente subyacentes U. Si elegimos U como un conjunto de modelos de documentos, entonces para construir este conjunto, la consulta q se emitiría contra la colección, y se asume que los primeros n devueltos son relevantes para el tema, y por lo tanto se tratan como muestras del modelo de tema. (Tenga en cuenta que en su lugar podrían usarse modelos candidatos). Con los modelos de documentos formando U, la probabilidad conjunta del término y la consulta se convierte en: p(t, q1, . . . , qm) = X d∈U p(d) ˘ p(t|θd) mY i=1 p(qi|θd) ¯ . (7) Aquí, p(d) denota la distribución previa sobre el conjunto U, que refleja la relevancia del documento para el tema. Suponemos que p(d) es uniforme en todo U. Para clasificar a los candidatos según el modelo de tema definido, utilizamos la métrica de divergencia de Kullback-Leibler (KL, [8]) para medir la diferencia entre los modelos de los candidatos y el modelo de tema: KL(θk||θca) = X t p(t|θk) log p(t|θk) p(t|θca) . (8) Se considera que los candidatos con una divergencia menor respecto al modelo de tema son más propensos a ser expertos en ese tema. El modelo candidato θca está definido en la Ecuación 4. Al utilizar la divergencia de Kullback-Leibler en lugar de la probabilidad de un candidato dado el modelo de tema p(ca|θk), evitamos problemas de normalización. 4.2 Asociaciones documento-candidato Para nuestros modelos, necesitamos poder estimar la probabilidad p(d|ca), que expresa en qué medida un documento d caracteriza al candidato ca. En [4], se presentan dos métodos para estimar esta probabilidad, basados en el número de nombres de personas reconocidos en un documento. Sin embargo, en nuestro entorno de intranet es razonable asumir que los autores de los documentos pueden ser identificados de manera inequívoca (por ejemplo, como el autor de un artículo, el profesor asignado a un curso, el propietario de una página web, etc.). Por lo tanto, establecemos p(d|ca) en 1 si el candidato ca es el autor del documento d, de lo contrario la probabilidad es 0. En la Sección 6 describimos cómo se puede determinar la autoría en diferentes tipos de documentos dentro de la colección. 5. La colección de expertos de UvT utilizada en los experimentos de este artículo se ajusta al escenario descrito en la Sección 3. La colección se basa en el sistema Webwijs (Webwise) desarrollado en la Universidad de Tilburg (UvT) en los Países Bajos. Webwijs (http://www.uvt.nl/webwijs/) es una base de datos de acceso público de empleados de la UvT que están involucrados en investigación o enseñanza; actualmente, Webwijs contiene información sobre 1168 expertos, cada uno de los cuales tiene una página con información de contacto y, si está disponible por parte del experto, una descripción de investigación y lista de publicaciones. Además, cada experto puede seleccionar áreas de especialización de una lista de 1491 temas y se le anima a sugerir nuevos temas que necesitan ser aprobados por el editor de Webwijs. Cada tema tiene una página separada que muestra a todos los expertos asociados con ese tema y, si está disponible, una lista de temas relacionados. Webwijs está disponible en holandés e inglés, y esta bilingüidad se ha preservado en la colección. Cada página de Webwijs en holandés tiene una traducción al inglés. No todos los temas en holandés tienen una traducción al inglés, pero lo contrario es cierto: los 981 temas en inglés tienen un equivalente en holandés. Aproximadamente el 42% de los expertos imparten cursos en la Universidad de Tilburg; estos cursos también fueron rastreados e incluidos en el perfil. Además, aproximadamente el 27% de los expertos enlazan su página académica desde su página de Webwijs. Estas páginas de inicio fueron rastreadas y añadidas a la colección. (Esto significa que si los expertos colocaron las versiones completas de sus publicaciones en sus páginas de inicio académicas, estas también estaban disponibles para su indexación). También obtuvimos 1880 versiones completas de publicaciones del repositorio institucional de UvT y el número de expertos holandeses en inglés es de 1168, con 743 expertos con ≥ 1 tema. El número de temas es de 1491 y 981, y el número de pares experto-tema es de 4318 y 3251, con un promedio de temas por experto de 5.8 y 5.9 respectivamente. El máximo número de temas por experto es de 60 (1) y 35 (1), y el mínimo es de 1 (74) y 1 (106) respectivamente. El promedio de expertos por tema es de 2.9 y 3.3, con un máximo de 30 (1) y un mínimo de 1 (615) y 1 (346) respectivamente. Hay 318 expertos con HP y CD, con un promedio de 3.5 CDs por experto docente. Además, hay 329 expertos con RD y 734 con PUB, con un promedio de 27.0 publicaciones por experto, 25.2 citas por experto y 1.8 publicaciones completas por experto. Tabla 2: Estadísticas descriptivas de las versiones holandesas e inglesas de la colección de expertos de UvT. Ejecutamos el identificador de idioma TextCat [23] para clasificar el idioma de las páginas de inicio y las publicaciones de texto completo. Nos limitamos a las páginas en las que el clasificador tenía confianza en el idioma utilizado en la página. Esto resultó en cuatro tipos de documentos: descripciones de investigación (RD), descripciones de cursos (CD), publicaciones (PUB; versiones de texto completo y solo de cita) y páginas web académicas (HP). Todo fue agrupado en la colección UvT Expert que está disponible en http://ilk.uvt.nl/uvt-expert-collection/. La colección UvT Expert fue extraída de un entorno organizativo diferente a la colección de la W3C y difiere de ella en varios aspectos. El entorno de UvT es uno con cantidades relativamente pequeñas de datos multilingües. Las asociaciones entre los documentos y los autores son claras y los datos están estructurados y limpios. La colección abarca una amplia gama de áreas de especialización, como suele encontrarse en intranets de universidades y otros institutos intensivos en conocimiento. Además, nuestro entorno universitario cuenta con varios tipos de estructura (temática y organizativa), así como múltiples tipos de documentos. Otra diferencia importante entre los dos conjuntos de datos es que las áreas de especialización en la colección de Expertos de UvT son autoseleccionadas en lugar de basarse en la membresía de un grupo o asignaciones de otros. El tamaño es otra dimensión en la que difieren las colecciones de expertos de W3C y UvT: esta última es la más pequeña de las dos. También son realistas las grandes diferencias en la cantidad de información disponible para cada experto. Utilizar Webwijs es voluntario; 425 expertos holandeses no seleccionaron ningún tema en absoluto. Esto nos deja con 743 perfiles de expertos holandeses y 727 perfiles de expertos en inglés utilizables. La Tabla 2 proporciona estadísticas descriptivas para la colección de Expertos de UvT. Las universidades tienden a tener una estructura jerárquica que va desde el nivel de la facultad, pasando por los departamentos, grupos de investigación, hasta llegar a los investigadores individuales. En la colección de Expertos de UvT tenemos información sobre las afiliaciones de los investigadores con facultades e institutos, lo que nos proporciona una jerarquía organizativa de dos niveles. La Universidad de Tilburg tiene 22 unidades organizativas a nivel de facultad (incluida la oficina universitaria y varios institutos de investigación) y 71 departamentos, lo que equivale a 3.2 departamentos por facultad. En cuanto a la jerarquía temática utilizada por Webwijs, 131 de los 1491 temas son nodos principales en la jerarquía. Esta jerarquía tiene una longitud promedio de cadena de temas de 2.65 y una longitud máxima de 7 temas. 6. EVALUACIÓN A continuación, evaluamos los modelos de la Sección 4 para la búsqueda y perfilado de expertos en la colección de Expertos de UvT. Detallamos nuestras preguntas de investigación y configuración experimental, y luego presentamos nuestros resultados. 6.1 Preguntas de investigación Abordamos las siguientes preguntas de investigación. Tanto la búsqueda de expertos como el perfilado dependen de las estimaciones de p(q|ca). La pregunta es cómo se comparan los modelos en las diferentes tareas, y en el contexto de la colección de expertos de UvT. En [4], el Modelo 2 superó al Modelo 1 en la colección de la W3C. ¿Cómo se comparan en nuestro conjunto de datos? ¿Y cómo se compara el Modelo 3 con el Modelo 1? ¿Qué hay de las diferencias de rendimiento entre los dos idiomas en nuestra colección de pruebas? 6.2 Configuración Experimental La salida de nuestros modelos fue evaluada frente a las etiquetas de temas autoasignadas, que fueron tratadas como juicios de relevancia. Los resultados fueron evaluados por separado para inglés y holandés. Para inglés solo usamos temas para los cuales había traducción al neerlandés; para neerlandés se consideraron todos los temas. Los resultados se promediaron para las consultas en la intersección de las evaluaciones de relevancia y los resultados; las consultas faltantes no contribuyen con un valor de 0 a las puntuaciones. Utilizamos medidas estándar de recuperación de información, como la Precisión Media Promedio (MAP) y la Reciprocidad Media Promedio (MRR). También informamos el porcentaje de temas (%q) y candidatos (%ca) cubiertos, respectivamente, para las tareas de búsqueda y perfilado de expertos. 6.3 Resultados La Tabla 1 muestra el rendimiento de los Modelos 1, 2 y 3 en las tareas de búsqueda y perfilado de expertos. Las filas de la tabla corresponden a los diversos tipos de documentos (RD, CD, PUB y HP) y a sus combinaciones. RD+CD+PUB+HP es equivalente a la colección completa y será referida como la LÍNEA BASE de nuestros experimentos. Al observar la Tabla 1, vemos que el Modelo 2 tiene el mejor rendimiento en general. Sin embargo, cuando los datos están limpios y muy enfocados (RD), el Modelo 3 lo supera en varios casos. El modelo 1 tiene la mejor cobertura de candidatos (%ca) y temas (%q). Los diversos tipos de documentos difieren en sus características y en cómo mejoran las tareas de búsqueda y perfilado. El perfilado de expertos se beneficia mucho de los datos limpios presentes en los tipos de documentos RD y CD, mientras que las publicaciones contribuyen principalmente a la tarea de encontrar expertos. Agregar las páginas de inicio no resulta ser particularmente útil. Cuando comparamos los resultados entre idiomas, encontramos que la cobertura de los temas en inglés (%q) es mayor que la de los temas en neerlandés para la búsqueda de expertos. Además de eso, las puntuaciones caen en el mismo rango para ambos idiomas. Para la tarea de perfilado, la cobertura de los candidatos (%ca) es muy similar para ambos idiomas. Sin embargo, el rendimiento es considerablemente mejor para los temas en inglés. Si bien es difícil comparar las puntuaciones entre colecciones, concluimos con una breve comparación de las puntuaciones absolutas en la Tabla 1 con las reportadas en [3, 4] en el conjunto de pruebas de W3C (edición de 2005). Para el experto que busca, los puntajes MAP para el Modelo 2 reportados aquí son aproximadamente un 50% más altos que las cifras correspondientes en [4], mientras que nuestros puntajes MRR están ligeramente por debajo de los de [4]. Para el perfilado de expertos, las diferencias son mucho más dramáticas: las puntuaciones MAP para el Modelo 2 reportadas aquí son aproximadamente un 50% más bajas que las puntuaciones en [3], mientras que las puntuaciones MRR (mejores) son aproximadamente iguales a las de [3]. La causa de estas diferencias parece residir en el número de áreas de conocimiento consideradas aquí, aproximadamente 30 veces más que en el entorno del W3C. MODELOS AVANZADOS Ahora que hemos desarrollado y evaluado técnicas básicas de modelado de lenguaje para la recuperación de expertos, nos enfocamos en modelos refinados que explotan características especiales de nuestra colección de pruebas. 7.1 Explotando la similitud en el área de conocimiento Una forma de mejorar la puntuación de una consulta dada un candidato es considerar qué otras solicitudes satisfaría el candidato y usarlas como evidencia adicional para respaldar la consulta original, proporcionalmente. Búsqueda de expertos Perfilado de expertos Tipos de documentos Modelo 1 Modelo 2 Modelo 3 Modelo 1 Modelo 2 Modelo 3 %q MAP MRR %q MAP MRR %q MAP MRR %ca MAP MRR %ca MAP MRR %ca MAP MRR Inglés RD 97.8 0.126 0.269 83.5 0.144 0.311 83.3 0.129 0.271 100 0.089 0.189 39.3 0.232 0.465 41.1 0.166 0.337 CD 97.8 0.118 0.227 91.7 0.123 0.248 91.7 0.118 0.226 32.8 0.188 0.381 32.4 0.195 0.385 32.7 0.203 0.370 PUB 97.8 0.200 0.330 98.0 0.216 0.372 98.0 0.145 0.257 78.9 0.167 0.364 74.5 0.212 0.442 78.9 0.135 0.299 HP 97.8 0.081 0.186 97.4 0.071 0.168 97.2 0.062 0.149 31.2 0.150 0.299 28.8 0.185 0.335 30.1 0.136 0.287 RD+CD 97.8 0.188 0.352 92.9 0.193 0.360 92.9 0.150 0.273 100 0.145 0.286 61.3 0.251 0.477 63.2 0.217 0.416 RD+CD+PUB 97.8 0.235 0.373 98.1 0.277 0.439 98.1 0.178 0.305 100 0.196 0.380 87.2 0.280 0.533 89.5 0.170 0.344 RD+CD+PUB+HP 97.8 0.237 0.372 98.6 0.280 0.441 98.5 0.166 0.293 100 0.199 0.387 88.7 0.281 0.525 90.9 0.169 0.329 Holandés RD 61.3 0.094 0.229 38.4 0.137 0.336 38.3 0.127 0.295 38.0 0.127 0.386 34.1 0.138 0.420 38.0 0.105 0.327 CD 61.3 0.107 0.212 49.7 0.128 0.256 49.7 0.136 0.261 32.5 0.151 0.389 31.8 0.158 0.396 32.5 0.170 0.380 PUB 61.3 0.193 0.319 59.5 0.218 0.368 59.4 0.173 0.291 78.8 0.126 0.364 76.0 0.150 0.424 78.8 0.103 0.294 HP 61.3 0.063 0.169 56.6 0.064 0.175 56.4 0.062 0.163 29.8 0.108 0.308 27.8 0.125 0.338 29.8 0.098 0.255 RD+CD 61.3 0.159 0.314 51.9 0.184 0.360 51.9 0.169 0.324 60.5 0.151 0.410 57.2 0.166 0.431 60.4 0.159 0.384 RD+CD+PUB 61.3 0.244 0.398 61.5 0.260 0.424 61.4 0.210 0.350 90.3 0.165 0.445 88.2 0.189 0.479 90.3 0.126 0.339 RD+CD+PUB+HP 61.3 0.249 0.401 62.6 0.265 0.436 62.6 0.195 0.344 91.9 0.164 0.426 90.1 0.195 0.488 91.9 0.125 0.328 Tabla 1: Rendimiento de los modelos en las tareas de búsqueda y perfilado de expertos, utilizando diferentes tipos de documentos y sus combinaciones. %q es el número de temas cubiertos (se aplica a la tarea de búsqueda de expertos), %ca es el número de candidatos cubiertos (se aplica a la tarea de perfilado de expertos). Los bloques superior e inferior corresponden al inglés y al holandés respectivamente. Las mejores puntuaciones están en negrita. Esto se puede modelar interpolando entre el p(q|ca) y la evidencia adicional de todos los pedidos similares q, de la siguiente manera: p(q|ca) = λp(q|ca) + (1 − λ) X q p(q|q )p(q |ca), (9) donde p(q|q ) representa la similitud entre los dos temas q y q. Para poder trabajar con métodos de similitud que no son necesariamente probabilidades, establecemos p(q|q ) = w(q,q ) γ , donde γ es una constante de normalización, de modo que γ = P q w(q , q ). Consideramos cuatro métodos para calcular la puntuación de similitud entre dos temas. Tres enfoques se basan estrictamente en el contenido y establecen similitudes examinando patrones de co-ocurrencia de temas dentro de la colección, mientras que el último enfoque explota la estructura jerárquica de áreas temáticas que pueden estar presentes dentro de una organización (ver [7] para más ejemplos de integrar relaciones entre palabras en modelos de lenguaje). La métrica de divergencia Kullback-Leibler (KL) definida en la ecuación 8 proporciona una medida de cuán diferentes o similares son dos distribuciones de probabilidad. Se infiere un modelo de tema para q y q utilizando el método presentado en la Sección 4.1 para describir la consulta en todo el vocabulario. Dado que un puntaje KL más bajo significa que las consultas son más similares, permitimos que w(q, q ) = max(KL(θq||·) − KL(θq||θq )). El Información Mutua Puntual (PMI, [17]) es una medida de asociación utilizada en teoría de la información para determinar el grado de independencia entre variables. La dependencia entre dos consultas se refleja en la puntuación SI(q, q), donde las puntuaciones mayores que cero indican que es probable que exista una dependencia, lo que interpretamos como que las consultas son probablemente similares: SI(q, q) = log p(q, q) / p(q)p(q). Estimamos la probabilidad de un tema p(q) utilizando el número de documentos relevantes para la consulta q dentro de la colección. La probabilidad conjunta p(q, q) se estima de manera similar, utilizando la concatenación de q y q como una consulta. Para obtener p(q|q), luego establecemos w(q, q) = SI(q, q) cuando SI(q, q) > 0, de lo contrario w(q, q) = 0, ya que solo estamos interesados en incluir consultas que sean similares. La estadística de log-verosimilitud proporciona otra medida de dependencia, que es más confiable que la medida de información mutua puntual [17]. Sea k1 el número de co-ocurrencias de q y q, k2 el número de ocurrencias de q que no co-ocurren con q, n1 el número total de ocurrencias de q, y n2 el número total de tokens de tema menos el número de ocurrencias de q. Entonces, sea p1 = k1/n1, p2 = k2/n2 y p = (k1 + k2)/(n1 + n2), (q, q ) = 2( (p1, k1, n1) + (p2, k2, n2) − (p, k1, n1) − (p, k2, n2)), donde (p, n, k) = k log p + (n − k) log(1 − p). El puntaje más alto indica que las consultas también son probablemente similares, por lo tanto, establecemos w(q, q ) = (q, q ). Finalmente, también estimamos la similitud de dos temas basándonos en su distancia dentro de la jerarquía de temas. La jerarquía de temas se visualiza como un grafo dirigido, y para todos los pares de temas se calcula el camino más corto SP(q, q). Establecimos el puntaje de similitud como el recíproco del camino más corto: w(q, q ) = 1/SP(q, q ). 7.2 Información contextual Dada la jerarquía de una organización, las unidades a las que pertenece una persona se consideran como un contexto para compensar la escasez de datos. Lo modelamos de la siguiente manera: p (q|ca) = 1 − P ou∈OU(ca) λou · p(q|ca) + P ou∈OU(ca) λou · p(q|ou), donde OU(ca) es el conjunto de unidades organizativas de las cuales el candidato ca es miembro, y p(q|o) expresa la fuerza de la asociación entre la consulta q y la unidad ou. La última probabilidad se puede estimar utilizando cualquiera de los tres modelos básicos, simplemente reemplazando ca con ou en las ecuaciones correspondientes. Una unidad organizativa está asociada con todos los documentos que sus miembros han escrito. Es decir, p(d|ou) = maxca∈ou p(d|ca). 7.3 Un modelo multilingüe simple Para institutos de conocimiento en Europa, ya sea académicos u otros, un entorno multilingüe (o al menos bilingüe) es típico. El siguiente modelo se basa en un tipo de suposición de independencia: no hay transferencia de conocimientos/perfiles a través de las barreras del idioma. Si bien es una simplificación, esta es una aproximación sensata en primer lugar. Eso es: p (q|ca) =P l∈L λl · p(ql|ca), donde L es el conjunto de idiomas utilizados en la colección, ql es la traducción de la consulta q al idioma l, y λl es un parámetro de suavizado específico del idioma, tal que P l∈L λl = 1. 8. MODELOS AVANZADOS: EVALUACIÓN En esta sección presentamos una evaluación experimental de nuestros modelos avanzados. Búsqueda de expertos Perfilado de expertos Modelo de lenguaje 1 Modelo 2 Modelo 3 Modelo 1 Modelo 2 Modelo 3 %q MAP MRR %q MAP MRR %q MAP MRR %ca MAP MRR %ca MAP MRR %ca MAP MRR Solo inglés 97.8 0.237 0.372 98.6 0.280 0.441 98.5 0.166 0.293 100 0.199 0.387 88.7 0.281 0.525 90.9 0.169 0.329 Solo holandés 61.3 0.249 0.401 62.6 0.265 0.436 62.6 0.195 0.344 91.9 0.164 0.426 90.1 0.195 0.488 91.9 0.125 0.328 Combinación 99.4 0.297 0.444 99.7 0.324 0.491 99.7 0.223 0.388 100 0.241 0.445 92.1 0.313 0.564 93.2 0.224 0.411 Tabla 3: Rendimiento de la combinación de idiomas en las tareas de búsqueda y perfilado de expertos (en candidatos). Los mejores puntajes para cada modelo están en cursiva, los mejores puntajes absolutos para las tareas de búsqueda y perfilado de expertos están en negrita. Tabla 4: Rendimiento en las tareas de búsqueda de expertos (arriba) y perfilado (abajo), utilizando similitudes de áreas de conocimiento. Las carreras fueron evaluadas en los temas principales establecidos. Las mejores puntuaciones están en negrita. 8.1 Preguntas de investigación Nuestras preguntas siguen los refinamientos presentados en la sección anterior: ¿Explotar la similitud en el área de conocimiento mejora la efectividad? ¿Cuál de los diversos métodos para capturar las relaciones entre palabras es más efectivo? Además, ¿es útil nuestra forma de incorporar información contextual? ¿Para qué tareas? Y finalmente, ¿es nuestra forma simple de combinar las puntuaciones monolingües suficiente para obtener mejoras significativas? 8.2 Configuración experimental Dado que las autoevaluaciones también son escasas en nuestra colección, para poder medir las diferencias entre los diversos modelos, seleccionamos un subconjunto de temas y evaluamos (algunas de las) ejecuciones solo en este subconjunto. Este conjunto se denomina como temas principales, y consiste en temas que se encuentran en el nivel superior de la jerarquía temática. (Un tema principal tiene subtemas, pero no es un subtema de ningún otro tema). Este conjunto principal consiste en 132 temas en holandés y 119 en inglés. Los juicios de relevancia se limitaron al conjunto de temas principales, pero no se ampliaron con subtemas. 8.3 Explotando la similitud de áreas de conocimiento. La Tabla 4 presenta los resultados. Los cuatro métodos utilizados para estimar la similitud de áreas de conocimiento son la divergencia de KL (KLDIV) y PointLang. Modelo 1 Modelo 2 Modelo 3 MAP MRR MAP MRR MAP MRR Búsqueda de expertos UK TODOS 0.423 0.545 0.654 0.799 0.494 0.629 UK PRINCIPAL 0.500 0.621 0.704 0.834 0.587 0.699 NL TODOS 0.439 0.560 0.672 0.826 0.480 0.630 NL PRINCIPAL 0.440 0.584 0.645 0.816 0.515 0.655 Perfilado de expertos UK TODOS 0.240 0.640 0.306 0.778 0.223 0.616 UK PRINCIPAL 0.523 0.677 0.519 0.648 0.461 0.587 NL TODOS 0.203 0.716 0.254 0.770 0.183 0.627 NL PRINCIPAL 0.332 0.576 0.380 0.624 0.332 0.549 Tabla 5: Evaluación de los modelos de contexto en unidades organizativas. información mutua ponderada (PMI), log-verosimilitud (LL) y distancia dentro de la jerarquía de temas (HDIST). Logramos mejorar el resultado base en todos los casos, pero la mejora es más notable para la tarea de perfilado. Para ambas tareas, el método LL tuvo el mejor rendimiento. Los enfoques basados en el contenido tuvieron un rendimiento consistentemente mejor que HDIST. 8.4 Información contextual Una jerarquía de dos niveles de unidades organizativas (facultades e institutos) está disponible en la colección de Expertos de UvT. La unidad a la que pertenece una persona se utiliza como contexto para esa persona. Primero, evaluamos los modelos de las unidades organizativas, utilizando todos los temas (TODOS) y solo los temas principales (PRINCIPALES). Una unidad organizativa se considera relevante para un tema dado (o viceversa) si al menos un miembro de la unidad seleccionó el tema dado como área de especialización. La Tabla 5 informa sobre los resultados. En lo que respecta a la búsqueda de expertos, dado un tema, la unidad organizativa correspondiente puede ser identificada con alta precisión. Sin embargo, la tarea de perfilado de expertos muestra una imagen diferente: las puntuaciones son bajas y la tarea parece difícil. La explicación puede ser que los conceptos generales (es decir, nuestros temas principales) pueden pertenecer a varias unidades organizativas. Segundo, realizamos otra evaluación, donde combinamos los modelos contextuales con los modelos candidatos (para puntuar a los candidatos nuevamente). La tabla 6 informa sobre los resultados. Encontramos un impacto positivo de los modelos de contexto solo para la búsqueda de expertos. Notablemente, para la búsqueda de expertos (y Modelo 1), mejora más del 50% (para inglés) y más del 70% (para holandés) en el MAP. El bajo rendimiento en la creación de perfiles de expertos puede deberse al hecho de que los modelos de contexto por sí solos no tuvieron un buen desempeño en la tarea de creación de perfiles desde el principio. 8.5 Modelos multilingües En esta subsección evaluamos el método para combinar resultados en múltiples idiomas que describimos en la Sección 7.3. En nuestro entorno, el conjunto de idiomas consiste en inglés y holandés: L = {UK, NL}. Los pesos de estos idiomas se establecieron para ser idénticos (λUK = λNL = 0.5). Realizamos experimentos con diferentes configuraciones de λ, pero no observamos diferencias significativas en el rendimiento. La Tabla 3 informa sobre los resultados multilingües, donde el rendimiento se evalúa en el conjunto completo de temas. Los tres modelos tienen una diferencia significativa. Tabla 6: Rendimiento de los modelos de contexto (CT) en comparación con la línea base (BL). Las mejores puntuaciones están en negrita, demostradas en todas las medidas para ambas tareas. La cobertura de temas y candidatos para las tareas de búsqueda y perfilado de expertos, respectivamente, es cercana al 100% en todos los casos. La mejora relativa de las puntuaciones de precisión varía del 10% al 80%. Estas puntuaciones demuestran que, a pesar de su simplicidad, nuestro método para combinar resultados en varios idiomas logra mejoras sustanciales sobre el punto de referencia. CONCLUSIONES En este artículo nos enfocamos en la recuperación de expertos (búsqueda y perfilado de expertos) en un nuevo entorno de una organización típica intensiva en conocimiento en la que los datos disponibles son de alta calidad, multilingües y abarcan una amplia gama de áreas de expertise. Normalmente, la cantidad de datos disponibles en una organización como una universidad, un instituto de investigación o un laboratorio de investigación es limitada en comparación con la colección de la W3C que ha sido utilizada principalmente para la evaluación experimental de la recuperación de la experiencia hasta ahora. Para examinar la recuperación de la experiencia en este entorno, presentamos (y lanzamos) la colección de Expertos de UvT como un caso representativo de organizaciones intensivas en conocimiento. La nueva colección refleja las propiedades típicas de los institutos intensivos en conocimiento mencionados anteriormente e incluye también varias características que pueden ser potencialmente útiles para la recuperación de expertos, como la estructura temática y organizativa. Evaluamos cómo los modelos de vanguardia para la búsqueda y perfilado de expertos se desempeñaron en este nuevo entorno y luego refinamos estos modelos para intentar explotar las diferentes características dentro del entorno de datos (idioma, actualidad y estructura organizativa). Encontramos que los modelos actuales de recuperación de experiencia se generalizan bien a este nuevo entorno; además, descubrimos que refinar los modelos para tener en cuenta las diferencias resulta en mejoras significativas, compensando así los problemas causados por la escasez de datos. El trabajo futuro incluye establecer evaluaciones manuales de perfiles generados automáticamente por los propios empleados, especialmente en casos en los que los empleados no han proporcionado un perfil ellos mismos. 10. AGRADECIMIENTOS Krisztian Balog fue apoyado por la Organización Neerlandesa para la Investigación Científica (NWO) bajo el número de proyecto 220-80-001. Maarten de Rijke también recibió apoyo de NWO bajo los números de proyecto 017.001.190, 220-80-001, 264-70-050, 354-20-005, 600.065.120, 612-13-001, 612.000.106, 612.066.302, 612.069.006, 640.001.501, 640.002.501, y de la Unión Europea. Programa IST del 6º Programa Marco para la I+D en virtud del contrato del proyecto MultiMATCH IST-033104. El trabajo de Toine Bogers y Antal van den Bosch fue financiado por el programa IOP-MMI de SenterNovem / Ministerio de Asuntos Económicos de los Países Bajos, como parte del proyecto 'A Propos'. 11. REFERENCIAS [1] L. Azzopardi. Incorporando contexto en el marco de modelado del lenguaje para la recuperación de información ad-hoc. Tesis doctoral, Universidad de Paisley, 2005. [2] K. Balog y M. de Rijke. Encontrando expertos similares. En este volumen, 2007. [3] K. Balog y M. de Rijke. Determinación de perfiles de expertos (con una aplicación a la búsqueda de expertos). En IJCAI 07: Proc. 20th Intern. Conferencia Conjunta sobre Inteligencia Artificial, páginas 2657-2662, 2007. [4] K. Balog, L. Azzopardi y M. de Rijke. Modelos formales para la búsqueda de expertos en corporaciones empresariales. En SIGIR 06: Actas de la 29ª conferencia internacional anual. Conferencia ACM SIGIR sobre Investigación y desarrollo en recuperación de información, páginas 43-50, 2006. [5] I. Becerra-Fernández. El papel de las tecnologías de inteligencia artificial en la implementación de sistemas de gestión del conocimiento de localización de personas. En el taller de AAAI sobre la incorporación de conocimiento en los procesos de negocio, marzo de 2000. [6] C. S. Campbell, P. P. Maglio, A. Cozzi y B. Dom. Identificación de la experiencia utilizando comunicaciones por correo electrónico. En CIKM 03: Actas de la duodécima conferencia internacional sobre gestión de la información y el conocimiento, páginas 528-531, 2003. [7] G. Cao, J.-Y. Nie, y J. Bai. Integrando relaciones entre palabras en modelos de lenguaje. En SIGIR 05: Actas de la 28ª conferencia internacional anual. Conferencia ACM SIGIR sobre Investigación y desarrollo en recuperación de información, páginas 298-305, 2005. [8] T. M. Cover y J. A. Thomas. Elementos de la teoría de la información. Wiley-Interscience, 1991. [9] N. Craswell, D. Hawking, A. M. Vercoustre y P. Wilkins. Experto en P@noptic: Buscando expertos no solo documentos. En Ausweb, 2001. [10] N. Craswell, A. de Vries e I. Soboroff. Resumen de la pista empresarial TREC2005. En la Decimocuarta Conferencia de Recuperación de Información. Proc. (TREC 2005), 2006. [11] T. H. Davenport y L. Prusak. Conocimiento en Acción: Cómo las Organizaciones Gestionan lo que Saben. Harvard Business School Press, Boston, MA, 1998. [12] T. Dunning.\nHarvard Business School Press, Boston, MA, 1998. [12] T. Dunning. Métodos precisos para la estadística de sorpresa y coincidencia. Lingüística Computacional, 19(1):61-74, 1993. [13] E. Filatova y J. Prager. Dime qué haces y te diré quién eres: Aprendiendo actividades relacionadas con la ocupación para biografías. En HLT/EMNLP, 2005. [14] V. Lavrenko y W. B. Croft. Modelos de lenguaje basados en relevancia. En SIGIR 01: Actas de la 24ª conferencia internacional anual. Conferencia ACM SIGIR sobre Investigación y Desarrollo en Recuperación de Información, páginas 120-127, 2001. [15] V. Lavrenko, M. Choquette y W. B. Croft. Modelos de relevancia multilingües. En SIGIR 02: Actas de la 25ª conferencia internacional anual. Conferencia ACM SIGIR sobre Investigación y Desarrollo en Recuperación de Información, páginas 175-182, 2002. [16] C. Macdonald e I. Ounis. Votación de candidatos: adaptando técnicas de fusión de datos para una tarea de búsqueda de expertos. En CIKM 06: Actas de la 15ª conferencia internacional de ACM sobre gestión de la información y el conocimiento, páginas 387-396, 2006. [17] C. Manning y H. Schütze. Fundamentos del Procesamiento del Lenguaje Natural Estadístico. El MIT Press, 1999. [18] A. Mockus y J. D. Herbsleb. Explorador de experticia: un enfoque cuantitativo para identificar la experticia. En ICSE 02: Proc. 24th Intern. Conf. en Ingeniería de Software, páginas 503-512, 2002. [19] D. Petkova y W. B. Croft. Modelos de lenguaje jerárquicos para la búsqueda de expertos en corporaciones empresariales. En Proc. ICTAI 2006, páginas 599-608, 2006. [20] I. Soboroff, A. de Vries y N. Craswell. Resumen de la pista empresarial TREC 2006. En las Notas de Trabajo de TREC 2006, 2006. [21] T. Tao, X. Wang, Q. Mei y C. Zhai. Recuperación de información del modelo de lenguaje con expansión de documentos. En HLT-NAACL 2006, 2006. [22] TREC. Ruta empresarial, 2005. URL: http://www.ins.cwi.nl/projects/trec-ent/wiki/. [23] G. van Noord.\nURL: http://www.ins.cwi.nl/projects/trec-ent/wiki/. [23] G. van Noord. Adivinador de idioma TextCat. URL: http://www.let.rug.nl/˜vannoord/TextCat/. [24] W3C. La colección de pruebas del W3C, 2005. URL: http://research.microsoft.com/users/nickcr/w3c-summary.html. ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "expert finding task": {
            "translated_key": "",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Broad Expertise Retrieval in Sparse Data Environments Krisztian Balog ISLA, University of Amsterdam Kruislaan 403, 1098 SJ Amsterdam, The Netherlands kbalog@science.uva.nl Toine Bogers ILK, Tilburg University P.O.",
                "Box 90153, 5000 LE Tilburg, The Netherlands A.M.Bogers@uvt.nl Leif Azzopardi Dept. of Computing Science University of Glasgow, Glasgow, G12 8QQ leif@dcs.gla.ac.uk Maarten de Rijke ISLA, University of Amsterdam Kruislaan 403, 1098 SJ Amsterdam, The Netherlands mdr@science.uva.nl Antal van den Bosch ILK, Tilburg University P.O.",
                "Box 90153, 5000 LE Tilburg, The Netherlands Antal.vdnBosch@uvt.nl ABSTRACT Expertise retrieval has been largely unexplored on data other than the W3C collection.",
                "At the same time, many intranets of universities and other knowledge-intensive organisations offer examples of relatively small but clean multilingual expertise data, covering broad ranges of expertise areas.",
                "We first present two main expertise retrieval tasks, along with a set of baseline approaches based on generative language modeling, aimed at finding expertise relations between topics and people.",
                "For our experimental evaluation, we introduce (and release) a new test set based on a crawl of a university site.",
                "Using this test set, we conduct two series of experiments.",
                "The first is aimed at determining the effectiveness of baseline expertise retrieval methods applied to the new test set.",
                "The second is aimed at assessing refined models that exploit characteristic features of the new test set, such as the organizational structure of the university, and the hierarchical structure of the topics in the test set.",
                "Expertise retrieval models are shown to be robust with respect to environments smaller than the W3C collection, and current techniques appear to be generalizable to other settings.",
                "Categories and Subject Descriptors H.3 [Information Storage and Retrieval]: H.3.1 Content Analysis and Indexing; H.3.3 Information Search and Retrieval; H.3.4 Systems and Software; H.4 [Information Systems Applications]: H.4.2 Types of Systems; H.4.m Miscellaneous General Terms Algorithms, Measurement, Performance, Experimentation 1.",
                "INTRODUCTION An organizations intranet provides a means for exchanging information between employees and for facilitating employee collaborations.",
                "To efficiently and effectively achieve this, it is necessary to provide search facilities that enable employees not only to access documents, but also to identify expert colleagues.",
                "At the TREC Enterprise Track [22] the need to study and understand expertise retrieval has been recognized through the introduction of Expert Finding tasks.",
                "The goal of expert finding is to identify a list of people who are knowledgeable about a given topic.",
                "This task is usually addressed by uncovering associations between people and topics [10]; commonly, a co-occurrence of the name of a person with topics in the same context is assumed to be evidence of expertise.",
                "An alternative task, which using the same idea of people-topic associations, is expert profiling, where the task is to return a list of topics that a person is knowledgeable about [3].",
                "The launch of the <br>expert finding task</br> at TREC has generated a lot of interest in expertise retrieval, with rapid progress being made in terms of modeling, algorithms, and evaluation aspects.",
                "However, nearly all of the expert finding or profiling work performed has been validated experimentally using the W3C collection [24] from the Enterprise Track.",
                "While this collection is currently the only publicly available test collection for expertise retrieval tasks, it only represents one type of intranet.",
                "With only one test collection it is not possible to generalize conclusions to other realistic settings.",
                "In this paper we focus on expertise retrieval in a realistic setting that differs from the W3C setting-one in which relatively small amounts of clean, multilingual data are available, that cover a broad range of expertise areas, as can be found on the intranets of universities and other knowledge-intensive organizations.",
                "Typically, this setting features several additional types of structure: topical structure (e.g., topic hierarchies as employed by the organization), organizational structure (faculty, department, ...), as well as multiple types of documents (research and course descriptions, publications, and academic homepages).",
                "This setting is quite different from the W3C setting in ways that might impact upon the performance of expertise retrieval tasks.",
                "We focus on a number of research questions in this paper: Does the relatively small amount of data available on an intranet affect the quality of the topic-person associations that lie at the heart of expertise retrieval algorithms?",
                "How do state-of-the-art algorithms developed on the W3C data set perform in the alternative scenario of the type described above?",
                "More generally, do the lessons from the <br>expert finding task</br> at TREC carry over to this setting?",
                "How does the inclusion or exclusion of different documents affect expertise retrieval tasks?",
                "In addition to, how can the topical and organizational structure be used for retrieval purposes?",
                "To answer our research questions, we first present a set of baseline approaches, based on generative language modeling, aimed at finding associations between topics and people.",
                "This allows us to formulate the expert finding and expert profiling tasks in a uniform way, and has the added benefit of allowing us to understand the relations between the two tasks.",
                "For our experimental evaluation, we introduce a new data set (the UvT Expert Collection) which is representative of the type of intranet that we described above.",
                "Our collection is based on publicly available data, crawled from the website of Tilburg University (UvT).",
                "This type of data is particularly interesting, since (1) it is clean, heterogeneous, structured, and focused, but comprises a limited number of documents; (2) contains information on the organizational hierarchy; (3) it is bilingual (English and Dutch); and (4) the list of expertise areas of an individual are provided by the employees themselves.",
                "Using the UvT Expert collection, we conduct two sets of experiments.",
                "The first is aimed at determining the effectiveness of baseline expertise finding and profiling methods in this new setting.",
                "A second group of experiments is aimed at extensions of the baseline methods that exploit characteristic features of the UvT Expert Collection; specifically, we propose and evaluate refined expert finding and profiling methods that incorporate topicality and organizational structure.",
                "Apart from the research questions and data set that we contribute, our main contributions are as follows.",
                "The baseline models developed for expertise finding perform well on the new data set.",
                "While on the W3C setting the <br>expert finding task</br> appears to be more difficult than profiling, for the UvT data the opposite is the case.",
                "We find that profiling on the UvT data set is considerably more difficult than on the W3C set, which we believe is due to the large (but realistic) number of topical areas that we used for profiling: about 1,500 for the UvT set, versus 50 in the W3C case.",
                "Taking the similarity between topics into account can significantly improve retrieval performance.",
                "The best performing similarity measures are content-based, therefore they can be applied on the W3C (and other) settings as well.",
                "Finally, we demonstrate that the organizational structure can be exploited in the form of a context model, improving MAP scores for certain models by up to 70%.",
                "The remainder of this paper is organized as follows.",
                "In the next section we review related work.",
                "Then, in Section 3 we provide detailed descriptions of the expertise retrieval tasks that we address in this paper: expert finding and expert profiling.",
                "In Section 4 we present our baseline models, of which the performance is then assessed in Section 6 using the UvT data set that we introduce in Section 5.",
                "Advanced models exploiting specific features of our data are presented in Section 7 and evaluated in Section 8.",
                "We formulate our conclusions in Section 9. 2.",
                "RELATED WORK Initial approaches to expertise finding often employed databases containing information on the skills and knowledge of each individual in the organization [11].",
                "Most of these tools (usually called yellow pages or people-finding systems) rely on people to self-assess their skills against a predefined set of keywords.",
                "For updating profiles in these systems in an automatic fashion there is a need for intelligent technologies [5].",
                "More recent approaches use specific document sets (such as email [6] or software [18]) to find expertise.",
                "In contrast with focusing on particular document types, there is also an increased interest in the development of systems that index and mine published intranet documents as sources of evidence for expertise.",
                "One such published approach is the P@noptic system [9], which builds a representation of each person by concatenating all documents associated with that person-this is similar to Model 1 of Balog et al. [4], who formalize and compare two methods.",
                "Balog et al.s Model 1 directly models the knowledge of an expert from associated documents, while their Model 2 first locates documents on the topic and then finds the associated experts.",
                "In the reported experiments the second method performs significantly better when there are sufficiently many associated documents per candidate.",
                "Most systems that took part in the 2005 and 2006 editions of the <br>expert finding task</br> at TREC implemented (variations on) one of these two models; see [10, 20].",
                "Macdonald and Ounis [16] propose a different approach for ranking candidate expertise with respect to a topic based on data fusion techniques, without using collectionspecific heuristics; they find that applying field-based weighting models improves the ranking of candidates.",
                "Petkova and Croft [19] propose yet another approach, based on a combination of the above Model 1 and 2, explicitly modeling topics.",
                "Turning to other expert retrieval tasks that can also be addressed using topic-people associations, Balog and de Rijke [3] addressed the task of determining topical expert profiles.",
                "While their methods proved to be efficient on the W3C corpus, they require an amount of data that may not be available in the typical knowledge-intensive organization.",
                "Balog and de Rijke [2] study the related task of finding experts that are similar to a small set of experts given as input.",
                "As an aside, creating a textual summary of a person shows some similarities to biography finding, which has received a considerable amount of attention recently; see e.g., [13].",
                "We use generative language modeling to find associations between topics and people.",
                "In our modeling of expert finding and profiling we collect evidence for expertise from multiple sources, in a heterogeneous collection, and integrate it with the co-occurrence of candidates names and query terms-the language modeling setting allows us to do this in a transparent manner.",
                "Our modeling proceeds in two steps.",
                "In the first step, we consider three baseline models, two taken from [4] (the Models 1 and 2 mentioned above), and one a refined version of a model introduced in [3] (which we refer to as Model 3 below); this third model is also similar to the model described by Petkova and Croft [19].",
                "The models we consider in our second round of experiments are mixture models similar to contextual language models [1] and to the expanded documents of Tao et al. [21]; however, the features that we use for definining our expansions-including topical structure and organizational structure-have not been used in this way before. 3.",
                "TASKS In the expertise retrieval scenario that we envisage, users seeking expertise within an organization have access to an interface that combines a search box (where they can search for experts or topics) with navigational structures (of experts and of topics) that allows them to click their way to an expert page (providing the profile of a person) or a topic page (providing a list of experts on the topic).",
                "To feed the above interface, we face two expertise retrieval tasks, expert finding and expert profiling, that we first define and then formalize using generative language models.",
                "In order to model either task, the probability of the query topic being associated to a candidate expert plays a key role in the final estimates for searching and profiling.",
                "By using language models, both the candidates and the query are characterized by distributions of terms in the vocabulary (used in the documents made available by the organization whose expertise retrieval needs we are addressing). 3.1 Expert finding Expert finding involves the task of finding the right person with the appropriate skills and knowledge: Who are the experts on topic X?.",
                "E.g., an employee wants to ascertain who worked on a particular project to find out why particular decisions were made without having to trawl through documentation (if there is any).",
                "Or, they may be in need a trained specialist for consultancy on a specific problem.",
                "Within an organization there are usually many possible candidates who could be experts for given topic.",
                "We can state this problem as follows: What is the probability of a candidate ca being an expert given the query topic q?",
                "That is, we determine p(ca|q), and rank candidates ca according to this probability.",
                "The candidates with the highest probability given the query are deemed the most likely experts for that topic.",
                "The challenge is how to estimate this probability accurately.",
                "Since the query is likely to consist of only a few terms to describe the expertise required, we should be able to obtain a more accurate estimate by invoking Bayes Theorem, and estimating: p(ca|q) = p(q|ca)p(ca) p(q) , (1) where p(ca) is the probability of a candidate and p(q) is the probability of a query.",
                "Since p(q) is a constant, it can be ignored for ranking purposes.",
                "Thus, the probability of a candidate ca being an expert given the query q is proportional to the probability of a query given the candidate p(q|ca), weighted by the a priori belief p(ca) that candidate ca is an expert. p(ca|q) ∝ p(q|ca)p(ca) (2) In this paper our main focus is on estimating the probability of a query given the candidate p(q|ca), because this probability captures the extent to which the candidate knows about the query topic.",
                "Whereas the candidate priors are generally assumed to be uniformand thus will not influence the ranking-it has been demonstrated that a sensible choice of priors may improve the performance [20]. 3.2 Expert profiling While the task of expert searching was concerned with finding experts given a particular topic, the task of expert profiling seeks to answer a related question: What topics does a candidate know about?",
                "Essentially, this turns the questions of expert finding around.",
                "The profiling of an individual candidate involves the identification of areas of skills and knowledge that they have expertise about and an evaluation of the level of proficiency in each of these areas.",
                "This is the candidates topical profile.",
                "Generally, topical profiles within organizations consist of tabular structures which explicitly catalogue the skills and knowledge of each individual in the organization.",
                "However, such practice is limited by the resources available for defining, creating, maintaining, and updating these profiles over time.",
                "By focusing on automatic methods which draw upon the available evidence within the document repositories of an organization, our aim is to reduce the human effort associated with the maintenance of topical profiles1 .",
                "A topical profile of a candidate, then, is defined as a vector where each element i of the vector corresponds to the candidate cas expertise on a given topic ki, (i.e., s(ca, ki)).",
                "Each topic ki defines a particular knowledge area or skill that the organization uses to define the candidates topical profile.",
                "Thus, it is assumed that a list of topics, {k1, . . . , kn}, where n is the number of pre-defined topics, is given: profile(ca) = s(ca, k1), s(ca, k2), . . . , s(ca, kn) . (3) 1 Context and evidence are needed to help users of expertise finding systems to decide whom to contact when seeking expertise in a particular area.",
                "Examples of such context are: Who does she work with?",
                "What are her contact details?",
                "Is she well-connected, just in case she is not able to help us herself?",
                "What is her role in the organization?",
                "Who is her superior?",
                "Collaborators, and affiliations, etc. are all part of the candidates social profile, and can serve as a background against which the systems recommendations should be interpreted.",
                "In this paper we only address the problem of determining topical profiles, and leave social profiling to further work.",
                "We state the problem of quantifying the competence of a person on a certain knowledge area as follows: What is the probability of a knowledge area (ki) being part of the candidates (expertise) profile? where s(ca, ki) is defined by p(ki|ca).",
                "Our task, then, is to estimate p(ki|ca), which is equivalent to the problem of obtaining p(q|ca), where the topic ki is represented as a query topic q, i.e., a sequence of keywords representing the expertise required.",
                "Both the expert finding and profiling tasks rely on the accurate estimation of p(q|ca).",
                "The only difference derives from the prior probability that a person is an expert (p(ca)), which can be incorporated into the <br>expert finding task</br>.",
                "This prior does not apply to the profiling task since the candidate (individual) is fixed. 4.",
                "BASELINE MODELS In this section we describe our baseline models for estimating p(q|ca), i.e., associations between topics and people.",
                "Both expert finding and expert profiling boil down to this estimation.",
                "We employ three models for calculating this probability. 4.1 From topics to candidates Using Candidate Models: Model 1 Model 1 [4] defines the probability of a query given a candidate (p(q|ca)) using standard language modeling techniques, based on a multinomial unigram language model.",
                "For each candidate ca, a candidate language model θca is inferred such that the probability of a term given θca is nonzero for all terms, i.e., p(t|θca) > 0.",
                "From the candidate model the query is generated with the following probability: p(q|θca) = Y t∈q p(t|θca)n(t,q) , where each term t in the query q is sampled identically and independently, and n(t, q) is the number of times t occurs in q.",
                "The candidate language model is inferred as follows: (1) an empirical model p(t|ca) is computed; (2) it is smoothed with background probabilities.",
                "Using the associations between a candidate and a document, the probability p(t|ca) can be approximated by: p(t|ca) = X d p(t|d)p(d|ca), where p(d|ca) is the probability that candidate ca generates a supporting document d, and p(t|d) is the probability of a term t occurring in the document d. We use the maximum-likelihood estimate of a term, that is, the normalised frequency of the term t in document d. The strength of the association between document d and candidate ca expressed by p(d|ca) reflects the degree to which the candidates expertise is described using this document.",
                "The estimation of this probability is presented later, in Section 4.2.",
                "The candidate model is then constructed as a linear interpolation of p(t|ca) and the background model p(t) to ensure there are no zero probabilities, which results in the final estimation: p(q|θca) = (4) Y t∈q ( (1 − λ) X d p(t|d)p(d|ca) ! + λp(t) )n(t,q) .",
                "Model 1 amasses all the term information from all the documents associated with the candidate, and uses this to represent that candidate.",
                "This model is used to predict how likely a candidate would produce a query q.",
                "This can can be intuitively interpreted as the probability of this candidate talking about the query topic, where we assume that this is indicative of their expertise.",
                "Using Document Models: Model 2 Model 2 [4] takes a different approach.",
                "Here, the process is broken into two parts.",
                "Given a candidate ca, (1) a document that is associated with a candidate is selected with probability p(d|ca), and (2) from this document a query q is generated with probability p(q|d).",
                "Then the sum over all documents is taken to obtain p(q|ca), such that: p(q|ca) = X d p(q|d)p(d|ca). (5) The probability of a query given a document is estimated by inferring a document language model θd for each document d in a similar manner as the candidate model was inferred: p(t|θd) = (1 − λ)p(t|d) + λp(t), (6) where p(t|d) is the probability of the term in the document.",
                "The probability of a query given the document model is: p(q|θd) = Y t∈q p(t|θd)n(t,q) .",
                "The final estimate of p(q|ca) is obtained by substituting p(q|d) for p(q|θd) into Eq. 5 (see [4] for full details).",
                "Conceptually, Model 2 differs from Model 1 because the candidate is not directly modeled.",
                "Instead, the document acts like a hidden variable in the process which separates the query from the candidate.",
                "This process is akin to how a user may search for candidates with a standard search engine: initially by finding the documents which are relevant, and then seeing who is associated with that document.",
                "By examining a number of documents the user can obtain an idea of which candidates are more likely to discuss the topic q.",
                "Using Topic Models: Model 3 We introduce a third model, Model 3.",
                "Instead of attempting to model the query generation process via candidate or document models, we represent the query as a topic language model and directly estimate the probability of the candidate p(ca|q).",
                "This approach is similar to the model presented in [3, 19].",
                "As with the previous models, a language model is inferred, but this time for the query.",
                "We adapt the work of Lavrenko and Croft [14] to estimate a topic model from the query.",
                "The procedure is as follows.",
                "Given a collection of documents and a query topic q, it is assumed that there exists an unknown topic model θk that assigns probabilities p(t|θk) to the term occurrences in the topic documents.",
                "Both the query and the documents are samples from θk (as opposed to the previous approaches, where a query is assumed to be sampled from a specific document or candidate model).",
                "The main task is to estimate p(t|θk), the probability of a term given the topic model.",
                "Since the query q is very sparse, and as there are no examples of documents on the topic, this distribution needs to be approximated.",
                "Lavrenko and Croft [14] suggest a reasonable way of obtaining such an approximation, by assuming that p(t|θk) can be approximated by the probability of term t given the query q.",
                "We can then estimate p(t|q) using the joint probability of observing the term t together with the query terms, q1, . . . , qm, and dividing by the joint probability of the query terms: p(t|θk) ≈ p(t|q) = p(t, q1, . . . , qm) p(q1, . . . , qm) = p(t, q1, . . . , qm) P t ∈T p(t , q1, . . . , qm) , where p(q1, . . . , qm) = P t ∈T p(t , q1, . . . , qm), and T is the entire vocabulary of terms.",
                "In order to estimate the joint probability p(t, q1, . . . , qm), we follow [14, 15] and assume t and q1, . . . , qm are mutually independent, once we pick a source distribution from the set of underlying source distributions U.",
                "If we choose U to be a set of document models. then to construct this set, the query q would be issued against the collection, and the top n returned are assumed to be relevant to the topic, and thus treated as samples from the topic model. (Note that candidate models could be used instead.)",
                "With the document models forming U, the joint probability of term and query becomes: p(t, q1, . . . , qm) = X d∈U p(d) ˘ p(t|θd) mY i=1 p(qi|θd) ¯ . (7) Here, p(d) denotes the prior distribution over the set U, which reflects the relevance of the document to the topic.",
                "We assume that p(d) is uniform across U.",
                "In order to rank candidates according to the topic model defined, we use the Kullback-Leibler divergence metric (KL, [8]) to measure the difference between the candidate models and the topic model: KL(θk||θca) = X t p(t|θk) log p(t|θk) p(t|θca) . (8) Candidates with a smaller divergence from the topic model are considered to be more likely experts on that topic.",
                "The candidate model θca is defined in Eq. 4.",
                "By using KL divergence instead of the probability of a candidate given the topic model p(ca|θk), we avoid normalization problems. 4.2 Document-candidate associations For our models we need to be able to estimate the probability p(d|ca), which expresses the extent to which a document d characterizes the candidate ca.",
                "In [4], two methods are presented for estimating this probability, based on the number of person names recognized in a document.",
                "However, in our (intranet) setting it is reasonable to assume that authors of documents can unambiguously be identified (e.g., as the author of an article, the teacher assigned to a course, the owner of a web page, etc.)",
                "Hence, we set p(d|ca) to be 1 if candidate ca is author of document d, otherwise the probability is 0.",
                "In Section 6 we describe how authorship can be determined on different types of documents within the collection. 5.",
                "THE UVT EXPERT COLLECTION The UvT Expert collection used in the experiments in this paper fits the scenario outlined in Section 3.",
                "The collection is based on the Webwijs (Webwise) system developed at Tilburg University (UvT) in the Netherlands.",
                "Webwijs (http://www.uvt.nl/ webwijs/) is a publicly accessible database of UvT employees who are involved in research or teaching; currently, Webwijs contains information about 1168 experts, each of whom has a page with contact information and, if made available by the expert, a research description and publications list.",
                "In addition, each expert can select expertise areas from a list of 1491 topics and is encouraged to suggest new topics that need to be approved by the Webwijs editor.",
                "Each topic has a separate page that shows all experts associated with that topic and, if available, a list of related topics.",
                "Webwijs is available in Dutch and English, and this bilinguality has been preserved in the collection.",
                "Every Dutch Webwijs page has an English translation.",
                "Not all Dutch topics have an English translation, but the reverse is true: the 981 English topics all have a Dutch equivalent.",
                "About 42% of the experts teach courses at Tilburg University; these courses were also crawled and included in the profile.",
                "In addition, about 27% of the experts link to their academic homepage from their Webwijs page.",
                "These home pages were crawled and added to the collection. (This means that if experts put the full-text versions of their publications on their academic homepage, these were also available for indexing.)",
                "We also obtained 1880 full-text versions of publications from the UvT institutional repository and Dutch English no. of experts 1168 1168 no. of experts with ≥ 1 topic 743 727 no. of topics 1491 981 no. of expert-topic pairs 4318 3251 avg. no. of topics/expert 5.8 5.9 max. no. of topics/expert (no. of experts) 60 (1) 35 (1) min. no. of topics/expert (no. of experts) 1 (74) 1 (106) avg. no. of experts/topic 2.9 3.3 max. no. of experts/topic (no. of topics) 30 (1) 30 (1) min. no. of experts/topic (no. of topics) 1 (615) 1 (346) no. of experts with HP 318 318 no. of experts with CD 318 318 avg. no. of CDs per teaching expert 3.5 3.5 no. of experts with RD 329 313 no. of experts with PUB 734 734 avg. no. of PUBs per expert 27.0 27.0 avg. no. of PUB citations per expert 25.2 25.2 avg. no. of full-text PUBs per expert 1.8 1.8 Table 2: Descriptive statistics of the Dutch and English versions of the UvT Expert collection. converted them to plain text.",
                "We ran the TextCat [23] language identifier to classify the language of the home pages and the fulltext publications.",
                "We restricted ourselves to pages where the classifier was confident about the language used on the page.",
                "This resulted in four document types: research descriptions (RD), course descriptions (CD), publications (PUB; full-text and citationonly versions), and academic homepages (HP).",
                "Everything was bundled into the UvT Expert collection which is available at http: //ilk.uvt.nl/uvt-expert-collection/.",
                "The UvT Expert collection was extracted from a different organizational setting than the W3C collection and differs from it in a number of ways.",
                "The UvT setting is one with relatively small amounts of multilingual data.",
                "Document-author associations are clear and the data is structured and clean.",
                "The collection covers a broad range of expertise areas, as one can typically find on intranets of universities and other knowledge-intensive institutes.",
                "Additionally, our university setting features several types of structure (topical and organizational), as well as multiple document types.",
                "Another important difference between the two data sets is that the expertise areas in the UvT Expert collection are self-selected instead of being based on group membership or assignments by others.",
                "Size is another dimension along which the W3C and UvT Expert collections differ: the latter is the smaller of the two.",
                "Also realistic are the large differences in the amount of information available for each expert.",
                "Utilizing Webwijs is voluntary; 425 Dutch experts did not select any topics at all.",
                "This leaves us with 743 Dutch and 727 English usable expert profiles.",
                "Table 2 provides descriptive statistics for the UvT Expert collection.",
                "Universities tend to have a hierarchical structure that goes from the faculty level, to departments, research groups, down to the individual researchers.",
                "In the UvT Expert collection we have information about the affiliations of researchers with faculties and institutes, providing us with a two-level organizational hierarchy.",
                "Tilburg University has 22 organizational units at the faculty level (including the university office and several research institutes) and 71 departments, which amounts to 3.2 departments per faculty.",
                "As to the topical hierarchy used by Webwijs, 131 of the 1491 topics are top nodes in the hierarchy.",
                "This hierarchy has an average topic chain length of 2.65 and a maximum length of 7 topics. 6.",
                "EVALUATION Below, we evaluate Section 4s models for expert finding and profiling onthe UvT Expert collection.",
                "We detail our research questions and experimental setup, and then present our results. 6.1 Research Questions We address the following research questions.",
                "Both expert finding and profiling rely on the estimations of p(q|ca).",
                "The question is how the models compare on the different tasks, and in the setting of the UvT Expert collection.",
                "In [4], Model 2 outperformed Model 1 on the W3C collection.",
                "How do they compare on our data set?",
                "And how does Model 3 compare to Model 1?",
                "What about performance differences between the two languages in our test collection? 6.2 Experimental Setup The output of our models was evaluated against the self-assigned topic labels, which were treated as relevance judgements.",
                "Results were evaluated separately for English and Dutch.",
                "For English we only used topics for which the Dutch translation was available; for Dutch all topics were considered.",
                "The results were averaged for the queries in the intersection of relevance judgements and results; missing queries do not contribute a value of 0 to the scores.",
                "We use standard information retrieval measures, such as Mean Average Precision (MAP) and Mean Reciprocal Rank (MRR).",
                "We also report the percentage of topics (%q) and candidates (%ca) covered, for the expert finding and profiling tasks, respectively. 6.3 Results Table 1 shows the performance of Model 1, 2, and 3 on the expert finding and profiling tasks.",
                "The rows of the table correspond to the various document types (RD, CD, PUB, and HP) and to their combinations.",
                "RD+CD+PUB+HP is equivalent to the full collection and will be referred as the BASELINE of our experiments.",
                "Looking at Table 1 we see that Model 2 performs the best across the board.",
                "However, when the data is clean and very focused (RD), Model 3 outperforms it in a number of cases.",
                "Model 1 has the best coverage of candidates (%ca) and topics (%q).",
                "The various document types differ in their characteristics and how they improve the finding and profiling tasks.",
                "Expert profiling benefits much from the clean data present in the RD and CD document types, while the publications contribute the most to the <br>expert finding task</br>.",
                "Adding the homepages does not prove to be particularly useful.",
                "When we compare the results across languages, we find that the coverage of English topics (%q) is higher than of the Dutch ones for expert finding.",
                "Apart from that, the scores fall in the same range for both languages.",
                "For the profiling task the coverage of the candidates (%ca) is very similar for both languages.",
                "However, the performance is substantially better for the English topics.",
                "While it is hard to compare scores across collections, we conclude with a brief comparison of the absolute scores in Table 1 to those reported in [3, 4] on the W3C test set (2005 edition).",
                "For expert finding the MAP scores for Model 2 reported here are about 50% higher than the corresponding figures in [4], while our MRR scores are slightly below those in [4].",
                "For expert profiling, the differences are far more dramatic: the MAP scores for Model 2 reported here are around 50% below the scores in [3], while the (best) MRR scores are about the same as those in [3].",
                "The cause for the latter differences seems to reside in the number of knowledge areas considered here-approx. 30 times more than in the W3C setting. 7.",
                "ADVANCED MODELS Now that we have developed and assessed basic language modeling techniques for expertise retrieval, we turn to refined models that exploit special features of our test collection. 7.1 Exploiting knowledge area similarity One way to improve the scoring of a query given a candidate is to consider what other requests the candidate would satisfy and use them as further evidence to support the original query, proportional Expert finding Expert profiling Document types Model 1 Model 2 Model 3 Model 1 Model 2 Model 3 %q MAP MRR %q MAP MRR %q MAP MRR %ca MAP MRR %ca MAP MRR %ca MAP MRR English RD 97.8 0.126 0.269 83.5 0.144 0.311 83.3 0.129 0.271 100 0.089 0.189 39.3 0.232 0.465 41.1 0.166 0.337 CD 97.8 0.118 0.227 91.7 0.123 0.248 91.7 0.118 0.226 32.8 0.188 0.381 32.4 0.195 0.385 32.7 0.203 0.370 PUB 97.8 0.200 0.330 98.0 0.216 0.372 98.0 0.145 0.257 78.9 0.167 0.364 74.5 0.212 0.442 78.9 0.135 0.299 HP 97.8 0.081 0.186 97.4 0.071 0.168 97.2 0.062 0.149 31.2 0.150 0.299 28.8 0.185 0.335 30.1 0.136 0.287 RD+CD 97.8 0.188 0.352 92.9 0.193 0.360 92.9 0.150 0.273 100 0.145 0.286 61.3 0.251 0.477 63.2 0.217 0.416 RD+CD+PUB 97.8 0.235 0.373 98.1 0.277 0.439 98.1 0.178 0.305 100 0.196 0.380 87.2 0.280 0.533 89.5 0.170 0.344 RD+CD+PUB+HP 97.8 0.237 0.372 98.6 0.280 0.441 98.5 0.166 0.293 100 0.199 0.387 88.7 0.281 0.525 90.9 0.169 0.329 Dutch RD 61.3 0.094 0.229 38.4 0.137 0.336 38.3 0.127 0.295 38.0 0.127 0.386 34.1 0.138 0.420 38.0 0.105 0.327 CD 61.3 0.107 0.212 49.7 0.128 0.256 49.7 0.136 0.261 32.5 0.151 0.389 31.8 0.158 0.396 32.5 0.170 0.380 PUB 61.3 0.193 0.319 59.5 0.218 0.368 59.4 0.173 0.291 78.8 0.126 0.364 76.0 0.150 0.424 78.8 0.103 0.294 HP 61.3 0.063 0.169 56.6 0.064 0.175 56.4 0.062 0.163 29.8 0.108 0.308 27.8 0.125 0.338 29.8 0.098 0.255 RD+CD 61.3 0.159 0.314 51.9 0.184 0.360 51.9 0.169 0.324 60.5 0.151 0.410 57.2 0.166 0.431 60.4 0.159 0.384 RD+CD+PUB 61.3 0.244 0.398 61.5 0.260 0.424 61.4 0.210 0.350 90.3 0.165 0.445 88.2 0.189 0.479 90.3 0.126 0.339 RD+CD+PUB+HP 61.3 0.249 0.401 62.6 0.265 0.436 62.6 0.195 0.344 91.9 0.164 0.426 90.1 0.195 0.488 91.9 0.125 0.328 Table 1: Performance of the models on the expert finding and profiling tasks, using different document types and their combinations. %q is the number of topics covered (applies to the <br>expert finding task</br>), %ca is the number of candidates covered (applies to the expert profiling task).",
                "The top and bottom blocks correspond to English and Dutch respectively.",
                "The best scores are in boldface. to how related the other requests are to the original query.",
                "This can be modeled by interpolating between the p(q|ca) and the further supporting evidence from all similar requests q , as follows: p (q|ca) = λp(q|ca) + (1 − λ) X q p(q|q )p(q |ca), (9) where p(q|q ) represents the similarity between the two topics q and q .",
                "To be able to work with similarity methods that are not necessarily probabilities, we set p(q|q ) = w(q,q ) γ , where γ is a normalizing constant, such that γ = P q w(q , q ).",
                "We consider four methods for calculating the similarity score between two topics.",
                "Three approaches are strictly content-based, and establish similarity by examining co-occurrence patterns of topics within the collection, while the last approach exploits the hierarchical structure of topical areas that may be present within an organization (see [7] for further examples of integrating word relationships into language models).",
                "The Kullback-Leibler (KL) divergence metric defined in Eq. 8 provides a measure of how different or similar two probability distributions are.",
                "A topic model is inferred for q and q using the method presented in Section 4.1 to describe the query across the entire vocabulary.",
                "Since a lower KL score means the queries are more similar, we let w(q, q ) = max(KL(θq||·) − KL(θq||θq )).",
                "Pointwise Mutual Information (PMI, [17]) is a measure of association used in information theory to determine the extent of independence between variables.",
                "The dependence between two queries is reflected by the SI(q, q ) score, where scores greater than zero indicate that it is likely that there is a dependence, which we take to mean that the queries are likely to be similar: SI(q, q ) = log p(q, q ) p(q)p(q ) (10) We estimate the probability of a topic p(q) using the number of documents relevant to query q within the collection.",
                "The joint probability p(q, q ) is estimated similarly, by using the concatenation of q and q as a query.",
                "To obtain p(q|q ), we then set w(q, q ) = SI(q, q ) when SI(q, q ) > 0 otherwise w(q, q ) = 0, because we are only interested in including queries that are similar.",
                "The log-likelihood statistic provides another measure of dependence, which is more reliable than the pointwise mutual information measure [17].",
                "Let k1 be the number of co-occurrences of q and q , k2 the number of occurrences of q not co-occurring with q , n1 the total number of occurrences of q , and n2 the total number of topic tokens minus the number of occurrences of q .",
                "Then, let p1 = k1/n1, p2 = k2/n2, and p = (k1 + k2)/(n1 + n2), (q, q ) = 2( (p1, k1, n1) + (p2, k2, n2) − (p, k1, n1) − (p, k2, n2)), where (p, n, k) = k log p + (n − k) log(1 − p).",
                "The higher score indicate that queries are also likely to be similar, thus we set w(q, q ) = (q, q ).",
                "Finally, we also estimate the similarity of two topics based on their distance within the topic hierarchy.",
                "The topic hierarchy is viewed as a directed graph, and for all topic-pairs the shortest path SP(q, q ) is calculated.",
                "We set the similarity score to be the reciprocal of the shortest path: w(q, q ) = 1/SP(q, q ). 7.2 Contextual information Given the hierarchy of an organization, the units to which a person belong are regarded as a context so as to compensate for data sparseness.",
                "We model it as follows: p (q|ca) = 1 − P ou∈OU(ca) λou · p(q|ca) + P ou∈OU(ca) λou · p(q|ou), where OU(ca) is the set of organizational units of which candidate ca is a member of, and p(q|o) expresses the strength of the association between query q and the unit ou.",
                "The latter probability can be estimated using either of the three basic models, by simply replacing ca with ou in the corresponding equations.",
                "An organizational unit is associated with all the documents that its members have authored.",
                "That is, p(d|ou) = maxca∈ou p(d|ca). 7.3 A simple multilingual model For knowledge institutes in Europe, academic or otherwise, a multilingual (or at least bilingual) setting is typical.",
                "The following model builds on a kind of independence assumption: there is no spill-over of expertise/profiles across language boundaries.",
                "While a simplification, this is a sensible first approach.",
                "That is: p (q|ca) =P l∈L λl · p(ql|ca), where L is the set of languages used in the collection, ql is the translation of the query q to language l, and λl is a language specific smoothing parameter, such that P l∈L λl = 1. 8.",
                "ADVANCED MODELS: EVALUATION In this section we present an experimental evaluation of our advanced models.",
                "Expert finding Expert profiling Language Model 1 Model 2 Model 3 Model 1 Model 2 Model 3 %q MAP MRR %q MAP MRR %q MAP MRR %ca MAP MRR %ca MAP MRR %ca MAP MRR English only 97.8 0.237 0.372 98.6 0.280 0.441 98.5 0.166 0.293 100 0.199 0.387 88.7 0.281 0.525 90.9 0.169 0.329 Dutch only 61.3 0.249 0.401 62.6 0.265 0.436 62.6 0.195 0.344 91.9 0.164 0.426 90.1 0.195 0.488 91.9 0.125 0.328 Combination 99.4 0.297 0.444 99.7 0.324 0.491 99.7 0.223 0.388 100 0.241 0.445 92.1 0.313 0.564 93.2 0.224 0.411 Table 3: Performance of the combination of languages on the expert finding and profiling tasks (on candidates).",
                "Best scores for each model are in italic, absolute best scores for the expert finding and profiling tasks are in boldface.",
                "Method Model 1 Model 2 Model 3 MAP MRR MAP MRR MAP MRR English BASELINE 0.296 0.454 0.339 0.509 0.221 0.333 KLDIV 0.291 0.453 0.327 0.503 0.219 0.330 PMI 0.291 0.453 0.337 0.509 0.219 0.331 LL 0.319 0.490 0.360 0.524 0.233 0.368 HDIST 0.299 0.465 0.346 0.537 0.219 0.332 Dutch BASELINE 0.240 0.350 0.271 0.403 0.227 0.389 KLDIV 0.239 0.347 0.253 0.386 0.224 0.385 PMI 0.239 0.350 0.260 0.392 0.227 0.389 LL 0.255 0.372 0.281 0.425 0.231 0.389 HDIST 0.253 0.365 0.271 0.407 0.236 0.402 Method Model 1 Model 2 Model 3 MAP MRR MAP MRR MAP MRR English BASELINE 0.485 0.546 0.499 0.548 0.381 0.416 KLDIV 0.510 0.564 0.513 0.558 0.381 0.416 PMI 0.486 0.546 0.495 0.542 0.407 0.451 LL 0.558 0.589 0.586 0.617 0.408 0.453 HDIST 0.507 0.567 0.512 0.563 0.386 0.420 Dutch BASELINE 0.263 0.313 0.294 0.358 0.262 0.315 KLDIV 0.284 0.336 0.271 0.321 0.261 0.314 PMI 0.265 0.317 0.265 0.316 0.273 0.330 LL 0.312 0.351 0.330 0.377 0.284 0.331 HDIST 0.280 0.327 0.288 0.341 0.266 0.321 Table 4: Performance on the expert finding (top) and profiling (bottom) tasks, using knowledge area similarities.",
                "Runs were evaluated on the main topics set.",
                "Best scores are in boldface. 8.1 Research Questions Our questions follow the refinements presented in the preceding section: Does exploiting the knowledge area similarity improve effectiveness?",
                "Which of the various methods for capturing word relationships is most effective?",
                "Furthermore, is our way of bringing in contextual information useful?",
                "For which tasks?",
                "And finally, is our simple way of combining the monolingual scores sufficient for obtaining significant improvements? 8.2 Experimental setup Given that the self-assessments are also sparse in our collection, in order to be able to measure differences between the various models, we selected a subset of topics, and evaluated (some of the) runs only on this subset.",
                "This set is referred as main topics, and consists of topics that are located at the top level of the topical hierarchy. (A main topic has subtopics, but is not a subtopic of any other topic.)",
                "This main set consists of 132 Dutch and 119 English topics.",
                "The relevance judgements were restricted to the main topic set, but were not expanded with subtopics. 8.3 Exploiting knowledge area similarity Table 4 presents the results.",
                "The four methods used for estimating knowledge-area similarity are KL divergence (KLDIV), PointLang.",
                "Topics Model 1 Model 2 Model 3 MAP MRR MAP MRR MAP MRR Expert finding UK ALL 0.423 0.545 0.654 0.799 0.494 0.629 UK MAIN 0.500 0.621 0.704 0.834 0.587 0.699 NL ALL 0.439 0.560 0.672 0.826 0.480 0.630 NL MAIN 0.440 0.584 0.645 0.816 0.515 0.655 Expert profiling UK ALL 0.240 0.640 0.306 0.778 0.223 0.616 UK MAIN 0.523 0.677 0.519 0.648 0.461 0.587 NL ALL 0.203 0.716 0.254 0.770 0.183 0.627 NL MAIN 0.332 0.576 0.380 0.624 0.332 0.549 Table 5: Evaluating the context models on organizational units. wise mutual information (PMI), log-likelihood (LL), and distance within topic hierarchy (HDIST).",
                "We managed to improve upon the baseline in all cases, but the improvement is more noticeable for the profiling task.",
                "For both tasks, the LL method performed best.",
                "The content-based approaches performed consistently better than HDIST. 8.4 Contextual information A two level hierarchy of organizational units (faculties and institutes) is available in the UvT Expert collection.",
                "The unit a person belongs to is used as a context for that person.",
                "First, we evaluated the models of the organizational units, using all topics (ALL) and only the main topics (MAIN).",
                "An organizational unit is considered to be relevant for a given topic (or vice versa) if at least one member of the unit selected the given topic as an expertise area.",
                "Table 5 reports on the results.",
                "As far as expert finding goes, given a topic, the corresponding organizational unit can be identified with high precision.",
                "However, the expert profiling task shows a different picture: the scores are low, and the task seems hard.",
                "The explanation may be that general concepts (i.e., our main topics) may belong to several organizational units.",
                "Second, we performed another evaluation, where we combined the contextual models with the candidate models (to score candidates again).",
                "Table 6 reports on the results.",
                "We find a positive impact of the context models only for expert finding.",
                "Noticably, for expert finding (and Model 1), it improves over 50% (for English) and over 70% (for Dutch) on MAP.",
                "The poor performance on expert profiling may be due to the fact that context models alone did not perform very well on the profiling task to begin with. 8.5 Multilingual models In this subsection we evaluate the method for combining results across multiple languages that we described in Section 7.3.",
                "In our setting the set of languages consists of English and Dutch: L = {UK, NL}.",
                "The weights on these languages were set to be identical (λUK = λNL = 0.5).",
                "We performed experiments with various λ settings, but did not observe significant differences in performance.",
                "Table 3 reports on the multilingual results, where performance is evaluated on the full topic set.",
                "All three models significantly imLang.",
                "Method Model 1 Model 2 Model 3 MAP MRR MAP MRR MAP MRR Expert finding UK BL 0.296 0.454 0.339 0.509 0.221 0.333 UK CT 0.330 0.491 0.342 0.500 0.228 0.342 NL BL 0.240 0.350 0.271 0.403 0.227 0.389 NL CT 0.251 0.382 0.267 0.410 0.246 0.404 Expert profiling UK BL 0.485 0.546 0.499 0.548 0.381 0.416 UK CT 0.562 0.620 0.508 0.558 0.440 0.486 NL BL 0.263 0.313 0.294 0.358 0.262 0.315 NL CT 0.330 0.384 0.317 0.387 0.294 0.345 Table 6: Performance of the context models (CT) compared to the baseline (BL).",
                "Best scores are in boldface. proved over all measures for both tasks.",
                "The coverage of topics and candidates for the expert finding and profiling tasks, respectively, is close to 100% in all cases.",
                "The relative improvement of the precision scores ranges from 10% to 80%.",
                "These scores demonstrate that despite its simplicity, our method for combining results over multiple languages achieves substantial improvements over the baseline. 9.",
                "CONCLUSIONS In this paper we focused on expertise retrieval (expert finding and profiling) in a new setting of a typical knowledge-intensive organization in which the available data is of high quality, multilingual, and covering a broad range of expertise area.",
                "Typically, the amount of available data in such an organization (e.g., a university, a research institute, or a research lab) is limited when compared to the W3C collection that has mostly been used for the experimental evaluation of expertise retrieval so far.",
                "To examine expertise retrieval in this setting, we introduced (and released) the UvT Expert collection as a representative case of such knowledge intensive organizations.",
                "The new collection reflects the typical properties of knowledge-intensive institutes noted above and also includes several features which may are potentially useful for expertise retrieval, such as topical and organizational structure.",
                "We evaluated how current state-of-the-art models for expert finding and profiling performed in this new setting and then refined these models in order to try and exploit the different characteristics within the data environment (language, topicality, and organizational structure).",
                "We found that current models of expertise retrieval generalize well to this new environment; in addition we found that refining the models to account for the differences results in significant improvements, thus making up for problems caused by data sparseness issues.",
                "Future work includes setting up manual assessments of automatically generated profiles by the employees themselves, especially in cases where the employees have not provided a profile themselves. 10.",
                "ACKNOWLEDGMENTS Krisztian Balog was supported by the Netherlands Organisation for Scientific Research (NWO) under project number 220-80-001.",
                "Maarten de Rijke was also supported by NWO under project numbers 017.001.190, 220-80-001, 264-70-050, 354-20-005, 600.065.120, 612-13-001, 612.000.106, 612.066.302, 612.069.006, 640.001.501, 640.002.501, and by the E.U.",
                "IST programme of the 6th FP for RTD under project MultiMATCH contract IST-033104.",
                "The work of Toine Bogers and Antal van den Bosch was funded by the IOP-MMI-program of SenterNovem / The Dutch Ministry of Economic Affairs, as part of the `A Propos project. 11.",
                "REFERENCES [1] L. Azzopardi.",
                "Incorporating Context in the Language Modeling Framework for ad-hoc Information Retrieval.",
                "PhD thesis, University of Paisley, 2005. [2] K. Balog and M. de Rijke.",
                "Finding similar experts.",
                "In This volume, 2007. [3] K. Balog and M. de Rijke.",
                "Determining expert profiles (with an application to expert finding).",
                "In IJCAI 07: Proc. 20th Intern.",
                "Joint Conf. on Artificial Intelligence, pages 2657-2662, 2007. [4] K. Balog, L. Azzopardi, and M. de Rijke.",
                "Formal models for expert finding in enterprise corpora.",
                "In SIGIR 06: Proc. 29th annual intern.",
                "ACM SIGIR conf. on Research and development in information retrieval, pages 43-50, 2006. [5] I. Becerra-Fernandez.",
                "The role of artificial intelligence technologies in the implementation of people-finder knowledge management systems.",
                "In AAAI Workshop on Bringing Knowledge to Business Processes, March 2000. [6] C. S. Campbell, P. P. Maglio, A. Cozzi, and B. Dom.",
                "Expertise identification using email communications.",
                "In CIKM 03: Proc. twelfth intern. conf. on Information and knowledge management, pages 528531, 2003. [7] G. Cao, J.-Y.",
                "Nie, and J. Bai.",
                "Integrating word relationships into language models.",
                "In SIGIR 05: Proc. 28th annual intern.",
                "ACM SIGIR conf. on Research and development in information retrieval, pages 298-305, 2005. [8] T. M. Cover and J.",
                "A. Thomas.",
                "Elements of Information Theory.",
                "Wiley-Interscience, 1991. [9] N. Craswell, D. Hawking, A. M. Vercoustre, and P. Wilkins.",
                "P@noptic expert: Searching for experts not just for documents.",
                "In Ausweb, 2001. [10] N. Craswell, A. de Vries, and I. Soboroff.",
                "Overview of the TREC2005 Enterprise Track.",
                "In The Fourteenth Text REtrieval Conf.",
                "Proc. (TREC 2005), 2006. [11] T. H. Davenport and L. Prusak.",
                "Working Knowledge: How Organizations Manage What They Know.",
                "Harvard Business School Press, Boston, MA, 1998. [12] T. Dunning.",
                "Accurate methods for the statistics of surprise and coincidence.",
                "Computational Linguistics, 19(1):61-74, 1993. [13] E. Filatova and J. Prager.",
                "Tell me what you do and Ill tell you what you are: Learning occupation-related activities for biographies.",
                "In HLT/EMNLP, 2005. [14] V. Lavrenko and W. B. Croft.",
                "Relevance based language models.",
                "In SIGIR 01: Proc. 24th annual intern.",
                "ACM SIGIR conf. on Research and development in information retrieval, pages 120-127, 2001. [15] V. Lavrenko, M. Choquette, and W. B. Croft.",
                "Cross-lingual relevance models.",
                "In SIGIR 02: Proc. 25th annual intern.",
                "ACM SIGIR conf. on Research and development in information retrieval, pages 175-182, 2002. [16] C. Macdonald and I. Ounis.",
                "Voting for candidates: adapting data fusion techniques for an expert search task.",
                "In CIKM 06: Proc. 15th ACM intern. conf. on Information and knowledge management, pages 387-396, 2006. [17] C. Manning and H. Sch¨utze.",
                "Foundations of Statistical Natural Language Processing.",
                "The MIT Press, 1999. [18] A. Mockus and J. D. Herbsleb.",
                "Expertise browser: a quantitative approach to identifying expertise.",
                "In ICSE 02: Proc. 24th Intern.",
                "Conf. on Software Engineering, pages 503-512, 2002. [19] D. Petkova and W. B. Croft.",
                "Hierarchical language models for expert finding in enterprise corpora.",
                "In Proc.",
                "ICTAI 2006, pages 599-608, 2006. [20] I. Soboroff, A. de Vries, and N. Craswell.",
                "Overview of the TREC 2006 Enterprise Track.",
                "In TREC 2006 Working Notes, 2006. [21] T. Tao, X. Wang, Q. Mei, and C. Zhai.",
                "Language model information retrieval with document expansion.",
                "In HLT-NAACL 2006, 2006. [22] TREC.",
                "Enterprise track, 2005.",
                "URL: http://www.ins.cwi. nl/projects/trec-ent/wiki/. [23] G. van Noord.",
                "TextCat Language Guesser.",
                "URL: http://www. let.rug.nl/˜vannoord/TextCat/. [24] W3C.",
                "The W3C test collection, 2005.",
                "URL: http://research. microsoft.com/users/nickcr/w3c-summary.html."
            ],
            "original_annotated_samples": [
                "The launch of the <br>expert finding task</br> at TREC has generated a lot of interest in expertise retrieval, with rapid progress being made in terms of modeling, algorithms, and evaluation aspects.",
                "More generally, do the lessons from the <br>expert finding task</br> at TREC carry over to this setting?",
                "While on the W3C setting the <br>expert finding task</br> appears to be more difficult than profiling, for the UvT data the opposite is the case.",
                "Most systems that took part in the 2005 and 2006 editions of the <br>expert finding task</br> at TREC implemented (variations on) one of these two models; see [10, 20].",
                "The only difference derives from the prior probability that a person is an expert (p(ca)), which can be incorporated into the <br>expert finding task</br>."
            ],
            "translated_annotated_samples": [
                "El lanzamiento de la tarea de <br>Búsqueda de Expertos</br> en TREC ha generado mucho interés en la recuperación de expertos, con un rápido progreso en términos de modelado, algoritmos y aspectos de evaluación.",
                "Más en general, ¿se pueden aplicar las lecciones de la <br>tarea de Búsqueda de Expertos</br> en TREC a este contexto?",
                "Mientras que en el entorno del W3C la <br>tarea de encontrar expertos</br> parece ser más difícil que el perfilado, para los datos de UvT ocurre lo contrario.",
                "La mayoría de los sistemas que participaron en las ediciones de 2005 y 2006 de la tarea de <br>Búsqueda de Expertos</br> en TREC implementaron (variaciones de) uno de estos dos modelos; ver [10, 20].",
                "La única diferencia proviene de la probabilidad previa de que una persona sea experta (p(ca)), la cual puede ser incorporada en la <br>tarea de encontrar expertos</br>."
            ],
            "translated_text": "Recuperación de amplia experiencia en entornos de datos dispersos Krisztian Balog ISLA, Universidad de Ámsterdam Kruislaan 403, 1098 SJ Ámsterdam, Países Bajos kbalog@science.uva.nl Toine Bogers ILK, Universidad de Tilburg P.O. Caja 90153, 5000 LE Tilburg, Países Bajos A.M.Bogers@uvt.nl Leif Azzopardi Dept. de Ciencias de la Computación Universidad de Glasgow, Glasgow, G12 8QQ leif@dcs.gla.ac.uk Maarten de Rijke ISLA, Universidad de Ámsterdam Kruislaan 403, 1098 SJ Ámsterdam, Países Bajos mdr@science.uva.nl Antal van den Bosch ILK, Universidad de Tilburg P.O. La recuperación de la experiencia ha sido ampliamente inexplorada en datos distintos a la colección de la W3C. Al mismo tiempo, muchas intranets de universidades y otras organizaciones intensivas en conocimiento ofrecen ejemplos de datos de expertos multilingües relativamente pequeños pero precisos, que abarcan amplias áreas de especialización. Primero presentamos dos tareas principales de recuperación de expertos, junto con un conjunto de enfoques de referencia basados en modelado del lenguaje generativo, con el objetivo de encontrar relaciones de experticia entre temas y personas. Para nuestra evaluación experimental, introducimos (y liberamos) un nuevo conjunto de pruebas basado en un rastreo de un sitio universitario. Usando este conjunto de pruebas, llevamos a cabo dos series de experimentos. El primero tiene como objetivo determinar la efectividad de los métodos de recuperación de conocimientos básicos aplicados al nuevo conjunto de pruebas. El segundo tiene como objetivo evaluar modelos refinados que exploten las características distintivas del nuevo conjunto de pruebas, como la estructura organizativa de la universidad y la estructura jerárquica de los temas en el conjunto de pruebas. Los modelos de recuperación de experiencia se muestran robustos con respecto a entornos más pequeños que la colección de la W3C, y las técnicas actuales parecen ser generalizables a otros contextos. Categorías y Descriptores de Asignaturas H.3 [Almacenamiento y Recuperación de Información]: H.3.1 Análisis de Contenido e Indexación; H.3.3 Búsqueda y Recuperación de Información; H.3.4 Sistemas y Software; H.4 [Aplicaciones de Sistemas de Información]: H.4.2 Tipos de Sistemas; H.4.m Términos Generales Misceláneos Algoritmos, Medición, Rendimiento, Experimentación 1. INTRODUCCIÓN La intranet de una organización proporciona un medio para intercambiar información entre empleados y facilitar colaboraciones entre ellos. Para lograr esto de manera eficiente y efectiva, es necesario proporcionar instalaciones de búsqueda que permitan a los empleados no solo acceder a documentos, sino también identificar a colegas expertos. En la pista de Empresas de TREC [22] se ha reconocido la necesidad de estudiar y comprender la recuperación de expertos a través de la introducción de tareas de Búsqueda de Expertos. El objetivo de la búsqueda de expertos es identificar una lista de personas que tienen conocimientos sobre un tema específico. Esta tarea suele abordarse descubriendo asociaciones entre personas y temas [10]; comúnmente, se asume que la co-ocurrencia del nombre de una persona con temas en el mismo contexto es evidencia de experiencia. Una tarea alternativa, que utiliza la misma idea de asociaciones entre personas y temas, es el perfilado de expertos, donde la tarea consiste en devolver una lista de temas sobre los que una persona tiene conocimiento [3]. El lanzamiento de la tarea de <br>Búsqueda de Expertos</br> en TREC ha generado mucho interés en la recuperación de expertos, con un rápido progreso en términos de modelado, algoritmos y aspectos de evaluación. Sin embargo, casi todo el trabajo de búsqueda o perfilado de expertos realizado ha sido validado experimentalmente utilizando la colección de W3C [24] de la pista empresarial. Si bien esta colección es actualmente la única colección de pruebas disponible públicamente para tareas de recuperación de expertos, solo representa un tipo de intranet. Con solo una colección de pruebas no es posible generalizar conclusiones a otros escenarios realistas. En este artículo nos enfocamos en la recuperación de expertos en un entorno realista que difiere del entorno de la W3C, uno en el que hay cantidades relativamente pequeñas de datos limpios y multilingües disponibles, que abarcan una amplia gama de áreas de expertise, como se puede encontrar en las intranets de universidades y otras organizaciones intensivas en conocimiento. Normalmente, esta configuración incluye varios tipos adicionales de estructuras: estructura temática (por ejemplo, jerarquías de temas utilizadas por la organización), estructura organizativa (facultad, departamento, ...), así como múltiples tipos de documentos (descripciones de investigación y cursos, publicaciones y páginas web académicas). Este entorno es bastante diferente del entorno del W3C de maneras que podrían afectar el rendimiento de las tareas de recuperación de conocimientos especializados. Nos enfocamos en varias preguntas de investigación en este artículo: ¿La cantidad relativamente pequeña de datos disponibles en una intranet afecta la calidad de las asociaciones tema-persona que son fundamentales en los algoritmos de recuperación de expertos? ¿Cómo se desempeñan los algoritmos de vanguardia desarrollados en el conjunto de datos del W3C en el escenario alternativo del tipo descrito anteriormente? Más en general, ¿se pueden aplicar las lecciones de la <br>tarea de Búsqueda de Expertos</br> en TREC a este contexto? ¿Cómo afecta la inclusión o exclusión de diferentes documentos a las tareas de recuperación de expertos? Además, ¿cómo se puede utilizar la estructura temática y organizativa con fines de recuperación? Para responder a nuestras preguntas de investigación, primero presentamos un conjunto de enfoques de línea base, basados en modelado de lenguaje generativo, con el objetivo de encontrar asociaciones entre temas y personas. Esto nos permite formular las tareas de búsqueda de expertos y perfilado de expertos de manera uniforme, y tiene el beneficio adicional de permitirnos entender las relaciones entre las dos tareas. Para nuestra evaluación experimental, introducimos un nuevo conjunto de datos (la Colección de Expertos de UvT) que es representativo del tipo de intranet que describimos anteriormente. Nuestra colección se basa en datos de acceso público, extraídos del sitio web de la Universidad de Tilburg (UvT). Este tipo de datos es particularmente interesante, ya que (1) es limpio, heterogéneo, estructurado y enfocado, pero comprende un número limitado de documentos; (2) contiene información sobre la jerarquía organizativa; (3) es bilingüe (inglés y holandés); y (4) la lista de áreas de especialización de un individuo es proporcionada por los propios empleados. Usando la colección de expertos de UvT, llevamos a cabo dos conjuntos de experimentos. El primero tiene como objetivo determinar la efectividad de los métodos de búsqueda y perfilado de experiencia inicial en este nuevo entorno. Un segundo grupo de experimentos está dirigido a extensiones de los métodos base que aprovechan las características distintivas de la Colección de Expertos de UvT; específicamente, proponemos y evaluamos métodos refinados de búsqueda y perfilado de expertos que incorporan la topicalidad y la estructura organizativa. Además de las preguntas de investigación y el conjunto de datos que aportamos, nuestras principales contribuciones son las siguientes. Los modelos base desarrollados para encontrar expertos funcionan bien en el nuevo conjunto de datos. Mientras que en el entorno del W3C la <br>tarea de encontrar expertos</br> parece ser más difícil que el perfilado, para los datos de UvT ocurre lo contrario. Encontramos que el perfilado en el conjunto de datos UvT es considerablemente más difícil que en el conjunto de datos W3C, lo cual creemos que se debe al gran (pero realista) número de áreas temáticas que utilizamos para el perfilado: alrededor de 1,500 para el conjunto de datos UvT, frente a 50 en el caso de W3C. Teniendo en cuenta la similitud entre los temas puede mejorar significativamente el rendimiento de recuperación. Las medidas de similitud de mejor rendimiento son basadas en contenido, por lo tanto también se pueden aplicar en entornos de la W3C (y otros). Finalmente, demostramos que la estructura organizativa puede ser explotada en forma de un modelo de contexto, mejorando los puntajes de MAP para ciertos modelos hasta en un 70%. El resto de este documento está organizado de la siguiente manera. En la siguiente sección revisamos el trabajo relacionado. Luego, en la Sección 3 proporcionamos descripciones detalladas de las tareas de recuperación de expertos que abordamos en este artículo: la búsqueda de expertos y la creación de perfiles de expertos. En la Sección 4 presentamos nuestros modelos base, cuyo rendimiento luego es evaluado en la Sección 6 utilizando el conjunto de datos UvT que presentamos en la Sección 5. Los modelos avanzados que explotan características específicas de nuestros datos se presentan en la Sección 7 y se evalúan en la Sección 8. Formulamos nuestras conclusiones en la Sección 9.2. Trabajos relacionados Los enfoques iniciales para encontrar expertos a menudo empleaban bases de datos que contenían información sobre las habilidades y conocimientos de cada individuo en la organización [11]. La mayoría de estas herramientas (generalmente llamadas páginas amarillas o sistemas de búsqueda de personas) dependen de que las personas evalúen sus habilidades frente a un conjunto predefinido de palabras clave. Para actualizar perfiles en estos sistemas de forma automática, se necesita de tecnologías inteligentes [5]. Enfoques más recientes utilizan conjuntos de documentos específicos (como correos electrónicos [6] o software [18]) para encontrar experiencia. En contraste con centrarse en tipos de documentos particulares, también hay un interés creciente en el desarrollo de sistemas que indexan y extraen información de documentos publicados en intranets como fuentes de evidencia de experiencia. Un enfoque publicado es el sistema P@noptic [9], que construye una representación de cada persona concatenando todos los documentos asociados con esa persona, similar al Modelo 1 de Balog et al. [4], quienes formalizan y comparan dos métodos. El Modelo 1 de Balog et al. modela directamente el conocimiento de un experto a partir de documentos asociados, mientras que su Modelo 2 primero localiza documentos sobre el tema y luego encuentra a los expertos asociados. En los experimentos reportados, el segundo método tiene un rendimiento significativamente mejor cuando hay suficientes documentos asociados por candidato. La mayoría de los sistemas que participaron en las ediciones de 2005 y 2006 de la tarea de <br>Búsqueda de Expertos</br> en TREC implementaron (variaciones de) uno de estos dos modelos; ver [10, 20]. Macdonald y Ounis [16] proponen un enfoque diferente para clasificar la experiencia de los candidatos con respecto a un tema basado en técnicas de fusión de datos, sin utilizar heurísticas específicas de la colección; encuentran que la aplicación de modelos de ponderación basados en campos mejora la clasificación de los candidatos. Petkova y Croft [19] proponen otro enfoque, basado en una combinación de los Modelos 1 y 2 anteriores, modelando explícitamente los temas. Al abordar otras tareas de recuperación de expertos que también pueden abordarse utilizando asociaciones entre temas y personas, Balog y de Rijke [3] abordaron la tarea de determinar perfiles de expertos temáticos. Si bien sus métodos resultaron ser eficientes en el corpus de la W3C, requieren una cantidad de datos que puede que no esté disponible en la típica organización intensiva en conocimiento. Balog y de Rijke [2] estudian la tarea relacionada de encontrar expertos que sean similares a un pequeño conjunto de expertos dados como entrada. Por cierto, crear un resumen textual de una persona muestra algunas similitudes con la búsqueda de biografías, la cual ha recibido una considerable cantidad de atención recientemente; ver por ejemplo, [13]. Utilizamos modelado de lenguaje generativo para encontrar asociaciones entre temas y personas. En nuestro modelado de búsqueda y perfilado de expertos recopilamos evidencia de experiencia de múltiples fuentes, en una colección heterogénea, e integramos esta información con la co-ocurrencia de nombres de candidatos y términos de consulta: el entorno de modelado de lenguaje nos permite hacer esto de manera transparente. Nuestro modelado avanza en dos pasos. En el primer paso, consideramos tres modelos base, dos tomados de [4] (los Modelos 1 y 2 mencionados anteriormente), y uno una versión refinada de un modelo introducido en [3] (al que nos referimos como Modelo 3 a continuación); este tercer modelo también es similar al modelo descrito por Petkova y Croft [19]. Los modelos que consideramos en nuestra segunda ronda de experimentos son modelos de mezcla similares a los modelos de lenguaje contextual [1] y a los documentos ampliados de Tao et al. [21]; sin embargo, las características que utilizamos para definir nuestras expansiones, incluida la estructura temática y la estructura organizativa, no se han utilizado de esta manera antes. 3. En el escenario de recuperación de expertos que imaginamos, los usuarios que buscan expertos dentro de una organización tienen acceso a una interfaz que combina un cuadro de búsqueda (donde pueden buscar expertos o temas) con estructuras de navegación (de expertos y de temas) que les permite hacer clic hasta llegar a una página de experto (que proporciona el perfil de una persona) o a una página de tema (que proporciona una lista de expertos en el tema). Para alimentar la interfaz anterior, nos enfrentamos a dos tareas de recuperación de expertos, la búsqueda de expertos y el perfilado de expertos, que primero definimos y luego formalizamos utilizando modelos de lenguaje generativos. Para modelar cualquiera de las tareas, la probabilidad de que el tema de la consulta esté asociado a un experto candidato juega un papel clave en las estimaciones finales para la búsqueda y el perfilado. Al utilizar modelos de lenguaje, tanto los candidatos como la consulta se caracterizan por distribuciones de términos en el vocabulario (utilizado en los documentos puestos a disposición por la organización cuyas necesidades de recuperación de experiencia estamos abordando). 3.1 Búsqueda de expertos La búsqueda de expertos implica la tarea de encontrar a la persona adecuada con las habilidades y conocimientos apropiados: ¿Quiénes son los expertos en el tema X?. Por ejemplo, un empleado quiere averiguar quién trabajó en un proyecto en particular para descubrir por qué se tomaron decisiones específicas sin tener que revisar minuciosamente la documentación (si es que la hay). O pueden necesitar un especialista capacitado para consultoría sobre un problema específico. Dentro de una organización, generalmente hay muchos posibles candidatos que podrían ser expertos en un tema dado. Podemos plantear este problema de la siguiente manera: ¿Cuál es la probabilidad de que un candidato ca sea un experto dado el tema de consulta q? Es decir, determinamos p(ca|q) y clasificamos a los candidatos ca de acuerdo con esta probabilidad. Los candidatos con la probabilidad más alta dada la consulta son considerados los expertos más probables en ese tema. El desafío es cómo estimar esta probabilidad con precisión. Dado que es probable que la consulta consista solo en unos pocos términos para describir la experiencia requerida, deberíamos poder obtener una estimación más precisa al invocar el Teorema de Bayes, y estimar: p(ca|q) = p(q|ca)p(ca) p(q) , (1) donde p(ca) es la probabilidad de un candidato y p(q) es la probabilidad de una consulta. Dado que p(q) es una constante, se puede ignorar para fines de clasificación. Por lo tanto, la probabilidad de que un candidato ca sea un experto dado la consulta q es proporcional a la probabilidad de una consulta dada el candidato p(q|ca), ponderada por la creencia a priori p(ca) de que el candidato ca es un experto. p(ca|q) ∝ p(q|ca)p(ca) (2) En este artículo, nuestro enfoque principal se centra en estimar la probabilidad de una consulta dada el candidato p(q|ca), ya que esta probabilidad captura en qué medida el candidato conoce sobre el tema de la consulta. Si bien se asume generalmente que los priors del candidato son uniformes y, por lo tanto, no influirán en la clasificación, se ha demostrado que una elección sensata de priors puede mejorar el rendimiento [20]. Perfilado de expertos. Mientras que la tarea de búsqueda de expertos se centraba en encontrar expertos en un tema específico, la tarea de perfilado de expertos busca responder a una pregunta relacionada: ¿Sobre qué temas tiene conocimiento un candidato? Básicamente, esto da un giro a las preguntas sobre la búsqueda de expertos. El perfilado de un candidato individual implica la identificación de áreas de habilidades y conocimientos en las que tienen experiencia, así como una evaluación del nivel de competencia en cada una de estas áreas. Este es el perfil temático de los candidatos. Generalmente, los perfiles temáticos dentro de las organizaciones consisten en estructuras tabulares que catalogan explícitamente las habilidades y conocimientos de cada individuo en la organización. Sin embargo, esta práctica está limitada por los recursos disponibles para definir, crear, mantener y actualizar estos perfiles con el tiempo. Al centrarnos en métodos automáticos que se basan en la evidencia disponible dentro de los repositorios de documentos de una organización, nuestro objetivo es reducir el esfuerzo humano asociado con el mantenimiento de perfiles temáticos. Un perfil temático de un candidato, entonces, se define como un vector donde cada elemento i del vector corresponde a la experiencia del candidato en un tema dado ki, (es decir, s(ca, ki)). Cada tema ki define un área de conocimiento o habilidad particular que la organización utiliza para definir el perfil temático de los candidatos. Por lo tanto, se asume que se proporciona una lista de temas, {k1, . . . , kn}, donde n es el número de temas predefinidos: perfil(ca) = s(ca, k1), s(ca, k2), . . . , s(ca, kn) . Se necesita contexto y evidencia para ayudar a los usuarios de sistemas de búsqueda de expertos a decidir a quién contactar al buscar experiencia en un área particular. Ejemplos de dicho contexto son: ¿Con quién trabaja ella? ¿Cuáles son sus datos de contacto? ¿Está bien conectada, por si acaso no puede ayudarnos ella misma? ¿Cuál es su rol en la organización? ¿Quién es su superior? Los colaboradores, afiliaciones, etc. son parte del perfil social de los candidatos y pueden servir como antecedentes para interpretar las recomendaciones del sistema. En este documento solo abordamos el problema de determinar perfiles temáticos, dejando el perfilado social para trabajos futuros. Planteamos el problema de cuantificar la competencia de una persona en un área de conocimiento específica de la siguiente manera: ¿Cuál es la probabilidad de que un área de conocimiento (ki) forme parte del perfil de candidatos (expertise)? donde s(ca, ki) está definido por p(ki|ca). Nuestra tarea, entonces, es estimar p(ki|ca), lo cual es equivalente al problema de obtener p(q|ca), donde el tema ki se representa como un tema de consulta q, es decir, una secuencia de palabras clave que representan la experiencia requerida. Tanto la tarea de encontrar expertos como la de perfilar expertos dependen de la estimación precisa de p(q|ca). La única diferencia proviene de la probabilidad previa de que una persona sea experta (p(ca)), la cual puede ser incorporada en la <br>tarea de encontrar expertos</br>. ",
            "candidates": [],
            "error": [
                [
                    "Búsqueda de Expertos",
                    "tarea de Búsqueda de Expertos",
                    "tarea de encontrar expertos",
                    "Búsqueda de Expertos",
                    "tarea de encontrar expertos"
                ]
            ]
        },
        "co-occurrence": {
            "translated_key": "co-ocurrencia",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Broad Expertise Retrieval in Sparse Data Environments Krisztian Balog ISLA, University of Amsterdam Kruislaan 403, 1098 SJ Amsterdam, The Netherlands kbalog@science.uva.nl Toine Bogers ILK, Tilburg University P.O.",
                "Box 90153, 5000 LE Tilburg, The Netherlands A.M.Bogers@uvt.nl Leif Azzopardi Dept. of Computing Science University of Glasgow, Glasgow, G12 8QQ leif@dcs.gla.ac.uk Maarten de Rijke ISLA, University of Amsterdam Kruislaan 403, 1098 SJ Amsterdam, The Netherlands mdr@science.uva.nl Antal van den Bosch ILK, Tilburg University P.O.",
                "Box 90153, 5000 LE Tilburg, The Netherlands Antal.vdnBosch@uvt.nl ABSTRACT Expertise retrieval has been largely unexplored on data other than the W3C collection.",
                "At the same time, many intranets of universities and other knowledge-intensive organisations offer examples of relatively small but clean multilingual expertise data, covering broad ranges of expertise areas.",
                "We first present two main expertise retrieval tasks, along with a set of baseline approaches based on generative language modeling, aimed at finding expertise relations between topics and people.",
                "For our experimental evaluation, we introduce (and release) a new test set based on a crawl of a university site.",
                "Using this test set, we conduct two series of experiments.",
                "The first is aimed at determining the effectiveness of baseline expertise retrieval methods applied to the new test set.",
                "The second is aimed at assessing refined models that exploit characteristic features of the new test set, such as the organizational structure of the university, and the hierarchical structure of the topics in the test set.",
                "Expertise retrieval models are shown to be robust with respect to environments smaller than the W3C collection, and current techniques appear to be generalizable to other settings.",
                "Categories and Subject Descriptors H.3 [Information Storage and Retrieval]: H.3.1 Content Analysis and Indexing; H.3.3 Information Search and Retrieval; H.3.4 Systems and Software; H.4 [Information Systems Applications]: H.4.2 Types of Systems; H.4.m Miscellaneous General Terms Algorithms, Measurement, Performance, Experimentation 1.",
                "INTRODUCTION An organizations intranet provides a means for exchanging information between employees and for facilitating employee collaborations.",
                "To efficiently and effectively achieve this, it is necessary to provide search facilities that enable employees not only to access documents, but also to identify expert colleagues.",
                "At the TREC Enterprise Track [22] the need to study and understand expertise retrieval has been recognized through the introduction of Expert Finding tasks.",
                "The goal of expert finding is to identify a list of people who are knowledgeable about a given topic.",
                "This task is usually addressed by uncovering associations between people and topics [10]; commonly, a <br>co-occurrence</br> of the name of a person with topics in the same context is assumed to be evidence of expertise.",
                "An alternative task, which using the same idea of people-topic associations, is expert profiling, where the task is to return a list of topics that a person is knowledgeable about [3].",
                "The launch of the Expert Finding task at TREC has generated a lot of interest in expertise retrieval, with rapid progress being made in terms of modeling, algorithms, and evaluation aspects.",
                "However, nearly all of the expert finding or profiling work performed has been validated experimentally using the W3C collection [24] from the Enterprise Track.",
                "While this collection is currently the only publicly available test collection for expertise retrieval tasks, it only represents one type of intranet.",
                "With only one test collection it is not possible to generalize conclusions to other realistic settings.",
                "In this paper we focus on expertise retrieval in a realistic setting that differs from the W3C setting-one in which relatively small amounts of clean, multilingual data are available, that cover a broad range of expertise areas, as can be found on the intranets of universities and other knowledge-intensive organizations.",
                "Typically, this setting features several additional types of structure: topical structure (e.g., topic hierarchies as employed by the organization), organizational structure (faculty, department, ...), as well as multiple types of documents (research and course descriptions, publications, and academic homepages).",
                "This setting is quite different from the W3C setting in ways that might impact upon the performance of expertise retrieval tasks.",
                "We focus on a number of research questions in this paper: Does the relatively small amount of data available on an intranet affect the quality of the topic-person associations that lie at the heart of expertise retrieval algorithms?",
                "How do state-of-the-art algorithms developed on the W3C data set perform in the alternative scenario of the type described above?",
                "More generally, do the lessons from the Expert Finding task at TREC carry over to this setting?",
                "How does the inclusion or exclusion of different documents affect expertise retrieval tasks?",
                "In addition to, how can the topical and organizational structure be used for retrieval purposes?",
                "To answer our research questions, we first present a set of baseline approaches, based on generative language modeling, aimed at finding associations between topics and people.",
                "This allows us to formulate the expert finding and expert profiling tasks in a uniform way, and has the added benefit of allowing us to understand the relations between the two tasks.",
                "For our experimental evaluation, we introduce a new data set (the UvT Expert Collection) which is representative of the type of intranet that we described above.",
                "Our collection is based on publicly available data, crawled from the website of Tilburg University (UvT).",
                "This type of data is particularly interesting, since (1) it is clean, heterogeneous, structured, and focused, but comprises a limited number of documents; (2) contains information on the organizational hierarchy; (3) it is bilingual (English and Dutch); and (4) the list of expertise areas of an individual are provided by the employees themselves.",
                "Using the UvT Expert collection, we conduct two sets of experiments.",
                "The first is aimed at determining the effectiveness of baseline expertise finding and profiling methods in this new setting.",
                "A second group of experiments is aimed at extensions of the baseline methods that exploit characteristic features of the UvT Expert Collection; specifically, we propose and evaluate refined expert finding and profiling methods that incorporate topicality and organizational structure.",
                "Apart from the research questions and data set that we contribute, our main contributions are as follows.",
                "The baseline models developed for expertise finding perform well on the new data set.",
                "While on the W3C setting the expert finding task appears to be more difficult than profiling, for the UvT data the opposite is the case.",
                "We find that profiling on the UvT data set is considerably more difficult than on the W3C set, which we believe is due to the large (but realistic) number of topical areas that we used for profiling: about 1,500 for the UvT set, versus 50 in the W3C case.",
                "Taking the similarity between topics into account can significantly improve retrieval performance.",
                "The best performing similarity measures are content-based, therefore they can be applied on the W3C (and other) settings as well.",
                "Finally, we demonstrate that the organizational structure can be exploited in the form of a context model, improving MAP scores for certain models by up to 70%.",
                "The remainder of this paper is organized as follows.",
                "In the next section we review related work.",
                "Then, in Section 3 we provide detailed descriptions of the expertise retrieval tasks that we address in this paper: expert finding and expert profiling.",
                "In Section 4 we present our baseline models, of which the performance is then assessed in Section 6 using the UvT data set that we introduce in Section 5.",
                "Advanced models exploiting specific features of our data are presented in Section 7 and evaluated in Section 8.",
                "We formulate our conclusions in Section 9. 2.",
                "RELATED WORK Initial approaches to expertise finding often employed databases containing information on the skills and knowledge of each individual in the organization [11].",
                "Most of these tools (usually called yellow pages or people-finding systems) rely on people to self-assess their skills against a predefined set of keywords.",
                "For updating profiles in these systems in an automatic fashion there is a need for intelligent technologies [5].",
                "More recent approaches use specific document sets (such as email [6] or software [18]) to find expertise.",
                "In contrast with focusing on particular document types, there is also an increased interest in the development of systems that index and mine published intranet documents as sources of evidence for expertise.",
                "One such published approach is the P@noptic system [9], which builds a representation of each person by concatenating all documents associated with that person-this is similar to Model 1 of Balog et al. [4], who formalize and compare two methods.",
                "Balog et al.s Model 1 directly models the knowledge of an expert from associated documents, while their Model 2 first locates documents on the topic and then finds the associated experts.",
                "In the reported experiments the second method performs significantly better when there are sufficiently many associated documents per candidate.",
                "Most systems that took part in the 2005 and 2006 editions of the Expert Finding task at TREC implemented (variations on) one of these two models; see [10, 20].",
                "Macdonald and Ounis [16] propose a different approach for ranking candidate expertise with respect to a topic based on data fusion techniques, without using collectionspecific heuristics; they find that applying field-based weighting models improves the ranking of candidates.",
                "Petkova and Croft [19] propose yet another approach, based on a combination of the above Model 1 and 2, explicitly modeling topics.",
                "Turning to other expert retrieval tasks that can also be addressed using topic-people associations, Balog and de Rijke [3] addressed the task of determining topical expert profiles.",
                "While their methods proved to be efficient on the W3C corpus, they require an amount of data that may not be available in the typical knowledge-intensive organization.",
                "Balog and de Rijke [2] study the related task of finding experts that are similar to a small set of experts given as input.",
                "As an aside, creating a textual summary of a person shows some similarities to biography finding, which has received a considerable amount of attention recently; see e.g., [13].",
                "We use generative language modeling to find associations between topics and people.",
                "In our modeling of expert finding and profiling we collect evidence for expertise from multiple sources, in a heterogeneous collection, and integrate it with the <br>co-occurrence</br> of candidates names and query terms-the language modeling setting allows us to do this in a transparent manner.",
                "Our modeling proceeds in two steps.",
                "In the first step, we consider three baseline models, two taken from [4] (the Models 1 and 2 mentioned above), and one a refined version of a model introduced in [3] (which we refer to as Model 3 below); this third model is also similar to the model described by Petkova and Croft [19].",
                "The models we consider in our second round of experiments are mixture models similar to contextual language models [1] and to the expanded documents of Tao et al. [21]; however, the features that we use for definining our expansions-including topical structure and organizational structure-have not been used in this way before. 3.",
                "TASKS In the expertise retrieval scenario that we envisage, users seeking expertise within an organization have access to an interface that combines a search box (where they can search for experts or topics) with navigational structures (of experts and of topics) that allows them to click their way to an expert page (providing the profile of a person) or a topic page (providing a list of experts on the topic).",
                "To feed the above interface, we face two expertise retrieval tasks, expert finding and expert profiling, that we first define and then formalize using generative language models.",
                "In order to model either task, the probability of the query topic being associated to a candidate expert plays a key role in the final estimates for searching and profiling.",
                "By using language models, both the candidates and the query are characterized by distributions of terms in the vocabulary (used in the documents made available by the organization whose expertise retrieval needs we are addressing). 3.1 Expert finding Expert finding involves the task of finding the right person with the appropriate skills and knowledge: Who are the experts on topic X?.",
                "E.g., an employee wants to ascertain who worked on a particular project to find out why particular decisions were made without having to trawl through documentation (if there is any).",
                "Or, they may be in need a trained specialist for consultancy on a specific problem.",
                "Within an organization there are usually many possible candidates who could be experts for given topic.",
                "We can state this problem as follows: What is the probability of a candidate ca being an expert given the query topic q?",
                "That is, we determine p(ca|q), and rank candidates ca according to this probability.",
                "The candidates with the highest probability given the query are deemed the most likely experts for that topic.",
                "The challenge is how to estimate this probability accurately.",
                "Since the query is likely to consist of only a few terms to describe the expertise required, we should be able to obtain a more accurate estimate by invoking Bayes Theorem, and estimating: p(ca|q) = p(q|ca)p(ca) p(q) , (1) where p(ca) is the probability of a candidate and p(q) is the probability of a query.",
                "Since p(q) is a constant, it can be ignored for ranking purposes.",
                "Thus, the probability of a candidate ca being an expert given the query q is proportional to the probability of a query given the candidate p(q|ca), weighted by the a priori belief p(ca) that candidate ca is an expert. p(ca|q) ∝ p(q|ca)p(ca) (2) In this paper our main focus is on estimating the probability of a query given the candidate p(q|ca), because this probability captures the extent to which the candidate knows about the query topic.",
                "Whereas the candidate priors are generally assumed to be uniformand thus will not influence the ranking-it has been demonstrated that a sensible choice of priors may improve the performance [20]. 3.2 Expert profiling While the task of expert searching was concerned with finding experts given a particular topic, the task of expert profiling seeks to answer a related question: What topics does a candidate know about?",
                "Essentially, this turns the questions of expert finding around.",
                "The profiling of an individual candidate involves the identification of areas of skills and knowledge that they have expertise about and an evaluation of the level of proficiency in each of these areas.",
                "This is the candidates topical profile.",
                "Generally, topical profiles within organizations consist of tabular structures which explicitly catalogue the skills and knowledge of each individual in the organization.",
                "However, such practice is limited by the resources available for defining, creating, maintaining, and updating these profiles over time.",
                "By focusing on automatic methods which draw upon the available evidence within the document repositories of an organization, our aim is to reduce the human effort associated with the maintenance of topical profiles1 .",
                "A topical profile of a candidate, then, is defined as a vector where each element i of the vector corresponds to the candidate cas expertise on a given topic ki, (i.e., s(ca, ki)).",
                "Each topic ki defines a particular knowledge area or skill that the organization uses to define the candidates topical profile.",
                "Thus, it is assumed that a list of topics, {k1, . . . , kn}, where n is the number of pre-defined topics, is given: profile(ca) = s(ca, k1), s(ca, k2), . . . , s(ca, kn) . (3) 1 Context and evidence are needed to help users of expertise finding systems to decide whom to contact when seeking expertise in a particular area.",
                "Examples of such context are: Who does she work with?",
                "What are her contact details?",
                "Is she well-connected, just in case she is not able to help us herself?",
                "What is her role in the organization?",
                "Who is her superior?",
                "Collaborators, and affiliations, etc. are all part of the candidates social profile, and can serve as a background against which the systems recommendations should be interpreted.",
                "In this paper we only address the problem of determining topical profiles, and leave social profiling to further work.",
                "We state the problem of quantifying the competence of a person on a certain knowledge area as follows: What is the probability of a knowledge area (ki) being part of the candidates (expertise) profile? where s(ca, ki) is defined by p(ki|ca).",
                "Our task, then, is to estimate p(ki|ca), which is equivalent to the problem of obtaining p(q|ca), where the topic ki is represented as a query topic q, i.e., a sequence of keywords representing the expertise required.",
                "Both the expert finding and profiling tasks rely on the accurate estimation of p(q|ca).",
                "The only difference derives from the prior probability that a person is an expert (p(ca)), which can be incorporated into the expert finding task.",
                "This prior does not apply to the profiling task since the candidate (individual) is fixed. 4.",
                "BASELINE MODELS In this section we describe our baseline models for estimating p(q|ca), i.e., associations between topics and people.",
                "Both expert finding and expert profiling boil down to this estimation.",
                "We employ three models for calculating this probability. 4.1 From topics to candidates Using Candidate Models: Model 1 Model 1 [4] defines the probability of a query given a candidate (p(q|ca)) using standard language modeling techniques, based on a multinomial unigram language model.",
                "For each candidate ca, a candidate language model θca is inferred such that the probability of a term given θca is nonzero for all terms, i.e., p(t|θca) > 0.",
                "From the candidate model the query is generated with the following probability: p(q|θca) = Y t∈q p(t|θca)n(t,q) , where each term t in the query q is sampled identically and independently, and n(t, q) is the number of times t occurs in q.",
                "The candidate language model is inferred as follows: (1) an empirical model p(t|ca) is computed; (2) it is smoothed with background probabilities.",
                "Using the associations between a candidate and a document, the probability p(t|ca) can be approximated by: p(t|ca) = X d p(t|d)p(d|ca), where p(d|ca) is the probability that candidate ca generates a supporting document d, and p(t|d) is the probability of a term t occurring in the document d. We use the maximum-likelihood estimate of a term, that is, the normalised frequency of the term t in document d. The strength of the association between document d and candidate ca expressed by p(d|ca) reflects the degree to which the candidates expertise is described using this document.",
                "The estimation of this probability is presented later, in Section 4.2.",
                "The candidate model is then constructed as a linear interpolation of p(t|ca) and the background model p(t) to ensure there are no zero probabilities, which results in the final estimation: p(q|θca) = (4) Y t∈q ( (1 − λ) X d p(t|d)p(d|ca) ! + λp(t) )n(t,q) .",
                "Model 1 amasses all the term information from all the documents associated with the candidate, and uses this to represent that candidate.",
                "This model is used to predict how likely a candidate would produce a query q.",
                "This can can be intuitively interpreted as the probability of this candidate talking about the query topic, where we assume that this is indicative of their expertise.",
                "Using Document Models: Model 2 Model 2 [4] takes a different approach.",
                "Here, the process is broken into two parts.",
                "Given a candidate ca, (1) a document that is associated with a candidate is selected with probability p(d|ca), and (2) from this document a query q is generated with probability p(q|d).",
                "Then the sum over all documents is taken to obtain p(q|ca), such that: p(q|ca) = X d p(q|d)p(d|ca). (5) The probability of a query given a document is estimated by inferring a document language model θd for each document d in a similar manner as the candidate model was inferred: p(t|θd) = (1 − λ)p(t|d) + λp(t), (6) where p(t|d) is the probability of the term in the document.",
                "The probability of a query given the document model is: p(q|θd) = Y t∈q p(t|θd)n(t,q) .",
                "The final estimate of p(q|ca) is obtained by substituting p(q|d) for p(q|θd) into Eq. 5 (see [4] for full details).",
                "Conceptually, Model 2 differs from Model 1 because the candidate is not directly modeled.",
                "Instead, the document acts like a hidden variable in the process which separates the query from the candidate.",
                "This process is akin to how a user may search for candidates with a standard search engine: initially by finding the documents which are relevant, and then seeing who is associated with that document.",
                "By examining a number of documents the user can obtain an idea of which candidates are more likely to discuss the topic q.",
                "Using Topic Models: Model 3 We introduce a third model, Model 3.",
                "Instead of attempting to model the query generation process via candidate or document models, we represent the query as a topic language model and directly estimate the probability of the candidate p(ca|q).",
                "This approach is similar to the model presented in [3, 19].",
                "As with the previous models, a language model is inferred, but this time for the query.",
                "We adapt the work of Lavrenko and Croft [14] to estimate a topic model from the query.",
                "The procedure is as follows.",
                "Given a collection of documents and a query topic q, it is assumed that there exists an unknown topic model θk that assigns probabilities p(t|θk) to the term occurrences in the topic documents.",
                "Both the query and the documents are samples from θk (as opposed to the previous approaches, where a query is assumed to be sampled from a specific document or candidate model).",
                "The main task is to estimate p(t|θk), the probability of a term given the topic model.",
                "Since the query q is very sparse, and as there are no examples of documents on the topic, this distribution needs to be approximated.",
                "Lavrenko and Croft [14] suggest a reasonable way of obtaining such an approximation, by assuming that p(t|θk) can be approximated by the probability of term t given the query q.",
                "We can then estimate p(t|q) using the joint probability of observing the term t together with the query terms, q1, . . . , qm, and dividing by the joint probability of the query terms: p(t|θk) ≈ p(t|q) = p(t, q1, . . . , qm) p(q1, . . . , qm) = p(t, q1, . . . , qm) P t ∈T p(t , q1, . . . , qm) , where p(q1, . . . , qm) = P t ∈T p(t , q1, . . . , qm), and T is the entire vocabulary of terms.",
                "In order to estimate the joint probability p(t, q1, . . . , qm), we follow [14, 15] and assume t and q1, . . . , qm are mutually independent, once we pick a source distribution from the set of underlying source distributions U.",
                "If we choose U to be a set of document models. then to construct this set, the query q would be issued against the collection, and the top n returned are assumed to be relevant to the topic, and thus treated as samples from the topic model. (Note that candidate models could be used instead.)",
                "With the document models forming U, the joint probability of term and query becomes: p(t, q1, . . . , qm) = X d∈U p(d) ˘ p(t|θd) mY i=1 p(qi|θd) ¯ . (7) Here, p(d) denotes the prior distribution over the set U, which reflects the relevance of the document to the topic.",
                "We assume that p(d) is uniform across U.",
                "In order to rank candidates according to the topic model defined, we use the Kullback-Leibler divergence metric (KL, [8]) to measure the difference between the candidate models and the topic model: KL(θk||θca) = X t p(t|θk) log p(t|θk) p(t|θca) . (8) Candidates with a smaller divergence from the topic model are considered to be more likely experts on that topic.",
                "The candidate model θca is defined in Eq. 4.",
                "By using KL divergence instead of the probability of a candidate given the topic model p(ca|θk), we avoid normalization problems. 4.2 Document-candidate associations For our models we need to be able to estimate the probability p(d|ca), which expresses the extent to which a document d characterizes the candidate ca.",
                "In [4], two methods are presented for estimating this probability, based on the number of person names recognized in a document.",
                "However, in our (intranet) setting it is reasonable to assume that authors of documents can unambiguously be identified (e.g., as the author of an article, the teacher assigned to a course, the owner of a web page, etc.)",
                "Hence, we set p(d|ca) to be 1 if candidate ca is author of document d, otherwise the probability is 0.",
                "In Section 6 we describe how authorship can be determined on different types of documents within the collection. 5.",
                "THE UVT EXPERT COLLECTION The UvT Expert collection used in the experiments in this paper fits the scenario outlined in Section 3.",
                "The collection is based on the Webwijs (Webwise) system developed at Tilburg University (UvT) in the Netherlands.",
                "Webwijs (http://www.uvt.nl/ webwijs/) is a publicly accessible database of UvT employees who are involved in research or teaching; currently, Webwijs contains information about 1168 experts, each of whom has a page with contact information and, if made available by the expert, a research description and publications list.",
                "In addition, each expert can select expertise areas from a list of 1491 topics and is encouraged to suggest new topics that need to be approved by the Webwijs editor.",
                "Each topic has a separate page that shows all experts associated with that topic and, if available, a list of related topics.",
                "Webwijs is available in Dutch and English, and this bilinguality has been preserved in the collection.",
                "Every Dutch Webwijs page has an English translation.",
                "Not all Dutch topics have an English translation, but the reverse is true: the 981 English topics all have a Dutch equivalent.",
                "About 42% of the experts teach courses at Tilburg University; these courses were also crawled and included in the profile.",
                "In addition, about 27% of the experts link to their academic homepage from their Webwijs page.",
                "These home pages were crawled and added to the collection. (This means that if experts put the full-text versions of their publications on their academic homepage, these were also available for indexing.)",
                "We also obtained 1880 full-text versions of publications from the UvT institutional repository and Dutch English no. of experts 1168 1168 no. of experts with ≥ 1 topic 743 727 no. of topics 1491 981 no. of expert-topic pairs 4318 3251 avg. no. of topics/expert 5.8 5.9 max. no. of topics/expert (no. of experts) 60 (1) 35 (1) min. no. of topics/expert (no. of experts) 1 (74) 1 (106) avg. no. of experts/topic 2.9 3.3 max. no. of experts/topic (no. of topics) 30 (1) 30 (1) min. no. of experts/topic (no. of topics) 1 (615) 1 (346) no. of experts with HP 318 318 no. of experts with CD 318 318 avg. no. of CDs per teaching expert 3.5 3.5 no. of experts with RD 329 313 no. of experts with PUB 734 734 avg. no. of PUBs per expert 27.0 27.0 avg. no. of PUB citations per expert 25.2 25.2 avg. no. of full-text PUBs per expert 1.8 1.8 Table 2: Descriptive statistics of the Dutch and English versions of the UvT Expert collection. converted them to plain text.",
                "We ran the TextCat [23] language identifier to classify the language of the home pages and the fulltext publications.",
                "We restricted ourselves to pages where the classifier was confident about the language used on the page.",
                "This resulted in four document types: research descriptions (RD), course descriptions (CD), publications (PUB; full-text and citationonly versions), and academic homepages (HP).",
                "Everything was bundled into the UvT Expert collection which is available at http: //ilk.uvt.nl/uvt-expert-collection/.",
                "The UvT Expert collection was extracted from a different organizational setting than the W3C collection and differs from it in a number of ways.",
                "The UvT setting is one with relatively small amounts of multilingual data.",
                "Document-author associations are clear and the data is structured and clean.",
                "The collection covers a broad range of expertise areas, as one can typically find on intranets of universities and other knowledge-intensive institutes.",
                "Additionally, our university setting features several types of structure (topical and organizational), as well as multiple document types.",
                "Another important difference between the two data sets is that the expertise areas in the UvT Expert collection are self-selected instead of being based on group membership or assignments by others.",
                "Size is another dimension along which the W3C and UvT Expert collections differ: the latter is the smaller of the two.",
                "Also realistic are the large differences in the amount of information available for each expert.",
                "Utilizing Webwijs is voluntary; 425 Dutch experts did not select any topics at all.",
                "This leaves us with 743 Dutch and 727 English usable expert profiles.",
                "Table 2 provides descriptive statistics for the UvT Expert collection.",
                "Universities tend to have a hierarchical structure that goes from the faculty level, to departments, research groups, down to the individual researchers.",
                "In the UvT Expert collection we have information about the affiliations of researchers with faculties and institutes, providing us with a two-level organizational hierarchy.",
                "Tilburg University has 22 organizational units at the faculty level (including the university office and several research institutes) and 71 departments, which amounts to 3.2 departments per faculty.",
                "As to the topical hierarchy used by Webwijs, 131 of the 1491 topics are top nodes in the hierarchy.",
                "This hierarchy has an average topic chain length of 2.65 and a maximum length of 7 topics. 6.",
                "EVALUATION Below, we evaluate Section 4s models for expert finding and profiling onthe UvT Expert collection.",
                "We detail our research questions and experimental setup, and then present our results. 6.1 Research Questions We address the following research questions.",
                "Both expert finding and profiling rely on the estimations of p(q|ca).",
                "The question is how the models compare on the different tasks, and in the setting of the UvT Expert collection.",
                "In [4], Model 2 outperformed Model 1 on the W3C collection.",
                "How do they compare on our data set?",
                "And how does Model 3 compare to Model 1?",
                "What about performance differences between the two languages in our test collection? 6.2 Experimental Setup The output of our models was evaluated against the self-assigned topic labels, which were treated as relevance judgements.",
                "Results were evaluated separately for English and Dutch.",
                "For English we only used topics for which the Dutch translation was available; for Dutch all topics were considered.",
                "The results were averaged for the queries in the intersection of relevance judgements and results; missing queries do not contribute a value of 0 to the scores.",
                "We use standard information retrieval measures, such as Mean Average Precision (MAP) and Mean Reciprocal Rank (MRR).",
                "We also report the percentage of topics (%q) and candidates (%ca) covered, for the expert finding and profiling tasks, respectively. 6.3 Results Table 1 shows the performance of Model 1, 2, and 3 on the expert finding and profiling tasks.",
                "The rows of the table correspond to the various document types (RD, CD, PUB, and HP) and to their combinations.",
                "RD+CD+PUB+HP is equivalent to the full collection and will be referred as the BASELINE of our experiments.",
                "Looking at Table 1 we see that Model 2 performs the best across the board.",
                "However, when the data is clean and very focused (RD), Model 3 outperforms it in a number of cases.",
                "Model 1 has the best coverage of candidates (%ca) and topics (%q).",
                "The various document types differ in their characteristics and how they improve the finding and profiling tasks.",
                "Expert profiling benefits much from the clean data present in the RD and CD document types, while the publications contribute the most to the expert finding task.",
                "Adding the homepages does not prove to be particularly useful.",
                "When we compare the results across languages, we find that the coverage of English topics (%q) is higher than of the Dutch ones for expert finding.",
                "Apart from that, the scores fall in the same range for both languages.",
                "For the profiling task the coverage of the candidates (%ca) is very similar for both languages.",
                "However, the performance is substantially better for the English topics.",
                "While it is hard to compare scores across collections, we conclude with a brief comparison of the absolute scores in Table 1 to those reported in [3, 4] on the W3C test set (2005 edition).",
                "For expert finding the MAP scores for Model 2 reported here are about 50% higher than the corresponding figures in [4], while our MRR scores are slightly below those in [4].",
                "For expert profiling, the differences are far more dramatic: the MAP scores for Model 2 reported here are around 50% below the scores in [3], while the (best) MRR scores are about the same as those in [3].",
                "The cause for the latter differences seems to reside in the number of knowledge areas considered here-approx. 30 times more than in the W3C setting. 7.",
                "ADVANCED MODELS Now that we have developed and assessed basic language modeling techniques for expertise retrieval, we turn to refined models that exploit special features of our test collection. 7.1 Exploiting knowledge area similarity One way to improve the scoring of a query given a candidate is to consider what other requests the candidate would satisfy and use them as further evidence to support the original query, proportional Expert finding Expert profiling Document types Model 1 Model 2 Model 3 Model 1 Model 2 Model 3 %q MAP MRR %q MAP MRR %q MAP MRR %ca MAP MRR %ca MAP MRR %ca MAP MRR English RD 97.8 0.126 0.269 83.5 0.144 0.311 83.3 0.129 0.271 100 0.089 0.189 39.3 0.232 0.465 41.1 0.166 0.337 CD 97.8 0.118 0.227 91.7 0.123 0.248 91.7 0.118 0.226 32.8 0.188 0.381 32.4 0.195 0.385 32.7 0.203 0.370 PUB 97.8 0.200 0.330 98.0 0.216 0.372 98.0 0.145 0.257 78.9 0.167 0.364 74.5 0.212 0.442 78.9 0.135 0.299 HP 97.8 0.081 0.186 97.4 0.071 0.168 97.2 0.062 0.149 31.2 0.150 0.299 28.8 0.185 0.335 30.1 0.136 0.287 RD+CD 97.8 0.188 0.352 92.9 0.193 0.360 92.9 0.150 0.273 100 0.145 0.286 61.3 0.251 0.477 63.2 0.217 0.416 RD+CD+PUB 97.8 0.235 0.373 98.1 0.277 0.439 98.1 0.178 0.305 100 0.196 0.380 87.2 0.280 0.533 89.5 0.170 0.344 RD+CD+PUB+HP 97.8 0.237 0.372 98.6 0.280 0.441 98.5 0.166 0.293 100 0.199 0.387 88.7 0.281 0.525 90.9 0.169 0.329 Dutch RD 61.3 0.094 0.229 38.4 0.137 0.336 38.3 0.127 0.295 38.0 0.127 0.386 34.1 0.138 0.420 38.0 0.105 0.327 CD 61.3 0.107 0.212 49.7 0.128 0.256 49.7 0.136 0.261 32.5 0.151 0.389 31.8 0.158 0.396 32.5 0.170 0.380 PUB 61.3 0.193 0.319 59.5 0.218 0.368 59.4 0.173 0.291 78.8 0.126 0.364 76.0 0.150 0.424 78.8 0.103 0.294 HP 61.3 0.063 0.169 56.6 0.064 0.175 56.4 0.062 0.163 29.8 0.108 0.308 27.8 0.125 0.338 29.8 0.098 0.255 RD+CD 61.3 0.159 0.314 51.9 0.184 0.360 51.9 0.169 0.324 60.5 0.151 0.410 57.2 0.166 0.431 60.4 0.159 0.384 RD+CD+PUB 61.3 0.244 0.398 61.5 0.260 0.424 61.4 0.210 0.350 90.3 0.165 0.445 88.2 0.189 0.479 90.3 0.126 0.339 RD+CD+PUB+HP 61.3 0.249 0.401 62.6 0.265 0.436 62.6 0.195 0.344 91.9 0.164 0.426 90.1 0.195 0.488 91.9 0.125 0.328 Table 1: Performance of the models on the expert finding and profiling tasks, using different document types and their combinations. %q is the number of topics covered (applies to the expert finding task), %ca is the number of candidates covered (applies to the expert profiling task).",
                "The top and bottom blocks correspond to English and Dutch respectively.",
                "The best scores are in boldface. to how related the other requests are to the original query.",
                "This can be modeled by interpolating between the p(q|ca) and the further supporting evidence from all similar requests q , as follows: p (q|ca) = λp(q|ca) + (1 − λ) X q p(q|q )p(q |ca), (9) where p(q|q ) represents the similarity between the two topics q and q .",
                "To be able to work with similarity methods that are not necessarily probabilities, we set p(q|q ) = w(q,q ) γ , where γ is a normalizing constant, such that γ = P q w(q , q ).",
                "We consider four methods for calculating the similarity score between two topics.",
                "Three approaches are strictly content-based, and establish similarity by examining <br>co-occurrence</br> patterns of topics within the collection, while the last approach exploits the hierarchical structure of topical areas that may be present within an organization (see [7] for further examples of integrating word relationships into language models).",
                "The Kullback-Leibler (KL) divergence metric defined in Eq. 8 provides a measure of how different or similar two probability distributions are.",
                "A topic model is inferred for q and q using the method presented in Section 4.1 to describe the query across the entire vocabulary.",
                "Since a lower KL score means the queries are more similar, we let w(q, q ) = max(KL(θq||·) − KL(θq||θq )).",
                "Pointwise Mutual Information (PMI, [17]) is a measure of association used in information theory to determine the extent of independence between variables.",
                "The dependence between two queries is reflected by the SI(q, q ) score, where scores greater than zero indicate that it is likely that there is a dependence, which we take to mean that the queries are likely to be similar: SI(q, q ) = log p(q, q ) p(q)p(q ) (10) We estimate the probability of a topic p(q) using the number of documents relevant to query q within the collection.",
                "The joint probability p(q, q ) is estimated similarly, by using the concatenation of q and q as a query.",
                "To obtain p(q|q ), we then set w(q, q ) = SI(q, q ) when SI(q, q ) > 0 otherwise w(q, q ) = 0, because we are only interested in including queries that are similar.",
                "The log-likelihood statistic provides another measure of dependence, which is more reliable than the pointwise mutual information measure [17].",
                "Let k1 be the number of co-occurrences of q and q , k2 the number of occurrences of q not co-occurring with q , n1 the total number of occurrences of q , and n2 the total number of topic tokens minus the number of occurrences of q .",
                "Then, let p1 = k1/n1, p2 = k2/n2, and p = (k1 + k2)/(n1 + n2), (q, q ) = 2( (p1, k1, n1) + (p2, k2, n2) − (p, k1, n1) − (p, k2, n2)), where (p, n, k) = k log p + (n − k) log(1 − p).",
                "The higher score indicate that queries are also likely to be similar, thus we set w(q, q ) = (q, q ).",
                "Finally, we also estimate the similarity of two topics based on their distance within the topic hierarchy.",
                "The topic hierarchy is viewed as a directed graph, and for all topic-pairs the shortest path SP(q, q ) is calculated.",
                "We set the similarity score to be the reciprocal of the shortest path: w(q, q ) = 1/SP(q, q ). 7.2 Contextual information Given the hierarchy of an organization, the units to which a person belong are regarded as a context so as to compensate for data sparseness.",
                "We model it as follows: p (q|ca) = 1 − P ou∈OU(ca) λou · p(q|ca) + P ou∈OU(ca) λou · p(q|ou), where OU(ca) is the set of organizational units of which candidate ca is a member of, and p(q|o) expresses the strength of the association between query q and the unit ou.",
                "The latter probability can be estimated using either of the three basic models, by simply replacing ca with ou in the corresponding equations.",
                "An organizational unit is associated with all the documents that its members have authored.",
                "That is, p(d|ou) = maxca∈ou p(d|ca). 7.3 A simple multilingual model For knowledge institutes in Europe, academic or otherwise, a multilingual (or at least bilingual) setting is typical.",
                "The following model builds on a kind of independence assumption: there is no spill-over of expertise/profiles across language boundaries.",
                "While a simplification, this is a sensible first approach.",
                "That is: p (q|ca) =P l∈L λl · p(ql|ca), where L is the set of languages used in the collection, ql is the translation of the query q to language l, and λl is a language specific smoothing parameter, such that P l∈L λl = 1. 8.",
                "ADVANCED MODELS: EVALUATION In this section we present an experimental evaluation of our advanced models.",
                "Expert finding Expert profiling Language Model 1 Model 2 Model 3 Model 1 Model 2 Model 3 %q MAP MRR %q MAP MRR %q MAP MRR %ca MAP MRR %ca MAP MRR %ca MAP MRR English only 97.8 0.237 0.372 98.6 0.280 0.441 98.5 0.166 0.293 100 0.199 0.387 88.7 0.281 0.525 90.9 0.169 0.329 Dutch only 61.3 0.249 0.401 62.6 0.265 0.436 62.6 0.195 0.344 91.9 0.164 0.426 90.1 0.195 0.488 91.9 0.125 0.328 Combination 99.4 0.297 0.444 99.7 0.324 0.491 99.7 0.223 0.388 100 0.241 0.445 92.1 0.313 0.564 93.2 0.224 0.411 Table 3: Performance of the combination of languages on the expert finding and profiling tasks (on candidates).",
                "Best scores for each model are in italic, absolute best scores for the expert finding and profiling tasks are in boldface.",
                "Method Model 1 Model 2 Model 3 MAP MRR MAP MRR MAP MRR English BASELINE 0.296 0.454 0.339 0.509 0.221 0.333 KLDIV 0.291 0.453 0.327 0.503 0.219 0.330 PMI 0.291 0.453 0.337 0.509 0.219 0.331 LL 0.319 0.490 0.360 0.524 0.233 0.368 HDIST 0.299 0.465 0.346 0.537 0.219 0.332 Dutch BASELINE 0.240 0.350 0.271 0.403 0.227 0.389 KLDIV 0.239 0.347 0.253 0.386 0.224 0.385 PMI 0.239 0.350 0.260 0.392 0.227 0.389 LL 0.255 0.372 0.281 0.425 0.231 0.389 HDIST 0.253 0.365 0.271 0.407 0.236 0.402 Method Model 1 Model 2 Model 3 MAP MRR MAP MRR MAP MRR English BASELINE 0.485 0.546 0.499 0.548 0.381 0.416 KLDIV 0.510 0.564 0.513 0.558 0.381 0.416 PMI 0.486 0.546 0.495 0.542 0.407 0.451 LL 0.558 0.589 0.586 0.617 0.408 0.453 HDIST 0.507 0.567 0.512 0.563 0.386 0.420 Dutch BASELINE 0.263 0.313 0.294 0.358 0.262 0.315 KLDIV 0.284 0.336 0.271 0.321 0.261 0.314 PMI 0.265 0.317 0.265 0.316 0.273 0.330 LL 0.312 0.351 0.330 0.377 0.284 0.331 HDIST 0.280 0.327 0.288 0.341 0.266 0.321 Table 4: Performance on the expert finding (top) and profiling (bottom) tasks, using knowledge area similarities.",
                "Runs were evaluated on the main topics set.",
                "Best scores are in boldface. 8.1 Research Questions Our questions follow the refinements presented in the preceding section: Does exploiting the knowledge area similarity improve effectiveness?",
                "Which of the various methods for capturing word relationships is most effective?",
                "Furthermore, is our way of bringing in contextual information useful?",
                "For which tasks?",
                "And finally, is our simple way of combining the monolingual scores sufficient for obtaining significant improvements? 8.2 Experimental setup Given that the self-assessments are also sparse in our collection, in order to be able to measure differences between the various models, we selected a subset of topics, and evaluated (some of the) runs only on this subset.",
                "This set is referred as main topics, and consists of topics that are located at the top level of the topical hierarchy. (A main topic has subtopics, but is not a subtopic of any other topic.)",
                "This main set consists of 132 Dutch and 119 English topics.",
                "The relevance judgements were restricted to the main topic set, but were not expanded with subtopics. 8.3 Exploiting knowledge area similarity Table 4 presents the results.",
                "The four methods used for estimating knowledge-area similarity are KL divergence (KLDIV), PointLang.",
                "Topics Model 1 Model 2 Model 3 MAP MRR MAP MRR MAP MRR Expert finding UK ALL 0.423 0.545 0.654 0.799 0.494 0.629 UK MAIN 0.500 0.621 0.704 0.834 0.587 0.699 NL ALL 0.439 0.560 0.672 0.826 0.480 0.630 NL MAIN 0.440 0.584 0.645 0.816 0.515 0.655 Expert profiling UK ALL 0.240 0.640 0.306 0.778 0.223 0.616 UK MAIN 0.523 0.677 0.519 0.648 0.461 0.587 NL ALL 0.203 0.716 0.254 0.770 0.183 0.627 NL MAIN 0.332 0.576 0.380 0.624 0.332 0.549 Table 5: Evaluating the context models on organizational units. wise mutual information (PMI), log-likelihood (LL), and distance within topic hierarchy (HDIST).",
                "We managed to improve upon the baseline in all cases, but the improvement is more noticeable for the profiling task.",
                "For both tasks, the LL method performed best.",
                "The content-based approaches performed consistently better than HDIST. 8.4 Contextual information A two level hierarchy of organizational units (faculties and institutes) is available in the UvT Expert collection.",
                "The unit a person belongs to is used as a context for that person.",
                "First, we evaluated the models of the organizational units, using all topics (ALL) and only the main topics (MAIN).",
                "An organizational unit is considered to be relevant for a given topic (or vice versa) if at least one member of the unit selected the given topic as an expertise area.",
                "Table 5 reports on the results.",
                "As far as expert finding goes, given a topic, the corresponding organizational unit can be identified with high precision.",
                "However, the expert profiling task shows a different picture: the scores are low, and the task seems hard.",
                "The explanation may be that general concepts (i.e., our main topics) may belong to several organizational units.",
                "Second, we performed another evaluation, where we combined the contextual models with the candidate models (to score candidates again).",
                "Table 6 reports on the results.",
                "We find a positive impact of the context models only for expert finding.",
                "Noticably, for expert finding (and Model 1), it improves over 50% (for English) and over 70% (for Dutch) on MAP.",
                "The poor performance on expert profiling may be due to the fact that context models alone did not perform very well on the profiling task to begin with. 8.5 Multilingual models In this subsection we evaluate the method for combining results across multiple languages that we described in Section 7.3.",
                "In our setting the set of languages consists of English and Dutch: L = {UK, NL}.",
                "The weights on these languages were set to be identical (λUK = λNL = 0.5).",
                "We performed experiments with various λ settings, but did not observe significant differences in performance.",
                "Table 3 reports on the multilingual results, where performance is evaluated on the full topic set.",
                "All three models significantly imLang.",
                "Method Model 1 Model 2 Model 3 MAP MRR MAP MRR MAP MRR Expert finding UK BL 0.296 0.454 0.339 0.509 0.221 0.333 UK CT 0.330 0.491 0.342 0.500 0.228 0.342 NL BL 0.240 0.350 0.271 0.403 0.227 0.389 NL CT 0.251 0.382 0.267 0.410 0.246 0.404 Expert profiling UK BL 0.485 0.546 0.499 0.548 0.381 0.416 UK CT 0.562 0.620 0.508 0.558 0.440 0.486 NL BL 0.263 0.313 0.294 0.358 0.262 0.315 NL CT 0.330 0.384 0.317 0.387 0.294 0.345 Table 6: Performance of the context models (CT) compared to the baseline (BL).",
                "Best scores are in boldface. proved over all measures for both tasks.",
                "The coverage of topics and candidates for the expert finding and profiling tasks, respectively, is close to 100% in all cases.",
                "The relative improvement of the precision scores ranges from 10% to 80%.",
                "These scores demonstrate that despite its simplicity, our method for combining results over multiple languages achieves substantial improvements over the baseline. 9.",
                "CONCLUSIONS In this paper we focused on expertise retrieval (expert finding and profiling) in a new setting of a typical knowledge-intensive organization in which the available data is of high quality, multilingual, and covering a broad range of expertise area.",
                "Typically, the amount of available data in such an organization (e.g., a university, a research institute, or a research lab) is limited when compared to the W3C collection that has mostly been used for the experimental evaluation of expertise retrieval so far.",
                "To examine expertise retrieval in this setting, we introduced (and released) the UvT Expert collection as a representative case of such knowledge intensive organizations.",
                "The new collection reflects the typical properties of knowledge-intensive institutes noted above and also includes several features which may are potentially useful for expertise retrieval, such as topical and organizational structure.",
                "We evaluated how current state-of-the-art models for expert finding and profiling performed in this new setting and then refined these models in order to try and exploit the different characteristics within the data environment (language, topicality, and organizational structure).",
                "We found that current models of expertise retrieval generalize well to this new environment; in addition we found that refining the models to account for the differences results in significant improvements, thus making up for problems caused by data sparseness issues.",
                "Future work includes setting up manual assessments of automatically generated profiles by the employees themselves, especially in cases where the employees have not provided a profile themselves. 10.",
                "ACKNOWLEDGMENTS Krisztian Balog was supported by the Netherlands Organisation for Scientific Research (NWO) under project number 220-80-001.",
                "Maarten de Rijke was also supported by NWO under project numbers 017.001.190, 220-80-001, 264-70-050, 354-20-005, 600.065.120, 612-13-001, 612.000.106, 612.066.302, 612.069.006, 640.001.501, 640.002.501, and by the E.U.",
                "IST programme of the 6th FP for RTD under project MultiMATCH contract IST-033104.",
                "The work of Toine Bogers and Antal van den Bosch was funded by the IOP-MMI-program of SenterNovem / The Dutch Ministry of Economic Affairs, as part of the `A Propos project. 11.",
                "REFERENCES [1] L. Azzopardi.",
                "Incorporating Context in the Language Modeling Framework for ad-hoc Information Retrieval.",
                "PhD thesis, University of Paisley, 2005. [2] K. Balog and M. de Rijke.",
                "Finding similar experts.",
                "In This volume, 2007. [3] K. Balog and M. de Rijke.",
                "Determining expert profiles (with an application to expert finding).",
                "In IJCAI 07: Proc. 20th Intern.",
                "Joint Conf. on Artificial Intelligence, pages 2657-2662, 2007. [4] K. Balog, L. Azzopardi, and M. de Rijke.",
                "Formal models for expert finding in enterprise corpora.",
                "In SIGIR 06: Proc. 29th annual intern.",
                "ACM SIGIR conf. on Research and development in information retrieval, pages 43-50, 2006. [5] I. Becerra-Fernandez.",
                "The role of artificial intelligence technologies in the implementation of people-finder knowledge management systems.",
                "In AAAI Workshop on Bringing Knowledge to Business Processes, March 2000. [6] C. S. Campbell, P. P. Maglio, A. Cozzi, and B. Dom.",
                "Expertise identification using email communications.",
                "In CIKM 03: Proc. twelfth intern. conf. on Information and knowledge management, pages 528531, 2003. [7] G. Cao, J.-Y.",
                "Nie, and J. Bai.",
                "Integrating word relationships into language models.",
                "In SIGIR 05: Proc. 28th annual intern.",
                "ACM SIGIR conf. on Research and development in information retrieval, pages 298-305, 2005. [8] T. M. Cover and J.",
                "A. Thomas.",
                "Elements of Information Theory.",
                "Wiley-Interscience, 1991. [9] N. Craswell, D. Hawking, A. M. Vercoustre, and P. Wilkins.",
                "P@noptic expert: Searching for experts not just for documents.",
                "In Ausweb, 2001. [10] N. Craswell, A. de Vries, and I. Soboroff.",
                "Overview of the TREC2005 Enterprise Track.",
                "In The Fourteenth Text REtrieval Conf.",
                "Proc. (TREC 2005), 2006. [11] T. H. Davenport and L. Prusak.",
                "Working Knowledge: How Organizations Manage What They Know.",
                "Harvard Business School Press, Boston, MA, 1998. [12] T. Dunning.",
                "Accurate methods for the statistics of surprise and coincidence.",
                "Computational Linguistics, 19(1):61-74, 1993. [13] E. Filatova and J. Prager.",
                "Tell me what you do and Ill tell you what you are: Learning occupation-related activities for biographies.",
                "In HLT/EMNLP, 2005. [14] V. Lavrenko and W. B. Croft.",
                "Relevance based language models.",
                "In SIGIR 01: Proc. 24th annual intern.",
                "ACM SIGIR conf. on Research and development in information retrieval, pages 120-127, 2001. [15] V. Lavrenko, M. Choquette, and W. B. Croft.",
                "Cross-lingual relevance models.",
                "In SIGIR 02: Proc. 25th annual intern.",
                "ACM SIGIR conf. on Research and development in information retrieval, pages 175-182, 2002. [16] C. Macdonald and I. Ounis.",
                "Voting for candidates: adapting data fusion techniques for an expert search task.",
                "In CIKM 06: Proc. 15th ACM intern. conf. on Information and knowledge management, pages 387-396, 2006. [17] C. Manning and H. Sch¨utze.",
                "Foundations of Statistical Natural Language Processing.",
                "The MIT Press, 1999. [18] A. Mockus and J. D. Herbsleb.",
                "Expertise browser: a quantitative approach to identifying expertise.",
                "In ICSE 02: Proc. 24th Intern.",
                "Conf. on Software Engineering, pages 503-512, 2002. [19] D. Petkova and W. B. Croft.",
                "Hierarchical language models for expert finding in enterprise corpora.",
                "In Proc.",
                "ICTAI 2006, pages 599-608, 2006. [20] I. Soboroff, A. de Vries, and N. Craswell.",
                "Overview of the TREC 2006 Enterprise Track.",
                "In TREC 2006 Working Notes, 2006. [21] T. Tao, X. Wang, Q. Mei, and C. Zhai.",
                "Language model information retrieval with document expansion.",
                "In HLT-NAACL 2006, 2006. [22] TREC.",
                "Enterprise track, 2005.",
                "URL: http://www.ins.cwi. nl/projects/trec-ent/wiki/. [23] G. van Noord.",
                "TextCat Language Guesser.",
                "URL: http://www. let.rug.nl/˜vannoord/TextCat/. [24] W3C.",
                "The W3C test collection, 2005.",
                "URL: http://research. microsoft.com/users/nickcr/w3c-summary.html."
            ],
            "original_annotated_samples": [
                "This task is usually addressed by uncovering associations between people and topics [10]; commonly, a <br>co-occurrence</br> of the name of a person with topics in the same context is assumed to be evidence of expertise.",
                "In our modeling of expert finding and profiling we collect evidence for expertise from multiple sources, in a heterogeneous collection, and integrate it with the <br>co-occurrence</br> of candidates names and query terms-the language modeling setting allows us to do this in a transparent manner.",
                "Three approaches are strictly content-based, and establish similarity by examining <br>co-occurrence</br> patterns of topics within the collection, while the last approach exploits the hierarchical structure of topical areas that may be present within an organization (see [7] for further examples of integrating word relationships into language models)."
            ],
            "translated_annotated_samples": [
                "Esta tarea suele abordarse descubriendo asociaciones entre personas y temas [10]; comúnmente, se asume que la <br>co-ocurrencia</br> del nombre de una persona con temas en el mismo contexto es evidencia de experiencia.",
                "En nuestro modelado de búsqueda y perfilado de expertos recopilamos evidencia de experiencia de múltiples fuentes, en una colección heterogénea, e integramos esta información con la <br>co-ocurrencia</br> de nombres de candidatos y términos de consulta: el entorno de modelado de lenguaje nos permite hacer esto de manera transparente.",
                "Tres enfoques se basan estrictamente en el contenido y establecen similitudes examinando patrones de <br>co-ocurrencia</br> de temas dentro de la colección, mientras que el último enfoque explota la estructura jerárquica de áreas temáticas que pueden estar presentes dentro de una organización (ver [7] para más ejemplos de integrar relaciones entre palabras en modelos de lenguaje)."
            ],
            "translated_text": "Recuperación de amplia experiencia en entornos de datos dispersos Krisztian Balog ISLA, Universidad de Ámsterdam Kruislaan 403, 1098 SJ Ámsterdam, Países Bajos kbalog@science.uva.nl Toine Bogers ILK, Universidad de Tilburg P.O. Caja 90153, 5000 LE Tilburg, Países Bajos A.M.Bogers@uvt.nl Leif Azzopardi Dept. de Ciencias de la Computación Universidad de Glasgow, Glasgow, G12 8QQ leif@dcs.gla.ac.uk Maarten de Rijke ISLA, Universidad de Ámsterdam Kruislaan 403, 1098 SJ Ámsterdam, Países Bajos mdr@science.uva.nl Antal van den Bosch ILK, Universidad de Tilburg P.O. La recuperación de la experiencia ha sido ampliamente inexplorada en datos distintos a la colección de la W3C. Al mismo tiempo, muchas intranets de universidades y otras organizaciones intensivas en conocimiento ofrecen ejemplos de datos de expertos multilingües relativamente pequeños pero precisos, que abarcan amplias áreas de especialización. Primero presentamos dos tareas principales de recuperación de expertos, junto con un conjunto de enfoques de referencia basados en modelado del lenguaje generativo, con el objetivo de encontrar relaciones de experticia entre temas y personas. Para nuestra evaluación experimental, introducimos (y liberamos) un nuevo conjunto de pruebas basado en un rastreo de un sitio universitario. Usando este conjunto de pruebas, llevamos a cabo dos series de experimentos. El primero tiene como objetivo determinar la efectividad de los métodos de recuperación de conocimientos básicos aplicados al nuevo conjunto de pruebas. El segundo tiene como objetivo evaluar modelos refinados que exploten las características distintivas del nuevo conjunto de pruebas, como la estructura organizativa de la universidad y la estructura jerárquica de los temas en el conjunto de pruebas. Los modelos de recuperación de experiencia se muestran robustos con respecto a entornos más pequeños que la colección de la W3C, y las técnicas actuales parecen ser generalizables a otros contextos. Categorías y Descriptores de Asignaturas H.3 [Almacenamiento y Recuperación de Información]: H.3.1 Análisis de Contenido e Indexación; H.3.3 Búsqueda y Recuperación de Información; H.3.4 Sistemas y Software; H.4 [Aplicaciones de Sistemas de Información]: H.4.2 Tipos de Sistemas; H.4.m Términos Generales Misceláneos Algoritmos, Medición, Rendimiento, Experimentación 1. INTRODUCCIÓN La intranet de una organización proporciona un medio para intercambiar información entre empleados y facilitar colaboraciones entre ellos. Para lograr esto de manera eficiente y efectiva, es necesario proporcionar instalaciones de búsqueda que permitan a los empleados no solo acceder a documentos, sino también identificar a colegas expertos. En la pista de Empresas de TREC [22] se ha reconocido la necesidad de estudiar y comprender la recuperación de expertos a través de la introducción de tareas de Búsqueda de Expertos. El objetivo de la búsqueda de expertos es identificar una lista de personas que tienen conocimientos sobre un tema específico. Esta tarea suele abordarse descubriendo asociaciones entre personas y temas [10]; comúnmente, se asume que la <br>co-ocurrencia</br> del nombre de una persona con temas en el mismo contexto es evidencia de experiencia. Una tarea alternativa, que utiliza la misma idea de asociaciones entre personas y temas, es el perfilado de expertos, donde la tarea consiste en devolver una lista de temas sobre los que una persona tiene conocimiento [3]. El lanzamiento de la tarea de Búsqueda de Expertos en TREC ha generado mucho interés en la recuperación de expertos, con un rápido progreso en términos de modelado, algoritmos y aspectos de evaluación. Sin embargo, casi todo el trabajo de búsqueda o perfilado de expertos realizado ha sido validado experimentalmente utilizando la colección de W3C [24] de la pista empresarial. Si bien esta colección es actualmente la única colección de pruebas disponible públicamente para tareas de recuperación de expertos, solo representa un tipo de intranet. Con solo una colección de pruebas no es posible generalizar conclusiones a otros escenarios realistas. En este artículo nos enfocamos en la recuperación de expertos en un entorno realista que difiere del entorno de la W3C, uno en el que hay cantidades relativamente pequeñas de datos limpios y multilingües disponibles, que abarcan una amplia gama de áreas de expertise, como se puede encontrar en las intranets de universidades y otras organizaciones intensivas en conocimiento. Normalmente, esta configuración incluye varios tipos adicionales de estructuras: estructura temática (por ejemplo, jerarquías de temas utilizadas por la organización), estructura organizativa (facultad, departamento, ...), así como múltiples tipos de documentos (descripciones de investigación y cursos, publicaciones y páginas web académicas). Este entorno es bastante diferente del entorno del W3C de maneras que podrían afectar el rendimiento de las tareas de recuperación de conocimientos especializados. Nos enfocamos en varias preguntas de investigación en este artículo: ¿La cantidad relativamente pequeña de datos disponibles en una intranet afecta la calidad de las asociaciones tema-persona que son fundamentales en los algoritmos de recuperación de expertos? ¿Cómo se desempeñan los algoritmos de vanguardia desarrollados en el conjunto de datos del W3C en el escenario alternativo del tipo descrito anteriormente? Más en general, ¿se pueden aplicar las lecciones de la tarea de Búsqueda de Expertos en TREC a este contexto? ¿Cómo afecta la inclusión o exclusión de diferentes documentos a las tareas de recuperación de expertos? Además, ¿cómo se puede utilizar la estructura temática y organizativa con fines de recuperación? Para responder a nuestras preguntas de investigación, primero presentamos un conjunto de enfoques de línea base, basados en modelado de lenguaje generativo, con el objetivo de encontrar asociaciones entre temas y personas. Esto nos permite formular las tareas de búsqueda de expertos y perfilado de expertos de manera uniforme, y tiene el beneficio adicional de permitirnos entender las relaciones entre las dos tareas. Para nuestra evaluación experimental, introducimos un nuevo conjunto de datos (la Colección de Expertos de UvT) que es representativo del tipo de intranet que describimos anteriormente. Nuestra colección se basa en datos de acceso público, extraídos del sitio web de la Universidad de Tilburg (UvT). Este tipo de datos es particularmente interesante, ya que (1) es limpio, heterogéneo, estructurado y enfocado, pero comprende un número limitado de documentos; (2) contiene información sobre la jerarquía organizativa; (3) es bilingüe (inglés y holandés); y (4) la lista de áreas de especialización de un individuo es proporcionada por los propios empleados. Usando la colección de expertos de UvT, llevamos a cabo dos conjuntos de experimentos. El primero tiene como objetivo determinar la efectividad de los métodos de búsqueda y perfilado de experiencia inicial en este nuevo entorno. Un segundo grupo de experimentos está dirigido a extensiones de los métodos base que aprovechan las características distintivas de la Colección de Expertos de UvT; específicamente, proponemos y evaluamos métodos refinados de búsqueda y perfilado de expertos que incorporan la topicalidad y la estructura organizativa. Además de las preguntas de investigación y el conjunto de datos que aportamos, nuestras principales contribuciones son las siguientes. Los modelos base desarrollados para encontrar expertos funcionan bien en el nuevo conjunto de datos. Mientras que en el entorno del W3C la tarea de encontrar expertos parece ser más difícil que el perfilado, para los datos de UvT ocurre lo contrario. Encontramos que el perfilado en el conjunto de datos UvT es considerablemente más difícil que en el conjunto de datos W3C, lo cual creemos que se debe al gran (pero realista) número de áreas temáticas que utilizamos para el perfilado: alrededor de 1,500 para el conjunto de datos UvT, frente a 50 en el caso de W3C. Teniendo en cuenta la similitud entre los temas puede mejorar significativamente el rendimiento de recuperación. Las medidas de similitud de mejor rendimiento son basadas en contenido, por lo tanto también se pueden aplicar en entornos de la W3C (y otros). Finalmente, demostramos que la estructura organizativa puede ser explotada en forma de un modelo de contexto, mejorando los puntajes de MAP para ciertos modelos hasta en un 70%. El resto de este documento está organizado de la siguiente manera. En la siguiente sección revisamos el trabajo relacionado. Luego, en la Sección 3 proporcionamos descripciones detalladas de las tareas de recuperación de expertos que abordamos en este artículo: la búsqueda de expertos y la creación de perfiles de expertos. En la Sección 4 presentamos nuestros modelos base, cuyo rendimiento luego es evaluado en la Sección 6 utilizando el conjunto de datos UvT que presentamos en la Sección 5. Los modelos avanzados que explotan características específicas de nuestros datos se presentan en la Sección 7 y se evalúan en la Sección 8. Formulamos nuestras conclusiones en la Sección 9.2. Trabajos relacionados Los enfoques iniciales para encontrar expertos a menudo empleaban bases de datos que contenían información sobre las habilidades y conocimientos de cada individuo en la organización [11]. La mayoría de estas herramientas (generalmente llamadas páginas amarillas o sistemas de búsqueda de personas) dependen de que las personas evalúen sus habilidades frente a un conjunto predefinido de palabras clave. Para actualizar perfiles en estos sistemas de forma automática, se necesita de tecnologías inteligentes [5]. Enfoques más recientes utilizan conjuntos de documentos específicos (como correos electrónicos [6] o software [18]) para encontrar experiencia. En contraste con centrarse en tipos de documentos particulares, también hay un interés creciente en el desarrollo de sistemas que indexan y extraen información de documentos publicados en intranets como fuentes de evidencia de experiencia. Un enfoque publicado es el sistema P@noptic [9], que construye una representación de cada persona concatenando todos los documentos asociados con esa persona, similar al Modelo 1 de Balog et al. [4], quienes formalizan y comparan dos métodos. El Modelo 1 de Balog et al. modela directamente el conocimiento de un experto a partir de documentos asociados, mientras que su Modelo 2 primero localiza documentos sobre el tema y luego encuentra a los expertos asociados. En los experimentos reportados, el segundo método tiene un rendimiento significativamente mejor cuando hay suficientes documentos asociados por candidato. La mayoría de los sistemas que participaron en las ediciones de 2005 y 2006 de la tarea de Búsqueda de Expertos en TREC implementaron (variaciones de) uno de estos dos modelos; ver [10, 20]. Macdonald y Ounis [16] proponen un enfoque diferente para clasificar la experiencia de los candidatos con respecto a un tema basado en técnicas de fusión de datos, sin utilizar heurísticas específicas de la colección; encuentran que la aplicación de modelos de ponderación basados en campos mejora la clasificación de los candidatos. Petkova y Croft [19] proponen otro enfoque, basado en una combinación de los Modelos 1 y 2 anteriores, modelando explícitamente los temas. Al abordar otras tareas de recuperación de expertos que también pueden abordarse utilizando asociaciones entre temas y personas, Balog y de Rijke [3] abordaron la tarea de determinar perfiles de expertos temáticos. Si bien sus métodos resultaron ser eficientes en el corpus de la W3C, requieren una cantidad de datos que puede que no esté disponible en la típica organización intensiva en conocimiento. Balog y de Rijke [2] estudian la tarea relacionada de encontrar expertos que sean similares a un pequeño conjunto de expertos dados como entrada. Por cierto, crear un resumen textual de una persona muestra algunas similitudes con la búsqueda de biografías, la cual ha recibido una considerable cantidad de atención recientemente; ver por ejemplo, [13]. Utilizamos modelado de lenguaje generativo para encontrar asociaciones entre temas y personas. En nuestro modelado de búsqueda y perfilado de expertos recopilamos evidencia de experiencia de múltiples fuentes, en una colección heterogénea, e integramos esta información con la <br>co-ocurrencia</br> de nombres de candidatos y términos de consulta: el entorno de modelado de lenguaje nos permite hacer esto de manera transparente. Nuestro modelado avanza en dos pasos. En el primer paso, consideramos tres modelos base, dos tomados de [4] (los Modelos 1 y 2 mencionados anteriormente), y uno una versión refinada de un modelo introducido en [3] (al que nos referimos como Modelo 3 a continuación); este tercer modelo también es similar al modelo descrito por Petkova y Croft [19]. Los modelos que consideramos en nuestra segunda ronda de experimentos son modelos de mezcla similares a los modelos de lenguaje contextual [1] y a los documentos ampliados de Tao et al. [21]; sin embargo, las características que utilizamos para definir nuestras expansiones, incluida la estructura temática y la estructura organizativa, no se han utilizado de esta manera antes. 3. En el escenario de recuperación de expertos que imaginamos, los usuarios que buscan expertos dentro de una organización tienen acceso a una interfaz que combina un cuadro de búsqueda (donde pueden buscar expertos o temas) con estructuras de navegación (de expertos y de temas) que les permite hacer clic hasta llegar a una página de experto (que proporciona el perfil de una persona) o a una página de tema (que proporciona una lista de expertos en el tema). Para alimentar la interfaz anterior, nos enfrentamos a dos tareas de recuperación de expertos, la búsqueda de expertos y el perfilado de expertos, que primero definimos y luego formalizamos utilizando modelos de lenguaje generativos. Para modelar cualquiera de las tareas, la probabilidad de que el tema de la consulta esté asociado a un experto candidato juega un papel clave en las estimaciones finales para la búsqueda y el perfilado. Al utilizar modelos de lenguaje, tanto los candidatos como la consulta se caracterizan por distribuciones de términos en el vocabulario (utilizado en los documentos puestos a disposición por la organización cuyas necesidades de recuperación de experiencia estamos abordando). 3.1 Búsqueda de expertos La búsqueda de expertos implica la tarea de encontrar a la persona adecuada con las habilidades y conocimientos apropiados: ¿Quiénes son los expertos en el tema X?. Por ejemplo, un empleado quiere averiguar quién trabajó en un proyecto en particular para descubrir por qué se tomaron decisiones específicas sin tener que revisar minuciosamente la documentación (si es que la hay). O pueden necesitar un especialista capacitado para consultoría sobre un problema específico. Dentro de una organización, generalmente hay muchos posibles candidatos que podrían ser expertos en un tema dado. Podemos plantear este problema de la siguiente manera: ¿Cuál es la probabilidad de que un candidato ca sea un experto dado el tema de consulta q? Es decir, determinamos p(ca|q) y clasificamos a los candidatos ca de acuerdo con esta probabilidad. Los candidatos con la probabilidad más alta dada la consulta son considerados los expertos más probables en ese tema. El desafío es cómo estimar esta probabilidad con precisión. Dado que es probable que la consulta consista solo en unos pocos términos para describir la experiencia requerida, deberíamos poder obtener una estimación más precisa al invocar el Teorema de Bayes, y estimar: p(ca|q) = p(q|ca)p(ca) p(q) , (1) donde p(ca) es la probabilidad de un candidato y p(q) es la probabilidad de una consulta. Dado que p(q) es una constante, se puede ignorar para fines de clasificación. Por lo tanto, la probabilidad de que un candidato ca sea un experto dado la consulta q es proporcional a la probabilidad de una consulta dada el candidato p(q|ca), ponderada por la creencia a priori p(ca) de que el candidato ca es un experto. p(ca|q) ∝ p(q|ca)p(ca) (2) En este artículo, nuestro enfoque principal se centra en estimar la probabilidad de una consulta dada el candidato p(q|ca), ya que esta probabilidad captura en qué medida el candidato conoce sobre el tema de la consulta. Si bien se asume generalmente que los priors del candidato son uniformes y, por lo tanto, no influirán en la clasificación, se ha demostrado que una elección sensata de priors puede mejorar el rendimiento [20]. Perfilado de expertos. Mientras que la tarea de búsqueda de expertos se centraba en encontrar expertos en un tema específico, la tarea de perfilado de expertos busca responder a una pregunta relacionada: ¿Sobre qué temas tiene conocimiento un candidato? Básicamente, esto da un giro a las preguntas sobre la búsqueda de expertos. El perfilado de un candidato individual implica la identificación de áreas de habilidades y conocimientos en las que tienen experiencia, así como una evaluación del nivel de competencia en cada una de estas áreas. Este es el perfil temático de los candidatos. Generalmente, los perfiles temáticos dentro de las organizaciones consisten en estructuras tabulares que catalogan explícitamente las habilidades y conocimientos de cada individuo en la organización. Sin embargo, esta práctica está limitada por los recursos disponibles para definir, crear, mantener y actualizar estos perfiles con el tiempo. Al centrarnos en métodos automáticos que se basan en la evidencia disponible dentro de los repositorios de documentos de una organización, nuestro objetivo es reducir el esfuerzo humano asociado con el mantenimiento de perfiles temáticos. Un perfil temático de un candidato, entonces, se define como un vector donde cada elemento i del vector corresponde a la experiencia del candidato en un tema dado ki, (es decir, s(ca, ki)). Cada tema ki define un área de conocimiento o habilidad particular que la organización utiliza para definir el perfil temático de los candidatos. Por lo tanto, se asume que se proporciona una lista de temas, {k1, . . . , kn}, donde n es el número de temas predefinidos: perfil(ca) = s(ca, k1), s(ca, k2), . . . , s(ca, kn) . Se necesita contexto y evidencia para ayudar a los usuarios de sistemas de búsqueda de expertos a decidir a quién contactar al buscar experiencia en un área particular. Ejemplos de dicho contexto son: ¿Con quién trabaja ella? ¿Cuáles son sus datos de contacto? ¿Está bien conectada, por si acaso no puede ayudarnos ella misma? ¿Cuál es su rol en la organización? ¿Quién es su superior? Los colaboradores, afiliaciones, etc. son parte del perfil social de los candidatos y pueden servir como antecedentes para interpretar las recomendaciones del sistema. En este documento solo abordamos el problema de determinar perfiles temáticos, dejando el perfilado social para trabajos futuros. Planteamos el problema de cuantificar la competencia de una persona en un área de conocimiento específica de la siguiente manera: ¿Cuál es la probabilidad de que un área de conocimiento (ki) forme parte del perfil de candidatos (expertise)? donde s(ca, ki) está definido por p(ki|ca). Nuestra tarea, entonces, es estimar p(ki|ca), lo cual es equivalente al problema de obtener p(q|ca), donde el tema ki se representa como un tema de consulta q, es decir, una secuencia de palabras clave que representan la experiencia requerida. Tanto la tarea de encontrar expertos como la de perfilar expertos dependen de la estimación precisa de p(q|ca). La única diferencia proviene de la probabilidad previa de que una persona sea experta (p(ca)), la cual puede ser incorporada en la tarea de encontrar expertos. Este antecedente no se aplica a la tarea de perfilado ya que el candidato (individuo) está fijo. 4. MODELOS DE REFERENCIA En esta sección describimos nuestros modelos de referencia para estimar p(q|ca), es decir, las asociaciones entre temas y personas. Tanto la búsqueda de expertos como el perfilado de expertos se reducen a esta estimación. Empleamos tres modelos para calcular esta probabilidad. 4.1 De temas a candidatos Utilizando Modelos de Candidatos: El Modelo 1 [4] define la probabilidad de una consulta dada un candidato (p(q|ca)) utilizando técnicas estándar de modelado de lenguaje, basadas en un modelo de lenguaje multinomial de unigrama. Para cada candidato ca, se infiere un modelo de lenguaje candidato θca de tal manera que la probabilidad de un término dado θca no sea cero para todos los términos, es decir, p(t|θca) > 0. A partir del modelo de candidato, la consulta se genera con la siguiente probabilidad: p(q|θca) = Y t∈q p(t|θca)n(t,q), donde cada término t en la consulta q se muestrea de manera idéntica e independiente, y n(t, q) es el número de veces que t ocurre en q. El modelo de lenguaje candidato se infiere de la siguiente manera: (1) se calcula un modelo empírico p(t|ca); (2) se suaviza con probabilidades de fondo. Usando las asociaciones entre un candidato y un documento, la probabilidad p(t|ca) puede aproximarse por: p(t|ca) = X d p(t|d)p(d|ca), donde p(d|ca) es la probabilidad de que el candidato ca genere un documento de apoyo d, y p(t|d) es la probabilidad de que un término t ocurra en el documento d. Utilizamos la estimación de máxima verosimilitud de un término, es decir, la frecuencia normalizada del término t en el documento d. La fuerza de la asociación entre el documento d y el candidato ca expresada por p(d|ca) refleja el grado en que la experiencia de los candidatos se describe utilizando este documento. La estimación de esta probabilidad se presenta más adelante, en la Sección 4.2. El modelo candidato se construye entonces como una interpolación lineal de p(t|ca) y el modelo de fondo p(t) para asegurar que no haya probabilidades nulas, lo que resulta en la estimación final: p(q|θca) = (4) Y t∈q ( (1 − λ) X d p(t|d)p(d|ca) ! + λp(t) )n(t,q) . El Modelo 1 recopila toda la información de términos de todos los documentos asociados con el candidato, y la utiliza para representar a ese candidato. Este modelo se utiliza para predecir qué tan probable es que un candidato produzca una consulta q. Esto puede interpretarse intuitivamente como la probabilidad de que este candidato hable sobre el tema de la consulta, donde asumimos que esto es indicativo de su experiencia. Usando Modelos de Documentos: El Modelo 2 [4] toma un enfoque diferente. Aquí, el proceso se divide en dos partes. Dado un candidato ca, (1) se selecciona un documento asociado con un candidato con probabilidad p(d|ca), y (2) a partir de este documento se genera una consulta q con probabilidad p(q|d). Entonces se toma la suma sobre todos los documentos para obtener p(q|ca), de modo que: p(q|ca) = Σ d p(q|d)p(d|ca). (5) La probabilidad de una consulta dada un documento se estima inferiendo un modelo de lenguaje del documento θd para cada documento d de manera similar a como se infería el modelo candidato: p(t|θd) = (1 − λ)p(t|d) + λp(t), (6) donde p(t|d) es la probabilidad del término en el documento. La probabilidad de una consulta dada el modelo de documento es: p(q|θd) = Y t∈q p(t|θd)n(t,q) . La estimación final de p(q|ca) se obtiene sustituyendo p(q|d) por p(q|θd) en la Ecuación 5 (ver [4] para más detalles). Conceptualmente, el Modelo 2 difiere del Modelo 1 porque el candidato no está modelado directamente. En cambio, el documento actúa como una variable oculta en el proceso que separa la consulta del candidato. Este proceso es similar a cómo un usuario puede buscar candidatos con un motor de búsqueda estándar: inicialmente encontrando los documentos relevantes y luego viendo quién está asociado con ese documento. Al examinar una serie de documentos, el usuario puede obtener una idea de qué candidatos son más propensos a discutir el tema q. Usando Modelos de Temas: Modelo 3 Introducimos un tercer modelo, Modelo 3. En lugar de intentar modelar el proceso de generación de consultas a través de modelos de candidatos o documentos, representamos la consulta como un modelo de lenguaje de temas y estimamos directamente la probabilidad del candidato p(ca|q). Este enfoque es similar al modelo presentado en [3, 19]. Como en los modelos anteriores, se infiere un modelo de lenguaje, pero esta vez para la consulta. Adaptamos el trabajo de Lavrenko y Croft [14] para estimar un modelo de tema a partir de la consulta. El procedimiento es el siguiente. Dada una colección de documentos y un tema de consulta q, se asume que existe un modelo de tema desconocido θk que asigna probabilidades p(t|θk) a las ocurrencias de términos en los documentos del tema. Tanto la consulta como los documentos son muestras de θk (a diferencia de los enfoques anteriores, donde se asume que una consulta se extrae de un documento específico o un modelo candidato). La tarea principal es estimar p(t|θk), la probabilidad de un término dado el modelo de tema. Dado que la consulta q es muy dispersa y no hay ejemplos de documentos sobre el tema, es necesario aproximar esta distribución. Lavrenko y Croft [14] sugieren una forma razonable de obtener dicha aproximación, asumiendo que p(t|θk) puede aproximarse por la probabilidad del término t dado la consulta q. Entonces podemos estimar p(t|q) utilizando la probabilidad conjunta de observar el término t junto con los términos de la consulta, q1, . . . , qm, y dividiendo por la probabilidad conjunta de los términos de la consulta: p(t|θk) ≈ p(t|q) = p(t, q1, . . . , qm) p(q1, . . . , qm) = p(t, q1, . . . , qm) P t ∈T p(t , q1, . . . , qm) , donde p(q1, . . . , qm) = P t ∈T p(t , q1, . . . , qm), y T es el vocabulario completo de términos. Para estimar la probabilidad conjunta p(t, q1, . . . , qm), seguimos [14, 15] y asumimos que t y q1, . . . , qm son mutuamente independientes, una vez que elegimos una distribución fuente del conjunto de distribuciones fuente subyacentes U. Si elegimos U como un conjunto de modelos de documentos, entonces para construir este conjunto, la consulta q se emitiría contra la colección, y se asume que los primeros n devueltos son relevantes para el tema, y por lo tanto se tratan como muestras del modelo de tema. (Tenga en cuenta que en su lugar podrían usarse modelos candidatos). Con los modelos de documentos formando U, la probabilidad conjunta del término y la consulta se convierte en: p(t, q1, . . . , qm) = X d∈U p(d) ˘ p(t|θd) mY i=1 p(qi|θd) ¯ . (7) Aquí, p(d) denota la distribución previa sobre el conjunto U, que refleja la relevancia del documento para el tema. Suponemos que p(d) es uniforme en todo U. Para clasificar a los candidatos según el modelo de tema definido, utilizamos la métrica de divergencia de Kullback-Leibler (KL, [8]) para medir la diferencia entre los modelos de los candidatos y el modelo de tema: KL(θk||θca) = X t p(t|θk) log p(t|θk) p(t|θca) . (8) Se considera que los candidatos con una divergencia menor respecto al modelo de tema son más propensos a ser expertos en ese tema. El modelo candidato θca está definido en la Ecuación 4. Al utilizar la divergencia de Kullback-Leibler en lugar de la probabilidad de un candidato dado el modelo de tema p(ca|θk), evitamos problemas de normalización. 4.2 Asociaciones documento-candidato Para nuestros modelos, necesitamos poder estimar la probabilidad p(d|ca), que expresa en qué medida un documento d caracteriza al candidato ca. En [4], se presentan dos métodos para estimar esta probabilidad, basados en el número de nombres de personas reconocidos en un documento. Sin embargo, en nuestro entorno de intranet es razonable asumir que los autores de los documentos pueden ser identificados de manera inequívoca (por ejemplo, como el autor de un artículo, el profesor asignado a un curso, el propietario de una página web, etc.). Por lo tanto, establecemos p(d|ca) en 1 si el candidato ca es el autor del documento d, de lo contrario la probabilidad es 0. En la Sección 6 describimos cómo se puede determinar la autoría en diferentes tipos de documentos dentro de la colección. 5. La colección de expertos de UvT utilizada en los experimentos de este artículo se ajusta al escenario descrito en la Sección 3. La colección se basa en el sistema Webwijs (Webwise) desarrollado en la Universidad de Tilburg (UvT) en los Países Bajos. Webwijs (http://www.uvt.nl/webwijs/) es una base de datos de acceso público de empleados de la UvT que están involucrados en investigación o enseñanza; actualmente, Webwijs contiene información sobre 1168 expertos, cada uno de los cuales tiene una página con información de contacto y, si está disponible por parte del experto, una descripción de investigación y lista de publicaciones. Además, cada experto puede seleccionar áreas de especialización de una lista de 1491 temas y se le anima a sugerir nuevos temas que necesitan ser aprobados por el editor de Webwijs. Cada tema tiene una página separada que muestra a todos los expertos asociados con ese tema y, si está disponible, una lista de temas relacionados. Webwijs está disponible en holandés e inglés, y esta bilingüidad se ha preservado en la colección. Cada página de Webwijs en holandés tiene una traducción al inglés. No todos los temas en holandés tienen una traducción al inglés, pero lo contrario es cierto: los 981 temas en inglés tienen un equivalente en holandés. Aproximadamente el 42% de los expertos imparten cursos en la Universidad de Tilburg; estos cursos también fueron rastreados e incluidos en el perfil. Además, aproximadamente el 27% de los expertos enlazan su página académica desde su página de Webwijs. Estas páginas de inicio fueron rastreadas y añadidas a la colección. (Esto significa que si los expertos colocaron las versiones completas de sus publicaciones en sus páginas de inicio académicas, estas también estaban disponibles para su indexación). También obtuvimos 1880 versiones completas de publicaciones del repositorio institucional de UvT y el número de expertos holandeses en inglés es de 1168, con 743 expertos con ≥ 1 tema. El número de temas es de 1491 y 981, y el número de pares experto-tema es de 4318 y 3251, con un promedio de temas por experto de 5.8 y 5.9 respectivamente. El máximo número de temas por experto es de 60 (1) y 35 (1), y el mínimo es de 1 (74) y 1 (106) respectivamente. El promedio de expertos por tema es de 2.9 y 3.3, con un máximo de 30 (1) y un mínimo de 1 (615) y 1 (346) respectivamente. Hay 318 expertos con HP y CD, con un promedio de 3.5 CDs por experto docente. Además, hay 329 expertos con RD y 734 con PUB, con un promedio de 27.0 publicaciones por experto, 25.2 citas por experto y 1.8 publicaciones completas por experto. Tabla 2: Estadísticas descriptivas de las versiones holandesas e inglesas de la colección de expertos de UvT. Ejecutamos el identificador de idioma TextCat [23] para clasificar el idioma de las páginas de inicio y las publicaciones de texto completo. Nos limitamos a las páginas en las que el clasificador tenía confianza en el idioma utilizado en la página. Esto resultó en cuatro tipos de documentos: descripciones de investigación (RD), descripciones de cursos (CD), publicaciones (PUB; versiones de texto completo y solo de cita) y páginas web académicas (HP). Todo fue agrupado en la colección UvT Expert que está disponible en http://ilk.uvt.nl/uvt-expert-collection/. La colección UvT Expert fue extraída de un entorno organizativo diferente a la colección de la W3C y difiere de ella en varios aspectos. El entorno de UvT es uno con cantidades relativamente pequeñas de datos multilingües. Las asociaciones entre los documentos y los autores son claras y los datos están estructurados y limpios. La colección abarca una amplia gama de áreas de especialización, como suele encontrarse en intranets de universidades y otros institutos intensivos en conocimiento. Además, nuestro entorno universitario cuenta con varios tipos de estructura (temática y organizativa), así como múltiples tipos de documentos. Otra diferencia importante entre los dos conjuntos de datos es que las áreas de especialización en la colección de Expertos de UvT son autoseleccionadas en lugar de basarse en la membresía de un grupo o asignaciones de otros. El tamaño es otra dimensión en la que difieren las colecciones de expertos de W3C y UvT: esta última es la más pequeña de las dos. También son realistas las grandes diferencias en la cantidad de información disponible para cada experto. Utilizar Webwijs es voluntario; 425 expertos holandeses no seleccionaron ningún tema en absoluto. Esto nos deja con 743 perfiles de expertos holandeses y 727 perfiles de expertos en inglés utilizables. La Tabla 2 proporciona estadísticas descriptivas para la colección de Expertos de UvT. Las universidades tienden a tener una estructura jerárquica que va desde el nivel de la facultad, pasando por los departamentos, grupos de investigación, hasta llegar a los investigadores individuales. En la colección de Expertos de UvT tenemos información sobre las afiliaciones de los investigadores con facultades e institutos, lo que nos proporciona una jerarquía organizativa de dos niveles. La Universidad de Tilburg tiene 22 unidades organizativas a nivel de facultad (incluida la oficina universitaria y varios institutos de investigación) y 71 departamentos, lo que equivale a 3.2 departamentos por facultad. En cuanto a la jerarquía temática utilizada por Webwijs, 131 de los 1491 temas son nodos principales en la jerarquía. Esta jerarquía tiene una longitud promedio de cadena de temas de 2.65 y una longitud máxima de 7 temas. 6. EVALUACIÓN A continuación, evaluamos los modelos de la Sección 4 para la búsqueda y perfilado de expertos en la colección de Expertos de UvT. Detallamos nuestras preguntas de investigación y configuración experimental, y luego presentamos nuestros resultados. 6.1 Preguntas de investigación Abordamos las siguientes preguntas de investigación. Tanto la búsqueda de expertos como el perfilado dependen de las estimaciones de p(q|ca). La pregunta es cómo se comparan los modelos en las diferentes tareas, y en el contexto de la colección de expertos de UvT. En [4], el Modelo 2 superó al Modelo 1 en la colección de la W3C. ¿Cómo se comparan en nuestro conjunto de datos? ¿Y cómo se compara el Modelo 3 con el Modelo 1? ¿Qué hay de las diferencias de rendimiento entre los dos idiomas en nuestra colección de pruebas? 6.2 Configuración Experimental La salida de nuestros modelos fue evaluada frente a las etiquetas de temas autoasignadas, que fueron tratadas como juicios de relevancia. Los resultados fueron evaluados por separado para inglés y holandés. Para inglés solo usamos temas para los cuales había traducción al neerlandés; para neerlandés se consideraron todos los temas. Los resultados se promediaron para las consultas en la intersección de las evaluaciones de relevancia y los resultados; las consultas faltantes no contribuyen con un valor de 0 a las puntuaciones. Utilizamos medidas estándar de recuperación de información, como la Precisión Media Promedio (MAP) y la Reciprocidad Media Promedio (MRR). También informamos el porcentaje de temas (%q) y candidatos (%ca) cubiertos, respectivamente, para las tareas de búsqueda y perfilado de expertos. 6.3 Resultados La Tabla 1 muestra el rendimiento de los Modelos 1, 2 y 3 en las tareas de búsqueda y perfilado de expertos. Las filas de la tabla corresponden a los diversos tipos de documentos (RD, CD, PUB y HP) y a sus combinaciones. RD+CD+PUB+HP es equivalente a la colección completa y será referida como la LÍNEA BASE de nuestros experimentos. Al observar la Tabla 1, vemos que el Modelo 2 tiene el mejor rendimiento en general. Sin embargo, cuando los datos están limpios y muy enfocados (RD), el Modelo 3 lo supera en varios casos. El modelo 1 tiene la mejor cobertura de candidatos (%ca) y temas (%q). Los diversos tipos de documentos difieren en sus características y en cómo mejoran las tareas de búsqueda y perfilado. El perfilado de expertos se beneficia mucho de los datos limpios presentes en los tipos de documentos RD y CD, mientras que las publicaciones contribuyen principalmente a la tarea de encontrar expertos. Agregar las páginas de inicio no resulta ser particularmente útil. Cuando comparamos los resultados entre idiomas, encontramos que la cobertura de los temas en inglés (%q) es mayor que la de los temas en neerlandés para la búsqueda de expertos. Además de eso, las puntuaciones caen en el mismo rango para ambos idiomas. Para la tarea de perfilado, la cobertura de los candidatos (%ca) es muy similar para ambos idiomas. Sin embargo, el rendimiento es considerablemente mejor para los temas en inglés. Si bien es difícil comparar las puntuaciones entre colecciones, concluimos con una breve comparación de las puntuaciones absolutas en la Tabla 1 con las reportadas en [3, 4] en el conjunto de pruebas de W3C (edición de 2005). Para el experto que busca, los puntajes MAP para el Modelo 2 reportados aquí son aproximadamente un 50% más altos que las cifras correspondientes en [4], mientras que nuestros puntajes MRR están ligeramente por debajo de los de [4]. Para el perfilado de expertos, las diferencias son mucho más dramáticas: las puntuaciones MAP para el Modelo 2 reportadas aquí son aproximadamente un 50% más bajas que las puntuaciones en [3], mientras que las puntuaciones MRR (mejores) son aproximadamente iguales a las de [3]. La causa de estas diferencias parece residir en el número de áreas de conocimiento consideradas aquí, aproximadamente 30 veces más que en el entorno del W3C. MODELOS AVANZADOS Ahora que hemos desarrollado y evaluado técnicas básicas de modelado de lenguaje para la recuperación de expertos, nos enfocamos en modelos refinados que explotan características especiales de nuestra colección de pruebas. 7.1 Explotando la similitud en el área de conocimiento Una forma de mejorar la puntuación de una consulta dada un candidato es considerar qué otras solicitudes satisfaría el candidato y usarlas como evidencia adicional para respaldar la consulta original, proporcionalmente. Búsqueda de expertos Perfilado de expertos Tipos de documentos Modelo 1 Modelo 2 Modelo 3 Modelo 1 Modelo 2 Modelo 3 %q MAP MRR %q MAP MRR %q MAP MRR %ca MAP MRR %ca MAP MRR %ca MAP MRR Inglés RD 97.8 0.126 0.269 83.5 0.144 0.311 83.3 0.129 0.271 100 0.089 0.189 39.3 0.232 0.465 41.1 0.166 0.337 CD 97.8 0.118 0.227 91.7 0.123 0.248 91.7 0.118 0.226 32.8 0.188 0.381 32.4 0.195 0.385 32.7 0.203 0.370 PUB 97.8 0.200 0.330 98.0 0.216 0.372 98.0 0.145 0.257 78.9 0.167 0.364 74.5 0.212 0.442 78.9 0.135 0.299 HP 97.8 0.081 0.186 97.4 0.071 0.168 97.2 0.062 0.149 31.2 0.150 0.299 28.8 0.185 0.335 30.1 0.136 0.287 RD+CD 97.8 0.188 0.352 92.9 0.193 0.360 92.9 0.150 0.273 100 0.145 0.286 61.3 0.251 0.477 63.2 0.217 0.416 RD+CD+PUB 97.8 0.235 0.373 98.1 0.277 0.439 98.1 0.178 0.305 100 0.196 0.380 87.2 0.280 0.533 89.5 0.170 0.344 RD+CD+PUB+HP 97.8 0.237 0.372 98.6 0.280 0.441 98.5 0.166 0.293 100 0.199 0.387 88.7 0.281 0.525 90.9 0.169 0.329 Holandés RD 61.3 0.094 0.229 38.4 0.137 0.336 38.3 0.127 0.295 38.0 0.127 0.386 34.1 0.138 0.420 38.0 0.105 0.327 CD 61.3 0.107 0.212 49.7 0.128 0.256 49.7 0.136 0.261 32.5 0.151 0.389 31.8 0.158 0.396 32.5 0.170 0.380 PUB 61.3 0.193 0.319 59.5 0.218 0.368 59.4 0.173 0.291 78.8 0.126 0.364 76.0 0.150 0.424 78.8 0.103 0.294 HP 61.3 0.063 0.169 56.6 0.064 0.175 56.4 0.062 0.163 29.8 0.108 0.308 27.8 0.125 0.338 29.8 0.098 0.255 RD+CD 61.3 0.159 0.314 51.9 0.184 0.360 51.9 0.169 0.324 60.5 0.151 0.410 57.2 0.166 0.431 60.4 0.159 0.384 RD+CD+PUB 61.3 0.244 0.398 61.5 0.260 0.424 61.4 0.210 0.350 90.3 0.165 0.445 88.2 0.189 0.479 90.3 0.126 0.339 RD+CD+PUB+HP 61.3 0.249 0.401 62.6 0.265 0.436 62.6 0.195 0.344 91.9 0.164 0.426 90.1 0.195 0.488 91.9 0.125 0.328 Tabla 1: Rendimiento de los modelos en las tareas de búsqueda y perfilado de expertos, utilizando diferentes tipos de documentos y sus combinaciones. %q es el número de temas cubiertos (se aplica a la tarea de búsqueda de expertos), %ca es el número de candidatos cubiertos (se aplica a la tarea de perfilado de expertos). Los bloques superior e inferior corresponden al inglés y al holandés respectivamente. Las mejores puntuaciones están en negrita. Esto se puede modelar interpolando entre el p(q|ca) y la evidencia adicional de todos los pedidos similares q, de la siguiente manera: p(q|ca) = λp(q|ca) + (1 − λ) X q p(q|q )p(q |ca), (9) donde p(q|q ) representa la similitud entre los dos temas q y q. Para poder trabajar con métodos de similitud que no son necesariamente probabilidades, establecemos p(q|q ) = w(q,q ) γ , donde γ es una constante de normalización, de modo que γ = P q w(q , q ). Consideramos cuatro métodos para calcular la puntuación de similitud entre dos temas. Tres enfoques se basan estrictamente en el contenido y establecen similitudes examinando patrones de <br>co-ocurrencia</br> de temas dentro de la colección, mientras que el último enfoque explota la estructura jerárquica de áreas temáticas que pueden estar presentes dentro de una organización (ver [7] para más ejemplos de integrar relaciones entre palabras en modelos de lenguaje). La métrica de divergencia Kullback-Leibler (KL) definida en la ecuación 8 proporciona una medida de cuán diferentes o similares son dos distribuciones de probabilidad. Se infiere un modelo de tema para q y q utilizando el método presentado en la Sección 4.1 para describir la consulta en todo el vocabulario. Dado que un puntaje KL más bajo significa que las consultas son más similares, permitimos que w(q, q ) = max(KL(θq||·) − KL(θq||θq )). El Información Mutua Puntual (PMI, [17]) es una medida de asociación utilizada en teoría de la información para determinar el grado de independencia entre variables. La dependencia entre dos consultas se refleja en la puntuación SI(q, q), donde las puntuaciones mayores que cero indican que es probable que exista una dependencia, lo que interpretamos como que las consultas son probablemente similares: SI(q, q) = log p(q, q) / p(q)p(q). Estimamos la probabilidad de un tema p(q) utilizando el número de documentos relevantes para la consulta q dentro de la colección. La probabilidad conjunta p(q, q) se estima de manera similar, utilizando la concatenación de q y q como una consulta. Para obtener p(q|q), luego establecemos w(q, q) = SI(q, q) cuando SI(q, q) > 0, de lo contrario w(q, q) = 0, ya que solo estamos interesados en incluir consultas que sean similares. La estadística de log-verosimilitud proporciona otra medida de dependencia, que es más confiable que la medida de información mutua puntual [17]. Sea k1 el número de co-ocurrencias de q y q, k2 el número de ocurrencias de q que no co-ocurren con q, n1 el número total de ocurrencias de q, y n2 el número total de tokens de tema menos el número de ocurrencias de q. Entonces, sea p1 = k1/n1, p2 = k2/n2 y p = (k1 + k2)/(n1 + n2), (q, q ) = 2( (p1, k1, n1) + (p2, k2, n2) − (p, k1, n1) − (p, k2, n2)), donde (p, n, k) = k log p + (n − k) log(1 − p). El puntaje más alto indica que las consultas también son probablemente similares, por lo tanto, establecemos w(q, q ) = (q, q ). Finalmente, también estimamos la similitud de dos temas basándonos en su distancia dentro de la jerarquía de temas. La jerarquía de temas se visualiza como un grafo dirigido, y para todos los pares de temas se calcula el camino más corto SP(q, q). Establecimos el puntaje de similitud como el recíproco del camino más corto: w(q, q ) = 1/SP(q, q ). 7.2 Información contextual Dada la jerarquía de una organización, las unidades a las que pertenece una persona se consideran como un contexto para compensar la escasez de datos. Lo modelamos de la siguiente manera: p (q|ca) = 1 − P ou∈OU(ca) λou · p(q|ca) + P ou∈OU(ca) λou · p(q|ou), donde OU(ca) es el conjunto de unidades organizativas de las cuales el candidato ca es miembro, y p(q|o) expresa la fuerza de la asociación entre la consulta q y la unidad ou. La última probabilidad se puede estimar utilizando cualquiera de los tres modelos básicos, simplemente reemplazando ca con ou en las ecuaciones correspondientes. Una unidad organizativa está asociada con todos los documentos que sus miembros han escrito. Es decir, p(d|ou) = maxca∈ou p(d|ca). 7.3 Un modelo multilingüe simple Para institutos de conocimiento en Europa, ya sea académicos u otros, un entorno multilingüe (o al menos bilingüe) es típico. El siguiente modelo se basa en un tipo de suposición de independencia: no hay transferencia de conocimientos/perfiles a través de las barreras del idioma. Si bien es una simplificación, esta es una aproximación sensata en primer lugar. Eso es: p (q|ca) =P l∈L λl · p(ql|ca), donde L es el conjunto de idiomas utilizados en la colección, ql es la traducción de la consulta q al idioma l, y λl es un parámetro de suavizado específico del idioma, tal que P l∈L λl = 1. 8. MODELOS AVANZADOS: EVALUACIÓN En esta sección presentamos una evaluación experimental de nuestros modelos avanzados. Búsqueda de expertos Perfilado de expertos Modelo de lenguaje 1 Modelo 2 Modelo 3 Modelo 1 Modelo 2 Modelo 3 %q MAP MRR %q MAP MRR %q MAP MRR %ca MAP MRR %ca MAP MRR %ca MAP MRR Solo inglés 97.8 0.237 0.372 98.6 0.280 0.441 98.5 0.166 0.293 100 0.199 0.387 88.7 0.281 0.525 90.9 0.169 0.329 Solo holandés 61.3 0.249 0.401 62.6 0.265 0.436 62.6 0.195 0.344 91.9 0.164 0.426 90.1 0.195 0.488 91.9 0.125 0.328 Combinación 99.4 0.297 0.444 99.7 0.324 0.491 99.7 0.223 0.388 100 0.241 0.445 92.1 0.313 0.564 93.2 0.224 0.411 Tabla 3: Rendimiento de la combinación de idiomas en las tareas de búsqueda y perfilado de expertos (en candidatos). Los mejores puntajes para cada modelo están en cursiva, los mejores puntajes absolutos para las tareas de búsqueda y perfilado de expertos están en negrita. Tabla 4: Rendimiento en las tareas de búsqueda de expertos (arriba) y perfilado (abajo), utilizando similitudes de áreas de conocimiento. Las carreras fueron evaluadas en los temas principales establecidos. Las mejores puntuaciones están en negrita. 8.1 Preguntas de investigación Nuestras preguntas siguen los refinamientos presentados en la sección anterior: ¿Explotar la similitud en el área de conocimiento mejora la efectividad? ¿Cuál de los diversos métodos para capturar las relaciones entre palabras es más efectivo? Además, ¿es útil nuestra forma de incorporar información contextual? ¿Para qué tareas? Y finalmente, ¿es nuestra forma simple de combinar las puntuaciones monolingües suficiente para obtener mejoras significativas? 8.2 Configuración experimental Dado que las autoevaluaciones también son escasas en nuestra colección, para poder medir las diferencias entre los diversos modelos, seleccionamos un subconjunto de temas y evaluamos (algunas de las) ejecuciones solo en este subconjunto. Este conjunto se denomina como temas principales, y consiste en temas que se encuentran en el nivel superior de la jerarquía temática. (Un tema principal tiene subtemas, pero no es un subtema de ningún otro tema). Este conjunto principal consiste en 132 temas en holandés y 119 en inglés. Los juicios de relevancia se limitaron al conjunto de temas principales, pero no se ampliaron con subtemas. 8.3 Explotando la similitud de áreas de conocimiento. La Tabla 4 presenta los resultados. Los cuatro métodos utilizados para estimar la similitud de áreas de conocimiento son la divergencia de KL (KLDIV) y PointLang. Modelo 1 Modelo 2 Modelo 3 MAP MRR MAP MRR MAP MRR Búsqueda de expertos UK TODOS 0.423 0.545 0.654 0.799 0.494 0.629 UK PRINCIPAL 0.500 0.621 0.704 0.834 0.587 0.699 NL TODOS 0.439 0.560 0.672 0.826 0.480 0.630 NL PRINCIPAL 0.440 0.584 0.645 0.816 0.515 0.655 Perfilado de expertos UK TODOS 0.240 0.640 0.306 0.778 0.223 0.616 UK PRINCIPAL 0.523 0.677 0.519 0.648 0.461 0.587 NL TODOS 0.203 0.716 0.254 0.770 0.183 0.627 NL PRINCIPAL 0.332 0.576 0.380 0.624 0.332 0.549 Tabla 5: Evaluación de los modelos de contexto en unidades organizativas. información mutua ponderada (PMI), log-verosimilitud (LL) y distancia dentro de la jerarquía de temas (HDIST). Logramos mejorar el resultado base en todos los casos, pero la mejora es más notable para la tarea de perfilado. Para ambas tareas, el método LL tuvo el mejor rendimiento. Los enfoques basados en el contenido tuvieron un rendimiento consistentemente mejor que HDIST. 8.4 Información contextual Una jerarquía de dos niveles de unidades organizativas (facultades e institutos) está disponible en la colección de Expertos de UvT. La unidad a la que pertenece una persona se utiliza como contexto para esa persona. Primero, evaluamos los modelos de las unidades organizativas, utilizando todos los temas (TODOS) y solo los temas principales (PRINCIPALES). Una unidad organizativa se considera relevante para un tema dado (o viceversa) si al menos un miembro de la unidad seleccionó el tema dado como área de especialización. La Tabla 5 informa sobre los resultados. En lo que respecta a la búsqueda de expertos, dado un tema, la unidad organizativa correspondiente puede ser identificada con alta precisión. Sin embargo, la tarea de perfilado de expertos muestra una imagen diferente: las puntuaciones son bajas y la tarea parece difícil. La explicación puede ser que los conceptos generales (es decir, nuestros temas principales) pueden pertenecer a varias unidades organizativas. Segundo, realizamos otra evaluación, donde combinamos los modelos contextuales con los modelos candidatos (para puntuar a los candidatos nuevamente). La tabla 6 informa sobre los resultados. Encontramos un impacto positivo de los modelos de contexto solo para la búsqueda de expertos. Notablemente, para la búsqueda de expertos (y Modelo 1), mejora más del 50% (para inglés) y más del 70% (para holandés) en el MAP. El bajo rendimiento en la creación de perfiles de expertos puede deberse al hecho de que los modelos de contexto por sí solos no tuvieron un buen desempeño en la tarea de creación de perfiles desde el principio. 8.5 Modelos multilingües En esta subsección evaluamos el método para combinar resultados en múltiples idiomas que describimos en la Sección 7.3. En nuestro entorno, el conjunto de idiomas consiste en inglés y holandés: L = {UK, NL}. Los pesos de estos idiomas se establecieron para ser idénticos (λUK = λNL = 0.5). Realizamos experimentos con diferentes configuraciones de λ, pero no observamos diferencias significativas en el rendimiento. La Tabla 3 informa sobre los resultados multilingües, donde el rendimiento se evalúa en el conjunto completo de temas. Los tres modelos tienen una diferencia significativa. Tabla 6: Rendimiento de los modelos de contexto (CT) en comparación con la línea base (BL). Las mejores puntuaciones están en negrita, demostradas en todas las medidas para ambas tareas. La cobertura de temas y candidatos para las tareas de búsqueda y perfilado de expertos, respectivamente, es cercana al 100% en todos los casos. La mejora relativa de las puntuaciones de precisión varía del 10% al 80%. Estas puntuaciones demuestran que, a pesar de su simplicidad, nuestro método para combinar resultados en varios idiomas logra mejoras sustanciales sobre el punto de referencia. CONCLUSIONES En este artículo nos enfocamos en la recuperación de expertos (búsqueda y perfilado de expertos) en un nuevo entorno de una organización típica intensiva en conocimiento en la que los datos disponibles son de alta calidad, multilingües y abarcan una amplia gama de áreas de expertise. Normalmente, la cantidad de datos disponibles en una organización como una universidad, un instituto de investigación o un laboratorio de investigación es limitada en comparación con la colección de la W3C que ha sido utilizada principalmente para la evaluación experimental de la recuperación de la experiencia hasta ahora. Para examinar la recuperación de la experiencia en este entorno, presentamos (y lanzamos) la colección de Expertos de UvT como un caso representativo de organizaciones intensivas en conocimiento. La nueva colección refleja las propiedades típicas de los institutos intensivos en conocimiento mencionados anteriormente e incluye también varias características que pueden ser potencialmente útiles para la recuperación de expertos, como la estructura temática y organizativa. Evaluamos cómo los modelos de vanguardia para la búsqueda y perfilado de expertos se desempeñaron en este nuevo entorno y luego refinamos estos modelos para intentar explotar las diferentes características dentro del entorno de datos (idioma, actualidad y estructura organizativa). Encontramos que los modelos actuales de recuperación de experiencia se generalizan bien a este nuevo entorno; además, descubrimos que refinar los modelos para tener en cuenta las diferencias resulta en mejoras significativas, compensando así los problemas causados por la escasez de datos. El trabajo futuro incluye establecer evaluaciones manuales de perfiles generados automáticamente por los propios empleados, especialmente en casos en los que los empleados no han proporcionado un perfil ellos mismos. 10. AGRADECIMIENTOS Krisztian Balog fue apoyado por la Organización Neerlandesa para la Investigación Científica (NWO) bajo el número de proyecto 220-80-001. Maarten de Rijke también recibió apoyo de NWO bajo los números de proyecto 017.001.190, 220-80-001, 264-70-050, 354-20-005, 600.065.120, 612-13-001, 612.000.106, 612.066.302, 612.069.006, 640.001.501, 640.002.501, y de la Unión Europea. Programa IST del 6º Programa Marco para la I+D en virtud del contrato del proyecto MultiMATCH IST-033104. El trabajo de Toine Bogers y Antal van den Bosch fue financiado por el programa IOP-MMI de SenterNovem / Ministerio de Asuntos Económicos de los Países Bajos, como parte del proyecto 'A Propos'. 11. REFERENCIAS [1] L. Azzopardi. Incorporando contexto en el marco de modelado del lenguaje para la recuperación de información ad-hoc. Tesis doctoral, Universidad de Paisley, 2005. [2] K. Balog y M. de Rijke. Encontrando expertos similares. En este volumen, 2007. [3] K. Balog y M. de Rijke. Determinación de perfiles de expertos (con una aplicación a la búsqueda de expertos). En IJCAI 07: Proc. 20th Intern. Conferencia Conjunta sobre Inteligencia Artificial, páginas 2657-2662, 2007. [4] K. Balog, L. Azzopardi y M. de Rijke. Modelos formales para la búsqueda de expertos en corporaciones empresariales. En SIGIR 06: Actas de la 29ª conferencia internacional anual. Conferencia ACM SIGIR sobre Investigación y desarrollo en recuperación de información, páginas 43-50, 2006. [5] I. Becerra-Fernández. El papel de las tecnologías de inteligencia artificial en la implementación de sistemas de gestión del conocimiento de localización de personas. En el taller de AAAI sobre la incorporación de conocimiento en los procesos de negocio, marzo de 2000. [6] C. S. Campbell, P. P. Maglio, A. Cozzi y B. Dom. Identificación de la experiencia utilizando comunicaciones por correo electrónico. En CIKM 03: Actas de la duodécima conferencia internacional sobre gestión de la información y el conocimiento, páginas 528-531, 2003. [7] G. Cao, J.-Y. Nie, y J. Bai. Integrando relaciones entre palabras en modelos de lenguaje. En SIGIR 05: Actas de la 28ª conferencia internacional anual. Conferencia ACM SIGIR sobre Investigación y desarrollo en recuperación de información, páginas 298-305, 2005. [8] T. M. Cover y J. A. Thomas. Elementos de la teoría de la información. Wiley-Interscience, 1991. [9] N. Craswell, D. Hawking, A. M. Vercoustre y P. Wilkins. Experto en P@noptic: Buscando expertos no solo documentos. En Ausweb, 2001. [10] N. Craswell, A. de Vries e I. Soboroff. Resumen de la pista empresarial TREC2005. En la Decimocuarta Conferencia de Recuperación de Información. Proc. (TREC 2005), 2006. [11] T. H. Davenport y L. Prusak. Conocimiento en Acción: Cómo las Organizaciones Gestionan lo que Saben. Harvard Business School Press, Boston, MA, 1998. [12] T. Dunning.\nHarvard Business School Press, Boston, MA, 1998. [12] T. Dunning. Métodos precisos para la estadística de sorpresa y coincidencia. Lingüística Computacional, 19(1):61-74, 1993. [13] E. Filatova y J. Prager. Dime qué haces y te diré quién eres: Aprendiendo actividades relacionadas con la ocupación para biografías. En HLT/EMNLP, 2005. [14] V. Lavrenko y W. B. Croft. Modelos de lenguaje basados en relevancia. En SIGIR 01: Actas de la 24ª conferencia internacional anual. Conferencia ACM SIGIR sobre Investigación y Desarrollo en Recuperación de Información, páginas 120-127, 2001. [15] V. Lavrenko, M. Choquette y W. B. Croft. Modelos de relevancia multilingües. En SIGIR 02: Actas de la 25ª conferencia internacional anual. Conferencia ACM SIGIR sobre Investigación y Desarrollo en Recuperación de Información, páginas 175-182, 2002. [16] C. Macdonald e I. Ounis. Votación de candidatos: adaptando técnicas de fusión de datos para una tarea de búsqueda de expertos. En CIKM 06: Actas de la 15ª conferencia internacional de ACM sobre gestión de la información y el conocimiento, páginas 387-396, 2006. [17] C. Manning y H. Schütze. Fundamentos del Procesamiento del Lenguaje Natural Estadístico. El MIT Press, 1999. [18] A. Mockus y J. D. Herbsleb. Explorador de experticia: un enfoque cuantitativo para identificar la experticia. En ICSE 02: Proc. 24th Intern. Conf. en Ingeniería de Software, páginas 503-512, 2002. [19] D. Petkova y W. B. Croft. Modelos de lenguaje jerárquicos para la búsqueda de expertos en corporaciones empresariales. En Proc. ICTAI 2006, páginas 599-608, 2006. [20] I. Soboroff, A. de Vries y N. Craswell. Resumen de la pista empresarial TREC 2006. En las Notas de Trabajo de TREC 2006, 2006. [21] T. Tao, X. Wang, Q. Mei y C. Zhai. Recuperación de información del modelo de lenguaje con expansión de documentos. En HLT-NAACL 2006, 2006. [22] TREC. Ruta empresarial, 2005. URL: http://www.ins.cwi.nl/projects/trec-ent/wiki/. [23] G. van Noord.\nURL: http://www.ins.cwi.nl/projects/trec-ent/wiki/. [23] G. van Noord. Adivinador de idioma TextCat. URL: http://www.let.rug.nl/˜vannoord/TextCat/. [24] W3C. La colección de pruebas del W3C, 2005. URL: http://research.microsoft.com/users/nickcr/w3c-summary.html. ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "topicality and organizational structure": {
            "translated_key": "",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Broad Expertise Retrieval in Sparse Data Environments Krisztian Balog ISLA, University of Amsterdam Kruislaan 403, 1098 SJ Amsterdam, The Netherlands kbalog@science.uva.nl Toine Bogers ILK, Tilburg University P.O.",
                "Box 90153, 5000 LE Tilburg, The Netherlands A.M.Bogers@uvt.nl Leif Azzopardi Dept. of Computing Science University of Glasgow, Glasgow, G12 8QQ leif@dcs.gla.ac.uk Maarten de Rijke ISLA, University of Amsterdam Kruislaan 403, 1098 SJ Amsterdam, The Netherlands mdr@science.uva.nl Antal van den Bosch ILK, Tilburg University P.O.",
                "Box 90153, 5000 LE Tilburg, The Netherlands Antal.vdnBosch@uvt.nl ABSTRACT Expertise retrieval has been largely unexplored on data other than the W3C collection.",
                "At the same time, many intranets of universities and other knowledge-intensive organisations offer examples of relatively small but clean multilingual expertise data, covering broad ranges of expertise areas.",
                "We first present two main expertise retrieval tasks, along with a set of baseline approaches based on generative language modeling, aimed at finding expertise relations between topics and people.",
                "For our experimental evaluation, we introduce (and release) a new test set based on a crawl of a university site.",
                "Using this test set, we conduct two series of experiments.",
                "The first is aimed at determining the effectiveness of baseline expertise retrieval methods applied to the new test set.",
                "The second is aimed at assessing refined models that exploit characteristic features of the new test set, such as the organizational structure of the university, and the hierarchical structure of the topics in the test set.",
                "Expertise retrieval models are shown to be robust with respect to environments smaller than the W3C collection, and current techniques appear to be generalizable to other settings.",
                "Categories and Subject Descriptors H.3 [Information Storage and Retrieval]: H.3.1 Content Analysis and Indexing; H.3.3 Information Search and Retrieval; H.3.4 Systems and Software; H.4 [Information Systems Applications]: H.4.2 Types of Systems; H.4.m Miscellaneous General Terms Algorithms, Measurement, Performance, Experimentation 1.",
                "INTRODUCTION An organizations intranet provides a means for exchanging information between employees and for facilitating employee collaborations.",
                "To efficiently and effectively achieve this, it is necessary to provide search facilities that enable employees not only to access documents, but also to identify expert colleagues.",
                "At the TREC Enterprise Track [22] the need to study and understand expertise retrieval has been recognized through the introduction of Expert Finding tasks.",
                "The goal of expert finding is to identify a list of people who are knowledgeable about a given topic.",
                "This task is usually addressed by uncovering associations between people and topics [10]; commonly, a co-occurrence of the name of a person with topics in the same context is assumed to be evidence of expertise.",
                "An alternative task, which using the same idea of people-topic associations, is expert profiling, where the task is to return a list of topics that a person is knowledgeable about [3].",
                "The launch of the Expert Finding task at TREC has generated a lot of interest in expertise retrieval, with rapid progress being made in terms of modeling, algorithms, and evaluation aspects.",
                "However, nearly all of the expert finding or profiling work performed has been validated experimentally using the W3C collection [24] from the Enterprise Track.",
                "While this collection is currently the only publicly available test collection for expertise retrieval tasks, it only represents one type of intranet.",
                "With only one test collection it is not possible to generalize conclusions to other realistic settings.",
                "In this paper we focus on expertise retrieval in a realistic setting that differs from the W3C setting-one in which relatively small amounts of clean, multilingual data are available, that cover a broad range of expertise areas, as can be found on the intranets of universities and other knowledge-intensive organizations.",
                "Typically, this setting features several additional types of structure: topical structure (e.g., topic hierarchies as employed by the organization), organizational structure (faculty, department, ...), as well as multiple types of documents (research and course descriptions, publications, and academic homepages).",
                "This setting is quite different from the W3C setting in ways that might impact upon the performance of expertise retrieval tasks.",
                "We focus on a number of research questions in this paper: Does the relatively small amount of data available on an intranet affect the quality of the topic-person associations that lie at the heart of expertise retrieval algorithms?",
                "How do state-of-the-art algorithms developed on the W3C data set perform in the alternative scenario of the type described above?",
                "More generally, do the lessons from the Expert Finding task at TREC carry over to this setting?",
                "How does the inclusion or exclusion of different documents affect expertise retrieval tasks?",
                "In addition to, how can the topical and organizational structure be used for retrieval purposes?",
                "To answer our research questions, we first present a set of baseline approaches, based on generative language modeling, aimed at finding associations between topics and people.",
                "This allows us to formulate the expert finding and expert profiling tasks in a uniform way, and has the added benefit of allowing us to understand the relations between the two tasks.",
                "For our experimental evaluation, we introduce a new data set (the UvT Expert Collection) which is representative of the type of intranet that we described above.",
                "Our collection is based on publicly available data, crawled from the website of Tilburg University (UvT).",
                "This type of data is particularly interesting, since (1) it is clean, heterogeneous, structured, and focused, but comprises a limited number of documents; (2) contains information on the organizational hierarchy; (3) it is bilingual (English and Dutch); and (4) the list of expertise areas of an individual are provided by the employees themselves.",
                "Using the UvT Expert collection, we conduct two sets of experiments.",
                "The first is aimed at determining the effectiveness of baseline expertise finding and profiling methods in this new setting.",
                "A second group of experiments is aimed at extensions of the baseline methods that exploit characteristic features of the UvT Expert Collection; specifically, we propose and evaluate refined expert finding and profiling methods that incorporate <br>topicality and organizational structure</br>.",
                "Apart from the research questions and data set that we contribute, our main contributions are as follows.",
                "The baseline models developed for expertise finding perform well on the new data set.",
                "While on the W3C setting the expert finding task appears to be more difficult than profiling, for the UvT data the opposite is the case.",
                "We find that profiling on the UvT data set is considerably more difficult than on the W3C set, which we believe is due to the large (but realistic) number of topical areas that we used for profiling: about 1,500 for the UvT set, versus 50 in the W3C case.",
                "Taking the similarity between topics into account can significantly improve retrieval performance.",
                "The best performing similarity measures are content-based, therefore they can be applied on the W3C (and other) settings as well.",
                "Finally, we demonstrate that the organizational structure can be exploited in the form of a context model, improving MAP scores for certain models by up to 70%.",
                "The remainder of this paper is organized as follows.",
                "In the next section we review related work.",
                "Then, in Section 3 we provide detailed descriptions of the expertise retrieval tasks that we address in this paper: expert finding and expert profiling.",
                "In Section 4 we present our baseline models, of which the performance is then assessed in Section 6 using the UvT data set that we introduce in Section 5.",
                "Advanced models exploiting specific features of our data are presented in Section 7 and evaluated in Section 8.",
                "We formulate our conclusions in Section 9. 2.",
                "RELATED WORK Initial approaches to expertise finding often employed databases containing information on the skills and knowledge of each individual in the organization [11].",
                "Most of these tools (usually called yellow pages or people-finding systems) rely on people to self-assess their skills against a predefined set of keywords.",
                "For updating profiles in these systems in an automatic fashion there is a need for intelligent technologies [5].",
                "More recent approaches use specific document sets (such as email [6] or software [18]) to find expertise.",
                "In contrast with focusing on particular document types, there is also an increased interest in the development of systems that index and mine published intranet documents as sources of evidence for expertise.",
                "One such published approach is the P@noptic system [9], which builds a representation of each person by concatenating all documents associated with that person-this is similar to Model 1 of Balog et al. [4], who formalize and compare two methods.",
                "Balog et al.s Model 1 directly models the knowledge of an expert from associated documents, while their Model 2 first locates documents on the topic and then finds the associated experts.",
                "In the reported experiments the second method performs significantly better when there are sufficiently many associated documents per candidate.",
                "Most systems that took part in the 2005 and 2006 editions of the Expert Finding task at TREC implemented (variations on) one of these two models; see [10, 20].",
                "Macdonald and Ounis [16] propose a different approach for ranking candidate expertise with respect to a topic based on data fusion techniques, without using collectionspecific heuristics; they find that applying field-based weighting models improves the ranking of candidates.",
                "Petkova and Croft [19] propose yet another approach, based on a combination of the above Model 1 and 2, explicitly modeling topics.",
                "Turning to other expert retrieval tasks that can also be addressed using topic-people associations, Balog and de Rijke [3] addressed the task of determining topical expert profiles.",
                "While their methods proved to be efficient on the W3C corpus, they require an amount of data that may not be available in the typical knowledge-intensive organization.",
                "Balog and de Rijke [2] study the related task of finding experts that are similar to a small set of experts given as input.",
                "As an aside, creating a textual summary of a person shows some similarities to biography finding, which has received a considerable amount of attention recently; see e.g., [13].",
                "We use generative language modeling to find associations between topics and people.",
                "In our modeling of expert finding and profiling we collect evidence for expertise from multiple sources, in a heterogeneous collection, and integrate it with the co-occurrence of candidates names and query terms-the language modeling setting allows us to do this in a transparent manner.",
                "Our modeling proceeds in two steps.",
                "In the first step, we consider three baseline models, two taken from [4] (the Models 1 and 2 mentioned above), and one a refined version of a model introduced in [3] (which we refer to as Model 3 below); this third model is also similar to the model described by Petkova and Croft [19].",
                "The models we consider in our second round of experiments are mixture models similar to contextual language models [1] and to the expanded documents of Tao et al. [21]; however, the features that we use for definining our expansions-including topical structure and organizational structure-have not been used in this way before. 3.",
                "TASKS In the expertise retrieval scenario that we envisage, users seeking expertise within an organization have access to an interface that combines a search box (where they can search for experts or topics) with navigational structures (of experts and of topics) that allows them to click their way to an expert page (providing the profile of a person) or a topic page (providing a list of experts on the topic).",
                "To feed the above interface, we face two expertise retrieval tasks, expert finding and expert profiling, that we first define and then formalize using generative language models.",
                "In order to model either task, the probability of the query topic being associated to a candidate expert plays a key role in the final estimates for searching and profiling.",
                "By using language models, both the candidates and the query are characterized by distributions of terms in the vocabulary (used in the documents made available by the organization whose expertise retrieval needs we are addressing). 3.1 Expert finding Expert finding involves the task of finding the right person with the appropriate skills and knowledge: Who are the experts on topic X?.",
                "E.g., an employee wants to ascertain who worked on a particular project to find out why particular decisions were made without having to trawl through documentation (if there is any).",
                "Or, they may be in need a trained specialist for consultancy on a specific problem.",
                "Within an organization there are usually many possible candidates who could be experts for given topic.",
                "We can state this problem as follows: What is the probability of a candidate ca being an expert given the query topic q?",
                "That is, we determine p(ca|q), and rank candidates ca according to this probability.",
                "The candidates with the highest probability given the query are deemed the most likely experts for that topic.",
                "The challenge is how to estimate this probability accurately.",
                "Since the query is likely to consist of only a few terms to describe the expertise required, we should be able to obtain a more accurate estimate by invoking Bayes Theorem, and estimating: p(ca|q) = p(q|ca)p(ca) p(q) , (1) where p(ca) is the probability of a candidate and p(q) is the probability of a query.",
                "Since p(q) is a constant, it can be ignored for ranking purposes.",
                "Thus, the probability of a candidate ca being an expert given the query q is proportional to the probability of a query given the candidate p(q|ca), weighted by the a priori belief p(ca) that candidate ca is an expert. p(ca|q) ∝ p(q|ca)p(ca) (2) In this paper our main focus is on estimating the probability of a query given the candidate p(q|ca), because this probability captures the extent to which the candidate knows about the query topic.",
                "Whereas the candidate priors are generally assumed to be uniformand thus will not influence the ranking-it has been demonstrated that a sensible choice of priors may improve the performance [20]. 3.2 Expert profiling While the task of expert searching was concerned with finding experts given a particular topic, the task of expert profiling seeks to answer a related question: What topics does a candidate know about?",
                "Essentially, this turns the questions of expert finding around.",
                "The profiling of an individual candidate involves the identification of areas of skills and knowledge that they have expertise about and an evaluation of the level of proficiency in each of these areas.",
                "This is the candidates topical profile.",
                "Generally, topical profiles within organizations consist of tabular structures which explicitly catalogue the skills and knowledge of each individual in the organization.",
                "However, such practice is limited by the resources available for defining, creating, maintaining, and updating these profiles over time.",
                "By focusing on automatic methods which draw upon the available evidence within the document repositories of an organization, our aim is to reduce the human effort associated with the maintenance of topical profiles1 .",
                "A topical profile of a candidate, then, is defined as a vector where each element i of the vector corresponds to the candidate cas expertise on a given topic ki, (i.e., s(ca, ki)).",
                "Each topic ki defines a particular knowledge area or skill that the organization uses to define the candidates topical profile.",
                "Thus, it is assumed that a list of topics, {k1, . . . , kn}, where n is the number of pre-defined topics, is given: profile(ca) = s(ca, k1), s(ca, k2), . . . , s(ca, kn) . (3) 1 Context and evidence are needed to help users of expertise finding systems to decide whom to contact when seeking expertise in a particular area.",
                "Examples of such context are: Who does she work with?",
                "What are her contact details?",
                "Is she well-connected, just in case she is not able to help us herself?",
                "What is her role in the organization?",
                "Who is her superior?",
                "Collaborators, and affiliations, etc. are all part of the candidates social profile, and can serve as a background against which the systems recommendations should be interpreted.",
                "In this paper we only address the problem of determining topical profiles, and leave social profiling to further work.",
                "We state the problem of quantifying the competence of a person on a certain knowledge area as follows: What is the probability of a knowledge area (ki) being part of the candidates (expertise) profile? where s(ca, ki) is defined by p(ki|ca).",
                "Our task, then, is to estimate p(ki|ca), which is equivalent to the problem of obtaining p(q|ca), where the topic ki is represented as a query topic q, i.e., a sequence of keywords representing the expertise required.",
                "Both the expert finding and profiling tasks rely on the accurate estimation of p(q|ca).",
                "The only difference derives from the prior probability that a person is an expert (p(ca)), which can be incorporated into the expert finding task.",
                "This prior does not apply to the profiling task since the candidate (individual) is fixed. 4.",
                "BASELINE MODELS In this section we describe our baseline models for estimating p(q|ca), i.e., associations between topics and people.",
                "Both expert finding and expert profiling boil down to this estimation.",
                "We employ three models for calculating this probability. 4.1 From topics to candidates Using Candidate Models: Model 1 Model 1 [4] defines the probability of a query given a candidate (p(q|ca)) using standard language modeling techniques, based on a multinomial unigram language model.",
                "For each candidate ca, a candidate language model θca is inferred such that the probability of a term given θca is nonzero for all terms, i.e., p(t|θca) > 0.",
                "From the candidate model the query is generated with the following probability: p(q|θca) = Y t∈q p(t|θca)n(t,q) , where each term t in the query q is sampled identically and independently, and n(t, q) is the number of times t occurs in q.",
                "The candidate language model is inferred as follows: (1) an empirical model p(t|ca) is computed; (2) it is smoothed with background probabilities.",
                "Using the associations between a candidate and a document, the probability p(t|ca) can be approximated by: p(t|ca) = X d p(t|d)p(d|ca), where p(d|ca) is the probability that candidate ca generates a supporting document d, and p(t|d) is the probability of a term t occurring in the document d. We use the maximum-likelihood estimate of a term, that is, the normalised frequency of the term t in document d. The strength of the association between document d and candidate ca expressed by p(d|ca) reflects the degree to which the candidates expertise is described using this document.",
                "The estimation of this probability is presented later, in Section 4.2.",
                "The candidate model is then constructed as a linear interpolation of p(t|ca) and the background model p(t) to ensure there are no zero probabilities, which results in the final estimation: p(q|θca) = (4) Y t∈q ( (1 − λ) X d p(t|d)p(d|ca) ! + λp(t) )n(t,q) .",
                "Model 1 amasses all the term information from all the documents associated with the candidate, and uses this to represent that candidate.",
                "This model is used to predict how likely a candidate would produce a query q.",
                "This can can be intuitively interpreted as the probability of this candidate talking about the query topic, where we assume that this is indicative of their expertise.",
                "Using Document Models: Model 2 Model 2 [4] takes a different approach.",
                "Here, the process is broken into two parts.",
                "Given a candidate ca, (1) a document that is associated with a candidate is selected with probability p(d|ca), and (2) from this document a query q is generated with probability p(q|d).",
                "Then the sum over all documents is taken to obtain p(q|ca), such that: p(q|ca) = X d p(q|d)p(d|ca). (5) The probability of a query given a document is estimated by inferring a document language model θd for each document d in a similar manner as the candidate model was inferred: p(t|θd) = (1 − λ)p(t|d) + λp(t), (6) where p(t|d) is the probability of the term in the document.",
                "The probability of a query given the document model is: p(q|θd) = Y t∈q p(t|θd)n(t,q) .",
                "The final estimate of p(q|ca) is obtained by substituting p(q|d) for p(q|θd) into Eq. 5 (see [4] for full details).",
                "Conceptually, Model 2 differs from Model 1 because the candidate is not directly modeled.",
                "Instead, the document acts like a hidden variable in the process which separates the query from the candidate.",
                "This process is akin to how a user may search for candidates with a standard search engine: initially by finding the documents which are relevant, and then seeing who is associated with that document.",
                "By examining a number of documents the user can obtain an idea of which candidates are more likely to discuss the topic q.",
                "Using Topic Models: Model 3 We introduce a third model, Model 3.",
                "Instead of attempting to model the query generation process via candidate or document models, we represent the query as a topic language model and directly estimate the probability of the candidate p(ca|q).",
                "This approach is similar to the model presented in [3, 19].",
                "As with the previous models, a language model is inferred, but this time for the query.",
                "We adapt the work of Lavrenko and Croft [14] to estimate a topic model from the query.",
                "The procedure is as follows.",
                "Given a collection of documents and a query topic q, it is assumed that there exists an unknown topic model θk that assigns probabilities p(t|θk) to the term occurrences in the topic documents.",
                "Both the query and the documents are samples from θk (as opposed to the previous approaches, where a query is assumed to be sampled from a specific document or candidate model).",
                "The main task is to estimate p(t|θk), the probability of a term given the topic model.",
                "Since the query q is very sparse, and as there are no examples of documents on the topic, this distribution needs to be approximated.",
                "Lavrenko and Croft [14] suggest a reasonable way of obtaining such an approximation, by assuming that p(t|θk) can be approximated by the probability of term t given the query q.",
                "We can then estimate p(t|q) using the joint probability of observing the term t together with the query terms, q1, . . . , qm, and dividing by the joint probability of the query terms: p(t|θk) ≈ p(t|q) = p(t, q1, . . . , qm) p(q1, . . . , qm) = p(t, q1, . . . , qm) P t ∈T p(t , q1, . . . , qm) , where p(q1, . . . , qm) = P t ∈T p(t , q1, . . . , qm), and T is the entire vocabulary of terms.",
                "In order to estimate the joint probability p(t, q1, . . . , qm), we follow [14, 15] and assume t and q1, . . . , qm are mutually independent, once we pick a source distribution from the set of underlying source distributions U.",
                "If we choose U to be a set of document models. then to construct this set, the query q would be issued against the collection, and the top n returned are assumed to be relevant to the topic, and thus treated as samples from the topic model. (Note that candidate models could be used instead.)",
                "With the document models forming U, the joint probability of term and query becomes: p(t, q1, . . . , qm) = X d∈U p(d) ˘ p(t|θd) mY i=1 p(qi|θd) ¯ . (7) Here, p(d) denotes the prior distribution over the set U, which reflects the relevance of the document to the topic.",
                "We assume that p(d) is uniform across U.",
                "In order to rank candidates according to the topic model defined, we use the Kullback-Leibler divergence metric (KL, [8]) to measure the difference between the candidate models and the topic model: KL(θk||θca) = X t p(t|θk) log p(t|θk) p(t|θca) . (8) Candidates with a smaller divergence from the topic model are considered to be more likely experts on that topic.",
                "The candidate model θca is defined in Eq. 4.",
                "By using KL divergence instead of the probability of a candidate given the topic model p(ca|θk), we avoid normalization problems. 4.2 Document-candidate associations For our models we need to be able to estimate the probability p(d|ca), which expresses the extent to which a document d characterizes the candidate ca.",
                "In [4], two methods are presented for estimating this probability, based on the number of person names recognized in a document.",
                "However, in our (intranet) setting it is reasonable to assume that authors of documents can unambiguously be identified (e.g., as the author of an article, the teacher assigned to a course, the owner of a web page, etc.)",
                "Hence, we set p(d|ca) to be 1 if candidate ca is author of document d, otherwise the probability is 0.",
                "In Section 6 we describe how authorship can be determined on different types of documents within the collection. 5.",
                "THE UVT EXPERT COLLECTION The UvT Expert collection used in the experiments in this paper fits the scenario outlined in Section 3.",
                "The collection is based on the Webwijs (Webwise) system developed at Tilburg University (UvT) in the Netherlands.",
                "Webwijs (http://www.uvt.nl/ webwijs/) is a publicly accessible database of UvT employees who are involved in research or teaching; currently, Webwijs contains information about 1168 experts, each of whom has a page with contact information and, if made available by the expert, a research description and publications list.",
                "In addition, each expert can select expertise areas from a list of 1491 topics and is encouraged to suggest new topics that need to be approved by the Webwijs editor.",
                "Each topic has a separate page that shows all experts associated with that topic and, if available, a list of related topics.",
                "Webwijs is available in Dutch and English, and this bilinguality has been preserved in the collection.",
                "Every Dutch Webwijs page has an English translation.",
                "Not all Dutch topics have an English translation, but the reverse is true: the 981 English topics all have a Dutch equivalent.",
                "About 42% of the experts teach courses at Tilburg University; these courses were also crawled and included in the profile.",
                "In addition, about 27% of the experts link to their academic homepage from their Webwijs page.",
                "These home pages were crawled and added to the collection. (This means that if experts put the full-text versions of their publications on their academic homepage, these were also available for indexing.)",
                "We also obtained 1880 full-text versions of publications from the UvT institutional repository and Dutch English no. of experts 1168 1168 no. of experts with ≥ 1 topic 743 727 no. of topics 1491 981 no. of expert-topic pairs 4318 3251 avg. no. of topics/expert 5.8 5.9 max. no. of topics/expert (no. of experts) 60 (1) 35 (1) min. no. of topics/expert (no. of experts) 1 (74) 1 (106) avg. no. of experts/topic 2.9 3.3 max. no. of experts/topic (no. of topics) 30 (1) 30 (1) min. no. of experts/topic (no. of topics) 1 (615) 1 (346) no. of experts with HP 318 318 no. of experts with CD 318 318 avg. no. of CDs per teaching expert 3.5 3.5 no. of experts with RD 329 313 no. of experts with PUB 734 734 avg. no. of PUBs per expert 27.0 27.0 avg. no. of PUB citations per expert 25.2 25.2 avg. no. of full-text PUBs per expert 1.8 1.8 Table 2: Descriptive statistics of the Dutch and English versions of the UvT Expert collection. converted them to plain text.",
                "We ran the TextCat [23] language identifier to classify the language of the home pages and the fulltext publications.",
                "We restricted ourselves to pages where the classifier was confident about the language used on the page.",
                "This resulted in four document types: research descriptions (RD), course descriptions (CD), publications (PUB; full-text and citationonly versions), and academic homepages (HP).",
                "Everything was bundled into the UvT Expert collection which is available at http: //ilk.uvt.nl/uvt-expert-collection/.",
                "The UvT Expert collection was extracted from a different organizational setting than the W3C collection and differs from it in a number of ways.",
                "The UvT setting is one with relatively small amounts of multilingual data.",
                "Document-author associations are clear and the data is structured and clean.",
                "The collection covers a broad range of expertise areas, as one can typically find on intranets of universities and other knowledge-intensive institutes.",
                "Additionally, our university setting features several types of structure (topical and organizational), as well as multiple document types.",
                "Another important difference between the two data sets is that the expertise areas in the UvT Expert collection are self-selected instead of being based on group membership or assignments by others.",
                "Size is another dimension along which the W3C and UvT Expert collections differ: the latter is the smaller of the two.",
                "Also realistic are the large differences in the amount of information available for each expert.",
                "Utilizing Webwijs is voluntary; 425 Dutch experts did not select any topics at all.",
                "This leaves us with 743 Dutch and 727 English usable expert profiles.",
                "Table 2 provides descriptive statistics for the UvT Expert collection.",
                "Universities tend to have a hierarchical structure that goes from the faculty level, to departments, research groups, down to the individual researchers.",
                "In the UvT Expert collection we have information about the affiliations of researchers with faculties and institutes, providing us with a two-level organizational hierarchy.",
                "Tilburg University has 22 organizational units at the faculty level (including the university office and several research institutes) and 71 departments, which amounts to 3.2 departments per faculty.",
                "As to the topical hierarchy used by Webwijs, 131 of the 1491 topics are top nodes in the hierarchy.",
                "This hierarchy has an average topic chain length of 2.65 and a maximum length of 7 topics. 6.",
                "EVALUATION Below, we evaluate Section 4s models for expert finding and profiling onthe UvT Expert collection.",
                "We detail our research questions and experimental setup, and then present our results. 6.1 Research Questions We address the following research questions.",
                "Both expert finding and profiling rely on the estimations of p(q|ca).",
                "The question is how the models compare on the different tasks, and in the setting of the UvT Expert collection.",
                "In [4], Model 2 outperformed Model 1 on the W3C collection.",
                "How do they compare on our data set?",
                "And how does Model 3 compare to Model 1?",
                "What about performance differences between the two languages in our test collection? 6.2 Experimental Setup The output of our models was evaluated against the self-assigned topic labels, which were treated as relevance judgements.",
                "Results were evaluated separately for English and Dutch.",
                "For English we only used topics for which the Dutch translation was available; for Dutch all topics were considered.",
                "The results were averaged for the queries in the intersection of relevance judgements and results; missing queries do not contribute a value of 0 to the scores.",
                "We use standard information retrieval measures, such as Mean Average Precision (MAP) and Mean Reciprocal Rank (MRR).",
                "We also report the percentage of topics (%q) and candidates (%ca) covered, for the expert finding and profiling tasks, respectively. 6.3 Results Table 1 shows the performance of Model 1, 2, and 3 on the expert finding and profiling tasks.",
                "The rows of the table correspond to the various document types (RD, CD, PUB, and HP) and to their combinations.",
                "RD+CD+PUB+HP is equivalent to the full collection and will be referred as the BASELINE of our experiments.",
                "Looking at Table 1 we see that Model 2 performs the best across the board.",
                "However, when the data is clean and very focused (RD), Model 3 outperforms it in a number of cases.",
                "Model 1 has the best coverage of candidates (%ca) and topics (%q).",
                "The various document types differ in their characteristics and how they improve the finding and profiling tasks.",
                "Expert profiling benefits much from the clean data present in the RD and CD document types, while the publications contribute the most to the expert finding task.",
                "Adding the homepages does not prove to be particularly useful.",
                "When we compare the results across languages, we find that the coverage of English topics (%q) is higher than of the Dutch ones for expert finding.",
                "Apart from that, the scores fall in the same range for both languages.",
                "For the profiling task the coverage of the candidates (%ca) is very similar for both languages.",
                "However, the performance is substantially better for the English topics.",
                "While it is hard to compare scores across collections, we conclude with a brief comparison of the absolute scores in Table 1 to those reported in [3, 4] on the W3C test set (2005 edition).",
                "For expert finding the MAP scores for Model 2 reported here are about 50% higher than the corresponding figures in [4], while our MRR scores are slightly below those in [4].",
                "For expert profiling, the differences are far more dramatic: the MAP scores for Model 2 reported here are around 50% below the scores in [3], while the (best) MRR scores are about the same as those in [3].",
                "The cause for the latter differences seems to reside in the number of knowledge areas considered here-approx. 30 times more than in the W3C setting. 7.",
                "ADVANCED MODELS Now that we have developed and assessed basic language modeling techniques for expertise retrieval, we turn to refined models that exploit special features of our test collection. 7.1 Exploiting knowledge area similarity One way to improve the scoring of a query given a candidate is to consider what other requests the candidate would satisfy and use them as further evidence to support the original query, proportional Expert finding Expert profiling Document types Model 1 Model 2 Model 3 Model 1 Model 2 Model 3 %q MAP MRR %q MAP MRR %q MAP MRR %ca MAP MRR %ca MAP MRR %ca MAP MRR English RD 97.8 0.126 0.269 83.5 0.144 0.311 83.3 0.129 0.271 100 0.089 0.189 39.3 0.232 0.465 41.1 0.166 0.337 CD 97.8 0.118 0.227 91.7 0.123 0.248 91.7 0.118 0.226 32.8 0.188 0.381 32.4 0.195 0.385 32.7 0.203 0.370 PUB 97.8 0.200 0.330 98.0 0.216 0.372 98.0 0.145 0.257 78.9 0.167 0.364 74.5 0.212 0.442 78.9 0.135 0.299 HP 97.8 0.081 0.186 97.4 0.071 0.168 97.2 0.062 0.149 31.2 0.150 0.299 28.8 0.185 0.335 30.1 0.136 0.287 RD+CD 97.8 0.188 0.352 92.9 0.193 0.360 92.9 0.150 0.273 100 0.145 0.286 61.3 0.251 0.477 63.2 0.217 0.416 RD+CD+PUB 97.8 0.235 0.373 98.1 0.277 0.439 98.1 0.178 0.305 100 0.196 0.380 87.2 0.280 0.533 89.5 0.170 0.344 RD+CD+PUB+HP 97.8 0.237 0.372 98.6 0.280 0.441 98.5 0.166 0.293 100 0.199 0.387 88.7 0.281 0.525 90.9 0.169 0.329 Dutch RD 61.3 0.094 0.229 38.4 0.137 0.336 38.3 0.127 0.295 38.0 0.127 0.386 34.1 0.138 0.420 38.0 0.105 0.327 CD 61.3 0.107 0.212 49.7 0.128 0.256 49.7 0.136 0.261 32.5 0.151 0.389 31.8 0.158 0.396 32.5 0.170 0.380 PUB 61.3 0.193 0.319 59.5 0.218 0.368 59.4 0.173 0.291 78.8 0.126 0.364 76.0 0.150 0.424 78.8 0.103 0.294 HP 61.3 0.063 0.169 56.6 0.064 0.175 56.4 0.062 0.163 29.8 0.108 0.308 27.8 0.125 0.338 29.8 0.098 0.255 RD+CD 61.3 0.159 0.314 51.9 0.184 0.360 51.9 0.169 0.324 60.5 0.151 0.410 57.2 0.166 0.431 60.4 0.159 0.384 RD+CD+PUB 61.3 0.244 0.398 61.5 0.260 0.424 61.4 0.210 0.350 90.3 0.165 0.445 88.2 0.189 0.479 90.3 0.126 0.339 RD+CD+PUB+HP 61.3 0.249 0.401 62.6 0.265 0.436 62.6 0.195 0.344 91.9 0.164 0.426 90.1 0.195 0.488 91.9 0.125 0.328 Table 1: Performance of the models on the expert finding and profiling tasks, using different document types and their combinations. %q is the number of topics covered (applies to the expert finding task), %ca is the number of candidates covered (applies to the expert profiling task).",
                "The top and bottom blocks correspond to English and Dutch respectively.",
                "The best scores are in boldface. to how related the other requests are to the original query.",
                "This can be modeled by interpolating between the p(q|ca) and the further supporting evidence from all similar requests q , as follows: p (q|ca) = λp(q|ca) + (1 − λ) X q p(q|q )p(q |ca), (9) where p(q|q ) represents the similarity between the two topics q and q .",
                "To be able to work with similarity methods that are not necessarily probabilities, we set p(q|q ) = w(q,q ) γ , where γ is a normalizing constant, such that γ = P q w(q , q ).",
                "We consider four methods for calculating the similarity score between two topics.",
                "Three approaches are strictly content-based, and establish similarity by examining co-occurrence patterns of topics within the collection, while the last approach exploits the hierarchical structure of topical areas that may be present within an organization (see [7] for further examples of integrating word relationships into language models).",
                "The Kullback-Leibler (KL) divergence metric defined in Eq. 8 provides a measure of how different or similar two probability distributions are.",
                "A topic model is inferred for q and q using the method presented in Section 4.1 to describe the query across the entire vocabulary.",
                "Since a lower KL score means the queries are more similar, we let w(q, q ) = max(KL(θq||·) − KL(θq||θq )).",
                "Pointwise Mutual Information (PMI, [17]) is a measure of association used in information theory to determine the extent of independence between variables.",
                "The dependence between two queries is reflected by the SI(q, q ) score, where scores greater than zero indicate that it is likely that there is a dependence, which we take to mean that the queries are likely to be similar: SI(q, q ) = log p(q, q ) p(q)p(q ) (10) We estimate the probability of a topic p(q) using the number of documents relevant to query q within the collection.",
                "The joint probability p(q, q ) is estimated similarly, by using the concatenation of q and q as a query.",
                "To obtain p(q|q ), we then set w(q, q ) = SI(q, q ) when SI(q, q ) > 0 otherwise w(q, q ) = 0, because we are only interested in including queries that are similar.",
                "The log-likelihood statistic provides another measure of dependence, which is more reliable than the pointwise mutual information measure [17].",
                "Let k1 be the number of co-occurrences of q and q , k2 the number of occurrences of q not co-occurring with q , n1 the total number of occurrences of q , and n2 the total number of topic tokens minus the number of occurrences of q .",
                "Then, let p1 = k1/n1, p2 = k2/n2, and p = (k1 + k2)/(n1 + n2), (q, q ) = 2( (p1, k1, n1) + (p2, k2, n2) − (p, k1, n1) − (p, k2, n2)), where (p, n, k) = k log p + (n − k) log(1 − p).",
                "The higher score indicate that queries are also likely to be similar, thus we set w(q, q ) = (q, q ).",
                "Finally, we also estimate the similarity of two topics based on their distance within the topic hierarchy.",
                "The topic hierarchy is viewed as a directed graph, and for all topic-pairs the shortest path SP(q, q ) is calculated.",
                "We set the similarity score to be the reciprocal of the shortest path: w(q, q ) = 1/SP(q, q ). 7.2 Contextual information Given the hierarchy of an organization, the units to which a person belong are regarded as a context so as to compensate for data sparseness.",
                "We model it as follows: p (q|ca) = 1 − P ou∈OU(ca) λou · p(q|ca) + P ou∈OU(ca) λou · p(q|ou), where OU(ca) is the set of organizational units of which candidate ca is a member of, and p(q|o) expresses the strength of the association between query q and the unit ou.",
                "The latter probability can be estimated using either of the three basic models, by simply replacing ca with ou in the corresponding equations.",
                "An organizational unit is associated with all the documents that its members have authored.",
                "That is, p(d|ou) = maxca∈ou p(d|ca). 7.3 A simple multilingual model For knowledge institutes in Europe, academic or otherwise, a multilingual (or at least bilingual) setting is typical.",
                "The following model builds on a kind of independence assumption: there is no spill-over of expertise/profiles across language boundaries.",
                "While a simplification, this is a sensible first approach.",
                "That is: p (q|ca) =P l∈L λl · p(ql|ca), where L is the set of languages used in the collection, ql is the translation of the query q to language l, and λl is a language specific smoothing parameter, such that P l∈L λl = 1. 8.",
                "ADVANCED MODELS: EVALUATION In this section we present an experimental evaluation of our advanced models.",
                "Expert finding Expert profiling Language Model 1 Model 2 Model 3 Model 1 Model 2 Model 3 %q MAP MRR %q MAP MRR %q MAP MRR %ca MAP MRR %ca MAP MRR %ca MAP MRR English only 97.8 0.237 0.372 98.6 0.280 0.441 98.5 0.166 0.293 100 0.199 0.387 88.7 0.281 0.525 90.9 0.169 0.329 Dutch only 61.3 0.249 0.401 62.6 0.265 0.436 62.6 0.195 0.344 91.9 0.164 0.426 90.1 0.195 0.488 91.9 0.125 0.328 Combination 99.4 0.297 0.444 99.7 0.324 0.491 99.7 0.223 0.388 100 0.241 0.445 92.1 0.313 0.564 93.2 0.224 0.411 Table 3: Performance of the combination of languages on the expert finding and profiling tasks (on candidates).",
                "Best scores for each model are in italic, absolute best scores for the expert finding and profiling tasks are in boldface.",
                "Method Model 1 Model 2 Model 3 MAP MRR MAP MRR MAP MRR English BASELINE 0.296 0.454 0.339 0.509 0.221 0.333 KLDIV 0.291 0.453 0.327 0.503 0.219 0.330 PMI 0.291 0.453 0.337 0.509 0.219 0.331 LL 0.319 0.490 0.360 0.524 0.233 0.368 HDIST 0.299 0.465 0.346 0.537 0.219 0.332 Dutch BASELINE 0.240 0.350 0.271 0.403 0.227 0.389 KLDIV 0.239 0.347 0.253 0.386 0.224 0.385 PMI 0.239 0.350 0.260 0.392 0.227 0.389 LL 0.255 0.372 0.281 0.425 0.231 0.389 HDIST 0.253 0.365 0.271 0.407 0.236 0.402 Method Model 1 Model 2 Model 3 MAP MRR MAP MRR MAP MRR English BASELINE 0.485 0.546 0.499 0.548 0.381 0.416 KLDIV 0.510 0.564 0.513 0.558 0.381 0.416 PMI 0.486 0.546 0.495 0.542 0.407 0.451 LL 0.558 0.589 0.586 0.617 0.408 0.453 HDIST 0.507 0.567 0.512 0.563 0.386 0.420 Dutch BASELINE 0.263 0.313 0.294 0.358 0.262 0.315 KLDIV 0.284 0.336 0.271 0.321 0.261 0.314 PMI 0.265 0.317 0.265 0.316 0.273 0.330 LL 0.312 0.351 0.330 0.377 0.284 0.331 HDIST 0.280 0.327 0.288 0.341 0.266 0.321 Table 4: Performance on the expert finding (top) and profiling (bottom) tasks, using knowledge area similarities.",
                "Runs were evaluated on the main topics set.",
                "Best scores are in boldface. 8.1 Research Questions Our questions follow the refinements presented in the preceding section: Does exploiting the knowledge area similarity improve effectiveness?",
                "Which of the various methods for capturing word relationships is most effective?",
                "Furthermore, is our way of bringing in contextual information useful?",
                "For which tasks?",
                "And finally, is our simple way of combining the monolingual scores sufficient for obtaining significant improvements? 8.2 Experimental setup Given that the self-assessments are also sparse in our collection, in order to be able to measure differences between the various models, we selected a subset of topics, and evaluated (some of the) runs only on this subset.",
                "This set is referred as main topics, and consists of topics that are located at the top level of the topical hierarchy. (A main topic has subtopics, but is not a subtopic of any other topic.)",
                "This main set consists of 132 Dutch and 119 English topics.",
                "The relevance judgements were restricted to the main topic set, but were not expanded with subtopics. 8.3 Exploiting knowledge area similarity Table 4 presents the results.",
                "The four methods used for estimating knowledge-area similarity are KL divergence (KLDIV), PointLang.",
                "Topics Model 1 Model 2 Model 3 MAP MRR MAP MRR MAP MRR Expert finding UK ALL 0.423 0.545 0.654 0.799 0.494 0.629 UK MAIN 0.500 0.621 0.704 0.834 0.587 0.699 NL ALL 0.439 0.560 0.672 0.826 0.480 0.630 NL MAIN 0.440 0.584 0.645 0.816 0.515 0.655 Expert profiling UK ALL 0.240 0.640 0.306 0.778 0.223 0.616 UK MAIN 0.523 0.677 0.519 0.648 0.461 0.587 NL ALL 0.203 0.716 0.254 0.770 0.183 0.627 NL MAIN 0.332 0.576 0.380 0.624 0.332 0.549 Table 5: Evaluating the context models on organizational units. wise mutual information (PMI), log-likelihood (LL), and distance within topic hierarchy (HDIST).",
                "We managed to improve upon the baseline in all cases, but the improvement is more noticeable for the profiling task.",
                "For both tasks, the LL method performed best.",
                "The content-based approaches performed consistently better than HDIST. 8.4 Contextual information A two level hierarchy of organizational units (faculties and institutes) is available in the UvT Expert collection.",
                "The unit a person belongs to is used as a context for that person.",
                "First, we evaluated the models of the organizational units, using all topics (ALL) and only the main topics (MAIN).",
                "An organizational unit is considered to be relevant for a given topic (or vice versa) if at least one member of the unit selected the given topic as an expertise area.",
                "Table 5 reports on the results.",
                "As far as expert finding goes, given a topic, the corresponding organizational unit can be identified with high precision.",
                "However, the expert profiling task shows a different picture: the scores are low, and the task seems hard.",
                "The explanation may be that general concepts (i.e., our main topics) may belong to several organizational units.",
                "Second, we performed another evaluation, where we combined the contextual models with the candidate models (to score candidates again).",
                "Table 6 reports on the results.",
                "We find a positive impact of the context models only for expert finding.",
                "Noticably, for expert finding (and Model 1), it improves over 50% (for English) and over 70% (for Dutch) on MAP.",
                "The poor performance on expert profiling may be due to the fact that context models alone did not perform very well on the profiling task to begin with. 8.5 Multilingual models In this subsection we evaluate the method for combining results across multiple languages that we described in Section 7.3.",
                "In our setting the set of languages consists of English and Dutch: L = {UK, NL}.",
                "The weights on these languages were set to be identical (λUK = λNL = 0.5).",
                "We performed experiments with various λ settings, but did not observe significant differences in performance.",
                "Table 3 reports on the multilingual results, where performance is evaluated on the full topic set.",
                "All three models significantly imLang.",
                "Method Model 1 Model 2 Model 3 MAP MRR MAP MRR MAP MRR Expert finding UK BL 0.296 0.454 0.339 0.509 0.221 0.333 UK CT 0.330 0.491 0.342 0.500 0.228 0.342 NL BL 0.240 0.350 0.271 0.403 0.227 0.389 NL CT 0.251 0.382 0.267 0.410 0.246 0.404 Expert profiling UK BL 0.485 0.546 0.499 0.548 0.381 0.416 UK CT 0.562 0.620 0.508 0.558 0.440 0.486 NL BL 0.263 0.313 0.294 0.358 0.262 0.315 NL CT 0.330 0.384 0.317 0.387 0.294 0.345 Table 6: Performance of the context models (CT) compared to the baseline (BL).",
                "Best scores are in boldface. proved over all measures for both tasks.",
                "The coverage of topics and candidates for the expert finding and profiling tasks, respectively, is close to 100% in all cases.",
                "The relative improvement of the precision scores ranges from 10% to 80%.",
                "These scores demonstrate that despite its simplicity, our method for combining results over multiple languages achieves substantial improvements over the baseline. 9.",
                "CONCLUSIONS In this paper we focused on expertise retrieval (expert finding and profiling) in a new setting of a typical knowledge-intensive organization in which the available data is of high quality, multilingual, and covering a broad range of expertise area.",
                "Typically, the amount of available data in such an organization (e.g., a university, a research institute, or a research lab) is limited when compared to the W3C collection that has mostly been used for the experimental evaluation of expertise retrieval so far.",
                "To examine expertise retrieval in this setting, we introduced (and released) the UvT Expert collection as a representative case of such knowledge intensive organizations.",
                "The new collection reflects the typical properties of knowledge-intensive institutes noted above and also includes several features which may are potentially useful for expertise retrieval, such as topical and organizational structure.",
                "We evaluated how current state-of-the-art models for expert finding and profiling performed in this new setting and then refined these models in order to try and exploit the different characteristics within the data environment (language, topicality, and organizational structure).",
                "We found that current models of expertise retrieval generalize well to this new environment; in addition we found that refining the models to account for the differences results in significant improvements, thus making up for problems caused by data sparseness issues.",
                "Future work includes setting up manual assessments of automatically generated profiles by the employees themselves, especially in cases where the employees have not provided a profile themselves. 10.",
                "ACKNOWLEDGMENTS Krisztian Balog was supported by the Netherlands Organisation for Scientific Research (NWO) under project number 220-80-001.",
                "Maarten de Rijke was also supported by NWO under project numbers 017.001.190, 220-80-001, 264-70-050, 354-20-005, 600.065.120, 612-13-001, 612.000.106, 612.066.302, 612.069.006, 640.001.501, 640.002.501, and by the E.U.",
                "IST programme of the 6th FP for RTD under project MultiMATCH contract IST-033104.",
                "The work of Toine Bogers and Antal van den Bosch was funded by the IOP-MMI-program of SenterNovem / The Dutch Ministry of Economic Affairs, as part of the `A Propos project. 11.",
                "REFERENCES [1] L. Azzopardi.",
                "Incorporating Context in the Language Modeling Framework for ad-hoc Information Retrieval.",
                "PhD thesis, University of Paisley, 2005. [2] K. Balog and M. de Rijke.",
                "Finding similar experts.",
                "In This volume, 2007. [3] K. Balog and M. de Rijke.",
                "Determining expert profiles (with an application to expert finding).",
                "In IJCAI 07: Proc. 20th Intern.",
                "Joint Conf. on Artificial Intelligence, pages 2657-2662, 2007. [4] K. Balog, L. Azzopardi, and M. de Rijke.",
                "Formal models for expert finding in enterprise corpora.",
                "In SIGIR 06: Proc. 29th annual intern.",
                "ACM SIGIR conf. on Research and development in information retrieval, pages 43-50, 2006. [5] I. Becerra-Fernandez.",
                "The role of artificial intelligence technologies in the implementation of people-finder knowledge management systems.",
                "In AAAI Workshop on Bringing Knowledge to Business Processes, March 2000. [6] C. S. Campbell, P. P. Maglio, A. Cozzi, and B. Dom.",
                "Expertise identification using email communications.",
                "In CIKM 03: Proc. twelfth intern. conf. on Information and knowledge management, pages 528531, 2003. [7] G. Cao, J.-Y.",
                "Nie, and J. Bai.",
                "Integrating word relationships into language models.",
                "In SIGIR 05: Proc. 28th annual intern.",
                "ACM SIGIR conf. on Research and development in information retrieval, pages 298-305, 2005. [8] T. M. Cover and J.",
                "A. Thomas.",
                "Elements of Information Theory.",
                "Wiley-Interscience, 1991. [9] N. Craswell, D. Hawking, A. M. Vercoustre, and P. Wilkins.",
                "P@noptic expert: Searching for experts not just for documents.",
                "In Ausweb, 2001. [10] N. Craswell, A. de Vries, and I. Soboroff.",
                "Overview of the TREC2005 Enterprise Track.",
                "In The Fourteenth Text REtrieval Conf.",
                "Proc. (TREC 2005), 2006. [11] T. H. Davenport and L. Prusak.",
                "Working Knowledge: How Organizations Manage What They Know.",
                "Harvard Business School Press, Boston, MA, 1998. [12] T. Dunning.",
                "Accurate methods for the statistics of surprise and coincidence.",
                "Computational Linguistics, 19(1):61-74, 1993. [13] E. Filatova and J. Prager.",
                "Tell me what you do and Ill tell you what you are: Learning occupation-related activities for biographies.",
                "In HLT/EMNLP, 2005. [14] V. Lavrenko and W. B. Croft.",
                "Relevance based language models.",
                "In SIGIR 01: Proc. 24th annual intern.",
                "ACM SIGIR conf. on Research and development in information retrieval, pages 120-127, 2001. [15] V. Lavrenko, M. Choquette, and W. B. Croft.",
                "Cross-lingual relevance models.",
                "In SIGIR 02: Proc. 25th annual intern.",
                "ACM SIGIR conf. on Research and development in information retrieval, pages 175-182, 2002. [16] C. Macdonald and I. Ounis.",
                "Voting for candidates: adapting data fusion techniques for an expert search task.",
                "In CIKM 06: Proc. 15th ACM intern. conf. on Information and knowledge management, pages 387-396, 2006. [17] C. Manning and H. Sch¨utze.",
                "Foundations of Statistical Natural Language Processing.",
                "The MIT Press, 1999. [18] A. Mockus and J. D. Herbsleb.",
                "Expertise browser: a quantitative approach to identifying expertise.",
                "In ICSE 02: Proc. 24th Intern.",
                "Conf. on Software Engineering, pages 503-512, 2002. [19] D. Petkova and W. B. Croft.",
                "Hierarchical language models for expert finding in enterprise corpora.",
                "In Proc.",
                "ICTAI 2006, pages 599-608, 2006. [20] I. Soboroff, A. de Vries, and N. Craswell.",
                "Overview of the TREC 2006 Enterprise Track.",
                "In TREC 2006 Working Notes, 2006. [21] T. Tao, X. Wang, Q. Mei, and C. Zhai.",
                "Language model information retrieval with document expansion.",
                "In HLT-NAACL 2006, 2006. [22] TREC.",
                "Enterprise track, 2005.",
                "URL: http://www.ins.cwi. nl/projects/trec-ent/wiki/. [23] G. van Noord.",
                "TextCat Language Guesser.",
                "URL: http://www. let.rug.nl/˜vannoord/TextCat/. [24] W3C.",
                "The W3C test collection, 2005.",
                "URL: http://research. microsoft.com/users/nickcr/w3c-summary.html."
            ],
            "original_annotated_samples": [
                "A second group of experiments is aimed at extensions of the baseline methods that exploit characteristic features of the UvT Expert Collection; specifically, we propose and evaluate refined expert finding and profiling methods that incorporate <br>topicality and organizational structure</br>."
            ],
            "translated_annotated_samples": [
                "Un segundo grupo de experimentos está dirigido a extensiones de los métodos base que aprovechan las características distintivas de la Colección de Expertos de UvT; específicamente, proponemos y evaluamos métodos refinados de búsqueda y perfilado de expertos que incorporan la topicalidad y la estructura organizativa."
            ],
            "translated_text": "Recuperación de amplia experiencia en entornos de datos dispersos Krisztian Balog ISLA, Universidad de Ámsterdam Kruislaan 403, 1098 SJ Ámsterdam, Países Bajos kbalog@science.uva.nl Toine Bogers ILK, Universidad de Tilburg P.O. Caja 90153, 5000 LE Tilburg, Países Bajos A.M.Bogers@uvt.nl Leif Azzopardi Dept. de Ciencias de la Computación Universidad de Glasgow, Glasgow, G12 8QQ leif@dcs.gla.ac.uk Maarten de Rijke ISLA, Universidad de Ámsterdam Kruislaan 403, 1098 SJ Ámsterdam, Países Bajos mdr@science.uva.nl Antal van den Bosch ILK, Universidad de Tilburg P.O. La recuperación de la experiencia ha sido ampliamente inexplorada en datos distintos a la colección de la W3C. Al mismo tiempo, muchas intranets de universidades y otras organizaciones intensivas en conocimiento ofrecen ejemplos de datos de expertos multilingües relativamente pequeños pero precisos, que abarcan amplias áreas de especialización. Primero presentamos dos tareas principales de recuperación de expertos, junto con un conjunto de enfoques de referencia basados en modelado del lenguaje generativo, con el objetivo de encontrar relaciones de experticia entre temas y personas. Para nuestra evaluación experimental, introducimos (y liberamos) un nuevo conjunto de pruebas basado en un rastreo de un sitio universitario. Usando este conjunto de pruebas, llevamos a cabo dos series de experimentos. El primero tiene como objetivo determinar la efectividad de los métodos de recuperación de conocimientos básicos aplicados al nuevo conjunto de pruebas. El segundo tiene como objetivo evaluar modelos refinados que exploten las características distintivas del nuevo conjunto de pruebas, como la estructura organizativa de la universidad y la estructura jerárquica de los temas en el conjunto de pruebas. Los modelos de recuperación de experiencia se muestran robustos con respecto a entornos más pequeños que la colección de la W3C, y las técnicas actuales parecen ser generalizables a otros contextos. Categorías y Descriptores de Asignaturas H.3 [Almacenamiento y Recuperación de Información]: H.3.1 Análisis de Contenido e Indexación; H.3.3 Búsqueda y Recuperación de Información; H.3.4 Sistemas y Software; H.4 [Aplicaciones de Sistemas de Información]: H.4.2 Tipos de Sistemas; H.4.m Términos Generales Misceláneos Algoritmos, Medición, Rendimiento, Experimentación 1. INTRODUCCIÓN La intranet de una organización proporciona un medio para intercambiar información entre empleados y facilitar colaboraciones entre ellos. Para lograr esto de manera eficiente y efectiva, es necesario proporcionar instalaciones de búsqueda que permitan a los empleados no solo acceder a documentos, sino también identificar a colegas expertos. En la pista de Empresas de TREC [22] se ha reconocido la necesidad de estudiar y comprender la recuperación de expertos a través de la introducción de tareas de Búsqueda de Expertos. El objetivo de la búsqueda de expertos es identificar una lista de personas que tienen conocimientos sobre un tema específico. Esta tarea suele abordarse descubriendo asociaciones entre personas y temas [10]; comúnmente, se asume que la co-ocurrencia del nombre de una persona con temas en el mismo contexto es evidencia de experiencia. Una tarea alternativa, que utiliza la misma idea de asociaciones entre personas y temas, es el perfilado de expertos, donde la tarea consiste en devolver una lista de temas sobre los que una persona tiene conocimiento [3]. El lanzamiento de la tarea de Búsqueda de Expertos en TREC ha generado mucho interés en la recuperación de expertos, con un rápido progreso en términos de modelado, algoritmos y aspectos de evaluación. Sin embargo, casi todo el trabajo de búsqueda o perfilado de expertos realizado ha sido validado experimentalmente utilizando la colección de W3C [24] de la pista empresarial. Si bien esta colección es actualmente la única colección de pruebas disponible públicamente para tareas de recuperación de expertos, solo representa un tipo de intranet. Con solo una colección de pruebas no es posible generalizar conclusiones a otros escenarios realistas. En este artículo nos enfocamos en la recuperación de expertos en un entorno realista que difiere del entorno de la W3C, uno en el que hay cantidades relativamente pequeñas de datos limpios y multilingües disponibles, que abarcan una amplia gama de áreas de expertise, como se puede encontrar en las intranets de universidades y otras organizaciones intensivas en conocimiento. Normalmente, esta configuración incluye varios tipos adicionales de estructuras: estructura temática (por ejemplo, jerarquías de temas utilizadas por la organización), estructura organizativa (facultad, departamento, ...), así como múltiples tipos de documentos (descripciones de investigación y cursos, publicaciones y páginas web académicas). Este entorno es bastante diferente del entorno del W3C de maneras que podrían afectar el rendimiento de las tareas de recuperación de conocimientos especializados. Nos enfocamos en varias preguntas de investigación en este artículo: ¿La cantidad relativamente pequeña de datos disponibles en una intranet afecta la calidad de las asociaciones tema-persona que son fundamentales en los algoritmos de recuperación de expertos? ¿Cómo se desempeñan los algoritmos de vanguardia desarrollados en el conjunto de datos del W3C en el escenario alternativo del tipo descrito anteriormente? Más en general, ¿se pueden aplicar las lecciones de la tarea de Búsqueda de Expertos en TREC a este contexto? ¿Cómo afecta la inclusión o exclusión de diferentes documentos a las tareas de recuperación de expertos? Además, ¿cómo se puede utilizar la estructura temática y organizativa con fines de recuperación? Para responder a nuestras preguntas de investigación, primero presentamos un conjunto de enfoques de línea base, basados en modelado de lenguaje generativo, con el objetivo de encontrar asociaciones entre temas y personas. Esto nos permite formular las tareas de búsqueda de expertos y perfilado de expertos de manera uniforme, y tiene el beneficio adicional de permitirnos entender las relaciones entre las dos tareas. Para nuestra evaluación experimental, introducimos un nuevo conjunto de datos (la Colección de Expertos de UvT) que es representativo del tipo de intranet que describimos anteriormente. Nuestra colección se basa en datos de acceso público, extraídos del sitio web de la Universidad de Tilburg (UvT). Este tipo de datos es particularmente interesante, ya que (1) es limpio, heterogéneo, estructurado y enfocado, pero comprende un número limitado de documentos; (2) contiene información sobre la jerarquía organizativa; (3) es bilingüe (inglés y holandés); y (4) la lista de áreas de especialización de un individuo es proporcionada por los propios empleados. Usando la colección de expertos de UvT, llevamos a cabo dos conjuntos de experimentos. El primero tiene como objetivo determinar la efectividad de los métodos de búsqueda y perfilado de experiencia inicial en este nuevo entorno. Un segundo grupo de experimentos está dirigido a extensiones de los métodos base que aprovechan las características distintivas de la Colección de Expertos de UvT; específicamente, proponemos y evaluamos métodos refinados de búsqueda y perfilado de expertos que incorporan la topicalidad y la estructura organizativa. Además de las preguntas de investigación y el conjunto de datos que aportamos, nuestras principales contribuciones son las siguientes. Los modelos base desarrollados para encontrar expertos funcionan bien en el nuevo conjunto de datos. Mientras que en el entorno del W3C la tarea de encontrar expertos parece ser más difícil que el perfilado, para los datos de UvT ocurre lo contrario. Encontramos que el perfilado en el conjunto de datos UvT es considerablemente más difícil que en el conjunto de datos W3C, lo cual creemos que se debe al gran (pero realista) número de áreas temáticas que utilizamos para el perfilado: alrededor de 1,500 para el conjunto de datos UvT, frente a 50 en el caso de W3C. Teniendo en cuenta la similitud entre los temas puede mejorar significativamente el rendimiento de recuperación. Las medidas de similitud de mejor rendimiento son basadas en contenido, por lo tanto también se pueden aplicar en entornos de la W3C (y otros). Finalmente, demostramos que la estructura organizativa puede ser explotada en forma de un modelo de contexto, mejorando los puntajes de MAP para ciertos modelos hasta en un 70%. El resto de este documento está organizado de la siguiente manera. En la siguiente sección revisamos el trabajo relacionado. Luego, en la Sección 3 proporcionamos descripciones detalladas de las tareas de recuperación de expertos que abordamos en este artículo: la búsqueda de expertos y la creación de perfiles de expertos. En la Sección 4 presentamos nuestros modelos base, cuyo rendimiento luego es evaluado en la Sección 6 utilizando el conjunto de datos UvT que presentamos en la Sección 5. Los modelos avanzados que explotan características específicas de nuestros datos se presentan en la Sección 7 y se evalúan en la Sección 8. Formulamos nuestras conclusiones en la Sección 9.2. Trabajos relacionados Los enfoques iniciales para encontrar expertos a menudo empleaban bases de datos que contenían información sobre las habilidades y conocimientos de cada individuo en la organización [11]. La mayoría de estas herramientas (generalmente llamadas páginas amarillas o sistemas de búsqueda de personas) dependen de que las personas evalúen sus habilidades frente a un conjunto predefinido de palabras clave. Para actualizar perfiles en estos sistemas de forma automática, se necesita de tecnologías inteligentes [5]. Enfoques más recientes utilizan conjuntos de documentos específicos (como correos electrónicos [6] o software [18]) para encontrar experiencia. En contraste con centrarse en tipos de documentos particulares, también hay un interés creciente en el desarrollo de sistemas que indexan y extraen información de documentos publicados en intranets como fuentes de evidencia de experiencia. Un enfoque publicado es el sistema P@noptic [9], que construye una representación de cada persona concatenando todos los documentos asociados con esa persona, similar al Modelo 1 de Balog et al. [4], quienes formalizan y comparan dos métodos. El Modelo 1 de Balog et al. modela directamente el conocimiento de un experto a partir de documentos asociados, mientras que su Modelo 2 primero localiza documentos sobre el tema y luego encuentra a los expertos asociados. En los experimentos reportados, el segundo método tiene un rendimiento significativamente mejor cuando hay suficientes documentos asociados por candidato. La mayoría de los sistemas que participaron en las ediciones de 2005 y 2006 de la tarea de Búsqueda de Expertos en TREC implementaron (variaciones de) uno de estos dos modelos; ver [10, 20]. Macdonald y Ounis [16] proponen un enfoque diferente para clasificar la experiencia de los candidatos con respecto a un tema basado en técnicas de fusión de datos, sin utilizar heurísticas específicas de la colección; encuentran que la aplicación de modelos de ponderación basados en campos mejora la clasificación de los candidatos. Petkova y Croft [19] proponen otro enfoque, basado en una combinación de los Modelos 1 y 2 anteriores, modelando explícitamente los temas. Al abordar otras tareas de recuperación de expertos que también pueden abordarse utilizando asociaciones entre temas y personas, Balog y de Rijke [3] abordaron la tarea de determinar perfiles de expertos temáticos. Si bien sus métodos resultaron ser eficientes en el corpus de la W3C, requieren una cantidad de datos que puede que no esté disponible en la típica organización intensiva en conocimiento. Balog y de Rijke [2] estudian la tarea relacionada de encontrar expertos que sean similares a un pequeño conjunto de expertos dados como entrada. Por cierto, crear un resumen textual de una persona muestra algunas similitudes con la búsqueda de biografías, la cual ha recibido una considerable cantidad de atención recientemente; ver por ejemplo, [13]. Utilizamos modelado de lenguaje generativo para encontrar asociaciones entre temas y personas. En nuestro modelado de búsqueda y perfilado de expertos recopilamos evidencia de experiencia de múltiples fuentes, en una colección heterogénea, e integramos esta información con la co-ocurrencia de nombres de candidatos y términos de consulta: el entorno de modelado de lenguaje nos permite hacer esto de manera transparente. Nuestro modelado avanza en dos pasos. En el primer paso, consideramos tres modelos base, dos tomados de [4] (los Modelos 1 y 2 mencionados anteriormente), y uno una versión refinada de un modelo introducido en [3] (al que nos referimos como Modelo 3 a continuación); este tercer modelo también es similar al modelo descrito por Petkova y Croft [19]. Los modelos que consideramos en nuestra segunda ronda de experimentos son modelos de mezcla similares a los modelos de lenguaje contextual [1] y a los documentos ampliados de Tao et al. [21]; sin embargo, las características que utilizamos para definir nuestras expansiones, incluida la estructura temática y la estructura organizativa, no se han utilizado de esta manera antes. 3. En el escenario de recuperación de expertos que imaginamos, los usuarios que buscan expertos dentro de una organización tienen acceso a una interfaz que combina un cuadro de búsqueda (donde pueden buscar expertos o temas) con estructuras de navegación (de expertos y de temas) que les permite hacer clic hasta llegar a una página de experto (que proporciona el perfil de una persona) o a una página de tema (que proporciona una lista de expertos en el tema). Para alimentar la interfaz anterior, nos enfrentamos a dos tareas de recuperación de expertos, la búsqueda de expertos y el perfilado de expertos, que primero definimos y luego formalizamos utilizando modelos de lenguaje generativos. Para modelar cualquiera de las tareas, la probabilidad de que el tema de la consulta esté asociado a un experto candidato juega un papel clave en las estimaciones finales para la búsqueda y el perfilado. Al utilizar modelos de lenguaje, tanto los candidatos como la consulta se caracterizan por distribuciones de términos en el vocabulario (utilizado en los documentos puestos a disposición por la organización cuyas necesidades de recuperación de experiencia estamos abordando). 3.1 Búsqueda de expertos La búsqueda de expertos implica la tarea de encontrar a la persona adecuada con las habilidades y conocimientos apropiados: ¿Quiénes son los expertos en el tema X?. Por ejemplo, un empleado quiere averiguar quién trabajó en un proyecto en particular para descubrir por qué se tomaron decisiones específicas sin tener que revisar minuciosamente la documentación (si es que la hay). O pueden necesitar un especialista capacitado para consultoría sobre un problema específico. Dentro de una organización, generalmente hay muchos posibles candidatos que podrían ser expertos en un tema dado. Podemos plantear este problema de la siguiente manera: ¿Cuál es la probabilidad de que un candidato ca sea un experto dado el tema de consulta q? Es decir, determinamos p(ca|q) y clasificamos a los candidatos ca de acuerdo con esta probabilidad. Los candidatos con la probabilidad más alta dada la consulta son considerados los expertos más probables en ese tema. El desafío es cómo estimar esta probabilidad con precisión. Dado que es probable que la consulta consista solo en unos pocos términos para describir la experiencia requerida, deberíamos poder obtener una estimación más precisa al invocar el Teorema de Bayes, y estimar: p(ca|q) = p(q|ca)p(ca) p(q) , (1) donde p(ca) es la probabilidad de un candidato y p(q) es la probabilidad de una consulta. Dado que p(q) es una constante, se puede ignorar para fines de clasificación. Por lo tanto, la probabilidad de que un candidato ca sea un experto dado la consulta q es proporcional a la probabilidad de una consulta dada el candidato p(q|ca), ponderada por la creencia a priori p(ca) de que el candidato ca es un experto. p(ca|q) ∝ p(q|ca)p(ca) (2) En este artículo, nuestro enfoque principal se centra en estimar la probabilidad de una consulta dada el candidato p(q|ca), ya que esta probabilidad captura en qué medida el candidato conoce sobre el tema de la consulta. Si bien se asume generalmente que los priors del candidato son uniformes y, por lo tanto, no influirán en la clasificación, se ha demostrado que una elección sensata de priors puede mejorar el rendimiento [20]. Perfilado de expertos. Mientras que la tarea de búsqueda de expertos se centraba en encontrar expertos en un tema específico, la tarea de perfilado de expertos busca responder a una pregunta relacionada: ¿Sobre qué temas tiene conocimiento un candidato? Básicamente, esto da un giro a las preguntas sobre la búsqueda de expertos. El perfilado de un candidato individual implica la identificación de áreas de habilidades y conocimientos en las que tienen experiencia, así como una evaluación del nivel de competencia en cada una de estas áreas. Este es el perfil temático de los candidatos. Generalmente, los perfiles temáticos dentro de las organizaciones consisten en estructuras tabulares que catalogan explícitamente las habilidades y conocimientos de cada individuo en la organización. Sin embargo, esta práctica está limitada por los recursos disponibles para definir, crear, mantener y actualizar estos perfiles con el tiempo. Al centrarnos en métodos automáticos que se basan en la evidencia disponible dentro de los repositorios de documentos de una organización, nuestro objetivo es reducir el esfuerzo humano asociado con el mantenimiento de perfiles temáticos. Un perfil temático de un candidato, entonces, se define como un vector donde cada elemento i del vector corresponde a la experiencia del candidato en un tema dado ki, (es decir, s(ca, ki)). Cada tema ki define un área de conocimiento o habilidad particular que la organización utiliza para definir el perfil temático de los candidatos. Por lo tanto, se asume que se proporciona una lista de temas, {k1, . . . , kn}, donde n es el número de temas predefinidos: perfil(ca) = s(ca, k1), s(ca, k2), . . . , s(ca, kn) . Se necesita contexto y evidencia para ayudar a los usuarios de sistemas de búsqueda de expertos a decidir a quién contactar al buscar experiencia en un área particular. Ejemplos de dicho contexto son: ¿Con quién trabaja ella? ¿Cuáles son sus datos de contacto? ¿Está bien conectada, por si acaso no puede ayudarnos ella misma? ¿Cuál es su rol en la organización? ¿Quién es su superior? Los colaboradores, afiliaciones, etc. son parte del perfil social de los candidatos y pueden servir como antecedentes para interpretar las recomendaciones del sistema. En este documento solo abordamos el problema de determinar perfiles temáticos, dejando el perfilado social para trabajos futuros. Planteamos el problema de cuantificar la competencia de una persona en un área de conocimiento específica de la siguiente manera: ¿Cuál es la probabilidad de que un área de conocimiento (ki) forme parte del perfil de candidatos (expertise)? donde s(ca, ki) está definido por p(ki|ca). Nuestra tarea, entonces, es estimar p(ki|ca), lo cual es equivalente al problema de obtener p(q|ca), donde el tema ki se representa como un tema de consulta q, es decir, una secuencia de palabras clave que representan la experiencia requerida. Tanto la tarea de encontrar expertos como la de perfilar expertos dependen de la estimación precisa de p(q|ca). La única diferencia proviene de la probabilidad previa de que una persona sea experta (p(ca)), la cual puede ser incorporada en la tarea de encontrar expertos. Este antecedente no se aplica a la tarea de perfilado ya que el candidato (individuo) está fijo. 4. MODELOS DE REFERENCIA En esta sección describimos nuestros modelos de referencia para estimar p(q|ca), es decir, las asociaciones entre temas y personas. Tanto la búsqueda de expertos como el perfilado de expertos se reducen a esta estimación. Empleamos tres modelos para calcular esta probabilidad. 4.1 De temas a candidatos Utilizando Modelos de Candidatos: El Modelo 1 [4] define la probabilidad de una consulta dada un candidato (p(q|ca)) utilizando técnicas estándar de modelado de lenguaje, basadas en un modelo de lenguaje multinomial de unigrama. Para cada candidato ca, se infiere un modelo de lenguaje candidato θca de tal manera que la probabilidad de un término dado θca no sea cero para todos los términos, es decir, p(t|θca) > 0. A partir del modelo de candidato, la consulta se genera con la siguiente probabilidad: p(q|θca) = Y t∈q p(t|θca)n(t,q), donde cada término t en la consulta q se muestrea de manera idéntica e independiente, y n(t, q) es el número de veces que t ocurre en q. El modelo de lenguaje candidato se infiere de la siguiente manera: (1) se calcula un modelo empírico p(t|ca); (2) se suaviza con probabilidades de fondo. Usando las asociaciones entre un candidato y un documento, la probabilidad p(t|ca) puede aproximarse por: p(t|ca) = X d p(t|d)p(d|ca), donde p(d|ca) es la probabilidad de que el candidato ca genere un documento de apoyo d, y p(t|d) es la probabilidad de que un término t ocurra en el documento d. Utilizamos la estimación de máxima verosimilitud de un término, es decir, la frecuencia normalizada del término t en el documento d. La fuerza de la asociación entre el documento d y el candidato ca expresada por p(d|ca) refleja el grado en que la experiencia de los candidatos se describe utilizando este documento. La estimación de esta probabilidad se presenta más adelante, en la Sección 4.2. El modelo candidato se construye entonces como una interpolación lineal de p(t|ca) y el modelo de fondo p(t) para asegurar que no haya probabilidades nulas, lo que resulta en la estimación final: p(q|θca) = (4) Y t∈q ( (1 − λ) X d p(t|d)p(d|ca) ! + λp(t) )n(t,q) . El Modelo 1 recopila toda la información de términos de todos los documentos asociados con el candidato, y la utiliza para representar a ese candidato. Este modelo se utiliza para predecir qué tan probable es que un candidato produzca una consulta q. Esto puede interpretarse intuitivamente como la probabilidad de que este candidato hable sobre el tema de la consulta, donde asumimos que esto es indicativo de su experiencia. Usando Modelos de Documentos: El Modelo 2 [4] toma un enfoque diferente. Aquí, el proceso se divide en dos partes. Dado un candidato ca, (1) se selecciona un documento asociado con un candidato con probabilidad p(d|ca), y (2) a partir de este documento se genera una consulta q con probabilidad p(q|d). Entonces se toma la suma sobre todos los documentos para obtener p(q|ca), de modo que: p(q|ca) = Σ d p(q|d)p(d|ca). (5) La probabilidad de una consulta dada un documento se estima inferiendo un modelo de lenguaje del documento θd para cada documento d de manera similar a como se infería el modelo candidato: p(t|θd) = (1 − λ)p(t|d) + λp(t), (6) donde p(t|d) es la probabilidad del término en el documento. La probabilidad de una consulta dada el modelo de documento es: p(q|θd) = Y t∈q p(t|θd)n(t,q) . La estimación final de p(q|ca) se obtiene sustituyendo p(q|d) por p(q|θd) en la Ecuación 5 (ver [4] para más detalles). Conceptualmente, el Modelo 2 difiere del Modelo 1 porque el candidato no está modelado directamente. En cambio, el documento actúa como una variable oculta en el proceso que separa la consulta del candidato. Este proceso es similar a cómo un usuario puede buscar candidatos con un motor de búsqueda estándar: inicialmente encontrando los documentos relevantes y luego viendo quién está asociado con ese documento. Al examinar una serie de documentos, el usuario puede obtener una idea de qué candidatos son más propensos a discutir el tema q. Usando Modelos de Temas: Modelo 3 Introducimos un tercer modelo, Modelo 3. En lugar de intentar modelar el proceso de generación de consultas a través de modelos de candidatos o documentos, representamos la consulta como un modelo de lenguaje de temas y estimamos directamente la probabilidad del candidato p(ca|q). Este enfoque es similar al modelo presentado en [3, 19]. Como en los modelos anteriores, se infiere un modelo de lenguaje, pero esta vez para la consulta. Adaptamos el trabajo de Lavrenko y Croft [14] para estimar un modelo de tema a partir de la consulta. El procedimiento es el siguiente. Dada una colección de documentos y un tema de consulta q, se asume que existe un modelo de tema desconocido θk que asigna probabilidades p(t|θk) a las ocurrencias de términos en los documentos del tema. Tanto la consulta como los documentos son muestras de θk (a diferencia de los enfoques anteriores, donde se asume que una consulta se extrae de un documento específico o un modelo candidato). La tarea principal es estimar p(t|θk), la probabilidad de un término dado el modelo de tema. Dado que la consulta q es muy dispersa y no hay ejemplos de documentos sobre el tema, es necesario aproximar esta distribución. Lavrenko y Croft [14] sugieren una forma razonable de obtener dicha aproximación, asumiendo que p(t|θk) puede aproximarse por la probabilidad del término t dado la consulta q. Entonces podemos estimar p(t|q) utilizando la probabilidad conjunta de observar el término t junto con los términos de la consulta, q1, . . . , qm, y dividiendo por la probabilidad conjunta de los términos de la consulta: p(t|θk) ≈ p(t|q) = p(t, q1, . . . , qm) p(q1, . . . , qm) = p(t, q1, . . . , qm) P t ∈T p(t , q1, . . . , qm) , donde p(q1, . . . , qm) = P t ∈T p(t , q1, . . . , qm), y T es el vocabulario completo de términos. Para estimar la probabilidad conjunta p(t, q1, . . . , qm), seguimos [14, 15] y asumimos que t y q1, . . . , qm son mutuamente independientes, una vez que elegimos una distribución fuente del conjunto de distribuciones fuente subyacentes U. Si elegimos U como un conjunto de modelos de documentos, entonces para construir este conjunto, la consulta q se emitiría contra la colección, y se asume que los primeros n devueltos son relevantes para el tema, y por lo tanto se tratan como muestras del modelo de tema. (Tenga en cuenta que en su lugar podrían usarse modelos candidatos). Con los modelos de documentos formando U, la probabilidad conjunta del término y la consulta se convierte en: p(t, q1, . . . , qm) = X d∈U p(d) ˘ p(t|θd) mY i=1 p(qi|θd) ¯ . (7) Aquí, p(d) denota la distribución previa sobre el conjunto U, que refleja la relevancia del documento para el tema. Suponemos que p(d) es uniforme en todo U. Para clasificar a los candidatos según el modelo de tema definido, utilizamos la métrica de divergencia de Kullback-Leibler (KL, [8]) para medir la diferencia entre los modelos de los candidatos y el modelo de tema: KL(θk||θca) = X t p(t|θk) log p(t|θk) p(t|θca) . (8) Se considera que los candidatos con una divergencia menor respecto al modelo de tema son más propensos a ser expertos en ese tema. El modelo candidato θca está definido en la Ecuación 4. Al utilizar la divergencia de Kullback-Leibler en lugar de la probabilidad de un candidato dado el modelo de tema p(ca|θk), evitamos problemas de normalización. 4.2 Asociaciones documento-candidato Para nuestros modelos, necesitamos poder estimar la probabilidad p(d|ca), que expresa en qué medida un documento d caracteriza al candidato ca. En [4], se presentan dos métodos para estimar esta probabilidad, basados en el número de nombres de personas reconocidos en un documento. Sin embargo, en nuestro entorno de intranet es razonable asumir que los autores de los documentos pueden ser identificados de manera inequívoca (por ejemplo, como el autor de un artículo, el profesor asignado a un curso, el propietario de una página web, etc.). Por lo tanto, establecemos p(d|ca) en 1 si el candidato ca es el autor del documento d, de lo contrario la probabilidad es 0. En la Sección 6 describimos cómo se puede determinar la autoría en diferentes tipos de documentos dentro de la colección. 5. La colección de expertos de UvT utilizada en los experimentos de este artículo se ajusta al escenario descrito en la Sección 3. La colección se basa en el sistema Webwijs (Webwise) desarrollado en la Universidad de Tilburg (UvT) en los Países Bajos. Webwijs (http://www.uvt.nl/webwijs/) es una base de datos de acceso público de empleados de la UvT que están involucrados en investigación o enseñanza; actualmente, Webwijs contiene información sobre 1168 expertos, cada uno de los cuales tiene una página con información de contacto y, si está disponible por parte del experto, una descripción de investigación y lista de publicaciones. Además, cada experto puede seleccionar áreas de especialización de una lista de 1491 temas y se le anima a sugerir nuevos temas que necesitan ser aprobados por el editor de Webwijs. Cada tema tiene una página separada que muestra a todos los expertos asociados con ese tema y, si está disponible, una lista de temas relacionados. Webwijs está disponible en holandés e inglés, y esta bilingüidad se ha preservado en la colección. Cada página de Webwijs en holandés tiene una traducción al inglés. No todos los temas en holandés tienen una traducción al inglés, pero lo contrario es cierto: los 981 temas en inglés tienen un equivalente en holandés. Aproximadamente el 42% de los expertos imparten cursos en la Universidad de Tilburg; estos cursos también fueron rastreados e incluidos en el perfil. Además, aproximadamente el 27% de los expertos enlazan su página académica desde su página de Webwijs. Estas páginas de inicio fueron rastreadas y añadidas a la colección. (Esto significa que si los expertos colocaron las versiones completas de sus publicaciones en sus páginas de inicio académicas, estas también estaban disponibles para su indexación). También obtuvimos 1880 versiones completas de publicaciones del repositorio institucional de UvT y el número de expertos holandeses en inglés es de 1168, con 743 expertos con ≥ 1 tema. El número de temas es de 1491 y 981, y el número de pares experto-tema es de 4318 y 3251, con un promedio de temas por experto de 5.8 y 5.9 respectivamente. El máximo número de temas por experto es de 60 (1) y 35 (1), y el mínimo es de 1 (74) y 1 (106) respectivamente. El promedio de expertos por tema es de 2.9 y 3.3, con un máximo de 30 (1) y un mínimo de 1 (615) y 1 (346) respectivamente. Hay 318 expertos con HP y CD, con un promedio de 3.5 CDs por experto docente. Además, hay 329 expertos con RD y 734 con PUB, con un promedio de 27.0 publicaciones por experto, 25.2 citas por experto y 1.8 publicaciones completas por experto. Tabla 2: Estadísticas descriptivas de las versiones holandesas e inglesas de la colección de expertos de UvT. Ejecutamos el identificador de idioma TextCat [23] para clasificar el idioma de las páginas de inicio y las publicaciones de texto completo. Nos limitamos a las páginas en las que el clasificador tenía confianza en el idioma utilizado en la página. Esto resultó en cuatro tipos de documentos: descripciones de investigación (RD), descripciones de cursos (CD), publicaciones (PUB; versiones de texto completo y solo de cita) y páginas web académicas (HP). Todo fue agrupado en la colección UvT Expert que está disponible en http://ilk.uvt.nl/uvt-expert-collection/. La colección UvT Expert fue extraída de un entorno organizativo diferente a la colección de la W3C y difiere de ella en varios aspectos. El entorno de UvT es uno con cantidades relativamente pequeñas de datos multilingües. Las asociaciones entre los documentos y los autores son claras y los datos están estructurados y limpios. La colección abarca una amplia gama de áreas de especialización, como suele encontrarse en intranets de universidades y otros institutos intensivos en conocimiento. Además, nuestro entorno universitario cuenta con varios tipos de estructura (temática y organizativa), así como múltiples tipos de documentos. Otra diferencia importante entre los dos conjuntos de datos es que las áreas de especialización en la colección de Expertos de UvT son autoseleccionadas en lugar de basarse en la membresía de un grupo o asignaciones de otros. El tamaño es otra dimensión en la que difieren las colecciones de expertos de W3C y UvT: esta última es la más pequeña de las dos. También son realistas las grandes diferencias en la cantidad de información disponible para cada experto. Utilizar Webwijs es voluntario; 425 expertos holandeses no seleccionaron ningún tema en absoluto. Esto nos deja con 743 perfiles de expertos holandeses y 727 perfiles de expertos en inglés utilizables. La Tabla 2 proporciona estadísticas descriptivas para la colección de Expertos de UvT. Las universidades tienden a tener una estructura jerárquica que va desde el nivel de la facultad, pasando por los departamentos, grupos de investigación, hasta llegar a los investigadores individuales. En la colección de Expertos de UvT tenemos información sobre las afiliaciones de los investigadores con facultades e institutos, lo que nos proporciona una jerarquía organizativa de dos niveles. La Universidad de Tilburg tiene 22 unidades organizativas a nivel de facultad (incluida la oficina universitaria y varios institutos de investigación) y 71 departamentos, lo que equivale a 3.2 departamentos por facultad. En cuanto a la jerarquía temática utilizada por Webwijs, 131 de los 1491 temas son nodos principales en la jerarquía. Esta jerarquía tiene una longitud promedio de cadena de temas de 2.65 y una longitud máxima de 7 temas. 6. EVALUACIÓN A continuación, evaluamos los modelos de la Sección 4 para la búsqueda y perfilado de expertos en la colección de Expertos de UvT. Detallamos nuestras preguntas de investigación y configuración experimental, y luego presentamos nuestros resultados. 6.1 Preguntas de investigación Abordamos las siguientes preguntas de investigación. Tanto la búsqueda de expertos como el perfilado dependen de las estimaciones de p(q|ca). La pregunta es cómo se comparan los modelos en las diferentes tareas, y en el contexto de la colección de expertos de UvT. En [4], el Modelo 2 superó al Modelo 1 en la colección de la W3C. ¿Cómo se comparan en nuestro conjunto de datos? ¿Y cómo se compara el Modelo 3 con el Modelo 1? ¿Qué hay de las diferencias de rendimiento entre los dos idiomas en nuestra colección de pruebas? 6.2 Configuración Experimental La salida de nuestros modelos fue evaluada frente a las etiquetas de temas autoasignadas, que fueron tratadas como juicios de relevancia. Los resultados fueron evaluados por separado para inglés y holandés. Para inglés solo usamos temas para los cuales había traducción al neerlandés; para neerlandés se consideraron todos los temas. Los resultados se promediaron para las consultas en la intersección de las evaluaciones de relevancia y los resultados; las consultas faltantes no contribuyen con un valor de 0 a las puntuaciones. Utilizamos medidas estándar de recuperación de información, como la Precisión Media Promedio (MAP) y la Reciprocidad Media Promedio (MRR). También informamos el porcentaje de temas (%q) y candidatos (%ca) cubiertos, respectivamente, para las tareas de búsqueda y perfilado de expertos. 6.3 Resultados La Tabla 1 muestra el rendimiento de los Modelos 1, 2 y 3 en las tareas de búsqueda y perfilado de expertos. Las filas de la tabla corresponden a los diversos tipos de documentos (RD, CD, PUB y HP) y a sus combinaciones. RD+CD+PUB+HP es equivalente a la colección completa y será referida como la LÍNEA BASE de nuestros experimentos. Al observar la Tabla 1, vemos que el Modelo 2 tiene el mejor rendimiento en general. Sin embargo, cuando los datos están limpios y muy enfocados (RD), el Modelo 3 lo supera en varios casos. El modelo 1 tiene la mejor cobertura de candidatos (%ca) y temas (%q). Los diversos tipos de documentos difieren en sus características y en cómo mejoran las tareas de búsqueda y perfilado. El perfilado de expertos se beneficia mucho de los datos limpios presentes en los tipos de documentos RD y CD, mientras que las publicaciones contribuyen principalmente a la tarea de encontrar expertos. Agregar las páginas de inicio no resulta ser particularmente útil. Cuando comparamos los resultados entre idiomas, encontramos que la cobertura de los temas en inglés (%q) es mayor que la de los temas en neerlandés para la búsqueda de expertos. Además de eso, las puntuaciones caen en el mismo rango para ambos idiomas. Para la tarea de perfilado, la cobertura de los candidatos (%ca) es muy similar para ambos idiomas. Sin embargo, el rendimiento es considerablemente mejor para los temas en inglés. Si bien es difícil comparar las puntuaciones entre colecciones, concluimos con una breve comparación de las puntuaciones absolutas en la Tabla 1 con las reportadas en [3, 4] en el conjunto de pruebas de W3C (edición de 2005). Para el experto que busca, los puntajes MAP para el Modelo 2 reportados aquí son aproximadamente un 50% más altos que las cifras correspondientes en [4], mientras que nuestros puntajes MRR están ligeramente por debajo de los de [4]. Para el perfilado de expertos, las diferencias son mucho más dramáticas: las puntuaciones MAP para el Modelo 2 reportadas aquí son aproximadamente un 50% más bajas que las puntuaciones en [3], mientras que las puntuaciones MRR (mejores) son aproximadamente iguales a las de [3]. La causa de estas diferencias parece residir en el número de áreas de conocimiento consideradas aquí, aproximadamente 30 veces más que en el entorno del W3C. MODELOS AVANZADOS Ahora que hemos desarrollado y evaluado técnicas básicas de modelado de lenguaje para la recuperación de expertos, nos enfocamos en modelos refinados que explotan características especiales de nuestra colección de pruebas. 7.1 Explotando la similitud en el área de conocimiento Una forma de mejorar la puntuación de una consulta dada un candidato es considerar qué otras solicitudes satisfaría el candidato y usarlas como evidencia adicional para respaldar la consulta original, proporcionalmente. Búsqueda de expertos Perfilado de expertos Tipos de documentos Modelo 1 Modelo 2 Modelo 3 Modelo 1 Modelo 2 Modelo 3 %q MAP MRR %q MAP MRR %q MAP MRR %ca MAP MRR %ca MAP MRR %ca MAP MRR Inglés RD 97.8 0.126 0.269 83.5 0.144 0.311 83.3 0.129 0.271 100 0.089 0.189 39.3 0.232 0.465 41.1 0.166 0.337 CD 97.8 0.118 0.227 91.7 0.123 0.248 91.7 0.118 0.226 32.8 0.188 0.381 32.4 0.195 0.385 32.7 0.203 0.370 PUB 97.8 0.200 0.330 98.0 0.216 0.372 98.0 0.145 0.257 78.9 0.167 0.364 74.5 0.212 0.442 78.9 0.135 0.299 HP 97.8 0.081 0.186 97.4 0.071 0.168 97.2 0.062 0.149 31.2 0.150 0.299 28.8 0.185 0.335 30.1 0.136 0.287 RD+CD 97.8 0.188 0.352 92.9 0.193 0.360 92.9 0.150 0.273 100 0.145 0.286 61.3 0.251 0.477 63.2 0.217 0.416 RD+CD+PUB 97.8 0.235 0.373 98.1 0.277 0.439 98.1 0.178 0.305 100 0.196 0.380 87.2 0.280 0.533 89.5 0.170 0.344 RD+CD+PUB+HP 97.8 0.237 0.372 98.6 0.280 0.441 98.5 0.166 0.293 100 0.199 0.387 88.7 0.281 0.525 90.9 0.169 0.329 Holandés RD 61.3 0.094 0.229 38.4 0.137 0.336 38.3 0.127 0.295 38.0 0.127 0.386 34.1 0.138 0.420 38.0 0.105 0.327 CD 61.3 0.107 0.212 49.7 0.128 0.256 49.7 0.136 0.261 32.5 0.151 0.389 31.8 0.158 0.396 32.5 0.170 0.380 PUB 61.3 0.193 0.319 59.5 0.218 0.368 59.4 0.173 0.291 78.8 0.126 0.364 76.0 0.150 0.424 78.8 0.103 0.294 HP 61.3 0.063 0.169 56.6 0.064 0.175 56.4 0.062 0.163 29.8 0.108 0.308 27.8 0.125 0.338 29.8 0.098 0.255 RD+CD 61.3 0.159 0.314 51.9 0.184 0.360 51.9 0.169 0.324 60.5 0.151 0.410 57.2 0.166 0.431 60.4 0.159 0.384 RD+CD+PUB 61.3 0.244 0.398 61.5 0.260 0.424 61.4 0.210 0.350 90.3 0.165 0.445 88.2 0.189 0.479 90.3 0.126 0.339 RD+CD+PUB+HP 61.3 0.249 0.401 62.6 0.265 0.436 62.6 0.195 0.344 91.9 0.164 0.426 90.1 0.195 0.488 91.9 0.125 0.328 Tabla 1: Rendimiento de los modelos en las tareas de búsqueda y perfilado de expertos, utilizando diferentes tipos de documentos y sus combinaciones. %q es el número de temas cubiertos (se aplica a la tarea de búsqueda de expertos), %ca es el número de candidatos cubiertos (se aplica a la tarea de perfilado de expertos). Los bloques superior e inferior corresponden al inglés y al holandés respectivamente. Las mejores puntuaciones están en negrita. Esto se puede modelar interpolando entre el p(q|ca) y la evidencia adicional de todos los pedidos similares q, de la siguiente manera: p(q|ca) = λp(q|ca) + (1 − λ) X q p(q|q )p(q |ca), (9) donde p(q|q ) representa la similitud entre los dos temas q y q. Para poder trabajar con métodos de similitud que no son necesariamente probabilidades, establecemos p(q|q ) = w(q,q ) γ , donde γ es una constante de normalización, de modo que γ = P q w(q , q ). Consideramos cuatro métodos para calcular la puntuación de similitud entre dos temas. Tres enfoques se basan estrictamente en el contenido y establecen similitudes examinando patrones de co-ocurrencia de temas dentro de la colección, mientras que el último enfoque explota la estructura jerárquica de áreas temáticas que pueden estar presentes dentro de una organización (ver [7] para más ejemplos de integrar relaciones entre palabras en modelos de lenguaje). La métrica de divergencia Kullback-Leibler (KL) definida en la ecuación 8 proporciona una medida de cuán diferentes o similares son dos distribuciones de probabilidad. Se infiere un modelo de tema para q y q utilizando el método presentado en la Sección 4.1 para describir la consulta en todo el vocabulario. Dado que un puntaje KL más bajo significa que las consultas son más similares, permitimos que w(q, q ) = max(KL(θq||·) − KL(θq||θq )). El Información Mutua Puntual (PMI, [17]) es una medida de asociación utilizada en teoría de la información para determinar el grado de independencia entre variables. La dependencia entre dos consultas se refleja en la puntuación SI(q, q), donde las puntuaciones mayores que cero indican que es probable que exista una dependencia, lo que interpretamos como que las consultas son probablemente similares: SI(q, q) = log p(q, q) / p(q)p(q). Estimamos la probabilidad de un tema p(q) utilizando el número de documentos relevantes para la consulta q dentro de la colección. La probabilidad conjunta p(q, q) se estima de manera similar, utilizando la concatenación de q y q como una consulta. Para obtener p(q|q), luego establecemos w(q, q) = SI(q, q) cuando SI(q, q) > 0, de lo contrario w(q, q) = 0, ya que solo estamos interesados en incluir consultas que sean similares. La estadística de log-verosimilitud proporciona otra medida de dependencia, que es más confiable que la medida de información mutua puntual [17]. Sea k1 el número de co-ocurrencias de q y q, k2 el número de ocurrencias de q que no co-ocurren con q, n1 el número total de ocurrencias de q, y n2 el número total de tokens de tema menos el número de ocurrencias de q. Entonces, sea p1 = k1/n1, p2 = k2/n2 y p = (k1 + k2)/(n1 + n2), (q, q ) = 2( (p1, k1, n1) + (p2, k2, n2) − (p, k1, n1) − (p, k2, n2)), donde (p, n, k) = k log p + (n − k) log(1 − p). El puntaje más alto indica que las consultas también son probablemente similares, por lo tanto, establecemos w(q, q ) = (q, q ). Finalmente, también estimamos la similitud de dos temas basándonos en su distancia dentro de la jerarquía de temas. La jerarquía de temas se visualiza como un grafo dirigido, y para todos los pares de temas se calcula el camino más corto SP(q, q). Establecimos el puntaje de similitud como el recíproco del camino más corto: w(q, q ) = 1/SP(q, q ). 7.2 Información contextual Dada la jerarquía de una organización, las unidades a las que pertenece una persona se consideran como un contexto para compensar la escasez de datos. Lo modelamos de la siguiente manera: p (q|ca) = 1 − P ou∈OU(ca) λou · p(q|ca) + P ou∈OU(ca) λou · p(q|ou), donde OU(ca) es el conjunto de unidades organizativas de las cuales el candidato ca es miembro, y p(q|o) expresa la fuerza de la asociación entre la consulta q y la unidad ou. La última probabilidad se puede estimar utilizando cualquiera de los tres modelos básicos, simplemente reemplazando ca con ou en las ecuaciones correspondientes. Una unidad organizativa está asociada con todos los documentos que sus miembros han escrito. Es decir, p(d|ou) = maxca∈ou p(d|ca). 7.3 Un modelo multilingüe simple Para institutos de conocimiento en Europa, ya sea académicos u otros, un entorno multilingüe (o al menos bilingüe) es típico. El siguiente modelo se basa en un tipo de suposición de independencia: no hay transferencia de conocimientos/perfiles a través de las barreras del idioma. Si bien es una simplificación, esta es una aproximación sensata en primer lugar. Eso es: p (q|ca) =P l∈L λl · p(ql|ca), donde L es el conjunto de idiomas utilizados en la colección, ql es la traducción de la consulta q al idioma l, y λl es un parámetro de suavizado específico del idioma, tal que P l∈L λl = 1. 8. MODELOS AVANZADOS: EVALUACIÓN En esta sección presentamos una evaluación experimental de nuestros modelos avanzados. Búsqueda de expertos Perfilado de expertos Modelo de lenguaje 1 Modelo 2 Modelo 3 Modelo 1 Modelo 2 Modelo 3 %q MAP MRR %q MAP MRR %q MAP MRR %ca MAP MRR %ca MAP MRR %ca MAP MRR Solo inglés 97.8 0.237 0.372 98.6 0.280 0.441 98.5 0.166 0.293 100 0.199 0.387 88.7 0.281 0.525 90.9 0.169 0.329 Solo holandés 61.3 0.249 0.401 62.6 0.265 0.436 62.6 0.195 0.344 91.9 0.164 0.426 90.1 0.195 0.488 91.9 0.125 0.328 Combinación 99.4 0.297 0.444 99.7 0.324 0.491 99.7 0.223 0.388 100 0.241 0.445 92.1 0.313 0.564 93.2 0.224 0.411 Tabla 3: Rendimiento de la combinación de idiomas en las tareas de búsqueda y perfilado de expertos (en candidatos). Los mejores puntajes para cada modelo están en cursiva, los mejores puntajes absolutos para las tareas de búsqueda y perfilado de expertos están en negrita. Tabla 4: Rendimiento en las tareas de búsqueda de expertos (arriba) y perfilado (abajo), utilizando similitudes de áreas de conocimiento. Las carreras fueron evaluadas en los temas principales establecidos. Las mejores puntuaciones están en negrita. 8.1 Preguntas de investigación Nuestras preguntas siguen los refinamientos presentados en la sección anterior: ¿Explotar la similitud en el área de conocimiento mejora la efectividad? ¿Cuál de los diversos métodos para capturar las relaciones entre palabras es más efectivo? Además, ¿es útil nuestra forma de incorporar información contextual? ¿Para qué tareas? Y finalmente, ¿es nuestra forma simple de combinar las puntuaciones monolingües suficiente para obtener mejoras significativas? 8.2 Configuración experimental Dado que las autoevaluaciones también son escasas en nuestra colección, para poder medir las diferencias entre los diversos modelos, seleccionamos un subconjunto de temas y evaluamos (algunas de las) ejecuciones solo en este subconjunto. Este conjunto se denomina como temas principales, y consiste en temas que se encuentran en el nivel superior de la jerarquía temática. (Un tema principal tiene subtemas, pero no es un subtema de ningún otro tema). Este conjunto principal consiste en 132 temas en holandés y 119 en inglés. Los juicios de relevancia se limitaron al conjunto de temas principales, pero no se ampliaron con subtemas. 8.3 Explotando la similitud de áreas de conocimiento. La Tabla 4 presenta los resultados. Los cuatro métodos utilizados para estimar la similitud de áreas de conocimiento son la divergencia de KL (KLDIV) y PointLang. Modelo 1 Modelo 2 Modelo 3 MAP MRR MAP MRR MAP MRR Búsqueda de expertos UK TODOS 0.423 0.545 0.654 0.799 0.494 0.629 UK PRINCIPAL 0.500 0.621 0.704 0.834 0.587 0.699 NL TODOS 0.439 0.560 0.672 0.826 0.480 0.630 NL PRINCIPAL 0.440 0.584 0.645 0.816 0.515 0.655 Perfilado de expertos UK TODOS 0.240 0.640 0.306 0.778 0.223 0.616 UK PRINCIPAL 0.523 0.677 0.519 0.648 0.461 0.587 NL TODOS 0.203 0.716 0.254 0.770 0.183 0.627 NL PRINCIPAL 0.332 0.576 0.380 0.624 0.332 0.549 Tabla 5: Evaluación de los modelos de contexto en unidades organizativas. información mutua ponderada (PMI), log-verosimilitud (LL) y distancia dentro de la jerarquía de temas (HDIST). Logramos mejorar el resultado base en todos los casos, pero la mejora es más notable para la tarea de perfilado. Para ambas tareas, el método LL tuvo el mejor rendimiento. Los enfoques basados en el contenido tuvieron un rendimiento consistentemente mejor que HDIST. 8.4 Información contextual Una jerarquía de dos niveles de unidades organizativas (facultades e institutos) está disponible en la colección de Expertos de UvT. La unidad a la que pertenece una persona se utiliza como contexto para esa persona. Primero, evaluamos los modelos de las unidades organizativas, utilizando todos los temas (TODOS) y solo los temas principales (PRINCIPALES). Una unidad organizativa se considera relevante para un tema dado (o viceversa) si al menos un miembro de la unidad seleccionó el tema dado como área de especialización. La Tabla 5 informa sobre los resultados. En lo que respecta a la búsqueda de expertos, dado un tema, la unidad organizativa correspondiente puede ser identificada con alta precisión. Sin embargo, la tarea de perfilado de expertos muestra una imagen diferente: las puntuaciones son bajas y la tarea parece difícil. La explicación puede ser que los conceptos generales (es decir, nuestros temas principales) pueden pertenecer a varias unidades organizativas. Segundo, realizamos otra evaluación, donde combinamos los modelos contextuales con los modelos candidatos (para puntuar a los candidatos nuevamente). La tabla 6 informa sobre los resultados. Encontramos un impacto positivo de los modelos de contexto solo para la búsqueda de expertos. Notablemente, para la búsqueda de expertos (y Modelo 1), mejora más del 50% (para inglés) y más del 70% (para holandés) en el MAP. El bajo rendimiento en la creación de perfiles de expertos puede deberse al hecho de que los modelos de contexto por sí solos no tuvieron un buen desempeño en la tarea de creación de perfiles desde el principio. 8.5 Modelos multilingües En esta subsección evaluamos el método para combinar resultados en múltiples idiomas que describimos en la Sección 7.3. En nuestro entorno, el conjunto de idiomas consiste en inglés y holandés: L = {UK, NL}. Los pesos de estos idiomas se establecieron para ser idénticos (λUK = λNL = 0.5). Realizamos experimentos con diferentes configuraciones de λ, pero no observamos diferencias significativas en el rendimiento. La Tabla 3 informa sobre los resultados multilingües, donde el rendimiento se evalúa en el conjunto completo de temas. Los tres modelos tienen una diferencia significativa. Tabla 6: Rendimiento de los modelos de contexto (CT) en comparación con la línea base (BL). Las mejores puntuaciones están en negrita, demostradas en todas las medidas para ambas tareas. La cobertura de temas y candidatos para las tareas de búsqueda y perfilado de expertos, respectivamente, es cercana al 100% en todos los casos. La mejora relativa de las puntuaciones de precisión varía del 10% al 80%. Estas puntuaciones demuestran que, a pesar de su simplicidad, nuestro método para combinar resultados en varios idiomas logra mejoras sustanciales sobre el punto de referencia. CONCLUSIONES En este artículo nos enfocamos en la recuperación de expertos (búsqueda y perfilado de expertos) en un nuevo entorno de una organización típica intensiva en conocimiento en la que los datos disponibles son de alta calidad, multilingües y abarcan una amplia gama de áreas de expertise. Normalmente, la cantidad de datos disponibles en una organización como una universidad, un instituto de investigación o un laboratorio de investigación es limitada en comparación con la colección de la W3C que ha sido utilizada principalmente para la evaluación experimental de la recuperación de la experiencia hasta ahora. Para examinar la recuperación de la experiencia en este entorno, presentamos (y lanzamos) la colección de Expertos de UvT como un caso representativo de organizaciones intensivas en conocimiento. La nueva colección refleja las propiedades típicas de los institutos intensivos en conocimiento mencionados anteriormente e incluye también varias características que pueden ser potencialmente útiles para la recuperación de expertos, como la estructura temática y organizativa. Evaluamos cómo los modelos de vanguardia para la búsqueda y perfilado de expertos se desempeñaron en este nuevo entorno y luego refinamos estos modelos para intentar explotar las diferentes características dentro del entorno de datos (idioma, actualidad y estructura organizativa). Encontramos que los modelos actuales de recuperación de experiencia se generalizan bien a este nuevo entorno; además, descubrimos que refinar los modelos para tener en cuenta las diferencias resulta en mejoras significativas, compensando así los problemas causados por la escasez de datos. El trabajo futuro incluye establecer evaluaciones manuales de perfiles generados automáticamente por los propios empleados, especialmente en casos en los que los empleados no han proporcionado un perfil ellos mismos. 10. AGRADECIMIENTOS Krisztian Balog fue apoyado por la Organización Neerlandesa para la Investigación Científica (NWO) bajo el número de proyecto 220-80-001. Maarten de Rijke también recibió apoyo de NWO bajo los números de proyecto 017.001.190, 220-80-001, 264-70-050, 354-20-005, 600.065.120, 612-13-001, 612.000.106, 612.066.302, 612.069.006, 640.001.501, 640.002.501, y de la Unión Europea. Programa IST del 6º Programa Marco para la I+D en virtud del contrato del proyecto MultiMATCH IST-033104. El trabajo de Toine Bogers y Antal van den Bosch fue financiado por el programa IOP-MMI de SenterNovem / Ministerio de Asuntos Económicos de los Países Bajos, como parte del proyecto 'A Propos'. 11. REFERENCIAS [1] L. Azzopardi. Incorporando contexto en el marco de modelado del lenguaje para la recuperación de información ad-hoc. Tesis doctoral, Universidad de Paisley, 2005. [2] K. Balog y M. de Rijke. Encontrando expertos similares. En este volumen, 2007. [3] K. Balog y M. de Rijke. Determinación de perfiles de expertos (con una aplicación a la búsqueda de expertos). En IJCAI 07: Proc. 20th Intern. Conferencia Conjunta sobre Inteligencia Artificial, páginas 2657-2662, 2007. [4] K. Balog, L. Azzopardi y M. de Rijke. Modelos formales para la búsqueda de expertos en corporaciones empresariales. En SIGIR 06: Actas de la 29ª conferencia internacional anual. Conferencia ACM SIGIR sobre Investigación y desarrollo en recuperación de información, páginas 43-50, 2006. [5] I. Becerra-Fernández. El papel de las tecnologías de inteligencia artificial en la implementación de sistemas de gestión del conocimiento de localización de personas. En el taller de AAAI sobre la incorporación de conocimiento en los procesos de negocio, marzo de 2000. [6] C. S. Campbell, P. P. Maglio, A. Cozzi y B. Dom. Identificación de la experiencia utilizando comunicaciones por correo electrónico. En CIKM 03: Actas de la duodécima conferencia internacional sobre gestión de la información y el conocimiento, páginas 528-531, 2003. [7] G. Cao, J.-Y. Nie, y J. Bai. Integrando relaciones entre palabras en modelos de lenguaje. En SIGIR 05: Actas de la 28ª conferencia internacional anual. Conferencia ACM SIGIR sobre Investigación y desarrollo en recuperación de información, páginas 298-305, 2005. [8] T. M. Cover y J. A. Thomas. Elementos de la teoría de la información. Wiley-Interscience, 1991. [9] N. Craswell, D. Hawking, A. M. Vercoustre y P. Wilkins. Experto en P@noptic: Buscando expertos no solo documentos. En Ausweb, 2001. [10] N. Craswell, A. de Vries e I. Soboroff. Resumen de la pista empresarial TREC2005. En la Decimocuarta Conferencia de Recuperación de Información. Proc. (TREC 2005), 2006. [11] T. H. Davenport y L. Prusak. Conocimiento en Acción: Cómo las Organizaciones Gestionan lo que Saben. Harvard Business School Press, Boston, MA, 1998. [12] T. Dunning.\nHarvard Business School Press, Boston, MA, 1998. [12] T. Dunning. Métodos precisos para la estadística de sorpresa y coincidencia. Lingüística Computacional, 19(1):61-74, 1993. [13] E. Filatova y J. Prager. Dime qué haces y te diré quién eres: Aprendiendo actividades relacionadas con la ocupación para biografías. En HLT/EMNLP, 2005. [14] V. Lavrenko y W. B. Croft. Modelos de lenguaje basados en relevancia. En SIGIR 01: Actas de la 24ª conferencia internacional anual. Conferencia ACM SIGIR sobre Investigación y Desarrollo en Recuperación de Información, páginas 120-127, 2001. [15] V. Lavrenko, M. Choquette y W. B. Croft. Modelos de relevancia multilingües. En SIGIR 02: Actas de la 25ª conferencia internacional anual. Conferencia ACM SIGIR sobre Investigación y Desarrollo en Recuperación de Información, páginas 175-182, 2002. [16] C. Macdonald e I. Ounis. Votación de candidatos: adaptando técnicas de fusión de datos para una tarea de búsqueda de expertos. En CIKM 06: Actas de la 15ª conferencia internacional de ACM sobre gestión de la información y el conocimiento, páginas 387-396, 2006. [17] C. Manning y H. Schütze. Fundamentos del Procesamiento del Lenguaje Natural Estadístico. El MIT Press, 1999. [18] A. Mockus y J. D. Herbsleb. Explorador de experticia: un enfoque cuantitativo para identificar la experticia. En ICSE 02: Proc. 24th Intern. Conf. en Ingeniería de Software, páginas 503-512, 2002. [19] D. Petkova y W. B. Croft. Modelos de lenguaje jerárquicos para la búsqueda de expertos en corporaciones empresariales. En Proc. ICTAI 2006, páginas 599-608, 2006. [20] I. Soboroff, A. de Vries y N. Craswell. Resumen de la pista empresarial TREC 2006. En las Notas de Trabajo de TREC 2006, 2006. [21] T. Tao, X. Wang, Q. Mei y C. Zhai. Recuperación de información del modelo de lenguaje con expansión de documentos. En HLT-NAACL 2006, 2006. [22] TREC. Ruta empresarial, 2005. URL: http://www.ins.cwi.nl/projects/trec-ent/wiki/. [23] G. van Noord.\nURL: http://www.ins.cwi.nl/projects/trec-ent/wiki/. [23] G. van Noord. Adivinador de idioma TextCat. URL: http://www.let.rug.nl/˜vannoord/TextCat/. [24] W3C. La colección de pruebas del W3C, 2005. URL: http://research.microsoft.com/users/nickcr/w3c-summary.html. ",
            "candidates": [],
            "error": [
                []
            ]
        },
        "bayes theorem": {
            "translated_key": "Teorema de Bayes",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Broad Expertise Retrieval in Sparse Data Environments Krisztian Balog ISLA, University of Amsterdam Kruislaan 403, 1098 SJ Amsterdam, The Netherlands kbalog@science.uva.nl Toine Bogers ILK, Tilburg University P.O.",
                "Box 90153, 5000 LE Tilburg, The Netherlands A.M.Bogers@uvt.nl Leif Azzopardi Dept. of Computing Science University of Glasgow, Glasgow, G12 8QQ leif@dcs.gla.ac.uk Maarten de Rijke ISLA, University of Amsterdam Kruislaan 403, 1098 SJ Amsterdam, The Netherlands mdr@science.uva.nl Antal van den Bosch ILK, Tilburg University P.O.",
                "Box 90153, 5000 LE Tilburg, The Netherlands Antal.vdnBosch@uvt.nl ABSTRACT Expertise retrieval has been largely unexplored on data other than the W3C collection.",
                "At the same time, many intranets of universities and other knowledge-intensive organisations offer examples of relatively small but clean multilingual expertise data, covering broad ranges of expertise areas.",
                "We first present two main expertise retrieval tasks, along with a set of baseline approaches based on generative language modeling, aimed at finding expertise relations between topics and people.",
                "For our experimental evaluation, we introduce (and release) a new test set based on a crawl of a university site.",
                "Using this test set, we conduct two series of experiments.",
                "The first is aimed at determining the effectiveness of baseline expertise retrieval methods applied to the new test set.",
                "The second is aimed at assessing refined models that exploit characteristic features of the new test set, such as the organizational structure of the university, and the hierarchical structure of the topics in the test set.",
                "Expertise retrieval models are shown to be robust with respect to environments smaller than the W3C collection, and current techniques appear to be generalizable to other settings.",
                "Categories and Subject Descriptors H.3 [Information Storage and Retrieval]: H.3.1 Content Analysis and Indexing; H.3.3 Information Search and Retrieval; H.3.4 Systems and Software; H.4 [Information Systems Applications]: H.4.2 Types of Systems; H.4.m Miscellaneous General Terms Algorithms, Measurement, Performance, Experimentation 1.",
                "INTRODUCTION An organizations intranet provides a means for exchanging information between employees and for facilitating employee collaborations.",
                "To efficiently and effectively achieve this, it is necessary to provide search facilities that enable employees not only to access documents, but also to identify expert colleagues.",
                "At the TREC Enterprise Track [22] the need to study and understand expertise retrieval has been recognized through the introduction of Expert Finding tasks.",
                "The goal of expert finding is to identify a list of people who are knowledgeable about a given topic.",
                "This task is usually addressed by uncovering associations between people and topics [10]; commonly, a co-occurrence of the name of a person with topics in the same context is assumed to be evidence of expertise.",
                "An alternative task, which using the same idea of people-topic associations, is expert profiling, where the task is to return a list of topics that a person is knowledgeable about [3].",
                "The launch of the Expert Finding task at TREC has generated a lot of interest in expertise retrieval, with rapid progress being made in terms of modeling, algorithms, and evaluation aspects.",
                "However, nearly all of the expert finding or profiling work performed has been validated experimentally using the W3C collection [24] from the Enterprise Track.",
                "While this collection is currently the only publicly available test collection for expertise retrieval tasks, it only represents one type of intranet.",
                "With only one test collection it is not possible to generalize conclusions to other realistic settings.",
                "In this paper we focus on expertise retrieval in a realistic setting that differs from the W3C setting-one in which relatively small amounts of clean, multilingual data are available, that cover a broad range of expertise areas, as can be found on the intranets of universities and other knowledge-intensive organizations.",
                "Typically, this setting features several additional types of structure: topical structure (e.g., topic hierarchies as employed by the organization), organizational structure (faculty, department, ...), as well as multiple types of documents (research and course descriptions, publications, and academic homepages).",
                "This setting is quite different from the W3C setting in ways that might impact upon the performance of expertise retrieval tasks.",
                "We focus on a number of research questions in this paper: Does the relatively small amount of data available on an intranet affect the quality of the topic-person associations that lie at the heart of expertise retrieval algorithms?",
                "How do state-of-the-art algorithms developed on the W3C data set perform in the alternative scenario of the type described above?",
                "More generally, do the lessons from the Expert Finding task at TREC carry over to this setting?",
                "How does the inclusion or exclusion of different documents affect expertise retrieval tasks?",
                "In addition to, how can the topical and organizational structure be used for retrieval purposes?",
                "To answer our research questions, we first present a set of baseline approaches, based on generative language modeling, aimed at finding associations between topics and people.",
                "This allows us to formulate the expert finding and expert profiling tasks in a uniform way, and has the added benefit of allowing us to understand the relations between the two tasks.",
                "For our experimental evaluation, we introduce a new data set (the UvT Expert Collection) which is representative of the type of intranet that we described above.",
                "Our collection is based on publicly available data, crawled from the website of Tilburg University (UvT).",
                "This type of data is particularly interesting, since (1) it is clean, heterogeneous, structured, and focused, but comprises a limited number of documents; (2) contains information on the organizational hierarchy; (3) it is bilingual (English and Dutch); and (4) the list of expertise areas of an individual are provided by the employees themselves.",
                "Using the UvT Expert collection, we conduct two sets of experiments.",
                "The first is aimed at determining the effectiveness of baseline expertise finding and profiling methods in this new setting.",
                "A second group of experiments is aimed at extensions of the baseline methods that exploit characteristic features of the UvT Expert Collection; specifically, we propose and evaluate refined expert finding and profiling methods that incorporate topicality and organizational structure.",
                "Apart from the research questions and data set that we contribute, our main contributions are as follows.",
                "The baseline models developed for expertise finding perform well on the new data set.",
                "While on the W3C setting the expert finding task appears to be more difficult than profiling, for the UvT data the opposite is the case.",
                "We find that profiling on the UvT data set is considerably more difficult than on the W3C set, which we believe is due to the large (but realistic) number of topical areas that we used for profiling: about 1,500 for the UvT set, versus 50 in the W3C case.",
                "Taking the similarity between topics into account can significantly improve retrieval performance.",
                "The best performing similarity measures are content-based, therefore they can be applied on the W3C (and other) settings as well.",
                "Finally, we demonstrate that the organizational structure can be exploited in the form of a context model, improving MAP scores for certain models by up to 70%.",
                "The remainder of this paper is organized as follows.",
                "In the next section we review related work.",
                "Then, in Section 3 we provide detailed descriptions of the expertise retrieval tasks that we address in this paper: expert finding and expert profiling.",
                "In Section 4 we present our baseline models, of which the performance is then assessed in Section 6 using the UvT data set that we introduce in Section 5.",
                "Advanced models exploiting specific features of our data are presented in Section 7 and evaluated in Section 8.",
                "We formulate our conclusions in Section 9. 2.",
                "RELATED WORK Initial approaches to expertise finding often employed databases containing information on the skills and knowledge of each individual in the organization [11].",
                "Most of these tools (usually called yellow pages or people-finding systems) rely on people to self-assess their skills against a predefined set of keywords.",
                "For updating profiles in these systems in an automatic fashion there is a need for intelligent technologies [5].",
                "More recent approaches use specific document sets (such as email [6] or software [18]) to find expertise.",
                "In contrast with focusing on particular document types, there is also an increased interest in the development of systems that index and mine published intranet documents as sources of evidence for expertise.",
                "One such published approach is the P@noptic system [9], which builds a representation of each person by concatenating all documents associated with that person-this is similar to Model 1 of Balog et al. [4], who formalize and compare two methods.",
                "Balog et al.s Model 1 directly models the knowledge of an expert from associated documents, while their Model 2 first locates documents on the topic and then finds the associated experts.",
                "In the reported experiments the second method performs significantly better when there are sufficiently many associated documents per candidate.",
                "Most systems that took part in the 2005 and 2006 editions of the Expert Finding task at TREC implemented (variations on) one of these two models; see [10, 20].",
                "Macdonald and Ounis [16] propose a different approach for ranking candidate expertise with respect to a topic based on data fusion techniques, without using collectionspecific heuristics; they find that applying field-based weighting models improves the ranking of candidates.",
                "Petkova and Croft [19] propose yet another approach, based on a combination of the above Model 1 and 2, explicitly modeling topics.",
                "Turning to other expert retrieval tasks that can also be addressed using topic-people associations, Balog and de Rijke [3] addressed the task of determining topical expert profiles.",
                "While their methods proved to be efficient on the W3C corpus, they require an amount of data that may not be available in the typical knowledge-intensive organization.",
                "Balog and de Rijke [2] study the related task of finding experts that are similar to a small set of experts given as input.",
                "As an aside, creating a textual summary of a person shows some similarities to biography finding, which has received a considerable amount of attention recently; see e.g., [13].",
                "We use generative language modeling to find associations between topics and people.",
                "In our modeling of expert finding and profiling we collect evidence for expertise from multiple sources, in a heterogeneous collection, and integrate it with the co-occurrence of candidates names and query terms-the language modeling setting allows us to do this in a transparent manner.",
                "Our modeling proceeds in two steps.",
                "In the first step, we consider three baseline models, two taken from [4] (the Models 1 and 2 mentioned above), and one a refined version of a model introduced in [3] (which we refer to as Model 3 below); this third model is also similar to the model described by Petkova and Croft [19].",
                "The models we consider in our second round of experiments are mixture models similar to contextual language models [1] and to the expanded documents of Tao et al. [21]; however, the features that we use for definining our expansions-including topical structure and organizational structure-have not been used in this way before. 3.",
                "TASKS In the expertise retrieval scenario that we envisage, users seeking expertise within an organization have access to an interface that combines a search box (where they can search for experts or topics) with navigational structures (of experts and of topics) that allows them to click their way to an expert page (providing the profile of a person) or a topic page (providing a list of experts on the topic).",
                "To feed the above interface, we face two expertise retrieval tasks, expert finding and expert profiling, that we first define and then formalize using generative language models.",
                "In order to model either task, the probability of the query topic being associated to a candidate expert plays a key role in the final estimates for searching and profiling.",
                "By using language models, both the candidates and the query are characterized by distributions of terms in the vocabulary (used in the documents made available by the organization whose expertise retrieval needs we are addressing). 3.1 Expert finding Expert finding involves the task of finding the right person with the appropriate skills and knowledge: Who are the experts on topic X?.",
                "E.g., an employee wants to ascertain who worked on a particular project to find out why particular decisions were made without having to trawl through documentation (if there is any).",
                "Or, they may be in need a trained specialist for consultancy on a specific problem.",
                "Within an organization there are usually many possible candidates who could be experts for given topic.",
                "We can state this problem as follows: What is the probability of a candidate ca being an expert given the query topic q?",
                "That is, we determine p(ca|q), and rank candidates ca according to this probability.",
                "The candidates with the highest probability given the query are deemed the most likely experts for that topic.",
                "The challenge is how to estimate this probability accurately.",
                "Since the query is likely to consist of only a few terms to describe the expertise required, we should be able to obtain a more accurate estimate by invoking <br>bayes theorem</br>, and estimating: p(ca|q) = p(q|ca)p(ca) p(q) , (1) where p(ca) is the probability of a candidate and p(q) is the probability of a query.",
                "Since p(q) is a constant, it can be ignored for ranking purposes.",
                "Thus, the probability of a candidate ca being an expert given the query q is proportional to the probability of a query given the candidate p(q|ca), weighted by the a priori belief p(ca) that candidate ca is an expert. p(ca|q) ∝ p(q|ca)p(ca) (2) In this paper our main focus is on estimating the probability of a query given the candidate p(q|ca), because this probability captures the extent to which the candidate knows about the query topic.",
                "Whereas the candidate priors are generally assumed to be uniformand thus will not influence the ranking-it has been demonstrated that a sensible choice of priors may improve the performance [20]. 3.2 Expert profiling While the task of expert searching was concerned with finding experts given a particular topic, the task of expert profiling seeks to answer a related question: What topics does a candidate know about?",
                "Essentially, this turns the questions of expert finding around.",
                "The profiling of an individual candidate involves the identification of areas of skills and knowledge that they have expertise about and an evaluation of the level of proficiency in each of these areas.",
                "This is the candidates topical profile.",
                "Generally, topical profiles within organizations consist of tabular structures which explicitly catalogue the skills and knowledge of each individual in the organization.",
                "However, such practice is limited by the resources available for defining, creating, maintaining, and updating these profiles over time.",
                "By focusing on automatic methods which draw upon the available evidence within the document repositories of an organization, our aim is to reduce the human effort associated with the maintenance of topical profiles1 .",
                "A topical profile of a candidate, then, is defined as a vector where each element i of the vector corresponds to the candidate cas expertise on a given topic ki, (i.e., s(ca, ki)).",
                "Each topic ki defines a particular knowledge area or skill that the organization uses to define the candidates topical profile.",
                "Thus, it is assumed that a list of topics, {k1, . . . , kn}, where n is the number of pre-defined topics, is given: profile(ca) = s(ca, k1), s(ca, k2), . . . , s(ca, kn) . (3) 1 Context and evidence are needed to help users of expertise finding systems to decide whom to contact when seeking expertise in a particular area.",
                "Examples of such context are: Who does she work with?",
                "What are her contact details?",
                "Is she well-connected, just in case she is not able to help us herself?",
                "What is her role in the organization?",
                "Who is her superior?",
                "Collaborators, and affiliations, etc. are all part of the candidates social profile, and can serve as a background against which the systems recommendations should be interpreted.",
                "In this paper we only address the problem of determining topical profiles, and leave social profiling to further work.",
                "We state the problem of quantifying the competence of a person on a certain knowledge area as follows: What is the probability of a knowledge area (ki) being part of the candidates (expertise) profile? where s(ca, ki) is defined by p(ki|ca).",
                "Our task, then, is to estimate p(ki|ca), which is equivalent to the problem of obtaining p(q|ca), where the topic ki is represented as a query topic q, i.e., a sequence of keywords representing the expertise required.",
                "Both the expert finding and profiling tasks rely on the accurate estimation of p(q|ca).",
                "The only difference derives from the prior probability that a person is an expert (p(ca)), which can be incorporated into the expert finding task.",
                "This prior does not apply to the profiling task since the candidate (individual) is fixed. 4.",
                "BASELINE MODELS In this section we describe our baseline models for estimating p(q|ca), i.e., associations between topics and people.",
                "Both expert finding and expert profiling boil down to this estimation.",
                "We employ three models for calculating this probability. 4.1 From topics to candidates Using Candidate Models: Model 1 Model 1 [4] defines the probability of a query given a candidate (p(q|ca)) using standard language modeling techniques, based on a multinomial unigram language model.",
                "For each candidate ca, a candidate language model θca is inferred such that the probability of a term given θca is nonzero for all terms, i.e., p(t|θca) > 0.",
                "From the candidate model the query is generated with the following probability: p(q|θca) = Y t∈q p(t|θca)n(t,q) , where each term t in the query q is sampled identically and independently, and n(t, q) is the number of times t occurs in q.",
                "The candidate language model is inferred as follows: (1) an empirical model p(t|ca) is computed; (2) it is smoothed with background probabilities.",
                "Using the associations between a candidate and a document, the probability p(t|ca) can be approximated by: p(t|ca) = X d p(t|d)p(d|ca), where p(d|ca) is the probability that candidate ca generates a supporting document d, and p(t|d) is the probability of a term t occurring in the document d. We use the maximum-likelihood estimate of a term, that is, the normalised frequency of the term t in document d. The strength of the association between document d and candidate ca expressed by p(d|ca) reflects the degree to which the candidates expertise is described using this document.",
                "The estimation of this probability is presented later, in Section 4.2.",
                "The candidate model is then constructed as a linear interpolation of p(t|ca) and the background model p(t) to ensure there are no zero probabilities, which results in the final estimation: p(q|θca) = (4) Y t∈q ( (1 − λ) X d p(t|d)p(d|ca) ! + λp(t) )n(t,q) .",
                "Model 1 amasses all the term information from all the documents associated with the candidate, and uses this to represent that candidate.",
                "This model is used to predict how likely a candidate would produce a query q.",
                "This can can be intuitively interpreted as the probability of this candidate talking about the query topic, where we assume that this is indicative of their expertise.",
                "Using Document Models: Model 2 Model 2 [4] takes a different approach.",
                "Here, the process is broken into two parts.",
                "Given a candidate ca, (1) a document that is associated with a candidate is selected with probability p(d|ca), and (2) from this document a query q is generated with probability p(q|d).",
                "Then the sum over all documents is taken to obtain p(q|ca), such that: p(q|ca) = X d p(q|d)p(d|ca). (5) The probability of a query given a document is estimated by inferring a document language model θd for each document d in a similar manner as the candidate model was inferred: p(t|θd) = (1 − λ)p(t|d) + λp(t), (6) where p(t|d) is the probability of the term in the document.",
                "The probability of a query given the document model is: p(q|θd) = Y t∈q p(t|θd)n(t,q) .",
                "The final estimate of p(q|ca) is obtained by substituting p(q|d) for p(q|θd) into Eq. 5 (see [4] for full details).",
                "Conceptually, Model 2 differs from Model 1 because the candidate is not directly modeled.",
                "Instead, the document acts like a hidden variable in the process which separates the query from the candidate.",
                "This process is akin to how a user may search for candidates with a standard search engine: initially by finding the documents which are relevant, and then seeing who is associated with that document.",
                "By examining a number of documents the user can obtain an idea of which candidates are more likely to discuss the topic q.",
                "Using Topic Models: Model 3 We introduce a third model, Model 3.",
                "Instead of attempting to model the query generation process via candidate or document models, we represent the query as a topic language model and directly estimate the probability of the candidate p(ca|q).",
                "This approach is similar to the model presented in [3, 19].",
                "As with the previous models, a language model is inferred, but this time for the query.",
                "We adapt the work of Lavrenko and Croft [14] to estimate a topic model from the query.",
                "The procedure is as follows.",
                "Given a collection of documents and a query topic q, it is assumed that there exists an unknown topic model θk that assigns probabilities p(t|θk) to the term occurrences in the topic documents.",
                "Both the query and the documents are samples from θk (as opposed to the previous approaches, where a query is assumed to be sampled from a specific document or candidate model).",
                "The main task is to estimate p(t|θk), the probability of a term given the topic model.",
                "Since the query q is very sparse, and as there are no examples of documents on the topic, this distribution needs to be approximated.",
                "Lavrenko and Croft [14] suggest a reasonable way of obtaining such an approximation, by assuming that p(t|θk) can be approximated by the probability of term t given the query q.",
                "We can then estimate p(t|q) using the joint probability of observing the term t together with the query terms, q1, . . . , qm, and dividing by the joint probability of the query terms: p(t|θk) ≈ p(t|q) = p(t, q1, . . . , qm) p(q1, . . . , qm) = p(t, q1, . . . , qm) P t ∈T p(t , q1, . . . , qm) , where p(q1, . . . , qm) = P t ∈T p(t , q1, . . . , qm), and T is the entire vocabulary of terms.",
                "In order to estimate the joint probability p(t, q1, . . . , qm), we follow [14, 15] and assume t and q1, . . . , qm are mutually independent, once we pick a source distribution from the set of underlying source distributions U.",
                "If we choose U to be a set of document models. then to construct this set, the query q would be issued against the collection, and the top n returned are assumed to be relevant to the topic, and thus treated as samples from the topic model. (Note that candidate models could be used instead.)",
                "With the document models forming U, the joint probability of term and query becomes: p(t, q1, . . . , qm) = X d∈U p(d) ˘ p(t|θd) mY i=1 p(qi|θd) ¯ . (7) Here, p(d) denotes the prior distribution over the set U, which reflects the relevance of the document to the topic.",
                "We assume that p(d) is uniform across U.",
                "In order to rank candidates according to the topic model defined, we use the Kullback-Leibler divergence metric (KL, [8]) to measure the difference between the candidate models and the topic model: KL(θk||θca) = X t p(t|θk) log p(t|θk) p(t|θca) . (8) Candidates with a smaller divergence from the topic model are considered to be more likely experts on that topic.",
                "The candidate model θca is defined in Eq. 4.",
                "By using KL divergence instead of the probability of a candidate given the topic model p(ca|θk), we avoid normalization problems. 4.2 Document-candidate associations For our models we need to be able to estimate the probability p(d|ca), which expresses the extent to which a document d characterizes the candidate ca.",
                "In [4], two methods are presented for estimating this probability, based on the number of person names recognized in a document.",
                "However, in our (intranet) setting it is reasonable to assume that authors of documents can unambiguously be identified (e.g., as the author of an article, the teacher assigned to a course, the owner of a web page, etc.)",
                "Hence, we set p(d|ca) to be 1 if candidate ca is author of document d, otherwise the probability is 0.",
                "In Section 6 we describe how authorship can be determined on different types of documents within the collection. 5.",
                "THE UVT EXPERT COLLECTION The UvT Expert collection used in the experiments in this paper fits the scenario outlined in Section 3.",
                "The collection is based on the Webwijs (Webwise) system developed at Tilburg University (UvT) in the Netherlands.",
                "Webwijs (http://www.uvt.nl/ webwijs/) is a publicly accessible database of UvT employees who are involved in research or teaching; currently, Webwijs contains information about 1168 experts, each of whom has a page with contact information and, if made available by the expert, a research description and publications list.",
                "In addition, each expert can select expertise areas from a list of 1491 topics and is encouraged to suggest new topics that need to be approved by the Webwijs editor.",
                "Each topic has a separate page that shows all experts associated with that topic and, if available, a list of related topics.",
                "Webwijs is available in Dutch and English, and this bilinguality has been preserved in the collection.",
                "Every Dutch Webwijs page has an English translation.",
                "Not all Dutch topics have an English translation, but the reverse is true: the 981 English topics all have a Dutch equivalent.",
                "About 42% of the experts teach courses at Tilburg University; these courses were also crawled and included in the profile.",
                "In addition, about 27% of the experts link to their academic homepage from their Webwijs page.",
                "These home pages were crawled and added to the collection. (This means that if experts put the full-text versions of their publications on their academic homepage, these were also available for indexing.)",
                "We also obtained 1880 full-text versions of publications from the UvT institutional repository and Dutch English no. of experts 1168 1168 no. of experts with ≥ 1 topic 743 727 no. of topics 1491 981 no. of expert-topic pairs 4318 3251 avg. no. of topics/expert 5.8 5.9 max. no. of topics/expert (no. of experts) 60 (1) 35 (1) min. no. of topics/expert (no. of experts) 1 (74) 1 (106) avg. no. of experts/topic 2.9 3.3 max. no. of experts/topic (no. of topics) 30 (1) 30 (1) min. no. of experts/topic (no. of topics) 1 (615) 1 (346) no. of experts with HP 318 318 no. of experts with CD 318 318 avg. no. of CDs per teaching expert 3.5 3.5 no. of experts with RD 329 313 no. of experts with PUB 734 734 avg. no. of PUBs per expert 27.0 27.0 avg. no. of PUB citations per expert 25.2 25.2 avg. no. of full-text PUBs per expert 1.8 1.8 Table 2: Descriptive statistics of the Dutch and English versions of the UvT Expert collection. converted them to plain text.",
                "We ran the TextCat [23] language identifier to classify the language of the home pages and the fulltext publications.",
                "We restricted ourselves to pages where the classifier was confident about the language used on the page.",
                "This resulted in four document types: research descriptions (RD), course descriptions (CD), publications (PUB; full-text and citationonly versions), and academic homepages (HP).",
                "Everything was bundled into the UvT Expert collection which is available at http: //ilk.uvt.nl/uvt-expert-collection/.",
                "The UvT Expert collection was extracted from a different organizational setting than the W3C collection and differs from it in a number of ways.",
                "The UvT setting is one with relatively small amounts of multilingual data.",
                "Document-author associations are clear and the data is structured and clean.",
                "The collection covers a broad range of expertise areas, as one can typically find on intranets of universities and other knowledge-intensive institutes.",
                "Additionally, our university setting features several types of structure (topical and organizational), as well as multiple document types.",
                "Another important difference between the two data sets is that the expertise areas in the UvT Expert collection are self-selected instead of being based on group membership or assignments by others.",
                "Size is another dimension along which the W3C and UvT Expert collections differ: the latter is the smaller of the two.",
                "Also realistic are the large differences in the amount of information available for each expert.",
                "Utilizing Webwijs is voluntary; 425 Dutch experts did not select any topics at all.",
                "This leaves us with 743 Dutch and 727 English usable expert profiles.",
                "Table 2 provides descriptive statistics for the UvT Expert collection.",
                "Universities tend to have a hierarchical structure that goes from the faculty level, to departments, research groups, down to the individual researchers.",
                "In the UvT Expert collection we have information about the affiliations of researchers with faculties and institutes, providing us with a two-level organizational hierarchy.",
                "Tilburg University has 22 organizational units at the faculty level (including the university office and several research institutes) and 71 departments, which amounts to 3.2 departments per faculty.",
                "As to the topical hierarchy used by Webwijs, 131 of the 1491 topics are top nodes in the hierarchy.",
                "This hierarchy has an average topic chain length of 2.65 and a maximum length of 7 topics. 6.",
                "EVALUATION Below, we evaluate Section 4s models for expert finding and profiling onthe UvT Expert collection.",
                "We detail our research questions and experimental setup, and then present our results. 6.1 Research Questions We address the following research questions.",
                "Both expert finding and profiling rely on the estimations of p(q|ca).",
                "The question is how the models compare on the different tasks, and in the setting of the UvT Expert collection.",
                "In [4], Model 2 outperformed Model 1 on the W3C collection.",
                "How do they compare on our data set?",
                "And how does Model 3 compare to Model 1?",
                "What about performance differences between the two languages in our test collection? 6.2 Experimental Setup The output of our models was evaluated against the self-assigned topic labels, which were treated as relevance judgements.",
                "Results were evaluated separately for English and Dutch.",
                "For English we only used topics for which the Dutch translation was available; for Dutch all topics were considered.",
                "The results were averaged for the queries in the intersection of relevance judgements and results; missing queries do not contribute a value of 0 to the scores.",
                "We use standard information retrieval measures, such as Mean Average Precision (MAP) and Mean Reciprocal Rank (MRR).",
                "We also report the percentage of topics (%q) and candidates (%ca) covered, for the expert finding and profiling tasks, respectively. 6.3 Results Table 1 shows the performance of Model 1, 2, and 3 on the expert finding and profiling tasks.",
                "The rows of the table correspond to the various document types (RD, CD, PUB, and HP) and to their combinations.",
                "RD+CD+PUB+HP is equivalent to the full collection and will be referred as the BASELINE of our experiments.",
                "Looking at Table 1 we see that Model 2 performs the best across the board.",
                "However, when the data is clean and very focused (RD), Model 3 outperforms it in a number of cases.",
                "Model 1 has the best coverage of candidates (%ca) and topics (%q).",
                "The various document types differ in their characteristics and how they improve the finding and profiling tasks.",
                "Expert profiling benefits much from the clean data present in the RD and CD document types, while the publications contribute the most to the expert finding task.",
                "Adding the homepages does not prove to be particularly useful.",
                "When we compare the results across languages, we find that the coverage of English topics (%q) is higher than of the Dutch ones for expert finding.",
                "Apart from that, the scores fall in the same range for both languages.",
                "For the profiling task the coverage of the candidates (%ca) is very similar for both languages.",
                "However, the performance is substantially better for the English topics.",
                "While it is hard to compare scores across collections, we conclude with a brief comparison of the absolute scores in Table 1 to those reported in [3, 4] on the W3C test set (2005 edition).",
                "For expert finding the MAP scores for Model 2 reported here are about 50% higher than the corresponding figures in [4], while our MRR scores are slightly below those in [4].",
                "For expert profiling, the differences are far more dramatic: the MAP scores for Model 2 reported here are around 50% below the scores in [3], while the (best) MRR scores are about the same as those in [3].",
                "The cause for the latter differences seems to reside in the number of knowledge areas considered here-approx. 30 times more than in the W3C setting. 7.",
                "ADVANCED MODELS Now that we have developed and assessed basic language modeling techniques for expertise retrieval, we turn to refined models that exploit special features of our test collection. 7.1 Exploiting knowledge area similarity One way to improve the scoring of a query given a candidate is to consider what other requests the candidate would satisfy and use them as further evidence to support the original query, proportional Expert finding Expert profiling Document types Model 1 Model 2 Model 3 Model 1 Model 2 Model 3 %q MAP MRR %q MAP MRR %q MAP MRR %ca MAP MRR %ca MAP MRR %ca MAP MRR English RD 97.8 0.126 0.269 83.5 0.144 0.311 83.3 0.129 0.271 100 0.089 0.189 39.3 0.232 0.465 41.1 0.166 0.337 CD 97.8 0.118 0.227 91.7 0.123 0.248 91.7 0.118 0.226 32.8 0.188 0.381 32.4 0.195 0.385 32.7 0.203 0.370 PUB 97.8 0.200 0.330 98.0 0.216 0.372 98.0 0.145 0.257 78.9 0.167 0.364 74.5 0.212 0.442 78.9 0.135 0.299 HP 97.8 0.081 0.186 97.4 0.071 0.168 97.2 0.062 0.149 31.2 0.150 0.299 28.8 0.185 0.335 30.1 0.136 0.287 RD+CD 97.8 0.188 0.352 92.9 0.193 0.360 92.9 0.150 0.273 100 0.145 0.286 61.3 0.251 0.477 63.2 0.217 0.416 RD+CD+PUB 97.8 0.235 0.373 98.1 0.277 0.439 98.1 0.178 0.305 100 0.196 0.380 87.2 0.280 0.533 89.5 0.170 0.344 RD+CD+PUB+HP 97.8 0.237 0.372 98.6 0.280 0.441 98.5 0.166 0.293 100 0.199 0.387 88.7 0.281 0.525 90.9 0.169 0.329 Dutch RD 61.3 0.094 0.229 38.4 0.137 0.336 38.3 0.127 0.295 38.0 0.127 0.386 34.1 0.138 0.420 38.0 0.105 0.327 CD 61.3 0.107 0.212 49.7 0.128 0.256 49.7 0.136 0.261 32.5 0.151 0.389 31.8 0.158 0.396 32.5 0.170 0.380 PUB 61.3 0.193 0.319 59.5 0.218 0.368 59.4 0.173 0.291 78.8 0.126 0.364 76.0 0.150 0.424 78.8 0.103 0.294 HP 61.3 0.063 0.169 56.6 0.064 0.175 56.4 0.062 0.163 29.8 0.108 0.308 27.8 0.125 0.338 29.8 0.098 0.255 RD+CD 61.3 0.159 0.314 51.9 0.184 0.360 51.9 0.169 0.324 60.5 0.151 0.410 57.2 0.166 0.431 60.4 0.159 0.384 RD+CD+PUB 61.3 0.244 0.398 61.5 0.260 0.424 61.4 0.210 0.350 90.3 0.165 0.445 88.2 0.189 0.479 90.3 0.126 0.339 RD+CD+PUB+HP 61.3 0.249 0.401 62.6 0.265 0.436 62.6 0.195 0.344 91.9 0.164 0.426 90.1 0.195 0.488 91.9 0.125 0.328 Table 1: Performance of the models on the expert finding and profiling tasks, using different document types and their combinations. %q is the number of topics covered (applies to the expert finding task), %ca is the number of candidates covered (applies to the expert profiling task).",
                "The top and bottom blocks correspond to English and Dutch respectively.",
                "The best scores are in boldface. to how related the other requests are to the original query.",
                "This can be modeled by interpolating between the p(q|ca) and the further supporting evidence from all similar requests q , as follows: p (q|ca) = λp(q|ca) + (1 − λ) X q p(q|q )p(q |ca), (9) where p(q|q ) represents the similarity between the two topics q and q .",
                "To be able to work with similarity methods that are not necessarily probabilities, we set p(q|q ) = w(q,q ) γ , where γ is a normalizing constant, such that γ = P q w(q , q ).",
                "We consider four methods for calculating the similarity score between two topics.",
                "Three approaches are strictly content-based, and establish similarity by examining co-occurrence patterns of topics within the collection, while the last approach exploits the hierarchical structure of topical areas that may be present within an organization (see [7] for further examples of integrating word relationships into language models).",
                "The Kullback-Leibler (KL) divergence metric defined in Eq. 8 provides a measure of how different or similar two probability distributions are.",
                "A topic model is inferred for q and q using the method presented in Section 4.1 to describe the query across the entire vocabulary.",
                "Since a lower KL score means the queries are more similar, we let w(q, q ) = max(KL(θq||·) − KL(θq||θq )).",
                "Pointwise Mutual Information (PMI, [17]) is a measure of association used in information theory to determine the extent of independence between variables.",
                "The dependence between two queries is reflected by the SI(q, q ) score, where scores greater than zero indicate that it is likely that there is a dependence, which we take to mean that the queries are likely to be similar: SI(q, q ) = log p(q, q ) p(q)p(q ) (10) We estimate the probability of a topic p(q) using the number of documents relevant to query q within the collection.",
                "The joint probability p(q, q ) is estimated similarly, by using the concatenation of q and q as a query.",
                "To obtain p(q|q ), we then set w(q, q ) = SI(q, q ) when SI(q, q ) > 0 otherwise w(q, q ) = 0, because we are only interested in including queries that are similar.",
                "The log-likelihood statistic provides another measure of dependence, which is more reliable than the pointwise mutual information measure [17].",
                "Let k1 be the number of co-occurrences of q and q , k2 the number of occurrences of q not co-occurring with q , n1 the total number of occurrences of q , and n2 the total number of topic tokens minus the number of occurrences of q .",
                "Then, let p1 = k1/n1, p2 = k2/n2, and p = (k1 + k2)/(n1 + n2), (q, q ) = 2( (p1, k1, n1) + (p2, k2, n2) − (p, k1, n1) − (p, k2, n2)), where (p, n, k) = k log p + (n − k) log(1 − p).",
                "The higher score indicate that queries are also likely to be similar, thus we set w(q, q ) = (q, q ).",
                "Finally, we also estimate the similarity of two topics based on their distance within the topic hierarchy.",
                "The topic hierarchy is viewed as a directed graph, and for all topic-pairs the shortest path SP(q, q ) is calculated.",
                "We set the similarity score to be the reciprocal of the shortest path: w(q, q ) = 1/SP(q, q ). 7.2 Contextual information Given the hierarchy of an organization, the units to which a person belong are regarded as a context so as to compensate for data sparseness.",
                "We model it as follows: p (q|ca) = 1 − P ou∈OU(ca) λou · p(q|ca) + P ou∈OU(ca) λou · p(q|ou), where OU(ca) is the set of organizational units of which candidate ca is a member of, and p(q|o) expresses the strength of the association between query q and the unit ou.",
                "The latter probability can be estimated using either of the three basic models, by simply replacing ca with ou in the corresponding equations.",
                "An organizational unit is associated with all the documents that its members have authored.",
                "That is, p(d|ou) = maxca∈ou p(d|ca). 7.3 A simple multilingual model For knowledge institutes in Europe, academic or otherwise, a multilingual (or at least bilingual) setting is typical.",
                "The following model builds on a kind of independence assumption: there is no spill-over of expertise/profiles across language boundaries.",
                "While a simplification, this is a sensible first approach.",
                "That is: p (q|ca) =P l∈L λl · p(ql|ca), where L is the set of languages used in the collection, ql is the translation of the query q to language l, and λl is a language specific smoothing parameter, such that P l∈L λl = 1. 8.",
                "ADVANCED MODELS: EVALUATION In this section we present an experimental evaluation of our advanced models.",
                "Expert finding Expert profiling Language Model 1 Model 2 Model 3 Model 1 Model 2 Model 3 %q MAP MRR %q MAP MRR %q MAP MRR %ca MAP MRR %ca MAP MRR %ca MAP MRR English only 97.8 0.237 0.372 98.6 0.280 0.441 98.5 0.166 0.293 100 0.199 0.387 88.7 0.281 0.525 90.9 0.169 0.329 Dutch only 61.3 0.249 0.401 62.6 0.265 0.436 62.6 0.195 0.344 91.9 0.164 0.426 90.1 0.195 0.488 91.9 0.125 0.328 Combination 99.4 0.297 0.444 99.7 0.324 0.491 99.7 0.223 0.388 100 0.241 0.445 92.1 0.313 0.564 93.2 0.224 0.411 Table 3: Performance of the combination of languages on the expert finding and profiling tasks (on candidates).",
                "Best scores for each model are in italic, absolute best scores for the expert finding and profiling tasks are in boldface.",
                "Method Model 1 Model 2 Model 3 MAP MRR MAP MRR MAP MRR English BASELINE 0.296 0.454 0.339 0.509 0.221 0.333 KLDIV 0.291 0.453 0.327 0.503 0.219 0.330 PMI 0.291 0.453 0.337 0.509 0.219 0.331 LL 0.319 0.490 0.360 0.524 0.233 0.368 HDIST 0.299 0.465 0.346 0.537 0.219 0.332 Dutch BASELINE 0.240 0.350 0.271 0.403 0.227 0.389 KLDIV 0.239 0.347 0.253 0.386 0.224 0.385 PMI 0.239 0.350 0.260 0.392 0.227 0.389 LL 0.255 0.372 0.281 0.425 0.231 0.389 HDIST 0.253 0.365 0.271 0.407 0.236 0.402 Method Model 1 Model 2 Model 3 MAP MRR MAP MRR MAP MRR English BASELINE 0.485 0.546 0.499 0.548 0.381 0.416 KLDIV 0.510 0.564 0.513 0.558 0.381 0.416 PMI 0.486 0.546 0.495 0.542 0.407 0.451 LL 0.558 0.589 0.586 0.617 0.408 0.453 HDIST 0.507 0.567 0.512 0.563 0.386 0.420 Dutch BASELINE 0.263 0.313 0.294 0.358 0.262 0.315 KLDIV 0.284 0.336 0.271 0.321 0.261 0.314 PMI 0.265 0.317 0.265 0.316 0.273 0.330 LL 0.312 0.351 0.330 0.377 0.284 0.331 HDIST 0.280 0.327 0.288 0.341 0.266 0.321 Table 4: Performance on the expert finding (top) and profiling (bottom) tasks, using knowledge area similarities.",
                "Runs were evaluated on the main topics set.",
                "Best scores are in boldface. 8.1 Research Questions Our questions follow the refinements presented in the preceding section: Does exploiting the knowledge area similarity improve effectiveness?",
                "Which of the various methods for capturing word relationships is most effective?",
                "Furthermore, is our way of bringing in contextual information useful?",
                "For which tasks?",
                "And finally, is our simple way of combining the monolingual scores sufficient for obtaining significant improvements? 8.2 Experimental setup Given that the self-assessments are also sparse in our collection, in order to be able to measure differences between the various models, we selected a subset of topics, and evaluated (some of the) runs only on this subset.",
                "This set is referred as main topics, and consists of topics that are located at the top level of the topical hierarchy. (A main topic has subtopics, but is not a subtopic of any other topic.)",
                "This main set consists of 132 Dutch and 119 English topics.",
                "The relevance judgements were restricted to the main topic set, but were not expanded with subtopics. 8.3 Exploiting knowledge area similarity Table 4 presents the results.",
                "The four methods used for estimating knowledge-area similarity are KL divergence (KLDIV), PointLang.",
                "Topics Model 1 Model 2 Model 3 MAP MRR MAP MRR MAP MRR Expert finding UK ALL 0.423 0.545 0.654 0.799 0.494 0.629 UK MAIN 0.500 0.621 0.704 0.834 0.587 0.699 NL ALL 0.439 0.560 0.672 0.826 0.480 0.630 NL MAIN 0.440 0.584 0.645 0.816 0.515 0.655 Expert profiling UK ALL 0.240 0.640 0.306 0.778 0.223 0.616 UK MAIN 0.523 0.677 0.519 0.648 0.461 0.587 NL ALL 0.203 0.716 0.254 0.770 0.183 0.627 NL MAIN 0.332 0.576 0.380 0.624 0.332 0.549 Table 5: Evaluating the context models on organizational units. wise mutual information (PMI), log-likelihood (LL), and distance within topic hierarchy (HDIST).",
                "We managed to improve upon the baseline in all cases, but the improvement is more noticeable for the profiling task.",
                "For both tasks, the LL method performed best.",
                "The content-based approaches performed consistently better than HDIST. 8.4 Contextual information A two level hierarchy of organizational units (faculties and institutes) is available in the UvT Expert collection.",
                "The unit a person belongs to is used as a context for that person.",
                "First, we evaluated the models of the organizational units, using all topics (ALL) and only the main topics (MAIN).",
                "An organizational unit is considered to be relevant for a given topic (or vice versa) if at least one member of the unit selected the given topic as an expertise area.",
                "Table 5 reports on the results.",
                "As far as expert finding goes, given a topic, the corresponding organizational unit can be identified with high precision.",
                "However, the expert profiling task shows a different picture: the scores are low, and the task seems hard.",
                "The explanation may be that general concepts (i.e., our main topics) may belong to several organizational units.",
                "Second, we performed another evaluation, where we combined the contextual models with the candidate models (to score candidates again).",
                "Table 6 reports on the results.",
                "We find a positive impact of the context models only for expert finding.",
                "Noticably, for expert finding (and Model 1), it improves over 50% (for English) and over 70% (for Dutch) on MAP.",
                "The poor performance on expert profiling may be due to the fact that context models alone did not perform very well on the profiling task to begin with. 8.5 Multilingual models In this subsection we evaluate the method for combining results across multiple languages that we described in Section 7.3.",
                "In our setting the set of languages consists of English and Dutch: L = {UK, NL}.",
                "The weights on these languages were set to be identical (λUK = λNL = 0.5).",
                "We performed experiments with various λ settings, but did not observe significant differences in performance.",
                "Table 3 reports on the multilingual results, where performance is evaluated on the full topic set.",
                "All three models significantly imLang.",
                "Method Model 1 Model 2 Model 3 MAP MRR MAP MRR MAP MRR Expert finding UK BL 0.296 0.454 0.339 0.509 0.221 0.333 UK CT 0.330 0.491 0.342 0.500 0.228 0.342 NL BL 0.240 0.350 0.271 0.403 0.227 0.389 NL CT 0.251 0.382 0.267 0.410 0.246 0.404 Expert profiling UK BL 0.485 0.546 0.499 0.548 0.381 0.416 UK CT 0.562 0.620 0.508 0.558 0.440 0.486 NL BL 0.263 0.313 0.294 0.358 0.262 0.315 NL CT 0.330 0.384 0.317 0.387 0.294 0.345 Table 6: Performance of the context models (CT) compared to the baseline (BL).",
                "Best scores are in boldface. proved over all measures for both tasks.",
                "The coverage of topics and candidates for the expert finding and profiling tasks, respectively, is close to 100% in all cases.",
                "The relative improvement of the precision scores ranges from 10% to 80%.",
                "These scores demonstrate that despite its simplicity, our method for combining results over multiple languages achieves substantial improvements over the baseline. 9.",
                "CONCLUSIONS In this paper we focused on expertise retrieval (expert finding and profiling) in a new setting of a typical knowledge-intensive organization in which the available data is of high quality, multilingual, and covering a broad range of expertise area.",
                "Typically, the amount of available data in such an organization (e.g., a university, a research institute, or a research lab) is limited when compared to the W3C collection that has mostly been used for the experimental evaluation of expertise retrieval so far.",
                "To examine expertise retrieval in this setting, we introduced (and released) the UvT Expert collection as a representative case of such knowledge intensive organizations.",
                "The new collection reflects the typical properties of knowledge-intensive institutes noted above and also includes several features which may are potentially useful for expertise retrieval, such as topical and organizational structure.",
                "We evaluated how current state-of-the-art models for expert finding and profiling performed in this new setting and then refined these models in order to try and exploit the different characteristics within the data environment (language, topicality, and organizational structure).",
                "We found that current models of expertise retrieval generalize well to this new environment; in addition we found that refining the models to account for the differences results in significant improvements, thus making up for problems caused by data sparseness issues.",
                "Future work includes setting up manual assessments of automatically generated profiles by the employees themselves, especially in cases where the employees have not provided a profile themselves. 10.",
                "ACKNOWLEDGMENTS Krisztian Balog was supported by the Netherlands Organisation for Scientific Research (NWO) under project number 220-80-001.",
                "Maarten de Rijke was also supported by NWO under project numbers 017.001.190, 220-80-001, 264-70-050, 354-20-005, 600.065.120, 612-13-001, 612.000.106, 612.066.302, 612.069.006, 640.001.501, 640.002.501, and by the E.U.",
                "IST programme of the 6th FP for RTD under project MultiMATCH contract IST-033104.",
                "The work of Toine Bogers and Antal van den Bosch was funded by the IOP-MMI-program of SenterNovem / The Dutch Ministry of Economic Affairs, as part of the `A Propos project. 11.",
                "REFERENCES [1] L. Azzopardi.",
                "Incorporating Context in the Language Modeling Framework for ad-hoc Information Retrieval.",
                "PhD thesis, University of Paisley, 2005. [2] K. Balog and M. de Rijke.",
                "Finding similar experts.",
                "In This volume, 2007. [3] K. Balog and M. de Rijke.",
                "Determining expert profiles (with an application to expert finding).",
                "In IJCAI 07: Proc. 20th Intern.",
                "Joint Conf. on Artificial Intelligence, pages 2657-2662, 2007. [4] K. Balog, L. Azzopardi, and M. de Rijke.",
                "Formal models for expert finding in enterprise corpora.",
                "In SIGIR 06: Proc. 29th annual intern.",
                "ACM SIGIR conf. on Research and development in information retrieval, pages 43-50, 2006. [5] I. Becerra-Fernandez.",
                "The role of artificial intelligence technologies in the implementation of people-finder knowledge management systems.",
                "In AAAI Workshop on Bringing Knowledge to Business Processes, March 2000. [6] C. S. Campbell, P. P. Maglio, A. Cozzi, and B. Dom.",
                "Expertise identification using email communications.",
                "In CIKM 03: Proc. twelfth intern. conf. on Information and knowledge management, pages 528531, 2003. [7] G. Cao, J.-Y.",
                "Nie, and J. Bai.",
                "Integrating word relationships into language models.",
                "In SIGIR 05: Proc. 28th annual intern.",
                "ACM SIGIR conf. on Research and development in information retrieval, pages 298-305, 2005. [8] T. M. Cover and J.",
                "A. Thomas.",
                "Elements of Information Theory.",
                "Wiley-Interscience, 1991. [9] N. Craswell, D. Hawking, A. M. Vercoustre, and P. Wilkins.",
                "P@noptic expert: Searching for experts not just for documents.",
                "In Ausweb, 2001. [10] N. Craswell, A. de Vries, and I. Soboroff.",
                "Overview of the TREC2005 Enterprise Track.",
                "In The Fourteenth Text REtrieval Conf.",
                "Proc. (TREC 2005), 2006. [11] T. H. Davenport and L. Prusak.",
                "Working Knowledge: How Organizations Manage What They Know.",
                "Harvard Business School Press, Boston, MA, 1998. [12] T. Dunning.",
                "Accurate methods for the statistics of surprise and coincidence.",
                "Computational Linguistics, 19(1):61-74, 1993. [13] E. Filatova and J. Prager.",
                "Tell me what you do and Ill tell you what you are: Learning occupation-related activities for biographies.",
                "In HLT/EMNLP, 2005. [14] V. Lavrenko and W. B. Croft.",
                "Relevance based language models.",
                "In SIGIR 01: Proc. 24th annual intern.",
                "ACM SIGIR conf. on Research and development in information retrieval, pages 120-127, 2001. [15] V. Lavrenko, M. Choquette, and W. B. Croft.",
                "Cross-lingual relevance models.",
                "In SIGIR 02: Proc. 25th annual intern.",
                "ACM SIGIR conf. on Research and development in information retrieval, pages 175-182, 2002. [16] C. Macdonald and I. Ounis.",
                "Voting for candidates: adapting data fusion techniques for an expert search task.",
                "In CIKM 06: Proc. 15th ACM intern. conf. on Information and knowledge management, pages 387-396, 2006. [17] C. Manning and H. Sch¨utze.",
                "Foundations of Statistical Natural Language Processing.",
                "The MIT Press, 1999. [18] A. Mockus and J. D. Herbsleb.",
                "Expertise browser: a quantitative approach to identifying expertise.",
                "In ICSE 02: Proc. 24th Intern.",
                "Conf. on Software Engineering, pages 503-512, 2002. [19] D. Petkova and W. B. Croft.",
                "Hierarchical language models for expert finding in enterprise corpora.",
                "In Proc.",
                "ICTAI 2006, pages 599-608, 2006. [20] I. Soboroff, A. de Vries, and N. Craswell.",
                "Overview of the TREC 2006 Enterprise Track.",
                "In TREC 2006 Working Notes, 2006. [21] T. Tao, X. Wang, Q. Mei, and C. Zhai.",
                "Language model information retrieval with document expansion.",
                "In HLT-NAACL 2006, 2006. [22] TREC.",
                "Enterprise track, 2005.",
                "URL: http://www.ins.cwi. nl/projects/trec-ent/wiki/. [23] G. van Noord.",
                "TextCat Language Guesser.",
                "URL: http://www. let.rug.nl/˜vannoord/TextCat/. [24] W3C.",
                "The W3C test collection, 2005.",
                "URL: http://research. microsoft.com/users/nickcr/w3c-summary.html."
            ],
            "original_annotated_samples": [
                "Since the query is likely to consist of only a few terms to describe the expertise required, we should be able to obtain a more accurate estimate by invoking <br>bayes theorem</br>, and estimating: p(ca|q) = p(q|ca)p(ca) p(q) , (1) where p(ca) is the probability of a candidate and p(q) is the probability of a query."
            ],
            "translated_annotated_samples": [
                "Dado que es probable que la consulta consista solo en unos pocos términos para describir la experiencia requerida, deberíamos poder obtener una estimación más precisa al invocar el <br>Teorema de Bayes</br>, y estimar: p(ca|q) = p(q|ca)p(ca) p(q) , (1) donde p(ca) es la probabilidad de un candidato y p(q) es la probabilidad de una consulta."
            ],
            "translated_text": "Recuperación de amplia experiencia en entornos de datos dispersos Krisztian Balog ISLA, Universidad de Ámsterdam Kruislaan 403, 1098 SJ Ámsterdam, Países Bajos kbalog@science.uva.nl Toine Bogers ILK, Universidad de Tilburg P.O. Caja 90153, 5000 LE Tilburg, Países Bajos A.M.Bogers@uvt.nl Leif Azzopardi Dept. de Ciencias de la Computación Universidad de Glasgow, Glasgow, G12 8QQ leif@dcs.gla.ac.uk Maarten de Rijke ISLA, Universidad de Ámsterdam Kruislaan 403, 1098 SJ Ámsterdam, Países Bajos mdr@science.uva.nl Antal van den Bosch ILK, Universidad de Tilburg P.O. La recuperación de la experiencia ha sido ampliamente inexplorada en datos distintos a la colección de la W3C. Al mismo tiempo, muchas intranets de universidades y otras organizaciones intensivas en conocimiento ofrecen ejemplos de datos de expertos multilingües relativamente pequeños pero precisos, que abarcan amplias áreas de especialización. Primero presentamos dos tareas principales de recuperación de expertos, junto con un conjunto de enfoques de referencia basados en modelado del lenguaje generativo, con el objetivo de encontrar relaciones de experticia entre temas y personas. Para nuestra evaluación experimental, introducimos (y liberamos) un nuevo conjunto de pruebas basado en un rastreo de un sitio universitario. Usando este conjunto de pruebas, llevamos a cabo dos series de experimentos. El primero tiene como objetivo determinar la efectividad de los métodos de recuperación de conocimientos básicos aplicados al nuevo conjunto de pruebas. El segundo tiene como objetivo evaluar modelos refinados que exploten las características distintivas del nuevo conjunto de pruebas, como la estructura organizativa de la universidad y la estructura jerárquica de los temas en el conjunto de pruebas. Los modelos de recuperación de experiencia se muestran robustos con respecto a entornos más pequeños que la colección de la W3C, y las técnicas actuales parecen ser generalizables a otros contextos. Categorías y Descriptores de Asignaturas H.3 [Almacenamiento y Recuperación de Información]: H.3.1 Análisis de Contenido e Indexación; H.3.3 Búsqueda y Recuperación de Información; H.3.4 Sistemas y Software; H.4 [Aplicaciones de Sistemas de Información]: H.4.2 Tipos de Sistemas; H.4.m Términos Generales Misceláneos Algoritmos, Medición, Rendimiento, Experimentación 1. INTRODUCCIÓN La intranet de una organización proporciona un medio para intercambiar información entre empleados y facilitar colaboraciones entre ellos. Para lograr esto de manera eficiente y efectiva, es necesario proporcionar instalaciones de búsqueda que permitan a los empleados no solo acceder a documentos, sino también identificar a colegas expertos. En la pista de Empresas de TREC [22] se ha reconocido la necesidad de estudiar y comprender la recuperación de expertos a través de la introducción de tareas de Búsqueda de Expertos. El objetivo de la búsqueda de expertos es identificar una lista de personas que tienen conocimientos sobre un tema específico. Esta tarea suele abordarse descubriendo asociaciones entre personas y temas [10]; comúnmente, se asume que la co-ocurrencia del nombre de una persona con temas en el mismo contexto es evidencia de experiencia. Una tarea alternativa, que utiliza la misma idea de asociaciones entre personas y temas, es el perfilado de expertos, donde la tarea consiste en devolver una lista de temas sobre los que una persona tiene conocimiento [3]. El lanzamiento de la tarea de Búsqueda de Expertos en TREC ha generado mucho interés en la recuperación de expertos, con un rápido progreso en términos de modelado, algoritmos y aspectos de evaluación. Sin embargo, casi todo el trabajo de búsqueda o perfilado de expertos realizado ha sido validado experimentalmente utilizando la colección de W3C [24] de la pista empresarial. Si bien esta colección es actualmente la única colección de pruebas disponible públicamente para tareas de recuperación de expertos, solo representa un tipo de intranet. Con solo una colección de pruebas no es posible generalizar conclusiones a otros escenarios realistas. En este artículo nos enfocamos en la recuperación de expertos en un entorno realista que difiere del entorno de la W3C, uno en el que hay cantidades relativamente pequeñas de datos limpios y multilingües disponibles, que abarcan una amplia gama de áreas de expertise, como se puede encontrar en las intranets de universidades y otras organizaciones intensivas en conocimiento. Normalmente, esta configuración incluye varios tipos adicionales de estructuras: estructura temática (por ejemplo, jerarquías de temas utilizadas por la organización), estructura organizativa (facultad, departamento, ...), así como múltiples tipos de documentos (descripciones de investigación y cursos, publicaciones y páginas web académicas). Este entorno es bastante diferente del entorno del W3C de maneras que podrían afectar el rendimiento de las tareas de recuperación de conocimientos especializados. Nos enfocamos en varias preguntas de investigación en este artículo: ¿La cantidad relativamente pequeña de datos disponibles en una intranet afecta la calidad de las asociaciones tema-persona que son fundamentales en los algoritmos de recuperación de expertos? ¿Cómo se desempeñan los algoritmos de vanguardia desarrollados en el conjunto de datos del W3C en el escenario alternativo del tipo descrito anteriormente? Más en general, ¿se pueden aplicar las lecciones de la tarea de Búsqueda de Expertos en TREC a este contexto? ¿Cómo afecta la inclusión o exclusión de diferentes documentos a las tareas de recuperación de expertos? Además, ¿cómo se puede utilizar la estructura temática y organizativa con fines de recuperación? Para responder a nuestras preguntas de investigación, primero presentamos un conjunto de enfoques de línea base, basados en modelado de lenguaje generativo, con el objetivo de encontrar asociaciones entre temas y personas. Esto nos permite formular las tareas de búsqueda de expertos y perfilado de expertos de manera uniforme, y tiene el beneficio adicional de permitirnos entender las relaciones entre las dos tareas. Para nuestra evaluación experimental, introducimos un nuevo conjunto de datos (la Colección de Expertos de UvT) que es representativo del tipo de intranet que describimos anteriormente. Nuestra colección se basa en datos de acceso público, extraídos del sitio web de la Universidad de Tilburg (UvT). Este tipo de datos es particularmente interesante, ya que (1) es limpio, heterogéneo, estructurado y enfocado, pero comprende un número limitado de documentos; (2) contiene información sobre la jerarquía organizativa; (3) es bilingüe (inglés y holandés); y (4) la lista de áreas de especialización de un individuo es proporcionada por los propios empleados. Usando la colección de expertos de UvT, llevamos a cabo dos conjuntos de experimentos. El primero tiene como objetivo determinar la efectividad de los métodos de búsqueda y perfilado de experiencia inicial en este nuevo entorno. Un segundo grupo de experimentos está dirigido a extensiones de los métodos base que aprovechan las características distintivas de la Colección de Expertos de UvT; específicamente, proponemos y evaluamos métodos refinados de búsqueda y perfilado de expertos que incorporan la topicalidad y la estructura organizativa. Además de las preguntas de investigación y el conjunto de datos que aportamos, nuestras principales contribuciones son las siguientes. Los modelos base desarrollados para encontrar expertos funcionan bien en el nuevo conjunto de datos. Mientras que en el entorno del W3C la tarea de encontrar expertos parece ser más difícil que el perfilado, para los datos de UvT ocurre lo contrario. Encontramos que el perfilado en el conjunto de datos UvT es considerablemente más difícil que en el conjunto de datos W3C, lo cual creemos que se debe al gran (pero realista) número de áreas temáticas que utilizamos para el perfilado: alrededor de 1,500 para el conjunto de datos UvT, frente a 50 en el caso de W3C. Teniendo en cuenta la similitud entre los temas puede mejorar significativamente el rendimiento de recuperación. Las medidas de similitud de mejor rendimiento son basadas en contenido, por lo tanto también se pueden aplicar en entornos de la W3C (y otros). Finalmente, demostramos que la estructura organizativa puede ser explotada en forma de un modelo de contexto, mejorando los puntajes de MAP para ciertos modelos hasta en un 70%. El resto de este documento está organizado de la siguiente manera. En la siguiente sección revisamos el trabajo relacionado. Luego, en la Sección 3 proporcionamos descripciones detalladas de las tareas de recuperación de expertos que abordamos en este artículo: la búsqueda de expertos y la creación de perfiles de expertos. En la Sección 4 presentamos nuestros modelos base, cuyo rendimiento luego es evaluado en la Sección 6 utilizando el conjunto de datos UvT que presentamos en la Sección 5. Los modelos avanzados que explotan características específicas de nuestros datos se presentan en la Sección 7 y se evalúan en la Sección 8. Formulamos nuestras conclusiones en la Sección 9.2. Trabajos relacionados Los enfoques iniciales para encontrar expertos a menudo empleaban bases de datos que contenían información sobre las habilidades y conocimientos de cada individuo en la organización [11]. La mayoría de estas herramientas (generalmente llamadas páginas amarillas o sistemas de búsqueda de personas) dependen de que las personas evalúen sus habilidades frente a un conjunto predefinido de palabras clave. Para actualizar perfiles en estos sistemas de forma automática, se necesita de tecnologías inteligentes [5]. Enfoques más recientes utilizan conjuntos de documentos específicos (como correos electrónicos [6] o software [18]) para encontrar experiencia. En contraste con centrarse en tipos de documentos particulares, también hay un interés creciente en el desarrollo de sistemas que indexan y extraen información de documentos publicados en intranets como fuentes de evidencia de experiencia. Un enfoque publicado es el sistema P@noptic [9], que construye una representación de cada persona concatenando todos los documentos asociados con esa persona, similar al Modelo 1 de Balog et al. [4], quienes formalizan y comparan dos métodos. El Modelo 1 de Balog et al. modela directamente el conocimiento de un experto a partir de documentos asociados, mientras que su Modelo 2 primero localiza documentos sobre el tema y luego encuentra a los expertos asociados. En los experimentos reportados, el segundo método tiene un rendimiento significativamente mejor cuando hay suficientes documentos asociados por candidato. La mayoría de los sistemas que participaron en las ediciones de 2005 y 2006 de la tarea de Búsqueda de Expertos en TREC implementaron (variaciones de) uno de estos dos modelos; ver [10, 20]. Macdonald y Ounis [16] proponen un enfoque diferente para clasificar la experiencia de los candidatos con respecto a un tema basado en técnicas de fusión de datos, sin utilizar heurísticas específicas de la colección; encuentran que la aplicación de modelos de ponderación basados en campos mejora la clasificación de los candidatos. Petkova y Croft [19] proponen otro enfoque, basado en una combinación de los Modelos 1 y 2 anteriores, modelando explícitamente los temas. Al abordar otras tareas de recuperación de expertos que también pueden abordarse utilizando asociaciones entre temas y personas, Balog y de Rijke [3] abordaron la tarea de determinar perfiles de expertos temáticos. Si bien sus métodos resultaron ser eficientes en el corpus de la W3C, requieren una cantidad de datos que puede que no esté disponible en la típica organización intensiva en conocimiento. Balog y de Rijke [2] estudian la tarea relacionada de encontrar expertos que sean similares a un pequeño conjunto de expertos dados como entrada. Por cierto, crear un resumen textual de una persona muestra algunas similitudes con la búsqueda de biografías, la cual ha recibido una considerable cantidad de atención recientemente; ver por ejemplo, [13]. Utilizamos modelado de lenguaje generativo para encontrar asociaciones entre temas y personas. En nuestro modelado de búsqueda y perfilado de expertos recopilamos evidencia de experiencia de múltiples fuentes, en una colección heterogénea, e integramos esta información con la co-ocurrencia de nombres de candidatos y términos de consulta: el entorno de modelado de lenguaje nos permite hacer esto de manera transparente. Nuestro modelado avanza en dos pasos. En el primer paso, consideramos tres modelos base, dos tomados de [4] (los Modelos 1 y 2 mencionados anteriormente), y uno una versión refinada de un modelo introducido en [3] (al que nos referimos como Modelo 3 a continuación); este tercer modelo también es similar al modelo descrito por Petkova y Croft [19]. Los modelos que consideramos en nuestra segunda ronda de experimentos son modelos de mezcla similares a los modelos de lenguaje contextual [1] y a los documentos ampliados de Tao et al. [21]; sin embargo, las características que utilizamos para definir nuestras expansiones, incluida la estructura temática y la estructura organizativa, no se han utilizado de esta manera antes. 3. En el escenario de recuperación de expertos que imaginamos, los usuarios que buscan expertos dentro de una organización tienen acceso a una interfaz que combina un cuadro de búsqueda (donde pueden buscar expertos o temas) con estructuras de navegación (de expertos y de temas) que les permite hacer clic hasta llegar a una página de experto (que proporciona el perfil de una persona) o a una página de tema (que proporciona una lista de expertos en el tema). Para alimentar la interfaz anterior, nos enfrentamos a dos tareas de recuperación de expertos, la búsqueda de expertos y el perfilado de expertos, que primero definimos y luego formalizamos utilizando modelos de lenguaje generativos. Para modelar cualquiera de las tareas, la probabilidad de que el tema de la consulta esté asociado a un experto candidato juega un papel clave en las estimaciones finales para la búsqueda y el perfilado. Al utilizar modelos de lenguaje, tanto los candidatos como la consulta se caracterizan por distribuciones de términos en el vocabulario (utilizado en los documentos puestos a disposición por la organización cuyas necesidades de recuperación de experiencia estamos abordando). 3.1 Búsqueda de expertos La búsqueda de expertos implica la tarea de encontrar a la persona adecuada con las habilidades y conocimientos apropiados: ¿Quiénes son los expertos en el tema X?. Por ejemplo, un empleado quiere averiguar quién trabajó en un proyecto en particular para descubrir por qué se tomaron decisiones específicas sin tener que revisar minuciosamente la documentación (si es que la hay). O pueden necesitar un especialista capacitado para consultoría sobre un problema específico. Dentro de una organización, generalmente hay muchos posibles candidatos que podrían ser expertos en un tema dado. Podemos plantear este problema de la siguiente manera: ¿Cuál es la probabilidad de que un candidato ca sea un experto dado el tema de consulta q? Es decir, determinamos p(ca|q) y clasificamos a los candidatos ca de acuerdo con esta probabilidad. Los candidatos con la probabilidad más alta dada la consulta son considerados los expertos más probables en ese tema. El desafío es cómo estimar esta probabilidad con precisión. Dado que es probable que la consulta consista solo en unos pocos términos para describir la experiencia requerida, deberíamos poder obtener una estimación más precisa al invocar el <br>Teorema de Bayes</br>, y estimar: p(ca|q) = p(q|ca)p(ca) p(q) , (1) donde p(ca) es la probabilidad de un candidato y p(q) es la probabilidad de una consulta. Dado que p(q) es una constante, se puede ignorar para fines de clasificación. Por lo tanto, la probabilidad de que un candidato ca sea un experto dado la consulta q es proporcional a la probabilidad de una consulta dada el candidato p(q|ca), ponderada por la creencia a priori p(ca) de que el candidato ca es un experto. p(ca|q) ∝ p(q|ca)p(ca) (2) En este artículo, nuestro enfoque principal se centra en estimar la probabilidad de una consulta dada el candidato p(q|ca), ya que esta probabilidad captura en qué medida el candidato conoce sobre el tema de la consulta. Si bien se asume generalmente que los priors del candidato son uniformes y, por lo tanto, no influirán en la clasificación, se ha demostrado que una elección sensata de priors puede mejorar el rendimiento [20]. Perfilado de expertos. Mientras que la tarea de búsqueda de expertos se centraba en encontrar expertos en un tema específico, la tarea de perfilado de expertos busca responder a una pregunta relacionada: ¿Sobre qué temas tiene conocimiento un candidato? Básicamente, esto da un giro a las preguntas sobre la búsqueda de expertos. El perfilado de un candidato individual implica la identificación de áreas de habilidades y conocimientos en las que tienen experiencia, así como una evaluación del nivel de competencia en cada una de estas áreas. Este es el perfil temático de los candidatos. Generalmente, los perfiles temáticos dentro de las organizaciones consisten en estructuras tabulares que catalogan explícitamente las habilidades y conocimientos de cada individuo en la organización. Sin embargo, esta práctica está limitada por los recursos disponibles para definir, crear, mantener y actualizar estos perfiles con el tiempo. Al centrarnos en métodos automáticos que se basan en la evidencia disponible dentro de los repositorios de documentos de una organización, nuestro objetivo es reducir el esfuerzo humano asociado con el mantenimiento de perfiles temáticos. Un perfil temático de un candidato, entonces, se define como un vector donde cada elemento i del vector corresponde a la experiencia del candidato en un tema dado ki, (es decir, s(ca, ki)). Cada tema ki define un área de conocimiento o habilidad particular que la organización utiliza para definir el perfil temático de los candidatos. Por lo tanto, se asume que se proporciona una lista de temas, {k1, . . . , kn}, donde n es el número de temas predefinidos: perfil(ca) = s(ca, k1), s(ca, k2), . . . , s(ca, kn) . Se necesita contexto y evidencia para ayudar a los usuarios de sistemas de búsqueda de expertos a decidir a quién contactar al buscar experiencia en un área particular. Ejemplos de dicho contexto son: ¿Con quién trabaja ella? ¿Cuáles son sus datos de contacto? ¿Está bien conectada, por si acaso no puede ayudarnos ella misma? ¿Cuál es su rol en la organización? ¿Quién es su superior? Los colaboradores, afiliaciones, etc. son parte del perfil social de los candidatos y pueden servir como antecedentes para interpretar las recomendaciones del sistema. En este documento solo abordamos el problema de determinar perfiles temáticos, dejando el perfilado social para trabajos futuros. Planteamos el problema de cuantificar la competencia de una persona en un área de conocimiento específica de la siguiente manera: ¿Cuál es la probabilidad de que un área de conocimiento (ki) forme parte del perfil de candidatos (expertise)? donde s(ca, ki) está definido por p(ki|ca). Nuestra tarea, entonces, es estimar p(ki|ca), lo cual es equivalente al problema de obtener p(q|ca), donde el tema ki se representa como un tema de consulta q, es decir, una secuencia de palabras clave que representan la experiencia requerida. Tanto la tarea de encontrar expertos como la de perfilar expertos dependen de la estimación precisa de p(q|ca). La única diferencia proviene de la probabilidad previa de que una persona sea experta (p(ca)), la cual puede ser incorporada en la tarea de encontrar expertos. Este antecedente no se aplica a la tarea de perfilado ya que el candidato (individuo) está fijo. 4. MODELOS DE REFERENCIA En esta sección describimos nuestros modelos de referencia para estimar p(q|ca), es decir, las asociaciones entre temas y personas. Tanto la búsqueda de expertos como el perfilado de expertos se reducen a esta estimación. Empleamos tres modelos para calcular esta probabilidad. 4.1 De temas a candidatos Utilizando Modelos de Candidatos: El Modelo 1 [4] define la probabilidad de una consulta dada un candidato (p(q|ca)) utilizando técnicas estándar de modelado de lenguaje, basadas en un modelo de lenguaje multinomial de unigrama. Para cada candidato ca, se infiere un modelo de lenguaje candidato θca de tal manera que la probabilidad de un término dado θca no sea cero para todos los términos, es decir, p(t|θca) > 0. A partir del modelo de candidato, la consulta se genera con la siguiente probabilidad: p(q|θca) = Y t∈q p(t|θca)n(t,q), donde cada término t en la consulta q se muestrea de manera idéntica e independiente, y n(t, q) es el número de veces que t ocurre en q. El modelo de lenguaje candidato se infiere de la siguiente manera: (1) se calcula un modelo empírico p(t|ca); (2) se suaviza con probabilidades de fondo. Usando las asociaciones entre un candidato y un documento, la probabilidad p(t|ca) puede aproximarse por: p(t|ca) = X d p(t|d)p(d|ca), donde p(d|ca) es la probabilidad de que el candidato ca genere un documento de apoyo d, y p(t|d) es la probabilidad de que un término t ocurra en el documento d. Utilizamos la estimación de máxima verosimilitud de un término, es decir, la frecuencia normalizada del término t en el documento d. La fuerza de la asociación entre el documento d y el candidato ca expresada por p(d|ca) refleja el grado en que la experiencia de los candidatos se describe utilizando este documento. La estimación de esta probabilidad se presenta más adelante, en la Sección 4.2. El modelo candidato se construye entonces como una interpolación lineal de p(t|ca) y el modelo de fondo p(t) para asegurar que no haya probabilidades nulas, lo que resulta en la estimación final: p(q|θca) = (4) Y t∈q ( (1 − λ) X d p(t|d)p(d|ca) ! + λp(t) )n(t,q) . El Modelo 1 recopila toda la información de términos de todos los documentos asociados con el candidato, y la utiliza para representar a ese candidato. Este modelo se utiliza para predecir qué tan probable es que un candidato produzca una consulta q. Esto puede interpretarse intuitivamente como la probabilidad de que este candidato hable sobre el tema de la consulta, donde asumimos que esto es indicativo de su experiencia. Usando Modelos de Documentos: El Modelo 2 [4] toma un enfoque diferente. Aquí, el proceso se divide en dos partes. Dado un candidato ca, (1) se selecciona un documento asociado con un candidato con probabilidad p(d|ca), y (2) a partir de este documento se genera una consulta q con probabilidad p(q|d). Entonces se toma la suma sobre todos los documentos para obtener p(q|ca), de modo que: p(q|ca) = Σ d p(q|d)p(d|ca). (5) La probabilidad de una consulta dada un documento se estima inferiendo un modelo de lenguaje del documento θd para cada documento d de manera similar a como se infería el modelo candidato: p(t|θd) = (1 − λ)p(t|d) + λp(t), (6) donde p(t|d) es la probabilidad del término en el documento. La probabilidad de una consulta dada el modelo de documento es: p(q|θd) = Y t∈q p(t|θd)n(t,q) . La estimación final de p(q|ca) se obtiene sustituyendo p(q|d) por p(q|θd) en la Ecuación 5 (ver [4] para más detalles). Conceptualmente, el Modelo 2 difiere del Modelo 1 porque el candidato no está modelado directamente. En cambio, el documento actúa como una variable oculta en el proceso que separa la consulta del candidato. Este proceso es similar a cómo un usuario puede buscar candidatos con un motor de búsqueda estándar: inicialmente encontrando los documentos relevantes y luego viendo quién está asociado con ese documento. Al examinar una serie de documentos, el usuario puede obtener una idea de qué candidatos son más propensos a discutir el tema q. Usando Modelos de Temas: Modelo 3 Introducimos un tercer modelo, Modelo 3. En lugar de intentar modelar el proceso de generación de consultas a través de modelos de candidatos o documentos, representamos la consulta como un modelo de lenguaje de temas y estimamos directamente la probabilidad del candidato p(ca|q). Este enfoque es similar al modelo presentado en [3, 19]. Como en los modelos anteriores, se infiere un modelo de lenguaje, pero esta vez para la consulta. Adaptamos el trabajo de Lavrenko y Croft [14] para estimar un modelo de tema a partir de la consulta. El procedimiento es el siguiente. Dada una colección de documentos y un tema de consulta q, se asume que existe un modelo de tema desconocido θk que asigna probabilidades p(t|θk) a las ocurrencias de términos en los documentos del tema. Tanto la consulta como los documentos son muestras de θk (a diferencia de los enfoques anteriores, donde se asume que una consulta se extrae de un documento específico o un modelo candidato). La tarea principal es estimar p(t|θk), la probabilidad de un término dado el modelo de tema. Dado que la consulta q es muy dispersa y no hay ejemplos de documentos sobre el tema, es necesario aproximar esta distribución. Lavrenko y Croft [14] sugieren una forma razonable de obtener dicha aproximación, asumiendo que p(t|θk) puede aproximarse por la probabilidad del término t dado la consulta q. Entonces podemos estimar p(t|q) utilizando la probabilidad conjunta de observar el término t junto con los términos de la consulta, q1, . . . , qm, y dividiendo por la probabilidad conjunta de los términos de la consulta: p(t|θk) ≈ p(t|q) = p(t, q1, . . . , qm) p(q1, . . . , qm) = p(t, q1, . . . , qm) P t ∈T p(t , q1, . . . , qm) , donde p(q1, . . . , qm) = P t ∈T p(t , q1, . . . , qm), y T es el vocabulario completo de términos. Para estimar la probabilidad conjunta p(t, q1, . . . , qm), seguimos [14, 15] y asumimos que t y q1, . . . , qm son mutuamente independientes, una vez que elegimos una distribución fuente del conjunto de distribuciones fuente subyacentes U. Si elegimos U como un conjunto de modelos de documentos, entonces para construir este conjunto, la consulta q se emitiría contra la colección, y se asume que los primeros n devueltos son relevantes para el tema, y por lo tanto se tratan como muestras del modelo de tema. (Tenga en cuenta que en su lugar podrían usarse modelos candidatos). Con los modelos de documentos formando U, la probabilidad conjunta del término y la consulta se convierte en: p(t, q1, . . . , qm) = X d∈U p(d) ˘ p(t|θd) mY i=1 p(qi|θd) ¯ . (7) Aquí, p(d) denota la distribución previa sobre el conjunto U, que refleja la relevancia del documento para el tema. Suponemos que p(d) es uniforme en todo U. Para clasificar a los candidatos según el modelo de tema definido, utilizamos la métrica de divergencia de Kullback-Leibler (KL, [8]) para medir la diferencia entre los modelos de los candidatos y el modelo de tema: KL(θk||θca) = X t p(t|θk) log p(t|θk) p(t|θca) . (8) Se considera que los candidatos con una divergencia menor respecto al modelo de tema son más propensos a ser expertos en ese tema. El modelo candidato θca está definido en la Ecuación 4. Al utilizar la divergencia de Kullback-Leibler en lugar de la probabilidad de un candidato dado el modelo de tema p(ca|θk), evitamos problemas de normalización. 4.2 Asociaciones documento-candidato Para nuestros modelos, necesitamos poder estimar la probabilidad p(d|ca), que expresa en qué medida un documento d caracteriza al candidato ca. En [4], se presentan dos métodos para estimar esta probabilidad, basados en el número de nombres de personas reconocidos en un documento. Sin embargo, en nuestro entorno de intranet es razonable asumir que los autores de los documentos pueden ser identificados de manera inequívoca (por ejemplo, como el autor de un artículo, el profesor asignado a un curso, el propietario de una página web, etc.). Por lo tanto, establecemos p(d|ca) en 1 si el candidato ca es el autor del documento d, de lo contrario la probabilidad es 0. En la Sección 6 describimos cómo se puede determinar la autoría en diferentes tipos de documentos dentro de la colección. 5. La colección de expertos de UvT utilizada en los experimentos de este artículo se ajusta al escenario descrito en la Sección 3. La colección se basa en el sistema Webwijs (Webwise) desarrollado en la Universidad de Tilburg (UvT) en los Países Bajos. Webwijs (http://www.uvt.nl/webwijs/) es una base de datos de acceso público de empleados de la UvT que están involucrados en investigación o enseñanza; actualmente, Webwijs contiene información sobre 1168 expertos, cada uno de los cuales tiene una página con información de contacto y, si está disponible por parte del experto, una descripción de investigación y lista de publicaciones. Además, cada experto puede seleccionar áreas de especialización de una lista de 1491 temas y se le anima a sugerir nuevos temas que necesitan ser aprobados por el editor de Webwijs. Cada tema tiene una página separada que muestra a todos los expertos asociados con ese tema y, si está disponible, una lista de temas relacionados. Webwijs está disponible en holandés e inglés, y esta bilingüidad se ha preservado en la colección. Cada página de Webwijs en holandés tiene una traducción al inglés. No todos los temas en holandés tienen una traducción al inglés, pero lo contrario es cierto: los 981 temas en inglés tienen un equivalente en holandés. Aproximadamente el 42% de los expertos imparten cursos en la Universidad de Tilburg; estos cursos también fueron rastreados e incluidos en el perfil. Además, aproximadamente el 27% de los expertos enlazan su página académica desde su página de Webwijs. Estas páginas de inicio fueron rastreadas y añadidas a la colección. (Esto significa que si los expertos colocaron las versiones completas de sus publicaciones en sus páginas de inicio académicas, estas también estaban disponibles para su indexación). También obtuvimos 1880 versiones completas de publicaciones del repositorio institucional de UvT y el número de expertos holandeses en inglés es de 1168, con 743 expertos con ≥ 1 tema. El número de temas es de 1491 y 981, y el número de pares experto-tema es de 4318 y 3251, con un promedio de temas por experto de 5.8 y 5.9 respectivamente. El máximo número de temas por experto es de 60 (1) y 35 (1), y el mínimo es de 1 (74) y 1 (106) respectivamente. El promedio de expertos por tema es de 2.9 y 3.3, con un máximo de 30 (1) y un mínimo de 1 (615) y 1 (346) respectivamente. Hay 318 expertos con HP y CD, con un promedio de 3.5 CDs por experto docente. Además, hay 329 expertos con RD y 734 con PUB, con un promedio de 27.0 publicaciones por experto, 25.2 citas por experto y 1.8 publicaciones completas por experto. Tabla 2: Estadísticas descriptivas de las versiones holandesas e inglesas de la colección de expertos de UvT. Ejecutamos el identificador de idioma TextCat [23] para clasificar el idioma de las páginas de inicio y las publicaciones de texto completo. Nos limitamos a las páginas en las que el clasificador tenía confianza en el idioma utilizado en la página. Esto resultó en cuatro tipos de documentos: descripciones de investigación (RD), descripciones de cursos (CD), publicaciones (PUB; versiones de texto completo y solo de cita) y páginas web académicas (HP). Todo fue agrupado en la colección UvT Expert que está disponible en http://ilk.uvt.nl/uvt-expert-collection/. La colección UvT Expert fue extraída de un entorno organizativo diferente a la colección de la W3C y difiere de ella en varios aspectos. El entorno de UvT es uno con cantidades relativamente pequeñas de datos multilingües. Las asociaciones entre los documentos y los autores son claras y los datos están estructurados y limpios. La colección abarca una amplia gama de áreas de especialización, como suele encontrarse en intranets de universidades y otros institutos intensivos en conocimiento. Además, nuestro entorno universitario cuenta con varios tipos de estructura (temática y organizativa), así como múltiples tipos de documentos. Otra diferencia importante entre los dos conjuntos de datos es que las áreas de especialización en la colección de Expertos de UvT son autoseleccionadas en lugar de basarse en la membresía de un grupo o asignaciones de otros. El tamaño es otra dimensión en la que difieren las colecciones de expertos de W3C y UvT: esta última es la más pequeña de las dos. También son realistas las grandes diferencias en la cantidad de información disponible para cada experto. Utilizar Webwijs es voluntario; 425 expertos holandeses no seleccionaron ningún tema en absoluto. Esto nos deja con 743 perfiles de expertos holandeses y 727 perfiles de expertos en inglés utilizables. La Tabla 2 proporciona estadísticas descriptivas para la colección de Expertos de UvT. Las universidades tienden a tener una estructura jerárquica que va desde el nivel de la facultad, pasando por los departamentos, grupos de investigación, hasta llegar a los investigadores individuales. En la colección de Expertos de UvT tenemos información sobre las afiliaciones de los investigadores con facultades e institutos, lo que nos proporciona una jerarquía organizativa de dos niveles. La Universidad de Tilburg tiene 22 unidades organizativas a nivel de facultad (incluida la oficina universitaria y varios institutos de investigación) y 71 departamentos, lo que equivale a 3.2 departamentos por facultad. En cuanto a la jerarquía temática utilizada por Webwijs, 131 de los 1491 temas son nodos principales en la jerarquía. Esta jerarquía tiene una longitud promedio de cadena de temas de 2.65 y una longitud máxima de 7 temas. 6. EVALUACIÓN A continuación, evaluamos los modelos de la Sección 4 para la búsqueda y perfilado de expertos en la colección de Expertos de UvT. Detallamos nuestras preguntas de investigación y configuración experimental, y luego presentamos nuestros resultados. 6.1 Preguntas de investigación Abordamos las siguientes preguntas de investigación. Tanto la búsqueda de expertos como el perfilado dependen de las estimaciones de p(q|ca). La pregunta es cómo se comparan los modelos en las diferentes tareas, y en el contexto de la colección de expertos de UvT. En [4], el Modelo 2 superó al Modelo 1 en la colección de la W3C. ¿Cómo se comparan en nuestro conjunto de datos? ¿Y cómo se compara el Modelo 3 con el Modelo 1? ¿Qué hay de las diferencias de rendimiento entre los dos idiomas en nuestra colección de pruebas? 6.2 Configuración Experimental La salida de nuestros modelos fue evaluada frente a las etiquetas de temas autoasignadas, que fueron tratadas como juicios de relevancia. Los resultados fueron evaluados por separado para inglés y holandés. Para inglés solo usamos temas para los cuales había traducción al neerlandés; para neerlandés se consideraron todos los temas. Los resultados se promediaron para las consultas en la intersección de las evaluaciones de relevancia y los resultados; las consultas faltantes no contribuyen con un valor de 0 a las puntuaciones. Utilizamos medidas estándar de recuperación de información, como la Precisión Media Promedio (MAP) y la Reciprocidad Media Promedio (MRR). También informamos el porcentaje de temas (%q) y candidatos (%ca) cubiertos, respectivamente, para las tareas de búsqueda y perfilado de expertos. 6.3 Resultados La Tabla 1 muestra el rendimiento de los Modelos 1, 2 y 3 en las tareas de búsqueda y perfilado de expertos. Las filas de la tabla corresponden a los diversos tipos de documentos (RD, CD, PUB y HP) y a sus combinaciones. RD+CD+PUB+HP es equivalente a la colección completa y será referida como la LÍNEA BASE de nuestros experimentos. Al observar la Tabla 1, vemos que el Modelo 2 tiene el mejor rendimiento en general. Sin embargo, cuando los datos están limpios y muy enfocados (RD), el Modelo 3 lo supera en varios casos. El modelo 1 tiene la mejor cobertura de candidatos (%ca) y temas (%q). Los diversos tipos de documentos difieren en sus características y en cómo mejoran las tareas de búsqueda y perfilado. El perfilado de expertos se beneficia mucho de los datos limpios presentes en los tipos de documentos RD y CD, mientras que las publicaciones contribuyen principalmente a la tarea de encontrar expertos. Agregar las páginas de inicio no resulta ser particularmente útil. Cuando comparamos los resultados entre idiomas, encontramos que la cobertura de los temas en inglés (%q) es mayor que la de los temas en neerlandés para la búsqueda de expertos. Además de eso, las puntuaciones caen en el mismo rango para ambos idiomas. Para la tarea de perfilado, la cobertura de los candidatos (%ca) es muy similar para ambos idiomas. Sin embargo, el rendimiento es considerablemente mejor para los temas en inglés. Si bien es difícil comparar las puntuaciones entre colecciones, concluimos con una breve comparación de las puntuaciones absolutas en la Tabla 1 con las reportadas en [3, 4] en el conjunto de pruebas de W3C (edición de 2005). Para el experto que busca, los puntajes MAP para el Modelo 2 reportados aquí son aproximadamente un 50% más altos que las cifras correspondientes en [4], mientras que nuestros puntajes MRR están ligeramente por debajo de los de [4]. Para el perfilado de expertos, las diferencias son mucho más dramáticas: las puntuaciones MAP para el Modelo 2 reportadas aquí son aproximadamente un 50% más bajas que las puntuaciones en [3], mientras que las puntuaciones MRR (mejores) son aproximadamente iguales a las de [3]. La causa de estas diferencias parece residir en el número de áreas de conocimiento consideradas aquí, aproximadamente 30 veces más que en el entorno del W3C. MODELOS AVANZADOS Ahora que hemos desarrollado y evaluado técnicas básicas de modelado de lenguaje para la recuperación de expertos, nos enfocamos en modelos refinados que explotan características especiales de nuestra colección de pruebas. 7.1 Explotando la similitud en el área de conocimiento Una forma de mejorar la puntuación de una consulta dada un candidato es considerar qué otras solicitudes satisfaría el candidato y usarlas como evidencia adicional para respaldar la consulta original, proporcionalmente. Búsqueda de expertos Perfilado de expertos Tipos de documentos Modelo 1 Modelo 2 Modelo 3 Modelo 1 Modelo 2 Modelo 3 %q MAP MRR %q MAP MRR %q MAP MRR %ca MAP MRR %ca MAP MRR %ca MAP MRR Inglés RD 97.8 0.126 0.269 83.5 0.144 0.311 83.3 0.129 0.271 100 0.089 0.189 39.3 0.232 0.465 41.1 0.166 0.337 CD 97.8 0.118 0.227 91.7 0.123 0.248 91.7 0.118 0.226 32.8 0.188 0.381 32.4 0.195 0.385 32.7 0.203 0.370 PUB 97.8 0.200 0.330 98.0 0.216 0.372 98.0 0.145 0.257 78.9 0.167 0.364 74.5 0.212 0.442 78.9 0.135 0.299 HP 97.8 0.081 0.186 97.4 0.071 0.168 97.2 0.062 0.149 31.2 0.150 0.299 28.8 0.185 0.335 30.1 0.136 0.287 RD+CD 97.8 0.188 0.352 92.9 0.193 0.360 92.9 0.150 0.273 100 0.145 0.286 61.3 0.251 0.477 63.2 0.217 0.416 RD+CD+PUB 97.8 0.235 0.373 98.1 0.277 0.439 98.1 0.178 0.305 100 0.196 0.380 87.2 0.280 0.533 89.5 0.170 0.344 RD+CD+PUB+HP 97.8 0.237 0.372 98.6 0.280 0.441 98.5 0.166 0.293 100 0.199 0.387 88.7 0.281 0.525 90.9 0.169 0.329 Holandés RD 61.3 0.094 0.229 38.4 0.137 0.336 38.3 0.127 0.295 38.0 0.127 0.386 34.1 0.138 0.420 38.0 0.105 0.327 CD 61.3 0.107 0.212 49.7 0.128 0.256 49.7 0.136 0.261 32.5 0.151 0.389 31.8 0.158 0.396 32.5 0.170 0.380 PUB 61.3 0.193 0.319 59.5 0.218 0.368 59.4 0.173 0.291 78.8 0.126 0.364 76.0 0.150 0.424 78.8 0.103 0.294 HP 61.3 0.063 0.169 56.6 0.064 0.175 56.4 0.062 0.163 29.8 0.108 0.308 27.8 0.125 0.338 29.8 0.098 0.255 RD+CD 61.3 0.159 0.314 51.9 0.184 0.360 51.9 0.169 0.324 60.5 0.151 0.410 57.2 0.166 0.431 60.4 0.159 0.384 RD+CD+PUB 61.3 0.244 0.398 61.5 0.260 0.424 61.4 0.210 0.350 90.3 0.165 0.445 88.2 0.189 0.479 90.3 0.126 0.339 RD+CD+PUB+HP 61.3 0.249 0.401 62.6 0.265 0.436 62.6 0.195 0.344 91.9 0.164 0.426 90.1 0.195 0.488 91.9 0.125 0.328 Tabla 1: Rendimiento de los modelos en las tareas de búsqueda y perfilado de expertos, utilizando diferentes tipos de documentos y sus combinaciones. %q es el número de temas cubiertos (se aplica a la tarea de búsqueda de expertos), %ca es el número de candidatos cubiertos (se aplica a la tarea de perfilado de expertos). Los bloques superior e inferior corresponden al inglés y al holandés respectivamente. Las mejores puntuaciones están en negrita. Esto se puede modelar interpolando entre el p(q|ca) y la evidencia adicional de todos los pedidos similares q, de la siguiente manera: p(q|ca) = λp(q|ca) + (1 − λ) X q p(q|q )p(q |ca), (9) donde p(q|q ) representa la similitud entre los dos temas q y q. Para poder trabajar con métodos de similitud que no son necesariamente probabilidades, establecemos p(q|q ) = w(q,q ) γ , donde γ es una constante de normalización, de modo que γ = P q w(q , q ). Consideramos cuatro métodos para calcular la puntuación de similitud entre dos temas. Tres enfoques se basan estrictamente en el contenido y establecen similitudes examinando patrones de co-ocurrencia de temas dentro de la colección, mientras que el último enfoque explota la estructura jerárquica de áreas temáticas que pueden estar presentes dentro de una organización (ver [7] para más ejemplos de integrar relaciones entre palabras en modelos de lenguaje). La métrica de divergencia Kullback-Leibler (KL) definida en la ecuación 8 proporciona una medida de cuán diferentes o similares son dos distribuciones de probabilidad. Se infiere un modelo de tema para q y q utilizando el método presentado en la Sección 4.1 para describir la consulta en todo el vocabulario. Dado que un puntaje KL más bajo significa que las consultas son más similares, permitimos que w(q, q ) = max(KL(θq||·) − KL(θq||θq )). El Información Mutua Puntual (PMI, [17]) es una medida de asociación utilizada en teoría de la información para determinar el grado de independencia entre variables. La dependencia entre dos consultas se refleja en la puntuación SI(q, q), donde las puntuaciones mayores que cero indican que es probable que exista una dependencia, lo que interpretamos como que las consultas son probablemente similares: SI(q, q) = log p(q, q) / p(q)p(q). Estimamos la probabilidad de un tema p(q) utilizando el número de documentos relevantes para la consulta q dentro de la colección. La probabilidad conjunta p(q, q) se estima de manera similar, utilizando la concatenación de q y q como una consulta. Para obtener p(q|q), luego establecemos w(q, q) = SI(q, q) cuando SI(q, q) > 0, de lo contrario w(q, q) = 0, ya que solo estamos interesados en incluir consultas que sean similares. La estadística de log-verosimilitud proporciona otra medida de dependencia, que es más confiable que la medida de información mutua puntual [17]. Sea k1 el número de co-ocurrencias de q y q, k2 el número de ocurrencias de q que no co-ocurren con q, n1 el número total de ocurrencias de q, y n2 el número total de tokens de tema menos el número de ocurrencias de q. Entonces, sea p1 = k1/n1, p2 = k2/n2 y p = (k1 + k2)/(n1 + n2), (q, q ) = 2( (p1, k1, n1) + (p2, k2, n2) − (p, k1, n1) − (p, k2, n2)), donde (p, n, k) = k log p + (n − k) log(1 − p). El puntaje más alto indica que las consultas también son probablemente similares, por lo tanto, establecemos w(q, q ) = (q, q ). Finalmente, también estimamos la similitud de dos temas basándonos en su distancia dentro de la jerarquía de temas. La jerarquía de temas se visualiza como un grafo dirigido, y para todos los pares de temas se calcula el camino más corto SP(q, q). Establecimos el puntaje de similitud como el recíproco del camino más corto: w(q, q ) = 1/SP(q, q ). 7.2 Información contextual Dada la jerarquía de una organización, las unidades a las que pertenece una persona se consideran como un contexto para compensar la escasez de datos. Lo modelamos de la siguiente manera: p (q|ca) = 1 − P ou∈OU(ca) λou · p(q|ca) + P ou∈OU(ca) λou · p(q|ou), donde OU(ca) es el conjunto de unidades organizativas de las cuales el candidato ca es miembro, y p(q|o) expresa la fuerza de la asociación entre la consulta q y la unidad ou. La última probabilidad se puede estimar utilizando cualquiera de los tres modelos básicos, simplemente reemplazando ca con ou en las ecuaciones correspondientes. Una unidad organizativa está asociada con todos los documentos que sus miembros han escrito. Es decir, p(d|ou) = maxca∈ou p(d|ca). 7.3 Un modelo multilingüe simple Para institutos de conocimiento en Europa, ya sea académicos u otros, un entorno multilingüe (o al menos bilingüe) es típico. El siguiente modelo se basa en un tipo de suposición de independencia: no hay transferencia de conocimientos/perfiles a través de las barreras del idioma. Si bien es una simplificación, esta es una aproximación sensata en primer lugar. Eso es: p (q|ca) =P l∈L λl · p(ql|ca), donde L es el conjunto de idiomas utilizados en la colección, ql es la traducción de la consulta q al idioma l, y λl es un parámetro de suavizado específico del idioma, tal que P l∈L λl = 1. 8. MODELOS AVANZADOS: EVALUACIÓN En esta sección presentamos una evaluación experimental de nuestros modelos avanzados. Búsqueda de expertos Perfilado de expertos Modelo de lenguaje 1 Modelo 2 Modelo 3 Modelo 1 Modelo 2 Modelo 3 %q MAP MRR %q MAP MRR %q MAP MRR %ca MAP MRR %ca MAP MRR %ca MAP MRR Solo inglés 97.8 0.237 0.372 98.6 0.280 0.441 98.5 0.166 0.293 100 0.199 0.387 88.7 0.281 0.525 90.9 0.169 0.329 Solo holandés 61.3 0.249 0.401 62.6 0.265 0.436 62.6 0.195 0.344 91.9 0.164 0.426 90.1 0.195 0.488 91.9 0.125 0.328 Combinación 99.4 0.297 0.444 99.7 0.324 0.491 99.7 0.223 0.388 100 0.241 0.445 92.1 0.313 0.564 93.2 0.224 0.411 Tabla 3: Rendimiento de la combinación de idiomas en las tareas de búsqueda y perfilado de expertos (en candidatos). Los mejores puntajes para cada modelo están en cursiva, los mejores puntajes absolutos para las tareas de búsqueda y perfilado de expertos están en negrita. Tabla 4: Rendimiento en las tareas de búsqueda de expertos (arriba) y perfilado (abajo), utilizando similitudes de áreas de conocimiento. Las carreras fueron evaluadas en los temas principales establecidos. Las mejores puntuaciones están en negrita. 8.1 Preguntas de investigación Nuestras preguntas siguen los refinamientos presentados en la sección anterior: ¿Explotar la similitud en el área de conocimiento mejora la efectividad? ¿Cuál de los diversos métodos para capturar las relaciones entre palabras es más efectivo? Además, ¿es útil nuestra forma de incorporar información contextual? ¿Para qué tareas? Y finalmente, ¿es nuestra forma simple de combinar las puntuaciones monolingües suficiente para obtener mejoras significativas? 8.2 Configuración experimental Dado que las autoevaluaciones también son escasas en nuestra colección, para poder medir las diferencias entre los diversos modelos, seleccionamos un subconjunto de temas y evaluamos (algunas de las) ejecuciones solo en este subconjunto. Este conjunto se denomina como temas principales, y consiste en temas que se encuentran en el nivel superior de la jerarquía temática. (Un tema principal tiene subtemas, pero no es un subtema de ningún otro tema). Este conjunto principal consiste en 132 temas en holandés y 119 en inglés. Los juicios de relevancia se limitaron al conjunto de temas principales, pero no se ampliaron con subtemas. 8.3 Explotando la similitud de áreas de conocimiento. La Tabla 4 presenta los resultados. Los cuatro métodos utilizados para estimar la similitud de áreas de conocimiento son la divergencia de KL (KLDIV) y PointLang. Modelo 1 Modelo 2 Modelo 3 MAP MRR MAP MRR MAP MRR Búsqueda de expertos UK TODOS 0.423 0.545 0.654 0.799 0.494 0.629 UK PRINCIPAL 0.500 0.621 0.704 0.834 0.587 0.699 NL TODOS 0.439 0.560 0.672 0.826 0.480 0.630 NL PRINCIPAL 0.440 0.584 0.645 0.816 0.515 0.655 Perfilado de expertos UK TODOS 0.240 0.640 0.306 0.778 0.223 0.616 UK PRINCIPAL 0.523 0.677 0.519 0.648 0.461 0.587 NL TODOS 0.203 0.716 0.254 0.770 0.183 0.627 NL PRINCIPAL 0.332 0.576 0.380 0.624 0.332 0.549 Tabla 5: Evaluación de los modelos de contexto en unidades organizativas. información mutua ponderada (PMI), log-verosimilitud (LL) y distancia dentro de la jerarquía de temas (HDIST). Logramos mejorar el resultado base en todos los casos, pero la mejora es más notable para la tarea de perfilado. Para ambas tareas, el método LL tuvo el mejor rendimiento. Los enfoques basados en el contenido tuvieron un rendimiento consistentemente mejor que HDIST. 8.4 Información contextual Una jerarquía de dos niveles de unidades organizativas (facultades e institutos) está disponible en la colección de Expertos de UvT. La unidad a la que pertenece una persona se utiliza como contexto para esa persona. Primero, evaluamos los modelos de las unidades organizativas, utilizando todos los temas (TODOS) y solo los temas principales (PRINCIPALES). Una unidad organizativa se considera relevante para un tema dado (o viceversa) si al menos un miembro de la unidad seleccionó el tema dado como área de especialización. La Tabla 5 informa sobre los resultados. En lo que respecta a la búsqueda de expertos, dado un tema, la unidad organizativa correspondiente puede ser identificada con alta precisión. Sin embargo, la tarea de perfilado de expertos muestra una imagen diferente: las puntuaciones son bajas y la tarea parece difícil. La explicación puede ser que los conceptos generales (es decir, nuestros temas principales) pueden pertenecer a varias unidades organizativas. Segundo, realizamos otra evaluación, donde combinamos los modelos contextuales con los modelos candidatos (para puntuar a los candidatos nuevamente). La tabla 6 informa sobre los resultados. Encontramos un impacto positivo de los modelos de contexto solo para la búsqueda de expertos. Notablemente, para la búsqueda de expertos (y Modelo 1), mejora más del 50% (para inglés) y más del 70% (para holandés) en el MAP. El bajo rendimiento en la creación de perfiles de expertos puede deberse al hecho de que los modelos de contexto por sí solos no tuvieron un buen desempeño en la tarea de creación de perfiles desde el principio. 8.5 Modelos multilingües En esta subsección evaluamos el método para combinar resultados en múltiples idiomas que describimos en la Sección 7.3. En nuestro entorno, el conjunto de idiomas consiste en inglés y holandés: L = {UK, NL}. Los pesos de estos idiomas se establecieron para ser idénticos (λUK = λNL = 0.5). Realizamos experimentos con diferentes configuraciones de λ, pero no observamos diferencias significativas en el rendimiento. La Tabla 3 informa sobre los resultados multilingües, donde el rendimiento se evalúa en el conjunto completo de temas. Los tres modelos tienen una diferencia significativa. Tabla 6: Rendimiento de los modelos de contexto (CT) en comparación con la línea base (BL). Las mejores puntuaciones están en negrita, demostradas en todas las medidas para ambas tareas. La cobertura de temas y candidatos para las tareas de búsqueda y perfilado de expertos, respectivamente, es cercana al 100% en todos los casos. La mejora relativa de las puntuaciones de precisión varía del 10% al 80%. Estas puntuaciones demuestran que, a pesar de su simplicidad, nuestro método para combinar resultados en varios idiomas logra mejoras sustanciales sobre el punto de referencia. CONCLUSIONES En este artículo nos enfocamos en la recuperación de expertos (búsqueda y perfilado de expertos) en un nuevo entorno de una organización típica intensiva en conocimiento en la que los datos disponibles son de alta calidad, multilingües y abarcan una amplia gama de áreas de expertise. Normalmente, la cantidad de datos disponibles en una organización como una universidad, un instituto de investigación o un laboratorio de investigación es limitada en comparación con la colección de la W3C que ha sido utilizada principalmente para la evaluación experimental de la recuperación de la experiencia hasta ahora. Para examinar la recuperación de la experiencia en este entorno, presentamos (y lanzamos) la colección de Expertos de UvT como un caso representativo de organizaciones intensivas en conocimiento. La nueva colección refleja las propiedades típicas de los institutos intensivos en conocimiento mencionados anteriormente e incluye también varias características que pueden ser potencialmente útiles para la recuperación de expertos, como la estructura temática y organizativa. Evaluamos cómo los modelos de vanguardia para la búsqueda y perfilado de expertos se desempeñaron en este nuevo entorno y luego refinamos estos modelos para intentar explotar las diferentes características dentro del entorno de datos (idioma, actualidad y estructura organizativa). Encontramos que los modelos actuales de recuperación de experiencia se generalizan bien a este nuevo entorno; además, descubrimos que refinar los modelos para tener en cuenta las diferencias resulta en mejoras significativas, compensando así los problemas causados por la escasez de datos. El trabajo futuro incluye establecer evaluaciones manuales de perfiles generados automáticamente por los propios empleados, especialmente en casos en los que los empleados no han proporcionado un perfil ellos mismos. 10. AGRADECIMIENTOS Krisztian Balog fue apoyado por la Organización Neerlandesa para la Investigación Científica (NWO) bajo el número de proyecto 220-80-001. Maarten de Rijke también recibió apoyo de NWO bajo los números de proyecto 017.001.190, 220-80-001, 264-70-050, 354-20-005, 600.065.120, 612-13-001, 612.000.106, 612.066.302, 612.069.006, 640.001.501, 640.002.501, y de la Unión Europea. Programa IST del 6º Programa Marco para la I+D en virtud del contrato del proyecto MultiMATCH IST-033104. El trabajo de Toine Bogers y Antal van den Bosch fue financiado por el programa IOP-MMI de SenterNovem / Ministerio de Asuntos Económicos de los Países Bajos, como parte del proyecto 'A Propos'. 11. REFERENCIAS [1] L. Azzopardi. Incorporando contexto en el marco de modelado del lenguaje para la recuperación de información ad-hoc. Tesis doctoral, Universidad de Paisley, 2005. [2] K. Balog y M. de Rijke. Encontrando expertos similares. En este volumen, 2007. [3] K. Balog y M. de Rijke. Determinación de perfiles de expertos (con una aplicación a la búsqueda de expertos). En IJCAI 07: Proc. 20th Intern. Conferencia Conjunta sobre Inteligencia Artificial, páginas 2657-2662, 2007. [4] K. Balog, L. Azzopardi y M. de Rijke. Modelos formales para la búsqueda de expertos en corporaciones empresariales. En SIGIR 06: Actas de la 29ª conferencia internacional anual. Conferencia ACM SIGIR sobre Investigación y desarrollo en recuperación de información, páginas 43-50, 2006. [5] I. Becerra-Fernández. El papel de las tecnologías de inteligencia artificial en la implementación de sistemas de gestión del conocimiento de localización de personas. En el taller de AAAI sobre la incorporación de conocimiento en los procesos de negocio, marzo de 2000. [6] C. S. Campbell, P. P. Maglio, A. Cozzi y B. Dom. Identificación de la experiencia utilizando comunicaciones por correo electrónico. En CIKM 03: Actas de la duodécima conferencia internacional sobre gestión de la información y el conocimiento, páginas 528-531, 2003. [7] G. Cao, J.-Y. Nie, y J. Bai. Integrando relaciones entre palabras en modelos de lenguaje. En SIGIR 05: Actas de la 28ª conferencia internacional anual. Conferencia ACM SIGIR sobre Investigación y desarrollo en recuperación de información, páginas 298-305, 2005. [8] T. M. Cover y J. A. Thomas. Elementos de la teoría de la información. Wiley-Interscience, 1991. [9] N. Craswell, D. Hawking, A. M. Vercoustre y P. Wilkins. Experto en P@noptic: Buscando expertos no solo documentos. En Ausweb, 2001. [10] N. Craswell, A. de Vries e I. Soboroff. Resumen de la pista empresarial TREC2005. En la Decimocuarta Conferencia de Recuperación de Información. Proc. (TREC 2005), 2006. [11] T. H. Davenport y L. Prusak. Conocimiento en Acción: Cómo las Organizaciones Gestionan lo que Saben. Harvard Business School Press, Boston, MA, 1998. [12] T. Dunning.\nHarvard Business School Press, Boston, MA, 1998. [12] T. Dunning. Métodos precisos para la estadística de sorpresa y coincidencia. Lingüística Computacional, 19(1):61-74, 1993. [13] E. Filatova y J. Prager. Dime qué haces y te diré quién eres: Aprendiendo actividades relacionadas con la ocupación para biografías. En HLT/EMNLP, 2005. [14] V. Lavrenko y W. B. Croft. Modelos de lenguaje basados en relevancia. En SIGIR 01: Actas de la 24ª conferencia internacional anual. Conferencia ACM SIGIR sobre Investigación y Desarrollo en Recuperación de Información, páginas 120-127, 2001. [15] V. Lavrenko, M. Choquette y W. B. Croft. Modelos de relevancia multilingües. En SIGIR 02: Actas de la 25ª conferencia internacional anual. Conferencia ACM SIGIR sobre Investigación y Desarrollo en Recuperación de Información, páginas 175-182, 2002. [16] C. Macdonald e I. Ounis. Votación de candidatos: adaptando técnicas de fusión de datos para una tarea de búsqueda de expertos. En CIKM 06: Actas de la 15ª conferencia internacional de ACM sobre gestión de la información y el conocimiento, páginas 387-396, 2006. [17] C. Manning y H. Schütze. Fundamentos del Procesamiento del Lenguaje Natural Estadístico. El MIT Press, 1999. [18] A. Mockus y J. D. Herbsleb. Explorador de experticia: un enfoque cuantitativo para identificar la experticia. En ICSE 02: Proc. 24th Intern. Conf. en Ingeniería de Software, páginas 503-512, 2002. [19] D. Petkova y W. B. Croft. Modelos de lenguaje jerárquicos para la búsqueda de expertos en corporaciones empresariales. En Proc. ICTAI 2006, páginas 599-608, 2006. [20] I. Soboroff, A. de Vries y N. Craswell. Resumen de la pista empresarial TREC 2006. En las Notas de Trabajo de TREC 2006, 2006. [21] T. Tao, X. Wang, Q. Mei y C. Zhai. Recuperación de información del modelo de lenguaje con expansión de documentos. En HLT-NAACL 2006, 2006. [22] TREC. Ruta empresarial, 2005. URL: http://www.ins.cwi.nl/projects/trec-ent/wiki/. [23] G. van Noord.\nURL: http://www.ins.cwi.nl/projects/trec-ent/wiki/. [23] G. van Noord. Adivinador de idioma TextCat. URL: http://www.let.rug.nl/˜vannoord/TextCat/. [24] W3C. La colección de pruebas del W3C, 2005. URL: http://research.microsoft.com/users/nickcr/w3c-summary.html. ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "baseline model": {
            "translated_key": "",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Broad Expertise Retrieval in Sparse Data Environments Krisztian Balog ISLA, University of Amsterdam Kruislaan 403, 1098 SJ Amsterdam, The Netherlands kbalog@science.uva.nl Toine Bogers ILK, Tilburg University P.O.",
                "Box 90153, 5000 LE Tilburg, The Netherlands A.M.Bogers@uvt.nl Leif Azzopardi Dept. of Computing Science University of Glasgow, Glasgow, G12 8QQ leif@dcs.gla.ac.uk Maarten de Rijke ISLA, University of Amsterdam Kruislaan 403, 1098 SJ Amsterdam, The Netherlands mdr@science.uva.nl Antal van den Bosch ILK, Tilburg University P.O.",
                "Box 90153, 5000 LE Tilburg, The Netherlands Antal.vdnBosch@uvt.nl ABSTRACT Expertise retrieval has been largely unexplored on data other than the W3C collection.",
                "At the same time, many intranets of universities and other knowledge-intensive organisations offer examples of relatively small but clean multilingual expertise data, covering broad ranges of expertise areas.",
                "We first present two main expertise retrieval tasks, along with a set of baseline approaches based on generative language modeling, aimed at finding expertise relations between topics and people.",
                "For our experimental evaluation, we introduce (and release) a new test set based on a crawl of a university site.",
                "Using this test set, we conduct two series of experiments.",
                "The first is aimed at determining the effectiveness of baseline expertise retrieval methods applied to the new test set.",
                "The second is aimed at assessing refined models that exploit characteristic features of the new test set, such as the organizational structure of the university, and the hierarchical structure of the topics in the test set.",
                "Expertise retrieval models are shown to be robust with respect to environments smaller than the W3C collection, and current techniques appear to be generalizable to other settings.",
                "Categories and Subject Descriptors H.3 [Information Storage and Retrieval]: H.3.1 Content Analysis and Indexing; H.3.3 Information Search and Retrieval; H.3.4 Systems and Software; H.4 [Information Systems Applications]: H.4.2 Types of Systems; H.4.m Miscellaneous General Terms Algorithms, Measurement, Performance, Experimentation 1.",
                "INTRODUCTION An organizations intranet provides a means for exchanging information between employees and for facilitating employee collaborations.",
                "To efficiently and effectively achieve this, it is necessary to provide search facilities that enable employees not only to access documents, but also to identify expert colleagues.",
                "At the TREC Enterprise Track [22] the need to study and understand expertise retrieval has been recognized through the introduction of Expert Finding tasks.",
                "The goal of expert finding is to identify a list of people who are knowledgeable about a given topic.",
                "This task is usually addressed by uncovering associations between people and topics [10]; commonly, a co-occurrence of the name of a person with topics in the same context is assumed to be evidence of expertise.",
                "An alternative task, which using the same idea of people-topic associations, is expert profiling, where the task is to return a list of topics that a person is knowledgeable about [3].",
                "The launch of the Expert Finding task at TREC has generated a lot of interest in expertise retrieval, with rapid progress being made in terms of modeling, algorithms, and evaluation aspects.",
                "However, nearly all of the expert finding or profiling work performed has been validated experimentally using the W3C collection [24] from the Enterprise Track.",
                "While this collection is currently the only publicly available test collection for expertise retrieval tasks, it only represents one type of intranet.",
                "With only one test collection it is not possible to generalize conclusions to other realistic settings.",
                "In this paper we focus on expertise retrieval in a realistic setting that differs from the W3C setting-one in which relatively small amounts of clean, multilingual data are available, that cover a broad range of expertise areas, as can be found on the intranets of universities and other knowledge-intensive organizations.",
                "Typically, this setting features several additional types of structure: topical structure (e.g., topic hierarchies as employed by the organization), organizational structure (faculty, department, ...), as well as multiple types of documents (research and course descriptions, publications, and academic homepages).",
                "This setting is quite different from the W3C setting in ways that might impact upon the performance of expertise retrieval tasks.",
                "We focus on a number of research questions in this paper: Does the relatively small amount of data available on an intranet affect the quality of the topic-person associations that lie at the heart of expertise retrieval algorithms?",
                "How do state-of-the-art algorithms developed on the W3C data set perform in the alternative scenario of the type described above?",
                "More generally, do the lessons from the Expert Finding task at TREC carry over to this setting?",
                "How does the inclusion or exclusion of different documents affect expertise retrieval tasks?",
                "In addition to, how can the topical and organizational structure be used for retrieval purposes?",
                "To answer our research questions, we first present a set of baseline approaches, based on generative language modeling, aimed at finding associations between topics and people.",
                "This allows us to formulate the expert finding and expert profiling tasks in a uniform way, and has the added benefit of allowing us to understand the relations between the two tasks.",
                "For our experimental evaluation, we introduce a new data set (the UvT Expert Collection) which is representative of the type of intranet that we described above.",
                "Our collection is based on publicly available data, crawled from the website of Tilburg University (UvT).",
                "This type of data is particularly interesting, since (1) it is clean, heterogeneous, structured, and focused, but comprises a limited number of documents; (2) contains information on the organizational hierarchy; (3) it is bilingual (English and Dutch); and (4) the list of expertise areas of an individual are provided by the employees themselves.",
                "Using the UvT Expert collection, we conduct two sets of experiments.",
                "The first is aimed at determining the effectiveness of baseline expertise finding and profiling methods in this new setting.",
                "A second group of experiments is aimed at extensions of the baseline methods that exploit characteristic features of the UvT Expert Collection; specifically, we propose and evaluate refined expert finding and profiling methods that incorporate topicality and organizational structure.",
                "Apart from the research questions and data set that we contribute, our main contributions are as follows.",
                "The <br>baseline model</br>s developed for expertise finding perform well on the new data set.",
                "While on the W3C setting the expert finding task appears to be more difficult than profiling, for the UvT data the opposite is the case.",
                "We find that profiling on the UvT data set is considerably more difficult than on the W3C set, which we believe is due to the large (but realistic) number of topical areas that we used for profiling: about 1,500 for the UvT set, versus 50 in the W3C case.",
                "Taking the similarity between topics into account can significantly improve retrieval performance.",
                "The best performing similarity measures are content-based, therefore they can be applied on the W3C (and other) settings as well.",
                "Finally, we demonstrate that the organizational structure can be exploited in the form of a context model, improving MAP scores for certain models by up to 70%.",
                "The remainder of this paper is organized as follows.",
                "In the next section we review related work.",
                "Then, in Section 3 we provide detailed descriptions of the expertise retrieval tasks that we address in this paper: expert finding and expert profiling.",
                "In Section 4 we present our <br>baseline model</br>s, of which the performance is then assessed in Section 6 using the UvT data set that we introduce in Section 5.",
                "Advanced models exploiting specific features of our data are presented in Section 7 and evaluated in Section 8.",
                "We formulate our conclusions in Section 9. 2.",
                "RELATED WORK Initial approaches to expertise finding often employed databases containing information on the skills and knowledge of each individual in the organization [11].",
                "Most of these tools (usually called yellow pages or people-finding systems) rely on people to self-assess their skills against a predefined set of keywords.",
                "For updating profiles in these systems in an automatic fashion there is a need for intelligent technologies [5].",
                "More recent approaches use specific document sets (such as email [6] or software [18]) to find expertise.",
                "In contrast with focusing on particular document types, there is also an increased interest in the development of systems that index and mine published intranet documents as sources of evidence for expertise.",
                "One such published approach is the P@noptic system [9], which builds a representation of each person by concatenating all documents associated with that person-this is similar to Model 1 of Balog et al. [4], who formalize and compare two methods.",
                "Balog et al.s Model 1 directly models the knowledge of an expert from associated documents, while their Model 2 first locates documents on the topic and then finds the associated experts.",
                "In the reported experiments the second method performs significantly better when there are sufficiently many associated documents per candidate.",
                "Most systems that took part in the 2005 and 2006 editions of the Expert Finding task at TREC implemented (variations on) one of these two models; see [10, 20].",
                "Macdonald and Ounis [16] propose a different approach for ranking candidate expertise with respect to a topic based on data fusion techniques, without using collectionspecific heuristics; they find that applying field-based weighting models improves the ranking of candidates.",
                "Petkova and Croft [19] propose yet another approach, based on a combination of the above Model 1 and 2, explicitly modeling topics.",
                "Turning to other expert retrieval tasks that can also be addressed using topic-people associations, Balog and de Rijke [3] addressed the task of determining topical expert profiles.",
                "While their methods proved to be efficient on the W3C corpus, they require an amount of data that may not be available in the typical knowledge-intensive organization.",
                "Balog and de Rijke [2] study the related task of finding experts that are similar to a small set of experts given as input.",
                "As an aside, creating a textual summary of a person shows some similarities to biography finding, which has received a considerable amount of attention recently; see e.g., [13].",
                "We use generative language modeling to find associations between topics and people.",
                "In our modeling of expert finding and profiling we collect evidence for expertise from multiple sources, in a heterogeneous collection, and integrate it with the co-occurrence of candidates names and query terms-the language modeling setting allows us to do this in a transparent manner.",
                "Our modeling proceeds in two steps.",
                "In the first step, we consider three <br>baseline model</br>s, two taken from [4] (the Models 1 and 2 mentioned above), and one a refined version of a model introduced in [3] (which we refer to as Model 3 below); this third model is also similar to the model described by Petkova and Croft [19].",
                "The models we consider in our second round of experiments are mixture models similar to contextual language models [1] and to the expanded documents of Tao et al. [21]; however, the features that we use for definining our expansions-including topical structure and organizational structure-have not been used in this way before. 3.",
                "TASKS In the expertise retrieval scenario that we envisage, users seeking expertise within an organization have access to an interface that combines a search box (where they can search for experts or topics) with navigational structures (of experts and of topics) that allows them to click their way to an expert page (providing the profile of a person) or a topic page (providing a list of experts on the topic).",
                "To feed the above interface, we face two expertise retrieval tasks, expert finding and expert profiling, that we first define and then formalize using generative language models.",
                "In order to model either task, the probability of the query topic being associated to a candidate expert plays a key role in the final estimates for searching and profiling.",
                "By using language models, both the candidates and the query are characterized by distributions of terms in the vocabulary (used in the documents made available by the organization whose expertise retrieval needs we are addressing). 3.1 Expert finding Expert finding involves the task of finding the right person with the appropriate skills and knowledge: Who are the experts on topic X?.",
                "E.g., an employee wants to ascertain who worked on a particular project to find out why particular decisions were made without having to trawl through documentation (if there is any).",
                "Or, they may be in need a trained specialist for consultancy on a specific problem.",
                "Within an organization there are usually many possible candidates who could be experts for given topic.",
                "We can state this problem as follows: What is the probability of a candidate ca being an expert given the query topic q?",
                "That is, we determine p(ca|q), and rank candidates ca according to this probability.",
                "The candidates with the highest probability given the query are deemed the most likely experts for that topic.",
                "The challenge is how to estimate this probability accurately.",
                "Since the query is likely to consist of only a few terms to describe the expertise required, we should be able to obtain a more accurate estimate by invoking Bayes Theorem, and estimating: p(ca|q) = p(q|ca)p(ca) p(q) , (1) where p(ca) is the probability of a candidate and p(q) is the probability of a query.",
                "Since p(q) is a constant, it can be ignored for ranking purposes.",
                "Thus, the probability of a candidate ca being an expert given the query q is proportional to the probability of a query given the candidate p(q|ca), weighted by the a priori belief p(ca) that candidate ca is an expert. p(ca|q) ∝ p(q|ca)p(ca) (2) In this paper our main focus is on estimating the probability of a query given the candidate p(q|ca), because this probability captures the extent to which the candidate knows about the query topic.",
                "Whereas the candidate priors are generally assumed to be uniformand thus will not influence the ranking-it has been demonstrated that a sensible choice of priors may improve the performance [20]. 3.2 Expert profiling While the task of expert searching was concerned with finding experts given a particular topic, the task of expert profiling seeks to answer a related question: What topics does a candidate know about?",
                "Essentially, this turns the questions of expert finding around.",
                "The profiling of an individual candidate involves the identification of areas of skills and knowledge that they have expertise about and an evaluation of the level of proficiency in each of these areas.",
                "This is the candidates topical profile.",
                "Generally, topical profiles within organizations consist of tabular structures which explicitly catalogue the skills and knowledge of each individual in the organization.",
                "However, such practice is limited by the resources available for defining, creating, maintaining, and updating these profiles over time.",
                "By focusing on automatic methods which draw upon the available evidence within the document repositories of an organization, our aim is to reduce the human effort associated with the maintenance of topical profiles1 .",
                "A topical profile of a candidate, then, is defined as a vector where each element i of the vector corresponds to the candidate cas expertise on a given topic ki, (i.e., s(ca, ki)).",
                "Each topic ki defines a particular knowledge area or skill that the organization uses to define the candidates topical profile.",
                "Thus, it is assumed that a list of topics, {k1, . . . , kn}, where n is the number of pre-defined topics, is given: profile(ca) = s(ca, k1), s(ca, k2), . . . , s(ca, kn) . (3) 1 Context and evidence are needed to help users of expertise finding systems to decide whom to contact when seeking expertise in a particular area.",
                "Examples of such context are: Who does she work with?",
                "What are her contact details?",
                "Is she well-connected, just in case she is not able to help us herself?",
                "What is her role in the organization?",
                "Who is her superior?",
                "Collaborators, and affiliations, etc. are all part of the candidates social profile, and can serve as a background against which the systems recommendations should be interpreted.",
                "In this paper we only address the problem of determining topical profiles, and leave social profiling to further work.",
                "We state the problem of quantifying the competence of a person on a certain knowledge area as follows: What is the probability of a knowledge area (ki) being part of the candidates (expertise) profile? where s(ca, ki) is defined by p(ki|ca).",
                "Our task, then, is to estimate p(ki|ca), which is equivalent to the problem of obtaining p(q|ca), where the topic ki is represented as a query topic q, i.e., a sequence of keywords representing the expertise required.",
                "Both the expert finding and profiling tasks rely on the accurate estimation of p(q|ca).",
                "The only difference derives from the prior probability that a person is an expert (p(ca)), which can be incorporated into the expert finding task.",
                "This prior does not apply to the profiling task since the candidate (individual) is fixed. 4.",
                "BASELINE MODELS In this section we describe our <br>baseline model</br>s for estimating p(q|ca), i.e., associations between topics and people.",
                "Both expert finding and expert profiling boil down to this estimation.",
                "We employ three models for calculating this probability. 4.1 From topics to candidates Using Candidate Models: Model 1 Model 1 [4] defines the probability of a query given a candidate (p(q|ca)) using standard language modeling techniques, based on a multinomial unigram language model.",
                "For each candidate ca, a candidate language model θca is inferred such that the probability of a term given θca is nonzero for all terms, i.e., p(t|θca) > 0.",
                "From the candidate model the query is generated with the following probability: p(q|θca) = Y t∈q p(t|θca)n(t,q) , where each term t in the query q is sampled identically and independently, and n(t, q) is the number of times t occurs in q.",
                "The candidate language model is inferred as follows: (1) an empirical model p(t|ca) is computed; (2) it is smoothed with background probabilities.",
                "Using the associations between a candidate and a document, the probability p(t|ca) can be approximated by: p(t|ca) = X d p(t|d)p(d|ca), where p(d|ca) is the probability that candidate ca generates a supporting document d, and p(t|d) is the probability of a term t occurring in the document d. We use the maximum-likelihood estimate of a term, that is, the normalised frequency of the term t in document d. The strength of the association between document d and candidate ca expressed by p(d|ca) reflects the degree to which the candidates expertise is described using this document.",
                "The estimation of this probability is presented later, in Section 4.2.",
                "The candidate model is then constructed as a linear interpolation of p(t|ca) and the background model p(t) to ensure there are no zero probabilities, which results in the final estimation: p(q|θca) = (4) Y t∈q ( (1 − λ) X d p(t|d)p(d|ca) ! + λp(t) )n(t,q) .",
                "Model 1 amasses all the term information from all the documents associated with the candidate, and uses this to represent that candidate.",
                "This model is used to predict how likely a candidate would produce a query q.",
                "This can can be intuitively interpreted as the probability of this candidate talking about the query topic, where we assume that this is indicative of their expertise.",
                "Using Document Models: Model 2 Model 2 [4] takes a different approach.",
                "Here, the process is broken into two parts.",
                "Given a candidate ca, (1) a document that is associated with a candidate is selected with probability p(d|ca), and (2) from this document a query q is generated with probability p(q|d).",
                "Then the sum over all documents is taken to obtain p(q|ca), such that: p(q|ca) = X d p(q|d)p(d|ca). (5) The probability of a query given a document is estimated by inferring a document language model θd for each document d in a similar manner as the candidate model was inferred: p(t|θd) = (1 − λ)p(t|d) + λp(t), (6) where p(t|d) is the probability of the term in the document.",
                "The probability of a query given the document model is: p(q|θd) = Y t∈q p(t|θd)n(t,q) .",
                "The final estimate of p(q|ca) is obtained by substituting p(q|d) for p(q|θd) into Eq. 5 (see [4] for full details).",
                "Conceptually, Model 2 differs from Model 1 because the candidate is not directly modeled.",
                "Instead, the document acts like a hidden variable in the process which separates the query from the candidate.",
                "This process is akin to how a user may search for candidates with a standard search engine: initially by finding the documents which are relevant, and then seeing who is associated with that document.",
                "By examining a number of documents the user can obtain an idea of which candidates are more likely to discuss the topic q.",
                "Using Topic Models: Model 3 We introduce a third model, Model 3.",
                "Instead of attempting to model the query generation process via candidate or document models, we represent the query as a topic language model and directly estimate the probability of the candidate p(ca|q).",
                "This approach is similar to the model presented in [3, 19].",
                "As with the previous models, a language model is inferred, but this time for the query.",
                "We adapt the work of Lavrenko and Croft [14] to estimate a topic model from the query.",
                "The procedure is as follows.",
                "Given a collection of documents and a query topic q, it is assumed that there exists an unknown topic model θk that assigns probabilities p(t|θk) to the term occurrences in the topic documents.",
                "Both the query and the documents are samples from θk (as opposed to the previous approaches, where a query is assumed to be sampled from a specific document or candidate model).",
                "The main task is to estimate p(t|θk), the probability of a term given the topic model.",
                "Since the query q is very sparse, and as there are no examples of documents on the topic, this distribution needs to be approximated.",
                "Lavrenko and Croft [14] suggest a reasonable way of obtaining such an approximation, by assuming that p(t|θk) can be approximated by the probability of term t given the query q.",
                "We can then estimate p(t|q) using the joint probability of observing the term t together with the query terms, q1, . . . , qm, and dividing by the joint probability of the query terms: p(t|θk) ≈ p(t|q) = p(t, q1, . . . , qm) p(q1, . . . , qm) = p(t, q1, . . . , qm) P t ∈T p(t , q1, . . . , qm) , where p(q1, . . . , qm) = P t ∈T p(t , q1, . . . , qm), and T is the entire vocabulary of terms.",
                "In order to estimate the joint probability p(t, q1, . . . , qm), we follow [14, 15] and assume t and q1, . . . , qm are mutually independent, once we pick a source distribution from the set of underlying source distributions U.",
                "If we choose U to be a set of document models. then to construct this set, the query q would be issued against the collection, and the top n returned are assumed to be relevant to the topic, and thus treated as samples from the topic model. (Note that candidate models could be used instead.)",
                "With the document models forming U, the joint probability of term and query becomes: p(t, q1, . . . , qm) = X d∈U p(d) ˘ p(t|θd) mY i=1 p(qi|θd) ¯ . (7) Here, p(d) denotes the prior distribution over the set U, which reflects the relevance of the document to the topic.",
                "We assume that p(d) is uniform across U.",
                "In order to rank candidates according to the topic model defined, we use the Kullback-Leibler divergence metric (KL, [8]) to measure the difference between the candidate models and the topic model: KL(θk||θca) = X t p(t|θk) log p(t|θk) p(t|θca) . (8) Candidates with a smaller divergence from the topic model are considered to be more likely experts on that topic.",
                "The candidate model θca is defined in Eq. 4.",
                "By using KL divergence instead of the probability of a candidate given the topic model p(ca|θk), we avoid normalization problems. 4.2 Document-candidate associations For our models we need to be able to estimate the probability p(d|ca), which expresses the extent to which a document d characterizes the candidate ca.",
                "In [4], two methods are presented for estimating this probability, based on the number of person names recognized in a document.",
                "However, in our (intranet) setting it is reasonable to assume that authors of documents can unambiguously be identified (e.g., as the author of an article, the teacher assigned to a course, the owner of a web page, etc.)",
                "Hence, we set p(d|ca) to be 1 if candidate ca is author of document d, otherwise the probability is 0.",
                "In Section 6 we describe how authorship can be determined on different types of documents within the collection. 5.",
                "THE UVT EXPERT COLLECTION The UvT Expert collection used in the experiments in this paper fits the scenario outlined in Section 3.",
                "The collection is based on the Webwijs (Webwise) system developed at Tilburg University (UvT) in the Netherlands.",
                "Webwijs (http://www.uvt.nl/ webwijs/) is a publicly accessible database of UvT employees who are involved in research or teaching; currently, Webwijs contains information about 1168 experts, each of whom has a page with contact information and, if made available by the expert, a research description and publications list.",
                "In addition, each expert can select expertise areas from a list of 1491 topics and is encouraged to suggest new topics that need to be approved by the Webwijs editor.",
                "Each topic has a separate page that shows all experts associated with that topic and, if available, a list of related topics.",
                "Webwijs is available in Dutch and English, and this bilinguality has been preserved in the collection.",
                "Every Dutch Webwijs page has an English translation.",
                "Not all Dutch topics have an English translation, but the reverse is true: the 981 English topics all have a Dutch equivalent.",
                "About 42% of the experts teach courses at Tilburg University; these courses were also crawled and included in the profile.",
                "In addition, about 27% of the experts link to their academic homepage from their Webwijs page.",
                "These home pages were crawled and added to the collection. (This means that if experts put the full-text versions of their publications on their academic homepage, these were also available for indexing.)",
                "We also obtained 1880 full-text versions of publications from the UvT institutional repository and Dutch English no. of experts 1168 1168 no. of experts with ≥ 1 topic 743 727 no. of topics 1491 981 no. of expert-topic pairs 4318 3251 avg. no. of topics/expert 5.8 5.9 max. no. of topics/expert (no. of experts) 60 (1) 35 (1) min. no. of topics/expert (no. of experts) 1 (74) 1 (106) avg. no. of experts/topic 2.9 3.3 max. no. of experts/topic (no. of topics) 30 (1) 30 (1) min. no. of experts/topic (no. of topics) 1 (615) 1 (346) no. of experts with HP 318 318 no. of experts with CD 318 318 avg. no. of CDs per teaching expert 3.5 3.5 no. of experts with RD 329 313 no. of experts with PUB 734 734 avg. no. of PUBs per expert 27.0 27.0 avg. no. of PUB citations per expert 25.2 25.2 avg. no. of full-text PUBs per expert 1.8 1.8 Table 2: Descriptive statistics of the Dutch and English versions of the UvT Expert collection. converted them to plain text.",
                "We ran the TextCat [23] language identifier to classify the language of the home pages and the fulltext publications.",
                "We restricted ourselves to pages where the classifier was confident about the language used on the page.",
                "This resulted in four document types: research descriptions (RD), course descriptions (CD), publications (PUB; full-text and citationonly versions), and academic homepages (HP).",
                "Everything was bundled into the UvT Expert collection which is available at http: //ilk.uvt.nl/uvt-expert-collection/.",
                "The UvT Expert collection was extracted from a different organizational setting than the W3C collection and differs from it in a number of ways.",
                "The UvT setting is one with relatively small amounts of multilingual data.",
                "Document-author associations are clear and the data is structured and clean.",
                "The collection covers a broad range of expertise areas, as one can typically find on intranets of universities and other knowledge-intensive institutes.",
                "Additionally, our university setting features several types of structure (topical and organizational), as well as multiple document types.",
                "Another important difference between the two data sets is that the expertise areas in the UvT Expert collection are self-selected instead of being based on group membership or assignments by others.",
                "Size is another dimension along which the W3C and UvT Expert collections differ: the latter is the smaller of the two.",
                "Also realistic are the large differences in the amount of information available for each expert.",
                "Utilizing Webwijs is voluntary; 425 Dutch experts did not select any topics at all.",
                "This leaves us with 743 Dutch and 727 English usable expert profiles.",
                "Table 2 provides descriptive statistics for the UvT Expert collection.",
                "Universities tend to have a hierarchical structure that goes from the faculty level, to departments, research groups, down to the individual researchers.",
                "In the UvT Expert collection we have information about the affiliations of researchers with faculties and institutes, providing us with a two-level organizational hierarchy.",
                "Tilburg University has 22 organizational units at the faculty level (including the university office and several research institutes) and 71 departments, which amounts to 3.2 departments per faculty.",
                "As to the topical hierarchy used by Webwijs, 131 of the 1491 topics are top nodes in the hierarchy.",
                "This hierarchy has an average topic chain length of 2.65 and a maximum length of 7 topics. 6.",
                "EVALUATION Below, we evaluate Section 4s models for expert finding and profiling onthe UvT Expert collection.",
                "We detail our research questions and experimental setup, and then present our results. 6.1 Research Questions We address the following research questions.",
                "Both expert finding and profiling rely on the estimations of p(q|ca).",
                "The question is how the models compare on the different tasks, and in the setting of the UvT Expert collection.",
                "In [4], Model 2 outperformed Model 1 on the W3C collection.",
                "How do they compare on our data set?",
                "And how does Model 3 compare to Model 1?",
                "What about performance differences between the two languages in our test collection? 6.2 Experimental Setup The output of our models was evaluated against the self-assigned topic labels, which were treated as relevance judgements.",
                "Results were evaluated separately for English and Dutch.",
                "For English we only used topics for which the Dutch translation was available; for Dutch all topics were considered.",
                "The results were averaged for the queries in the intersection of relevance judgements and results; missing queries do not contribute a value of 0 to the scores.",
                "We use standard information retrieval measures, such as Mean Average Precision (MAP) and Mean Reciprocal Rank (MRR).",
                "We also report the percentage of topics (%q) and candidates (%ca) covered, for the expert finding and profiling tasks, respectively. 6.3 Results Table 1 shows the performance of Model 1, 2, and 3 on the expert finding and profiling tasks.",
                "The rows of the table correspond to the various document types (RD, CD, PUB, and HP) and to their combinations.",
                "RD+CD+PUB+HP is equivalent to the full collection and will be referred as the BASELINE of our experiments.",
                "Looking at Table 1 we see that Model 2 performs the best across the board.",
                "However, when the data is clean and very focused (RD), Model 3 outperforms it in a number of cases.",
                "Model 1 has the best coverage of candidates (%ca) and topics (%q).",
                "The various document types differ in their characteristics and how they improve the finding and profiling tasks.",
                "Expert profiling benefits much from the clean data present in the RD and CD document types, while the publications contribute the most to the expert finding task.",
                "Adding the homepages does not prove to be particularly useful.",
                "When we compare the results across languages, we find that the coverage of English topics (%q) is higher than of the Dutch ones for expert finding.",
                "Apart from that, the scores fall in the same range for both languages.",
                "For the profiling task the coverage of the candidates (%ca) is very similar for both languages.",
                "However, the performance is substantially better for the English topics.",
                "While it is hard to compare scores across collections, we conclude with a brief comparison of the absolute scores in Table 1 to those reported in [3, 4] on the W3C test set (2005 edition).",
                "For expert finding the MAP scores for Model 2 reported here are about 50% higher than the corresponding figures in [4], while our MRR scores are slightly below those in [4].",
                "For expert profiling, the differences are far more dramatic: the MAP scores for Model 2 reported here are around 50% below the scores in [3], while the (best) MRR scores are about the same as those in [3].",
                "The cause for the latter differences seems to reside in the number of knowledge areas considered here-approx. 30 times more than in the W3C setting. 7.",
                "ADVANCED MODELS Now that we have developed and assessed basic language modeling techniques for expertise retrieval, we turn to refined models that exploit special features of our test collection. 7.1 Exploiting knowledge area similarity One way to improve the scoring of a query given a candidate is to consider what other requests the candidate would satisfy and use them as further evidence to support the original query, proportional Expert finding Expert profiling Document types Model 1 Model 2 Model 3 Model 1 Model 2 Model 3 %q MAP MRR %q MAP MRR %q MAP MRR %ca MAP MRR %ca MAP MRR %ca MAP MRR English RD 97.8 0.126 0.269 83.5 0.144 0.311 83.3 0.129 0.271 100 0.089 0.189 39.3 0.232 0.465 41.1 0.166 0.337 CD 97.8 0.118 0.227 91.7 0.123 0.248 91.7 0.118 0.226 32.8 0.188 0.381 32.4 0.195 0.385 32.7 0.203 0.370 PUB 97.8 0.200 0.330 98.0 0.216 0.372 98.0 0.145 0.257 78.9 0.167 0.364 74.5 0.212 0.442 78.9 0.135 0.299 HP 97.8 0.081 0.186 97.4 0.071 0.168 97.2 0.062 0.149 31.2 0.150 0.299 28.8 0.185 0.335 30.1 0.136 0.287 RD+CD 97.8 0.188 0.352 92.9 0.193 0.360 92.9 0.150 0.273 100 0.145 0.286 61.3 0.251 0.477 63.2 0.217 0.416 RD+CD+PUB 97.8 0.235 0.373 98.1 0.277 0.439 98.1 0.178 0.305 100 0.196 0.380 87.2 0.280 0.533 89.5 0.170 0.344 RD+CD+PUB+HP 97.8 0.237 0.372 98.6 0.280 0.441 98.5 0.166 0.293 100 0.199 0.387 88.7 0.281 0.525 90.9 0.169 0.329 Dutch RD 61.3 0.094 0.229 38.4 0.137 0.336 38.3 0.127 0.295 38.0 0.127 0.386 34.1 0.138 0.420 38.0 0.105 0.327 CD 61.3 0.107 0.212 49.7 0.128 0.256 49.7 0.136 0.261 32.5 0.151 0.389 31.8 0.158 0.396 32.5 0.170 0.380 PUB 61.3 0.193 0.319 59.5 0.218 0.368 59.4 0.173 0.291 78.8 0.126 0.364 76.0 0.150 0.424 78.8 0.103 0.294 HP 61.3 0.063 0.169 56.6 0.064 0.175 56.4 0.062 0.163 29.8 0.108 0.308 27.8 0.125 0.338 29.8 0.098 0.255 RD+CD 61.3 0.159 0.314 51.9 0.184 0.360 51.9 0.169 0.324 60.5 0.151 0.410 57.2 0.166 0.431 60.4 0.159 0.384 RD+CD+PUB 61.3 0.244 0.398 61.5 0.260 0.424 61.4 0.210 0.350 90.3 0.165 0.445 88.2 0.189 0.479 90.3 0.126 0.339 RD+CD+PUB+HP 61.3 0.249 0.401 62.6 0.265 0.436 62.6 0.195 0.344 91.9 0.164 0.426 90.1 0.195 0.488 91.9 0.125 0.328 Table 1: Performance of the models on the expert finding and profiling tasks, using different document types and their combinations. %q is the number of topics covered (applies to the expert finding task), %ca is the number of candidates covered (applies to the expert profiling task).",
                "The top and bottom blocks correspond to English and Dutch respectively.",
                "The best scores are in boldface. to how related the other requests are to the original query.",
                "This can be modeled by interpolating between the p(q|ca) and the further supporting evidence from all similar requests q , as follows: p (q|ca) = λp(q|ca) + (1 − λ) X q p(q|q )p(q |ca), (9) where p(q|q ) represents the similarity between the two topics q and q .",
                "To be able to work with similarity methods that are not necessarily probabilities, we set p(q|q ) = w(q,q ) γ , where γ is a normalizing constant, such that γ = P q w(q , q ).",
                "We consider four methods for calculating the similarity score between two topics.",
                "Three approaches are strictly content-based, and establish similarity by examining co-occurrence patterns of topics within the collection, while the last approach exploits the hierarchical structure of topical areas that may be present within an organization (see [7] for further examples of integrating word relationships into language models).",
                "The Kullback-Leibler (KL) divergence metric defined in Eq. 8 provides a measure of how different or similar two probability distributions are.",
                "A topic model is inferred for q and q using the method presented in Section 4.1 to describe the query across the entire vocabulary.",
                "Since a lower KL score means the queries are more similar, we let w(q, q ) = max(KL(θq||·) − KL(θq||θq )).",
                "Pointwise Mutual Information (PMI, [17]) is a measure of association used in information theory to determine the extent of independence between variables.",
                "The dependence between two queries is reflected by the SI(q, q ) score, where scores greater than zero indicate that it is likely that there is a dependence, which we take to mean that the queries are likely to be similar: SI(q, q ) = log p(q, q ) p(q)p(q ) (10) We estimate the probability of a topic p(q) using the number of documents relevant to query q within the collection.",
                "The joint probability p(q, q ) is estimated similarly, by using the concatenation of q and q as a query.",
                "To obtain p(q|q ), we then set w(q, q ) = SI(q, q ) when SI(q, q ) > 0 otherwise w(q, q ) = 0, because we are only interested in including queries that are similar.",
                "The log-likelihood statistic provides another measure of dependence, which is more reliable than the pointwise mutual information measure [17].",
                "Let k1 be the number of co-occurrences of q and q , k2 the number of occurrences of q not co-occurring with q , n1 the total number of occurrences of q , and n2 the total number of topic tokens minus the number of occurrences of q .",
                "Then, let p1 = k1/n1, p2 = k2/n2, and p = (k1 + k2)/(n1 + n2), (q, q ) = 2( (p1, k1, n1) + (p2, k2, n2) − (p, k1, n1) − (p, k2, n2)), where (p, n, k) = k log p + (n − k) log(1 − p).",
                "The higher score indicate that queries are also likely to be similar, thus we set w(q, q ) = (q, q ).",
                "Finally, we also estimate the similarity of two topics based on their distance within the topic hierarchy.",
                "The topic hierarchy is viewed as a directed graph, and for all topic-pairs the shortest path SP(q, q ) is calculated.",
                "We set the similarity score to be the reciprocal of the shortest path: w(q, q ) = 1/SP(q, q ). 7.2 Contextual information Given the hierarchy of an organization, the units to which a person belong are regarded as a context so as to compensate for data sparseness.",
                "We model it as follows: p (q|ca) = 1 − P ou∈OU(ca) λou · p(q|ca) + P ou∈OU(ca) λou · p(q|ou), where OU(ca) is the set of organizational units of which candidate ca is a member of, and p(q|o) expresses the strength of the association between query q and the unit ou.",
                "The latter probability can be estimated using either of the three basic models, by simply replacing ca with ou in the corresponding equations.",
                "An organizational unit is associated with all the documents that its members have authored.",
                "That is, p(d|ou) = maxca∈ou p(d|ca). 7.3 A simple multilingual model For knowledge institutes in Europe, academic or otherwise, a multilingual (or at least bilingual) setting is typical.",
                "The following model builds on a kind of independence assumption: there is no spill-over of expertise/profiles across language boundaries.",
                "While a simplification, this is a sensible first approach.",
                "That is: p (q|ca) =P l∈L λl · p(ql|ca), where L is the set of languages used in the collection, ql is the translation of the query q to language l, and λl is a language specific smoothing parameter, such that P l∈L λl = 1. 8.",
                "ADVANCED MODELS: EVALUATION In this section we present an experimental evaluation of our advanced models.",
                "Expert finding Expert profiling Language Model 1 Model 2 Model 3 Model 1 Model 2 Model 3 %q MAP MRR %q MAP MRR %q MAP MRR %ca MAP MRR %ca MAP MRR %ca MAP MRR English only 97.8 0.237 0.372 98.6 0.280 0.441 98.5 0.166 0.293 100 0.199 0.387 88.7 0.281 0.525 90.9 0.169 0.329 Dutch only 61.3 0.249 0.401 62.6 0.265 0.436 62.6 0.195 0.344 91.9 0.164 0.426 90.1 0.195 0.488 91.9 0.125 0.328 Combination 99.4 0.297 0.444 99.7 0.324 0.491 99.7 0.223 0.388 100 0.241 0.445 92.1 0.313 0.564 93.2 0.224 0.411 Table 3: Performance of the combination of languages on the expert finding and profiling tasks (on candidates).",
                "Best scores for each model are in italic, absolute best scores for the expert finding and profiling tasks are in boldface.",
                "Method Model 1 Model 2 Model 3 MAP MRR MAP MRR MAP MRR English BASELINE 0.296 0.454 0.339 0.509 0.221 0.333 KLDIV 0.291 0.453 0.327 0.503 0.219 0.330 PMI 0.291 0.453 0.337 0.509 0.219 0.331 LL 0.319 0.490 0.360 0.524 0.233 0.368 HDIST 0.299 0.465 0.346 0.537 0.219 0.332 Dutch BASELINE 0.240 0.350 0.271 0.403 0.227 0.389 KLDIV 0.239 0.347 0.253 0.386 0.224 0.385 PMI 0.239 0.350 0.260 0.392 0.227 0.389 LL 0.255 0.372 0.281 0.425 0.231 0.389 HDIST 0.253 0.365 0.271 0.407 0.236 0.402 Method Model 1 Model 2 Model 3 MAP MRR MAP MRR MAP MRR English BASELINE 0.485 0.546 0.499 0.548 0.381 0.416 KLDIV 0.510 0.564 0.513 0.558 0.381 0.416 PMI 0.486 0.546 0.495 0.542 0.407 0.451 LL 0.558 0.589 0.586 0.617 0.408 0.453 HDIST 0.507 0.567 0.512 0.563 0.386 0.420 Dutch BASELINE 0.263 0.313 0.294 0.358 0.262 0.315 KLDIV 0.284 0.336 0.271 0.321 0.261 0.314 PMI 0.265 0.317 0.265 0.316 0.273 0.330 LL 0.312 0.351 0.330 0.377 0.284 0.331 HDIST 0.280 0.327 0.288 0.341 0.266 0.321 Table 4: Performance on the expert finding (top) and profiling (bottom) tasks, using knowledge area similarities.",
                "Runs were evaluated on the main topics set.",
                "Best scores are in boldface. 8.1 Research Questions Our questions follow the refinements presented in the preceding section: Does exploiting the knowledge area similarity improve effectiveness?",
                "Which of the various methods for capturing word relationships is most effective?",
                "Furthermore, is our way of bringing in contextual information useful?",
                "For which tasks?",
                "And finally, is our simple way of combining the monolingual scores sufficient for obtaining significant improvements? 8.2 Experimental setup Given that the self-assessments are also sparse in our collection, in order to be able to measure differences between the various models, we selected a subset of topics, and evaluated (some of the) runs only on this subset.",
                "This set is referred as main topics, and consists of topics that are located at the top level of the topical hierarchy. (A main topic has subtopics, but is not a subtopic of any other topic.)",
                "This main set consists of 132 Dutch and 119 English topics.",
                "The relevance judgements were restricted to the main topic set, but were not expanded with subtopics. 8.3 Exploiting knowledge area similarity Table 4 presents the results.",
                "The four methods used for estimating knowledge-area similarity are KL divergence (KLDIV), PointLang.",
                "Topics Model 1 Model 2 Model 3 MAP MRR MAP MRR MAP MRR Expert finding UK ALL 0.423 0.545 0.654 0.799 0.494 0.629 UK MAIN 0.500 0.621 0.704 0.834 0.587 0.699 NL ALL 0.439 0.560 0.672 0.826 0.480 0.630 NL MAIN 0.440 0.584 0.645 0.816 0.515 0.655 Expert profiling UK ALL 0.240 0.640 0.306 0.778 0.223 0.616 UK MAIN 0.523 0.677 0.519 0.648 0.461 0.587 NL ALL 0.203 0.716 0.254 0.770 0.183 0.627 NL MAIN 0.332 0.576 0.380 0.624 0.332 0.549 Table 5: Evaluating the context models on organizational units. wise mutual information (PMI), log-likelihood (LL), and distance within topic hierarchy (HDIST).",
                "We managed to improve upon the baseline in all cases, but the improvement is more noticeable for the profiling task.",
                "For both tasks, the LL method performed best.",
                "The content-based approaches performed consistently better than HDIST. 8.4 Contextual information A two level hierarchy of organizational units (faculties and institutes) is available in the UvT Expert collection.",
                "The unit a person belongs to is used as a context for that person.",
                "First, we evaluated the models of the organizational units, using all topics (ALL) and only the main topics (MAIN).",
                "An organizational unit is considered to be relevant for a given topic (or vice versa) if at least one member of the unit selected the given topic as an expertise area.",
                "Table 5 reports on the results.",
                "As far as expert finding goes, given a topic, the corresponding organizational unit can be identified with high precision.",
                "However, the expert profiling task shows a different picture: the scores are low, and the task seems hard.",
                "The explanation may be that general concepts (i.e., our main topics) may belong to several organizational units.",
                "Second, we performed another evaluation, where we combined the contextual models with the candidate models (to score candidates again).",
                "Table 6 reports on the results.",
                "We find a positive impact of the context models only for expert finding.",
                "Noticably, for expert finding (and Model 1), it improves over 50% (for English) and over 70% (for Dutch) on MAP.",
                "The poor performance on expert profiling may be due to the fact that context models alone did not perform very well on the profiling task to begin with. 8.5 Multilingual models In this subsection we evaluate the method for combining results across multiple languages that we described in Section 7.3.",
                "In our setting the set of languages consists of English and Dutch: L = {UK, NL}.",
                "The weights on these languages were set to be identical (λUK = λNL = 0.5).",
                "We performed experiments with various λ settings, but did not observe significant differences in performance.",
                "Table 3 reports on the multilingual results, where performance is evaluated on the full topic set.",
                "All three models significantly imLang.",
                "Method Model 1 Model 2 Model 3 MAP MRR MAP MRR MAP MRR Expert finding UK BL 0.296 0.454 0.339 0.509 0.221 0.333 UK CT 0.330 0.491 0.342 0.500 0.228 0.342 NL BL 0.240 0.350 0.271 0.403 0.227 0.389 NL CT 0.251 0.382 0.267 0.410 0.246 0.404 Expert profiling UK BL 0.485 0.546 0.499 0.548 0.381 0.416 UK CT 0.562 0.620 0.508 0.558 0.440 0.486 NL BL 0.263 0.313 0.294 0.358 0.262 0.315 NL CT 0.330 0.384 0.317 0.387 0.294 0.345 Table 6: Performance of the context models (CT) compared to the baseline (BL).",
                "Best scores are in boldface. proved over all measures for both tasks.",
                "The coverage of topics and candidates for the expert finding and profiling tasks, respectively, is close to 100% in all cases.",
                "The relative improvement of the precision scores ranges from 10% to 80%.",
                "These scores demonstrate that despite its simplicity, our method for combining results over multiple languages achieves substantial improvements over the baseline. 9.",
                "CONCLUSIONS In this paper we focused on expertise retrieval (expert finding and profiling) in a new setting of a typical knowledge-intensive organization in which the available data is of high quality, multilingual, and covering a broad range of expertise area.",
                "Typically, the amount of available data in such an organization (e.g., a university, a research institute, or a research lab) is limited when compared to the W3C collection that has mostly been used for the experimental evaluation of expertise retrieval so far.",
                "To examine expertise retrieval in this setting, we introduced (and released) the UvT Expert collection as a representative case of such knowledge intensive organizations.",
                "The new collection reflects the typical properties of knowledge-intensive institutes noted above and also includes several features which may are potentially useful for expertise retrieval, such as topical and organizational structure.",
                "We evaluated how current state-of-the-art models for expert finding and profiling performed in this new setting and then refined these models in order to try and exploit the different characteristics within the data environment (language, topicality, and organizational structure).",
                "We found that current models of expertise retrieval generalize well to this new environment; in addition we found that refining the models to account for the differences results in significant improvements, thus making up for problems caused by data sparseness issues.",
                "Future work includes setting up manual assessments of automatically generated profiles by the employees themselves, especially in cases where the employees have not provided a profile themselves. 10.",
                "ACKNOWLEDGMENTS Krisztian Balog was supported by the Netherlands Organisation for Scientific Research (NWO) under project number 220-80-001.",
                "Maarten de Rijke was also supported by NWO under project numbers 017.001.190, 220-80-001, 264-70-050, 354-20-005, 600.065.120, 612-13-001, 612.000.106, 612.066.302, 612.069.006, 640.001.501, 640.002.501, and by the E.U.",
                "IST programme of the 6th FP for RTD under project MultiMATCH contract IST-033104.",
                "The work of Toine Bogers and Antal van den Bosch was funded by the IOP-MMI-program of SenterNovem / The Dutch Ministry of Economic Affairs, as part of the `A Propos project. 11.",
                "REFERENCES [1] L. Azzopardi.",
                "Incorporating Context in the Language Modeling Framework for ad-hoc Information Retrieval.",
                "PhD thesis, University of Paisley, 2005. [2] K. Balog and M. de Rijke.",
                "Finding similar experts.",
                "In This volume, 2007. [3] K. Balog and M. de Rijke.",
                "Determining expert profiles (with an application to expert finding).",
                "In IJCAI 07: Proc. 20th Intern.",
                "Joint Conf. on Artificial Intelligence, pages 2657-2662, 2007. [4] K. Balog, L. Azzopardi, and M. de Rijke.",
                "Formal models for expert finding in enterprise corpora.",
                "In SIGIR 06: Proc. 29th annual intern.",
                "ACM SIGIR conf. on Research and development in information retrieval, pages 43-50, 2006. [5] I. Becerra-Fernandez.",
                "The role of artificial intelligence technologies in the implementation of people-finder knowledge management systems.",
                "In AAAI Workshop on Bringing Knowledge to Business Processes, March 2000. [6] C. S. Campbell, P. P. Maglio, A. Cozzi, and B. Dom.",
                "Expertise identification using email communications.",
                "In CIKM 03: Proc. twelfth intern. conf. on Information and knowledge management, pages 528531, 2003. [7] G. Cao, J.-Y.",
                "Nie, and J. Bai.",
                "Integrating word relationships into language models.",
                "In SIGIR 05: Proc. 28th annual intern.",
                "ACM SIGIR conf. on Research and development in information retrieval, pages 298-305, 2005. [8] T. M. Cover and J.",
                "A. Thomas.",
                "Elements of Information Theory.",
                "Wiley-Interscience, 1991. [9] N. Craswell, D. Hawking, A. M. Vercoustre, and P. Wilkins.",
                "P@noptic expert: Searching for experts not just for documents.",
                "In Ausweb, 2001. [10] N. Craswell, A. de Vries, and I. Soboroff.",
                "Overview of the TREC2005 Enterprise Track.",
                "In The Fourteenth Text REtrieval Conf.",
                "Proc. (TREC 2005), 2006. [11] T. H. Davenport and L. Prusak.",
                "Working Knowledge: How Organizations Manage What They Know.",
                "Harvard Business School Press, Boston, MA, 1998. [12] T. Dunning.",
                "Accurate methods for the statistics of surprise and coincidence.",
                "Computational Linguistics, 19(1):61-74, 1993. [13] E. Filatova and J. Prager.",
                "Tell me what you do and Ill tell you what you are: Learning occupation-related activities for biographies.",
                "In HLT/EMNLP, 2005. [14] V. Lavrenko and W. B. Croft.",
                "Relevance based language models.",
                "In SIGIR 01: Proc. 24th annual intern.",
                "ACM SIGIR conf. on Research and development in information retrieval, pages 120-127, 2001. [15] V. Lavrenko, M. Choquette, and W. B. Croft.",
                "Cross-lingual relevance models.",
                "In SIGIR 02: Proc. 25th annual intern.",
                "ACM SIGIR conf. on Research and development in information retrieval, pages 175-182, 2002. [16] C. Macdonald and I. Ounis.",
                "Voting for candidates: adapting data fusion techniques for an expert search task.",
                "In CIKM 06: Proc. 15th ACM intern. conf. on Information and knowledge management, pages 387-396, 2006. [17] C. Manning and H. Sch¨utze.",
                "Foundations of Statistical Natural Language Processing.",
                "The MIT Press, 1999. [18] A. Mockus and J. D. Herbsleb.",
                "Expertise browser: a quantitative approach to identifying expertise.",
                "In ICSE 02: Proc. 24th Intern.",
                "Conf. on Software Engineering, pages 503-512, 2002. [19] D. Petkova and W. B. Croft.",
                "Hierarchical language models for expert finding in enterprise corpora.",
                "In Proc.",
                "ICTAI 2006, pages 599-608, 2006. [20] I. Soboroff, A. de Vries, and N. Craswell.",
                "Overview of the TREC 2006 Enterprise Track.",
                "In TREC 2006 Working Notes, 2006. [21] T. Tao, X. Wang, Q. Mei, and C. Zhai.",
                "Language model information retrieval with document expansion.",
                "In HLT-NAACL 2006, 2006. [22] TREC.",
                "Enterprise track, 2005.",
                "URL: http://www.ins.cwi. nl/projects/trec-ent/wiki/. [23] G. van Noord.",
                "TextCat Language Guesser.",
                "URL: http://www. let.rug.nl/˜vannoord/TextCat/. [24] W3C.",
                "The W3C test collection, 2005.",
                "URL: http://research. microsoft.com/users/nickcr/w3c-summary.html."
            ],
            "original_annotated_samples": [
                "The <br>baseline model</br>s developed for expertise finding perform well on the new data set.",
                "In Section 4 we present our <br>baseline model</br>s, of which the performance is then assessed in Section 6 using the UvT data set that we introduce in Section 5.",
                "In the first step, we consider three <br>baseline model</br>s, two taken from [4] (the Models 1 and 2 mentioned above), and one a refined version of a model introduced in [3] (which we refer to as Model 3 below); this third model is also similar to the model described by Petkova and Croft [19].",
                "BASELINE MODELS In this section we describe our <br>baseline model</br>s for estimating p(q|ca), i.e., associations between topics and people."
            ],
            "translated_annotated_samples": [
                "Los <br>modelos base</br> desarrollados para encontrar expertos funcionan bien en el nuevo conjunto de datos.",
                "En la Sección 4 presentamos nuestros <br>modelos base</br>, cuyo rendimiento luego es evaluado en la Sección 6 utilizando el conjunto de datos UvT que presentamos en la Sección 5.",
                "En el primer paso, consideramos tres <br>modelos base</br>, dos tomados de [4] (los Modelos 1 y 2 mencionados anteriormente), y uno una versión refinada de un modelo introducido en [3] (al que nos referimos como Modelo 3 a continuación); este tercer modelo también es similar al modelo descrito por Petkova y Croft [19].",
                "MODELOS DE REFERENCIA En esta sección describimos nuestros <br>modelos de referencia</br> para estimar p(q|ca), es decir, las asociaciones entre temas y personas."
            ],
            "translated_text": "Recuperación de amplia experiencia en entornos de datos dispersos Krisztian Balog ISLA, Universidad de Ámsterdam Kruislaan 403, 1098 SJ Ámsterdam, Países Bajos kbalog@science.uva.nl Toine Bogers ILK, Universidad de Tilburg P.O. Caja 90153, 5000 LE Tilburg, Países Bajos A.M.Bogers@uvt.nl Leif Azzopardi Dept. de Ciencias de la Computación Universidad de Glasgow, Glasgow, G12 8QQ leif@dcs.gla.ac.uk Maarten de Rijke ISLA, Universidad de Ámsterdam Kruislaan 403, 1098 SJ Ámsterdam, Países Bajos mdr@science.uva.nl Antal van den Bosch ILK, Universidad de Tilburg P.O. La recuperación de la experiencia ha sido ampliamente inexplorada en datos distintos a la colección de la W3C. Al mismo tiempo, muchas intranets de universidades y otras organizaciones intensivas en conocimiento ofrecen ejemplos de datos de expertos multilingües relativamente pequeños pero precisos, que abarcan amplias áreas de especialización. Primero presentamos dos tareas principales de recuperación de expertos, junto con un conjunto de enfoques de referencia basados en modelado del lenguaje generativo, con el objetivo de encontrar relaciones de experticia entre temas y personas. Para nuestra evaluación experimental, introducimos (y liberamos) un nuevo conjunto de pruebas basado en un rastreo de un sitio universitario. Usando este conjunto de pruebas, llevamos a cabo dos series de experimentos. El primero tiene como objetivo determinar la efectividad de los métodos de recuperación de conocimientos básicos aplicados al nuevo conjunto de pruebas. El segundo tiene como objetivo evaluar modelos refinados que exploten las características distintivas del nuevo conjunto de pruebas, como la estructura organizativa de la universidad y la estructura jerárquica de los temas en el conjunto de pruebas. Los modelos de recuperación de experiencia se muestran robustos con respecto a entornos más pequeños que la colección de la W3C, y las técnicas actuales parecen ser generalizables a otros contextos. Categorías y Descriptores de Asignaturas H.3 [Almacenamiento y Recuperación de Información]: H.3.1 Análisis de Contenido e Indexación; H.3.3 Búsqueda y Recuperación de Información; H.3.4 Sistemas y Software; H.4 [Aplicaciones de Sistemas de Información]: H.4.2 Tipos de Sistemas; H.4.m Términos Generales Misceláneos Algoritmos, Medición, Rendimiento, Experimentación 1. INTRODUCCIÓN La intranet de una organización proporciona un medio para intercambiar información entre empleados y facilitar colaboraciones entre ellos. Para lograr esto de manera eficiente y efectiva, es necesario proporcionar instalaciones de búsqueda que permitan a los empleados no solo acceder a documentos, sino también identificar a colegas expertos. En la pista de Empresas de TREC [22] se ha reconocido la necesidad de estudiar y comprender la recuperación de expertos a través de la introducción de tareas de Búsqueda de Expertos. El objetivo de la búsqueda de expertos es identificar una lista de personas que tienen conocimientos sobre un tema específico. Esta tarea suele abordarse descubriendo asociaciones entre personas y temas [10]; comúnmente, se asume que la co-ocurrencia del nombre de una persona con temas en el mismo contexto es evidencia de experiencia. Una tarea alternativa, que utiliza la misma idea de asociaciones entre personas y temas, es el perfilado de expertos, donde la tarea consiste en devolver una lista de temas sobre los que una persona tiene conocimiento [3]. El lanzamiento de la tarea de Búsqueda de Expertos en TREC ha generado mucho interés en la recuperación de expertos, con un rápido progreso en términos de modelado, algoritmos y aspectos de evaluación. Sin embargo, casi todo el trabajo de búsqueda o perfilado de expertos realizado ha sido validado experimentalmente utilizando la colección de W3C [24] de la pista empresarial. Si bien esta colección es actualmente la única colección de pruebas disponible públicamente para tareas de recuperación de expertos, solo representa un tipo de intranet. Con solo una colección de pruebas no es posible generalizar conclusiones a otros escenarios realistas. En este artículo nos enfocamos en la recuperación de expertos en un entorno realista que difiere del entorno de la W3C, uno en el que hay cantidades relativamente pequeñas de datos limpios y multilingües disponibles, que abarcan una amplia gama de áreas de expertise, como se puede encontrar en las intranets de universidades y otras organizaciones intensivas en conocimiento. Normalmente, esta configuración incluye varios tipos adicionales de estructuras: estructura temática (por ejemplo, jerarquías de temas utilizadas por la organización), estructura organizativa (facultad, departamento, ...), así como múltiples tipos de documentos (descripciones de investigación y cursos, publicaciones y páginas web académicas). Este entorno es bastante diferente del entorno del W3C de maneras que podrían afectar el rendimiento de las tareas de recuperación de conocimientos especializados. Nos enfocamos en varias preguntas de investigación en este artículo: ¿La cantidad relativamente pequeña de datos disponibles en una intranet afecta la calidad de las asociaciones tema-persona que son fundamentales en los algoritmos de recuperación de expertos? ¿Cómo se desempeñan los algoritmos de vanguardia desarrollados en el conjunto de datos del W3C en el escenario alternativo del tipo descrito anteriormente? Más en general, ¿se pueden aplicar las lecciones de la tarea de Búsqueda de Expertos en TREC a este contexto? ¿Cómo afecta la inclusión o exclusión de diferentes documentos a las tareas de recuperación de expertos? Además, ¿cómo se puede utilizar la estructura temática y organizativa con fines de recuperación? Para responder a nuestras preguntas de investigación, primero presentamos un conjunto de enfoques de línea base, basados en modelado de lenguaje generativo, con el objetivo de encontrar asociaciones entre temas y personas. Esto nos permite formular las tareas de búsqueda de expertos y perfilado de expertos de manera uniforme, y tiene el beneficio adicional de permitirnos entender las relaciones entre las dos tareas. Para nuestra evaluación experimental, introducimos un nuevo conjunto de datos (la Colección de Expertos de UvT) que es representativo del tipo de intranet que describimos anteriormente. Nuestra colección se basa en datos de acceso público, extraídos del sitio web de la Universidad de Tilburg (UvT). Este tipo de datos es particularmente interesante, ya que (1) es limpio, heterogéneo, estructurado y enfocado, pero comprende un número limitado de documentos; (2) contiene información sobre la jerarquía organizativa; (3) es bilingüe (inglés y holandés); y (4) la lista de áreas de especialización de un individuo es proporcionada por los propios empleados. Usando la colección de expertos de UvT, llevamos a cabo dos conjuntos de experimentos. El primero tiene como objetivo determinar la efectividad de los métodos de búsqueda y perfilado de experiencia inicial en este nuevo entorno. Un segundo grupo de experimentos está dirigido a extensiones de los métodos base que aprovechan las características distintivas de la Colección de Expertos de UvT; específicamente, proponemos y evaluamos métodos refinados de búsqueda y perfilado de expertos que incorporan la topicalidad y la estructura organizativa. Además de las preguntas de investigación y el conjunto de datos que aportamos, nuestras principales contribuciones son las siguientes. Los <br>modelos base</br> desarrollados para encontrar expertos funcionan bien en el nuevo conjunto de datos. Mientras que en el entorno del W3C la tarea de encontrar expertos parece ser más difícil que el perfilado, para los datos de UvT ocurre lo contrario. Encontramos que el perfilado en el conjunto de datos UvT es considerablemente más difícil que en el conjunto de datos W3C, lo cual creemos que se debe al gran (pero realista) número de áreas temáticas que utilizamos para el perfilado: alrededor de 1,500 para el conjunto de datos UvT, frente a 50 en el caso de W3C. Teniendo en cuenta la similitud entre los temas puede mejorar significativamente el rendimiento de recuperación. Las medidas de similitud de mejor rendimiento son basadas en contenido, por lo tanto también se pueden aplicar en entornos de la W3C (y otros). Finalmente, demostramos que la estructura organizativa puede ser explotada en forma de un modelo de contexto, mejorando los puntajes de MAP para ciertos modelos hasta en un 70%. El resto de este documento está organizado de la siguiente manera. En la siguiente sección revisamos el trabajo relacionado. Luego, en la Sección 3 proporcionamos descripciones detalladas de las tareas de recuperación de expertos que abordamos en este artículo: la búsqueda de expertos y la creación de perfiles de expertos. En la Sección 4 presentamos nuestros <br>modelos base</br>, cuyo rendimiento luego es evaluado en la Sección 6 utilizando el conjunto de datos UvT que presentamos en la Sección 5. Los modelos avanzados que explotan características específicas de nuestros datos se presentan en la Sección 7 y se evalúan en la Sección 8. Formulamos nuestras conclusiones en la Sección 9.2. Trabajos relacionados Los enfoques iniciales para encontrar expertos a menudo empleaban bases de datos que contenían información sobre las habilidades y conocimientos de cada individuo en la organización [11]. La mayoría de estas herramientas (generalmente llamadas páginas amarillas o sistemas de búsqueda de personas) dependen de que las personas evalúen sus habilidades frente a un conjunto predefinido de palabras clave. Para actualizar perfiles en estos sistemas de forma automática, se necesita de tecnologías inteligentes [5]. Enfoques más recientes utilizan conjuntos de documentos específicos (como correos electrónicos [6] o software [18]) para encontrar experiencia. En contraste con centrarse en tipos de documentos particulares, también hay un interés creciente en el desarrollo de sistemas que indexan y extraen información de documentos publicados en intranets como fuentes de evidencia de experiencia. Un enfoque publicado es el sistema P@noptic [9], que construye una representación de cada persona concatenando todos los documentos asociados con esa persona, similar al Modelo 1 de Balog et al. [4], quienes formalizan y comparan dos métodos. El Modelo 1 de Balog et al. modela directamente el conocimiento de un experto a partir de documentos asociados, mientras que su Modelo 2 primero localiza documentos sobre el tema y luego encuentra a los expertos asociados. En los experimentos reportados, el segundo método tiene un rendimiento significativamente mejor cuando hay suficientes documentos asociados por candidato. La mayoría de los sistemas que participaron en las ediciones de 2005 y 2006 de la tarea de Búsqueda de Expertos en TREC implementaron (variaciones de) uno de estos dos modelos; ver [10, 20]. Macdonald y Ounis [16] proponen un enfoque diferente para clasificar la experiencia de los candidatos con respecto a un tema basado en técnicas de fusión de datos, sin utilizar heurísticas específicas de la colección; encuentran que la aplicación de modelos de ponderación basados en campos mejora la clasificación de los candidatos. Petkova y Croft [19] proponen otro enfoque, basado en una combinación de los Modelos 1 y 2 anteriores, modelando explícitamente los temas. Al abordar otras tareas de recuperación de expertos que también pueden abordarse utilizando asociaciones entre temas y personas, Balog y de Rijke [3] abordaron la tarea de determinar perfiles de expertos temáticos. Si bien sus métodos resultaron ser eficientes en el corpus de la W3C, requieren una cantidad de datos que puede que no esté disponible en la típica organización intensiva en conocimiento. Balog y de Rijke [2] estudian la tarea relacionada de encontrar expertos que sean similares a un pequeño conjunto de expertos dados como entrada. Por cierto, crear un resumen textual de una persona muestra algunas similitudes con la búsqueda de biografías, la cual ha recibido una considerable cantidad de atención recientemente; ver por ejemplo, [13]. Utilizamos modelado de lenguaje generativo para encontrar asociaciones entre temas y personas. En nuestro modelado de búsqueda y perfilado de expertos recopilamos evidencia de experiencia de múltiples fuentes, en una colección heterogénea, e integramos esta información con la co-ocurrencia de nombres de candidatos y términos de consulta: el entorno de modelado de lenguaje nos permite hacer esto de manera transparente. Nuestro modelado avanza en dos pasos. En el primer paso, consideramos tres <br>modelos base</br>, dos tomados de [4] (los Modelos 1 y 2 mencionados anteriormente), y uno una versión refinada de un modelo introducido en [3] (al que nos referimos como Modelo 3 a continuación); este tercer modelo también es similar al modelo descrito por Petkova y Croft [19]. Los modelos que consideramos en nuestra segunda ronda de experimentos son modelos de mezcla similares a los modelos de lenguaje contextual [1] y a los documentos ampliados de Tao et al. [21]; sin embargo, las características que utilizamos para definir nuestras expansiones, incluida la estructura temática y la estructura organizativa, no se han utilizado de esta manera antes. 3. En el escenario de recuperación de expertos que imaginamos, los usuarios que buscan expertos dentro de una organización tienen acceso a una interfaz que combina un cuadro de búsqueda (donde pueden buscar expertos o temas) con estructuras de navegación (de expertos y de temas) que les permite hacer clic hasta llegar a una página de experto (que proporciona el perfil de una persona) o a una página de tema (que proporciona una lista de expertos en el tema). Para alimentar la interfaz anterior, nos enfrentamos a dos tareas de recuperación de expertos, la búsqueda de expertos y el perfilado de expertos, que primero definimos y luego formalizamos utilizando modelos de lenguaje generativos. Para modelar cualquiera de las tareas, la probabilidad de que el tema de la consulta esté asociado a un experto candidato juega un papel clave en las estimaciones finales para la búsqueda y el perfilado. Al utilizar modelos de lenguaje, tanto los candidatos como la consulta se caracterizan por distribuciones de términos en el vocabulario (utilizado en los documentos puestos a disposición por la organización cuyas necesidades de recuperación de experiencia estamos abordando). 3.1 Búsqueda de expertos La búsqueda de expertos implica la tarea de encontrar a la persona adecuada con las habilidades y conocimientos apropiados: ¿Quiénes son los expertos en el tema X?. Por ejemplo, un empleado quiere averiguar quién trabajó en un proyecto en particular para descubrir por qué se tomaron decisiones específicas sin tener que revisar minuciosamente la documentación (si es que la hay). O pueden necesitar un especialista capacitado para consultoría sobre un problema específico. Dentro de una organización, generalmente hay muchos posibles candidatos que podrían ser expertos en un tema dado. Podemos plantear este problema de la siguiente manera: ¿Cuál es la probabilidad de que un candidato ca sea un experto dado el tema de consulta q? Es decir, determinamos p(ca|q) y clasificamos a los candidatos ca de acuerdo con esta probabilidad. Los candidatos con la probabilidad más alta dada la consulta son considerados los expertos más probables en ese tema. El desafío es cómo estimar esta probabilidad con precisión. Dado que es probable que la consulta consista solo en unos pocos términos para describir la experiencia requerida, deberíamos poder obtener una estimación más precisa al invocar el Teorema de Bayes, y estimar: p(ca|q) = p(q|ca)p(ca) p(q) , (1) donde p(ca) es la probabilidad de un candidato y p(q) es la probabilidad de una consulta. Dado que p(q) es una constante, se puede ignorar para fines de clasificación. Por lo tanto, la probabilidad de que un candidato ca sea un experto dado la consulta q es proporcional a la probabilidad de una consulta dada el candidato p(q|ca), ponderada por la creencia a priori p(ca) de que el candidato ca es un experto. p(ca|q) ∝ p(q|ca)p(ca) (2) En este artículo, nuestro enfoque principal se centra en estimar la probabilidad de una consulta dada el candidato p(q|ca), ya que esta probabilidad captura en qué medida el candidato conoce sobre el tema de la consulta. Si bien se asume generalmente que los priors del candidato son uniformes y, por lo tanto, no influirán en la clasificación, se ha demostrado que una elección sensata de priors puede mejorar el rendimiento [20]. Perfilado de expertos. Mientras que la tarea de búsqueda de expertos se centraba en encontrar expertos en un tema específico, la tarea de perfilado de expertos busca responder a una pregunta relacionada: ¿Sobre qué temas tiene conocimiento un candidato? Básicamente, esto da un giro a las preguntas sobre la búsqueda de expertos. El perfilado de un candidato individual implica la identificación de áreas de habilidades y conocimientos en las que tienen experiencia, así como una evaluación del nivel de competencia en cada una de estas áreas. Este es el perfil temático de los candidatos. Generalmente, los perfiles temáticos dentro de las organizaciones consisten en estructuras tabulares que catalogan explícitamente las habilidades y conocimientos de cada individuo en la organización. Sin embargo, esta práctica está limitada por los recursos disponibles para definir, crear, mantener y actualizar estos perfiles con el tiempo. Al centrarnos en métodos automáticos que se basan en la evidencia disponible dentro de los repositorios de documentos de una organización, nuestro objetivo es reducir el esfuerzo humano asociado con el mantenimiento de perfiles temáticos. Un perfil temático de un candidato, entonces, se define como un vector donde cada elemento i del vector corresponde a la experiencia del candidato en un tema dado ki, (es decir, s(ca, ki)). Cada tema ki define un área de conocimiento o habilidad particular que la organización utiliza para definir el perfil temático de los candidatos. Por lo tanto, se asume que se proporciona una lista de temas, {k1, . . . , kn}, donde n es el número de temas predefinidos: perfil(ca) = s(ca, k1), s(ca, k2), . . . , s(ca, kn) . Se necesita contexto y evidencia para ayudar a los usuarios de sistemas de búsqueda de expertos a decidir a quién contactar al buscar experiencia en un área particular. Ejemplos de dicho contexto son: ¿Con quién trabaja ella? ¿Cuáles son sus datos de contacto? ¿Está bien conectada, por si acaso no puede ayudarnos ella misma? ¿Cuál es su rol en la organización? ¿Quién es su superior? Los colaboradores, afiliaciones, etc. son parte del perfil social de los candidatos y pueden servir como antecedentes para interpretar las recomendaciones del sistema. En este documento solo abordamos el problema de determinar perfiles temáticos, dejando el perfilado social para trabajos futuros. Planteamos el problema de cuantificar la competencia de una persona en un área de conocimiento específica de la siguiente manera: ¿Cuál es la probabilidad de que un área de conocimiento (ki) forme parte del perfil de candidatos (expertise)? donde s(ca, ki) está definido por p(ki|ca). Nuestra tarea, entonces, es estimar p(ki|ca), lo cual es equivalente al problema de obtener p(q|ca), donde el tema ki se representa como un tema de consulta q, es decir, una secuencia de palabras clave que representan la experiencia requerida. Tanto la tarea de encontrar expertos como la de perfilar expertos dependen de la estimación precisa de p(q|ca). La única diferencia proviene de la probabilidad previa de que una persona sea experta (p(ca)), la cual puede ser incorporada en la tarea de encontrar expertos. Este antecedente no se aplica a la tarea de perfilado ya que el candidato (individuo) está fijo. 4. MODELOS DE REFERENCIA En esta sección describimos nuestros <br>modelos de referencia</br> para estimar p(q|ca), es decir, las asociaciones entre temas y personas. Tanto la búsqueda de expertos como el perfilado de expertos se reducen a esta estimación. Empleamos tres modelos para calcular esta probabilidad. 4.1 De temas a candidatos Utilizando Modelos de Candidatos: El Modelo 1 [4] define la probabilidad de una consulta dada un candidato (p(q|ca)) utilizando técnicas estándar de modelado de lenguaje, basadas en un modelo de lenguaje multinomial de unigrama. Para cada candidato ca, se infiere un modelo de lenguaje candidato θca de tal manera que la probabilidad de un término dado θca no sea cero para todos los términos, es decir, p(t|θca) > 0. A partir del modelo de candidato, la consulta se genera con la siguiente probabilidad: p(q|θca) = Y t∈q p(t|θca)n(t,q), donde cada término t en la consulta q se muestrea de manera idéntica e independiente, y n(t, q) es el número de veces que t ocurre en q. El modelo de lenguaje candidato se infiere de la siguiente manera: (1) se calcula un modelo empírico p(t|ca); (2) se suaviza con probabilidades de fondo. Usando las asociaciones entre un candidato y un documento, la probabilidad p(t|ca) puede aproximarse por: p(t|ca) = X d p(t|d)p(d|ca), donde p(d|ca) es la probabilidad de que el candidato ca genere un documento de apoyo d, y p(t|d) es la probabilidad de que un término t ocurra en el documento d. Utilizamos la estimación de máxima verosimilitud de un término, es decir, la frecuencia normalizada del término t en el documento d. La fuerza de la asociación entre el documento d y el candidato ca expresada por p(d|ca) refleja el grado en que la experiencia de los candidatos se describe utilizando este documento. La estimación de esta probabilidad se presenta más adelante, en la Sección 4.2. El modelo candidato se construye entonces como una interpolación lineal de p(t|ca) y el modelo de fondo p(t) para asegurar que no haya probabilidades nulas, lo que resulta en la estimación final: p(q|θca) = (4) Y t∈q ( (1 − λ) X d p(t|d)p(d|ca) ! + λp(t) )n(t,q) . El Modelo 1 recopila toda la información de términos de todos los documentos asociados con el candidato, y la utiliza para representar a ese candidato. Este modelo se utiliza para predecir qué tan probable es que un candidato produzca una consulta q. Esto puede interpretarse intuitivamente como la probabilidad de que este candidato hable sobre el tema de la consulta, donde asumimos que esto es indicativo de su experiencia. Usando Modelos de Documentos: El Modelo 2 [4] toma un enfoque diferente. Aquí, el proceso se divide en dos partes. Dado un candidato ca, (1) se selecciona un documento asociado con un candidato con probabilidad p(d|ca), y (2) a partir de este documento se genera una consulta q con probabilidad p(q|d). Entonces se toma la suma sobre todos los documentos para obtener p(q|ca), de modo que: p(q|ca) = Σ d p(q|d)p(d|ca). (5) La probabilidad de una consulta dada un documento se estima inferiendo un modelo de lenguaje del documento θd para cada documento d de manera similar a como se infería el modelo candidato: p(t|θd) = (1 − λ)p(t|d) + λp(t), (6) donde p(t|d) es la probabilidad del término en el documento. La probabilidad de una consulta dada el modelo de documento es: p(q|θd) = Y t∈q p(t|θd)n(t,q) . La estimación final de p(q|ca) se obtiene sustituyendo p(q|d) por p(q|θd) en la Ecuación 5 (ver [4] para más detalles). Conceptualmente, el Modelo 2 difiere del Modelo 1 porque el candidato no está modelado directamente. En cambio, el documento actúa como una variable oculta en el proceso que separa la consulta del candidato. Este proceso es similar a cómo un usuario puede buscar candidatos con un motor de búsqueda estándar: inicialmente encontrando los documentos relevantes y luego viendo quién está asociado con ese documento. Al examinar una serie de documentos, el usuario puede obtener una idea de qué candidatos son más propensos a discutir el tema q. Usando Modelos de Temas: Modelo 3 Introducimos un tercer modelo, Modelo 3. En lugar de intentar modelar el proceso de generación de consultas a través de modelos de candidatos o documentos, representamos la consulta como un modelo de lenguaje de temas y estimamos directamente la probabilidad del candidato p(ca|q). Este enfoque es similar al modelo presentado en [3, 19]. Como en los modelos anteriores, se infiere un modelo de lenguaje, pero esta vez para la consulta. Adaptamos el trabajo de Lavrenko y Croft [14] para estimar un modelo de tema a partir de la consulta. El procedimiento es el siguiente. Dada una colección de documentos y un tema de consulta q, se asume que existe un modelo de tema desconocido θk que asigna probabilidades p(t|θk) a las ocurrencias de términos en los documentos del tema. Tanto la consulta como los documentos son muestras de θk (a diferencia de los enfoques anteriores, donde se asume que una consulta se extrae de un documento específico o un modelo candidato). La tarea principal es estimar p(t|θk), la probabilidad de un término dado el modelo de tema. Dado que la consulta q es muy dispersa y no hay ejemplos de documentos sobre el tema, es necesario aproximar esta distribución. Lavrenko y Croft [14] sugieren una forma razonable de obtener dicha aproximación, asumiendo que p(t|θk) puede aproximarse por la probabilidad del término t dado la consulta q. Entonces podemos estimar p(t|q) utilizando la probabilidad conjunta de observar el término t junto con los términos de la consulta, q1, . . . , qm, y dividiendo por la probabilidad conjunta de los términos de la consulta: p(t|θk) ≈ p(t|q) = p(t, q1, . . . , qm) p(q1, . . . , qm) = p(t, q1, . . . , qm) P t ∈T p(t , q1, . . . , qm) , donde p(q1, . . . , qm) = P t ∈T p(t , q1, . . . , qm), y T es el vocabulario completo de términos. Para estimar la probabilidad conjunta p(t, q1, . . . , qm), seguimos [14, 15] y asumimos que t y q1, . . . , qm son mutuamente independientes, una vez que elegimos una distribución fuente del conjunto de distribuciones fuente subyacentes U. Si elegimos U como un conjunto de modelos de documentos, entonces para construir este conjunto, la consulta q se emitiría contra la colección, y se asume que los primeros n devueltos son relevantes para el tema, y por lo tanto se tratan como muestras del modelo de tema. (Tenga en cuenta que en su lugar podrían usarse modelos candidatos). Con los modelos de documentos formando U, la probabilidad conjunta del término y la consulta se convierte en: p(t, q1, . . . , qm) = X d∈U p(d) ˘ p(t|θd) mY i=1 p(qi|θd) ¯ . (7) Aquí, p(d) denota la distribución previa sobre el conjunto U, que refleja la relevancia del documento para el tema. Suponemos que p(d) es uniforme en todo U. Para clasificar a los candidatos según el modelo de tema definido, utilizamos la métrica de divergencia de Kullback-Leibler (KL, [8]) para medir la diferencia entre los modelos de los candidatos y el modelo de tema: KL(θk||θca) = X t p(t|θk) log p(t|θk) p(t|θca) . (8) Se considera que los candidatos con una divergencia menor respecto al modelo de tema son más propensos a ser expertos en ese tema. El modelo candidato θca está definido en la Ecuación 4. Al utilizar la divergencia de Kullback-Leibler en lugar de la probabilidad de un candidato dado el modelo de tema p(ca|θk), evitamos problemas de normalización. 4.2 Asociaciones documento-candidato Para nuestros modelos, necesitamos poder estimar la probabilidad p(d|ca), que expresa en qué medida un documento d caracteriza al candidato ca. En [4], se presentan dos métodos para estimar esta probabilidad, basados en el número de nombres de personas reconocidos en un documento. Sin embargo, en nuestro entorno de intranet es razonable asumir que los autores de los documentos pueden ser identificados de manera inequívoca (por ejemplo, como el autor de un artículo, el profesor asignado a un curso, el propietario de una página web, etc.). Por lo tanto, establecemos p(d|ca) en 1 si el candidato ca es el autor del documento d, de lo contrario la probabilidad es 0. En la Sección 6 describimos cómo se puede determinar la autoría en diferentes tipos de documentos dentro de la colección. 5. La colección de expertos de UvT utilizada en los experimentos de este artículo se ajusta al escenario descrito en la Sección 3. La colección se basa en el sistema Webwijs (Webwise) desarrollado en la Universidad de Tilburg (UvT) en los Países Bajos. Webwijs (http://www.uvt.nl/webwijs/) es una base de datos de acceso público de empleados de la UvT que están involucrados en investigación o enseñanza; actualmente, Webwijs contiene información sobre 1168 expertos, cada uno de los cuales tiene una página con información de contacto y, si está disponible por parte del experto, una descripción de investigación y lista de publicaciones. Además, cada experto puede seleccionar áreas de especialización de una lista de 1491 temas y se le anima a sugerir nuevos temas que necesitan ser aprobados por el editor de Webwijs. Cada tema tiene una página separada que muestra a todos los expertos asociados con ese tema y, si está disponible, una lista de temas relacionados. Webwijs está disponible en holandés e inglés, y esta bilingüidad se ha preservado en la colección. Cada página de Webwijs en holandés tiene una traducción al inglés. No todos los temas en holandés tienen una traducción al inglés, pero lo contrario es cierto: los 981 temas en inglés tienen un equivalente en holandés. Aproximadamente el 42% de los expertos imparten cursos en la Universidad de Tilburg; estos cursos también fueron rastreados e incluidos en el perfil. Además, aproximadamente el 27% de los expertos enlazan su página académica desde su página de Webwijs. Estas páginas de inicio fueron rastreadas y añadidas a la colección. (Esto significa que si los expertos colocaron las versiones completas de sus publicaciones en sus páginas de inicio académicas, estas también estaban disponibles para su indexación). También obtuvimos 1880 versiones completas de publicaciones del repositorio institucional de UvT y el número de expertos holandeses en inglés es de 1168, con 743 expertos con ≥ 1 tema. El número de temas es de 1491 y 981, y el número de pares experto-tema es de 4318 y 3251, con un promedio de temas por experto de 5.8 y 5.9 respectivamente. El máximo número de temas por experto es de 60 (1) y 35 (1), y el mínimo es de 1 (74) y 1 (106) respectivamente. El promedio de expertos por tema es de 2.9 y 3.3, con un máximo de 30 (1) y un mínimo de 1 (615) y 1 (346) respectivamente. Hay 318 expertos con HP y CD, con un promedio de 3.5 CDs por experto docente. Además, hay 329 expertos con RD y 734 con PUB, con un promedio de 27.0 publicaciones por experto, 25.2 citas por experto y 1.8 publicaciones completas por experto. Tabla 2: Estadísticas descriptivas de las versiones holandesas e inglesas de la colección de expertos de UvT. Ejecutamos el identificador de idioma TextCat [23] para clasificar el idioma de las páginas de inicio y las publicaciones de texto completo. Nos limitamos a las páginas en las que el clasificador tenía confianza en el idioma utilizado en la página. Esto resultó en cuatro tipos de documentos: descripciones de investigación (RD), descripciones de cursos (CD), publicaciones (PUB; versiones de texto completo y solo de cita) y páginas web académicas (HP). Todo fue agrupado en la colección UvT Expert que está disponible en http://ilk.uvt.nl/uvt-expert-collection/. La colección UvT Expert fue extraída de un entorno organizativo diferente a la colección de la W3C y difiere de ella en varios aspectos. El entorno de UvT es uno con cantidades relativamente pequeñas de datos multilingües. Las asociaciones entre los documentos y los autores son claras y los datos están estructurados y limpios. La colección abarca una amplia gama de áreas de especialización, como suele encontrarse en intranets de universidades y otros institutos intensivos en conocimiento. Además, nuestro entorno universitario cuenta con varios tipos de estructura (temática y organizativa), así como múltiples tipos de documentos. Otra diferencia importante entre los dos conjuntos de datos es que las áreas de especialización en la colección de Expertos de UvT son autoseleccionadas en lugar de basarse en la membresía de un grupo o asignaciones de otros. El tamaño es otra dimensión en la que difieren las colecciones de expertos de W3C y UvT: esta última es la más pequeña de las dos. También son realistas las grandes diferencias en la cantidad de información disponible para cada experto. Utilizar Webwijs es voluntario; 425 expertos holandeses no seleccionaron ningún tema en absoluto. Esto nos deja con 743 perfiles de expertos holandeses y 727 perfiles de expertos en inglés utilizables. La Tabla 2 proporciona estadísticas descriptivas para la colección de Expertos de UvT. Las universidades tienden a tener una estructura jerárquica que va desde el nivel de la facultad, pasando por los departamentos, grupos de investigación, hasta llegar a los investigadores individuales. En la colección de Expertos de UvT tenemos información sobre las afiliaciones de los investigadores con facultades e institutos, lo que nos proporciona una jerarquía organizativa de dos niveles. La Universidad de Tilburg tiene 22 unidades organizativas a nivel de facultad (incluida la oficina universitaria y varios institutos de investigación) y 71 departamentos, lo que equivale a 3.2 departamentos por facultad. En cuanto a la jerarquía temática utilizada por Webwijs, 131 de los 1491 temas son nodos principales en la jerarquía. Esta jerarquía tiene una longitud promedio de cadena de temas de 2.65 y una longitud máxima de 7 temas. 6. EVALUACIÓN A continuación, evaluamos los modelos de la Sección 4 para la búsqueda y perfilado de expertos en la colección de Expertos de UvT. Detallamos nuestras preguntas de investigación y configuración experimental, y luego presentamos nuestros resultados. 6.1 Preguntas de investigación Abordamos las siguientes preguntas de investigación. Tanto la búsqueda de expertos como el perfilado dependen de las estimaciones de p(q|ca). La pregunta es cómo se comparan los modelos en las diferentes tareas, y en el contexto de la colección de expertos de UvT. En [4], el Modelo 2 superó al Modelo 1 en la colección de la W3C. ¿Cómo se comparan en nuestro conjunto de datos? ¿Y cómo se compara el Modelo 3 con el Modelo 1? ¿Qué hay de las diferencias de rendimiento entre los dos idiomas en nuestra colección de pruebas? 6.2 Configuración Experimental La salida de nuestros modelos fue evaluada frente a las etiquetas de temas autoasignadas, que fueron tratadas como juicios de relevancia. Los resultados fueron evaluados por separado para inglés y holandés. Para inglés solo usamos temas para los cuales había traducción al neerlandés; para neerlandés se consideraron todos los temas. Los resultados se promediaron para las consultas en la intersección de las evaluaciones de relevancia y los resultados; las consultas faltantes no contribuyen con un valor de 0 a las puntuaciones. Utilizamos medidas estándar de recuperación de información, como la Precisión Media Promedio (MAP) y la Reciprocidad Media Promedio (MRR). También informamos el porcentaje de temas (%q) y candidatos (%ca) cubiertos, respectivamente, para las tareas de búsqueda y perfilado de expertos. 6.3 Resultados La Tabla 1 muestra el rendimiento de los Modelos 1, 2 y 3 en las tareas de búsqueda y perfilado de expertos. Las filas de la tabla corresponden a los diversos tipos de documentos (RD, CD, PUB y HP) y a sus combinaciones. RD+CD+PUB+HP es equivalente a la colección completa y será referida como la LÍNEA BASE de nuestros experimentos. Al observar la Tabla 1, vemos que el Modelo 2 tiene el mejor rendimiento en general. Sin embargo, cuando los datos están limpios y muy enfocados (RD), el Modelo 3 lo supera en varios casos. El modelo 1 tiene la mejor cobertura de candidatos (%ca) y temas (%q). Los diversos tipos de documentos difieren en sus características y en cómo mejoran las tareas de búsqueda y perfilado. El perfilado de expertos se beneficia mucho de los datos limpios presentes en los tipos de documentos RD y CD, mientras que las publicaciones contribuyen principalmente a la tarea de encontrar expertos. Agregar las páginas de inicio no resulta ser particularmente útil. Cuando comparamos los resultados entre idiomas, encontramos que la cobertura de los temas en inglés (%q) es mayor que la de los temas en neerlandés para la búsqueda de expertos. Además de eso, las puntuaciones caen en el mismo rango para ambos idiomas. Para la tarea de perfilado, la cobertura de los candidatos (%ca) es muy similar para ambos idiomas. Sin embargo, el rendimiento es considerablemente mejor para los temas en inglés. Si bien es difícil comparar las puntuaciones entre colecciones, concluimos con una breve comparación de las puntuaciones absolutas en la Tabla 1 con las reportadas en [3, 4] en el conjunto de pruebas de W3C (edición de 2005). Para el experto que busca, los puntajes MAP para el Modelo 2 reportados aquí son aproximadamente un 50% más altos que las cifras correspondientes en [4], mientras que nuestros puntajes MRR están ligeramente por debajo de los de [4]. Para el perfilado de expertos, las diferencias son mucho más dramáticas: las puntuaciones MAP para el Modelo 2 reportadas aquí son aproximadamente un 50% más bajas que las puntuaciones en [3], mientras que las puntuaciones MRR (mejores) son aproximadamente iguales a las de [3]. La causa de estas diferencias parece residir en el número de áreas de conocimiento consideradas aquí, aproximadamente 30 veces más que en el entorno del W3C. MODELOS AVANZADOS Ahora que hemos desarrollado y evaluado técnicas básicas de modelado de lenguaje para la recuperación de expertos, nos enfocamos en modelos refinados que explotan características especiales de nuestra colección de pruebas. 7.1 Explotando la similitud en el área de conocimiento Una forma de mejorar la puntuación de una consulta dada un candidato es considerar qué otras solicitudes satisfaría el candidato y usarlas como evidencia adicional para respaldar la consulta original, proporcionalmente. Búsqueda de expertos Perfilado de expertos Tipos de documentos Modelo 1 Modelo 2 Modelo 3 Modelo 1 Modelo 2 Modelo 3 %q MAP MRR %q MAP MRR %q MAP MRR %ca MAP MRR %ca MAP MRR %ca MAP MRR Inglés RD 97.8 0.126 0.269 83.5 0.144 0.311 83.3 0.129 0.271 100 0.089 0.189 39.3 0.232 0.465 41.1 0.166 0.337 CD 97.8 0.118 0.227 91.7 0.123 0.248 91.7 0.118 0.226 32.8 0.188 0.381 32.4 0.195 0.385 32.7 0.203 0.370 PUB 97.8 0.200 0.330 98.0 0.216 0.372 98.0 0.145 0.257 78.9 0.167 0.364 74.5 0.212 0.442 78.9 0.135 0.299 HP 97.8 0.081 0.186 97.4 0.071 0.168 97.2 0.062 0.149 31.2 0.150 0.299 28.8 0.185 0.335 30.1 0.136 0.287 RD+CD 97.8 0.188 0.352 92.9 0.193 0.360 92.9 0.150 0.273 100 0.145 0.286 61.3 0.251 0.477 63.2 0.217 0.416 RD+CD+PUB 97.8 0.235 0.373 98.1 0.277 0.439 98.1 0.178 0.305 100 0.196 0.380 87.2 0.280 0.533 89.5 0.170 0.344 RD+CD+PUB+HP 97.8 0.237 0.372 98.6 0.280 0.441 98.5 0.166 0.293 100 0.199 0.387 88.7 0.281 0.525 90.9 0.169 0.329 Holandés RD 61.3 0.094 0.229 38.4 0.137 0.336 38.3 0.127 0.295 38.0 0.127 0.386 34.1 0.138 0.420 38.0 0.105 0.327 CD 61.3 0.107 0.212 49.7 0.128 0.256 49.7 0.136 0.261 32.5 0.151 0.389 31.8 0.158 0.396 32.5 0.170 0.380 PUB 61.3 0.193 0.319 59.5 0.218 0.368 59.4 0.173 0.291 78.8 0.126 0.364 76.0 0.150 0.424 78.8 0.103 0.294 HP 61.3 0.063 0.169 56.6 0.064 0.175 56.4 0.062 0.163 29.8 0.108 0.308 27.8 0.125 0.338 29.8 0.098 0.255 RD+CD 61.3 0.159 0.314 51.9 0.184 0.360 51.9 0.169 0.324 60.5 0.151 0.410 57.2 0.166 0.431 60.4 0.159 0.384 RD+CD+PUB 61.3 0.244 0.398 61.5 0.260 0.424 61.4 0.210 0.350 90.3 0.165 0.445 88.2 0.189 0.479 90.3 0.126 0.339 RD+CD+PUB+HP 61.3 0.249 0.401 62.6 0.265 0.436 62.6 0.195 0.344 91.9 0.164 0.426 90.1 0.195 0.488 91.9 0.125 0.328 Tabla 1: Rendimiento de los modelos en las tareas de búsqueda y perfilado de expertos, utilizando diferentes tipos de documentos y sus combinaciones. %q es el número de temas cubiertos (se aplica a la tarea de búsqueda de expertos), %ca es el número de candidatos cubiertos (se aplica a la tarea de perfilado de expertos). Los bloques superior e inferior corresponden al inglés y al holandés respectivamente. Las mejores puntuaciones están en negrita. Esto se puede modelar interpolando entre el p(q|ca) y la evidencia adicional de todos los pedidos similares q, de la siguiente manera: p(q|ca) = λp(q|ca) + (1 − λ) X q p(q|q )p(q |ca), (9) donde p(q|q ) representa la similitud entre los dos temas q y q. Para poder trabajar con métodos de similitud que no son necesariamente probabilidades, establecemos p(q|q ) = w(q,q ) γ , donde γ es una constante de normalización, de modo que γ = P q w(q , q ). Consideramos cuatro métodos para calcular la puntuación de similitud entre dos temas. Tres enfoques se basan estrictamente en el contenido y establecen similitudes examinando patrones de co-ocurrencia de temas dentro de la colección, mientras que el último enfoque explota la estructura jerárquica de áreas temáticas que pueden estar presentes dentro de una organización (ver [7] para más ejemplos de integrar relaciones entre palabras en modelos de lenguaje). La métrica de divergencia Kullback-Leibler (KL) definida en la ecuación 8 proporciona una medida de cuán diferentes o similares son dos distribuciones de probabilidad. Se infiere un modelo de tema para q y q utilizando el método presentado en la Sección 4.1 para describir la consulta en todo el vocabulario. Dado que un puntaje KL más bajo significa que las consultas son más similares, permitimos que w(q, q ) = max(KL(θq||·) − KL(θq||θq )). El Información Mutua Puntual (PMI, [17]) es una medida de asociación utilizada en teoría de la información para determinar el grado de independencia entre variables. La dependencia entre dos consultas se refleja en la puntuación SI(q, q), donde las puntuaciones mayores que cero indican que es probable que exista una dependencia, lo que interpretamos como que las consultas son probablemente similares: SI(q, q) = log p(q, q) / p(q)p(q). Estimamos la probabilidad de un tema p(q) utilizando el número de documentos relevantes para la consulta q dentro de la colección. La probabilidad conjunta p(q, q) se estima de manera similar, utilizando la concatenación de q y q como una consulta. Para obtener p(q|q), luego establecemos w(q, q) = SI(q, q) cuando SI(q, q) > 0, de lo contrario w(q, q) = 0, ya que solo estamos interesados en incluir consultas que sean similares. La estadística de log-verosimilitud proporciona otra medida de dependencia, que es más confiable que la medida de información mutua puntual [17]. Sea k1 el número de co-ocurrencias de q y q, k2 el número de ocurrencias de q que no co-ocurren con q, n1 el número total de ocurrencias de q, y n2 el número total de tokens de tema menos el número de ocurrencias de q. Entonces, sea p1 = k1/n1, p2 = k2/n2 y p = (k1 + k2)/(n1 + n2), (q, q ) = 2( (p1, k1, n1) + (p2, k2, n2) − (p, k1, n1) − (p, k2, n2)), donde (p, n, k) = k log p + (n − k) log(1 − p). El puntaje más alto indica que las consultas también son probablemente similares, por lo tanto, establecemos w(q, q ) = (q, q ). Finalmente, también estimamos la similitud de dos temas basándonos en su distancia dentro de la jerarquía de temas. La jerarquía de temas se visualiza como un grafo dirigido, y para todos los pares de temas se calcula el camino más corto SP(q, q). Establecimos el puntaje de similitud como el recíproco del camino más corto: w(q, q ) = 1/SP(q, q ). 7.2 Información contextual Dada la jerarquía de una organización, las unidades a las que pertenece una persona se consideran como un contexto para compensar la escasez de datos. Lo modelamos de la siguiente manera: p (q|ca) = 1 − P ou∈OU(ca) λou · p(q|ca) + P ou∈OU(ca) λou · p(q|ou), donde OU(ca) es el conjunto de unidades organizativas de las cuales el candidato ca es miembro, y p(q|o) expresa la fuerza de la asociación entre la consulta q y la unidad ou. La última probabilidad se puede estimar utilizando cualquiera de los tres modelos básicos, simplemente reemplazando ca con ou en las ecuaciones correspondientes. Una unidad organizativa está asociada con todos los documentos que sus miembros han escrito. Es decir, p(d|ou) = maxca∈ou p(d|ca). 7.3 Un modelo multilingüe simple Para institutos de conocimiento en Europa, ya sea académicos u otros, un entorno multilingüe (o al menos bilingüe) es típico. El siguiente modelo se basa en un tipo de suposición de independencia: no hay transferencia de conocimientos/perfiles a través de las barreras del idioma. Si bien es una simplificación, esta es una aproximación sensata en primer lugar. Eso es: p (q|ca) =P l∈L λl · p(ql|ca), donde L es el conjunto de idiomas utilizados en la colección, ql es la traducción de la consulta q al idioma l, y λl es un parámetro de suavizado específico del idioma, tal que P l∈L λl = 1. 8. MODELOS AVANZADOS: EVALUACIÓN En esta sección presentamos una evaluación experimental de nuestros modelos avanzados. Búsqueda de expertos Perfilado de expertos Modelo de lenguaje 1 Modelo 2 Modelo 3 Modelo 1 Modelo 2 Modelo 3 %q MAP MRR %q MAP MRR %q MAP MRR %ca MAP MRR %ca MAP MRR %ca MAP MRR Solo inglés 97.8 0.237 0.372 98.6 0.280 0.441 98.5 0.166 0.293 100 0.199 0.387 88.7 0.281 0.525 90.9 0.169 0.329 Solo holandés 61.3 0.249 0.401 62.6 0.265 0.436 62.6 0.195 0.344 91.9 0.164 0.426 90.1 0.195 0.488 91.9 0.125 0.328 Combinación 99.4 0.297 0.444 99.7 0.324 0.491 99.7 0.223 0.388 100 0.241 0.445 92.1 0.313 0.564 93.2 0.224 0.411 Tabla 3: Rendimiento de la combinación de idiomas en las tareas de búsqueda y perfilado de expertos (en candidatos). Los mejores puntajes para cada modelo están en cursiva, los mejores puntajes absolutos para las tareas de búsqueda y perfilado de expertos están en negrita. Tabla 4: Rendimiento en las tareas de búsqueda de expertos (arriba) y perfilado (abajo), utilizando similitudes de áreas de conocimiento. Las carreras fueron evaluadas en los temas principales establecidos. Las mejores puntuaciones están en negrita. 8.1 Preguntas de investigación Nuestras preguntas siguen los refinamientos presentados en la sección anterior: ¿Explotar la similitud en el área de conocimiento mejora la efectividad? ¿Cuál de los diversos métodos para capturar las relaciones entre palabras es más efectivo? Además, ¿es útil nuestra forma de incorporar información contextual? ¿Para qué tareas? Y finalmente, ¿es nuestra forma simple de combinar las puntuaciones monolingües suficiente para obtener mejoras significativas? 8.2 Configuración experimental Dado que las autoevaluaciones también son escasas en nuestra colección, para poder medir las diferencias entre los diversos modelos, seleccionamos un subconjunto de temas y evaluamos (algunas de las) ejecuciones solo en este subconjunto. Este conjunto se denomina como temas principales, y consiste en temas que se encuentran en el nivel superior de la jerarquía temática. (Un tema principal tiene subtemas, pero no es un subtema de ningún otro tema). Este conjunto principal consiste en 132 temas en holandés y 119 en inglés. Los juicios de relevancia se limitaron al conjunto de temas principales, pero no se ampliaron con subtemas. 8.3 Explotando la similitud de áreas de conocimiento. La Tabla 4 presenta los resultados. Los cuatro métodos utilizados para estimar la similitud de áreas de conocimiento son la divergencia de KL (KLDIV) y PointLang. Modelo 1 Modelo 2 Modelo 3 MAP MRR MAP MRR MAP MRR Búsqueda de expertos UK TODOS 0.423 0.545 0.654 0.799 0.494 0.629 UK PRINCIPAL 0.500 0.621 0.704 0.834 0.587 0.699 NL TODOS 0.439 0.560 0.672 0.826 0.480 0.630 NL PRINCIPAL 0.440 0.584 0.645 0.816 0.515 0.655 Perfilado de expertos UK TODOS 0.240 0.640 0.306 0.778 0.223 0.616 UK PRINCIPAL 0.523 0.677 0.519 0.648 0.461 0.587 NL TODOS 0.203 0.716 0.254 0.770 0.183 0.627 NL PRINCIPAL 0.332 0.576 0.380 0.624 0.332 0.549 Tabla 5: Evaluación de los modelos de contexto en unidades organizativas. información mutua ponderada (PMI), log-verosimilitud (LL) y distancia dentro de la jerarquía de temas (HDIST). Logramos mejorar el resultado base en todos los casos, pero la mejora es más notable para la tarea de perfilado. Para ambas tareas, el método LL tuvo el mejor rendimiento. Los enfoques basados en el contenido tuvieron un rendimiento consistentemente mejor que HDIST. 8.4 Información contextual Una jerarquía de dos niveles de unidades organizativas (facultades e institutos) está disponible en la colección de Expertos de UvT. La unidad a la que pertenece una persona se utiliza como contexto para esa persona. Primero, evaluamos los modelos de las unidades organizativas, utilizando todos los temas (TODOS) y solo los temas principales (PRINCIPALES). Una unidad organizativa se considera relevante para un tema dado (o viceversa) si al menos un miembro de la unidad seleccionó el tema dado como área de especialización. La Tabla 5 informa sobre los resultados. En lo que respecta a la búsqueda de expertos, dado un tema, la unidad organizativa correspondiente puede ser identificada con alta precisión. Sin embargo, la tarea de perfilado de expertos muestra una imagen diferente: las puntuaciones son bajas y la tarea parece difícil. La explicación puede ser que los conceptos generales (es decir, nuestros temas principales) pueden pertenecer a varias unidades organizativas. Segundo, realizamos otra evaluación, donde combinamos los modelos contextuales con los modelos candidatos (para puntuar a los candidatos nuevamente). La tabla 6 informa sobre los resultados. Encontramos un impacto positivo de los modelos de contexto solo para la búsqueda de expertos. Notablemente, para la búsqueda de expertos (y Modelo 1), mejora más del 50% (para inglés) y más del 70% (para holandés) en el MAP. El bajo rendimiento en la creación de perfiles de expertos puede deberse al hecho de que los modelos de contexto por sí solos no tuvieron un buen desempeño en la tarea de creación de perfiles desde el principio. 8.5 Modelos multilingües En esta subsección evaluamos el método para combinar resultados en múltiples idiomas que describimos en la Sección 7.3. En nuestro entorno, el conjunto de idiomas consiste en inglés y holandés: L = {UK, NL}. Los pesos de estos idiomas se establecieron para ser idénticos (λUK = λNL = 0.5). Realizamos experimentos con diferentes configuraciones de λ, pero no observamos diferencias significativas en el rendimiento. La Tabla 3 informa sobre los resultados multilingües, donde el rendimiento se evalúa en el conjunto completo de temas. Los tres modelos tienen una diferencia significativa. Tabla 6: Rendimiento de los modelos de contexto (CT) en comparación con la línea base (BL). Las mejores puntuaciones están en negrita, demostradas en todas las medidas para ambas tareas. La cobertura de temas y candidatos para las tareas de búsqueda y perfilado de expertos, respectivamente, es cercana al 100% en todos los casos. La mejora relativa de las puntuaciones de precisión varía del 10% al 80%. Estas puntuaciones demuestran que, a pesar de su simplicidad, nuestro método para combinar resultados en varios idiomas logra mejoras sustanciales sobre el punto de referencia. CONCLUSIONES En este artículo nos enfocamos en la recuperación de expertos (búsqueda y perfilado de expertos) en un nuevo entorno de una organización típica intensiva en conocimiento en la que los datos disponibles son de alta calidad, multilingües y abarcan una amplia gama de áreas de expertise. Normalmente, la cantidad de datos disponibles en una organización como una universidad, un instituto de investigación o un laboratorio de investigación es limitada en comparación con la colección de la W3C que ha sido utilizada principalmente para la evaluación experimental de la recuperación de la experiencia hasta ahora. Para examinar la recuperación de la experiencia en este entorno, presentamos (y lanzamos) la colección de Expertos de UvT como un caso representativo de organizaciones intensivas en conocimiento. La nueva colección refleja las propiedades típicas de los institutos intensivos en conocimiento mencionados anteriormente e incluye también varias características que pueden ser potencialmente útiles para la recuperación de expertos, como la estructura temática y organizativa. Evaluamos cómo los modelos de vanguardia para la búsqueda y perfilado de expertos se desempeñaron en este nuevo entorno y luego refinamos estos modelos para intentar explotar las diferentes características dentro del entorno de datos (idioma, actualidad y estructura organizativa). Encontramos que los modelos actuales de recuperación de experiencia se generalizan bien a este nuevo entorno; además, descubrimos que refinar los modelos para tener en cuenta las diferencias resulta en mejoras significativas, compensando así los problemas causados por la escasez de datos. El trabajo futuro incluye establecer evaluaciones manuales de perfiles generados automáticamente por los propios empleados, especialmente en casos en los que los empleados no han proporcionado un perfil ellos mismos. 10. AGRADECIMIENTOS Krisztian Balog fue apoyado por la Organización Neerlandesa para la Investigación Científica (NWO) bajo el número de proyecto 220-80-001. Maarten de Rijke también recibió apoyo de NWO bajo los números de proyecto 017.001.190, 220-80-001, 264-70-050, 354-20-005, 600.065.120, 612-13-001, 612.000.106, 612.066.302, 612.069.006, 640.001.501, 640.002.501, y de la Unión Europea. Programa IST del 6º Programa Marco para la I+D en virtud del contrato del proyecto MultiMATCH IST-033104. El trabajo de Toine Bogers y Antal van den Bosch fue financiado por el programa IOP-MMI de SenterNovem / Ministerio de Asuntos Económicos de los Países Bajos, como parte del proyecto 'A Propos'. 11. REFERENCIAS [1] L. Azzopardi. Incorporando contexto en el marco de modelado del lenguaje para la recuperación de información ad-hoc. Tesis doctoral, Universidad de Paisley, 2005. [2] K. Balog y M. de Rijke. Encontrando expertos similares. En este volumen, 2007. [3] K. Balog y M. de Rijke. Determinación de perfiles de expertos (con una aplicación a la búsqueda de expertos). En IJCAI 07: Proc. 20th Intern. Conferencia Conjunta sobre Inteligencia Artificial, páginas 2657-2662, 2007. [4] K. Balog, L. Azzopardi y M. de Rijke. Modelos formales para la búsqueda de expertos en corporaciones empresariales. En SIGIR 06: Actas de la 29ª conferencia internacional anual. Conferencia ACM SIGIR sobre Investigación y desarrollo en recuperación de información, páginas 43-50, 2006. [5] I. Becerra-Fernández. El papel de las tecnologías de inteligencia artificial en la implementación de sistemas de gestión del conocimiento de localización de personas. En el taller de AAAI sobre la incorporación de conocimiento en los procesos de negocio, marzo de 2000. [6] C. S. Campbell, P. P. Maglio, A. Cozzi y B. Dom. Identificación de la experiencia utilizando comunicaciones por correo electrónico. En CIKM 03: Actas de la duodécima conferencia internacional sobre gestión de la información y el conocimiento, páginas 528-531, 2003. [7] G. Cao, J.-Y. Nie, y J. Bai. Integrando relaciones entre palabras en modelos de lenguaje. En SIGIR 05: Actas de la 28ª conferencia internacional anual. Conferencia ACM SIGIR sobre Investigación y desarrollo en recuperación de información, páginas 298-305, 2005. [8] T. M. Cover y J. A. Thomas. Elementos de la teoría de la información. Wiley-Interscience, 1991. [9] N. Craswell, D. Hawking, A. M. Vercoustre y P. Wilkins. Experto en P@noptic: Buscando expertos no solo documentos. En Ausweb, 2001. [10] N. Craswell, A. de Vries e I. Soboroff. Resumen de la pista empresarial TREC2005. En la Decimocuarta Conferencia de Recuperación de Información. Proc. (TREC 2005), 2006. [11] T. H. Davenport y L. Prusak. Conocimiento en Acción: Cómo las Organizaciones Gestionan lo que Saben. Harvard Business School Press, Boston, MA, 1998. [12] T. Dunning.\nHarvard Business School Press, Boston, MA, 1998. [12] T. Dunning. Métodos precisos para la estadística de sorpresa y coincidencia. Lingüística Computacional, 19(1):61-74, 1993. [13] E. Filatova y J. Prager. Dime qué haces y te diré quién eres: Aprendiendo actividades relacionadas con la ocupación para biografías. En HLT/EMNLP, 2005. [14] V. Lavrenko y W. B. Croft. Modelos de lenguaje basados en relevancia. En SIGIR 01: Actas de la 24ª conferencia internacional anual. Conferencia ACM SIGIR sobre Investigación y Desarrollo en Recuperación de Información, páginas 120-127, 2001. [15] V. Lavrenko, M. Choquette y W. B. Croft. Modelos de relevancia multilingües. En SIGIR 02: Actas de la 25ª conferencia internacional anual. Conferencia ACM SIGIR sobre Investigación y Desarrollo en Recuperación de Información, páginas 175-182, 2002. [16] C. Macdonald e I. Ounis. Votación de candidatos: adaptando técnicas de fusión de datos para una tarea de búsqueda de expertos. En CIKM 06: Actas de la 15ª conferencia internacional de ACM sobre gestión de la información y el conocimiento, páginas 387-396, 2006. [17] C. Manning y H. Schütze. Fundamentos del Procesamiento del Lenguaje Natural Estadístico. El MIT Press, 1999. [18] A. Mockus y J. D. Herbsleb. Explorador de experticia: un enfoque cuantitativo para identificar la experticia. En ICSE 02: Proc. 24th Intern. Conf. en Ingeniería de Software, páginas 503-512, 2002. [19] D. Petkova y W. B. Croft. Modelos de lenguaje jerárquicos para la búsqueda de expertos en corporaciones empresariales. En Proc. ICTAI 2006, páginas 599-608, 2006. [20] I. Soboroff, A. de Vries y N. Craswell. Resumen de la pista empresarial TREC 2006. En las Notas de Trabajo de TREC 2006, 2006. [21] T. Tao, X. Wang, Q. Mei y C. Zhai. Recuperación de información del modelo de lenguaje con expansión de documentos. En HLT-NAACL 2006, 2006. [22] TREC. Ruta empresarial, 2005. URL: http://www.ins.cwi.nl/projects/trec-ent/wiki/. [23] G. van Noord.\nURL: http://www.ins.cwi.nl/projects/trec-ent/wiki/. [23] G. van Noord. Adivinador de idioma TextCat. URL: http://www.let.rug.nl/˜vannoord/TextCat/. [24] W3C. La colección de pruebas del W3C, 2005. URL: http://research.microsoft.com/users/nickcr/w3c-summary.html. ",
            "candidates": [],
            "error": [
                [
                    "modelos base",
                    "modelos base",
                    "modelos base",
                    "modelos de referencia"
                ]
            ]
        },
        "expertise search": {
            "translated_key": "búsqueda de experiencia",
            "is_in_text": false,
            "original_annotated_sentences": [
                "Broad Expertise Retrieval in Sparse Data Environments Krisztian Balog ISLA, University of Amsterdam Kruislaan 403, 1098 SJ Amsterdam, The Netherlands kbalog@science.uva.nl Toine Bogers ILK, Tilburg University P.O.",
                "Box 90153, 5000 LE Tilburg, The Netherlands A.M.Bogers@uvt.nl Leif Azzopardi Dept. of Computing Science University of Glasgow, Glasgow, G12 8QQ leif@dcs.gla.ac.uk Maarten de Rijke ISLA, University of Amsterdam Kruislaan 403, 1098 SJ Amsterdam, The Netherlands mdr@science.uva.nl Antal van den Bosch ILK, Tilburg University P.O.",
                "Box 90153, 5000 LE Tilburg, The Netherlands Antal.vdnBosch@uvt.nl ABSTRACT Expertise retrieval has been largely unexplored on data other than the W3C collection.",
                "At the same time, many intranets of universities and other knowledge-intensive organisations offer examples of relatively small but clean multilingual expertise data, covering broad ranges of expertise areas.",
                "We first present two main expertise retrieval tasks, along with a set of baseline approaches based on generative language modeling, aimed at finding expertise relations between topics and people.",
                "For our experimental evaluation, we introduce (and release) a new test set based on a crawl of a university site.",
                "Using this test set, we conduct two series of experiments.",
                "The first is aimed at determining the effectiveness of baseline expertise retrieval methods applied to the new test set.",
                "The second is aimed at assessing refined models that exploit characteristic features of the new test set, such as the organizational structure of the university, and the hierarchical structure of the topics in the test set.",
                "Expertise retrieval models are shown to be robust with respect to environments smaller than the W3C collection, and current techniques appear to be generalizable to other settings.",
                "Categories and Subject Descriptors H.3 [Information Storage and Retrieval]: H.3.1 Content Analysis and Indexing; H.3.3 Information Search and Retrieval; H.3.4 Systems and Software; H.4 [Information Systems Applications]: H.4.2 Types of Systems; H.4.m Miscellaneous General Terms Algorithms, Measurement, Performance, Experimentation 1.",
                "INTRODUCTION An organizations intranet provides a means for exchanging information between employees and for facilitating employee collaborations.",
                "To efficiently and effectively achieve this, it is necessary to provide search facilities that enable employees not only to access documents, but also to identify expert colleagues.",
                "At the TREC Enterprise Track [22] the need to study and understand expertise retrieval has been recognized through the introduction of Expert Finding tasks.",
                "The goal of expert finding is to identify a list of people who are knowledgeable about a given topic.",
                "This task is usually addressed by uncovering associations between people and topics [10]; commonly, a co-occurrence of the name of a person with topics in the same context is assumed to be evidence of expertise.",
                "An alternative task, which using the same idea of people-topic associations, is expert profiling, where the task is to return a list of topics that a person is knowledgeable about [3].",
                "The launch of the Expert Finding task at TREC has generated a lot of interest in expertise retrieval, with rapid progress being made in terms of modeling, algorithms, and evaluation aspects.",
                "However, nearly all of the expert finding or profiling work performed has been validated experimentally using the W3C collection [24] from the Enterprise Track.",
                "While this collection is currently the only publicly available test collection for expertise retrieval tasks, it only represents one type of intranet.",
                "With only one test collection it is not possible to generalize conclusions to other realistic settings.",
                "In this paper we focus on expertise retrieval in a realistic setting that differs from the W3C setting-one in which relatively small amounts of clean, multilingual data are available, that cover a broad range of expertise areas, as can be found on the intranets of universities and other knowledge-intensive organizations.",
                "Typically, this setting features several additional types of structure: topical structure (e.g., topic hierarchies as employed by the organization), organizational structure (faculty, department, ...), as well as multiple types of documents (research and course descriptions, publications, and academic homepages).",
                "This setting is quite different from the W3C setting in ways that might impact upon the performance of expertise retrieval tasks.",
                "We focus on a number of research questions in this paper: Does the relatively small amount of data available on an intranet affect the quality of the topic-person associations that lie at the heart of expertise retrieval algorithms?",
                "How do state-of-the-art algorithms developed on the W3C data set perform in the alternative scenario of the type described above?",
                "More generally, do the lessons from the Expert Finding task at TREC carry over to this setting?",
                "How does the inclusion or exclusion of different documents affect expertise retrieval tasks?",
                "In addition to, how can the topical and organizational structure be used for retrieval purposes?",
                "To answer our research questions, we first present a set of baseline approaches, based on generative language modeling, aimed at finding associations between topics and people.",
                "This allows us to formulate the expert finding and expert profiling tasks in a uniform way, and has the added benefit of allowing us to understand the relations between the two tasks.",
                "For our experimental evaluation, we introduce a new data set (the UvT Expert Collection) which is representative of the type of intranet that we described above.",
                "Our collection is based on publicly available data, crawled from the website of Tilburg University (UvT).",
                "This type of data is particularly interesting, since (1) it is clean, heterogeneous, structured, and focused, but comprises a limited number of documents; (2) contains information on the organizational hierarchy; (3) it is bilingual (English and Dutch); and (4) the list of expertise areas of an individual are provided by the employees themselves.",
                "Using the UvT Expert collection, we conduct two sets of experiments.",
                "The first is aimed at determining the effectiveness of baseline expertise finding and profiling methods in this new setting.",
                "A second group of experiments is aimed at extensions of the baseline methods that exploit characteristic features of the UvT Expert Collection; specifically, we propose and evaluate refined expert finding and profiling methods that incorporate topicality and organizational structure.",
                "Apart from the research questions and data set that we contribute, our main contributions are as follows.",
                "The baseline models developed for expertise finding perform well on the new data set.",
                "While on the W3C setting the expert finding task appears to be more difficult than profiling, for the UvT data the opposite is the case.",
                "We find that profiling on the UvT data set is considerably more difficult than on the W3C set, which we believe is due to the large (but realistic) number of topical areas that we used for profiling: about 1,500 for the UvT set, versus 50 in the W3C case.",
                "Taking the similarity between topics into account can significantly improve retrieval performance.",
                "The best performing similarity measures are content-based, therefore they can be applied on the W3C (and other) settings as well.",
                "Finally, we demonstrate that the organizational structure can be exploited in the form of a context model, improving MAP scores for certain models by up to 70%.",
                "The remainder of this paper is organized as follows.",
                "In the next section we review related work.",
                "Then, in Section 3 we provide detailed descriptions of the expertise retrieval tasks that we address in this paper: expert finding and expert profiling.",
                "In Section 4 we present our baseline models, of which the performance is then assessed in Section 6 using the UvT data set that we introduce in Section 5.",
                "Advanced models exploiting specific features of our data are presented in Section 7 and evaluated in Section 8.",
                "We formulate our conclusions in Section 9. 2.",
                "RELATED WORK Initial approaches to expertise finding often employed databases containing information on the skills and knowledge of each individual in the organization [11].",
                "Most of these tools (usually called yellow pages or people-finding systems) rely on people to self-assess their skills against a predefined set of keywords.",
                "For updating profiles in these systems in an automatic fashion there is a need for intelligent technologies [5].",
                "More recent approaches use specific document sets (such as email [6] or software [18]) to find expertise.",
                "In contrast with focusing on particular document types, there is also an increased interest in the development of systems that index and mine published intranet documents as sources of evidence for expertise.",
                "One such published approach is the P@noptic system [9], which builds a representation of each person by concatenating all documents associated with that person-this is similar to Model 1 of Balog et al. [4], who formalize and compare two methods.",
                "Balog et al.s Model 1 directly models the knowledge of an expert from associated documents, while their Model 2 first locates documents on the topic and then finds the associated experts.",
                "In the reported experiments the second method performs significantly better when there are sufficiently many associated documents per candidate.",
                "Most systems that took part in the 2005 and 2006 editions of the Expert Finding task at TREC implemented (variations on) one of these two models; see [10, 20].",
                "Macdonald and Ounis [16] propose a different approach for ranking candidate expertise with respect to a topic based on data fusion techniques, without using collectionspecific heuristics; they find that applying field-based weighting models improves the ranking of candidates.",
                "Petkova and Croft [19] propose yet another approach, based on a combination of the above Model 1 and 2, explicitly modeling topics.",
                "Turning to other expert retrieval tasks that can also be addressed using topic-people associations, Balog and de Rijke [3] addressed the task of determining topical expert profiles.",
                "While their methods proved to be efficient on the W3C corpus, they require an amount of data that may not be available in the typical knowledge-intensive organization.",
                "Balog and de Rijke [2] study the related task of finding experts that are similar to a small set of experts given as input.",
                "As an aside, creating a textual summary of a person shows some similarities to biography finding, which has received a considerable amount of attention recently; see e.g., [13].",
                "We use generative language modeling to find associations between topics and people.",
                "In our modeling of expert finding and profiling we collect evidence for expertise from multiple sources, in a heterogeneous collection, and integrate it with the co-occurrence of candidates names and query terms-the language modeling setting allows us to do this in a transparent manner.",
                "Our modeling proceeds in two steps.",
                "In the first step, we consider three baseline models, two taken from [4] (the Models 1 and 2 mentioned above), and one a refined version of a model introduced in [3] (which we refer to as Model 3 below); this third model is also similar to the model described by Petkova and Croft [19].",
                "The models we consider in our second round of experiments are mixture models similar to contextual language models [1] and to the expanded documents of Tao et al. [21]; however, the features that we use for definining our expansions-including topical structure and organizational structure-have not been used in this way before. 3.",
                "TASKS In the expertise retrieval scenario that we envisage, users seeking expertise within an organization have access to an interface that combines a search box (where they can search for experts or topics) with navigational structures (of experts and of topics) that allows them to click their way to an expert page (providing the profile of a person) or a topic page (providing a list of experts on the topic).",
                "To feed the above interface, we face two expertise retrieval tasks, expert finding and expert profiling, that we first define and then formalize using generative language models.",
                "In order to model either task, the probability of the query topic being associated to a candidate expert plays a key role in the final estimates for searching and profiling.",
                "By using language models, both the candidates and the query are characterized by distributions of terms in the vocabulary (used in the documents made available by the organization whose expertise retrieval needs we are addressing). 3.1 Expert finding Expert finding involves the task of finding the right person with the appropriate skills and knowledge: Who are the experts on topic X?.",
                "E.g., an employee wants to ascertain who worked on a particular project to find out why particular decisions were made without having to trawl through documentation (if there is any).",
                "Or, they may be in need a trained specialist for consultancy on a specific problem.",
                "Within an organization there are usually many possible candidates who could be experts for given topic.",
                "We can state this problem as follows: What is the probability of a candidate ca being an expert given the query topic q?",
                "That is, we determine p(ca|q), and rank candidates ca according to this probability.",
                "The candidates with the highest probability given the query are deemed the most likely experts for that topic.",
                "The challenge is how to estimate this probability accurately.",
                "Since the query is likely to consist of only a few terms to describe the expertise required, we should be able to obtain a more accurate estimate by invoking Bayes Theorem, and estimating: p(ca|q) = p(q|ca)p(ca) p(q) , (1) where p(ca) is the probability of a candidate and p(q) is the probability of a query.",
                "Since p(q) is a constant, it can be ignored for ranking purposes.",
                "Thus, the probability of a candidate ca being an expert given the query q is proportional to the probability of a query given the candidate p(q|ca), weighted by the a priori belief p(ca) that candidate ca is an expert. p(ca|q) ∝ p(q|ca)p(ca) (2) In this paper our main focus is on estimating the probability of a query given the candidate p(q|ca), because this probability captures the extent to which the candidate knows about the query topic.",
                "Whereas the candidate priors are generally assumed to be uniformand thus will not influence the ranking-it has been demonstrated that a sensible choice of priors may improve the performance [20]. 3.2 Expert profiling While the task of expert searching was concerned with finding experts given a particular topic, the task of expert profiling seeks to answer a related question: What topics does a candidate know about?",
                "Essentially, this turns the questions of expert finding around.",
                "The profiling of an individual candidate involves the identification of areas of skills and knowledge that they have expertise about and an evaluation of the level of proficiency in each of these areas.",
                "This is the candidates topical profile.",
                "Generally, topical profiles within organizations consist of tabular structures which explicitly catalogue the skills and knowledge of each individual in the organization.",
                "However, such practice is limited by the resources available for defining, creating, maintaining, and updating these profiles over time.",
                "By focusing on automatic methods which draw upon the available evidence within the document repositories of an organization, our aim is to reduce the human effort associated with the maintenance of topical profiles1 .",
                "A topical profile of a candidate, then, is defined as a vector where each element i of the vector corresponds to the candidate cas expertise on a given topic ki, (i.e., s(ca, ki)).",
                "Each topic ki defines a particular knowledge area or skill that the organization uses to define the candidates topical profile.",
                "Thus, it is assumed that a list of topics, {k1, . . . , kn}, where n is the number of pre-defined topics, is given: profile(ca) = s(ca, k1), s(ca, k2), . . . , s(ca, kn) . (3) 1 Context and evidence are needed to help users of expertise finding systems to decide whom to contact when seeking expertise in a particular area.",
                "Examples of such context are: Who does she work with?",
                "What are her contact details?",
                "Is she well-connected, just in case she is not able to help us herself?",
                "What is her role in the organization?",
                "Who is her superior?",
                "Collaborators, and affiliations, etc. are all part of the candidates social profile, and can serve as a background against which the systems recommendations should be interpreted.",
                "In this paper we only address the problem of determining topical profiles, and leave social profiling to further work.",
                "We state the problem of quantifying the competence of a person on a certain knowledge area as follows: What is the probability of a knowledge area (ki) being part of the candidates (expertise) profile? where s(ca, ki) is defined by p(ki|ca).",
                "Our task, then, is to estimate p(ki|ca), which is equivalent to the problem of obtaining p(q|ca), where the topic ki is represented as a query topic q, i.e., a sequence of keywords representing the expertise required.",
                "Both the expert finding and profiling tasks rely on the accurate estimation of p(q|ca).",
                "The only difference derives from the prior probability that a person is an expert (p(ca)), which can be incorporated into the expert finding task.",
                "This prior does not apply to the profiling task since the candidate (individual) is fixed. 4.",
                "BASELINE MODELS In this section we describe our baseline models for estimating p(q|ca), i.e., associations between topics and people.",
                "Both expert finding and expert profiling boil down to this estimation.",
                "We employ three models for calculating this probability. 4.1 From topics to candidates Using Candidate Models: Model 1 Model 1 [4] defines the probability of a query given a candidate (p(q|ca)) using standard language modeling techniques, based on a multinomial unigram language model.",
                "For each candidate ca, a candidate language model θca is inferred such that the probability of a term given θca is nonzero for all terms, i.e., p(t|θca) > 0.",
                "From the candidate model the query is generated with the following probability: p(q|θca) = Y t∈q p(t|θca)n(t,q) , where each term t in the query q is sampled identically and independently, and n(t, q) is the number of times t occurs in q.",
                "The candidate language model is inferred as follows: (1) an empirical model p(t|ca) is computed; (2) it is smoothed with background probabilities.",
                "Using the associations between a candidate and a document, the probability p(t|ca) can be approximated by: p(t|ca) = X d p(t|d)p(d|ca), where p(d|ca) is the probability that candidate ca generates a supporting document d, and p(t|d) is the probability of a term t occurring in the document d. We use the maximum-likelihood estimate of a term, that is, the normalised frequency of the term t in document d. The strength of the association between document d and candidate ca expressed by p(d|ca) reflects the degree to which the candidates expertise is described using this document.",
                "The estimation of this probability is presented later, in Section 4.2.",
                "The candidate model is then constructed as a linear interpolation of p(t|ca) and the background model p(t) to ensure there are no zero probabilities, which results in the final estimation: p(q|θca) = (4) Y t∈q ( (1 − λ) X d p(t|d)p(d|ca) ! + λp(t) )n(t,q) .",
                "Model 1 amasses all the term information from all the documents associated with the candidate, and uses this to represent that candidate.",
                "This model is used to predict how likely a candidate would produce a query q.",
                "This can can be intuitively interpreted as the probability of this candidate talking about the query topic, where we assume that this is indicative of their expertise.",
                "Using Document Models: Model 2 Model 2 [4] takes a different approach.",
                "Here, the process is broken into two parts.",
                "Given a candidate ca, (1) a document that is associated with a candidate is selected with probability p(d|ca), and (2) from this document a query q is generated with probability p(q|d).",
                "Then the sum over all documents is taken to obtain p(q|ca), such that: p(q|ca) = X d p(q|d)p(d|ca). (5) The probability of a query given a document is estimated by inferring a document language model θd for each document d in a similar manner as the candidate model was inferred: p(t|θd) = (1 − λ)p(t|d) + λp(t), (6) where p(t|d) is the probability of the term in the document.",
                "The probability of a query given the document model is: p(q|θd) = Y t∈q p(t|θd)n(t,q) .",
                "The final estimate of p(q|ca) is obtained by substituting p(q|d) for p(q|θd) into Eq. 5 (see [4] for full details).",
                "Conceptually, Model 2 differs from Model 1 because the candidate is not directly modeled.",
                "Instead, the document acts like a hidden variable in the process which separates the query from the candidate.",
                "This process is akin to how a user may search for candidates with a standard search engine: initially by finding the documents which are relevant, and then seeing who is associated with that document.",
                "By examining a number of documents the user can obtain an idea of which candidates are more likely to discuss the topic q.",
                "Using Topic Models: Model 3 We introduce a third model, Model 3.",
                "Instead of attempting to model the query generation process via candidate or document models, we represent the query as a topic language model and directly estimate the probability of the candidate p(ca|q).",
                "This approach is similar to the model presented in [3, 19].",
                "As with the previous models, a language model is inferred, but this time for the query.",
                "We adapt the work of Lavrenko and Croft [14] to estimate a topic model from the query.",
                "The procedure is as follows.",
                "Given a collection of documents and a query topic q, it is assumed that there exists an unknown topic model θk that assigns probabilities p(t|θk) to the term occurrences in the topic documents.",
                "Both the query and the documents are samples from θk (as opposed to the previous approaches, where a query is assumed to be sampled from a specific document or candidate model).",
                "The main task is to estimate p(t|θk), the probability of a term given the topic model.",
                "Since the query q is very sparse, and as there are no examples of documents on the topic, this distribution needs to be approximated.",
                "Lavrenko and Croft [14] suggest a reasonable way of obtaining such an approximation, by assuming that p(t|θk) can be approximated by the probability of term t given the query q.",
                "We can then estimate p(t|q) using the joint probability of observing the term t together with the query terms, q1, . . . , qm, and dividing by the joint probability of the query terms: p(t|θk) ≈ p(t|q) = p(t, q1, . . . , qm) p(q1, . . . , qm) = p(t, q1, . . . , qm) P t ∈T p(t , q1, . . . , qm) , where p(q1, . . . , qm) = P t ∈T p(t , q1, . . . , qm), and T is the entire vocabulary of terms.",
                "In order to estimate the joint probability p(t, q1, . . . , qm), we follow [14, 15] and assume t and q1, . . . , qm are mutually independent, once we pick a source distribution from the set of underlying source distributions U.",
                "If we choose U to be a set of document models. then to construct this set, the query q would be issued against the collection, and the top n returned are assumed to be relevant to the topic, and thus treated as samples from the topic model. (Note that candidate models could be used instead.)",
                "With the document models forming U, the joint probability of term and query becomes: p(t, q1, . . . , qm) = X d∈U p(d) ˘ p(t|θd) mY i=1 p(qi|θd) ¯ . (7) Here, p(d) denotes the prior distribution over the set U, which reflects the relevance of the document to the topic.",
                "We assume that p(d) is uniform across U.",
                "In order to rank candidates according to the topic model defined, we use the Kullback-Leibler divergence metric (KL, [8]) to measure the difference between the candidate models and the topic model: KL(θk||θca) = X t p(t|θk) log p(t|θk) p(t|θca) . (8) Candidates with a smaller divergence from the topic model are considered to be more likely experts on that topic.",
                "The candidate model θca is defined in Eq. 4.",
                "By using KL divergence instead of the probability of a candidate given the topic model p(ca|θk), we avoid normalization problems. 4.2 Document-candidate associations For our models we need to be able to estimate the probability p(d|ca), which expresses the extent to which a document d characterizes the candidate ca.",
                "In [4], two methods are presented for estimating this probability, based on the number of person names recognized in a document.",
                "However, in our (intranet) setting it is reasonable to assume that authors of documents can unambiguously be identified (e.g., as the author of an article, the teacher assigned to a course, the owner of a web page, etc.)",
                "Hence, we set p(d|ca) to be 1 if candidate ca is author of document d, otherwise the probability is 0.",
                "In Section 6 we describe how authorship can be determined on different types of documents within the collection. 5.",
                "THE UVT EXPERT COLLECTION The UvT Expert collection used in the experiments in this paper fits the scenario outlined in Section 3.",
                "The collection is based on the Webwijs (Webwise) system developed at Tilburg University (UvT) in the Netherlands.",
                "Webwijs (http://www.uvt.nl/ webwijs/) is a publicly accessible database of UvT employees who are involved in research or teaching; currently, Webwijs contains information about 1168 experts, each of whom has a page with contact information and, if made available by the expert, a research description and publications list.",
                "In addition, each expert can select expertise areas from a list of 1491 topics and is encouraged to suggest new topics that need to be approved by the Webwijs editor.",
                "Each topic has a separate page that shows all experts associated with that topic and, if available, a list of related topics.",
                "Webwijs is available in Dutch and English, and this bilinguality has been preserved in the collection.",
                "Every Dutch Webwijs page has an English translation.",
                "Not all Dutch topics have an English translation, but the reverse is true: the 981 English topics all have a Dutch equivalent.",
                "About 42% of the experts teach courses at Tilburg University; these courses were also crawled and included in the profile.",
                "In addition, about 27% of the experts link to their academic homepage from their Webwijs page.",
                "These home pages were crawled and added to the collection. (This means that if experts put the full-text versions of their publications on their academic homepage, these were also available for indexing.)",
                "We also obtained 1880 full-text versions of publications from the UvT institutional repository and Dutch English no. of experts 1168 1168 no. of experts with ≥ 1 topic 743 727 no. of topics 1491 981 no. of expert-topic pairs 4318 3251 avg. no. of topics/expert 5.8 5.9 max. no. of topics/expert (no. of experts) 60 (1) 35 (1) min. no. of topics/expert (no. of experts) 1 (74) 1 (106) avg. no. of experts/topic 2.9 3.3 max. no. of experts/topic (no. of topics) 30 (1) 30 (1) min. no. of experts/topic (no. of topics) 1 (615) 1 (346) no. of experts with HP 318 318 no. of experts with CD 318 318 avg. no. of CDs per teaching expert 3.5 3.5 no. of experts with RD 329 313 no. of experts with PUB 734 734 avg. no. of PUBs per expert 27.0 27.0 avg. no. of PUB citations per expert 25.2 25.2 avg. no. of full-text PUBs per expert 1.8 1.8 Table 2: Descriptive statistics of the Dutch and English versions of the UvT Expert collection. converted them to plain text.",
                "We ran the TextCat [23] language identifier to classify the language of the home pages and the fulltext publications.",
                "We restricted ourselves to pages where the classifier was confident about the language used on the page.",
                "This resulted in four document types: research descriptions (RD), course descriptions (CD), publications (PUB; full-text and citationonly versions), and academic homepages (HP).",
                "Everything was bundled into the UvT Expert collection which is available at http: //ilk.uvt.nl/uvt-expert-collection/.",
                "The UvT Expert collection was extracted from a different organizational setting than the W3C collection and differs from it in a number of ways.",
                "The UvT setting is one with relatively small amounts of multilingual data.",
                "Document-author associations are clear and the data is structured and clean.",
                "The collection covers a broad range of expertise areas, as one can typically find on intranets of universities and other knowledge-intensive institutes.",
                "Additionally, our university setting features several types of structure (topical and organizational), as well as multiple document types.",
                "Another important difference between the two data sets is that the expertise areas in the UvT Expert collection are self-selected instead of being based on group membership or assignments by others.",
                "Size is another dimension along which the W3C and UvT Expert collections differ: the latter is the smaller of the two.",
                "Also realistic are the large differences in the amount of information available for each expert.",
                "Utilizing Webwijs is voluntary; 425 Dutch experts did not select any topics at all.",
                "This leaves us with 743 Dutch and 727 English usable expert profiles.",
                "Table 2 provides descriptive statistics for the UvT Expert collection.",
                "Universities tend to have a hierarchical structure that goes from the faculty level, to departments, research groups, down to the individual researchers.",
                "In the UvT Expert collection we have information about the affiliations of researchers with faculties and institutes, providing us with a two-level organizational hierarchy.",
                "Tilburg University has 22 organizational units at the faculty level (including the university office and several research institutes) and 71 departments, which amounts to 3.2 departments per faculty.",
                "As to the topical hierarchy used by Webwijs, 131 of the 1491 topics are top nodes in the hierarchy.",
                "This hierarchy has an average topic chain length of 2.65 and a maximum length of 7 topics. 6.",
                "EVALUATION Below, we evaluate Section 4s models for expert finding and profiling onthe UvT Expert collection.",
                "We detail our research questions and experimental setup, and then present our results. 6.1 Research Questions We address the following research questions.",
                "Both expert finding and profiling rely on the estimations of p(q|ca).",
                "The question is how the models compare on the different tasks, and in the setting of the UvT Expert collection.",
                "In [4], Model 2 outperformed Model 1 on the W3C collection.",
                "How do they compare on our data set?",
                "And how does Model 3 compare to Model 1?",
                "What about performance differences between the two languages in our test collection? 6.2 Experimental Setup The output of our models was evaluated against the self-assigned topic labels, which were treated as relevance judgements.",
                "Results were evaluated separately for English and Dutch.",
                "For English we only used topics for which the Dutch translation was available; for Dutch all topics were considered.",
                "The results were averaged for the queries in the intersection of relevance judgements and results; missing queries do not contribute a value of 0 to the scores.",
                "We use standard information retrieval measures, such as Mean Average Precision (MAP) and Mean Reciprocal Rank (MRR).",
                "We also report the percentage of topics (%q) and candidates (%ca) covered, for the expert finding and profiling tasks, respectively. 6.3 Results Table 1 shows the performance of Model 1, 2, and 3 on the expert finding and profiling tasks.",
                "The rows of the table correspond to the various document types (RD, CD, PUB, and HP) and to their combinations.",
                "RD+CD+PUB+HP is equivalent to the full collection and will be referred as the BASELINE of our experiments.",
                "Looking at Table 1 we see that Model 2 performs the best across the board.",
                "However, when the data is clean and very focused (RD), Model 3 outperforms it in a number of cases.",
                "Model 1 has the best coverage of candidates (%ca) and topics (%q).",
                "The various document types differ in their characteristics and how they improve the finding and profiling tasks.",
                "Expert profiling benefits much from the clean data present in the RD and CD document types, while the publications contribute the most to the expert finding task.",
                "Adding the homepages does not prove to be particularly useful.",
                "When we compare the results across languages, we find that the coverage of English topics (%q) is higher than of the Dutch ones for expert finding.",
                "Apart from that, the scores fall in the same range for both languages.",
                "For the profiling task the coverage of the candidates (%ca) is very similar for both languages.",
                "However, the performance is substantially better for the English topics.",
                "While it is hard to compare scores across collections, we conclude with a brief comparison of the absolute scores in Table 1 to those reported in [3, 4] on the W3C test set (2005 edition).",
                "For expert finding the MAP scores for Model 2 reported here are about 50% higher than the corresponding figures in [4], while our MRR scores are slightly below those in [4].",
                "For expert profiling, the differences are far more dramatic: the MAP scores for Model 2 reported here are around 50% below the scores in [3], while the (best) MRR scores are about the same as those in [3].",
                "The cause for the latter differences seems to reside in the number of knowledge areas considered here-approx. 30 times more than in the W3C setting. 7.",
                "ADVANCED MODELS Now that we have developed and assessed basic language modeling techniques for expertise retrieval, we turn to refined models that exploit special features of our test collection. 7.1 Exploiting knowledge area similarity One way to improve the scoring of a query given a candidate is to consider what other requests the candidate would satisfy and use them as further evidence to support the original query, proportional Expert finding Expert profiling Document types Model 1 Model 2 Model 3 Model 1 Model 2 Model 3 %q MAP MRR %q MAP MRR %q MAP MRR %ca MAP MRR %ca MAP MRR %ca MAP MRR English RD 97.8 0.126 0.269 83.5 0.144 0.311 83.3 0.129 0.271 100 0.089 0.189 39.3 0.232 0.465 41.1 0.166 0.337 CD 97.8 0.118 0.227 91.7 0.123 0.248 91.7 0.118 0.226 32.8 0.188 0.381 32.4 0.195 0.385 32.7 0.203 0.370 PUB 97.8 0.200 0.330 98.0 0.216 0.372 98.0 0.145 0.257 78.9 0.167 0.364 74.5 0.212 0.442 78.9 0.135 0.299 HP 97.8 0.081 0.186 97.4 0.071 0.168 97.2 0.062 0.149 31.2 0.150 0.299 28.8 0.185 0.335 30.1 0.136 0.287 RD+CD 97.8 0.188 0.352 92.9 0.193 0.360 92.9 0.150 0.273 100 0.145 0.286 61.3 0.251 0.477 63.2 0.217 0.416 RD+CD+PUB 97.8 0.235 0.373 98.1 0.277 0.439 98.1 0.178 0.305 100 0.196 0.380 87.2 0.280 0.533 89.5 0.170 0.344 RD+CD+PUB+HP 97.8 0.237 0.372 98.6 0.280 0.441 98.5 0.166 0.293 100 0.199 0.387 88.7 0.281 0.525 90.9 0.169 0.329 Dutch RD 61.3 0.094 0.229 38.4 0.137 0.336 38.3 0.127 0.295 38.0 0.127 0.386 34.1 0.138 0.420 38.0 0.105 0.327 CD 61.3 0.107 0.212 49.7 0.128 0.256 49.7 0.136 0.261 32.5 0.151 0.389 31.8 0.158 0.396 32.5 0.170 0.380 PUB 61.3 0.193 0.319 59.5 0.218 0.368 59.4 0.173 0.291 78.8 0.126 0.364 76.0 0.150 0.424 78.8 0.103 0.294 HP 61.3 0.063 0.169 56.6 0.064 0.175 56.4 0.062 0.163 29.8 0.108 0.308 27.8 0.125 0.338 29.8 0.098 0.255 RD+CD 61.3 0.159 0.314 51.9 0.184 0.360 51.9 0.169 0.324 60.5 0.151 0.410 57.2 0.166 0.431 60.4 0.159 0.384 RD+CD+PUB 61.3 0.244 0.398 61.5 0.260 0.424 61.4 0.210 0.350 90.3 0.165 0.445 88.2 0.189 0.479 90.3 0.126 0.339 RD+CD+PUB+HP 61.3 0.249 0.401 62.6 0.265 0.436 62.6 0.195 0.344 91.9 0.164 0.426 90.1 0.195 0.488 91.9 0.125 0.328 Table 1: Performance of the models on the expert finding and profiling tasks, using different document types and their combinations. %q is the number of topics covered (applies to the expert finding task), %ca is the number of candidates covered (applies to the expert profiling task).",
                "The top and bottom blocks correspond to English and Dutch respectively.",
                "The best scores are in boldface. to how related the other requests are to the original query.",
                "This can be modeled by interpolating between the p(q|ca) and the further supporting evidence from all similar requests q , as follows: p (q|ca) = λp(q|ca) + (1 − λ) X q p(q|q )p(q |ca), (9) where p(q|q ) represents the similarity between the two topics q and q .",
                "To be able to work with similarity methods that are not necessarily probabilities, we set p(q|q ) = w(q,q ) γ , where γ is a normalizing constant, such that γ = P q w(q , q ).",
                "We consider four methods for calculating the similarity score between two topics.",
                "Three approaches are strictly content-based, and establish similarity by examining co-occurrence patterns of topics within the collection, while the last approach exploits the hierarchical structure of topical areas that may be present within an organization (see [7] for further examples of integrating word relationships into language models).",
                "The Kullback-Leibler (KL) divergence metric defined in Eq. 8 provides a measure of how different or similar two probability distributions are.",
                "A topic model is inferred for q and q using the method presented in Section 4.1 to describe the query across the entire vocabulary.",
                "Since a lower KL score means the queries are more similar, we let w(q, q ) = max(KL(θq||·) − KL(θq||θq )).",
                "Pointwise Mutual Information (PMI, [17]) is a measure of association used in information theory to determine the extent of independence between variables.",
                "The dependence between two queries is reflected by the SI(q, q ) score, where scores greater than zero indicate that it is likely that there is a dependence, which we take to mean that the queries are likely to be similar: SI(q, q ) = log p(q, q ) p(q)p(q ) (10) We estimate the probability of a topic p(q) using the number of documents relevant to query q within the collection.",
                "The joint probability p(q, q ) is estimated similarly, by using the concatenation of q and q as a query.",
                "To obtain p(q|q ), we then set w(q, q ) = SI(q, q ) when SI(q, q ) > 0 otherwise w(q, q ) = 0, because we are only interested in including queries that are similar.",
                "The log-likelihood statistic provides another measure of dependence, which is more reliable than the pointwise mutual information measure [17].",
                "Let k1 be the number of co-occurrences of q and q , k2 the number of occurrences of q not co-occurring with q , n1 the total number of occurrences of q , and n2 the total number of topic tokens minus the number of occurrences of q .",
                "Then, let p1 = k1/n1, p2 = k2/n2, and p = (k1 + k2)/(n1 + n2), (q, q ) = 2( (p1, k1, n1) + (p2, k2, n2) − (p, k1, n1) − (p, k2, n2)), where (p, n, k) = k log p + (n − k) log(1 − p).",
                "The higher score indicate that queries are also likely to be similar, thus we set w(q, q ) = (q, q ).",
                "Finally, we also estimate the similarity of two topics based on their distance within the topic hierarchy.",
                "The topic hierarchy is viewed as a directed graph, and for all topic-pairs the shortest path SP(q, q ) is calculated.",
                "We set the similarity score to be the reciprocal of the shortest path: w(q, q ) = 1/SP(q, q ). 7.2 Contextual information Given the hierarchy of an organization, the units to which a person belong are regarded as a context so as to compensate for data sparseness.",
                "We model it as follows: p (q|ca) = 1 − P ou∈OU(ca) λou · p(q|ca) + P ou∈OU(ca) λou · p(q|ou), where OU(ca) is the set of organizational units of which candidate ca is a member of, and p(q|o) expresses the strength of the association between query q and the unit ou.",
                "The latter probability can be estimated using either of the three basic models, by simply replacing ca with ou in the corresponding equations.",
                "An organizational unit is associated with all the documents that its members have authored.",
                "That is, p(d|ou) = maxca∈ou p(d|ca). 7.3 A simple multilingual model For knowledge institutes in Europe, academic or otherwise, a multilingual (or at least bilingual) setting is typical.",
                "The following model builds on a kind of independence assumption: there is no spill-over of expertise/profiles across language boundaries.",
                "While a simplification, this is a sensible first approach.",
                "That is: p (q|ca) =P l∈L λl · p(ql|ca), where L is the set of languages used in the collection, ql is the translation of the query q to language l, and λl is a language specific smoothing parameter, such that P l∈L λl = 1. 8.",
                "ADVANCED MODELS: EVALUATION In this section we present an experimental evaluation of our advanced models.",
                "Expert finding Expert profiling Language Model 1 Model 2 Model 3 Model 1 Model 2 Model 3 %q MAP MRR %q MAP MRR %q MAP MRR %ca MAP MRR %ca MAP MRR %ca MAP MRR English only 97.8 0.237 0.372 98.6 0.280 0.441 98.5 0.166 0.293 100 0.199 0.387 88.7 0.281 0.525 90.9 0.169 0.329 Dutch only 61.3 0.249 0.401 62.6 0.265 0.436 62.6 0.195 0.344 91.9 0.164 0.426 90.1 0.195 0.488 91.9 0.125 0.328 Combination 99.4 0.297 0.444 99.7 0.324 0.491 99.7 0.223 0.388 100 0.241 0.445 92.1 0.313 0.564 93.2 0.224 0.411 Table 3: Performance of the combination of languages on the expert finding and profiling tasks (on candidates).",
                "Best scores for each model are in italic, absolute best scores for the expert finding and profiling tasks are in boldface.",
                "Method Model 1 Model 2 Model 3 MAP MRR MAP MRR MAP MRR English BASELINE 0.296 0.454 0.339 0.509 0.221 0.333 KLDIV 0.291 0.453 0.327 0.503 0.219 0.330 PMI 0.291 0.453 0.337 0.509 0.219 0.331 LL 0.319 0.490 0.360 0.524 0.233 0.368 HDIST 0.299 0.465 0.346 0.537 0.219 0.332 Dutch BASELINE 0.240 0.350 0.271 0.403 0.227 0.389 KLDIV 0.239 0.347 0.253 0.386 0.224 0.385 PMI 0.239 0.350 0.260 0.392 0.227 0.389 LL 0.255 0.372 0.281 0.425 0.231 0.389 HDIST 0.253 0.365 0.271 0.407 0.236 0.402 Method Model 1 Model 2 Model 3 MAP MRR MAP MRR MAP MRR English BASELINE 0.485 0.546 0.499 0.548 0.381 0.416 KLDIV 0.510 0.564 0.513 0.558 0.381 0.416 PMI 0.486 0.546 0.495 0.542 0.407 0.451 LL 0.558 0.589 0.586 0.617 0.408 0.453 HDIST 0.507 0.567 0.512 0.563 0.386 0.420 Dutch BASELINE 0.263 0.313 0.294 0.358 0.262 0.315 KLDIV 0.284 0.336 0.271 0.321 0.261 0.314 PMI 0.265 0.317 0.265 0.316 0.273 0.330 LL 0.312 0.351 0.330 0.377 0.284 0.331 HDIST 0.280 0.327 0.288 0.341 0.266 0.321 Table 4: Performance on the expert finding (top) and profiling (bottom) tasks, using knowledge area similarities.",
                "Runs were evaluated on the main topics set.",
                "Best scores are in boldface. 8.1 Research Questions Our questions follow the refinements presented in the preceding section: Does exploiting the knowledge area similarity improve effectiveness?",
                "Which of the various methods for capturing word relationships is most effective?",
                "Furthermore, is our way of bringing in contextual information useful?",
                "For which tasks?",
                "And finally, is our simple way of combining the monolingual scores sufficient for obtaining significant improvements? 8.2 Experimental setup Given that the self-assessments are also sparse in our collection, in order to be able to measure differences between the various models, we selected a subset of topics, and evaluated (some of the) runs only on this subset.",
                "This set is referred as main topics, and consists of topics that are located at the top level of the topical hierarchy. (A main topic has subtopics, but is not a subtopic of any other topic.)",
                "This main set consists of 132 Dutch and 119 English topics.",
                "The relevance judgements were restricted to the main topic set, but were not expanded with subtopics. 8.3 Exploiting knowledge area similarity Table 4 presents the results.",
                "The four methods used for estimating knowledge-area similarity are KL divergence (KLDIV), PointLang.",
                "Topics Model 1 Model 2 Model 3 MAP MRR MAP MRR MAP MRR Expert finding UK ALL 0.423 0.545 0.654 0.799 0.494 0.629 UK MAIN 0.500 0.621 0.704 0.834 0.587 0.699 NL ALL 0.439 0.560 0.672 0.826 0.480 0.630 NL MAIN 0.440 0.584 0.645 0.816 0.515 0.655 Expert profiling UK ALL 0.240 0.640 0.306 0.778 0.223 0.616 UK MAIN 0.523 0.677 0.519 0.648 0.461 0.587 NL ALL 0.203 0.716 0.254 0.770 0.183 0.627 NL MAIN 0.332 0.576 0.380 0.624 0.332 0.549 Table 5: Evaluating the context models on organizational units. wise mutual information (PMI), log-likelihood (LL), and distance within topic hierarchy (HDIST).",
                "We managed to improve upon the baseline in all cases, but the improvement is more noticeable for the profiling task.",
                "For both tasks, the LL method performed best.",
                "The content-based approaches performed consistently better than HDIST. 8.4 Contextual information A two level hierarchy of organizational units (faculties and institutes) is available in the UvT Expert collection.",
                "The unit a person belongs to is used as a context for that person.",
                "First, we evaluated the models of the organizational units, using all topics (ALL) and only the main topics (MAIN).",
                "An organizational unit is considered to be relevant for a given topic (or vice versa) if at least one member of the unit selected the given topic as an expertise area.",
                "Table 5 reports on the results.",
                "As far as expert finding goes, given a topic, the corresponding organizational unit can be identified with high precision.",
                "However, the expert profiling task shows a different picture: the scores are low, and the task seems hard.",
                "The explanation may be that general concepts (i.e., our main topics) may belong to several organizational units.",
                "Second, we performed another evaluation, where we combined the contextual models with the candidate models (to score candidates again).",
                "Table 6 reports on the results.",
                "We find a positive impact of the context models only for expert finding.",
                "Noticably, for expert finding (and Model 1), it improves over 50% (for English) and over 70% (for Dutch) on MAP.",
                "The poor performance on expert profiling may be due to the fact that context models alone did not perform very well on the profiling task to begin with. 8.5 Multilingual models In this subsection we evaluate the method for combining results across multiple languages that we described in Section 7.3.",
                "In our setting the set of languages consists of English and Dutch: L = {UK, NL}.",
                "The weights on these languages were set to be identical (λUK = λNL = 0.5).",
                "We performed experiments with various λ settings, but did not observe significant differences in performance.",
                "Table 3 reports on the multilingual results, where performance is evaluated on the full topic set.",
                "All three models significantly imLang.",
                "Method Model 1 Model 2 Model 3 MAP MRR MAP MRR MAP MRR Expert finding UK BL 0.296 0.454 0.339 0.509 0.221 0.333 UK CT 0.330 0.491 0.342 0.500 0.228 0.342 NL BL 0.240 0.350 0.271 0.403 0.227 0.389 NL CT 0.251 0.382 0.267 0.410 0.246 0.404 Expert profiling UK BL 0.485 0.546 0.499 0.548 0.381 0.416 UK CT 0.562 0.620 0.508 0.558 0.440 0.486 NL BL 0.263 0.313 0.294 0.358 0.262 0.315 NL CT 0.330 0.384 0.317 0.387 0.294 0.345 Table 6: Performance of the context models (CT) compared to the baseline (BL).",
                "Best scores are in boldface. proved over all measures for both tasks.",
                "The coverage of topics and candidates for the expert finding and profiling tasks, respectively, is close to 100% in all cases.",
                "The relative improvement of the precision scores ranges from 10% to 80%.",
                "These scores demonstrate that despite its simplicity, our method for combining results over multiple languages achieves substantial improvements over the baseline. 9.",
                "CONCLUSIONS In this paper we focused on expertise retrieval (expert finding and profiling) in a new setting of a typical knowledge-intensive organization in which the available data is of high quality, multilingual, and covering a broad range of expertise area.",
                "Typically, the amount of available data in such an organization (e.g., a university, a research institute, or a research lab) is limited when compared to the W3C collection that has mostly been used for the experimental evaluation of expertise retrieval so far.",
                "To examine expertise retrieval in this setting, we introduced (and released) the UvT Expert collection as a representative case of such knowledge intensive organizations.",
                "The new collection reflects the typical properties of knowledge-intensive institutes noted above and also includes several features which may are potentially useful for expertise retrieval, such as topical and organizational structure.",
                "We evaluated how current state-of-the-art models for expert finding and profiling performed in this new setting and then refined these models in order to try and exploit the different characteristics within the data environment (language, topicality, and organizational structure).",
                "We found that current models of expertise retrieval generalize well to this new environment; in addition we found that refining the models to account for the differences results in significant improvements, thus making up for problems caused by data sparseness issues.",
                "Future work includes setting up manual assessments of automatically generated profiles by the employees themselves, especially in cases where the employees have not provided a profile themselves. 10.",
                "ACKNOWLEDGMENTS Krisztian Balog was supported by the Netherlands Organisation for Scientific Research (NWO) under project number 220-80-001.",
                "Maarten de Rijke was also supported by NWO under project numbers 017.001.190, 220-80-001, 264-70-050, 354-20-005, 600.065.120, 612-13-001, 612.000.106, 612.066.302, 612.069.006, 640.001.501, 640.002.501, and by the E.U.",
                "IST programme of the 6th FP for RTD under project MultiMATCH contract IST-033104.",
                "The work of Toine Bogers and Antal van den Bosch was funded by the IOP-MMI-program of SenterNovem / The Dutch Ministry of Economic Affairs, as part of the `A Propos project. 11.",
                "REFERENCES [1] L. Azzopardi.",
                "Incorporating Context in the Language Modeling Framework for ad-hoc Information Retrieval.",
                "PhD thesis, University of Paisley, 2005. [2] K. Balog and M. de Rijke.",
                "Finding similar experts.",
                "In This volume, 2007. [3] K. Balog and M. de Rijke.",
                "Determining expert profiles (with an application to expert finding).",
                "In IJCAI 07: Proc. 20th Intern.",
                "Joint Conf. on Artificial Intelligence, pages 2657-2662, 2007. [4] K. Balog, L. Azzopardi, and M. de Rijke.",
                "Formal models for expert finding in enterprise corpora.",
                "In SIGIR 06: Proc. 29th annual intern.",
                "ACM SIGIR conf. on Research and development in information retrieval, pages 43-50, 2006. [5] I. Becerra-Fernandez.",
                "The role of artificial intelligence technologies in the implementation of people-finder knowledge management systems.",
                "In AAAI Workshop on Bringing Knowledge to Business Processes, March 2000. [6] C. S. Campbell, P. P. Maglio, A. Cozzi, and B. Dom.",
                "Expertise identification using email communications.",
                "In CIKM 03: Proc. twelfth intern. conf. on Information and knowledge management, pages 528531, 2003. [7] G. Cao, J.-Y.",
                "Nie, and J. Bai.",
                "Integrating word relationships into language models.",
                "In SIGIR 05: Proc. 28th annual intern.",
                "ACM SIGIR conf. on Research and development in information retrieval, pages 298-305, 2005. [8] T. M. Cover and J.",
                "A. Thomas.",
                "Elements of Information Theory.",
                "Wiley-Interscience, 1991. [9] N. Craswell, D. Hawking, A. M. Vercoustre, and P. Wilkins.",
                "P@noptic expert: Searching for experts not just for documents.",
                "In Ausweb, 2001. [10] N. Craswell, A. de Vries, and I. Soboroff.",
                "Overview of the TREC2005 Enterprise Track.",
                "In The Fourteenth Text REtrieval Conf.",
                "Proc. (TREC 2005), 2006. [11] T. H. Davenport and L. Prusak.",
                "Working Knowledge: How Organizations Manage What They Know.",
                "Harvard Business School Press, Boston, MA, 1998. [12] T. Dunning.",
                "Accurate methods for the statistics of surprise and coincidence.",
                "Computational Linguistics, 19(1):61-74, 1993. [13] E. Filatova and J. Prager.",
                "Tell me what you do and Ill tell you what you are: Learning occupation-related activities for biographies.",
                "In HLT/EMNLP, 2005. [14] V. Lavrenko and W. B. Croft.",
                "Relevance based language models.",
                "In SIGIR 01: Proc. 24th annual intern.",
                "ACM SIGIR conf. on Research and development in information retrieval, pages 120-127, 2001. [15] V. Lavrenko, M. Choquette, and W. B. Croft.",
                "Cross-lingual relevance models.",
                "In SIGIR 02: Proc. 25th annual intern.",
                "ACM SIGIR conf. on Research and development in information retrieval, pages 175-182, 2002. [16] C. Macdonald and I. Ounis.",
                "Voting for candidates: adapting data fusion techniques for an expert search task.",
                "In CIKM 06: Proc. 15th ACM intern. conf. on Information and knowledge management, pages 387-396, 2006. [17] C. Manning and H. Sch¨utze.",
                "Foundations of Statistical Natural Language Processing.",
                "The MIT Press, 1999. [18] A. Mockus and J. D. Herbsleb.",
                "Expertise browser: a quantitative approach to identifying expertise.",
                "In ICSE 02: Proc. 24th Intern.",
                "Conf. on Software Engineering, pages 503-512, 2002. [19] D. Petkova and W. B. Croft.",
                "Hierarchical language models for expert finding in enterprise corpora.",
                "In Proc.",
                "ICTAI 2006, pages 599-608, 2006. [20] I. Soboroff, A. de Vries, and N. Craswell.",
                "Overview of the TREC 2006 Enterprise Track.",
                "In TREC 2006 Working Notes, 2006. [21] T. Tao, X. Wang, Q. Mei, and C. Zhai.",
                "Language model information retrieval with document expansion.",
                "In HLT-NAACL 2006, 2006. [22] TREC.",
                "Enterprise track, 2005.",
                "URL: http://www.ins.cwi. nl/projects/trec-ent/wiki/. [23] G. van Noord.",
                "TextCat Language Guesser.",
                "URL: http://www. let.rug.nl/˜vannoord/TextCat/. [24] W3C.",
                "The W3C test collection, 2005.",
                "URL: http://research. microsoft.com/users/nickcr/w3c-summary.html."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [],
            "translated_text": "",
            "candidates": [],
            "error": [
                []
            ]
        },
        "expert find": {
            "translated_key": "",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Broad Expertise Retrieval in Sparse Data Environments Krisztian Balog ISLA, University of Amsterdam Kruislaan 403, 1098 SJ Amsterdam, The Netherlands kbalog@science.uva.nl Toine Bogers ILK, Tilburg University P.O.",
                "Box 90153, 5000 LE Tilburg, The Netherlands A.M.Bogers@uvt.nl Leif Azzopardi Dept. of Computing Science University of Glasgow, Glasgow, G12 8QQ leif@dcs.gla.ac.uk Maarten de Rijke ISLA, University of Amsterdam Kruislaan 403, 1098 SJ Amsterdam, The Netherlands mdr@science.uva.nl Antal van den Bosch ILK, Tilburg University P.O.",
                "Box 90153, 5000 LE Tilburg, The Netherlands Antal.vdnBosch@uvt.nl ABSTRACT Expertise retrieval has been largely unexplored on data other than the W3C collection.",
                "At the same time, many intranets of universities and other knowledge-intensive organisations offer examples of relatively small but clean multilingual expertise data, covering broad ranges of expertise areas.",
                "We first present two main expertise retrieval tasks, along with a set of baseline approaches based on generative language modeling, aimed at finding expertise relations between topics and people.",
                "For our experimental evaluation, we introduce (and release) a new test set based on a crawl of a university site.",
                "Using this test set, we conduct two series of experiments.",
                "The first is aimed at determining the effectiveness of baseline expertise retrieval methods applied to the new test set.",
                "The second is aimed at assessing refined models that exploit characteristic features of the new test set, such as the organizational structure of the university, and the hierarchical structure of the topics in the test set.",
                "Expertise retrieval models are shown to be robust with respect to environments smaller than the W3C collection, and current techniques appear to be generalizable to other settings.",
                "Categories and Subject Descriptors H.3 [Information Storage and Retrieval]: H.3.1 Content Analysis and Indexing; H.3.3 Information Search and Retrieval; H.3.4 Systems and Software; H.4 [Information Systems Applications]: H.4.2 Types of Systems; H.4.m Miscellaneous General Terms Algorithms, Measurement, Performance, Experimentation 1.",
                "INTRODUCTION An organizations intranet provides a means for exchanging information between employees and for facilitating employee collaborations.",
                "To efficiently and effectively achieve this, it is necessary to provide search facilities that enable employees not only to access documents, but also to identify expert colleagues.",
                "At the TREC Enterprise Track [22] the need to study and understand expertise retrieval has been recognized through the introduction of Expert Finding tasks.",
                "The goal of <br>expert find</br>ing is to identify a list of people who are knowledgeable about a given topic.",
                "This task is usually addressed by uncovering associations between people and topics [10]; commonly, a co-occurrence of the name of a person with topics in the same context is assumed to be evidence of expertise.",
                "An alternative task, which using the same idea of people-topic associations, is expert profiling, where the task is to return a list of topics that a person is knowledgeable about [3].",
                "The launch of the Expert Finding task at TREC has generated a lot of interest in expertise retrieval, with rapid progress being made in terms of modeling, algorithms, and evaluation aspects.",
                "However, nearly all of the <br>expert find</br>ing or profiling work performed has been validated experimentally using the W3C collection [24] from the Enterprise Track.",
                "While this collection is currently the only publicly available test collection for expertise retrieval tasks, it only represents one type of intranet.",
                "With only one test collection it is not possible to generalize conclusions to other realistic settings.",
                "In this paper we focus on expertise retrieval in a realistic setting that differs from the W3C setting-one in which relatively small amounts of clean, multilingual data are available, that cover a broad range of expertise areas, as can be found on the intranets of universities and other knowledge-intensive organizations.",
                "Typically, this setting features several additional types of structure: topical structure (e.g., topic hierarchies as employed by the organization), organizational structure (faculty, department, ...), as well as multiple types of documents (research and course descriptions, publications, and academic homepages).",
                "This setting is quite different from the W3C setting in ways that might impact upon the performance of expertise retrieval tasks.",
                "We focus on a number of research questions in this paper: Does the relatively small amount of data available on an intranet affect the quality of the topic-person associations that lie at the heart of expertise retrieval algorithms?",
                "How do state-of-the-art algorithms developed on the W3C data set perform in the alternative scenario of the type described above?",
                "More generally, do the lessons from the Expert Finding task at TREC carry over to this setting?",
                "How does the inclusion or exclusion of different documents affect expertise retrieval tasks?",
                "In addition to, how can the topical and organizational structure be used for retrieval purposes?",
                "To answer our research questions, we first present a set of baseline approaches, based on generative language modeling, aimed at finding associations between topics and people.",
                "This allows us to formulate the <br>expert find</br>ing and expert profiling tasks in a uniform way, and has the added benefit of allowing us to understand the relations between the two tasks.",
                "For our experimental evaluation, we introduce a new data set (the UvT Expert Collection) which is representative of the type of intranet that we described above.",
                "Our collection is based on publicly available data, crawled from the website of Tilburg University (UvT).",
                "This type of data is particularly interesting, since (1) it is clean, heterogeneous, structured, and focused, but comprises a limited number of documents; (2) contains information on the organizational hierarchy; (3) it is bilingual (English and Dutch); and (4) the list of expertise areas of an individual are provided by the employees themselves.",
                "Using the UvT Expert collection, we conduct two sets of experiments.",
                "The first is aimed at determining the effectiveness of baseline expertise finding and profiling methods in this new setting.",
                "A second group of experiments is aimed at extensions of the baseline methods that exploit characteristic features of the UvT Expert Collection; specifically, we propose and evaluate refined <br>expert find</br>ing and profiling methods that incorporate topicality and organizational structure.",
                "Apart from the research questions and data set that we contribute, our main contributions are as follows.",
                "The baseline models developed for expertise finding perform well on the new data set.",
                "While on the W3C setting the <br>expert find</br>ing task appears to be more difficult than profiling, for the UvT data the opposite is the case.",
                "We find that profiling on the UvT data set is considerably more difficult than on the W3C set, which we believe is due to the large (but realistic) number of topical areas that we used for profiling: about 1,500 for the UvT set, versus 50 in the W3C case.",
                "Taking the similarity between topics into account can significantly improve retrieval performance.",
                "The best performing similarity measures are content-based, therefore they can be applied on the W3C (and other) settings as well.",
                "Finally, we demonstrate that the organizational structure can be exploited in the form of a context model, improving MAP scores for certain models by up to 70%.",
                "The remainder of this paper is organized as follows.",
                "In the next section we review related work.",
                "Then, in Section 3 we provide detailed descriptions of the expertise retrieval tasks that we address in this paper: <br>expert find</br>ing and expert profiling.",
                "In Section 4 we present our baseline models, of which the performance is then assessed in Section 6 using the UvT data set that we introduce in Section 5.",
                "Advanced models exploiting specific features of our data are presented in Section 7 and evaluated in Section 8.",
                "We formulate our conclusions in Section 9. 2.",
                "RELATED WORK Initial approaches to expertise finding often employed databases containing information on the skills and knowledge of each individual in the organization [11].",
                "Most of these tools (usually called yellow pages or people-finding systems) rely on people to self-assess their skills against a predefined set of keywords.",
                "For updating profiles in these systems in an automatic fashion there is a need for intelligent technologies [5].",
                "More recent approaches use specific document sets (such as email [6] or software [18]) to find expertise.",
                "In contrast with focusing on particular document types, there is also an increased interest in the development of systems that index and mine published intranet documents as sources of evidence for expertise.",
                "One such published approach is the P@noptic system [9], which builds a representation of each person by concatenating all documents associated with that person-this is similar to Model 1 of Balog et al. [4], who formalize and compare two methods.",
                "Balog et al.s Model 1 directly models the knowledge of an expert from associated documents, while their Model 2 first locates documents on the topic and then finds the associated experts.",
                "In the reported experiments the second method performs significantly better when there are sufficiently many associated documents per candidate.",
                "Most systems that took part in the 2005 and 2006 editions of the Expert Finding task at TREC implemented (variations on) one of these two models; see [10, 20].",
                "Macdonald and Ounis [16] propose a different approach for ranking candidate expertise with respect to a topic based on data fusion techniques, without using collectionspecific heuristics; they find that applying field-based weighting models improves the ranking of candidates.",
                "Petkova and Croft [19] propose yet another approach, based on a combination of the above Model 1 and 2, explicitly modeling topics.",
                "Turning to other expert retrieval tasks that can also be addressed using topic-people associations, Balog and de Rijke [3] addressed the task of determining topical expert profiles.",
                "While their methods proved to be efficient on the W3C corpus, they require an amount of data that may not be available in the typical knowledge-intensive organization.",
                "Balog and de Rijke [2] study the related task of finding experts that are similar to a small set of experts given as input.",
                "As an aside, creating a textual summary of a person shows some similarities to biography finding, which has received a considerable amount of attention recently; see e.g., [13].",
                "We use generative language modeling to find associations between topics and people.",
                "In our modeling of <br>expert find</br>ing and profiling we collect evidence for expertise from multiple sources, in a heterogeneous collection, and integrate it with the co-occurrence of candidates names and query terms-the language modeling setting allows us to do this in a transparent manner.",
                "Our modeling proceeds in two steps.",
                "In the first step, we consider three baseline models, two taken from [4] (the Models 1 and 2 mentioned above), and one a refined version of a model introduced in [3] (which we refer to as Model 3 below); this third model is also similar to the model described by Petkova and Croft [19].",
                "The models we consider in our second round of experiments are mixture models similar to contextual language models [1] and to the expanded documents of Tao et al. [21]; however, the features that we use for definining our expansions-including topical structure and organizational structure-have not been used in this way before. 3.",
                "TASKS In the expertise retrieval scenario that we envisage, users seeking expertise within an organization have access to an interface that combines a search box (where they can search for experts or topics) with navigational structures (of experts and of topics) that allows them to click their way to an expert page (providing the profile of a person) or a topic page (providing a list of experts on the topic).",
                "To feed the above interface, we face two expertise retrieval tasks, <br>expert find</br>ing and expert profiling, that we first define and then formalize using generative language models.",
                "In order to model either task, the probability of the query topic being associated to a candidate expert plays a key role in the final estimates for searching and profiling.",
                "By using language models, both the candidates and the query are characterized by distributions of terms in the vocabulary (used in the documents made available by the organization whose expertise retrieval needs we are addressing). 3.1 Expert finding Expert finding involves the task of finding the right person with the appropriate skills and knowledge: Who are the experts on topic X?.",
                "E.g., an employee wants to ascertain who worked on a particular project to find out why particular decisions were made without having to trawl through documentation (if there is any).",
                "Or, they may be in need a trained specialist for consultancy on a specific problem.",
                "Within an organization there are usually many possible candidates who could be experts for given topic.",
                "We can state this problem as follows: What is the probability of a candidate ca being an expert given the query topic q?",
                "That is, we determine p(ca|q), and rank candidates ca according to this probability.",
                "The candidates with the highest probability given the query are deemed the most likely experts for that topic.",
                "The challenge is how to estimate this probability accurately.",
                "Since the query is likely to consist of only a few terms to describe the expertise required, we should be able to obtain a more accurate estimate by invoking Bayes Theorem, and estimating: p(ca|q) = p(q|ca)p(ca) p(q) , (1) where p(ca) is the probability of a candidate and p(q) is the probability of a query.",
                "Since p(q) is a constant, it can be ignored for ranking purposes.",
                "Thus, the probability of a candidate ca being an expert given the query q is proportional to the probability of a query given the candidate p(q|ca), weighted by the a priori belief p(ca) that candidate ca is an expert. p(ca|q) ∝ p(q|ca)p(ca) (2) In this paper our main focus is on estimating the probability of a query given the candidate p(q|ca), because this probability captures the extent to which the candidate knows about the query topic.",
                "Whereas the candidate priors are generally assumed to be uniformand thus will not influence the ranking-it has been demonstrated that a sensible choice of priors may improve the performance [20]. 3.2 Expert profiling While the task of expert searching was concerned with finding experts given a particular topic, the task of expert profiling seeks to answer a related question: What topics does a candidate know about?",
                "Essentially, this turns the questions of <br>expert find</br>ing around.",
                "The profiling of an individual candidate involves the identification of areas of skills and knowledge that they have expertise about and an evaluation of the level of proficiency in each of these areas.",
                "This is the candidates topical profile.",
                "Generally, topical profiles within organizations consist of tabular structures which explicitly catalogue the skills and knowledge of each individual in the organization.",
                "However, such practice is limited by the resources available for defining, creating, maintaining, and updating these profiles over time.",
                "By focusing on automatic methods which draw upon the available evidence within the document repositories of an organization, our aim is to reduce the human effort associated with the maintenance of topical profiles1 .",
                "A topical profile of a candidate, then, is defined as a vector where each element i of the vector corresponds to the candidate cas expertise on a given topic ki, (i.e., s(ca, ki)).",
                "Each topic ki defines a particular knowledge area or skill that the organization uses to define the candidates topical profile.",
                "Thus, it is assumed that a list of topics, {k1, . . . , kn}, where n is the number of pre-defined topics, is given: profile(ca) = s(ca, k1), s(ca, k2), . . . , s(ca, kn) . (3) 1 Context and evidence are needed to help users of expertise finding systems to decide whom to contact when seeking expertise in a particular area.",
                "Examples of such context are: Who does she work with?",
                "What are her contact details?",
                "Is she well-connected, just in case she is not able to help us herself?",
                "What is her role in the organization?",
                "Who is her superior?",
                "Collaborators, and affiliations, etc. are all part of the candidates social profile, and can serve as a background against which the systems recommendations should be interpreted.",
                "In this paper we only address the problem of determining topical profiles, and leave social profiling to further work.",
                "We state the problem of quantifying the competence of a person on a certain knowledge area as follows: What is the probability of a knowledge area (ki) being part of the candidates (expertise) profile? where s(ca, ki) is defined by p(ki|ca).",
                "Our task, then, is to estimate p(ki|ca), which is equivalent to the problem of obtaining p(q|ca), where the topic ki is represented as a query topic q, i.e., a sequence of keywords representing the expertise required.",
                "Both the <br>expert find</br>ing and profiling tasks rely on the accurate estimation of p(q|ca).",
                "The only difference derives from the prior probability that a person is an expert (p(ca)), which can be incorporated into the <br>expert find</br>ing task.",
                "This prior does not apply to the profiling task since the candidate (individual) is fixed. 4.",
                "BASELINE MODELS In this section we describe our baseline models for estimating p(q|ca), i.e., associations between topics and people.",
                "Both <br>expert find</br>ing and expert profiling boil down to this estimation.",
                "We employ three models for calculating this probability. 4.1 From topics to candidates Using Candidate Models: Model 1 Model 1 [4] defines the probability of a query given a candidate (p(q|ca)) using standard language modeling techniques, based on a multinomial unigram language model.",
                "For each candidate ca, a candidate language model θca is inferred such that the probability of a term given θca is nonzero for all terms, i.e., p(t|θca) > 0.",
                "From the candidate model the query is generated with the following probability: p(q|θca) = Y t∈q p(t|θca)n(t,q) , where each term t in the query q is sampled identically and independently, and n(t, q) is the number of times t occurs in q.",
                "The candidate language model is inferred as follows: (1) an empirical model p(t|ca) is computed; (2) it is smoothed with background probabilities.",
                "Using the associations between a candidate and a document, the probability p(t|ca) can be approximated by: p(t|ca) = X d p(t|d)p(d|ca), where p(d|ca) is the probability that candidate ca generates a supporting document d, and p(t|d) is the probability of a term t occurring in the document d. We use the maximum-likelihood estimate of a term, that is, the normalised frequency of the term t in document d. The strength of the association between document d and candidate ca expressed by p(d|ca) reflects the degree to which the candidates expertise is described using this document.",
                "The estimation of this probability is presented later, in Section 4.2.",
                "The candidate model is then constructed as a linear interpolation of p(t|ca) and the background model p(t) to ensure there are no zero probabilities, which results in the final estimation: p(q|θca) = (4) Y t∈q ( (1 − λ) X d p(t|d)p(d|ca) ! + λp(t) )n(t,q) .",
                "Model 1 amasses all the term information from all the documents associated with the candidate, and uses this to represent that candidate.",
                "This model is used to predict how likely a candidate would produce a query q.",
                "This can can be intuitively interpreted as the probability of this candidate talking about the query topic, where we assume that this is indicative of their expertise.",
                "Using Document Models: Model 2 Model 2 [4] takes a different approach.",
                "Here, the process is broken into two parts.",
                "Given a candidate ca, (1) a document that is associated with a candidate is selected with probability p(d|ca), and (2) from this document a query q is generated with probability p(q|d).",
                "Then the sum over all documents is taken to obtain p(q|ca), such that: p(q|ca) = X d p(q|d)p(d|ca). (5) The probability of a query given a document is estimated by inferring a document language model θd for each document d in a similar manner as the candidate model was inferred: p(t|θd) = (1 − λ)p(t|d) + λp(t), (6) where p(t|d) is the probability of the term in the document.",
                "The probability of a query given the document model is: p(q|θd) = Y t∈q p(t|θd)n(t,q) .",
                "The final estimate of p(q|ca) is obtained by substituting p(q|d) for p(q|θd) into Eq. 5 (see [4] for full details).",
                "Conceptually, Model 2 differs from Model 1 because the candidate is not directly modeled.",
                "Instead, the document acts like a hidden variable in the process which separates the query from the candidate.",
                "This process is akin to how a user may search for candidates with a standard search engine: initially by finding the documents which are relevant, and then seeing who is associated with that document.",
                "By examining a number of documents the user can obtain an idea of which candidates are more likely to discuss the topic q.",
                "Using Topic Models: Model 3 We introduce a third model, Model 3.",
                "Instead of attempting to model the query generation process via candidate or document models, we represent the query as a topic language model and directly estimate the probability of the candidate p(ca|q).",
                "This approach is similar to the model presented in [3, 19].",
                "As with the previous models, a language model is inferred, but this time for the query.",
                "We adapt the work of Lavrenko and Croft [14] to estimate a topic model from the query.",
                "The procedure is as follows.",
                "Given a collection of documents and a query topic q, it is assumed that there exists an unknown topic model θk that assigns probabilities p(t|θk) to the term occurrences in the topic documents.",
                "Both the query and the documents are samples from θk (as opposed to the previous approaches, where a query is assumed to be sampled from a specific document or candidate model).",
                "The main task is to estimate p(t|θk), the probability of a term given the topic model.",
                "Since the query q is very sparse, and as there are no examples of documents on the topic, this distribution needs to be approximated.",
                "Lavrenko and Croft [14] suggest a reasonable way of obtaining such an approximation, by assuming that p(t|θk) can be approximated by the probability of term t given the query q.",
                "We can then estimate p(t|q) using the joint probability of observing the term t together with the query terms, q1, . . . , qm, and dividing by the joint probability of the query terms: p(t|θk) ≈ p(t|q) = p(t, q1, . . . , qm) p(q1, . . . , qm) = p(t, q1, . . . , qm) P t ∈T p(t , q1, . . . , qm) , where p(q1, . . . , qm) = P t ∈T p(t , q1, . . . , qm), and T is the entire vocabulary of terms.",
                "In order to estimate the joint probability p(t, q1, . . . , qm), we follow [14, 15] and assume t and q1, . . . , qm are mutually independent, once we pick a source distribution from the set of underlying source distributions U.",
                "If we choose U to be a set of document models. then to construct this set, the query q would be issued against the collection, and the top n returned are assumed to be relevant to the topic, and thus treated as samples from the topic model. (Note that candidate models could be used instead.)",
                "With the document models forming U, the joint probability of term and query becomes: p(t, q1, . . . , qm) = X d∈U p(d) ˘ p(t|θd) mY i=1 p(qi|θd) ¯ . (7) Here, p(d) denotes the prior distribution over the set U, which reflects the relevance of the document to the topic.",
                "We assume that p(d) is uniform across U.",
                "In order to rank candidates according to the topic model defined, we use the Kullback-Leibler divergence metric (KL, [8]) to measure the difference between the candidate models and the topic model: KL(θk||θca) = X t p(t|θk) log p(t|θk) p(t|θca) . (8) Candidates with a smaller divergence from the topic model are considered to be more likely experts on that topic.",
                "The candidate model θca is defined in Eq. 4.",
                "By using KL divergence instead of the probability of a candidate given the topic model p(ca|θk), we avoid normalization problems. 4.2 Document-candidate associations For our models we need to be able to estimate the probability p(d|ca), which expresses the extent to which a document d characterizes the candidate ca.",
                "In [4], two methods are presented for estimating this probability, based on the number of person names recognized in a document.",
                "However, in our (intranet) setting it is reasonable to assume that authors of documents can unambiguously be identified (e.g., as the author of an article, the teacher assigned to a course, the owner of a web page, etc.)",
                "Hence, we set p(d|ca) to be 1 if candidate ca is author of document d, otherwise the probability is 0.",
                "In Section 6 we describe how authorship can be determined on different types of documents within the collection. 5.",
                "THE UVT EXPERT COLLECTION The UvT Expert collection used in the experiments in this paper fits the scenario outlined in Section 3.",
                "The collection is based on the Webwijs (Webwise) system developed at Tilburg University (UvT) in the Netherlands.",
                "Webwijs (http://www.uvt.nl/ webwijs/) is a publicly accessible database of UvT employees who are involved in research or teaching; currently, Webwijs contains information about 1168 experts, each of whom has a page with contact information and, if made available by the expert, a research description and publications list.",
                "In addition, each expert can select expertise areas from a list of 1491 topics and is encouraged to suggest new topics that need to be approved by the Webwijs editor.",
                "Each topic has a separate page that shows all experts associated with that topic and, if available, a list of related topics.",
                "Webwijs is available in Dutch and English, and this bilinguality has been preserved in the collection.",
                "Every Dutch Webwijs page has an English translation.",
                "Not all Dutch topics have an English translation, but the reverse is true: the 981 English topics all have a Dutch equivalent.",
                "About 42% of the experts teach courses at Tilburg University; these courses were also crawled and included in the profile.",
                "In addition, about 27% of the experts link to their academic homepage from their Webwijs page.",
                "These home pages were crawled and added to the collection. (This means that if experts put the full-text versions of their publications on their academic homepage, these were also available for indexing.)",
                "We also obtained 1880 full-text versions of publications from the UvT institutional repository and Dutch English no. of experts 1168 1168 no. of experts with ≥ 1 topic 743 727 no. of topics 1491 981 no. of expert-topic pairs 4318 3251 avg. no. of topics/expert 5.8 5.9 max. no. of topics/expert (no. of experts) 60 (1) 35 (1) min. no. of topics/expert (no. of experts) 1 (74) 1 (106) avg. no. of experts/topic 2.9 3.3 max. no. of experts/topic (no. of topics) 30 (1) 30 (1) min. no. of experts/topic (no. of topics) 1 (615) 1 (346) no. of experts with HP 318 318 no. of experts with CD 318 318 avg. no. of CDs per teaching expert 3.5 3.5 no. of experts with RD 329 313 no. of experts with PUB 734 734 avg. no. of PUBs per expert 27.0 27.0 avg. no. of PUB citations per expert 25.2 25.2 avg. no. of full-text PUBs per expert 1.8 1.8 Table 2: Descriptive statistics of the Dutch and English versions of the UvT Expert collection. converted them to plain text.",
                "We ran the TextCat [23] language identifier to classify the language of the home pages and the fulltext publications.",
                "We restricted ourselves to pages where the classifier was confident about the language used on the page.",
                "This resulted in four document types: research descriptions (RD), course descriptions (CD), publications (PUB; full-text and citationonly versions), and academic homepages (HP).",
                "Everything was bundled into the UvT Expert collection which is available at http: //ilk.uvt.nl/uvt-expert-collection/.",
                "The UvT Expert collection was extracted from a different organizational setting than the W3C collection and differs from it in a number of ways.",
                "The UvT setting is one with relatively small amounts of multilingual data.",
                "Document-author associations are clear and the data is structured and clean.",
                "The collection covers a broad range of expertise areas, as one can typically find on intranets of universities and other knowledge-intensive institutes.",
                "Additionally, our university setting features several types of structure (topical and organizational), as well as multiple document types.",
                "Another important difference between the two data sets is that the expertise areas in the UvT Expert collection are self-selected instead of being based on group membership or assignments by others.",
                "Size is another dimension along which the W3C and UvT Expert collections differ: the latter is the smaller of the two.",
                "Also realistic are the large differences in the amount of information available for each expert.",
                "Utilizing Webwijs is voluntary; 425 Dutch experts did not select any topics at all.",
                "This leaves us with 743 Dutch and 727 English usable expert profiles.",
                "Table 2 provides descriptive statistics for the UvT Expert collection.",
                "Universities tend to have a hierarchical structure that goes from the faculty level, to departments, research groups, down to the individual researchers.",
                "In the UvT Expert collection we have information about the affiliations of researchers with faculties and institutes, providing us with a two-level organizational hierarchy.",
                "Tilburg University has 22 organizational units at the faculty level (including the university office and several research institutes) and 71 departments, which amounts to 3.2 departments per faculty.",
                "As to the topical hierarchy used by Webwijs, 131 of the 1491 topics are top nodes in the hierarchy.",
                "This hierarchy has an average topic chain length of 2.65 and a maximum length of 7 topics. 6.",
                "EVALUATION Below, we evaluate Section 4s models for <br>expert find</br>ing and profiling onthe UvT Expert collection.",
                "We detail our research questions and experimental setup, and then present our results. 6.1 Research Questions We address the following research questions.",
                "Both <br>expert find</br>ing and profiling rely on the estimations of p(q|ca).",
                "The question is how the models compare on the different tasks, and in the setting of the UvT Expert collection.",
                "In [4], Model 2 outperformed Model 1 on the W3C collection.",
                "How do they compare on our data set?",
                "And how does Model 3 compare to Model 1?",
                "What about performance differences between the two languages in our test collection? 6.2 Experimental Setup The output of our models was evaluated against the self-assigned topic labels, which were treated as relevance judgements.",
                "Results were evaluated separately for English and Dutch.",
                "For English we only used topics for which the Dutch translation was available; for Dutch all topics were considered.",
                "The results were averaged for the queries in the intersection of relevance judgements and results; missing queries do not contribute a value of 0 to the scores.",
                "We use standard information retrieval measures, such as Mean Average Precision (MAP) and Mean Reciprocal Rank (MRR).",
                "We also report the percentage of topics (%q) and candidates (%ca) covered, for the <br>expert find</br>ing and profiling tasks, respectively. 6.3 Results Table 1 shows the performance of Model 1, 2, and 3 on the <br>expert find</br>ing and profiling tasks.",
                "The rows of the table correspond to the various document types (RD, CD, PUB, and HP) and to their combinations.",
                "RD+CD+PUB+HP is equivalent to the full collection and will be referred as the BASELINE of our experiments.",
                "Looking at Table 1 we see that Model 2 performs the best across the board.",
                "However, when the data is clean and very focused (RD), Model 3 outperforms it in a number of cases.",
                "Model 1 has the best coverage of candidates (%ca) and topics (%q).",
                "The various document types differ in their characteristics and how they improve the finding and profiling tasks.",
                "Expert profiling benefits much from the clean data present in the RD and CD document types, while the publications contribute the most to the <br>expert find</br>ing task.",
                "Adding the homepages does not prove to be particularly useful.",
                "When we compare the results across languages, we find that the coverage of English topics (%q) is higher than of the Dutch ones for <br>expert find</br>ing.",
                "Apart from that, the scores fall in the same range for both languages.",
                "For the profiling task the coverage of the candidates (%ca) is very similar for both languages.",
                "However, the performance is substantially better for the English topics.",
                "While it is hard to compare scores across collections, we conclude with a brief comparison of the absolute scores in Table 1 to those reported in [3, 4] on the W3C test set (2005 edition).",
                "For <br>expert find</br>ing the MAP scores for Model 2 reported here are about 50% higher than the corresponding figures in [4], while our MRR scores are slightly below those in [4].",
                "For expert profiling, the differences are far more dramatic: the MAP scores for Model 2 reported here are around 50% below the scores in [3], while the (best) MRR scores are about the same as those in [3].",
                "The cause for the latter differences seems to reside in the number of knowledge areas considered here-approx. 30 times more than in the W3C setting. 7.",
                "ADVANCED MODELS Now that we have developed and assessed basic language modeling techniques for expertise retrieval, we turn to refined models that exploit special features of our test collection. 7.1 Exploiting knowledge area similarity One way to improve the scoring of a query given a candidate is to consider what other requests the candidate would satisfy and use them as further evidence to support the original query, proportional Expert finding Expert profiling Document types Model 1 Model 2 Model 3 Model 1 Model 2 Model 3 %q MAP MRR %q MAP MRR %q MAP MRR %ca MAP MRR %ca MAP MRR %ca MAP MRR English RD 97.8 0.126 0.269 83.5 0.144 0.311 83.3 0.129 0.271 100 0.089 0.189 39.3 0.232 0.465 41.1 0.166 0.337 CD 97.8 0.118 0.227 91.7 0.123 0.248 91.7 0.118 0.226 32.8 0.188 0.381 32.4 0.195 0.385 32.7 0.203 0.370 PUB 97.8 0.200 0.330 98.0 0.216 0.372 98.0 0.145 0.257 78.9 0.167 0.364 74.5 0.212 0.442 78.9 0.135 0.299 HP 97.8 0.081 0.186 97.4 0.071 0.168 97.2 0.062 0.149 31.2 0.150 0.299 28.8 0.185 0.335 30.1 0.136 0.287 RD+CD 97.8 0.188 0.352 92.9 0.193 0.360 92.9 0.150 0.273 100 0.145 0.286 61.3 0.251 0.477 63.2 0.217 0.416 RD+CD+PUB 97.8 0.235 0.373 98.1 0.277 0.439 98.1 0.178 0.305 100 0.196 0.380 87.2 0.280 0.533 89.5 0.170 0.344 RD+CD+PUB+HP 97.8 0.237 0.372 98.6 0.280 0.441 98.5 0.166 0.293 100 0.199 0.387 88.7 0.281 0.525 90.9 0.169 0.329 Dutch RD 61.3 0.094 0.229 38.4 0.137 0.336 38.3 0.127 0.295 38.0 0.127 0.386 34.1 0.138 0.420 38.0 0.105 0.327 CD 61.3 0.107 0.212 49.7 0.128 0.256 49.7 0.136 0.261 32.5 0.151 0.389 31.8 0.158 0.396 32.5 0.170 0.380 PUB 61.3 0.193 0.319 59.5 0.218 0.368 59.4 0.173 0.291 78.8 0.126 0.364 76.0 0.150 0.424 78.8 0.103 0.294 HP 61.3 0.063 0.169 56.6 0.064 0.175 56.4 0.062 0.163 29.8 0.108 0.308 27.8 0.125 0.338 29.8 0.098 0.255 RD+CD 61.3 0.159 0.314 51.9 0.184 0.360 51.9 0.169 0.324 60.5 0.151 0.410 57.2 0.166 0.431 60.4 0.159 0.384 RD+CD+PUB 61.3 0.244 0.398 61.5 0.260 0.424 61.4 0.210 0.350 90.3 0.165 0.445 88.2 0.189 0.479 90.3 0.126 0.339 RD+CD+PUB+HP 61.3 0.249 0.401 62.6 0.265 0.436 62.6 0.195 0.344 91.9 0.164 0.426 90.1 0.195 0.488 91.9 0.125 0.328 Table 1: Performance of the models on the <br>expert find</br>ing and profiling tasks, using different document types and their combinations. %q is the number of topics covered (applies to the <br>expert find</br>ing task), %ca is the number of candidates covered (applies to the expert profiling task).",
                "The top and bottom blocks correspond to English and Dutch respectively.",
                "The best scores are in boldface. to how related the other requests are to the original query.",
                "This can be modeled by interpolating between the p(q|ca) and the further supporting evidence from all similar requests q , as follows: p (q|ca) = λp(q|ca) + (1 − λ) X q p(q|q )p(q |ca), (9) where p(q|q ) represents the similarity between the two topics q and q .",
                "To be able to work with similarity methods that are not necessarily probabilities, we set p(q|q ) = w(q,q ) γ , where γ is a normalizing constant, such that γ = P q w(q , q ).",
                "We consider four methods for calculating the similarity score between two topics.",
                "Three approaches are strictly content-based, and establish similarity by examining co-occurrence patterns of topics within the collection, while the last approach exploits the hierarchical structure of topical areas that may be present within an organization (see [7] for further examples of integrating word relationships into language models).",
                "The Kullback-Leibler (KL) divergence metric defined in Eq. 8 provides a measure of how different or similar two probability distributions are.",
                "A topic model is inferred for q and q using the method presented in Section 4.1 to describe the query across the entire vocabulary.",
                "Since a lower KL score means the queries are more similar, we let w(q, q ) = max(KL(θq||·) − KL(θq||θq )).",
                "Pointwise Mutual Information (PMI, [17]) is a measure of association used in information theory to determine the extent of independence between variables.",
                "The dependence between two queries is reflected by the SI(q, q ) score, where scores greater than zero indicate that it is likely that there is a dependence, which we take to mean that the queries are likely to be similar: SI(q, q ) = log p(q, q ) p(q)p(q ) (10) We estimate the probability of a topic p(q) using the number of documents relevant to query q within the collection.",
                "The joint probability p(q, q ) is estimated similarly, by using the concatenation of q and q as a query.",
                "To obtain p(q|q ), we then set w(q, q ) = SI(q, q ) when SI(q, q ) > 0 otherwise w(q, q ) = 0, because we are only interested in including queries that are similar.",
                "The log-likelihood statistic provides another measure of dependence, which is more reliable than the pointwise mutual information measure [17].",
                "Let k1 be the number of co-occurrences of q and q , k2 the number of occurrences of q not co-occurring with q , n1 the total number of occurrences of q , and n2 the total number of topic tokens minus the number of occurrences of q .",
                "Then, let p1 = k1/n1, p2 = k2/n2, and p = (k1 + k2)/(n1 + n2), (q, q ) = 2( (p1, k1, n1) + (p2, k2, n2) − (p, k1, n1) − (p, k2, n2)), where (p, n, k) = k log p + (n − k) log(1 − p).",
                "The higher score indicate that queries are also likely to be similar, thus we set w(q, q ) = (q, q ).",
                "Finally, we also estimate the similarity of two topics based on their distance within the topic hierarchy.",
                "The topic hierarchy is viewed as a directed graph, and for all topic-pairs the shortest path SP(q, q ) is calculated.",
                "We set the similarity score to be the reciprocal of the shortest path: w(q, q ) = 1/SP(q, q ). 7.2 Contextual information Given the hierarchy of an organization, the units to which a person belong are regarded as a context so as to compensate for data sparseness.",
                "We model it as follows: p (q|ca) = 1 − P ou∈OU(ca) λou · p(q|ca) + P ou∈OU(ca) λou · p(q|ou), where OU(ca) is the set of organizational units of which candidate ca is a member of, and p(q|o) expresses the strength of the association between query q and the unit ou.",
                "The latter probability can be estimated using either of the three basic models, by simply replacing ca with ou in the corresponding equations.",
                "An organizational unit is associated with all the documents that its members have authored.",
                "That is, p(d|ou) = maxca∈ou p(d|ca). 7.3 A simple multilingual model For knowledge institutes in Europe, academic or otherwise, a multilingual (or at least bilingual) setting is typical.",
                "The following model builds on a kind of independence assumption: there is no spill-over of expertise/profiles across language boundaries.",
                "While a simplification, this is a sensible first approach.",
                "That is: p (q|ca) =P l∈L λl · p(ql|ca), where L is the set of languages used in the collection, ql is the translation of the query q to language l, and λl is a language specific smoothing parameter, such that P l∈L λl = 1. 8.",
                "ADVANCED MODELS: EVALUATION In this section we present an experimental evaluation of our advanced models.",
                "Expert finding Expert profiling Language Model 1 Model 2 Model 3 Model 1 Model 2 Model 3 %q MAP MRR %q MAP MRR %q MAP MRR %ca MAP MRR %ca MAP MRR %ca MAP MRR English only 97.8 0.237 0.372 98.6 0.280 0.441 98.5 0.166 0.293 100 0.199 0.387 88.7 0.281 0.525 90.9 0.169 0.329 Dutch only 61.3 0.249 0.401 62.6 0.265 0.436 62.6 0.195 0.344 91.9 0.164 0.426 90.1 0.195 0.488 91.9 0.125 0.328 Combination 99.4 0.297 0.444 99.7 0.324 0.491 99.7 0.223 0.388 100 0.241 0.445 92.1 0.313 0.564 93.2 0.224 0.411 Table 3: Performance of the combination of languages on the <br>expert find</br>ing and profiling tasks (on candidates).",
                "Best scores for each model are in italic, absolute best scores for the <br>expert find</br>ing and profiling tasks are in boldface.",
                "Method Model 1 Model 2 Model 3 MAP MRR MAP MRR MAP MRR English BASELINE 0.296 0.454 0.339 0.509 0.221 0.333 KLDIV 0.291 0.453 0.327 0.503 0.219 0.330 PMI 0.291 0.453 0.337 0.509 0.219 0.331 LL 0.319 0.490 0.360 0.524 0.233 0.368 HDIST 0.299 0.465 0.346 0.537 0.219 0.332 Dutch BASELINE 0.240 0.350 0.271 0.403 0.227 0.389 KLDIV 0.239 0.347 0.253 0.386 0.224 0.385 PMI 0.239 0.350 0.260 0.392 0.227 0.389 LL 0.255 0.372 0.281 0.425 0.231 0.389 HDIST 0.253 0.365 0.271 0.407 0.236 0.402 Method Model 1 Model 2 Model 3 MAP MRR MAP MRR MAP MRR English BASELINE 0.485 0.546 0.499 0.548 0.381 0.416 KLDIV 0.510 0.564 0.513 0.558 0.381 0.416 PMI 0.486 0.546 0.495 0.542 0.407 0.451 LL 0.558 0.589 0.586 0.617 0.408 0.453 HDIST 0.507 0.567 0.512 0.563 0.386 0.420 Dutch BASELINE 0.263 0.313 0.294 0.358 0.262 0.315 KLDIV 0.284 0.336 0.271 0.321 0.261 0.314 PMI 0.265 0.317 0.265 0.316 0.273 0.330 LL 0.312 0.351 0.330 0.377 0.284 0.331 HDIST 0.280 0.327 0.288 0.341 0.266 0.321 Table 4: Performance on the <br>expert find</br>ing (top) and profiling (bottom) tasks, using knowledge area similarities.",
                "Runs were evaluated on the main topics set.",
                "Best scores are in boldface. 8.1 Research Questions Our questions follow the refinements presented in the preceding section: Does exploiting the knowledge area similarity improve effectiveness?",
                "Which of the various methods for capturing word relationships is most effective?",
                "Furthermore, is our way of bringing in contextual information useful?",
                "For which tasks?",
                "And finally, is our simple way of combining the monolingual scores sufficient for obtaining significant improvements? 8.2 Experimental setup Given that the self-assessments are also sparse in our collection, in order to be able to measure differences between the various models, we selected a subset of topics, and evaluated (some of the) runs only on this subset.",
                "This set is referred as main topics, and consists of topics that are located at the top level of the topical hierarchy. (A main topic has subtopics, but is not a subtopic of any other topic.)",
                "This main set consists of 132 Dutch and 119 English topics.",
                "The relevance judgements were restricted to the main topic set, but were not expanded with subtopics. 8.3 Exploiting knowledge area similarity Table 4 presents the results.",
                "The four methods used for estimating knowledge-area similarity are KL divergence (KLDIV), PointLang.",
                "Topics Model 1 Model 2 Model 3 MAP MRR MAP MRR MAP MRR Expert finding UK ALL 0.423 0.545 0.654 0.799 0.494 0.629 UK MAIN 0.500 0.621 0.704 0.834 0.587 0.699 NL ALL 0.439 0.560 0.672 0.826 0.480 0.630 NL MAIN 0.440 0.584 0.645 0.816 0.515 0.655 Expert profiling UK ALL 0.240 0.640 0.306 0.778 0.223 0.616 UK MAIN 0.523 0.677 0.519 0.648 0.461 0.587 NL ALL 0.203 0.716 0.254 0.770 0.183 0.627 NL MAIN 0.332 0.576 0.380 0.624 0.332 0.549 Table 5: Evaluating the context models on organizational units. wise mutual information (PMI), log-likelihood (LL), and distance within topic hierarchy (HDIST).",
                "We managed to improve upon the baseline in all cases, but the improvement is more noticeable for the profiling task.",
                "For both tasks, the LL method performed best.",
                "The content-based approaches performed consistently better than HDIST. 8.4 Contextual information A two level hierarchy of organizational units (faculties and institutes) is available in the UvT Expert collection.",
                "The unit a person belongs to is used as a context for that person.",
                "First, we evaluated the models of the organizational units, using all topics (ALL) and only the main topics (MAIN).",
                "An organizational unit is considered to be relevant for a given topic (or vice versa) if at least one member of the unit selected the given topic as an expertise area.",
                "Table 5 reports on the results.",
                "As far as <br>expert find</br>ing goes, given a topic, the corresponding organizational unit can be identified with high precision.",
                "However, the expert profiling task shows a different picture: the scores are low, and the task seems hard.",
                "The explanation may be that general concepts (i.e., our main topics) may belong to several organizational units.",
                "Second, we performed another evaluation, where we combined the contextual models with the candidate models (to score candidates again).",
                "Table 6 reports on the results.",
                "We find a positive impact of the context models only for <br>expert find</br>ing.",
                "Noticably, for <br>expert find</br>ing (and Model 1), it improves over 50% (for English) and over 70% (for Dutch) on MAP.",
                "The poor performance on expert profiling may be due to the fact that context models alone did not perform very well on the profiling task to begin with. 8.5 Multilingual models In this subsection we evaluate the method for combining results across multiple languages that we described in Section 7.3.",
                "In our setting the set of languages consists of English and Dutch: L = {UK, NL}.",
                "The weights on these languages were set to be identical (λUK = λNL = 0.5).",
                "We performed experiments with various λ settings, but did not observe significant differences in performance.",
                "Table 3 reports on the multilingual results, where performance is evaluated on the full topic set.",
                "All three models significantly imLang.",
                "Method Model 1 Model 2 Model 3 MAP MRR MAP MRR MAP MRR Expert finding UK BL 0.296 0.454 0.339 0.509 0.221 0.333 UK CT 0.330 0.491 0.342 0.500 0.228 0.342 NL BL 0.240 0.350 0.271 0.403 0.227 0.389 NL CT 0.251 0.382 0.267 0.410 0.246 0.404 Expert profiling UK BL 0.485 0.546 0.499 0.548 0.381 0.416 UK CT 0.562 0.620 0.508 0.558 0.440 0.486 NL BL 0.263 0.313 0.294 0.358 0.262 0.315 NL CT 0.330 0.384 0.317 0.387 0.294 0.345 Table 6: Performance of the context models (CT) compared to the baseline (BL).",
                "Best scores are in boldface. proved over all measures for both tasks.",
                "The coverage of topics and candidates for the <br>expert find</br>ing and profiling tasks, respectively, is close to 100% in all cases.",
                "The relative improvement of the precision scores ranges from 10% to 80%.",
                "These scores demonstrate that despite its simplicity, our method for combining results over multiple languages achieves substantial improvements over the baseline. 9.",
                "CONCLUSIONS In this paper we focused on expertise retrieval (<br>expert find</br>ing and profiling) in a new setting of a typical knowledge-intensive organization in which the available data is of high quality, multilingual, and covering a broad range of expertise area.",
                "Typically, the amount of available data in such an organization (e.g., a university, a research institute, or a research lab) is limited when compared to the W3C collection that has mostly been used for the experimental evaluation of expertise retrieval so far.",
                "To examine expertise retrieval in this setting, we introduced (and released) the UvT Expert collection as a representative case of such knowledge intensive organizations.",
                "The new collection reflects the typical properties of knowledge-intensive institutes noted above and also includes several features which may are potentially useful for expertise retrieval, such as topical and organizational structure.",
                "We evaluated how current state-of-the-art models for <br>expert find</br>ing and profiling performed in this new setting and then refined these models in order to try and exploit the different characteristics within the data environment (language, topicality, and organizational structure).",
                "We found that current models of expertise retrieval generalize well to this new environment; in addition we found that refining the models to account for the differences results in significant improvements, thus making up for problems caused by data sparseness issues.",
                "Future work includes setting up manual assessments of automatically generated profiles by the employees themselves, especially in cases where the employees have not provided a profile themselves. 10.",
                "ACKNOWLEDGMENTS Krisztian Balog was supported by the Netherlands Organisation for Scientific Research (NWO) under project number 220-80-001.",
                "Maarten de Rijke was also supported by NWO under project numbers 017.001.190, 220-80-001, 264-70-050, 354-20-005, 600.065.120, 612-13-001, 612.000.106, 612.066.302, 612.069.006, 640.001.501, 640.002.501, and by the E.U.",
                "IST programme of the 6th FP for RTD under project MultiMATCH contract IST-033104.",
                "The work of Toine Bogers and Antal van den Bosch was funded by the IOP-MMI-program of SenterNovem / The Dutch Ministry of Economic Affairs, as part of the `A Propos project. 11.",
                "REFERENCES [1] L. Azzopardi.",
                "Incorporating Context in the Language Modeling Framework for ad-hoc Information Retrieval.",
                "PhD thesis, University of Paisley, 2005. [2] K. Balog and M. de Rijke.",
                "Finding similar experts.",
                "In This volume, 2007. [3] K. Balog and M. de Rijke.",
                "Determining expert profiles (with an application to <br>expert find</br>ing).",
                "In IJCAI 07: Proc. 20th Intern.",
                "Joint Conf. on Artificial Intelligence, pages 2657-2662, 2007. [4] K. Balog, L. Azzopardi, and M. de Rijke.",
                "Formal models for <br>expert find</br>ing in enterprise corpora.",
                "In SIGIR 06: Proc. 29th annual intern.",
                "ACM SIGIR conf. on Research and development in information retrieval, pages 43-50, 2006. [5] I. Becerra-Fernandez.",
                "The role of artificial intelligence technologies in the implementation of people-finder knowledge management systems.",
                "In AAAI Workshop on Bringing Knowledge to Business Processes, March 2000. [6] C. S. Campbell, P. P. Maglio, A. Cozzi, and B. Dom.",
                "Expertise identification using email communications.",
                "In CIKM 03: Proc. twelfth intern. conf. on Information and knowledge management, pages 528531, 2003. [7] G. Cao, J.-Y.",
                "Nie, and J. Bai.",
                "Integrating word relationships into language models.",
                "In SIGIR 05: Proc. 28th annual intern.",
                "ACM SIGIR conf. on Research and development in information retrieval, pages 298-305, 2005. [8] T. M. Cover and J.",
                "A. Thomas.",
                "Elements of Information Theory.",
                "Wiley-Interscience, 1991. [9] N. Craswell, D. Hawking, A. M. Vercoustre, and P. Wilkins.",
                "P@noptic expert: Searching for experts not just for documents.",
                "In Ausweb, 2001. [10] N. Craswell, A. de Vries, and I. Soboroff.",
                "Overview of the TREC2005 Enterprise Track.",
                "In The Fourteenth Text REtrieval Conf.",
                "Proc. (TREC 2005), 2006. [11] T. H. Davenport and L. Prusak.",
                "Working Knowledge: How Organizations Manage What They Know.",
                "Harvard Business School Press, Boston, MA, 1998. [12] T. Dunning.",
                "Accurate methods for the statistics of surprise and coincidence.",
                "Computational Linguistics, 19(1):61-74, 1993. [13] E. Filatova and J. Prager.",
                "Tell me what you do and Ill tell you what you are: Learning occupation-related activities for biographies.",
                "In HLT/EMNLP, 2005. [14] V. Lavrenko and W. B. Croft.",
                "Relevance based language models.",
                "In SIGIR 01: Proc. 24th annual intern.",
                "ACM SIGIR conf. on Research and development in information retrieval, pages 120-127, 2001. [15] V. Lavrenko, M. Choquette, and W. B. Croft.",
                "Cross-lingual relevance models.",
                "In SIGIR 02: Proc. 25th annual intern.",
                "ACM SIGIR conf. on Research and development in information retrieval, pages 175-182, 2002. [16] C. Macdonald and I. Ounis.",
                "Voting for candidates: adapting data fusion techniques for an expert search task.",
                "In CIKM 06: Proc. 15th ACM intern. conf. on Information and knowledge management, pages 387-396, 2006. [17] C. Manning and H. Sch¨utze.",
                "Foundations of Statistical Natural Language Processing.",
                "The MIT Press, 1999. [18] A. Mockus and J. D. Herbsleb.",
                "Expertise browser: a quantitative approach to identifying expertise.",
                "In ICSE 02: Proc. 24th Intern.",
                "Conf. on Software Engineering, pages 503-512, 2002. [19] D. Petkova and W. B. Croft.",
                "Hierarchical language models for <br>expert find</br>ing in enterprise corpora.",
                "In Proc.",
                "ICTAI 2006, pages 599-608, 2006. [20] I. Soboroff, A. de Vries, and N. Craswell.",
                "Overview of the TREC 2006 Enterprise Track.",
                "In TREC 2006 Working Notes, 2006. [21] T. Tao, X. Wang, Q. Mei, and C. Zhai.",
                "Language model information retrieval with document expansion.",
                "In HLT-NAACL 2006, 2006. [22] TREC.",
                "Enterprise track, 2005.",
                "URL: http://www.ins.cwi. nl/projects/trec-ent/wiki/. [23] G. van Noord.",
                "TextCat Language Guesser.",
                "URL: http://www. let.rug.nl/˜vannoord/TextCat/. [24] W3C.",
                "The W3C test collection, 2005.",
                "URL: http://research. microsoft.com/users/nickcr/w3c-summary.html."
            ],
            "original_annotated_samples": [
                "The goal of <br>expert find</br>ing is to identify a list of people who are knowledgeable about a given topic.",
                "However, nearly all of the <br>expert find</br>ing or profiling work performed has been validated experimentally using the W3C collection [24] from the Enterprise Track.",
                "This allows us to formulate the <br>expert find</br>ing and expert profiling tasks in a uniform way, and has the added benefit of allowing us to understand the relations between the two tasks.",
                "A second group of experiments is aimed at extensions of the baseline methods that exploit characteristic features of the UvT Expert Collection; specifically, we propose and evaluate refined <br>expert find</br>ing and profiling methods that incorporate topicality and organizational structure.",
                "While on the W3C setting the <br>expert find</br>ing task appears to be more difficult than profiling, for the UvT data the opposite is the case."
            ],
            "translated_annotated_samples": [
                "El objetivo de la <br>búsqueda de expertos</br> es identificar una lista de personas que tienen conocimientos sobre un tema específico.",
                "Sin embargo, casi todo el trabajo de <br>búsqueda o perfilado de expertos</br> realizado ha sido validado experimentalmente utilizando la colección de W3C [24] de la pista empresarial.",
                "Esto nos permite formular las tareas de <br>búsqueda de expertos</br> y perfilado de expertos de manera uniforme, y tiene el beneficio adicional de permitirnos entender las relaciones entre las dos tareas.",
                "Un segundo grupo de experimentos está dirigido a extensiones de los métodos base que aprovechan las características distintivas de la Colección de Expertos de UvT; específicamente, proponemos y evaluamos métodos refinados de búsqueda y perfilado de expertos que incorporan la topicalidad y la estructura organizativa.",
                "Mientras que en el entorno del W3C la <br>tarea de encontrar expertos</br> parece ser más difícil que el perfilado, para los datos de UvT ocurre lo contrario."
            ],
            "translated_text": "Recuperación de amplia experiencia en entornos de datos dispersos Krisztian Balog ISLA, Universidad de Ámsterdam Kruislaan 403, 1098 SJ Ámsterdam, Países Bajos kbalog@science.uva.nl Toine Bogers ILK, Universidad de Tilburg P.O. Caja 90153, 5000 LE Tilburg, Países Bajos A.M.Bogers@uvt.nl Leif Azzopardi Dept. de Ciencias de la Computación Universidad de Glasgow, Glasgow, G12 8QQ leif@dcs.gla.ac.uk Maarten de Rijke ISLA, Universidad de Ámsterdam Kruislaan 403, 1098 SJ Ámsterdam, Países Bajos mdr@science.uva.nl Antal van den Bosch ILK, Universidad de Tilburg P.O. La recuperación de la experiencia ha sido ampliamente inexplorada en datos distintos a la colección de la W3C. Al mismo tiempo, muchas intranets de universidades y otras organizaciones intensivas en conocimiento ofrecen ejemplos de datos de expertos multilingües relativamente pequeños pero precisos, que abarcan amplias áreas de especialización. Primero presentamos dos tareas principales de recuperación de expertos, junto con un conjunto de enfoques de referencia basados en modelado del lenguaje generativo, con el objetivo de encontrar relaciones de experticia entre temas y personas. Para nuestra evaluación experimental, introducimos (y liberamos) un nuevo conjunto de pruebas basado en un rastreo de un sitio universitario. Usando este conjunto de pruebas, llevamos a cabo dos series de experimentos. El primero tiene como objetivo determinar la efectividad de los métodos de recuperación de conocimientos básicos aplicados al nuevo conjunto de pruebas. El segundo tiene como objetivo evaluar modelos refinados que exploten las características distintivas del nuevo conjunto de pruebas, como la estructura organizativa de la universidad y la estructura jerárquica de los temas en el conjunto de pruebas. Los modelos de recuperación de experiencia se muestran robustos con respecto a entornos más pequeños que la colección de la W3C, y las técnicas actuales parecen ser generalizables a otros contextos. Categorías y Descriptores de Asignaturas H.3 [Almacenamiento y Recuperación de Información]: H.3.1 Análisis de Contenido e Indexación; H.3.3 Búsqueda y Recuperación de Información; H.3.4 Sistemas y Software; H.4 [Aplicaciones de Sistemas de Información]: H.4.2 Tipos de Sistemas; H.4.m Términos Generales Misceláneos Algoritmos, Medición, Rendimiento, Experimentación 1. INTRODUCCIÓN La intranet de una organización proporciona un medio para intercambiar información entre empleados y facilitar colaboraciones entre ellos. Para lograr esto de manera eficiente y efectiva, es necesario proporcionar instalaciones de búsqueda que permitan a los empleados no solo acceder a documentos, sino también identificar a colegas expertos. En la pista de Empresas de TREC [22] se ha reconocido la necesidad de estudiar y comprender la recuperación de expertos a través de la introducción de tareas de Búsqueda de Expertos. El objetivo de la <br>búsqueda de expertos</br> es identificar una lista de personas que tienen conocimientos sobre un tema específico. Esta tarea suele abordarse descubriendo asociaciones entre personas y temas [10]; comúnmente, se asume que la co-ocurrencia del nombre de una persona con temas en el mismo contexto es evidencia de experiencia. Una tarea alternativa, que utiliza la misma idea de asociaciones entre personas y temas, es el perfilado de expertos, donde la tarea consiste en devolver una lista de temas sobre los que una persona tiene conocimiento [3]. El lanzamiento de la tarea de Búsqueda de Expertos en TREC ha generado mucho interés en la recuperación de expertos, con un rápido progreso en términos de modelado, algoritmos y aspectos de evaluación. Sin embargo, casi todo el trabajo de <br>búsqueda o perfilado de expertos</br> realizado ha sido validado experimentalmente utilizando la colección de W3C [24] de la pista empresarial. Si bien esta colección es actualmente la única colección de pruebas disponible públicamente para tareas de recuperación de expertos, solo representa un tipo de intranet. Con solo una colección de pruebas no es posible generalizar conclusiones a otros escenarios realistas. En este artículo nos enfocamos en la recuperación de expertos en un entorno realista que difiere del entorno de la W3C, uno en el que hay cantidades relativamente pequeñas de datos limpios y multilingües disponibles, que abarcan una amplia gama de áreas de expertise, como se puede encontrar en las intranets de universidades y otras organizaciones intensivas en conocimiento. Normalmente, esta configuración incluye varios tipos adicionales de estructuras: estructura temática (por ejemplo, jerarquías de temas utilizadas por la organización), estructura organizativa (facultad, departamento, ...), así como múltiples tipos de documentos (descripciones de investigación y cursos, publicaciones y páginas web académicas). Este entorno es bastante diferente del entorno del W3C de maneras que podrían afectar el rendimiento de las tareas de recuperación de conocimientos especializados. Nos enfocamos en varias preguntas de investigación en este artículo: ¿La cantidad relativamente pequeña de datos disponibles en una intranet afecta la calidad de las asociaciones tema-persona que son fundamentales en los algoritmos de recuperación de expertos? ¿Cómo se desempeñan los algoritmos de vanguardia desarrollados en el conjunto de datos del W3C en el escenario alternativo del tipo descrito anteriormente? Más en general, ¿se pueden aplicar las lecciones de la tarea de Búsqueda de Expertos en TREC a este contexto? ¿Cómo afecta la inclusión o exclusión de diferentes documentos a las tareas de recuperación de expertos? Además, ¿cómo se puede utilizar la estructura temática y organizativa con fines de recuperación? Para responder a nuestras preguntas de investigación, primero presentamos un conjunto de enfoques de línea base, basados en modelado de lenguaje generativo, con el objetivo de encontrar asociaciones entre temas y personas. Esto nos permite formular las tareas de <br>búsqueda de expertos</br> y perfilado de expertos de manera uniforme, y tiene el beneficio adicional de permitirnos entender las relaciones entre las dos tareas. Para nuestra evaluación experimental, introducimos un nuevo conjunto de datos (la Colección de Expertos de UvT) que es representativo del tipo de intranet que describimos anteriormente. Nuestra colección se basa en datos de acceso público, extraídos del sitio web de la Universidad de Tilburg (UvT). Este tipo de datos es particularmente interesante, ya que (1) es limpio, heterogéneo, estructurado y enfocado, pero comprende un número limitado de documentos; (2) contiene información sobre la jerarquía organizativa; (3) es bilingüe (inglés y holandés); y (4) la lista de áreas de especialización de un individuo es proporcionada por los propios empleados. Usando la colección de expertos de UvT, llevamos a cabo dos conjuntos de experimentos. El primero tiene como objetivo determinar la efectividad de los métodos de búsqueda y perfilado de experiencia inicial en este nuevo entorno. Un segundo grupo de experimentos está dirigido a extensiones de los métodos base que aprovechan las características distintivas de la Colección de Expertos de UvT; específicamente, proponemos y evaluamos métodos refinados de búsqueda y perfilado de expertos que incorporan la topicalidad y la estructura organizativa. Además de las preguntas de investigación y el conjunto de datos que aportamos, nuestras principales contribuciones son las siguientes. Los modelos base desarrollados para encontrar expertos funcionan bien en el nuevo conjunto de datos. Mientras que en el entorno del W3C la <br>tarea de encontrar expertos</br> parece ser más difícil que el perfilado, para los datos de UvT ocurre lo contrario. ",
            "candidates": [],
            "error": [
                [
                    "búsqueda de expertos",
                    "búsqueda o perfilado de expertos",
                    "búsqueda de expertos",
                    "tarea de encontrar expertos"
                ]
            ]
        }
    }
}