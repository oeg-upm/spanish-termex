{
    "id": "H-8",
    "original_text": "Robust Test Collections for Retrieval Evaluation Ben Carterette Center for Intelligent Information Retrieval Computer Science Department University of Massachusetts Amherst Amherst, MA 01003 carteret@cs.umass.edu ABSTRACT Low-cost methods for acquiring relevance judgments can be a boon to researchers who need to evaluate new retrieval tasks or topics but do not have the resources to make thousands of judgments. While these judgments are very useful for a one-time evaluation, it is not clear that they can be trusted when re-used to evaluate new systems. In this work, we formally define what it means for judgments to be reusable: the confidence in an evaluation of new systems can be accurately assessed from an existing set of relevance judgments. We then present a method for augmenting a set of relevance judgments with relevance estimates that require no additional assessor effort. Using this method practically guarantees reusability: with as few as five judgments per topic taken from only two systems, we can reliably evaluate a larger set of ten systems. Even the smallest sets of judgments can be useful for evaluation of new systems. Categories and Subject Descriptors: H.3 Information Storage and Retrieval; H.3.4 Systems and Software: Performance Evaluation General Terms: Experimentation, Measurement, Reliability 1. INTRODUCTION Consider an information retrieval researcher who has invented a new retrieval task. She has built a system to perform the task and wants to evaluate it. Since the task is new, it is unlikely that there are any extant relevance judgments. She does not have the time or resources to judge every document, or even every retrieved document. She can only judge the documents that seem to be the most informative and stop when she has a reasonable degree of confidence in her conclusions. But what happens when she develops a new system and needs to evaluate it? Or another research group decides to implement a system to perform the task? Can they reliably reuse the original judgments? Can they evaluate without more relevance judgments? Evaluation is an important aspect of information retrieval research, but it is only a semi-solved problem: for most retrieval tasks, it is impossible to judge the relevance of every document; there are simply too many of them. The solution used by NIST at TREC (Text REtrieval Conference) is the pooling method [19, 20]: all competing systems contribute N documents to a pool, and every document in that pool is judged. This method creates large sets of judgments that are reusable for training or evaluating new systems that did not contribute to the pool [21]. This solution is not adequate for our hypothetical researcher. The pooling method gives thousands of relevance judgments, but it requires many hours of (paid) annotator time. As a result, there have been a slew of recent papers on reducing annotator effort in producing test collections: Cormack et al. [11], Zobel [21], Sanderson and Joho [17], Carterette et al. [8], and Aslam et al. [4], among others. As we will see, the judgments these methods produce can significantly bias the evaluation of a new set of systems. Returning to our hypothetical resesarcher, can she reuse her relevance judgments? First we must formally define what it means to be reusable. In previous work, reusability has been tested by simply assessing the accuracy of a set of relevance judgments at evaluating unseen systems. While we can say that it was right 75% of the time, or that it had a rank correlation of 0.8, these numbers do not have any predictive power: they do not tell us which systems are likely to be wrong or how confident we should be in any one. We need a more careful definition of reusability. Specifically, the question of reusability is not how accurately we can evaluate new systems. A malicious adversary can always produce a new ranked list that has not retrieved any of the judged documents. The real question is how much confidence we have in our evaluations, and, more importantly, whether we can trust our estimates of confidence. Even if confidence is not high, as long as we can trust it, we can identify which systems need more judgments in order to increase confidence. Any set of judgments, no matter how small, becomes reusable to some degree. Small, reusable test collections could have a huge impact on information retrieval research. Research groups would be able to share the relevance judgments they have done in-house for pilot studies, new tasks, or new topics. The amount of data available to researchers would grow exponentially over time. 2. ROBUST EVALUATION Above we gave an intuitive definition of reusability: a collection is reusable if we can trust our estimates of confidence in an evaluation. By that we mean that if we have made some relevance judgments and have, for example, 75% confidence that system A is better than system B, we would like there to be no more than 25% chance that our assessment of the relative quality of the systems will change as we continue to judge documents. Our evaluation should be robust to missing judgments. In our previous work, we defined confidence as the probability that the difference in an evaluation measure calculated for two systems is less than zero [8]. This notion of confidence is defined in the context of a particular evaluation task that we call comparative evaluation: determining the sign of the difference in an evaluation measure. Other evaluation tasks could be defined; estimating the magnitude of the difference or the values of the measures themselves are examples that entail different notions of confidence. We therefore see confidence as a probability estimate. One of the questions we must ask about a probability estimate is what it means. What does it mean to have 75% confidence that system A is better than system B? As described above, we want it to mean that if we continue to judge documents, there will only be a 25% chance that our assessment will change. If this is what it means, we can trust the confidence estimates. But do we know it has that meaning? Our calculation of confidence rested on an assumption about the probability of relevance of unjudged documents, specifically that each unjudged document was equally likely to be relevant or nonrelevant. This assumption is almost certainly not realistic in most IR applications. As it turns out, it is this assumption that determines whether the confidence estimates can eb trusted. Before elaborating on this, we formally define confidence. 2.1 Estimating Confidence Average precision (AP) is a standard evaluation metric that captures both the ability of a system to rank relevant documents highly (precision) as well as its ability to retrieve relevant documents (recall). It is typically written as the mean precision at the ranks of relevant documents: AP = 1 |R| i∈R prec@r(i) where R is the set of relevant documents and r(i) is the rank of document i. Let Xi be a random variable indicating the relevance of document i. If documents are ordered by rank, we can express precision as prec@i = 1/i i j=1 Xj . Average precision then becomes the quadratic equation AP = 1 Xi n i=1 Xi/i i j=1 Xj = 1 Xi n i=1 j≥i aijXiXj where aij = 1/ max{r(i), r(j)}. Using aij instead of 1/i allows us to number the documents arbitrarily. To see why this is true, consider a toy example: a list of 3 documents with relevant documents B, C at ranks 1 and 3 and nonrelevant document A at rank 2. Average precision will be 1 2 (1 1 x2 B+ 1 2 xBxA+ 1 3 xBxC + 1 2 x2 A+ 1 3 xAxC + 1 3 x2 C) = 1 2 1 + 2 3 because xA = 0, xB = 1, xC = 1. Though the ordering B, A, C is different from the labeling A, B, C, it does not affect the computation. We can now see average precision itself is a random variable with a distribution over all possible assignments of relevance to all documents. This random variable has an expectation, a variance, confidence intervals, and a certain probability of being less than or equal to a given value. All of these are dependent on the probability that document i is relevant: pi = p(Xi = 1). Suppose in our previous example we do not know the relevance judgments, but we believe pA = 0.4, pB = 0.8, pC = 0.7. We can then compute e.g. P(AP = 0) = 0.2 · 0.6 · 0.3 = 0.036, or P(AP = 1 2 ) = 0.2 · 0.4 · 0.7 = 0.056. Summing over all possibilities, we can compute expectation and variance: E[AP] ≈ 1 pi aiipi + j>i aij pipj V ar[AP] ≈ 1 ( pi)2 n i a2 iipiqi + j>i a2 ijpipj(1 − pipj) + i=j 2aiiaijpipj(1 − pi) + k>j=i 2aijaikpipjpk(1 − pi) AP asymptotically converges to a normal distribution with expectation and variance as defined above.1 For our comparative evaluation task we are interested in the sign of the difference in two average precisions: ΔAP = AP1 − AP2. As we showed in our previous work, ΔAP has a closed form when documents are ordered arbitrarily: ΔAP = 1 Xi n i=1 j≥i cij XiXj cij = aij − bij where bij is defined analogously to aij for the second ranking. Since AP is normal, ΔAP is normal as well, meaning we can use the normal cumulative density function to determine the confidence that a difference in AP is less than zero. Since topics are independent, we can easily extend this to mean average precision (MAP). MAP is also normally distributed; its expectation and variance are: EMAP = 1 T t∈T E[APt] (1) VMAP = 1 T2 t∈T V ar[APt] ΔMAP = MAP1 − MAP2 Confidence can then be estimated by calculating the expectation and variance and using the normal density function to find P(ΔMAP < 0). 2.2 Confidence and Robustness Having defined confidence, we turn back to the issue of trust in confidence estimates, and show how it ties into the robustness of the collection to missing judgments. 1 These are actually approximations to the true expectation and variance, but the error is a negligible O(n2−n ). Let Z be the set of all pairs of ranked results for a common set of topics. Suppose we have a set of m relevance judgments xm = {x1, x2, ..., xm} (using small x rather than capital X to distinguish between judged and unjudged documents); these are the judgments against which we compute confidence. Let Zα be the subset of pairs in Z for which we predict that ΔMAP = −1 with confidence α given the judgments xm . For the confidence estimates to be accurate, we need at least α · |Zα| of these pairs to actually have ΔMAP = −1 after we have judged every document. If they do, we can trust the confidence estimates; our evaluation will be robust to missing judgments. If our confidence estimates are based on unrealistic assumptions, we cannot expect them to be accurate. The assumptions they are based on are the probabilities of relevance pi. We need these to be realistic. We argue that the best possible distribution of relevance p(Xi) is the one that explains all of the data (all of the observations made about the retrieval systems) while at the same time making no unwarranted assumptions. This is known as the principle of maximum entropy [13]. The entropy of a random variable X with distribution p(X) is defined as H(p) = − i p(X = i) log p(X = i). This has found a wide array of uses in computer science and information retrieval. The maximum entropy distribution is the one that maximizes H. This distribution is unique and has an exponential form. The following theorem shows the utility of a maximum entropy distribution for relevance when estimating confidence. Theorem 1. If p(Xn |I, xm ) = argmaxpH(p), confidence estimates will be accurate. where xm is the set of relevance judgments defined above, Xn is the full set of documents that we wish to estimate the relevance of, and I is some information about the documents (unspecified as of now). We forgo the proof for the time being, but it is quite simple. This says that the better the estimates of relevance, the more accurate the evaluation. The task of creating a reusable test collection thus becomes the task of estimating the relevance of unjudged documents. The theorem and its proof say nothing whatsoever about the evaluation metric. The probability estimates are entirely indepedent of the measure we are interested in. This means the same probability estimates can tell us about average precision as well as precision, recall, bpref, etc. Furthermore, we could assume that the relevance of documents i and j is independent and achieve the same result, which we state as a corollary: Corollary 1. If p(Xi|I, xm ) = argmaxpH(p), confidence estimates will be accurate. The task therefore becomes the imputation of the missing values of relevance. The theorem implies that the closer we get to the maximum entropy distribution of relevance, the closer we get to robustness. 3. PREDICTING RELEVANCE In our statement of Theorem 1, we left the nature of the information I unspecified. One of the advantages of our confidence estimates is that they admit information from a wide variety of sources; essentially anything that can be modeled can be used as information for predicting relevance. A natural source of information is the retrieval systems themselves: how they ranked the judged documents, how often they failed to rank relevant documents, how they perform across topics, and so on. If we treat each system as an information retrieval expert providing an opinion about the relevance of each document, the problem becomes one of expert opinion aggregation. This is similar to the metasearch or data fusion problem in which the task is to take k input systems and merge them into a single ranking. Aslam et al. [3] previously identified a connection between evaluation and metasearch. Our problem has two key differences: 1. We explicitly need probabilities of relevance that we can plug into Eq. 1; metasearch algorithms have no such requirement. 2. We are accumulating relevance judgments as we proceed with the evaluation and are able to re-estimate relevance given each new judgment. In light of (1) above, we introduce a probabilistic model for expert combination. 3.1 A Model for Expert Opinion Aggregation Suppose that each expert j provides a probability of relevance qij = pj(Xi = 1). The information about the relevance of document i will then be the set of k expert opinions I = qi = (qi1, qi2, · · · , qik). The probability distribution we wish to find is the one that maximizes the entropy of pi = p(Xi = 1|qi). As it turns out, finding the maximum entropy model is equivalent to finding the parameters that maximize the likelihood [5]. Blower [6] explicitly shows that finding the maximum entropy model for a binary variable is equivalent to solving a logistic regression. Then pi = p(Xi = 1|qi) = exp k j=1 λjqij 1 + exp k j=1 λj qij (2) where λ1, · · · , λk are the regression parameters. We include a beta prior for p(λj) with parameters α, β. This can be seen as a type of smoothing to account for the fact that the training data is highly biased. This model has the advantage of including the statistical dependence between the experts. A model of the same form was shown by Clemen & Winkler to be the best for aggregating expert probabilities [10]. A similar maximumentropy-motivated approach has been used for expert aggregation [15]. Aslam & Montague [1] used a similar model for metasearch, but assumed independence among experts. Where do the qij s come from? Using raw, uncalibrated scores as predictors will not work because score distributions vary too much between topics. A language modeling ranker, for instance, will typically give a much higher score to the top retrieved document for a short query than to the top retrieved document for a long query. We could train a separate predicting model for each topic, but that does not take advantage of all of the information we have: we may only have a handful of judgments for a topic, not enough to train a model to any confidence. Furthermore, it seems reasonable to assume that if an expert makes good predictions for one topic, it will make good predictions for other topics as well. We could use a hierarchical model [12], but that will not generalize to unseen topics. Instead, we will calibrate the scores of each expert individually so that scores can be compared both within topic and between topic. Thus our model takes into account not only the dependence between experts, but also the dependence between experts performances on different tasks (topics). 3.2 Calibrating Experts Each expert gives us a score and a rank for each document. We need to convert these to probabilities. A method such as the one used by Manmatha et al. [14] could be used to convert scores into probabilities of relevance. The pairwise preference method of Carterette & Petkova [9] could also be used, interpeting the ranking of one document over another as an expression of preference. Let q∗ ij be expert js self-reported probability that document i is relevant. Intuitively it seems clear that q∗ ij should decrease with rank, and it should be zero if document i is unranked (the expert did not believe it to be relevant). The pairwise preference model can handle these two requirements easily, so we will use it. Let θrj (i) be the relevance coefficient of the document at rank rj(i). We want to find the θs that maximize the likelihood function: Ljt(Θ) = rj (i)<rj (k) exp(θrj (i) − θrj (k)) 1 + exp(θrj (i) − θrj (k)) We again include a beta prior on p(θrj(i)) with parameters |Rt| + 1 and |Nt| + 1, the size of the sets of judged relevant and nonrelevant documents respectively. Using these as prior parameters ensures that the resulting probabilities will be concentrated around the ratio of relevant documents that have been discovered for topic t. This means that the probability estimates decrease by rank and are higher for topics that have more relevant documents. After finding the Θ that maximizes the likelihood, we have q∗ ij = exp(θrj (i)) 1+exp(θrj (i)) . We define θ∞ = −∞, so that the probability that an unranked document is relevant is 0. Since q∗ ij is based on the rank at which a document is retrieved rather than the identity of the document itself, the probabilities are identical from expert to expert, e.g. if expert E put document A at rank 1, and expert D put document B at rank 1, we will have q∗ AE = q∗ BD. Therefore we only have to solve this once for each topic. The above model gives topic-independent probabilities for each document. But suppose an expert who reports 90% probability is only right 50% of the time. Its opinion should be discounted based on its observed performance. Specifically, we want to learn a calibration function qij = Cj(q∗ ij) that will ensure that the predicted probabilities are tuned to the experts ability to retrieve relevant documents given the judgments that have been made to this point. Platts SVM calibration method [16] fits a sigmoid function between q∗ ij and the relevance judgments to obtain qij = Cj (q∗ ij) = exp(Aj +Bjq∗ ij ) 1+exp(Aj +Bj q∗ ij ) . Since q∗ ij is topic-independent, we only need to learn one calibration function for each expert. Once we have the calibration function, it is applied to adjust the experts predictions to their actual performance. The calibrated probabilities are plugged into model (2) to find the document probabilities. Figure 1: Conceptual diagram of our aggregation model. Experts E1, E2 have ranked documents A, B, C for topic T1 and documents D, E, F for topic T2. The first step is to obtain q∗ ij. Next is calibration to true performance to find qij . Finally we obtain pi = p(Xi = 1|qi1, qi2), · · · . 3.3 Model Summary Our model has three components that differ by the data they take as input and what they produce as output. A conceptual diagram is shown in Figure 1. 1. ranks → probabilities (per system per topic). This gives us q∗ ij, expert js self-reported probability of the relevance of document i. This is unsupervised; it requires no labeled data (though if we have some, we use it to set prior parameters). 2. probabilities → calibrated probabilities (per system). This gives us qij = Cj (q∗ ij), expert js calibrated probability of the relevance of document i. This is semisupervised; we have relevance judgments at some ranks which we use to impute the probability of relevance at other ranks. 3. calibrated probabilities → document probabilities. This gives us pi = p(Xi = 1|qi), the probability of relevance of document i given calibrated expert probabilities qij . This is supervised; we learn coefficients from a set of judged documents and use those to estimate the relevance of the unjudged documents. Although the model appears rather complex, it is really just three successive applications of logistic regression. As such, it can be implemented in a statistical programming language such as R in a few lines of code. The use of beta (conjugate) priors ensures that no expensive computational methods such as MCMC are necessary [12], so the model is trained and applied fast enough to be used on-line. Our code is available at http://ciir.cs.umass.edu/~carteret/. 4. EXPERIMENTS Three hypotheses are under consideration. The first, and most important, is that using our expert aggregation model to predict relevance produces test collections that are robust enough to be reusable; that is, we can trust the estimates of confidence when we evaluate systems that did not contribute any judgments to the pool. The other two hypotheses relate to the improvement we see by using better estimates of relevance than we did in our previous work [8]. These are that (a) it takes fewer relevance track no. topics no. runs no. judged no. rel ad-hoc 94 50 40 97,319 9,805 ad-hoc 95 49 33 87,069 6,503 ad-hoc 96 50 61 133,681 5,524 ad-hoc 97 50 74 72,270 4,611 ad-hoc 98 50 103 80,345 4,674 ad-hoc 99 50 129 86,830 4,728 web 04 225 74 88,566 1,763 robust 05 50 74 37,798 6,561 terabyte 05 50 58 45,291 10,407 Table 1: Number of topics, number of runs, number of documents judged, and number found relevant for each of our data sets. judgments to reach 95% confidence and (b) the accuracy of the predictions is higher than if we were to simply assume pi = 0.5 for all unjudged documents. 4.1 Data We obtained full ad-hoc runs submitted to TRECs 3 through 8. Each run ranks at most 1000 documents for 50 topics (49 topics for TREC 4). Additionally, we obtained all runs from the Web track of TREC 13, the Robust2 track of TREC 14, and the Terabyte (ad-hoc) track of TREC 14. These are the tracks that have replaced the ad-hoc track since its end in 1999. Statistics are shown in Table 1. We set aside the TREC 4 (ad-hoc 95) set for training, TRECs 3 and 5-8 (ad-hoc 94 and 96-99) for primary testing, and the remaining sets for additional testing. We use the qrels files assembled by NIST as truth. The number of relevance judgments made and relevant documents found for each track are listed in Table 1. For computational reasons, we truncate ranked lists at 100 documents. There is no reason that we could not go deeper, but calculating variance is O(n3 ) and thus very timeconsuming. Because of the reciprocal rank nature of AP, we do not lose much information by truncating at rank 100. 4.2 Algorithms We will compare three algorithms for acquiring relevance judgments. The baseline is a variation of TREC pooling that we will call incremental pooling (IP). This algorithm takes a number k as input and presents the first k documents in rank order (without regard to topic) to be judged. It does not estimate the relevance of unjudged documents; it simply assumes any unjudged document is nonrelevant. The second algorithm is that presented in Carterette et al. [8] (Algorithm 1). Documents are selected based on how interesting they are in determining whether a difference in mean average precision exists. For this approach pi = 0.5 for all i; there is no estimation of probabilities. We will call this MTC for minimal test collection. The third algorithm augments MTC with updated estimates of probabilities of relevance. We will call this RTC for robust test collection. It is identical to Algorithm 1, except that every 10th iteration we estimate pi for all unjudged documents i using the expert aggregation model of Section 3. RTC has smoothing (prior distribution) parameters that must be set. We trained using the ad-hoc 95 set. We limited 2 Robust here means robust retrieval; this is different from our goal of robust evaluation. Algorithm 1 (MTC) Given two ranked lists and confidence level α, predict the sign of ΔMAP. 1: pi ← 0.5 for all documents i 2: while P(ΔMAP < 0) < α do 3: calculate weight wi for all unjudged documents i (see Carterette et al. [8] for details) 4: j ← argmaxiwi 5: xj ← 1 if document j is relevant, 0 otherwise 6: pj ← xj 7: end while the search to uniform priors with relatively high variance. For expert aggregation, the prior parameters are α = β = 1. 4.3 Experimental Design First, we want to know whether we can augment a set of relevance judgments with a set of relevance probabilities in order to reuse the judgments to evaluate a new set of systems. For each experimental trial: 1. Pick a random subset of k runs. 2. From those k, pick an initial c < k to evaluate. 3. Run RTC to 95% confidence on the initial c. 4. Using the model from Section 3, estimate the probabilities of relevance for all documents retrieved by all k runs. 5. Calculate EMAP for all k runs, and P(ΔMAP < 0) for all pairs of runs. We do the same for MTC, but omit step 4. Note that after evaluating the first c systems, we make no additional relevance judgments. To put our method to the test, we selected c = 2: we will build a set of judgments from evaluating only two initial systems. We will then generalize to a set of k = 10 (of which those two are a subset). As we run more trials, we obtain the data we need to test all three of our hypotheses. 4.4 Experimental Evaluation Recall that a set of judgments is robust if the accuracy of the predictions it makes is at least its estimated confidence. One way to evaluate robustness is to bin pairs by their confidence, then calculate the accuracy over all the pairs in each bin. We would like the accuracy to be no less than the lowest confidence score in the bin, but preferably higher. Since summary statistics are useful, we devised the following metric. Suppose we are a bookmaker taking bets on whether ΔMAP < 0. We use RTC or MTC to set the odds O = P (ΔMAP <0) 1−P (ΔMAP <0) . Suppose a bettor wagers $1 on ΔMAP ≥ 0. If it turns out that ΔMAP < 0, we win the dollar. Otherwise, we pay out O. If our confidence estimates are perfectly accurate, we break even. If confidence is greater than accuracy, we lose money; we win if accuracy is greater than confidence. Counterintuitively, the most desirable outcome is breaking even: if we lose money, we cannot trust the confidence estimates, but if we win money, we have either underestimated confidence or judged more documents than necessary. However, the cost of not being able to trust the confidence estimates is higher than the cost of extra relevance judgments, so we will treat positive outcomes as good. The amount we win on each pairwise comparison i is: Wi = yi − (1 − yi) Pi 1 − Pi = yi − Pi 1 − Pi yi = 1 if ΔMAP < 0 and 0 otherwise, and Pi = P(ΔMAP < 0). The summary statistic is W, the mean of Wi. Note that as Pi increases, we lose more for being wrong. This is as it should be: the penalty should be great for missing the high probability predictions. However, since our losses grow without bound as predictions approach certainty, we cap −Wi at 100. For our hypothesis that RTC requires fewer judgments than MTC, we are interested in the number of judgments needed to reach 95% confidence on the first pair of systems. The median is more interesting than the mean: most pairs require a few hundred judgments, but a few pairs require several thousand. The distribution is therefore highly skewed, and the mean strongly affected by those outliers. Finally, for our hypothesis that RTC is more accurate than MTC, we will look at Kendalls τ correlation between a ranking of k systems by a small set of judgments and the true ranking using the full set of judgments. Kendalls τ, a nonparametric statistic based on pairwise swaps between two lists, is a standard evaluation for this type of study. It ranges from −1 (perfectly anti-correlated) to 1 (rankings identical), with 0 meaning that half of the pairs are swapped. As we touched on in the introduction, though, an accuracy measure like rank correlation is not a good evaluation of reusability. We include it for completeness. 4.4.1 Hypothesis Testing Running multiple trials allows the use of statistical hypothesis testing to compare algorithms. Using the same sets of systems allows the use of paired tests. As we stated above, we are more interested in the median number of judgments than the mean. A test for difference in median is the Wilcoxon sign rank test. We can also use a paired t-test to test for a difference in mean. For rank correlation, we can use a paired t-test to test for a difference in τ. 5. RESULTS AND ANALYSIS The comparison between MTC and RTC is shown in Table 2. With MTC and uniform probabilities of relevance, the results are far from robust. We cannot reuse the relevance judgments with much confidence. But with RTC, the results are very robust. There is a slight dip in accuracy when confidence gets above 0.95; nonetheless, the confidence predictions are trustworthy. Mean Wi shows that RTC is much closer to 0 than MTC. The distribution of confidence scores shows that at least 80% confidence is achieved more than 35% of the time, indicating that neither algorithm is being too conservative in its confidence estimates. The confidence estimates are rather low overall; that is because we have built a test collection from only two initial systems. Recall from Section 1 that we cannot require (or even expect) a minimum level of confidence when we generalize to new systems. More detailed results for both algorithms are shown in Figure 2. The solid line is the ideal result that would give W = 0. RTC is on or above this line at all points until confidence reaches about 0.97. After that there is a slight dip in accuracy which we discuss below. Note that both MTC RTC confidence % in bin accuracy % in bin accuracy 0.5 − 0.6 33.7% 61.7% 28.6% 61.9% 0.6 − 0.7 18.1% 73.1% 20.1% 76.3% 0.7 − 0.8 10.4% 70.1% 15.5% 78.0% 0.8 − 0.9 9.4% 69.0% 12.1% 84.9% 0.9 − 0.95 7.3% 78.0% 6.6% 93.1% 0.95 − 0.99 17.9% 70.4% 12.4% 93.4% 1.0 3.3% 68.3% 4.7% 98.9% W −5.34 −0.39 median judged 251 235 mean τ 0.393 0.555 Table 2: Confidence that P(ΔMAP < 0) and accuracy of prediction when generalizing a set of relevance judgments acquired using MTC and RTC. Each bin contains over 1,000 trials from the adhoc 3, 5-8 sets. RTC is much more robust than MTC. W is defined in Section 4.4; closer to 0 is better. Median judged is the number of judgments to reach 95% confidence on the first two systems. Mean τ is the average rank correlation for all 10 systems. 0.5 0.6 0.7 0.8 0.9 1 0.5 0.6 0.7 0.8 0.9 1 accuracy confidence breakeven RTC MTC Figure 2: Confidence vs. accuracy of MTC and RTC. The solid line is the perfect result that would give W = 0; performance should be on or above this line. Each point represents at least 500 pairwise comparisons. algorithms are well above the line up to around confidence 0.7. This is because the baseline performance on these data sets is high; it is quite easy to achieve 75% accuracy doing very little work [7]. Number of Judgments: The median number of judgments required by MTC to reach 95% confidence on the first two systems is 251, an average of 5 per topic. The median required by RTC is 235, about 4.7 per topic. Although the numbers are close, RTCs median is significantly lower by a paired Wilcoxon test (p < 0.0001). For comparison, a pool of depth 100 would result in a minimum of 5,000 judgments for each pair. The difference in means is much greater. MTC required a mean of 823 judgments, 16 per topic, while RTC required a mean of 502, 10 per topic. (Recall that means are strongly skewed by a few pairs that take thousands of judgments.) This difference is significant by a paired t-test (p < 0.0001). Ten percent of the sets resulted in 100 or fewer judgments (less than two per topic). Performance on these is very high: W = 0.41, and 99.7% accuracy when confidence is at least 0.9. This shows that even tiny collections can be reusable. For the 50% of sets with more than 235 judgments, accuracy is 93% when confidence is at least 0.9. Rank Correlation: MTC and RTC both rank the 10 systems by EMAP (Eq. (1)) calculated using their respective probability estimates. The mean τ rank correlation between true MAP and EMAP is 0.393 for MTC and 0.555 for RTC. This difference is significant by a paired t-test (p < 0.0001). Note that we do not expect the τ correlations to be high, since we are ranking the systems with so few relevance judgments. It is more important that we estimate confidence in each pairwise comparison correctly. We ran IP for the same number of judgments that MTC took for each pair, then ranked the systems by MAP using only those judgments (all unjudged documents assumed nonrelevant). We calculated the τ correlation to the true ranking. The mean τ correlation is 0.398, which is not significantly different from MTC, but is significantly lower than RTC. Using uniform estimates of probability is indistinguishable from the baseline, whereas estimating relevance by expert aggregation boosts performance a great deal: nearly 40% over both MTC and IP. Overfitting: It is possible to overfit: if too many judgments come from the first two systems, the variance in ΔMAP is reduced and the confidence estimates become unreliable. We saw this in Table 2 and Figure 2 where RTC exhibits a dip in accuracy when confidence is around 97%. In fact, the number of judgments made prior to a wrong prediction is over 50% greater than the number made prior to a correct prediction. Overfitting is difficult to quantify exactly, because making more relevance judgments does not always cause it: at higher confidence levels, more relevance judgments are made, and as Table 2 shows, accuracy is greater at those higher confidences. Obviously having more relevance judgments should increase both confidence and accuracy; the difference seems to be when one system has a great deal more judgments than the other. Pairwise Comparisons: Our pairwise comparisons fall into one of three groups: 1. the two original runs from which relevance judgments are acquired; 2. one of the original runs vs. one of the new runs; 3. two new runs. Table 3 shows confidence vs. accuracy results for each of these three groups. Interestingly, performance is worst when comparing one of the original runs to one of the additional runs. This is most likely due to a large difference in the number of judgments affecting the variance of ΔMAP. Nevertheless, performance is quite good on all three subsets. Worst Case: The case intuitively most likely to produce an error is when the two systems being compared have retrieved very few documents in common. If we want the judgments to be reusable, we should to be able to generalize even to runs that are very different from the ones used to acquire the relevance judgments. A simple measure of similarity of two runs is the average percentage of documents they retrieved in common for each topic [2]. We calculated this for all pairs, then looked at performance on pairs with low similarity. Results are shown in accuracy confidence two original one original no original 0.5 − 0.6 - 48.1% 62.8% 0.6 − 0.7 - 57.1% 79.2% 0.7 − 0.8 - 67.9% 81.7% 0.8 − 0.9 - 82.2% 86.3% 0.9 − 0.95 95.9% 93.7% 92.6% 0.95 − 0.99 96.2% 92.5% 93.1% 1.0 100% 98.0% 99.1% W −1.11 −0.87 −0.27 Table 3: Confidence vs. accuracy of RTC when comparing the two original runs, one original run and one new run, and two new runs. RTC is robust in all three cases. accuracy when similar confidence 0 − 0.1 0.1 − 0.2 0.2 − 0.3 0.5 − 0.6 68.4% 63.1% 61.4% 0.6 − 0.7 84.2% 78.6% 76.6% 0.7 − 0.8 82.0% 79.8% 78.9% 0.8 − 0.9 93.6% 83.3% 82.1% 0.9 − 0.95 99.3% 92.7% 92.4% 0.95 − 0.99 98.7% 93.4% 93.3% 1.0 99.9% 97.9% 98.1% W 0.44 −0.45 −0.49 Table 4: Confidence vs. accuracy of RTC when a pair of systems retrieved 0-30% documents in common (broken out into 0%-10%, 10%-20%, and 20%30%). RTC is robust in all three cases. Table 4. Performance is in fact very robust even when similarity is low. When the two runs share very few documents in common, W is actually positive. MTC and IP both performed quite poorly in these cases. When the similarity was between 0 and 10%, both MTC and IP correctly predicted ΔMAP only 60% of the time, compared to an 87.6% success rate for RTC. By Data Set: All the previous results have only been on the ad-hoc collections. We did the same experiments on our additional data sets, and broke out the results by data set to see how performance varies. The results in Table 5 show everything about each set, including binned accuracy, W, mean τ, and median number of judgments to reach 95% confidence on the first two systems. The results are highly consistent from collection to collection, suggesting that our method is not overfitting to any particular data set. 6. CONCLUSIONS AND FUTURE WORK In this work we have offered the first formal definition of the common idea of reusability of a test collection and presented a model that is able to achieve reusability with very small sets of relevance judgments. Table 2 and Figure 2 together show how biased a small set of judgments can be: MTC is dramatically overestimating confidence and is much less accurate than RTC, which is able to remove the bias to give a robust evaluation. The confidence estimates of RTC, in addition to being accurate, provide a guide for obtaining additional judgments: focus on judging documents from the lowest-confidence comparisons. In the long run, we see small sets of relevance judgaccuracy confidence ad-hoc 94 ad-hoc 96 ad-hoc 97 ad-hoc 98 ad-hoc 99 web 04 robust 05 terabyte 05 0.5 − 0.6 64.1% 61.8% 62.2% 62.0% 59.4% 64.3% 61.5% 61.6% 0.6 − 0.7 76.1% 77.8% 74.5% 78.2% 74.3% 78.1% 75.9% 75.9% 0.7 − 0.8 75.2% 78.9% 77.6% 80.0% 78.6% 82.6% 77.5% 80.4% 0.8 − 0.9 83.2% 85.5% 84.6% 84.9% 86.8% 84.5% 86.7% 87.7% 0.9 − 0.95 93.0% 93.6% 92.8% 93.7% 92.6% 94.2% 93.9% 94.2% 0.95 − 0.99 93.1% 94.3% 93.1% 93.7% 92.8% 95.0% 93.9% 91.6% 1.0 99.2% 96.8% 98.7% 99.5% 99.6% 100% 99.2% 98.3% W -0.34 -0.34 -0.48 -0.35 -0.44 -0.07 -0.41 -0.67 median judged 235 276 243 213 179 448 310 320 mean τ 0.538 0.573 0.556 0.579 0.532 0.596 0.565 0.574 Table 5: Accuracy, W, mean τ, and median number of judgments for all 8 testing sets. The results are highly consistent across data sets. ments being shared by researchers, each group contributing a few more judgments to gain more confidence about their particular systems. As time goes on, the number of judgments grows until there is 100% confidence in every evaluation-and there is a full test collection for the task. We see further use for this method in scenarios such as web retrieval in which the corpus is frequently changing. It could be applied to evaluation on a dynamic test collection as defined by Soboroff [18]. The model we presented in Section 3 is by no means the only possibility for creating a robust test collection. A simpler expert aggregation model might perform as well or better (though all our efforts to simplify failed). In addition to expert aggregation, we could estimate probabilities by looking at similarities between documents. This is an obvious area for future exploration. Additionally, it will be worthwhile to investigate the issue of overfitting: the circumstances it occurs under and what can be done to prevent it. In the meantime, capping confidence estimates at 95% is a hack that solves the problem. We have many more experimental results that we unfortunately did not have space for but that reinforce the notion that RTC is highly robust: with just a few judgments per topic, we can accurately assess the confidence in any pairwise comparison of systems. Acknowledgments This work was supported in part by the Center for Intelligent Information Retrieval and in part by the Defense Advanced Research Projects Agency (DARPA) under contract number HR0011-06-C-0023. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the author and do not necessarily reflect those of the sponsor. 7. REFERENCES [1] J. Aslam and M. Montague. Models for Metasearch. In Proceedings of SIGIR, pages 275-285, 2001. [2] J. Aslam and R. Savell. On the effectiveness of evaluating retrieval systems in the absence of relevance judgments. In Proceedings of SIGIR, pages 361-362, 2003. [3] J. A. Aslam, V. Pavlu, and R. Savell. A unified model for metasearch, pooling, and system evaluation. In Proceedings of CIKM, pages 484-491, 2003. [4] J. A. Aslam, V. Pavlu, and E. Yilmaz. A statistical method for system evaluation using incomplete judgments. In Proceedings of SIGIR, pages 541-548, 2006. [5] A. L. Berger, S. D. Pietra, and V. J. D. Pietra. A maximum entropy approach to natural language processing. Computational Linguistics, 22(1):39-71, 1996. [6] D. J. Blower. An easy derivation of logistic regression from the bayesian and maximum entropy perspective. In Proceedings of the 23rd International Workship on Bayesian Inference and Maximum Entropy Methods in Science and Engineering, pages 30-43, 2004. [7] B. Carterette and J. Allan. Research methodology in studies of assessor effort for retrieval evaluation. In Proceedings of RIAO, 2007. [8] B. Carterette, J. Allan, and R. K. Sitaraman. Minimal test collections for retrieval evaluation. In Proceedings of SIGIR, pages 268-275, 2006. [9] B. Carterette and D. I. Petkova. Learning a ranking from pairwise preferences. In Proceedings of SIGIR, 2006. [10] R. T. Clemen and R. L. Winkler. Unanimity and compromise among probability forecasters. Management Science, 36(7):767-779, July 1990. [11] G. V. Cormack, C. R. Palmer, and C. L. Clarke. Efficient Construction of Large Test Collections. In Proceedings of SIGIR, pages 282-289, 1998. [12] A. Gelman, J. B. Carlin, H. S. Stern, and D. B. Rubin. Bayesian Data Analysis. Chapman & Hall/CRC, 2004. [13] E. T. Jaynes. Probability Theory: The Logic of Science. Cambridge University Press, 2003. [14] R. Manmatha and H. Sever. A Formal Approach to Score Normalization for Metasearch. In Proceedings of HLT, pages 88-93, 2002. [15] I. J. Myung, S. Ramamoorti, and J. Andrew D. Baily. Maximum entropy aggregation of expert predictions. Management Science, 42(10):1420-1436, October 1996. [16] J. Platt. Probabilistic outputs for support vector machines and comparison to regularized likelihood methods. pages 61-74, 2000. [17] M. Sanderson and H. Joho. Forming test collections with no system pooling. In Proceedings of the 27th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 33-40, 2004. [18] I. Soboroff. Dynamic test collections: measuring search effectiveness on the live web. In Proceedings of SIGIR, pages 276-283, 2006. [19] K. Sparck Jones and C. J. van Rijsbergen. Information Retrieval Test Collections. Journal of Documentation, 32(1):59-75, 1976. [20] E. M. Voorhees and D. K. Harman, editors. TREC: Experiment and Evaluation in Information Retrieval. MIT Press, 2005. [21] J. Zobel. How Reliable are the Results of Large-Scale Information Retrieval Experiments? In Proceedings of SIGIR, pages 307-314, 1998.",
    "original_translation": "Colecciones de pruebas robustas para la evaluación de recuperación Ben Carterette Centro de Recuperación de Información Inteligente Departamento de Ciencias de la Computación Universidad de Massachusetts Amherst Amherst, MA 01003 carteret@cs.umass.edu RESUMEN Los métodos de bajo costo para adquirir juicios de relevancia pueden ser una bendición para los investigadores que necesitan evaluar nuevas tareas o temas de recuperación pero no tienen los recursos para hacer miles de juicios. Si bien estos juicios son muy útiles para una evaluación única, no está claro que se puedan confiar al ser reutilizados para evaluar nuevos sistemas. En este trabajo, definimos formalmente lo que significa que las evaluaciones sean reutilizables: la confianza en una evaluación de nuevos sistemas puede ser evaluada con precisión a partir de un conjunto existente de juicios de relevancia. Luego presentamos un método para aumentar un conjunto de juicios de relevancia con estimaciones de relevancia que no requieren esfuerzo adicional del evaluador. Usar este método garantiza prácticamente la reutilización: con tan solo cinco juicios por tema tomados de solo dos sistemas, podemos evaluar de manera confiable un conjunto más grande de diez sistemas. Incluso los conjuntos más pequeños de juicios pueden ser útiles para la evaluación de nuevos sistemas. Categorías y Descriptores de Asignaturas: H.3 Almacenamiento y Recuperación de Información; H.3.4 Sistemas y Software: Evaluación del Rendimiento Términos Generales: Experimentación, Medición, Confiabilidad 1. INTRODUCCIÓN Imagina a un investigador de recuperación de información que ha inventado una nueva tarea de recuperación. Ella ha construido un sistema para realizar la tarea y quiere evaluarlo. Dado que la tarea es nueva, es poco probable que existan juicios de relevancia vigentes. Ella no tiene el tiempo ni los recursos para juzgar cada documento, ni siquiera cada documento recuperado. Ella solo puede juzgar los documentos que parecen ser los más informativos y detenerse cuando tenga un grado razonable de confianza en sus conclusiones. Pero ¿qué sucede cuando ella desarrolla un nuevo sistema y necesita evaluarlo? ¿Otro grupo de investigación decide implementar un sistema para realizar la tarea? ¿Pueden reutilizar de manera confiable los juicios originales? ¿Pueden evaluar sin más juicios de relevancia? La evaluación es un aspecto importante de la investigación en recuperación de información, pero es solo un problema parcialmente resuelto: para la mayoría de las tareas de recuperación, es imposible juzgar la relevancia de cada documento; simplemente hay demasiados. La solución utilizada por NIST en TREC (Text REtrieval Conference) es el método de agrupación [19, 20]: todos los sistemas competidores contribuyen con N documentos a un grupo, y cada documento en ese grupo es evaluado. Este método crea grandes conjuntos de juicios que son reutilizables para entrenar o evaluar nuevos sistemas que no contribuyeron al conjunto [21]. Esta solución no es adecuada para nuestro investigador hipotético. El método de agrupación proporciona miles de juicios de relevancia, pero requiere muchas horas de tiempo de anotadores (remunerados). Como resultado, ha habido una serie de artículos recientes sobre la reducción del esfuerzo del anotador en la producción de colecciones de pruebas: Cormack et al. [11], Zobel [21], Sanderson y Joho [17], Carterette et al. [8], y Aslam et al. [4], entre otros. Como veremos, los juicios que estos métodos producen pueden sesgar significativamente la evaluación de un nuevo conjunto de sistemas. ¿Volviendo a nuestra investigadora hipotética, puede reutilizar sus juicios de relevancia? Primero debemos definir formalmente lo que significa ser reutilizable. En trabajos anteriores, la reutilización se ha probado simplemente evaluando la precisión de un conjunto de juicios de relevancia al evaluar sistemas no vistos. Si bien podemos decir que tuvo razón el 75% del tiempo, o que tuvo una correlación de rango de 0.8, estos números no tienen ningún poder predictivo: no nos dicen qué sistemas es probable que estén equivocados ni cuán seguros deberíamos estar en uno en particular. Necesitamos una definición más cuidadosa de reutilización. Específicamente, la cuestión de la reutilización no es qué tan precisamente podemos evaluar los nuevos sistemas. Un adversario malicioso siempre puede producir una nueva lista clasificada que no haya recuperado ninguno de los documentos evaluados. La verdadera pregunta es cuánta confianza tenemos en nuestras evaluaciones, y, más importante aún, si podemos confiar en nuestras estimaciones de confianza. Aunque la confianza no sea alta, siempre y cuando podamos confiar en ello, podemos identificar qué sistemas necesitan más juicios para aumentar la confianza. Cualquier conjunto de juicios, por pequeño que sea, se vuelve reutilizable en cierta medida. Pequeñas colecciones de pruebas reutilizables podrían tener un gran impacto en la investigación de recuperación de información. Los grupos de investigación podrían compartir las evaluaciones de relevancia que han realizado internamente para estudios piloto, nuevas tareas o nuevos temas. La cantidad de datos disponible para los investigadores crecería de forma exponencial con el tiempo. EVALUACIÓN ROBUSTA Anteriormente dimos una definición intuitiva de reutilización: una colección es reutilizable si podemos confiar en nuestras estimaciones de confianza en una evaluación. Con eso queremos decir que si hemos realizado algunos juicios de relevancia y tenemos, por ejemplo, un 75% de confianza en que el sistema A es mejor que el sistema B, nos gustaría que no haya más del 25% de posibilidad de que nuestra evaluación de la calidad relativa de los sistemas cambie a medida que continuamos evaluando documentos. Nuestra evaluación debe ser robusta ante juicios faltantes. En nuestro trabajo anterior, definimos la confianza como la probabilidad de que la diferencia en una medida de evaluación calculada para dos sistemas sea menor que cero [8]. Esta noción de confianza se define en el contexto de una tarea de evaluación particular que llamamos evaluación comparativa: determinar el signo de la diferencia en una medida de evaluación. Otras tareas de evaluación podrían ser definidas; estimar la magnitud de la diferencia o los valores de las medidas mismas son ejemplos que implican diferentes nociones de confianza. Por lo tanto, consideramos la confianza como una estimación de probabilidad. Una de las preguntas que debemos hacer acerca de una estimación de probabilidad es qué significa. ¿Qué significa tener un 75% de confianza de que el sistema A es mejor que el sistema B? Como se describe arriba, queremos que signifique que si seguimos evaluando documentos, solo habrá un 25% de probabilidad de que nuestra evaluación cambie. Si esto es lo que significa, podemos confiar en las estimaciones de confianza. ¿Pero sabemos que tiene ese significado? Nuestro cálculo de confianza se basó en una suposición sobre la probabilidad de relevancia de los documentos no evaluados, específicamente que cada documento no evaluado tenía la misma probabilidad de ser relevante o no relevante. Esta suposición casi con seguridad no es realista en la mayoría de las aplicaciones de IR. Resulta que es esta suposición la que determina si se pueden confiar en las estimaciones de confianza. Antes de profundizar en esto, definimos formalmente la confianza. 2.1 Estimación de la confianza La precisión promedio (AP) es una métrica de evaluación estándar que captura tanto la capacidad de un sistema para clasificar documentos relevantes de manera alta (precisión) como su capacidad para recuperar documentos relevantes (recuperación). Normalmente se escribe como la precisión media en los rangos de documentos relevantes: AP = 1 |R| i∈R prec@r(i) donde R es el conjunto de documentos relevantes y r(i) es el rango del documento i. Sea Xi una variable aleatoria que indica la relevancia del documento i. Si los documentos están ordenados por rango, podemos expresar la precisión como prec@i = 1/i i j=1 Xj. La precisión promedio se convierte entonces en la ecuación cuadrática AP = 1 Xi n i=1 Xi/i i j=1 Xj = 1 Xi n i=1 j≥i aijXiXj donde aij = 1/ max{r(i), r(j)}. Usar aij en lugar de 1/i nos permite numerar los documentos de forma arbitraria. Para ver por qué esto es cierto, considera un ejemplo simple: una lista de 3 documentos con los documentos relevantes B, C en los rangos 1 y 3 y el documento no relevante A en el rango 2. La precisión promedio será 1 2 (1 1 x2 B+ 1 2 xBxA+ 1 3 xBxC + 1 2 x2 A+ 1 3 xAxC + 1 3 x2 C) = 1 2 1 + 2 3 porque xA = 0, xB = 1, xC = 1. Aunque el orden B, A, C es diferente de la etiqueta A, B, C, no afecta al cálculo. Ahora podemos ver que la precisión promedio en sí misma es una variable aleatoria con una distribución sobre todas las posibles asignaciones de relevancia a todos los documentos. Esta variable aleatoria tiene una esperanza, una varianza, intervalos de confianza y una cierta probabilidad de ser menor o igual a un valor dado. Todos estos dependen de la probabilidad de que el documento i sea relevante: pi = p(Xi = 1). Supongamos que en nuestro ejemplo anterior no conocemos las valoraciones de relevancia, pero creemos que pA = 0.4, pB = 0.8, pC = 0.7. Podemos entonces calcular, por ejemplo. P(AP = 0) = 0.2 · 0.6 · 0.3 = 0.036, o P(AP = 1 2 ) = 0.2 · 0.4 · 0.7 = 0.056. \n\nLa traducción al español sería: P(AP = 0) = 0.2 · 0.6 · 0.3 = 0.036, o P(AP = 1 2 ) = 0.2 · 0.4 · 0.7 = 0.056. Sumando sobre todas las posibilidades, podemos calcular la esperanza y la varianza: E[AP] ≈ 1 pi aiipi + j>i aij pipj V ar[AP] ≈ 1 ( pi)2 n i a2 iipiqi + j>i a2 ijpipj(1 − pipj) + i=j 2aiiaijpipj(1 − pi) + k>j=i 2aijaikpipjpk(1 − pi) AP converge asintóticamente a una distribución normal con la esperanza y la varianza definidas anteriormente. Para nuestra tarea de evaluación comparativa, estamos interesados en el signo de la diferencia en dos precisiones promedio: ΔAP = AP1 − AP2. Como mostramos en nuestro trabajo anterior, ΔAP tiene una forma cerrada cuando los documentos están ordenados arbitrariamente: ΔAP = 1 Xi n i=1 j≥i cij XiXj cij = aij − bij donde bij está definido de manera análoga a aij para la segunda clasificación. Dado que AP es normal, ΔAP también es normal, lo que significa que podemos usar la función de densidad acumulativa normal para determinar la confianza de que una diferencia en AP sea menor que cero. Dado que los temas son independientes, podemos extender fácilmente esto para referirnos a la precisión promedio (MAP). MAP también se distribuye normalmente; su esperanza y varianza son: EMAP = 1 T t∈T E[APt] (1) VMAP = 1 T2 t∈T V ar[APt] ΔMAP = MAP1 − MAP2 La confianza puede entonces estimarse calculando la esperanza y varianza y utilizando la función de densidad normal para encontrar P(ΔMAP < 0). 2.2 Confianza y Robustez Habiendo definido la confianza, volvemos al tema de la confianza en las estimaciones de confianza, y mostramos cómo se relaciona con la robustez de la colección ante juicios faltantes. 1 Estas son en realidad aproximaciones a la verdadera esperanza y varianza, pero el error es despreciable O(n2−n ). Sea Z el conjunto de todos los pares de resultados clasificados para un conjunto común de temas. Supongamos que tenemos un conjunto de m juicios de relevancia xm = {x1, x2, ..., xm} (usando x minúscula en lugar de X mayúscula para distinguir entre documentos juzgados y no juzgados); estos son los juicios contra los cuales calculamos la confianza. Sea Zα el subconjunto de pares en Z para los cuales predecimos que ΔMAP = −1 con confianza α dadas las evaluaciones xm. Para que las estimaciones de confianza sean precisas, necesitamos que al menos α · |Zα| de estos pares realmente tengan ΔMAP = −1 después de haber evaluado cada documento. Si lo hacen, podemos confiar en las estimaciones de confianza; nuestra evaluación será robusta ante juicios faltantes. Si nuestras estimaciones de confianza se basan en suposiciones poco realistas, no podemos esperar que sean precisas. Las suposiciones en las que se basan son las probabilidades de relevancia pi. Necesitamos que esto sea realista. Sostenemos que la mejor distribución posible de relevancia p(Xi) es aquella que explica todos los datos (todas las observaciones realizadas sobre los sistemas de recuperación) al mismo tiempo que no hace suposiciones infundadas. Esto se conoce como el principio de máxima entropía [13]. La entropía de una variable aleatoria X con distribución p(X) se define como H(p) = − i p(X = i) log p(X = i). Esto ha encontrado una amplia variedad de usos en la informática y la recuperación de información. La distribución de entropía máxima es aquella que maximiza H. Esta distribución es única y tiene una forma exponencial. El siguiente teorema muestra la utilidad de una distribución de entropía máxima para la relevancia al estimar la confianza. Teorema 1. Si p(Xn |I, xm ) = argmaxpH(p), las estimaciones de confianza serán precisas, donde xm es el conjunto de juicios de relevancia definidos anteriormente, Xn es el conjunto completo de documentos cuya relevancia deseamos estimar, e I es alguna información sobre los documentos (no especificada hasta ahora). Por el momento prescindimos de la prueba, pero es bastante simple. Esto indica que cuanto mejores sean las estimaciones de relevancia, más precisa será la evaluación. La tarea de crear una colección de pruebas reutilizable se convierte entonces en la tarea de estimar la relevancia de los documentos no evaluados. El teorema y su demostración no dicen absolutamente nada sobre la métrica de evaluación. Las estimaciones de probabilidad son completamente independientes de la medida en la que estamos interesados. Esto significa que los mismos cálculos de probabilidad pueden informarnos sobre la precisión promedio, así como sobre la precisión, la recuperación, el bpref, etc. Además, podríamos asumir que la relevancia de los documentos i y j es independiente y lograr el mismo resultado, lo cual afirmamos como un corolario: Corolario 1. Si p(Xi|I, xm) = argmaxpH(p), las estimaciones de confianza serán precisas. La tarea, por lo tanto, consiste en la imputación de los valores faltantes de relevancia. El teorema implica que cuanto más nos acerquemos a la distribución de entropía máxima relevante, más nos acercaremos a la robustez. 3. PREDICIENDO LA RELEVANCIA En nuestra declaración del Teorema 1, dejamos sin especificar la naturaleza de la información I. Una de las ventajas de nuestras estimaciones de confianza es que admiten información de una amplia variedad de fuentes; básicamente cualquier cosa que pueda ser modelada puede ser utilizada como información para predecir la relevancia. Una fuente natural de información son los propios sistemas de recuperación: cómo clasificaron los documentos evaluados, con qué frecuencia no lograron clasificar documentos relevantes, cómo se desempeñan en diferentes temas, y así sucesivamente. Si tratamos cada sistema como un experto en recuperación de información que proporciona una opinión sobre la relevancia de cada documento, el problema se convierte en uno de agregación de opiniones de expertos. Esto es similar al problema de metabúsqueda o fusión de datos en el que la tarea consiste en tomar k sistemas de entrada y fusionarlos en un solo ranking. Aslam et al. [3] identificaron previamente una conexión entre la evaluación y la metabúsqueda. Nuestro problema tiene dos diferencias clave: 1. Necesitamos explícitamente probabilidades de relevancia que podamos insertar en la Ecuación 1; los algoritmos de metabusqueda no tienen tal requisito. Estamos acumulando juicios de relevancia a medida que avanzamos con la evaluación y somos capaces de reestimar la relevancia dada cada nuevo juicio. A la luz de lo anterior, introducimos un modelo probabilístico para la combinación de expertos. 3.1 Un Modelo para la Agregación de Opiniones de Expertos Supongamos que cada experto j proporciona una probabilidad de relevancia qij = pj(Xi = 1). La información sobre la relevancia del documento i será entonces el conjunto de k opiniones de expertos I = qi = (qi1, qi2, · · · , qik). La distribución de probabilidad que deseamos encontrar es aquella que maximiza la entropía de pi = p(Xi = 1|qi). Resulta que encontrar el modelo de entropía máxima es equivalente a encontrar los parámetros que maximizan la verosimilitud [5]. Blower [6] muestra explícitamente que encontrar el modelo de entropía máxima para una variable binaria es equivalente a resolver una regresión logística. Entonces pi = p(Xi = 1|qi) = exp k j=1 λjqij 1 + exp k j=1 λj qij (2) donde λ1, · · · , λk son los parámetros de regresión. Incluimos una distribución beta a priori para p(λj) con parámetros α, β. Esto se puede ver como un tipo de suavizado para tener en cuenta el hecho de que los datos de entrenamiento están altamente sesgados. Este modelo tiene la ventaja de incluir la dependencia estadística entre los expertos. Un modelo de la misma forma fue demostrado por Clemen & Winkler como el mejor para agregar probabilidades de expertos [10]. Un enfoque similar motivado por la entropía máxima se ha utilizado para la agregación de expertos [15]. Aslam & Montague [1] utilizaron un modelo similar para la metabúsqueda, pero asumieron independencia entre los expertos. ¿De dónde vienen los qij? Utilizar puntuaciones sin procesar y no calibradas como predictores no funcionará porque las distribuciones de puntuaciones varían demasiado entre los temas. Un clasificador de modelado de lenguaje, por ejemplo, suele otorgar una puntuación mucho más alta al documento recuperado en la parte superior para una consulta corta que al documento recuperado en la parte superior para una consulta larga. Podríamos entrenar un modelo predictivo separado para cada tema, pero eso no aprovecha toda la información que tenemos: es posible que solo tengamos un puñado de juicios para un tema, no suficientes para entrenar un modelo con confianza. Además, parece razonable asumir que si un experto hace buenas predicciones para un tema, también hará buenas predicciones para otros temas. Podríamos usar un modelo jerárquico [12], pero eso no se generalizará a temas no vistos. En cambio, calibraremos las puntuaciones de cada experto de forma individual para que las puntuaciones puedan ser comparadas tanto dentro de un tema como entre temas. Por lo tanto, nuestro modelo tiene en cuenta no solo la dependencia entre expertos, sino también la dependencia entre el rendimiento de los expertos en diferentes tareas (temas). 3.2 Calibración de Expertos Cada experto nos proporciona una puntuación y un rango para cada documento. Necesitamos convertir estos en probabilidades. Un método como el utilizado por Manmatha et al. [14] podría ser utilizado para convertir puntuaciones en probabilidades de relevancia. El método de preferencia por pares de Carterette & Petkova [9] también podría ser utilizado, interpretando la clasificación de un documento sobre otro como una expresión de preferencia. Que q∗ ij sea la probabilidad autoinformada por el experto j de que el documento i sea relevante. De manera intuitiva, parece claro que q∗ ij debería disminuir con el rango, y debería ser cero si el documento i no está clasificado (el experto no consideró que fuera relevante). El modelo de preferencia por pares puede manejar fácilmente estos dos requisitos, por lo que lo utilizaremos. Que θrj (i) sea el coeficiente de relevancia del documento en la posición rj(i). Queremos encontrar los θs que maximizan la función de verosimilitud: Ljt(Θ) = rj (i)<rj (k) exp(θrj (i) − θrj (k)) 1 + exp(θrj (i) − θrj (k)) Incluimos nuevamente una distribución beta previa en p(θrj(i)) con parámetros |Rt| + 1 y |Nt| + 1, el tamaño de los conjuntos de documentos relevantes y no relevantes juzgados respectivamente. El uso de estos parámetros previos garantiza que las probabilidades resultantes se concentren alrededor de la proporción de documentos relevantes que se han descubierto para el tema t. Esto significa que las estimaciones de probabilidad disminuyen por rango y son más altas para los temas que tienen más documentos relevantes. Después de encontrar el Θ que maximiza la verosimilitud, tenemos q∗ ij = exp(θrj (i)) 1+exp(θrj (i)) . Definimos θ∞ = −∞, de modo que la probabilidad de que un documento no clasificado sea relevante es 0. Dado que q∗ ij se basa en la posición en la que se recupera un documento en lugar de en la identidad del documento en sí, las probabilidades son idénticas de un experto a otro, por ejemplo, si el experto E colocó el documento A en la posición 1, y el experto D colocó el documento B en la posición 1, tendremos que q∗ AE = q∗ BD. Por lo tanto, solo tenemos que resolver esto una vez para cada tema. El modelo anterior proporciona probabilidades independientes del tema para cada documento. Pero supongamos que un experto que informa una probabilidad del 90% solo tiene razón el 50% del tiempo. Su opinión debería ser descartada basándose en su desempeño observado. Específicamente, queremos aprender una función de calibración qij = Cj(q∗ ij) que garantice que las probabilidades predichas estén ajustadas a la capacidad de los expertos para recuperar documentos relevantes dados los juicios que se han realizado hasta este momento. El método de calibración SVM de Platts [16] ajusta una función sigmoide entre q∗ ij y las evaluaciones de relevancia para obtener qij = Cj (q∗ ij) = exp(Aj +Bjq∗ ij ) 1+exp(Aj +Bj q∗ ij ) . Dado que q∗ ij es independiente del tema, solo necesitamos aprender una función de calibración para cada experto. Una vez que tenemos la función de calibración, se aplica para ajustar las predicciones de los expertos a su rendimiento real. Las probabilidades calibradas se introducen en el modelo (2) para encontrar las probabilidades de los documentos. Figura 1: Diagrama conceptual de nuestro modelo de agregación. Los expertos E1 y E2 han clasificado los documentos A, B, C para el tema T1 y los documentos D, E, F para el tema T2. El primer paso es obtener q∗ ij. A continuación se realiza la calibración al rendimiento real para encontrar qij. Finalmente obtenemos pi = p(Xi = 1|qi1, qi2), · · · . 3.3 Resumen del modelo Nuestro modelo tiene tres componentes que difieren en los datos que toman como entrada y en lo que producen como salida. Se muestra un diagrama conceptual en la Figura 1. 1. rangos → probabilidades (por sistema por tema). Esto nos da q∗ ij, la probabilidad autoinformada por el experto j de la relevancia del documento i. Esto es no supervisado; no requiere datos etiquetados (aunque si los tenemos, los usamos para establecer parámetros previos). 2. probabilidades → probabilidades calibradas (por sistema). Esto nos da qij = Cj (q∗ ij), la probabilidad calibrada por el experto j de la relevancia del documento i. Esto es semisupervisado; tenemos juicios de relevancia en algunos rangos que usamos para imputar la probabilidad de relevancia en otros rangos. 3. probabilidades calibradas → probabilidades de documentos. Esto nos da pi = p(Xi = 1|qi), la probabilidad de relevancia del documento i dada las probabilidades de experto calibradas qij. Esto es supervisado; aprendemos coeficientes de un conjunto de documentos juzgados y los utilizamos para estimar la relevancia de los documentos no juzgados. Aunque el modelo parece bastante complejo, en realidad son solo tres aplicaciones sucesivas de regresión logística. Por lo tanto, se puede implementar en un lenguaje de programación estadística como R en unas pocas líneas de código. El uso de priors beta (conjugados) garantiza que no sean necesarios métodos computacionales costosos como MCMC [12], por lo que el modelo se entrena y aplica lo suficientemente rápido como para ser utilizado en línea. Nuestro código está disponible en http://ciir.cs.umass.edu/~carteret/. 4. EXPERIMENTOS Se están considerando tres hipótesis. El primero, y más importante, es que el uso de nuestro modelo de agregación de expertos para predecir la relevancia produce colecciones de pruebas lo suficientemente robustas como para ser reutilizables; es decir, podemos confiar en las estimaciones de confianza cuando evaluamos sistemas que no contribuyeron con ningún juicio al conjunto. Las otras dos hipótesis se relacionan con la mejora que observamos al utilizar estimaciones de relevancia mejores que las que utilizamos en nuestro trabajo anterior [8]. Estos son (a) que se necesitan menos temas de relevancia, número de ejecuciones, número de documentos evaluados y número de documentos relevantes para cada uno de nuestros conjuntos de datos y (b) la precisión de las predicciones es mayor que si simplemente asumiéramos que pi = 0.5 para todos los documentos no evaluados. 4.1 Datos Obtuvimos ejecuciones completas de búsqueda ad-hoc presentadas en las TRECs 3 a 8. Cada ejecución clasifica como máximo 1000 documentos para 50 temas (49 temas para TREC 4). Además, obtuvimos todas las ejecuciones de la pista Web de TREC 13, la pista Robust2 de TREC 14 y la pista Terabyte (ad-hoc) de TREC 14. Estas son las pistas que han reemplazado la pista ad-hoc desde su final en 1999. Las estadísticas se muestran en la Tabla 1. Reservamos el conjunto TREC 4 (ad-hoc 95) para entrenamiento, los TRECs 3 y 5-8 (ad-hoc 94 y 96-99) para pruebas principales, y los conjuntos restantes para pruebas adicionales. Utilizamos los archivos qrels recopilados por NIST como verdad. El número de juicios de relevancia realizados y documentos relevantes encontrados para cada pista se enumeran en la Tabla 1. Por razones computacionales, truncamos las listas clasificadas en 100 documentos. No hay razón por la que no pudiéramos profundizar más, pero el cálculo de la varianza es O(n3) y, por lo tanto, muy consumidor de tiempo. Debido a la naturaleza del rango recíproco de AP, no perdemos mucha información al truncar en el rango 100. 4.2 Algoritmos Compararemos tres algoritmos para adquirir juicios de relevancia. La línea base es una variación del agrupamiento de TREC que llamaremos agrupamiento incremental (IP). Este algoritmo toma un número k como entrada y presenta los primeros k documentos en orden de rango (sin tener en cuenta el tema) para ser evaluados. No estima la relevancia de los documentos no evaluados; simplemente asume que cualquier documento no evaluado no es relevante. El segundo algoritmo es el presentado en Carterette et al. [8] (Algoritmo 1). Los documentos se seleccionan en función de lo interesantes que sean para determinar si existe una diferencia en la precisión media promedio. Para este enfoque pi = 0.5 para todos los i; no hay estimación de probabilidades. Llamaremos a esto MTC por colección de pruebas mínimas. El tercer algoritmo aumenta MTC con estimaciones actualizadas de las probabilidades de relevancia. Llamaremos a esto RTC por colección de pruebas robusta. Es idéntico al Algoritmo 1, excepto que cada décima iteración estimamos pi para todos los documentos no evaluados i utilizando el modelo de agregación de expertos de la Sección 3. RTC tiene parámetros de suavizado (distribución previa) que deben ser configurados. Entrenamos utilizando el conjunto ad-hoc 95. Limitamos 2 robustos aquí significa recuperación robusta; esto es diferente de nuestro objetivo de evaluación robusta. Algoritmo 1 (MTC) Dadas dos listas clasificadas y un nivel de confianza α, predecir el signo de ΔMAP. 1: pi ← 0.5 para todos los documentos i 2: mientras P(ΔMAP < 0) < α hacer 3: calcular el peso wi para todos los documentos i no clasificados (ver Carterette et al. [8] para más detalles) 4: j ← argmaxiwi 5: xj ← 1 si el documento j es relevante, 0 en caso contrario 6: pj ← xj 7: fin mientras la búsqueda de priors uniformes con una varianza relativamente alta. Para la agregación de expertos, los parámetros previos son α = β = 1. 4.3 Diseño Experimental Primero, queremos saber si podemos aumentar un conjunto de juicios de relevancia con un conjunto de probabilidades de relevancia para reutilizar los juicios y evaluar un nuevo conjunto de sistemas. Para cada prueba experimental: 1. Selecciona un subconjunto aleatorio de k ejecuciones. De esos k, elige un c inicial < k para evaluar. 3. Ejecutar RTC al 95% de confianza en el c inicial de aproximadamente 4. Utilizando el modelo de la Sección 3, estime las probabilidades de relevancia para todos los documentos recuperados por todos los k experimentos. Calcular EMAP para todas las ejecuciones k, y P(ΔMAP < 0) para todos los pares de ejecuciones. Hacemos lo mismo para MTC, pero omitimos el paso 4. Ten en cuenta que después de evaluar los primeros c sistemas, no realizamos juicios de relevancia adicionales. Para poner a prueba nuestro método, seleccionamos c = 2: construiremos un conjunto de juicios a partir de evaluar solo dos sistemas iniciales. Luego generalizaremos a un conjunto de k = 10 (de los cuales esos dos son un subconjunto). A medida que realizamos más pruebas, obtenemos los datos que necesitamos para probar las tres hipótesis. 4.4 Evaluación Experimental Recuerde que un conjunto de juicios es robusto si la precisión de las predicciones que realiza es al menos su confianza estimada. Una forma de evaluar la robustez es agrupar pares por su confianza, luego calcular la precisión sobre todos los pares en cada grupo. Nos gustaría que la precisión no fuera menor que la puntuación de confianza más baja en el contenedor, pero preferiblemente más alta. Dado que las estadísticas resumen son útiles, ideamos la siguiente métrica. Supongamos que somos una casa de apuestas tomando apuestas sobre si ΔMAP < 0. Utilizamos RTC o MTC para establecer las probabilidades O = P (ΔMAP <0) 1−P (ΔMAP <0) . Supongamos que un apostador apuesta $1 a que ΔMAP ≥ 0. Si resulta que ΔMAP < 0, ganamos el dólar. De lo contrario, pagamos O. Si nuestras estimaciones de confianza son perfectamente precisas, empatamos. Si la confianza es mayor que la precisión, perdemos dinero; ganamos si la precisión es mayor que la confianza. Contra intuitivamente, el resultado más deseable es empatar: si perdemos dinero, no podemos confiar en las estimaciones de confianza, pero si ganamos dinero, hemos subestimado la confianza o evaluado más documentos de los necesarios. Sin embargo, el costo de no poder confiar en las estimaciones de confianza es mayor que el costo de realizar juicios de relevancia adicionales, por lo que consideraremos los resultados positivos como buenos. La cantidad que ganamos en cada comparación par a par i es: Wi = yi − (1 − yi) Pi 1 − Pi = yi − Pi 1 − Pi yi = 1 si ΔMAP < 0 y 0 en caso contrario, y Pi = P(ΔMAP < 0). La estadística resumen es W, la media de Wi. Ten en cuenta que a medida que Pi aumenta, perdemos más por estar equivocados. Esto es como debería ser: la penalización debería ser grande por fallar en las predicciones de alta probabilidad. Sin embargo, dado que nuestras pérdidas crecen sin límite a medida que las predicciones se acercan a la certeza, limitamos −Wi a 100. Para nuestra hipótesis de que RTC requiere menos juicios que MTC, estamos interesados en el número de juicios necesarios para alcanzar el 95% de confianza en el primer par de sistemas. La mediana es más interesante que la media: la mayoría de los pares requieren unos cientos de juicios, pero algunos pares requieren varios miles. La distribución es, por lo tanto, altamente sesgada y la media se ve fuertemente afectada por esos valores atípicos. Finalmente, para nuestra hipótesis de que RTC es más preciso que MTC, analizaremos la correlación de Kendalls τ entre una clasificación de k sistemas por un pequeño conjunto de juicios y la clasificación real utilizando el conjunto completo de juicios. El tau de Kendall, una estadística no paramétrica basada en intercambios de pares entre dos listas, es una evaluación estándar para este tipo de estudio. Va desde −1 (perfectamente anticorrelacionado) hasta 1 (clasificaciones idénticas), con 0 significando que la mitad de los pares están intercambiados. Como mencionamos en la introducción, sin embargo, una medida de precisión como la correlación de rangos no es una buena evaluación de la reutilización. Lo incluimos por completitud. 4.4.1 Prueba de hipótesis Realizar múltiples pruebas permite el uso de pruebas de hipótesis estadísticas para comparar algoritmos. El uso de los mismos conjuntos de sistemas permite el uso de pruebas emparejadas. Como mencionamos anteriormente, estamos más interesados en la mediana del número de juicios que en la media. Una prueba para la diferencia en la mediana es la prueba de rango con signo de Wilcoxon. También podemos usar una prueba t pareada para probar una diferencia en la media. Para la correlación de rangos, podemos usar una prueba t pareada para probar si hay una diferencia en τ. 5. RESULTADOS Y ANÁLISIS La comparación entre MTC y RTC se muestra en la Tabla 2. Con MTC y probabilidades uniformes de relevancia, los resultados están lejos de ser robustos. No podemos reutilizar las evaluaciones de relevancia con mucha confianza. Pero con RTC, los resultados son muy robustos. Existe una ligera disminución en la precisión cuando la confianza supera el 0.95; sin embargo, las predicciones de confianza son confiables. La media de Wi muestra que RTC está mucho más cerca de 0 que MTC. La distribución de las puntuaciones de confianza muestra que al menos el 80% de confianza se logra más del 35% del tiempo, lo que indica que ninguno de los algoritmos está siendo demasiado conservador en sus estimaciones de confianza. Las estimaciones de confianza son bastante bajas en general; esto se debe a que hemos construido una colección de pruebas a partir de solo dos sistemas iniciales. Recuerde de la Sección 1 que no podemos requerir (o incluso esperar) un nivel mínimo de confianza al generalizar a nuevos sistemas. Los resultados más detallados de ambos algoritmos se muestran en la Figura 2. La línea sólida es el resultado ideal que daría W = 0. RTC está en o por encima de esta línea en todos los puntos hasta que la confianza alcance aproximadamente 0.97. Después de eso, hay una ligera disminución en la precisión que discutimos a continuación. Ten en cuenta que tanto la confianza MTC RTC % en la precisión del intervalo % en la precisión del intervalo 0.5 − 0.6 33.7% 61.7% 28.6% 61.9% 0.6 − 0.7 18.1% 73.1% 20.1% 76.3% 0.7 − 0.8 10.4% 70.1% 15.5% 78.0% 0.8 − 0.9 9.4% 69.0% 12.1% 84.9% 0.9 − 0.95 7.3% 78.0% 6.6% 93.1% 0.95 − 0.99 17.9% 70.4% 12.4% 93.4% 1.0 3.3% 68.3% 4.7% 98.9% W −5.34 −0.39 mediana juzgada 251 235 media τ 0.393 0.555 Tabla 2: Confianza en que P(ΔMAP < 0) y precisión de la predicción al generalizar un conjunto de juicios de relevancia adquiridos utilizando MTC y RTC. Cada contenedor contiene más de 1,000 pruebas de los conjuntos ad hoc 3, 5-8. RTC es mucho más robusto que MTC. W está definido en la Sección 4.4; cuanto más cerca de 0, mejor. La mediana juzgada es el número de juicios necesarios para alcanzar un 95% de confianza en los dos primeros sistemas. La media τ es la correlación de rango promedio para los 10 sistemas. 0.5 0.6 0.7 0.8 0.9 1 0.5 0.6 0.7 0.8 0.9 1 precisión confianza punto de equilibrio RTC MTC Figura 2: Confianza vs. precisión de MTC y RTC. La línea sólida es el resultado perfecto que daría W = 0; el rendimiento debería estar en o por encima de esta línea. Cada punto representa al menos 500 comparaciones en pares. Los algoritmos están claramente por encima de la línea hasta aproximadamente una confianza de 0.7. Esto se debe a que el rendimiento base en estos conjuntos de datos es alto; es bastante fácil lograr un 75% de precisión haciendo muy poco trabajo [7]. Número de juicios: La mediana del número de juicios requeridos por MTC para alcanzar un 95% de confianza en los dos primeros sistemas es de 251, un promedio de 5 por tema. La mediana requerida por RTC es 235, aproximadamente 4.7 por tema. Aunque los números son cercanos, la mediana de RTCs es significativamente más baja según una prueba de Wilcoxon pareada (p < 0.0001). Para comparación, una piscina de profundidad 100 resultaría en un mínimo de 5,000 juicios para cada par. La diferencia en las medias es mucho mayor. MTC requirió una media de 823 juicios, 16 por tema, mientras que RTC requirió una media de 502, 10 por tema. (Recordemos que las medias están fuertemente sesgadas por algunas parejas que requieren miles de juicios). Esta diferencia es significativa mediante una prueba t de muestras pareadas (p < 0.0001). El diez por ciento de los conjuntos resultaron en 100 o menos juicios (menos de dos por tema). El rendimiento en estos es muy alto: W = 0.41 y un 99.7% de precisión cuando la confianza es de al menos 0.9. Esto demuestra que incluso las colecciones pequeñas pueden ser reutilizables. Para el 50% de los conjuntos con más de 235 juicios, la precisión es del 93% cuando la confianza es de al menos 0.9. Correlación de rangos: MTC y RTC clasifican los 10 sistemas por EMAP (Eq. (1)) calculado utilizando sus respectivas estimaciones de probabilidad. La correlación de rango τ promedio entre el MAP verdadero y el EMAP es de 0.393 para MTC y 0.555 para RTC. Esta diferencia es significativa mediante una prueba t de muestras pareadas (p < 0.0001). Ten en cuenta que no esperamos que las correlaciones τ sean altas, ya que estamos clasificando los sistemas con tan pocos juicios de relevancia. Es más importante que estimemos correctamente la confianza en cada comparación de pares. Ejecutamos IP para el mismo número de juicios que MTC tomó para cada par, luego clasificamos los sistemas por MAP utilizando solo esos juicios (todos los documentos no juzgados se consideraron no relevantes). Calculamos la correlación τ con la clasificación real. La correlación media τ es de 0.398, lo cual no es significativamente diferente de MTC, pero es significativamente menor que RTC. El uso de estimaciones uniformes de probabilidad es indistinguible de la línea base, mientras que la estimación de relevancia mediante la agregación de expertos mejora el rendimiento en gran medida: casi un 40% más que tanto MTC como IP. Sobreajuste: Es posible sobreajustar: si provienen demasiados juicios de los dos primeros sistemas, la varianza en ΔMAP se reduce y las estimaciones de confianza se vuelven poco fiables. Vimos esto en la Tabla 2 y la Figura 2, donde RTC muestra una disminución en la precisión cuando la confianza está alrededor del 97%. De hecho, el número de juicios realizados antes de una predicción incorrecta es más de un 50% mayor que el número realizado antes de una predicción correcta. El sobreajuste es difícil de cuantificar exactamente, porque hacer más juicios de relevancia no siempre lo causa: a niveles de confianza más altos, se hacen más juicios de relevancia, y como muestra la Tabla 2, la precisión es mayor en esas mayores confianzas. Obviamente, tener más juicios de relevancia debería aumentar tanto la confianza como la precisión; la diferencia parece estar cuando un sistema tiene muchos más juicios que el otro. Comparaciones por pares: Nuestras comparaciones por pares se dividen en uno de tres grupos: 1. las dos ejecuciones originales de las que se obtienen los juicios de relevancia; 2. una de las ejecuciones originales vs. una de las nuevas ejecuciones; 3. dos nuevas ejecuciones. La Tabla 3 muestra los resultados de confianza frente a precisión para cada uno de estos tres grupos. Curiosamente, el rendimiento es peor al comparar una de las carreras originales con una de las carreras adicionales. Esto se debe muy probablemente a una gran diferencia en el número de juicios que afectan la varianza de ΔMAP. Sin embargo, el rendimiento es bastante bueno en los tres subconjuntos. Peor caso: El caso intuitivamente más probable de producir un error es cuando los dos sistemas que se comparan han recuperado muy pocos documentos en común. Si queremos que los juicios sean reutilizables, deberíamos ser capaces de generalizar incluso a ejecuciones que son muy diferentes de las utilizadas para adquirir los juicios de relevancia. Una medida simple de similitud de dos ejecuciones es el porcentaje promedio de documentos que recuperaron en común para cada tema [2]. Calculamos esto para todos los pares, luego observamos el rendimiento en pares con baja similitud. Los resultados se muestran en la confianza de precisión de dos ejecuciones originales, una ejecución original, ninguna ejecución original. 0.5 − 0.6 - 48.1% 62.8% 0.6 − 0.7 - 57.1% 79.2% 0.7 − 0.8 - 67.9% 81.7% 0.8 − 0.9 - 82.2% 86.3% 0.9 − 0.95 95.9% 93.7% 92.6% 0.95 − 0.99 96.2% 92.5% 93.1% 1.0 100% 98.0% 99.1% W −1.11 −0.87 −0.27 Tabla 3: Confianza vs. precisión de RTC al comparar las dos ejecuciones originales, una ejecución original y una nueva ejecución, y dos nuevas ejecuciones. RTC es robusto en los tres casos. precisión cuando la confianza es similar 0 − 0.1 0.1 − 0.2 0.2 − 0.3 0.5 − 0.6 68.4% 63.1% 61.4% 0.6 − 0.7 84.2% 78.6% 76.6% 0.7 − 0.8 82.0% 79.8% 78.9% 0.8 − 0.9 93.6% 83.3% 82.1% 0.9 − 0.95 99.3% 92.7% 92.4% 0.95 − 0.99 98.7% 93.4% 93.3% 1.0 99.9% 97.9% 98.1% W 0.44 −0.45 −0.49 Tabla 4: Confianza vs. precisión de RTC cuando un par de sistemas recuperó el 0-30% de documentos en común (dividido en 0%-10%, 10%-20% y 20%-30%). RTC es robusto en los tres casos. Tabla 4. El rendimiento es de hecho muy robusto incluso cuando la similitud es baja. Cuando las dos corridas comparten muy pocos documentos en común, W es en realidad positivo. Tanto MTC como IP tuvieron un rendimiento bastante deficiente en estos casos. Cuando la similitud estaba entre 0 y 10%, tanto MTC como IP predijeron correctamente ΔMAP solo el 60% del tiempo, en comparación con una tasa de éxito del 87.6% para RTC. Por conjunto de datos: Todos los resultados anteriores han sido solo en las colecciones ad-hoc. Realizamos los mismos experimentos en nuestros conjuntos de datos adicionales y desglosamos los resultados por conjunto de datos para ver cómo varía el rendimiento. Los resultados en la Tabla 5 muestran todo acerca de cada conjunto, incluyendo la precisión agrupada, W, la media de τ, y el número medio de juicios para alcanzar el 95% de confianza en los dos primeros sistemas. Los resultados son altamente consistentes de una colección a otra, lo que sugiere que nuestro método no está sobreajustado a ningún conjunto de datos en particular. CONCLUSIONES Y TRABAJO FUTURO En este trabajo hemos ofrecido la primera definición formal de la idea común de reutilización de una colección de pruebas y presentado un modelo que es capaz de lograr la reutilización con conjuntos muy pequeños de juicios de relevancia. La Tabla 2 y la Figura 2 juntas muestran cuán sesgado puede ser un pequeño conjunto de juicios: MTC está sobreestimando drásticamente la confianza y es mucho menos preciso que RTC, que puede eliminar el sesgo para proporcionar una evaluación sólida. Las estimaciones de confianza de RTC, además de ser precisas, proporcionan una guía para obtener juicios adicionales: enfóquese en juzgar documentos de las comparaciones de menor confianza. A largo plazo, observamos pequeños conjuntos de juicios de relevancia, precisión, confianza ad-hoc 94, ad-hoc 96, ad-hoc 97, ad-hoc 98, ad-hoc 99, web 04, robust 05, terabyte 05, 0.5 − 0.6 64.1%, 61.8%, 62.2%, 62.0%, 59.4%, 64.3%, 61.5%, 61.6%, 0.6 − 0.7 76.1%, 77.8%, 74.5%, 78.2%, 74.3%, 78.1%, 75.9%, 75.9%, 0.7 − 0.8 75.2%, 78.9%, 77.6%, 80.0%, 78.6%, 82.6%, 77.5%, 80.4%, 0.8 − 0.9 83.2%, 85.5%, 84.6%, 84.9%, 86.8%, 84.5%, 86.7%, 87.7%, 0.9 − 0.95 93.0%, 93.6%, 92.8%, 93.7%, 92.6%, 94.2%, 93.9%, 94.2%, 0.95 − 0.99 93.1%, 94.3%, 93.1%, 93.7%, 92.8%, 95.0%, 93.9%, 91.6%, 1.0 99.2%, 96.8%, 98.7%, 99.5%, 99.6%, 100%, 99.2%, 98.3%, W -0.34, -0.34, -0.48, -0.35, -0.44, -0.07, -0.41, -0.67, mediana juzgada 235, 276, 243, 213, 179, 448, 310, 320, media τ 0.538, 0.573, 0.556, 0.579, 0.532, 0.596, 0.565, 0.574. Tabla 5: Precisión, W, media τ y mediana del número de juicios para los 8 conjuntos de pruebas. Los resultados son altamente consistentes en todos los conjuntos de datos. A medida que pasa el tiempo, el número de juicios crece hasta que hay un 100% de confianza en cada evaluación, y hay una colección completa de pruebas para la tarea. Vemos un uso adicional para este método en escenarios como la recuperación web en los que el corpus cambia con frecuencia. Podría aplicarse a la evaluación en una colección de pruebas dinámica según lo definido por Soboroff [18]. El modelo que presentamos en la Sección 3 no es en absoluto la única posibilidad para crear una colección de pruebas robusta. Un modelo de agregación de expertos más simple podría funcionar igual de bien o mejor (aunque todos nuestros esfuerzos por simplificar fallaron). Además de la agregación de expertos, podríamos estimar probabilidades al observar similitudes entre documentos. Esta es un área obvia para futuras exploraciones. Además, será valioso investigar el problema del sobreajuste: las circunstancias en las que ocurre y qué se puede hacer para prevenirlo. Mientras tanto, limitar las estimaciones de confianza al 95% es un truco que resuelve el problema. Tenemos muchos más resultados experimentales que desafortunadamente no tuvimos espacio para incluir, pero que refuerzan la idea de que RTC es altamente robusto: con solo unos pocos juicios por tema, podemos evaluar con precisión la confianza en cualquier comparación de sistemas de forma pareja. Agradecimientos Este trabajo fue apoyado en parte por el Centro de Recuperación de Información Inteligente y en parte por la Agencia de Proyectos de Investigación Avanzada de Defensa (DARPA) bajo el número de contrato HR0011-06-C-0023. Cualquier opinión, hallazgo, conclusión o recomendación expresada en este material es responsabilidad del autor y no necesariamente refleja la del patrocinador. REFERENCIAS [1] J. Aslam y M. Montague. Modelos para Metabúsqueda. En Actas de SIGIR, páginas 275-285, 2001. [2] J. Aslam y R. Savell. Sobre la efectividad de evaluar sistemas de recuperación en ausencia de juicios de relevancia. En Actas de SIGIR, páginas 361-362, 2003. [3] J. A. Aslam, V. Pavlu y R. Savell. Un modelo unificado para metabúsqueda, agrupación y evaluación de sistemas. En Actas de CIKM, páginas 484-491, 2003. [4] J. A. Aslam, V. Pavlu y E. Yilmaz. Un método estadístico para la evaluación de sistemas utilizando juicios incompletos. En Actas de SIGIR, páginas 541-548, 2006. [5] A. L. Berger, S. D. Pietra y V. J. D. Pietra. Un enfoque de entropía máxima para el procesamiento del lenguaje natural. Lingüística Computacional, 22(1):39-71, 1996. [6] D. J. Blower. Una derivación sencilla de la regresión logística desde la perspectiva bayesiana y de máxima entropía. En Actas del 23º Taller Internacional sobre Inferencia Bayesiana y Métodos de Entropía Máxima en Ciencia e Ingeniería, páginas 30-43, 2004. [7] B. Carterette y J. Allan. Metodología de investigación en estudios del esfuerzo del evaluador para la evaluación de recuperación. En Actas de RIAO, 2007. [8] B. Carterette, J. Allan y R. K. Sitaraman. Colecciones de prueba mínimas para evaluación de recuperación. En Actas de SIGIR, páginas 268-275, 2006. [9] B. Carterette y D. I. Petkova. Aprendiendo un ranking a partir de preferencias por pares. En Actas de SIGIR, 2006. [10] R. T. Clemen y R. L. Winkler. Unanimidad y compromiso entre pronosticadores de probabilidad. Ciencia de la Gestión, 36(7):767-779, julio de 1990. [11] G. V. Cormack, C. R. Palmer y C. L. Clarke. Construcción eficiente de grandes colecciones de pruebas. En Actas de SIGIR, páginas 282-289, 1998. [12] A. Gelman, J. B. Carlin, H. S. Stern y D. B. Rubin. Análisis de datos bayesianos. Chapman & Hall/CRC, 2004. [13] E. T. Jaynes.\nChapman & Hall/CRC, 2004. [13] E. T. Jaynes. Teoría de la probabilidad: La lógica de la ciencia. Cambridge University Press, 2003. [14] R. Manmatha y H. Sever. Un enfoque formal para la normalización de puntuaciones en metabúsqueda. En Actas de HLT, páginas 88-93, 2002. [15] I. J. Myung, S. Ramamoorti y J. Andrew D. Baily. Agregación de predicciones de expertos mediante entropía máxima. Ciencia de la Gestión, 42(10):1420-1436, octubre de 1996. [16] J. Platt. Salidas probabilísticas para máquinas de vectores de soporte y comparación con métodos de verosimilitud regularizados. páginas 61-74, 2000. [17] M. Sanderson y H. Joho. Formando colecciones de pruebas sin agrupamiento de sistemas. En Actas de la 27ª Conferencia Internacional Anual de ACM SIGIR sobre Investigación y Desarrollo en Recuperación de Información, páginas 33-40, 2004. [18] I. Soboroff. Colecciones de pruebas dinámicas: midiendo la efectividad de la búsqueda en la web en vivo. En Actas de SIGIR, páginas 276-283, 2006. [19] K. Sparck Jones y C. J. van Rijsbergen. Colecciones de pruebas de recuperación de información. Revista de Documentación, 32(1):59-75, 1976. [20] E. M. Voorhees y D. K. Harman, editores. TREC: Experimento y Evaluación en Recuperación de Información. MIT Press, 2005. [21] J. Zobel. \n\nMIT Press, 2005. [21] J. Zobel. ¿Qué tan confiables son los resultados de los experimentos de recuperación de información a gran escala? En Actas de SIGIR, páginas 307-314, 1998.",
    "original_sentences": [
        "Robust Test Collections for Retrieval Evaluation Ben Carterette Center for Intelligent Information Retrieval Computer Science Department University of Massachusetts Amherst Amherst, MA 01003 carteret@cs.umass.edu ABSTRACT Low-cost methods for acquiring relevance judgments can be a boon to researchers who need to evaluate new retrieval tasks or topics but do not have the resources to make thousands of judgments.",
        "While these judgments are very useful for a one-time evaluation, it is not clear that they can be trusted when re-used to evaluate new systems.",
        "In this work, we formally define what it means for judgments to be reusable: the confidence in an evaluation of new systems can be accurately assessed from an existing set of relevance judgments.",
        "We then present a method for augmenting a set of relevance judgments with relevance estimates that require no additional assessor effort.",
        "Using this method practically guarantees reusability: with as few as five judgments per topic taken from only two systems, we can reliably evaluate a larger set of ten systems.",
        "Even the smallest sets of judgments can be useful for evaluation of new systems.",
        "Categories and Subject Descriptors: H.3 Information Storage and Retrieval; H.3.4 Systems and Software: Performance Evaluation General Terms: Experimentation, Measurement, Reliability 1.",
        "INTRODUCTION Consider an information retrieval researcher who has invented a new retrieval task.",
        "She has built a system to perform the task and wants to evaluate it.",
        "Since the task is new, it is unlikely that there are any extant relevance judgments.",
        "She does not have the time or resources to judge every document, or even every retrieved document.",
        "She can only judge the documents that seem to be the most informative and stop when she has a reasonable degree of confidence in her conclusions.",
        "But what happens when she develops a new system and needs to evaluate it?",
        "Or another research group decides to implement a system to perform the task?",
        "Can they reliably reuse the original judgments?",
        "Can they evaluate without more relevance judgments?",
        "Evaluation is an important aspect of information retrieval research, but it is only a semi-solved problem: for most retrieval tasks, it is impossible to judge the relevance of every document; there are simply too many of them.",
        "The solution used by NIST at TREC (Text REtrieval Conference) is the pooling method [19, 20]: all competing systems contribute N documents to a pool, and every document in that pool is judged.",
        "This method creates large sets of judgments that are reusable for training or evaluating new systems that did not contribute to the pool [21].",
        "This solution is not adequate for our hypothetical researcher.",
        "The pooling method gives thousands of relevance judgments, but it requires many hours of (paid) annotator time.",
        "As a result, there have been a slew of recent papers on reducing annotator effort in producing test collections: Cormack et al. [11], Zobel [21], Sanderson and Joho [17], Carterette et al. [8], and Aslam et al. [4], among others.",
        "As we will see, the judgments these methods produce can significantly bias the evaluation of a new set of systems.",
        "Returning to our hypothetical resesarcher, can she reuse her relevance judgments?",
        "First we must formally define what it means to be reusable.",
        "In previous work, reusability has been tested by simply assessing the accuracy of a set of relevance judgments at evaluating unseen systems.",
        "While we can say that it was right 75% of the time, or that it had a rank correlation of 0.8, these numbers do not have any predictive power: they do not tell us which systems are likely to be wrong or how confident we should be in any one.",
        "We need a more careful definition of reusability.",
        "Specifically, the question of reusability is not how accurately we can evaluate new systems.",
        "A malicious adversary can always produce a new ranked list that has not retrieved any of the judged documents.",
        "The real question is how much confidence we have in our evaluations, and, more importantly, whether we can trust our estimates of confidence.",
        "Even if confidence is not high, as long as we can trust it, we can identify which systems need more judgments in order to increase confidence.",
        "Any set of judgments, no matter how small, becomes reusable to some degree.",
        "Small, reusable test collections could have a huge impact on information retrieval research.",
        "Research groups would be able to share the relevance judgments they have done in-house for pilot studies, new tasks, or new topics.",
        "The amount of data available to researchers would grow exponentially over time. 2.",
        "ROBUST EVALUATION Above we gave an intuitive definition of reusability: a collection is reusable if we can trust our estimates of confidence in an evaluation.",
        "By that we mean that if we have made some relevance judgments and have, for example, 75% confidence that system A is better than system B, we would like there to be no more than 25% chance that our assessment of the relative quality of the systems will change as we continue to judge documents.",
        "Our evaluation should be robust to missing judgments.",
        "In our previous work, we defined confidence as the probability that the difference in an evaluation measure calculated for two systems is less than zero [8].",
        "This notion of confidence is defined in the context of a particular evaluation task that we call comparative evaluation: determining the sign of the difference in an evaluation measure.",
        "Other evaluation tasks could be defined; estimating the magnitude of the difference or the values of the measures themselves are examples that entail different notions of confidence.",
        "We therefore see confidence as a probability estimate.",
        "One of the questions we must ask about a probability estimate is what it means.",
        "What does it mean to have 75% confidence that system A is better than system B?",
        "As described above, we want it to mean that if we continue to judge documents, there will only be a 25% chance that our assessment will change.",
        "If this is what it means, we can trust the confidence estimates.",
        "But do we know it has that meaning?",
        "Our calculation of confidence rested on an assumption about the probability of relevance of unjudged documents, specifically that each unjudged document was equally likely to be relevant or nonrelevant.",
        "This assumption is almost certainly not realistic in most IR applications.",
        "As it turns out, it is this assumption that determines whether the confidence estimates can eb trusted.",
        "Before elaborating on this, we formally define confidence. 2.1 Estimating Confidence Average precision (AP) is a standard evaluation metric that captures both the ability of a system to rank relevant documents highly (precision) as well as its ability to retrieve relevant documents (recall).",
        "It is typically written as the mean precision at the ranks of relevant documents: AP = 1 |R| i∈R prec@r(i) where R is the set of relevant documents and r(i) is the rank of document i.",
        "Let Xi be a random variable indicating the relevance of document i.",
        "If documents are ordered by rank, we can express precision as prec@i = 1/i i j=1 Xj .",
        "Average precision then becomes the quadratic equation AP = 1 Xi n i=1 Xi/i i j=1 Xj = 1 Xi n i=1 j≥i aijXiXj where aij = 1/ max{r(i), r(j)}.",
        "Using aij instead of 1/i allows us to number the documents arbitrarily.",
        "To see why this is true, consider a toy example: a list of 3 documents with relevant documents B, C at ranks 1 and 3 and nonrelevant document A at rank 2.",
        "Average precision will be 1 2 (1 1 x2 B+ 1 2 xBxA+ 1 3 xBxC + 1 2 x2 A+ 1 3 xAxC + 1 3 x2 C) = 1 2 1 + 2 3 because xA = 0, xB = 1, xC = 1.",
        "Though the ordering B, A, C is different from the labeling A, B, C, it does not affect the computation.",
        "We can now see average precision itself is a random variable with a distribution over all possible assignments of relevance to all documents.",
        "This random variable has an expectation, a variance, confidence intervals, and a certain probability of being less than or equal to a given value.",
        "All of these are dependent on the probability that document i is relevant: pi = p(Xi = 1).",
        "Suppose in our previous example we do not know the relevance judgments, but we believe pA = 0.4, pB = 0.8, pC = 0.7.",
        "We can then compute e.g.",
        "P(AP = 0) = 0.2 · 0.6 · 0.3 = 0.036, or P(AP = 1 2 ) = 0.2 · 0.4 · 0.7 = 0.056.",
        "Summing over all possibilities, we can compute expectation and variance: E[AP] ≈ 1 pi aiipi + j>i aij pipj V ar[AP] ≈ 1 ( pi)2 n i a2 iipiqi + j>i a2 ijpipj(1 − pipj) + i=j 2aiiaijpipj(1 − pi) + k>j=i 2aijaikpipjpk(1 − pi) AP asymptotically converges to a normal distribution with expectation and variance as defined above.1 For our comparative evaluation task we are interested in the sign of the difference in two average precisions: ΔAP = AP1 − AP2.",
        "As we showed in our previous work, ΔAP has a closed form when documents are ordered arbitrarily: ΔAP = 1 Xi n i=1 j≥i cij XiXj cij = aij − bij where bij is defined analogously to aij for the second ranking.",
        "Since AP is normal, ΔAP is normal as well, meaning we can use the normal cumulative density function to determine the confidence that a difference in AP is less than zero.",
        "Since topics are independent, we can easily extend this to mean average precision (MAP).",
        "MAP is also normally distributed; its expectation and variance are: EMAP = 1 T t∈T E[APt] (1) VMAP = 1 T2 t∈T V ar[APt] ΔMAP = MAP1 − MAP2 Confidence can then be estimated by calculating the expectation and variance and using the normal density function to find P(ΔMAP < 0). 2.2 Confidence and Robustness Having defined confidence, we turn back to the issue of trust in confidence estimates, and show how it ties into the robustness of the collection to missing judgments. 1 These are actually approximations to the true expectation and variance, but the error is a negligible O(n2−n ).",
        "Let Z be the set of all pairs of ranked results for a common set of topics.",
        "Suppose we have a set of m relevance judgments xm = {x1, x2, ..., xm} (using small x rather than capital X to distinguish between judged and unjudged documents); these are the judgments against which we compute confidence.",
        "Let Zα be the subset of pairs in Z for which we predict that ΔMAP = −1 with confidence α given the judgments xm .",
        "For the confidence estimates to be accurate, we need at least α · |Zα| of these pairs to actually have ΔMAP = −1 after we have judged every document.",
        "If they do, we can trust the confidence estimates; our evaluation will be robust to missing judgments.",
        "If our confidence estimates are based on unrealistic assumptions, we cannot expect them to be accurate.",
        "The assumptions they are based on are the probabilities of relevance pi.",
        "We need these to be realistic.",
        "We argue that the best possible distribution of relevance p(Xi) is the one that explains all of the data (all of the observations made about the retrieval systems) while at the same time making no unwarranted assumptions.",
        "This is known as the principle of maximum entropy [13].",
        "The entropy of a random variable X with distribution p(X) is defined as H(p) = − i p(X = i) log p(X = i).",
        "This has found a wide array of uses in computer science and information retrieval.",
        "The maximum entropy distribution is the one that maximizes H. This distribution is unique and has an exponential form.",
        "The following theorem shows the utility of a maximum entropy distribution for relevance when estimating confidence.",
        "Theorem 1.",
        "If p(Xn |I, xm ) = argmaxpH(p), confidence estimates will be accurate. where xm is the set of relevance judgments defined above, Xn is the full set of documents that we wish to estimate the relevance of, and I is some information about the documents (unspecified as of now).",
        "We forgo the proof for the time being, but it is quite simple.",
        "This says that the better the estimates of relevance, the more accurate the evaluation.",
        "The task of creating a reusable test collection thus becomes the task of estimating the relevance of unjudged documents.",
        "The theorem and its proof say nothing whatsoever about the evaluation metric.",
        "The probability estimates are entirely indepedent of the measure we are interested in.",
        "This means the same probability estimates can tell us about average precision as well as precision, recall, bpref, etc.",
        "Furthermore, we could assume that the relevance of documents i and j is independent and achieve the same result, which we state as a corollary: Corollary 1.",
        "If p(Xi|I, xm ) = argmaxpH(p), confidence estimates will be accurate.",
        "The task therefore becomes the imputation of the missing values of relevance.",
        "The theorem implies that the closer we get to the maximum entropy distribution of relevance, the closer we get to robustness. 3.",
        "PREDICTING RELEVANCE In our statement of Theorem 1, we left the nature of the information I unspecified.",
        "One of the advantages of our confidence estimates is that they admit information from a wide variety of sources; essentially anything that can be modeled can be used as information for predicting relevance.",
        "A natural source of information is the retrieval systems themselves: how they ranked the judged documents, how often they failed to rank relevant documents, how they perform across topics, and so on.",
        "If we treat each system as an information retrieval expert providing an opinion about the relevance of each document, the problem becomes one of expert opinion aggregation.",
        "This is similar to the metasearch or data fusion problem in which the task is to take k input systems and merge them into a single ranking.",
        "Aslam et al. [3] previously identified a connection between evaluation and metasearch.",
        "Our problem has two key differences: 1.",
        "We explicitly need probabilities of relevance that we can plug into Eq. 1; metasearch algorithms have no such requirement. 2.",
        "We are accumulating relevance judgments as we proceed with the evaluation and are able to re-estimate relevance given each new judgment.",
        "In light of (1) above, we introduce a probabilistic model for expert combination. 3.1 A Model for Expert Opinion Aggregation Suppose that each expert j provides a probability of relevance qij = pj(Xi = 1).",
        "The information about the relevance of document i will then be the set of k expert opinions I = qi = (qi1, qi2, · · · , qik).",
        "The probability distribution we wish to find is the one that maximizes the entropy of pi = p(Xi = 1|qi).",
        "As it turns out, finding the maximum entropy model is equivalent to finding the parameters that maximize the likelihood [5].",
        "Blower [6] explicitly shows that finding the maximum entropy model for a binary variable is equivalent to solving a logistic regression.",
        "Then pi = p(Xi = 1|qi) = exp k j=1 λjqij 1 + exp k j=1 λj qij (2) where λ1, · · · , λk are the regression parameters.",
        "We include a beta prior for p(λj) with parameters α, β.",
        "This can be seen as a type of smoothing to account for the fact that the training data is highly biased.",
        "This model has the advantage of including the statistical dependence between the experts.",
        "A model of the same form was shown by Clemen & Winkler to be the best for aggregating expert probabilities [10].",
        "A similar maximumentropy-motivated approach has been used for expert aggregation [15].",
        "Aslam & Montague [1] used a similar model for metasearch, but assumed independence among experts.",
        "Where do the qij s come from?",
        "Using raw, uncalibrated scores as predictors will not work because score distributions vary too much between topics.",
        "A language modeling ranker, for instance, will typically give a much higher score to the top retrieved document for a short query than to the top retrieved document for a long query.",
        "We could train a separate predicting model for each topic, but that does not take advantage of all of the information we have: we may only have a handful of judgments for a topic, not enough to train a model to any confidence.",
        "Furthermore, it seems reasonable to assume that if an expert makes good predictions for one topic, it will make good predictions for other topics as well.",
        "We could use a hierarchical model [12], but that will not generalize to unseen topics.",
        "Instead, we will calibrate the scores of each expert individually so that scores can be compared both within topic and between topic.",
        "Thus our model takes into account not only the dependence between experts, but also the dependence between experts performances on different tasks (topics). 3.2 Calibrating Experts Each expert gives us a score and a rank for each document.",
        "We need to convert these to probabilities.",
        "A method such as the one used by Manmatha et al. [14] could be used to convert scores into probabilities of relevance.",
        "The pairwise preference method of Carterette & Petkova [9] could also be used, interpeting the ranking of one document over another as an expression of preference.",
        "Let q∗ ij be expert js self-reported probability that document i is relevant.",
        "Intuitively it seems clear that q∗ ij should decrease with rank, and it should be zero if document i is unranked (the expert did not believe it to be relevant).",
        "The pairwise preference model can handle these two requirements easily, so we will use it.",
        "Let θrj (i) be the relevance coefficient of the document at rank rj(i).",
        "We want to find the θs that maximize the likelihood function: Ljt(Θ) = rj (i)<rj (k) exp(θrj (i) − θrj (k)) 1 + exp(θrj (i) − θrj (k)) We again include a beta prior on p(θrj(i)) with parameters |Rt| + 1 and |Nt| + 1, the size of the sets of judged relevant and nonrelevant documents respectively.",
        "Using these as prior parameters ensures that the resulting probabilities will be concentrated around the ratio of relevant documents that have been discovered for topic t. This means that the probability estimates decrease by rank and are higher for topics that have more relevant documents.",
        "After finding the Θ that maximizes the likelihood, we have q∗ ij = exp(θrj (i)) 1+exp(θrj (i)) .",
        "We define θ∞ = −∞, so that the probability that an unranked document is relevant is 0.",
        "Since q∗ ij is based on the rank at which a document is retrieved rather than the identity of the document itself, the probabilities are identical from expert to expert, e.g. if expert E put document A at rank 1, and expert D put document B at rank 1, we will have q∗ AE = q∗ BD.",
        "Therefore we only have to solve this once for each topic.",
        "The above model gives topic-independent probabilities for each document.",
        "But suppose an expert who reports 90% probability is only right 50% of the time.",
        "Its opinion should be discounted based on its observed performance.",
        "Specifically, we want to learn a calibration function qij = Cj(q∗ ij) that will ensure that the predicted probabilities are tuned to the experts ability to retrieve relevant documents given the judgments that have been made to this point.",
        "Platts SVM calibration method [16] fits a sigmoid function between q∗ ij and the relevance judgments to obtain qij = Cj (q∗ ij) = exp(Aj +Bjq∗ ij ) 1+exp(Aj +Bj q∗ ij ) .",
        "Since q∗ ij is topic-independent, we only need to learn one calibration function for each expert.",
        "Once we have the calibration function, it is applied to adjust the experts predictions to their actual performance.",
        "The calibrated probabilities are plugged into model (2) to find the document probabilities.",
        "Figure 1: Conceptual diagram of our aggregation model.",
        "Experts E1, E2 have ranked documents A, B, C for topic T1 and documents D, E, F for topic T2.",
        "The first step is to obtain q∗ ij.",
        "Next is calibration to true performance to find qij .",
        "Finally we obtain pi = p(Xi = 1|qi1, qi2), · · · . 3.3 Model Summary Our model has three components that differ by the data they take as input and what they produce as output.",
        "A conceptual diagram is shown in Figure 1. 1. ranks → probabilities (per system per topic).",
        "This gives us q∗ ij, expert js self-reported probability of the relevance of document i.",
        "This is unsupervised; it requires no labeled data (though if we have some, we use it to set prior parameters). 2. probabilities → calibrated probabilities (per system).",
        "This gives us qij = Cj (q∗ ij), expert js calibrated probability of the relevance of document i.",
        "This is semisupervised; we have relevance judgments at some ranks which we use to impute the probability of relevance at other ranks. 3. calibrated probabilities → document probabilities.",
        "This gives us pi = p(Xi = 1|qi), the probability of relevance of document i given calibrated expert probabilities qij .",
        "This is supervised; we learn coefficients from a set of judged documents and use those to estimate the relevance of the unjudged documents.",
        "Although the model appears rather complex, it is really just three successive applications of logistic regression.",
        "As such, it can be implemented in a statistical programming language such as R in a few lines of code.",
        "The use of beta (conjugate) priors ensures that no expensive computational methods such as MCMC are necessary [12], so the model is trained and applied fast enough to be used on-line.",
        "Our code is available at http://ciir.cs.umass.edu/~carteret/. 4.",
        "EXPERIMENTS Three hypotheses are under consideration.",
        "The first, and most important, is that using our expert aggregation model to predict relevance produces test collections that are robust enough to be reusable; that is, we can trust the estimates of confidence when we evaluate systems that did not contribute any judgments to the pool.",
        "The other two hypotheses relate to the improvement we see by using better estimates of relevance than we did in our previous work [8].",
        "These are that (a) it takes fewer relevance track no. topics no. runs no. judged no. rel ad-hoc 94 50 40 97,319 9,805 ad-hoc 95 49 33 87,069 6,503 ad-hoc 96 50 61 133,681 5,524 ad-hoc 97 50 74 72,270 4,611 ad-hoc 98 50 103 80,345 4,674 ad-hoc 99 50 129 86,830 4,728 web 04 225 74 88,566 1,763 robust 05 50 74 37,798 6,561 terabyte 05 50 58 45,291 10,407 Table 1: Number of topics, number of runs, number of documents judged, and number found relevant for each of our data sets. judgments to reach 95% confidence and (b) the accuracy of the predictions is higher than if we were to simply assume pi = 0.5 for all unjudged documents. 4.1 Data We obtained full ad-hoc runs submitted to TRECs 3 through 8.",
        "Each run ranks at most 1000 documents for 50 topics (49 topics for TREC 4).",
        "Additionally, we obtained all runs from the Web track of TREC 13, the Robust2 track of TREC 14, and the Terabyte (ad-hoc) track of TREC 14.",
        "These are the tracks that have replaced the ad-hoc track since its end in 1999.",
        "Statistics are shown in Table 1.",
        "We set aside the TREC 4 (ad-hoc 95) set for training, TRECs 3 and 5-8 (ad-hoc 94 and 96-99) for primary testing, and the remaining sets for additional testing.",
        "We use the qrels files assembled by NIST as truth.",
        "The number of relevance judgments made and relevant documents found for each track are listed in Table 1.",
        "For computational reasons, we truncate ranked lists at 100 documents.",
        "There is no reason that we could not go deeper, but calculating variance is O(n3 ) and thus very timeconsuming.",
        "Because of the reciprocal rank nature of AP, we do not lose much information by truncating at rank 100. 4.2 Algorithms We will compare three algorithms for acquiring relevance judgments.",
        "The baseline is a variation of TREC pooling that we will call incremental pooling (IP).",
        "This algorithm takes a number k as input and presents the first k documents in rank order (without regard to topic) to be judged.",
        "It does not estimate the relevance of unjudged documents; it simply assumes any unjudged document is nonrelevant.",
        "The second algorithm is that presented in Carterette et al. [8] (Algorithm 1).",
        "Documents are selected based on how interesting they are in determining whether a difference in mean average precision exists.",
        "For this approach pi = 0.5 for all i; there is no estimation of probabilities.",
        "We will call this MTC for minimal test collection.",
        "The third algorithm augments MTC with updated estimates of probabilities of relevance.",
        "We will call this RTC for robust test collection.",
        "It is identical to Algorithm 1, except that every 10th iteration we estimate pi for all unjudged documents i using the expert aggregation model of Section 3.",
        "RTC has smoothing (prior distribution) parameters that must be set.",
        "We trained using the ad-hoc 95 set.",
        "We limited 2 Robust here means robust retrieval; this is different from our goal of robust evaluation.",
        "Algorithm 1 (MTC) Given two ranked lists and confidence level α, predict the sign of ΔMAP. 1: pi ← 0.5 for all documents i 2: while P(ΔMAP < 0) < α do 3: calculate weight wi for all unjudged documents i (see Carterette et al. [8] for details) 4: j ← argmaxiwi 5: xj ← 1 if document j is relevant, 0 otherwise 6: pj ← xj 7: end while the search to uniform priors with relatively high variance.",
        "For expert aggregation, the prior parameters are α = β = 1. 4.3 Experimental Design First, we want to know whether we can augment a set of relevance judgments with a set of relevance probabilities in order to reuse the judgments to evaluate a new set of systems.",
        "For each experimental trial: 1.",
        "Pick a random subset of k runs. 2.",
        "From those k, pick an initial c < k to evaluate. 3.",
        "Run RTC to 95% confidence on the initial c. 4.",
        "Using the model from Section 3, estimate the probabilities of relevance for all documents retrieved by all k runs. 5.",
        "Calculate EMAP for all k runs, and P(ΔMAP < 0) for all pairs of runs.",
        "We do the same for MTC, but omit step 4.",
        "Note that after evaluating the first c systems, we make no additional relevance judgments.",
        "To put our method to the test, we selected c = 2: we will build a set of judgments from evaluating only two initial systems.",
        "We will then generalize to a set of k = 10 (of which those two are a subset).",
        "As we run more trials, we obtain the data we need to test all three of our hypotheses. 4.4 Experimental Evaluation Recall that a set of judgments is robust if the accuracy of the predictions it makes is at least its estimated confidence.",
        "One way to evaluate robustness is to bin pairs by their confidence, then calculate the accuracy over all the pairs in each bin.",
        "We would like the accuracy to be no less than the lowest confidence score in the bin, but preferably higher.",
        "Since summary statistics are useful, we devised the following metric.",
        "Suppose we are a bookmaker taking bets on whether ΔMAP < 0.",
        "We use RTC or MTC to set the odds O = P (ΔMAP <0) 1−P (ΔMAP <0) .",
        "Suppose a bettor wagers $1 on ΔMAP ≥ 0.",
        "If it turns out that ΔMAP < 0, we win the dollar.",
        "Otherwise, we pay out O.",
        "If our confidence estimates are perfectly accurate, we break even.",
        "If confidence is greater than accuracy, we lose money; we win if accuracy is greater than confidence.",
        "Counterintuitively, the most desirable outcome is breaking even: if we lose money, we cannot trust the confidence estimates, but if we win money, we have either underestimated confidence or judged more documents than necessary.",
        "However, the cost of not being able to trust the confidence estimates is higher than the cost of extra relevance judgments, so we will treat positive outcomes as good.",
        "The amount we win on each pairwise comparison i is: Wi = yi − (1 − yi) Pi 1 − Pi = yi − Pi 1 − Pi yi = 1 if ΔMAP < 0 and 0 otherwise, and Pi = P(ΔMAP < 0).",
        "The summary statistic is W, the mean of Wi.",
        "Note that as Pi increases, we lose more for being wrong.",
        "This is as it should be: the penalty should be great for missing the high probability predictions.",
        "However, since our losses grow without bound as predictions approach certainty, we cap −Wi at 100.",
        "For our hypothesis that RTC requires fewer judgments than MTC, we are interested in the number of judgments needed to reach 95% confidence on the first pair of systems.",
        "The median is more interesting than the mean: most pairs require a few hundred judgments, but a few pairs require several thousand.",
        "The distribution is therefore highly skewed, and the mean strongly affected by those outliers.",
        "Finally, for our hypothesis that RTC is more accurate than MTC, we will look at Kendalls τ correlation between a ranking of k systems by a small set of judgments and the true ranking using the full set of judgments.",
        "Kendalls τ, a nonparametric statistic based on pairwise swaps between two lists, is a standard evaluation for this type of study.",
        "It ranges from −1 (perfectly anti-correlated) to 1 (rankings identical), with 0 meaning that half of the pairs are swapped.",
        "As we touched on in the introduction, though, an accuracy measure like rank correlation is not a good evaluation of reusability.",
        "We include it for completeness. 4.4.1 Hypothesis Testing Running multiple trials allows the use of statistical hypothesis testing to compare algorithms.",
        "Using the same sets of systems allows the use of paired tests.",
        "As we stated above, we are more interested in the median number of judgments than the mean.",
        "A test for difference in median is the Wilcoxon sign rank test.",
        "We can also use a paired t-test to test for a difference in mean.",
        "For rank correlation, we can use a paired t-test to test for a difference in τ. 5.",
        "RESULTS AND ANALYSIS The comparison between MTC and RTC is shown in Table 2.",
        "With MTC and uniform probabilities of relevance, the results are far from robust.",
        "We cannot reuse the relevance judgments with much confidence.",
        "But with RTC, the results are very robust.",
        "There is a slight dip in accuracy when confidence gets above 0.95; nonetheless, the confidence predictions are trustworthy.",
        "Mean Wi shows that RTC is much closer to 0 than MTC.",
        "The distribution of confidence scores shows that at least 80% confidence is achieved more than 35% of the time, indicating that neither algorithm is being too conservative in its confidence estimates.",
        "The confidence estimates are rather low overall; that is because we have built a test collection from only two initial systems.",
        "Recall from Section 1 that we cannot require (or even expect) a minimum level of confidence when we generalize to new systems.",
        "More detailed results for both algorithms are shown in Figure 2.",
        "The solid line is the ideal result that would give W = 0.",
        "RTC is on or above this line at all points until confidence reaches about 0.97.",
        "After that there is a slight dip in accuracy which we discuss below.",
        "Note that both MTC RTC confidence % in bin accuracy % in bin accuracy 0.5 − 0.6 33.7% 61.7% 28.6% 61.9% 0.6 − 0.7 18.1% 73.1% 20.1% 76.3% 0.7 − 0.8 10.4% 70.1% 15.5% 78.0% 0.8 − 0.9 9.4% 69.0% 12.1% 84.9% 0.9 − 0.95 7.3% 78.0% 6.6% 93.1% 0.95 − 0.99 17.9% 70.4% 12.4% 93.4% 1.0 3.3% 68.3% 4.7% 98.9% W −5.34 −0.39 median judged 251 235 mean τ 0.393 0.555 Table 2: Confidence that P(ΔMAP < 0) and accuracy of prediction when generalizing a set of relevance judgments acquired using MTC and RTC.",
        "Each bin contains over 1,000 trials from the adhoc 3, 5-8 sets.",
        "RTC is much more robust than MTC.",
        "W is defined in Section 4.4; closer to 0 is better.",
        "Median judged is the number of judgments to reach 95% confidence on the first two systems.",
        "Mean τ is the average rank correlation for all 10 systems. 0.5 0.6 0.7 0.8 0.9 1 0.5 0.6 0.7 0.8 0.9 1 accuracy confidence breakeven RTC MTC Figure 2: Confidence vs. accuracy of MTC and RTC.",
        "The solid line is the perfect result that would give W = 0; performance should be on or above this line.",
        "Each point represents at least 500 pairwise comparisons. algorithms are well above the line up to around confidence 0.7.",
        "This is because the baseline performance on these data sets is high; it is quite easy to achieve 75% accuracy doing very little work [7].",
        "Number of Judgments: The median number of judgments required by MTC to reach 95% confidence on the first two systems is 251, an average of 5 per topic.",
        "The median required by RTC is 235, about 4.7 per topic.",
        "Although the numbers are close, RTCs median is significantly lower by a paired Wilcoxon test (p < 0.0001).",
        "For comparison, a pool of depth 100 would result in a minimum of 5,000 judgments for each pair.",
        "The difference in means is much greater.",
        "MTC required a mean of 823 judgments, 16 per topic, while RTC required a mean of 502, 10 per topic. (Recall that means are strongly skewed by a few pairs that take thousands of judgments.)",
        "This difference is significant by a paired t-test (p < 0.0001).",
        "Ten percent of the sets resulted in 100 or fewer judgments (less than two per topic).",
        "Performance on these is very high: W = 0.41, and 99.7% accuracy when confidence is at least 0.9.",
        "This shows that even tiny collections can be reusable.",
        "For the 50% of sets with more than 235 judgments, accuracy is 93% when confidence is at least 0.9.",
        "Rank Correlation: MTC and RTC both rank the 10 systems by EMAP (Eq. (1)) calculated using their respective probability estimates.",
        "The mean τ rank correlation between true MAP and EMAP is 0.393 for MTC and 0.555 for RTC.",
        "This difference is significant by a paired t-test (p < 0.0001).",
        "Note that we do not expect the τ correlations to be high, since we are ranking the systems with so few relevance judgments.",
        "It is more important that we estimate confidence in each pairwise comparison correctly.",
        "We ran IP for the same number of judgments that MTC took for each pair, then ranked the systems by MAP using only those judgments (all unjudged documents assumed nonrelevant).",
        "We calculated the τ correlation to the true ranking.",
        "The mean τ correlation is 0.398, which is not significantly different from MTC, but is significantly lower than RTC.",
        "Using uniform estimates of probability is indistinguishable from the baseline, whereas estimating relevance by expert aggregation boosts performance a great deal: nearly 40% over both MTC and IP.",
        "Overfitting: It is possible to overfit: if too many judgments come from the first two systems, the variance in ΔMAP is reduced and the confidence estimates become unreliable.",
        "We saw this in Table 2 and Figure 2 where RTC exhibits a dip in accuracy when confidence is around 97%.",
        "In fact, the number of judgments made prior to a wrong prediction is over 50% greater than the number made prior to a correct prediction.",
        "Overfitting is difficult to quantify exactly, because making more relevance judgments does not always cause it: at higher confidence levels, more relevance judgments are made, and as Table 2 shows, accuracy is greater at those higher confidences.",
        "Obviously having more relevance judgments should increase both confidence and accuracy; the difference seems to be when one system has a great deal more judgments than the other.",
        "Pairwise Comparisons: Our pairwise comparisons fall into one of three groups: 1. the two original runs from which relevance judgments are acquired; 2. one of the original runs vs. one of the new runs; 3. two new runs.",
        "Table 3 shows confidence vs. accuracy results for each of these three groups.",
        "Interestingly, performance is worst when comparing one of the original runs to one of the additional runs.",
        "This is most likely due to a large difference in the number of judgments affecting the variance of ΔMAP.",
        "Nevertheless, performance is quite good on all three subsets.",
        "Worst Case: The case intuitively most likely to produce an error is when the two systems being compared have retrieved very few documents in common.",
        "If we want the judgments to be reusable, we should to be able to generalize even to runs that are very different from the ones used to acquire the relevance judgments.",
        "A simple measure of similarity of two runs is the average percentage of documents they retrieved in common for each topic [2].",
        "We calculated this for all pairs, then looked at performance on pairs with low similarity.",
        "Results are shown in accuracy confidence two original one original no original 0.5 − 0.6 - 48.1% 62.8% 0.6 − 0.7 - 57.1% 79.2% 0.7 − 0.8 - 67.9% 81.7% 0.8 − 0.9 - 82.2% 86.3% 0.9 − 0.95 95.9% 93.7% 92.6% 0.95 − 0.99 96.2% 92.5% 93.1% 1.0 100% 98.0% 99.1% W −1.11 −0.87 −0.27 Table 3: Confidence vs. accuracy of RTC when comparing the two original runs, one original run and one new run, and two new runs.",
        "RTC is robust in all three cases. accuracy when similar confidence 0 − 0.1 0.1 − 0.2 0.2 − 0.3 0.5 − 0.6 68.4% 63.1% 61.4% 0.6 − 0.7 84.2% 78.6% 76.6% 0.7 − 0.8 82.0% 79.8% 78.9% 0.8 − 0.9 93.6% 83.3% 82.1% 0.9 − 0.95 99.3% 92.7% 92.4% 0.95 − 0.99 98.7% 93.4% 93.3% 1.0 99.9% 97.9% 98.1% W 0.44 −0.45 −0.49 Table 4: Confidence vs. accuracy of RTC when a pair of systems retrieved 0-30% documents in common (broken out into 0%-10%, 10%-20%, and 20%30%).",
        "RTC is robust in all three cases.",
        "Table 4.",
        "Performance is in fact very robust even when similarity is low.",
        "When the two runs share very few documents in common, W is actually positive.",
        "MTC and IP both performed quite poorly in these cases.",
        "When the similarity was between 0 and 10%, both MTC and IP correctly predicted ΔMAP only 60% of the time, compared to an 87.6% success rate for RTC.",
        "By Data Set: All the previous results have only been on the ad-hoc collections.",
        "We did the same experiments on our additional data sets, and broke out the results by data set to see how performance varies.",
        "The results in Table 5 show everything about each set, including binned accuracy, W, mean τ, and median number of judgments to reach 95% confidence on the first two systems.",
        "The results are highly consistent from collection to collection, suggesting that our method is not overfitting to any particular data set. 6.",
        "CONCLUSIONS AND FUTURE WORK In this work we have offered the first formal definition of the common idea of reusability of a test collection and presented a model that is able to achieve reusability with very small sets of relevance judgments.",
        "Table 2 and Figure 2 together show how biased a small set of judgments can be: MTC is dramatically overestimating confidence and is much less accurate than RTC, which is able to remove the bias to give a robust evaluation.",
        "The confidence estimates of RTC, in addition to being accurate, provide a guide for obtaining additional judgments: focus on judging documents from the lowest-confidence comparisons.",
        "In the long run, we see small sets of relevance judgaccuracy confidence ad-hoc 94 ad-hoc 96 ad-hoc 97 ad-hoc 98 ad-hoc 99 web 04 robust 05 terabyte 05 0.5 − 0.6 64.1% 61.8% 62.2% 62.0% 59.4% 64.3% 61.5% 61.6% 0.6 − 0.7 76.1% 77.8% 74.5% 78.2% 74.3% 78.1% 75.9% 75.9% 0.7 − 0.8 75.2% 78.9% 77.6% 80.0% 78.6% 82.6% 77.5% 80.4% 0.8 − 0.9 83.2% 85.5% 84.6% 84.9% 86.8% 84.5% 86.7% 87.7% 0.9 − 0.95 93.0% 93.6% 92.8% 93.7% 92.6% 94.2% 93.9% 94.2% 0.95 − 0.99 93.1% 94.3% 93.1% 93.7% 92.8% 95.0% 93.9% 91.6% 1.0 99.2% 96.8% 98.7% 99.5% 99.6% 100% 99.2% 98.3% W -0.34 -0.34 -0.48 -0.35 -0.44 -0.07 -0.41 -0.67 median judged 235 276 243 213 179 448 310 320 mean τ 0.538 0.573 0.556 0.579 0.532 0.596 0.565 0.574 Table 5: Accuracy, W, mean τ, and median number of judgments for all 8 testing sets.",
        "The results are highly consistent across data sets. ments being shared by researchers, each group contributing a few more judgments to gain more confidence about their particular systems.",
        "As time goes on, the number of judgments grows until there is 100% confidence in every evaluation-and there is a full test collection for the task.",
        "We see further use for this method in scenarios such as web retrieval in which the corpus is frequently changing.",
        "It could be applied to evaluation on a dynamic test collection as defined by Soboroff [18].",
        "The model we presented in Section 3 is by no means the only possibility for creating a robust test collection.",
        "A simpler expert aggregation model might perform as well or better (though all our efforts to simplify failed).",
        "In addition to expert aggregation, we could estimate probabilities by looking at similarities between documents.",
        "This is an obvious area for future exploration.",
        "Additionally, it will be worthwhile to investigate the issue of overfitting: the circumstances it occurs under and what can be done to prevent it.",
        "In the meantime, capping confidence estimates at 95% is a hack that solves the problem.",
        "We have many more experimental results that we unfortunately did not have space for but that reinforce the notion that RTC is highly robust: with just a few judgments per topic, we can accurately assess the confidence in any pairwise comparison of systems.",
        "Acknowledgments This work was supported in part by the Center for Intelligent Information Retrieval and in part by the Defense Advanced Research Projects Agency (DARPA) under contract number HR0011-06-C-0023.",
        "Any opinions, findings, and conclusions or recommendations expressed in this material are those of the author and do not necessarily reflect those of the sponsor. 7.",
        "REFERENCES [1] J. Aslam and M. Montague.",
        "Models for Metasearch.",
        "In Proceedings of SIGIR, pages 275-285, 2001. [2] J. Aslam and R. Savell.",
        "On the effectiveness of evaluating retrieval systems in the absence of relevance judgments.",
        "In Proceedings of SIGIR, pages 361-362, 2003. [3] J.",
        "A. Aslam, V. Pavlu, and R. Savell.",
        "A unified model for metasearch, pooling, and system evaluation.",
        "In Proceedings of CIKM, pages 484-491, 2003. [4] J.",
        "A. Aslam, V. Pavlu, and E. Yilmaz.",
        "A statistical method for system evaluation using incomplete judgments.",
        "In Proceedings of SIGIR, pages 541-548, 2006. [5] A. L. Berger, S. D. Pietra, and V. J. D. Pietra.",
        "A maximum entropy approach to natural language processing.",
        "Computational Linguistics, 22(1):39-71, 1996. [6] D. J. Blower.",
        "An easy derivation of logistic regression from the bayesian and maximum entropy perspective.",
        "In Proceedings of the 23rd International Workship on Bayesian Inference and Maximum Entropy Methods in Science and Engineering, pages 30-43, 2004. [7] B. Carterette and J. Allan.",
        "Research methodology in studies of assessor effort for retrieval evaluation.",
        "In Proceedings of RIAO, 2007. [8] B. Carterette, J. Allan, and R. K. Sitaraman.",
        "Minimal test collections for retrieval evaluation.",
        "In Proceedings of SIGIR, pages 268-275, 2006. [9] B. Carterette and D. I. Petkova.",
        "Learning a ranking from pairwise preferences.",
        "In Proceedings of SIGIR, 2006. [10] R. T. Clemen and R. L. Winkler.",
        "Unanimity and compromise among probability forecasters.",
        "Management Science, 36(7):767-779, July 1990. [11] G. V. Cormack, C. R. Palmer, and C. L. Clarke.",
        "Efficient Construction of Large Test Collections.",
        "In Proceedings of SIGIR, pages 282-289, 1998. [12] A. Gelman, J.",
        "B. Carlin, H. S. Stern, and D. B. Rubin.",
        "Bayesian Data Analysis.",
        "Chapman & Hall/CRC, 2004. [13] E. T. Jaynes.",
        "Probability Theory: The Logic of Science.",
        "Cambridge University Press, 2003. [14] R. Manmatha and H. Sever.",
        "A Formal Approach to Score Normalization for Metasearch.",
        "In Proceedings of HLT, pages 88-93, 2002. [15] I. J. Myung, S. Ramamoorti, and J. Andrew D. Baily.",
        "Maximum entropy aggregation of expert predictions.",
        "Management Science, 42(10):1420-1436, October 1996. [16] J. Platt.",
        "Probabilistic outputs for support vector machines and comparison to regularized likelihood methods. pages 61-74, 2000. [17] M. Sanderson and H. Joho.",
        "Forming test collections with no system pooling.",
        "In Proceedings of the 27th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 33-40, 2004. [18] I. Soboroff.",
        "Dynamic test collections: measuring search effectiveness on the live web.",
        "In Proceedings of SIGIR, pages 276-283, 2006. [19] K. Sparck Jones and C. J. van Rijsbergen.",
        "Information Retrieval Test Collections.",
        "Journal of Documentation, 32(1):59-75, 1976. [20] E. M. Voorhees and D. K. Harman, editors.",
        "TREC: Experiment and Evaluation in Information Retrieval.",
        "MIT Press, 2005. [21] J. Zobel.",
        "How Reliable are the Results of Large-Scale Information Retrieval Experiments?",
        "In Proceedings of SIGIR, pages 307-314, 1998."
    ],
    "translated_text_sentences": [
        "Colecciones de pruebas robustas para la evaluación de recuperación Ben Carterette Centro de Recuperación de Información Inteligente Departamento de Ciencias de la Computación Universidad de Massachusetts Amherst Amherst, MA 01003 carteret@cs.umass.edu RESUMEN Los métodos de bajo costo para adquirir juicios de relevancia pueden ser una bendición para los investigadores que necesitan evaluar nuevas tareas o temas de recuperación pero no tienen los recursos para hacer miles de juicios.",
        "Si bien estos juicios son muy útiles para una evaluación única, no está claro que se puedan confiar al ser reutilizados para evaluar nuevos sistemas.",
        "En este trabajo, definimos formalmente lo que significa que las evaluaciones sean reutilizables: la confianza en una evaluación de nuevos sistemas puede ser evaluada con precisión a partir de un conjunto existente de juicios de relevancia.",
        "Luego presentamos un método para aumentar un conjunto de juicios de relevancia con estimaciones de relevancia que no requieren esfuerzo adicional del evaluador.",
        "Usar este método garantiza prácticamente la reutilización: con tan solo cinco juicios por tema tomados de solo dos sistemas, podemos evaluar de manera confiable un conjunto más grande de diez sistemas.",
        "Incluso los conjuntos más pequeños de juicios pueden ser útiles para la evaluación de nuevos sistemas.",
        "Categorías y Descriptores de Asignaturas: H.3 Almacenamiento y Recuperación de Información; H.3.4 Sistemas y Software: Evaluación del Rendimiento Términos Generales: Experimentación, Medición, Confiabilidad 1.",
        "INTRODUCCIÓN Imagina a un investigador de recuperación de información que ha inventado una nueva tarea de recuperación.",
        "Ella ha construido un sistema para realizar la tarea y quiere evaluarlo.",
        "Dado que la tarea es nueva, es poco probable que existan juicios de relevancia vigentes.",
        "Ella no tiene el tiempo ni los recursos para juzgar cada documento, ni siquiera cada documento recuperado.",
        "Ella solo puede juzgar los documentos que parecen ser los más informativos y detenerse cuando tenga un grado razonable de confianza en sus conclusiones.",
        "Pero ¿qué sucede cuando ella desarrolla un nuevo sistema y necesita evaluarlo?",
        "¿Otro grupo de investigación decide implementar un sistema para realizar la tarea?",
        "¿Pueden reutilizar de manera confiable los juicios originales?",
        "¿Pueden evaluar sin más juicios de relevancia?",
        "La evaluación es un aspecto importante de la investigación en recuperación de información, pero es solo un problema parcialmente resuelto: para la mayoría de las tareas de recuperación, es imposible juzgar la relevancia de cada documento; simplemente hay demasiados.",
        "La solución utilizada por NIST en TREC (Text REtrieval Conference) es el método de agrupación [19, 20]: todos los sistemas competidores contribuyen con N documentos a un grupo, y cada documento en ese grupo es evaluado.",
        "Este método crea grandes conjuntos de juicios que son reutilizables para entrenar o evaluar nuevos sistemas que no contribuyeron al conjunto [21].",
        "Esta solución no es adecuada para nuestro investigador hipotético.",
        "El método de agrupación proporciona miles de juicios de relevancia, pero requiere muchas horas de tiempo de anotadores (remunerados).",
        "Como resultado, ha habido una serie de artículos recientes sobre la reducción del esfuerzo del anotador en la producción de colecciones de pruebas: Cormack et al. [11], Zobel [21], Sanderson y Joho [17], Carterette et al. [8], y Aslam et al. [4], entre otros.",
        "Como veremos, los juicios que estos métodos producen pueden sesgar significativamente la evaluación de un nuevo conjunto de sistemas.",
        "¿Volviendo a nuestra investigadora hipotética, puede reutilizar sus juicios de relevancia?",
        "Primero debemos definir formalmente lo que significa ser reutilizable.",
        "En trabajos anteriores, la reutilización se ha probado simplemente evaluando la precisión de un conjunto de juicios de relevancia al evaluar sistemas no vistos.",
        "Si bien podemos decir que tuvo razón el 75% del tiempo, o que tuvo una correlación de rango de 0.8, estos números no tienen ningún poder predictivo: no nos dicen qué sistemas es probable que estén equivocados ni cuán seguros deberíamos estar en uno en particular.",
        "Necesitamos una definición más cuidadosa de reutilización.",
        "Específicamente, la cuestión de la reutilización no es qué tan precisamente podemos evaluar los nuevos sistemas.",
        "Un adversario malicioso siempre puede producir una nueva lista clasificada que no haya recuperado ninguno de los documentos evaluados.",
        "La verdadera pregunta es cuánta confianza tenemos en nuestras evaluaciones, y, más importante aún, si podemos confiar en nuestras estimaciones de confianza.",
        "Aunque la confianza no sea alta, siempre y cuando podamos confiar en ello, podemos identificar qué sistemas necesitan más juicios para aumentar la confianza.",
        "Cualquier conjunto de juicios, por pequeño que sea, se vuelve reutilizable en cierta medida.",
        "Pequeñas colecciones de pruebas reutilizables podrían tener un gran impacto en la investigación de recuperación de información.",
        "Los grupos de investigación podrían compartir las evaluaciones de relevancia que han realizado internamente para estudios piloto, nuevas tareas o nuevos temas.",
        "La cantidad de datos disponible para los investigadores crecería de forma exponencial con el tiempo.",
        "EVALUACIÓN ROBUSTA Anteriormente dimos una definición intuitiva de reutilización: una colección es reutilizable si podemos confiar en nuestras estimaciones de confianza en una evaluación.",
        "Con eso queremos decir que si hemos realizado algunos juicios de relevancia y tenemos, por ejemplo, un 75% de confianza en que el sistema A es mejor que el sistema B, nos gustaría que no haya más del 25% de posibilidad de que nuestra evaluación de la calidad relativa de los sistemas cambie a medida que continuamos evaluando documentos.",
        "Nuestra evaluación debe ser robusta ante juicios faltantes.",
        "En nuestro trabajo anterior, definimos la confianza como la probabilidad de que la diferencia en una medida de evaluación calculada para dos sistemas sea menor que cero [8].",
        "Esta noción de confianza se define en el contexto de una tarea de evaluación particular que llamamos evaluación comparativa: determinar el signo de la diferencia en una medida de evaluación.",
        "Otras tareas de evaluación podrían ser definidas; estimar la magnitud de la diferencia o los valores de las medidas mismas son ejemplos que implican diferentes nociones de confianza.",
        "Por lo tanto, consideramos la confianza como una estimación de probabilidad.",
        "Una de las preguntas que debemos hacer acerca de una estimación de probabilidad es qué significa.",
        "¿Qué significa tener un 75% de confianza de que el sistema A es mejor que el sistema B?",
        "Como se describe arriba, queremos que signifique que si seguimos evaluando documentos, solo habrá un 25% de probabilidad de que nuestra evaluación cambie.",
        "Si esto es lo que significa, podemos confiar en las estimaciones de confianza.",
        "¿Pero sabemos que tiene ese significado?",
        "Nuestro cálculo de confianza se basó en una suposición sobre la probabilidad de relevancia de los documentos no evaluados, específicamente que cada documento no evaluado tenía la misma probabilidad de ser relevante o no relevante.",
        "Esta suposición casi con seguridad no es realista en la mayoría de las aplicaciones de IR.",
        "Resulta que es esta suposición la que determina si se pueden confiar en las estimaciones de confianza.",
        "Antes de profundizar en esto, definimos formalmente la confianza. 2.1 Estimación de la confianza La precisión promedio (AP) es una métrica de evaluación estándar que captura tanto la capacidad de un sistema para clasificar documentos relevantes de manera alta (precisión) como su capacidad para recuperar documentos relevantes (recuperación).",
        "Normalmente se escribe como la precisión media en los rangos de documentos relevantes: AP = 1 |R| i∈R prec@r(i) donde R es el conjunto de documentos relevantes y r(i) es el rango del documento i.",
        "Sea Xi una variable aleatoria que indica la relevancia del documento i.",
        "Si los documentos están ordenados por rango, podemos expresar la precisión como prec@i = 1/i i j=1 Xj.",
        "La precisión promedio se convierte entonces en la ecuación cuadrática AP = 1 Xi n i=1 Xi/i i j=1 Xj = 1 Xi n i=1 j≥i aijXiXj donde aij = 1/ max{r(i), r(j)}.",
        "Usar aij en lugar de 1/i nos permite numerar los documentos de forma arbitraria.",
        "Para ver por qué esto es cierto, considera un ejemplo simple: una lista de 3 documentos con los documentos relevantes B, C en los rangos 1 y 3 y el documento no relevante A en el rango 2.",
        "La precisión promedio será 1 2 (1 1 x2 B+ 1 2 xBxA+ 1 3 xBxC + 1 2 x2 A+ 1 3 xAxC + 1 3 x2 C) = 1 2 1 + 2 3 porque xA = 0, xB = 1, xC = 1.",
        "Aunque el orden B, A, C es diferente de la etiqueta A, B, C, no afecta al cálculo.",
        "Ahora podemos ver que la precisión promedio en sí misma es una variable aleatoria con una distribución sobre todas las posibles asignaciones de relevancia a todos los documentos.",
        "Esta variable aleatoria tiene una esperanza, una varianza, intervalos de confianza y una cierta probabilidad de ser menor o igual a un valor dado.",
        "Todos estos dependen de la probabilidad de que el documento i sea relevante: pi = p(Xi = 1).",
        "Supongamos que en nuestro ejemplo anterior no conocemos las valoraciones de relevancia, pero creemos que pA = 0.4, pB = 0.8, pC = 0.7.",
        "Podemos entonces calcular, por ejemplo.",
        "P(AP = 0) = 0.2 · 0.6 · 0.3 = 0.036, o P(AP = 1 2 ) = 0.2 · 0.4 · 0.7 = 0.056. \n\nLa traducción al español sería: P(AP = 0) = 0.2 · 0.6 · 0.3 = 0.036, o P(AP = 1 2 ) = 0.2 · 0.4 · 0.7 = 0.056.",
        "Sumando sobre todas las posibilidades, podemos calcular la esperanza y la varianza: E[AP] ≈ 1 pi aiipi + j>i aij pipj V ar[AP] ≈ 1 ( pi)2 n i a2 iipiqi + j>i a2 ijpipj(1 − pipj) + i=j 2aiiaijpipj(1 − pi) + k>j=i 2aijaikpipjpk(1 − pi) AP converge asintóticamente a una distribución normal con la esperanza y la varianza definidas anteriormente. Para nuestra tarea de evaluación comparativa, estamos interesados en el signo de la diferencia en dos precisiones promedio: ΔAP = AP1 − AP2.",
        "Como mostramos en nuestro trabajo anterior, ΔAP tiene una forma cerrada cuando los documentos están ordenados arbitrariamente: ΔAP = 1 Xi n i=1 j≥i cij XiXj cij = aij − bij donde bij está definido de manera análoga a aij para la segunda clasificación.",
        "Dado que AP es normal, ΔAP también es normal, lo que significa que podemos usar la función de densidad acumulativa normal para determinar la confianza de que una diferencia en AP sea menor que cero.",
        "Dado que los temas son independientes, podemos extender fácilmente esto para referirnos a la precisión promedio (MAP).",
        "MAP también se distribuye normalmente; su esperanza y varianza son: EMAP = 1 T t∈T E[APt] (1) VMAP = 1 T2 t∈T V ar[APt] ΔMAP = MAP1 − MAP2 La confianza puede entonces estimarse calculando la esperanza y varianza y utilizando la función de densidad normal para encontrar P(ΔMAP < 0). 2.2 Confianza y Robustez Habiendo definido la confianza, volvemos al tema de la confianza en las estimaciones de confianza, y mostramos cómo se relaciona con la robustez de la colección ante juicios faltantes. 1 Estas son en realidad aproximaciones a la verdadera esperanza y varianza, pero el error es despreciable O(n2−n ).",
        "Sea Z el conjunto de todos los pares de resultados clasificados para un conjunto común de temas.",
        "Supongamos que tenemos un conjunto de m juicios de relevancia xm = {x1, x2, ..., xm} (usando x minúscula en lugar de X mayúscula para distinguir entre documentos juzgados y no juzgados); estos son los juicios contra los cuales calculamos la confianza.",
        "Sea Zα el subconjunto de pares en Z para los cuales predecimos que ΔMAP = −1 con confianza α dadas las evaluaciones xm.",
        "Para que las estimaciones de confianza sean precisas, necesitamos que al menos α · |Zα| de estos pares realmente tengan ΔMAP = −1 después de haber evaluado cada documento.",
        "Si lo hacen, podemos confiar en las estimaciones de confianza; nuestra evaluación será robusta ante juicios faltantes.",
        "Si nuestras estimaciones de confianza se basan en suposiciones poco realistas, no podemos esperar que sean precisas.",
        "Las suposiciones en las que se basan son las probabilidades de relevancia pi.",
        "Necesitamos que esto sea realista.",
        "Sostenemos que la mejor distribución posible de relevancia p(Xi) es aquella que explica todos los datos (todas las observaciones realizadas sobre los sistemas de recuperación) al mismo tiempo que no hace suposiciones infundadas.",
        "Esto se conoce como el principio de máxima entropía [13].",
        "La entropía de una variable aleatoria X con distribución p(X) se define como H(p) = − i p(X = i) log p(X = i).",
        "Esto ha encontrado una amplia variedad de usos en la informática y la recuperación de información.",
        "La distribución de entropía máxima es aquella que maximiza H. Esta distribución es única y tiene una forma exponencial.",
        "El siguiente teorema muestra la utilidad de una distribución de entropía máxima para la relevancia al estimar la confianza.",
        "Teorema 1.",
        "Si p(Xn |I, xm ) = argmaxpH(p), las estimaciones de confianza serán precisas, donde xm es el conjunto de juicios de relevancia definidos anteriormente, Xn es el conjunto completo de documentos cuya relevancia deseamos estimar, e I es alguna información sobre los documentos (no especificada hasta ahora).",
        "Por el momento prescindimos de la prueba, pero es bastante simple.",
        "Esto indica que cuanto mejores sean las estimaciones de relevancia, más precisa será la evaluación.",
        "La tarea de crear una colección de pruebas reutilizable se convierte entonces en la tarea de estimar la relevancia de los documentos no evaluados.",
        "El teorema y su demostración no dicen absolutamente nada sobre la métrica de evaluación.",
        "Las estimaciones de probabilidad son completamente independientes de la medida en la que estamos interesados.",
        "Esto significa que los mismos cálculos de probabilidad pueden informarnos sobre la precisión promedio, así como sobre la precisión, la recuperación, el bpref, etc.",
        "Además, podríamos asumir que la relevancia de los documentos i y j es independiente y lograr el mismo resultado, lo cual afirmamos como un corolario: Corolario 1.",
        "Si p(Xi|I, xm) = argmaxpH(p), las estimaciones de confianza serán precisas.",
        "La tarea, por lo tanto, consiste en la imputación de los valores faltantes de relevancia.",
        "El teorema implica que cuanto más nos acerquemos a la distribución de entropía máxima relevante, más nos acercaremos a la robustez. 3.",
        "PREDICIENDO LA RELEVANCIA En nuestra declaración del Teorema 1, dejamos sin especificar la naturaleza de la información I.",
        "Una de las ventajas de nuestras estimaciones de confianza es que admiten información de una amplia variedad de fuentes; básicamente cualquier cosa que pueda ser modelada puede ser utilizada como información para predecir la relevancia.",
        "Una fuente natural de información son los propios sistemas de recuperación: cómo clasificaron los documentos evaluados, con qué frecuencia no lograron clasificar documentos relevantes, cómo se desempeñan en diferentes temas, y así sucesivamente.",
        "Si tratamos cada sistema como un experto en recuperación de información que proporciona una opinión sobre la relevancia de cada documento, el problema se convierte en uno de agregación de opiniones de expertos.",
        "Esto es similar al problema de metabúsqueda o fusión de datos en el que la tarea consiste en tomar k sistemas de entrada y fusionarlos en un solo ranking.",
        "Aslam et al. [3] identificaron previamente una conexión entre la evaluación y la metabúsqueda.",
        "Nuestro problema tiene dos diferencias clave: 1.",
        "Necesitamos explícitamente probabilidades de relevancia que podamos insertar en la Ecuación 1; los algoritmos de metabusqueda no tienen tal requisito.",
        "Estamos acumulando juicios de relevancia a medida que avanzamos con la evaluación y somos capaces de reestimar la relevancia dada cada nuevo juicio.",
        "A la luz de lo anterior, introducimos un modelo probabilístico para la combinación de expertos. 3.1 Un Modelo para la Agregación de Opiniones de Expertos Supongamos que cada experto j proporciona una probabilidad de relevancia qij = pj(Xi = 1).",
        "La información sobre la relevancia del documento i será entonces el conjunto de k opiniones de expertos I = qi = (qi1, qi2, · · · , qik).",
        "La distribución de probabilidad que deseamos encontrar es aquella que maximiza la entropía de pi = p(Xi = 1|qi).",
        "Resulta que encontrar el modelo de entropía máxima es equivalente a encontrar los parámetros que maximizan la verosimilitud [5].",
        "Blower [6] muestra explícitamente que encontrar el modelo de entropía máxima para una variable binaria es equivalente a resolver una regresión logística.",
        "Entonces pi = p(Xi = 1|qi) = exp k j=1 λjqij 1 + exp k j=1 λj qij (2) donde λ1, · · · , λk son los parámetros de regresión.",
        "Incluimos una distribución beta a priori para p(λj) con parámetros α, β.",
        "Esto se puede ver como un tipo de suavizado para tener en cuenta el hecho de que los datos de entrenamiento están altamente sesgados.",
        "Este modelo tiene la ventaja de incluir la dependencia estadística entre los expertos.",
        "Un modelo de la misma forma fue demostrado por Clemen & Winkler como el mejor para agregar probabilidades de expertos [10].",
        "Un enfoque similar motivado por la entropía máxima se ha utilizado para la agregación de expertos [15].",
        "Aslam & Montague [1] utilizaron un modelo similar para la metabúsqueda, pero asumieron independencia entre los expertos.",
        "¿De dónde vienen los qij?",
        "Utilizar puntuaciones sin procesar y no calibradas como predictores no funcionará porque las distribuciones de puntuaciones varían demasiado entre los temas.",
        "Un clasificador de modelado de lenguaje, por ejemplo, suele otorgar una puntuación mucho más alta al documento recuperado en la parte superior para una consulta corta que al documento recuperado en la parte superior para una consulta larga.",
        "Podríamos entrenar un modelo predictivo separado para cada tema, pero eso no aprovecha toda la información que tenemos: es posible que solo tengamos un puñado de juicios para un tema, no suficientes para entrenar un modelo con confianza.",
        "Además, parece razonable asumir que si un experto hace buenas predicciones para un tema, también hará buenas predicciones para otros temas.",
        "Podríamos usar un modelo jerárquico [12], pero eso no se generalizará a temas no vistos.",
        "En cambio, calibraremos las puntuaciones de cada experto de forma individual para que las puntuaciones puedan ser comparadas tanto dentro de un tema como entre temas.",
        "Por lo tanto, nuestro modelo tiene en cuenta no solo la dependencia entre expertos, sino también la dependencia entre el rendimiento de los expertos en diferentes tareas (temas). 3.2 Calibración de Expertos Cada experto nos proporciona una puntuación y un rango para cada documento.",
        "Necesitamos convertir estos en probabilidades.",
        "Un método como el utilizado por Manmatha et al. [14] podría ser utilizado para convertir puntuaciones en probabilidades de relevancia.",
        "El método de preferencia por pares de Carterette & Petkova [9] también podría ser utilizado, interpretando la clasificación de un documento sobre otro como una expresión de preferencia.",
        "Que q∗ ij sea la probabilidad autoinformada por el experto j de que el documento i sea relevante.",
        "De manera intuitiva, parece claro que q∗ ij debería disminuir con el rango, y debería ser cero si el documento i no está clasificado (el experto no consideró que fuera relevante).",
        "El modelo de preferencia por pares puede manejar fácilmente estos dos requisitos, por lo que lo utilizaremos.",
        "Que θrj (i) sea el coeficiente de relevancia del documento en la posición rj(i).",
        "Queremos encontrar los θs que maximizan la función de verosimilitud: Ljt(Θ) = rj (i)<rj (k) exp(θrj (i) − θrj (k)) 1 + exp(θrj (i) − θrj (k)) Incluimos nuevamente una distribución beta previa en p(θrj(i)) con parámetros |Rt| + 1 y |Nt| + 1, el tamaño de los conjuntos de documentos relevantes y no relevantes juzgados respectivamente.",
        "El uso de estos parámetros previos garantiza que las probabilidades resultantes se concentren alrededor de la proporción de documentos relevantes que se han descubierto para el tema t. Esto significa que las estimaciones de probabilidad disminuyen por rango y son más altas para los temas que tienen más documentos relevantes.",
        "Después de encontrar el Θ que maximiza la verosimilitud, tenemos q∗ ij = exp(θrj (i)) 1+exp(θrj (i)) .",
        "Definimos θ∞ = −∞, de modo que la probabilidad de que un documento no clasificado sea relevante es 0.",
        "Dado que q∗ ij se basa en la posición en la que se recupera un documento en lugar de en la identidad del documento en sí, las probabilidades son idénticas de un experto a otro, por ejemplo, si el experto E colocó el documento A en la posición 1, y el experto D colocó el documento B en la posición 1, tendremos que q∗ AE = q∗ BD.",
        "Por lo tanto, solo tenemos que resolver esto una vez para cada tema.",
        "El modelo anterior proporciona probabilidades independientes del tema para cada documento.",
        "Pero supongamos que un experto que informa una probabilidad del 90% solo tiene razón el 50% del tiempo.",
        "Su opinión debería ser descartada basándose en su desempeño observado.",
        "Específicamente, queremos aprender una función de calibración qij = Cj(q∗ ij) que garantice que las probabilidades predichas estén ajustadas a la capacidad de los expertos para recuperar documentos relevantes dados los juicios que se han realizado hasta este momento.",
        "El método de calibración SVM de Platts [16] ajusta una función sigmoide entre q∗ ij y las evaluaciones de relevancia para obtener qij = Cj (q∗ ij) = exp(Aj +Bjq∗ ij ) 1+exp(Aj +Bj q∗ ij ) .",
        "Dado que q∗ ij es independiente del tema, solo necesitamos aprender una función de calibración para cada experto.",
        "Una vez que tenemos la función de calibración, se aplica para ajustar las predicciones de los expertos a su rendimiento real.",
        "Las probabilidades calibradas se introducen en el modelo (2) para encontrar las probabilidades de los documentos.",
        "Figura 1: Diagrama conceptual de nuestro modelo de agregación.",
        "Los expertos E1 y E2 han clasificado los documentos A, B, C para el tema T1 y los documentos D, E, F para el tema T2.",
        "El primer paso es obtener q∗ ij.",
        "A continuación se realiza la calibración al rendimiento real para encontrar qij.",
        "Finalmente obtenemos pi = p(Xi = 1|qi1, qi2), · · · . 3.3 Resumen del modelo Nuestro modelo tiene tres componentes que difieren en los datos que toman como entrada y en lo que producen como salida.",
        "Se muestra un diagrama conceptual en la Figura 1. 1. rangos → probabilidades (por sistema por tema).",
        "Esto nos da q∗ ij, la probabilidad autoinformada por el experto j de la relevancia del documento i.",
        "Esto es no supervisado; no requiere datos etiquetados (aunque si los tenemos, los usamos para establecer parámetros previos). 2. probabilidades → probabilidades calibradas (por sistema).",
        "Esto nos da qij = Cj (q∗ ij), la probabilidad calibrada por el experto j de la relevancia del documento i.",
        "Esto es semisupervisado; tenemos juicios de relevancia en algunos rangos que usamos para imputar la probabilidad de relevancia en otros rangos. 3. probabilidades calibradas → probabilidades de documentos.",
        "Esto nos da pi = p(Xi = 1|qi), la probabilidad de relevancia del documento i dada las probabilidades de experto calibradas qij.",
        "Esto es supervisado; aprendemos coeficientes de un conjunto de documentos juzgados y los utilizamos para estimar la relevancia de los documentos no juzgados.",
        "Aunque el modelo parece bastante complejo, en realidad son solo tres aplicaciones sucesivas de regresión logística.",
        "Por lo tanto, se puede implementar en un lenguaje de programación estadística como R en unas pocas líneas de código.",
        "El uso de priors beta (conjugados) garantiza que no sean necesarios métodos computacionales costosos como MCMC [12], por lo que el modelo se entrena y aplica lo suficientemente rápido como para ser utilizado en línea.",
        "Nuestro código está disponible en http://ciir.cs.umass.edu/~carteret/. 4.",
        "EXPERIMENTOS Se están considerando tres hipótesis.",
        "El primero, y más importante, es que el uso de nuestro modelo de agregación de expertos para predecir la relevancia produce colecciones de pruebas lo suficientemente robustas como para ser reutilizables; es decir, podemos confiar en las estimaciones de confianza cuando evaluamos sistemas que no contribuyeron con ningún juicio al conjunto.",
        "Las otras dos hipótesis se relacionan con la mejora que observamos al utilizar estimaciones de relevancia mejores que las que utilizamos en nuestro trabajo anterior [8].",
        "Estos son (a) que se necesitan menos temas de relevancia, número de ejecuciones, número de documentos evaluados y número de documentos relevantes para cada uno de nuestros conjuntos de datos y (b) la precisión de las predicciones es mayor que si simplemente asumiéramos que pi = 0.5 para todos los documentos no evaluados. 4.1 Datos Obtuvimos ejecuciones completas de búsqueda ad-hoc presentadas en las TRECs 3 a 8.",
        "Cada ejecución clasifica como máximo 1000 documentos para 50 temas (49 temas para TREC 4).",
        "Además, obtuvimos todas las ejecuciones de la pista Web de TREC 13, la pista Robust2 de TREC 14 y la pista Terabyte (ad-hoc) de TREC 14.",
        "Estas son las pistas que han reemplazado la pista ad-hoc desde su final en 1999.",
        "Las estadísticas se muestran en la Tabla 1.",
        "Reservamos el conjunto TREC 4 (ad-hoc 95) para entrenamiento, los TRECs 3 y 5-8 (ad-hoc 94 y 96-99) para pruebas principales, y los conjuntos restantes para pruebas adicionales.",
        "Utilizamos los archivos qrels recopilados por NIST como verdad.",
        "El número de juicios de relevancia realizados y documentos relevantes encontrados para cada pista se enumeran en la Tabla 1.",
        "Por razones computacionales, truncamos las listas clasificadas en 100 documentos.",
        "No hay razón por la que no pudiéramos profundizar más, pero el cálculo de la varianza es O(n3) y, por lo tanto, muy consumidor de tiempo.",
        "Debido a la naturaleza del rango recíproco de AP, no perdemos mucha información al truncar en el rango 100. 4.2 Algoritmos Compararemos tres algoritmos para adquirir juicios de relevancia.",
        "La línea base es una variación del agrupamiento de TREC que llamaremos agrupamiento incremental (IP).",
        "Este algoritmo toma un número k como entrada y presenta los primeros k documentos en orden de rango (sin tener en cuenta el tema) para ser evaluados.",
        "No estima la relevancia de los documentos no evaluados; simplemente asume que cualquier documento no evaluado no es relevante.",
        "El segundo algoritmo es el presentado en Carterette et al. [8] (Algoritmo 1).",
        "Los documentos se seleccionan en función de lo interesantes que sean para determinar si existe una diferencia en la precisión media promedio.",
        "Para este enfoque pi = 0.5 para todos los i; no hay estimación de probabilidades.",
        "Llamaremos a esto MTC por colección de pruebas mínimas.",
        "El tercer algoritmo aumenta MTC con estimaciones actualizadas de las probabilidades de relevancia.",
        "Llamaremos a esto RTC por colección de pruebas robusta.",
        "Es idéntico al Algoritmo 1, excepto que cada décima iteración estimamos pi para todos los documentos no evaluados i utilizando el modelo de agregación de expertos de la Sección 3.",
        "RTC tiene parámetros de suavizado (distribución previa) que deben ser configurados.",
        "Entrenamos utilizando el conjunto ad-hoc 95.",
        "Limitamos 2 robustos aquí significa recuperación robusta; esto es diferente de nuestro objetivo de evaluación robusta.",
        "Algoritmo 1 (MTC) Dadas dos listas clasificadas y un nivel de confianza α, predecir el signo de ΔMAP. 1: pi ← 0.5 para todos los documentos i 2: mientras P(ΔMAP < 0) < α hacer 3: calcular el peso wi para todos los documentos i no clasificados (ver Carterette et al. [8] para más detalles) 4: j ← argmaxiwi 5: xj ← 1 si el documento j es relevante, 0 en caso contrario 6: pj ← xj 7: fin mientras la búsqueda de priors uniformes con una varianza relativamente alta.",
        "Para la agregación de expertos, los parámetros previos son α = β = 1. 4.3 Diseño Experimental Primero, queremos saber si podemos aumentar un conjunto de juicios de relevancia con un conjunto de probabilidades de relevancia para reutilizar los juicios y evaluar un nuevo conjunto de sistemas.",
        "Para cada prueba experimental: 1.",
        "Selecciona un subconjunto aleatorio de k ejecuciones.",
        "De esos k, elige un c inicial < k para evaluar. 3.",
        "Ejecutar RTC al 95% de confianza en el c inicial de aproximadamente 4.",
        "Utilizando el modelo de la Sección 3, estime las probabilidades de relevancia para todos los documentos recuperados por todos los k experimentos.",
        "Calcular EMAP para todas las ejecuciones k, y P(ΔMAP < 0) para todos los pares de ejecuciones.",
        "Hacemos lo mismo para MTC, pero omitimos el paso 4.",
        "Ten en cuenta que después de evaluar los primeros c sistemas, no realizamos juicios de relevancia adicionales.",
        "Para poner a prueba nuestro método, seleccionamos c = 2: construiremos un conjunto de juicios a partir de evaluar solo dos sistemas iniciales.",
        "Luego generalizaremos a un conjunto de k = 10 (de los cuales esos dos son un subconjunto).",
        "A medida que realizamos más pruebas, obtenemos los datos que necesitamos para probar las tres hipótesis. 4.4 Evaluación Experimental Recuerde que un conjunto de juicios es robusto si la precisión de las predicciones que realiza es al menos su confianza estimada.",
        "Una forma de evaluar la robustez es agrupar pares por su confianza, luego calcular la precisión sobre todos los pares en cada grupo.",
        "Nos gustaría que la precisión no fuera menor que la puntuación de confianza más baja en el contenedor, pero preferiblemente más alta.",
        "Dado que las estadísticas resumen son útiles, ideamos la siguiente métrica.",
        "Supongamos que somos una casa de apuestas tomando apuestas sobre si ΔMAP < 0.",
        "Utilizamos RTC o MTC para establecer las probabilidades O = P (ΔMAP <0) 1−P (ΔMAP <0) .",
        "Supongamos que un apostador apuesta $1 a que ΔMAP ≥ 0.",
        "Si resulta que ΔMAP < 0, ganamos el dólar.",
        "De lo contrario, pagamos O.",
        "Si nuestras estimaciones de confianza son perfectamente precisas, empatamos.",
        "Si la confianza es mayor que la precisión, perdemos dinero; ganamos si la precisión es mayor que la confianza.",
        "Contra intuitivamente, el resultado más deseable es empatar: si perdemos dinero, no podemos confiar en las estimaciones de confianza, pero si ganamos dinero, hemos subestimado la confianza o evaluado más documentos de los necesarios.",
        "Sin embargo, el costo de no poder confiar en las estimaciones de confianza es mayor que el costo de realizar juicios de relevancia adicionales, por lo que consideraremos los resultados positivos como buenos.",
        "La cantidad que ganamos en cada comparación par a par i es: Wi = yi − (1 − yi) Pi 1 − Pi = yi − Pi 1 − Pi yi = 1 si ΔMAP < 0 y 0 en caso contrario, y Pi = P(ΔMAP < 0).",
        "La estadística resumen es W, la media de Wi.",
        "Ten en cuenta que a medida que Pi aumenta, perdemos más por estar equivocados.",
        "Esto es como debería ser: la penalización debería ser grande por fallar en las predicciones de alta probabilidad.",
        "Sin embargo, dado que nuestras pérdidas crecen sin límite a medida que las predicciones se acercan a la certeza, limitamos −Wi a 100.",
        "Para nuestra hipótesis de que RTC requiere menos juicios que MTC, estamos interesados en el número de juicios necesarios para alcanzar el 95% de confianza en el primer par de sistemas.",
        "La mediana es más interesante que la media: la mayoría de los pares requieren unos cientos de juicios, pero algunos pares requieren varios miles.",
        "La distribución es, por lo tanto, altamente sesgada y la media se ve fuertemente afectada por esos valores atípicos.",
        "Finalmente, para nuestra hipótesis de que RTC es más preciso que MTC, analizaremos la correlación de Kendalls τ entre una clasificación de k sistemas por un pequeño conjunto de juicios y la clasificación real utilizando el conjunto completo de juicios.",
        "El tau de Kendall, una estadística no paramétrica basada en intercambios de pares entre dos listas, es una evaluación estándar para este tipo de estudio.",
        "Va desde −1 (perfectamente anticorrelacionado) hasta 1 (clasificaciones idénticas), con 0 significando que la mitad de los pares están intercambiados.",
        "Como mencionamos en la introducción, sin embargo, una medida de precisión como la correlación de rangos no es una buena evaluación de la reutilización.",
        "Lo incluimos por completitud. 4.4.1 Prueba de hipótesis Realizar múltiples pruebas permite el uso de pruebas de hipótesis estadísticas para comparar algoritmos.",
        "El uso de los mismos conjuntos de sistemas permite el uso de pruebas emparejadas.",
        "Como mencionamos anteriormente, estamos más interesados en la mediana del número de juicios que en la media.",
        "Una prueba para la diferencia en la mediana es la prueba de rango con signo de Wilcoxon.",
        "También podemos usar una prueba t pareada para probar una diferencia en la media.",
        "Para la correlación de rangos, podemos usar una prueba t pareada para probar si hay una diferencia en τ. 5.",
        "RESULTADOS Y ANÁLISIS La comparación entre MTC y RTC se muestra en la Tabla 2.",
        "Con MTC y probabilidades uniformes de relevancia, los resultados están lejos de ser robustos.",
        "No podemos reutilizar las evaluaciones de relevancia con mucha confianza.",
        "Pero con RTC, los resultados son muy robustos.",
        "Existe una ligera disminución en la precisión cuando la confianza supera el 0.95; sin embargo, las predicciones de confianza son confiables.",
        "La media de Wi muestra que RTC está mucho más cerca de 0 que MTC.",
        "La distribución de las puntuaciones de confianza muestra que al menos el 80% de confianza se logra más del 35% del tiempo, lo que indica que ninguno de los algoritmos está siendo demasiado conservador en sus estimaciones de confianza.",
        "Las estimaciones de confianza son bastante bajas en general; esto se debe a que hemos construido una colección de pruebas a partir de solo dos sistemas iniciales.",
        "Recuerde de la Sección 1 que no podemos requerir (o incluso esperar) un nivel mínimo de confianza al generalizar a nuevos sistemas.",
        "Los resultados más detallados de ambos algoritmos se muestran en la Figura 2.",
        "La línea sólida es el resultado ideal que daría W = 0.",
        "RTC está en o por encima de esta línea en todos los puntos hasta que la confianza alcance aproximadamente 0.97.",
        "Después de eso, hay una ligera disminución en la precisión que discutimos a continuación.",
        "Ten en cuenta que tanto la confianza MTC RTC % en la precisión del intervalo % en la precisión del intervalo 0.5 − 0.6 33.7% 61.7% 28.6% 61.9% 0.6 − 0.7 18.1% 73.1% 20.1% 76.3% 0.7 − 0.8 10.4% 70.1% 15.5% 78.0% 0.8 − 0.9 9.4% 69.0% 12.1% 84.9% 0.9 − 0.95 7.3% 78.0% 6.6% 93.1% 0.95 − 0.99 17.9% 70.4% 12.4% 93.4% 1.0 3.3% 68.3% 4.7% 98.9% W −5.34 −0.39 mediana juzgada 251 235 media τ 0.393 0.555 Tabla 2: Confianza en que P(ΔMAP < 0) y precisión de la predicción al generalizar un conjunto de juicios de relevancia adquiridos utilizando MTC y RTC.",
        "Cada contenedor contiene más de 1,000 pruebas de los conjuntos ad hoc 3, 5-8.",
        "RTC es mucho más robusto que MTC.",
        "W está definido en la Sección 4.4; cuanto más cerca de 0, mejor.",
        "La mediana juzgada es el número de juicios necesarios para alcanzar un 95% de confianza en los dos primeros sistemas.",
        "La media τ es la correlación de rango promedio para los 10 sistemas. 0.5 0.6 0.7 0.8 0.9 1 0.5 0.6 0.7 0.8 0.9 1 precisión confianza punto de equilibrio RTC MTC Figura 2: Confianza vs. precisión de MTC y RTC.",
        "La línea sólida es el resultado perfecto que daría W = 0; el rendimiento debería estar en o por encima de esta línea.",
        "Cada punto representa al menos 500 comparaciones en pares. Los algoritmos están claramente por encima de la línea hasta aproximadamente una confianza de 0.7.",
        "Esto se debe a que el rendimiento base en estos conjuntos de datos es alto; es bastante fácil lograr un 75% de precisión haciendo muy poco trabajo [7].",
        "Número de juicios: La mediana del número de juicios requeridos por MTC para alcanzar un 95% de confianza en los dos primeros sistemas es de 251, un promedio de 5 por tema.",
        "La mediana requerida por RTC es 235, aproximadamente 4.7 por tema.",
        "Aunque los números son cercanos, la mediana de RTCs es significativamente más baja según una prueba de Wilcoxon pareada (p < 0.0001).",
        "Para comparación, una piscina de profundidad 100 resultaría en un mínimo de 5,000 juicios para cada par.",
        "La diferencia en las medias es mucho mayor.",
        "MTC requirió una media de 823 juicios, 16 por tema, mientras que RTC requirió una media de 502, 10 por tema. (Recordemos que las medias están fuertemente sesgadas por algunas parejas que requieren miles de juicios).",
        "Esta diferencia es significativa mediante una prueba t de muestras pareadas (p < 0.0001).",
        "El diez por ciento de los conjuntos resultaron en 100 o menos juicios (menos de dos por tema).",
        "El rendimiento en estos es muy alto: W = 0.41 y un 99.7% de precisión cuando la confianza es de al menos 0.9.",
        "Esto demuestra que incluso las colecciones pequeñas pueden ser reutilizables.",
        "Para el 50% de los conjuntos con más de 235 juicios, la precisión es del 93% cuando la confianza es de al menos 0.9.",
        "Correlación de rangos: MTC y RTC clasifican los 10 sistemas por EMAP (Eq. (1)) calculado utilizando sus respectivas estimaciones de probabilidad.",
        "La correlación de rango τ promedio entre el MAP verdadero y el EMAP es de 0.393 para MTC y 0.555 para RTC.",
        "Esta diferencia es significativa mediante una prueba t de muestras pareadas (p < 0.0001).",
        "Ten en cuenta que no esperamos que las correlaciones τ sean altas, ya que estamos clasificando los sistemas con tan pocos juicios de relevancia.",
        "Es más importante que estimemos correctamente la confianza en cada comparación de pares.",
        "Ejecutamos IP para el mismo número de juicios que MTC tomó para cada par, luego clasificamos los sistemas por MAP utilizando solo esos juicios (todos los documentos no juzgados se consideraron no relevantes).",
        "Calculamos la correlación τ con la clasificación real.",
        "La correlación media τ es de 0.398, lo cual no es significativamente diferente de MTC, pero es significativamente menor que RTC.",
        "El uso de estimaciones uniformes de probabilidad es indistinguible de la línea base, mientras que la estimación de relevancia mediante la agregación de expertos mejora el rendimiento en gran medida: casi un 40% más que tanto MTC como IP.",
        "Sobreajuste: Es posible sobreajustar: si provienen demasiados juicios de los dos primeros sistemas, la varianza en ΔMAP se reduce y las estimaciones de confianza se vuelven poco fiables.",
        "Vimos esto en la Tabla 2 y la Figura 2, donde RTC muestra una disminución en la precisión cuando la confianza está alrededor del 97%.",
        "De hecho, el número de juicios realizados antes de una predicción incorrecta es más de un 50% mayor que el número realizado antes de una predicción correcta.",
        "El sobreajuste es difícil de cuantificar exactamente, porque hacer más juicios de relevancia no siempre lo causa: a niveles de confianza más altos, se hacen más juicios de relevancia, y como muestra la Tabla 2, la precisión es mayor en esas mayores confianzas.",
        "Obviamente, tener más juicios de relevancia debería aumentar tanto la confianza como la precisión; la diferencia parece estar cuando un sistema tiene muchos más juicios que el otro.",
        "Comparaciones por pares: Nuestras comparaciones por pares se dividen en uno de tres grupos: 1. las dos ejecuciones originales de las que se obtienen los juicios de relevancia; 2. una de las ejecuciones originales vs. una de las nuevas ejecuciones; 3. dos nuevas ejecuciones.",
        "La Tabla 3 muestra los resultados de confianza frente a precisión para cada uno de estos tres grupos.",
        "Curiosamente, el rendimiento es peor al comparar una de las carreras originales con una de las carreras adicionales.",
        "Esto se debe muy probablemente a una gran diferencia en el número de juicios que afectan la varianza de ΔMAP.",
        "Sin embargo, el rendimiento es bastante bueno en los tres subconjuntos.",
        "Peor caso: El caso intuitivamente más probable de producir un error es cuando los dos sistemas que se comparan han recuperado muy pocos documentos en común.",
        "Si queremos que los juicios sean reutilizables, deberíamos ser capaces de generalizar incluso a ejecuciones que son muy diferentes de las utilizadas para adquirir los juicios de relevancia.",
        "Una medida simple de similitud de dos ejecuciones es el porcentaje promedio de documentos que recuperaron en común para cada tema [2].",
        "Calculamos esto para todos los pares, luego observamos el rendimiento en pares con baja similitud.",
        "Los resultados se muestran en la confianza de precisión de dos ejecuciones originales, una ejecución original, ninguna ejecución original. 0.5 − 0.6 - 48.1% 62.8% 0.6 − 0.7 - 57.1% 79.2% 0.7 − 0.8 - 67.9% 81.7% 0.8 − 0.9 - 82.2% 86.3% 0.9 − 0.95 95.9% 93.7% 92.6% 0.95 − 0.99 96.2% 92.5% 93.1% 1.0 100% 98.0% 99.1% W −1.11 −0.87 −0.27 Tabla 3: Confianza vs. precisión de RTC al comparar las dos ejecuciones originales, una ejecución original y una nueva ejecución, y dos nuevas ejecuciones.",
        "RTC es robusto en los tres casos. precisión cuando la confianza es similar 0 − 0.1 0.1 − 0.2 0.2 − 0.3 0.5 − 0.6 68.4% 63.1% 61.4% 0.6 − 0.7 84.2% 78.6% 76.6% 0.7 − 0.8 82.0% 79.8% 78.9% 0.8 − 0.9 93.6% 83.3% 82.1% 0.9 − 0.95 99.3% 92.7% 92.4% 0.95 − 0.99 98.7% 93.4% 93.3% 1.0 99.9% 97.9% 98.1% W 0.44 −0.45 −0.49 Tabla 4: Confianza vs. precisión de RTC cuando un par de sistemas recuperó el 0-30% de documentos en común (dividido en 0%-10%, 10%-20% y 20%-30%).",
        "RTC es robusto en los tres casos.",
        "Tabla 4.",
        "El rendimiento es de hecho muy robusto incluso cuando la similitud es baja.",
        "Cuando las dos corridas comparten muy pocos documentos en común, W es en realidad positivo.",
        "Tanto MTC como IP tuvieron un rendimiento bastante deficiente en estos casos.",
        "Cuando la similitud estaba entre 0 y 10%, tanto MTC como IP predijeron correctamente ΔMAP solo el 60% del tiempo, en comparación con una tasa de éxito del 87.6% para RTC.",
        "Por conjunto de datos: Todos los resultados anteriores han sido solo en las colecciones ad-hoc.",
        "Realizamos los mismos experimentos en nuestros conjuntos de datos adicionales y desglosamos los resultados por conjunto de datos para ver cómo varía el rendimiento.",
        "Los resultados en la Tabla 5 muestran todo acerca de cada conjunto, incluyendo la precisión agrupada, W, la media de τ, y el número medio de juicios para alcanzar el 95% de confianza en los dos primeros sistemas.",
        "Los resultados son altamente consistentes de una colección a otra, lo que sugiere que nuestro método no está sobreajustado a ningún conjunto de datos en particular.",
        "CONCLUSIONES Y TRABAJO FUTURO En este trabajo hemos ofrecido la primera definición formal de la idea común de reutilización de una colección de pruebas y presentado un modelo que es capaz de lograr la reutilización con conjuntos muy pequeños de juicios de relevancia.",
        "La Tabla 2 y la Figura 2 juntas muestran cuán sesgado puede ser un pequeño conjunto de juicios: MTC está sobreestimando drásticamente la confianza y es mucho menos preciso que RTC, que puede eliminar el sesgo para proporcionar una evaluación sólida.",
        "Las estimaciones de confianza de RTC, además de ser precisas, proporcionan una guía para obtener juicios adicionales: enfóquese en juzgar documentos de las comparaciones de menor confianza.",
        "A largo plazo, observamos pequeños conjuntos de juicios de relevancia, precisión, confianza ad-hoc 94, ad-hoc 96, ad-hoc 97, ad-hoc 98, ad-hoc 99, web 04, robust 05, terabyte 05, 0.5 − 0.6 64.1%, 61.8%, 62.2%, 62.0%, 59.4%, 64.3%, 61.5%, 61.6%, 0.6 − 0.7 76.1%, 77.8%, 74.5%, 78.2%, 74.3%, 78.1%, 75.9%, 75.9%, 0.7 − 0.8 75.2%, 78.9%, 77.6%, 80.0%, 78.6%, 82.6%, 77.5%, 80.4%, 0.8 − 0.9 83.2%, 85.5%, 84.6%, 84.9%, 86.8%, 84.5%, 86.7%, 87.7%, 0.9 − 0.95 93.0%, 93.6%, 92.8%, 93.7%, 92.6%, 94.2%, 93.9%, 94.2%, 0.95 − 0.99 93.1%, 94.3%, 93.1%, 93.7%, 92.8%, 95.0%, 93.9%, 91.6%, 1.0 99.2%, 96.8%, 98.7%, 99.5%, 99.6%, 100%, 99.2%, 98.3%, W -0.34, -0.34, -0.48, -0.35, -0.44, -0.07, -0.41, -0.67, mediana juzgada 235, 276, 243, 213, 179, 448, 310, 320, media τ 0.538, 0.573, 0.556, 0.579, 0.532, 0.596, 0.565, 0.574. Tabla 5: Precisión, W, media τ y mediana del número de juicios para los 8 conjuntos de pruebas.",
        "Los resultados son altamente consistentes en todos los conjuntos de datos.",
        "A medida que pasa el tiempo, el número de juicios crece hasta que hay un 100% de confianza en cada evaluación, y hay una colección completa de pruebas para la tarea.",
        "Vemos un uso adicional para este método en escenarios como la recuperación web en los que el corpus cambia con frecuencia.",
        "Podría aplicarse a la evaluación en una colección de pruebas dinámica según lo definido por Soboroff [18].",
        "El modelo que presentamos en la Sección 3 no es en absoluto la única posibilidad para crear una colección de pruebas robusta.",
        "Un modelo de agregación de expertos más simple podría funcionar igual de bien o mejor (aunque todos nuestros esfuerzos por simplificar fallaron).",
        "Además de la agregación de expertos, podríamos estimar probabilidades al observar similitudes entre documentos.",
        "Esta es un área obvia para futuras exploraciones.",
        "Además, será valioso investigar el problema del sobreajuste: las circunstancias en las que ocurre y qué se puede hacer para prevenirlo.",
        "Mientras tanto, limitar las estimaciones de confianza al 95% es un truco que resuelve el problema.",
        "Tenemos muchos más resultados experimentales que desafortunadamente no tuvimos espacio para incluir, pero que refuerzan la idea de que RTC es altamente robusto: con solo unos pocos juicios por tema, podemos evaluar con precisión la confianza en cualquier comparación de sistemas de forma pareja.",
        "Agradecimientos Este trabajo fue apoyado en parte por el Centro de Recuperación de Información Inteligente y en parte por la Agencia de Proyectos de Investigación Avanzada de Defensa (DARPA) bajo el número de contrato HR0011-06-C-0023.",
        "Cualquier opinión, hallazgo, conclusión o recomendación expresada en este material es responsabilidad del autor y no necesariamente refleja la del patrocinador.",
        "REFERENCIAS [1] J. Aslam y M. Montague.",
        "Modelos para Metabúsqueda.",
        "En Actas de SIGIR, páginas 275-285, 2001. [2] J. Aslam y R. Savell.",
        "Sobre la efectividad de evaluar sistemas de recuperación en ausencia de juicios de relevancia.",
        "En Actas de SIGIR, páginas 361-362, 2003. [3] J.",
        "A. Aslam, V. Pavlu y R. Savell.",
        "Un modelo unificado para metabúsqueda, agrupación y evaluación de sistemas.",
        "En Actas de CIKM, páginas 484-491, 2003. [4] J.",
        "A. Aslam, V. Pavlu y E. Yilmaz.",
        "Un método estadístico para la evaluación de sistemas utilizando juicios incompletos.",
        "En Actas de SIGIR, páginas 541-548, 2006. [5] A. L. Berger, S. D. Pietra y V. J. D. Pietra.",
        "Un enfoque de entropía máxima para el procesamiento del lenguaje natural.",
        "Lingüística Computacional, 22(1):39-71, 1996. [6] D. J. Blower.",
        "Una derivación sencilla de la regresión logística desde la perspectiva bayesiana y de máxima entropía.",
        "En Actas del 23º Taller Internacional sobre Inferencia Bayesiana y Métodos de Entropía Máxima en Ciencia e Ingeniería, páginas 30-43, 2004. [7] B. Carterette y J. Allan.",
        "Metodología de investigación en estudios del esfuerzo del evaluador para la evaluación de recuperación.",
        "En Actas de RIAO, 2007. [8] B. Carterette, J. Allan y R. K. Sitaraman.",
        "Colecciones de prueba mínimas para evaluación de recuperación.",
        "En Actas de SIGIR, páginas 268-275, 2006. [9] B. Carterette y D. I. Petkova.",
        "Aprendiendo un ranking a partir de preferencias por pares.",
        "En Actas de SIGIR, 2006. [10] R. T. Clemen y R. L. Winkler.",
        "Unanimidad y compromiso entre pronosticadores de probabilidad.",
        "Ciencia de la Gestión, 36(7):767-779, julio de 1990. [11] G. V. Cormack, C. R. Palmer y C. L. Clarke.",
        "Construcción eficiente de grandes colecciones de pruebas.",
        "En Actas de SIGIR, páginas 282-289, 1998. [12] A. Gelman, J.",
        "B. Carlin, H. S. Stern y D. B. Rubin.",
        "Análisis de datos bayesianos.",
        "Chapman & Hall/CRC, 2004. [13] E. T. Jaynes.\nChapman & Hall/CRC, 2004. [13] E. T. Jaynes.",
        "Teoría de la probabilidad: La lógica de la ciencia.",
        "Cambridge University Press, 2003. [14] R. Manmatha y H. Sever.",
        "Un enfoque formal para la normalización de puntuaciones en metabúsqueda.",
        "En Actas de HLT, páginas 88-93, 2002. [15] I. J. Myung, S. Ramamoorti y J. Andrew D. Baily.",
        "Agregación de predicciones de expertos mediante entropía máxima.",
        "Ciencia de la Gestión, 42(10):1420-1436, octubre de 1996. [16] J. Platt.",
        "Salidas probabilísticas para máquinas de vectores de soporte y comparación con métodos de verosimilitud regularizados. páginas 61-74, 2000. [17] M. Sanderson y H. Joho.",
        "Formando colecciones de pruebas sin agrupamiento de sistemas.",
        "En Actas de la 27ª Conferencia Internacional Anual de ACM SIGIR sobre Investigación y Desarrollo en Recuperación de Información, páginas 33-40, 2004. [18] I. Soboroff.",
        "Colecciones de pruebas dinámicas: midiendo la efectividad de la búsqueda en la web en vivo.",
        "En Actas de SIGIR, páginas 276-283, 2006. [19] K. Sparck Jones y C. J. van Rijsbergen.",
        "Colecciones de pruebas de recuperación de información.",
        "Revista de Documentación, 32(1):59-75, 1976. [20] E. M. Voorhees y D. K. Harman, editores.",
        "TREC: Experimento y Evaluación en Recuperación de Información.",
        "MIT Press, 2005. [21] J. Zobel. \n\nMIT Press, 2005. [21] J. Zobel.",
        "¿Qué tan confiables son los resultados de los experimentos de recuperación de información a gran escala?",
        "En Actas de SIGIR, páginas 307-314, 1998."
    ],
    "error_count": 5,
    "keys": {
        "information retrieval": {
            "translated_key": "recuperación de información",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Robust Test Collections for Retrieval Evaluation Ben Carterette Center for Intelligent <br>information retrieval</br> Computer Science Department University of Massachusetts Amherst Amherst, MA 01003 carteret@cs.umass.edu ABSTRACT Low-cost methods for acquiring relevance judgments can be a boon to researchers who need to evaluate new retrieval tasks or topics but do not have the resources to make thousands of judgments.",
                "While these judgments are very useful for a one-time evaluation, it is not clear that they can be trusted when re-used to evaluate new systems.",
                "In this work, we formally define what it means for judgments to be reusable: the confidence in an evaluation of new systems can be accurately assessed from an existing set of relevance judgments.",
                "We then present a method for augmenting a set of relevance judgments with relevance estimates that require no additional assessor effort.",
                "Using this method practically guarantees reusability: with as few as five judgments per topic taken from only two systems, we can reliably evaluate a larger set of ten systems.",
                "Even the smallest sets of judgments can be useful for evaluation of new systems.",
                "Categories and Subject Descriptors: H.3 Information Storage and Retrieval; H.3.4 Systems and Software: Performance Evaluation General Terms: Experimentation, Measurement, Reliability 1.",
                "INTRODUCTION Consider an <br>information retrieval</br> researcher who has invented a new retrieval task.",
                "She has built a system to perform the task and wants to evaluate it.",
                "Since the task is new, it is unlikely that there are any extant relevance judgments.",
                "She does not have the time or resources to judge every document, or even every retrieved document.",
                "She can only judge the documents that seem to be the most informative and stop when she has a reasonable degree of confidence in her conclusions.",
                "But what happens when she develops a new system and needs to evaluate it?",
                "Or another research group decides to implement a system to perform the task?",
                "Can they reliably reuse the original judgments?",
                "Can they evaluate without more relevance judgments?",
                "Evaluation is an important aspect of <br>information retrieval</br> research, but it is only a semi-solved problem: for most retrieval tasks, it is impossible to judge the relevance of every document; there are simply too many of them.",
                "The solution used by NIST at TREC (Text REtrieval Conference) is the pooling method [19, 20]: all competing systems contribute N documents to a pool, and every document in that pool is judged.",
                "This method creates large sets of judgments that are reusable for training or evaluating new systems that did not contribute to the pool [21].",
                "This solution is not adequate for our hypothetical researcher.",
                "The pooling method gives thousands of relevance judgments, but it requires many hours of (paid) annotator time.",
                "As a result, there have been a slew of recent papers on reducing annotator effort in producing test collections: Cormack et al. [11], Zobel [21], Sanderson and Joho [17], Carterette et al. [8], and Aslam et al. [4], among others.",
                "As we will see, the judgments these methods produce can significantly bias the evaluation of a new set of systems.",
                "Returning to our hypothetical resesarcher, can she reuse her relevance judgments?",
                "First we must formally define what it means to be reusable.",
                "In previous work, reusability has been tested by simply assessing the accuracy of a set of relevance judgments at evaluating unseen systems.",
                "While we can say that it was right 75% of the time, or that it had a rank correlation of 0.8, these numbers do not have any predictive power: they do not tell us which systems are likely to be wrong or how confident we should be in any one.",
                "We need a more careful definition of reusability.",
                "Specifically, the question of reusability is not how accurately we can evaluate new systems.",
                "A malicious adversary can always produce a new ranked list that has not retrieved any of the judged documents.",
                "The real question is how much confidence we have in our evaluations, and, more importantly, whether we can trust our estimates of confidence.",
                "Even if confidence is not high, as long as we can trust it, we can identify which systems need more judgments in order to increase confidence.",
                "Any set of judgments, no matter how small, becomes reusable to some degree.",
                "Small, reusable test collections could have a huge impact on <br>information retrieval</br> research.",
                "Research groups would be able to share the relevance judgments they have done in-house for pilot studies, new tasks, or new topics.",
                "The amount of data available to researchers would grow exponentially over time. 2.",
                "ROBUST EVALUATION Above we gave an intuitive definition of reusability: a collection is reusable if we can trust our estimates of confidence in an evaluation.",
                "By that we mean that if we have made some relevance judgments and have, for example, 75% confidence that system A is better than system B, we would like there to be no more than 25% chance that our assessment of the relative quality of the systems will change as we continue to judge documents.",
                "Our evaluation should be robust to missing judgments.",
                "In our previous work, we defined confidence as the probability that the difference in an evaluation measure calculated for two systems is less than zero [8].",
                "This notion of confidence is defined in the context of a particular evaluation task that we call comparative evaluation: determining the sign of the difference in an evaluation measure.",
                "Other evaluation tasks could be defined; estimating the magnitude of the difference or the values of the measures themselves are examples that entail different notions of confidence.",
                "We therefore see confidence as a probability estimate.",
                "One of the questions we must ask about a probability estimate is what it means.",
                "What does it mean to have 75% confidence that system A is better than system B?",
                "As described above, we want it to mean that if we continue to judge documents, there will only be a 25% chance that our assessment will change.",
                "If this is what it means, we can trust the confidence estimates.",
                "But do we know it has that meaning?",
                "Our calculation of confidence rested on an assumption about the probability of relevance of unjudged documents, specifically that each unjudged document was equally likely to be relevant or nonrelevant.",
                "This assumption is almost certainly not realistic in most IR applications.",
                "As it turns out, it is this assumption that determines whether the confidence estimates can eb trusted.",
                "Before elaborating on this, we formally define confidence. 2.1 Estimating Confidence Average precision (AP) is a standard evaluation metric that captures both the ability of a system to rank relevant documents highly (precision) as well as its ability to retrieve relevant documents (recall).",
                "It is typically written as the mean precision at the ranks of relevant documents: AP = 1 |R| i∈R prec@r(i) where R is the set of relevant documents and r(i) is the rank of document i.",
                "Let Xi be a random variable indicating the relevance of document i.",
                "If documents are ordered by rank, we can express precision as prec@i = 1/i i j=1 Xj .",
                "Average precision then becomes the quadratic equation AP = 1 Xi n i=1 Xi/i i j=1 Xj = 1 Xi n i=1 j≥i aijXiXj where aij = 1/ max{r(i), r(j)}.",
                "Using aij instead of 1/i allows us to number the documents arbitrarily.",
                "To see why this is true, consider a toy example: a list of 3 documents with relevant documents B, C at ranks 1 and 3 and nonrelevant document A at rank 2.",
                "Average precision will be 1 2 (1 1 x2 B+ 1 2 xBxA+ 1 3 xBxC + 1 2 x2 A+ 1 3 xAxC + 1 3 x2 C) = 1 2 1 + 2 3 because xA = 0, xB = 1, xC = 1.",
                "Though the ordering B, A, C is different from the labeling A, B, C, it does not affect the computation.",
                "We can now see average precision itself is a random variable with a distribution over all possible assignments of relevance to all documents.",
                "This random variable has an expectation, a variance, confidence intervals, and a certain probability of being less than or equal to a given value.",
                "All of these are dependent on the probability that document i is relevant: pi = p(Xi = 1).",
                "Suppose in our previous example we do not know the relevance judgments, but we believe pA = 0.4, pB = 0.8, pC = 0.7.",
                "We can then compute e.g.",
                "P(AP = 0) = 0.2 · 0.6 · 0.3 = 0.036, or P(AP = 1 2 ) = 0.2 · 0.4 · 0.7 = 0.056.",
                "Summing over all possibilities, we can compute expectation and variance: E[AP] ≈ 1 pi aiipi + j>i aij pipj V ar[AP] ≈ 1 ( pi)2 n i a2 iipiqi + j>i a2 ijpipj(1 − pipj) + i=j 2aiiaijpipj(1 − pi) + k>j=i 2aijaikpipjpk(1 − pi) AP asymptotically converges to a normal distribution with expectation and variance as defined above.1 For our comparative evaluation task we are interested in the sign of the difference in two average precisions: ΔAP = AP1 − AP2.",
                "As we showed in our previous work, ΔAP has a closed form when documents are ordered arbitrarily: ΔAP = 1 Xi n i=1 j≥i cij XiXj cij = aij − bij where bij is defined analogously to aij for the second ranking.",
                "Since AP is normal, ΔAP is normal as well, meaning we can use the normal cumulative density function to determine the confidence that a difference in AP is less than zero.",
                "Since topics are independent, we can easily extend this to mean average precision (MAP).",
                "MAP is also normally distributed; its expectation and variance are: EMAP = 1 T t∈T E[APt] (1) VMAP = 1 T2 t∈T V ar[APt] ΔMAP = MAP1 − MAP2 Confidence can then be estimated by calculating the expectation and variance and using the normal density function to find P(ΔMAP < 0). 2.2 Confidence and Robustness Having defined confidence, we turn back to the issue of trust in confidence estimates, and show how it ties into the robustness of the collection to missing judgments. 1 These are actually approximations to the true expectation and variance, but the error is a negligible O(n2−n ).",
                "Let Z be the set of all pairs of ranked results for a common set of topics.",
                "Suppose we have a set of m relevance judgments xm = {x1, x2, ..., xm} (using small x rather than capital X to distinguish between judged and unjudged documents); these are the judgments against which we compute confidence.",
                "Let Zα be the subset of pairs in Z for which we predict that ΔMAP = −1 with confidence α given the judgments xm .",
                "For the confidence estimates to be accurate, we need at least α · |Zα| of these pairs to actually have ΔMAP = −1 after we have judged every document.",
                "If they do, we can trust the confidence estimates; our evaluation will be robust to missing judgments.",
                "If our confidence estimates are based on unrealistic assumptions, we cannot expect them to be accurate.",
                "The assumptions they are based on are the probabilities of relevance pi.",
                "We need these to be realistic.",
                "We argue that the best possible distribution of relevance p(Xi) is the one that explains all of the data (all of the observations made about the retrieval systems) while at the same time making no unwarranted assumptions.",
                "This is known as the principle of maximum entropy [13].",
                "The entropy of a random variable X with distribution p(X) is defined as H(p) = − i p(X = i) log p(X = i).",
                "This has found a wide array of uses in computer science and <br>information retrieval</br>.",
                "The maximum entropy distribution is the one that maximizes H. This distribution is unique and has an exponential form.",
                "The following theorem shows the utility of a maximum entropy distribution for relevance when estimating confidence.",
                "Theorem 1.",
                "If p(Xn |I, xm ) = argmaxpH(p), confidence estimates will be accurate. where xm is the set of relevance judgments defined above, Xn is the full set of documents that we wish to estimate the relevance of, and I is some information about the documents (unspecified as of now).",
                "We forgo the proof for the time being, but it is quite simple.",
                "This says that the better the estimates of relevance, the more accurate the evaluation.",
                "The task of creating a reusable test collection thus becomes the task of estimating the relevance of unjudged documents.",
                "The theorem and its proof say nothing whatsoever about the evaluation metric.",
                "The probability estimates are entirely indepedent of the measure we are interested in.",
                "This means the same probability estimates can tell us about average precision as well as precision, recall, bpref, etc.",
                "Furthermore, we could assume that the relevance of documents i and j is independent and achieve the same result, which we state as a corollary: Corollary 1.",
                "If p(Xi|I, xm ) = argmaxpH(p), confidence estimates will be accurate.",
                "The task therefore becomes the imputation of the missing values of relevance.",
                "The theorem implies that the closer we get to the maximum entropy distribution of relevance, the closer we get to robustness. 3.",
                "PREDICTING RELEVANCE In our statement of Theorem 1, we left the nature of the information I unspecified.",
                "One of the advantages of our confidence estimates is that they admit information from a wide variety of sources; essentially anything that can be modeled can be used as information for predicting relevance.",
                "A natural source of information is the retrieval systems themselves: how they ranked the judged documents, how often they failed to rank relevant documents, how they perform across topics, and so on.",
                "If we treat each system as an <br>information retrieval</br> expert providing an opinion about the relevance of each document, the problem becomes one of expert opinion aggregation.",
                "This is similar to the metasearch or data fusion problem in which the task is to take k input systems and merge them into a single ranking.",
                "Aslam et al. [3] previously identified a connection between evaluation and metasearch.",
                "Our problem has two key differences: 1.",
                "We explicitly need probabilities of relevance that we can plug into Eq. 1; metasearch algorithms have no such requirement. 2.",
                "We are accumulating relevance judgments as we proceed with the evaluation and are able to re-estimate relevance given each new judgment.",
                "In light of (1) above, we introduce a probabilistic model for expert combination. 3.1 A Model for Expert Opinion Aggregation Suppose that each expert j provides a probability of relevance qij = pj(Xi = 1).",
                "The information about the relevance of document i will then be the set of k expert opinions I = qi = (qi1, qi2, · · · , qik).",
                "The probability distribution we wish to find is the one that maximizes the entropy of pi = p(Xi = 1|qi).",
                "As it turns out, finding the maximum entropy model is equivalent to finding the parameters that maximize the likelihood [5].",
                "Blower [6] explicitly shows that finding the maximum entropy model for a binary variable is equivalent to solving a logistic regression.",
                "Then pi = p(Xi = 1|qi) = exp k j=1 λjqij 1 + exp k j=1 λj qij (2) where λ1, · · · , λk are the regression parameters.",
                "We include a beta prior for p(λj) with parameters α, β.",
                "This can be seen as a type of smoothing to account for the fact that the training data is highly biased.",
                "This model has the advantage of including the statistical dependence between the experts.",
                "A model of the same form was shown by Clemen & Winkler to be the best for aggregating expert probabilities [10].",
                "A similar maximumentropy-motivated approach has been used for expert aggregation [15].",
                "Aslam & Montague [1] used a similar model for metasearch, but assumed independence among experts.",
                "Where do the qij s come from?",
                "Using raw, uncalibrated scores as predictors will not work because score distributions vary too much between topics.",
                "A language modeling ranker, for instance, will typically give a much higher score to the top retrieved document for a short query than to the top retrieved document for a long query.",
                "We could train a separate predicting model for each topic, but that does not take advantage of all of the information we have: we may only have a handful of judgments for a topic, not enough to train a model to any confidence.",
                "Furthermore, it seems reasonable to assume that if an expert makes good predictions for one topic, it will make good predictions for other topics as well.",
                "We could use a hierarchical model [12], but that will not generalize to unseen topics.",
                "Instead, we will calibrate the scores of each expert individually so that scores can be compared both within topic and between topic.",
                "Thus our model takes into account not only the dependence between experts, but also the dependence between experts performances on different tasks (topics). 3.2 Calibrating Experts Each expert gives us a score and a rank for each document.",
                "We need to convert these to probabilities.",
                "A method such as the one used by Manmatha et al. [14] could be used to convert scores into probabilities of relevance.",
                "The pairwise preference method of Carterette & Petkova [9] could also be used, interpeting the ranking of one document over another as an expression of preference.",
                "Let q∗ ij be expert js self-reported probability that document i is relevant.",
                "Intuitively it seems clear that q∗ ij should decrease with rank, and it should be zero if document i is unranked (the expert did not believe it to be relevant).",
                "The pairwise preference model can handle these two requirements easily, so we will use it.",
                "Let θrj (i) be the relevance coefficient of the document at rank rj(i).",
                "We want to find the θs that maximize the likelihood function: Ljt(Θ) = rj (i)<rj (k) exp(θrj (i) − θrj (k)) 1 + exp(θrj (i) − θrj (k)) We again include a beta prior on p(θrj(i)) with parameters |Rt| + 1 and |Nt| + 1, the size of the sets of judged relevant and nonrelevant documents respectively.",
                "Using these as prior parameters ensures that the resulting probabilities will be concentrated around the ratio of relevant documents that have been discovered for topic t. This means that the probability estimates decrease by rank and are higher for topics that have more relevant documents.",
                "After finding the Θ that maximizes the likelihood, we have q∗ ij = exp(θrj (i)) 1+exp(θrj (i)) .",
                "We define θ∞ = −∞, so that the probability that an unranked document is relevant is 0.",
                "Since q∗ ij is based on the rank at which a document is retrieved rather than the identity of the document itself, the probabilities are identical from expert to expert, e.g. if expert E put document A at rank 1, and expert D put document B at rank 1, we will have q∗ AE = q∗ BD.",
                "Therefore we only have to solve this once for each topic.",
                "The above model gives topic-independent probabilities for each document.",
                "But suppose an expert who reports 90% probability is only right 50% of the time.",
                "Its opinion should be discounted based on its observed performance.",
                "Specifically, we want to learn a calibration function qij = Cj(q∗ ij) that will ensure that the predicted probabilities are tuned to the experts ability to retrieve relevant documents given the judgments that have been made to this point.",
                "Platts SVM calibration method [16] fits a sigmoid function between q∗ ij and the relevance judgments to obtain qij = Cj (q∗ ij) = exp(Aj +Bjq∗ ij ) 1+exp(Aj +Bj q∗ ij ) .",
                "Since q∗ ij is topic-independent, we only need to learn one calibration function for each expert.",
                "Once we have the calibration function, it is applied to adjust the experts predictions to their actual performance.",
                "The calibrated probabilities are plugged into model (2) to find the document probabilities.",
                "Figure 1: Conceptual diagram of our aggregation model.",
                "Experts E1, E2 have ranked documents A, B, C for topic T1 and documents D, E, F for topic T2.",
                "The first step is to obtain q∗ ij.",
                "Next is calibration to true performance to find qij .",
                "Finally we obtain pi = p(Xi = 1|qi1, qi2), · · · . 3.3 Model Summary Our model has three components that differ by the data they take as input and what they produce as output.",
                "A conceptual diagram is shown in Figure 1. 1. ranks → probabilities (per system per topic).",
                "This gives us q∗ ij, expert js self-reported probability of the relevance of document i.",
                "This is unsupervised; it requires no labeled data (though if we have some, we use it to set prior parameters). 2. probabilities → calibrated probabilities (per system).",
                "This gives us qij = Cj (q∗ ij), expert js calibrated probability of the relevance of document i.",
                "This is semisupervised; we have relevance judgments at some ranks which we use to impute the probability of relevance at other ranks. 3. calibrated probabilities → document probabilities.",
                "This gives us pi = p(Xi = 1|qi), the probability of relevance of document i given calibrated expert probabilities qij .",
                "This is supervised; we learn coefficients from a set of judged documents and use those to estimate the relevance of the unjudged documents.",
                "Although the model appears rather complex, it is really just three successive applications of logistic regression.",
                "As such, it can be implemented in a statistical programming language such as R in a few lines of code.",
                "The use of beta (conjugate) priors ensures that no expensive computational methods such as MCMC are necessary [12], so the model is trained and applied fast enough to be used on-line.",
                "Our code is available at http://ciir.cs.umass.edu/~carteret/. 4.",
                "EXPERIMENTS Three hypotheses are under consideration.",
                "The first, and most important, is that using our expert aggregation model to predict relevance produces test collections that are robust enough to be reusable; that is, we can trust the estimates of confidence when we evaluate systems that did not contribute any judgments to the pool.",
                "The other two hypotheses relate to the improvement we see by using better estimates of relevance than we did in our previous work [8].",
                "These are that (a) it takes fewer relevance track no. topics no. runs no. judged no. rel ad-hoc 94 50 40 97,319 9,805 ad-hoc 95 49 33 87,069 6,503 ad-hoc 96 50 61 133,681 5,524 ad-hoc 97 50 74 72,270 4,611 ad-hoc 98 50 103 80,345 4,674 ad-hoc 99 50 129 86,830 4,728 web 04 225 74 88,566 1,763 robust 05 50 74 37,798 6,561 terabyte 05 50 58 45,291 10,407 Table 1: Number of topics, number of runs, number of documents judged, and number found relevant for each of our data sets. judgments to reach 95% confidence and (b) the accuracy of the predictions is higher than if we were to simply assume pi = 0.5 for all unjudged documents. 4.1 Data We obtained full ad-hoc runs submitted to TRECs 3 through 8.",
                "Each run ranks at most 1000 documents for 50 topics (49 topics for TREC 4).",
                "Additionally, we obtained all runs from the Web track of TREC 13, the Robust2 track of TREC 14, and the Terabyte (ad-hoc) track of TREC 14.",
                "These are the tracks that have replaced the ad-hoc track since its end in 1999.",
                "Statistics are shown in Table 1.",
                "We set aside the TREC 4 (ad-hoc 95) set for training, TRECs 3 and 5-8 (ad-hoc 94 and 96-99) for primary testing, and the remaining sets for additional testing.",
                "We use the qrels files assembled by NIST as truth.",
                "The number of relevance judgments made and relevant documents found for each track are listed in Table 1.",
                "For computational reasons, we truncate ranked lists at 100 documents.",
                "There is no reason that we could not go deeper, but calculating variance is O(n3 ) and thus very timeconsuming.",
                "Because of the reciprocal rank nature of AP, we do not lose much information by truncating at rank 100. 4.2 Algorithms We will compare three algorithms for acquiring relevance judgments.",
                "The baseline is a variation of TREC pooling that we will call incremental pooling (IP).",
                "This algorithm takes a number k as input and presents the first k documents in rank order (without regard to topic) to be judged.",
                "It does not estimate the relevance of unjudged documents; it simply assumes any unjudged document is nonrelevant.",
                "The second algorithm is that presented in Carterette et al. [8] (Algorithm 1).",
                "Documents are selected based on how interesting they are in determining whether a difference in mean average precision exists.",
                "For this approach pi = 0.5 for all i; there is no estimation of probabilities.",
                "We will call this MTC for minimal test collection.",
                "The third algorithm augments MTC with updated estimates of probabilities of relevance.",
                "We will call this RTC for robust test collection.",
                "It is identical to Algorithm 1, except that every 10th iteration we estimate pi for all unjudged documents i using the expert aggregation model of Section 3.",
                "RTC has smoothing (prior distribution) parameters that must be set.",
                "We trained using the ad-hoc 95 set.",
                "We limited 2 Robust here means robust retrieval; this is different from our goal of robust evaluation.",
                "Algorithm 1 (MTC) Given two ranked lists and confidence level α, predict the sign of ΔMAP. 1: pi ← 0.5 for all documents i 2: while P(ΔMAP < 0) < α do 3: calculate weight wi for all unjudged documents i (see Carterette et al. [8] for details) 4: j ← argmaxiwi 5: xj ← 1 if document j is relevant, 0 otherwise 6: pj ← xj 7: end while the search to uniform priors with relatively high variance.",
                "For expert aggregation, the prior parameters are α = β = 1. 4.3 Experimental Design First, we want to know whether we can augment a set of relevance judgments with a set of relevance probabilities in order to reuse the judgments to evaluate a new set of systems.",
                "For each experimental trial: 1.",
                "Pick a random subset of k runs. 2.",
                "From those k, pick an initial c < k to evaluate. 3.",
                "Run RTC to 95% confidence on the initial c. 4.",
                "Using the model from Section 3, estimate the probabilities of relevance for all documents retrieved by all k runs. 5.",
                "Calculate EMAP for all k runs, and P(ΔMAP < 0) for all pairs of runs.",
                "We do the same for MTC, but omit step 4.",
                "Note that after evaluating the first c systems, we make no additional relevance judgments.",
                "To put our method to the test, we selected c = 2: we will build a set of judgments from evaluating only two initial systems.",
                "We will then generalize to a set of k = 10 (of which those two are a subset).",
                "As we run more trials, we obtain the data we need to test all three of our hypotheses. 4.4 Experimental Evaluation Recall that a set of judgments is robust if the accuracy of the predictions it makes is at least its estimated confidence.",
                "One way to evaluate robustness is to bin pairs by their confidence, then calculate the accuracy over all the pairs in each bin.",
                "We would like the accuracy to be no less than the lowest confidence score in the bin, but preferably higher.",
                "Since summary statistics are useful, we devised the following metric.",
                "Suppose we are a bookmaker taking bets on whether ΔMAP < 0.",
                "We use RTC or MTC to set the odds O = P (ΔMAP <0) 1−P (ΔMAP <0) .",
                "Suppose a bettor wagers $1 on ΔMAP ≥ 0.",
                "If it turns out that ΔMAP < 0, we win the dollar.",
                "Otherwise, we pay out O.",
                "If our confidence estimates are perfectly accurate, we break even.",
                "If confidence is greater than accuracy, we lose money; we win if accuracy is greater than confidence.",
                "Counterintuitively, the most desirable outcome is breaking even: if we lose money, we cannot trust the confidence estimates, but if we win money, we have either underestimated confidence or judged more documents than necessary.",
                "However, the cost of not being able to trust the confidence estimates is higher than the cost of extra relevance judgments, so we will treat positive outcomes as good.",
                "The amount we win on each pairwise comparison i is: Wi = yi − (1 − yi) Pi 1 − Pi = yi − Pi 1 − Pi yi = 1 if ΔMAP < 0 and 0 otherwise, and Pi = P(ΔMAP < 0).",
                "The summary statistic is W, the mean of Wi.",
                "Note that as Pi increases, we lose more for being wrong.",
                "This is as it should be: the penalty should be great for missing the high probability predictions.",
                "However, since our losses grow without bound as predictions approach certainty, we cap −Wi at 100.",
                "For our hypothesis that RTC requires fewer judgments than MTC, we are interested in the number of judgments needed to reach 95% confidence on the first pair of systems.",
                "The median is more interesting than the mean: most pairs require a few hundred judgments, but a few pairs require several thousand.",
                "The distribution is therefore highly skewed, and the mean strongly affected by those outliers.",
                "Finally, for our hypothesis that RTC is more accurate than MTC, we will look at Kendalls τ correlation between a ranking of k systems by a small set of judgments and the true ranking using the full set of judgments.",
                "Kendalls τ, a nonparametric statistic based on pairwise swaps between two lists, is a standard evaluation for this type of study.",
                "It ranges from −1 (perfectly anti-correlated) to 1 (rankings identical), with 0 meaning that half of the pairs are swapped.",
                "As we touched on in the introduction, though, an accuracy measure like rank correlation is not a good evaluation of reusability.",
                "We include it for completeness. 4.4.1 Hypothesis Testing Running multiple trials allows the use of statistical hypothesis testing to compare algorithms.",
                "Using the same sets of systems allows the use of paired tests.",
                "As we stated above, we are more interested in the median number of judgments than the mean.",
                "A test for difference in median is the Wilcoxon sign rank test.",
                "We can also use a paired t-test to test for a difference in mean.",
                "For rank correlation, we can use a paired t-test to test for a difference in τ. 5.",
                "RESULTS AND ANALYSIS The comparison between MTC and RTC is shown in Table 2.",
                "With MTC and uniform probabilities of relevance, the results are far from robust.",
                "We cannot reuse the relevance judgments with much confidence.",
                "But with RTC, the results are very robust.",
                "There is a slight dip in accuracy when confidence gets above 0.95; nonetheless, the confidence predictions are trustworthy.",
                "Mean Wi shows that RTC is much closer to 0 than MTC.",
                "The distribution of confidence scores shows that at least 80% confidence is achieved more than 35% of the time, indicating that neither algorithm is being too conservative in its confidence estimates.",
                "The confidence estimates are rather low overall; that is because we have built a test collection from only two initial systems.",
                "Recall from Section 1 that we cannot require (or even expect) a minimum level of confidence when we generalize to new systems.",
                "More detailed results for both algorithms are shown in Figure 2.",
                "The solid line is the ideal result that would give W = 0.",
                "RTC is on or above this line at all points until confidence reaches about 0.97.",
                "After that there is a slight dip in accuracy which we discuss below.",
                "Note that both MTC RTC confidence % in bin accuracy % in bin accuracy 0.5 − 0.6 33.7% 61.7% 28.6% 61.9% 0.6 − 0.7 18.1% 73.1% 20.1% 76.3% 0.7 − 0.8 10.4% 70.1% 15.5% 78.0% 0.8 − 0.9 9.4% 69.0% 12.1% 84.9% 0.9 − 0.95 7.3% 78.0% 6.6% 93.1% 0.95 − 0.99 17.9% 70.4% 12.4% 93.4% 1.0 3.3% 68.3% 4.7% 98.9% W −5.34 −0.39 median judged 251 235 mean τ 0.393 0.555 Table 2: Confidence that P(ΔMAP < 0) and accuracy of prediction when generalizing a set of relevance judgments acquired using MTC and RTC.",
                "Each bin contains over 1,000 trials from the adhoc 3, 5-8 sets.",
                "RTC is much more robust than MTC.",
                "W is defined in Section 4.4; closer to 0 is better.",
                "Median judged is the number of judgments to reach 95% confidence on the first two systems.",
                "Mean τ is the average rank correlation for all 10 systems. 0.5 0.6 0.7 0.8 0.9 1 0.5 0.6 0.7 0.8 0.9 1 accuracy confidence breakeven RTC MTC Figure 2: Confidence vs. accuracy of MTC and RTC.",
                "The solid line is the perfect result that would give W = 0; performance should be on or above this line.",
                "Each point represents at least 500 pairwise comparisons. algorithms are well above the line up to around confidence 0.7.",
                "This is because the baseline performance on these data sets is high; it is quite easy to achieve 75% accuracy doing very little work [7].",
                "Number of Judgments: The median number of judgments required by MTC to reach 95% confidence on the first two systems is 251, an average of 5 per topic.",
                "The median required by RTC is 235, about 4.7 per topic.",
                "Although the numbers are close, RTCs median is significantly lower by a paired Wilcoxon test (p < 0.0001).",
                "For comparison, a pool of depth 100 would result in a minimum of 5,000 judgments for each pair.",
                "The difference in means is much greater.",
                "MTC required a mean of 823 judgments, 16 per topic, while RTC required a mean of 502, 10 per topic. (Recall that means are strongly skewed by a few pairs that take thousands of judgments.)",
                "This difference is significant by a paired t-test (p < 0.0001).",
                "Ten percent of the sets resulted in 100 or fewer judgments (less than two per topic).",
                "Performance on these is very high: W = 0.41, and 99.7% accuracy when confidence is at least 0.9.",
                "This shows that even tiny collections can be reusable.",
                "For the 50% of sets with more than 235 judgments, accuracy is 93% when confidence is at least 0.9.",
                "Rank Correlation: MTC and RTC both rank the 10 systems by EMAP (Eq. (1)) calculated using their respective probability estimates.",
                "The mean τ rank correlation between true MAP and EMAP is 0.393 for MTC and 0.555 for RTC.",
                "This difference is significant by a paired t-test (p < 0.0001).",
                "Note that we do not expect the τ correlations to be high, since we are ranking the systems with so few relevance judgments.",
                "It is more important that we estimate confidence in each pairwise comparison correctly.",
                "We ran IP for the same number of judgments that MTC took for each pair, then ranked the systems by MAP using only those judgments (all unjudged documents assumed nonrelevant).",
                "We calculated the τ correlation to the true ranking.",
                "The mean τ correlation is 0.398, which is not significantly different from MTC, but is significantly lower than RTC.",
                "Using uniform estimates of probability is indistinguishable from the baseline, whereas estimating relevance by expert aggregation boosts performance a great deal: nearly 40% over both MTC and IP.",
                "Overfitting: It is possible to overfit: if too many judgments come from the first two systems, the variance in ΔMAP is reduced and the confidence estimates become unreliable.",
                "We saw this in Table 2 and Figure 2 where RTC exhibits a dip in accuracy when confidence is around 97%.",
                "In fact, the number of judgments made prior to a wrong prediction is over 50% greater than the number made prior to a correct prediction.",
                "Overfitting is difficult to quantify exactly, because making more relevance judgments does not always cause it: at higher confidence levels, more relevance judgments are made, and as Table 2 shows, accuracy is greater at those higher confidences.",
                "Obviously having more relevance judgments should increase both confidence and accuracy; the difference seems to be when one system has a great deal more judgments than the other.",
                "Pairwise Comparisons: Our pairwise comparisons fall into one of three groups: 1. the two original runs from which relevance judgments are acquired; 2. one of the original runs vs. one of the new runs; 3. two new runs.",
                "Table 3 shows confidence vs. accuracy results for each of these three groups.",
                "Interestingly, performance is worst when comparing one of the original runs to one of the additional runs.",
                "This is most likely due to a large difference in the number of judgments affecting the variance of ΔMAP.",
                "Nevertheless, performance is quite good on all three subsets.",
                "Worst Case: The case intuitively most likely to produce an error is when the two systems being compared have retrieved very few documents in common.",
                "If we want the judgments to be reusable, we should to be able to generalize even to runs that are very different from the ones used to acquire the relevance judgments.",
                "A simple measure of similarity of two runs is the average percentage of documents they retrieved in common for each topic [2].",
                "We calculated this for all pairs, then looked at performance on pairs with low similarity.",
                "Results are shown in accuracy confidence two original one original no original 0.5 − 0.6 - 48.1% 62.8% 0.6 − 0.7 - 57.1% 79.2% 0.7 − 0.8 - 67.9% 81.7% 0.8 − 0.9 - 82.2% 86.3% 0.9 − 0.95 95.9% 93.7% 92.6% 0.95 − 0.99 96.2% 92.5% 93.1% 1.0 100% 98.0% 99.1% W −1.11 −0.87 −0.27 Table 3: Confidence vs. accuracy of RTC when comparing the two original runs, one original run and one new run, and two new runs.",
                "RTC is robust in all three cases. accuracy when similar confidence 0 − 0.1 0.1 − 0.2 0.2 − 0.3 0.5 − 0.6 68.4% 63.1% 61.4% 0.6 − 0.7 84.2% 78.6% 76.6% 0.7 − 0.8 82.0% 79.8% 78.9% 0.8 − 0.9 93.6% 83.3% 82.1% 0.9 − 0.95 99.3% 92.7% 92.4% 0.95 − 0.99 98.7% 93.4% 93.3% 1.0 99.9% 97.9% 98.1% W 0.44 −0.45 −0.49 Table 4: Confidence vs. accuracy of RTC when a pair of systems retrieved 0-30% documents in common (broken out into 0%-10%, 10%-20%, and 20%30%).",
                "RTC is robust in all three cases.",
                "Table 4.",
                "Performance is in fact very robust even when similarity is low.",
                "When the two runs share very few documents in common, W is actually positive.",
                "MTC and IP both performed quite poorly in these cases.",
                "When the similarity was between 0 and 10%, both MTC and IP correctly predicted ΔMAP only 60% of the time, compared to an 87.6% success rate for RTC.",
                "By Data Set: All the previous results have only been on the ad-hoc collections.",
                "We did the same experiments on our additional data sets, and broke out the results by data set to see how performance varies.",
                "The results in Table 5 show everything about each set, including binned accuracy, W, mean τ, and median number of judgments to reach 95% confidence on the first two systems.",
                "The results are highly consistent from collection to collection, suggesting that our method is not overfitting to any particular data set. 6.",
                "CONCLUSIONS AND FUTURE WORK In this work we have offered the first formal definition of the common idea of reusability of a test collection and presented a model that is able to achieve reusability with very small sets of relevance judgments.",
                "Table 2 and Figure 2 together show how biased a small set of judgments can be: MTC is dramatically overestimating confidence and is much less accurate than RTC, which is able to remove the bias to give a robust evaluation.",
                "The confidence estimates of RTC, in addition to being accurate, provide a guide for obtaining additional judgments: focus on judging documents from the lowest-confidence comparisons.",
                "In the long run, we see small sets of relevance judgaccuracy confidence ad-hoc 94 ad-hoc 96 ad-hoc 97 ad-hoc 98 ad-hoc 99 web 04 robust 05 terabyte 05 0.5 − 0.6 64.1% 61.8% 62.2% 62.0% 59.4% 64.3% 61.5% 61.6% 0.6 − 0.7 76.1% 77.8% 74.5% 78.2% 74.3% 78.1% 75.9% 75.9% 0.7 − 0.8 75.2% 78.9% 77.6% 80.0% 78.6% 82.6% 77.5% 80.4% 0.8 − 0.9 83.2% 85.5% 84.6% 84.9% 86.8% 84.5% 86.7% 87.7% 0.9 − 0.95 93.0% 93.6% 92.8% 93.7% 92.6% 94.2% 93.9% 94.2% 0.95 − 0.99 93.1% 94.3% 93.1% 93.7% 92.8% 95.0% 93.9% 91.6% 1.0 99.2% 96.8% 98.7% 99.5% 99.6% 100% 99.2% 98.3% W -0.34 -0.34 -0.48 -0.35 -0.44 -0.07 -0.41 -0.67 median judged 235 276 243 213 179 448 310 320 mean τ 0.538 0.573 0.556 0.579 0.532 0.596 0.565 0.574 Table 5: Accuracy, W, mean τ, and median number of judgments for all 8 testing sets.",
                "The results are highly consistent across data sets. ments being shared by researchers, each group contributing a few more judgments to gain more confidence about their particular systems.",
                "As time goes on, the number of judgments grows until there is 100% confidence in every evaluation-and there is a full test collection for the task.",
                "We see further use for this method in scenarios such as web retrieval in which the corpus is frequently changing.",
                "It could be applied to evaluation on a dynamic test collection as defined by Soboroff [18].",
                "The model we presented in Section 3 is by no means the only possibility for creating a robust test collection.",
                "A simpler expert aggregation model might perform as well or better (though all our efforts to simplify failed).",
                "In addition to expert aggregation, we could estimate probabilities by looking at similarities between documents.",
                "This is an obvious area for future exploration.",
                "Additionally, it will be worthwhile to investigate the issue of overfitting: the circumstances it occurs under and what can be done to prevent it.",
                "In the meantime, capping confidence estimates at 95% is a hack that solves the problem.",
                "We have many more experimental results that we unfortunately did not have space for but that reinforce the notion that RTC is highly robust: with just a few judgments per topic, we can accurately assess the confidence in any pairwise comparison of systems.",
                "Acknowledgments This work was supported in part by the Center for Intelligent <br>information retrieval</br> and in part by the Defense Advanced Research Projects Agency (DARPA) under contract number HR0011-06-C-0023.",
                "Any opinions, findings, and conclusions or recommendations expressed in this material are those of the author and do not necessarily reflect those of the sponsor. 7.",
                "REFERENCES [1] J. Aslam and M. Montague.",
                "Models for Metasearch.",
                "In Proceedings of SIGIR, pages 275-285, 2001. [2] J. Aslam and R. Savell.",
                "On the effectiveness of evaluating retrieval systems in the absence of relevance judgments.",
                "In Proceedings of SIGIR, pages 361-362, 2003. [3] J.",
                "A. Aslam, V. Pavlu, and R. Savell.",
                "A unified model for metasearch, pooling, and system evaluation.",
                "In Proceedings of CIKM, pages 484-491, 2003. [4] J.",
                "A. Aslam, V. Pavlu, and E. Yilmaz.",
                "A statistical method for system evaluation using incomplete judgments.",
                "In Proceedings of SIGIR, pages 541-548, 2006. [5] A. L. Berger, S. D. Pietra, and V. J. D. Pietra.",
                "A maximum entropy approach to natural language processing.",
                "Computational Linguistics, 22(1):39-71, 1996. [6] D. J. Blower.",
                "An easy derivation of logistic regression from the bayesian and maximum entropy perspective.",
                "In Proceedings of the 23rd International Workship on Bayesian Inference and Maximum Entropy Methods in Science and Engineering, pages 30-43, 2004. [7] B. Carterette and J. Allan.",
                "Research methodology in studies of assessor effort for retrieval evaluation.",
                "In Proceedings of RIAO, 2007. [8] B. Carterette, J. Allan, and R. K. Sitaraman.",
                "Minimal test collections for retrieval evaluation.",
                "In Proceedings of SIGIR, pages 268-275, 2006. [9] B. Carterette and D. I. Petkova.",
                "Learning a ranking from pairwise preferences.",
                "In Proceedings of SIGIR, 2006. [10] R. T. Clemen and R. L. Winkler.",
                "Unanimity and compromise among probability forecasters.",
                "Management Science, 36(7):767-779, July 1990. [11] G. V. Cormack, C. R. Palmer, and C. L. Clarke.",
                "Efficient Construction of Large Test Collections.",
                "In Proceedings of SIGIR, pages 282-289, 1998. [12] A. Gelman, J.",
                "B. Carlin, H. S. Stern, and D. B. Rubin.",
                "Bayesian Data Analysis.",
                "Chapman & Hall/CRC, 2004. [13] E. T. Jaynes.",
                "Probability Theory: The Logic of Science.",
                "Cambridge University Press, 2003. [14] R. Manmatha and H. Sever.",
                "A Formal Approach to Score Normalization for Metasearch.",
                "In Proceedings of HLT, pages 88-93, 2002. [15] I. J. Myung, S. Ramamoorti, and J. Andrew D. Baily.",
                "Maximum entropy aggregation of expert predictions.",
                "Management Science, 42(10):1420-1436, October 1996. [16] J. Platt.",
                "Probabilistic outputs for support vector machines and comparison to regularized likelihood methods. pages 61-74, 2000. [17] M. Sanderson and H. Joho.",
                "Forming test collections with no system pooling.",
                "In Proceedings of the 27th Annual International ACM SIGIR Conference on Research and Development in <br>information retrieval</br>, pages 33-40, 2004. [18] I. Soboroff.",
                "Dynamic test collections: measuring search effectiveness on the live web.",
                "In Proceedings of SIGIR, pages 276-283, 2006. [19] K. Sparck Jones and C. J. van Rijsbergen.",
                "<br>information retrieval</br> Test Collections.",
                "Journal of Documentation, 32(1):59-75, 1976. [20] E. M. Voorhees and D. K. Harman, editors.",
                "TREC: Experiment and Evaluation in <br>information retrieval</br>.",
                "MIT Press, 2005. [21] J. Zobel.",
                "How Reliable are the Results of Large-Scale <br>information retrieval</br> Experiments?",
                "In Proceedings of SIGIR, pages 307-314, 1998."
            ],
            "original_annotated_samples": [
                "Robust Test Collections for Retrieval Evaluation Ben Carterette Center for Intelligent <br>information retrieval</br> Computer Science Department University of Massachusetts Amherst Amherst, MA 01003 carteret@cs.umass.edu ABSTRACT Low-cost methods for acquiring relevance judgments can be a boon to researchers who need to evaluate new retrieval tasks or topics but do not have the resources to make thousands of judgments.",
                "INTRODUCTION Consider an <br>information retrieval</br> researcher who has invented a new retrieval task.",
                "Evaluation is an important aspect of <br>information retrieval</br> research, but it is only a semi-solved problem: for most retrieval tasks, it is impossible to judge the relevance of every document; there are simply too many of them.",
                "Small, reusable test collections could have a huge impact on <br>information retrieval</br> research.",
                "This has found a wide array of uses in computer science and <br>information retrieval</br>."
            ],
            "translated_annotated_samples": [
                "Colecciones de pruebas robustas para la evaluación de recuperación Ben Carterette Centro de Recuperación de Información Inteligente Departamento de Ciencias de la Computación Universidad de Massachusetts Amherst Amherst, MA 01003 carteret@cs.umass.edu RESUMEN Los métodos de bajo costo para adquirir juicios de relevancia pueden ser una bendición para los investigadores que necesitan evaluar nuevas tareas o temas de recuperación pero no tienen los recursos para hacer miles de juicios.",
                "INTRODUCCIÓN Imagina a un investigador de <br>recuperación de información</br> que ha inventado una nueva tarea de recuperación.",
                "La evaluación es un aspecto importante de la investigación en <br>recuperación de información</br>, pero es solo un problema parcialmente resuelto: para la mayoría de las tareas de recuperación, es imposible juzgar la relevancia de cada documento; simplemente hay demasiados.",
                "Pequeñas colecciones de pruebas reutilizables podrían tener un gran impacto en la investigación de <br>recuperación de información</br>.",
                "Esto ha encontrado una amplia variedad de usos en la informática y la <br>recuperación de información</br>."
            ],
            "translated_text": "Colecciones de pruebas robustas para la evaluación de recuperación Ben Carterette Centro de Recuperación de Información Inteligente Departamento de Ciencias de la Computación Universidad de Massachusetts Amherst Amherst, MA 01003 carteret@cs.umass.edu RESUMEN Los métodos de bajo costo para adquirir juicios de relevancia pueden ser una bendición para los investigadores que necesitan evaluar nuevas tareas o temas de recuperación pero no tienen los recursos para hacer miles de juicios. Si bien estos juicios son muy útiles para una evaluación única, no está claro que se puedan confiar al ser reutilizados para evaluar nuevos sistemas. En este trabajo, definimos formalmente lo que significa que las evaluaciones sean reutilizables: la confianza en una evaluación de nuevos sistemas puede ser evaluada con precisión a partir de un conjunto existente de juicios de relevancia. Luego presentamos un método para aumentar un conjunto de juicios de relevancia con estimaciones de relevancia que no requieren esfuerzo adicional del evaluador. Usar este método garantiza prácticamente la reutilización: con tan solo cinco juicios por tema tomados de solo dos sistemas, podemos evaluar de manera confiable un conjunto más grande de diez sistemas. Incluso los conjuntos más pequeños de juicios pueden ser útiles para la evaluación de nuevos sistemas. Categorías y Descriptores de Asignaturas: H.3 Almacenamiento y Recuperación de Información; H.3.4 Sistemas y Software: Evaluación del Rendimiento Términos Generales: Experimentación, Medición, Confiabilidad 1. INTRODUCCIÓN Imagina a un investigador de <br>recuperación de información</br> que ha inventado una nueva tarea de recuperación. Ella ha construido un sistema para realizar la tarea y quiere evaluarlo. Dado que la tarea es nueva, es poco probable que existan juicios de relevancia vigentes. Ella no tiene el tiempo ni los recursos para juzgar cada documento, ni siquiera cada documento recuperado. Ella solo puede juzgar los documentos que parecen ser los más informativos y detenerse cuando tenga un grado razonable de confianza en sus conclusiones. Pero ¿qué sucede cuando ella desarrolla un nuevo sistema y necesita evaluarlo? ¿Otro grupo de investigación decide implementar un sistema para realizar la tarea? ¿Pueden reutilizar de manera confiable los juicios originales? ¿Pueden evaluar sin más juicios de relevancia? La evaluación es un aspecto importante de la investigación en <br>recuperación de información</br>, pero es solo un problema parcialmente resuelto: para la mayoría de las tareas de recuperación, es imposible juzgar la relevancia de cada documento; simplemente hay demasiados. La solución utilizada por NIST en TREC (Text REtrieval Conference) es el método de agrupación [19, 20]: todos los sistemas competidores contribuyen con N documentos a un grupo, y cada documento en ese grupo es evaluado. Este método crea grandes conjuntos de juicios que son reutilizables para entrenar o evaluar nuevos sistemas que no contribuyeron al conjunto [21]. Esta solución no es adecuada para nuestro investigador hipotético. El método de agrupación proporciona miles de juicios de relevancia, pero requiere muchas horas de tiempo de anotadores (remunerados). Como resultado, ha habido una serie de artículos recientes sobre la reducción del esfuerzo del anotador en la producción de colecciones de pruebas: Cormack et al. [11], Zobel [21], Sanderson y Joho [17], Carterette et al. [8], y Aslam et al. [4], entre otros. Como veremos, los juicios que estos métodos producen pueden sesgar significativamente la evaluación de un nuevo conjunto de sistemas. ¿Volviendo a nuestra investigadora hipotética, puede reutilizar sus juicios de relevancia? Primero debemos definir formalmente lo que significa ser reutilizable. En trabajos anteriores, la reutilización se ha probado simplemente evaluando la precisión de un conjunto de juicios de relevancia al evaluar sistemas no vistos. Si bien podemos decir que tuvo razón el 75% del tiempo, o que tuvo una correlación de rango de 0.8, estos números no tienen ningún poder predictivo: no nos dicen qué sistemas es probable que estén equivocados ni cuán seguros deberíamos estar en uno en particular. Necesitamos una definición más cuidadosa de reutilización. Específicamente, la cuestión de la reutilización no es qué tan precisamente podemos evaluar los nuevos sistemas. Un adversario malicioso siempre puede producir una nueva lista clasificada que no haya recuperado ninguno de los documentos evaluados. La verdadera pregunta es cuánta confianza tenemos en nuestras evaluaciones, y, más importante aún, si podemos confiar en nuestras estimaciones de confianza. Aunque la confianza no sea alta, siempre y cuando podamos confiar en ello, podemos identificar qué sistemas necesitan más juicios para aumentar la confianza. Cualquier conjunto de juicios, por pequeño que sea, se vuelve reutilizable en cierta medida. Pequeñas colecciones de pruebas reutilizables podrían tener un gran impacto en la investigación de <br>recuperación de información</br>. Los grupos de investigación podrían compartir las evaluaciones de relevancia que han realizado internamente para estudios piloto, nuevas tareas o nuevos temas. La cantidad de datos disponible para los investigadores crecería de forma exponencial con el tiempo. EVALUACIÓN ROBUSTA Anteriormente dimos una definición intuitiva de reutilización: una colección es reutilizable si podemos confiar en nuestras estimaciones de confianza en una evaluación. Con eso queremos decir que si hemos realizado algunos juicios de relevancia y tenemos, por ejemplo, un 75% de confianza en que el sistema A es mejor que el sistema B, nos gustaría que no haya más del 25% de posibilidad de que nuestra evaluación de la calidad relativa de los sistemas cambie a medida que continuamos evaluando documentos. Nuestra evaluación debe ser robusta ante juicios faltantes. En nuestro trabajo anterior, definimos la confianza como la probabilidad de que la diferencia en una medida de evaluación calculada para dos sistemas sea menor que cero [8]. Esta noción de confianza se define en el contexto de una tarea de evaluación particular que llamamos evaluación comparativa: determinar el signo de la diferencia en una medida de evaluación. Otras tareas de evaluación podrían ser definidas; estimar la magnitud de la diferencia o los valores de las medidas mismas son ejemplos que implican diferentes nociones de confianza. Por lo tanto, consideramos la confianza como una estimación de probabilidad. Una de las preguntas que debemos hacer acerca de una estimación de probabilidad es qué significa. ¿Qué significa tener un 75% de confianza de que el sistema A es mejor que el sistema B? Como se describe arriba, queremos que signifique que si seguimos evaluando documentos, solo habrá un 25% de probabilidad de que nuestra evaluación cambie. Si esto es lo que significa, podemos confiar en las estimaciones de confianza. ¿Pero sabemos que tiene ese significado? Nuestro cálculo de confianza se basó en una suposición sobre la probabilidad de relevancia de los documentos no evaluados, específicamente que cada documento no evaluado tenía la misma probabilidad de ser relevante o no relevante. Esta suposición casi con seguridad no es realista en la mayoría de las aplicaciones de IR. Resulta que es esta suposición la que determina si se pueden confiar en las estimaciones de confianza. Antes de profundizar en esto, definimos formalmente la confianza. 2.1 Estimación de la confianza La precisión promedio (AP) es una métrica de evaluación estándar que captura tanto la capacidad de un sistema para clasificar documentos relevantes de manera alta (precisión) como su capacidad para recuperar documentos relevantes (recuperación). Normalmente se escribe como la precisión media en los rangos de documentos relevantes: AP = 1 |R| i∈R prec@r(i) donde R es el conjunto de documentos relevantes y r(i) es el rango del documento i. Sea Xi una variable aleatoria que indica la relevancia del documento i. Si los documentos están ordenados por rango, podemos expresar la precisión como prec@i = 1/i i j=1 Xj. La precisión promedio se convierte entonces en la ecuación cuadrática AP = 1 Xi n i=1 Xi/i i j=1 Xj = 1 Xi n i=1 j≥i aijXiXj donde aij = 1/ max{r(i), r(j)}. Usar aij en lugar de 1/i nos permite numerar los documentos de forma arbitraria. Para ver por qué esto es cierto, considera un ejemplo simple: una lista de 3 documentos con los documentos relevantes B, C en los rangos 1 y 3 y el documento no relevante A en el rango 2. La precisión promedio será 1 2 (1 1 x2 B+ 1 2 xBxA+ 1 3 xBxC + 1 2 x2 A+ 1 3 xAxC + 1 3 x2 C) = 1 2 1 + 2 3 porque xA = 0, xB = 1, xC = 1. Aunque el orden B, A, C es diferente de la etiqueta A, B, C, no afecta al cálculo. Ahora podemos ver que la precisión promedio en sí misma es una variable aleatoria con una distribución sobre todas las posibles asignaciones de relevancia a todos los documentos. Esta variable aleatoria tiene una esperanza, una varianza, intervalos de confianza y una cierta probabilidad de ser menor o igual a un valor dado. Todos estos dependen de la probabilidad de que el documento i sea relevante: pi = p(Xi = 1). Supongamos que en nuestro ejemplo anterior no conocemos las valoraciones de relevancia, pero creemos que pA = 0.4, pB = 0.8, pC = 0.7. Podemos entonces calcular, por ejemplo. P(AP = 0) = 0.2 · 0.6 · 0.3 = 0.036, o P(AP = 1 2 ) = 0.2 · 0.4 · 0.7 = 0.056. \n\nLa traducción al español sería: P(AP = 0) = 0.2 · 0.6 · 0.3 = 0.036, o P(AP = 1 2 ) = 0.2 · 0.4 · 0.7 = 0.056. Sumando sobre todas las posibilidades, podemos calcular la esperanza y la varianza: E[AP] ≈ 1 pi aiipi + j>i aij pipj V ar[AP] ≈ 1 ( pi)2 n i a2 iipiqi + j>i a2 ijpipj(1 − pipj) + i=j 2aiiaijpipj(1 − pi) + k>j=i 2aijaikpipjpk(1 − pi) AP converge asintóticamente a una distribución normal con la esperanza y la varianza definidas anteriormente. Para nuestra tarea de evaluación comparativa, estamos interesados en el signo de la diferencia en dos precisiones promedio: ΔAP = AP1 − AP2. Como mostramos en nuestro trabajo anterior, ΔAP tiene una forma cerrada cuando los documentos están ordenados arbitrariamente: ΔAP = 1 Xi n i=1 j≥i cij XiXj cij = aij − bij donde bij está definido de manera análoga a aij para la segunda clasificación. Dado que AP es normal, ΔAP también es normal, lo que significa que podemos usar la función de densidad acumulativa normal para determinar la confianza de que una diferencia en AP sea menor que cero. Dado que los temas son independientes, podemos extender fácilmente esto para referirnos a la precisión promedio (MAP). MAP también se distribuye normalmente; su esperanza y varianza son: EMAP = 1 T t∈T E[APt] (1) VMAP = 1 T2 t∈T V ar[APt] ΔMAP = MAP1 − MAP2 La confianza puede entonces estimarse calculando la esperanza y varianza y utilizando la función de densidad normal para encontrar P(ΔMAP < 0). 2.2 Confianza y Robustez Habiendo definido la confianza, volvemos al tema de la confianza en las estimaciones de confianza, y mostramos cómo se relaciona con la robustez de la colección ante juicios faltantes. 1 Estas son en realidad aproximaciones a la verdadera esperanza y varianza, pero el error es despreciable O(n2−n ). Sea Z el conjunto de todos los pares de resultados clasificados para un conjunto común de temas. Supongamos que tenemos un conjunto de m juicios de relevancia xm = {x1, x2, ..., xm} (usando x minúscula en lugar de X mayúscula para distinguir entre documentos juzgados y no juzgados); estos son los juicios contra los cuales calculamos la confianza. Sea Zα el subconjunto de pares en Z para los cuales predecimos que ΔMAP = −1 con confianza α dadas las evaluaciones xm. Para que las estimaciones de confianza sean precisas, necesitamos que al menos α · |Zα| de estos pares realmente tengan ΔMAP = −1 después de haber evaluado cada documento. Si lo hacen, podemos confiar en las estimaciones de confianza; nuestra evaluación será robusta ante juicios faltantes. Si nuestras estimaciones de confianza se basan en suposiciones poco realistas, no podemos esperar que sean precisas. Las suposiciones en las que se basan son las probabilidades de relevancia pi. Necesitamos que esto sea realista. Sostenemos que la mejor distribución posible de relevancia p(Xi) es aquella que explica todos los datos (todas las observaciones realizadas sobre los sistemas de recuperación) al mismo tiempo que no hace suposiciones infundadas. Esto se conoce como el principio de máxima entropía [13]. La entropía de una variable aleatoria X con distribución p(X) se define como H(p) = − i p(X = i) log p(X = i). Esto ha encontrado una amplia variedad de usos en la informática y la <br>recuperación de información</br>. ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "evaluation": {
            "translated_key": "evaluación",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Robust Test Collections for Retrieval <br>evaluation</br> Ben Carterette Center for Intelligent Information Retrieval Computer Science Department University of Massachusetts Amherst Amherst, MA 01003 carteret@cs.umass.edu ABSTRACT Low-cost methods for acquiring relevance judgments can be a boon to researchers who need to evaluate new retrieval tasks or topics but do not have the resources to make thousands of judgments.",
                "While these judgments are very useful for a one-time <br>evaluation</br>, it is not clear that they can be trusted when re-used to evaluate new systems.",
                "In this work, we formally define what it means for judgments to be reusable: the confidence in an <br>evaluation</br> of new systems can be accurately assessed from an existing set of relevance judgments.",
                "We then present a method for augmenting a set of relevance judgments with relevance estimates that require no additional assessor effort.",
                "Using this method practically guarantees reusability: with as few as five judgments per topic taken from only two systems, we can reliably evaluate a larger set of ten systems.",
                "Even the smallest sets of judgments can be useful for <br>evaluation</br> of new systems.",
                "Categories and Subject Descriptors: H.3 Information Storage and Retrieval; H.3.4 Systems and Software: Performance <br>evaluation</br> General Terms: Experimentation, Measurement, Reliability 1.",
                "INTRODUCTION Consider an information retrieval researcher who has invented a new retrieval task.",
                "She has built a system to perform the task and wants to evaluate it.",
                "Since the task is new, it is unlikely that there are any extant relevance judgments.",
                "She does not have the time or resources to judge every document, or even every retrieved document.",
                "She can only judge the documents that seem to be the most informative and stop when she has a reasonable degree of confidence in her conclusions.",
                "But what happens when she develops a new system and needs to evaluate it?",
                "Or another research group decides to implement a system to perform the task?",
                "Can they reliably reuse the original judgments?",
                "Can they evaluate without more relevance judgments?",
                "<br>evaluation</br> is an important aspect of information retrieval research, but it is only a semi-solved problem: for most retrieval tasks, it is impossible to judge the relevance of every document; there are simply too many of them.",
                "The solution used by NIST at TREC (Text REtrieval Conference) is the pooling method [19, 20]: all competing systems contribute N documents to a pool, and every document in that pool is judged.",
                "This method creates large sets of judgments that are reusable for training or evaluating new systems that did not contribute to the pool [21].",
                "This solution is not adequate for our hypothetical researcher.",
                "The pooling method gives thousands of relevance judgments, but it requires many hours of (paid) annotator time.",
                "As a result, there have been a slew of recent papers on reducing annotator effort in producing test collections: Cormack et al. [11], Zobel [21], Sanderson and Joho [17], Carterette et al. [8], and Aslam et al. [4], among others.",
                "As we will see, the judgments these methods produce can significantly bias the <br>evaluation</br> of a new set of systems.",
                "Returning to our hypothetical resesarcher, can she reuse her relevance judgments?",
                "First we must formally define what it means to be reusable.",
                "In previous work, reusability has been tested by simply assessing the accuracy of a set of relevance judgments at evaluating unseen systems.",
                "While we can say that it was right 75% of the time, or that it had a rank correlation of 0.8, these numbers do not have any predictive power: they do not tell us which systems are likely to be wrong or how confident we should be in any one.",
                "We need a more careful definition of reusability.",
                "Specifically, the question of reusability is not how accurately we can evaluate new systems.",
                "A malicious adversary can always produce a new ranked list that has not retrieved any of the judged documents.",
                "The real question is how much confidence we have in our evaluations, and, more importantly, whether we can trust our estimates of confidence.",
                "Even if confidence is not high, as long as we can trust it, we can identify which systems need more judgments in order to increase confidence.",
                "Any set of judgments, no matter how small, becomes reusable to some degree.",
                "Small, reusable test collections could have a huge impact on information retrieval research.",
                "Research groups would be able to share the relevance judgments they have done in-house for pilot studies, new tasks, or new topics.",
                "The amount of data available to researchers would grow exponentially over time. 2.",
                "ROBUST <br>evaluation</br> Above we gave an intuitive definition of reusability: a collection is reusable if we can trust our estimates of confidence in an <br>evaluation</br>.",
                "By that we mean that if we have made some relevance judgments and have, for example, 75% confidence that system A is better than system B, we would like there to be no more than 25% chance that our assessment of the relative quality of the systems will change as we continue to judge documents.",
                "Our <br>evaluation</br> should be robust to missing judgments.",
                "In our previous work, we defined confidence as the probability that the difference in an <br>evaluation</br> measure calculated for two systems is less than zero [8].",
                "This notion of confidence is defined in the context of a particular <br>evaluation</br> task that we call comparative <br>evaluation</br>: determining the sign of the difference in an evaluation measure.",
                "Other <br>evaluation</br> tasks could be defined; estimating the magnitude of the difference or the values of the measures themselves are examples that entail different notions of confidence.",
                "We therefore see confidence as a probability estimate.",
                "One of the questions we must ask about a probability estimate is what it means.",
                "What does it mean to have 75% confidence that system A is better than system B?",
                "As described above, we want it to mean that if we continue to judge documents, there will only be a 25% chance that our assessment will change.",
                "If this is what it means, we can trust the confidence estimates.",
                "But do we know it has that meaning?",
                "Our calculation of confidence rested on an assumption about the probability of relevance of unjudged documents, specifically that each unjudged document was equally likely to be relevant or nonrelevant.",
                "This assumption is almost certainly not realistic in most IR applications.",
                "As it turns out, it is this assumption that determines whether the confidence estimates can eb trusted.",
                "Before elaborating on this, we formally define confidence. 2.1 Estimating Confidence Average precision (AP) is a standard <br>evaluation</br> metric that captures both the ability of a system to rank relevant documents highly (precision) as well as its ability to retrieve relevant documents (recall).",
                "It is typically written as the mean precision at the ranks of relevant documents: AP = 1 |R| i∈R prec@r(i) where R is the set of relevant documents and r(i) is the rank of document i.",
                "Let Xi be a random variable indicating the relevance of document i.",
                "If documents are ordered by rank, we can express precision as prec@i = 1/i i j=1 Xj .",
                "Average precision then becomes the quadratic equation AP = 1 Xi n i=1 Xi/i i j=1 Xj = 1 Xi n i=1 j≥i aijXiXj where aij = 1/ max{r(i), r(j)}.",
                "Using aij instead of 1/i allows us to number the documents arbitrarily.",
                "To see why this is true, consider a toy example: a list of 3 documents with relevant documents B, C at ranks 1 and 3 and nonrelevant document A at rank 2.",
                "Average precision will be 1 2 (1 1 x2 B+ 1 2 xBxA+ 1 3 xBxC + 1 2 x2 A+ 1 3 xAxC + 1 3 x2 C) = 1 2 1 + 2 3 because xA = 0, xB = 1, xC = 1.",
                "Though the ordering B, A, C is different from the labeling A, B, C, it does not affect the computation.",
                "We can now see average precision itself is a random variable with a distribution over all possible assignments of relevance to all documents.",
                "This random variable has an expectation, a variance, confidence intervals, and a certain probability of being less than or equal to a given value.",
                "All of these are dependent on the probability that document i is relevant: pi = p(Xi = 1).",
                "Suppose in our previous example we do not know the relevance judgments, but we believe pA = 0.4, pB = 0.8, pC = 0.7.",
                "We can then compute e.g.",
                "P(AP = 0) = 0.2 · 0.6 · 0.3 = 0.036, or P(AP = 1 2 ) = 0.2 · 0.4 · 0.7 = 0.056.",
                "Summing over all possibilities, we can compute expectation and variance: E[AP] ≈ 1 pi aiipi + j>i aij pipj V ar[AP] ≈ 1 ( pi)2 n i a2 iipiqi + j>i a2 ijpipj(1 − pipj) + i=j 2aiiaijpipj(1 − pi) + k>j=i 2aijaikpipjpk(1 − pi) AP asymptotically converges to a normal distribution with expectation and variance as defined above.1 For our comparative <br>evaluation</br> task we are interested in the sign of the difference in two average precisions: ΔAP = AP1 − AP2.",
                "As we showed in our previous work, ΔAP has a closed form when documents are ordered arbitrarily: ΔAP = 1 Xi n i=1 j≥i cij XiXj cij = aij − bij where bij is defined analogously to aij for the second ranking.",
                "Since AP is normal, ΔAP is normal as well, meaning we can use the normal cumulative density function to determine the confidence that a difference in AP is less than zero.",
                "Since topics are independent, we can easily extend this to mean average precision (MAP).",
                "MAP is also normally distributed; its expectation and variance are: EMAP = 1 T t∈T E[APt] (1) VMAP = 1 T2 t∈T V ar[APt] ΔMAP = MAP1 − MAP2 Confidence can then be estimated by calculating the expectation and variance and using the normal density function to find P(ΔMAP < 0). 2.2 Confidence and Robustness Having defined confidence, we turn back to the issue of trust in confidence estimates, and show how it ties into the robustness of the collection to missing judgments. 1 These are actually approximations to the true expectation and variance, but the error is a negligible O(n2−n ).",
                "Let Z be the set of all pairs of ranked results for a common set of topics.",
                "Suppose we have a set of m relevance judgments xm = {x1, x2, ..., xm} (using small x rather than capital X to distinguish between judged and unjudged documents); these are the judgments against which we compute confidence.",
                "Let Zα be the subset of pairs in Z for which we predict that ΔMAP = −1 with confidence α given the judgments xm .",
                "For the confidence estimates to be accurate, we need at least α · |Zα| of these pairs to actually have ΔMAP = −1 after we have judged every document.",
                "If they do, we can trust the confidence estimates; our <br>evaluation</br> will be robust to missing judgments.",
                "If our confidence estimates are based on unrealistic assumptions, we cannot expect them to be accurate.",
                "The assumptions they are based on are the probabilities of relevance pi.",
                "We need these to be realistic.",
                "We argue that the best possible distribution of relevance p(Xi) is the one that explains all of the data (all of the observations made about the retrieval systems) while at the same time making no unwarranted assumptions.",
                "This is known as the principle of maximum entropy [13].",
                "The entropy of a random variable X with distribution p(X) is defined as H(p) = − i p(X = i) log p(X = i).",
                "This has found a wide array of uses in computer science and information retrieval.",
                "The maximum entropy distribution is the one that maximizes H. This distribution is unique and has an exponential form.",
                "The following theorem shows the utility of a maximum entropy distribution for relevance when estimating confidence.",
                "Theorem 1.",
                "If p(Xn |I, xm ) = argmaxpH(p), confidence estimates will be accurate. where xm is the set of relevance judgments defined above, Xn is the full set of documents that we wish to estimate the relevance of, and I is some information about the documents (unspecified as of now).",
                "We forgo the proof for the time being, but it is quite simple.",
                "This says that the better the estimates of relevance, the more accurate the <br>evaluation</br>.",
                "The task of creating a reusable test collection thus becomes the task of estimating the relevance of unjudged documents.",
                "The theorem and its proof say nothing whatsoever about the <br>evaluation</br> metric.",
                "The probability estimates are entirely indepedent of the measure we are interested in.",
                "This means the same probability estimates can tell us about average precision as well as precision, recall, bpref, etc.",
                "Furthermore, we could assume that the relevance of documents i and j is independent and achieve the same result, which we state as a corollary: Corollary 1.",
                "If p(Xi|I, xm ) = argmaxpH(p), confidence estimates will be accurate.",
                "The task therefore becomes the imputation of the missing values of relevance.",
                "The theorem implies that the closer we get to the maximum entropy distribution of relevance, the closer we get to robustness. 3.",
                "PREDICTING RELEVANCE In our statement of Theorem 1, we left the nature of the information I unspecified.",
                "One of the advantages of our confidence estimates is that they admit information from a wide variety of sources; essentially anything that can be modeled can be used as information for predicting relevance.",
                "A natural source of information is the retrieval systems themselves: how they ranked the judged documents, how often they failed to rank relevant documents, how they perform across topics, and so on.",
                "If we treat each system as an information retrieval expert providing an opinion about the relevance of each document, the problem becomes one of expert opinion aggregation.",
                "This is similar to the metasearch or data fusion problem in which the task is to take k input systems and merge them into a single ranking.",
                "Aslam et al. [3] previously identified a connection between <br>evaluation</br> and metasearch.",
                "Our problem has two key differences: 1.",
                "We explicitly need probabilities of relevance that we can plug into Eq. 1; metasearch algorithms have no such requirement. 2.",
                "We are accumulating relevance judgments as we proceed with the <br>evaluation</br> and are able to re-estimate relevance given each new judgment.",
                "In light of (1) above, we introduce a probabilistic model for expert combination. 3.1 A Model for Expert Opinion Aggregation Suppose that each expert j provides a probability of relevance qij = pj(Xi = 1).",
                "The information about the relevance of document i will then be the set of k expert opinions I = qi = (qi1, qi2, · · · , qik).",
                "The probability distribution we wish to find is the one that maximizes the entropy of pi = p(Xi = 1|qi).",
                "As it turns out, finding the maximum entropy model is equivalent to finding the parameters that maximize the likelihood [5].",
                "Blower [6] explicitly shows that finding the maximum entropy model for a binary variable is equivalent to solving a logistic regression.",
                "Then pi = p(Xi = 1|qi) = exp k j=1 λjqij 1 + exp k j=1 λj qij (2) where λ1, · · · , λk are the regression parameters.",
                "We include a beta prior for p(λj) with parameters α, β.",
                "This can be seen as a type of smoothing to account for the fact that the training data is highly biased.",
                "This model has the advantage of including the statistical dependence between the experts.",
                "A model of the same form was shown by Clemen & Winkler to be the best for aggregating expert probabilities [10].",
                "A similar maximumentropy-motivated approach has been used for expert aggregation [15].",
                "Aslam & Montague [1] used a similar model for metasearch, but assumed independence among experts.",
                "Where do the qij s come from?",
                "Using raw, uncalibrated scores as predictors will not work because score distributions vary too much between topics.",
                "A language modeling ranker, for instance, will typically give a much higher score to the top retrieved document for a short query than to the top retrieved document for a long query.",
                "We could train a separate predicting model for each topic, but that does not take advantage of all of the information we have: we may only have a handful of judgments for a topic, not enough to train a model to any confidence.",
                "Furthermore, it seems reasonable to assume that if an expert makes good predictions for one topic, it will make good predictions for other topics as well.",
                "We could use a hierarchical model [12], but that will not generalize to unseen topics.",
                "Instead, we will calibrate the scores of each expert individually so that scores can be compared both within topic and between topic.",
                "Thus our model takes into account not only the dependence between experts, but also the dependence between experts performances on different tasks (topics). 3.2 Calibrating Experts Each expert gives us a score and a rank for each document.",
                "We need to convert these to probabilities.",
                "A method such as the one used by Manmatha et al. [14] could be used to convert scores into probabilities of relevance.",
                "The pairwise preference method of Carterette & Petkova [9] could also be used, interpeting the ranking of one document over another as an expression of preference.",
                "Let q∗ ij be expert js self-reported probability that document i is relevant.",
                "Intuitively it seems clear that q∗ ij should decrease with rank, and it should be zero if document i is unranked (the expert did not believe it to be relevant).",
                "The pairwise preference model can handle these two requirements easily, so we will use it.",
                "Let θrj (i) be the relevance coefficient of the document at rank rj(i).",
                "We want to find the θs that maximize the likelihood function: Ljt(Θ) = rj (i)<rj (k) exp(θrj (i) − θrj (k)) 1 + exp(θrj (i) − θrj (k)) We again include a beta prior on p(θrj(i)) with parameters |Rt| + 1 and |Nt| + 1, the size of the sets of judged relevant and nonrelevant documents respectively.",
                "Using these as prior parameters ensures that the resulting probabilities will be concentrated around the ratio of relevant documents that have been discovered for topic t. This means that the probability estimates decrease by rank and are higher for topics that have more relevant documents.",
                "After finding the Θ that maximizes the likelihood, we have q∗ ij = exp(θrj (i)) 1+exp(θrj (i)) .",
                "We define θ∞ = −∞, so that the probability that an unranked document is relevant is 0.",
                "Since q∗ ij is based on the rank at which a document is retrieved rather than the identity of the document itself, the probabilities are identical from expert to expert, e.g. if expert E put document A at rank 1, and expert D put document B at rank 1, we will have q∗ AE = q∗ BD.",
                "Therefore we only have to solve this once for each topic.",
                "The above model gives topic-independent probabilities for each document.",
                "But suppose an expert who reports 90% probability is only right 50% of the time.",
                "Its opinion should be discounted based on its observed performance.",
                "Specifically, we want to learn a calibration function qij = Cj(q∗ ij) that will ensure that the predicted probabilities are tuned to the experts ability to retrieve relevant documents given the judgments that have been made to this point.",
                "Platts SVM calibration method [16] fits a sigmoid function between q∗ ij and the relevance judgments to obtain qij = Cj (q∗ ij) = exp(Aj +Bjq∗ ij ) 1+exp(Aj +Bj q∗ ij ) .",
                "Since q∗ ij is topic-independent, we only need to learn one calibration function for each expert.",
                "Once we have the calibration function, it is applied to adjust the experts predictions to their actual performance.",
                "The calibrated probabilities are plugged into model (2) to find the document probabilities.",
                "Figure 1: Conceptual diagram of our aggregation model.",
                "Experts E1, E2 have ranked documents A, B, C for topic T1 and documents D, E, F for topic T2.",
                "The first step is to obtain q∗ ij.",
                "Next is calibration to true performance to find qij .",
                "Finally we obtain pi = p(Xi = 1|qi1, qi2), · · · . 3.3 Model Summary Our model has three components that differ by the data they take as input and what they produce as output.",
                "A conceptual diagram is shown in Figure 1. 1. ranks → probabilities (per system per topic).",
                "This gives us q∗ ij, expert js self-reported probability of the relevance of document i.",
                "This is unsupervised; it requires no labeled data (though if we have some, we use it to set prior parameters). 2. probabilities → calibrated probabilities (per system).",
                "This gives us qij = Cj (q∗ ij), expert js calibrated probability of the relevance of document i.",
                "This is semisupervised; we have relevance judgments at some ranks which we use to impute the probability of relevance at other ranks. 3. calibrated probabilities → document probabilities.",
                "This gives us pi = p(Xi = 1|qi), the probability of relevance of document i given calibrated expert probabilities qij .",
                "This is supervised; we learn coefficients from a set of judged documents and use those to estimate the relevance of the unjudged documents.",
                "Although the model appears rather complex, it is really just three successive applications of logistic regression.",
                "As such, it can be implemented in a statistical programming language such as R in a few lines of code.",
                "The use of beta (conjugate) priors ensures that no expensive computational methods such as MCMC are necessary [12], so the model is trained and applied fast enough to be used on-line.",
                "Our code is available at http://ciir.cs.umass.edu/~carteret/. 4.",
                "EXPERIMENTS Three hypotheses are under consideration.",
                "The first, and most important, is that using our expert aggregation model to predict relevance produces test collections that are robust enough to be reusable; that is, we can trust the estimates of confidence when we evaluate systems that did not contribute any judgments to the pool.",
                "The other two hypotheses relate to the improvement we see by using better estimates of relevance than we did in our previous work [8].",
                "These are that (a) it takes fewer relevance track no. topics no. runs no. judged no. rel ad-hoc 94 50 40 97,319 9,805 ad-hoc 95 49 33 87,069 6,503 ad-hoc 96 50 61 133,681 5,524 ad-hoc 97 50 74 72,270 4,611 ad-hoc 98 50 103 80,345 4,674 ad-hoc 99 50 129 86,830 4,728 web 04 225 74 88,566 1,763 robust 05 50 74 37,798 6,561 terabyte 05 50 58 45,291 10,407 Table 1: Number of topics, number of runs, number of documents judged, and number found relevant for each of our data sets. judgments to reach 95% confidence and (b) the accuracy of the predictions is higher than if we were to simply assume pi = 0.5 for all unjudged documents. 4.1 Data We obtained full ad-hoc runs submitted to TRECs 3 through 8.",
                "Each run ranks at most 1000 documents for 50 topics (49 topics for TREC 4).",
                "Additionally, we obtained all runs from the Web track of TREC 13, the Robust2 track of TREC 14, and the Terabyte (ad-hoc) track of TREC 14.",
                "These are the tracks that have replaced the ad-hoc track since its end in 1999.",
                "Statistics are shown in Table 1.",
                "We set aside the TREC 4 (ad-hoc 95) set for training, TRECs 3 and 5-8 (ad-hoc 94 and 96-99) for primary testing, and the remaining sets for additional testing.",
                "We use the qrels files assembled by NIST as truth.",
                "The number of relevance judgments made and relevant documents found for each track are listed in Table 1.",
                "For computational reasons, we truncate ranked lists at 100 documents.",
                "There is no reason that we could not go deeper, but calculating variance is O(n3 ) and thus very timeconsuming.",
                "Because of the reciprocal rank nature of AP, we do not lose much information by truncating at rank 100. 4.2 Algorithms We will compare three algorithms for acquiring relevance judgments.",
                "The baseline is a variation of TREC pooling that we will call incremental pooling (IP).",
                "This algorithm takes a number k as input and presents the first k documents in rank order (without regard to topic) to be judged.",
                "It does not estimate the relevance of unjudged documents; it simply assumes any unjudged document is nonrelevant.",
                "The second algorithm is that presented in Carterette et al. [8] (Algorithm 1).",
                "Documents are selected based on how interesting they are in determining whether a difference in mean average precision exists.",
                "For this approach pi = 0.5 for all i; there is no estimation of probabilities.",
                "We will call this MTC for minimal test collection.",
                "The third algorithm augments MTC with updated estimates of probabilities of relevance.",
                "We will call this RTC for robust test collection.",
                "It is identical to Algorithm 1, except that every 10th iteration we estimate pi for all unjudged documents i using the expert aggregation model of Section 3.",
                "RTC has smoothing (prior distribution) parameters that must be set.",
                "We trained using the ad-hoc 95 set.",
                "We limited 2 Robust here means robust retrieval; this is different from our goal of robust <br>evaluation</br>.",
                "Algorithm 1 (MTC) Given two ranked lists and confidence level α, predict the sign of ΔMAP. 1: pi ← 0.5 for all documents i 2: while P(ΔMAP < 0) < α do 3: calculate weight wi for all unjudged documents i (see Carterette et al. [8] for details) 4: j ← argmaxiwi 5: xj ← 1 if document j is relevant, 0 otherwise 6: pj ← xj 7: end while the search to uniform priors with relatively high variance.",
                "For expert aggregation, the prior parameters are α = β = 1. 4.3 Experimental Design First, we want to know whether we can augment a set of relevance judgments with a set of relevance probabilities in order to reuse the judgments to evaluate a new set of systems.",
                "For each experimental trial: 1.",
                "Pick a random subset of k runs. 2.",
                "From those k, pick an initial c < k to evaluate. 3.",
                "Run RTC to 95% confidence on the initial c. 4.",
                "Using the model from Section 3, estimate the probabilities of relevance for all documents retrieved by all k runs. 5.",
                "Calculate EMAP for all k runs, and P(ΔMAP < 0) for all pairs of runs.",
                "We do the same for MTC, but omit step 4.",
                "Note that after evaluating the first c systems, we make no additional relevance judgments.",
                "To put our method to the test, we selected c = 2: we will build a set of judgments from evaluating only two initial systems.",
                "We will then generalize to a set of k = 10 (of which those two are a subset).",
                "As we run more trials, we obtain the data we need to test all three of our hypotheses. 4.4 Experimental <br>evaluation</br> Recall that a set of judgments is robust if the accuracy of the predictions it makes is at least its estimated confidence.",
                "One way to evaluate robustness is to bin pairs by their confidence, then calculate the accuracy over all the pairs in each bin.",
                "We would like the accuracy to be no less than the lowest confidence score in the bin, but preferably higher.",
                "Since summary statistics are useful, we devised the following metric.",
                "Suppose we are a bookmaker taking bets on whether ΔMAP < 0.",
                "We use RTC or MTC to set the odds O = P (ΔMAP <0) 1−P (ΔMAP <0) .",
                "Suppose a bettor wagers $1 on ΔMAP ≥ 0.",
                "If it turns out that ΔMAP < 0, we win the dollar.",
                "Otherwise, we pay out O.",
                "If our confidence estimates are perfectly accurate, we break even.",
                "If confidence is greater than accuracy, we lose money; we win if accuracy is greater than confidence.",
                "Counterintuitively, the most desirable outcome is breaking even: if we lose money, we cannot trust the confidence estimates, but if we win money, we have either underestimated confidence or judged more documents than necessary.",
                "However, the cost of not being able to trust the confidence estimates is higher than the cost of extra relevance judgments, so we will treat positive outcomes as good.",
                "The amount we win on each pairwise comparison i is: Wi = yi − (1 − yi) Pi 1 − Pi = yi − Pi 1 − Pi yi = 1 if ΔMAP < 0 and 0 otherwise, and Pi = P(ΔMAP < 0).",
                "The summary statistic is W, the mean of Wi.",
                "Note that as Pi increases, we lose more for being wrong.",
                "This is as it should be: the penalty should be great for missing the high probability predictions.",
                "However, since our losses grow without bound as predictions approach certainty, we cap −Wi at 100.",
                "For our hypothesis that RTC requires fewer judgments than MTC, we are interested in the number of judgments needed to reach 95% confidence on the first pair of systems.",
                "The median is more interesting than the mean: most pairs require a few hundred judgments, but a few pairs require several thousand.",
                "The distribution is therefore highly skewed, and the mean strongly affected by those outliers.",
                "Finally, for our hypothesis that RTC is more accurate than MTC, we will look at Kendalls τ correlation between a ranking of k systems by a small set of judgments and the true ranking using the full set of judgments.",
                "Kendalls τ, a nonparametric statistic based on pairwise swaps between two lists, is a standard <br>evaluation</br> for this type of study.",
                "It ranges from −1 (perfectly anti-correlated) to 1 (rankings identical), with 0 meaning that half of the pairs are swapped.",
                "As we touched on in the introduction, though, an accuracy measure like rank correlation is not a good <br>evaluation</br> of reusability.",
                "We include it for completeness. 4.4.1 Hypothesis Testing Running multiple trials allows the use of statistical hypothesis testing to compare algorithms.",
                "Using the same sets of systems allows the use of paired tests.",
                "As we stated above, we are more interested in the median number of judgments than the mean.",
                "A test for difference in median is the Wilcoxon sign rank test.",
                "We can also use a paired t-test to test for a difference in mean.",
                "For rank correlation, we can use a paired t-test to test for a difference in τ. 5.",
                "RESULTS AND ANALYSIS The comparison between MTC and RTC is shown in Table 2.",
                "With MTC and uniform probabilities of relevance, the results are far from robust.",
                "We cannot reuse the relevance judgments with much confidence.",
                "But with RTC, the results are very robust.",
                "There is a slight dip in accuracy when confidence gets above 0.95; nonetheless, the confidence predictions are trustworthy.",
                "Mean Wi shows that RTC is much closer to 0 than MTC.",
                "The distribution of confidence scores shows that at least 80% confidence is achieved more than 35% of the time, indicating that neither algorithm is being too conservative in its confidence estimates.",
                "The confidence estimates are rather low overall; that is because we have built a test collection from only two initial systems.",
                "Recall from Section 1 that we cannot require (or even expect) a minimum level of confidence when we generalize to new systems.",
                "More detailed results for both algorithms are shown in Figure 2.",
                "The solid line is the ideal result that would give W = 0.",
                "RTC is on or above this line at all points until confidence reaches about 0.97.",
                "After that there is a slight dip in accuracy which we discuss below.",
                "Note that both MTC RTC confidence % in bin accuracy % in bin accuracy 0.5 − 0.6 33.7% 61.7% 28.6% 61.9% 0.6 − 0.7 18.1% 73.1% 20.1% 76.3% 0.7 − 0.8 10.4% 70.1% 15.5% 78.0% 0.8 − 0.9 9.4% 69.0% 12.1% 84.9% 0.9 − 0.95 7.3% 78.0% 6.6% 93.1% 0.95 − 0.99 17.9% 70.4% 12.4% 93.4% 1.0 3.3% 68.3% 4.7% 98.9% W −5.34 −0.39 median judged 251 235 mean τ 0.393 0.555 Table 2: Confidence that P(ΔMAP < 0) and accuracy of prediction when generalizing a set of relevance judgments acquired using MTC and RTC.",
                "Each bin contains over 1,000 trials from the adhoc 3, 5-8 sets.",
                "RTC is much more robust than MTC.",
                "W is defined in Section 4.4; closer to 0 is better.",
                "Median judged is the number of judgments to reach 95% confidence on the first two systems.",
                "Mean τ is the average rank correlation for all 10 systems. 0.5 0.6 0.7 0.8 0.9 1 0.5 0.6 0.7 0.8 0.9 1 accuracy confidence breakeven RTC MTC Figure 2: Confidence vs. accuracy of MTC and RTC.",
                "The solid line is the perfect result that would give W = 0; performance should be on or above this line.",
                "Each point represents at least 500 pairwise comparisons. algorithms are well above the line up to around confidence 0.7.",
                "This is because the baseline performance on these data sets is high; it is quite easy to achieve 75% accuracy doing very little work [7].",
                "Number of Judgments: The median number of judgments required by MTC to reach 95% confidence on the first two systems is 251, an average of 5 per topic.",
                "The median required by RTC is 235, about 4.7 per topic.",
                "Although the numbers are close, RTCs median is significantly lower by a paired Wilcoxon test (p < 0.0001).",
                "For comparison, a pool of depth 100 would result in a minimum of 5,000 judgments for each pair.",
                "The difference in means is much greater.",
                "MTC required a mean of 823 judgments, 16 per topic, while RTC required a mean of 502, 10 per topic. (Recall that means are strongly skewed by a few pairs that take thousands of judgments.)",
                "This difference is significant by a paired t-test (p < 0.0001).",
                "Ten percent of the sets resulted in 100 or fewer judgments (less than two per topic).",
                "Performance on these is very high: W = 0.41, and 99.7% accuracy when confidence is at least 0.9.",
                "This shows that even tiny collections can be reusable.",
                "For the 50% of sets with more than 235 judgments, accuracy is 93% when confidence is at least 0.9.",
                "Rank Correlation: MTC and RTC both rank the 10 systems by EMAP (Eq. (1)) calculated using their respective probability estimates.",
                "The mean τ rank correlation between true MAP and EMAP is 0.393 for MTC and 0.555 for RTC.",
                "This difference is significant by a paired t-test (p < 0.0001).",
                "Note that we do not expect the τ correlations to be high, since we are ranking the systems with so few relevance judgments.",
                "It is more important that we estimate confidence in each pairwise comparison correctly.",
                "We ran IP for the same number of judgments that MTC took for each pair, then ranked the systems by MAP using only those judgments (all unjudged documents assumed nonrelevant).",
                "We calculated the τ correlation to the true ranking.",
                "The mean τ correlation is 0.398, which is not significantly different from MTC, but is significantly lower than RTC.",
                "Using uniform estimates of probability is indistinguishable from the baseline, whereas estimating relevance by expert aggregation boosts performance a great deal: nearly 40% over both MTC and IP.",
                "Overfitting: It is possible to overfit: if too many judgments come from the first two systems, the variance in ΔMAP is reduced and the confidence estimates become unreliable.",
                "We saw this in Table 2 and Figure 2 where RTC exhibits a dip in accuracy when confidence is around 97%.",
                "In fact, the number of judgments made prior to a wrong prediction is over 50% greater than the number made prior to a correct prediction.",
                "Overfitting is difficult to quantify exactly, because making more relevance judgments does not always cause it: at higher confidence levels, more relevance judgments are made, and as Table 2 shows, accuracy is greater at those higher confidences.",
                "Obviously having more relevance judgments should increase both confidence and accuracy; the difference seems to be when one system has a great deal more judgments than the other.",
                "Pairwise Comparisons: Our pairwise comparisons fall into one of three groups: 1. the two original runs from which relevance judgments are acquired; 2. one of the original runs vs. one of the new runs; 3. two new runs.",
                "Table 3 shows confidence vs. accuracy results for each of these three groups.",
                "Interestingly, performance is worst when comparing one of the original runs to one of the additional runs.",
                "This is most likely due to a large difference in the number of judgments affecting the variance of ΔMAP.",
                "Nevertheless, performance is quite good on all three subsets.",
                "Worst Case: The case intuitively most likely to produce an error is when the two systems being compared have retrieved very few documents in common.",
                "If we want the judgments to be reusable, we should to be able to generalize even to runs that are very different from the ones used to acquire the relevance judgments.",
                "A simple measure of similarity of two runs is the average percentage of documents they retrieved in common for each topic [2].",
                "We calculated this for all pairs, then looked at performance on pairs with low similarity.",
                "Results are shown in accuracy confidence two original one original no original 0.5 − 0.6 - 48.1% 62.8% 0.6 − 0.7 - 57.1% 79.2% 0.7 − 0.8 - 67.9% 81.7% 0.8 − 0.9 - 82.2% 86.3% 0.9 − 0.95 95.9% 93.7% 92.6% 0.95 − 0.99 96.2% 92.5% 93.1% 1.0 100% 98.0% 99.1% W −1.11 −0.87 −0.27 Table 3: Confidence vs. accuracy of RTC when comparing the two original runs, one original run and one new run, and two new runs.",
                "RTC is robust in all three cases. accuracy when similar confidence 0 − 0.1 0.1 − 0.2 0.2 − 0.3 0.5 − 0.6 68.4% 63.1% 61.4% 0.6 − 0.7 84.2% 78.6% 76.6% 0.7 − 0.8 82.0% 79.8% 78.9% 0.8 − 0.9 93.6% 83.3% 82.1% 0.9 − 0.95 99.3% 92.7% 92.4% 0.95 − 0.99 98.7% 93.4% 93.3% 1.0 99.9% 97.9% 98.1% W 0.44 −0.45 −0.49 Table 4: Confidence vs. accuracy of RTC when a pair of systems retrieved 0-30% documents in common (broken out into 0%-10%, 10%-20%, and 20%30%).",
                "RTC is robust in all three cases.",
                "Table 4.",
                "Performance is in fact very robust even when similarity is low.",
                "When the two runs share very few documents in common, W is actually positive.",
                "MTC and IP both performed quite poorly in these cases.",
                "When the similarity was between 0 and 10%, both MTC and IP correctly predicted ΔMAP only 60% of the time, compared to an 87.6% success rate for RTC.",
                "By Data Set: All the previous results have only been on the ad-hoc collections.",
                "We did the same experiments on our additional data sets, and broke out the results by data set to see how performance varies.",
                "The results in Table 5 show everything about each set, including binned accuracy, W, mean τ, and median number of judgments to reach 95% confidence on the first two systems.",
                "The results are highly consistent from collection to collection, suggesting that our method is not overfitting to any particular data set. 6.",
                "CONCLUSIONS AND FUTURE WORK In this work we have offered the first formal definition of the common idea of reusability of a test collection and presented a model that is able to achieve reusability with very small sets of relevance judgments.",
                "Table 2 and Figure 2 together show how biased a small set of judgments can be: MTC is dramatically overestimating confidence and is much less accurate than RTC, which is able to remove the bias to give a robust <br>evaluation</br>.",
                "The confidence estimates of RTC, in addition to being accurate, provide a guide for obtaining additional judgments: focus on judging documents from the lowest-confidence comparisons.",
                "In the long run, we see small sets of relevance judgaccuracy confidence ad-hoc 94 ad-hoc 96 ad-hoc 97 ad-hoc 98 ad-hoc 99 web 04 robust 05 terabyte 05 0.5 − 0.6 64.1% 61.8% 62.2% 62.0% 59.4% 64.3% 61.5% 61.6% 0.6 − 0.7 76.1% 77.8% 74.5% 78.2% 74.3% 78.1% 75.9% 75.9% 0.7 − 0.8 75.2% 78.9% 77.6% 80.0% 78.6% 82.6% 77.5% 80.4% 0.8 − 0.9 83.2% 85.5% 84.6% 84.9% 86.8% 84.5% 86.7% 87.7% 0.9 − 0.95 93.0% 93.6% 92.8% 93.7% 92.6% 94.2% 93.9% 94.2% 0.95 − 0.99 93.1% 94.3% 93.1% 93.7% 92.8% 95.0% 93.9% 91.6% 1.0 99.2% 96.8% 98.7% 99.5% 99.6% 100% 99.2% 98.3% W -0.34 -0.34 -0.48 -0.35 -0.44 -0.07 -0.41 -0.67 median judged 235 276 243 213 179 448 310 320 mean τ 0.538 0.573 0.556 0.579 0.532 0.596 0.565 0.574 Table 5: Accuracy, W, mean τ, and median number of judgments for all 8 testing sets.",
                "The results are highly consistent across data sets. ments being shared by researchers, each group contributing a few more judgments to gain more confidence about their particular systems.",
                "As time goes on, the number of judgments grows until there is 100% confidence in every <br>evaluation</br>-and there is a full test collection for the task.",
                "We see further use for this method in scenarios such as web retrieval in which the corpus is frequently changing.",
                "It could be applied to <br>evaluation</br> on a dynamic test collection as defined by Soboroff [18].",
                "The model we presented in Section 3 is by no means the only possibility for creating a robust test collection.",
                "A simpler expert aggregation model might perform as well or better (though all our efforts to simplify failed).",
                "In addition to expert aggregation, we could estimate probabilities by looking at similarities between documents.",
                "This is an obvious area for future exploration.",
                "Additionally, it will be worthwhile to investigate the issue of overfitting: the circumstances it occurs under and what can be done to prevent it.",
                "In the meantime, capping confidence estimates at 95% is a hack that solves the problem.",
                "We have many more experimental results that we unfortunately did not have space for but that reinforce the notion that RTC is highly robust: with just a few judgments per topic, we can accurately assess the confidence in any pairwise comparison of systems.",
                "Acknowledgments This work was supported in part by the Center for Intelligent Information Retrieval and in part by the Defense Advanced Research Projects Agency (DARPA) under contract number HR0011-06-C-0023.",
                "Any opinions, findings, and conclusions or recommendations expressed in this material are those of the author and do not necessarily reflect those of the sponsor. 7.",
                "REFERENCES [1] J. Aslam and M. Montague.",
                "Models for Metasearch.",
                "In Proceedings of SIGIR, pages 275-285, 2001. [2] J. Aslam and R. Savell.",
                "On the effectiveness of evaluating retrieval systems in the absence of relevance judgments.",
                "In Proceedings of SIGIR, pages 361-362, 2003. [3] J.",
                "A. Aslam, V. Pavlu, and R. Savell.",
                "A unified model for metasearch, pooling, and system <br>evaluation</br>.",
                "In Proceedings of CIKM, pages 484-491, 2003. [4] J.",
                "A. Aslam, V. Pavlu, and E. Yilmaz.",
                "A statistical method for system <br>evaluation</br> using incomplete judgments.",
                "In Proceedings of SIGIR, pages 541-548, 2006. [5] A. L. Berger, S. D. Pietra, and V. J. D. Pietra.",
                "A maximum entropy approach to natural language processing.",
                "Computational Linguistics, 22(1):39-71, 1996. [6] D. J. Blower.",
                "An easy derivation of logistic regression from the bayesian and maximum entropy perspective.",
                "In Proceedings of the 23rd International Workship on Bayesian Inference and Maximum Entropy Methods in Science and Engineering, pages 30-43, 2004. [7] B. Carterette and J. Allan.",
                "Research methodology in studies of assessor effort for retrieval <br>evaluation</br>.",
                "In Proceedings of RIAO, 2007. [8] B. Carterette, J. Allan, and R. K. Sitaraman.",
                "Minimal test collections for retrieval <br>evaluation</br>.",
                "In Proceedings of SIGIR, pages 268-275, 2006. [9] B. Carterette and D. I. Petkova.",
                "Learning a ranking from pairwise preferences.",
                "In Proceedings of SIGIR, 2006. [10] R. T. Clemen and R. L. Winkler.",
                "Unanimity and compromise among probability forecasters.",
                "Management Science, 36(7):767-779, July 1990. [11] G. V. Cormack, C. R. Palmer, and C. L. Clarke.",
                "Efficient Construction of Large Test Collections.",
                "In Proceedings of SIGIR, pages 282-289, 1998. [12] A. Gelman, J.",
                "B. Carlin, H. S. Stern, and D. B. Rubin.",
                "Bayesian Data Analysis.",
                "Chapman & Hall/CRC, 2004. [13] E. T. Jaynes.",
                "Probability Theory: The Logic of Science.",
                "Cambridge University Press, 2003. [14] R. Manmatha and H. Sever.",
                "A Formal Approach to Score Normalization for Metasearch.",
                "In Proceedings of HLT, pages 88-93, 2002. [15] I. J. Myung, S. Ramamoorti, and J. Andrew D. Baily.",
                "Maximum entropy aggregation of expert predictions.",
                "Management Science, 42(10):1420-1436, October 1996. [16] J. Platt.",
                "Probabilistic outputs for support vector machines and comparison to regularized likelihood methods. pages 61-74, 2000. [17] M. Sanderson and H. Joho.",
                "Forming test collections with no system pooling.",
                "In Proceedings of the 27th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 33-40, 2004. [18] I. Soboroff.",
                "Dynamic test collections: measuring search effectiveness on the live web.",
                "In Proceedings of SIGIR, pages 276-283, 2006. [19] K. Sparck Jones and C. J. van Rijsbergen.",
                "Information Retrieval Test Collections.",
                "Journal of Documentation, 32(1):59-75, 1976. [20] E. M. Voorhees and D. K. Harman, editors.",
                "TREC: Experiment and <br>evaluation</br> in Information Retrieval.",
                "MIT Press, 2005. [21] J. Zobel.",
                "How Reliable are the Results of Large-Scale Information Retrieval Experiments?",
                "In Proceedings of SIGIR, pages 307-314, 1998."
            ],
            "original_annotated_samples": [
                "Robust Test Collections for Retrieval <br>evaluation</br> Ben Carterette Center for Intelligent Information Retrieval Computer Science Department University of Massachusetts Amherst Amherst, MA 01003 carteret@cs.umass.edu ABSTRACT Low-cost methods for acquiring relevance judgments can be a boon to researchers who need to evaluate new retrieval tasks or topics but do not have the resources to make thousands of judgments.",
                "While these judgments are very useful for a one-time <br>evaluation</br>, it is not clear that they can be trusted when re-used to evaluate new systems.",
                "In this work, we formally define what it means for judgments to be reusable: the confidence in an <br>evaluation</br> of new systems can be accurately assessed from an existing set of relevance judgments.",
                "Even the smallest sets of judgments can be useful for <br>evaluation</br> of new systems.",
                "Categories and Subject Descriptors: H.3 Information Storage and Retrieval; H.3.4 Systems and Software: Performance <br>evaluation</br> General Terms: Experimentation, Measurement, Reliability 1."
            ],
            "translated_annotated_samples": [
                "Colecciones de pruebas robustas para la <br>evaluación</br> de recuperación Ben Carterette Centro de Recuperación de Información Inteligente Departamento de Ciencias de la Computación Universidad de Massachusetts Amherst Amherst, MA 01003 carteret@cs.umass.edu RESUMEN Los métodos de bajo costo para adquirir juicios de relevancia pueden ser una bendición para los investigadores que necesitan evaluar nuevas tareas o temas de recuperación pero no tienen los recursos para hacer miles de juicios.",
                "Si bien estos juicios son muy útiles para una <br>evaluación</br> única, no está claro que se puedan confiar al ser reutilizados para evaluar nuevos sistemas.",
                "En este trabajo, definimos formalmente lo que significa que las evaluaciones sean reutilizables: la confianza en una <br>evaluación</br> de nuevos sistemas puede ser evaluada con precisión a partir de un conjunto existente de juicios de relevancia.",
                "Incluso los conjuntos más pequeños de juicios pueden ser útiles para la <br>evaluación</br> de nuevos sistemas.",
                "Categorías y Descriptores de Asignaturas: H.3 Almacenamiento y Recuperación de Información; H.3.4 Sistemas y Software: Evaluación del Rendimiento Términos Generales: Experimentación, Medición, Confiabilidad 1."
            ],
            "translated_text": "Colecciones de pruebas robustas para la <br>evaluación</br> de recuperación Ben Carterette Centro de Recuperación de Información Inteligente Departamento de Ciencias de la Computación Universidad de Massachusetts Amherst Amherst, MA 01003 carteret@cs.umass.edu RESUMEN Los métodos de bajo costo para adquirir juicios de relevancia pueden ser una bendición para los investigadores que necesitan evaluar nuevas tareas o temas de recuperación pero no tienen los recursos para hacer miles de juicios. Si bien estos juicios son muy útiles para una <br>evaluación</br> única, no está claro que se puedan confiar al ser reutilizados para evaluar nuevos sistemas. En este trabajo, definimos formalmente lo que significa que las evaluaciones sean reutilizables: la confianza en una <br>evaluación</br> de nuevos sistemas puede ser evaluada con precisión a partir de un conjunto existente de juicios de relevancia. Luego presentamos un método para aumentar un conjunto de juicios de relevancia con estimaciones de relevancia que no requieren esfuerzo adicional del evaluador. Usar este método garantiza prácticamente la reutilización: con tan solo cinco juicios por tema tomados de solo dos sistemas, podemos evaluar de manera confiable un conjunto más grande de diez sistemas. Incluso los conjuntos más pequeños de juicios pueden ser útiles para la <br>evaluación</br> de nuevos sistemas. Categorías y Descriptores de Asignaturas: H.3 Almacenamiento y Recuperación de Información; H.3.4 Sistemas y Software: Evaluación del Rendimiento Términos Generales: Experimentación, Medición, Confiabilidad 1. ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "relevance judgement": {
            "translated_key": "juicio de relevancia",
            "is_in_text": false,
            "original_annotated_sentences": [
                "Robust Test Collections for Retrieval Evaluation Ben Carterette Center for Intelligent Information Retrieval Computer Science Department University of Massachusetts Amherst Amherst, MA 01003 carteret@cs.umass.edu ABSTRACT Low-cost methods for acquiring relevance judgments can be a boon to researchers who need to evaluate new retrieval tasks or topics but do not have the resources to make thousands of judgments.",
                "While these judgments are very useful for a one-time evaluation, it is not clear that they can be trusted when re-used to evaluate new systems.",
                "In this work, we formally define what it means for judgments to be reusable: the confidence in an evaluation of new systems can be accurately assessed from an existing set of relevance judgments.",
                "We then present a method for augmenting a set of relevance judgments with relevance estimates that require no additional assessor effort.",
                "Using this method practically guarantees reusability: with as few as five judgments per topic taken from only two systems, we can reliably evaluate a larger set of ten systems.",
                "Even the smallest sets of judgments can be useful for evaluation of new systems.",
                "Categories and Subject Descriptors: H.3 Information Storage and Retrieval; H.3.4 Systems and Software: Performance Evaluation General Terms: Experimentation, Measurement, Reliability 1.",
                "INTRODUCTION Consider an information retrieval researcher who has invented a new retrieval task.",
                "She has built a system to perform the task and wants to evaluate it.",
                "Since the task is new, it is unlikely that there are any extant relevance judgments.",
                "She does not have the time or resources to judge every document, or even every retrieved document.",
                "She can only judge the documents that seem to be the most informative and stop when she has a reasonable degree of confidence in her conclusions.",
                "But what happens when she develops a new system and needs to evaluate it?",
                "Or another research group decides to implement a system to perform the task?",
                "Can they reliably reuse the original judgments?",
                "Can they evaluate without more relevance judgments?",
                "Evaluation is an important aspect of information retrieval research, but it is only a semi-solved problem: for most retrieval tasks, it is impossible to judge the relevance of every document; there are simply too many of them.",
                "The solution used by NIST at TREC (Text REtrieval Conference) is the pooling method [19, 20]: all competing systems contribute N documents to a pool, and every document in that pool is judged.",
                "This method creates large sets of judgments that are reusable for training or evaluating new systems that did not contribute to the pool [21].",
                "This solution is not adequate for our hypothetical researcher.",
                "The pooling method gives thousands of relevance judgments, but it requires many hours of (paid) annotator time.",
                "As a result, there have been a slew of recent papers on reducing annotator effort in producing test collections: Cormack et al. [11], Zobel [21], Sanderson and Joho [17], Carterette et al. [8], and Aslam et al. [4], among others.",
                "As we will see, the judgments these methods produce can significantly bias the evaluation of a new set of systems.",
                "Returning to our hypothetical resesarcher, can she reuse her relevance judgments?",
                "First we must formally define what it means to be reusable.",
                "In previous work, reusability has been tested by simply assessing the accuracy of a set of relevance judgments at evaluating unseen systems.",
                "While we can say that it was right 75% of the time, or that it had a rank correlation of 0.8, these numbers do not have any predictive power: they do not tell us which systems are likely to be wrong or how confident we should be in any one.",
                "We need a more careful definition of reusability.",
                "Specifically, the question of reusability is not how accurately we can evaluate new systems.",
                "A malicious adversary can always produce a new ranked list that has not retrieved any of the judged documents.",
                "The real question is how much confidence we have in our evaluations, and, more importantly, whether we can trust our estimates of confidence.",
                "Even if confidence is not high, as long as we can trust it, we can identify which systems need more judgments in order to increase confidence.",
                "Any set of judgments, no matter how small, becomes reusable to some degree.",
                "Small, reusable test collections could have a huge impact on information retrieval research.",
                "Research groups would be able to share the relevance judgments they have done in-house for pilot studies, new tasks, or new topics.",
                "The amount of data available to researchers would grow exponentially over time. 2.",
                "ROBUST EVALUATION Above we gave an intuitive definition of reusability: a collection is reusable if we can trust our estimates of confidence in an evaluation.",
                "By that we mean that if we have made some relevance judgments and have, for example, 75% confidence that system A is better than system B, we would like there to be no more than 25% chance that our assessment of the relative quality of the systems will change as we continue to judge documents.",
                "Our evaluation should be robust to missing judgments.",
                "In our previous work, we defined confidence as the probability that the difference in an evaluation measure calculated for two systems is less than zero [8].",
                "This notion of confidence is defined in the context of a particular evaluation task that we call comparative evaluation: determining the sign of the difference in an evaluation measure.",
                "Other evaluation tasks could be defined; estimating the magnitude of the difference or the values of the measures themselves are examples that entail different notions of confidence.",
                "We therefore see confidence as a probability estimate.",
                "One of the questions we must ask about a probability estimate is what it means.",
                "What does it mean to have 75% confidence that system A is better than system B?",
                "As described above, we want it to mean that if we continue to judge documents, there will only be a 25% chance that our assessment will change.",
                "If this is what it means, we can trust the confidence estimates.",
                "But do we know it has that meaning?",
                "Our calculation of confidence rested on an assumption about the probability of relevance of unjudged documents, specifically that each unjudged document was equally likely to be relevant or nonrelevant.",
                "This assumption is almost certainly not realistic in most IR applications.",
                "As it turns out, it is this assumption that determines whether the confidence estimates can eb trusted.",
                "Before elaborating on this, we formally define confidence. 2.1 Estimating Confidence Average precision (AP) is a standard evaluation metric that captures both the ability of a system to rank relevant documents highly (precision) as well as its ability to retrieve relevant documents (recall).",
                "It is typically written as the mean precision at the ranks of relevant documents: AP = 1 |R| i∈R prec@r(i) where R is the set of relevant documents and r(i) is the rank of document i.",
                "Let Xi be a random variable indicating the relevance of document i.",
                "If documents are ordered by rank, we can express precision as prec@i = 1/i i j=1 Xj .",
                "Average precision then becomes the quadratic equation AP = 1 Xi n i=1 Xi/i i j=1 Xj = 1 Xi n i=1 j≥i aijXiXj where aij = 1/ max{r(i), r(j)}.",
                "Using aij instead of 1/i allows us to number the documents arbitrarily.",
                "To see why this is true, consider a toy example: a list of 3 documents with relevant documents B, C at ranks 1 and 3 and nonrelevant document A at rank 2.",
                "Average precision will be 1 2 (1 1 x2 B+ 1 2 xBxA+ 1 3 xBxC + 1 2 x2 A+ 1 3 xAxC + 1 3 x2 C) = 1 2 1 + 2 3 because xA = 0, xB = 1, xC = 1.",
                "Though the ordering B, A, C is different from the labeling A, B, C, it does not affect the computation.",
                "We can now see average precision itself is a random variable with a distribution over all possible assignments of relevance to all documents.",
                "This random variable has an expectation, a variance, confidence intervals, and a certain probability of being less than or equal to a given value.",
                "All of these are dependent on the probability that document i is relevant: pi = p(Xi = 1).",
                "Suppose in our previous example we do not know the relevance judgments, but we believe pA = 0.4, pB = 0.8, pC = 0.7.",
                "We can then compute e.g.",
                "P(AP = 0) = 0.2 · 0.6 · 0.3 = 0.036, or P(AP = 1 2 ) = 0.2 · 0.4 · 0.7 = 0.056.",
                "Summing over all possibilities, we can compute expectation and variance: E[AP] ≈ 1 pi aiipi + j>i aij pipj V ar[AP] ≈ 1 ( pi)2 n i a2 iipiqi + j>i a2 ijpipj(1 − pipj) + i=j 2aiiaijpipj(1 − pi) + k>j=i 2aijaikpipjpk(1 − pi) AP asymptotically converges to a normal distribution with expectation and variance as defined above.1 For our comparative evaluation task we are interested in the sign of the difference in two average precisions: ΔAP = AP1 − AP2.",
                "As we showed in our previous work, ΔAP has a closed form when documents are ordered arbitrarily: ΔAP = 1 Xi n i=1 j≥i cij XiXj cij = aij − bij where bij is defined analogously to aij for the second ranking.",
                "Since AP is normal, ΔAP is normal as well, meaning we can use the normal cumulative density function to determine the confidence that a difference in AP is less than zero.",
                "Since topics are independent, we can easily extend this to mean average precision (MAP).",
                "MAP is also normally distributed; its expectation and variance are: EMAP = 1 T t∈T E[APt] (1) VMAP = 1 T2 t∈T V ar[APt] ΔMAP = MAP1 − MAP2 Confidence can then be estimated by calculating the expectation and variance and using the normal density function to find P(ΔMAP < 0). 2.2 Confidence and Robustness Having defined confidence, we turn back to the issue of trust in confidence estimates, and show how it ties into the robustness of the collection to missing judgments. 1 These are actually approximations to the true expectation and variance, but the error is a negligible O(n2−n ).",
                "Let Z be the set of all pairs of ranked results for a common set of topics.",
                "Suppose we have a set of m relevance judgments xm = {x1, x2, ..., xm} (using small x rather than capital X to distinguish between judged and unjudged documents); these are the judgments against which we compute confidence.",
                "Let Zα be the subset of pairs in Z for which we predict that ΔMAP = −1 with confidence α given the judgments xm .",
                "For the confidence estimates to be accurate, we need at least α · |Zα| of these pairs to actually have ΔMAP = −1 after we have judged every document.",
                "If they do, we can trust the confidence estimates; our evaluation will be robust to missing judgments.",
                "If our confidence estimates are based on unrealistic assumptions, we cannot expect them to be accurate.",
                "The assumptions they are based on are the probabilities of relevance pi.",
                "We need these to be realistic.",
                "We argue that the best possible distribution of relevance p(Xi) is the one that explains all of the data (all of the observations made about the retrieval systems) while at the same time making no unwarranted assumptions.",
                "This is known as the principle of maximum entropy [13].",
                "The entropy of a random variable X with distribution p(X) is defined as H(p) = − i p(X = i) log p(X = i).",
                "This has found a wide array of uses in computer science and information retrieval.",
                "The maximum entropy distribution is the one that maximizes H. This distribution is unique and has an exponential form.",
                "The following theorem shows the utility of a maximum entropy distribution for relevance when estimating confidence.",
                "Theorem 1.",
                "If p(Xn |I, xm ) = argmaxpH(p), confidence estimates will be accurate. where xm is the set of relevance judgments defined above, Xn is the full set of documents that we wish to estimate the relevance of, and I is some information about the documents (unspecified as of now).",
                "We forgo the proof for the time being, but it is quite simple.",
                "This says that the better the estimates of relevance, the more accurate the evaluation.",
                "The task of creating a reusable test collection thus becomes the task of estimating the relevance of unjudged documents.",
                "The theorem and its proof say nothing whatsoever about the evaluation metric.",
                "The probability estimates are entirely indepedent of the measure we are interested in.",
                "This means the same probability estimates can tell us about average precision as well as precision, recall, bpref, etc.",
                "Furthermore, we could assume that the relevance of documents i and j is independent and achieve the same result, which we state as a corollary: Corollary 1.",
                "If p(Xi|I, xm ) = argmaxpH(p), confidence estimates will be accurate.",
                "The task therefore becomes the imputation of the missing values of relevance.",
                "The theorem implies that the closer we get to the maximum entropy distribution of relevance, the closer we get to robustness. 3.",
                "PREDICTING RELEVANCE In our statement of Theorem 1, we left the nature of the information I unspecified.",
                "One of the advantages of our confidence estimates is that they admit information from a wide variety of sources; essentially anything that can be modeled can be used as information for predicting relevance.",
                "A natural source of information is the retrieval systems themselves: how they ranked the judged documents, how often they failed to rank relevant documents, how they perform across topics, and so on.",
                "If we treat each system as an information retrieval expert providing an opinion about the relevance of each document, the problem becomes one of expert opinion aggregation.",
                "This is similar to the metasearch or data fusion problem in which the task is to take k input systems and merge them into a single ranking.",
                "Aslam et al. [3] previously identified a connection between evaluation and metasearch.",
                "Our problem has two key differences: 1.",
                "We explicitly need probabilities of relevance that we can plug into Eq. 1; metasearch algorithms have no such requirement. 2.",
                "We are accumulating relevance judgments as we proceed with the evaluation and are able to re-estimate relevance given each new judgment.",
                "In light of (1) above, we introduce a probabilistic model for expert combination. 3.1 A Model for Expert Opinion Aggregation Suppose that each expert j provides a probability of relevance qij = pj(Xi = 1).",
                "The information about the relevance of document i will then be the set of k expert opinions I = qi = (qi1, qi2, · · · , qik).",
                "The probability distribution we wish to find is the one that maximizes the entropy of pi = p(Xi = 1|qi).",
                "As it turns out, finding the maximum entropy model is equivalent to finding the parameters that maximize the likelihood [5].",
                "Blower [6] explicitly shows that finding the maximum entropy model for a binary variable is equivalent to solving a logistic regression.",
                "Then pi = p(Xi = 1|qi) = exp k j=1 λjqij 1 + exp k j=1 λj qij (2) where λ1, · · · , λk are the regression parameters.",
                "We include a beta prior for p(λj) with parameters α, β.",
                "This can be seen as a type of smoothing to account for the fact that the training data is highly biased.",
                "This model has the advantage of including the statistical dependence between the experts.",
                "A model of the same form was shown by Clemen & Winkler to be the best for aggregating expert probabilities [10].",
                "A similar maximumentropy-motivated approach has been used for expert aggregation [15].",
                "Aslam & Montague [1] used a similar model for metasearch, but assumed independence among experts.",
                "Where do the qij s come from?",
                "Using raw, uncalibrated scores as predictors will not work because score distributions vary too much between topics.",
                "A language modeling ranker, for instance, will typically give a much higher score to the top retrieved document for a short query than to the top retrieved document for a long query.",
                "We could train a separate predicting model for each topic, but that does not take advantage of all of the information we have: we may only have a handful of judgments for a topic, not enough to train a model to any confidence.",
                "Furthermore, it seems reasonable to assume that if an expert makes good predictions for one topic, it will make good predictions for other topics as well.",
                "We could use a hierarchical model [12], but that will not generalize to unseen topics.",
                "Instead, we will calibrate the scores of each expert individually so that scores can be compared both within topic and between topic.",
                "Thus our model takes into account not only the dependence between experts, but also the dependence between experts performances on different tasks (topics). 3.2 Calibrating Experts Each expert gives us a score and a rank for each document.",
                "We need to convert these to probabilities.",
                "A method such as the one used by Manmatha et al. [14] could be used to convert scores into probabilities of relevance.",
                "The pairwise preference method of Carterette & Petkova [9] could also be used, interpeting the ranking of one document over another as an expression of preference.",
                "Let q∗ ij be expert js self-reported probability that document i is relevant.",
                "Intuitively it seems clear that q∗ ij should decrease with rank, and it should be zero if document i is unranked (the expert did not believe it to be relevant).",
                "The pairwise preference model can handle these two requirements easily, so we will use it.",
                "Let θrj (i) be the relevance coefficient of the document at rank rj(i).",
                "We want to find the θs that maximize the likelihood function: Ljt(Θ) = rj (i)<rj (k) exp(θrj (i) − θrj (k)) 1 + exp(θrj (i) − θrj (k)) We again include a beta prior on p(θrj(i)) with parameters |Rt| + 1 and |Nt| + 1, the size of the sets of judged relevant and nonrelevant documents respectively.",
                "Using these as prior parameters ensures that the resulting probabilities will be concentrated around the ratio of relevant documents that have been discovered for topic t. This means that the probability estimates decrease by rank and are higher for topics that have more relevant documents.",
                "After finding the Θ that maximizes the likelihood, we have q∗ ij = exp(θrj (i)) 1+exp(θrj (i)) .",
                "We define θ∞ = −∞, so that the probability that an unranked document is relevant is 0.",
                "Since q∗ ij is based on the rank at which a document is retrieved rather than the identity of the document itself, the probabilities are identical from expert to expert, e.g. if expert E put document A at rank 1, and expert D put document B at rank 1, we will have q∗ AE = q∗ BD.",
                "Therefore we only have to solve this once for each topic.",
                "The above model gives topic-independent probabilities for each document.",
                "But suppose an expert who reports 90% probability is only right 50% of the time.",
                "Its opinion should be discounted based on its observed performance.",
                "Specifically, we want to learn a calibration function qij = Cj(q∗ ij) that will ensure that the predicted probabilities are tuned to the experts ability to retrieve relevant documents given the judgments that have been made to this point.",
                "Platts SVM calibration method [16] fits a sigmoid function between q∗ ij and the relevance judgments to obtain qij = Cj (q∗ ij) = exp(Aj +Bjq∗ ij ) 1+exp(Aj +Bj q∗ ij ) .",
                "Since q∗ ij is topic-independent, we only need to learn one calibration function for each expert.",
                "Once we have the calibration function, it is applied to adjust the experts predictions to their actual performance.",
                "The calibrated probabilities are plugged into model (2) to find the document probabilities.",
                "Figure 1: Conceptual diagram of our aggregation model.",
                "Experts E1, E2 have ranked documents A, B, C for topic T1 and documents D, E, F for topic T2.",
                "The first step is to obtain q∗ ij.",
                "Next is calibration to true performance to find qij .",
                "Finally we obtain pi = p(Xi = 1|qi1, qi2), · · · . 3.3 Model Summary Our model has three components that differ by the data they take as input and what they produce as output.",
                "A conceptual diagram is shown in Figure 1. 1. ranks → probabilities (per system per topic).",
                "This gives us q∗ ij, expert js self-reported probability of the relevance of document i.",
                "This is unsupervised; it requires no labeled data (though if we have some, we use it to set prior parameters). 2. probabilities → calibrated probabilities (per system).",
                "This gives us qij = Cj (q∗ ij), expert js calibrated probability of the relevance of document i.",
                "This is semisupervised; we have relevance judgments at some ranks which we use to impute the probability of relevance at other ranks. 3. calibrated probabilities → document probabilities.",
                "This gives us pi = p(Xi = 1|qi), the probability of relevance of document i given calibrated expert probabilities qij .",
                "This is supervised; we learn coefficients from a set of judged documents and use those to estimate the relevance of the unjudged documents.",
                "Although the model appears rather complex, it is really just three successive applications of logistic regression.",
                "As such, it can be implemented in a statistical programming language such as R in a few lines of code.",
                "The use of beta (conjugate) priors ensures that no expensive computational methods such as MCMC are necessary [12], so the model is trained and applied fast enough to be used on-line.",
                "Our code is available at http://ciir.cs.umass.edu/~carteret/. 4.",
                "EXPERIMENTS Three hypotheses are under consideration.",
                "The first, and most important, is that using our expert aggregation model to predict relevance produces test collections that are robust enough to be reusable; that is, we can trust the estimates of confidence when we evaluate systems that did not contribute any judgments to the pool.",
                "The other two hypotheses relate to the improvement we see by using better estimates of relevance than we did in our previous work [8].",
                "These are that (a) it takes fewer relevance track no. topics no. runs no. judged no. rel ad-hoc 94 50 40 97,319 9,805 ad-hoc 95 49 33 87,069 6,503 ad-hoc 96 50 61 133,681 5,524 ad-hoc 97 50 74 72,270 4,611 ad-hoc 98 50 103 80,345 4,674 ad-hoc 99 50 129 86,830 4,728 web 04 225 74 88,566 1,763 robust 05 50 74 37,798 6,561 terabyte 05 50 58 45,291 10,407 Table 1: Number of topics, number of runs, number of documents judged, and number found relevant for each of our data sets. judgments to reach 95% confidence and (b) the accuracy of the predictions is higher than if we were to simply assume pi = 0.5 for all unjudged documents. 4.1 Data We obtained full ad-hoc runs submitted to TRECs 3 through 8.",
                "Each run ranks at most 1000 documents for 50 topics (49 topics for TREC 4).",
                "Additionally, we obtained all runs from the Web track of TREC 13, the Robust2 track of TREC 14, and the Terabyte (ad-hoc) track of TREC 14.",
                "These are the tracks that have replaced the ad-hoc track since its end in 1999.",
                "Statistics are shown in Table 1.",
                "We set aside the TREC 4 (ad-hoc 95) set for training, TRECs 3 and 5-8 (ad-hoc 94 and 96-99) for primary testing, and the remaining sets for additional testing.",
                "We use the qrels files assembled by NIST as truth.",
                "The number of relevance judgments made and relevant documents found for each track are listed in Table 1.",
                "For computational reasons, we truncate ranked lists at 100 documents.",
                "There is no reason that we could not go deeper, but calculating variance is O(n3 ) and thus very timeconsuming.",
                "Because of the reciprocal rank nature of AP, we do not lose much information by truncating at rank 100. 4.2 Algorithms We will compare three algorithms for acquiring relevance judgments.",
                "The baseline is a variation of TREC pooling that we will call incremental pooling (IP).",
                "This algorithm takes a number k as input and presents the first k documents in rank order (without regard to topic) to be judged.",
                "It does not estimate the relevance of unjudged documents; it simply assumes any unjudged document is nonrelevant.",
                "The second algorithm is that presented in Carterette et al. [8] (Algorithm 1).",
                "Documents are selected based on how interesting they are in determining whether a difference in mean average precision exists.",
                "For this approach pi = 0.5 for all i; there is no estimation of probabilities.",
                "We will call this MTC for minimal test collection.",
                "The third algorithm augments MTC with updated estimates of probabilities of relevance.",
                "We will call this RTC for robust test collection.",
                "It is identical to Algorithm 1, except that every 10th iteration we estimate pi for all unjudged documents i using the expert aggregation model of Section 3.",
                "RTC has smoothing (prior distribution) parameters that must be set.",
                "We trained using the ad-hoc 95 set.",
                "We limited 2 Robust here means robust retrieval; this is different from our goal of robust evaluation.",
                "Algorithm 1 (MTC) Given two ranked lists and confidence level α, predict the sign of ΔMAP. 1: pi ← 0.5 for all documents i 2: while P(ΔMAP < 0) < α do 3: calculate weight wi for all unjudged documents i (see Carterette et al. [8] for details) 4: j ← argmaxiwi 5: xj ← 1 if document j is relevant, 0 otherwise 6: pj ← xj 7: end while the search to uniform priors with relatively high variance.",
                "For expert aggregation, the prior parameters are α = β = 1. 4.3 Experimental Design First, we want to know whether we can augment a set of relevance judgments with a set of relevance probabilities in order to reuse the judgments to evaluate a new set of systems.",
                "For each experimental trial: 1.",
                "Pick a random subset of k runs. 2.",
                "From those k, pick an initial c < k to evaluate. 3.",
                "Run RTC to 95% confidence on the initial c. 4.",
                "Using the model from Section 3, estimate the probabilities of relevance for all documents retrieved by all k runs. 5.",
                "Calculate EMAP for all k runs, and P(ΔMAP < 0) for all pairs of runs.",
                "We do the same for MTC, but omit step 4.",
                "Note that after evaluating the first c systems, we make no additional relevance judgments.",
                "To put our method to the test, we selected c = 2: we will build a set of judgments from evaluating only two initial systems.",
                "We will then generalize to a set of k = 10 (of which those two are a subset).",
                "As we run more trials, we obtain the data we need to test all three of our hypotheses. 4.4 Experimental Evaluation Recall that a set of judgments is robust if the accuracy of the predictions it makes is at least its estimated confidence.",
                "One way to evaluate robustness is to bin pairs by their confidence, then calculate the accuracy over all the pairs in each bin.",
                "We would like the accuracy to be no less than the lowest confidence score in the bin, but preferably higher.",
                "Since summary statistics are useful, we devised the following metric.",
                "Suppose we are a bookmaker taking bets on whether ΔMAP < 0.",
                "We use RTC or MTC to set the odds O = P (ΔMAP <0) 1−P (ΔMAP <0) .",
                "Suppose a bettor wagers $1 on ΔMAP ≥ 0.",
                "If it turns out that ΔMAP < 0, we win the dollar.",
                "Otherwise, we pay out O.",
                "If our confidence estimates are perfectly accurate, we break even.",
                "If confidence is greater than accuracy, we lose money; we win if accuracy is greater than confidence.",
                "Counterintuitively, the most desirable outcome is breaking even: if we lose money, we cannot trust the confidence estimates, but if we win money, we have either underestimated confidence or judged more documents than necessary.",
                "However, the cost of not being able to trust the confidence estimates is higher than the cost of extra relevance judgments, so we will treat positive outcomes as good.",
                "The amount we win on each pairwise comparison i is: Wi = yi − (1 − yi) Pi 1 − Pi = yi − Pi 1 − Pi yi = 1 if ΔMAP < 0 and 0 otherwise, and Pi = P(ΔMAP < 0).",
                "The summary statistic is W, the mean of Wi.",
                "Note that as Pi increases, we lose more for being wrong.",
                "This is as it should be: the penalty should be great for missing the high probability predictions.",
                "However, since our losses grow without bound as predictions approach certainty, we cap −Wi at 100.",
                "For our hypothesis that RTC requires fewer judgments than MTC, we are interested in the number of judgments needed to reach 95% confidence on the first pair of systems.",
                "The median is more interesting than the mean: most pairs require a few hundred judgments, but a few pairs require several thousand.",
                "The distribution is therefore highly skewed, and the mean strongly affected by those outliers.",
                "Finally, for our hypothesis that RTC is more accurate than MTC, we will look at Kendalls τ correlation between a ranking of k systems by a small set of judgments and the true ranking using the full set of judgments.",
                "Kendalls τ, a nonparametric statistic based on pairwise swaps between two lists, is a standard evaluation for this type of study.",
                "It ranges from −1 (perfectly anti-correlated) to 1 (rankings identical), with 0 meaning that half of the pairs are swapped.",
                "As we touched on in the introduction, though, an accuracy measure like rank correlation is not a good evaluation of reusability.",
                "We include it for completeness. 4.4.1 Hypothesis Testing Running multiple trials allows the use of statistical hypothesis testing to compare algorithms.",
                "Using the same sets of systems allows the use of paired tests.",
                "As we stated above, we are more interested in the median number of judgments than the mean.",
                "A test for difference in median is the Wilcoxon sign rank test.",
                "We can also use a paired t-test to test for a difference in mean.",
                "For rank correlation, we can use a paired t-test to test for a difference in τ. 5.",
                "RESULTS AND ANALYSIS The comparison between MTC and RTC is shown in Table 2.",
                "With MTC and uniform probabilities of relevance, the results are far from robust.",
                "We cannot reuse the relevance judgments with much confidence.",
                "But with RTC, the results are very robust.",
                "There is a slight dip in accuracy when confidence gets above 0.95; nonetheless, the confidence predictions are trustworthy.",
                "Mean Wi shows that RTC is much closer to 0 than MTC.",
                "The distribution of confidence scores shows that at least 80% confidence is achieved more than 35% of the time, indicating that neither algorithm is being too conservative in its confidence estimates.",
                "The confidence estimates are rather low overall; that is because we have built a test collection from only two initial systems.",
                "Recall from Section 1 that we cannot require (or even expect) a minimum level of confidence when we generalize to new systems.",
                "More detailed results for both algorithms are shown in Figure 2.",
                "The solid line is the ideal result that would give W = 0.",
                "RTC is on or above this line at all points until confidence reaches about 0.97.",
                "After that there is a slight dip in accuracy which we discuss below.",
                "Note that both MTC RTC confidence % in bin accuracy % in bin accuracy 0.5 − 0.6 33.7% 61.7% 28.6% 61.9% 0.6 − 0.7 18.1% 73.1% 20.1% 76.3% 0.7 − 0.8 10.4% 70.1% 15.5% 78.0% 0.8 − 0.9 9.4% 69.0% 12.1% 84.9% 0.9 − 0.95 7.3% 78.0% 6.6% 93.1% 0.95 − 0.99 17.9% 70.4% 12.4% 93.4% 1.0 3.3% 68.3% 4.7% 98.9% W −5.34 −0.39 median judged 251 235 mean τ 0.393 0.555 Table 2: Confidence that P(ΔMAP < 0) and accuracy of prediction when generalizing a set of relevance judgments acquired using MTC and RTC.",
                "Each bin contains over 1,000 trials from the adhoc 3, 5-8 sets.",
                "RTC is much more robust than MTC.",
                "W is defined in Section 4.4; closer to 0 is better.",
                "Median judged is the number of judgments to reach 95% confidence on the first two systems.",
                "Mean τ is the average rank correlation for all 10 systems. 0.5 0.6 0.7 0.8 0.9 1 0.5 0.6 0.7 0.8 0.9 1 accuracy confidence breakeven RTC MTC Figure 2: Confidence vs. accuracy of MTC and RTC.",
                "The solid line is the perfect result that would give W = 0; performance should be on or above this line.",
                "Each point represents at least 500 pairwise comparisons. algorithms are well above the line up to around confidence 0.7.",
                "This is because the baseline performance on these data sets is high; it is quite easy to achieve 75% accuracy doing very little work [7].",
                "Number of Judgments: The median number of judgments required by MTC to reach 95% confidence on the first two systems is 251, an average of 5 per topic.",
                "The median required by RTC is 235, about 4.7 per topic.",
                "Although the numbers are close, RTCs median is significantly lower by a paired Wilcoxon test (p < 0.0001).",
                "For comparison, a pool of depth 100 would result in a minimum of 5,000 judgments for each pair.",
                "The difference in means is much greater.",
                "MTC required a mean of 823 judgments, 16 per topic, while RTC required a mean of 502, 10 per topic. (Recall that means are strongly skewed by a few pairs that take thousands of judgments.)",
                "This difference is significant by a paired t-test (p < 0.0001).",
                "Ten percent of the sets resulted in 100 or fewer judgments (less than two per topic).",
                "Performance on these is very high: W = 0.41, and 99.7% accuracy when confidence is at least 0.9.",
                "This shows that even tiny collections can be reusable.",
                "For the 50% of sets with more than 235 judgments, accuracy is 93% when confidence is at least 0.9.",
                "Rank Correlation: MTC and RTC both rank the 10 systems by EMAP (Eq. (1)) calculated using their respective probability estimates.",
                "The mean τ rank correlation between true MAP and EMAP is 0.393 for MTC and 0.555 for RTC.",
                "This difference is significant by a paired t-test (p < 0.0001).",
                "Note that we do not expect the τ correlations to be high, since we are ranking the systems with so few relevance judgments.",
                "It is more important that we estimate confidence in each pairwise comparison correctly.",
                "We ran IP for the same number of judgments that MTC took for each pair, then ranked the systems by MAP using only those judgments (all unjudged documents assumed nonrelevant).",
                "We calculated the τ correlation to the true ranking.",
                "The mean τ correlation is 0.398, which is not significantly different from MTC, but is significantly lower than RTC.",
                "Using uniform estimates of probability is indistinguishable from the baseline, whereas estimating relevance by expert aggregation boosts performance a great deal: nearly 40% over both MTC and IP.",
                "Overfitting: It is possible to overfit: if too many judgments come from the first two systems, the variance in ΔMAP is reduced and the confidence estimates become unreliable.",
                "We saw this in Table 2 and Figure 2 where RTC exhibits a dip in accuracy when confidence is around 97%.",
                "In fact, the number of judgments made prior to a wrong prediction is over 50% greater than the number made prior to a correct prediction.",
                "Overfitting is difficult to quantify exactly, because making more relevance judgments does not always cause it: at higher confidence levels, more relevance judgments are made, and as Table 2 shows, accuracy is greater at those higher confidences.",
                "Obviously having more relevance judgments should increase both confidence and accuracy; the difference seems to be when one system has a great deal more judgments than the other.",
                "Pairwise Comparisons: Our pairwise comparisons fall into one of three groups: 1. the two original runs from which relevance judgments are acquired; 2. one of the original runs vs. one of the new runs; 3. two new runs.",
                "Table 3 shows confidence vs. accuracy results for each of these three groups.",
                "Interestingly, performance is worst when comparing one of the original runs to one of the additional runs.",
                "This is most likely due to a large difference in the number of judgments affecting the variance of ΔMAP.",
                "Nevertheless, performance is quite good on all three subsets.",
                "Worst Case: The case intuitively most likely to produce an error is when the two systems being compared have retrieved very few documents in common.",
                "If we want the judgments to be reusable, we should to be able to generalize even to runs that are very different from the ones used to acquire the relevance judgments.",
                "A simple measure of similarity of two runs is the average percentage of documents they retrieved in common for each topic [2].",
                "We calculated this for all pairs, then looked at performance on pairs with low similarity.",
                "Results are shown in accuracy confidence two original one original no original 0.5 − 0.6 - 48.1% 62.8% 0.6 − 0.7 - 57.1% 79.2% 0.7 − 0.8 - 67.9% 81.7% 0.8 − 0.9 - 82.2% 86.3% 0.9 − 0.95 95.9% 93.7% 92.6% 0.95 − 0.99 96.2% 92.5% 93.1% 1.0 100% 98.0% 99.1% W −1.11 −0.87 −0.27 Table 3: Confidence vs. accuracy of RTC when comparing the two original runs, one original run and one new run, and two new runs.",
                "RTC is robust in all three cases. accuracy when similar confidence 0 − 0.1 0.1 − 0.2 0.2 − 0.3 0.5 − 0.6 68.4% 63.1% 61.4% 0.6 − 0.7 84.2% 78.6% 76.6% 0.7 − 0.8 82.0% 79.8% 78.9% 0.8 − 0.9 93.6% 83.3% 82.1% 0.9 − 0.95 99.3% 92.7% 92.4% 0.95 − 0.99 98.7% 93.4% 93.3% 1.0 99.9% 97.9% 98.1% W 0.44 −0.45 −0.49 Table 4: Confidence vs. accuracy of RTC when a pair of systems retrieved 0-30% documents in common (broken out into 0%-10%, 10%-20%, and 20%30%).",
                "RTC is robust in all three cases.",
                "Table 4.",
                "Performance is in fact very robust even when similarity is low.",
                "When the two runs share very few documents in common, W is actually positive.",
                "MTC and IP both performed quite poorly in these cases.",
                "When the similarity was between 0 and 10%, both MTC and IP correctly predicted ΔMAP only 60% of the time, compared to an 87.6% success rate for RTC.",
                "By Data Set: All the previous results have only been on the ad-hoc collections.",
                "We did the same experiments on our additional data sets, and broke out the results by data set to see how performance varies.",
                "The results in Table 5 show everything about each set, including binned accuracy, W, mean τ, and median number of judgments to reach 95% confidence on the first two systems.",
                "The results are highly consistent from collection to collection, suggesting that our method is not overfitting to any particular data set. 6.",
                "CONCLUSIONS AND FUTURE WORK In this work we have offered the first formal definition of the common idea of reusability of a test collection and presented a model that is able to achieve reusability with very small sets of relevance judgments.",
                "Table 2 and Figure 2 together show how biased a small set of judgments can be: MTC is dramatically overestimating confidence and is much less accurate than RTC, which is able to remove the bias to give a robust evaluation.",
                "The confidence estimates of RTC, in addition to being accurate, provide a guide for obtaining additional judgments: focus on judging documents from the lowest-confidence comparisons.",
                "In the long run, we see small sets of relevance judgaccuracy confidence ad-hoc 94 ad-hoc 96 ad-hoc 97 ad-hoc 98 ad-hoc 99 web 04 robust 05 terabyte 05 0.5 − 0.6 64.1% 61.8% 62.2% 62.0% 59.4% 64.3% 61.5% 61.6% 0.6 − 0.7 76.1% 77.8% 74.5% 78.2% 74.3% 78.1% 75.9% 75.9% 0.7 − 0.8 75.2% 78.9% 77.6% 80.0% 78.6% 82.6% 77.5% 80.4% 0.8 − 0.9 83.2% 85.5% 84.6% 84.9% 86.8% 84.5% 86.7% 87.7% 0.9 − 0.95 93.0% 93.6% 92.8% 93.7% 92.6% 94.2% 93.9% 94.2% 0.95 − 0.99 93.1% 94.3% 93.1% 93.7% 92.8% 95.0% 93.9% 91.6% 1.0 99.2% 96.8% 98.7% 99.5% 99.6% 100% 99.2% 98.3% W -0.34 -0.34 -0.48 -0.35 -0.44 -0.07 -0.41 -0.67 median judged 235 276 243 213 179 448 310 320 mean τ 0.538 0.573 0.556 0.579 0.532 0.596 0.565 0.574 Table 5: Accuracy, W, mean τ, and median number of judgments for all 8 testing sets.",
                "The results are highly consistent across data sets. ments being shared by researchers, each group contributing a few more judgments to gain more confidence about their particular systems.",
                "As time goes on, the number of judgments grows until there is 100% confidence in every evaluation-and there is a full test collection for the task.",
                "We see further use for this method in scenarios such as web retrieval in which the corpus is frequently changing.",
                "It could be applied to evaluation on a dynamic test collection as defined by Soboroff [18].",
                "The model we presented in Section 3 is by no means the only possibility for creating a robust test collection.",
                "A simpler expert aggregation model might perform as well or better (though all our efforts to simplify failed).",
                "In addition to expert aggregation, we could estimate probabilities by looking at similarities between documents.",
                "This is an obvious area for future exploration.",
                "Additionally, it will be worthwhile to investigate the issue of overfitting: the circumstances it occurs under and what can be done to prevent it.",
                "In the meantime, capping confidence estimates at 95% is a hack that solves the problem.",
                "We have many more experimental results that we unfortunately did not have space for but that reinforce the notion that RTC is highly robust: with just a few judgments per topic, we can accurately assess the confidence in any pairwise comparison of systems.",
                "Acknowledgments This work was supported in part by the Center for Intelligent Information Retrieval and in part by the Defense Advanced Research Projects Agency (DARPA) under contract number HR0011-06-C-0023.",
                "Any opinions, findings, and conclusions or recommendations expressed in this material are those of the author and do not necessarily reflect those of the sponsor. 7.",
                "REFERENCES [1] J. Aslam and M. Montague.",
                "Models for Metasearch.",
                "In Proceedings of SIGIR, pages 275-285, 2001. [2] J. Aslam and R. Savell.",
                "On the effectiveness of evaluating retrieval systems in the absence of relevance judgments.",
                "In Proceedings of SIGIR, pages 361-362, 2003. [3] J.",
                "A. Aslam, V. Pavlu, and R. Savell.",
                "A unified model for metasearch, pooling, and system evaluation.",
                "In Proceedings of CIKM, pages 484-491, 2003. [4] J.",
                "A. Aslam, V. Pavlu, and E. Yilmaz.",
                "A statistical method for system evaluation using incomplete judgments.",
                "In Proceedings of SIGIR, pages 541-548, 2006. [5] A. L. Berger, S. D. Pietra, and V. J. D. Pietra.",
                "A maximum entropy approach to natural language processing.",
                "Computational Linguistics, 22(1):39-71, 1996. [6] D. J. Blower.",
                "An easy derivation of logistic regression from the bayesian and maximum entropy perspective.",
                "In Proceedings of the 23rd International Workship on Bayesian Inference and Maximum Entropy Methods in Science and Engineering, pages 30-43, 2004. [7] B. Carterette and J. Allan.",
                "Research methodology in studies of assessor effort for retrieval evaluation.",
                "In Proceedings of RIAO, 2007. [8] B. Carterette, J. Allan, and R. K. Sitaraman.",
                "Minimal test collections for retrieval evaluation.",
                "In Proceedings of SIGIR, pages 268-275, 2006. [9] B. Carterette and D. I. Petkova.",
                "Learning a ranking from pairwise preferences.",
                "In Proceedings of SIGIR, 2006. [10] R. T. Clemen and R. L. Winkler.",
                "Unanimity and compromise among probability forecasters.",
                "Management Science, 36(7):767-779, July 1990. [11] G. V. Cormack, C. R. Palmer, and C. L. Clarke.",
                "Efficient Construction of Large Test Collections.",
                "In Proceedings of SIGIR, pages 282-289, 1998. [12] A. Gelman, J.",
                "B. Carlin, H. S. Stern, and D. B. Rubin.",
                "Bayesian Data Analysis.",
                "Chapman & Hall/CRC, 2004. [13] E. T. Jaynes.",
                "Probability Theory: The Logic of Science.",
                "Cambridge University Press, 2003. [14] R. Manmatha and H. Sever.",
                "A Formal Approach to Score Normalization for Metasearch.",
                "In Proceedings of HLT, pages 88-93, 2002. [15] I. J. Myung, S. Ramamoorti, and J. Andrew D. Baily.",
                "Maximum entropy aggregation of expert predictions.",
                "Management Science, 42(10):1420-1436, October 1996. [16] J. Platt.",
                "Probabilistic outputs for support vector machines and comparison to regularized likelihood methods. pages 61-74, 2000. [17] M. Sanderson and H. Joho.",
                "Forming test collections with no system pooling.",
                "In Proceedings of the 27th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 33-40, 2004. [18] I. Soboroff.",
                "Dynamic test collections: measuring search effectiveness on the live web.",
                "In Proceedings of SIGIR, pages 276-283, 2006. [19] K. Sparck Jones and C. J. van Rijsbergen.",
                "Information Retrieval Test Collections.",
                "Journal of Documentation, 32(1):59-75, 1976. [20] E. M. Voorhees and D. K. Harman, editors.",
                "TREC: Experiment and Evaluation in Information Retrieval.",
                "MIT Press, 2005. [21] J. Zobel.",
                "How Reliable are the Results of Large-Scale Information Retrieval Experiments?",
                "In Proceedings of SIGIR, pages 307-314, 1998."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [],
            "translated_text": "",
            "candidates": [],
            "error": [
                []
            ]
        },
        "reusability": {
            "translated_key": "reutilización",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Robust Test Collections for Retrieval Evaluation Ben Carterette Center for Intelligent Information Retrieval Computer Science Department University of Massachusetts Amherst Amherst, MA 01003 carteret@cs.umass.edu ABSTRACT Low-cost methods for acquiring relevance judgments can be a boon to researchers who need to evaluate new retrieval tasks or topics but do not have the resources to make thousands of judgments.",
                "While these judgments are very useful for a one-time evaluation, it is not clear that they can be trusted when re-used to evaluate new systems.",
                "In this work, we formally define what it means for judgments to be reusable: the confidence in an evaluation of new systems can be accurately assessed from an existing set of relevance judgments.",
                "We then present a method for augmenting a set of relevance judgments with relevance estimates that require no additional assessor effort.",
                "Using this method practically guarantees <br>reusability</br>: with as few as five judgments per topic taken from only two systems, we can reliably evaluate a larger set of ten systems.",
                "Even the smallest sets of judgments can be useful for evaluation of new systems.",
                "Categories and Subject Descriptors: H.3 Information Storage and Retrieval; H.3.4 Systems and Software: Performance Evaluation General Terms: Experimentation, Measurement, Reliability 1.",
                "INTRODUCTION Consider an information retrieval researcher who has invented a new retrieval task.",
                "She has built a system to perform the task and wants to evaluate it.",
                "Since the task is new, it is unlikely that there are any extant relevance judgments.",
                "She does not have the time or resources to judge every document, or even every retrieved document.",
                "She can only judge the documents that seem to be the most informative and stop when she has a reasonable degree of confidence in her conclusions.",
                "But what happens when she develops a new system and needs to evaluate it?",
                "Or another research group decides to implement a system to perform the task?",
                "Can they reliably reuse the original judgments?",
                "Can they evaluate without more relevance judgments?",
                "Evaluation is an important aspect of information retrieval research, but it is only a semi-solved problem: for most retrieval tasks, it is impossible to judge the relevance of every document; there are simply too many of them.",
                "The solution used by NIST at TREC (Text REtrieval Conference) is the pooling method [19, 20]: all competing systems contribute N documents to a pool, and every document in that pool is judged.",
                "This method creates large sets of judgments that are reusable for training or evaluating new systems that did not contribute to the pool [21].",
                "This solution is not adequate for our hypothetical researcher.",
                "The pooling method gives thousands of relevance judgments, but it requires many hours of (paid) annotator time.",
                "As a result, there have been a slew of recent papers on reducing annotator effort in producing test collections: Cormack et al. [11], Zobel [21], Sanderson and Joho [17], Carterette et al. [8], and Aslam et al. [4], among others.",
                "As we will see, the judgments these methods produce can significantly bias the evaluation of a new set of systems.",
                "Returning to our hypothetical resesarcher, can she reuse her relevance judgments?",
                "First we must formally define what it means to be reusable.",
                "In previous work, <br>reusability</br> has been tested by simply assessing the accuracy of a set of relevance judgments at evaluating unseen systems.",
                "While we can say that it was right 75% of the time, or that it had a rank correlation of 0.8, these numbers do not have any predictive power: they do not tell us which systems are likely to be wrong or how confident we should be in any one.",
                "We need a more careful definition of <br>reusability</br>.",
                "Specifically, the question of <br>reusability</br> is not how accurately we can evaluate new systems.",
                "A malicious adversary can always produce a new ranked list that has not retrieved any of the judged documents.",
                "The real question is how much confidence we have in our evaluations, and, more importantly, whether we can trust our estimates of confidence.",
                "Even if confidence is not high, as long as we can trust it, we can identify which systems need more judgments in order to increase confidence.",
                "Any set of judgments, no matter how small, becomes reusable to some degree.",
                "Small, reusable test collections could have a huge impact on information retrieval research.",
                "Research groups would be able to share the relevance judgments they have done in-house for pilot studies, new tasks, or new topics.",
                "The amount of data available to researchers would grow exponentially over time. 2.",
                "ROBUST EVALUATION Above we gave an intuitive definition of <br>reusability</br>: a collection is reusable if we can trust our estimates of confidence in an evaluation.",
                "By that we mean that if we have made some relevance judgments and have, for example, 75% confidence that system A is better than system B, we would like there to be no more than 25% chance that our assessment of the relative quality of the systems will change as we continue to judge documents.",
                "Our evaluation should be robust to missing judgments.",
                "In our previous work, we defined confidence as the probability that the difference in an evaluation measure calculated for two systems is less than zero [8].",
                "This notion of confidence is defined in the context of a particular evaluation task that we call comparative evaluation: determining the sign of the difference in an evaluation measure.",
                "Other evaluation tasks could be defined; estimating the magnitude of the difference or the values of the measures themselves are examples that entail different notions of confidence.",
                "We therefore see confidence as a probability estimate.",
                "One of the questions we must ask about a probability estimate is what it means.",
                "What does it mean to have 75% confidence that system A is better than system B?",
                "As described above, we want it to mean that if we continue to judge documents, there will only be a 25% chance that our assessment will change.",
                "If this is what it means, we can trust the confidence estimates.",
                "But do we know it has that meaning?",
                "Our calculation of confidence rested on an assumption about the probability of relevance of unjudged documents, specifically that each unjudged document was equally likely to be relevant or nonrelevant.",
                "This assumption is almost certainly not realistic in most IR applications.",
                "As it turns out, it is this assumption that determines whether the confidence estimates can eb trusted.",
                "Before elaborating on this, we formally define confidence. 2.1 Estimating Confidence Average precision (AP) is a standard evaluation metric that captures both the ability of a system to rank relevant documents highly (precision) as well as its ability to retrieve relevant documents (recall).",
                "It is typically written as the mean precision at the ranks of relevant documents: AP = 1 |R| i∈R prec@r(i) where R is the set of relevant documents and r(i) is the rank of document i.",
                "Let Xi be a random variable indicating the relevance of document i.",
                "If documents are ordered by rank, we can express precision as prec@i = 1/i i j=1 Xj .",
                "Average precision then becomes the quadratic equation AP = 1 Xi n i=1 Xi/i i j=1 Xj = 1 Xi n i=1 j≥i aijXiXj where aij = 1/ max{r(i), r(j)}.",
                "Using aij instead of 1/i allows us to number the documents arbitrarily.",
                "To see why this is true, consider a toy example: a list of 3 documents with relevant documents B, C at ranks 1 and 3 and nonrelevant document A at rank 2.",
                "Average precision will be 1 2 (1 1 x2 B+ 1 2 xBxA+ 1 3 xBxC + 1 2 x2 A+ 1 3 xAxC + 1 3 x2 C) = 1 2 1 + 2 3 because xA = 0, xB = 1, xC = 1.",
                "Though the ordering B, A, C is different from the labeling A, B, C, it does not affect the computation.",
                "We can now see average precision itself is a random variable with a distribution over all possible assignments of relevance to all documents.",
                "This random variable has an expectation, a variance, confidence intervals, and a certain probability of being less than or equal to a given value.",
                "All of these are dependent on the probability that document i is relevant: pi = p(Xi = 1).",
                "Suppose in our previous example we do not know the relevance judgments, but we believe pA = 0.4, pB = 0.8, pC = 0.7.",
                "We can then compute e.g.",
                "P(AP = 0) = 0.2 · 0.6 · 0.3 = 0.036, or P(AP = 1 2 ) = 0.2 · 0.4 · 0.7 = 0.056.",
                "Summing over all possibilities, we can compute expectation and variance: E[AP] ≈ 1 pi aiipi + j>i aij pipj V ar[AP] ≈ 1 ( pi)2 n i a2 iipiqi + j>i a2 ijpipj(1 − pipj) + i=j 2aiiaijpipj(1 − pi) + k>j=i 2aijaikpipjpk(1 − pi) AP asymptotically converges to a normal distribution with expectation and variance as defined above.1 For our comparative evaluation task we are interested in the sign of the difference in two average precisions: ΔAP = AP1 − AP2.",
                "As we showed in our previous work, ΔAP has a closed form when documents are ordered arbitrarily: ΔAP = 1 Xi n i=1 j≥i cij XiXj cij = aij − bij where bij is defined analogously to aij for the second ranking.",
                "Since AP is normal, ΔAP is normal as well, meaning we can use the normal cumulative density function to determine the confidence that a difference in AP is less than zero.",
                "Since topics are independent, we can easily extend this to mean average precision (MAP).",
                "MAP is also normally distributed; its expectation and variance are: EMAP = 1 T t∈T E[APt] (1) VMAP = 1 T2 t∈T V ar[APt] ΔMAP = MAP1 − MAP2 Confidence can then be estimated by calculating the expectation and variance and using the normal density function to find P(ΔMAP < 0). 2.2 Confidence and Robustness Having defined confidence, we turn back to the issue of trust in confidence estimates, and show how it ties into the robustness of the collection to missing judgments. 1 These are actually approximations to the true expectation and variance, but the error is a negligible O(n2−n ).",
                "Let Z be the set of all pairs of ranked results for a common set of topics.",
                "Suppose we have a set of m relevance judgments xm = {x1, x2, ..., xm} (using small x rather than capital X to distinguish between judged and unjudged documents); these are the judgments against which we compute confidence.",
                "Let Zα be the subset of pairs in Z for which we predict that ΔMAP = −1 with confidence α given the judgments xm .",
                "For the confidence estimates to be accurate, we need at least α · |Zα| of these pairs to actually have ΔMAP = −1 after we have judged every document.",
                "If they do, we can trust the confidence estimates; our evaluation will be robust to missing judgments.",
                "If our confidence estimates are based on unrealistic assumptions, we cannot expect them to be accurate.",
                "The assumptions they are based on are the probabilities of relevance pi.",
                "We need these to be realistic.",
                "We argue that the best possible distribution of relevance p(Xi) is the one that explains all of the data (all of the observations made about the retrieval systems) while at the same time making no unwarranted assumptions.",
                "This is known as the principle of maximum entropy [13].",
                "The entropy of a random variable X with distribution p(X) is defined as H(p) = − i p(X = i) log p(X = i).",
                "This has found a wide array of uses in computer science and information retrieval.",
                "The maximum entropy distribution is the one that maximizes H. This distribution is unique and has an exponential form.",
                "The following theorem shows the utility of a maximum entropy distribution for relevance when estimating confidence.",
                "Theorem 1.",
                "If p(Xn |I, xm ) = argmaxpH(p), confidence estimates will be accurate. where xm is the set of relevance judgments defined above, Xn is the full set of documents that we wish to estimate the relevance of, and I is some information about the documents (unspecified as of now).",
                "We forgo the proof for the time being, but it is quite simple.",
                "This says that the better the estimates of relevance, the more accurate the evaluation.",
                "The task of creating a reusable test collection thus becomes the task of estimating the relevance of unjudged documents.",
                "The theorem and its proof say nothing whatsoever about the evaluation metric.",
                "The probability estimates are entirely indepedent of the measure we are interested in.",
                "This means the same probability estimates can tell us about average precision as well as precision, recall, bpref, etc.",
                "Furthermore, we could assume that the relevance of documents i and j is independent and achieve the same result, which we state as a corollary: Corollary 1.",
                "If p(Xi|I, xm ) = argmaxpH(p), confidence estimates will be accurate.",
                "The task therefore becomes the imputation of the missing values of relevance.",
                "The theorem implies that the closer we get to the maximum entropy distribution of relevance, the closer we get to robustness. 3.",
                "PREDICTING RELEVANCE In our statement of Theorem 1, we left the nature of the information I unspecified.",
                "One of the advantages of our confidence estimates is that they admit information from a wide variety of sources; essentially anything that can be modeled can be used as information for predicting relevance.",
                "A natural source of information is the retrieval systems themselves: how they ranked the judged documents, how often they failed to rank relevant documents, how they perform across topics, and so on.",
                "If we treat each system as an information retrieval expert providing an opinion about the relevance of each document, the problem becomes one of expert opinion aggregation.",
                "This is similar to the metasearch or data fusion problem in which the task is to take k input systems and merge them into a single ranking.",
                "Aslam et al. [3] previously identified a connection between evaluation and metasearch.",
                "Our problem has two key differences: 1.",
                "We explicitly need probabilities of relevance that we can plug into Eq. 1; metasearch algorithms have no such requirement. 2.",
                "We are accumulating relevance judgments as we proceed with the evaluation and are able to re-estimate relevance given each new judgment.",
                "In light of (1) above, we introduce a probabilistic model for expert combination. 3.1 A Model for Expert Opinion Aggregation Suppose that each expert j provides a probability of relevance qij = pj(Xi = 1).",
                "The information about the relevance of document i will then be the set of k expert opinions I = qi = (qi1, qi2, · · · , qik).",
                "The probability distribution we wish to find is the one that maximizes the entropy of pi = p(Xi = 1|qi).",
                "As it turns out, finding the maximum entropy model is equivalent to finding the parameters that maximize the likelihood [5].",
                "Blower [6] explicitly shows that finding the maximum entropy model for a binary variable is equivalent to solving a logistic regression.",
                "Then pi = p(Xi = 1|qi) = exp k j=1 λjqij 1 + exp k j=1 λj qij (2) where λ1, · · · , λk are the regression parameters.",
                "We include a beta prior for p(λj) with parameters α, β.",
                "This can be seen as a type of smoothing to account for the fact that the training data is highly biased.",
                "This model has the advantage of including the statistical dependence between the experts.",
                "A model of the same form was shown by Clemen & Winkler to be the best for aggregating expert probabilities [10].",
                "A similar maximumentropy-motivated approach has been used for expert aggregation [15].",
                "Aslam & Montague [1] used a similar model for metasearch, but assumed independence among experts.",
                "Where do the qij s come from?",
                "Using raw, uncalibrated scores as predictors will not work because score distributions vary too much between topics.",
                "A language modeling ranker, for instance, will typically give a much higher score to the top retrieved document for a short query than to the top retrieved document for a long query.",
                "We could train a separate predicting model for each topic, but that does not take advantage of all of the information we have: we may only have a handful of judgments for a topic, not enough to train a model to any confidence.",
                "Furthermore, it seems reasonable to assume that if an expert makes good predictions for one topic, it will make good predictions for other topics as well.",
                "We could use a hierarchical model [12], but that will not generalize to unseen topics.",
                "Instead, we will calibrate the scores of each expert individually so that scores can be compared both within topic and between topic.",
                "Thus our model takes into account not only the dependence between experts, but also the dependence between experts performances on different tasks (topics). 3.2 Calibrating Experts Each expert gives us a score and a rank for each document.",
                "We need to convert these to probabilities.",
                "A method such as the one used by Manmatha et al. [14] could be used to convert scores into probabilities of relevance.",
                "The pairwise preference method of Carterette & Petkova [9] could also be used, interpeting the ranking of one document over another as an expression of preference.",
                "Let q∗ ij be expert js self-reported probability that document i is relevant.",
                "Intuitively it seems clear that q∗ ij should decrease with rank, and it should be zero if document i is unranked (the expert did not believe it to be relevant).",
                "The pairwise preference model can handle these two requirements easily, so we will use it.",
                "Let θrj (i) be the relevance coefficient of the document at rank rj(i).",
                "We want to find the θs that maximize the likelihood function: Ljt(Θ) = rj (i)<rj (k) exp(θrj (i) − θrj (k)) 1 + exp(θrj (i) − θrj (k)) We again include a beta prior on p(θrj(i)) with parameters |Rt| + 1 and |Nt| + 1, the size of the sets of judged relevant and nonrelevant documents respectively.",
                "Using these as prior parameters ensures that the resulting probabilities will be concentrated around the ratio of relevant documents that have been discovered for topic t. This means that the probability estimates decrease by rank and are higher for topics that have more relevant documents.",
                "After finding the Θ that maximizes the likelihood, we have q∗ ij = exp(θrj (i)) 1+exp(θrj (i)) .",
                "We define θ∞ = −∞, so that the probability that an unranked document is relevant is 0.",
                "Since q∗ ij is based on the rank at which a document is retrieved rather than the identity of the document itself, the probabilities are identical from expert to expert, e.g. if expert E put document A at rank 1, and expert D put document B at rank 1, we will have q∗ AE = q∗ BD.",
                "Therefore we only have to solve this once for each topic.",
                "The above model gives topic-independent probabilities for each document.",
                "But suppose an expert who reports 90% probability is only right 50% of the time.",
                "Its opinion should be discounted based on its observed performance.",
                "Specifically, we want to learn a calibration function qij = Cj(q∗ ij) that will ensure that the predicted probabilities are tuned to the experts ability to retrieve relevant documents given the judgments that have been made to this point.",
                "Platts SVM calibration method [16] fits a sigmoid function between q∗ ij and the relevance judgments to obtain qij = Cj (q∗ ij) = exp(Aj +Bjq∗ ij ) 1+exp(Aj +Bj q∗ ij ) .",
                "Since q∗ ij is topic-independent, we only need to learn one calibration function for each expert.",
                "Once we have the calibration function, it is applied to adjust the experts predictions to their actual performance.",
                "The calibrated probabilities are plugged into model (2) to find the document probabilities.",
                "Figure 1: Conceptual diagram of our aggregation model.",
                "Experts E1, E2 have ranked documents A, B, C for topic T1 and documents D, E, F for topic T2.",
                "The first step is to obtain q∗ ij.",
                "Next is calibration to true performance to find qij .",
                "Finally we obtain pi = p(Xi = 1|qi1, qi2), · · · . 3.3 Model Summary Our model has three components that differ by the data they take as input and what they produce as output.",
                "A conceptual diagram is shown in Figure 1. 1. ranks → probabilities (per system per topic).",
                "This gives us q∗ ij, expert js self-reported probability of the relevance of document i.",
                "This is unsupervised; it requires no labeled data (though if we have some, we use it to set prior parameters). 2. probabilities → calibrated probabilities (per system).",
                "This gives us qij = Cj (q∗ ij), expert js calibrated probability of the relevance of document i.",
                "This is semisupervised; we have relevance judgments at some ranks which we use to impute the probability of relevance at other ranks. 3. calibrated probabilities → document probabilities.",
                "This gives us pi = p(Xi = 1|qi), the probability of relevance of document i given calibrated expert probabilities qij .",
                "This is supervised; we learn coefficients from a set of judged documents and use those to estimate the relevance of the unjudged documents.",
                "Although the model appears rather complex, it is really just three successive applications of logistic regression.",
                "As such, it can be implemented in a statistical programming language such as R in a few lines of code.",
                "The use of beta (conjugate) priors ensures that no expensive computational methods such as MCMC are necessary [12], so the model is trained and applied fast enough to be used on-line.",
                "Our code is available at http://ciir.cs.umass.edu/~carteret/. 4.",
                "EXPERIMENTS Three hypotheses are under consideration.",
                "The first, and most important, is that using our expert aggregation model to predict relevance produces test collections that are robust enough to be reusable; that is, we can trust the estimates of confidence when we evaluate systems that did not contribute any judgments to the pool.",
                "The other two hypotheses relate to the improvement we see by using better estimates of relevance than we did in our previous work [8].",
                "These are that (a) it takes fewer relevance track no. topics no. runs no. judged no. rel ad-hoc 94 50 40 97,319 9,805 ad-hoc 95 49 33 87,069 6,503 ad-hoc 96 50 61 133,681 5,524 ad-hoc 97 50 74 72,270 4,611 ad-hoc 98 50 103 80,345 4,674 ad-hoc 99 50 129 86,830 4,728 web 04 225 74 88,566 1,763 robust 05 50 74 37,798 6,561 terabyte 05 50 58 45,291 10,407 Table 1: Number of topics, number of runs, number of documents judged, and number found relevant for each of our data sets. judgments to reach 95% confidence and (b) the accuracy of the predictions is higher than if we were to simply assume pi = 0.5 for all unjudged documents. 4.1 Data We obtained full ad-hoc runs submitted to TRECs 3 through 8.",
                "Each run ranks at most 1000 documents for 50 topics (49 topics for TREC 4).",
                "Additionally, we obtained all runs from the Web track of TREC 13, the Robust2 track of TREC 14, and the Terabyte (ad-hoc) track of TREC 14.",
                "These are the tracks that have replaced the ad-hoc track since its end in 1999.",
                "Statistics are shown in Table 1.",
                "We set aside the TREC 4 (ad-hoc 95) set for training, TRECs 3 and 5-8 (ad-hoc 94 and 96-99) for primary testing, and the remaining sets for additional testing.",
                "We use the qrels files assembled by NIST as truth.",
                "The number of relevance judgments made and relevant documents found for each track are listed in Table 1.",
                "For computational reasons, we truncate ranked lists at 100 documents.",
                "There is no reason that we could not go deeper, but calculating variance is O(n3 ) and thus very timeconsuming.",
                "Because of the reciprocal rank nature of AP, we do not lose much information by truncating at rank 100. 4.2 Algorithms We will compare three algorithms for acquiring relevance judgments.",
                "The baseline is a variation of TREC pooling that we will call incremental pooling (IP).",
                "This algorithm takes a number k as input and presents the first k documents in rank order (without regard to topic) to be judged.",
                "It does not estimate the relevance of unjudged documents; it simply assumes any unjudged document is nonrelevant.",
                "The second algorithm is that presented in Carterette et al. [8] (Algorithm 1).",
                "Documents are selected based on how interesting they are in determining whether a difference in mean average precision exists.",
                "For this approach pi = 0.5 for all i; there is no estimation of probabilities.",
                "We will call this MTC for minimal test collection.",
                "The third algorithm augments MTC with updated estimates of probabilities of relevance.",
                "We will call this RTC for robust test collection.",
                "It is identical to Algorithm 1, except that every 10th iteration we estimate pi for all unjudged documents i using the expert aggregation model of Section 3.",
                "RTC has smoothing (prior distribution) parameters that must be set.",
                "We trained using the ad-hoc 95 set.",
                "We limited 2 Robust here means robust retrieval; this is different from our goal of robust evaluation.",
                "Algorithm 1 (MTC) Given two ranked lists and confidence level α, predict the sign of ΔMAP. 1: pi ← 0.5 for all documents i 2: while P(ΔMAP < 0) < α do 3: calculate weight wi for all unjudged documents i (see Carterette et al. [8] for details) 4: j ← argmaxiwi 5: xj ← 1 if document j is relevant, 0 otherwise 6: pj ← xj 7: end while the search to uniform priors with relatively high variance.",
                "For expert aggregation, the prior parameters are α = β = 1. 4.3 Experimental Design First, we want to know whether we can augment a set of relevance judgments with a set of relevance probabilities in order to reuse the judgments to evaluate a new set of systems.",
                "For each experimental trial: 1.",
                "Pick a random subset of k runs. 2.",
                "From those k, pick an initial c < k to evaluate. 3.",
                "Run RTC to 95% confidence on the initial c. 4.",
                "Using the model from Section 3, estimate the probabilities of relevance for all documents retrieved by all k runs. 5.",
                "Calculate EMAP for all k runs, and P(ΔMAP < 0) for all pairs of runs.",
                "We do the same for MTC, but omit step 4.",
                "Note that after evaluating the first c systems, we make no additional relevance judgments.",
                "To put our method to the test, we selected c = 2: we will build a set of judgments from evaluating only two initial systems.",
                "We will then generalize to a set of k = 10 (of which those two are a subset).",
                "As we run more trials, we obtain the data we need to test all three of our hypotheses. 4.4 Experimental Evaluation Recall that a set of judgments is robust if the accuracy of the predictions it makes is at least its estimated confidence.",
                "One way to evaluate robustness is to bin pairs by their confidence, then calculate the accuracy over all the pairs in each bin.",
                "We would like the accuracy to be no less than the lowest confidence score in the bin, but preferably higher.",
                "Since summary statistics are useful, we devised the following metric.",
                "Suppose we are a bookmaker taking bets on whether ΔMAP < 0.",
                "We use RTC or MTC to set the odds O = P (ΔMAP <0) 1−P (ΔMAP <0) .",
                "Suppose a bettor wagers $1 on ΔMAP ≥ 0.",
                "If it turns out that ΔMAP < 0, we win the dollar.",
                "Otherwise, we pay out O.",
                "If our confidence estimates are perfectly accurate, we break even.",
                "If confidence is greater than accuracy, we lose money; we win if accuracy is greater than confidence.",
                "Counterintuitively, the most desirable outcome is breaking even: if we lose money, we cannot trust the confidence estimates, but if we win money, we have either underestimated confidence or judged more documents than necessary.",
                "However, the cost of not being able to trust the confidence estimates is higher than the cost of extra relevance judgments, so we will treat positive outcomes as good.",
                "The amount we win on each pairwise comparison i is: Wi = yi − (1 − yi) Pi 1 − Pi = yi − Pi 1 − Pi yi = 1 if ΔMAP < 0 and 0 otherwise, and Pi = P(ΔMAP < 0).",
                "The summary statistic is W, the mean of Wi.",
                "Note that as Pi increases, we lose more for being wrong.",
                "This is as it should be: the penalty should be great for missing the high probability predictions.",
                "However, since our losses grow without bound as predictions approach certainty, we cap −Wi at 100.",
                "For our hypothesis that RTC requires fewer judgments than MTC, we are interested in the number of judgments needed to reach 95% confidence on the first pair of systems.",
                "The median is more interesting than the mean: most pairs require a few hundred judgments, but a few pairs require several thousand.",
                "The distribution is therefore highly skewed, and the mean strongly affected by those outliers.",
                "Finally, for our hypothesis that RTC is more accurate than MTC, we will look at Kendalls τ correlation between a ranking of k systems by a small set of judgments and the true ranking using the full set of judgments.",
                "Kendalls τ, a nonparametric statistic based on pairwise swaps between two lists, is a standard evaluation for this type of study.",
                "It ranges from −1 (perfectly anti-correlated) to 1 (rankings identical), with 0 meaning that half of the pairs are swapped.",
                "As we touched on in the introduction, though, an accuracy measure like rank correlation is not a good evaluation of <br>reusability</br>.",
                "We include it for completeness. 4.4.1 Hypothesis Testing Running multiple trials allows the use of statistical hypothesis testing to compare algorithms.",
                "Using the same sets of systems allows the use of paired tests.",
                "As we stated above, we are more interested in the median number of judgments than the mean.",
                "A test for difference in median is the Wilcoxon sign rank test.",
                "We can also use a paired t-test to test for a difference in mean.",
                "For rank correlation, we can use a paired t-test to test for a difference in τ. 5.",
                "RESULTS AND ANALYSIS The comparison between MTC and RTC is shown in Table 2.",
                "With MTC and uniform probabilities of relevance, the results are far from robust.",
                "We cannot reuse the relevance judgments with much confidence.",
                "But with RTC, the results are very robust.",
                "There is a slight dip in accuracy when confidence gets above 0.95; nonetheless, the confidence predictions are trustworthy.",
                "Mean Wi shows that RTC is much closer to 0 than MTC.",
                "The distribution of confidence scores shows that at least 80% confidence is achieved more than 35% of the time, indicating that neither algorithm is being too conservative in its confidence estimates.",
                "The confidence estimates are rather low overall; that is because we have built a test collection from only two initial systems.",
                "Recall from Section 1 that we cannot require (or even expect) a minimum level of confidence when we generalize to new systems.",
                "More detailed results for both algorithms are shown in Figure 2.",
                "The solid line is the ideal result that would give W = 0.",
                "RTC is on or above this line at all points until confidence reaches about 0.97.",
                "After that there is a slight dip in accuracy which we discuss below.",
                "Note that both MTC RTC confidence % in bin accuracy % in bin accuracy 0.5 − 0.6 33.7% 61.7% 28.6% 61.9% 0.6 − 0.7 18.1% 73.1% 20.1% 76.3% 0.7 − 0.8 10.4% 70.1% 15.5% 78.0% 0.8 − 0.9 9.4% 69.0% 12.1% 84.9% 0.9 − 0.95 7.3% 78.0% 6.6% 93.1% 0.95 − 0.99 17.9% 70.4% 12.4% 93.4% 1.0 3.3% 68.3% 4.7% 98.9% W −5.34 −0.39 median judged 251 235 mean τ 0.393 0.555 Table 2: Confidence that P(ΔMAP < 0) and accuracy of prediction when generalizing a set of relevance judgments acquired using MTC and RTC.",
                "Each bin contains over 1,000 trials from the adhoc 3, 5-8 sets.",
                "RTC is much more robust than MTC.",
                "W is defined in Section 4.4; closer to 0 is better.",
                "Median judged is the number of judgments to reach 95% confidence on the first two systems.",
                "Mean τ is the average rank correlation for all 10 systems. 0.5 0.6 0.7 0.8 0.9 1 0.5 0.6 0.7 0.8 0.9 1 accuracy confidence breakeven RTC MTC Figure 2: Confidence vs. accuracy of MTC and RTC.",
                "The solid line is the perfect result that would give W = 0; performance should be on or above this line.",
                "Each point represents at least 500 pairwise comparisons. algorithms are well above the line up to around confidence 0.7.",
                "This is because the baseline performance on these data sets is high; it is quite easy to achieve 75% accuracy doing very little work [7].",
                "Number of Judgments: The median number of judgments required by MTC to reach 95% confidence on the first two systems is 251, an average of 5 per topic.",
                "The median required by RTC is 235, about 4.7 per topic.",
                "Although the numbers are close, RTCs median is significantly lower by a paired Wilcoxon test (p < 0.0001).",
                "For comparison, a pool of depth 100 would result in a minimum of 5,000 judgments for each pair.",
                "The difference in means is much greater.",
                "MTC required a mean of 823 judgments, 16 per topic, while RTC required a mean of 502, 10 per topic. (Recall that means are strongly skewed by a few pairs that take thousands of judgments.)",
                "This difference is significant by a paired t-test (p < 0.0001).",
                "Ten percent of the sets resulted in 100 or fewer judgments (less than two per topic).",
                "Performance on these is very high: W = 0.41, and 99.7% accuracy when confidence is at least 0.9.",
                "This shows that even tiny collections can be reusable.",
                "For the 50% of sets with more than 235 judgments, accuracy is 93% when confidence is at least 0.9.",
                "Rank Correlation: MTC and RTC both rank the 10 systems by EMAP (Eq. (1)) calculated using their respective probability estimates.",
                "The mean τ rank correlation between true MAP and EMAP is 0.393 for MTC and 0.555 for RTC.",
                "This difference is significant by a paired t-test (p < 0.0001).",
                "Note that we do not expect the τ correlations to be high, since we are ranking the systems with so few relevance judgments.",
                "It is more important that we estimate confidence in each pairwise comparison correctly.",
                "We ran IP for the same number of judgments that MTC took for each pair, then ranked the systems by MAP using only those judgments (all unjudged documents assumed nonrelevant).",
                "We calculated the τ correlation to the true ranking.",
                "The mean τ correlation is 0.398, which is not significantly different from MTC, but is significantly lower than RTC.",
                "Using uniform estimates of probability is indistinguishable from the baseline, whereas estimating relevance by expert aggregation boosts performance a great deal: nearly 40% over both MTC and IP.",
                "Overfitting: It is possible to overfit: if too many judgments come from the first two systems, the variance in ΔMAP is reduced and the confidence estimates become unreliable.",
                "We saw this in Table 2 and Figure 2 where RTC exhibits a dip in accuracy when confidence is around 97%.",
                "In fact, the number of judgments made prior to a wrong prediction is over 50% greater than the number made prior to a correct prediction.",
                "Overfitting is difficult to quantify exactly, because making more relevance judgments does not always cause it: at higher confidence levels, more relevance judgments are made, and as Table 2 shows, accuracy is greater at those higher confidences.",
                "Obviously having more relevance judgments should increase both confidence and accuracy; the difference seems to be when one system has a great deal more judgments than the other.",
                "Pairwise Comparisons: Our pairwise comparisons fall into one of three groups: 1. the two original runs from which relevance judgments are acquired; 2. one of the original runs vs. one of the new runs; 3. two new runs.",
                "Table 3 shows confidence vs. accuracy results for each of these three groups.",
                "Interestingly, performance is worst when comparing one of the original runs to one of the additional runs.",
                "This is most likely due to a large difference in the number of judgments affecting the variance of ΔMAP.",
                "Nevertheless, performance is quite good on all three subsets.",
                "Worst Case: The case intuitively most likely to produce an error is when the two systems being compared have retrieved very few documents in common.",
                "If we want the judgments to be reusable, we should to be able to generalize even to runs that are very different from the ones used to acquire the relevance judgments.",
                "A simple measure of similarity of two runs is the average percentage of documents they retrieved in common for each topic [2].",
                "We calculated this for all pairs, then looked at performance on pairs with low similarity.",
                "Results are shown in accuracy confidence two original one original no original 0.5 − 0.6 - 48.1% 62.8% 0.6 − 0.7 - 57.1% 79.2% 0.7 − 0.8 - 67.9% 81.7% 0.8 − 0.9 - 82.2% 86.3% 0.9 − 0.95 95.9% 93.7% 92.6% 0.95 − 0.99 96.2% 92.5% 93.1% 1.0 100% 98.0% 99.1% W −1.11 −0.87 −0.27 Table 3: Confidence vs. accuracy of RTC when comparing the two original runs, one original run and one new run, and two new runs.",
                "RTC is robust in all three cases. accuracy when similar confidence 0 − 0.1 0.1 − 0.2 0.2 − 0.3 0.5 − 0.6 68.4% 63.1% 61.4% 0.6 − 0.7 84.2% 78.6% 76.6% 0.7 − 0.8 82.0% 79.8% 78.9% 0.8 − 0.9 93.6% 83.3% 82.1% 0.9 − 0.95 99.3% 92.7% 92.4% 0.95 − 0.99 98.7% 93.4% 93.3% 1.0 99.9% 97.9% 98.1% W 0.44 −0.45 −0.49 Table 4: Confidence vs. accuracy of RTC when a pair of systems retrieved 0-30% documents in common (broken out into 0%-10%, 10%-20%, and 20%30%).",
                "RTC is robust in all three cases.",
                "Table 4.",
                "Performance is in fact very robust even when similarity is low.",
                "When the two runs share very few documents in common, W is actually positive.",
                "MTC and IP both performed quite poorly in these cases.",
                "When the similarity was between 0 and 10%, both MTC and IP correctly predicted ΔMAP only 60% of the time, compared to an 87.6% success rate for RTC.",
                "By Data Set: All the previous results have only been on the ad-hoc collections.",
                "We did the same experiments on our additional data sets, and broke out the results by data set to see how performance varies.",
                "The results in Table 5 show everything about each set, including binned accuracy, W, mean τ, and median number of judgments to reach 95% confidence on the first two systems.",
                "The results are highly consistent from collection to collection, suggesting that our method is not overfitting to any particular data set. 6.",
                "CONCLUSIONS AND FUTURE WORK In this work we have offered the first formal definition of the common idea of <br>reusability</br> of a test collection and presented a model that is able to achieve <br>reusability</br> with very small sets of relevance judgments.",
                "Table 2 and Figure 2 together show how biased a small set of judgments can be: MTC is dramatically overestimating confidence and is much less accurate than RTC, which is able to remove the bias to give a robust evaluation.",
                "The confidence estimates of RTC, in addition to being accurate, provide a guide for obtaining additional judgments: focus on judging documents from the lowest-confidence comparisons.",
                "In the long run, we see small sets of relevance judgaccuracy confidence ad-hoc 94 ad-hoc 96 ad-hoc 97 ad-hoc 98 ad-hoc 99 web 04 robust 05 terabyte 05 0.5 − 0.6 64.1% 61.8% 62.2% 62.0% 59.4% 64.3% 61.5% 61.6% 0.6 − 0.7 76.1% 77.8% 74.5% 78.2% 74.3% 78.1% 75.9% 75.9% 0.7 − 0.8 75.2% 78.9% 77.6% 80.0% 78.6% 82.6% 77.5% 80.4% 0.8 − 0.9 83.2% 85.5% 84.6% 84.9% 86.8% 84.5% 86.7% 87.7% 0.9 − 0.95 93.0% 93.6% 92.8% 93.7% 92.6% 94.2% 93.9% 94.2% 0.95 − 0.99 93.1% 94.3% 93.1% 93.7% 92.8% 95.0% 93.9% 91.6% 1.0 99.2% 96.8% 98.7% 99.5% 99.6% 100% 99.2% 98.3% W -0.34 -0.34 -0.48 -0.35 -0.44 -0.07 -0.41 -0.67 median judged 235 276 243 213 179 448 310 320 mean τ 0.538 0.573 0.556 0.579 0.532 0.596 0.565 0.574 Table 5: Accuracy, W, mean τ, and median number of judgments for all 8 testing sets.",
                "The results are highly consistent across data sets. ments being shared by researchers, each group contributing a few more judgments to gain more confidence about their particular systems.",
                "As time goes on, the number of judgments grows until there is 100% confidence in every evaluation-and there is a full test collection for the task.",
                "We see further use for this method in scenarios such as web retrieval in which the corpus is frequently changing.",
                "It could be applied to evaluation on a dynamic test collection as defined by Soboroff [18].",
                "The model we presented in Section 3 is by no means the only possibility for creating a robust test collection.",
                "A simpler expert aggregation model might perform as well or better (though all our efforts to simplify failed).",
                "In addition to expert aggregation, we could estimate probabilities by looking at similarities between documents.",
                "This is an obvious area for future exploration.",
                "Additionally, it will be worthwhile to investigate the issue of overfitting: the circumstances it occurs under and what can be done to prevent it.",
                "In the meantime, capping confidence estimates at 95% is a hack that solves the problem.",
                "We have many more experimental results that we unfortunately did not have space for but that reinforce the notion that RTC is highly robust: with just a few judgments per topic, we can accurately assess the confidence in any pairwise comparison of systems.",
                "Acknowledgments This work was supported in part by the Center for Intelligent Information Retrieval and in part by the Defense Advanced Research Projects Agency (DARPA) under contract number HR0011-06-C-0023.",
                "Any opinions, findings, and conclusions or recommendations expressed in this material are those of the author and do not necessarily reflect those of the sponsor. 7.",
                "REFERENCES [1] J. Aslam and M. Montague.",
                "Models for Metasearch.",
                "In Proceedings of SIGIR, pages 275-285, 2001. [2] J. Aslam and R. Savell.",
                "On the effectiveness of evaluating retrieval systems in the absence of relevance judgments.",
                "In Proceedings of SIGIR, pages 361-362, 2003. [3] J.",
                "A. Aslam, V. Pavlu, and R. Savell.",
                "A unified model for metasearch, pooling, and system evaluation.",
                "In Proceedings of CIKM, pages 484-491, 2003. [4] J.",
                "A. Aslam, V. Pavlu, and E. Yilmaz.",
                "A statistical method for system evaluation using incomplete judgments.",
                "In Proceedings of SIGIR, pages 541-548, 2006. [5] A. L. Berger, S. D. Pietra, and V. J. D. Pietra.",
                "A maximum entropy approach to natural language processing.",
                "Computational Linguistics, 22(1):39-71, 1996. [6] D. J. Blower.",
                "An easy derivation of logistic regression from the bayesian and maximum entropy perspective.",
                "In Proceedings of the 23rd International Workship on Bayesian Inference and Maximum Entropy Methods in Science and Engineering, pages 30-43, 2004. [7] B. Carterette and J. Allan.",
                "Research methodology in studies of assessor effort for retrieval evaluation.",
                "In Proceedings of RIAO, 2007. [8] B. Carterette, J. Allan, and R. K. Sitaraman.",
                "Minimal test collections for retrieval evaluation.",
                "In Proceedings of SIGIR, pages 268-275, 2006. [9] B. Carterette and D. I. Petkova.",
                "Learning a ranking from pairwise preferences.",
                "In Proceedings of SIGIR, 2006. [10] R. T. Clemen and R. L. Winkler.",
                "Unanimity and compromise among probability forecasters.",
                "Management Science, 36(7):767-779, July 1990. [11] G. V. Cormack, C. R. Palmer, and C. L. Clarke.",
                "Efficient Construction of Large Test Collections.",
                "In Proceedings of SIGIR, pages 282-289, 1998. [12] A. Gelman, J.",
                "B. Carlin, H. S. Stern, and D. B. Rubin.",
                "Bayesian Data Analysis.",
                "Chapman & Hall/CRC, 2004. [13] E. T. Jaynes.",
                "Probability Theory: The Logic of Science.",
                "Cambridge University Press, 2003. [14] R. Manmatha and H. Sever.",
                "A Formal Approach to Score Normalization for Metasearch.",
                "In Proceedings of HLT, pages 88-93, 2002. [15] I. J. Myung, S. Ramamoorti, and J. Andrew D. Baily.",
                "Maximum entropy aggregation of expert predictions.",
                "Management Science, 42(10):1420-1436, October 1996. [16] J. Platt.",
                "Probabilistic outputs for support vector machines and comparison to regularized likelihood methods. pages 61-74, 2000. [17] M. Sanderson and H. Joho.",
                "Forming test collections with no system pooling.",
                "In Proceedings of the 27th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 33-40, 2004. [18] I. Soboroff.",
                "Dynamic test collections: measuring search effectiveness on the live web.",
                "In Proceedings of SIGIR, pages 276-283, 2006. [19] K. Sparck Jones and C. J. van Rijsbergen.",
                "Information Retrieval Test Collections.",
                "Journal of Documentation, 32(1):59-75, 1976. [20] E. M. Voorhees and D. K. Harman, editors.",
                "TREC: Experiment and Evaluation in Information Retrieval.",
                "MIT Press, 2005. [21] J. Zobel.",
                "How Reliable are the Results of Large-Scale Information Retrieval Experiments?",
                "In Proceedings of SIGIR, pages 307-314, 1998."
            ],
            "original_annotated_samples": [
                "Using this method practically guarantees <br>reusability</br>: with as few as five judgments per topic taken from only two systems, we can reliably evaluate a larger set of ten systems.",
                "In previous work, <br>reusability</br> has been tested by simply assessing the accuracy of a set of relevance judgments at evaluating unseen systems.",
                "We need a more careful definition of <br>reusability</br>.",
                "Specifically, the question of <br>reusability</br> is not how accurately we can evaluate new systems.",
                "ROBUST EVALUATION Above we gave an intuitive definition of <br>reusability</br>: a collection is reusable if we can trust our estimates of confidence in an evaluation."
            ],
            "translated_annotated_samples": [
                "Usar este método garantiza prácticamente la <br>reutilización</br>: con tan solo cinco juicios por tema tomados de solo dos sistemas, podemos evaluar de manera confiable un conjunto más grande de diez sistemas.",
                "En trabajos anteriores, la <br>reutilización</br> se ha probado simplemente evaluando la precisión de un conjunto de juicios de relevancia al evaluar sistemas no vistos.",
                "Necesitamos una definición más cuidadosa de <br>reutilización</br>.",
                "Específicamente, la cuestión de la <br>reutilización</br> no es qué tan precisamente podemos evaluar los nuevos sistemas.",
                "EVALUACIÓN ROBUSTA Anteriormente dimos una definición intuitiva de <br>reutilización</br>: una colección es reutilizable si podemos confiar en nuestras estimaciones de confianza en una evaluación."
            ],
            "translated_text": "Colecciones de pruebas robustas para la evaluación de recuperación Ben Carterette Centro de Recuperación de Información Inteligente Departamento de Ciencias de la Computación Universidad de Massachusetts Amherst Amherst, MA 01003 carteret@cs.umass.edu RESUMEN Los métodos de bajo costo para adquirir juicios de relevancia pueden ser una bendición para los investigadores que necesitan evaluar nuevas tareas o temas de recuperación pero no tienen los recursos para hacer miles de juicios. Si bien estos juicios son muy útiles para una evaluación única, no está claro que se puedan confiar al ser reutilizados para evaluar nuevos sistemas. En este trabajo, definimos formalmente lo que significa que las evaluaciones sean reutilizables: la confianza en una evaluación de nuevos sistemas puede ser evaluada con precisión a partir de un conjunto existente de juicios de relevancia. Luego presentamos un método para aumentar un conjunto de juicios de relevancia con estimaciones de relevancia que no requieren esfuerzo adicional del evaluador. Usar este método garantiza prácticamente la <br>reutilización</br>: con tan solo cinco juicios por tema tomados de solo dos sistemas, podemos evaluar de manera confiable un conjunto más grande de diez sistemas. Incluso los conjuntos más pequeños de juicios pueden ser útiles para la evaluación de nuevos sistemas. Categorías y Descriptores de Asignaturas: H.3 Almacenamiento y Recuperación de Información; H.3.4 Sistemas y Software: Evaluación del Rendimiento Términos Generales: Experimentación, Medición, Confiabilidad 1. INTRODUCCIÓN Imagina a un investigador de recuperación de información que ha inventado una nueva tarea de recuperación. Ella ha construido un sistema para realizar la tarea y quiere evaluarlo. Dado que la tarea es nueva, es poco probable que existan juicios de relevancia vigentes. Ella no tiene el tiempo ni los recursos para juzgar cada documento, ni siquiera cada documento recuperado. Ella solo puede juzgar los documentos que parecen ser los más informativos y detenerse cuando tenga un grado razonable de confianza en sus conclusiones. Pero ¿qué sucede cuando ella desarrolla un nuevo sistema y necesita evaluarlo? ¿Otro grupo de investigación decide implementar un sistema para realizar la tarea? ¿Pueden reutilizar de manera confiable los juicios originales? ¿Pueden evaluar sin más juicios de relevancia? La evaluación es un aspecto importante de la investigación en recuperación de información, pero es solo un problema parcialmente resuelto: para la mayoría de las tareas de recuperación, es imposible juzgar la relevancia de cada documento; simplemente hay demasiados. La solución utilizada por NIST en TREC (Text REtrieval Conference) es el método de agrupación [19, 20]: todos los sistemas competidores contribuyen con N documentos a un grupo, y cada documento en ese grupo es evaluado. Este método crea grandes conjuntos de juicios que son reutilizables para entrenar o evaluar nuevos sistemas que no contribuyeron al conjunto [21]. Esta solución no es adecuada para nuestro investigador hipotético. El método de agrupación proporciona miles de juicios de relevancia, pero requiere muchas horas de tiempo de anotadores (remunerados). Como resultado, ha habido una serie de artículos recientes sobre la reducción del esfuerzo del anotador en la producción de colecciones de pruebas: Cormack et al. [11], Zobel [21], Sanderson y Joho [17], Carterette et al. [8], y Aslam et al. [4], entre otros. Como veremos, los juicios que estos métodos producen pueden sesgar significativamente la evaluación de un nuevo conjunto de sistemas. ¿Volviendo a nuestra investigadora hipotética, puede reutilizar sus juicios de relevancia? Primero debemos definir formalmente lo que significa ser reutilizable. En trabajos anteriores, la <br>reutilización</br> se ha probado simplemente evaluando la precisión de un conjunto de juicios de relevancia al evaluar sistemas no vistos. Si bien podemos decir que tuvo razón el 75% del tiempo, o que tuvo una correlación de rango de 0.8, estos números no tienen ningún poder predictivo: no nos dicen qué sistemas es probable que estén equivocados ni cuán seguros deberíamos estar en uno en particular. Necesitamos una definición más cuidadosa de <br>reutilización</br>. Específicamente, la cuestión de la <br>reutilización</br> no es qué tan precisamente podemos evaluar los nuevos sistemas. Un adversario malicioso siempre puede producir una nueva lista clasificada que no haya recuperado ninguno de los documentos evaluados. La verdadera pregunta es cuánta confianza tenemos en nuestras evaluaciones, y, más importante aún, si podemos confiar en nuestras estimaciones de confianza. Aunque la confianza no sea alta, siempre y cuando podamos confiar en ello, podemos identificar qué sistemas necesitan más juicios para aumentar la confianza. Cualquier conjunto de juicios, por pequeño que sea, se vuelve reutilizable en cierta medida. Pequeñas colecciones de pruebas reutilizables podrían tener un gran impacto en la investigación de recuperación de información. Los grupos de investigación podrían compartir las evaluaciones de relevancia que han realizado internamente para estudios piloto, nuevas tareas o nuevos temas. La cantidad de datos disponible para los investigadores crecería de forma exponencial con el tiempo. EVALUACIÓN ROBUSTA Anteriormente dimos una definición intuitiva de <br>reutilización</br>: una colección es reutilizable si podemos confiar en nuestras estimaciones de confianza en una evaluación. ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "lowerest-confidence comparison": {
            "translated_key": "comparación de menor confianza",
            "is_in_text": false,
            "original_annotated_sentences": [
                "Robust Test Collections for Retrieval Evaluation Ben Carterette Center for Intelligent Information Retrieval Computer Science Department University of Massachusetts Amherst Amherst, MA 01003 carteret@cs.umass.edu ABSTRACT Low-cost methods for acquiring relevance judgments can be a boon to researchers who need to evaluate new retrieval tasks or topics but do not have the resources to make thousands of judgments.",
                "While these judgments are very useful for a one-time evaluation, it is not clear that they can be trusted when re-used to evaluate new systems.",
                "In this work, we formally define what it means for judgments to be reusable: the confidence in an evaluation of new systems can be accurately assessed from an existing set of relevance judgments.",
                "We then present a method for augmenting a set of relevance judgments with relevance estimates that require no additional assessor effort.",
                "Using this method practically guarantees reusability: with as few as five judgments per topic taken from only two systems, we can reliably evaluate a larger set of ten systems.",
                "Even the smallest sets of judgments can be useful for evaluation of new systems.",
                "Categories and Subject Descriptors: H.3 Information Storage and Retrieval; H.3.4 Systems and Software: Performance Evaluation General Terms: Experimentation, Measurement, Reliability 1.",
                "INTRODUCTION Consider an information retrieval researcher who has invented a new retrieval task.",
                "She has built a system to perform the task and wants to evaluate it.",
                "Since the task is new, it is unlikely that there are any extant relevance judgments.",
                "She does not have the time or resources to judge every document, or even every retrieved document.",
                "She can only judge the documents that seem to be the most informative and stop when she has a reasonable degree of confidence in her conclusions.",
                "But what happens when she develops a new system and needs to evaluate it?",
                "Or another research group decides to implement a system to perform the task?",
                "Can they reliably reuse the original judgments?",
                "Can they evaluate without more relevance judgments?",
                "Evaluation is an important aspect of information retrieval research, but it is only a semi-solved problem: for most retrieval tasks, it is impossible to judge the relevance of every document; there are simply too many of them.",
                "The solution used by NIST at TREC (Text REtrieval Conference) is the pooling method [19, 20]: all competing systems contribute N documents to a pool, and every document in that pool is judged.",
                "This method creates large sets of judgments that are reusable for training or evaluating new systems that did not contribute to the pool [21].",
                "This solution is not adequate for our hypothetical researcher.",
                "The pooling method gives thousands of relevance judgments, but it requires many hours of (paid) annotator time.",
                "As a result, there have been a slew of recent papers on reducing annotator effort in producing test collections: Cormack et al. [11], Zobel [21], Sanderson and Joho [17], Carterette et al. [8], and Aslam et al. [4], among others.",
                "As we will see, the judgments these methods produce can significantly bias the evaluation of a new set of systems.",
                "Returning to our hypothetical resesarcher, can she reuse her relevance judgments?",
                "First we must formally define what it means to be reusable.",
                "In previous work, reusability has been tested by simply assessing the accuracy of a set of relevance judgments at evaluating unseen systems.",
                "While we can say that it was right 75% of the time, or that it had a rank correlation of 0.8, these numbers do not have any predictive power: they do not tell us which systems are likely to be wrong or how confident we should be in any one.",
                "We need a more careful definition of reusability.",
                "Specifically, the question of reusability is not how accurately we can evaluate new systems.",
                "A malicious adversary can always produce a new ranked list that has not retrieved any of the judged documents.",
                "The real question is how much confidence we have in our evaluations, and, more importantly, whether we can trust our estimates of confidence.",
                "Even if confidence is not high, as long as we can trust it, we can identify which systems need more judgments in order to increase confidence.",
                "Any set of judgments, no matter how small, becomes reusable to some degree.",
                "Small, reusable test collections could have a huge impact on information retrieval research.",
                "Research groups would be able to share the relevance judgments they have done in-house for pilot studies, new tasks, or new topics.",
                "The amount of data available to researchers would grow exponentially over time. 2.",
                "ROBUST EVALUATION Above we gave an intuitive definition of reusability: a collection is reusable if we can trust our estimates of confidence in an evaluation.",
                "By that we mean that if we have made some relevance judgments and have, for example, 75% confidence that system A is better than system B, we would like there to be no more than 25% chance that our assessment of the relative quality of the systems will change as we continue to judge documents.",
                "Our evaluation should be robust to missing judgments.",
                "In our previous work, we defined confidence as the probability that the difference in an evaluation measure calculated for two systems is less than zero [8].",
                "This notion of confidence is defined in the context of a particular evaluation task that we call comparative evaluation: determining the sign of the difference in an evaluation measure.",
                "Other evaluation tasks could be defined; estimating the magnitude of the difference or the values of the measures themselves are examples that entail different notions of confidence.",
                "We therefore see confidence as a probability estimate.",
                "One of the questions we must ask about a probability estimate is what it means.",
                "What does it mean to have 75% confidence that system A is better than system B?",
                "As described above, we want it to mean that if we continue to judge documents, there will only be a 25% chance that our assessment will change.",
                "If this is what it means, we can trust the confidence estimates.",
                "But do we know it has that meaning?",
                "Our calculation of confidence rested on an assumption about the probability of relevance of unjudged documents, specifically that each unjudged document was equally likely to be relevant or nonrelevant.",
                "This assumption is almost certainly not realistic in most IR applications.",
                "As it turns out, it is this assumption that determines whether the confidence estimates can eb trusted.",
                "Before elaborating on this, we formally define confidence. 2.1 Estimating Confidence Average precision (AP) is a standard evaluation metric that captures both the ability of a system to rank relevant documents highly (precision) as well as its ability to retrieve relevant documents (recall).",
                "It is typically written as the mean precision at the ranks of relevant documents: AP = 1 |R| i∈R prec@r(i) where R is the set of relevant documents and r(i) is the rank of document i.",
                "Let Xi be a random variable indicating the relevance of document i.",
                "If documents are ordered by rank, we can express precision as prec@i = 1/i i j=1 Xj .",
                "Average precision then becomes the quadratic equation AP = 1 Xi n i=1 Xi/i i j=1 Xj = 1 Xi n i=1 j≥i aijXiXj where aij = 1/ max{r(i), r(j)}.",
                "Using aij instead of 1/i allows us to number the documents arbitrarily.",
                "To see why this is true, consider a toy example: a list of 3 documents with relevant documents B, C at ranks 1 and 3 and nonrelevant document A at rank 2.",
                "Average precision will be 1 2 (1 1 x2 B+ 1 2 xBxA+ 1 3 xBxC + 1 2 x2 A+ 1 3 xAxC + 1 3 x2 C) = 1 2 1 + 2 3 because xA = 0, xB = 1, xC = 1.",
                "Though the ordering B, A, C is different from the labeling A, B, C, it does not affect the computation.",
                "We can now see average precision itself is a random variable with a distribution over all possible assignments of relevance to all documents.",
                "This random variable has an expectation, a variance, confidence intervals, and a certain probability of being less than or equal to a given value.",
                "All of these are dependent on the probability that document i is relevant: pi = p(Xi = 1).",
                "Suppose in our previous example we do not know the relevance judgments, but we believe pA = 0.4, pB = 0.8, pC = 0.7.",
                "We can then compute e.g.",
                "P(AP = 0) = 0.2 · 0.6 · 0.3 = 0.036, or P(AP = 1 2 ) = 0.2 · 0.4 · 0.7 = 0.056.",
                "Summing over all possibilities, we can compute expectation and variance: E[AP] ≈ 1 pi aiipi + j>i aij pipj V ar[AP] ≈ 1 ( pi)2 n i a2 iipiqi + j>i a2 ijpipj(1 − pipj) + i=j 2aiiaijpipj(1 − pi) + k>j=i 2aijaikpipjpk(1 − pi) AP asymptotically converges to a normal distribution with expectation and variance as defined above.1 For our comparative evaluation task we are interested in the sign of the difference in two average precisions: ΔAP = AP1 − AP2.",
                "As we showed in our previous work, ΔAP has a closed form when documents are ordered arbitrarily: ΔAP = 1 Xi n i=1 j≥i cij XiXj cij = aij − bij where bij is defined analogously to aij for the second ranking.",
                "Since AP is normal, ΔAP is normal as well, meaning we can use the normal cumulative density function to determine the confidence that a difference in AP is less than zero.",
                "Since topics are independent, we can easily extend this to mean average precision (MAP).",
                "MAP is also normally distributed; its expectation and variance are: EMAP = 1 T t∈T E[APt] (1) VMAP = 1 T2 t∈T V ar[APt] ΔMAP = MAP1 − MAP2 Confidence can then be estimated by calculating the expectation and variance and using the normal density function to find P(ΔMAP < 0). 2.2 Confidence and Robustness Having defined confidence, we turn back to the issue of trust in confidence estimates, and show how it ties into the robustness of the collection to missing judgments. 1 These are actually approximations to the true expectation and variance, but the error is a negligible O(n2−n ).",
                "Let Z be the set of all pairs of ranked results for a common set of topics.",
                "Suppose we have a set of m relevance judgments xm = {x1, x2, ..., xm} (using small x rather than capital X to distinguish between judged and unjudged documents); these are the judgments against which we compute confidence.",
                "Let Zα be the subset of pairs in Z for which we predict that ΔMAP = −1 with confidence α given the judgments xm .",
                "For the confidence estimates to be accurate, we need at least α · |Zα| of these pairs to actually have ΔMAP = −1 after we have judged every document.",
                "If they do, we can trust the confidence estimates; our evaluation will be robust to missing judgments.",
                "If our confidence estimates are based on unrealistic assumptions, we cannot expect them to be accurate.",
                "The assumptions they are based on are the probabilities of relevance pi.",
                "We need these to be realistic.",
                "We argue that the best possible distribution of relevance p(Xi) is the one that explains all of the data (all of the observations made about the retrieval systems) while at the same time making no unwarranted assumptions.",
                "This is known as the principle of maximum entropy [13].",
                "The entropy of a random variable X with distribution p(X) is defined as H(p) = − i p(X = i) log p(X = i).",
                "This has found a wide array of uses in computer science and information retrieval.",
                "The maximum entropy distribution is the one that maximizes H. This distribution is unique and has an exponential form.",
                "The following theorem shows the utility of a maximum entropy distribution for relevance when estimating confidence.",
                "Theorem 1.",
                "If p(Xn |I, xm ) = argmaxpH(p), confidence estimates will be accurate. where xm is the set of relevance judgments defined above, Xn is the full set of documents that we wish to estimate the relevance of, and I is some information about the documents (unspecified as of now).",
                "We forgo the proof for the time being, but it is quite simple.",
                "This says that the better the estimates of relevance, the more accurate the evaluation.",
                "The task of creating a reusable test collection thus becomes the task of estimating the relevance of unjudged documents.",
                "The theorem and its proof say nothing whatsoever about the evaluation metric.",
                "The probability estimates are entirely indepedent of the measure we are interested in.",
                "This means the same probability estimates can tell us about average precision as well as precision, recall, bpref, etc.",
                "Furthermore, we could assume that the relevance of documents i and j is independent and achieve the same result, which we state as a corollary: Corollary 1.",
                "If p(Xi|I, xm ) = argmaxpH(p), confidence estimates will be accurate.",
                "The task therefore becomes the imputation of the missing values of relevance.",
                "The theorem implies that the closer we get to the maximum entropy distribution of relevance, the closer we get to robustness. 3.",
                "PREDICTING RELEVANCE In our statement of Theorem 1, we left the nature of the information I unspecified.",
                "One of the advantages of our confidence estimates is that they admit information from a wide variety of sources; essentially anything that can be modeled can be used as information for predicting relevance.",
                "A natural source of information is the retrieval systems themselves: how they ranked the judged documents, how often they failed to rank relevant documents, how they perform across topics, and so on.",
                "If we treat each system as an information retrieval expert providing an opinion about the relevance of each document, the problem becomes one of expert opinion aggregation.",
                "This is similar to the metasearch or data fusion problem in which the task is to take k input systems and merge them into a single ranking.",
                "Aslam et al. [3] previously identified a connection between evaluation and metasearch.",
                "Our problem has two key differences: 1.",
                "We explicitly need probabilities of relevance that we can plug into Eq. 1; metasearch algorithms have no such requirement. 2.",
                "We are accumulating relevance judgments as we proceed with the evaluation and are able to re-estimate relevance given each new judgment.",
                "In light of (1) above, we introduce a probabilistic model for expert combination. 3.1 A Model for Expert Opinion Aggregation Suppose that each expert j provides a probability of relevance qij = pj(Xi = 1).",
                "The information about the relevance of document i will then be the set of k expert opinions I = qi = (qi1, qi2, · · · , qik).",
                "The probability distribution we wish to find is the one that maximizes the entropy of pi = p(Xi = 1|qi).",
                "As it turns out, finding the maximum entropy model is equivalent to finding the parameters that maximize the likelihood [5].",
                "Blower [6] explicitly shows that finding the maximum entropy model for a binary variable is equivalent to solving a logistic regression.",
                "Then pi = p(Xi = 1|qi) = exp k j=1 λjqij 1 + exp k j=1 λj qij (2) where λ1, · · · , λk are the regression parameters.",
                "We include a beta prior for p(λj) with parameters α, β.",
                "This can be seen as a type of smoothing to account for the fact that the training data is highly biased.",
                "This model has the advantage of including the statistical dependence between the experts.",
                "A model of the same form was shown by Clemen & Winkler to be the best for aggregating expert probabilities [10].",
                "A similar maximumentropy-motivated approach has been used for expert aggregation [15].",
                "Aslam & Montague [1] used a similar model for metasearch, but assumed independence among experts.",
                "Where do the qij s come from?",
                "Using raw, uncalibrated scores as predictors will not work because score distributions vary too much between topics.",
                "A language modeling ranker, for instance, will typically give a much higher score to the top retrieved document for a short query than to the top retrieved document for a long query.",
                "We could train a separate predicting model for each topic, but that does not take advantage of all of the information we have: we may only have a handful of judgments for a topic, not enough to train a model to any confidence.",
                "Furthermore, it seems reasonable to assume that if an expert makes good predictions for one topic, it will make good predictions for other topics as well.",
                "We could use a hierarchical model [12], but that will not generalize to unseen topics.",
                "Instead, we will calibrate the scores of each expert individually so that scores can be compared both within topic and between topic.",
                "Thus our model takes into account not only the dependence between experts, but also the dependence between experts performances on different tasks (topics). 3.2 Calibrating Experts Each expert gives us a score and a rank for each document.",
                "We need to convert these to probabilities.",
                "A method such as the one used by Manmatha et al. [14] could be used to convert scores into probabilities of relevance.",
                "The pairwise preference method of Carterette & Petkova [9] could also be used, interpeting the ranking of one document over another as an expression of preference.",
                "Let q∗ ij be expert js self-reported probability that document i is relevant.",
                "Intuitively it seems clear that q∗ ij should decrease with rank, and it should be zero if document i is unranked (the expert did not believe it to be relevant).",
                "The pairwise preference model can handle these two requirements easily, so we will use it.",
                "Let θrj (i) be the relevance coefficient of the document at rank rj(i).",
                "We want to find the θs that maximize the likelihood function: Ljt(Θ) = rj (i)<rj (k) exp(θrj (i) − θrj (k)) 1 + exp(θrj (i) − θrj (k)) We again include a beta prior on p(θrj(i)) with parameters |Rt| + 1 and |Nt| + 1, the size of the sets of judged relevant and nonrelevant documents respectively.",
                "Using these as prior parameters ensures that the resulting probabilities will be concentrated around the ratio of relevant documents that have been discovered for topic t. This means that the probability estimates decrease by rank and are higher for topics that have more relevant documents.",
                "After finding the Θ that maximizes the likelihood, we have q∗ ij = exp(θrj (i)) 1+exp(θrj (i)) .",
                "We define θ∞ = −∞, so that the probability that an unranked document is relevant is 0.",
                "Since q∗ ij is based on the rank at which a document is retrieved rather than the identity of the document itself, the probabilities are identical from expert to expert, e.g. if expert E put document A at rank 1, and expert D put document B at rank 1, we will have q∗ AE = q∗ BD.",
                "Therefore we only have to solve this once for each topic.",
                "The above model gives topic-independent probabilities for each document.",
                "But suppose an expert who reports 90% probability is only right 50% of the time.",
                "Its opinion should be discounted based on its observed performance.",
                "Specifically, we want to learn a calibration function qij = Cj(q∗ ij) that will ensure that the predicted probabilities are tuned to the experts ability to retrieve relevant documents given the judgments that have been made to this point.",
                "Platts SVM calibration method [16] fits a sigmoid function between q∗ ij and the relevance judgments to obtain qij = Cj (q∗ ij) = exp(Aj +Bjq∗ ij ) 1+exp(Aj +Bj q∗ ij ) .",
                "Since q∗ ij is topic-independent, we only need to learn one calibration function for each expert.",
                "Once we have the calibration function, it is applied to adjust the experts predictions to their actual performance.",
                "The calibrated probabilities are plugged into model (2) to find the document probabilities.",
                "Figure 1: Conceptual diagram of our aggregation model.",
                "Experts E1, E2 have ranked documents A, B, C for topic T1 and documents D, E, F for topic T2.",
                "The first step is to obtain q∗ ij.",
                "Next is calibration to true performance to find qij .",
                "Finally we obtain pi = p(Xi = 1|qi1, qi2), · · · . 3.3 Model Summary Our model has three components that differ by the data they take as input and what they produce as output.",
                "A conceptual diagram is shown in Figure 1. 1. ranks → probabilities (per system per topic).",
                "This gives us q∗ ij, expert js self-reported probability of the relevance of document i.",
                "This is unsupervised; it requires no labeled data (though if we have some, we use it to set prior parameters). 2. probabilities → calibrated probabilities (per system).",
                "This gives us qij = Cj (q∗ ij), expert js calibrated probability of the relevance of document i.",
                "This is semisupervised; we have relevance judgments at some ranks which we use to impute the probability of relevance at other ranks. 3. calibrated probabilities → document probabilities.",
                "This gives us pi = p(Xi = 1|qi), the probability of relevance of document i given calibrated expert probabilities qij .",
                "This is supervised; we learn coefficients from a set of judged documents and use those to estimate the relevance of the unjudged documents.",
                "Although the model appears rather complex, it is really just three successive applications of logistic regression.",
                "As such, it can be implemented in a statistical programming language such as R in a few lines of code.",
                "The use of beta (conjugate) priors ensures that no expensive computational methods such as MCMC are necessary [12], so the model is trained and applied fast enough to be used on-line.",
                "Our code is available at http://ciir.cs.umass.edu/~carteret/. 4.",
                "EXPERIMENTS Three hypotheses are under consideration.",
                "The first, and most important, is that using our expert aggregation model to predict relevance produces test collections that are robust enough to be reusable; that is, we can trust the estimates of confidence when we evaluate systems that did not contribute any judgments to the pool.",
                "The other two hypotheses relate to the improvement we see by using better estimates of relevance than we did in our previous work [8].",
                "These are that (a) it takes fewer relevance track no. topics no. runs no. judged no. rel ad-hoc 94 50 40 97,319 9,805 ad-hoc 95 49 33 87,069 6,503 ad-hoc 96 50 61 133,681 5,524 ad-hoc 97 50 74 72,270 4,611 ad-hoc 98 50 103 80,345 4,674 ad-hoc 99 50 129 86,830 4,728 web 04 225 74 88,566 1,763 robust 05 50 74 37,798 6,561 terabyte 05 50 58 45,291 10,407 Table 1: Number of topics, number of runs, number of documents judged, and number found relevant for each of our data sets. judgments to reach 95% confidence and (b) the accuracy of the predictions is higher than if we were to simply assume pi = 0.5 for all unjudged documents. 4.1 Data We obtained full ad-hoc runs submitted to TRECs 3 through 8.",
                "Each run ranks at most 1000 documents for 50 topics (49 topics for TREC 4).",
                "Additionally, we obtained all runs from the Web track of TREC 13, the Robust2 track of TREC 14, and the Terabyte (ad-hoc) track of TREC 14.",
                "These are the tracks that have replaced the ad-hoc track since its end in 1999.",
                "Statistics are shown in Table 1.",
                "We set aside the TREC 4 (ad-hoc 95) set for training, TRECs 3 and 5-8 (ad-hoc 94 and 96-99) for primary testing, and the remaining sets for additional testing.",
                "We use the qrels files assembled by NIST as truth.",
                "The number of relevance judgments made and relevant documents found for each track are listed in Table 1.",
                "For computational reasons, we truncate ranked lists at 100 documents.",
                "There is no reason that we could not go deeper, but calculating variance is O(n3 ) and thus very timeconsuming.",
                "Because of the reciprocal rank nature of AP, we do not lose much information by truncating at rank 100. 4.2 Algorithms We will compare three algorithms for acquiring relevance judgments.",
                "The baseline is a variation of TREC pooling that we will call incremental pooling (IP).",
                "This algorithm takes a number k as input and presents the first k documents in rank order (without regard to topic) to be judged.",
                "It does not estimate the relevance of unjudged documents; it simply assumes any unjudged document is nonrelevant.",
                "The second algorithm is that presented in Carterette et al. [8] (Algorithm 1).",
                "Documents are selected based on how interesting they are in determining whether a difference in mean average precision exists.",
                "For this approach pi = 0.5 for all i; there is no estimation of probabilities.",
                "We will call this MTC for minimal test collection.",
                "The third algorithm augments MTC with updated estimates of probabilities of relevance.",
                "We will call this RTC for robust test collection.",
                "It is identical to Algorithm 1, except that every 10th iteration we estimate pi for all unjudged documents i using the expert aggregation model of Section 3.",
                "RTC has smoothing (prior distribution) parameters that must be set.",
                "We trained using the ad-hoc 95 set.",
                "We limited 2 Robust here means robust retrieval; this is different from our goal of robust evaluation.",
                "Algorithm 1 (MTC) Given two ranked lists and confidence level α, predict the sign of ΔMAP. 1: pi ← 0.5 for all documents i 2: while P(ΔMAP < 0) < α do 3: calculate weight wi for all unjudged documents i (see Carterette et al. [8] for details) 4: j ← argmaxiwi 5: xj ← 1 if document j is relevant, 0 otherwise 6: pj ← xj 7: end while the search to uniform priors with relatively high variance.",
                "For expert aggregation, the prior parameters are α = β = 1. 4.3 Experimental Design First, we want to know whether we can augment a set of relevance judgments with a set of relevance probabilities in order to reuse the judgments to evaluate a new set of systems.",
                "For each experimental trial: 1.",
                "Pick a random subset of k runs. 2.",
                "From those k, pick an initial c < k to evaluate. 3.",
                "Run RTC to 95% confidence on the initial c. 4.",
                "Using the model from Section 3, estimate the probabilities of relevance for all documents retrieved by all k runs. 5.",
                "Calculate EMAP for all k runs, and P(ΔMAP < 0) for all pairs of runs.",
                "We do the same for MTC, but omit step 4.",
                "Note that after evaluating the first c systems, we make no additional relevance judgments.",
                "To put our method to the test, we selected c = 2: we will build a set of judgments from evaluating only two initial systems.",
                "We will then generalize to a set of k = 10 (of which those two are a subset).",
                "As we run more trials, we obtain the data we need to test all three of our hypotheses. 4.4 Experimental Evaluation Recall that a set of judgments is robust if the accuracy of the predictions it makes is at least its estimated confidence.",
                "One way to evaluate robustness is to bin pairs by their confidence, then calculate the accuracy over all the pairs in each bin.",
                "We would like the accuracy to be no less than the lowest confidence score in the bin, but preferably higher.",
                "Since summary statistics are useful, we devised the following metric.",
                "Suppose we are a bookmaker taking bets on whether ΔMAP < 0.",
                "We use RTC or MTC to set the odds O = P (ΔMAP <0) 1−P (ΔMAP <0) .",
                "Suppose a bettor wagers $1 on ΔMAP ≥ 0.",
                "If it turns out that ΔMAP < 0, we win the dollar.",
                "Otherwise, we pay out O.",
                "If our confidence estimates are perfectly accurate, we break even.",
                "If confidence is greater than accuracy, we lose money; we win if accuracy is greater than confidence.",
                "Counterintuitively, the most desirable outcome is breaking even: if we lose money, we cannot trust the confidence estimates, but if we win money, we have either underestimated confidence or judged more documents than necessary.",
                "However, the cost of not being able to trust the confidence estimates is higher than the cost of extra relevance judgments, so we will treat positive outcomes as good.",
                "The amount we win on each pairwise comparison i is: Wi = yi − (1 − yi) Pi 1 − Pi = yi − Pi 1 − Pi yi = 1 if ΔMAP < 0 and 0 otherwise, and Pi = P(ΔMAP < 0).",
                "The summary statistic is W, the mean of Wi.",
                "Note that as Pi increases, we lose more for being wrong.",
                "This is as it should be: the penalty should be great for missing the high probability predictions.",
                "However, since our losses grow without bound as predictions approach certainty, we cap −Wi at 100.",
                "For our hypothesis that RTC requires fewer judgments than MTC, we are interested in the number of judgments needed to reach 95% confidence on the first pair of systems.",
                "The median is more interesting than the mean: most pairs require a few hundred judgments, but a few pairs require several thousand.",
                "The distribution is therefore highly skewed, and the mean strongly affected by those outliers.",
                "Finally, for our hypothesis that RTC is more accurate than MTC, we will look at Kendalls τ correlation between a ranking of k systems by a small set of judgments and the true ranking using the full set of judgments.",
                "Kendalls τ, a nonparametric statistic based on pairwise swaps between two lists, is a standard evaluation for this type of study.",
                "It ranges from −1 (perfectly anti-correlated) to 1 (rankings identical), with 0 meaning that half of the pairs are swapped.",
                "As we touched on in the introduction, though, an accuracy measure like rank correlation is not a good evaluation of reusability.",
                "We include it for completeness. 4.4.1 Hypothesis Testing Running multiple trials allows the use of statistical hypothesis testing to compare algorithms.",
                "Using the same sets of systems allows the use of paired tests.",
                "As we stated above, we are more interested in the median number of judgments than the mean.",
                "A test for difference in median is the Wilcoxon sign rank test.",
                "We can also use a paired t-test to test for a difference in mean.",
                "For rank correlation, we can use a paired t-test to test for a difference in τ. 5.",
                "RESULTS AND ANALYSIS The comparison between MTC and RTC is shown in Table 2.",
                "With MTC and uniform probabilities of relevance, the results are far from robust.",
                "We cannot reuse the relevance judgments with much confidence.",
                "But with RTC, the results are very robust.",
                "There is a slight dip in accuracy when confidence gets above 0.95; nonetheless, the confidence predictions are trustworthy.",
                "Mean Wi shows that RTC is much closer to 0 than MTC.",
                "The distribution of confidence scores shows that at least 80% confidence is achieved more than 35% of the time, indicating that neither algorithm is being too conservative in its confidence estimates.",
                "The confidence estimates are rather low overall; that is because we have built a test collection from only two initial systems.",
                "Recall from Section 1 that we cannot require (or even expect) a minimum level of confidence when we generalize to new systems.",
                "More detailed results for both algorithms are shown in Figure 2.",
                "The solid line is the ideal result that would give W = 0.",
                "RTC is on or above this line at all points until confidence reaches about 0.97.",
                "After that there is a slight dip in accuracy which we discuss below.",
                "Note that both MTC RTC confidence % in bin accuracy % in bin accuracy 0.5 − 0.6 33.7% 61.7% 28.6% 61.9% 0.6 − 0.7 18.1% 73.1% 20.1% 76.3% 0.7 − 0.8 10.4% 70.1% 15.5% 78.0% 0.8 − 0.9 9.4% 69.0% 12.1% 84.9% 0.9 − 0.95 7.3% 78.0% 6.6% 93.1% 0.95 − 0.99 17.9% 70.4% 12.4% 93.4% 1.0 3.3% 68.3% 4.7% 98.9% W −5.34 −0.39 median judged 251 235 mean τ 0.393 0.555 Table 2: Confidence that P(ΔMAP < 0) and accuracy of prediction when generalizing a set of relevance judgments acquired using MTC and RTC.",
                "Each bin contains over 1,000 trials from the adhoc 3, 5-8 sets.",
                "RTC is much more robust than MTC.",
                "W is defined in Section 4.4; closer to 0 is better.",
                "Median judged is the number of judgments to reach 95% confidence on the first two systems.",
                "Mean τ is the average rank correlation for all 10 systems. 0.5 0.6 0.7 0.8 0.9 1 0.5 0.6 0.7 0.8 0.9 1 accuracy confidence breakeven RTC MTC Figure 2: Confidence vs. accuracy of MTC and RTC.",
                "The solid line is the perfect result that would give W = 0; performance should be on or above this line.",
                "Each point represents at least 500 pairwise comparisons. algorithms are well above the line up to around confidence 0.7.",
                "This is because the baseline performance on these data sets is high; it is quite easy to achieve 75% accuracy doing very little work [7].",
                "Number of Judgments: The median number of judgments required by MTC to reach 95% confidence on the first two systems is 251, an average of 5 per topic.",
                "The median required by RTC is 235, about 4.7 per topic.",
                "Although the numbers are close, RTCs median is significantly lower by a paired Wilcoxon test (p < 0.0001).",
                "For comparison, a pool of depth 100 would result in a minimum of 5,000 judgments for each pair.",
                "The difference in means is much greater.",
                "MTC required a mean of 823 judgments, 16 per topic, while RTC required a mean of 502, 10 per topic. (Recall that means are strongly skewed by a few pairs that take thousands of judgments.)",
                "This difference is significant by a paired t-test (p < 0.0001).",
                "Ten percent of the sets resulted in 100 or fewer judgments (less than two per topic).",
                "Performance on these is very high: W = 0.41, and 99.7% accuracy when confidence is at least 0.9.",
                "This shows that even tiny collections can be reusable.",
                "For the 50% of sets with more than 235 judgments, accuracy is 93% when confidence is at least 0.9.",
                "Rank Correlation: MTC and RTC both rank the 10 systems by EMAP (Eq. (1)) calculated using their respective probability estimates.",
                "The mean τ rank correlation between true MAP and EMAP is 0.393 for MTC and 0.555 for RTC.",
                "This difference is significant by a paired t-test (p < 0.0001).",
                "Note that we do not expect the τ correlations to be high, since we are ranking the systems with so few relevance judgments.",
                "It is more important that we estimate confidence in each pairwise comparison correctly.",
                "We ran IP for the same number of judgments that MTC took for each pair, then ranked the systems by MAP using only those judgments (all unjudged documents assumed nonrelevant).",
                "We calculated the τ correlation to the true ranking.",
                "The mean τ correlation is 0.398, which is not significantly different from MTC, but is significantly lower than RTC.",
                "Using uniform estimates of probability is indistinguishable from the baseline, whereas estimating relevance by expert aggregation boosts performance a great deal: nearly 40% over both MTC and IP.",
                "Overfitting: It is possible to overfit: if too many judgments come from the first two systems, the variance in ΔMAP is reduced and the confidence estimates become unreliable.",
                "We saw this in Table 2 and Figure 2 where RTC exhibits a dip in accuracy when confidence is around 97%.",
                "In fact, the number of judgments made prior to a wrong prediction is over 50% greater than the number made prior to a correct prediction.",
                "Overfitting is difficult to quantify exactly, because making more relevance judgments does not always cause it: at higher confidence levels, more relevance judgments are made, and as Table 2 shows, accuracy is greater at those higher confidences.",
                "Obviously having more relevance judgments should increase both confidence and accuracy; the difference seems to be when one system has a great deal more judgments than the other.",
                "Pairwise Comparisons: Our pairwise comparisons fall into one of three groups: 1. the two original runs from which relevance judgments are acquired; 2. one of the original runs vs. one of the new runs; 3. two new runs.",
                "Table 3 shows confidence vs. accuracy results for each of these three groups.",
                "Interestingly, performance is worst when comparing one of the original runs to one of the additional runs.",
                "This is most likely due to a large difference in the number of judgments affecting the variance of ΔMAP.",
                "Nevertheless, performance is quite good on all three subsets.",
                "Worst Case: The case intuitively most likely to produce an error is when the two systems being compared have retrieved very few documents in common.",
                "If we want the judgments to be reusable, we should to be able to generalize even to runs that are very different from the ones used to acquire the relevance judgments.",
                "A simple measure of similarity of two runs is the average percentage of documents they retrieved in common for each topic [2].",
                "We calculated this for all pairs, then looked at performance on pairs with low similarity.",
                "Results are shown in accuracy confidence two original one original no original 0.5 − 0.6 - 48.1% 62.8% 0.6 − 0.7 - 57.1% 79.2% 0.7 − 0.8 - 67.9% 81.7% 0.8 − 0.9 - 82.2% 86.3% 0.9 − 0.95 95.9% 93.7% 92.6% 0.95 − 0.99 96.2% 92.5% 93.1% 1.0 100% 98.0% 99.1% W −1.11 −0.87 −0.27 Table 3: Confidence vs. accuracy of RTC when comparing the two original runs, one original run and one new run, and two new runs.",
                "RTC is robust in all three cases. accuracy when similar confidence 0 − 0.1 0.1 − 0.2 0.2 − 0.3 0.5 − 0.6 68.4% 63.1% 61.4% 0.6 − 0.7 84.2% 78.6% 76.6% 0.7 − 0.8 82.0% 79.8% 78.9% 0.8 − 0.9 93.6% 83.3% 82.1% 0.9 − 0.95 99.3% 92.7% 92.4% 0.95 − 0.99 98.7% 93.4% 93.3% 1.0 99.9% 97.9% 98.1% W 0.44 −0.45 −0.49 Table 4: Confidence vs. accuracy of RTC when a pair of systems retrieved 0-30% documents in common (broken out into 0%-10%, 10%-20%, and 20%30%).",
                "RTC is robust in all three cases.",
                "Table 4.",
                "Performance is in fact very robust even when similarity is low.",
                "When the two runs share very few documents in common, W is actually positive.",
                "MTC and IP both performed quite poorly in these cases.",
                "When the similarity was between 0 and 10%, both MTC and IP correctly predicted ΔMAP only 60% of the time, compared to an 87.6% success rate for RTC.",
                "By Data Set: All the previous results have only been on the ad-hoc collections.",
                "We did the same experiments on our additional data sets, and broke out the results by data set to see how performance varies.",
                "The results in Table 5 show everything about each set, including binned accuracy, W, mean τ, and median number of judgments to reach 95% confidence on the first two systems.",
                "The results are highly consistent from collection to collection, suggesting that our method is not overfitting to any particular data set. 6.",
                "CONCLUSIONS AND FUTURE WORK In this work we have offered the first formal definition of the common idea of reusability of a test collection and presented a model that is able to achieve reusability with very small sets of relevance judgments.",
                "Table 2 and Figure 2 together show how biased a small set of judgments can be: MTC is dramatically overestimating confidence and is much less accurate than RTC, which is able to remove the bias to give a robust evaluation.",
                "The confidence estimates of RTC, in addition to being accurate, provide a guide for obtaining additional judgments: focus on judging documents from the lowest-confidence comparisons.",
                "In the long run, we see small sets of relevance judgaccuracy confidence ad-hoc 94 ad-hoc 96 ad-hoc 97 ad-hoc 98 ad-hoc 99 web 04 robust 05 terabyte 05 0.5 − 0.6 64.1% 61.8% 62.2% 62.0% 59.4% 64.3% 61.5% 61.6% 0.6 − 0.7 76.1% 77.8% 74.5% 78.2% 74.3% 78.1% 75.9% 75.9% 0.7 − 0.8 75.2% 78.9% 77.6% 80.0% 78.6% 82.6% 77.5% 80.4% 0.8 − 0.9 83.2% 85.5% 84.6% 84.9% 86.8% 84.5% 86.7% 87.7% 0.9 − 0.95 93.0% 93.6% 92.8% 93.7% 92.6% 94.2% 93.9% 94.2% 0.95 − 0.99 93.1% 94.3% 93.1% 93.7% 92.8% 95.0% 93.9% 91.6% 1.0 99.2% 96.8% 98.7% 99.5% 99.6% 100% 99.2% 98.3% W -0.34 -0.34 -0.48 -0.35 -0.44 -0.07 -0.41 -0.67 median judged 235 276 243 213 179 448 310 320 mean τ 0.538 0.573 0.556 0.579 0.532 0.596 0.565 0.574 Table 5: Accuracy, W, mean τ, and median number of judgments for all 8 testing sets.",
                "The results are highly consistent across data sets. ments being shared by researchers, each group contributing a few more judgments to gain more confidence about their particular systems.",
                "As time goes on, the number of judgments grows until there is 100% confidence in every evaluation-and there is a full test collection for the task.",
                "We see further use for this method in scenarios such as web retrieval in which the corpus is frequently changing.",
                "It could be applied to evaluation on a dynamic test collection as defined by Soboroff [18].",
                "The model we presented in Section 3 is by no means the only possibility for creating a robust test collection.",
                "A simpler expert aggregation model might perform as well or better (though all our efforts to simplify failed).",
                "In addition to expert aggregation, we could estimate probabilities by looking at similarities between documents.",
                "This is an obvious area for future exploration.",
                "Additionally, it will be worthwhile to investigate the issue of overfitting: the circumstances it occurs under and what can be done to prevent it.",
                "In the meantime, capping confidence estimates at 95% is a hack that solves the problem.",
                "We have many more experimental results that we unfortunately did not have space for but that reinforce the notion that RTC is highly robust: with just a few judgments per topic, we can accurately assess the confidence in any pairwise comparison of systems.",
                "Acknowledgments This work was supported in part by the Center for Intelligent Information Retrieval and in part by the Defense Advanced Research Projects Agency (DARPA) under contract number HR0011-06-C-0023.",
                "Any opinions, findings, and conclusions or recommendations expressed in this material are those of the author and do not necessarily reflect those of the sponsor. 7.",
                "REFERENCES [1] J. Aslam and M. Montague.",
                "Models for Metasearch.",
                "In Proceedings of SIGIR, pages 275-285, 2001. [2] J. Aslam and R. Savell.",
                "On the effectiveness of evaluating retrieval systems in the absence of relevance judgments.",
                "In Proceedings of SIGIR, pages 361-362, 2003. [3] J.",
                "A. Aslam, V. Pavlu, and R. Savell.",
                "A unified model for metasearch, pooling, and system evaluation.",
                "In Proceedings of CIKM, pages 484-491, 2003. [4] J.",
                "A. Aslam, V. Pavlu, and E. Yilmaz.",
                "A statistical method for system evaluation using incomplete judgments.",
                "In Proceedings of SIGIR, pages 541-548, 2006. [5] A. L. Berger, S. D. Pietra, and V. J. D. Pietra.",
                "A maximum entropy approach to natural language processing.",
                "Computational Linguistics, 22(1):39-71, 1996. [6] D. J. Blower.",
                "An easy derivation of logistic regression from the bayesian and maximum entropy perspective.",
                "In Proceedings of the 23rd International Workship on Bayesian Inference and Maximum Entropy Methods in Science and Engineering, pages 30-43, 2004. [7] B. Carterette and J. Allan.",
                "Research methodology in studies of assessor effort for retrieval evaluation.",
                "In Proceedings of RIAO, 2007. [8] B. Carterette, J. Allan, and R. K. Sitaraman.",
                "Minimal test collections for retrieval evaluation.",
                "In Proceedings of SIGIR, pages 268-275, 2006. [9] B. Carterette and D. I. Petkova.",
                "Learning a ranking from pairwise preferences.",
                "In Proceedings of SIGIR, 2006. [10] R. T. Clemen and R. L. Winkler.",
                "Unanimity and compromise among probability forecasters.",
                "Management Science, 36(7):767-779, July 1990. [11] G. V. Cormack, C. R. Palmer, and C. L. Clarke.",
                "Efficient Construction of Large Test Collections.",
                "In Proceedings of SIGIR, pages 282-289, 1998. [12] A. Gelman, J.",
                "B. Carlin, H. S. Stern, and D. B. Rubin.",
                "Bayesian Data Analysis.",
                "Chapman & Hall/CRC, 2004. [13] E. T. Jaynes.",
                "Probability Theory: The Logic of Science.",
                "Cambridge University Press, 2003. [14] R. Manmatha and H. Sever.",
                "A Formal Approach to Score Normalization for Metasearch.",
                "In Proceedings of HLT, pages 88-93, 2002. [15] I. J. Myung, S. Ramamoorti, and J. Andrew D. Baily.",
                "Maximum entropy aggregation of expert predictions.",
                "Management Science, 42(10):1420-1436, October 1996. [16] J. Platt.",
                "Probabilistic outputs for support vector machines and comparison to regularized likelihood methods. pages 61-74, 2000. [17] M. Sanderson and H. Joho.",
                "Forming test collections with no system pooling.",
                "In Proceedings of the 27th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 33-40, 2004. [18] I. Soboroff.",
                "Dynamic test collections: measuring search effectiveness on the live web.",
                "In Proceedings of SIGIR, pages 276-283, 2006. [19] K. Sparck Jones and C. J. van Rijsbergen.",
                "Information Retrieval Test Collections.",
                "Journal of Documentation, 32(1):59-75, 1976. [20] E. M. Voorhees and D. K. Harman, editors.",
                "TREC: Experiment and Evaluation in Information Retrieval.",
                "MIT Press, 2005. [21] J. Zobel.",
                "How Reliable are the Results of Large-Scale Information Retrieval Experiments?",
                "In Proceedings of SIGIR, pages 307-314, 1998."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [],
            "translated_text": "",
            "candidates": [],
            "error": [
                []
            ]
        },
        "mtc": {
            "translated_key": "",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Robust Test Collections for Retrieval Evaluation Ben Carterette Center for Intelligent Information Retrieval Computer Science Department University of Massachusetts Amherst Amherst, MA 01003 carteret@cs.umass.edu ABSTRACT Low-cost methods for acquiring relevance judgments can be a boon to researchers who need to evaluate new retrieval tasks or topics but do not have the resources to make thousands of judgments.",
                "While these judgments are very useful for a one-time evaluation, it is not clear that they can be trusted when re-used to evaluate new systems.",
                "In this work, we formally define what it means for judgments to be reusable: the confidence in an evaluation of new systems can be accurately assessed from an existing set of relevance judgments.",
                "We then present a method for augmenting a set of relevance judgments with relevance estimates that require no additional assessor effort.",
                "Using this method practically guarantees reusability: with as few as five judgments per topic taken from only two systems, we can reliably evaluate a larger set of ten systems.",
                "Even the smallest sets of judgments can be useful for evaluation of new systems.",
                "Categories and Subject Descriptors: H.3 Information Storage and Retrieval; H.3.4 Systems and Software: Performance Evaluation General Terms: Experimentation, Measurement, Reliability 1.",
                "INTRODUCTION Consider an information retrieval researcher who has invented a new retrieval task.",
                "She has built a system to perform the task and wants to evaluate it.",
                "Since the task is new, it is unlikely that there are any extant relevance judgments.",
                "She does not have the time or resources to judge every document, or even every retrieved document.",
                "She can only judge the documents that seem to be the most informative and stop when she has a reasonable degree of confidence in her conclusions.",
                "But what happens when she develops a new system and needs to evaluate it?",
                "Or another research group decides to implement a system to perform the task?",
                "Can they reliably reuse the original judgments?",
                "Can they evaluate without more relevance judgments?",
                "Evaluation is an important aspect of information retrieval research, but it is only a semi-solved problem: for most retrieval tasks, it is impossible to judge the relevance of every document; there are simply too many of them.",
                "The solution used by NIST at TREC (Text REtrieval Conference) is the pooling method [19, 20]: all competing systems contribute N documents to a pool, and every document in that pool is judged.",
                "This method creates large sets of judgments that are reusable for training or evaluating new systems that did not contribute to the pool [21].",
                "This solution is not adequate for our hypothetical researcher.",
                "The pooling method gives thousands of relevance judgments, but it requires many hours of (paid) annotator time.",
                "As a result, there have been a slew of recent papers on reducing annotator effort in producing test collections: Cormack et al. [11], Zobel [21], Sanderson and Joho [17], Carterette et al. [8], and Aslam et al. [4], among others.",
                "As we will see, the judgments these methods produce can significantly bias the evaluation of a new set of systems.",
                "Returning to our hypothetical resesarcher, can she reuse her relevance judgments?",
                "First we must formally define what it means to be reusable.",
                "In previous work, reusability has been tested by simply assessing the accuracy of a set of relevance judgments at evaluating unseen systems.",
                "While we can say that it was right 75% of the time, or that it had a rank correlation of 0.8, these numbers do not have any predictive power: they do not tell us which systems are likely to be wrong or how confident we should be in any one.",
                "We need a more careful definition of reusability.",
                "Specifically, the question of reusability is not how accurately we can evaluate new systems.",
                "A malicious adversary can always produce a new ranked list that has not retrieved any of the judged documents.",
                "The real question is how much confidence we have in our evaluations, and, more importantly, whether we can trust our estimates of confidence.",
                "Even if confidence is not high, as long as we can trust it, we can identify which systems need more judgments in order to increase confidence.",
                "Any set of judgments, no matter how small, becomes reusable to some degree.",
                "Small, reusable test collections could have a huge impact on information retrieval research.",
                "Research groups would be able to share the relevance judgments they have done in-house for pilot studies, new tasks, or new topics.",
                "The amount of data available to researchers would grow exponentially over time. 2.",
                "ROBUST EVALUATION Above we gave an intuitive definition of reusability: a collection is reusable if we can trust our estimates of confidence in an evaluation.",
                "By that we mean that if we have made some relevance judgments and have, for example, 75% confidence that system A is better than system B, we would like there to be no more than 25% chance that our assessment of the relative quality of the systems will change as we continue to judge documents.",
                "Our evaluation should be robust to missing judgments.",
                "In our previous work, we defined confidence as the probability that the difference in an evaluation measure calculated for two systems is less than zero [8].",
                "This notion of confidence is defined in the context of a particular evaluation task that we call comparative evaluation: determining the sign of the difference in an evaluation measure.",
                "Other evaluation tasks could be defined; estimating the magnitude of the difference or the values of the measures themselves are examples that entail different notions of confidence.",
                "We therefore see confidence as a probability estimate.",
                "One of the questions we must ask about a probability estimate is what it means.",
                "What does it mean to have 75% confidence that system A is better than system B?",
                "As described above, we want it to mean that if we continue to judge documents, there will only be a 25% chance that our assessment will change.",
                "If this is what it means, we can trust the confidence estimates.",
                "But do we know it has that meaning?",
                "Our calculation of confidence rested on an assumption about the probability of relevance of unjudged documents, specifically that each unjudged document was equally likely to be relevant or nonrelevant.",
                "This assumption is almost certainly not realistic in most IR applications.",
                "As it turns out, it is this assumption that determines whether the confidence estimates can eb trusted.",
                "Before elaborating on this, we formally define confidence. 2.1 Estimating Confidence Average precision (AP) is a standard evaluation metric that captures both the ability of a system to rank relevant documents highly (precision) as well as its ability to retrieve relevant documents (recall).",
                "It is typically written as the mean precision at the ranks of relevant documents: AP = 1 |R| i∈R prec@r(i) where R is the set of relevant documents and r(i) is the rank of document i.",
                "Let Xi be a random variable indicating the relevance of document i.",
                "If documents are ordered by rank, we can express precision as prec@i = 1/i i j=1 Xj .",
                "Average precision then becomes the quadratic equation AP = 1 Xi n i=1 Xi/i i j=1 Xj = 1 Xi n i=1 j≥i aijXiXj where aij = 1/ max{r(i), r(j)}.",
                "Using aij instead of 1/i allows us to number the documents arbitrarily.",
                "To see why this is true, consider a toy example: a list of 3 documents with relevant documents B, C at ranks 1 and 3 and nonrelevant document A at rank 2.",
                "Average precision will be 1 2 (1 1 x2 B+ 1 2 xBxA+ 1 3 xBxC + 1 2 x2 A+ 1 3 xAxC + 1 3 x2 C) = 1 2 1 + 2 3 because xA = 0, xB = 1, xC = 1.",
                "Though the ordering B, A, C is different from the labeling A, B, C, it does not affect the computation.",
                "We can now see average precision itself is a random variable with a distribution over all possible assignments of relevance to all documents.",
                "This random variable has an expectation, a variance, confidence intervals, and a certain probability of being less than or equal to a given value.",
                "All of these are dependent on the probability that document i is relevant: pi = p(Xi = 1).",
                "Suppose in our previous example we do not know the relevance judgments, but we believe pA = 0.4, pB = 0.8, pC = 0.7.",
                "We can then compute e.g.",
                "P(AP = 0) = 0.2 · 0.6 · 0.3 = 0.036, or P(AP = 1 2 ) = 0.2 · 0.4 · 0.7 = 0.056.",
                "Summing over all possibilities, we can compute expectation and variance: E[AP] ≈ 1 pi aiipi + j>i aij pipj V ar[AP] ≈ 1 ( pi)2 n i a2 iipiqi + j>i a2 ijpipj(1 − pipj) + i=j 2aiiaijpipj(1 − pi) + k>j=i 2aijaikpipjpk(1 − pi) AP asymptotically converges to a normal distribution with expectation and variance as defined above.1 For our comparative evaluation task we are interested in the sign of the difference in two average precisions: ΔAP = AP1 − AP2.",
                "As we showed in our previous work, ΔAP has a closed form when documents are ordered arbitrarily: ΔAP = 1 Xi n i=1 j≥i cij XiXj cij = aij − bij where bij is defined analogously to aij for the second ranking.",
                "Since AP is normal, ΔAP is normal as well, meaning we can use the normal cumulative density function to determine the confidence that a difference in AP is less than zero.",
                "Since topics are independent, we can easily extend this to mean average precision (MAP).",
                "MAP is also normally distributed; its expectation and variance are: EMAP = 1 T t∈T E[APt] (1) VMAP = 1 T2 t∈T V ar[APt] ΔMAP = MAP1 − MAP2 Confidence can then be estimated by calculating the expectation and variance and using the normal density function to find P(ΔMAP < 0). 2.2 Confidence and Robustness Having defined confidence, we turn back to the issue of trust in confidence estimates, and show how it ties into the robustness of the collection to missing judgments. 1 These are actually approximations to the true expectation and variance, but the error is a negligible O(n2−n ).",
                "Let Z be the set of all pairs of ranked results for a common set of topics.",
                "Suppose we have a set of m relevance judgments xm = {x1, x2, ..., xm} (using small x rather than capital X to distinguish between judged and unjudged documents); these are the judgments against which we compute confidence.",
                "Let Zα be the subset of pairs in Z for which we predict that ΔMAP = −1 with confidence α given the judgments xm .",
                "For the confidence estimates to be accurate, we need at least α · |Zα| of these pairs to actually have ΔMAP = −1 after we have judged every document.",
                "If they do, we can trust the confidence estimates; our evaluation will be robust to missing judgments.",
                "If our confidence estimates are based on unrealistic assumptions, we cannot expect them to be accurate.",
                "The assumptions they are based on are the probabilities of relevance pi.",
                "We need these to be realistic.",
                "We argue that the best possible distribution of relevance p(Xi) is the one that explains all of the data (all of the observations made about the retrieval systems) while at the same time making no unwarranted assumptions.",
                "This is known as the principle of maximum entropy [13].",
                "The entropy of a random variable X with distribution p(X) is defined as H(p) = − i p(X = i) log p(X = i).",
                "This has found a wide array of uses in computer science and information retrieval.",
                "The maximum entropy distribution is the one that maximizes H. This distribution is unique and has an exponential form.",
                "The following theorem shows the utility of a maximum entropy distribution for relevance when estimating confidence.",
                "Theorem 1.",
                "If p(Xn |I, xm ) = argmaxpH(p), confidence estimates will be accurate. where xm is the set of relevance judgments defined above, Xn is the full set of documents that we wish to estimate the relevance of, and I is some information about the documents (unspecified as of now).",
                "We forgo the proof for the time being, but it is quite simple.",
                "This says that the better the estimates of relevance, the more accurate the evaluation.",
                "The task of creating a reusable test collection thus becomes the task of estimating the relevance of unjudged documents.",
                "The theorem and its proof say nothing whatsoever about the evaluation metric.",
                "The probability estimates are entirely indepedent of the measure we are interested in.",
                "This means the same probability estimates can tell us about average precision as well as precision, recall, bpref, etc.",
                "Furthermore, we could assume that the relevance of documents i and j is independent and achieve the same result, which we state as a corollary: Corollary 1.",
                "If p(Xi|I, xm ) = argmaxpH(p), confidence estimates will be accurate.",
                "The task therefore becomes the imputation of the missing values of relevance.",
                "The theorem implies that the closer we get to the maximum entropy distribution of relevance, the closer we get to robustness. 3.",
                "PREDICTING RELEVANCE In our statement of Theorem 1, we left the nature of the information I unspecified.",
                "One of the advantages of our confidence estimates is that they admit information from a wide variety of sources; essentially anything that can be modeled can be used as information for predicting relevance.",
                "A natural source of information is the retrieval systems themselves: how they ranked the judged documents, how often they failed to rank relevant documents, how they perform across topics, and so on.",
                "If we treat each system as an information retrieval expert providing an opinion about the relevance of each document, the problem becomes one of expert opinion aggregation.",
                "This is similar to the metasearch or data fusion problem in which the task is to take k input systems and merge them into a single ranking.",
                "Aslam et al. [3] previously identified a connection between evaluation and metasearch.",
                "Our problem has two key differences: 1.",
                "We explicitly need probabilities of relevance that we can plug into Eq. 1; metasearch algorithms have no such requirement. 2.",
                "We are accumulating relevance judgments as we proceed with the evaluation and are able to re-estimate relevance given each new judgment.",
                "In light of (1) above, we introduce a probabilistic model for expert combination. 3.1 A Model for Expert Opinion Aggregation Suppose that each expert j provides a probability of relevance qij = pj(Xi = 1).",
                "The information about the relevance of document i will then be the set of k expert opinions I = qi = (qi1, qi2, · · · , qik).",
                "The probability distribution we wish to find is the one that maximizes the entropy of pi = p(Xi = 1|qi).",
                "As it turns out, finding the maximum entropy model is equivalent to finding the parameters that maximize the likelihood [5].",
                "Blower [6] explicitly shows that finding the maximum entropy model for a binary variable is equivalent to solving a logistic regression.",
                "Then pi = p(Xi = 1|qi) = exp k j=1 λjqij 1 + exp k j=1 λj qij (2) where λ1, · · · , λk are the regression parameters.",
                "We include a beta prior for p(λj) with parameters α, β.",
                "This can be seen as a type of smoothing to account for the fact that the training data is highly biased.",
                "This model has the advantage of including the statistical dependence between the experts.",
                "A model of the same form was shown by Clemen & Winkler to be the best for aggregating expert probabilities [10].",
                "A similar maximumentropy-motivated approach has been used for expert aggregation [15].",
                "Aslam & Montague [1] used a similar model for metasearch, but assumed independence among experts.",
                "Where do the qij s come from?",
                "Using raw, uncalibrated scores as predictors will not work because score distributions vary too much between topics.",
                "A language modeling ranker, for instance, will typically give a much higher score to the top retrieved document for a short query than to the top retrieved document for a long query.",
                "We could train a separate predicting model for each topic, but that does not take advantage of all of the information we have: we may only have a handful of judgments for a topic, not enough to train a model to any confidence.",
                "Furthermore, it seems reasonable to assume that if an expert makes good predictions for one topic, it will make good predictions for other topics as well.",
                "We could use a hierarchical model [12], but that will not generalize to unseen topics.",
                "Instead, we will calibrate the scores of each expert individually so that scores can be compared both within topic and between topic.",
                "Thus our model takes into account not only the dependence between experts, but also the dependence between experts performances on different tasks (topics). 3.2 Calibrating Experts Each expert gives us a score and a rank for each document.",
                "We need to convert these to probabilities.",
                "A method such as the one used by Manmatha et al. [14] could be used to convert scores into probabilities of relevance.",
                "The pairwise preference method of Carterette & Petkova [9] could also be used, interpeting the ranking of one document over another as an expression of preference.",
                "Let q∗ ij be expert js self-reported probability that document i is relevant.",
                "Intuitively it seems clear that q∗ ij should decrease with rank, and it should be zero if document i is unranked (the expert did not believe it to be relevant).",
                "The pairwise preference model can handle these two requirements easily, so we will use it.",
                "Let θrj (i) be the relevance coefficient of the document at rank rj(i).",
                "We want to find the θs that maximize the likelihood function: Ljt(Θ) = rj (i)<rj (k) exp(θrj (i) − θrj (k)) 1 + exp(θrj (i) − θrj (k)) We again include a beta prior on p(θrj(i)) with parameters |Rt| + 1 and |Nt| + 1, the size of the sets of judged relevant and nonrelevant documents respectively.",
                "Using these as prior parameters ensures that the resulting probabilities will be concentrated around the ratio of relevant documents that have been discovered for topic t. This means that the probability estimates decrease by rank and are higher for topics that have more relevant documents.",
                "After finding the Θ that maximizes the likelihood, we have q∗ ij = exp(θrj (i)) 1+exp(θrj (i)) .",
                "We define θ∞ = −∞, so that the probability that an unranked document is relevant is 0.",
                "Since q∗ ij is based on the rank at which a document is retrieved rather than the identity of the document itself, the probabilities are identical from expert to expert, e.g. if expert E put document A at rank 1, and expert D put document B at rank 1, we will have q∗ AE = q∗ BD.",
                "Therefore we only have to solve this once for each topic.",
                "The above model gives topic-independent probabilities for each document.",
                "But suppose an expert who reports 90% probability is only right 50% of the time.",
                "Its opinion should be discounted based on its observed performance.",
                "Specifically, we want to learn a calibration function qij = Cj(q∗ ij) that will ensure that the predicted probabilities are tuned to the experts ability to retrieve relevant documents given the judgments that have been made to this point.",
                "Platts SVM calibration method [16] fits a sigmoid function between q∗ ij and the relevance judgments to obtain qij = Cj (q∗ ij) = exp(Aj +Bjq∗ ij ) 1+exp(Aj +Bj q∗ ij ) .",
                "Since q∗ ij is topic-independent, we only need to learn one calibration function for each expert.",
                "Once we have the calibration function, it is applied to adjust the experts predictions to their actual performance.",
                "The calibrated probabilities are plugged into model (2) to find the document probabilities.",
                "Figure 1: Conceptual diagram of our aggregation model.",
                "Experts E1, E2 have ranked documents A, B, C for topic T1 and documents D, E, F for topic T2.",
                "The first step is to obtain q∗ ij.",
                "Next is calibration to true performance to find qij .",
                "Finally we obtain pi = p(Xi = 1|qi1, qi2), · · · . 3.3 Model Summary Our model has three components that differ by the data they take as input and what they produce as output.",
                "A conceptual diagram is shown in Figure 1. 1. ranks → probabilities (per system per topic).",
                "This gives us q∗ ij, expert js self-reported probability of the relevance of document i.",
                "This is unsupervised; it requires no labeled data (though if we have some, we use it to set prior parameters). 2. probabilities → calibrated probabilities (per system).",
                "This gives us qij = Cj (q∗ ij), expert js calibrated probability of the relevance of document i.",
                "This is semisupervised; we have relevance judgments at some ranks which we use to impute the probability of relevance at other ranks. 3. calibrated probabilities → document probabilities.",
                "This gives us pi = p(Xi = 1|qi), the probability of relevance of document i given calibrated expert probabilities qij .",
                "This is supervised; we learn coefficients from a set of judged documents and use those to estimate the relevance of the unjudged documents.",
                "Although the model appears rather complex, it is really just three successive applications of logistic regression.",
                "As such, it can be implemented in a statistical programming language such as R in a few lines of code.",
                "The use of beta (conjugate) priors ensures that no expensive computational methods such as MCMC are necessary [12], so the model is trained and applied fast enough to be used on-line.",
                "Our code is available at http://ciir.cs.umass.edu/~carteret/. 4.",
                "EXPERIMENTS Three hypotheses are under consideration.",
                "The first, and most important, is that using our expert aggregation model to predict relevance produces test collections that are robust enough to be reusable; that is, we can trust the estimates of confidence when we evaluate systems that did not contribute any judgments to the pool.",
                "The other two hypotheses relate to the improvement we see by using better estimates of relevance than we did in our previous work [8].",
                "These are that (a) it takes fewer relevance track no. topics no. runs no. judged no. rel ad-hoc 94 50 40 97,319 9,805 ad-hoc 95 49 33 87,069 6,503 ad-hoc 96 50 61 133,681 5,524 ad-hoc 97 50 74 72,270 4,611 ad-hoc 98 50 103 80,345 4,674 ad-hoc 99 50 129 86,830 4,728 web 04 225 74 88,566 1,763 robust 05 50 74 37,798 6,561 terabyte 05 50 58 45,291 10,407 Table 1: Number of topics, number of runs, number of documents judged, and number found relevant for each of our data sets. judgments to reach 95% confidence and (b) the accuracy of the predictions is higher than if we were to simply assume pi = 0.5 for all unjudged documents. 4.1 Data We obtained full ad-hoc runs submitted to TRECs 3 through 8.",
                "Each run ranks at most 1000 documents for 50 topics (49 topics for TREC 4).",
                "Additionally, we obtained all runs from the Web track of TREC 13, the Robust2 track of TREC 14, and the Terabyte (ad-hoc) track of TREC 14.",
                "These are the tracks that have replaced the ad-hoc track since its end in 1999.",
                "Statistics are shown in Table 1.",
                "We set aside the TREC 4 (ad-hoc 95) set for training, TRECs 3 and 5-8 (ad-hoc 94 and 96-99) for primary testing, and the remaining sets for additional testing.",
                "We use the qrels files assembled by NIST as truth.",
                "The number of relevance judgments made and relevant documents found for each track are listed in Table 1.",
                "For computational reasons, we truncate ranked lists at 100 documents.",
                "There is no reason that we could not go deeper, but calculating variance is O(n3 ) and thus very timeconsuming.",
                "Because of the reciprocal rank nature of AP, we do not lose much information by truncating at rank 100. 4.2 Algorithms We will compare three algorithms for acquiring relevance judgments.",
                "The baseline is a variation of TREC pooling that we will call incremental pooling (IP).",
                "This algorithm takes a number k as input and presents the first k documents in rank order (without regard to topic) to be judged.",
                "It does not estimate the relevance of unjudged documents; it simply assumes any unjudged document is nonrelevant.",
                "The second algorithm is that presented in Carterette et al. [8] (Algorithm 1).",
                "Documents are selected based on how interesting they are in determining whether a difference in mean average precision exists.",
                "For this approach pi = 0.5 for all i; there is no estimation of probabilities.",
                "We will call this <br>mtc</br> for minimal test collection.",
                "The third algorithm augments <br>mtc</br> with updated estimates of probabilities of relevance.",
                "We will call this RTC for robust test collection.",
                "It is identical to Algorithm 1, except that every 10th iteration we estimate pi for all unjudged documents i using the expert aggregation model of Section 3.",
                "RTC has smoothing (prior distribution) parameters that must be set.",
                "We trained using the ad-hoc 95 set.",
                "We limited 2 Robust here means robust retrieval; this is different from our goal of robust evaluation.",
                "Algorithm 1 (<br>mtc</br>) Given two ranked lists and confidence level α, predict the sign of ΔMAP. 1: pi ← 0.5 for all documents i 2: while P(ΔMAP < 0) < α do 3: calculate weight wi for all unjudged documents i (see Carterette et al. [8] for details) 4: j ← argmaxiwi 5: xj ← 1 if document j is relevant, 0 otherwise 6: pj ← xj 7: end while the search to uniform priors with relatively high variance.",
                "For expert aggregation, the prior parameters are α = β = 1. 4.3 Experimental Design First, we want to know whether we can augment a set of relevance judgments with a set of relevance probabilities in order to reuse the judgments to evaluate a new set of systems.",
                "For each experimental trial: 1.",
                "Pick a random subset of k runs. 2.",
                "From those k, pick an initial c < k to evaluate. 3.",
                "Run RTC to 95% confidence on the initial c. 4.",
                "Using the model from Section 3, estimate the probabilities of relevance for all documents retrieved by all k runs. 5.",
                "Calculate EMAP for all k runs, and P(ΔMAP < 0) for all pairs of runs.",
                "We do the same for <br>mtc</br>, but omit step 4.",
                "Note that after evaluating the first c systems, we make no additional relevance judgments.",
                "To put our method to the test, we selected c = 2: we will build a set of judgments from evaluating only two initial systems.",
                "We will then generalize to a set of k = 10 (of which those two are a subset).",
                "As we run more trials, we obtain the data we need to test all three of our hypotheses. 4.4 Experimental Evaluation Recall that a set of judgments is robust if the accuracy of the predictions it makes is at least its estimated confidence.",
                "One way to evaluate robustness is to bin pairs by their confidence, then calculate the accuracy over all the pairs in each bin.",
                "We would like the accuracy to be no less than the lowest confidence score in the bin, but preferably higher.",
                "Since summary statistics are useful, we devised the following metric.",
                "Suppose we are a bookmaker taking bets on whether ΔMAP < 0.",
                "We use RTC or <br>mtc</br> to set the odds O = P (ΔMAP <0) 1−P (ΔMAP <0) .",
                "Suppose a bettor wagers $1 on ΔMAP ≥ 0.",
                "If it turns out that ΔMAP < 0, we win the dollar.",
                "Otherwise, we pay out O.",
                "If our confidence estimates are perfectly accurate, we break even.",
                "If confidence is greater than accuracy, we lose money; we win if accuracy is greater than confidence.",
                "Counterintuitively, the most desirable outcome is breaking even: if we lose money, we cannot trust the confidence estimates, but if we win money, we have either underestimated confidence or judged more documents than necessary.",
                "However, the cost of not being able to trust the confidence estimates is higher than the cost of extra relevance judgments, so we will treat positive outcomes as good.",
                "The amount we win on each pairwise comparison i is: Wi = yi − (1 − yi) Pi 1 − Pi = yi − Pi 1 − Pi yi = 1 if ΔMAP < 0 and 0 otherwise, and Pi = P(ΔMAP < 0).",
                "The summary statistic is W, the mean of Wi.",
                "Note that as Pi increases, we lose more for being wrong.",
                "This is as it should be: the penalty should be great for missing the high probability predictions.",
                "However, since our losses grow without bound as predictions approach certainty, we cap −Wi at 100.",
                "For our hypothesis that RTC requires fewer judgments than <br>mtc</br>, we are interested in the number of judgments needed to reach 95% confidence on the first pair of systems.",
                "The median is more interesting than the mean: most pairs require a few hundred judgments, but a few pairs require several thousand.",
                "The distribution is therefore highly skewed, and the mean strongly affected by those outliers.",
                "Finally, for our hypothesis that RTC is more accurate than <br>mtc</br>, we will look at Kendalls τ correlation between a ranking of k systems by a small set of judgments and the true ranking using the full set of judgments.",
                "Kendalls τ, a nonparametric statistic based on pairwise swaps between two lists, is a standard evaluation for this type of study.",
                "It ranges from −1 (perfectly anti-correlated) to 1 (rankings identical), with 0 meaning that half of the pairs are swapped.",
                "As we touched on in the introduction, though, an accuracy measure like rank correlation is not a good evaluation of reusability.",
                "We include it for completeness. 4.4.1 Hypothesis Testing Running multiple trials allows the use of statistical hypothesis testing to compare algorithms.",
                "Using the same sets of systems allows the use of paired tests.",
                "As we stated above, we are more interested in the median number of judgments than the mean.",
                "A test for difference in median is the Wilcoxon sign rank test.",
                "We can also use a paired t-test to test for a difference in mean.",
                "For rank correlation, we can use a paired t-test to test for a difference in τ. 5.",
                "RESULTS AND ANALYSIS The comparison between <br>mtc</br> and RTC is shown in Table 2.",
                "With <br>mtc</br> and uniform probabilities of relevance, the results are far from robust.",
                "We cannot reuse the relevance judgments with much confidence.",
                "But with RTC, the results are very robust.",
                "There is a slight dip in accuracy when confidence gets above 0.95; nonetheless, the confidence predictions are trustworthy.",
                "Mean Wi shows that RTC is much closer to 0 than <br>mtc</br>.",
                "The distribution of confidence scores shows that at least 80% confidence is achieved more than 35% of the time, indicating that neither algorithm is being too conservative in its confidence estimates.",
                "The confidence estimates are rather low overall; that is because we have built a test collection from only two initial systems.",
                "Recall from Section 1 that we cannot require (or even expect) a minimum level of confidence when we generalize to new systems.",
                "More detailed results for both algorithms are shown in Figure 2.",
                "The solid line is the ideal result that would give W = 0.",
                "RTC is on or above this line at all points until confidence reaches about 0.97.",
                "After that there is a slight dip in accuracy which we discuss below.",
                "Note that both <br>mtc</br> RTC confidence % in bin accuracy % in bin accuracy 0.5 − 0.6 33.7% 61.7% 28.6% 61.9% 0.6 − 0.7 18.1% 73.1% 20.1% 76.3% 0.7 − 0.8 10.4% 70.1% 15.5% 78.0% 0.8 − 0.9 9.4% 69.0% 12.1% 84.9% 0.9 − 0.95 7.3% 78.0% 6.6% 93.1% 0.95 − 0.99 17.9% 70.4% 12.4% 93.4% 1.0 3.3% 68.3% 4.7% 98.9% W −5.34 −0.39 median judged 251 235 mean τ 0.393 0.555 Table 2: Confidence that P(ΔMAP < 0) and accuracy of prediction when generalizing a set of relevance judgments acquired using <br>mtc</br> and RTC.",
                "Each bin contains over 1,000 trials from the adhoc 3, 5-8 sets.",
                "RTC is much more robust than <br>mtc</br>.",
                "W is defined in Section 4.4; closer to 0 is better.",
                "Median judged is the number of judgments to reach 95% confidence on the first two systems.",
                "Mean τ is the average rank correlation for all 10 systems. 0.5 0.6 0.7 0.8 0.9 1 0.5 0.6 0.7 0.8 0.9 1 accuracy confidence breakeven RTC <br>mtc</br> Figure 2: Confidence vs. accuracy of <br>mtc</br> and RTC.",
                "The solid line is the perfect result that would give W = 0; performance should be on or above this line.",
                "Each point represents at least 500 pairwise comparisons. algorithms are well above the line up to around confidence 0.7.",
                "This is because the baseline performance on these data sets is high; it is quite easy to achieve 75% accuracy doing very little work [7].",
                "Number of Judgments: The median number of judgments required by <br>mtc</br> to reach 95% confidence on the first two systems is 251, an average of 5 per topic.",
                "The median required by RTC is 235, about 4.7 per topic.",
                "Although the numbers are close, RTCs median is significantly lower by a paired Wilcoxon test (p < 0.0001).",
                "For comparison, a pool of depth 100 would result in a minimum of 5,000 judgments for each pair.",
                "The difference in means is much greater.",
                "<br>mtc</br> required a mean of 823 judgments, 16 per topic, while RTC required a mean of 502, 10 per topic. (Recall that means are strongly skewed by a few pairs that take thousands of judgments.)",
                "This difference is significant by a paired t-test (p < 0.0001).",
                "Ten percent of the sets resulted in 100 or fewer judgments (less than two per topic).",
                "Performance on these is very high: W = 0.41, and 99.7% accuracy when confidence is at least 0.9.",
                "This shows that even tiny collections can be reusable.",
                "For the 50% of sets with more than 235 judgments, accuracy is 93% when confidence is at least 0.9.",
                "Rank Correlation: <br>mtc</br> and RTC both rank the 10 systems by EMAP (Eq. (1)) calculated using their respective probability estimates.",
                "The mean τ rank correlation between true MAP and EMAP is 0.393 for <br>mtc</br> and 0.555 for RTC.",
                "This difference is significant by a paired t-test (p < 0.0001).",
                "Note that we do not expect the τ correlations to be high, since we are ranking the systems with so few relevance judgments.",
                "It is more important that we estimate confidence in each pairwise comparison correctly.",
                "We ran IP for the same number of judgments that <br>mtc</br> took for each pair, then ranked the systems by MAP using only those judgments (all unjudged documents assumed nonrelevant).",
                "We calculated the τ correlation to the true ranking.",
                "The mean τ correlation is 0.398, which is not significantly different from <br>mtc</br>, but is significantly lower than RTC.",
                "Using uniform estimates of probability is indistinguishable from the baseline, whereas estimating relevance by expert aggregation boosts performance a great deal: nearly 40% over both <br>mtc</br> and IP.",
                "Overfitting: It is possible to overfit: if too many judgments come from the first two systems, the variance in ΔMAP is reduced and the confidence estimates become unreliable.",
                "We saw this in Table 2 and Figure 2 where RTC exhibits a dip in accuracy when confidence is around 97%.",
                "In fact, the number of judgments made prior to a wrong prediction is over 50% greater than the number made prior to a correct prediction.",
                "Overfitting is difficult to quantify exactly, because making more relevance judgments does not always cause it: at higher confidence levels, more relevance judgments are made, and as Table 2 shows, accuracy is greater at those higher confidences.",
                "Obviously having more relevance judgments should increase both confidence and accuracy; the difference seems to be when one system has a great deal more judgments than the other.",
                "Pairwise Comparisons: Our pairwise comparisons fall into one of three groups: 1. the two original runs from which relevance judgments are acquired; 2. one of the original runs vs. one of the new runs; 3. two new runs.",
                "Table 3 shows confidence vs. accuracy results for each of these three groups.",
                "Interestingly, performance is worst when comparing one of the original runs to one of the additional runs.",
                "This is most likely due to a large difference in the number of judgments affecting the variance of ΔMAP.",
                "Nevertheless, performance is quite good on all three subsets.",
                "Worst Case: The case intuitively most likely to produce an error is when the two systems being compared have retrieved very few documents in common.",
                "If we want the judgments to be reusable, we should to be able to generalize even to runs that are very different from the ones used to acquire the relevance judgments.",
                "A simple measure of similarity of two runs is the average percentage of documents they retrieved in common for each topic [2].",
                "We calculated this for all pairs, then looked at performance on pairs with low similarity.",
                "Results are shown in accuracy confidence two original one original no original 0.5 − 0.6 - 48.1% 62.8% 0.6 − 0.7 - 57.1% 79.2% 0.7 − 0.8 - 67.9% 81.7% 0.8 − 0.9 - 82.2% 86.3% 0.9 − 0.95 95.9% 93.7% 92.6% 0.95 − 0.99 96.2% 92.5% 93.1% 1.0 100% 98.0% 99.1% W −1.11 −0.87 −0.27 Table 3: Confidence vs. accuracy of RTC when comparing the two original runs, one original run and one new run, and two new runs.",
                "RTC is robust in all three cases. accuracy when similar confidence 0 − 0.1 0.1 − 0.2 0.2 − 0.3 0.5 − 0.6 68.4% 63.1% 61.4% 0.6 − 0.7 84.2% 78.6% 76.6% 0.7 − 0.8 82.0% 79.8% 78.9% 0.8 − 0.9 93.6% 83.3% 82.1% 0.9 − 0.95 99.3% 92.7% 92.4% 0.95 − 0.99 98.7% 93.4% 93.3% 1.0 99.9% 97.9% 98.1% W 0.44 −0.45 −0.49 Table 4: Confidence vs. accuracy of RTC when a pair of systems retrieved 0-30% documents in common (broken out into 0%-10%, 10%-20%, and 20%30%).",
                "RTC is robust in all three cases.",
                "Table 4.",
                "Performance is in fact very robust even when similarity is low.",
                "When the two runs share very few documents in common, W is actually positive.",
                "<br>mtc</br> and IP both performed quite poorly in these cases.",
                "When the similarity was between 0 and 10%, both <br>mtc</br> and IP correctly predicted ΔMAP only 60% of the time, compared to an 87.6% success rate for RTC.",
                "By Data Set: All the previous results have only been on the ad-hoc collections.",
                "We did the same experiments on our additional data sets, and broke out the results by data set to see how performance varies.",
                "The results in Table 5 show everything about each set, including binned accuracy, W, mean τ, and median number of judgments to reach 95% confidence on the first two systems.",
                "The results are highly consistent from collection to collection, suggesting that our method is not overfitting to any particular data set. 6.",
                "CONCLUSIONS AND FUTURE WORK In this work we have offered the first formal definition of the common idea of reusability of a test collection and presented a model that is able to achieve reusability with very small sets of relevance judgments.",
                "Table 2 and Figure 2 together show how biased a small set of judgments can be: <br>mtc</br> is dramatically overestimating confidence and is much less accurate than RTC, which is able to remove the bias to give a robust evaluation.",
                "The confidence estimates of RTC, in addition to being accurate, provide a guide for obtaining additional judgments: focus on judging documents from the lowest-confidence comparisons.",
                "In the long run, we see small sets of relevance judgaccuracy confidence ad-hoc 94 ad-hoc 96 ad-hoc 97 ad-hoc 98 ad-hoc 99 web 04 robust 05 terabyte 05 0.5 − 0.6 64.1% 61.8% 62.2% 62.0% 59.4% 64.3% 61.5% 61.6% 0.6 − 0.7 76.1% 77.8% 74.5% 78.2% 74.3% 78.1% 75.9% 75.9% 0.7 − 0.8 75.2% 78.9% 77.6% 80.0% 78.6% 82.6% 77.5% 80.4% 0.8 − 0.9 83.2% 85.5% 84.6% 84.9% 86.8% 84.5% 86.7% 87.7% 0.9 − 0.95 93.0% 93.6% 92.8% 93.7% 92.6% 94.2% 93.9% 94.2% 0.95 − 0.99 93.1% 94.3% 93.1% 93.7% 92.8% 95.0% 93.9% 91.6% 1.0 99.2% 96.8% 98.7% 99.5% 99.6% 100% 99.2% 98.3% W -0.34 -0.34 -0.48 -0.35 -0.44 -0.07 -0.41 -0.67 median judged 235 276 243 213 179 448 310 320 mean τ 0.538 0.573 0.556 0.579 0.532 0.596 0.565 0.574 Table 5: Accuracy, W, mean τ, and median number of judgments for all 8 testing sets.",
                "The results are highly consistent across data sets. ments being shared by researchers, each group contributing a few more judgments to gain more confidence about their particular systems.",
                "As time goes on, the number of judgments grows until there is 100% confidence in every evaluation-and there is a full test collection for the task.",
                "We see further use for this method in scenarios such as web retrieval in which the corpus is frequently changing.",
                "It could be applied to evaluation on a dynamic test collection as defined by Soboroff [18].",
                "The model we presented in Section 3 is by no means the only possibility for creating a robust test collection.",
                "A simpler expert aggregation model might perform as well or better (though all our efforts to simplify failed).",
                "In addition to expert aggregation, we could estimate probabilities by looking at similarities between documents.",
                "This is an obvious area for future exploration.",
                "Additionally, it will be worthwhile to investigate the issue of overfitting: the circumstances it occurs under and what can be done to prevent it.",
                "In the meantime, capping confidence estimates at 95% is a hack that solves the problem.",
                "We have many more experimental results that we unfortunately did not have space for but that reinforce the notion that RTC is highly robust: with just a few judgments per topic, we can accurately assess the confidence in any pairwise comparison of systems.",
                "Acknowledgments This work was supported in part by the Center for Intelligent Information Retrieval and in part by the Defense Advanced Research Projects Agency (DARPA) under contract number HR0011-06-C-0023.",
                "Any opinions, findings, and conclusions or recommendations expressed in this material are those of the author and do not necessarily reflect those of the sponsor. 7.",
                "REFERENCES [1] J. Aslam and M. Montague.",
                "Models for Metasearch.",
                "In Proceedings of SIGIR, pages 275-285, 2001. [2] J. Aslam and R. Savell.",
                "On the effectiveness of evaluating retrieval systems in the absence of relevance judgments.",
                "In Proceedings of SIGIR, pages 361-362, 2003. [3] J.",
                "A. Aslam, V. Pavlu, and R. Savell.",
                "A unified model for metasearch, pooling, and system evaluation.",
                "In Proceedings of CIKM, pages 484-491, 2003. [4] J.",
                "A. Aslam, V. Pavlu, and E. Yilmaz.",
                "A statistical method for system evaluation using incomplete judgments.",
                "In Proceedings of SIGIR, pages 541-548, 2006. [5] A. L. Berger, S. D. Pietra, and V. J. D. Pietra.",
                "A maximum entropy approach to natural language processing.",
                "Computational Linguistics, 22(1):39-71, 1996. [6] D. J. Blower.",
                "An easy derivation of logistic regression from the bayesian and maximum entropy perspective.",
                "In Proceedings of the 23rd International Workship on Bayesian Inference and Maximum Entropy Methods in Science and Engineering, pages 30-43, 2004. [7] B. Carterette and J. Allan.",
                "Research methodology in studies of assessor effort for retrieval evaluation.",
                "In Proceedings of RIAO, 2007. [8] B. Carterette, J. Allan, and R. K. Sitaraman.",
                "Minimal test collections for retrieval evaluation.",
                "In Proceedings of SIGIR, pages 268-275, 2006. [9] B. Carterette and D. I. Petkova.",
                "Learning a ranking from pairwise preferences.",
                "In Proceedings of SIGIR, 2006. [10] R. T. Clemen and R. L. Winkler.",
                "Unanimity and compromise among probability forecasters.",
                "Management Science, 36(7):767-779, July 1990. [11] G. V. Cormack, C. R. Palmer, and C. L. Clarke.",
                "Efficient Construction of Large Test Collections.",
                "In Proceedings of SIGIR, pages 282-289, 1998. [12] A. Gelman, J.",
                "B. Carlin, H. S. Stern, and D. B. Rubin.",
                "Bayesian Data Analysis.",
                "Chapman & Hall/CRC, 2004. [13] E. T. Jaynes.",
                "Probability Theory: The Logic of Science.",
                "Cambridge University Press, 2003. [14] R. Manmatha and H. Sever.",
                "A Formal Approach to Score Normalization for Metasearch.",
                "In Proceedings of HLT, pages 88-93, 2002. [15] I. J. Myung, S. Ramamoorti, and J. Andrew D. Baily.",
                "Maximum entropy aggregation of expert predictions.",
                "Management Science, 42(10):1420-1436, October 1996. [16] J. Platt.",
                "Probabilistic outputs for support vector machines and comparison to regularized likelihood methods. pages 61-74, 2000. [17] M. Sanderson and H. Joho.",
                "Forming test collections with no system pooling.",
                "In Proceedings of the 27th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 33-40, 2004. [18] I. Soboroff.",
                "Dynamic test collections: measuring search effectiveness on the live web.",
                "In Proceedings of SIGIR, pages 276-283, 2006. [19] K. Sparck Jones and C. J. van Rijsbergen.",
                "Information Retrieval Test Collections.",
                "Journal of Documentation, 32(1):59-75, 1976. [20] E. M. Voorhees and D. K. Harman, editors.",
                "TREC: Experiment and Evaluation in Information Retrieval.",
                "MIT Press, 2005. [21] J. Zobel.",
                "How Reliable are the Results of Large-Scale Information Retrieval Experiments?",
                "In Proceedings of SIGIR, pages 307-314, 1998."
            ],
            "original_annotated_samples": [
                "We will call this <br>mtc</br> for minimal test collection.",
                "The third algorithm augments <br>mtc</br> with updated estimates of probabilities of relevance.",
                "Algorithm 1 (<br>mtc</br>) Given two ranked lists and confidence level α, predict the sign of ΔMAP. 1: pi ← 0.5 for all documents i 2: while P(ΔMAP < 0) < α do 3: calculate weight wi for all unjudged documents i (see Carterette et al. [8] for details) 4: j ← argmaxiwi 5: xj ← 1 if document j is relevant, 0 otherwise 6: pj ← xj 7: end while the search to uniform priors with relatively high variance.",
                "We do the same for <br>mtc</br>, but omit step 4.",
                "We use RTC or <br>mtc</br> to set the odds O = P (ΔMAP <0) 1−P (ΔMAP <0) ."
            ],
            "translated_annotated_samples": [
                "Llamaremos a esto MTC por <br>colección de pruebas mínimas</br>.",
                "El tercer algoritmo aumenta MTC con estimaciones actualizadas de las probabilidades de relevancia.",
                "Algoritmo 1 (MTC) Dadas dos listas clasificadas y un nivel de confianza α, predecir el signo de ΔMAP. 1: pi ← 0.5 para todos los documentos i 2: mientras P(ΔMAP < 0) < α hacer 3: calcular el peso wi para todos los documentos i no clasificados (ver Carterette et al. [8] para más detalles) 4: j ← argmaxiwi 5: xj ← 1 si el documento j es relevante, 0 en caso contrario 6: pj ← xj 7: fin mientras la búsqueda de priors uniformes con una varianza relativamente alta.",
                "Hacemos lo mismo para <br>MTC</br>, pero omitimos el paso 4.",
                "Utilizamos RTC o MTC para establecer las probabilidades O = P (ΔMAP <0) 1−P (ΔMAP <0) ."
            ],
            "translated_text": "Colecciones de pruebas robustas para la evaluación de recuperación Ben Carterette Centro de Recuperación de Información Inteligente Departamento de Ciencias de la Computación Universidad de Massachusetts Amherst Amherst, MA 01003 carteret@cs.umass.edu RESUMEN Los métodos de bajo costo para adquirir juicios de relevancia pueden ser una bendición para los investigadores que necesitan evaluar nuevas tareas o temas de recuperación pero no tienen los recursos para hacer miles de juicios. Si bien estos juicios son muy útiles para una evaluación única, no está claro que se puedan confiar al ser reutilizados para evaluar nuevos sistemas. En este trabajo, definimos formalmente lo que significa que las evaluaciones sean reutilizables: la confianza en una evaluación de nuevos sistemas puede ser evaluada con precisión a partir de un conjunto existente de juicios de relevancia. Luego presentamos un método para aumentar un conjunto de juicios de relevancia con estimaciones de relevancia que no requieren esfuerzo adicional del evaluador. Usar este método garantiza prácticamente la reutilización: con tan solo cinco juicios por tema tomados de solo dos sistemas, podemos evaluar de manera confiable un conjunto más grande de diez sistemas. Incluso los conjuntos más pequeños de juicios pueden ser útiles para la evaluación de nuevos sistemas. Categorías y Descriptores de Asignaturas: H.3 Almacenamiento y Recuperación de Información; H.3.4 Sistemas y Software: Evaluación del Rendimiento Términos Generales: Experimentación, Medición, Confiabilidad 1. INTRODUCCIÓN Imagina a un investigador de recuperación de información que ha inventado una nueva tarea de recuperación. Ella ha construido un sistema para realizar la tarea y quiere evaluarlo. Dado que la tarea es nueva, es poco probable que existan juicios de relevancia vigentes. Ella no tiene el tiempo ni los recursos para juzgar cada documento, ni siquiera cada documento recuperado. Ella solo puede juzgar los documentos que parecen ser los más informativos y detenerse cuando tenga un grado razonable de confianza en sus conclusiones. Pero ¿qué sucede cuando ella desarrolla un nuevo sistema y necesita evaluarlo? ¿Otro grupo de investigación decide implementar un sistema para realizar la tarea? ¿Pueden reutilizar de manera confiable los juicios originales? ¿Pueden evaluar sin más juicios de relevancia? La evaluación es un aspecto importante de la investigación en recuperación de información, pero es solo un problema parcialmente resuelto: para la mayoría de las tareas de recuperación, es imposible juzgar la relevancia de cada documento; simplemente hay demasiados. La solución utilizada por NIST en TREC (Text REtrieval Conference) es el método de agrupación [19, 20]: todos los sistemas competidores contribuyen con N documentos a un grupo, y cada documento en ese grupo es evaluado. Este método crea grandes conjuntos de juicios que son reutilizables para entrenar o evaluar nuevos sistemas que no contribuyeron al conjunto [21]. Esta solución no es adecuada para nuestro investigador hipotético. El método de agrupación proporciona miles de juicios de relevancia, pero requiere muchas horas de tiempo de anotadores (remunerados). Como resultado, ha habido una serie de artículos recientes sobre la reducción del esfuerzo del anotador en la producción de colecciones de pruebas: Cormack et al. [11], Zobel [21], Sanderson y Joho [17], Carterette et al. [8], y Aslam et al. [4], entre otros. Como veremos, los juicios que estos métodos producen pueden sesgar significativamente la evaluación de un nuevo conjunto de sistemas. ¿Volviendo a nuestra investigadora hipotética, puede reutilizar sus juicios de relevancia? Primero debemos definir formalmente lo que significa ser reutilizable. En trabajos anteriores, la reutilización se ha probado simplemente evaluando la precisión de un conjunto de juicios de relevancia al evaluar sistemas no vistos. Si bien podemos decir que tuvo razón el 75% del tiempo, o que tuvo una correlación de rango de 0.8, estos números no tienen ningún poder predictivo: no nos dicen qué sistemas es probable que estén equivocados ni cuán seguros deberíamos estar en uno en particular. Necesitamos una definición más cuidadosa de reutilización. Específicamente, la cuestión de la reutilización no es qué tan precisamente podemos evaluar los nuevos sistemas. Un adversario malicioso siempre puede producir una nueva lista clasificada que no haya recuperado ninguno de los documentos evaluados. La verdadera pregunta es cuánta confianza tenemos en nuestras evaluaciones, y, más importante aún, si podemos confiar en nuestras estimaciones de confianza. Aunque la confianza no sea alta, siempre y cuando podamos confiar en ello, podemos identificar qué sistemas necesitan más juicios para aumentar la confianza. Cualquier conjunto de juicios, por pequeño que sea, se vuelve reutilizable en cierta medida. Pequeñas colecciones de pruebas reutilizables podrían tener un gran impacto en la investigación de recuperación de información. Los grupos de investigación podrían compartir las evaluaciones de relevancia que han realizado internamente para estudios piloto, nuevas tareas o nuevos temas. La cantidad de datos disponible para los investigadores crecería de forma exponencial con el tiempo. EVALUACIÓN ROBUSTA Anteriormente dimos una definición intuitiva de reutilización: una colección es reutilizable si podemos confiar en nuestras estimaciones de confianza en una evaluación. Con eso queremos decir que si hemos realizado algunos juicios de relevancia y tenemos, por ejemplo, un 75% de confianza en que el sistema A es mejor que el sistema B, nos gustaría que no haya más del 25% de posibilidad de que nuestra evaluación de la calidad relativa de los sistemas cambie a medida que continuamos evaluando documentos. Nuestra evaluación debe ser robusta ante juicios faltantes. En nuestro trabajo anterior, definimos la confianza como la probabilidad de que la diferencia en una medida de evaluación calculada para dos sistemas sea menor que cero [8]. Esta noción de confianza se define en el contexto de una tarea de evaluación particular que llamamos evaluación comparativa: determinar el signo de la diferencia en una medida de evaluación. Otras tareas de evaluación podrían ser definidas; estimar la magnitud de la diferencia o los valores de las medidas mismas son ejemplos que implican diferentes nociones de confianza. Por lo tanto, consideramos la confianza como una estimación de probabilidad. Una de las preguntas que debemos hacer acerca de una estimación de probabilidad es qué significa. ¿Qué significa tener un 75% de confianza de que el sistema A es mejor que el sistema B? Como se describe arriba, queremos que signifique que si seguimos evaluando documentos, solo habrá un 25% de probabilidad de que nuestra evaluación cambie. Si esto es lo que significa, podemos confiar en las estimaciones de confianza. ¿Pero sabemos que tiene ese significado? Nuestro cálculo de confianza se basó en una suposición sobre la probabilidad de relevancia de los documentos no evaluados, específicamente que cada documento no evaluado tenía la misma probabilidad de ser relevante o no relevante. Esta suposición casi con seguridad no es realista en la mayoría de las aplicaciones de IR. Resulta que es esta suposición la que determina si se pueden confiar en las estimaciones de confianza. Antes de profundizar en esto, definimos formalmente la confianza. 2.1 Estimación de la confianza La precisión promedio (AP) es una métrica de evaluación estándar que captura tanto la capacidad de un sistema para clasificar documentos relevantes de manera alta (precisión) como su capacidad para recuperar documentos relevantes (recuperación). Normalmente se escribe como la precisión media en los rangos de documentos relevantes: AP = 1 |R| i∈R prec@r(i) donde R es el conjunto de documentos relevantes y r(i) es el rango del documento i. Sea Xi una variable aleatoria que indica la relevancia del documento i. Si los documentos están ordenados por rango, podemos expresar la precisión como prec@i = 1/i i j=1 Xj. La precisión promedio se convierte entonces en la ecuación cuadrática AP = 1 Xi n i=1 Xi/i i j=1 Xj = 1 Xi n i=1 j≥i aijXiXj donde aij = 1/ max{r(i), r(j)}. Usar aij en lugar de 1/i nos permite numerar los documentos de forma arbitraria. Para ver por qué esto es cierto, considera un ejemplo simple: una lista de 3 documentos con los documentos relevantes B, C en los rangos 1 y 3 y el documento no relevante A en el rango 2. La precisión promedio será 1 2 (1 1 x2 B+ 1 2 xBxA+ 1 3 xBxC + 1 2 x2 A+ 1 3 xAxC + 1 3 x2 C) = 1 2 1 + 2 3 porque xA = 0, xB = 1, xC = 1. Aunque el orden B, A, C es diferente de la etiqueta A, B, C, no afecta al cálculo. Ahora podemos ver que la precisión promedio en sí misma es una variable aleatoria con una distribución sobre todas las posibles asignaciones de relevancia a todos los documentos. Esta variable aleatoria tiene una esperanza, una varianza, intervalos de confianza y una cierta probabilidad de ser menor o igual a un valor dado. Todos estos dependen de la probabilidad de que el documento i sea relevante: pi = p(Xi = 1). Supongamos que en nuestro ejemplo anterior no conocemos las valoraciones de relevancia, pero creemos que pA = 0.4, pB = 0.8, pC = 0.7. Podemos entonces calcular, por ejemplo. P(AP = 0) = 0.2 · 0.6 · 0.3 = 0.036, o P(AP = 1 2 ) = 0.2 · 0.4 · 0.7 = 0.056. \n\nLa traducción al español sería: P(AP = 0) = 0.2 · 0.6 · 0.3 = 0.036, o P(AP = 1 2 ) = 0.2 · 0.4 · 0.7 = 0.056. Sumando sobre todas las posibilidades, podemos calcular la esperanza y la varianza: E[AP] ≈ 1 pi aiipi + j>i aij pipj V ar[AP] ≈ 1 ( pi)2 n i a2 iipiqi + j>i a2 ijpipj(1 − pipj) + i=j 2aiiaijpipj(1 − pi) + k>j=i 2aijaikpipjpk(1 − pi) AP converge asintóticamente a una distribución normal con la esperanza y la varianza definidas anteriormente. Para nuestra tarea de evaluación comparativa, estamos interesados en el signo de la diferencia en dos precisiones promedio: ΔAP = AP1 − AP2. Como mostramos en nuestro trabajo anterior, ΔAP tiene una forma cerrada cuando los documentos están ordenados arbitrariamente: ΔAP = 1 Xi n i=1 j≥i cij XiXj cij = aij − bij donde bij está definido de manera análoga a aij para la segunda clasificación. Dado que AP es normal, ΔAP también es normal, lo que significa que podemos usar la función de densidad acumulativa normal para determinar la confianza de que una diferencia en AP sea menor que cero. Dado que los temas son independientes, podemos extender fácilmente esto para referirnos a la precisión promedio (MAP). MAP también se distribuye normalmente; su esperanza y varianza son: EMAP = 1 T t∈T E[APt] (1) VMAP = 1 T2 t∈T V ar[APt] ΔMAP = MAP1 − MAP2 La confianza puede entonces estimarse calculando la esperanza y varianza y utilizando la función de densidad normal para encontrar P(ΔMAP < 0). 2.2 Confianza y Robustez Habiendo definido la confianza, volvemos al tema de la confianza en las estimaciones de confianza, y mostramos cómo se relaciona con la robustez de la colección ante juicios faltantes. 1 Estas son en realidad aproximaciones a la verdadera esperanza y varianza, pero el error es despreciable O(n2−n ). Sea Z el conjunto de todos los pares de resultados clasificados para un conjunto común de temas. Supongamos que tenemos un conjunto de m juicios de relevancia xm = {x1, x2, ..., xm} (usando x minúscula en lugar de X mayúscula para distinguir entre documentos juzgados y no juzgados); estos son los juicios contra los cuales calculamos la confianza. Sea Zα el subconjunto de pares en Z para los cuales predecimos que ΔMAP = −1 con confianza α dadas las evaluaciones xm. Para que las estimaciones de confianza sean precisas, necesitamos que al menos α · |Zα| de estos pares realmente tengan ΔMAP = −1 después de haber evaluado cada documento. Si lo hacen, podemos confiar en las estimaciones de confianza; nuestra evaluación será robusta ante juicios faltantes. Si nuestras estimaciones de confianza se basan en suposiciones poco realistas, no podemos esperar que sean precisas. Las suposiciones en las que se basan son las probabilidades de relevancia pi. Necesitamos que esto sea realista. Sostenemos que la mejor distribución posible de relevancia p(Xi) es aquella que explica todos los datos (todas las observaciones realizadas sobre los sistemas de recuperación) al mismo tiempo que no hace suposiciones infundadas. Esto se conoce como el principio de máxima entropía [13]. La entropía de una variable aleatoria X con distribución p(X) se define como H(p) = − i p(X = i) log p(X = i). Esto ha encontrado una amplia variedad de usos en la informática y la recuperación de información. La distribución de entropía máxima es aquella que maximiza H. Esta distribución es única y tiene una forma exponencial. El siguiente teorema muestra la utilidad de una distribución de entropía máxima para la relevancia al estimar la confianza. Teorema 1. Si p(Xn |I, xm ) = argmaxpH(p), las estimaciones de confianza serán precisas, donde xm es el conjunto de juicios de relevancia definidos anteriormente, Xn es el conjunto completo de documentos cuya relevancia deseamos estimar, e I es alguna información sobre los documentos (no especificada hasta ahora). Por el momento prescindimos de la prueba, pero es bastante simple. Esto indica que cuanto mejores sean las estimaciones de relevancia, más precisa será la evaluación. La tarea de crear una colección de pruebas reutilizable se convierte entonces en la tarea de estimar la relevancia de los documentos no evaluados. El teorema y su demostración no dicen absolutamente nada sobre la métrica de evaluación. Las estimaciones de probabilidad son completamente independientes de la medida en la que estamos interesados. Esto significa que los mismos cálculos de probabilidad pueden informarnos sobre la precisión promedio, así como sobre la precisión, la recuperación, el bpref, etc. Además, podríamos asumir que la relevancia de los documentos i y j es independiente y lograr el mismo resultado, lo cual afirmamos como un corolario: Corolario 1. Si p(Xi|I, xm) = argmaxpH(p), las estimaciones de confianza serán precisas. La tarea, por lo tanto, consiste en la imputación de los valores faltantes de relevancia. El teorema implica que cuanto más nos acerquemos a la distribución de entropía máxima relevante, más nos acercaremos a la robustez. 3. PREDICIENDO LA RELEVANCIA En nuestra declaración del Teorema 1, dejamos sin especificar la naturaleza de la información I. Una de las ventajas de nuestras estimaciones de confianza es que admiten información de una amplia variedad de fuentes; básicamente cualquier cosa que pueda ser modelada puede ser utilizada como información para predecir la relevancia. Una fuente natural de información son los propios sistemas de recuperación: cómo clasificaron los documentos evaluados, con qué frecuencia no lograron clasificar documentos relevantes, cómo se desempeñan en diferentes temas, y así sucesivamente. Si tratamos cada sistema como un experto en recuperación de información que proporciona una opinión sobre la relevancia de cada documento, el problema se convierte en uno de agregación de opiniones de expertos. Esto es similar al problema de metabúsqueda o fusión de datos en el que la tarea consiste en tomar k sistemas de entrada y fusionarlos en un solo ranking. Aslam et al. [3] identificaron previamente una conexión entre la evaluación y la metabúsqueda. Nuestro problema tiene dos diferencias clave: 1. Necesitamos explícitamente probabilidades de relevancia que podamos insertar en la Ecuación 1; los algoritmos de metabusqueda no tienen tal requisito. Estamos acumulando juicios de relevancia a medida que avanzamos con la evaluación y somos capaces de reestimar la relevancia dada cada nuevo juicio. A la luz de lo anterior, introducimos un modelo probabilístico para la combinación de expertos. 3.1 Un Modelo para la Agregación de Opiniones de Expertos Supongamos que cada experto j proporciona una probabilidad de relevancia qij = pj(Xi = 1). La información sobre la relevancia del documento i será entonces el conjunto de k opiniones de expertos I = qi = (qi1, qi2, · · · , qik). La distribución de probabilidad que deseamos encontrar es aquella que maximiza la entropía de pi = p(Xi = 1|qi). Resulta que encontrar el modelo de entropía máxima es equivalente a encontrar los parámetros que maximizan la verosimilitud [5]. Blower [6] muestra explícitamente que encontrar el modelo de entropía máxima para una variable binaria es equivalente a resolver una regresión logística. Entonces pi = p(Xi = 1|qi) = exp k j=1 λjqij 1 + exp k j=1 λj qij (2) donde λ1, · · · , λk son los parámetros de regresión. Incluimos una distribución beta a priori para p(λj) con parámetros α, β. Esto se puede ver como un tipo de suavizado para tener en cuenta el hecho de que los datos de entrenamiento están altamente sesgados. Este modelo tiene la ventaja de incluir la dependencia estadística entre los expertos. Un modelo de la misma forma fue demostrado por Clemen & Winkler como el mejor para agregar probabilidades de expertos [10]. Un enfoque similar motivado por la entropía máxima se ha utilizado para la agregación de expertos [15]. Aslam & Montague [1] utilizaron un modelo similar para la metabúsqueda, pero asumieron independencia entre los expertos. ¿De dónde vienen los qij? Utilizar puntuaciones sin procesar y no calibradas como predictores no funcionará porque las distribuciones de puntuaciones varían demasiado entre los temas. Un clasificador de modelado de lenguaje, por ejemplo, suele otorgar una puntuación mucho más alta al documento recuperado en la parte superior para una consulta corta que al documento recuperado en la parte superior para una consulta larga. Podríamos entrenar un modelo predictivo separado para cada tema, pero eso no aprovecha toda la información que tenemos: es posible que solo tengamos un puñado de juicios para un tema, no suficientes para entrenar un modelo con confianza. Además, parece razonable asumir que si un experto hace buenas predicciones para un tema, también hará buenas predicciones para otros temas. Podríamos usar un modelo jerárquico [12], pero eso no se generalizará a temas no vistos. En cambio, calibraremos las puntuaciones de cada experto de forma individual para que las puntuaciones puedan ser comparadas tanto dentro de un tema como entre temas. Por lo tanto, nuestro modelo tiene en cuenta no solo la dependencia entre expertos, sino también la dependencia entre el rendimiento de los expertos en diferentes tareas (temas). 3.2 Calibración de Expertos Cada experto nos proporciona una puntuación y un rango para cada documento. Necesitamos convertir estos en probabilidades. Un método como el utilizado por Manmatha et al. [14] podría ser utilizado para convertir puntuaciones en probabilidades de relevancia. El método de preferencia por pares de Carterette & Petkova [9] también podría ser utilizado, interpretando la clasificación de un documento sobre otro como una expresión de preferencia. Que q∗ ij sea la probabilidad autoinformada por el experto j de que el documento i sea relevante. De manera intuitiva, parece claro que q∗ ij debería disminuir con el rango, y debería ser cero si el documento i no está clasificado (el experto no consideró que fuera relevante). El modelo de preferencia por pares puede manejar fácilmente estos dos requisitos, por lo que lo utilizaremos. Que θrj (i) sea el coeficiente de relevancia del documento en la posición rj(i). Queremos encontrar los θs que maximizan la función de verosimilitud: Ljt(Θ) = rj (i)<rj (k) exp(θrj (i) − θrj (k)) 1 + exp(θrj (i) − θrj (k)) Incluimos nuevamente una distribución beta previa en p(θrj(i)) con parámetros |Rt| + 1 y |Nt| + 1, el tamaño de los conjuntos de documentos relevantes y no relevantes juzgados respectivamente. El uso de estos parámetros previos garantiza que las probabilidades resultantes se concentren alrededor de la proporción de documentos relevantes que se han descubierto para el tema t. Esto significa que las estimaciones de probabilidad disminuyen por rango y son más altas para los temas que tienen más documentos relevantes. Después de encontrar el Θ que maximiza la verosimilitud, tenemos q∗ ij = exp(θrj (i)) 1+exp(θrj (i)) . Definimos θ∞ = −∞, de modo que la probabilidad de que un documento no clasificado sea relevante es 0. Dado que q∗ ij se basa en la posición en la que se recupera un documento en lugar de en la identidad del documento en sí, las probabilidades son idénticas de un experto a otro, por ejemplo, si el experto E colocó el documento A en la posición 1, y el experto D colocó el documento B en la posición 1, tendremos que q∗ AE = q∗ BD. Por lo tanto, solo tenemos que resolver esto una vez para cada tema. El modelo anterior proporciona probabilidades independientes del tema para cada documento. Pero supongamos que un experto que informa una probabilidad del 90% solo tiene razón el 50% del tiempo. Su opinión debería ser descartada basándose en su desempeño observado. Específicamente, queremos aprender una función de calibración qij = Cj(q∗ ij) que garantice que las probabilidades predichas estén ajustadas a la capacidad de los expertos para recuperar documentos relevantes dados los juicios que se han realizado hasta este momento. El método de calibración SVM de Platts [16] ajusta una función sigmoide entre q∗ ij y las evaluaciones de relevancia para obtener qij = Cj (q∗ ij) = exp(Aj +Bjq∗ ij ) 1+exp(Aj +Bj q∗ ij ) . Dado que q∗ ij es independiente del tema, solo necesitamos aprender una función de calibración para cada experto. Una vez que tenemos la función de calibración, se aplica para ajustar las predicciones de los expertos a su rendimiento real. Las probabilidades calibradas se introducen en el modelo (2) para encontrar las probabilidades de los documentos. Figura 1: Diagrama conceptual de nuestro modelo de agregación. Los expertos E1 y E2 han clasificado los documentos A, B, C para el tema T1 y los documentos D, E, F para el tema T2. El primer paso es obtener q∗ ij. A continuación se realiza la calibración al rendimiento real para encontrar qij. Finalmente obtenemos pi = p(Xi = 1|qi1, qi2), · · · . 3.3 Resumen del modelo Nuestro modelo tiene tres componentes que difieren en los datos que toman como entrada y en lo que producen como salida. Se muestra un diagrama conceptual en la Figura 1. 1. rangos → probabilidades (por sistema por tema). Esto nos da q∗ ij, la probabilidad autoinformada por el experto j de la relevancia del documento i. Esto es no supervisado; no requiere datos etiquetados (aunque si los tenemos, los usamos para establecer parámetros previos). 2. probabilidades → probabilidades calibradas (por sistema). Esto nos da qij = Cj (q∗ ij), la probabilidad calibrada por el experto j de la relevancia del documento i. Esto es semisupervisado; tenemos juicios de relevancia en algunos rangos que usamos para imputar la probabilidad de relevancia en otros rangos. 3. probabilidades calibradas → probabilidades de documentos. Esto nos da pi = p(Xi = 1|qi), la probabilidad de relevancia del documento i dada las probabilidades de experto calibradas qij. Esto es supervisado; aprendemos coeficientes de un conjunto de documentos juzgados y los utilizamos para estimar la relevancia de los documentos no juzgados. Aunque el modelo parece bastante complejo, en realidad son solo tres aplicaciones sucesivas de regresión logística. Por lo tanto, se puede implementar en un lenguaje de programación estadística como R en unas pocas líneas de código. El uso de priors beta (conjugados) garantiza que no sean necesarios métodos computacionales costosos como MCMC [12], por lo que el modelo se entrena y aplica lo suficientemente rápido como para ser utilizado en línea. Nuestro código está disponible en http://ciir.cs.umass.edu/~carteret/. 4. EXPERIMENTOS Se están considerando tres hipótesis. El primero, y más importante, es que el uso de nuestro modelo de agregación de expertos para predecir la relevancia produce colecciones de pruebas lo suficientemente robustas como para ser reutilizables; es decir, podemos confiar en las estimaciones de confianza cuando evaluamos sistemas que no contribuyeron con ningún juicio al conjunto. Las otras dos hipótesis se relacionan con la mejora que observamos al utilizar estimaciones de relevancia mejores que las que utilizamos en nuestro trabajo anterior [8]. Estos son (a) que se necesitan menos temas de relevancia, número de ejecuciones, número de documentos evaluados y número de documentos relevantes para cada uno de nuestros conjuntos de datos y (b) la precisión de las predicciones es mayor que si simplemente asumiéramos que pi = 0.5 para todos los documentos no evaluados. 4.1 Datos Obtuvimos ejecuciones completas de búsqueda ad-hoc presentadas en las TRECs 3 a 8. Cada ejecución clasifica como máximo 1000 documentos para 50 temas (49 temas para TREC 4). Además, obtuvimos todas las ejecuciones de la pista Web de TREC 13, la pista Robust2 de TREC 14 y la pista Terabyte (ad-hoc) de TREC 14. Estas son las pistas que han reemplazado la pista ad-hoc desde su final en 1999. Las estadísticas se muestran en la Tabla 1. Reservamos el conjunto TREC 4 (ad-hoc 95) para entrenamiento, los TRECs 3 y 5-8 (ad-hoc 94 y 96-99) para pruebas principales, y los conjuntos restantes para pruebas adicionales. Utilizamos los archivos qrels recopilados por NIST como verdad. El número de juicios de relevancia realizados y documentos relevantes encontrados para cada pista se enumeran en la Tabla 1. Por razones computacionales, truncamos las listas clasificadas en 100 documentos. No hay razón por la que no pudiéramos profundizar más, pero el cálculo de la varianza es O(n3) y, por lo tanto, muy consumidor de tiempo. Debido a la naturaleza del rango recíproco de AP, no perdemos mucha información al truncar en el rango 100. 4.2 Algoritmos Compararemos tres algoritmos para adquirir juicios de relevancia. La línea base es una variación del agrupamiento de TREC que llamaremos agrupamiento incremental (IP). Este algoritmo toma un número k como entrada y presenta los primeros k documentos en orden de rango (sin tener en cuenta el tema) para ser evaluados. No estima la relevancia de los documentos no evaluados; simplemente asume que cualquier documento no evaluado no es relevante. El segundo algoritmo es el presentado en Carterette et al. [8] (Algoritmo 1). Los documentos se seleccionan en función de lo interesantes que sean para determinar si existe una diferencia en la precisión media promedio. Para este enfoque pi = 0.5 para todos los i; no hay estimación de probabilidades. Llamaremos a esto MTC por <br>colección de pruebas mínimas</br>. El tercer algoritmo aumenta MTC con estimaciones actualizadas de las probabilidades de relevancia. Llamaremos a esto RTC por colección de pruebas robusta. Es idéntico al Algoritmo 1, excepto que cada décima iteración estimamos pi para todos los documentos no evaluados i utilizando el modelo de agregación de expertos de la Sección 3. RTC tiene parámetros de suavizado (distribución previa) que deben ser configurados. Entrenamos utilizando el conjunto ad-hoc 95. Limitamos 2 robustos aquí significa recuperación robusta; esto es diferente de nuestro objetivo de evaluación robusta. Algoritmo 1 (MTC) Dadas dos listas clasificadas y un nivel de confianza α, predecir el signo de ΔMAP. 1: pi ← 0.5 para todos los documentos i 2: mientras P(ΔMAP < 0) < α hacer 3: calcular el peso wi para todos los documentos i no clasificados (ver Carterette et al. [8] para más detalles) 4: j ← argmaxiwi 5: xj ← 1 si el documento j es relevante, 0 en caso contrario 6: pj ← xj 7: fin mientras la búsqueda de priors uniformes con una varianza relativamente alta. Para la agregación de expertos, los parámetros previos son α = β = 1. 4.3 Diseño Experimental Primero, queremos saber si podemos aumentar un conjunto de juicios de relevancia con un conjunto de probabilidades de relevancia para reutilizar los juicios y evaluar un nuevo conjunto de sistemas. Para cada prueba experimental: 1. Selecciona un subconjunto aleatorio de k ejecuciones. De esos k, elige un c inicial < k para evaluar. 3. Ejecutar RTC al 95% de confianza en el c inicial de aproximadamente 4. Utilizando el modelo de la Sección 3, estime las probabilidades de relevancia para todos los documentos recuperados por todos los k experimentos. Calcular EMAP para todas las ejecuciones k, y P(ΔMAP < 0) para todos los pares de ejecuciones. Hacemos lo mismo para <br>MTC</br>, pero omitimos el paso 4. Ten en cuenta que después de evaluar los primeros c sistemas, no realizamos juicios de relevancia adicionales. Para poner a prueba nuestro método, seleccionamos c = 2: construiremos un conjunto de juicios a partir de evaluar solo dos sistemas iniciales. Luego generalizaremos a un conjunto de k = 10 (de los cuales esos dos son un subconjunto). A medida que realizamos más pruebas, obtenemos los datos que necesitamos para probar las tres hipótesis. 4.4 Evaluación Experimental Recuerde que un conjunto de juicios es robusto si la precisión de las predicciones que realiza es al menos su confianza estimada. Una forma de evaluar la robustez es agrupar pares por su confianza, luego calcular la precisión sobre todos los pares en cada grupo. Nos gustaría que la precisión no fuera menor que la puntuación de confianza más baja en el contenedor, pero preferiblemente más alta. Dado que las estadísticas resumen son útiles, ideamos la siguiente métrica. Supongamos que somos una casa de apuestas tomando apuestas sobre si ΔMAP < 0. Utilizamos RTC o MTC para establecer las probabilidades O = P (ΔMAP <0) 1−P (ΔMAP <0) . ",
            "candidates": [],
            "error": [
                [
                    "colección de pruebas mínimas",
                    "MTC"
                ]
            ]
        },
        "rtc": {
            "translated_key": "",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Robust Test Collections for Retrieval Evaluation Ben Carterette Center for Intelligent Information Retrieval Computer Science Department University of Massachusetts Amherst Amherst, MA 01003 carteret@cs.umass.edu ABSTRACT Low-cost methods for acquiring relevance judgments can be a boon to researchers who need to evaluate new retrieval tasks or topics but do not have the resources to make thousands of judgments.",
                "While these judgments are very useful for a one-time evaluation, it is not clear that they can be trusted when re-used to evaluate new systems.",
                "In this work, we formally define what it means for judgments to be reusable: the confidence in an evaluation of new systems can be accurately assessed from an existing set of relevance judgments.",
                "We then present a method for augmenting a set of relevance judgments with relevance estimates that require no additional assessor effort.",
                "Using this method practically guarantees reusability: with as few as five judgments per topic taken from only two systems, we can reliably evaluate a larger set of ten systems.",
                "Even the smallest sets of judgments can be useful for evaluation of new systems.",
                "Categories and Subject Descriptors: H.3 Information Storage and Retrieval; H.3.4 Systems and Software: Performance Evaluation General Terms: Experimentation, Measurement, Reliability 1.",
                "INTRODUCTION Consider an information retrieval researcher who has invented a new retrieval task.",
                "She has built a system to perform the task and wants to evaluate it.",
                "Since the task is new, it is unlikely that there are any extant relevance judgments.",
                "She does not have the time or resources to judge every document, or even every retrieved document.",
                "She can only judge the documents that seem to be the most informative and stop when she has a reasonable degree of confidence in her conclusions.",
                "But what happens when she develops a new system and needs to evaluate it?",
                "Or another research group decides to implement a system to perform the task?",
                "Can they reliably reuse the original judgments?",
                "Can they evaluate without more relevance judgments?",
                "Evaluation is an important aspect of information retrieval research, but it is only a semi-solved problem: for most retrieval tasks, it is impossible to judge the relevance of every document; there are simply too many of them.",
                "The solution used by NIST at TREC (Text REtrieval Conference) is the pooling method [19, 20]: all competing systems contribute N documents to a pool, and every document in that pool is judged.",
                "This method creates large sets of judgments that are reusable for training or evaluating new systems that did not contribute to the pool [21].",
                "This solution is not adequate for our hypothetical researcher.",
                "The pooling method gives thousands of relevance judgments, but it requires many hours of (paid) annotator time.",
                "As a result, there have been a slew of recent papers on reducing annotator effort in producing test collections: Cormack et al. [11], Zobel [21], Sanderson and Joho [17], Carterette et al. [8], and Aslam et al. [4], among others.",
                "As we will see, the judgments these methods produce can significantly bias the evaluation of a new set of systems.",
                "Returning to our hypothetical resesarcher, can she reuse her relevance judgments?",
                "First we must formally define what it means to be reusable.",
                "In previous work, reusability has been tested by simply assessing the accuracy of a set of relevance judgments at evaluating unseen systems.",
                "While we can say that it was right 75% of the time, or that it had a rank correlation of 0.8, these numbers do not have any predictive power: they do not tell us which systems are likely to be wrong or how confident we should be in any one.",
                "We need a more careful definition of reusability.",
                "Specifically, the question of reusability is not how accurately we can evaluate new systems.",
                "A malicious adversary can always produce a new ranked list that has not retrieved any of the judged documents.",
                "The real question is how much confidence we have in our evaluations, and, more importantly, whether we can trust our estimates of confidence.",
                "Even if confidence is not high, as long as we can trust it, we can identify which systems need more judgments in order to increase confidence.",
                "Any set of judgments, no matter how small, becomes reusable to some degree.",
                "Small, reusable test collections could have a huge impact on information retrieval research.",
                "Research groups would be able to share the relevance judgments they have done in-house for pilot studies, new tasks, or new topics.",
                "The amount of data available to researchers would grow exponentially over time. 2.",
                "ROBUST EVALUATION Above we gave an intuitive definition of reusability: a collection is reusable if we can trust our estimates of confidence in an evaluation.",
                "By that we mean that if we have made some relevance judgments and have, for example, 75% confidence that system A is better than system B, we would like there to be no more than 25% chance that our assessment of the relative quality of the systems will change as we continue to judge documents.",
                "Our evaluation should be robust to missing judgments.",
                "In our previous work, we defined confidence as the probability that the difference in an evaluation measure calculated for two systems is less than zero [8].",
                "This notion of confidence is defined in the context of a particular evaluation task that we call comparative evaluation: determining the sign of the difference in an evaluation measure.",
                "Other evaluation tasks could be defined; estimating the magnitude of the difference or the values of the measures themselves are examples that entail different notions of confidence.",
                "We therefore see confidence as a probability estimate.",
                "One of the questions we must ask about a probability estimate is what it means.",
                "What does it mean to have 75% confidence that system A is better than system B?",
                "As described above, we want it to mean that if we continue to judge documents, there will only be a 25% chance that our assessment will change.",
                "If this is what it means, we can trust the confidence estimates.",
                "But do we know it has that meaning?",
                "Our calculation of confidence rested on an assumption about the probability of relevance of unjudged documents, specifically that each unjudged document was equally likely to be relevant or nonrelevant.",
                "This assumption is almost certainly not realistic in most IR applications.",
                "As it turns out, it is this assumption that determines whether the confidence estimates can eb trusted.",
                "Before elaborating on this, we formally define confidence. 2.1 Estimating Confidence Average precision (AP) is a standard evaluation metric that captures both the ability of a system to rank relevant documents highly (precision) as well as its ability to retrieve relevant documents (recall).",
                "It is typically written as the mean precision at the ranks of relevant documents: AP = 1 |R| i∈R prec@r(i) where R is the set of relevant documents and r(i) is the rank of document i.",
                "Let Xi be a random variable indicating the relevance of document i.",
                "If documents are ordered by rank, we can express precision as prec@i = 1/i i j=1 Xj .",
                "Average precision then becomes the quadratic equation AP = 1 Xi n i=1 Xi/i i j=1 Xj = 1 Xi n i=1 j≥i aijXiXj where aij = 1/ max{r(i), r(j)}.",
                "Using aij instead of 1/i allows us to number the documents arbitrarily.",
                "To see why this is true, consider a toy example: a list of 3 documents with relevant documents B, C at ranks 1 and 3 and nonrelevant document A at rank 2.",
                "Average precision will be 1 2 (1 1 x2 B+ 1 2 xBxA+ 1 3 xBxC + 1 2 x2 A+ 1 3 xAxC + 1 3 x2 C) = 1 2 1 + 2 3 because xA = 0, xB = 1, xC = 1.",
                "Though the ordering B, A, C is different from the labeling A, B, C, it does not affect the computation.",
                "We can now see average precision itself is a random variable with a distribution over all possible assignments of relevance to all documents.",
                "This random variable has an expectation, a variance, confidence intervals, and a certain probability of being less than or equal to a given value.",
                "All of these are dependent on the probability that document i is relevant: pi = p(Xi = 1).",
                "Suppose in our previous example we do not know the relevance judgments, but we believe pA = 0.4, pB = 0.8, pC = 0.7.",
                "We can then compute e.g.",
                "P(AP = 0) = 0.2 · 0.6 · 0.3 = 0.036, or P(AP = 1 2 ) = 0.2 · 0.4 · 0.7 = 0.056.",
                "Summing over all possibilities, we can compute expectation and variance: E[AP] ≈ 1 pi aiipi + j>i aij pipj V ar[AP] ≈ 1 ( pi)2 n i a2 iipiqi + j>i a2 ijpipj(1 − pipj) + i=j 2aiiaijpipj(1 − pi) + k>j=i 2aijaikpipjpk(1 − pi) AP asymptotically converges to a normal distribution with expectation and variance as defined above.1 For our comparative evaluation task we are interested in the sign of the difference in two average precisions: ΔAP = AP1 − AP2.",
                "As we showed in our previous work, ΔAP has a closed form when documents are ordered arbitrarily: ΔAP = 1 Xi n i=1 j≥i cij XiXj cij = aij − bij where bij is defined analogously to aij for the second ranking.",
                "Since AP is normal, ΔAP is normal as well, meaning we can use the normal cumulative density function to determine the confidence that a difference in AP is less than zero.",
                "Since topics are independent, we can easily extend this to mean average precision (MAP).",
                "MAP is also normally distributed; its expectation and variance are: EMAP = 1 T t∈T E[APt] (1) VMAP = 1 T2 t∈T V ar[APt] ΔMAP = MAP1 − MAP2 Confidence can then be estimated by calculating the expectation and variance and using the normal density function to find P(ΔMAP < 0). 2.2 Confidence and Robustness Having defined confidence, we turn back to the issue of trust in confidence estimates, and show how it ties into the robustness of the collection to missing judgments. 1 These are actually approximations to the true expectation and variance, but the error is a negligible O(n2−n ).",
                "Let Z be the set of all pairs of ranked results for a common set of topics.",
                "Suppose we have a set of m relevance judgments xm = {x1, x2, ..., xm} (using small x rather than capital X to distinguish between judged and unjudged documents); these are the judgments against which we compute confidence.",
                "Let Zα be the subset of pairs in Z for which we predict that ΔMAP = −1 with confidence α given the judgments xm .",
                "For the confidence estimates to be accurate, we need at least α · |Zα| of these pairs to actually have ΔMAP = −1 after we have judged every document.",
                "If they do, we can trust the confidence estimates; our evaluation will be robust to missing judgments.",
                "If our confidence estimates are based on unrealistic assumptions, we cannot expect them to be accurate.",
                "The assumptions they are based on are the probabilities of relevance pi.",
                "We need these to be realistic.",
                "We argue that the best possible distribution of relevance p(Xi) is the one that explains all of the data (all of the observations made about the retrieval systems) while at the same time making no unwarranted assumptions.",
                "This is known as the principle of maximum entropy [13].",
                "The entropy of a random variable X with distribution p(X) is defined as H(p) = − i p(X = i) log p(X = i).",
                "This has found a wide array of uses in computer science and information retrieval.",
                "The maximum entropy distribution is the one that maximizes H. This distribution is unique and has an exponential form.",
                "The following theorem shows the utility of a maximum entropy distribution for relevance when estimating confidence.",
                "Theorem 1.",
                "If p(Xn |I, xm ) = argmaxpH(p), confidence estimates will be accurate. where xm is the set of relevance judgments defined above, Xn is the full set of documents that we wish to estimate the relevance of, and I is some information about the documents (unspecified as of now).",
                "We forgo the proof for the time being, but it is quite simple.",
                "This says that the better the estimates of relevance, the more accurate the evaluation.",
                "The task of creating a reusable test collection thus becomes the task of estimating the relevance of unjudged documents.",
                "The theorem and its proof say nothing whatsoever about the evaluation metric.",
                "The probability estimates are entirely indepedent of the measure we are interested in.",
                "This means the same probability estimates can tell us about average precision as well as precision, recall, bpref, etc.",
                "Furthermore, we could assume that the relevance of documents i and j is independent and achieve the same result, which we state as a corollary: Corollary 1.",
                "If p(Xi|I, xm ) = argmaxpH(p), confidence estimates will be accurate.",
                "The task therefore becomes the imputation of the missing values of relevance.",
                "The theorem implies that the closer we get to the maximum entropy distribution of relevance, the closer we get to robustness. 3.",
                "PREDICTING RELEVANCE In our statement of Theorem 1, we left the nature of the information I unspecified.",
                "One of the advantages of our confidence estimates is that they admit information from a wide variety of sources; essentially anything that can be modeled can be used as information for predicting relevance.",
                "A natural source of information is the retrieval systems themselves: how they ranked the judged documents, how often they failed to rank relevant documents, how they perform across topics, and so on.",
                "If we treat each system as an information retrieval expert providing an opinion about the relevance of each document, the problem becomes one of expert opinion aggregation.",
                "This is similar to the metasearch or data fusion problem in which the task is to take k input systems and merge them into a single ranking.",
                "Aslam et al. [3] previously identified a connection between evaluation and metasearch.",
                "Our problem has two key differences: 1.",
                "We explicitly need probabilities of relevance that we can plug into Eq. 1; metasearch algorithms have no such requirement. 2.",
                "We are accumulating relevance judgments as we proceed with the evaluation and are able to re-estimate relevance given each new judgment.",
                "In light of (1) above, we introduce a probabilistic model for expert combination. 3.1 A Model for Expert Opinion Aggregation Suppose that each expert j provides a probability of relevance qij = pj(Xi = 1).",
                "The information about the relevance of document i will then be the set of k expert opinions I = qi = (qi1, qi2, · · · , qik).",
                "The probability distribution we wish to find is the one that maximizes the entropy of pi = p(Xi = 1|qi).",
                "As it turns out, finding the maximum entropy model is equivalent to finding the parameters that maximize the likelihood [5].",
                "Blower [6] explicitly shows that finding the maximum entropy model for a binary variable is equivalent to solving a logistic regression.",
                "Then pi = p(Xi = 1|qi) = exp k j=1 λjqij 1 + exp k j=1 λj qij (2) where λ1, · · · , λk are the regression parameters.",
                "We include a beta prior for p(λj) with parameters α, β.",
                "This can be seen as a type of smoothing to account for the fact that the training data is highly biased.",
                "This model has the advantage of including the statistical dependence between the experts.",
                "A model of the same form was shown by Clemen & Winkler to be the best for aggregating expert probabilities [10].",
                "A similar maximumentropy-motivated approach has been used for expert aggregation [15].",
                "Aslam & Montague [1] used a similar model for metasearch, but assumed independence among experts.",
                "Where do the qij s come from?",
                "Using raw, uncalibrated scores as predictors will not work because score distributions vary too much between topics.",
                "A language modeling ranker, for instance, will typically give a much higher score to the top retrieved document for a short query than to the top retrieved document for a long query.",
                "We could train a separate predicting model for each topic, but that does not take advantage of all of the information we have: we may only have a handful of judgments for a topic, not enough to train a model to any confidence.",
                "Furthermore, it seems reasonable to assume that if an expert makes good predictions for one topic, it will make good predictions for other topics as well.",
                "We could use a hierarchical model [12], but that will not generalize to unseen topics.",
                "Instead, we will calibrate the scores of each expert individually so that scores can be compared both within topic and between topic.",
                "Thus our model takes into account not only the dependence between experts, but also the dependence between experts performances on different tasks (topics). 3.2 Calibrating Experts Each expert gives us a score and a rank for each document.",
                "We need to convert these to probabilities.",
                "A method such as the one used by Manmatha et al. [14] could be used to convert scores into probabilities of relevance.",
                "The pairwise preference method of Carterette & Petkova [9] could also be used, interpeting the ranking of one document over another as an expression of preference.",
                "Let q∗ ij be expert js self-reported probability that document i is relevant.",
                "Intuitively it seems clear that q∗ ij should decrease with rank, and it should be zero if document i is unranked (the expert did not believe it to be relevant).",
                "The pairwise preference model can handle these two requirements easily, so we will use it.",
                "Let θrj (i) be the relevance coefficient of the document at rank rj(i).",
                "We want to find the θs that maximize the likelihood function: Ljt(Θ) = rj (i)<rj (k) exp(θrj (i) − θrj (k)) 1 + exp(θrj (i) − θrj (k)) We again include a beta prior on p(θrj(i)) with parameters |Rt| + 1 and |Nt| + 1, the size of the sets of judged relevant and nonrelevant documents respectively.",
                "Using these as prior parameters ensures that the resulting probabilities will be concentrated around the ratio of relevant documents that have been discovered for topic t. This means that the probability estimates decrease by rank and are higher for topics that have more relevant documents.",
                "After finding the Θ that maximizes the likelihood, we have q∗ ij = exp(θrj (i)) 1+exp(θrj (i)) .",
                "We define θ∞ = −∞, so that the probability that an unranked document is relevant is 0.",
                "Since q∗ ij is based on the rank at which a document is retrieved rather than the identity of the document itself, the probabilities are identical from expert to expert, e.g. if expert E put document A at rank 1, and expert D put document B at rank 1, we will have q∗ AE = q∗ BD.",
                "Therefore we only have to solve this once for each topic.",
                "The above model gives topic-independent probabilities for each document.",
                "But suppose an expert who reports 90% probability is only right 50% of the time.",
                "Its opinion should be discounted based on its observed performance.",
                "Specifically, we want to learn a calibration function qij = Cj(q∗ ij) that will ensure that the predicted probabilities are tuned to the experts ability to retrieve relevant documents given the judgments that have been made to this point.",
                "Platts SVM calibration method [16] fits a sigmoid function between q∗ ij and the relevance judgments to obtain qij = Cj (q∗ ij) = exp(Aj +Bjq∗ ij ) 1+exp(Aj +Bj q∗ ij ) .",
                "Since q∗ ij is topic-independent, we only need to learn one calibration function for each expert.",
                "Once we have the calibration function, it is applied to adjust the experts predictions to their actual performance.",
                "The calibrated probabilities are plugged into model (2) to find the document probabilities.",
                "Figure 1: Conceptual diagram of our aggregation model.",
                "Experts E1, E2 have ranked documents A, B, C for topic T1 and documents D, E, F for topic T2.",
                "The first step is to obtain q∗ ij.",
                "Next is calibration to true performance to find qij .",
                "Finally we obtain pi = p(Xi = 1|qi1, qi2), · · · . 3.3 Model Summary Our model has three components that differ by the data they take as input and what they produce as output.",
                "A conceptual diagram is shown in Figure 1. 1. ranks → probabilities (per system per topic).",
                "This gives us q∗ ij, expert js self-reported probability of the relevance of document i.",
                "This is unsupervised; it requires no labeled data (though if we have some, we use it to set prior parameters). 2. probabilities → calibrated probabilities (per system).",
                "This gives us qij = Cj (q∗ ij), expert js calibrated probability of the relevance of document i.",
                "This is semisupervised; we have relevance judgments at some ranks which we use to impute the probability of relevance at other ranks. 3. calibrated probabilities → document probabilities.",
                "This gives us pi = p(Xi = 1|qi), the probability of relevance of document i given calibrated expert probabilities qij .",
                "This is supervised; we learn coefficients from a set of judged documents and use those to estimate the relevance of the unjudged documents.",
                "Although the model appears rather complex, it is really just three successive applications of logistic regression.",
                "As such, it can be implemented in a statistical programming language such as R in a few lines of code.",
                "The use of beta (conjugate) priors ensures that no expensive computational methods such as MCMC are necessary [12], so the model is trained and applied fast enough to be used on-line.",
                "Our code is available at http://ciir.cs.umass.edu/~carteret/. 4.",
                "EXPERIMENTS Three hypotheses are under consideration.",
                "The first, and most important, is that using our expert aggregation model to predict relevance produces test collections that are robust enough to be reusable; that is, we can trust the estimates of confidence when we evaluate systems that did not contribute any judgments to the pool.",
                "The other two hypotheses relate to the improvement we see by using better estimates of relevance than we did in our previous work [8].",
                "These are that (a) it takes fewer relevance track no. topics no. runs no. judged no. rel ad-hoc 94 50 40 97,319 9,805 ad-hoc 95 49 33 87,069 6,503 ad-hoc 96 50 61 133,681 5,524 ad-hoc 97 50 74 72,270 4,611 ad-hoc 98 50 103 80,345 4,674 ad-hoc 99 50 129 86,830 4,728 web 04 225 74 88,566 1,763 robust 05 50 74 37,798 6,561 terabyte 05 50 58 45,291 10,407 Table 1: Number of topics, number of runs, number of documents judged, and number found relevant for each of our data sets. judgments to reach 95% confidence and (b) the accuracy of the predictions is higher than if we were to simply assume pi = 0.5 for all unjudged documents. 4.1 Data We obtained full ad-hoc runs submitted to TRECs 3 through 8.",
                "Each run ranks at most 1000 documents for 50 topics (49 topics for TREC 4).",
                "Additionally, we obtained all runs from the Web track of TREC 13, the Robust2 track of TREC 14, and the Terabyte (ad-hoc) track of TREC 14.",
                "These are the tracks that have replaced the ad-hoc track since its end in 1999.",
                "Statistics are shown in Table 1.",
                "We set aside the TREC 4 (ad-hoc 95) set for training, TRECs 3 and 5-8 (ad-hoc 94 and 96-99) for primary testing, and the remaining sets for additional testing.",
                "We use the qrels files assembled by NIST as truth.",
                "The number of relevance judgments made and relevant documents found for each track are listed in Table 1.",
                "For computational reasons, we truncate ranked lists at 100 documents.",
                "There is no reason that we could not go deeper, but calculating variance is O(n3 ) and thus very timeconsuming.",
                "Because of the reciprocal rank nature of AP, we do not lose much information by truncating at rank 100. 4.2 Algorithms We will compare three algorithms for acquiring relevance judgments.",
                "The baseline is a variation of TREC pooling that we will call incremental pooling (IP).",
                "This algorithm takes a number k as input and presents the first k documents in rank order (without regard to topic) to be judged.",
                "It does not estimate the relevance of unjudged documents; it simply assumes any unjudged document is nonrelevant.",
                "The second algorithm is that presented in Carterette et al. [8] (Algorithm 1).",
                "Documents are selected based on how interesting they are in determining whether a difference in mean average precision exists.",
                "For this approach pi = 0.5 for all i; there is no estimation of probabilities.",
                "We will call this MTC for minimal test collection.",
                "The third algorithm augments MTC with updated estimates of probabilities of relevance.",
                "We will call this <br>rtc</br> for robust test collection.",
                "It is identical to Algorithm 1, except that every 10th iteration we estimate pi for all unjudged documents i using the expert aggregation model of Section 3.",
                "<br>rtc</br> has smoothing (prior distribution) parameters that must be set.",
                "We trained using the ad-hoc 95 set.",
                "We limited 2 Robust here means robust retrieval; this is different from our goal of robust evaluation.",
                "Algorithm 1 (MTC) Given two ranked lists and confidence level α, predict the sign of ΔMAP. 1: pi ← 0.5 for all documents i 2: while P(ΔMAP < 0) < α do 3: calculate weight wi for all unjudged documents i (see Carterette et al. [8] for details) 4: j ← argmaxiwi 5: xj ← 1 if document j is relevant, 0 otherwise 6: pj ← xj 7: end while the search to uniform priors with relatively high variance.",
                "For expert aggregation, the prior parameters are α = β = 1. 4.3 Experimental Design First, we want to know whether we can augment a set of relevance judgments with a set of relevance probabilities in order to reuse the judgments to evaluate a new set of systems.",
                "For each experimental trial: 1.",
                "Pick a random subset of k runs. 2.",
                "From those k, pick an initial c < k to evaluate. 3.",
                "Run <br>rtc</br> to 95% confidence on the initial c. 4.",
                "Using the model from Section 3, estimate the probabilities of relevance for all documents retrieved by all k runs. 5.",
                "Calculate EMAP for all k runs, and P(ΔMAP < 0) for all pairs of runs.",
                "We do the same for MTC, but omit step 4.",
                "Note that after evaluating the first c systems, we make no additional relevance judgments.",
                "To put our method to the test, we selected c = 2: we will build a set of judgments from evaluating only two initial systems.",
                "We will then generalize to a set of k = 10 (of which those two are a subset).",
                "As we run more trials, we obtain the data we need to test all three of our hypotheses. 4.4 Experimental Evaluation Recall that a set of judgments is robust if the accuracy of the predictions it makes is at least its estimated confidence.",
                "One way to evaluate robustness is to bin pairs by their confidence, then calculate the accuracy over all the pairs in each bin.",
                "We would like the accuracy to be no less than the lowest confidence score in the bin, but preferably higher.",
                "Since summary statistics are useful, we devised the following metric.",
                "Suppose we are a bookmaker taking bets on whether ΔMAP < 0.",
                "We use <br>rtc</br> or MTC to set the odds O = P (ΔMAP <0) 1−P (ΔMAP <0) .",
                "Suppose a bettor wagers $1 on ΔMAP ≥ 0.",
                "If it turns out that ΔMAP < 0, we win the dollar.",
                "Otherwise, we pay out O.",
                "If our confidence estimates are perfectly accurate, we break even.",
                "If confidence is greater than accuracy, we lose money; we win if accuracy is greater than confidence.",
                "Counterintuitively, the most desirable outcome is breaking even: if we lose money, we cannot trust the confidence estimates, but if we win money, we have either underestimated confidence or judged more documents than necessary.",
                "However, the cost of not being able to trust the confidence estimates is higher than the cost of extra relevance judgments, so we will treat positive outcomes as good.",
                "The amount we win on each pairwise comparison i is: Wi = yi − (1 − yi) Pi 1 − Pi = yi − Pi 1 − Pi yi = 1 if ΔMAP < 0 and 0 otherwise, and Pi = P(ΔMAP < 0).",
                "The summary statistic is W, the mean of Wi.",
                "Note that as Pi increases, we lose more for being wrong.",
                "This is as it should be: the penalty should be great for missing the high probability predictions.",
                "However, since our losses grow without bound as predictions approach certainty, we cap −Wi at 100.",
                "For our hypothesis that <br>rtc</br> requires fewer judgments than MTC, we are interested in the number of judgments needed to reach 95% confidence on the first pair of systems.",
                "The median is more interesting than the mean: most pairs require a few hundred judgments, but a few pairs require several thousand.",
                "The distribution is therefore highly skewed, and the mean strongly affected by those outliers.",
                "Finally, for our hypothesis that <br>rtc</br> is more accurate than MTC, we will look at Kendalls τ correlation between a ranking of k systems by a small set of judgments and the true ranking using the full set of judgments.",
                "Kendalls τ, a nonparametric statistic based on pairwise swaps between two lists, is a standard evaluation for this type of study.",
                "It ranges from −1 (perfectly anti-correlated) to 1 (rankings identical), with 0 meaning that half of the pairs are swapped.",
                "As we touched on in the introduction, though, an accuracy measure like rank correlation is not a good evaluation of reusability.",
                "We include it for completeness. 4.4.1 Hypothesis Testing Running multiple trials allows the use of statistical hypothesis testing to compare algorithms.",
                "Using the same sets of systems allows the use of paired tests.",
                "As we stated above, we are more interested in the median number of judgments than the mean.",
                "A test for difference in median is the Wilcoxon sign rank test.",
                "We can also use a paired t-test to test for a difference in mean.",
                "For rank correlation, we can use a paired t-test to test for a difference in τ. 5.",
                "RESULTS AND ANALYSIS The comparison between MTC and <br>rtc</br> is shown in Table 2.",
                "With MTC and uniform probabilities of relevance, the results are far from robust.",
                "We cannot reuse the relevance judgments with much confidence.",
                "But with <br>rtc</br>, the results are very robust.",
                "There is a slight dip in accuracy when confidence gets above 0.95; nonetheless, the confidence predictions are trustworthy.",
                "Mean Wi shows that <br>rtc</br> is much closer to 0 than MTC.",
                "The distribution of confidence scores shows that at least 80% confidence is achieved more than 35% of the time, indicating that neither algorithm is being too conservative in its confidence estimates.",
                "The confidence estimates are rather low overall; that is because we have built a test collection from only two initial systems.",
                "Recall from Section 1 that we cannot require (or even expect) a minimum level of confidence when we generalize to new systems.",
                "More detailed results for both algorithms are shown in Figure 2.",
                "The solid line is the ideal result that would give W = 0.",
                "<br>rtc</br> is on or above this line at all points until confidence reaches about 0.97.",
                "After that there is a slight dip in accuracy which we discuss below.",
                "Note that both MTC <br>rtc</br> confidence % in bin accuracy % in bin accuracy 0.5 − 0.6 33.7% 61.7% 28.6% 61.9% 0.6 − 0.7 18.1% 73.1% 20.1% 76.3% 0.7 − 0.8 10.4% 70.1% 15.5% 78.0% 0.8 − 0.9 9.4% 69.0% 12.1% 84.9% 0.9 − 0.95 7.3% 78.0% 6.6% 93.1% 0.95 − 0.99 17.9% 70.4% 12.4% 93.4% 1.0 3.3% 68.3% 4.7% 98.9% W −5.34 −0.39 median judged 251 235 mean τ 0.393 0.555 Table 2: Confidence that P(ΔMAP < 0) and accuracy of prediction when generalizing a set of relevance judgments acquired using MTC and <br>rtc</br>.",
                "Each bin contains over 1,000 trials from the adhoc 3, 5-8 sets.",
                "<br>rtc</br> is much more robust than MTC.",
                "W is defined in Section 4.4; closer to 0 is better.",
                "Median judged is the number of judgments to reach 95% confidence on the first two systems.",
                "Mean τ is the average rank correlation for all 10 systems. 0.5 0.6 0.7 0.8 0.9 1 0.5 0.6 0.7 0.8 0.9 1 accuracy confidence breakeven <br>rtc</br> MTC Figure 2: Confidence vs. accuracy of MTC and <br>rtc</br>.",
                "The solid line is the perfect result that would give W = 0; performance should be on or above this line.",
                "Each point represents at least 500 pairwise comparisons. algorithms are well above the line up to around confidence 0.7.",
                "This is because the baseline performance on these data sets is high; it is quite easy to achieve 75% accuracy doing very little work [7].",
                "Number of Judgments: The median number of judgments required by MTC to reach 95% confidence on the first two systems is 251, an average of 5 per topic.",
                "The median required by <br>rtc</br> is 235, about 4.7 per topic.",
                "Although the numbers are close, RTCs median is significantly lower by a paired Wilcoxon test (p < 0.0001).",
                "For comparison, a pool of depth 100 would result in a minimum of 5,000 judgments for each pair.",
                "The difference in means is much greater.",
                "MTC required a mean of 823 judgments, 16 per topic, while <br>rtc</br> required a mean of 502, 10 per topic. (Recall that means are strongly skewed by a few pairs that take thousands of judgments.)",
                "This difference is significant by a paired t-test (p < 0.0001).",
                "Ten percent of the sets resulted in 100 or fewer judgments (less than two per topic).",
                "Performance on these is very high: W = 0.41, and 99.7% accuracy when confidence is at least 0.9.",
                "This shows that even tiny collections can be reusable.",
                "For the 50% of sets with more than 235 judgments, accuracy is 93% when confidence is at least 0.9.",
                "Rank Correlation: MTC and <br>rtc</br> both rank the 10 systems by EMAP (Eq. (1)) calculated using their respective probability estimates.",
                "The mean τ rank correlation between true MAP and EMAP is 0.393 for MTC and 0.555 for <br>rtc</br>.",
                "This difference is significant by a paired t-test (p < 0.0001).",
                "Note that we do not expect the τ correlations to be high, since we are ranking the systems with so few relevance judgments.",
                "It is more important that we estimate confidence in each pairwise comparison correctly.",
                "We ran IP for the same number of judgments that MTC took for each pair, then ranked the systems by MAP using only those judgments (all unjudged documents assumed nonrelevant).",
                "We calculated the τ correlation to the true ranking.",
                "The mean τ correlation is 0.398, which is not significantly different from MTC, but is significantly lower than <br>rtc</br>.",
                "Using uniform estimates of probability is indistinguishable from the baseline, whereas estimating relevance by expert aggregation boosts performance a great deal: nearly 40% over both MTC and IP.",
                "Overfitting: It is possible to overfit: if too many judgments come from the first two systems, the variance in ΔMAP is reduced and the confidence estimates become unreliable.",
                "We saw this in Table 2 and Figure 2 where <br>rtc</br> exhibits a dip in accuracy when confidence is around 97%.",
                "In fact, the number of judgments made prior to a wrong prediction is over 50% greater than the number made prior to a correct prediction.",
                "Overfitting is difficult to quantify exactly, because making more relevance judgments does not always cause it: at higher confidence levels, more relevance judgments are made, and as Table 2 shows, accuracy is greater at those higher confidences.",
                "Obviously having more relevance judgments should increase both confidence and accuracy; the difference seems to be when one system has a great deal more judgments than the other.",
                "Pairwise Comparisons: Our pairwise comparisons fall into one of three groups: 1. the two original runs from which relevance judgments are acquired; 2. one of the original runs vs. one of the new runs; 3. two new runs.",
                "Table 3 shows confidence vs. accuracy results for each of these three groups.",
                "Interestingly, performance is worst when comparing one of the original runs to one of the additional runs.",
                "This is most likely due to a large difference in the number of judgments affecting the variance of ΔMAP.",
                "Nevertheless, performance is quite good on all three subsets.",
                "Worst Case: The case intuitively most likely to produce an error is when the two systems being compared have retrieved very few documents in common.",
                "If we want the judgments to be reusable, we should to be able to generalize even to runs that are very different from the ones used to acquire the relevance judgments.",
                "A simple measure of similarity of two runs is the average percentage of documents they retrieved in common for each topic [2].",
                "We calculated this for all pairs, then looked at performance on pairs with low similarity.",
                "Results are shown in accuracy confidence two original one original no original 0.5 − 0.6 - 48.1% 62.8% 0.6 − 0.7 - 57.1% 79.2% 0.7 − 0.8 - 67.9% 81.7% 0.8 − 0.9 - 82.2% 86.3% 0.9 − 0.95 95.9% 93.7% 92.6% 0.95 − 0.99 96.2% 92.5% 93.1% 1.0 100% 98.0% 99.1% W −1.11 −0.87 −0.27 Table 3: Confidence vs. accuracy of <br>rtc</br> when comparing the two original runs, one original run and one new run, and two new runs.",
                "<br>rtc</br> is robust in all three cases. accuracy when similar confidence 0 − 0.1 0.1 − 0.2 0.2 − 0.3 0.5 − 0.6 68.4% 63.1% 61.4% 0.6 − 0.7 84.2% 78.6% 76.6% 0.7 − 0.8 82.0% 79.8% 78.9% 0.8 − 0.9 93.6% 83.3% 82.1% 0.9 − 0.95 99.3% 92.7% 92.4% 0.95 − 0.99 98.7% 93.4% 93.3% 1.0 99.9% 97.9% 98.1% W 0.44 −0.45 −0.49 Table 4: Confidence vs. accuracy of <br>rtc</br> when a pair of systems retrieved 0-30% documents in common (broken out into 0%-10%, 10%-20%, and 20%30%).",
                "<br>rtc</br> is robust in all three cases.",
                "Table 4.",
                "Performance is in fact very robust even when similarity is low.",
                "When the two runs share very few documents in common, W is actually positive.",
                "MTC and IP both performed quite poorly in these cases.",
                "When the similarity was between 0 and 10%, both MTC and IP correctly predicted ΔMAP only 60% of the time, compared to an 87.6% success rate for <br>rtc</br>.",
                "By Data Set: All the previous results have only been on the ad-hoc collections.",
                "We did the same experiments on our additional data sets, and broke out the results by data set to see how performance varies.",
                "The results in Table 5 show everything about each set, including binned accuracy, W, mean τ, and median number of judgments to reach 95% confidence on the first two systems.",
                "The results are highly consistent from collection to collection, suggesting that our method is not overfitting to any particular data set. 6.",
                "CONCLUSIONS AND FUTURE WORK In this work we have offered the first formal definition of the common idea of reusability of a test collection and presented a model that is able to achieve reusability with very small sets of relevance judgments.",
                "Table 2 and Figure 2 together show how biased a small set of judgments can be: MTC is dramatically overestimating confidence and is much less accurate than <br>rtc</br>, which is able to remove the bias to give a robust evaluation.",
                "The confidence estimates of <br>rtc</br>, in addition to being accurate, provide a guide for obtaining additional judgments: focus on judging documents from the lowest-confidence comparisons.",
                "In the long run, we see small sets of relevance judgaccuracy confidence ad-hoc 94 ad-hoc 96 ad-hoc 97 ad-hoc 98 ad-hoc 99 web 04 robust 05 terabyte 05 0.5 − 0.6 64.1% 61.8% 62.2% 62.0% 59.4% 64.3% 61.5% 61.6% 0.6 − 0.7 76.1% 77.8% 74.5% 78.2% 74.3% 78.1% 75.9% 75.9% 0.7 − 0.8 75.2% 78.9% 77.6% 80.0% 78.6% 82.6% 77.5% 80.4% 0.8 − 0.9 83.2% 85.5% 84.6% 84.9% 86.8% 84.5% 86.7% 87.7% 0.9 − 0.95 93.0% 93.6% 92.8% 93.7% 92.6% 94.2% 93.9% 94.2% 0.95 − 0.99 93.1% 94.3% 93.1% 93.7% 92.8% 95.0% 93.9% 91.6% 1.0 99.2% 96.8% 98.7% 99.5% 99.6% 100% 99.2% 98.3% W -0.34 -0.34 -0.48 -0.35 -0.44 -0.07 -0.41 -0.67 median judged 235 276 243 213 179 448 310 320 mean τ 0.538 0.573 0.556 0.579 0.532 0.596 0.565 0.574 Table 5: Accuracy, W, mean τ, and median number of judgments for all 8 testing sets.",
                "The results are highly consistent across data sets. ments being shared by researchers, each group contributing a few more judgments to gain more confidence about their particular systems.",
                "As time goes on, the number of judgments grows until there is 100% confidence in every evaluation-and there is a full test collection for the task.",
                "We see further use for this method in scenarios such as web retrieval in which the corpus is frequently changing.",
                "It could be applied to evaluation on a dynamic test collection as defined by Soboroff [18].",
                "The model we presented in Section 3 is by no means the only possibility for creating a robust test collection.",
                "A simpler expert aggregation model might perform as well or better (though all our efforts to simplify failed).",
                "In addition to expert aggregation, we could estimate probabilities by looking at similarities between documents.",
                "This is an obvious area for future exploration.",
                "Additionally, it will be worthwhile to investigate the issue of overfitting: the circumstances it occurs under and what can be done to prevent it.",
                "In the meantime, capping confidence estimates at 95% is a hack that solves the problem.",
                "We have many more experimental results that we unfortunately did not have space for but that reinforce the notion that <br>rtc</br> is highly robust: with just a few judgments per topic, we can accurately assess the confidence in any pairwise comparison of systems.",
                "Acknowledgments This work was supported in part by the Center for Intelligent Information Retrieval and in part by the Defense Advanced Research Projects Agency (DARPA) under contract number HR0011-06-C-0023.",
                "Any opinions, findings, and conclusions or recommendations expressed in this material are those of the author and do not necessarily reflect those of the sponsor. 7.",
                "REFERENCES [1] J. Aslam and M. Montague.",
                "Models for Metasearch.",
                "In Proceedings of SIGIR, pages 275-285, 2001. [2] J. Aslam and R. Savell.",
                "On the effectiveness of evaluating retrieval systems in the absence of relevance judgments.",
                "In Proceedings of SIGIR, pages 361-362, 2003. [3] J.",
                "A. Aslam, V. Pavlu, and R. Savell.",
                "A unified model for metasearch, pooling, and system evaluation.",
                "In Proceedings of CIKM, pages 484-491, 2003. [4] J.",
                "A. Aslam, V. Pavlu, and E. Yilmaz.",
                "A statistical method for system evaluation using incomplete judgments.",
                "In Proceedings of SIGIR, pages 541-548, 2006. [5] A. L. Berger, S. D. Pietra, and V. J. D. Pietra.",
                "A maximum entropy approach to natural language processing.",
                "Computational Linguistics, 22(1):39-71, 1996. [6] D. J. Blower.",
                "An easy derivation of logistic regression from the bayesian and maximum entropy perspective.",
                "In Proceedings of the 23rd International Workship on Bayesian Inference and Maximum Entropy Methods in Science and Engineering, pages 30-43, 2004. [7] B. Carterette and J. Allan.",
                "Research methodology in studies of assessor effort for retrieval evaluation.",
                "In Proceedings of RIAO, 2007. [8] B. Carterette, J. Allan, and R. K. Sitaraman.",
                "Minimal test collections for retrieval evaluation.",
                "In Proceedings of SIGIR, pages 268-275, 2006. [9] B. Carterette and D. I. Petkova.",
                "Learning a ranking from pairwise preferences.",
                "In Proceedings of SIGIR, 2006. [10] R. T. Clemen and R. L. Winkler.",
                "Unanimity and compromise among probability forecasters.",
                "Management Science, 36(7):767-779, July 1990. [11] G. V. Cormack, C. R. Palmer, and C. L. Clarke.",
                "Efficient Construction of Large Test Collections.",
                "In Proceedings of SIGIR, pages 282-289, 1998. [12] A. Gelman, J.",
                "B. Carlin, H. S. Stern, and D. B. Rubin.",
                "Bayesian Data Analysis.",
                "Chapman & Hall/CRC, 2004. [13] E. T. Jaynes.",
                "Probability Theory: The Logic of Science.",
                "Cambridge University Press, 2003. [14] R. Manmatha and H. Sever.",
                "A Formal Approach to Score Normalization for Metasearch.",
                "In Proceedings of HLT, pages 88-93, 2002. [15] I. J. Myung, S. Ramamoorti, and J. Andrew D. Baily.",
                "Maximum entropy aggregation of expert predictions.",
                "Management Science, 42(10):1420-1436, October 1996. [16] J. Platt.",
                "Probabilistic outputs for support vector machines and comparison to regularized likelihood methods. pages 61-74, 2000. [17] M. Sanderson and H. Joho.",
                "Forming test collections with no system pooling.",
                "In Proceedings of the 27th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 33-40, 2004. [18] I. Soboroff.",
                "Dynamic test collections: measuring search effectiveness on the live web.",
                "In Proceedings of SIGIR, pages 276-283, 2006. [19] K. Sparck Jones and C. J. van Rijsbergen.",
                "Information Retrieval Test Collections.",
                "Journal of Documentation, 32(1):59-75, 1976. [20] E. M. Voorhees and D. K. Harman, editors.",
                "TREC: Experiment and Evaluation in Information Retrieval.",
                "MIT Press, 2005. [21] J. Zobel.",
                "How Reliable are the Results of Large-Scale Information Retrieval Experiments?",
                "In Proceedings of SIGIR, pages 307-314, 1998."
            ],
            "original_annotated_samples": [
                "We will call this <br>rtc</br> for robust test collection.",
                "<br>rtc</br> has smoothing (prior distribution) parameters that must be set.",
                "Run <br>rtc</br> to 95% confidence on the initial c. 4.",
                "We use <br>rtc</br> or MTC to set the odds O = P (ΔMAP <0) 1−P (ΔMAP <0) .",
                "For our hypothesis that <br>rtc</br> requires fewer judgments than MTC, we are interested in the number of judgments needed to reach 95% confidence on the first pair of systems."
            ],
            "translated_annotated_samples": [
                "Llamaremos a esto RTC por <br>colección de pruebas robusta</br>.",
                "RTC tiene <br>parámetros de suavizado</br> (distribución previa) que deben ser configurados.",
                "Ejecutar RTC al 95% de confianza en el c inicial de aproximadamente 4.",
                "Utilizamos RTC o MTC para establecer las probabilidades O = P (ΔMAP <0) 1−P (ΔMAP <0) .",
                "Para nuestra hipótesis de que RTC requiere menos juicios que MTC, estamos interesados en el número de juicios necesarios para alcanzar el 95% de confianza en el primer par de sistemas."
            ],
            "translated_text": "Colecciones de pruebas robustas para la evaluación de recuperación Ben Carterette Centro de Recuperación de Información Inteligente Departamento de Ciencias de la Computación Universidad de Massachusetts Amherst Amherst, MA 01003 carteret@cs.umass.edu RESUMEN Los métodos de bajo costo para adquirir juicios de relevancia pueden ser una bendición para los investigadores que necesitan evaluar nuevas tareas o temas de recuperación pero no tienen los recursos para hacer miles de juicios. Si bien estos juicios son muy útiles para una evaluación única, no está claro que se puedan confiar al ser reutilizados para evaluar nuevos sistemas. En este trabajo, definimos formalmente lo que significa que las evaluaciones sean reutilizables: la confianza en una evaluación de nuevos sistemas puede ser evaluada con precisión a partir de un conjunto existente de juicios de relevancia. Luego presentamos un método para aumentar un conjunto de juicios de relevancia con estimaciones de relevancia que no requieren esfuerzo adicional del evaluador. Usar este método garantiza prácticamente la reutilización: con tan solo cinco juicios por tema tomados de solo dos sistemas, podemos evaluar de manera confiable un conjunto más grande de diez sistemas. Incluso los conjuntos más pequeños de juicios pueden ser útiles para la evaluación de nuevos sistemas. Categorías y Descriptores de Asignaturas: H.3 Almacenamiento y Recuperación de Información; H.3.4 Sistemas y Software: Evaluación del Rendimiento Términos Generales: Experimentación, Medición, Confiabilidad 1. INTRODUCCIÓN Imagina a un investigador de recuperación de información que ha inventado una nueva tarea de recuperación. Ella ha construido un sistema para realizar la tarea y quiere evaluarlo. Dado que la tarea es nueva, es poco probable que existan juicios de relevancia vigentes. Ella no tiene el tiempo ni los recursos para juzgar cada documento, ni siquiera cada documento recuperado. Ella solo puede juzgar los documentos que parecen ser los más informativos y detenerse cuando tenga un grado razonable de confianza en sus conclusiones. Pero ¿qué sucede cuando ella desarrolla un nuevo sistema y necesita evaluarlo? ¿Otro grupo de investigación decide implementar un sistema para realizar la tarea? ¿Pueden reutilizar de manera confiable los juicios originales? ¿Pueden evaluar sin más juicios de relevancia? La evaluación es un aspecto importante de la investigación en recuperación de información, pero es solo un problema parcialmente resuelto: para la mayoría de las tareas de recuperación, es imposible juzgar la relevancia de cada documento; simplemente hay demasiados. La solución utilizada por NIST en TREC (Text REtrieval Conference) es el método de agrupación [19, 20]: todos los sistemas competidores contribuyen con N documentos a un grupo, y cada documento en ese grupo es evaluado. Este método crea grandes conjuntos de juicios que son reutilizables para entrenar o evaluar nuevos sistemas que no contribuyeron al conjunto [21]. Esta solución no es adecuada para nuestro investigador hipotético. El método de agrupación proporciona miles de juicios de relevancia, pero requiere muchas horas de tiempo de anotadores (remunerados). Como resultado, ha habido una serie de artículos recientes sobre la reducción del esfuerzo del anotador en la producción de colecciones de pruebas: Cormack et al. [11], Zobel [21], Sanderson y Joho [17], Carterette et al. [8], y Aslam et al. [4], entre otros. Como veremos, los juicios que estos métodos producen pueden sesgar significativamente la evaluación de un nuevo conjunto de sistemas. ¿Volviendo a nuestra investigadora hipotética, puede reutilizar sus juicios de relevancia? Primero debemos definir formalmente lo que significa ser reutilizable. En trabajos anteriores, la reutilización se ha probado simplemente evaluando la precisión de un conjunto de juicios de relevancia al evaluar sistemas no vistos. Si bien podemos decir que tuvo razón el 75% del tiempo, o que tuvo una correlación de rango de 0.8, estos números no tienen ningún poder predictivo: no nos dicen qué sistemas es probable que estén equivocados ni cuán seguros deberíamos estar en uno en particular. Necesitamos una definición más cuidadosa de reutilización. Específicamente, la cuestión de la reutilización no es qué tan precisamente podemos evaluar los nuevos sistemas. Un adversario malicioso siempre puede producir una nueva lista clasificada que no haya recuperado ninguno de los documentos evaluados. La verdadera pregunta es cuánta confianza tenemos en nuestras evaluaciones, y, más importante aún, si podemos confiar en nuestras estimaciones de confianza. Aunque la confianza no sea alta, siempre y cuando podamos confiar en ello, podemos identificar qué sistemas necesitan más juicios para aumentar la confianza. Cualquier conjunto de juicios, por pequeño que sea, se vuelve reutilizable en cierta medida. Pequeñas colecciones de pruebas reutilizables podrían tener un gran impacto en la investigación de recuperación de información. Los grupos de investigación podrían compartir las evaluaciones de relevancia que han realizado internamente para estudios piloto, nuevas tareas o nuevos temas. La cantidad de datos disponible para los investigadores crecería de forma exponencial con el tiempo. EVALUACIÓN ROBUSTA Anteriormente dimos una definición intuitiva de reutilización: una colección es reutilizable si podemos confiar en nuestras estimaciones de confianza en una evaluación. Con eso queremos decir que si hemos realizado algunos juicios de relevancia y tenemos, por ejemplo, un 75% de confianza en que el sistema A es mejor que el sistema B, nos gustaría que no haya más del 25% de posibilidad de que nuestra evaluación de la calidad relativa de los sistemas cambie a medida que continuamos evaluando documentos. Nuestra evaluación debe ser robusta ante juicios faltantes. En nuestro trabajo anterior, definimos la confianza como la probabilidad de que la diferencia en una medida de evaluación calculada para dos sistemas sea menor que cero [8]. Esta noción de confianza se define en el contexto de una tarea de evaluación particular que llamamos evaluación comparativa: determinar el signo de la diferencia en una medida de evaluación. Otras tareas de evaluación podrían ser definidas; estimar la magnitud de la diferencia o los valores de las medidas mismas son ejemplos que implican diferentes nociones de confianza. Por lo tanto, consideramos la confianza como una estimación de probabilidad. Una de las preguntas que debemos hacer acerca de una estimación de probabilidad es qué significa. ¿Qué significa tener un 75% de confianza de que el sistema A es mejor que el sistema B? Como se describe arriba, queremos que signifique que si seguimos evaluando documentos, solo habrá un 25% de probabilidad de que nuestra evaluación cambie. Si esto es lo que significa, podemos confiar en las estimaciones de confianza. ¿Pero sabemos que tiene ese significado? Nuestro cálculo de confianza se basó en una suposición sobre la probabilidad de relevancia de los documentos no evaluados, específicamente que cada documento no evaluado tenía la misma probabilidad de ser relevante o no relevante. Esta suposición casi con seguridad no es realista en la mayoría de las aplicaciones de IR. Resulta que es esta suposición la que determina si se pueden confiar en las estimaciones de confianza. Antes de profundizar en esto, definimos formalmente la confianza. 2.1 Estimación de la confianza La precisión promedio (AP) es una métrica de evaluación estándar que captura tanto la capacidad de un sistema para clasificar documentos relevantes de manera alta (precisión) como su capacidad para recuperar documentos relevantes (recuperación). Normalmente se escribe como la precisión media en los rangos de documentos relevantes: AP = 1 |R| i∈R prec@r(i) donde R es el conjunto de documentos relevantes y r(i) es el rango del documento i. Sea Xi una variable aleatoria que indica la relevancia del documento i. Si los documentos están ordenados por rango, podemos expresar la precisión como prec@i = 1/i i j=1 Xj. La precisión promedio se convierte entonces en la ecuación cuadrática AP = 1 Xi n i=1 Xi/i i j=1 Xj = 1 Xi n i=1 j≥i aijXiXj donde aij = 1/ max{r(i), r(j)}. Usar aij en lugar de 1/i nos permite numerar los documentos de forma arbitraria. Para ver por qué esto es cierto, considera un ejemplo simple: una lista de 3 documentos con los documentos relevantes B, C en los rangos 1 y 3 y el documento no relevante A en el rango 2. La precisión promedio será 1 2 (1 1 x2 B+ 1 2 xBxA+ 1 3 xBxC + 1 2 x2 A+ 1 3 xAxC + 1 3 x2 C) = 1 2 1 + 2 3 porque xA = 0, xB = 1, xC = 1. Aunque el orden B, A, C es diferente de la etiqueta A, B, C, no afecta al cálculo. Ahora podemos ver que la precisión promedio en sí misma es una variable aleatoria con una distribución sobre todas las posibles asignaciones de relevancia a todos los documentos. Esta variable aleatoria tiene una esperanza, una varianza, intervalos de confianza y una cierta probabilidad de ser menor o igual a un valor dado. Todos estos dependen de la probabilidad de que el documento i sea relevante: pi = p(Xi = 1). Supongamos que en nuestro ejemplo anterior no conocemos las valoraciones de relevancia, pero creemos que pA = 0.4, pB = 0.8, pC = 0.7. Podemos entonces calcular, por ejemplo. P(AP = 0) = 0.2 · 0.6 · 0.3 = 0.036, o P(AP = 1 2 ) = 0.2 · 0.4 · 0.7 = 0.056. \n\nLa traducción al español sería: P(AP = 0) = 0.2 · 0.6 · 0.3 = 0.036, o P(AP = 1 2 ) = 0.2 · 0.4 · 0.7 = 0.056. Sumando sobre todas las posibilidades, podemos calcular la esperanza y la varianza: E[AP] ≈ 1 pi aiipi + j>i aij pipj V ar[AP] ≈ 1 ( pi)2 n i a2 iipiqi + j>i a2 ijpipj(1 − pipj) + i=j 2aiiaijpipj(1 − pi) + k>j=i 2aijaikpipjpk(1 − pi) AP converge asintóticamente a una distribución normal con la esperanza y la varianza definidas anteriormente. Para nuestra tarea de evaluación comparativa, estamos interesados en el signo de la diferencia en dos precisiones promedio: ΔAP = AP1 − AP2. Como mostramos en nuestro trabajo anterior, ΔAP tiene una forma cerrada cuando los documentos están ordenados arbitrariamente: ΔAP = 1 Xi n i=1 j≥i cij XiXj cij = aij − bij donde bij está definido de manera análoga a aij para la segunda clasificación. Dado que AP es normal, ΔAP también es normal, lo que significa que podemos usar la función de densidad acumulativa normal para determinar la confianza de que una diferencia en AP sea menor que cero. Dado que los temas son independientes, podemos extender fácilmente esto para referirnos a la precisión promedio (MAP). MAP también se distribuye normalmente; su esperanza y varianza son: EMAP = 1 T t∈T E[APt] (1) VMAP = 1 T2 t∈T V ar[APt] ΔMAP = MAP1 − MAP2 La confianza puede entonces estimarse calculando la esperanza y varianza y utilizando la función de densidad normal para encontrar P(ΔMAP < 0). 2.2 Confianza y Robustez Habiendo definido la confianza, volvemos al tema de la confianza en las estimaciones de confianza, y mostramos cómo se relaciona con la robustez de la colección ante juicios faltantes. 1 Estas son en realidad aproximaciones a la verdadera esperanza y varianza, pero el error es despreciable O(n2−n ). Sea Z el conjunto de todos los pares de resultados clasificados para un conjunto común de temas. Supongamos que tenemos un conjunto de m juicios de relevancia xm = {x1, x2, ..., xm} (usando x minúscula en lugar de X mayúscula para distinguir entre documentos juzgados y no juzgados); estos son los juicios contra los cuales calculamos la confianza. Sea Zα el subconjunto de pares en Z para los cuales predecimos que ΔMAP = −1 con confianza α dadas las evaluaciones xm. Para que las estimaciones de confianza sean precisas, necesitamos que al menos α · |Zα| de estos pares realmente tengan ΔMAP = −1 después de haber evaluado cada documento. Si lo hacen, podemos confiar en las estimaciones de confianza; nuestra evaluación será robusta ante juicios faltantes. Si nuestras estimaciones de confianza se basan en suposiciones poco realistas, no podemos esperar que sean precisas. Las suposiciones en las que se basan son las probabilidades de relevancia pi. Necesitamos que esto sea realista. Sostenemos que la mejor distribución posible de relevancia p(Xi) es aquella que explica todos los datos (todas las observaciones realizadas sobre los sistemas de recuperación) al mismo tiempo que no hace suposiciones infundadas. Esto se conoce como el principio de máxima entropía [13]. La entropía de una variable aleatoria X con distribución p(X) se define como H(p) = − i p(X = i) log p(X = i). Esto ha encontrado una amplia variedad de usos en la informática y la recuperación de información. La distribución de entropía máxima es aquella que maximiza H. Esta distribución es única y tiene una forma exponencial. El siguiente teorema muestra la utilidad de una distribución de entropía máxima para la relevancia al estimar la confianza. Teorema 1. Si p(Xn |I, xm ) = argmaxpH(p), las estimaciones de confianza serán precisas, donde xm es el conjunto de juicios de relevancia definidos anteriormente, Xn es el conjunto completo de documentos cuya relevancia deseamos estimar, e I es alguna información sobre los documentos (no especificada hasta ahora). Por el momento prescindimos de la prueba, pero es bastante simple. Esto indica que cuanto mejores sean las estimaciones de relevancia, más precisa será la evaluación. La tarea de crear una colección de pruebas reutilizable se convierte entonces en la tarea de estimar la relevancia de los documentos no evaluados. El teorema y su demostración no dicen absolutamente nada sobre la métrica de evaluación. Las estimaciones de probabilidad son completamente independientes de la medida en la que estamos interesados. Esto significa que los mismos cálculos de probabilidad pueden informarnos sobre la precisión promedio, así como sobre la precisión, la recuperación, el bpref, etc. Además, podríamos asumir que la relevancia de los documentos i y j es independiente y lograr el mismo resultado, lo cual afirmamos como un corolario: Corolario 1. Si p(Xi|I, xm) = argmaxpH(p), las estimaciones de confianza serán precisas. La tarea, por lo tanto, consiste en la imputación de los valores faltantes de relevancia. El teorema implica que cuanto más nos acerquemos a la distribución de entropía máxima relevante, más nos acercaremos a la robustez. 3. PREDICIENDO LA RELEVANCIA En nuestra declaración del Teorema 1, dejamos sin especificar la naturaleza de la información I. Una de las ventajas de nuestras estimaciones de confianza es que admiten información de una amplia variedad de fuentes; básicamente cualquier cosa que pueda ser modelada puede ser utilizada como información para predecir la relevancia. Una fuente natural de información son los propios sistemas de recuperación: cómo clasificaron los documentos evaluados, con qué frecuencia no lograron clasificar documentos relevantes, cómo se desempeñan en diferentes temas, y así sucesivamente. Si tratamos cada sistema como un experto en recuperación de información que proporciona una opinión sobre la relevancia de cada documento, el problema se convierte en uno de agregación de opiniones de expertos. Esto es similar al problema de metabúsqueda o fusión de datos en el que la tarea consiste en tomar k sistemas de entrada y fusionarlos en un solo ranking. Aslam et al. [3] identificaron previamente una conexión entre la evaluación y la metabúsqueda. Nuestro problema tiene dos diferencias clave: 1. Necesitamos explícitamente probabilidades de relevancia que podamos insertar en la Ecuación 1; los algoritmos de metabusqueda no tienen tal requisito. Estamos acumulando juicios de relevancia a medida que avanzamos con la evaluación y somos capaces de reestimar la relevancia dada cada nuevo juicio. A la luz de lo anterior, introducimos un modelo probabilístico para la combinación de expertos. 3.1 Un Modelo para la Agregación de Opiniones de Expertos Supongamos que cada experto j proporciona una probabilidad de relevancia qij = pj(Xi = 1). La información sobre la relevancia del documento i será entonces el conjunto de k opiniones de expertos I = qi = (qi1, qi2, · · · , qik). La distribución de probabilidad que deseamos encontrar es aquella que maximiza la entropía de pi = p(Xi = 1|qi). Resulta que encontrar el modelo de entropía máxima es equivalente a encontrar los parámetros que maximizan la verosimilitud [5]. Blower [6] muestra explícitamente que encontrar el modelo de entropía máxima para una variable binaria es equivalente a resolver una regresión logística. Entonces pi = p(Xi = 1|qi) = exp k j=1 λjqij 1 + exp k j=1 λj qij (2) donde λ1, · · · , λk son los parámetros de regresión. Incluimos una distribución beta a priori para p(λj) con parámetros α, β. Esto se puede ver como un tipo de suavizado para tener en cuenta el hecho de que los datos de entrenamiento están altamente sesgados. Este modelo tiene la ventaja de incluir la dependencia estadística entre los expertos. Un modelo de la misma forma fue demostrado por Clemen & Winkler como el mejor para agregar probabilidades de expertos [10]. Un enfoque similar motivado por la entropía máxima se ha utilizado para la agregación de expertos [15]. Aslam & Montague [1] utilizaron un modelo similar para la metabúsqueda, pero asumieron independencia entre los expertos. ¿De dónde vienen los qij? Utilizar puntuaciones sin procesar y no calibradas como predictores no funcionará porque las distribuciones de puntuaciones varían demasiado entre los temas. Un clasificador de modelado de lenguaje, por ejemplo, suele otorgar una puntuación mucho más alta al documento recuperado en la parte superior para una consulta corta que al documento recuperado en la parte superior para una consulta larga. Podríamos entrenar un modelo predictivo separado para cada tema, pero eso no aprovecha toda la información que tenemos: es posible que solo tengamos un puñado de juicios para un tema, no suficientes para entrenar un modelo con confianza. Además, parece razonable asumir que si un experto hace buenas predicciones para un tema, también hará buenas predicciones para otros temas. Podríamos usar un modelo jerárquico [12], pero eso no se generalizará a temas no vistos. En cambio, calibraremos las puntuaciones de cada experto de forma individual para que las puntuaciones puedan ser comparadas tanto dentro de un tema como entre temas. Por lo tanto, nuestro modelo tiene en cuenta no solo la dependencia entre expertos, sino también la dependencia entre el rendimiento de los expertos en diferentes tareas (temas). 3.2 Calibración de Expertos Cada experto nos proporciona una puntuación y un rango para cada documento. Necesitamos convertir estos en probabilidades. Un método como el utilizado por Manmatha et al. [14] podría ser utilizado para convertir puntuaciones en probabilidades de relevancia. El método de preferencia por pares de Carterette & Petkova [9] también podría ser utilizado, interpretando la clasificación de un documento sobre otro como una expresión de preferencia. Que q∗ ij sea la probabilidad autoinformada por el experto j de que el documento i sea relevante. De manera intuitiva, parece claro que q∗ ij debería disminuir con el rango, y debería ser cero si el documento i no está clasificado (el experto no consideró que fuera relevante). El modelo de preferencia por pares puede manejar fácilmente estos dos requisitos, por lo que lo utilizaremos. Que θrj (i) sea el coeficiente de relevancia del documento en la posición rj(i). Queremos encontrar los θs que maximizan la función de verosimilitud: Ljt(Θ) = rj (i)<rj (k) exp(θrj (i) − θrj (k)) 1 + exp(θrj (i) − θrj (k)) Incluimos nuevamente una distribución beta previa en p(θrj(i)) con parámetros |Rt| + 1 y |Nt| + 1, el tamaño de los conjuntos de documentos relevantes y no relevantes juzgados respectivamente. El uso de estos parámetros previos garantiza que las probabilidades resultantes se concentren alrededor de la proporción de documentos relevantes que se han descubierto para el tema t. Esto significa que las estimaciones de probabilidad disminuyen por rango y son más altas para los temas que tienen más documentos relevantes. Después de encontrar el Θ que maximiza la verosimilitud, tenemos q∗ ij = exp(θrj (i)) 1+exp(θrj (i)) . Definimos θ∞ = −∞, de modo que la probabilidad de que un documento no clasificado sea relevante es 0. Dado que q∗ ij se basa en la posición en la que se recupera un documento en lugar de en la identidad del documento en sí, las probabilidades son idénticas de un experto a otro, por ejemplo, si el experto E colocó el documento A en la posición 1, y el experto D colocó el documento B en la posición 1, tendremos que q∗ AE = q∗ BD. Por lo tanto, solo tenemos que resolver esto una vez para cada tema. El modelo anterior proporciona probabilidades independientes del tema para cada documento. Pero supongamos que un experto que informa una probabilidad del 90% solo tiene razón el 50% del tiempo. Su opinión debería ser descartada basándose en su desempeño observado. Específicamente, queremos aprender una función de calibración qij = Cj(q∗ ij) que garantice que las probabilidades predichas estén ajustadas a la capacidad de los expertos para recuperar documentos relevantes dados los juicios que se han realizado hasta este momento. El método de calibración SVM de Platts [16] ajusta una función sigmoide entre q∗ ij y las evaluaciones de relevancia para obtener qij = Cj (q∗ ij) = exp(Aj +Bjq∗ ij ) 1+exp(Aj +Bj q∗ ij ) . Dado que q∗ ij es independiente del tema, solo necesitamos aprender una función de calibración para cada experto. Una vez que tenemos la función de calibración, se aplica para ajustar las predicciones de los expertos a su rendimiento real. Las probabilidades calibradas se introducen en el modelo (2) para encontrar las probabilidades de los documentos. Figura 1: Diagrama conceptual de nuestro modelo de agregación. Los expertos E1 y E2 han clasificado los documentos A, B, C para el tema T1 y los documentos D, E, F para el tema T2. El primer paso es obtener q∗ ij. A continuación se realiza la calibración al rendimiento real para encontrar qij. Finalmente obtenemos pi = p(Xi = 1|qi1, qi2), · · · . 3.3 Resumen del modelo Nuestro modelo tiene tres componentes que difieren en los datos que toman como entrada y en lo que producen como salida. Se muestra un diagrama conceptual en la Figura 1. 1. rangos → probabilidades (por sistema por tema). Esto nos da q∗ ij, la probabilidad autoinformada por el experto j de la relevancia del documento i. Esto es no supervisado; no requiere datos etiquetados (aunque si los tenemos, los usamos para establecer parámetros previos). 2. probabilidades → probabilidades calibradas (por sistema). Esto nos da qij = Cj (q∗ ij), la probabilidad calibrada por el experto j de la relevancia del documento i. Esto es semisupervisado; tenemos juicios de relevancia en algunos rangos que usamos para imputar la probabilidad de relevancia en otros rangos. 3. probabilidades calibradas → probabilidades de documentos. Esto nos da pi = p(Xi = 1|qi), la probabilidad de relevancia del documento i dada las probabilidades de experto calibradas qij. Esto es supervisado; aprendemos coeficientes de un conjunto de documentos juzgados y los utilizamos para estimar la relevancia de los documentos no juzgados. Aunque el modelo parece bastante complejo, en realidad son solo tres aplicaciones sucesivas de regresión logística. Por lo tanto, se puede implementar en un lenguaje de programación estadística como R en unas pocas líneas de código. El uso de priors beta (conjugados) garantiza que no sean necesarios métodos computacionales costosos como MCMC [12], por lo que el modelo se entrena y aplica lo suficientemente rápido como para ser utilizado en línea. Nuestro código está disponible en http://ciir.cs.umass.edu/~carteret/. 4. EXPERIMENTOS Se están considerando tres hipótesis. El primero, y más importante, es que el uso de nuestro modelo de agregación de expertos para predecir la relevancia produce colecciones de pruebas lo suficientemente robustas como para ser reutilizables; es decir, podemos confiar en las estimaciones de confianza cuando evaluamos sistemas que no contribuyeron con ningún juicio al conjunto. Las otras dos hipótesis se relacionan con la mejora que observamos al utilizar estimaciones de relevancia mejores que las que utilizamos en nuestro trabajo anterior [8]. Estos son (a) que se necesitan menos temas de relevancia, número de ejecuciones, número de documentos evaluados y número de documentos relevantes para cada uno de nuestros conjuntos de datos y (b) la precisión de las predicciones es mayor que si simplemente asumiéramos que pi = 0.5 para todos los documentos no evaluados. 4.1 Datos Obtuvimos ejecuciones completas de búsqueda ad-hoc presentadas en las TRECs 3 a 8. Cada ejecución clasifica como máximo 1000 documentos para 50 temas (49 temas para TREC 4). Además, obtuvimos todas las ejecuciones de la pista Web de TREC 13, la pista Robust2 de TREC 14 y la pista Terabyte (ad-hoc) de TREC 14. Estas son las pistas que han reemplazado la pista ad-hoc desde su final en 1999. Las estadísticas se muestran en la Tabla 1. Reservamos el conjunto TREC 4 (ad-hoc 95) para entrenamiento, los TRECs 3 y 5-8 (ad-hoc 94 y 96-99) para pruebas principales, y los conjuntos restantes para pruebas adicionales. Utilizamos los archivos qrels recopilados por NIST como verdad. El número de juicios de relevancia realizados y documentos relevantes encontrados para cada pista se enumeran en la Tabla 1. Por razones computacionales, truncamos las listas clasificadas en 100 documentos. No hay razón por la que no pudiéramos profundizar más, pero el cálculo de la varianza es O(n3) y, por lo tanto, muy consumidor de tiempo. Debido a la naturaleza del rango recíproco de AP, no perdemos mucha información al truncar en el rango 100. 4.2 Algoritmos Compararemos tres algoritmos para adquirir juicios de relevancia. La línea base es una variación del agrupamiento de TREC que llamaremos agrupamiento incremental (IP). Este algoritmo toma un número k como entrada y presenta los primeros k documentos en orden de rango (sin tener en cuenta el tema) para ser evaluados. No estima la relevancia de los documentos no evaluados; simplemente asume que cualquier documento no evaluado no es relevante. El segundo algoritmo es el presentado en Carterette et al. [8] (Algoritmo 1). Los documentos se seleccionan en función de lo interesantes que sean para determinar si existe una diferencia en la precisión media promedio. Para este enfoque pi = 0.5 para todos los i; no hay estimación de probabilidades. Llamaremos a esto MTC por colección de pruebas mínimas. El tercer algoritmo aumenta MTC con estimaciones actualizadas de las probabilidades de relevancia. Llamaremos a esto RTC por <br>colección de pruebas robusta</br>. Es idéntico al Algoritmo 1, excepto que cada décima iteración estimamos pi para todos los documentos no evaluados i utilizando el modelo de agregación de expertos de la Sección 3. RTC tiene <br>parámetros de suavizado</br> (distribución previa) que deben ser configurados. Entrenamos utilizando el conjunto ad-hoc 95. Limitamos 2 robustos aquí significa recuperación robusta; esto es diferente de nuestro objetivo de evaluación robusta. Algoritmo 1 (MTC) Dadas dos listas clasificadas y un nivel de confianza α, predecir el signo de ΔMAP. 1: pi ← 0.5 para todos los documentos i 2: mientras P(ΔMAP < 0) < α hacer 3: calcular el peso wi para todos los documentos i no clasificados (ver Carterette et al. [8] para más detalles) 4: j ← argmaxiwi 5: xj ← 1 si el documento j es relevante, 0 en caso contrario 6: pj ← xj 7: fin mientras la búsqueda de priors uniformes con una varianza relativamente alta. Para la agregación de expertos, los parámetros previos son α = β = 1. 4.3 Diseño Experimental Primero, queremos saber si podemos aumentar un conjunto de juicios de relevancia con un conjunto de probabilidades de relevancia para reutilizar los juicios y evaluar un nuevo conjunto de sistemas. Para cada prueba experimental: 1. Selecciona un subconjunto aleatorio de k ejecuciones. De esos k, elige un c inicial < k para evaluar. 3. Ejecutar RTC al 95% de confianza en el c inicial de aproximadamente 4. Utilizando el modelo de la Sección 3, estime las probabilidades de relevancia para todos los documentos recuperados por todos los k experimentos. Calcular EMAP para todas las ejecuciones k, y P(ΔMAP < 0) para todos los pares de ejecuciones. Hacemos lo mismo para MTC, pero omitimos el paso 4. Ten en cuenta que después de evaluar los primeros c sistemas, no realizamos juicios de relevancia adicionales. Para poner a prueba nuestro método, seleccionamos c = 2: construiremos un conjunto de juicios a partir de evaluar solo dos sistemas iniciales. Luego generalizaremos a un conjunto de k = 10 (de los cuales esos dos son un subconjunto). A medida que realizamos más pruebas, obtenemos los datos que necesitamos para probar las tres hipótesis. 4.4 Evaluación Experimental Recuerde que un conjunto de juicios es robusto si la precisión de las predicciones que realiza es al menos su confianza estimada. Una forma de evaluar la robustez es agrupar pares por su confianza, luego calcular la precisión sobre todos los pares en cada grupo. Nos gustaría que la precisión no fuera menor que la puntuación de confianza más baja en el contenedor, pero preferiblemente más alta. Dado que las estadísticas resumen son útiles, ideamos la siguiente métrica. Supongamos que somos una casa de apuestas tomando apuestas sobre si ΔMAP < 0. Utilizamos RTC o MTC para establecer las probabilidades O = P (ΔMAP <0) 1−P (ΔMAP <0) . Supongamos que un apostador apuesta $1 a que ΔMAP ≥ 0. Si resulta que ΔMAP < 0, ganamos el dólar. De lo contrario, pagamos O. Si nuestras estimaciones de confianza son perfectamente precisas, empatamos. Si la confianza es mayor que la precisión, perdemos dinero; ganamos si la precisión es mayor que la confianza. Contra intuitivamente, el resultado más deseable es empatar: si perdemos dinero, no podemos confiar en las estimaciones de confianza, pero si ganamos dinero, hemos subestimado la confianza o evaluado más documentos de los necesarios. Sin embargo, el costo de no poder confiar en las estimaciones de confianza es mayor que el costo de realizar juicios de relevancia adicionales, por lo que consideraremos los resultados positivos como buenos. La cantidad que ganamos en cada comparación par a par i es: Wi = yi − (1 − yi) Pi 1 − Pi = yi − Pi 1 − Pi yi = 1 si ΔMAP < 0 y 0 en caso contrario, y Pi = P(ΔMAP < 0). La estadística resumen es W, la media de Wi. Ten en cuenta que a medida que Pi aumenta, perdemos más por estar equivocados. Esto es como debería ser: la penalización debería ser grande por fallar en las predicciones de alta probabilidad. Sin embargo, dado que nuestras pérdidas crecen sin límite a medida que las predicciones se acercan a la certeza, limitamos −Wi a 100. Para nuestra hipótesis de que RTC requiere menos juicios que MTC, estamos interesados en el número de juicios necesarios para alcanzar el 95% de confianza en el primer par de sistemas. ",
            "candidates": [],
            "error": [
                [
                    "colección de pruebas robusta",
                    "parámetros de suavizado"
                ]
            ]
        },
        "expectation": {
            "translated_key": "esperanza",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Robust Test Collections for Retrieval Evaluation Ben Carterette Center for Intelligent Information Retrieval Computer Science Department University of Massachusetts Amherst Amherst, MA 01003 carteret@cs.umass.edu ABSTRACT Low-cost methods for acquiring relevance judgments can be a boon to researchers who need to evaluate new retrieval tasks or topics but do not have the resources to make thousands of judgments.",
                "While these judgments are very useful for a one-time evaluation, it is not clear that they can be trusted when re-used to evaluate new systems.",
                "In this work, we formally define what it means for judgments to be reusable: the confidence in an evaluation of new systems can be accurately assessed from an existing set of relevance judgments.",
                "We then present a method for augmenting a set of relevance judgments with relevance estimates that require no additional assessor effort.",
                "Using this method practically guarantees reusability: with as few as five judgments per topic taken from only two systems, we can reliably evaluate a larger set of ten systems.",
                "Even the smallest sets of judgments can be useful for evaluation of new systems.",
                "Categories and Subject Descriptors: H.3 Information Storage and Retrieval; H.3.4 Systems and Software: Performance Evaluation General Terms: Experimentation, Measurement, Reliability 1.",
                "INTRODUCTION Consider an information retrieval researcher who has invented a new retrieval task.",
                "She has built a system to perform the task and wants to evaluate it.",
                "Since the task is new, it is unlikely that there are any extant relevance judgments.",
                "She does not have the time or resources to judge every document, or even every retrieved document.",
                "She can only judge the documents that seem to be the most informative and stop when she has a reasonable degree of confidence in her conclusions.",
                "But what happens when she develops a new system and needs to evaluate it?",
                "Or another research group decides to implement a system to perform the task?",
                "Can they reliably reuse the original judgments?",
                "Can they evaluate without more relevance judgments?",
                "Evaluation is an important aspect of information retrieval research, but it is only a semi-solved problem: for most retrieval tasks, it is impossible to judge the relevance of every document; there are simply too many of them.",
                "The solution used by NIST at TREC (Text REtrieval Conference) is the pooling method [19, 20]: all competing systems contribute N documents to a pool, and every document in that pool is judged.",
                "This method creates large sets of judgments that are reusable for training or evaluating new systems that did not contribute to the pool [21].",
                "This solution is not adequate for our hypothetical researcher.",
                "The pooling method gives thousands of relevance judgments, but it requires many hours of (paid) annotator time.",
                "As a result, there have been a slew of recent papers on reducing annotator effort in producing test collections: Cormack et al. [11], Zobel [21], Sanderson and Joho [17], Carterette et al. [8], and Aslam et al. [4], among others.",
                "As we will see, the judgments these methods produce can significantly bias the evaluation of a new set of systems.",
                "Returning to our hypothetical resesarcher, can she reuse her relevance judgments?",
                "First we must formally define what it means to be reusable.",
                "In previous work, reusability has been tested by simply assessing the accuracy of a set of relevance judgments at evaluating unseen systems.",
                "While we can say that it was right 75% of the time, or that it had a rank correlation of 0.8, these numbers do not have any predictive power: they do not tell us which systems are likely to be wrong or how confident we should be in any one.",
                "We need a more careful definition of reusability.",
                "Specifically, the question of reusability is not how accurately we can evaluate new systems.",
                "A malicious adversary can always produce a new ranked list that has not retrieved any of the judged documents.",
                "The real question is how much confidence we have in our evaluations, and, more importantly, whether we can trust our estimates of confidence.",
                "Even if confidence is not high, as long as we can trust it, we can identify which systems need more judgments in order to increase confidence.",
                "Any set of judgments, no matter how small, becomes reusable to some degree.",
                "Small, reusable test collections could have a huge impact on information retrieval research.",
                "Research groups would be able to share the relevance judgments they have done in-house for pilot studies, new tasks, or new topics.",
                "The amount of data available to researchers would grow exponentially over time. 2.",
                "ROBUST EVALUATION Above we gave an intuitive definition of reusability: a collection is reusable if we can trust our estimates of confidence in an evaluation.",
                "By that we mean that if we have made some relevance judgments and have, for example, 75% confidence that system A is better than system B, we would like there to be no more than 25% chance that our assessment of the relative quality of the systems will change as we continue to judge documents.",
                "Our evaluation should be robust to missing judgments.",
                "In our previous work, we defined confidence as the probability that the difference in an evaluation measure calculated for two systems is less than zero [8].",
                "This notion of confidence is defined in the context of a particular evaluation task that we call comparative evaluation: determining the sign of the difference in an evaluation measure.",
                "Other evaluation tasks could be defined; estimating the magnitude of the difference or the values of the measures themselves are examples that entail different notions of confidence.",
                "We therefore see confidence as a probability estimate.",
                "One of the questions we must ask about a probability estimate is what it means.",
                "What does it mean to have 75% confidence that system A is better than system B?",
                "As described above, we want it to mean that if we continue to judge documents, there will only be a 25% chance that our assessment will change.",
                "If this is what it means, we can trust the confidence estimates.",
                "But do we know it has that meaning?",
                "Our calculation of confidence rested on an assumption about the probability of relevance of unjudged documents, specifically that each unjudged document was equally likely to be relevant or nonrelevant.",
                "This assumption is almost certainly not realistic in most IR applications.",
                "As it turns out, it is this assumption that determines whether the confidence estimates can eb trusted.",
                "Before elaborating on this, we formally define confidence. 2.1 Estimating Confidence Average precision (AP) is a standard evaluation metric that captures both the ability of a system to rank relevant documents highly (precision) as well as its ability to retrieve relevant documents (recall).",
                "It is typically written as the mean precision at the ranks of relevant documents: AP = 1 |R| i∈R prec@r(i) where R is the set of relevant documents and r(i) is the rank of document i.",
                "Let Xi be a random variable indicating the relevance of document i.",
                "If documents are ordered by rank, we can express precision as prec@i = 1/i i j=1 Xj .",
                "Average precision then becomes the quadratic equation AP = 1 Xi n i=1 Xi/i i j=1 Xj = 1 Xi n i=1 j≥i aijXiXj where aij = 1/ max{r(i), r(j)}.",
                "Using aij instead of 1/i allows us to number the documents arbitrarily.",
                "To see why this is true, consider a toy example: a list of 3 documents with relevant documents B, C at ranks 1 and 3 and nonrelevant document A at rank 2.",
                "Average precision will be 1 2 (1 1 x2 B+ 1 2 xBxA+ 1 3 xBxC + 1 2 x2 A+ 1 3 xAxC + 1 3 x2 C) = 1 2 1 + 2 3 because xA = 0, xB = 1, xC = 1.",
                "Though the ordering B, A, C is different from the labeling A, B, C, it does not affect the computation.",
                "We can now see average precision itself is a random variable with a distribution over all possible assignments of relevance to all documents.",
                "This random variable has an <br>expectation</br>, a variance, confidence intervals, and a certain probability of being less than or equal to a given value.",
                "All of these are dependent on the probability that document i is relevant: pi = p(Xi = 1).",
                "Suppose in our previous example we do not know the relevance judgments, but we believe pA = 0.4, pB = 0.8, pC = 0.7.",
                "We can then compute e.g.",
                "P(AP = 0) = 0.2 · 0.6 · 0.3 = 0.036, or P(AP = 1 2 ) = 0.2 · 0.4 · 0.7 = 0.056.",
                "Summing over all possibilities, we can compute <br>expectation</br> and variance: E[AP] ≈ 1 pi aiipi + j>i aij pipj V ar[AP] ≈ 1 ( pi)2 n i a2 iipiqi + j>i a2 ijpipj(1 − pipj) + i=j 2aiiaijpipj(1 − pi) + k>j=i 2aijaikpipjpk(1 − pi) AP asymptotically converges to a normal distribution with <br>expectation</br> and variance as defined above.1 For our comparative evaluation task we are interested in the sign of the difference in two average precisions: ΔAP = AP1 − AP2.",
                "As we showed in our previous work, ΔAP has a closed form when documents are ordered arbitrarily: ΔAP = 1 Xi n i=1 j≥i cij XiXj cij = aij − bij where bij is defined analogously to aij for the second ranking.",
                "Since AP is normal, ΔAP is normal as well, meaning we can use the normal cumulative density function to determine the confidence that a difference in AP is less than zero.",
                "Since topics are independent, we can easily extend this to mean average precision (MAP).",
                "MAP is also normally distributed; its <br>expectation</br> and variance are: EMAP = 1 T t∈T E[APt] (1) VMAP = 1 T2 t∈T V ar[APt] ΔMAP = MAP1 − MAP2 Confidence can then be estimated by calculating the <br>expectation</br> and variance and using the normal density function to find P(ΔMAP < 0). 2.2 Confidence and Robustness Having defined confidence, we turn back to the issue of trust in confidence estimates, and show how it ties into the robustness of the collection to missing judgments. 1 These are actually approximations to the true expectation and variance, but the error is a negligible O(n2−n ).",
                "Let Z be the set of all pairs of ranked results for a common set of topics.",
                "Suppose we have a set of m relevance judgments xm = {x1, x2, ..., xm} (using small x rather than capital X to distinguish between judged and unjudged documents); these are the judgments against which we compute confidence.",
                "Let Zα be the subset of pairs in Z for which we predict that ΔMAP = −1 with confidence α given the judgments xm .",
                "For the confidence estimates to be accurate, we need at least α · |Zα| of these pairs to actually have ΔMAP = −1 after we have judged every document.",
                "If they do, we can trust the confidence estimates; our evaluation will be robust to missing judgments.",
                "If our confidence estimates are based on unrealistic assumptions, we cannot expect them to be accurate.",
                "The assumptions they are based on are the probabilities of relevance pi.",
                "We need these to be realistic.",
                "We argue that the best possible distribution of relevance p(Xi) is the one that explains all of the data (all of the observations made about the retrieval systems) while at the same time making no unwarranted assumptions.",
                "This is known as the principle of maximum entropy [13].",
                "The entropy of a random variable X with distribution p(X) is defined as H(p) = − i p(X = i) log p(X = i).",
                "This has found a wide array of uses in computer science and information retrieval.",
                "The maximum entropy distribution is the one that maximizes H. This distribution is unique and has an exponential form.",
                "The following theorem shows the utility of a maximum entropy distribution for relevance when estimating confidence.",
                "Theorem 1.",
                "If p(Xn |I, xm ) = argmaxpH(p), confidence estimates will be accurate. where xm is the set of relevance judgments defined above, Xn is the full set of documents that we wish to estimate the relevance of, and I is some information about the documents (unspecified as of now).",
                "We forgo the proof for the time being, but it is quite simple.",
                "This says that the better the estimates of relevance, the more accurate the evaluation.",
                "The task of creating a reusable test collection thus becomes the task of estimating the relevance of unjudged documents.",
                "The theorem and its proof say nothing whatsoever about the evaluation metric.",
                "The probability estimates are entirely indepedent of the measure we are interested in.",
                "This means the same probability estimates can tell us about average precision as well as precision, recall, bpref, etc.",
                "Furthermore, we could assume that the relevance of documents i and j is independent and achieve the same result, which we state as a corollary: Corollary 1.",
                "If p(Xi|I, xm ) = argmaxpH(p), confidence estimates will be accurate.",
                "The task therefore becomes the imputation of the missing values of relevance.",
                "The theorem implies that the closer we get to the maximum entropy distribution of relevance, the closer we get to robustness. 3.",
                "PREDICTING RELEVANCE In our statement of Theorem 1, we left the nature of the information I unspecified.",
                "One of the advantages of our confidence estimates is that they admit information from a wide variety of sources; essentially anything that can be modeled can be used as information for predicting relevance.",
                "A natural source of information is the retrieval systems themselves: how they ranked the judged documents, how often they failed to rank relevant documents, how they perform across topics, and so on.",
                "If we treat each system as an information retrieval expert providing an opinion about the relevance of each document, the problem becomes one of expert opinion aggregation.",
                "This is similar to the metasearch or data fusion problem in which the task is to take k input systems and merge them into a single ranking.",
                "Aslam et al. [3] previously identified a connection between evaluation and metasearch.",
                "Our problem has two key differences: 1.",
                "We explicitly need probabilities of relevance that we can plug into Eq. 1; metasearch algorithms have no such requirement. 2.",
                "We are accumulating relevance judgments as we proceed with the evaluation and are able to re-estimate relevance given each new judgment.",
                "In light of (1) above, we introduce a probabilistic model for expert combination. 3.1 A Model for Expert Opinion Aggregation Suppose that each expert j provides a probability of relevance qij = pj(Xi = 1).",
                "The information about the relevance of document i will then be the set of k expert opinions I = qi = (qi1, qi2, · · · , qik).",
                "The probability distribution we wish to find is the one that maximizes the entropy of pi = p(Xi = 1|qi).",
                "As it turns out, finding the maximum entropy model is equivalent to finding the parameters that maximize the likelihood [5].",
                "Blower [6] explicitly shows that finding the maximum entropy model for a binary variable is equivalent to solving a logistic regression.",
                "Then pi = p(Xi = 1|qi) = exp k j=1 λjqij 1 + exp k j=1 λj qij (2) where λ1, · · · , λk are the regression parameters.",
                "We include a beta prior for p(λj) with parameters α, β.",
                "This can be seen as a type of smoothing to account for the fact that the training data is highly biased.",
                "This model has the advantage of including the statistical dependence between the experts.",
                "A model of the same form was shown by Clemen & Winkler to be the best for aggregating expert probabilities [10].",
                "A similar maximumentropy-motivated approach has been used for expert aggregation [15].",
                "Aslam & Montague [1] used a similar model for metasearch, but assumed independence among experts.",
                "Where do the qij s come from?",
                "Using raw, uncalibrated scores as predictors will not work because score distributions vary too much between topics.",
                "A language modeling ranker, for instance, will typically give a much higher score to the top retrieved document for a short query than to the top retrieved document for a long query.",
                "We could train a separate predicting model for each topic, but that does not take advantage of all of the information we have: we may only have a handful of judgments for a topic, not enough to train a model to any confidence.",
                "Furthermore, it seems reasonable to assume that if an expert makes good predictions for one topic, it will make good predictions for other topics as well.",
                "We could use a hierarchical model [12], but that will not generalize to unseen topics.",
                "Instead, we will calibrate the scores of each expert individually so that scores can be compared both within topic and between topic.",
                "Thus our model takes into account not only the dependence between experts, but also the dependence between experts performances on different tasks (topics). 3.2 Calibrating Experts Each expert gives us a score and a rank for each document.",
                "We need to convert these to probabilities.",
                "A method such as the one used by Manmatha et al. [14] could be used to convert scores into probabilities of relevance.",
                "The pairwise preference method of Carterette & Petkova [9] could also be used, interpeting the ranking of one document over another as an expression of preference.",
                "Let q∗ ij be expert js self-reported probability that document i is relevant.",
                "Intuitively it seems clear that q∗ ij should decrease with rank, and it should be zero if document i is unranked (the expert did not believe it to be relevant).",
                "The pairwise preference model can handle these two requirements easily, so we will use it.",
                "Let θrj (i) be the relevance coefficient of the document at rank rj(i).",
                "We want to find the θs that maximize the likelihood function: Ljt(Θ) = rj (i)<rj (k) exp(θrj (i) − θrj (k)) 1 + exp(θrj (i) − θrj (k)) We again include a beta prior on p(θrj(i)) with parameters |Rt| + 1 and |Nt| + 1, the size of the sets of judged relevant and nonrelevant documents respectively.",
                "Using these as prior parameters ensures that the resulting probabilities will be concentrated around the ratio of relevant documents that have been discovered for topic t. This means that the probability estimates decrease by rank and are higher for topics that have more relevant documents.",
                "After finding the Θ that maximizes the likelihood, we have q∗ ij = exp(θrj (i)) 1+exp(θrj (i)) .",
                "We define θ∞ = −∞, so that the probability that an unranked document is relevant is 0.",
                "Since q∗ ij is based on the rank at which a document is retrieved rather than the identity of the document itself, the probabilities are identical from expert to expert, e.g. if expert E put document A at rank 1, and expert D put document B at rank 1, we will have q∗ AE = q∗ BD.",
                "Therefore we only have to solve this once for each topic.",
                "The above model gives topic-independent probabilities for each document.",
                "But suppose an expert who reports 90% probability is only right 50% of the time.",
                "Its opinion should be discounted based on its observed performance.",
                "Specifically, we want to learn a calibration function qij = Cj(q∗ ij) that will ensure that the predicted probabilities are tuned to the experts ability to retrieve relevant documents given the judgments that have been made to this point.",
                "Platts SVM calibration method [16] fits a sigmoid function between q∗ ij and the relevance judgments to obtain qij = Cj (q∗ ij) = exp(Aj +Bjq∗ ij ) 1+exp(Aj +Bj q∗ ij ) .",
                "Since q∗ ij is topic-independent, we only need to learn one calibration function for each expert.",
                "Once we have the calibration function, it is applied to adjust the experts predictions to their actual performance.",
                "The calibrated probabilities are plugged into model (2) to find the document probabilities.",
                "Figure 1: Conceptual diagram of our aggregation model.",
                "Experts E1, E2 have ranked documents A, B, C for topic T1 and documents D, E, F for topic T2.",
                "The first step is to obtain q∗ ij.",
                "Next is calibration to true performance to find qij .",
                "Finally we obtain pi = p(Xi = 1|qi1, qi2), · · · . 3.3 Model Summary Our model has three components that differ by the data they take as input and what they produce as output.",
                "A conceptual diagram is shown in Figure 1. 1. ranks → probabilities (per system per topic).",
                "This gives us q∗ ij, expert js self-reported probability of the relevance of document i.",
                "This is unsupervised; it requires no labeled data (though if we have some, we use it to set prior parameters). 2. probabilities → calibrated probabilities (per system).",
                "This gives us qij = Cj (q∗ ij), expert js calibrated probability of the relevance of document i.",
                "This is semisupervised; we have relevance judgments at some ranks which we use to impute the probability of relevance at other ranks. 3. calibrated probabilities → document probabilities.",
                "This gives us pi = p(Xi = 1|qi), the probability of relevance of document i given calibrated expert probabilities qij .",
                "This is supervised; we learn coefficients from a set of judged documents and use those to estimate the relevance of the unjudged documents.",
                "Although the model appears rather complex, it is really just three successive applications of logistic regression.",
                "As such, it can be implemented in a statistical programming language such as R in a few lines of code.",
                "The use of beta (conjugate) priors ensures that no expensive computational methods such as MCMC are necessary [12], so the model is trained and applied fast enough to be used on-line.",
                "Our code is available at http://ciir.cs.umass.edu/~carteret/. 4.",
                "EXPERIMENTS Three hypotheses are under consideration.",
                "The first, and most important, is that using our expert aggregation model to predict relevance produces test collections that are robust enough to be reusable; that is, we can trust the estimates of confidence when we evaluate systems that did not contribute any judgments to the pool.",
                "The other two hypotheses relate to the improvement we see by using better estimates of relevance than we did in our previous work [8].",
                "These are that (a) it takes fewer relevance track no. topics no. runs no. judged no. rel ad-hoc 94 50 40 97,319 9,805 ad-hoc 95 49 33 87,069 6,503 ad-hoc 96 50 61 133,681 5,524 ad-hoc 97 50 74 72,270 4,611 ad-hoc 98 50 103 80,345 4,674 ad-hoc 99 50 129 86,830 4,728 web 04 225 74 88,566 1,763 robust 05 50 74 37,798 6,561 terabyte 05 50 58 45,291 10,407 Table 1: Number of topics, number of runs, number of documents judged, and number found relevant for each of our data sets. judgments to reach 95% confidence and (b) the accuracy of the predictions is higher than if we were to simply assume pi = 0.5 for all unjudged documents. 4.1 Data We obtained full ad-hoc runs submitted to TRECs 3 through 8.",
                "Each run ranks at most 1000 documents for 50 topics (49 topics for TREC 4).",
                "Additionally, we obtained all runs from the Web track of TREC 13, the Robust2 track of TREC 14, and the Terabyte (ad-hoc) track of TREC 14.",
                "These are the tracks that have replaced the ad-hoc track since its end in 1999.",
                "Statistics are shown in Table 1.",
                "We set aside the TREC 4 (ad-hoc 95) set for training, TRECs 3 and 5-8 (ad-hoc 94 and 96-99) for primary testing, and the remaining sets for additional testing.",
                "We use the qrels files assembled by NIST as truth.",
                "The number of relevance judgments made and relevant documents found for each track are listed in Table 1.",
                "For computational reasons, we truncate ranked lists at 100 documents.",
                "There is no reason that we could not go deeper, but calculating variance is O(n3 ) and thus very timeconsuming.",
                "Because of the reciprocal rank nature of AP, we do not lose much information by truncating at rank 100. 4.2 Algorithms We will compare three algorithms for acquiring relevance judgments.",
                "The baseline is a variation of TREC pooling that we will call incremental pooling (IP).",
                "This algorithm takes a number k as input and presents the first k documents in rank order (without regard to topic) to be judged.",
                "It does not estimate the relevance of unjudged documents; it simply assumes any unjudged document is nonrelevant.",
                "The second algorithm is that presented in Carterette et al. [8] (Algorithm 1).",
                "Documents are selected based on how interesting they are in determining whether a difference in mean average precision exists.",
                "For this approach pi = 0.5 for all i; there is no estimation of probabilities.",
                "We will call this MTC for minimal test collection.",
                "The third algorithm augments MTC with updated estimates of probabilities of relevance.",
                "We will call this RTC for robust test collection.",
                "It is identical to Algorithm 1, except that every 10th iteration we estimate pi for all unjudged documents i using the expert aggregation model of Section 3.",
                "RTC has smoothing (prior distribution) parameters that must be set.",
                "We trained using the ad-hoc 95 set.",
                "We limited 2 Robust here means robust retrieval; this is different from our goal of robust evaluation.",
                "Algorithm 1 (MTC) Given two ranked lists and confidence level α, predict the sign of ΔMAP. 1: pi ← 0.5 for all documents i 2: while P(ΔMAP < 0) < α do 3: calculate weight wi for all unjudged documents i (see Carterette et al. [8] for details) 4: j ← argmaxiwi 5: xj ← 1 if document j is relevant, 0 otherwise 6: pj ← xj 7: end while the search to uniform priors with relatively high variance.",
                "For expert aggregation, the prior parameters are α = β = 1. 4.3 Experimental Design First, we want to know whether we can augment a set of relevance judgments with a set of relevance probabilities in order to reuse the judgments to evaluate a new set of systems.",
                "For each experimental trial: 1.",
                "Pick a random subset of k runs. 2.",
                "From those k, pick an initial c < k to evaluate. 3.",
                "Run RTC to 95% confidence on the initial c. 4.",
                "Using the model from Section 3, estimate the probabilities of relevance for all documents retrieved by all k runs. 5.",
                "Calculate EMAP for all k runs, and P(ΔMAP < 0) for all pairs of runs.",
                "We do the same for MTC, but omit step 4.",
                "Note that after evaluating the first c systems, we make no additional relevance judgments.",
                "To put our method to the test, we selected c = 2: we will build a set of judgments from evaluating only two initial systems.",
                "We will then generalize to a set of k = 10 (of which those two are a subset).",
                "As we run more trials, we obtain the data we need to test all three of our hypotheses. 4.4 Experimental Evaluation Recall that a set of judgments is robust if the accuracy of the predictions it makes is at least its estimated confidence.",
                "One way to evaluate robustness is to bin pairs by their confidence, then calculate the accuracy over all the pairs in each bin.",
                "We would like the accuracy to be no less than the lowest confidence score in the bin, but preferably higher.",
                "Since summary statistics are useful, we devised the following metric.",
                "Suppose we are a bookmaker taking bets on whether ΔMAP < 0.",
                "We use RTC or MTC to set the odds O = P (ΔMAP <0) 1−P (ΔMAP <0) .",
                "Suppose a bettor wagers $1 on ΔMAP ≥ 0.",
                "If it turns out that ΔMAP < 0, we win the dollar.",
                "Otherwise, we pay out O.",
                "If our confidence estimates are perfectly accurate, we break even.",
                "If confidence is greater than accuracy, we lose money; we win if accuracy is greater than confidence.",
                "Counterintuitively, the most desirable outcome is breaking even: if we lose money, we cannot trust the confidence estimates, but if we win money, we have either underestimated confidence or judged more documents than necessary.",
                "However, the cost of not being able to trust the confidence estimates is higher than the cost of extra relevance judgments, so we will treat positive outcomes as good.",
                "The amount we win on each pairwise comparison i is: Wi = yi − (1 − yi) Pi 1 − Pi = yi − Pi 1 − Pi yi = 1 if ΔMAP < 0 and 0 otherwise, and Pi = P(ΔMAP < 0).",
                "The summary statistic is W, the mean of Wi.",
                "Note that as Pi increases, we lose more for being wrong.",
                "This is as it should be: the penalty should be great for missing the high probability predictions.",
                "However, since our losses grow without bound as predictions approach certainty, we cap −Wi at 100.",
                "For our hypothesis that RTC requires fewer judgments than MTC, we are interested in the number of judgments needed to reach 95% confidence on the first pair of systems.",
                "The median is more interesting than the mean: most pairs require a few hundred judgments, but a few pairs require several thousand.",
                "The distribution is therefore highly skewed, and the mean strongly affected by those outliers.",
                "Finally, for our hypothesis that RTC is more accurate than MTC, we will look at Kendalls τ correlation between a ranking of k systems by a small set of judgments and the true ranking using the full set of judgments.",
                "Kendalls τ, a nonparametric statistic based on pairwise swaps between two lists, is a standard evaluation for this type of study.",
                "It ranges from −1 (perfectly anti-correlated) to 1 (rankings identical), with 0 meaning that half of the pairs are swapped.",
                "As we touched on in the introduction, though, an accuracy measure like rank correlation is not a good evaluation of reusability.",
                "We include it for completeness. 4.4.1 Hypothesis Testing Running multiple trials allows the use of statistical hypothesis testing to compare algorithms.",
                "Using the same sets of systems allows the use of paired tests.",
                "As we stated above, we are more interested in the median number of judgments than the mean.",
                "A test for difference in median is the Wilcoxon sign rank test.",
                "We can also use a paired t-test to test for a difference in mean.",
                "For rank correlation, we can use a paired t-test to test for a difference in τ. 5.",
                "RESULTS AND ANALYSIS The comparison between MTC and RTC is shown in Table 2.",
                "With MTC and uniform probabilities of relevance, the results are far from robust.",
                "We cannot reuse the relevance judgments with much confidence.",
                "But with RTC, the results are very robust.",
                "There is a slight dip in accuracy when confidence gets above 0.95; nonetheless, the confidence predictions are trustworthy.",
                "Mean Wi shows that RTC is much closer to 0 than MTC.",
                "The distribution of confidence scores shows that at least 80% confidence is achieved more than 35% of the time, indicating that neither algorithm is being too conservative in its confidence estimates.",
                "The confidence estimates are rather low overall; that is because we have built a test collection from only two initial systems.",
                "Recall from Section 1 that we cannot require (or even expect) a minimum level of confidence when we generalize to new systems.",
                "More detailed results for both algorithms are shown in Figure 2.",
                "The solid line is the ideal result that would give W = 0.",
                "RTC is on or above this line at all points until confidence reaches about 0.97.",
                "After that there is a slight dip in accuracy which we discuss below.",
                "Note that both MTC RTC confidence % in bin accuracy % in bin accuracy 0.5 − 0.6 33.7% 61.7% 28.6% 61.9% 0.6 − 0.7 18.1% 73.1% 20.1% 76.3% 0.7 − 0.8 10.4% 70.1% 15.5% 78.0% 0.8 − 0.9 9.4% 69.0% 12.1% 84.9% 0.9 − 0.95 7.3% 78.0% 6.6% 93.1% 0.95 − 0.99 17.9% 70.4% 12.4% 93.4% 1.0 3.3% 68.3% 4.7% 98.9% W −5.34 −0.39 median judged 251 235 mean τ 0.393 0.555 Table 2: Confidence that P(ΔMAP < 0) and accuracy of prediction when generalizing a set of relevance judgments acquired using MTC and RTC.",
                "Each bin contains over 1,000 trials from the adhoc 3, 5-8 sets.",
                "RTC is much more robust than MTC.",
                "W is defined in Section 4.4; closer to 0 is better.",
                "Median judged is the number of judgments to reach 95% confidence on the first two systems.",
                "Mean τ is the average rank correlation for all 10 systems. 0.5 0.6 0.7 0.8 0.9 1 0.5 0.6 0.7 0.8 0.9 1 accuracy confidence breakeven RTC MTC Figure 2: Confidence vs. accuracy of MTC and RTC.",
                "The solid line is the perfect result that would give W = 0; performance should be on or above this line.",
                "Each point represents at least 500 pairwise comparisons. algorithms are well above the line up to around confidence 0.7.",
                "This is because the baseline performance on these data sets is high; it is quite easy to achieve 75% accuracy doing very little work [7].",
                "Number of Judgments: The median number of judgments required by MTC to reach 95% confidence on the first two systems is 251, an average of 5 per topic.",
                "The median required by RTC is 235, about 4.7 per topic.",
                "Although the numbers are close, RTCs median is significantly lower by a paired Wilcoxon test (p < 0.0001).",
                "For comparison, a pool of depth 100 would result in a minimum of 5,000 judgments for each pair.",
                "The difference in means is much greater.",
                "MTC required a mean of 823 judgments, 16 per topic, while RTC required a mean of 502, 10 per topic. (Recall that means are strongly skewed by a few pairs that take thousands of judgments.)",
                "This difference is significant by a paired t-test (p < 0.0001).",
                "Ten percent of the sets resulted in 100 or fewer judgments (less than two per topic).",
                "Performance on these is very high: W = 0.41, and 99.7% accuracy when confidence is at least 0.9.",
                "This shows that even tiny collections can be reusable.",
                "For the 50% of sets with more than 235 judgments, accuracy is 93% when confidence is at least 0.9.",
                "Rank Correlation: MTC and RTC both rank the 10 systems by EMAP (Eq. (1)) calculated using their respective probability estimates.",
                "The mean τ rank correlation between true MAP and EMAP is 0.393 for MTC and 0.555 for RTC.",
                "This difference is significant by a paired t-test (p < 0.0001).",
                "Note that we do not expect the τ correlations to be high, since we are ranking the systems with so few relevance judgments.",
                "It is more important that we estimate confidence in each pairwise comparison correctly.",
                "We ran IP for the same number of judgments that MTC took for each pair, then ranked the systems by MAP using only those judgments (all unjudged documents assumed nonrelevant).",
                "We calculated the τ correlation to the true ranking.",
                "The mean τ correlation is 0.398, which is not significantly different from MTC, but is significantly lower than RTC.",
                "Using uniform estimates of probability is indistinguishable from the baseline, whereas estimating relevance by expert aggregation boosts performance a great deal: nearly 40% over both MTC and IP.",
                "Overfitting: It is possible to overfit: if too many judgments come from the first two systems, the variance in ΔMAP is reduced and the confidence estimates become unreliable.",
                "We saw this in Table 2 and Figure 2 where RTC exhibits a dip in accuracy when confidence is around 97%.",
                "In fact, the number of judgments made prior to a wrong prediction is over 50% greater than the number made prior to a correct prediction.",
                "Overfitting is difficult to quantify exactly, because making more relevance judgments does not always cause it: at higher confidence levels, more relevance judgments are made, and as Table 2 shows, accuracy is greater at those higher confidences.",
                "Obviously having more relevance judgments should increase both confidence and accuracy; the difference seems to be when one system has a great deal more judgments than the other.",
                "Pairwise Comparisons: Our pairwise comparisons fall into one of three groups: 1. the two original runs from which relevance judgments are acquired; 2. one of the original runs vs. one of the new runs; 3. two new runs.",
                "Table 3 shows confidence vs. accuracy results for each of these three groups.",
                "Interestingly, performance is worst when comparing one of the original runs to one of the additional runs.",
                "This is most likely due to a large difference in the number of judgments affecting the variance of ΔMAP.",
                "Nevertheless, performance is quite good on all three subsets.",
                "Worst Case: The case intuitively most likely to produce an error is when the two systems being compared have retrieved very few documents in common.",
                "If we want the judgments to be reusable, we should to be able to generalize even to runs that are very different from the ones used to acquire the relevance judgments.",
                "A simple measure of similarity of two runs is the average percentage of documents they retrieved in common for each topic [2].",
                "We calculated this for all pairs, then looked at performance on pairs with low similarity.",
                "Results are shown in accuracy confidence two original one original no original 0.5 − 0.6 - 48.1% 62.8% 0.6 − 0.7 - 57.1% 79.2% 0.7 − 0.8 - 67.9% 81.7% 0.8 − 0.9 - 82.2% 86.3% 0.9 − 0.95 95.9% 93.7% 92.6% 0.95 − 0.99 96.2% 92.5% 93.1% 1.0 100% 98.0% 99.1% W −1.11 −0.87 −0.27 Table 3: Confidence vs. accuracy of RTC when comparing the two original runs, one original run and one new run, and two new runs.",
                "RTC is robust in all three cases. accuracy when similar confidence 0 − 0.1 0.1 − 0.2 0.2 − 0.3 0.5 − 0.6 68.4% 63.1% 61.4% 0.6 − 0.7 84.2% 78.6% 76.6% 0.7 − 0.8 82.0% 79.8% 78.9% 0.8 − 0.9 93.6% 83.3% 82.1% 0.9 − 0.95 99.3% 92.7% 92.4% 0.95 − 0.99 98.7% 93.4% 93.3% 1.0 99.9% 97.9% 98.1% W 0.44 −0.45 −0.49 Table 4: Confidence vs. accuracy of RTC when a pair of systems retrieved 0-30% documents in common (broken out into 0%-10%, 10%-20%, and 20%30%).",
                "RTC is robust in all three cases.",
                "Table 4.",
                "Performance is in fact very robust even when similarity is low.",
                "When the two runs share very few documents in common, W is actually positive.",
                "MTC and IP both performed quite poorly in these cases.",
                "When the similarity was between 0 and 10%, both MTC and IP correctly predicted ΔMAP only 60% of the time, compared to an 87.6% success rate for RTC.",
                "By Data Set: All the previous results have only been on the ad-hoc collections.",
                "We did the same experiments on our additional data sets, and broke out the results by data set to see how performance varies.",
                "The results in Table 5 show everything about each set, including binned accuracy, W, mean τ, and median number of judgments to reach 95% confidence on the first two systems.",
                "The results are highly consistent from collection to collection, suggesting that our method is not overfitting to any particular data set. 6.",
                "CONCLUSIONS AND FUTURE WORK In this work we have offered the first formal definition of the common idea of reusability of a test collection and presented a model that is able to achieve reusability with very small sets of relevance judgments.",
                "Table 2 and Figure 2 together show how biased a small set of judgments can be: MTC is dramatically overestimating confidence and is much less accurate than RTC, which is able to remove the bias to give a robust evaluation.",
                "The confidence estimates of RTC, in addition to being accurate, provide a guide for obtaining additional judgments: focus on judging documents from the lowest-confidence comparisons.",
                "In the long run, we see small sets of relevance judgaccuracy confidence ad-hoc 94 ad-hoc 96 ad-hoc 97 ad-hoc 98 ad-hoc 99 web 04 robust 05 terabyte 05 0.5 − 0.6 64.1% 61.8% 62.2% 62.0% 59.4% 64.3% 61.5% 61.6% 0.6 − 0.7 76.1% 77.8% 74.5% 78.2% 74.3% 78.1% 75.9% 75.9% 0.7 − 0.8 75.2% 78.9% 77.6% 80.0% 78.6% 82.6% 77.5% 80.4% 0.8 − 0.9 83.2% 85.5% 84.6% 84.9% 86.8% 84.5% 86.7% 87.7% 0.9 − 0.95 93.0% 93.6% 92.8% 93.7% 92.6% 94.2% 93.9% 94.2% 0.95 − 0.99 93.1% 94.3% 93.1% 93.7% 92.8% 95.0% 93.9% 91.6% 1.0 99.2% 96.8% 98.7% 99.5% 99.6% 100% 99.2% 98.3% W -0.34 -0.34 -0.48 -0.35 -0.44 -0.07 -0.41 -0.67 median judged 235 276 243 213 179 448 310 320 mean τ 0.538 0.573 0.556 0.579 0.532 0.596 0.565 0.574 Table 5: Accuracy, W, mean τ, and median number of judgments for all 8 testing sets.",
                "The results are highly consistent across data sets. ments being shared by researchers, each group contributing a few more judgments to gain more confidence about their particular systems.",
                "As time goes on, the number of judgments grows until there is 100% confidence in every evaluation-and there is a full test collection for the task.",
                "We see further use for this method in scenarios such as web retrieval in which the corpus is frequently changing.",
                "It could be applied to evaluation on a dynamic test collection as defined by Soboroff [18].",
                "The model we presented in Section 3 is by no means the only possibility for creating a robust test collection.",
                "A simpler expert aggregation model might perform as well or better (though all our efforts to simplify failed).",
                "In addition to expert aggregation, we could estimate probabilities by looking at similarities between documents.",
                "This is an obvious area for future exploration.",
                "Additionally, it will be worthwhile to investigate the issue of overfitting: the circumstances it occurs under and what can be done to prevent it.",
                "In the meantime, capping confidence estimates at 95% is a hack that solves the problem.",
                "We have many more experimental results that we unfortunately did not have space for but that reinforce the notion that RTC is highly robust: with just a few judgments per topic, we can accurately assess the confidence in any pairwise comparison of systems.",
                "Acknowledgments This work was supported in part by the Center for Intelligent Information Retrieval and in part by the Defense Advanced Research Projects Agency (DARPA) under contract number HR0011-06-C-0023.",
                "Any opinions, findings, and conclusions or recommendations expressed in this material are those of the author and do not necessarily reflect those of the sponsor. 7.",
                "REFERENCES [1] J. Aslam and M. Montague.",
                "Models for Metasearch.",
                "In Proceedings of SIGIR, pages 275-285, 2001. [2] J. Aslam and R. Savell.",
                "On the effectiveness of evaluating retrieval systems in the absence of relevance judgments.",
                "In Proceedings of SIGIR, pages 361-362, 2003. [3] J.",
                "A. Aslam, V. Pavlu, and R. Savell.",
                "A unified model for metasearch, pooling, and system evaluation.",
                "In Proceedings of CIKM, pages 484-491, 2003. [4] J.",
                "A. Aslam, V. Pavlu, and E. Yilmaz.",
                "A statistical method for system evaluation using incomplete judgments.",
                "In Proceedings of SIGIR, pages 541-548, 2006. [5] A. L. Berger, S. D. Pietra, and V. J. D. Pietra.",
                "A maximum entropy approach to natural language processing.",
                "Computational Linguistics, 22(1):39-71, 1996. [6] D. J. Blower.",
                "An easy derivation of logistic regression from the bayesian and maximum entropy perspective.",
                "In Proceedings of the 23rd International Workship on Bayesian Inference and Maximum Entropy Methods in Science and Engineering, pages 30-43, 2004. [7] B. Carterette and J. Allan.",
                "Research methodology in studies of assessor effort for retrieval evaluation.",
                "In Proceedings of RIAO, 2007. [8] B. Carterette, J. Allan, and R. K. Sitaraman.",
                "Minimal test collections for retrieval evaluation.",
                "In Proceedings of SIGIR, pages 268-275, 2006. [9] B. Carterette and D. I. Petkova.",
                "Learning a ranking from pairwise preferences.",
                "In Proceedings of SIGIR, 2006. [10] R. T. Clemen and R. L. Winkler.",
                "Unanimity and compromise among probability forecasters.",
                "Management Science, 36(7):767-779, July 1990. [11] G. V. Cormack, C. R. Palmer, and C. L. Clarke.",
                "Efficient Construction of Large Test Collections.",
                "In Proceedings of SIGIR, pages 282-289, 1998. [12] A. Gelman, J.",
                "B. Carlin, H. S. Stern, and D. B. Rubin.",
                "Bayesian Data Analysis.",
                "Chapman & Hall/CRC, 2004. [13] E. T. Jaynes.",
                "Probability Theory: The Logic of Science.",
                "Cambridge University Press, 2003. [14] R. Manmatha and H. Sever.",
                "A Formal Approach to Score Normalization for Metasearch.",
                "In Proceedings of HLT, pages 88-93, 2002. [15] I. J. Myung, S. Ramamoorti, and J. Andrew D. Baily.",
                "Maximum entropy aggregation of expert predictions.",
                "Management Science, 42(10):1420-1436, October 1996. [16] J. Platt.",
                "Probabilistic outputs for support vector machines and comparison to regularized likelihood methods. pages 61-74, 2000. [17] M. Sanderson and H. Joho.",
                "Forming test collections with no system pooling.",
                "In Proceedings of the 27th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 33-40, 2004. [18] I. Soboroff.",
                "Dynamic test collections: measuring search effectiveness on the live web.",
                "In Proceedings of SIGIR, pages 276-283, 2006. [19] K. Sparck Jones and C. J. van Rijsbergen.",
                "Information Retrieval Test Collections.",
                "Journal of Documentation, 32(1):59-75, 1976. [20] E. M. Voorhees and D. K. Harman, editors.",
                "TREC: Experiment and Evaluation in Information Retrieval.",
                "MIT Press, 2005. [21] J. Zobel.",
                "How Reliable are the Results of Large-Scale Information Retrieval Experiments?",
                "In Proceedings of SIGIR, pages 307-314, 1998."
            ],
            "original_annotated_samples": [
                "This random variable has an <br>expectation</br>, a variance, confidence intervals, and a certain probability of being less than or equal to a given value.",
                "Summing over all possibilities, we can compute <br>expectation</br> and variance: E[AP] ≈ 1 pi aiipi + j>i aij pipj V ar[AP] ≈ 1 ( pi)2 n i a2 iipiqi + j>i a2 ijpipj(1 − pipj) + i=j 2aiiaijpipj(1 − pi) + k>j=i 2aijaikpipjpk(1 − pi) AP asymptotically converges to a normal distribution with <br>expectation</br> and variance as defined above.1 For our comparative evaluation task we are interested in the sign of the difference in two average precisions: ΔAP = AP1 − AP2.",
                "MAP is also normally distributed; its <br>expectation</br> and variance are: EMAP = 1 T t∈T E[APt] (1) VMAP = 1 T2 t∈T V ar[APt] ΔMAP = MAP1 − MAP2 Confidence can then be estimated by calculating the <br>expectation</br> and variance and using the normal density function to find P(ΔMAP < 0). 2.2 Confidence and Robustness Having defined confidence, we turn back to the issue of trust in confidence estimates, and show how it ties into the robustness of the collection to missing judgments. 1 These are actually approximations to the true expectation and variance, but the error is a negligible O(n2−n )."
            ],
            "translated_annotated_samples": [
                "Esta variable aleatoria tiene una <br>esperanza</br>, una varianza, intervalos de confianza y una cierta probabilidad de ser menor o igual a un valor dado.",
                "Sumando sobre todas las posibilidades, podemos calcular la <br>esperanza</br> y la varianza: E[AP] ≈ 1 pi aiipi + j>i aij pipj V ar[AP] ≈ 1 ( pi)2 n i a2 iipiqi + j>i a2 ijpipj(1 − pipj) + i=j 2aiiaijpipj(1 − pi) + k>j=i 2aijaikpipjpk(1 − pi) AP converge asintóticamente a una distribución normal con la <br>esperanza</br> y la varianza definidas anteriormente. Para nuestra tarea de evaluación comparativa, estamos interesados en el signo de la diferencia en dos precisiones promedio: ΔAP = AP1 − AP2.",
                "MAP también se distribuye normalmente; su <br>esperanza</br> y varianza son: EMAP = 1 T t∈T E[APt] (1) VMAP = 1 T2 t∈T V ar[APt] ΔMAP = MAP1 − MAP2 La confianza puede entonces estimarse calculando la <br>esperanza</br> y varianza y utilizando la función de densidad normal para encontrar P(ΔMAP < 0). 2.2 Confianza y Robustez Habiendo definido la confianza, volvemos al tema de la confianza en las estimaciones de confianza, y mostramos cómo se relaciona con la robustez de la colección ante juicios faltantes. 1 Estas son en realidad aproximaciones a la verdadera esperanza y varianza, pero el error es despreciable O(n2−n )."
            ],
            "translated_text": "Colecciones de pruebas robustas para la evaluación de recuperación Ben Carterette Centro de Recuperación de Información Inteligente Departamento de Ciencias de la Computación Universidad de Massachusetts Amherst Amherst, MA 01003 carteret@cs.umass.edu RESUMEN Los métodos de bajo costo para adquirir juicios de relevancia pueden ser una bendición para los investigadores que necesitan evaluar nuevas tareas o temas de recuperación pero no tienen los recursos para hacer miles de juicios. Si bien estos juicios son muy útiles para una evaluación única, no está claro que se puedan confiar al ser reutilizados para evaluar nuevos sistemas. En este trabajo, definimos formalmente lo que significa que las evaluaciones sean reutilizables: la confianza en una evaluación de nuevos sistemas puede ser evaluada con precisión a partir de un conjunto existente de juicios de relevancia. Luego presentamos un método para aumentar un conjunto de juicios de relevancia con estimaciones de relevancia que no requieren esfuerzo adicional del evaluador. Usar este método garantiza prácticamente la reutilización: con tan solo cinco juicios por tema tomados de solo dos sistemas, podemos evaluar de manera confiable un conjunto más grande de diez sistemas. Incluso los conjuntos más pequeños de juicios pueden ser útiles para la evaluación de nuevos sistemas. Categorías y Descriptores de Asignaturas: H.3 Almacenamiento y Recuperación de Información; H.3.4 Sistemas y Software: Evaluación del Rendimiento Términos Generales: Experimentación, Medición, Confiabilidad 1. INTRODUCCIÓN Imagina a un investigador de recuperación de información que ha inventado una nueva tarea de recuperación. Ella ha construido un sistema para realizar la tarea y quiere evaluarlo. Dado que la tarea es nueva, es poco probable que existan juicios de relevancia vigentes. Ella no tiene el tiempo ni los recursos para juzgar cada documento, ni siquiera cada documento recuperado. Ella solo puede juzgar los documentos que parecen ser los más informativos y detenerse cuando tenga un grado razonable de confianza en sus conclusiones. Pero ¿qué sucede cuando ella desarrolla un nuevo sistema y necesita evaluarlo? ¿Otro grupo de investigación decide implementar un sistema para realizar la tarea? ¿Pueden reutilizar de manera confiable los juicios originales? ¿Pueden evaluar sin más juicios de relevancia? La evaluación es un aspecto importante de la investigación en recuperación de información, pero es solo un problema parcialmente resuelto: para la mayoría de las tareas de recuperación, es imposible juzgar la relevancia de cada documento; simplemente hay demasiados. La solución utilizada por NIST en TREC (Text REtrieval Conference) es el método de agrupación [19, 20]: todos los sistemas competidores contribuyen con N documentos a un grupo, y cada documento en ese grupo es evaluado. Este método crea grandes conjuntos de juicios que son reutilizables para entrenar o evaluar nuevos sistemas que no contribuyeron al conjunto [21]. Esta solución no es adecuada para nuestro investigador hipotético. El método de agrupación proporciona miles de juicios de relevancia, pero requiere muchas horas de tiempo de anotadores (remunerados). Como resultado, ha habido una serie de artículos recientes sobre la reducción del esfuerzo del anotador en la producción de colecciones de pruebas: Cormack et al. [11], Zobel [21], Sanderson y Joho [17], Carterette et al. [8], y Aslam et al. [4], entre otros. Como veremos, los juicios que estos métodos producen pueden sesgar significativamente la evaluación de un nuevo conjunto de sistemas. ¿Volviendo a nuestra investigadora hipotética, puede reutilizar sus juicios de relevancia? Primero debemos definir formalmente lo que significa ser reutilizable. En trabajos anteriores, la reutilización se ha probado simplemente evaluando la precisión de un conjunto de juicios de relevancia al evaluar sistemas no vistos. Si bien podemos decir que tuvo razón el 75% del tiempo, o que tuvo una correlación de rango de 0.8, estos números no tienen ningún poder predictivo: no nos dicen qué sistemas es probable que estén equivocados ni cuán seguros deberíamos estar en uno en particular. Necesitamos una definición más cuidadosa de reutilización. Específicamente, la cuestión de la reutilización no es qué tan precisamente podemos evaluar los nuevos sistemas. Un adversario malicioso siempre puede producir una nueva lista clasificada que no haya recuperado ninguno de los documentos evaluados. La verdadera pregunta es cuánta confianza tenemos en nuestras evaluaciones, y, más importante aún, si podemos confiar en nuestras estimaciones de confianza. Aunque la confianza no sea alta, siempre y cuando podamos confiar en ello, podemos identificar qué sistemas necesitan más juicios para aumentar la confianza. Cualquier conjunto de juicios, por pequeño que sea, se vuelve reutilizable en cierta medida. Pequeñas colecciones de pruebas reutilizables podrían tener un gran impacto en la investigación de recuperación de información. Los grupos de investigación podrían compartir las evaluaciones de relevancia que han realizado internamente para estudios piloto, nuevas tareas o nuevos temas. La cantidad de datos disponible para los investigadores crecería de forma exponencial con el tiempo. EVALUACIÓN ROBUSTA Anteriormente dimos una definición intuitiva de reutilización: una colección es reutilizable si podemos confiar en nuestras estimaciones de confianza en una evaluación. Con eso queremos decir que si hemos realizado algunos juicios de relevancia y tenemos, por ejemplo, un 75% de confianza en que el sistema A es mejor que el sistema B, nos gustaría que no haya más del 25% de posibilidad de que nuestra evaluación de la calidad relativa de los sistemas cambie a medida que continuamos evaluando documentos. Nuestra evaluación debe ser robusta ante juicios faltantes. En nuestro trabajo anterior, definimos la confianza como la probabilidad de que la diferencia en una medida de evaluación calculada para dos sistemas sea menor que cero [8]. Esta noción de confianza se define en el contexto de una tarea de evaluación particular que llamamos evaluación comparativa: determinar el signo de la diferencia en una medida de evaluación. Otras tareas de evaluación podrían ser definidas; estimar la magnitud de la diferencia o los valores de las medidas mismas son ejemplos que implican diferentes nociones de confianza. Por lo tanto, consideramos la confianza como una estimación de probabilidad. Una de las preguntas que debemos hacer acerca de una estimación de probabilidad es qué significa. ¿Qué significa tener un 75% de confianza de que el sistema A es mejor que el sistema B? Como se describe arriba, queremos que signifique que si seguimos evaluando documentos, solo habrá un 25% de probabilidad de que nuestra evaluación cambie. Si esto es lo que significa, podemos confiar en las estimaciones de confianza. ¿Pero sabemos que tiene ese significado? Nuestro cálculo de confianza se basó en una suposición sobre la probabilidad de relevancia de los documentos no evaluados, específicamente que cada documento no evaluado tenía la misma probabilidad de ser relevante o no relevante. Esta suposición casi con seguridad no es realista en la mayoría de las aplicaciones de IR. Resulta que es esta suposición la que determina si se pueden confiar en las estimaciones de confianza. Antes de profundizar en esto, definimos formalmente la confianza. 2.1 Estimación de la confianza La precisión promedio (AP) es una métrica de evaluación estándar que captura tanto la capacidad de un sistema para clasificar documentos relevantes de manera alta (precisión) como su capacidad para recuperar documentos relevantes (recuperación). Normalmente se escribe como la precisión media en los rangos de documentos relevantes: AP = 1 |R| i∈R prec@r(i) donde R es el conjunto de documentos relevantes y r(i) es el rango del documento i. Sea Xi una variable aleatoria que indica la relevancia del documento i. Si los documentos están ordenados por rango, podemos expresar la precisión como prec@i = 1/i i j=1 Xj. La precisión promedio se convierte entonces en la ecuación cuadrática AP = 1 Xi n i=1 Xi/i i j=1 Xj = 1 Xi n i=1 j≥i aijXiXj donde aij = 1/ max{r(i), r(j)}. Usar aij en lugar de 1/i nos permite numerar los documentos de forma arbitraria. Para ver por qué esto es cierto, considera un ejemplo simple: una lista de 3 documentos con los documentos relevantes B, C en los rangos 1 y 3 y el documento no relevante A en el rango 2. La precisión promedio será 1 2 (1 1 x2 B+ 1 2 xBxA+ 1 3 xBxC + 1 2 x2 A+ 1 3 xAxC + 1 3 x2 C) = 1 2 1 + 2 3 porque xA = 0, xB = 1, xC = 1. Aunque el orden B, A, C es diferente de la etiqueta A, B, C, no afecta al cálculo. Ahora podemos ver que la precisión promedio en sí misma es una variable aleatoria con una distribución sobre todas las posibles asignaciones de relevancia a todos los documentos. Esta variable aleatoria tiene una <br>esperanza</br>, una varianza, intervalos de confianza y una cierta probabilidad de ser menor o igual a un valor dado. Todos estos dependen de la probabilidad de que el documento i sea relevante: pi = p(Xi = 1). Supongamos que en nuestro ejemplo anterior no conocemos las valoraciones de relevancia, pero creemos que pA = 0.4, pB = 0.8, pC = 0.7. Podemos entonces calcular, por ejemplo. P(AP = 0) = 0.2 · 0.6 · 0.3 = 0.036, o P(AP = 1 2 ) = 0.2 · 0.4 · 0.7 = 0.056. \n\nLa traducción al español sería: P(AP = 0) = 0.2 · 0.6 · 0.3 = 0.036, o P(AP = 1 2 ) = 0.2 · 0.4 · 0.7 = 0.056. Sumando sobre todas las posibilidades, podemos calcular la <br>esperanza</br> y la varianza: E[AP] ≈ 1 pi aiipi + j>i aij pipj V ar[AP] ≈ 1 ( pi)2 n i a2 iipiqi + j>i a2 ijpipj(1 − pipj) + i=j 2aiiaijpipj(1 − pi) + k>j=i 2aijaikpipjpk(1 − pi) AP converge asintóticamente a una distribución normal con la <br>esperanza</br> y la varianza definidas anteriormente. Para nuestra tarea de evaluación comparativa, estamos interesados en el signo de la diferencia en dos precisiones promedio: ΔAP = AP1 − AP2. Como mostramos en nuestro trabajo anterior, ΔAP tiene una forma cerrada cuando los documentos están ordenados arbitrariamente: ΔAP = 1 Xi n i=1 j≥i cij XiXj cij = aij − bij donde bij está definido de manera análoga a aij para la segunda clasificación. Dado que AP es normal, ΔAP también es normal, lo que significa que podemos usar la función de densidad acumulativa normal para determinar la confianza de que una diferencia en AP sea menor que cero. Dado que los temas son independientes, podemos extender fácilmente esto para referirnos a la precisión promedio (MAP). MAP también se distribuye normalmente; su <br>esperanza</br> y varianza son: EMAP = 1 T t∈T E[APt] (1) VMAP = 1 T2 t∈T V ar[APt] ΔMAP = MAP1 − MAP2 La confianza puede entonces estimarse calculando la <br>esperanza</br> y varianza y utilizando la función de densidad normal para encontrar P(ΔMAP < 0). 2.2 Confianza y Robustez Habiendo definido la confianza, volvemos al tema de la confianza en las estimaciones de confianza, y mostramos cómo se relaciona con la robustez de la colección ante juicios faltantes. 1 Estas son en realidad aproximaciones a la verdadera esperanza y varianza, pero el error es despreciable O(n2−n ). Sea Z el conjunto de todos los pares de resultados clasificados para un conjunto común de temas. Supongamos que tenemos un conjunto de m juicios de relevancia xm = {x1, x2, ..., xm} (usando x minúscula en lugar de X mayúscula para distinguir entre documentos juzgados y no juzgados); estos son los juicios contra los cuales calculamos la confianza. Sea Zα el subconjunto de pares en Z para los cuales predecimos que ΔMAP = −1 con confianza α dadas las evaluaciones xm. Para que las estimaciones de confianza sean precisas, necesitamos que al menos α · |Zα| de estos pares realmente tengan ΔMAP = −1 después de haber evaluado cada documento. Si lo hacen, podemos confiar en las estimaciones de confianza; nuestra evaluación será robusta ante juicios faltantes. Si nuestras estimaciones de confianza se basan en suposiciones poco realistas, no podemos esperar que sean precisas. Las suposiciones en las que se basan son las probabilidades de relevancia pi. Necesitamos que esto sea realista. Sostenemos que la mejor distribución posible de relevancia p(Xi) es aquella que explica todos los datos (todas las observaciones realizadas sobre los sistemas de recuperación) al mismo tiempo que no hace suposiciones infundadas. Esto se conoce como el principio de máxima entropía [13]. La entropía de una variable aleatoria X con distribución p(X) se define como H(p) = − i p(X = i) log p(X = i). Esto ha encontrado una amplia variedad de usos en la informática y la recuperación de información. La distribución de entropía máxima es aquella que maximiza H. Esta distribución es única y tiene una forma exponencial. El siguiente teorema muestra la utilidad de una distribución de entropía máxima para la relevancia al estimar la confianza. Teorema 1. Si p(Xn |I, xm ) = argmaxpH(p), las estimaciones de confianza serán precisas, donde xm es el conjunto de juicios de relevancia definidos anteriormente, Xn es el conjunto completo de documentos cuya relevancia deseamos estimar, e I es alguna información sobre los documentos (no especificada hasta ahora). Por el momento prescindimos de la prueba, pero es bastante simple. Esto indica que cuanto mejores sean las estimaciones de relevancia, más precisa será la evaluación. La tarea de crear una colección de pruebas reutilizable se convierte entonces en la tarea de estimar la relevancia de los documentos no evaluados. El teorema y su demostración no dicen absolutamente nada sobre la métrica de evaluación. Las estimaciones de probabilidad son completamente independientes de la medida en la que estamos interesados. Esto significa que los mismos cálculos de probabilidad pueden informarnos sobre la precisión promedio, así como sobre la precisión, la recuperación, el bpref, etc. Además, podríamos asumir que la relevancia de los documentos i y j es independiente y lograr el mismo resultado, lo cual afirmamos como un corolario: Corolario 1. Si p(Xi|I, xm) = argmaxpH(p), las estimaciones de confianza serán precisas. La tarea, por lo tanto, consiste en la imputación de los valores faltantes de relevancia. El teorema implica que cuanto más nos acerquemos a la distribución de entropía máxima relevante, más nos acercaremos a la robustez. 3. PREDICIENDO LA RELEVANCIA En nuestra declaración del Teorema 1, dejamos sin especificar la naturaleza de la información I. Una de las ventajas de nuestras estimaciones de confianza es que admiten información de una amplia variedad de fuentes; básicamente cualquier cosa que pueda ser modelada puede ser utilizada como información para predecir la relevancia. Una fuente natural de información son los propios sistemas de recuperación: cómo clasificaron los documentos evaluados, con qué frecuencia no lograron clasificar documentos relevantes, cómo se desempeñan en diferentes temas, y así sucesivamente. Si tratamos cada sistema como un experto en recuperación de información que proporciona una opinión sobre la relevancia de cada documento, el problema se convierte en uno de agregación de opiniones de expertos. Esto es similar al problema de metabúsqueda o fusión de datos en el que la tarea consiste en tomar k sistemas de entrada y fusionarlos en un solo ranking. Aslam et al. [3] identificaron previamente una conexión entre la evaluación y la metabúsqueda. Nuestro problema tiene dos diferencias clave: 1. Necesitamos explícitamente probabilidades de relevancia que podamos insertar en la Ecuación 1; los algoritmos de metabusqueda no tienen tal requisito. Estamos acumulando juicios de relevancia a medida que avanzamos con la evaluación y somos capaces de reestimar la relevancia dada cada nuevo juicio. A la luz de lo anterior, introducimos un modelo probabilístico para la combinación de expertos. 3.1 Un Modelo para la Agregación de Opiniones de Expertos Supongamos que cada experto j proporciona una probabilidad de relevancia qij = pj(Xi = 1). La información sobre la relevancia del documento i será entonces el conjunto de k opiniones de expertos I = qi = (qi1, qi2, · · · , qik). La distribución de probabilidad que deseamos encontrar es aquella que maximiza la entropía de pi = p(Xi = 1|qi). Resulta que encontrar el modelo de entropía máxima es equivalente a encontrar los parámetros que maximizan la verosimilitud [5]. Blower [6] muestra explícitamente que encontrar el modelo de entropía máxima para una variable binaria es equivalente a resolver una regresión logística. Entonces pi = p(Xi = 1|qi) = exp k j=1 λjqij 1 + exp k j=1 λj qij (2) donde λ1, · · · , λk son los parámetros de regresión. Incluimos una distribución beta a priori para p(λj) con parámetros α, β. Esto se puede ver como un tipo de suavizado para tener en cuenta el hecho de que los datos de entrenamiento están altamente sesgados. Este modelo tiene la ventaja de incluir la dependencia estadística entre los expertos. Un modelo de la misma forma fue demostrado por Clemen & Winkler como el mejor para agregar probabilidades de expertos [10]. Un enfoque similar motivado por la entropía máxima se ha utilizado para la agregación de expertos [15]. Aslam & Montague [1] utilizaron un modelo similar para la metabúsqueda, pero asumieron independencia entre los expertos. ¿De dónde vienen los qij? Utilizar puntuaciones sin procesar y no calibradas como predictores no funcionará porque las distribuciones de puntuaciones varían demasiado entre los temas. Un clasificador de modelado de lenguaje, por ejemplo, suele otorgar una puntuación mucho más alta al documento recuperado en la parte superior para una consulta corta que al documento recuperado en la parte superior para una consulta larga. Podríamos entrenar un modelo predictivo separado para cada tema, pero eso no aprovecha toda la información que tenemos: es posible que solo tengamos un puñado de juicios para un tema, no suficientes para entrenar un modelo con confianza. Además, parece razonable asumir que si un experto hace buenas predicciones para un tema, también hará buenas predicciones para otros temas. Podríamos usar un modelo jerárquico [12], pero eso no se generalizará a temas no vistos. En cambio, calibraremos las puntuaciones de cada experto de forma individual para que las puntuaciones puedan ser comparadas tanto dentro de un tema como entre temas. Por lo tanto, nuestro modelo tiene en cuenta no solo la dependencia entre expertos, sino también la dependencia entre el rendimiento de los expertos en diferentes tareas (temas). 3.2 Calibración de Expertos Cada experto nos proporciona una puntuación y un rango para cada documento. Necesitamos convertir estos en probabilidades. Un método como el utilizado por Manmatha et al. [14] podría ser utilizado para convertir puntuaciones en probabilidades de relevancia. El método de preferencia por pares de Carterette & Petkova [9] también podría ser utilizado, interpretando la clasificación de un documento sobre otro como una expresión de preferencia. Que q∗ ij sea la probabilidad autoinformada por el experto j de que el documento i sea relevante. De manera intuitiva, parece claro que q∗ ij debería disminuir con el rango, y debería ser cero si el documento i no está clasificado (el experto no consideró que fuera relevante). El modelo de preferencia por pares puede manejar fácilmente estos dos requisitos, por lo que lo utilizaremos. Que θrj (i) sea el coeficiente de relevancia del documento en la posición rj(i). Queremos encontrar los θs que maximizan la función de verosimilitud: Ljt(Θ) = rj (i)<rj (k) exp(θrj (i) − θrj (k)) 1 + exp(θrj (i) − θrj (k)) Incluimos nuevamente una distribución beta previa en p(θrj(i)) con parámetros |Rt| + 1 y |Nt| + 1, el tamaño de los conjuntos de documentos relevantes y no relevantes juzgados respectivamente. El uso de estos parámetros previos garantiza que las probabilidades resultantes se concentren alrededor de la proporción de documentos relevantes que se han descubierto para el tema t. Esto significa que las estimaciones de probabilidad disminuyen por rango y son más altas para los temas que tienen más documentos relevantes. Después de encontrar el Θ que maximiza la verosimilitud, tenemos q∗ ij = exp(θrj (i)) 1+exp(θrj (i)) . Definimos θ∞ = −∞, de modo que la probabilidad de que un documento no clasificado sea relevante es 0. Dado que q∗ ij se basa en la posición en la que se recupera un documento en lugar de en la identidad del documento en sí, las probabilidades son idénticas de un experto a otro, por ejemplo, si el experto E colocó el documento A en la posición 1, y el experto D colocó el documento B en la posición 1, tendremos que q∗ AE = q∗ BD. Por lo tanto, solo tenemos que resolver esto una vez para cada tema. El modelo anterior proporciona probabilidades independientes del tema para cada documento. Pero supongamos que un experto que informa una probabilidad del 90% solo tiene razón el 50% del tiempo. Su opinión debería ser descartada basándose en su desempeño observado. Específicamente, queremos aprender una función de calibración qij = Cj(q∗ ij) que garantice que las probabilidades predichas estén ajustadas a la capacidad de los expertos para recuperar documentos relevantes dados los juicios que se han realizado hasta este momento. El método de calibración SVM de Platts [16] ajusta una función sigmoide entre q∗ ij y las evaluaciones de relevancia para obtener qij = Cj (q∗ ij) = exp(Aj +Bjq∗ ij ) 1+exp(Aj +Bj q∗ ij ) . Dado que q∗ ij es independiente del tema, solo necesitamos aprender una función de calibración para cada experto. Una vez que tenemos la función de calibración, se aplica para ajustar las predicciones de los expertos a su rendimiento real. Las probabilidades calibradas se introducen en el modelo (2) para encontrar las probabilidades de los documentos. Figura 1: Diagrama conceptual de nuestro modelo de agregación. Los expertos E1 y E2 han clasificado los documentos A, B, C para el tema T1 y los documentos D, E, F para el tema T2. El primer paso es obtener q∗ ij. A continuación se realiza la calibración al rendimiento real para encontrar qij. Finalmente obtenemos pi = p(Xi = 1|qi1, qi2), · · · . 3.3 Resumen del modelo Nuestro modelo tiene tres componentes que difieren en los datos que toman como entrada y en lo que producen como salida. Se muestra un diagrama conceptual en la Figura 1. 1. rangos → probabilidades (por sistema por tema). Esto nos da q∗ ij, la probabilidad autoinformada por el experto j de la relevancia del documento i. Esto es no supervisado; no requiere datos etiquetados (aunque si los tenemos, los usamos para establecer parámetros previos). 2. probabilidades → probabilidades calibradas (por sistema). Esto nos da qij = Cj (q∗ ij), la probabilidad calibrada por el experto j de la relevancia del documento i. Esto es semisupervisado; tenemos juicios de relevancia en algunos rangos que usamos para imputar la probabilidad de relevancia en otros rangos. 3. probabilidades calibradas → probabilidades de documentos. Esto nos da pi = p(Xi = 1|qi), la probabilidad de relevancia del documento i dada las probabilidades de experto calibradas qij. Esto es supervisado; aprendemos coeficientes de un conjunto de documentos juzgados y los utilizamos para estimar la relevancia de los documentos no juzgados. Aunque el modelo parece bastante complejo, en realidad son solo tres aplicaciones sucesivas de regresión logística. Por lo tanto, se puede implementar en un lenguaje de programación estadística como R en unas pocas líneas de código. El uso de priors beta (conjugados) garantiza que no sean necesarios métodos computacionales costosos como MCMC [12], por lo que el modelo se entrena y aplica lo suficientemente rápido como para ser utilizado en línea. Nuestro código está disponible en http://ciir.cs.umass.edu/~carteret/. 4. EXPERIMENTOS Se están considerando tres hipótesis. El primero, y más importante, es que el uso de nuestro modelo de agregación de expertos para predecir la relevancia produce colecciones de pruebas lo suficientemente robustas como para ser reutilizables; es decir, podemos confiar en las estimaciones de confianza cuando evaluamos sistemas que no contribuyeron con ningún juicio al conjunto. Las otras dos hipótesis se relacionan con la mejora que observamos al utilizar estimaciones de relevancia mejores que las que utilizamos en nuestro trabajo anterior [8]. Estos son (a) que se necesitan menos temas de relevancia, número de ejecuciones, número de documentos evaluados y número de documentos relevantes para cada uno de nuestros conjuntos de datos y (b) la precisión de las predicciones es mayor que si simplemente asumiéramos que pi = 0.5 para todos los documentos no evaluados. 4.1 Datos Obtuvimos ejecuciones completas de búsqueda ad-hoc presentadas en las TRECs 3 a 8. Cada ejecución clasifica como máximo 1000 documentos para 50 temas (49 temas para TREC 4). Además, obtuvimos todas las ejecuciones de la pista Web de TREC 13, la pista Robust2 de TREC 14 y la pista Terabyte (ad-hoc) de TREC 14. Estas son las pistas que han reemplazado la pista ad-hoc desde su final en 1999. Las estadísticas se muestran en la Tabla 1. Reservamos el conjunto TREC 4 (ad-hoc 95) para entrenamiento, los TRECs 3 y 5-8 (ad-hoc 94 y 96-99) para pruebas principales, y los conjuntos restantes para pruebas adicionales. Utilizamos los archivos qrels recopilados por NIST como verdad. El número de juicios de relevancia realizados y documentos relevantes encontrados para cada pista se enumeran en la Tabla 1. Por razones computacionales, truncamos las listas clasificadas en 100 documentos. No hay razón por la que no pudiéramos profundizar más, pero el cálculo de la varianza es O(n3) y, por lo tanto, muy consumidor de tiempo. Debido a la naturaleza del rango recíproco de AP, no perdemos mucha información al truncar en el rango 100. 4.2 Algoritmos Compararemos tres algoritmos para adquirir juicios de relevancia. La línea base es una variación del agrupamiento de TREC que llamaremos agrupamiento incremental (IP). Este algoritmo toma un número k como entrada y presenta los primeros k documentos en orden de rango (sin tener en cuenta el tema) para ser evaluados. No estima la relevancia de los documentos no evaluados; simplemente asume que cualquier documento no evaluado no es relevante. El segundo algoritmo es el presentado en Carterette et al. [8] (Algoritmo 1). Los documentos se seleccionan en función de lo interesantes que sean para determinar si existe una diferencia en la precisión media promedio. Para este enfoque pi = 0.5 para todos los i; no hay estimación de probabilidades. Llamaremos a esto MTC por colección de pruebas mínimas. El tercer algoritmo aumenta MTC con estimaciones actualizadas de las probabilidades de relevancia. Llamaremos a esto RTC por colección de pruebas robusta. Es idéntico al Algoritmo 1, excepto que cada décima iteración estimamos pi para todos los documentos no evaluados i utilizando el modelo de agregación de expertos de la Sección 3. RTC tiene parámetros de suavizado (distribución previa) que deben ser configurados. Entrenamos utilizando el conjunto ad-hoc 95. Limitamos 2 robustos aquí significa recuperación robusta; esto es diferente de nuestro objetivo de evaluación robusta. Algoritmo 1 (MTC) Dadas dos listas clasificadas y un nivel de confianza α, predecir el signo de ΔMAP. 1: pi ← 0.5 para todos los documentos i 2: mientras P(ΔMAP < 0) < α hacer 3: calcular el peso wi para todos los documentos i no clasificados (ver Carterette et al. [8] para más detalles) 4: j ← argmaxiwi 5: xj ← 1 si el documento j es relevante, 0 en caso contrario 6: pj ← xj 7: fin mientras la búsqueda de priors uniformes con una varianza relativamente alta. Para la agregación de expertos, los parámetros previos son α = β = 1. 4.3 Diseño Experimental Primero, queremos saber si podemos aumentar un conjunto de juicios de relevancia con un conjunto de probabilidades de relevancia para reutilizar los juicios y evaluar un nuevo conjunto de sistemas. Para cada prueba experimental: 1. Selecciona un subconjunto aleatorio de k ejecuciones. De esos k, elige un c inicial < k para evaluar. 3. Ejecutar RTC al 95% de confianza en el c inicial de aproximadamente 4. Utilizando el modelo de la Sección 3, estime las probabilidades de relevancia para todos los documentos recuperados por todos los k experimentos. Calcular EMAP para todas las ejecuciones k, y P(ΔMAP < 0) para todos los pares de ejecuciones. Hacemos lo mismo para MTC, pero omitimos el paso 4. Ten en cuenta que después de evaluar los primeros c sistemas, no realizamos juicios de relevancia adicionales. Para poner a prueba nuestro método, seleccionamos c = 2: construiremos un conjunto de juicios a partir de evaluar solo dos sistemas iniciales. Luego generalizaremos a un conjunto de k = 10 (de los cuales esos dos son un subconjunto). A medida que realizamos más pruebas, obtenemos los datos que necesitamos para probar las tres hipótesis. 4.4 Evaluación Experimental Recuerde que un conjunto de juicios es robusto si la precisión de las predicciones que realiza es al menos su confianza estimada. Una forma de evaluar la robustez es agrupar pares por su confianza, luego calcular la precisión sobre todos los pares en cada grupo. Nos gustaría que la precisión no fuera menor que la puntuación de confianza más baja en el contenedor, pero preferiblemente más alta. Dado que las estadísticas resumen son útiles, ideamos la siguiente métrica. Supongamos que somos una casa de apuestas tomando apuestas sobre si ΔMAP < 0. Utilizamos RTC o MTC para establecer las probabilidades O = P (ΔMAP <0) 1−P (ΔMAP <0) . Supongamos que un apostador apuesta $1 a que ΔMAP ≥ 0. Si resulta que ΔMAP < 0, ganamos el dólar. De lo contrario, pagamos O. Si nuestras estimaciones de confianza son perfectamente precisas, empatamos. Si la confianza es mayor que la precisión, perdemos dinero; ganamos si la precisión es mayor que la confianza. Contra intuitivamente, el resultado más deseable es empatar: si perdemos dinero, no podemos confiar en las estimaciones de confianza, pero si ganamos dinero, hemos subestimado la confianza o evaluado más documentos de los necesarios. Sin embargo, el costo de no poder confiar en las estimaciones de confianza es mayor que el costo de realizar juicios de relevancia adicionales, por lo que consideraremos los resultados positivos como buenos. La cantidad que ganamos en cada comparación par a par i es: Wi = yi − (1 − yi) Pi 1 − Pi = yi − Pi 1 − Pi yi = 1 si ΔMAP < 0 y 0 en caso contrario, y Pi = P(ΔMAP < 0). La estadística resumen es W, la media de Wi. Ten en cuenta que a medida que Pi aumenta, perdemos más por estar equivocados. Esto es como debería ser: la penalización debería ser grande por fallar en las predicciones de alta probabilidad. Sin embargo, dado que nuestras pérdidas crecen sin límite a medida que las predicciones se acercan a la certeza, limitamos −Wi a 100. Para nuestra hipótesis de que RTC requiere menos juicios que MTC, estamos interesados en el número de juicios necesarios para alcanzar el 95% de confianza en el primer par de sistemas. La mediana es más interesante que la media: la mayoría de los pares requieren unos cientos de juicios, pero algunos pares requieren varios miles. La distribución es, por lo tanto, altamente sesgada y la media se ve fuertemente afectada por esos valores atípicos. Finalmente, para nuestra hipótesis de que RTC es más preciso que MTC, analizaremos la correlación de Kendalls τ entre una clasificación de k sistemas por un pequeño conjunto de juicios y la clasificación real utilizando el conjunto completo de juicios. El tau de Kendall, una estadística no paramétrica basada en intercambios de pares entre dos listas, es una evaluación estándar para este tipo de estudio. Va desde −1 (perfectamente anticorrelacionado) hasta 1 (clasificaciones idénticas), con 0 significando que la mitad de los pares están intercambiados. Como mencionamos en la introducción, sin embargo, una medida de precisión como la correlación de rangos no es una buena evaluación de la reutilización. Lo incluimos por completitud. 4.4.1 Prueba de hipótesis Realizar múltiples pruebas permite el uso de pruebas de hipótesis estadísticas para comparar algoritmos. El uso de los mismos conjuntos de sistemas permite el uso de pruebas emparejadas. Como mencionamos anteriormente, estamos más interesados en la mediana del número de juicios que en la media. Una prueba para la diferencia en la mediana es la prueba de rango con signo de Wilcoxon. También podemos usar una prueba t pareada para probar una diferencia en la media. Para la correlación de rangos, podemos usar una prueba t pareada para probar si hay una diferencia en τ. 5. RESULTADOS Y ANÁLISIS La comparación entre MTC y RTC se muestra en la Tabla 2. Con MTC y probabilidades uniformes de relevancia, los resultados están lejos de ser robustos. No podemos reutilizar las evaluaciones de relevancia con mucha confianza. Pero con RTC, los resultados son muy robustos. Existe una ligera disminución en la precisión cuando la confianza supera el 0.95; sin embargo, las predicciones de confianza son confiables. La media de Wi muestra que RTC está mucho más cerca de 0 que MTC. La distribución de las puntuaciones de confianza muestra que al menos el 80% de confianza se logra más del 35% del tiempo, lo que indica que ninguno de los algoritmos está siendo demasiado conservador en sus estimaciones de confianza. Las estimaciones de confianza son bastante bajas en general; esto se debe a que hemos construido una colección de pruebas a partir de solo dos sistemas iniciales. Recuerde de la Sección 1 que no podemos requerir (o incluso esperar) un nivel mínimo de confianza al generalizar a nuevos sistemas. Los resultados más detallados de ambos algoritmos se muestran en la Figura 2. La línea sólida es el resultado ideal que daría W = 0. RTC está en o por encima de esta línea en todos los puntos hasta que la confianza alcance aproximadamente 0.97. Después de eso, hay una ligera disminución en la precisión que discutimos a continuación. Ten en cuenta que tanto la confianza MTC RTC % en la precisión del intervalo % en la precisión del intervalo 0.5 − 0.6 33.7% 61.7% 28.6% 61.9% 0.6 − 0.7 18.1% 73.1% 20.1% 76.3% 0.7 − 0.8 10.4% 70.1% 15.5% 78.0% 0.8 − 0.9 9.4% 69.0% 12.1% 84.9% 0.9 − 0.95 7.3% 78.0% 6.6% 93.1% 0.95 − 0.99 17.9% 70.4% 12.4% 93.4% 1.0 3.3% 68.3% 4.7% 98.9% W −5.34 −0.39 mediana juzgada 251 235 media τ 0.393 0.555 Tabla 2: Confianza en que P(ΔMAP < 0) y precisión de la predicción al generalizar un conjunto de juicios de relevancia adquiridos utilizando MTC y RTC. Cada contenedor contiene más de 1,000 pruebas de los conjuntos ad hoc 3, 5-8. RTC es mucho más robusto que MTC. W está definido en la Sección 4.4; cuanto más cerca de 0, mejor. La mediana juzgada es el número de juicios necesarios para alcanzar un 95% de confianza en los dos primeros sistemas. La media τ es la correlación de rango promedio para los 10 sistemas. 0.5 0.6 0.7 0.8 0.9 1 0.5 0.6 0.7 0.8 0.9 1 precisión confianza punto de equilibrio RTC MTC Figura 2: Confianza vs. precisión de MTC y RTC. La línea sólida es el resultado perfecto que daría W = 0; el rendimiento debería estar en o por encima de esta línea. Cada punto representa al menos 500 comparaciones en pares. Los algoritmos están claramente por encima de la línea hasta aproximadamente una confianza de 0.7. Esto se debe a que el rendimiento base en estos conjuntos de datos es alto; es bastante fácil lograr un 75% de precisión haciendo muy poco trabajo [7]. Número de juicios: La mediana del número de juicios requeridos por MTC para alcanzar un 95% de confianza en los dos primeros sistemas es de 251, un promedio de 5 por tema. La mediana requerida por RTC es 235, aproximadamente 4.7 por tema. Aunque los números son cercanos, la mediana de RTCs es significativamente más baja según una prueba de Wilcoxon pareada (p < 0.0001). Para comparación, una piscina de profundidad 100 resultaría en un mínimo de 5,000 juicios para cada par. La diferencia en las medias es mucho mayor. MTC requirió una media de 823 juicios, 16 por tema, mientras que RTC requirió una media de 502, 10 por tema. (Recordemos que las medias están fuertemente sesgadas por algunas parejas que requieren miles de juicios). Esta diferencia es significativa mediante una prueba t de muestras pareadas (p < 0.0001). El diez por ciento de los conjuntos resultaron en 100 o menos juicios (menos de dos por tema). El rendimiento en estos es muy alto: W = 0.41 y un 99.7% de precisión cuando la confianza es de al menos 0.9. Esto demuestra que incluso las colecciones pequeñas pueden ser reutilizables. Para el 50% de los conjuntos con más de 235 juicios, la precisión es del 93% cuando la confianza es de al menos 0.9. Correlación de rangos: MTC y RTC clasifican los 10 sistemas por EMAP (Eq. (1)) calculado utilizando sus respectivas estimaciones de probabilidad. La correlación de rango τ promedio entre el MAP verdadero y el EMAP es de 0.393 para MTC y 0.555 para RTC. Esta diferencia es significativa mediante una prueba t de muestras pareadas (p < 0.0001). Ten en cuenta que no esperamos que las correlaciones τ sean altas, ya que estamos clasificando los sistemas con tan pocos juicios de relevancia. Es más importante que estimemos correctamente la confianza en cada comparación de pares. Ejecutamos IP para el mismo número de juicios que MTC tomó para cada par, luego clasificamos los sistemas por MAP utilizando solo esos juicios (todos los documentos no juzgados se consideraron no relevantes). Calculamos la correlación τ con la clasificación real. La correlación media τ es de 0.398, lo cual no es significativamente diferente de MTC, pero es significativamente menor que RTC. El uso de estimaciones uniformes de probabilidad es indistinguible de la línea base, mientras que la estimación de relevancia mediante la agregación de expertos mejora el rendimiento en gran medida: casi un 40% más que tanto MTC como IP. Sobreajuste: Es posible sobreajustar: si provienen demasiados juicios de los dos primeros sistemas, la varianza en ΔMAP se reduce y las estimaciones de confianza se vuelven poco fiables. Vimos esto en la Tabla 2 y la Figura 2, donde RTC muestra una disminución en la precisión cuando la confianza está alrededor del 97%. De hecho, el número de juicios realizados antes de una predicción incorrecta es más de un 50% mayor que el número realizado antes de una predicción correcta. El sobreajuste es difícil de cuantificar exactamente, porque hacer más juicios de relevancia no siempre lo causa: a niveles de confianza más altos, se hacen más juicios de relevancia, y como muestra la Tabla 2, la precisión es mayor en esas mayores confianzas. Obviamente, tener más juicios de relevancia debería aumentar tanto la confianza como la precisión; la diferencia parece estar cuando un sistema tiene muchos más juicios que el otro. Comparaciones por pares: Nuestras comparaciones por pares se dividen en uno de tres grupos: 1. las dos ejecuciones originales de las que se obtienen los juicios de relevancia; 2. una de las ejecuciones originales vs. una de las nuevas ejecuciones; 3. dos nuevas ejecuciones. La Tabla 3 muestra los resultados de confianza frente a precisión para cada uno de estos tres grupos. Curiosamente, el rendimiento es peor al comparar una de las carreras originales con una de las carreras adicionales. Esto se debe muy probablemente a una gran diferencia en el número de juicios que afectan la varianza de ΔMAP. Sin embargo, el rendimiento es bastante bueno en los tres subconjuntos. Peor caso: El caso intuitivamente más probable de producir un error es cuando los dos sistemas que se comparan han recuperado muy pocos documentos en común. Si queremos que los juicios sean reutilizables, deberíamos ser capaces de generalizar incluso a ejecuciones que son muy diferentes de las utilizadas para adquirir los juicios de relevancia. Una medida simple de similitud de dos ejecuciones es el porcentaje promedio de documentos que recuperaron en común para cada tema [2]. Calculamos esto para todos los pares, luego observamos el rendimiento en pares con baja similitud. Los resultados se muestran en la confianza de precisión de dos ejecuciones originales, una ejecución original, ninguna ejecución original. 0.5 − 0.6 - 48.1% 62.8% 0.6 − 0.7 - 57.1% 79.2% 0.7 − 0.8 - 67.9% 81.7% 0.8 − 0.9 - 82.2% 86.3% 0.9 − 0.95 95.9% 93.7% 92.6% 0.95 − 0.99 96.2% 92.5% 93.1% 1.0 100% 98.0% 99.1% W −1.11 −0.87 −0.27 Tabla 3: Confianza vs. precisión de RTC al comparar las dos ejecuciones originales, una ejecución original y una nueva ejecución, y dos nuevas ejecuciones. RTC es robusto en los tres casos. precisión cuando la confianza es similar 0 − 0.1 0.1 − 0.2 0.2 − 0.3 0.5 − 0.6 68.4% 63.1% 61.4% 0.6 − 0.7 84.2% 78.6% 76.6% 0.7 − 0.8 82.0% 79.8% 78.9% 0.8 − 0.9 93.6% 83.3% 82.1% 0.9 − 0.95 99.3% 92.7% 92.4% 0.95 − 0.99 98.7% 93.4% 93.3% 1.0 99.9% 97.9% 98.1% W 0.44 −0.45 −0.49 Tabla 4: Confianza vs. precisión de RTC cuando un par de sistemas recuperó el 0-30% de documentos en común (dividido en 0%-10%, 10%-20% y 20%-30%). RTC es robusto en los tres casos. Tabla 4. El rendimiento es de hecho muy robusto incluso cuando la similitud es baja. Cuando las dos corridas comparten muy pocos documentos en común, W es en realidad positivo. Tanto MTC como IP tuvieron un rendimiento bastante deficiente en estos casos. Cuando la similitud estaba entre 0 y 10%, tanto MTC como IP predijeron correctamente ΔMAP solo el 60% del tiempo, en comparación con una tasa de éxito del 87.6% para RTC. Por conjunto de datos: Todos los resultados anteriores han sido solo en las colecciones ad-hoc. Realizamos los mismos experimentos en nuestros conjuntos de datos adicionales y desglosamos los resultados por conjunto de datos para ver cómo varía el rendimiento. Los resultados en la Tabla 5 muestran todo acerca de cada conjunto, incluyendo la precisión agrupada, W, la media de τ, y el número medio de juicios para alcanzar el 95% de confianza en los dos primeros sistemas. Los resultados son altamente consistentes de una colección a otra, lo que sugiere que nuestro método no está sobreajustado a ningún conjunto de datos en particular. CONCLUSIONES Y TRABAJO FUTURO En este trabajo hemos ofrecido la primera definición formal de la idea común de reutilización de una colección de pruebas y presentado un modelo que es capaz de lograr la reutilización con conjuntos muy pequeños de juicios de relevancia. La Tabla 2 y la Figura 2 juntas muestran cuán sesgado puede ser un pequeño conjunto de juicios: MTC está sobreestimando drásticamente la confianza y es mucho menos preciso que RTC, que puede eliminar el sesgo para proporcionar una evaluación sólida. Las estimaciones de confianza de RTC, además de ser precisas, proporcionan una guía para obtener juicios adicionales: enfóquese en juzgar documentos de las comparaciones de menor confianza. A largo plazo, observamos pequeños conjuntos de juicios de relevancia, precisión, confianza ad-hoc 94, ad-hoc 96, ad-hoc 97, ad-hoc 98, ad-hoc 99, web 04, robust 05, terabyte 05, 0.5 − 0.6 64.1%, 61.8%, 62.2%, 62.0%, 59.4%, 64.3%, 61.5%, 61.6%, 0.6 − 0.7 76.1%, 77.8%, 74.5%, 78.2%, 74.3%, 78.1%, 75.9%, 75.9%, 0.7 − 0.8 75.2%, 78.9%, 77.6%, 80.0%, 78.6%, 82.6%, 77.5%, 80.4%, 0.8 − 0.9 83.2%, 85.5%, 84.6%, 84.9%, 86.8%, 84.5%, 86.7%, 87.7%, 0.9 − 0.95 93.0%, 93.6%, 92.8%, 93.7%, 92.6%, 94.2%, 93.9%, 94.2%, 0.95 − 0.99 93.1%, 94.3%, 93.1%, 93.7%, 92.8%, 95.0%, 93.9%, 91.6%, 1.0 99.2%, 96.8%, 98.7%, 99.5%, 99.6%, 100%, 99.2%, 98.3%, W -0.34, -0.34, -0.48, -0.35, -0.44, -0.07, -0.41, -0.67, mediana juzgada 235, 276, 243, 213, 179, 448, 310, 320, media τ 0.538, 0.573, 0.556, 0.579, 0.532, 0.596, 0.565, 0.574. Tabla 5: Precisión, W, media τ y mediana del número de juicios para los 8 conjuntos de pruebas. Los resultados son altamente consistentes en todos los conjuntos de datos. A medida que pasa el tiempo, el número de juicios crece hasta que hay un 100% de confianza en cada evaluación, y hay una colección completa de pruebas para la tarea. Vemos un uso adicional para este método en escenarios como la recuperación web en los que el corpus cambia con frecuencia. Podría aplicarse a la evaluación en una colección de pruebas dinámica según lo definido por Soboroff [18]. El modelo que presentamos en la Sección 3 no es en absoluto la única posibilidad para crear una colección de pruebas robusta. Un modelo de agregación de expertos más simple podría funcionar igual de bien o mejor (aunque todos nuestros esfuerzos por simplificar fallaron). Además de la agregación de expertos, podríamos estimar probabilidades al observar similitudes entre documentos. Esta es un área obvia para futuras exploraciones. Además, será valioso investigar el problema del sobreajuste: las circunstancias en las que ocurre y qué se puede hacer para prevenirlo. Mientras tanto, limitar las estimaciones de confianza al 95% es un truco que resuelve el problema. Tenemos muchos más resultados experimentales que desafortunadamente no tuvimos espacio para incluir, pero que refuerzan la idea de que RTC es altamente robusto: con solo unos pocos juicios por tema, podemos evaluar con precisión la confianza en cualquier comparación de sistemas de forma pareja. Agradecimientos Este trabajo fue apoyado en parte por el Centro de Recuperación de Información Inteligente y en parte por la Agencia de Proyectos de Investigación Avanzada de Defensa (DARPA) bajo el número de contrato HR0011-06-C-0023. Cualquier opinión, hallazgo, conclusión o recomendación expresada en este material es responsabilidad del autor y no necesariamente refleja la del patrocinador. REFERENCIAS [1] J. Aslam y M. Montague. Modelos para Metabúsqueda. En Actas de SIGIR, páginas 275-285, 2001. [2] J. Aslam y R. Savell. Sobre la efectividad de evaluar sistemas de recuperación en ausencia de juicios de relevancia. En Actas de SIGIR, páginas 361-362, 2003. [3] J. A. Aslam, V. Pavlu y R. Savell. Un modelo unificado para metabúsqueda, agrupación y evaluación de sistemas. En Actas de CIKM, páginas 484-491, 2003. [4] J. A. Aslam, V. Pavlu y E. Yilmaz. Un método estadístico para la evaluación de sistemas utilizando juicios incompletos. En Actas de SIGIR, páginas 541-548, 2006. [5] A. L. Berger, S. D. Pietra y V. J. D. Pietra. Un enfoque de entropía máxima para el procesamiento del lenguaje natural. Lingüística Computacional, 22(1):39-71, 1996. [6] D. J. Blower. Una derivación sencilla de la regresión logística desde la perspectiva bayesiana y de máxima entropía. En Actas del 23º Taller Internacional sobre Inferencia Bayesiana y Métodos de Entropía Máxima en Ciencia e Ingeniería, páginas 30-43, 2004. [7] B. Carterette y J. Allan. Metodología de investigación en estudios del esfuerzo del evaluador para la evaluación de recuperación. En Actas de RIAO, 2007. [8] B. Carterette, J. Allan y R. K. Sitaraman. Colecciones de prueba mínimas para evaluación de recuperación. En Actas de SIGIR, páginas 268-275, 2006. [9] B. Carterette y D. I. Petkova. Aprendiendo un ranking a partir de preferencias por pares. En Actas de SIGIR, 2006. [10] R. T. Clemen y R. L. Winkler. Unanimidad y compromiso entre pronosticadores de probabilidad. Ciencia de la Gestión, 36(7):767-779, julio de 1990. [11] G. V. Cormack, C. R. Palmer y C. L. Clarke. Construcción eficiente de grandes colecciones de pruebas. En Actas de SIGIR, páginas 282-289, 1998. [12] A. Gelman, J. B. Carlin, H. S. Stern y D. B. Rubin. Análisis de datos bayesianos. Chapman & Hall/CRC, 2004. [13] E. T. Jaynes.\nChapman & Hall/CRC, 2004. [13] E. T. Jaynes. Teoría de la probabilidad: La lógica de la ciencia. Cambridge University Press, 2003. [14] R. Manmatha y H. Sever. Un enfoque formal para la normalización de puntuaciones en metabúsqueda. En Actas de HLT, páginas 88-93, 2002. [15] I. J. Myung, S. Ramamoorti y J. Andrew D. Baily. Agregación de predicciones de expertos mediante entropía máxima. Ciencia de la Gestión, 42(10):1420-1436, octubre de 1996. [16] J. Platt. Salidas probabilísticas para máquinas de vectores de soporte y comparación con métodos de verosimilitud regularizados. páginas 61-74, 2000. [17] M. Sanderson y H. Joho. Formando colecciones de pruebas sin agrupamiento de sistemas. En Actas de la 27ª Conferencia Internacional Anual de ACM SIGIR sobre Investigación y Desarrollo en Recuperación de Información, páginas 33-40, 2004. [18] I. Soboroff. Colecciones de pruebas dinámicas: midiendo la efectividad de la búsqueda en la web en vivo. En Actas de SIGIR, páginas 276-283, 2006. [19] K. Sparck Jones y C. J. van Rijsbergen. Colecciones de pruebas de recuperación de información. Revista de Documentación, 32(1):59-75, 1976. [20] E. M. Voorhees y D. K. Harman, editores. TREC: Experimento y Evaluación en Recuperación de Información. MIT Press, 2005. [21] J. Zobel. \n\nMIT Press, 2005. [21] J. Zobel. ¿Qué tan confiables son los resultados de los experimentos de recuperación de información a gran escala? En Actas de SIGIR, páginas 307-314, 1998. ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "variance": {
            "translated_key": "varianza",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Robust Test Collections for Retrieval Evaluation Ben Carterette Center for Intelligent Information Retrieval Computer Science Department University of Massachusetts Amherst Amherst, MA 01003 carteret@cs.umass.edu ABSTRACT Low-cost methods for acquiring relevance judgments can be a boon to researchers who need to evaluate new retrieval tasks or topics but do not have the resources to make thousands of judgments.",
                "While these judgments are very useful for a one-time evaluation, it is not clear that they can be trusted when re-used to evaluate new systems.",
                "In this work, we formally define what it means for judgments to be reusable: the confidence in an evaluation of new systems can be accurately assessed from an existing set of relevance judgments.",
                "We then present a method for augmenting a set of relevance judgments with relevance estimates that require no additional assessor effort.",
                "Using this method practically guarantees reusability: with as few as five judgments per topic taken from only two systems, we can reliably evaluate a larger set of ten systems.",
                "Even the smallest sets of judgments can be useful for evaluation of new systems.",
                "Categories and Subject Descriptors: H.3 Information Storage and Retrieval; H.3.4 Systems and Software: Performance Evaluation General Terms: Experimentation, Measurement, Reliability 1.",
                "INTRODUCTION Consider an information retrieval researcher who has invented a new retrieval task.",
                "She has built a system to perform the task and wants to evaluate it.",
                "Since the task is new, it is unlikely that there are any extant relevance judgments.",
                "She does not have the time or resources to judge every document, or even every retrieved document.",
                "She can only judge the documents that seem to be the most informative and stop when she has a reasonable degree of confidence in her conclusions.",
                "But what happens when she develops a new system and needs to evaluate it?",
                "Or another research group decides to implement a system to perform the task?",
                "Can they reliably reuse the original judgments?",
                "Can they evaluate without more relevance judgments?",
                "Evaluation is an important aspect of information retrieval research, but it is only a semi-solved problem: for most retrieval tasks, it is impossible to judge the relevance of every document; there are simply too many of them.",
                "The solution used by NIST at TREC (Text REtrieval Conference) is the pooling method [19, 20]: all competing systems contribute N documents to a pool, and every document in that pool is judged.",
                "This method creates large sets of judgments that are reusable for training or evaluating new systems that did not contribute to the pool [21].",
                "This solution is not adequate for our hypothetical researcher.",
                "The pooling method gives thousands of relevance judgments, but it requires many hours of (paid) annotator time.",
                "As a result, there have been a slew of recent papers on reducing annotator effort in producing test collections: Cormack et al. [11], Zobel [21], Sanderson and Joho [17], Carterette et al. [8], and Aslam et al. [4], among others.",
                "As we will see, the judgments these methods produce can significantly bias the evaluation of a new set of systems.",
                "Returning to our hypothetical resesarcher, can she reuse her relevance judgments?",
                "First we must formally define what it means to be reusable.",
                "In previous work, reusability has been tested by simply assessing the accuracy of a set of relevance judgments at evaluating unseen systems.",
                "While we can say that it was right 75% of the time, or that it had a rank correlation of 0.8, these numbers do not have any predictive power: they do not tell us which systems are likely to be wrong or how confident we should be in any one.",
                "We need a more careful definition of reusability.",
                "Specifically, the question of reusability is not how accurately we can evaluate new systems.",
                "A malicious adversary can always produce a new ranked list that has not retrieved any of the judged documents.",
                "The real question is how much confidence we have in our evaluations, and, more importantly, whether we can trust our estimates of confidence.",
                "Even if confidence is not high, as long as we can trust it, we can identify which systems need more judgments in order to increase confidence.",
                "Any set of judgments, no matter how small, becomes reusable to some degree.",
                "Small, reusable test collections could have a huge impact on information retrieval research.",
                "Research groups would be able to share the relevance judgments they have done in-house for pilot studies, new tasks, or new topics.",
                "The amount of data available to researchers would grow exponentially over time. 2.",
                "ROBUST EVALUATION Above we gave an intuitive definition of reusability: a collection is reusable if we can trust our estimates of confidence in an evaluation.",
                "By that we mean that if we have made some relevance judgments and have, for example, 75% confidence that system A is better than system B, we would like there to be no more than 25% chance that our assessment of the relative quality of the systems will change as we continue to judge documents.",
                "Our evaluation should be robust to missing judgments.",
                "In our previous work, we defined confidence as the probability that the difference in an evaluation measure calculated for two systems is less than zero [8].",
                "This notion of confidence is defined in the context of a particular evaluation task that we call comparative evaluation: determining the sign of the difference in an evaluation measure.",
                "Other evaluation tasks could be defined; estimating the magnitude of the difference or the values of the measures themselves are examples that entail different notions of confidence.",
                "We therefore see confidence as a probability estimate.",
                "One of the questions we must ask about a probability estimate is what it means.",
                "What does it mean to have 75% confidence that system A is better than system B?",
                "As described above, we want it to mean that if we continue to judge documents, there will only be a 25% chance that our assessment will change.",
                "If this is what it means, we can trust the confidence estimates.",
                "But do we know it has that meaning?",
                "Our calculation of confidence rested on an assumption about the probability of relevance of unjudged documents, specifically that each unjudged document was equally likely to be relevant or nonrelevant.",
                "This assumption is almost certainly not realistic in most IR applications.",
                "As it turns out, it is this assumption that determines whether the confidence estimates can eb trusted.",
                "Before elaborating on this, we formally define confidence. 2.1 Estimating Confidence Average precision (AP) is a standard evaluation metric that captures both the ability of a system to rank relevant documents highly (precision) as well as its ability to retrieve relevant documents (recall).",
                "It is typically written as the mean precision at the ranks of relevant documents: AP = 1 |R| i∈R prec@r(i) where R is the set of relevant documents and r(i) is the rank of document i.",
                "Let Xi be a random variable indicating the relevance of document i.",
                "If documents are ordered by rank, we can express precision as prec@i = 1/i i j=1 Xj .",
                "Average precision then becomes the quadratic equation AP = 1 Xi n i=1 Xi/i i j=1 Xj = 1 Xi n i=1 j≥i aijXiXj where aij = 1/ max{r(i), r(j)}.",
                "Using aij instead of 1/i allows us to number the documents arbitrarily.",
                "To see why this is true, consider a toy example: a list of 3 documents with relevant documents B, C at ranks 1 and 3 and nonrelevant document A at rank 2.",
                "Average precision will be 1 2 (1 1 x2 B+ 1 2 xBxA+ 1 3 xBxC + 1 2 x2 A+ 1 3 xAxC + 1 3 x2 C) = 1 2 1 + 2 3 because xA = 0, xB = 1, xC = 1.",
                "Though the ordering B, A, C is different from the labeling A, B, C, it does not affect the computation.",
                "We can now see average precision itself is a random variable with a distribution over all possible assignments of relevance to all documents.",
                "This random variable has an expectation, a <br>variance</br>, confidence intervals, and a certain probability of being less than or equal to a given value.",
                "All of these are dependent on the probability that document i is relevant: pi = p(Xi = 1).",
                "Suppose in our previous example we do not know the relevance judgments, but we believe pA = 0.4, pB = 0.8, pC = 0.7.",
                "We can then compute e.g.",
                "P(AP = 0) = 0.2 · 0.6 · 0.3 = 0.036, or P(AP = 1 2 ) = 0.2 · 0.4 · 0.7 = 0.056.",
                "Summing over all possibilities, we can compute expectation and <br>variance</br>: E[AP] ≈ 1 pi aiipi + j>i aij pipj V ar[AP] ≈ 1 ( pi)2 n i a2 iipiqi + j>i a2 ijpipj(1 − pipj) + i=j 2aiiaijpipj(1 − pi) + k>j=i 2aijaikpipjpk(1 − pi) AP asymptotically converges to a normal distribution with expectation and <br>variance</br> as defined above.1 For our comparative evaluation task we are interested in the sign of the difference in two average precisions: ΔAP = AP1 − AP2.",
                "As we showed in our previous work, ΔAP has a closed form when documents are ordered arbitrarily: ΔAP = 1 Xi n i=1 j≥i cij XiXj cij = aij − bij where bij is defined analogously to aij for the second ranking.",
                "Since AP is normal, ΔAP is normal as well, meaning we can use the normal cumulative density function to determine the confidence that a difference in AP is less than zero.",
                "Since topics are independent, we can easily extend this to mean average precision (MAP).",
                "MAP is also normally distributed; its expectation and <br>variance</br> are: EMAP = 1 T t∈T E[APt] (1) VMAP = 1 T2 t∈T V ar[APt] ΔMAP = MAP1 − MAP2 Confidence can then be estimated by calculating the expectation and <br>variance</br> and using the normal density function to find P(ΔMAP < 0). 2.2 Confidence and Robustness Having defined confidence, we turn back to the issue of trust in confidence estimates, and show how it ties into the robustness of the collection to missing judgments. 1 These are actually approximations to the true expectation and variance, but the error is a negligible O(n2−n ).",
                "Let Z be the set of all pairs of ranked results for a common set of topics.",
                "Suppose we have a set of m relevance judgments xm = {x1, x2, ..., xm} (using small x rather than capital X to distinguish between judged and unjudged documents); these are the judgments against which we compute confidence.",
                "Let Zα be the subset of pairs in Z for which we predict that ΔMAP = −1 with confidence α given the judgments xm .",
                "For the confidence estimates to be accurate, we need at least α · |Zα| of these pairs to actually have ΔMAP = −1 after we have judged every document.",
                "If they do, we can trust the confidence estimates; our evaluation will be robust to missing judgments.",
                "If our confidence estimates are based on unrealistic assumptions, we cannot expect them to be accurate.",
                "The assumptions they are based on are the probabilities of relevance pi.",
                "We need these to be realistic.",
                "We argue that the best possible distribution of relevance p(Xi) is the one that explains all of the data (all of the observations made about the retrieval systems) while at the same time making no unwarranted assumptions.",
                "This is known as the principle of maximum entropy [13].",
                "The entropy of a random variable X with distribution p(X) is defined as H(p) = − i p(X = i) log p(X = i).",
                "This has found a wide array of uses in computer science and information retrieval.",
                "The maximum entropy distribution is the one that maximizes H. This distribution is unique and has an exponential form.",
                "The following theorem shows the utility of a maximum entropy distribution for relevance when estimating confidence.",
                "Theorem 1.",
                "If p(Xn |I, xm ) = argmaxpH(p), confidence estimates will be accurate. where xm is the set of relevance judgments defined above, Xn is the full set of documents that we wish to estimate the relevance of, and I is some information about the documents (unspecified as of now).",
                "We forgo the proof for the time being, but it is quite simple.",
                "This says that the better the estimates of relevance, the more accurate the evaluation.",
                "The task of creating a reusable test collection thus becomes the task of estimating the relevance of unjudged documents.",
                "The theorem and its proof say nothing whatsoever about the evaluation metric.",
                "The probability estimates are entirely indepedent of the measure we are interested in.",
                "This means the same probability estimates can tell us about average precision as well as precision, recall, bpref, etc.",
                "Furthermore, we could assume that the relevance of documents i and j is independent and achieve the same result, which we state as a corollary: Corollary 1.",
                "If p(Xi|I, xm ) = argmaxpH(p), confidence estimates will be accurate.",
                "The task therefore becomes the imputation of the missing values of relevance.",
                "The theorem implies that the closer we get to the maximum entropy distribution of relevance, the closer we get to robustness. 3.",
                "PREDICTING RELEVANCE In our statement of Theorem 1, we left the nature of the information I unspecified.",
                "One of the advantages of our confidence estimates is that they admit information from a wide variety of sources; essentially anything that can be modeled can be used as information for predicting relevance.",
                "A natural source of information is the retrieval systems themselves: how they ranked the judged documents, how often they failed to rank relevant documents, how they perform across topics, and so on.",
                "If we treat each system as an information retrieval expert providing an opinion about the relevance of each document, the problem becomes one of expert opinion aggregation.",
                "This is similar to the metasearch or data fusion problem in which the task is to take k input systems and merge them into a single ranking.",
                "Aslam et al. [3] previously identified a connection between evaluation and metasearch.",
                "Our problem has two key differences: 1.",
                "We explicitly need probabilities of relevance that we can plug into Eq. 1; metasearch algorithms have no such requirement. 2.",
                "We are accumulating relevance judgments as we proceed with the evaluation and are able to re-estimate relevance given each new judgment.",
                "In light of (1) above, we introduce a probabilistic model for expert combination. 3.1 A Model for Expert Opinion Aggregation Suppose that each expert j provides a probability of relevance qij = pj(Xi = 1).",
                "The information about the relevance of document i will then be the set of k expert opinions I = qi = (qi1, qi2, · · · , qik).",
                "The probability distribution we wish to find is the one that maximizes the entropy of pi = p(Xi = 1|qi).",
                "As it turns out, finding the maximum entropy model is equivalent to finding the parameters that maximize the likelihood [5].",
                "Blower [6] explicitly shows that finding the maximum entropy model for a binary variable is equivalent to solving a logistic regression.",
                "Then pi = p(Xi = 1|qi) = exp k j=1 λjqij 1 + exp k j=1 λj qij (2) where λ1, · · · , λk are the regression parameters.",
                "We include a beta prior for p(λj) with parameters α, β.",
                "This can be seen as a type of smoothing to account for the fact that the training data is highly biased.",
                "This model has the advantage of including the statistical dependence between the experts.",
                "A model of the same form was shown by Clemen & Winkler to be the best for aggregating expert probabilities [10].",
                "A similar maximumentropy-motivated approach has been used for expert aggregation [15].",
                "Aslam & Montague [1] used a similar model for metasearch, but assumed independence among experts.",
                "Where do the qij s come from?",
                "Using raw, uncalibrated scores as predictors will not work because score distributions vary too much between topics.",
                "A language modeling ranker, for instance, will typically give a much higher score to the top retrieved document for a short query than to the top retrieved document for a long query.",
                "We could train a separate predicting model for each topic, but that does not take advantage of all of the information we have: we may only have a handful of judgments for a topic, not enough to train a model to any confidence.",
                "Furthermore, it seems reasonable to assume that if an expert makes good predictions for one topic, it will make good predictions for other topics as well.",
                "We could use a hierarchical model [12], but that will not generalize to unseen topics.",
                "Instead, we will calibrate the scores of each expert individually so that scores can be compared both within topic and between topic.",
                "Thus our model takes into account not only the dependence between experts, but also the dependence between experts performances on different tasks (topics). 3.2 Calibrating Experts Each expert gives us a score and a rank for each document.",
                "We need to convert these to probabilities.",
                "A method such as the one used by Manmatha et al. [14] could be used to convert scores into probabilities of relevance.",
                "The pairwise preference method of Carterette & Petkova [9] could also be used, interpeting the ranking of one document over another as an expression of preference.",
                "Let q∗ ij be expert js self-reported probability that document i is relevant.",
                "Intuitively it seems clear that q∗ ij should decrease with rank, and it should be zero if document i is unranked (the expert did not believe it to be relevant).",
                "The pairwise preference model can handle these two requirements easily, so we will use it.",
                "Let θrj (i) be the relevance coefficient of the document at rank rj(i).",
                "We want to find the θs that maximize the likelihood function: Ljt(Θ) = rj (i)<rj (k) exp(θrj (i) − θrj (k)) 1 + exp(θrj (i) − θrj (k)) We again include a beta prior on p(θrj(i)) with parameters |Rt| + 1 and |Nt| + 1, the size of the sets of judged relevant and nonrelevant documents respectively.",
                "Using these as prior parameters ensures that the resulting probabilities will be concentrated around the ratio of relevant documents that have been discovered for topic t. This means that the probability estimates decrease by rank and are higher for topics that have more relevant documents.",
                "After finding the Θ that maximizes the likelihood, we have q∗ ij = exp(θrj (i)) 1+exp(θrj (i)) .",
                "We define θ∞ = −∞, so that the probability that an unranked document is relevant is 0.",
                "Since q∗ ij is based on the rank at which a document is retrieved rather than the identity of the document itself, the probabilities are identical from expert to expert, e.g. if expert E put document A at rank 1, and expert D put document B at rank 1, we will have q∗ AE = q∗ BD.",
                "Therefore we only have to solve this once for each topic.",
                "The above model gives topic-independent probabilities for each document.",
                "But suppose an expert who reports 90% probability is only right 50% of the time.",
                "Its opinion should be discounted based on its observed performance.",
                "Specifically, we want to learn a calibration function qij = Cj(q∗ ij) that will ensure that the predicted probabilities are tuned to the experts ability to retrieve relevant documents given the judgments that have been made to this point.",
                "Platts SVM calibration method [16] fits a sigmoid function between q∗ ij and the relevance judgments to obtain qij = Cj (q∗ ij) = exp(Aj +Bjq∗ ij ) 1+exp(Aj +Bj q∗ ij ) .",
                "Since q∗ ij is topic-independent, we only need to learn one calibration function for each expert.",
                "Once we have the calibration function, it is applied to adjust the experts predictions to their actual performance.",
                "The calibrated probabilities are plugged into model (2) to find the document probabilities.",
                "Figure 1: Conceptual diagram of our aggregation model.",
                "Experts E1, E2 have ranked documents A, B, C for topic T1 and documents D, E, F for topic T2.",
                "The first step is to obtain q∗ ij.",
                "Next is calibration to true performance to find qij .",
                "Finally we obtain pi = p(Xi = 1|qi1, qi2), · · · . 3.3 Model Summary Our model has three components that differ by the data they take as input and what they produce as output.",
                "A conceptual diagram is shown in Figure 1. 1. ranks → probabilities (per system per topic).",
                "This gives us q∗ ij, expert js self-reported probability of the relevance of document i.",
                "This is unsupervised; it requires no labeled data (though if we have some, we use it to set prior parameters). 2. probabilities → calibrated probabilities (per system).",
                "This gives us qij = Cj (q∗ ij), expert js calibrated probability of the relevance of document i.",
                "This is semisupervised; we have relevance judgments at some ranks which we use to impute the probability of relevance at other ranks. 3. calibrated probabilities → document probabilities.",
                "This gives us pi = p(Xi = 1|qi), the probability of relevance of document i given calibrated expert probabilities qij .",
                "This is supervised; we learn coefficients from a set of judged documents and use those to estimate the relevance of the unjudged documents.",
                "Although the model appears rather complex, it is really just three successive applications of logistic regression.",
                "As such, it can be implemented in a statistical programming language such as R in a few lines of code.",
                "The use of beta (conjugate) priors ensures that no expensive computational methods such as MCMC are necessary [12], so the model is trained and applied fast enough to be used on-line.",
                "Our code is available at http://ciir.cs.umass.edu/~carteret/. 4.",
                "EXPERIMENTS Three hypotheses are under consideration.",
                "The first, and most important, is that using our expert aggregation model to predict relevance produces test collections that are robust enough to be reusable; that is, we can trust the estimates of confidence when we evaluate systems that did not contribute any judgments to the pool.",
                "The other two hypotheses relate to the improvement we see by using better estimates of relevance than we did in our previous work [8].",
                "These are that (a) it takes fewer relevance track no. topics no. runs no. judged no. rel ad-hoc 94 50 40 97,319 9,805 ad-hoc 95 49 33 87,069 6,503 ad-hoc 96 50 61 133,681 5,524 ad-hoc 97 50 74 72,270 4,611 ad-hoc 98 50 103 80,345 4,674 ad-hoc 99 50 129 86,830 4,728 web 04 225 74 88,566 1,763 robust 05 50 74 37,798 6,561 terabyte 05 50 58 45,291 10,407 Table 1: Number of topics, number of runs, number of documents judged, and number found relevant for each of our data sets. judgments to reach 95% confidence and (b) the accuracy of the predictions is higher than if we were to simply assume pi = 0.5 for all unjudged documents. 4.1 Data We obtained full ad-hoc runs submitted to TRECs 3 through 8.",
                "Each run ranks at most 1000 documents for 50 topics (49 topics for TREC 4).",
                "Additionally, we obtained all runs from the Web track of TREC 13, the Robust2 track of TREC 14, and the Terabyte (ad-hoc) track of TREC 14.",
                "These are the tracks that have replaced the ad-hoc track since its end in 1999.",
                "Statistics are shown in Table 1.",
                "We set aside the TREC 4 (ad-hoc 95) set for training, TRECs 3 and 5-8 (ad-hoc 94 and 96-99) for primary testing, and the remaining sets for additional testing.",
                "We use the qrels files assembled by NIST as truth.",
                "The number of relevance judgments made and relevant documents found for each track are listed in Table 1.",
                "For computational reasons, we truncate ranked lists at 100 documents.",
                "There is no reason that we could not go deeper, but calculating <br>variance</br> is O(n3 ) and thus very timeconsuming.",
                "Because of the reciprocal rank nature of AP, we do not lose much information by truncating at rank 100. 4.2 Algorithms We will compare three algorithms for acquiring relevance judgments.",
                "The baseline is a variation of TREC pooling that we will call incremental pooling (IP).",
                "This algorithm takes a number k as input and presents the first k documents in rank order (without regard to topic) to be judged.",
                "It does not estimate the relevance of unjudged documents; it simply assumes any unjudged document is nonrelevant.",
                "The second algorithm is that presented in Carterette et al. [8] (Algorithm 1).",
                "Documents are selected based on how interesting they are in determining whether a difference in mean average precision exists.",
                "For this approach pi = 0.5 for all i; there is no estimation of probabilities.",
                "We will call this MTC for minimal test collection.",
                "The third algorithm augments MTC with updated estimates of probabilities of relevance.",
                "We will call this RTC for robust test collection.",
                "It is identical to Algorithm 1, except that every 10th iteration we estimate pi for all unjudged documents i using the expert aggregation model of Section 3.",
                "RTC has smoothing (prior distribution) parameters that must be set.",
                "We trained using the ad-hoc 95 set.",
                "We limited 2 Robust here means robust retrieval; this is different from our goal of robust evaluation.",
                "Algorithm 1 (MTC) Given two ranked lists and confidence level α, predict the sign of ΔMAP. 1: pi ← 0.5 for all documents i 2: while P(ΔMAP < 0) < α do 3: calculate weight wi for all unjudged documents i (see Carterette et al. [8] for details) 4: j ← argmaxiwi 5: xj ← 1 if document j is relevant, 0 otherwise 6: pj ← xj 7: end while the search to uniform priors with relatively high <br>variance</br>.",
                "For expert aggregation, the prior parameters are α = β = 1. 4.3 Experimental Design First, we want to know whether we can augment a set of relevance judgments with a set of relevance probabilities in order to reuse the judgments to evaluate a new set of systems.",
                "For each experimental trial: 1.",
                "Pick a random subset of k runs. 2.",
                "From those k, pick an initial c < k to evaluate. 3.",
                "Run RTC to 95% confidence on the initial c. 4.",
                "Using the model from Section 3, estimate the probabilities of relevance for all documents retrieved by all k runs. 5.",
                "Calculate EMAP for all k runs, and P(ΔMAP < 0) for all pairs of runs.",
                "We do the same for MTC, but omit step 4.",
                "Note that after evaluating the first c systems, we make no additional relevance judgments.",
                "To put our method to the test, we selected c = 2: we will build a set of judgments from evaluating only two initial systems.",
                "We will then generalize to a set of k = 10 (of which those two are a subset).",
                "As we run more trials, we obtain the data we need to test all three of our hypotheses. 4.4 Experimental Evaluation Recall that a set of judgments is robust if the accuracy of the predictions it makes is at least its estimated confidence.",
                "One way to evaluate robustness is to bin pairs by their confidence, then calculate the accuracy over all the pairs in each bin.",
                "We would like the accuracy to be no less than the lowest confidence score in the bin, but preferably higher.",
                "Since summary statistics are useful, we devised the following metric.",
                "Suppose we are a bookmaker taking bets on whether ΔMAP < 0.",
                "We use RTC or MTC to set the odds O = P (ΔMAP <0) 1−P (ΔMAP <0) .",
                "Suppose a bettor wagers $1 on ΔMAP ≥ 0.",
                "If it turns out that ΔMAP < 0, we win the dollar.",
                "Otherwise, we pay out O.",
                "If our confidence estimates are perfectly accurate, we break even.",
                "If confidence is greater than accuracy, we lose money; we win if accuracy is greater than confidence.",
                "Counterintuitively, the most desirable outcome is breaking even: if we lose money, we cannot trust the confidence estimates, but if we win money, we have either underestimated confidence or judged more documents than necessary.",
                "However, the cost of not being able to trust the confidence estimates is higher than the cost of extra relevance judgments, so we will treat positive outcomes as good.",
                "The amount we win on each pairwise comparison i is: Wi = yi − (1 − yi) Pi 1 − Pi = yi − Pi 1 − Pi yi = 1 if ΔMAP < 0 and 0 otherwise, and Pi = P(ΔMAP < 0).",
                "The summary statistic is W, the mean of Wi.",
                "Note that as Pi increases, we lose more for being wrong.",
                "This is as it should be: the penalty should be great for missing the high probability predictions.",
                "However, since our losses grow without bound as predictions approach certainty, we cap −Wi at 100.",
                "For our hypothesis that RTC requires fewer judgments than MTC, we are interested in the number of judgments needed to reach 95% confidence on the first pair of systems.",
                "The median is more interesting than the mean: most pairs require a few hundred judgments, but a few pairs require several thousand.",
                "The distribution is therefore highly skewed, and the mean strongly affected by those outliers.",
                "Finally, for our hypothesis that RTC is more accurate than MTC, we will look at Kendalls τ correlation between a ranking of k systems by a small set of judgments and the true ranking using the full set of judgments.",
                "Kendalls τ, a nonparametric statistic based on pairwise swaps between two lists, is a standard evaluation for this type of study.",
                "It ranges from −1 (perfectly anti-correlated) to 1 (rankings identical), with 0 meaning that half of the pairs are swapped.",
                "As we touched on in the introduction, though, an accuracy measure like rank correlation is not a good evaluation of reusability.",
                "We include it for completeness. 4.4.1 Hypothesis Testing Running multiple trials allows the use of statistical hypothesis testing to compare algorithms.",
                "Using the same sets of systems allows the use of paired tests.",
                "As we stated above, we are more interested in the median number of judgments than the mean.",
                "A test for difference in median is the Wilcoxon sign rank test.",
                "We can also use a paired t-test to test for a difference in mean.",
                "For rank correlation, we can use a paired t-test to test for a difference in τ. 5.",
                "RESULTS AND ANALYSIS The comparison between MTC and RTC is shown in Table 2.",
                "With MTC and uniform probabilities of relevance, the results are far from robust.",
                "We cannot reuse the relevance judgments with much confidence.",
                "But with RTC, the results are very robust.",
                "There is a slight dip in accuracy when confidence gets above 0.95; nonetheless, the confidence predictions are trustworthy.",
                "Mean Wi shows that RTC is much closer to 0 than MTC.",
                "The distribution of confidence scores shows that at least 80% confidence is achieved more than 35% of the time, indicating that neither algorithm is being too conservative in its confidence estimates.",
                "The confidence estimates are rather low overall; that is because we have built a test collection from only two initial systems.",
                "Recall from Section 1 that we cannot require (or even expect) a minimum level of confidence when we generalize to new systems.",
                "More detailed results for both algorithms are shown in Figure 2.",
                "The solid line is the ideal result that would give W = 0.",
                "RTC is on or above this line at all points until confidence reaches about 0.97.",
                "After that there is a slight dip in accuracy which we discuss below.",
                "Note that both MTC RTC confidence % in bin accuracy % in bin accuracy 0.5 − 0.6 33.7% 61.7% 28.6% 61.9% 0.6 − 0.7 18.1% 73.1% 20.1% 76.3% 0.7 − 0.8 10.4% 70.1% 15.5% 78.0% 0.8 − 0.9 9.4% 69.0% 12.1% 84.9% 0.9 − 0.95 7.3% 78.0% 6.6% 93.1% 0.95 − 0.99 17.9% 70.4% 12.4% 93.4% 1.0 3.3% 68.3% 4.7% 98.9% W −5.34 −0.39 median judged 251 235 mean τ 0.393 0.555 Table 2: Confidence that P(ΔMAP < 0) and accuracy of prediction when generalizing a set of relevance judgments acquired using MTC and RTC.",
                "Each bin contains over 1,000 trials from the adhoc 3, 5-8 sets.",
                "RTC is much more robust than MTC.",
                "W is defined in Section 4.4; closer to 0 is better.",
                "Median judged is the number of judgments to reach 95% confidence on the first two systems.",
                "Mean τ is the average rank correlation for all 10 systems. 0.5 0.6 0.7 0.8 0.9 1 0.5 0.6 0.7 0.8 0.9 1 accuracy confidence breakeven RTC MTC Figure 2: Confidence vs. accuracy of MTC and RTC.",
                "The solid line is the perfect result that would give W = 0; performance should be on or above this line.",
                "Each point represents at least 500 pairwise comparisons. algorithms are well above the line up to around confidence 0.7.",
                "This is because the baseline performance on these data sets is high; it is quite easy to achieve 75% accuracy doing very little work [7].",
                "Number of Judgments: The median number of judgments required by MTC to reach 95% confidence on the first two systems is 251, an average of 5 per topic.",
                "The median required by RTC is 235, about 4.7 per topic.",
                "Although the numbers are close, RTCs median is significantly lower by a paired Wilcoxon test (p < 0.0001).",
                "For comparison, a pool of depth 100 would result in a minimum of 5,000 judgments for each pair.",
                "The difference in means is much greater.",
                "MTC required a mean of 823 judgments, 16 per topic, while RTC required a mean of 502, 10 per topic. (Recall that means are strongly skewed by a few pairs that take thousands of judgments.)",
                "This difference is significant by a paired t-test (p < 0.0001).",
                "Ten percent of the sets resulted in 100 or fewer judgments (less than two per topic).",
                "Performance on these is very high: W = 0.41, and 99.7% accuracy when confidence is at least 0.9.",
                "This shows that even tiny collections can be reusable.",
                "For the 50% of sets with more than 235 judgments, accuracy is 93% when confidence is at least 0.9.",
                "Rank Correlation: MTC and RTC both rank the 10 systems by EMAP (Eq. (1)) calculated using their respective probability estimates.",
                "The mean τ rank correlation between true MAP and EMAP is 0.393 for MTC and 0.555 for RTC.",
                "This difference is significant by a paired t-test (p < 0.0001).",
                "Note that we do not expect the τ correlations to be high, since we are ranking the systems with so few relevance judgments.",
                "It is more important that we estimate confidence in each pairwise comparison correctly.",
                "We ran IP for the same number of judgments that MTC took for each pair, then ranked the systems by MAP using only those judgments (all unjudged documents assumed nonrelevant).",
                "We calculated the τ correlation to the true ranking.",
                "The mean τ correlation is 0.398, which is not significantly different from MTC, but is significantly lower than RTC.",
                "Using uniform estimates of probability is indistinguishable from the baseline, whereas estimating relevance by expert aggregation boosts performance a great deal: nearly 40% over both MTC and IP.",
                "Overfitting: It is possible to overfit: if too many judgments come from the first two systems, the <br>variance</br> in ΔMAP is reduced and the confidence estimates become unreliable.",
                "We saw this in Table 2 and Figure 2 where RTC exhibits a dip in accuracy when confidence is around 97%.",
                "In fact, the number of judgments made prior to a wrong prediction is over 50% greater than the number made prior to a correct prediction.",
                "Overfitting is difficult to quantify exactly, because making more relevance judgments does not always cause it: at higher confidence levels, more relevance judgments are made, and as Table 2 shows, accuracy is greater at those higher confidences.",
                "Obviously having more relevance judgments should increase both confidence and accuracy; the difference seems to be when one system has a great deal more judgments than the other.",
                "Pairwise Comparisons: Our pairwise comparisons fall into one of three groups: 1. the two original runs from which relevance judgments are acquired; 2. one of the original runs vs. one of the new runs; 3. two new runs.",
                "Table 3 shows confidence vs. accuracy results for each of these three groups.",
                "Interestingly, performance is worst when comparing one of the original runs to one of the additional runs.",
                "This is most likely due to a large difference in the number of judgments affecting the <br>variance</br> of ΔMAP.",
                "Nevertheless, performance is quite good on all three subsets.",
                "Worst Case: The case intuitively most likely to produce an error is when the two systems being compared have retrieved very few documents in common.",
                "If we want the judgments to be reusable, we should to be able to generalize even to runs that are very different from the ones used to acquire the relevance judgments.",
                "A simple measure of similarity of two runs is the average percentage of documents they retrieved in common for each topic [2].",
                "We calculated this for all pairs, then looked at performance on pairs with low similarity.",
                "Results are shown in accuracy confidence two original one original no original 0.5 − 0.6 - 48.1% 62.8% 0.6 − 0.7 - 57.1% 79.2% 0.7 − 0.8 - 67.9% 81.7% 0.8 − 0.9 - 82.2% 86.3% 0.9 − 0.95 95.9% 93.7% 92.6% 0.95 − 0.99 96.2% 92.5% 93.1% 1.0 100% 98.0% 99.1% W −1.11 −0.87 −0.27 Table 3: Confidence vs. accuracy of RTC when comparing the two original runs, one original run and one new run, and two new runs.",
                "RTC is robust in all three cases. accuracy when similar confidence 0 − 0.1 0.1 − 0.2 0.2 − 0.3 0.5 − 0.6 68.4% 63.1% 61.4% 0.6 − 0.7 84.2% 78.6% 76.6% 0.7 − 0.8 82.0% 79.8% 78.9% 0.8 − 0.9 93.6% 83.3% 82.1% 0.9 − 0.95 99.3% 92.7% 92.4% 0.95 − 0.99 98.7% 93.4% 93.3% 1.0 99.9% 97.9% 98.1% W 0.44 −0.45 −0.49 Table 4: Confidence vs. accuracy of RTC when a pair of systems retrieved 0-30% documents in common (broken out into 0%-10%, 10%-20%, and 20%30%).",
                "RTC is robust in all three cases.",
                "Table 4.",
                "Performance is in fact very robust even when similarity is low.",
                "When the two runs share very few documents in common, W is actually positive.",
                "MTC and IP both performed quite poorly in these cases.",
                "When the similarity was between 0 and 10%, both MTC and IP correctly predicted ΔMAP only 60% of the time, compared to an 87.6% success rate for RTC.",
                "By Data Set: All the previous results have only been on the ad-hoc collections.",
                "We did the same experiments on our additional data sets, and broke out the results by data set to see how performance varies.",
                "The results in Table 5 show everything about each set, including binned accuracy, W, mean τ, and median number of judgments to reach 95% confidence on the first two systems.",
                "The results are highly consistent from collection to collection, suggesting that our method is not overfitting to any particular data set. 6.",
                "CONCLUSIONS AND FUTURE WORK In this work we have offered the first formal definition of the common idea of reusability of a test collection and presented a model that is able to achieve reusability with very small sets of relevance judgments.",
                "Table 2 and Figure 2 together show how biased a small set of judgments can be: MTC is dramatically overestimating confidence and is much less accurate than RTC, which is able to remove the bias to give a robust evaluation.",
                "The confidence estimates of RTC, in addition to being accurate, provide a guide for obtaining additional judgments: focus on judging documents from the lowest-confidence comparisons.",
                "In the long run, we see small sets of relevance judgaccuracy confidence ad-hoc 94 ad-hoc 96 ad-hoc 97 ad-hoc 98 ad-hoc 99 web 04 robust 05 terabyte 05 0.5 − 0.6 64.1% 61.8% 62.2% 62.0% 59.4% 64.3% 61.5% 61.6% 0.6 − 0.7 76.1% 77.8% 74.5% 78.2% 74.3% 78.1% 75.9% 75.9% 0.7 − 0.8 75.2% 78.9% 77.6% 80.0% 78.6% 82.6% 77.5% 80.4% 0.8 − 0.9 83.2% 85.5% 84.6% 84.9% 86.8% 84.5% 86.7% 87.7% 0.9 − 0.95 93.0% 93.6% 92.8% 93.7% 92.6% 94.2% 93.9% 94.2% 0.95 − 0.99 93.1% 94.3% 93.1% 93.7% 92.8% 95.0% 93.9% 91.6% 1.0 99.2% 96.8% 98.7% 99.5% 99.6% 100% 99.2% 98.3% W -0.34 -0.34 -0.48 -0.35 -0.44 -0.07 -0.41 -0.67 median judged 235 276 243 213 179 448 310 320 mean τ 0.538 0.573 0.556 0.579 0.532 0.596 0.565 0.574 Table 5: Accuracy, W, mean τ, and median number of judgments for all 8 testing sets.",
                "The results are highly consistent across data sets. ments being shared by researchers, each group contributing a few more judgments to gain more confidence about their particular systems.",
                "As time goes on, the number of judgments grows until there is 100% confidence in every evaluation-and there is a full test collection for the task.",
                "We see further use for this method in scenarios such as web retrieval in which the corpus is frequently changing.",
                "It could be applied to evaluation on a dynamic test collection as defined by Soboroff [18].",
                "The model we presented in Section 3 is by no means the only possibility for creating a robust test collection.",
                "A simpler expert aggregation model might perform as well or better (though all our efforts to simplify failed).",
                "In addition to expert aggregation, we could estimate probabilities by looking at similarities between documents.",
                "This is an obvious area for future exploration.",
                "Additionally, it will be worthwhile to investigate the issue of overfitting: the circumstances it occurs under and what can be done to prevent it.",
                "In the meantime, capping confidence estimates at 95% is a hack that solves the problem.",
                "We have many more experimental results that we unfortunately did not have space for but that reinforce the notion that RTC is highly robust: with just a few judgments per topic, we can accurately assess the confidence in any pairwise comparison of systems.",
                "Acknowledgments This work was supported in part by the Center for Intelligent Information Retrieval and in part by the Defense Advanced Research Projects Agency (DARPA) under contract number HR0011-06-C-0023.",
                "Any opinions, findings, and conclusions or recommendations expressed in this material are those of the author and do not necessarily reflect those of the sponsor. 7.",
                "REFERENCES [1] J. Aslam and M. Montague.",
                "Models for Metasearch.",
                "In Proceedings of SIGIR, pages 275-285, 2001. [2] J. Aslam and R. Savell.",
                "On the effectiveness of evaluating retrieval systems in the absence of relevance judgments.",
                "In Proceedings of SIGIR, pages 361-362, 2003. [3] J.",
                "A. Aslam, V. Pavlu, and R. Savell.",
                "A unified model for metasearch, pooling, and system evaluation.",
                "In Proceedings of CIKM, pages 484-491, 2003. [4] J.",
                "A. Aslam, V. Pavlu, and E. Yilmaz.",
                "A statistical method for system evaluation using incomplete judgments.",
                "In Proceedings of SIGIR, pages 541-548, 2006. [5] A. L. Berger, S. D. Pietra, and V. J. D. Pietra.",
                "A maximum entropy approach to natural language processing.",
                "Computational Linguistics, 22(1):39-71, 1996. [6] D. J. Blower.",
                "An easy derivation of logistic regression from the bayesian and maximum entropy perspective.",
                "In Proceedings of the 23rd International Workship on Bayesian Inference and Maximum Entropy Methods in Science and Engineering, pages 30-43, 2004. [7] B. Carterette and J. Allan.",
                "Research methodology in studies of assessor effort for retrieval evaluation.",
                "In Proceedings of RIAO, 2007. [8] B. Carterette, J. Allan, and R. K. Sitaraman.",
                "Minimal test collections for retrieval evaluation.",
                "In Proceedings of SIGIR, pages 268-275, 2006. [9] B. Carterette and D. I. Petkova.",
                "Learning a ranking from pairwise preferences.",
                "In Proceedings of SIGIR, 2006. [10] R. T. Clemen and R. L. Winkler.",
                "Unanimity and compromise among probability forecasters.",
                "Management Science, 36(7):767-779, July 1990. [11] G. V. Cormack, C. R. Palmer, and C. L. Clarke.",
                "Efficient Construction of Large Test Collections.",
                "In Proceedings of SIGIR, pages 282-289, 1998. [12] A. Gelman, J.",
                "B. Carlin, H. S. Stern, and D. B. Rubin.",
                "Bayesian Data Analysis.",
                "Chapman & Hall/CRC, 2004. [13] E. T. Jaynes.",
                "Probability Theory: The Logic of Science.",
                "Cambridge University Press, 2003. [14] R. Manmatha and H. Sever.",
                "A Formal Approach to Score Normalization for Metasearch.",
                "In Proceedings of HLT, pages 88-93, 2002. [15] I. J. Myung, S. Ramamoorti, and J. Andrew D. Baily.",
                "Maximum entropy aggregation of expert predictions.",
                "Management Science, 42(10):1420-1436, October 1996. [16] J. Platt.",
                "Probabilistic outputs for support vector machines and comparison to regularized likelihood methods. pages 61-74, 2000. [17] M. Sanderson and H. Joho.",
                "Forming test collections with no system pooling.",
                "In Proceedings of the 27th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 33-40, 2004. [18] I. Soboroff.",
                "Dynamic test collections: measuring search effectiveness on the live web.",
                "In Proceedings of SIGIR, pages 276-283, 2006. [19] K. Sparck Jones and C. J. van Rijsbergen.",
                "Information Retrieval Test Collections.",
                "Journal of Documentation, 32(1):59-75, 1976. [20] E. M. Voorhees and D. K. Harman, editors.",
                "TREC: Experiment and Evaluation in Information Retrieval.",
                "MIT Press, 2005. [21] J. Zobel.",
                "How Reliable are the Results of Large-Scale Information Retrieval Experiments?",
                "In Proceedings of SIGIR, pages 307-314, 1998."
            ],
            "original_annotated_samples": [
                "This random variable has an expectation, a <br>variance</br>, confidence intervals, and a certain probability of being less than or equal to a given value.",
                "Summing over all possibilities, we can compute expectation and <br>variance</br>: E[AP] ≈ 1 pi aiipi + j>i aij pipj V ar[AP] ≈ 1 ( pi)2 n i a2 iipiqi + j>i a2 ijpipj(1 − pipj) + i=j 2aiiaijpipj(1 − pi) + k>j=i 2aijaikpipjpk(1 − pi) AP asymptotically converges to a normal distribution with expectation and <br>variance</br> as defined above.1 For our comparative evaluation task we are interested in the sign of the difference in two average precisions: ΔAP = AP1 − AP2.",
                "MAP is also normally distributed; its expectation and <br>variance</br> are: EMAP = 1 T t∈T E[APt] (1) VMAP = 1 T2 t∈T V ar[APt] ΔMAP = MAP1 − MAP2 Confidence can then be estimated by calculating the expectation and <br>variance</br> and using the normal density function to find P(ΔMAP < 0). 2.2 Confidence and Robustness Having defined confidence, we turn back to the issue of trust in confidence estimates, and show how it ties into the robustness of the collection to missing judgments. 1 These are actually approximations to the true expectation and variance, but the error is a negligible O(n2−n ).",
                "There is no reason that we could not go deeper, but calculating <br>variance</br> is O(n3 ) and thus very timeconsuming.",
                "Algorithm 1 (MTC) Given two ranked lists and confidence level α, predict the sign of ΔMAP. 1: pi ← 0.5 for all documents i 2: while P(ΔMAP < 0) < α do 3: calculate weight wi for all unjudged documents i (see Carterette et al. [8] for details) 4: j ← argmaxiwi 5: xj ← 1 if document j is relevant, 0 otherwise 6: pj ← xj 7: end while the search to uniform priors with relatively high <br>variance</br>."
            ],
            "translated_annotated_samples": [
                "Esta variable aleatoria tiene una esperanza, una <br>varianza</br>, intervalos de confianza y una cierta probabilidad de ser menor o igual a un valor dado.",
                "Sumando sobre todas las posibilidades, podemos calcular la esperanza y la <br>varianza</br>: E[AP] ≈ 1 pi aiipi + j>i aij pipj V ar[AP] ≈ 1 ( pi)2 n i a2 iipiqi + j>i a2 ijpipj(1 − pipj) + i=j 2aiiaijpipj(1 − pi) + k>j=i 2aijaikpipjpk(1 − pi) AP converge asintóticamente a una distribución normal con la esperanza y la <br>varianza</br> definidas anteriormente. Para nuestra tarea de evaluación comparativa, estamos interesados en el signo de la diferencia en dos precisiones promedio: ΔAP = AP1 − AP2.",
                "MAP también se distribuye normalmente; su esperanza y <br>varianza</br> son: EMAP = 1 T t∈T E[APt] (1) VMAP = 1 T2 t∈T V ar[APt] ΔMAP = MAP1 − MAP2 La confianza puede entonces estimarse calculando la esperanza y <br>varianza</br> y utilizando la función de densidad normal para encontrar P(ΔMAP < 0). 2.2 Confianza y Robustez Habiendo definido la confianza, volvemos al tema de la confianza en las estimaciones de confianza, y mostramos cómo se relaciona con la robustez de la colección ante juicios faltantes. 1 Estas son en realidad aproximaciones a la verdadera esperanza y varianza, pero el error es despreciable O(n2−n ).",
                "No hay razón por la que no pudiéramos profundizar más, pero el cálculo de la <br>varianza</br> es O(n3) y, por lo tanto, muy consumidor de tiempo.",
                "Algoritmo 1 (MTC) Dadas dos listas clasificadas y un nivel de confianza α, predecir el signo de ΔMAP. 1: pi ← 0.5 para todos los documentos i 2: mientras P(ΔMAP < 0) < α hacer 3: calcular el peso wi para todos los documentos i no clasificados (ver Carterette et al. [8] para más detalles) 4: j ← argmaxiwi 5: xj ← 1 si el documento j es relevante, 0 en caso contrario 6: pj ← xj 7: fin mientras la búsqueda de priors uniformes con una <br>varianza</br> relativamente alta."
            ],
            "translated_text": "Colecciones de pruebas robustas para la evaluación de recuperación Ben Carterette Centro de Recuperación de Información Inteligente Departamento de Ciencias de la Computación Universidad de Massachusetts Amherst Amherst, MA 01003 carteret@cs.umass.edu RESUMEN Los métodos de bajo costo para adquirir juicios de relevancia pueden ser una bendición para los investigadores que necesitan evaluar nuevas tareas o temas de recuperación pero no tienen los recursos para hacer miles de juicios. Si bien estos juicios son muy útiles para una evaluación única, no está claro que se puedan confiar al ser reutilizados para evaluar nuevos sistemas. En este trabajo, definimos formalmente lo que significa que las evaluaciones sean reutilizables: la confianza en una evaluación de nuevos sistemas puede ser evaluada con precisión a partir de un conjunto existente de juicios de relevancia. Luego presentamos un método para aumentar un conjunto de juicios de relevancia con estimaciones de relevancia que no requieren esfuerzo adicional del evaluador. Usar este método garantiza prácticamente la reutilización: con tan solo cinco juicios por tema tomados de solo dos sistemas, podemos evaluar de manera confiable un conjunto más grande de diez sistemas. Incluso los conjuntos más pequeños de juicios pueden ser útiles para la evaluación de nuevos sistemas. Categorías y Descriptores de Asignaturas: H.3 Almacenamiento y Recuperación de Información; H.3.4 Sistemas y Software: Evaluación del Rendimiento Términos Generales: Experimentación, Medición, Confiabilidad 1. INTRODUCCIÓN Imagina a un investigador de recuperación de información que ha inventado una nueva tarea de recuperación. Ella ha construido un sistema para realizar la tarea y quiere evaluarlo. Dado que la tarea es nueva, es poco probable que existan juicios de relevancia vigentes. Ella no tiene el tiempo ni los recursos para juzgar cada documento, ni siquiera cada documento recuperado. Ella solo puede juzgar los documentos que parecen ser los más informativos y detenerse cuando tenga un grado razonable de confianza en sus conclusiones. Pero ¿qué sucede cuando ella desarrolla un nuevo sistema y necesita evaluarlo? ¿Otro grupo de investigación decide implementar un sistema para realizar la tarea? ¿Pueden reutilizar de manera confiable los juicios originales? ¿Pueden evaluar sin más juicios de relevancia? La evaluación es un aspecto importante de la investigación en recuperación de información, pero es solo un problema parcialmente resuelto: para la mayoría de las tareas de recuperación, es imposible juzgar la relevancia de cada documento; simplemente hay demasiados. La solución utilizada por NIST en TREC (Text REtrieval Conference) es el método de agrupación [19, 20]: todos los sistemas competidores contribuyen con N documentos a un grupo, y cada documento en ese grupo es evaluado. Este método crea grandes conjuntos de juicios que son reutilizables para entrenar o evaluar nuevos sistemas que no contribuyeron al conjunto [21]. Esta solución no es adecuada para nuestro investigador hipotético. El método de agrupación proporciona miles de juicios de relevancia, pero requiere muchas horas de tiempo de anotadores (remunerados). Como resultado, ha habido una serie de artículos recientes sobre la reducción del esfuerzo del anotador en la producción de colecciones de pruebas: Cormack et al. [11], Zobel [21], Sanderson y Joho [17], Carterette et al. [8], y Aslam et al. [4], entre otros. Como veremos, los juicios que estos métodos producen pueden sesgar significativamente la evaluación de un nuevo conjunto de sistemas. ¿Volviendo a nuestra investigadora hipotética, puede reutilizar sus juicios de relevancia? Primero debemos definir formalmente lo que significa ser reutilizable. En trabajos anteriores, la reutilización se ha probado simplemente evaluando la precisión de un conjunto de juicios de relevancia al evaluar sistemas no vistos. Si bien podemos decir que tuvo razón el 75% del tiempo, o que tuvo una correlación de rango de 0.8, estos números no tienen ningún poder predictivo: no nos dicen qué sistemas es probable que estén equivocados ni cuán seguros deberíamos estar en uno en particular. Necesitamos una definición más cuidadosa de reutilización. Específicamente, la cuestión de la reutilización no es qué tan precisamente podemos evaluar los nuevos sistemas. Un adversario malicioso siempre puede producir una nueva lista clasificada que no haya recuperado ninguno de los documentos evaluados. La verdadera pregunta es cuánta confianza tenemos en nuestras evaluaciones, y, más importante aún, si podemos confiar en nuestras estimaciones de confianza. Aunque la confianza no sea alta, siempre y cuando podamos confiar en ello, podemos identificar qué sistemas necesitan más juicios para aumentar la confianza. Cualquier conjunto de juicios, por pequeño que sea, se vuelve reutilizable en cierta medida. Pequeñas colecciones de pruebas reutilizables podrían tener un gran impacto en la investigación de recuperación de información. Los grupos de investigación podrían compartir las evaluaciones de relevancia que han realizado internamente para estudios piloto, nuevas tareas o nuevos temas. La cantidad de datos disponible para los investigadores crecería de forma exponencial con el tiempo. EVALUACIÓN ROBUSTA Anteriormente dimos una definición intuitiva de reutilización: una colección es reutilizable si podemos confiar en nuestras estimaciones de confianza en una evaluación. Con eso queremos decir que si hemos realizado algunos juicios de relevancia y tenemos, por ejemplo, un 75% de confianza en que el sistema A es mejor que el sistema B, nos gustaría que no haya más del 25% de posibilidad de que nuestra evaluación de la calidad relativa de los sistemas cambie a medida que continuamos evaluando documentos. Nuestra evaluación debe ser robusta ante juicios faltantes. En nuestro trabajo anterior, definimos la confianza como la probabilidad de que la diferencia en una medida de evaluación calculada para dos sistemas sea menor que cero [8]. Esta noción de confianza se define en el contexto de una tarea de evaluación particular que llamamos evaluación comparativa: determinar el signo de la diferencia en una medida de evaluación. Otras tareas de evaluación podrían ser definidas; estimar la magnitud de la diferencia o los valores de las medidas mismas son ejemplos que implican diferentes nociones de confianza. Por lo tanto, consideramos la confianza como una estimación de probabilidad. Una de las preguntas que debemos hacer acerca de una estimación de probabilidad es qué significa. ¿Qué significa tener un 75% de confianza de que el sistema A es mejor que el sistema B? Como se describe arriba, queremos que signifique que si seguimos evaluando documentos, solo habrá un 25% de probabilidad de que nuestra evaluación cambie. Si esto es lo que significa, podemos confiar en las estimaciones de confianza. ¿Pero sabemos que tiene ese significado? Nuestro cálculo de confianza se basó en una suposición sobre la probabilidad de relevancia de los documentos no evaluados, específicamente que cada documento no evaluado tenía la misma probabilidad de ser relevante o no relevante. Esta suposición casi con seguridad no es realista en la mayoría de las aplicaciones de IR. Resulta que es esta suposición la que determina si se pueden confiar en las estimaciones de confianza. Antes de profundizar en esto, definimos formalmente la confianza. 2.1 Estimación de la confianza La precisión promedio (AP) es una métrica de evaluación estándar que captura tanto la capacidad de un sistema para clasificar documentos relevantes de manera alta (precisión) como su capacidad para recuperar documentos relevantes (recuperación). Normalmente se escribe como la precisión media en los rangos de documentos relevantes: AP = 1 |R| i∈R prec@r(i) donde R es el conjunto de documentos relevantes y r(i) es el rango del documento i. Sea Xi una variable aleatoria que indica la relevancia del documento i. Si los documentos están ordenados por rango, podemos expresar la precisión como prec@i = 1/i i j=1 Xj. La precisión promedio se convierte entonces en la ecuación cuadrática AP = 1 Xi n i=1 Xi/i i j=1 Xj = 1 Xi n i=1 j≥i aijXiXj donde aij = 1/ max{r(i), r(j)}. Usar aij en lugar de 1/i nos permite numerar los documentos de forma arbitraria. Para ver por qué esto es cierto, considera un ejemplo simple: una lista de 3 documentos con los documentos relevantes B, C en los rangos 1 y 3 y el documento no relevante A en el rango 2. La precisión promedio será 1 2 (1 1 x2 B+ 1 2 xBxA+ 1 3 xBxC + 1 2 x2 A+ 1 3 xAxC + 1 3 x2 C) = 1 2 1 + 2 3 porque xA = 0, xB = 1, xC = 1. Aunque el orden B, A, C es diferente de la etiqueta A, B, C, no afecta al cálculo. Ahora podemos ver que la precisión promedio en sí misma es una variable aleatoria con una distribución sobre todas las posibles asignaciones de relevancia a todos los documentos. Esta variable aleatoria tiene una esperanza, una <br>varianza</br>, intervalos de confianza y una cierta probabilidad de ser menor o igual a un valor dado. Todos estos dependen de la probabilidad de que el documento i sea relevante: pi = p(Xi = 1). Supongamos que en nuestro ejemplo anterior no conocemos las valoraciones de relevancia, pero creemos que pA = 0.4, pB = 0.8, pC = 0.7. Podemos entonces calcular, por ejemplo. P(AP = 0) = 0.2 · 0.6 · 0.3 = 0.036, o P(AP = 1 2 ) = 0.2 · 0.4 · 0.7 = 0.056. \n\nLa traducción al español sería: P(AP = 0) = 0.2 · 0.6 · 0.3 = 0.036, o P(AP = 1 2 ) = 0.2 · 0.4 · 0.7 = 0.056. Sumando sobre todas las posibilidades, podemos calcular la esperanza y la <br>varianza</br>: E[AP] ≈ 1 pi aiipi + j>i aij pipj V ar[AP] ≈ 1 ( pi)2 n i a2 iipiqi + j>i a2 ijpipj(1 − pipj) + i=j 2aiiaijpipj(1 − pi) + k>j=i 2aijaikpipjpk(1 − pi) AP converge asintóticamente a una distribución normal con la esperanza y la <br>varianza</br> definidas anteriormente. Para nuestra tarea de evaluación comparativa, estamos interesados en el signo de la diferencia en dos precisiones promedio: ΔAP = AP1 − AP2. Como mostramos en nuestro trabajo anterior, ΔAP tiene una forma cerrada cuando los documentos están ordenados arbitrariamente: ΔAP = 1 Xi n i=1 j≥i cij XiXj cij = aij − bij donde bij está definido de manera análoga a aij para la segunda clasificación. Dado que AP es normal, ΔAP también es normal, lo que significa que podemos usar la función de densidad acumulativa normal para determinar la confianza de que una diferencia en AP sea menor que cero. Dado que los temas son independientes, podemos extender fácilmente esto para referirnos a la precisión promedio (MAP). MAP también se distribuye normalmente; su esperanza y <br>varianza</br> son: EMAP = 1 T t∈T E[APt] (1) VMAP = 1 T2 t∈T V ar[APt] ΔMAP = MAP1 − MAP2 La confianza puede entonces estimarse calculando la esperanza y <br>varianza</br> y utilizando la función de densidad normal para encontrar P(ΔMAP < 0). 2.2 Confianza y Robustez Habiendo definido la confianza, volvemos al tema de la confianza en las estimaciones de confianza, y mostramos cómo se relaciona con la robustez de la colección ante juicios faltantes. 1 Estas son en realidad aproximaciones a la verdadera esperanza y varianza, pero el error es despreciable O(n2−n ). Sea Z el conjunto de todos los pares de resultados clasificados para un conjunto común de temas. Supongamos que tenemos un conjunto de m juicios de relevancia xm = {x1, x2, ..., xm} (usando x minúscula en lugar de X mayúscula para distinguir entre documentos juzgados y no juzgados); estos son los juicios contra los cuales calculamos la confianza. Sea Zα el subconjunto de pares en Z para los cuales predecimos que ΔMAP = −1 con confianza α dadas las evaluaciones xm. Para que las estimaciones de confianza sean precisas, necesitamos que al menos α · |Zα| de estos pares realmente tengan ΔMAP = −1 después de haber evaluado cada documento. Si lo hacen, podemos confiar en las estimaciones de confianza; nuestra evaluación será robusta ante juicios faltantes. Si nuestras estimaciones de confianza se basan en suposiciones poco realistas, no podemos esperar que sean precisas. Las suposiciones en las que se basan son las probabilidades de relevancia pi. Necesitamos que esto sea realista. Sostenemos que la mejor distribución posible de relevancia p(Xi) es aquella que explica todos los datos (todas las observaciones realizadas sobre los sistemas de recuperación) al mismo tiempo que no hace suposiciones infundadas. Esto se conoce como el principio de máxima entropía [13]. La entropía de una variable aleatoria X con distribución p(X) se define como H(p) = − i p(X = i) log p(X = i). Esto ha encontrado una amplia variedad de usos en la informática y la recuperación de información. La distribución de entropía máxima es aquella que maximiza H. Esta distribución es única y tiene una forma exponencial. El siguiente teorema muestra la utilidad de una distribución de entropía máxima para la relevancia al estimar la confianza. Teorema 1. Si p(Xn |I, xm ) = argmaxpH(p), las estimaciones de confianza serán precisas, donde xm es el conjunto de juicios de relevancia definidos anteriormente, Xn es el conjunto completo de documentos cuya relevancia deseamos estimar, e I es alguna información sobre los documentos (no especificada hasta ahora). Por el momento prescindimos de la prueba, pero es bastante simple. Esto indica que cuanto mejores sean las estimaciones de relevancia, más precisa será la evaluación. La tarea de crear una colección de pruebas reutilizable se convierte entonces en la tarea de estimar la relevancia de los documentos no evaluados. El teorema y su demostración no dicen absolutamente nada sobre la métrica de evaluación. Las estimaciones de probabilidad son completamente independientes de la medida en la que estamos interesados. Esto significa que los mismos cálculos de probabilidad pueden informarnos sobre la precisión promedio, así como sobre la precisión, la recuperación, el bpref, etc. Además, podríamos asumir que la relevancia de los documentos i y j es independiente y lograr el mismo resultado, lo cual afirmamos como un corolario: Corolario 1. Si p(Xi|I, xm) = argmaxpH(p), las estimaciones de confianza serán precisas. La tarea, por lo tanto, consiste en la imputación de los valores faltantes de relevancia. El teorema implica que cuanto más nos acerquemos a la distribución de entropía máxima relevante, más nos acercaremos a la robustez. 3. PREDICIENDO LA RELEVANCIA En nuestra declaración del Teorema 1, dejamos sin especificar la naturaleza de la información I. Una de las ventajas de nuestras estimaciones de confianza es que admiten información de una amplia variedad de fuentes; básicamente cualquier cosa que pueda ser modelada puede ser utilizada como información para predecir la relevancia. Una fuente natural de información son los propios sistemas de recuperación: cómo clasificaron los documentos evaluados, con qué frecuencia no lograron clasificar documentos relevantes, cómo se desempeñan en diferentes temas, y así sucesivamente. Si tratamos cada sistema como un experto en recuperación de información que proporciona una opinión sobre la relevancia de cada documento, el problema se convierte en uno de agregación de opiniones de expertos. Esto es similar al problema de metabúsqueda o fusión de datos en el que la tarea consiste en tomar k sistemas de entrada y fusionarlos en un solo ranking. Aslam et al. [3] identificaron previamente una conexión entre la evaluación y la metabúsqueda. Nuestro problema tiene dos diferencias clave: 1. Necesitamos explícitamente probabilidades de relevancia que podamos insertar en la Ecuación 1; los algoritmos de metabusqueda no tienen tal requisito. Estamos acumulando juicios de relevancia a medida que avanzamos con la evaluación y somos capaces de reestimar la relevancia dada cada nuevo juicio. A la luz de lo anterior, introducimos un modelo probabilístico para la combinación de expertos. 3.1 Un Modelo para la Agregación de Opiniones de Expertos Supongamos que cada experto j proporciona una probabilidad de relevancia qij = pj(Xi = 1). La información sobre la relevancia del documento i será entonces el conjunto de k opiniones de expertos I = qi = (qi1, qi2, · · · , qik). La distribución de probabilidad que deseamos encontrar es aquella que maximiza la entropía de pi = p(Xi = 1|qi). Resulta que encontrar el modelo de entropía máxima es equivalente a encontrar los parámetros que maximizan la verosimilitud [5]. Blower [6] muestra explícitamente que encontrar el modelo de entropía máxima para una variable binaria es equivalente a resolver una regresión logística. Entonces pi = p(Xi = 1|qi) = exp k j=1 λjqij 1 + exp k j=1 λj qij (2) donde λ1, · · · , λk son los parámetros de regresión. Incluimos una distribución beta a priori para p(λj) con parámetros α, β. Esto se puede ver como un tipo de suavizado para tener en cuenta el hecho de que los datos de entrenamiento están altamente sesgados. Este modelo tiene la ventaja de incluir la dependencia estadística entre los expertos. Un modelo de la misma forma fue demostrado por Clemen & Winkler como el mejor para agregar probabilidades de expertos [10]. Un enfoque similar motivado por la entropía máxima se ha utilizado para la agregación de expertos [15]. Aslam & Montague [1] utilizaron un modelo similar para la metabúsqueda, pero asumieron independencia entre los expertos. ¿De dónde vienen los qij? Utilizar puntuaciones sin procesar y no calibradas como predictores no funcionará porque las distribuciones de puntuaciones varían demasiado entre los temas. Un clasificador de modelado de lenguaje, por ejemplo, suele otorgar una puntuación mucho más alta al documento recuperado en la parte superior para una consulta corta que al documento recuperado en la parte superior para una consulta larga. Podríamos entrenar un modelo predictivo separado para cada tema, pero eso no aprovecha toda la información que tenemos: es posible que solo tengamos un puñado de juicios para un tema, no suficientes para entrenar un modelo con confianza. Además, parece razonable asumir que si un experto hace buenas predicciones para un tema, también hará buenas predicciones para otros temas. Podríamos usar un modelo jerárquico [12], pero eso no se generalizará a temas no vistos. En cambio, calibraremos las puntuaciones de cada experto de forma individual para que las puntuaciones puedan ser comparadas tanto dentro de un tema como entre temas. Por lo tanto, nuestro modelo tiene en cuenta no solo la dependencia entre expertos, sino también la dependencia entre el rendimiento de los expertos en diferentes tareas (temas). 3.2 Calibración de Expertos Cada experto nos proporciona una puntuación y un rango para cada documento. Necesitamos convertir estos en probabilidades. Un método como el utilizado por Manmatha et al. [14] podría ser utilizado para convertir puntuaciones en probabilidades de relevancia. El método de preferencia por pares de Carterette & Petkova [9] también podría ser utilizado, interpretando la clasificación de un documento sobre otro como una expresión de preferencia. Que q∗ ij sea la probabilidad autoinformada por el experto j de que el documento i sea relevante. De manera intuitiva, parece claro que q∗ ij debería disminuir con el rango, y debería ser cero si el documento i no está clasificado (el experto no consideró que fuera relevante). El modelo de preferencia por pares puede manejar fácilmente estos dos requisitos, por lo que lo utilizaremos. Que θrj (i) sea el coeficiente de relevancia del documento en la posición rj(i). Queremos encontrar los θs que maximizan la función de verosimilitud: Ljt(Θ) = rj (i)<rj (k) exp(θrj (i) − θrj (k)) 1 + exp(θrj (i) − θrj (k)) Incluimos nuevamente una distribución beta previa en p(θrj(i)) con parámetros |Rt| + 1 y |Nt| + 1, el tamaño de los conjuntos de documentos relevantes y no relevantes juzgados respectivamente. El uso de estos parámetros previos garantiza que las probabilidades resultantes se concentren alrededor de la proporción de documentos relevantes que se han descubierto para el tema t. Esto significa que las estimaciones de probabilidad disminuyen por rango y son más altas para los temas que tienen más documentos relevantes. Después de encontrar el Θ que maximiza la verosimilitud, tenemos q∗ ij = exp(θrj (i)) 1+exp(θrj (i)) . Definimos θ∞ = −∞, de modo que la probabilidad de que un documento no clasificado sea relevante es 0. Dado que q∗ ij se basa en la posición en la que se recupera un documento en lugar de en la identidad del documento en sí, las probabilidades son idénticas de un experto a otro, por ejemplo, si el experto E colocó el documento A en la posición 1, y el experto D colocó el documento B en la posición 1, tendremos que q∗ AE = q∗ BD. Por lo tanto, solo tenemos que resolver esto una vez para cada tema. El modelo anterior proporciona probabilidades independientes del tema para cada documento. Pero supongamos que un experto que informa una probabilidad del 90% solo tiene razón el 50% del tiempo. Su opinión debería ser descartada basándose en su desempeño observado. Específicamente, queremos aprender una función de calibración qij = Cj(q∗ ij) que garantice que las probabilidades predichas estén ajustadas a la capacidad de los expertos para recuperar documentos relevantes dados los juicios que se han realizado hasta este momento. El método de calibración SVM de Platts [16] ajusta una función sigmoide entre q∗ ij y las evaluaciones de relevancia para obtener qij = Cj (q∗ ij) = exp(Aj +Bjq∗ ij ) 1+exp(Aj +Bj q∗ ij ) . Dado que q∗ ij es independiente del tema, solo necesitamos aprender una función de calibración para cada experto. Una vez que tenemos la función de calibración, se aplica para ajustar las predicciones de los expertos a su rendimiento real. Las probabilidades calibradas se introducen en el modelo (2) para encontrar las probabilidades de los documentos. Figura 1: Diagrama conceptual de nuestro modelo de agregación. Los expertos E1 y E2 han clasificado los documentos A, B, C para el tema T1 y los documentos D, E, F para el tema T2. El primer paso es obtener q∗ ij. A continuación se realiza la calibración al rendimiento real para encontrar qij. Finalmente obtenemos pi = p(Xi = 1|qi1, qi2), · · · . 3.3 Resumen del modelo Nuestro modelo tiene tres componentes que difieren en los datos que toman como entrada y en lo que producen como salida. Se muestra un diagrama conceptual en la Figura 1. 1. rangos → probabilidades (por sistema por tema). Esto nos da q∗ ij, la probabilidad autoinformada por el experto j de la relevancia del documento i. Esto es no supervisado; no requiere datos etiquetados (aunque si los tenemos, los usamos para establecer parámetros previos). 2. probabilidades → probabilidades calibradas (por sistema). Esto nos da qij = Cj (q∗ ij), la probabilidad calibrada por el experto j de la relevancia del documento i. Esto es semisupervisado; tenemos juicios de relevancia en algunos rangos que usamos para imputar la probabilidad de relevancia en otros rangos. 3. probabilidades calibradas → probabilidades de documentos. Esto nos da pi = p(Xi = 1|qi), la probabilidad de relevancia del documento i dada las probabilidades de experto calibradas qij. Esto es supervisado; aprendemos coeficientes de un conjunto de documentos juzgados y los utilizamos para estimar la relevancia de los documentos no juzgados. Aunque el modelo parece bastante complejo, en realidad son solo tres aplicaciones sucesivas de regresión logística. Por lo tanto, se puede implementar en un lenguaje de programación estadística como R en unas pocas líneas de código. El uso de priors beta (conjugados) garantiza que no sean necesarios métodos computacionales costosos como MCMC [12], por lo que el modelo se entrena y aplica lo suficientemente rápido como para ser utilizado en línea. Nuestro código está disponible en http://ciir.cs.umass.edu/~carteret/. 4. EXPERIMENTOS Se están considerando tres hipótesis. El primero, y más importante, es que el uso de nuestro modelo de agregación de expertos para predecir la relevancia produce colecciones de pruebas lo suficientemente robustas como para ser reutilizables; es decir, podemos confiar en las estimaciones de confianza cuando evaluamos sistemas que no contribuyeron con ningún juicio al conjunto. Las otras dos hipótesis se relacionan con la mejora que observamos al utilizar estimaciones de relevancia mejores que las que utilizamos en nuestro trabajo anterior [8]. Estos son (a) que se necesitan menos temas de relevancia, número de ejecuciones, número de documentos evaluados y número de documentos relevantes para cada uno de nuestros conjuntos de datos y (b) la precisión de las predicciones es mayor que si simplemente asumiéramos que pi = 0.5 para todos los documentos no evaluados. 4.1 Datos Obtuvimos ejecuciones completas de búsqueda ad-hoc presentadas en las TRECs 3 a 8. Cada ejecución clasifica como máximo 1000 documentos para 50 temas (49 temas para TREC 4). Además, obtuvimos todas las ejecuciones de la pista Web de TREC 13, la pista Robust2 de TREC 14 y la pista Terabyte (ad-hoc) de TREC 14. Estas son las pistas que han reemplazado la pista ad-hoc desde su final en 1999. Las estadísticas se muestran en la Tabla 1. Reservamos el conjunto TREC 4 (ad-hoc 95) para entrenamiento, los TRECs 3 y 5-8 (ad-hoc 94 y 96-99) para pruebas principales, y los conjuntos restantes para pruebas adicionales. Utilizamos los archivos qrels recopilados por NIST como verdad. El número de juicios de relevancia realizados y documentos relevantes encontrados para cada pista se enumeran en la Tabla 1. Por razones computacionales, truncamos las listas clasificadas en 100 documentos. No hay razón por la que no pudiéramos profundizar más, pero el cálculo de la <br>varianza</br> es O(n3) y, por lo tanto, muy consumidor de tiempo. Debido a la naturaleza del rango recíproco de AP, no perdemos mucha información al truncar en el rango 100. 4.2 Algoritmos Compararemos tres algoritmos para adquirir juicios de relevancia. La línea base es una variación del agrupamiento de TREC que llamaremos agrupamiento incremental (IP). Este algoritmo toma un número k como entrada y presenta los primeros k documentos en orden de rango (sin tener en cuenta el tema) para ser evaluados. No estima la relevancia de los documentos no evaluados; simplemente asume que cualquier documento no evaluado no es relevante. El segundo algoritmo es el presentado en Carterette et al. [8] (Algoritmo 1). Los documentos se seleccionan en función de lo interesantes que sean para determinar si existe una diferencia en la precisión media promedio. Para este enfoque pi = 0.5 para todos los i; no hay estimación de probabilidades. Llamaremos a esto MTC por colección de pruebas mínimas. El tercer algoritmo aumenta MTC con estimaciones actualizadas de las probabilidades de relevancia. Llamaremos a esto RTC por colección de pruebas robusta. Es idéntico al Algoritmo 1, excepto que cada décima iteración estimamos pi para todos los documentos no evaluados i utilizando el modelo de agregación de expertos de la Sección 3. RTC tiene parámetros de suavizado (distribución previa) que deben ser configurados. Entrenamos utilizando el conjunto ad-hoc 95. Limitamos 2 robustos aquí significa recuperación robusta; esto es diferente de nuestro objetivo de evaluación robusta. Algoritmo 1 (MTC) Dadas dos listas clasificadas y un nivel de confianza α, predecir el signo de ΔMAP. 1: pi ← 0.5 para todos los documentos i 2: mientras P(ΔMAP < 0) < α hacer 3: calcular el peso wi para todos los documentos i no clasificados (ver Carterette et al. [8] para más detalles) 4: j ← argmaxiwi 5: xj ← 1 si el documento j es relevante, 0 en caso contrario 6: pj ← xj 7: fin mientras la búsqueda de priors uniformes con una <br>varianza</br> relativamente alta. ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "distribution of relevance": {
            "translated_key": "distribución de entropía máxima relevante",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Robust Test Collections for Retrieval Evaluation Ben Carterette Center for Intelligent Information Retrieval Computer Science Department University of Massachusetts Amherst Amherst, MA 01003 carteret@cs.umass.edu ABSTRACT Low-cost methods for acquiring relevance judgments can be a boon to researchers who need to evaluate new retrieval tasks or topics but do not have the resources to make thousands of judgments.",
                "While these judgments are very useful for a one-time evaluation, it is not clear that they can be trusted when re-used to evaluate new systems.",
                "In this work, we formally define what it means for judgments to be reusable: the confidence in an evaluation of new systems can be accurately assessed from an existing set of relevance judgments.",
                "We then present a method for augmenting a set of relevance judgments with relevance estimates that require no additional assessor effort.",
                "Using this method practically guarantees reusability: with as few as five judgments per topic taken from only two systems, we can reliably evaluate a larger set of ten systems.",
                "Even the smallest sets of judgments can be useful for evaluation of new systems.",
                "Categories and Subject Descriptors: H.3 Information Storage and Retrieval; H.3.4 Systems and Software: Performance Evaluation General Terms: Experimentation, Measurement, Reliability 1.",
                "INTRODUCTION Consider an information retrieval researcher who has invented a new retrieval task.",
                "She has built a system to perform the task and wants to evaluate it.",
                "Since the task is new, it is unlikely that there are any extant relevance judgments.",
                "She does not have the time or resources to judge every document, or even every retrieved document.",
                "She can only judge the documents that seem to be the most informative and stop when she has a reasonable degree of confidence in her conclusions.",
                "But what happens when she develops a new system and needs to evaluate it?",
                "Or another research group decides to implement a system to perform the task?",
                "Can they reliably reuse the original judgments?",
                "Can they evaluate without more relevance judgments?",
                "Evaluation is an important aspect of information retrieval research, but it is only a semi-solved problem: for most retrieval tasks, it is impossible to judge the relevance of every document; there are simply too many of them.",
                "The solution used by NIST at TREC (Text REtrieval Conference) is the pooling method [19, 20]: all competing systems contribute N documents to a pool, and every document in that pool is judged.",
                "This method creates large sets of judgments that are reusable for training or evaluating new systems that did not contribute to the pool [21].",
                "This solution is not adequate for our hypothetical researcher.",
                "The pooling method gives thousands of relevance judgments, but it requires many hours of (paid) annotator time.",
                "As a result, there have been a slew of recent papers on reducing annotator effort in producing test collections: Cormack et al. [11], Zobel [21], Sanderson and Joho [17], Carterette et al. [8], and Aslam et al. [4], among others.",
                "As we will see, the judgments these methods produce can significantly bias the evaluation of a new set of systems.",
                "Returning to our hypothetical resesarcher, can she reuse her relevance judgments?",
                "First we must formally define what it means to be reusable.",
                "In previous work, reusability has been tested by simply assessing the accuracy of a set of relevance judgments at evaluating unseen systems.",
                "While we can say that it was right 75% of the time, or that it had a rank correlation of 0.8, these numbers do not have any predictive power: they do not tell us which systems are likely to be wrong or how confident we should be in any one.",
                "We need a more careful definition of reusability.",
                "Specifically, the question of reusability is not how accurately we can evaluate new systems.",
                "A malicious adversary can always produce a new ranked list that has not retrieved any of the judged documents.",
                "The real question is how much confidence we have in our evaluations, and, more importantly, whether we can trust our estimates of confidence.",
                "Even if confidence is not high, as long as we can trust it, we can identify which systems need more judgments in order to increase confidence.",
                "Any set of judgments, no matter how small, becomes reusable to some degree.",
                "Small, reusable test collections could have a huge impact on information retrieval research.",
                "Research groups would be able to share the relevance judgments they have done in-house for pilot studies, new tasks, or new topics.",
                "The amount of data available to researchers would grow exponentially over time. 2.",
                "ROBUST EVALUATION Above we gave an intuitive definition of reusability: a collection is reusable if we can trust our estimates of confidence in an evaluation.",
                "By that we mean that if we have made some relevance judgments and have, for example, 75% confidence that system A is better than system B, we would like there to be no more than 25% chance that our assessment of the relative quality of the systems will change as we continue to judge documents.",
                "Our evaluation should be robust to missing judgments.",
                "In our previous work, we defined confidence as the probability that the difference in an evaluation measure calculated for two systems is less than zero [8].",
                "This notion of confidence is defined in the context of a particular evaluation task that we call comparative evaluation: determining the sign of the difference in an evaluation measure.",
                "Other evaluation tasks could be defined; estimating the magnitude of the difference or the values of the measures themselves are examples that entail different notions of confidence.",
                "We therefore see confidence as a probability estimate.",
                "One of the questions we must ask about a probability estimate is what it means.",
                "What does it mean to have 75% confidence that system A is better than system B?",
                "As described above, we want it to mean that if we continue to judge documents, there will only be a 25% chance that our assessment will change.",
                "If this is what it means, we can trust the confidence estimates.",
                "But do we know it has that meaning?",
                "Our calculation of confidence rested on an assumption about the probability of relevance of unjudged documents, specifically that each unjudged document was equally likely to be relevant or nonrelevant.",
                "This assumption is almost certainly not realistic in most IR applications.",
                "As it turns out, it is this assumption that determines whether the confidence estimates can eb trusted.",
                "Before elaborating on this, we formally define confidence. 2.1 Estimating Confidence Average precision (AP) is a standard evaluation metric that captures both the ability of a system to rank relevant documents highly (precision) as well as its ability to retrieve relevant documents (recall).",
                "It is typically written as the mean precision at the ranks of relevant documents: AP = 1 |R| i∈R prec@r(i) where R is the set of relevant documents and r(i) is the rank of document i.",
                "Let Xi be a random variable indicating the relevance of document i.",
                "If documents are ordered by rank, we can express precision as prec@i = 1/i i j=1 Xj .",
                "Average precision then becomes the quadratic equation AP = 1 Xi n i=1 Xi/i i j=1 Xj = 1 Xi n i=1 j≥i aijXiXj where aij = 1/ max{r(i), r(j)}.",
                "Using aij instead of 1/i allows us to number the documents arbitrarily.",
                "To see why this is true, consider a toy example: a list of 3 documents with relevant documents B, C at ranks 1 and 3 and nonrelevant document A at rank 2.",
                "Average precision will be 1 2 (1 1 x2 B+ 1 2 xBxA+ 1 3 xBxC + 1 2 x2 A+ 1 3 xAxC + 1 3 x2 C) = 1 2 1 + 2 3 because xA = 0, xB = 1, xC = 1.",
                "Though the ordering B, A, C is different from the labeling A, B, C, it does not affect the computation.",
                "We can now see average precision itself is a random variable with a distribution over all possible assignments of relevance to all documents.",
                "This random variable has an expectation, a variance, confidence intervals, and a certain probability of being less than or equal to a given value.",
                "All of these are dependent on the probability that document i is relevant: pi = p(Xi = 1).",
                "Suppose in our previous example we do not know the relevance judgments, but we believe pA = 0.4, pB = 0.8, pC = 0.7.",
                "We can then compute e.g.",
                "P(AP = 0) = 0.2 · 0.6 · 0.3 = 0.036, or P(AP = 1 2 ) = 0.2 · 0.4 · 0.7 = 0.056.",
                "Summing over all possibilities, we can compute expectation and variance: E[AP] ≈ 1 pi aiipi + j>i aij pipj V ar[AP] ≈ 1 ( pi)2 n i a2 iipiqi + j>i a2 ijpipj(1 − pipj) + i=j 2aiiaijpipj(1 − pi) + k>j=i 2aijaikpipjpk(1 − pi) AP asymptotically converges to a normal distribution with expectation and variance as defined above.1 For our comparative evaluation task we are interested in the sign of the difference in two average precisions: ΔAP = AP1 − AP2.",
                "As we showed in our previous work, ΔAP has a closed form when documents are ordered arbitrarily: ΔAP = 1 Xi n i=1 j≥i cij XiXj cij = aij − bij where bij is defined analogously to aij for the second ranking.",
                "Since AP is normal, ΔAP is normal as well, meaning we can use the normal cumulative density function to determine the confidence that a difference in AP is less than zero.",
                "Since topics are independent, we can easily extend this to mean average precision (MAP).",
                "MAP is also normally distributed; its expectation and variance are: EMAP = 1 T t∈T E[APt] (1) VMAP = 1 T2 t∈T V ar[APt] ΔMAP = MAP1 − MAP2 Confidence can then be estimated by calculating the expectation and variance and using the normal density function to find P(ΔMAP < 0). 2.2 Confidence and Robustness Having defined confidence, we turn back to the issue of trust in confidence estimates, and show how it ties into the robustness of the collection to missing judgments. 1 These are actually approximations to the true expectation and variance, but the error is a negligible O(n2−n ).",
                "Let Z be the set of all pairs of ranked results for a common set of topics.",
                "Suppose we have a set of m relevance judgments xm = {x1, x2, ..., xm} (using small x rather than capital X to distinguish between judged and unjudged documents); these are the judgments against which we compute confidence.",
                "Let Zα be the subset of pairs in Z for which we predict that ΔMAP = −1 with confidence α given the judgments xm .",
                "For the confidence estimates to be accurate, we need at least α · |Zα| of these pairs to actually have ΔMAP = −1 after we have judged every document.",
                "If they do, we can trust the confidence estimates; our evaluation will be robust to missing judgments.",
                "If our confidence estimates are based on unrealistic assumptions, we cannot expect them to be accurate.",
                "The assumptions they are based on are the probabilities of relevance pi.",
                "We need these to be realistic.",
                "We argue that the best possible <br>distribution of relevance</br> p(Xi) is the one that explains all of the data (all of the observations made about the retrieval systems) while at the same time making no unwarranted assumptions.",
                "This is known as the principle of maximum entropy [13].",
                "The entropy of a random variable X with distribution p(X) is defined as H(p) = − i p(X = i) log p(X = i).",
                "This has found a wide array of uses in computer science and information retrieval.",
                "The maximum entropy distribution is the one that maximizes H. This distribution is unique and has an exponential form.",
                "The following theorem shows the utility of a maximum entropy distribution for relevance when estimating confidence.",
                "Theorem 1.",
                "If p(Xn |I, xm ) = argmaxpH(p), confidence estimates will be accurate. where xm is the set of relevance judgments defined above, Xn is the full set of documents that we wish to estimate the relevance of, and I is some information about the documents (unspecified as of now).",
                "We forgo the proof for the time being, but it is quite simple.",
                "This says that the better the estimates of relevance, the more accurate the evaluation.",
                "The task of creating a reusable test collection thus becomes the task of estimating the relevance of unjudged documents.",
                "The theorem and its proof say nothing whatsoever about the evaluation metric.",
                "The probability estimates are entirely indepedent of the measure we are interested in.",
                "This means the same probability estimates can tell us about average precision as well as precision, recall, bpref, etc.",
                "Furthermore, we could assume that the relevance of documents i and j is independent and achieve the same result, which we state as a corollary: Corollary 1.",
                "If p(Xi|I, xm ) = argmaxpH(p), confidence estimates will be accurate.",
                "The task therefore becomes the imputation of the missing values of relevance.",
                "The theorem implies that the closer we get to the maximum entropy <br>distribution of relevance</br>, the closer we get to robustness. 3.",
                "PREDICTING RELEVANCE In our statement of Theorem 1, we left the nature of the information I unspecified.",
                "One of the advantages of our confidence estimates is that they admit information from a wide variety of sources; essentially anything that can be modeled can be used as information for predicting relevance.",
                "A natural source of information is the retrieval systems themselves: how they ranked the judged documents, how often they failed to rank relevant documents, how they perform across topics, and so on.",
                "If we treat each system as an information retrieval expert providing an opinion about the relevance of each document, the problem becomes one of expert opinion aggregation.",
                "This is similar to the metasearch or data fusion problem in which the task is to take k input systems and merge them into a single ranking.",
                "Aslam et al. [3] previously identified a connection between evaluation and metasearch.",
                "Our problem has two key differences: 1.",
                "We explicitly need probabilities of relevance that we can plug into Eq. 1; metasearch algorithms have no such requirement. 2.",
                "We are accumulating relevance judgments as we proceed with the evaluation and are able to re-estimate relevance given each new judgment.",
                "In light of (1) above, we introduce a probabilistic model for expert combination. 3.1 A Model for Expert Opinion Aggregation Suppose that each expert j provides a probability of relevance qij = pj(Xi = 1).",
                "The information about the relevance of document i will then be the set of k expert opinions I = qi = (qi1, qi2, · · · , qik).",
                "The probability distribution we wish to find is the one that maximizes the entropy of pi = p(Xi = 1|qi).",
                "As it turns out, finding the maximum entropy model is equivalent to finding the parameters that maximize the likelihood [5].",
                "Blower [6] explicitly shows that finding the maximum entropy model for a binary variable is equivalent to solving a logistic regression.",
                "Then pi = p(Xi = 1|qi) = exp k j=1 λjqij 1 + exp k j=1 λj qij (2) where λ1, · · · , λk are the regression parameters.",
                "We include a beta prior for p(λj) with parameters α, β.",
                "This can be seen as a type of smoothing to account for the fact that the training data is highly biased.",
                "This model has the advantage of including the statistical dependence between the experts.",
                "A model of the same form was shown by Clemen & Winkler to be the best for aggregating expert probabilities [10].",
                "A similar maximumentropy-motivated approach has been used for expert aggregation [15].",
                "Aslam & Montague [1] used a similar model for metasearch, but assumed independence among experts.",
                "Where do the qij s come from?",
                "Using raw, uncalibrated scores as predictors will not work because score distributions vary too much between topics.",
                "A language modeling ranker, for instance, will typically give a much higher score to the top retrieved document for a short query than to the top retrieved document for a long query.",
                "We could train a separate predicting model for each topic, but that does not take advantage of all of the information we have: we may only have a handful of judgments for a topic, not enough to train a model to any confidence.",
                "Furthermore, it seems reasonable to assume that if an expert makes good predictions for one topic, it will make good predictions for other topics as well.",
                "We could use a hierarchical model [12], but that will not generalize to unseen topics.",
                "Instead, we will calibrate the scores of each expert individually so that scores can be compared both within topic and between topic.",
                "Thus our model takes into account not only the dependence between experts, but also the dependence between experts performances on different tasks (topics). 3.2 Calibrating Experts Each expert gives us a score and a rank for each document.",
                "We need to convert these to probabilities.",
                "A method such as the one used by Manmatha et al. [14] could be used to convert scores into probabilities of relevance.",
                "The pairwise preference method of Carterette & Petkova [9] could also be used, interpeting the ranking of one document over another as an expression of preference.",
                "Let q∗ ij be expert js self-reported probability that document i is relevant.",
                "Intuitively it seems clear that q∗ ij should decrease with rank, and it should be zero if document i is unranked (the expert did not believe it to be relevant).",
                "The pairwise preference model can handle these two requirements easily, so we will use it.",
                "Let θrj (i) be the relevance coefficient of the document at rank rj(i).",
                "We want to find the θs that maximize the likelihood function: Ljt(Θ) = rj (i)<rj (k) exp(θrj (i) − θrj (k)) 1 + exp(θrj (i) − θrj (k)) We again include a beta prior on p(θrj(i)) with parameters |Rt| + 1 and |Nt| + 1, the size of the sets of judged relevant and nonrelevant documents respectively.",
                "Using these as prior parameters ensures that the resulting probabilities will be concentrated around the ratio of relevant documents that have been discovered for topic t. This means that the probability estimates decrease by rank and are higher for topics that have more relevant documents.",
                "After finding the Θ that maximizes the likelihood, we have q∗ ij = exp(θrj (i)) 1+exp(θrj (i)) .",
                "We define θ∞ = −∞, so that the probability that an unranked document is relevant is 0.",
                "Since q∗ ij is based on the rank at which a document is retrieved rather than the identity of the document itself, the probabilities are identical from expert to expert, e.g. if expert E put document A at rank 1, and expert D put document B at rank 1, we will have q∗ AE = q∗ BD.",
                "Therefore we only have to solve this once for each topic.",
                "The above model gives topic-independent probabilities for each document.",
                "But suppose an expert who reports 90% probability is only right 50% of the time.",
                "Its opinion should be discounted based on its observed performance.",
                "Specifically, we want to learn a calibration function qij = Cj(q∗ ij) that will ensure that the predicted probabilities are tuned to the experts ability to retrieve relevant documents given the judgments that have been made to this point.",
                "Platts SVM calibration method [16] fits a sigmoid function between q∗ ij and the relevance judgments to obtain qij = Cj (q∗ ij) = exp(Aj +Bjq∗ ij ) 1+exp(Aj +Bj q∗ ij ) .",
                "Since q∗ ij is topic-independent, we only need to learn one calibration function for each expert.",
                "Once we have the calibration function, it is applied to adjust the experts predictions to their actual performance.",
                "The calibrated probabilities are plugged into model (2) to find the document probabilities.",
                "Figure 1: Conceptual diagram of our aggregation model.",
                "Experts E1, E2 have ranked documents A, B, C for topic T1 and documents D, E, F for topic T2.",
                "The first step is to obtain q∗ ij.",
                "Next is calibration to true performance to find qij .",
                "Finally we obtain pi = p(Xi = 1|qi1, qi2), · · · . 3.3 Model Summary Our model has three components that differ by the data they take as input and what they produce as output.",
                "A conceptual diagram is shown in Figure 1. 1. ranks → probabilities (per system per topic).",
                "This gives us q∗ ij, expert js self-reported probability of the relevance of document i.",
                "This is unsupervised; it requires no labeled data (though if we have some, we use it to set prior parameters). 2. probabilities → calibrated probabilities (per system).",
                "This gives us qij = Cj (q∗ ij), expert js calibrated probability of the relevance of document i.",
                "This is semisupervised; we have relevance judgments at some ranks which we use to impute the probability of relevance at other ranks. 3. calibrated probabilities → document probabilities.",
                "This gives us pi = p(Xi = 1|qi), the probability of relevance of document i given calibrated expert probabilities qij .",
                "This is supervised; we learn coefficients from a set of judged documents and use those to estimate the relevance of the unjudged documents.",
                "Although the model appears rather complex, it is really just three successive applications of logistic regression.",
                "As such, it can be implemented in a statistical programming language such as R in a few lines of code.",
                "The use of beta (conjugate) priors ensures that no expensive computational methods such as MCMC are necessary [12], so the model is trained and applied fast enough to be used on-line.",
                "Our code is available at http://ciir.cs.umass.edu/~carteret/. 4.",
                "EXPERIMENTS Three hypotheses are under consideration.",
                "The first, and most important, is that using our expert aggregation model to predict relevance produces test collections that are robust enough to be reusable; that is, we can trust the estimates of confidence when we evaluate systems that did not contribute any judgments to the pool.",
                "The other two hypotheses relate to the improvement we see by using better estimates of relevance than we did in our previous work [8].",
                "These are that (a) it takes fewer relevance track no. topics no. runs no. judged no. rel ad-hoc 94 50 40 97,319 9,805 ad-hoc 95 49 33 87,069 6,503 ad-hoc 96 50 61 133,681 5,524 ad-hoc 97 50 74 72,270 4,611 ad-hoc 98 50 103 80,345 4,674 ad-hoc 99 50 129 86,830 4,728 web 04 225 74 88,566 1,763 robust 05 50 74 37,798 6,561 terabyte 05 50 58 45,291 10,407 Table 1: Number of topics, number of runs, number of documents judged, and number found relevant for each of our data sets. judgments to reach 95% confidence and (b) the accuracy of the predictions is higher than if we were to simply assume pi = 0.5 for all unjudged documents. 4.1 Data We obtained full ad-hoc runs submitted to TRECs 3 through 8.",
                "Each run ranks at most 1000 documents for 50 topics (49 topics for TREC 4).",
                "Additionally, we obtained all runs from the Web track of TREC 13, the Robust2 track of TREC 14, and the Terabyte (ad-hoc) track of TREC 14.",
                "These are the tracks that have replaced the ad-hoc track since its end in 1999.",
                "Statistics are shown in Table 1.",
                "We set aside the TREC 4 (ad-hoc 95) set for training, TRECs 3 and 5-8 (ad-hoc 94 and 96-99) for primary testing, and the remaining sets for additional testing.",
                "We use the qrels files assembled by NIST as truth.",
                "The number of relevance judgments made and relevant documents found for each track are listed in Table 1.",
                "For computational reasons, we truncate ranked lists at 100 documents.",
                "There is no reason that we could not go deeper, but calculating variance is O(n3 ) and thus very timeconsuming.",
                "Because of the reciprocal rank nature of AP, we do not lose much information by truncating at rank 100. 4.2 Algorithms We will compare three algorithms for acquiring relevance judgments.",
                "The baseline is a variation of TREC pooling that we will call incremental pooling (IP).",
                "This algorithm takes a number k as input and presents the first k documents in rank order (without regard to topic) to be judged.",
                "It does not estimate the relevance of unjudged documents; it simply assumes any unjudged document is nonrelevant.",
                "The second algorithm is that presented in Carterette et al. [8] (Algorithm 1).",
                "Documents are selected based on how interesting they are in determining whether a difference in mean average precision exists.",
                "For this approach pi = 0.5 for all i; there is no estimation of probabilities.",
                "We will call this MTC for minimal test collection.",
                "The third algorithm augments MTC with updated estimates of probabilities of relevance.",
                "We will call this RTC for robust test collection.",
                "It is identical to Algorithm 1, except that every 10th iteration we estimate pi for all unjudged documents i using the expert aggregation model of Section 3.",
                "RTC has smoothing (prior distribution) parameters that must be set.",
                "We trained using the ad-hoc 95 set.",
                "We limited 2 Robust here means robust retrieval; this is different from our goal of robust evaluation.",
                "Algorithm 1 (MTC) Given two ranked lists and confidence level α, predict the sign of ΔMAP. 1: pi ← 0.5 for all documents i 2: while P(ΔMAP < 0) < α do 3: calculate weight wi for all unjudged documents i (see Carterette et al. [8] for details) 4: j ← argmaxiwi 5: xj ← 1 if document j is relevant, 0 otherwise 6: pj ← xj 7: end while the search to uniform priors with relatively high variance.",
                "For expert aggregation, the prior parameters are α = β = 1. 4.3 Experimental Design First, we want to know whether we can augment a set of relevance judgments with a set of relevance probabilities in order to reuse the judgments to evaluate a new set of systems.",
                "For each experimental trial: 1.",
                "Pick a random subset of k runs. 2.",
                "From those k, pick an initial c < k to evaluate. 3.",
                "Run RTC to 95% confidence on the initial c. 4.",
                "Using the model from Section 3, estimate the probabilities of relevance for all documents retrieved by all k runs. 5.",
                "Calculate EMAP for all k runs, and P(ΔMAP < 0) for all pairs of runs.",
                "We do the same for MTC, but omit step 4.",
                "Note that after evaluating the first c systems, we make no additional relevance judgments.",
                "To put our method to the test, we selected c = 2: we will build a set of judgments from evaluating only two initial systems.",
                "We will then generalize to a set of k = 10 (of which those two are a subset).",
                "As we run more trials, we obtain the data we need to test all three of our hypotheses. 4.4 Experimental Evaluation Recall that a set of judgments is robust if the accuracy of the predictions it makes is at least its estimated confidence.",
                "One way to evaluate robustness is to bin pairs by their confidence, then calculate the accuracy over all the pairs in each bin.",
                "We would like the accuracy to be no less than the lowest confidence score in the bin, but preferably higher.",
                "Since summary statistics are useful, we devised the following metric.",
                "Suppose we are a bookmaker taking bets on whether ΔMAP < 0.",
                "We use RTC or MTC to set the odds O = P (ΔMAP <0) 1−P (ΔMAP <0) .",
                "Suppose a bettor wagers $1 on ΔMAP ≥ 0.",
                "If it turns out that ΔMAP < 0, we win the dollar.",
                "Otherwise, we pay out O.",
                "If our confidence estimates are perfectly accurate, we break even.",
                "If confidence is greater than accuracy, we lose money; we win if accuracy is greater than confidence.",
                "Counterintuitively, the most desirable outcome is breaking even: if we lose money, we cannot trust the confidence estimates, but if we win money, we have either underestimated confidence or judged more documents than necessary.",
                "However, the cost of not being able to trust the confidence estimates is higher than the cost of extra relevance judgments, so we will treat positive outcomes as good.",
                "The amount we win on each pairwise comparison i is: Wi = yi − (1 − yi) Pi 1 − Pi = yi − Pi 1 − Pi yi = 1 if ΔMAP < 0 and 0 otherwise, and Pi = P(ΔMAP < 0).",
                "The summary statistic is W, the mean of Wi.",
                "Note that as Pi increases, we lose more for being wrong.",
                "This is as it should be: the penalty should be great for missing the high probability predictions.",
                "However, since our losses grow without bound as predictions approach certainty, we cap −Wi at 100.",
                "For our hypothesis that RTC requires fewer judgments than MTC, we are interested in the number of judgments needed to reach 95% confidence on the first pair of systems.",
                "The median is more interesting than the mean: most pairs require a few hundred judgments, but a few pairs require several thousand.",
                "The distribution is therefore highly skewed, and the mean strongly affected by those outliers.",
                "Finally, for our hypothesis that RTC is more accurate than MTC, we will look at Kendalls τ correlation between a ranking of k systems by a small set of judgments and the true ranking using the full set of judgments.",
                "Kendalls τ, a nonparametric statistic based on pairwise swaps between two lists, is a standard evaluation for this type of study.",
                "It ranges from −1 (perfectly anti-correlated) to 1 (rankings identical), with 0 meaning that half of the pairs are swapped.",
                "As we touched on in the introduction, though, an accuracy measure like rank correlation is not a good evaluation of reusability.",
                "We include it for completeness. 4.4.1 Hypothesis Testing Running multiple trials allows the use of statistical hypothesis testing to compare algorithms.",
                "Using the same sets of systems allows the use of paired tests.",
                "As we stated above, we are more interested in the median number of judgments than the mean.",
                "A test for difference in median is the Wilcoxon sign rank test.",
                "We can also use a paired t-test to test for a difference in mean.",
                "For rank correlation, we can use a paired t-test to test for a difference in τ. 5.",
                "RESULTS AND ANALYSIS The comparison between MTC and RTC is shown in Table 2.",
                "With MTC and uniform probabilities of relevance, the results are far from robust.",
                "We cannot reuse the relevance judgments with much confidence.",
                "But with RTC, the results are very robust.",
                "There is a slight dip in accuracy when confidence gets above 0.95; nonetheless, the confidence predictions are trustworthy.",
                "Mean Wi shows that RTC is much closer to 0 than MTC.",
                "The distribution of confidence scores shows that at least 80% confidence is achieved more than 35% of the time, indicating that neither algorithm is being too conservative in its confidence estimates.",
                "The confidence estimates are rather low overall; that is because we have built a test collection from only two initial systems.",
                "Recall from Section 1 that we cannot require (or even expect) a minimum level of confidence when we generalize to new systems.",
                "More detailed results for both algorithms are shown in Figure 2.",
                "The solid line is the ideal result that would give W = 0.",
                "RTC is on or above this line at all points until confidence reaches about 0.97.",
                "After that there is a slight dip in accuracy which we discuss below.",
                "Note that both MTC RTC confidence % in bin accuracy % in bin accuracy 0.5 − 0.6 33.7% 61.7% 28.6% 61.9% 0.6 − 0.7 18.1% 73.1% 20.1% 76.3% 0.7 − 0.8 10.4% 70.1% 15.5% 78.0% 0.8 − 0.9 9.4% 69.0% 12.1% 84.9% 0.9 − 0.95 7.3% 78.0% 6.6% 93.1% 0.95 − 0.99 17.9% 70.4% 12.4% 93.4% 1.0 3.3% 68.3% 4.7% 98.9% W −5.34 −0.39 median judged 251 235 mean τ 0.393 0.555 Table 2: Confidence that P(ΔMAP < 0) and accuracy of prediction when generalizing a set of relevance judgments acquired using MTC and RTC.",
                "Each bin contains over 1,000 trials from the adhoc 3, 5-8 sets.",
                "RTC is much more robust than MTC.",
                "W is defined in Section 4.4; closer to 0 is better.",
                "Median judged is the number of judgments to reach 95% confidence on the first two systems.",
                "Mean τ is the average rank correlation for all 10 systems. 0.5 0.6 0.7 0.8 0.9 1 0.5 0.6 0.7 0.8 0.9 1 accuracy confidence breakeven RTC MTC Figure 2: Confidence vs. accuracy of MTC and RTC.",
                "The solid line is the perfect result that would give W = 0; performance should be on or above this line.",
                "Each point represents at least 500 pairwise comparisons. algorithms are well above the line up to around confidence 0.7.",
                "This is because the baseline performance on these data sets is high; it is quite easy to achieve 75% accuracy doing very little work [7].",
                "Number of Judgments: The median number of judgments required by MTC to reach 95% confidence on the first two systems is 251, an average of 5 per topic.",
                "The median required by RTC is 235, about 4.7 per topic.",
                "Although the numbers are close, RTCs median is significantly lower by a paired Wilcoxon test (p < 0.0001).",
                "For comparison, a pool of depth 100 would result in a minimum of 5,000 judgments for each pair.",
                "The difference in means is much greater.",
                "MTC required a mean of 823 judgments, 16 per topic, while RTC required a mean of 502, 10 per topic. (Recall that means are strongly skewed by a few pairs that take thousands of judgments.)",
                "This difference is significant by a paired t-test (p < 0.0001).",
                "Ten percent of the sets resulted in 100 or fewer judgments (less than two per topic).",
                "Performance on these is very high: W = 0.41, and 99.7% accuracy when confidence is at least 0.9.",
                "This shows that even tiny collections can be reusable.",
                "For the 50% of sets with more than 235 judgments, accuracy is 93% when confidence is at least 0.9.",
                "Rank Correlation: MTC and RTC both rank the 10 systems by EMAP (Eq. (1)) calculated using their respective probability estimates.",
                "The mean τ rank correlation between true MAP and EMAP is 0.393 for MTC and 0.555 for RTC.",
                "This difference is significant by a paired t-test (p < 0.0001).",
                "Note that we do not expect the τ correlations to be high, since we are ranking the systems with so few relevance judgments.",
                "It is more important that we estimate confidence in each pairwise comparison correctly.",
                "We ran IP for the same number of judgments that MTC took for each pair, then ranked the systems by MAP using only those judgments (all unjudged documents assumed nonrelevant).",
                "We calculated the τ correlation to the true ranking.",
                "The mean τ correlation is 0.398, which is not significantly different from MTC, but is significantly lower than RTC.",
                "Using uniform estimates of probability is indistinguishable from the baseline, whereas estimating relevance by expert aggregation boosts performance a great deal: nearly 40% over both MTC and IP.",
                "Overfitting: It is possible to overfit: if too many judgments come from the first two systems, the variance in ΔMAP is reduced and the confidence estimates become unreliable.",
                "We saw this in Table 2 and Figure 2 where RTC exhibits a dip in accuracy when confidence is around 97%.",
                "In fact, the number of judgments made prior to a wrong prediction is over 50% greater than the number made prior to a correct prediction.",
                "Overfitting is difficult to quantify exactly, because making more relevance judgments does not always cause it: at higher confidence levels, more relevance judgments are made, and as Table 2 shows, accuracy is greater at those higher confidences.",
                "Obviously having more relevance judgments should increase both confidence and accuracy; the difference seems to be when one system has a great deal more judgments than the other.",
                "Pairwise Comparisons: Our pairwise comparisons fall into one of three groups: 1. the two original runs from which relevance judgments are acquired; 2. one of the original runs vs. one of the new runs; 3. two new runs.",
                "Table 3 shows confidence vs. accuracy results for each of these three groups.",
                "Interestingly, performance is worst when comparing one of the original runs to one of the additional runs.",
                "This is most likely due to a large difference in the number of judgments affecting the variance of ΔMAP.",
                "Nevertheless, performance is quite good on all three subsets.",
                "Worst Case: The case intuitively most likely to produce an error is when the two systems being compared have retrieved very few documents in common.",
                "If we want the judgments to be reusable, we should to be able to generalize even to runs that are very different from the ones used to acquire the relevance judgments.",
                "A simple measure of similarity of two runs is the average percentage of documents they retrieved in common for each topic [2].",
                "We calculated this for all pairs, then looked at performance on pairs with low similarity.",
                "Results are shown in accuracy confidence two original one original no original 0.5 − 0.6 - 48.1% 62.8% 0.6 − 0.7 - 57.1% 79.2% 0.7 − 0.8 - 67.9% 81.7% 0.8 − 0.9 - 82.2% 86.3% 0.9 − 0.95 95.9% 93.7% 92.6% 0.95 − 0.99 96.2% 92.5% 93.1% 1.0 100% 98.0% 99.1% W −1.11 −0.87 −0.27 Table 3: Confidence vs. accuracy of RTC when comparing the two original runs, one original run and one new run, and two new runs.",
                "RTC is robust in all three cases. accuracy when similar confidence 0 − 0.1 0.1 − 0.2 0.2 − 0.3 0.5 − 0.6 68.4% 63.1% 61.4% 0.6 − 0.7 84.2% 78.6% 76.6% 0.7 − 0.8 82.0% 79.8% 78.9% 0.8 − 0.9 93.6% 83.3% 82.1% 0.9 − 0.95 99.3% 92.7% 92.4% 0.95 − 0.99 98.7% 93.4% 93.3% 1.0 99.9% 97.9% 98.1% W 0.44 −0.45 −0.49 Table 4: Confidence vs. accuracy of RTC when a pair of systems retrieved 0-30% documents in common (broken out into 0%-10%, 10%-20%, and 20%30%).",
                "RTC is robust in all three cases.",
                "Table 4.",
                "Performance is in fact very robust even when similarity is low.",
                "When the two runs share very few documents in common, W is actually positive.",
                "MTC and IP both performed quite poorly in these cases.",
                "When the similarity was between 0 and 10%, both MTC and IP correctly predicted ΔMAP only 60% of the time, compared to an 87.6% success rate for RTC.",
                "By Data Set: All the previous results have only been on the ad-hoc collections.",
                "We did the same experiments on our additional data sets, and broke out the results by data set to see how performance varies.",
                "The results in Table 5 show everything about each set, including binned accuracy, W, mean τ, and median number of judgments to reach 95% confidence on the first two systems.",
                "The results are highly consistent from collection to collection, suggesting that our method is not overfitting to any particular data set. 6.",
                "CONCLUSIONS AND FUTURE WORK In this work we have offered the first formal definition of the common idea of reusability of a test collection and presented a model that is able to achieve reusability with very small sets of relevance judgments.",
                "Table 2 and Figure 2 together show how biased a small set of judgments can be: MTC is dramatically overestimating confidence and is much less accurate than RTC, which is able to remove the bias to give a robust evaluation.",
                "The confidence estimates of RTC, in addition to being accurate, provide a guide for obtaining additional judgments: focus on judging documents from the lowest-confidence comparisons.",
                "In the long run, we see small sets of relevance judgaccuracy confidence ad-hoc 94 ad-hoc 96 ad-hoc 97 ad-hoc 98 ad-hoc 99 web 04 robust 05 terabyte 05 0.5 − 0.6 64.1% 61.8% 62.2% 62.0% 59.4% 64.3% 61.5% 61.6% 0.6 − 0.7 76.1% 77.8% 74.5% 78.2% 74.3% 78.1% 75.9% 75.9% 0.7 − 0.8 75.2% 78.9% 77.6% 80.0% 78.6% 82.6% 77.5% 80.4% 0.8 − 0.9 83.2% 85.5% 84.6% 84.9% 86.8% 84.5% 86.7% 87.7% 0.9 − 0.95 93.0% 93.6% 92.8% 93.7% 92.6% 94.2% 93.9% 94.2% 0.95 − 0.99 93.1% 94.3% 93.1% 93.7% 92.8% 95.0% 93.9% 91.6% 1.0 99.2% 96.8% 98.7% 99.5% 99.6% 100% 99.2% 98.3% W -0.34 -0.34 -0.48 -0.35 -0.44 -0.07 -0.41 -0.67 median judged 235 276 243 213 179 448 310 320 mean τ 0.538 0.573 0.556 0.579 0.532 0.596 0.565 0.574 Table 5: Accuracy, W, mean τ, and median number of judgments for all 8 testing sets.",
                "The results are highly consistent across data sets. ments being shared by researchers, each group contributing a few more judgments to gain more confidence about their particular systems.",
                "As time goes on, the number of judgments grows until there is 100% confidence in every evaluation-and there is a full test collection for the task.",
                "We see further use for this method in scenarios such as web retrieval in which the corpus is frequently changing.",
                "It could be applied to evaluation on a dynamic test collection as defined by Soboroff [18].",
                "The model we presented in Section 3 is by no means the only possibility for creating a robust test collection.",
                "A simpler expert aggregation model might perform as well or better (though all our efforts to simplify failed).",
                "In addition to expert aggregation, we could estimate probabilities by looking at similarities between documents.",
                "This is an obvious area for future exploration.",
                "Additionally, it will be worthwhile to investigate the issue of overfitting: the circumstances it occurs under and what can be done to prevent it.",
                "In the meantime, capping confidence estimates at 95% is a hack that solves the problem.",
                "We have many more experimental results that we unfortunately did not have space for but that reinforce the notion that RTC is highly robust: with just a few judgments per topic, we can accurately assess the confidence in any pairwise comparison of systems.",
                "Acknowledgments This work was supported in part by the Center for Intelligent Information Retrieval and in part by the Defense Advanced Research Projects Agency (DARPA) under contract number HR0011-06-C-0023.",
                "Any opinions, findings, and conclusions or recommendations expressed in this material are those of the author and do not necessarily reflect those of the sponsor. 7.",
                "REFERENCES [1] J. Aslam and M. Montague.",
                "Models for Metasearch.",
                "In Proceedings of SIGIR, pages 275-285, 2001. [2] J. Aslam and R. Savell.",
                "On the effectiveness of evaluating retrieval systems in the absence of relevance judgments.",
                "In Proceedings of SIGIR, pages 361-362, 2003. [3] J.",
                "A. Aslam, V. Pavlu, and R. Savell.",
                "A unified model for metasearch, pooling, and system evaluation.",
                "In Proceedings of CIKM, pages 484-491, 2003. [4] J.",
                "A. Aslam, V. Pavlu, and E. Yilmaz.",
                "A statistical method for system evaluation using incomplete judgments.",
                "In Proceedings of SIGIR, pages 541-548, 2006. [5] A. L. Berger, S. D. Pietra, and V. J. D. Pietra.",
                "A maximum entropy approach to natural language processing.",
                "Computational Linguistics, 22(1):39-71, 1996. [6] D. J. Blower.",
                "An easy derivation of logistic regression from the bayesian and maximum entropy perspective.",
                "In Proceedings of the 23rd International Workship on Bayesian Inference and Maximum Entropy Methods in Science and Engineering, pages 30-43, 2004. [7] B. Carterette and J. Allan.",
                "Research methodology in studies of assessor effort for retrieval evaluation.",
                "In Proceedings of RIAO, 2007. [8] B. Carterette, J. Allan, and R. K. Sitaraman.",
                "Minimal test collections for retrieval evaluation.",
                "In Proceedings of SIGIR, pages 268-275, 2006. [9] B. Carterette and D. I. Petkova.",
                "Learning a ranking from pairwise preferences.",
                "In Proceedings of SIGIR, 2006. [10] R. T. Clemen and R. L. Winkler.",
                "Unanimity and compromise among probability forecasters.",
                "Management Science, 36(7):767-779, July 1990. [11] G. V. Cormack, C. R. Palmer, and C. L. Clarke.",
                "Efficient Construction of Large Test Collections.",
                "In Proceedings of SIGIR, pages 282-289, 1998. [12] A. Gelman, J.",
                "B. Carlin, H. S. Stern, and D. B. Rubin.",
                "Bayesian Data Analysis.",
                "Chapman & Hall/CRC, 2004. [13] E. T. Jaynes.",
                "Probability Theory: The Logic of Science.",
                "Cambridge University Press, 2003. [14] R. Manmatha and H. Sever.",
                "A Formal Approach to Score Normalization for Metasearch.",
                "In Proceedings of HLT, pages 88-93, 2002. [15] I. J. Myung, S. Ramamoorti, and J. Andrew D. Baily.",
                "Maximum entropy aggregation of expert predictions.",
                "Management Science, 42(10):1420-1436, October 1996. [16] J. Platt.",
                "Probabilistic outputs for support vector machines and comparison to regularized likelihood methods. pages 61-74, 2000. [17] M. Sanderson and H. Joho.",
                "Forming test collections with no system pooling.",
                "In Proceedings of the 27th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 33-40, 2004. [18] I. Soboroff.",
                "Dynamic test collections: measuring search effectiveness on the live web.",
                "In Proceedings of SIGIR, pages 276-283, 2006. [19] K. Sparck Jones and C. J. van Rijsbergen.",
                "Information Retrieval Test Collections.",
                "Journal of Documentation, 32(1):59-75, 1976. [20] E. M. Voorhees and D. K. Harman, editors.",
                "TREC: Experiment and Evaluation in Information Retrieval.",
                "MIT Press, 2005. [21] J. Zobel.",
                "How Reliable are the Results of Large-Scale Information Retrieval Experiments?",
                "In Proceedings of SIGIR, pages 307-314, 1998."
            ],
            "original_annotated_samples": [
                "We argue that the best possible <br>distribution of relevance</br> p(Xi) is the one that explains all of the data (all of the observations made about the retrieval systems) while at the same time making no unwarranted assumptions.",
                "The theorem implies that the closer we get to the maximum entropy <br>distribution of relevance</br>, the closer we get to robustness. 3."
            ],
            "translated_annotated_samples": [
                "Sostenemos que la mejor distribución posible de relevancia p(Xi) es aquella que explica todos los datos (todas las observaciones realizadas sobre los sistemas de recuperación) al mismo tiempo que no hace suposiciones infundadas.",
                "El teorema implica que cuanto más nos acerquemos a la <br>distribución de entropía máxima relevante</br>, más nos acercaremos a la robustez. 3."
            ],
            "translated_text": "Colecciones de pruebas robustas para la evaluación de recuperación Ben Carterette Centro de Recuperación de Información Inteligente Departamento de Ciencias de la Computación Universidad de Massachusetts Amherst Amherst, MA 01003 carteret@cs.umass.edu RESUMEN Los métodos de bajo costo para adquirir juicios de relevancia pueden ser una bendición para los investigadores que necesitan evaluar nuevas tareas o temas de recuperación pero no tienen los recursos para hacer miles de juicios. Si bien estos juicios son muy útiles para una evaluación única, no está claro que se puedan confiar al ser reutilizados para evaluar nuevos sistemas. En este trabajo, definimos formalmente lo que significa que las evaluaciones sean reutilizables: la confianza en una evaluación de nuevos sistemas puede ser evaluada con precisión a partir de un conjunto existente de juicios de relevancia. Luego presentamos un método para aumentar un conjunto de juicios de relevancia con estimaciones de relevancia que no requieren esfuerzo adicional del evaluador. Usar este método garantiza prácticamente la reutilización: con tan solo cinco juicios por tema tomados de solo dos sistemas, podemos evaluar de manera confiable un conjunto más grande de diez sistemas. Incluso los conjuntos más pequeños de juicios pueden ser útiles para la evaluación de nuevos sistemas. Categorías y Descriptores de Asignaturas: H.3 Almacenamiento y Recuperación de Información; H.3.4 Sistemas y Software: Evaluación del Rendimiento Términos Generales: Experimentación, Medición, Confiabilidad 1. INTRODUCCIÓN Imagina a un investigador de recuperación de información que ha inventado una nueva tarea de recuperación. Ella ha construido un sistema para realizar la tarea y quiere evaluarlo. Dado que la tarea es nueva, es poco probable que existan juicios de relevancia vigentes. Ella no tiene el tiempo ni los recursos para juzgar cada documento, ni siquiera cada documento recuperado. Ella solo puede juzgar los documentos que parecen ser los más informativos y detenerse cuando tenga un grado razonable de confianza en sus conclusiones. Pero ¿qué sucede cuando ella desarrolla un nuevo sistema y necesita evaluarlo? ¿Otro grupo de investigación decide implementar un sistema para realizar la tarea? ¿Pueden reutilizar de manera confiable los juicios originales? ¿Pueden evaluar sin más juicios de relevancia? La evaluación es un aspecto importante de la investigación en recuperación de información, pero es solo un problema parcialmente resuelto: para la mayoría de las tareas de recuperación, es imposible juzgar la relevancia de cada documento; simplemente hay demasiados. La solución utilizada por NIST en TREC (Text REtrieval Conference) es el método de agrupación [19, 20]: todos los sistemas competidores contribuyen con N documentos a un grupo, y cada documento en ese grupo es evaluado. Este método crea grandes conjuntos de juicios que son reutilizables para entrenar o evaluar nuevos sistemas que no contribuyeron al conjunto [21]. Esta solución no es adecuada para nuestro investigador hipotético. El método de agrupación proporciona miles de juicios de relevancia, pero requiere muchas horas de tiempo de anotadores (remunerados). Como resultado, ha habido una serie de artículos recientes sobre la reducción del esfuerzo del anotador en la producción de colecciones de pruebas: Cormack et al. [11], Zobel [21], Sanderson y Joho [17], Carterette et al. [8], y Aslam et al. [4], entre otros. Como veremos, los juicios que estos métodos producen pueden sesgar significativamente la evaluación de un nuevo conjunto de sistemas. ¿Volviendo a nuestra investigadora hipotética, puede reutilizar sus juicios de relevancia? Primero debemos definir formalmente lo que significa ser reutilizable. En trabajos anteriores, la reutilización se ha probado simplemente evaluando la precisión de un conjunto de juicios de relevancia al evaluar sistemas no vistos. Si bien podemos decir que tuvo razón el 75% del tiempo, o que tuvo una correlación de rango de 0.8, estos números no tienen ningún poder predictivo: no nos dicen qué sistemas es probable que estén equivocados ni cuán seguros deberíamos estar en uno en particular. Necesitamos una definición más cuidadosa de reutilización. Específicamente, la cuestión de la reutilización no es qué tan precisamente podemos evaluar los nuevos sistemas. Un adversario malicioso siempre puede producir una nueva lista clasificada que no haya recuperado ninguno de los documentos evaluados. La verdadera pregunta es cuánta confianza tenemos en nuestras evaluaciones, y, más importante aún, si podemos confiar en nuestras estimaciones de confianza. Aunque la confianza no sea alta, siempre y cuando podamos confiar en ello, podemos identificar qué sistemas necesitan más juicios para aumentar la confianza. Cualquier conjunto de juicios, por pequeño que sea, se vuelve reutilizable en cierta medida. Pequeñas colecciones de pruebas reutilizables podrían tener un gran impacto en la investigación de recuperación de información. Los grupos de investigación podrían compartir las evaluaciones de relevancia que han realizado internamente para estudios piloto, nuevas tareas o nuevos temas. La cantidad de datos disponible para los investigadores crecería de forma exponencial con el tiempo. EVALUACIÓN ROBUSTA Anteriormente dimos una definición intuitiva de reutilización: una colección es reutilizable si podemos confiar en nuestras estimaciones de confianza en una evaluación. Con eso queremos decir que si hemos realizado algunos juicios de relevancia y tenemos, por ejemplo, un 75% de confianza en que el sistema A es mejor que el sistema B, nos gustaría que no haya más del 25% de posibilidad de que nuestra evaluación de la calidad relativa de los sistemas cambie a medida que continuamos evaluando documentos. Nuestra evaluación debe ser robusta ante juicios faltantes. En nuestro trabajo anterior, definimos la confianza como la probabilidad de que la diferencia en una medida de evaluación calculada para dos sistemas sea menor que cero [8]. Esta noción de confianza se define en el contexto de una tarea de evaluación particular que llamamos evaluación comparativa: determinar el signo de la diferencia en una medida de evaluación. Otras tareas de evaluación podrían ser definidas; estimar la magnitud de la diferencia o los valores de las medidas mismas son ejemplos que implican diferentes nociones de confianza. Por lo tanto, consideramos la confianza como una estimación de probabilidad. Una de las preguntas que debemos hacer acerca de una estimación de probabilidad es qué significa. ¿Qué significa tener un 75% de confianza de que el sistema A es mejor que el sistema B? Como se describe arriba, queremos que signifique que si seguimos evaluando documentos, solo habrá un 25% de probabilidad de que nuestra evaluación cambie. Si esto es lo que significa, podemos confiar en las estimaciones de confianza. ¿Pero sabemos que tiene ese significado? Nuestro cálculo de confianza se basó en una suposición sobre la probabilidad de relevancia de los documentos no evaluados, específicamente que cada documento no evaluado tenía la misma probabilidad de ser relevante o no relevante. Esta suposición casi con seguridad no es realista en la mayoría de las aplicaciones de IR. Resulta que es esta suposición la que determina si se pueden confiar en las estimaciones de confianza. Antes de profundizar en esto, definimos formalmente la confianza. 2.1 Estimación de la confianza La precisión promedio (AP) es una métrica de evaluación estándar que captura tanto la capacidad de un sistema para clasificar documentos relevantes de manera alta (precisión) como su capacidad para recuperar documentos relevantes (recuperación). Normalmente se escribe como la precisión media en los rangos de documentos relevantes: AP = 1 |R| i∈R prec@r(i) donde R es el conjunto de documentos relevantes y r(i) es el rango del documento i. Sea Xi una variable aleatoria que indica la relevancia del documento i. Si los documentos están ordenados por rango, podemos expresar la precisión como prec@i = 1/i i j=1 Xj. La precisión promedio se convierte entonces en la ecuación cuadrática AP = 1 Xi n i=1 Xi/i i j=1 Xj = 1 Xi n i=1 j≥i aijXiXj donde aij = 1/ max{r(i), r(j)}. Usar aij en lugar de 1/i nos permite numerar los documentos de forma arbitraria. Para ver por qué esto es cierto, considera un ejemplo simple: una lista de 3 documentos con los documentos relevantes B, C en los rangos 1 y 3 y el documento no relevante A en el rango 2. La precisión promedio será 1 2 (1 1 x2 B+ 1 2 xBxA+ 1 3 xBxC + 1 2 x2 A+ 1 3 xAxC + 1 3 x2 C) = 1 2 1 + 2 3 porque xA = 0, xB = 1, xC = 1. Aunque el orden B, A, C es diferente de la etiqueta A, B, C, no afecta al cálculo. Ahora podemos ver que la precisión promedio en sí misma es una variable aleatoria con una distribución sobre todas las posibles asignaciones de relevancia a todos los documentos. Esta variable aleatoria tiene una esperanza, una varianza, intervalos de confianza y una cierta probabilidad de ser menor o igual a un valor dado. Todos estos dependen de la probabilidad de que el documento i sea relevante: pi = p(Xi = 1). Supongamos que en nuestro ejemplo anterior no conocemos las valoraciones de relevancia, pero creemos que pA = 0.4, pB = 0.8, pC = 0.7. Podemos entonces calcular, por ejemplo. P(AP = 0) = 0.2 · 0.6 · 0.3 = 0.036, o P(AP = 1 2 ) = 0.2 · 0.4 · 0.7 = 0.056. \n\nLa traducción al español sería: P(AP = 0) = 0.2 · 0.6 · 0.3 = 0.036, o P(AP = 1 2 ) = 0.2 · 0.4 · 0.7 = 0.056. Sumando sobre todas las posibilidades, podemos calcular la esperanza y la varianza: E[AP] ≈ 1 pi aiipi + j>i aij pipj V ar[AP] ≈ 1 ( pi)2 n i a2 iipiqi + j>i a2 ijpipj(1 − pipj) + i=j 2aiiaijpipj(1 − pi) + k>j=i 2aijaikpipjpk(1 − pi) AP converge asintóticamente a una distribución normal con la esperanza y la varianza definidas anteriormente. Para nuestra tarea de evaluación comparativa, estamos interesados en el signo de la diferencia en dos precisiones promedio: ΔAP = AP1 − AP2. Como mostramos en nuestro trabajo anterior, ΔAP tiene una forma cerrada cuando los documentos están ordenados arbitrariamente: ΔAP = 1 Xi n i=1 j≥i cij XiXj cij = aij − bij donde bij está definido de manera análoga a aij para la segunda clasificación. Dado que AP es normal, ΔAP también es normal, lo que significa que podemos usar la función de densidad acumulativa normal para determinar la confianza de que una diferencia en AP sea menor que cero. Dado que los temas son independientes, podemos extender fácilmente esto para referirnos a la precisión promedio (MAP). MAP también se distribuye normalmente; su esperanza y varianza son: EMAP = 1 T t∈T E[APt] (1) VMAP = 1 T2 t∈T V ar[APt] ΔMAP = MAP1 − MAP2 La confianza puede entonces estimarse calculando la esperanza y varianza y utilizando la función de densidad normal para encontrar P(ΔMAP < 0). 2.2 Confianza y Robustez Habiendo definido la confianza, volvemos al tema de la confianza en las estimaciones de confianza, y mostramos cómo se relaciona con la robustez de la colección ante juicios faltantes. 1 Estas son en realidad aproximaciones a la verdadera esperanza y varianza, pero el error es despreciable O(n2−n ). Sea Z el conjunto de todos los pares de resultados clasificados para un conjunto común de temas. Supongamos que tenemos un conjunto de m juicios de relevancia xm = {x1, x2, ..., xm} (usando x minúscula en lugar de X mayúscula para distinguir entre documentos juzgados y no juzgados); estos son los juicios contra los cuales calculamos la confianza. Sea Zα el subconjunto de pares en Z para los cuales predecimos que ΔMAP = −1 con confianza α dadas las evaluaciones xm. Para que las estimaciones de confianza sean precisas, necesitamos que al menos α · |Zα| de estos pares realmente tengan ΔMAP = −1 después de haber evaluado cada documento. Si lo hacen, podemos confiar en las estimaciones de confianza; nuestra evaluación será robusta ante juicios faltantes. Si nuestras estimaciones de confianza se basan en suposiciones poco realistas, no podemos esperar que sean precisas. Las suposiciones en las que se basan son las probabilidades de relevancia pi. Necesitamos que esto sea realista. Sostenemos que la mejor distribución posible de relevancia p(Xi) es aquella que explica todos los datos (todas las observaciones realizadas sobre los sistemas de recuperación) al mismo tiempo que no hace suposiciones infundadas. Esto se conoce como el principio de máxima entropía [13]. La entropía de una variable aleatoria X con distribución p(X) se define como H(p) = − i p(X = i) log p(X = i). Esto ha encontrado una amplia variedad de usos en la informática y la recuperación de información. La distribución de entropía máxima es aquella que maximiza H. Esta distribución es única y tiene una forma exponencial. El siguiente teorema muestra la utilidad de una distribución de entropía máxima para la relevancia al estimar la confianza. Teorema 1. Si p(Xn |I, xm ) = argmaxpH(p), las estimaciones de confianza serán precisas, donde xm es el conjunto de juicios de relevancia definidos anteriormente, Xn es el conjunto completo de documentos cuya relevancia deseamos estimar, e I es alguna información sobre los documentos (no especificada hasta ahora). Por el momento prescindimos de la prueba, pero es bastante simple. Esto indica que cuanto mejores sean las estimaciones de relevancia, más precisa será la evaluación. La tarea de crear una colección de pruebas reutilizable se convierte entonces en la tarea de estimar la relevancia de los documentos no evaluados. El teorema y su demostración no dicen absolutamente nada sobre la métrica de evaluación. Las estimaciones de probabilidad son completamente independientes de la medida en la que estamos interesados. Esto significa que los mismos cálculos de probabilidad pueden informarnos sobre la precisión promedio, así como sobre la precisión, la recuperación, el bpref, etc. Además, podríamos asumir que la relevancia de los documentos i y j es independiente y lograr el mismo resultado, lo cual afirmamos como un corolario: Corolario 1. Si p(Xi|I, xm) = argmaxpH(p), las estimaciones de confianza serán precisas. La tarea, por lo tanto, consiste en la imputación de los valores faltantes de relevancia. El teorema implica que cuanto más nos acerquemos a la <br>distribución de entropía máxima relevante</br>, más nos acercaremos a la robustez. 3. PREDICIENDO LA RELEVANCIA En nuestra declaración del Teorema 1, dejamos sin especificar la naturaleza de la información I. Una de las ventajas de nuestras estimaciones de confianza es que admiten información de una amplia variedad de fuentes; básicamente cualquier cosa que pueda ser modelada puede ser utilizada como información para predecir la relevancia. Una fuente natural de información son los propios sistemas de recuperación: cómo clasificaron los documentos evaluados, con qué frecuencia no lograron clasificar documentos relevantes, cómo se desempeñan en diferentes temas, y así sucesivamente. Si tratamos cada sistema como un experto en recuperación de información que proporciona una opinión sobre la relevancia de cada documento, el problema se convierte en uno de agregación de opiniones de expertos. Esto es similar al problema de metabúsqueda o fusión de datos en el que la tarea consiste en tomar k sistemas de entrada y fusionarlos en un solo ranking. Aslam et al. [3] identificaron previamente una conexión entre la evaluación y la metabúsqueda. Nuestro problema tiene dos diferencias clave: 1. Necesitamos explícitamente probabilidades de relevancia que podamos insertar en la Ecuación 1; los algoritmos de metabusqueda no tienen tal requisito. Estamos acumulando juicios de relevancia a medida que avanzamos con la evaluación y somos capaces de reestimar la relevancia dada cada nuevo juicio. A la luz de lo anterior, introducimos un modelo probabilístico para la combinación de expertos. 3.1 Un Modelo para la Agregación de Opiniones de Expertos Supongamos que cada experto j proporciona una probabilidad de relevancia qij = pj(Xi = 1). La información sobre la relevancia del documento i será entonces el conjunto de k opiniones de expertos I = qi = (qi1, qi2, · · · , qik). La distribución de probabilidad que deseamos encontrar es aquella que maximiza la entropía de pi = p(Xi = 1|qi). Resulta que encontrar el modelo de entropía máxima es equivalente a encontrar los parámetros que maximizan la verosimilitud [5]. Blower [6] muestra explícitamente que encontrar el modelo de entropía máxima para una variable binaria es equivalente a resolver una regresión logística. Entonces pi = p(Xi = 1|qi) = exp k j=1 λjqij 1 + exp k j=1 λj qij (2) donde λ1, · · · , λk son los parámetros de regresión. Incluimos una distribución beta a priori para p(λj) con parámetros α, β. Esto se puede ver como un tipo de suavizado para tener en cuenta el hecho de que los datos de entrenamiento están altamente sesgados. Este modelo tiene la ventaja de incluir la dependencia estadística entre los expertos. Un modelo de la misma forma fue demostrado por Clemen & Winkler como el mejor para agregar probabilidades de expertos [10]. Un enfoque similar motivado por la entropía máxima se ha utilizado para la agregación de expertos [15]. Aslam & Montague [1] utilizaron un modelo similar para la metabúsqueda, pero asumieron independencia entre los expertos. ¿De dónde vienen los qij? Utilizar puntuaciones sin procesar y no calibradas como predictores no funcionará porque las distribuciones de puntuaciones varían demasiado entre los temas. Un clasificador de modelado de lenguaje, por ejemplo, suele otorgar una puntuación mucho más alta al documento recuperado en la parte superior para una consulta corta que al documento recuperado en la parte superior para una consulta larga. Podríamos entrenar un modelo predictivo separado para cada tema, pero eso no aprovecha toda la información que tenemos: es posible que solo tengamos un puñado de juicios para un tema, no suficientes para entrenar un modelo con confianza. Además, parece razonable asumir que si un experto hace buenas predicciones para un tema, también hará buenas predicciones para otros temas. Podríamos usar un modelo jerárquico [12], pero eso no se generalizará a temas no vistos. En cambio, calibraremos las puntuaciones de cada experto de forma individual para que las puntuaciones puedan ser comparadas tanto dentro de un tema como entre temas. Por lo tanto, nuestro modelo tiene en cuenta no solo la dependencia entre expertos, sino también la dependencia entre el rendimiento de los expertos en diferentes tareas (temas). 3.2 Calibración de Expertos Cada experto nos proporciona una puntuación y un rango para cada documento. Necesitamos convertir estos en probabilidades. Un método como el utilizado por Manmatha et al. [14] podría ser utilizado para convertir puntuaciones en probabilidades de relevancia. El método de preferencia por pares de Carterette & Petkova [9] también podría ser utilizado, interpretando la clasificación de un documento sobre otro como una expresión de preferencia. Que q∗ ij sea la probabilidad autoinformada por el experto j de que el documento i sea relevante. De manera intuitiva, parece claro que q∗ ij debería disminuir con el rango, y debería ser cero si el documento i no está clasificado (el experto no consideró que fuera relevante). El modelo de preferencia por pares puede manejar fácilmente estos dos requisitos, por lo que lo utilizaremos. Que θrj (i) sea el coeficiente de relevancia del documento en la posición rj(i). Queremos encontrar los θs que maximizan la función de verosimilitud: Ljt(Θ) = rj (i)<rj (k) exp(θrj (i) − θrj (k)) 1 + exp(θrj (i) − θrj (k)) Incluimos nuevamente una distribución beta previa en p(θrj(i)) con parámetros |Rt| + 1 y |Nt| + 1, el tamaño de los conjuntos de documentos relevantes y no relevantes juzgados respectivamente. El uso de estos parámetros previos garantiza que las probabilidades resultantes se concentren alrededor de la proporción de documentos relevantes que se han descubierto para el tema t. Esto significa que las estimaciones de probabilidad disminuyen por rango y son más altas para los temas que tienen más documentos relevantes. Después de encontrar el Θ que maximiza la verosimilitud, tenemos q∗ ij = exp(θrj (i)) 1+exp(θrj (i)) . Definimos θ∞ = −∞, de modo que la probabilidad de que un documento no clasificado sea relevante es 0. Dado que q∗ ij se basa en la posición en la que se recupera un documento en lugar de en la identidad del documento en sí, las probabilidades son idénticas de un experto a otro, por ejemplo, si el experto E colocó el documento A en la posición 1, y el experto D colocó el documento B en la posición 1, tendremos que q∗ AE = q∗ BD. Por lo tanto, solo tenemos que resolver esto una vez para cada tema. El modelo anterior proporciona probabilidades independientes del tema para cada documento. Pero supongamos que un experto que informa una probabilidad del 90% solo tiene razón el 50% del tiempo. Su opinión debería ser descartada basándose en su desempeño observado. Específicamente, queremos aprender una función de calibración qij = Cj(q∗ ij) que garantice que las probabilidades predichas estén ajustadas a la capacidad de los expertos para recuperar documentos relevantes dados los juicios que se han realizado hasta este momento. El método de calibración SVM de Platts [16] ajusta una función sigmoide entre q∗ ij y las evaluaciones de relevancia para obtener qij = Cj (q∗ ij) = exp(Aj +Bjq∗ ij ) 1+exp(Aj +Bj q∗ ij ) . Dado que q∗ ij es independiente del tema, solo necesitamos aprender una función de calibración para cada experto. Una vez que tenemos la función de calibración, se aplica para ajustar las predicciones de los expertos a su rendimiento real. Las probabilidades calibradas se introducen en el modelo (2) para encontrar las probabilidades de los documentos. Figura 1: Diagrama conceptual de nuestro modelo de agregación. Los expertos E1 y E2 han clasificado los documentos A, B, C para el tema T1 y los documentos D, E, F para el tema T2. El primer paso es obtener q∗ ij. A continuación se realiza la calibración al rendimiento real para encontrar qij. Finalmente obtenemos pi = p(Xi = 1|qi1, qi2), · · · . 3.3 Resumen del modelo Nuestro modelo tiene tres componentes que difieren en los datos que toman como entrada y en lo que producen como salida. Se muestra un diagrama conceptual en la Figura 1. 1. rangos → probabilidades (por sistema por tema). Esto nos da q∗ ij, la probabilidad autoinformada por el experto j de la relevancia del documento i. Esto es no supervisado; no requiere datos etiquetados (aunque si los tenemos, los usamos para establecer parámetros previos). 2. probabilidades → probabilidades calibradas (por sistema). Esto nos da qij = Cj (q∗ ij), la probabilidad calibrada por el experto j de la relevancia del documento i. Esto es semisupervisado; tenemos juicios de relevancia en algunos rangos que usamos para imputar la probabilidad de relevancia en otros rangos. 3. probabilidades calibradas → probabilidades de documentos. Esto nos da pi = p(Xi = 1|qi), la probabilidad de relevancia del documento i dada las probabilidades de experto calibradas qij. Esto es supervisado; aprendemos coeficientes de un conjunto de documentos juzgados y los utilizamos para estimar la relevancia de los documentos no juzgados. Aunque el modelo parece bastante complejo, en realidad son solo tres aplicaciones sucesivas de regresión logística. Por lo tanto, se puede implementar en un lenguaje de programación estadística como R en unas pocas líneas de código. El uso de priors beta (conjugados) garantiza que no sean necesarios métodos computacionales costosos como MCMC [12], por lo que el modelo se entrena y aplica lo suficientemente rápido como para ser utilizado en línea. Nuestro código está disponible en http://ciir.cs.umass.edu/~carteret/. 4. EXPERIMENTOS Se están considerando tres hipótesis. El primero, y más importante, es que el uso de nuestro modelo de agregación de expertos para predecir la relevancia produce colecciones de pruebas lo suficientemente robustas como para ser reutilizables; es decir, podemos confiar en las estimaciones de confianza cuando evaluamos sistemas que no contribuyeron con ningún juicio al conjunto. Las otras dos hipótesis se relacionan con la mejora que observamos al utilizar estimaciones de relevancia mejores que las que utilizamos en nuestro trabajo anterior [8]. Estos son (a) que se necesitan menos temas de relevancia, número de ejecuciones, número de documentos evaluados y número de documentos relevantes para cada uno de nuestros conjuntos de datos y (b) la precisión de las predicciones es mayor que si simplemente asumiéramos que pi = 0.5 para todos los documentos no evaluados. 4.1 Datos Obtuvimos ejecuciones completas de búsqueda ad-hoc presentadas en las TRECs 3 a 8. Cada ejecución clasifica como máximo 1000 documentos para 50 temas (49 temas para TREC 4). Además, obtuvimos todas las ejecuciones de la pista Web de TREC 13, la pista Robust2 de TREC 14 y la pista Terabyte (ad-hoc) de TREC 14. Estas son las pistas que han reemplazado la pista ad-hoc desde su final en 1999. Las estadísticas se muestran en la Tabla 1. Reservamos el conjunto TREC 4 (ad-hoc 95) para entrenamiento, los TRECs 3 y 5-8 (ad-hoc 94 y 96-99) para pruebas principales, y los conjuntos restantes para pruebas adicionales. Utilizamos los archivos qrels recopilados por NIST como verdad. El número de juicios de relevancia realizados y documentos relevantes encontrados para cada pista se enumeran en la Tabla 1. Por razones computacionales, truncamos las listas clasificadas en 100 documentos. No hay razón por la que no pudiéramos profundizar más, pero el cálculo de la varianza es O(n3) y, por lo tanto, muy consumidor de tiempo. Debido a la naturaleza del rango recíproco de AP, no perdemos mucha información al truncar en el rango 100. 4.2 Algoritmos Compararemos tres algoritmos para adquirir juicios de relevancia. La línea base es una variación del agrupamiento de TREC que llamaremos agrupamiento incremental (IP). Este algoritmo toma un número k como entrada y presenta los primeros k documentos en orden de rango (sin tener en cuenta el tema) para ser evaluados. No estima la relevancia de los documentos no evaluados; simplemente asume que cualquier documento no evaluado no es relevante. El segundo algoritmo es el presentado en Carterette et al. [8] (Algoritmo 1). Los documentos se seleccionan en función de lo interesantes que sean para determinar si existe una diferencia en la precisión media promedio. Para este enfoque pi = 0.5 para todos los i; no hay estimación de probabilidades. Llamaremos a esto MTC por colección de pruebas mínimas. El tercer algoritmo aumenta MTC con estimaciones actualizadas de las probabilidades de relevancia. Llamaremos a esto RTC por colección de pruebas robusta. Es idéntico al Algoritmo 1, excepto que cada décima iteración estimamos pi para todos los documentos no evaluados i utilizando el modelo de agregación de expertos de la Sección 3. RTC tiene parámetros de suavizado (distribución previa) que deben ser configurados. Entrenamos utilizando el conjunto ad-hoc 95. Limitamos 2 robustos aquí significa recuperación robusta; esto es diferente de nuestro objetivo de evaluación robusta. Algoritmo 1 (MTC) Dadas dos listas clasificadas y un nivel de confianza α, predecir el signo de ΔMAP. 1: pi ← 0.5 para todos los documentos i 2: mientras P(ΔMAP < 0) < α hacer 3: calcular el peso wi para todos los documentos i no clasificados (ver Carterette et al. [8] para más detalles) 4: j ← argmaxiwi 5: xj ← 1 si el documento j es relevante, 0 en caso contrario 6: pj ← xj 7: fin mientras la búsqueda de priors uniformes con una varianza relativamente alta. Para la agregación de expertos, los parámetros previos son α = β = 1. 4.3 Diseño Experimental Primero, queremos saber si podemos aumentar un conjunto de juicios de relevancia con un conjunto de probabilidades de relevancia para reutilizar los juicios y evaluar un nuevo conjunto de sistemas. Para cada prueba experimental: 1. Selecciona un subconjunto aleatorio de k ejecuciones. De esos k, elige un c inicial < k para evaluar. 3. Ejecutar RTC al 95% de confianza en el c inicial de aproximadamente 4. Utilizando el modelo de la Sección 3, estime las probabilidades de relevancia para todos los documentos recuperados por todos los k experimentos. Calcular EMAP para todas las ejecuciones k, y P(ΔMAP < 0) para todos los pares de ejecuciones. Hacemos lo mismo para MTC, pero omitimos el paso 4. Ten en cuenta que después de evaluar los primeros c sistemas, no realizamos juicios de relevancia adicionales. Para poner a prueba nuestro método, seleccionamos c = 2: construiremos un conjunto de juicios a partir de evaluar solo dos sistemas iniciales. Luego generalizaremos a un conjunto de k = 10 (de los cuales esos dos son un subconjunto). A medida que realizamos más pruebas, obtenemos los datos que necesitamos para probar las tres hipótesis. 4.4 Evaluación Experimental Recuerde que un conjunto de juicios es robusto si la precisión de las predicciones que realiza es al menos su confianza estimada. Una forma de evaluar la robustez es agrupar pares por su confianza, luego calcular la precisión sobre todos los pares en cada grupo. Nos gustaría que la precisión no fuera menor que la puntuación de confianza más baja en el contenedor, pero preferiblemente más alta. Dado que las estadísticas resumen son útiles, ideamos la siguiente métrica. Supongamos que somos una casa de apuestas tomando apuestas sobre si ΔMAP < 0. Utilizamos RTC o MTC para establecer las probabilidades O = P (ΔMAP <0) 1−P (ΔMAP <0) . Supongamos que un apostador apuesta $1 a que ΔMAP ≥ 0. Si resulta que ΔMAP < 0, ganamos el dólar. De lo contrario, pagamos O. Si nuestras estimaciones de confianza son perfectamente precisas, empatamos. Si la confianza es mayor que la precisión, perdemos dinero; ganamos si la precisión es mayor que la confianza. Contra intuitivamente, el resultado más deseable es empatar: si perdemos dinero, no podemos confiar en las estimaciones de confianza, pero si ganamos dinero, hemos subestimado la confianza o evaluado más documentos de los necesarios. Sin embargo, el costo de no poder confiar en las estimaciones de confianza es mayor que el costo de realizar juicios de relevancia adicionales, por lo que consideraremos los resultados positivos como buenos. La cantidad que ganamos en cada comparación par a par i es: Wi = yi − (1 − yi) Pi 1 − Pi = yi − Pi 1 − Pi yi = 1 si ΔMAP < 0 y 0 en caso contrario, y Pi = P(ΔMAP < 0). La estadística resumen es W, la media de Wi. Ten en cuenta que a medida que Pi aumenta, perdemos más por estar equivocados. Esto es como debería ser: la penalización debería ser grande por fallar en las predicciones de alta probabilidad. Sin embargo, dado que nuestras pérdidas crecen sin límite a medida que las predicciones se acercan a la certeza, limitamos −Wi a 100. Para nuestra hipótesis de que RTC requiere menos juicios que MTC, estamos interesados en el número de juicios necesarios para alcanzar el 95% de confianza en el primer par de sistemas. La mediana es más interesante que la media: la mayoría de los pares requieren unos cientos de juicios, pero algunos pares requieren varios miles. La distribución es, por lo tanto, altamente sesgada y la media se ve fuertemente afectada por esos valores atípicos. Finalmente, para nuestra hipótesis de que RTC es más preciso que MTC, analizaremos la correlación de Kendalls τ entre una clasificación de k sistemas por un pequeño conjunto de juicios y la clasificación real utilizando el conjunto completo de juicios. El tau de Kendall, una estadística no paramétrica basada en intercambios de pares entre dos listas, es una evaluación estándar para este tipo de estudio. Va desde −1 (perfectamente anticorrelacionado) hasta 1 (clasificaciones idénticas), con 0 significando que la mitad de los pares están intercambiados. Como mencionamos en la introducción, sin embargo, una medida de precisión como la correlación de rangos no es una buena evaluación de la reutilización. Lo incluimos por completitud. 4.4.1 Prueba de hipótesis Realizar múltiples pruebas permite el uso de pruebas de hipótesis estadísticas para comparar algoritmos. El uso de los mismos conjuntos de sistemas permite el uso de pruebas emparejadas. Como mencionamos anteriormente, estamos más interesados en la mediana del número de juicios que en la media. Una prueba para la diferencia en la mediana es la prueba de rango con signo de Wilcoxon. También podemos usar una prueba t pareada para probar una diferencia en la media. Para la correlación de rangos, podemos usar una prueba t pareada para probar si hay una diferencia en τ. 5. RESULTADOS Y ANÁLISIS La comparación entre MTC y RTC se muestra en la Tabla 2. Con MTC y probabilidades uniformes de relevancia, los resultados están lejos de ser robustos. No podemos reutilizar las evaluaciones de relevancia con mucha confianza. Pero con RTC, los resultados son muy robustos. Existe una ligera disminución en la precisión cuando la confianza supera el 0.95; sin embargo, las predicciones de confianza son confiables. La media de Wi muestra que RTC está mucho más cerca de 0 que MTC. La distribución de las puntuaciones de confianza muestra que al menos el 80% de confianza se logra más del 35% del tiempo, lo que indica que ninguno de los algoritmos está siendo demasiado conservador en sus estimaciones de confianza. Las estimaciones de confianza son bastante bajas en general; esto se debe a que hemos construido una colección de pruebas a partir de solo dos sistemas iniciales. Recuerde de la Sección 1 que no podemos requerir (o incluso esperar) un nivel mínimo de confianza al generalizar a nuevos sistemas. Los resultados más detallados de ambos algoritmos se muestran en la Figura 2. La línea sólida es el resultado ideal que daría W = 0. RTC está en o por encima de esta línea en todos los puntos hasta que la confianza alcance aproximadamente 0.97. Después de eso, hay una ligera disminución en la precisión que discutimos a continuación. Ten en cuenta que tanto la confianza MTC RTC % en la precisión del intervalo % en la precisión del intervalo 0.5 − 0.6 33.7% 61.7% 28.6% 61.9% 0.6 − 0.7 18.1% 73.1% 20.1% 76.3% 0.7 − 0.8 10.4% 70.1% 15.5% 78.0% 0.8 − 0.9 9.4% 69.0% 12.1% 84.9% 0.9 − 0.95 7.3% 78.0% 6.6% 93.1% 0.95 − 0.99 17.9% 70.4% 12.4% 93.4% 1.0 3.3% 68.3% 4.7% 98.9% W −5.34 −0.39 mediana juzgada 251 235 media τ 0.393 0.555 Tabla 2: Confianza en que P(ΔMAP < 0) y precisión de la predicción al generalizar un conjunto de juicios de relevancia adquiridos utilizando MTC y RTC. Cada contenedor contiene más de 1,000 pruebas de los conjuntos ad hoc 3, 5-8. RTC es mucho más robusto que MTC. W está definido en la Sección 4.4; cuanto más cerca de 0, mejor. La mediana juzgada es el número de juicios necesarios para alcanzar un 95% de confianza en los dos primeros sistemas. La media τ es la correlación de rango promedio para los 10 sistemas. 0.5 0.6 0.7 0.8 0.9 1 0.5 0.6 0.7 0.8 0.9 1 precisión confianza punto de equilibrio RTC MTC Figura 2: Confianza vs. precisión de MTC y RTC. La línea sólida es el resultado perfecto que daría W = 0; el rendimiento debería estar en o por encima de esta línea. Cada punto representa al menos 500 comparaciones en pares. Los algoritmos están claramente por encima de la línea hasta aproximadamente una confianza de 0.7. Esto se debe a que el rendimiento base en estos conjuntos de datos es alto; es bastante fácil lograr un 75% de precisión haciendo muy poco trabajo [7]. Número de juicios: La mediana del número de juicios requeridos por MTC para alcanzar un 95% de confianza en los dos primeros sistemas es de 251, un promedio de 5 por tema. La mediana requerida por RTC es 235, aproximadamente 4.7 por tema. Aunque los números son cercanos, la mediana de RTCs es significativamente más baja según una prueba de Wilcoxon pareada (p < 0.0001). Para comparación, una piscina de profundidad 100 resultaría en un mínimo de 5,000 juicios para cada par. La diferencia en las medias es mucho mayor. MTC requirió una media de 823 juicios, 16 por tema, mientras que RTC requirió una media de 502, 10 por tema. (Recordemos que las medias están fuertemente sesgadas por algunas parejas que requieren miles de juicios). Esta diferencia es significativa mediante una prueba t de muestras pareadas (p < 0.0001). El diez por ciento de los conjuntos resultaron en 100 o menos juicios (menos de dos por tema). El rendimiento en estos es muy alto: W = 0.41 y un 99.7% de precisión cuando la confianza es de al menos 0.9. Esto demuestra que incluso las colecciones pequeñas pueden ser reutilizables. Para el 50% de los conjuntos con más de 235 juicios, la precisión es del 93% cuando la confianza es de al menos 0.9. Correlación de rangos: MTC y RTC clasifican los 10 sistemas por EMAP (Eq. (1)) calculado utilizando sus respectivas estimaciones de probabilidad. La correlación de rango τ promedio entre el MAP verdadero y el EMAP es de 0.393 para MTC y 0.555 para RTC. Esta diferencia es significativa mediante una prueba t de muestras pareadas (p < 0.0001). Ten en cuenta que no esperamos que las correlaciones τ sean altas, ya que estamos clasificando los sistemas con tan pocos juicios de relevancia. Es más importante que estimemos correctamente la confianza en cada comparación de pares. Ejecutamos IP para el mismo número de juicios que MTC tomó para cada par, luego clasificamos los sistemas por MAP utilizando solo esos juicios (todos los documentos no juzgados se consideraron no relevantes). Calculamos la correlación τ con la clasificación real. La correlación media τ es de 0.398, lo cual no es significativamente diferente de MTC, pero es significativamente menor que RTC. El uso de estimaciones uniformes de probabilidad es indistinguible de la línea base, mientras que la estimación de relevancia mediante la agregación de expertos mejora el rendimiento en gran medida: casi un 40% más que tanto MTC como IP. Sobreajuste: Es posible sobreajustar: si provienen demasiados juicios de los dos primeros sistemas, la varianza en ΔMAP se reduce y las estimaciones de confianza se vuelven poco fiables. Vimos esto en la Tabla 2 y la Figura 2, donde RTC muestra una disminución en la precisión cuando la confianza está alrededor del 97%. De hecho, el número de juicios realizados antes de una predicción incorrecta es más de un 50% mayor que el número realizado antes de una predicción correcta. El sobreajuste es difícil de cuantificar exactamente, porque hacer más juicios de relevancia no siempre lo causa: a niveles de confianza más altos, se hacen más juicios de relevancia, y como muestra la Tabla 2, la precisión es mayor en esas mayores confianzas. Obviamente, tener más juicios de relevancia debería aumentar tanto la confianza como la precisión; la diferencia parece estar cuando un sistema tiene muchos más juicios que el otro. Comparaciones por pares: Nuestras comparaciones por pares se dividen en uno de tres grupos: 1. las dos ejecuciones originales de las que se obtienen los juicios de relevancia; 2. una de las ejecuciones originales vs. una de las nuevas ejecuciones; 3. dos nuevas ejecuciones. La Tabla 3 muestra los resultados de confianza frente a precisión para cada uno de estos tres grupos. Curiosamente, el rendimiento es peor al comparar una de las carreras originales con una de las carreras adicionales. Esto se debe muy probablemente a una gran diferencia en el número de juicios que afectan la varianza de ΔMAP. Sin embargo, el rendimiento es bastante bueno en los tres subconjuntos. Peor caso: El caso intuitivamente más probable de producir un error es cuando los dos sistemas que se comparan han recuperado muy pocos documentos en común. Si queremos que los juicios sean reutilizables, deberíamos ser capaces de generalizar incluso a ejecuciones que son muy diferentes de las utilizadas para adquirir los juicios de relevancia. Una medida simple de similitud de dos ejecuciones es el porcentaje promedio de documentos que recuperaron en común para cada tema [2]. Calculamos esto para todos los pares, luego observamos el rendimiento en pares con baja similitud. Los resultados se muestran en la confianza de precisión de dos ejecuciones originales, una ejecución original, ninguna ejecución original. 0.5 − 0.6 - 48.1% 62.8% 0.6 − 0.7 - 57.1% 79.2% 0.7 − 0.8 - 67.9% 81.7% 0.8 − 0.9 - 82.2% 86.3% 0.9 − 0.95 95.9% 93.7% 92.6% 0.95 − 0.99 96.2% 92.5% 93.1% 1.0 100% 98.0% 99.1% W −1.11 −0.87 −0.27 Tabla 3: Confianza vs. precisión de RTC al comparar las dos ejecuciones originales, una ejecución original y una nueva ejecución, y dos nuevas ejecuciones. RTC es robusto en los tres casos. precisión cuando la confianza es similar 0 − 0.1 0.1 − 0.2 0.2 − 0.3 0.5 − 0.6 68.4% 63.1% 61.4% 0.6 − 0.7 84.2% 78.6% 76.6% 0.7 − 0.8 82.0% 79.8% 78.9% 0.8 − 0.9 93.6% 83.3% 82.1% 0.9 − 0.95 99.3% 92.7% 92.4% 0.95 − 0.99 98.7% 93.4% 93.3% 1.0 99.9% 97.9% 98.1% W 0.44 −0.45 −0.49 Tabla 4: Confianza vs. precisión de RTC cuando un par de sistemas recuperó el 0-30% de documentos en común (dividido en 0%-10%, 10%-20% y 20%-30%). RTC es robusto en los tres casos. Tabla 4. El rendimiento es de hecho muy robusto incluso cuando la similitud es baja. Cuando las dos corridas comparten muy pocos documentos en común, W es en realidad positivo. Tanto MTC como IP tuvieron un rendimiento bastante deficiente en estos casos. Cuando la similitud estaba entre 0 y 10%, tanto MTC como IP predijeron correctamente ΔMAP solo el 60% del tiempo, en comparación con una tasa de éxito del 87.6% para RTC. Por conjunto de datos: Todos los resultados anteriores han sido solo en las colecciones ad-hoc. Realizamos los mismos experimentos en nuestros conjuntos de datos adicionales y desglosamos los resultados por conjunto de datos para ver cómo varía el rendimiento. Los resultados en la Tabla 5 muestran todo acerca de cada conjunto, incluyendo la precisión agrupada, W, la media de τ, y el número medio de juicios para alcanzar el 95% de confianza en los dos primeros sistemas. Los resultados son altamente consistentes de una colección a otra, lo que sugiere que nuestro método no está sobreajustado a ningún conjunto de datos en particular. CONCLUSIONES Y TRABAJO FUTURO En este trabajo hemos ofrecido la primera definición formal de la idea común de reutilización de una colección de pruebas y presentado un modelo que es capaz de lograr la reutilización con conjuntos muy pequeños de juicios de relevancia. La Tabla 2 y la Figura 2 juntas muestran cuán sesgado puede ser un pequeño conjunto de juicios: MTC está sobreestimando drásticamente la confianza y es mucho menos preciso que RTC, que puede eliminar el sesgo para proporcionar una evaluación sólida. Las estimaciones de confianza de RTC, además de ser precisas, proporcionan una guía para obtener juicios adicionales: enfóquese en juzgar documentos de las comparaciones de menor confianza. A largo plazo, observamos pequeños conjuntos de juicios de relevancia, precisión, confianza ad-hoc 94, ad-hoc 96, ad-hoc 97, ad-hoc 98, ad-hoc 99, web 04, robust 05, terabyte 05, 0.5 − 0.6 64.1%, 61.8%, 62.2%, 62.0%, 59.4%, 64.3%, 61.5%, 61.6%, 0.6 − 0.7 76.1%, 77.8%, 74.5%, 78.2%, 74.3%, 78.1%, 75.9%, 75.9%, 0.7 − 0.8 75.2%, 78.9%, 77.6%, 80.0%, 78.6%, 82.6%, 77.5%, 80.4%, 0.8 − 0.9 83.2%, 85.5%, 84.6%, 84.9%, 86.8%, 84.5%, 86.7%, 87.7%, 0.9 − 0.95 93.0%, 93.6%, 92.8%, 93.7%, 92.6%, 94.2%, 93.9%, 94.2%, 0.95 − 0.99 93.1%, 94.3%, 93.1%, 93.7%, 92.8%, 95.0%, 93.9%, 91.6%, 1.0 99.2%, 96.8%, 98.7%, 99.5%, 99.6%, 100%, 99.2%, 98.3%, W -0.34, -0.34, -0.48, -0.35, -0.44, -0.07, -0.41, -0.67, mediana juzgada 235, 276, 243, 213, 179, 448, 310, 320, media τ 0.538, 0.573, 0.556, 0.579, 0.532, 0.596, 0.565, 0.574. Tabla 5: Precisión, W, media τ y mediana del número de juicios para los 8 conjuntos de pruebas. Los resultados son altamente consistentes en todos los conjuntos de datos. A medida que pasa el tiempo, el número de juicios crece hasta que hay un 100% de confianza en cada evaluación, y hay una colección completa de pruebas para la tarea. Vemos un uso adicional para este método en escenarios como la recuperación web en los que el corpus cambia con frecuencia. Podría aplicarse a la evaluación en una colección de pruebas dinámica según lo definido por Soboroff [18]. El modelo que presentamos en la Sección 3 no es en absoluto la única posibilidad para crear una colección de pruebas robusta. Un modelo de agregación de expertos más simple podría funcionar igual de bien o mejor (aunque todos nuestros esfuerzos por simplificar fallaron). Además de la agregación de expertos, podríamos estimar probabilidades al observar similitudes entre documentos. Esta es un área obvia para futuras exploraciones. Además, será valioso investigar el problema del sobreajuste: las circunstancias en las que ocurre y qué se puede hacer para prevenirlo. Mientras tanto, limitar las estimaciones de confianza al 95% es un truco que resuelve el problema. Tenemos muchos más resultados experimentales que desafortunadamente no tuvimos espacio para incluir, pero que refuerzan la idea de que RTC es altamente robusto: con solo unos pocos juicios por tema, podemos evaluar con precisión la confianza en cualquier comparación de sistemas de forma pareja. Agradecimientos Este trabajo fue apoyado en parte por el Centro de Recuperación de Información Inteligente y en parte por la Agencia de Proyectos de Investigación Avanzada de Defensa (DARPA) bajo el número de contrato HR0011-06-C-0023. Cualquier opinión, hallazgo, conclusión o recomendación expresada en este material es responsabilidad del autor y no necesariamente refleja la del patrocinador. REFERENCIAS [1] J. Aslam y M. Montague. Modelos para Metabúsqueda. En Actas de SIGIR, páginas 275-285, 2001. [2] J. Aslam y R. Savell. Sobre la efectividad de evaluar sistemas de recuperación en ausencia de juicios de relevancia. En Actas de SIGIR, páginas 361-362, 2003. [3] J. A. Aslam, V. Pavlu y R. Savell. Un modelo unificado para metabúsqueda, agrupación y evaluación de sistemas. En Actas de CIKM, páginas 484-491, 2003. [4] J. A. Aslam, V. Pavlu y E. Yilmaz. Un método estadístico para la evaluación de sistemas utilizando juicios incompletos. En Actas de SIGIR, páginas 541-548, 2006. [5] A. L. Berger, S. D. Pietra y V. J. D. Pietra. Un enfoque de entropía máxima para el procesamiento del lenguaje natural. Lingüística Computacional, 22(1):39-71, 1996. [6] D. J. Blower. Una derivación sencilla de la regresión logística desde la perspectiva bayesiana y de máxima entropía. En Actas del 23º Taller Internacional sobre Inferencia Bayesiana y Métodos de Entropía Máxima en Ciencia e Ingeniería, páginas 30-43, 2004. [7] B. Carterette y J. Allan. Metodología de investigación en estudios del esfuerzo del evaluador para la evaluación de recuperación. En Actas de RIAO, 2007. [8] B. Carterette, J. Allan y R. K. Sitaraman. Colecciones de prueba mínimas para evaluación de recuperación. En Actas de SIGIR, páginas 268-275, 2006. [9] B. Carterette y D. I. Petkova. Aprendiendo un ranking a partir de preferencias por pares. En Actas de SIGIR, 2006. [10] R. T. Clemen y R. L. Winkler. Unanimidad y compromiso entre pronosticadores de probabilidad. Ciencia de la Gestión, 36(7):767-779, julio de 1990. [11] G. V. Cormack, C. R. Palmer y C. L. Clarke. Construcción eficiente de grandes colecciones de pruebas. En Actas de SIGIR, páginas 282-289, 1998. [12] A. Gelman, J. B. Carlin, H. S. Stern y D. B. Rubin. Análisis de datos bayesianos. Chapman & Hall/CRC, 2004. [13] E. T. Jaynes.\nChapman & Hall/CRC, 2004. [13] E. T. Jaynes. Teoría de la probabilidad: La lógica de la ciencia. Cambridge University Press, 2003. [14] R. Manmatha y H. Sever. Un enfoque formal para la normalización de puntuaciones en metabúsqueda. En Actas de HLT, páginas 88-93, 2002. [15] I. J. Myung, S. Ramamoorti y J. Andrew D. Baily. Agregación de predicciones de expertos mediante entropía máxima. Ciencia de la Gestión, 42(10):1420-1436, octubre de 1996. [16] J. Platt. Salidas probabilísticas para máquinas de vectores de soporte y comparación con métodos de verosimilitud regularizados. páginas 61-74, 2000. [17] M. Sanderson y H. Joho. Formando colecciones de pruebas sin agrupamiento de sistemas. En Actas de la 27ª Conferencia Internacional Anual de ACM SIGIR sobre Investigación y Desarrollo en Recuperación de Información, páginas 33-40, 2004. [18] I. Soboroff. Colecciones de pruebas dinámicas: midiendo la efectividad de la búsqueda en la web en vivo. En Actas de SIGIR, páginas 276-283, 2006. [19] K. Sparck Jones y C. J. van Rijsbergen. Colecciones de pruebas de recuperación de información. Revista de Documentación, 32(1):59-75, 1976. [20] E. M. Voorhees y D. K. Harman, editores. TREC: Experimento y Evaluación en Recuperación de Información. MIT Press, 2005. [21] J. Zobel. \n\nMIT Press, 2005. [21] J. Zobel. ¿Qué tan confiables son los resultados de los experimentos de recuperación de información a gran escala? En Actas de SIGIR, páginas 307-314, 1998. ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "relevance distribution": {
            "translated_key": "distribución de relevancia",
            "is_in_text": false,
            "original_annotated_sentences": [
                "Robust Test Collections for Retrieval Evaluation Ben Carterette Center for Intelligent Information Retrieval Computer Science Department University of Massachusetts Amherst Amherst, MA 01003 carteret@cs.umass.edu ABSTRACT Low-cost methods for acquiring relevance judgments can be a boon to researchers who need to evaluate new retrieval tasks or topics but do not have the resources to make thousands of judgments.",
                "While these judgments are very useful for a one-time evaluation, it is not clear that they can be trusted when re-used to evaluate new systems.",
                "In this work, we formally define what it means for judgments to be reusable: the confidence in an evaluation of new systems can be accurately assessed from an existing set of relevance judgments.",
                "We then present a method for augmenting a set of relevance judgments with relevance estimates that require no additional assessor effort.",
                "Using this method practically guarantees reusability: with as few as five judgments per topic taken from only two systems, we can reliably evaluate a larger set of ten systems.",
                "Even the smallest sets of judgments can be useful for evaluation of new systems.",
                "Categories and Subject Descriptors: H.3 Information Storage and Retrieval; H.3.4 Systems and Software: Performance Evaluation General Terms: Experimentation, Measurement, Reliability 1.",
                "INTRODUCTION Consider an information retrieval researcher who has invented a new retrieval task.",
                "She has built a system to perform the task and wants to evaluate it.",
                "Since the task is new, it is unlikely that there are any extant relevance judgments.",
                "She does not have the time or resources to judge every document, or even every retrieved document.",
                "She can only judge the documents that seem to be the most informative and stop when she has a reasonable degree of confidence in her conclusions.",
                "But what happens when she develops a new system and needs to evaluate it?",
                "Or another research group decides to implement a system to perform the task?",
                "Can they reliably reuse the original judgments?",
                "Can they evaluate without more relevance judgments?",
                "Evaluation is an important aspect of information retrieval research, but it is only a semi-solved problem: for most retrieval tasks, it is impossible to judge the relevance of every document; there are simply too many of them.",
                "The solution used by NIST at TREC (Text REtrieval Conference) is the pooling method [19, 20]: all competing systems contribute N documents to a pool, and every document in that pool is judged.",
                "This method creates large sets of judgments that are reusable for training or evaluating new systems that did not contribute to the pool [21].",
                "This solution is not adequate for our hypothetical researcher.",
                "The pooling method gives thousands of relevance judgments, but it requires many hours of (paid) annotator time.",
                "As a result, there have been a slew of recent papers on reducing annotator effort in producing test collections: Cormack et al. [11], Zobel [21], Sanderson and Joho [17], Carterette et al. [8], and Aslam et al. [4], among others.",
                "As we will see, the judgments these methods produce can significantly bias the evaluation of a new set of systems.",
                "Returning to our hypothetical resesarcher, can she reuse her relevance judgments?",
                "First we must formally define what it means to be reusable.",
                "In previous work, reusability has been tested by simply assessing the accuracy of a set of relevance judgments at evaluating unseen systems.",
                "While we can say that it was right 75% of the time, or that it had a rank correlation of 0.8, these numbers do not have any predictive power: they do not tell us which systems are likely to be wrong or how confident we should be in any one.",
                "We need a more careful definition of reusability.",
                "Specifically, the question of reusability is not how accurately we can evaluate new systems.",
                "A malicious adversary can always produce a new ranked list that has not retrieved any of the judged documents.",
                "The real question is how much confidence we have in our evaluations, and, more importantly, whether we can trust our estimates of confidence.",
                "Even if confidence is not high, as long as we can trust it, we can identify which systems need more judgments in order to increase confidence.",
                "Any set of judgments, no matter how small, becomes reusable to some degree.",
                "Small, reusable test collections could have a huge impact on information retrieval research.",
                "Research groups would be able to share the relevance judgments they have done in-house for pilot studies, new tasks, or new topics.",
                "The amount of data available to researchers would grow exponentially over time. 2.",
                "ROBUST EVALUATION Above we gave an intuitive definition of reusability: a collection is reusable if we can trust our estimates of confidence in an evaluation.",
                "By that we mean that if we have made some relevance judgments and have, for example, 75% confidence that system A is better than system B, we would like there to be no more than 25% chance that our assessment of the relative quality of the systems will change as we continue to judge documents.",
                "Our evaluation should be robust to missing judgments.",
                "In our previous work, we defined confidence as the probability that the difference in an evaluation measure calculated for two systems is less than zero [8].",
                "This notion of confidence is defined in the context of a particular evaluation task that we call comparative evaluation: determining the sign of the difference in an evaluation measure.",
                "Other evaluation tasks could be defined; estimating the magnitude of the difference or the values of the measures themselves are examples that entail different notions of confidence.",
                "We therefore see confidence as a probability estimate.",
                "One of the questions we must ask about a probability estimate is what it means.",
                "What does it mean to have 75% confidence that system A is better than system B?",
                "As described above, we want it to mean that if we continue to judge documents, there will only be a 25% chance that our assessment will change.",
                "If this is what it means, we can trust the confidence estimates.",
                "But do we know it has that meaning?",
                "Our calculation of confidence rested on an assumption about the probability of relevance of unjudged documents, specifically that each unjudged document was equally likely to be relevant or nonrelevant.",
                "This assumption is almost certainly not realistic in most IR applications.",
                "As it turns out, it is this assumption that determines whether the confidence estimates can eb trusted.",
                "Before elaborating on this, we formally define confidence. 2.1 Estimating Confidence Average precision (AP) is a standard evaluation metric that captures both the ability of a system to rank relevant documents highly (precision) as well as its ability to retrieve relevant documents (recall).",
                "It is typically written as the mean precision at the ranks of relevant documents: AP = 1 |R| i∈R prec@r(i) where R is the set of relevant documents and r(i) is the rank of document i.",
                "Let Xi be a random variable indicating the relevance of document i.",
                "If documents are ordered by rank, we can express precision as prec@i = 1/i i j=1 Xj .",
                "Average precision then becomes the quadratic equation AP = 1 Xi n i=1 Xi/i i j=1 Xj = 1 Xi n i=1 j≥i aijXiXj where aij = 1/ max{r(i), r(j)}.",
                "Using aij instead of 1/i allows us to number the documents arbitrarily.",
                "To see why this is true, consider a toy example: a list of 3 documents with relevant documents B, C at ranks 1 and 3 and nonrelevant document A at rank 2.",
                "Average precision will be 1 2 (1 1 x2 B+ 1 2 xBxA+ 1 3 xBxC + 1 2 x2 A+ 1 3 xAxC + 1 3 x2 C) = 1 2 1 + 2 3 because xA = 0, xB = 1, xC = 1.",
                "Though the ordering B, A, C is different from the labeling A, B, C, it does not affect the computation.",
                "We can now see average precision itself is a random variable with a distribution over all possible assignments of relevance to all documents.",
                "This random variable has an expectation, a variance, confidence intervals, and a certain probability of being less than or equal to a given value.",
                "All of these are dependent on the probability that document i is relevant: pi = p(Xi = 1).",
                "Suppose in our previous example we do not know the relevance judgments, but we believe pA = 0.4, pB = 0.8, pC = 0.7.",
                "We can then compute e.g.",
                "P(AP = 0) = 0.2 · 0.6 · 0.3 = 0.036, or P(AP = 1 2 ) = 0.2 · 0.4 · 0.7 = 0.056.",
                "Summing over all possibilities, we can compute expectation and variance: E[AP] ≈ 1 pi aiipi + j>i aij pipj V ar[AP] ≈ 1 ( pi)2 n i a2 iipiqi + j>i a2 ijpipj(1 − pipj) + i=j 2aiiaijpipj(1 − pi) + k>j=i 2aijaikpipjpk(1 − pi) AP asymptotically converges to a normal distribution with expectation and variance as defined above.1 For our comparative evaluation task we are interested in the sign of the difference in two average precisions: ΔAP = AP1 − AP2.",
                "As we showed in our previous work, ΔAP has a closed form when documents are ordered arbitrarily: ΔAP = 1 Xi n i=1 j≥i cij XiXj cij = aij − bij where bij is defined analogously to aij for the second ranking.",
                "Since AP is normal, ΔAP is normal as well, meaning we can use the normal cumulative density function to determine the confidence that a difference in AP is less than zero.",
                "Since topics are independent, we can easily extend this to mean average precision (MAP).",
                "MAP is also normally distributed; its expectation and variance are: EMAP = 1 T t∈T E[APt] (1) VMAP = 1 T2 t∈T V ar[APt] ΔMAP = MAP1 − MAP2 Confidence can then be estimated by calculating the expectation and variance and using the normal density function to find P(ΔMAP < 0). 2.2 Confidence and Robustness Having defined confidence, we turn back to the issue of trust in confidence estimates, and show how it ties into the robustness of the collection to missing judgments. 1 These are actually approximations to the true expectation and variance, but the error is a negligible O(n2−n ).",
                "Let Z be the set of all pairs of ranked results for a common set of topics.",
                "Suppose we have a set of m relevance judgments xm = {x1, x2, ..., xm} (using small x rather than capital X to distinguish between judged and unjudged documents); these are the judgments against which we compute confidence.",
                "Let Zα be the subset of pairs in Z for which we predict that ΔMAP = −1 with confidence α given the judgments xm .",
                "For the confidence estimates to be accurate, we need at least α · |Zα| of these pairs to actually have ΔMAP = −1 after we have judged every document.",
                "If they do, we can trust the confidence estimates; our evaluation will be robust to missing judgments.",
                "If our confidence estimates are based on unrealistic assumptions, we cannot expect them to be accurate.",
                "The assumptions they are based on are the probabilities of relevance pi.",
                "We need these to be realistic.",
                "We argue that the best possible distribution of relevance p(Xi) is the one that explains all of the data (all of the observations made about the retrieval systems) while at the same time making no unwarranted assumptions.",
                "This is known as the principle of maximum entropy [13].",
                "The entropy of a random variable X with distribution p(X) is defined as H(p) = − i p(X = i) log p(X = i).",
                "This has found a wide array of uses in computer science and information retrieval.",
                "The maximum entropy distribution is the one that maximizes H. This distribution is unique and has an exponential form.",
                "The following theorem shows the utility of a maximum entropy distribution for relevance when estimating confidence.",
                "Theorem 1.",
                "If p(Xn |I, xm ) = argmaxpH(p), confidence estimates will be accurate. where xm is the set of relevance judgments defined above, Xn is the full set of documents that we wish to estimate the relevance of, and I is some information about the documents (unspecified as of now).",
                "We forgo the proof for the time being, but it is quite simple.",
                "This says that the better the estimates of relevance, the more accurate the evaluation.",
                "The task of creating a reusable test collection thus becomes the task of estimating the relevance of unjudged documents.",
                "The theorem and its proof say nothing whatsoever about the evaluation metric.",
                "The probability estimates are entirely indepedent of the measure we are interested in.",
                "This means the same probability estimates can tell us about average precision as well as precision, recall, bpref, etc.",
                "Furthermore, we could assume that the relevance of documents i and j is independent and achieve the same result, which we state as a corollary: Corollary 1.",
                "If p(Xi|I, xm ) = argmaxpH(p), confidence estimates will be accurate.",
                "The task therefore becomes the imputation of the missing values of relevance.",
                "The theorem implies that the closer we get to the maximum entropy distribution of relevance, the closer we get to robustness. 3.",
                "PREDICTING RELEVANCE In our statement of Theorem 1, we left the nature of the information I unspecified.",
                "One of the advantages of our confidence estimates is that they admit information from a wide variety of sources; essentially anything that can be modeled can be used as information for predicting relevance.",
                "A natural source of information is the retrieval systems themselves: how they ranked the judged documents, how often they failed to rank relevant documents, how they perform across topics, and so on.",
                "If we treat each system as an information retrieval expert providing an opinion about the relevance of each document, the problem becomes one of expert opinion aggregation.",
                "This is similar to the metasearch or data fusion problem in which the task is to take k input systems and merge them into a single ranking.",
                "Aslam et al. [3] previously identified a connection between evaluation and metasearch.",
                "Our problem has two key differences: 1.",
                "We explicitly need probabilities of relevance that we can plug into Eq. 1; metasearch algorithms have no such requirement. 2.",
                "We are accumulating relevance judgments as we proceed with the evaluation and are able to re-estimate relevance given each new judgment.",
                "In light of (1) above, we introduce a probabilistic model for expert combination. 3.1 A Model for Expert Opinion Aggregation Suppose that each expert j provides a probability of relevance qij = pj(Xi = 1).",
                "The information about the relevance of document i will then be the set of k expert opinions I = qi = (qi1, qi2, · · · , qik).",
                "The probability distribution we wish to find is the one that maximizes the entropy of pi = p(Xi = 1|qi).",
                "As it turns out, finding the maximum entropy model is equivalent to finding the parameters that maximize the likelihood [5].",
                "Blower [6] explicitly shows that finding the maximum entropy model for a binary variable is equivalent to solving a logistic regression.",
                "Then pi = p(Xi = 1|qi) = exp k j=1 λjqij 1 + exp k j=1 λj qij (2) where λ1, · · · , λk are the regression parameters.",
                "We include a beta prior for p(λj) with parameters α, β.",
                "This can be seen as a type of smoothing to account for the fact that the training data is highly biased.",
                "This model has the advantage of including the statistical dependence between the experts.",
                "A model of the same form was shown by Clemen & Winkler to be the best for aggregating expert probabilities [10].",
                "A similar maximumentropy-motivated approach has been used for expert aggregation [15].",
                "Aslam & Montague [1] used a similar model for metasearch, but assumed independence among experts.",
                "Where do the qij s come from?",
                "Using raw, uncalibrated scores as predictors will not work because score distributions vary too much between topics.",
                "A language modeling ranker, for instance, will typically give a much higher score to the top retrieved document for a short query than to the top retrieved document for a long query.",
                "We could train a separate predicting model for each topic, but that does not take advantage of all of the information we have: we may only have a handful of judgments for a topic, not enough to train a model to any confidence.",
                "Furthermore, it seems reasonable to assume that if an expert makes good predictions for one topic, it will make good predictions for other topics as well.",
                "We could use a hierarchical model [12], but that will not generalize to unseen topics.",
                "Instead, we will calibrate the scores of each expert individually so that scores can be compared both within topic and between topic.",
                "Thus our model takes into account not only the dependence between experts, but also the dependence between experts performances on different tasks (topics). 3.2 Calibrating Experts Each expert gives us a score and a rank for each document.",
                "We need to convert these to probabilities.",
                "A method such as the one used by Manmatha et al. [14] could be used to convert scores into probabilities of relevance.",
                "The pairwise preference method of Carterette & Petkova [9] could also be used, interpeting the ranking of one document over another as an expression of preference.",
                "Let q∗ ij be expert js self-reported probability that document i is relevant.",
                "Intuitively it seems clear that q∗ ij should decrease with rank, and it should be zero if document i is unranked (the expert did not believe it to be relevant).",
                "The pairwise preference model can handle these two requirements easily, so we will use it.",
                "Let θrj (i) be the relevance coefficient of the document at rank rj(i).",
                "We want to find the θs that maximize the likelihood function: Ljt(Θ) = rj (i)<rj (k) exp(θrj (i) − θrj (k)) 1 + exp(θrj (i) − θrj (k)) We again include a beta prior on p(θrj(i)) with parameters |Rt| + 1 and |Nt| + 1, the size of the sets of judged relevant and nonrelevant documents respectively.",
                "Using these as prior parameters ensures that the resulting probabilities will be concentrated around the ratio of relevant documents that have been discovered for topic t. This means that the probability estimates decrease by rank and are higher for topics that have more relevant documents.",
                "After finding the Θ that maximizes the likelihood, we have q∗ ij = exp(θrj (i)) 1+exp(θrj (i)) .",
                "We define θ∞ = −∞, so that the probability that an unranked document is relevant is 0.",
                "Since q∗ ij is based on the rank at which a document is retrieved rather than the identity of the document itself, the probabilities are identical from expert to expert, e.g. if expert E put document A at rank 1, and expert D put document B at rank 1, we will have q∗ AE = q∗ BD.",
                "Therefore we only have to solve this once for each topic.",
                "The above model gives topic-independent probabilities for each document.",
                "But suppose an expert who reports 90% probability is only right 50% of the time.",
                "Its opinion should be discounted based on its observed performance.",
                "Specifically, we want to learn a calibration function qij = Cj(q∗ ij) that will ensure that the predicted probabilities are tuned to the experts ability to retrieve relevant documents given the judgments that have been made to this point.",
                "Platts SVM calibration method [16] fits a sigmoid function between q∗ ij and the relevance judgments to obtain qij = Cj (q∗ ij) = exp(Aj +Bjq∗ ij ) 1+exp(Aj +Bj q∗ ij ) .",
                "Since q∗ ij is topic-independent, we only need to learn one calibration function for each expert.",
                "Once we have the calibration function, it is applied to adjust the experts predictions to their actual performance.",
                "The calibrated probabilities are plugged into model (2) to find the document probabilities.",
                "Figure 1: Conceptual diagram of our aggregation model.",
                "Experts E1, E2 have ranked documents A, B, C for topic T1 and documents D, E, F for topic T2.",
                "The first step is to obtain q∗ ij.",
                "Next is calibration to true performance to find qij .",
                "Finally we obtain pi = p(Xi = 1|qi1, qi2), · · · . 3.3 Model Summary Our model has three components that differ by the data they take as input and what they produce as output.",
                "A conceptual diagram is shown in Figure 1. 1. ranks → probabilities (per system per topic).",
                "This gives us q∗ ij, expert js self-reported probability of the relevance of document i.",
                "This is unsupervised; it requires no labeled data (though if we have some, we use it to set prior parameters). 2. probabilities → calibrated probabilities (per system).",
                "This gives us qij = Cj (q∗ ij), expert js calibrated probability of the relevance of document i.",
                "This is semisupervised; we have relevance judgments at some ranks which we use to impute the probability of relevance at other ranks. 3. calibrated probabilities → document probabilities.",
                "This gives us pi = p(Xi = 1|qi), the probability of relevance of document i given calibrated expert probabilities qij .",
                "This is supervised; we learn coefficients from a set of judged documents and use those to estimate the relevance of the unjudged documents.",
                "Although the model appears rather complex, it is really just three successive applications of logistic regression.",
                "As such, it can be implemented in a statistical programming language such as R in a few lines of code.",
                "The use of beta (conjugate) priors ensures that no expensive computational methods such as MCMC are necessary [12], so the model is trained and applied fast enough to be used on-line.",
                "Our code is available at http://ciir.cs.umass.edu/~carteret/. 4.",
                "EXPERIMENTS Three hypotheses are under consideration.",
                "The first, and most important, is that using our expert aggregation model to predict relevance produces test collections that are robust enough to be reusable; that is, we can trust the estimates of confidence when we evaluate systems that did not contribute any judgments to the pool.",
                "The other two hypotheses relate to the improvement we see by using better estimates of relevance than we did in our previous work [8].",
                "These are that (a) it takes fewer relevance track no. topics no. runs no. judged no. rel ad-hoc 94 50 40 97,319 9,805 ad-hoc 95 49 33 87,069 6,503 ad-hoc 96 50 61 133,681 5,524 ad-hoc 97 50 74 72,270 4,611 ad-hoc 98 50 103 80,345 4,674 ad-hoc 99 50 129 86,830 4,728 web 04 225 74 88,566 1,763 robust 05 50 74 37,798 6,561 terabyte 05 50 58 45,291 10,407 Table 1: Number of topics, number of runs, number of documents judged, and number found relevant for each of our data sets. judgments to reach 95% confidence and (b) the accuracy of the predictions is higher than if we were to simply assume pi = 0.5 for all unjudged documents. 4.1 Data We obtained full ad-hoc runs submitted to TRECs 3 through 8.",
                "Each run ranks at most 1000 documents for 50 topics (49 topics for TREC 4).",
                "Additionally, we obtained all runs from the Web track of TREC 13, the Robust2 track of TREC 14, and the Terabyte (ad-hoc) track of TREC 14.",
                "These are the tracks that have replaced the ad-hoc track since its end in 1999.",
                "Statistics are shown in Table 1.",
                "We set aside the TREC 4 (ad-hoc 95) set for training, TRECs 3 and 5-8 (ad-hoc 94 and 96-99) for primary testing, and the remaining sets for additional testing.",
                "We use the qrels files assembled by NIST as truth.",
                "The number of relevance judgments made and relevant documents found for each track are listed in Table 1.",
                "For computational reasons, we truncate ranked lists at 100 documents.",
                "There is no reason that we could not go deeper, but calculating variance is O(n3 ) and thus very timeconsuming.",
                "Because of the reciprocal rank nature of AP, we do not lose much information by truncating at rank 100. 4.2 Algorithms We will compare three algorithms for acquiring relevance judgments.",
                "The baseline is a variation of TREC pooling that we will call incremental pooling (IP).",
                "This algorithm takes a number k as input and presents the first k documents in rank order (without regard to topic) to be judged.",
                "It does not estimate the relevance of unjudged documents; it simply assumes any unjudged document is nonrelevant.",
                "The second algorithm is that presented in Carterette et al. [8] (Algorithm 1).",
                "Documents are selected based on how interesting they are in determining whether a difference in mean average precision exists.",
                "For this approach pi = 0.5 for all i; there is no estimation of probabilities.",
                "We will call this MTC for minimal test collection.",
                "The third algorithm augments MTC with updated estimates of probabilities of relevance.",
                "We will call this RTC for robust test collection.",
                "It is identical to Algorithm 1, except that every 10th iteration we estimate pi for all unjudged documents i using the expert aggregation model of Section 3.",
                "RTC has smoothing (prior distribution) parameters that must be set.",
                "We trained using the ad-hoc 95 set.",
                "We limited 2 Robust here means robust retrieval; this is different from our goal of robust evaluation.",
                "Algorithm 1 (MTC) Given two ranked lists and confidence level α, predict the sign of ΔMAP. 1: pi ← 0.5 for all documents i 2: while P(ΔMAP < 0) < α do 3: calculate weight wi for all unjudged documents i (see Carterette et al. [8] for details) 4: j ← argmaxiwi 5: xj ← 1 if document j is relevant, 0 otherwise 6: pj ← xj 7: end while the search to uniform priors with relatively high variance.",
                "For expert aggregation, the prior parameters are α = β = 1. 4.3 Experimental Design First, we want to know whether we can augment a set of relevance judgments with a set of relevance probabilities in order to reuse the judgments to evaluate a new set of systems.",
                "For each experimental trial: 1.",
                "Pick a random subset of k runs. 2.",
                "From those k, pick an initial c < k to evaluate. 3.",
                "Run RTC to 95% confidence on the initial c. 4.",
                "Using the model from Section 3, estimate the probabilities of relevance for all documents retrieved by all k runs. 5.",
                "Calculate EMAP for all k runs, and P(ΔMAP < 0) for all pairs of runs.",
                "We do the same for MTC, but omit step 4.",
                "Note that after evaluating the first c systems, we make no additional relevance judgments.",
                "To put our method to the test, we selected c = 2: we will build a set of judgments from evaluating only two initial systems.",
                "We will then generalize to a set of k = 10 (of which those two are a subset).",
                "As we run more trials, we obtain the data we need to test all three of our hypotheses. 4.4 Experimental Evaluation Recall that a set of judgments is robust if the accuracy of the predictions it makes is at least its estimated confidence.",
                "One way to evaluate robustness is to bin pairs by their confidence, then calculate the accuracy over all the pairs in each bin.",
                "We would like the accuracy to be no less than the lowest confidence score in the bin, but preferably higher.",
                "Since summary statistics are useful, we devised the following metric.",
                "Suppose we are a bookmaker taking bets on whether ΔMAP < 0.",
                "We use RTC or MTC to set the odds O = P (ΔMAP <0) 1−P (ΔMAP <0) .",
                "Suppose a bettor wagers $1 on ΔMAP ≥ 0.",
                "If it turns out that ΔMAP < 0, we win the dollar.",
                "Otherwise, we pay out O.",
                "If our confidence estimates are perfectly accurate, we break even.",
                "If confidence is greater than accuracy, we lose money; we win if accuracy is greater than confidence.",
                "Counterintuitively, the most desirable outcome is breaking even: if we lose money, we cannot trust the confidence estimates, but if we win money, we have either underestimated confidence or judged more documents than necessary.",
                "However, the cost of not being able to trust the confidence estimates is higher than the cost of extra relevance judgments, so we will treat positive outcomes as good.",
                "The amount we win on each pairwise comparison i is: Wi = yi − (1 − yi) Pi 1 − Pi = yi − Pi 1 − Pi yi = 1 if ΔMAP < 0 and 0 otherwise, and Pi = P(ΔMAP < 0).",
                "The summary statistic is W, the mean of Wi.",
                "Note that as Pi increases, we lose more for being wrong.",
                "This is as it should be: the penalty should be great for missing the high probability predictions.",
                "However, since our losses grow without bound as predictions approach certainty, we cap −Wi at 100.",
                "For our hypothesis that RTC requires fewer judgments than MTC, we are interested in the number of judgments needed to reach 95% confidence on the first pair of systems.",
                "The median is more interesting than the mean: most pairs require a few hundred judgments, but a few pairs require several thousand.",
                "The distribution is therefore highly skewed, and the mean strongly affected by those outliers.",
                "Finally, for our hypothesis that RTC is more accurate than MTC, we will look at Kendalls τ correlation between a ranking of k systems by a small set of judgments and the true ranking using the full set of judgments.",
                "Kendalls τ, a nonparametric statistic based on pairwise swaps between two lists, is a standard evaluation for this type of study.",
                "It ranges from −1 (perfectly anti-correlated) to 1 (rankings identical), with 0 meaning that half of the pairs are swapped.",
                "As we touched on in the introduction, though, an accuracy measure like rank correlation is not a good evaluation of reusability.",
                "We include it for completeness. 4.4.1 Hypothesis Testing Running multiple trials allows the use of statistical hypothesis testing to compare algorithms.",
                "Using the same sets of systems allows the use of paired tests.",
                "As we stated above, we are more interested in the median number of judgments than the mean.",
                "A test for difference in median is the Wilcoxon sign rank test.",
                "We can also use a paired t-test to test for a difference in mean.",
                "For rank correlation, we can use a paired t-test to test for a difference in τ. 5.",
                "RESULTS AND ANALYSIS The comparison between MTC and RTC is shown in Table 2.",
                "With MTC and uniform probabilities of relevance, the results are far from robust.",
                "We cannot reuse the relevance judgments with much confidence.",
                "But with RTC, the results are very robust.",
                "There is a slight dip in accuracy when confidence gets above 0.95; nonetheless, the confidence predictions are trustworthy.",
                "Mean Wi shows that RTC is much closer to 0 than MTC.",
                "The distribution of confidence scores shows that at least 80% confidence is achieved more than 35% of the time, indicating that neither algorithm is being too conservative in its confidence estimates.",
                "The confidence estimates are rather low overall; that is because we have built a test collection from only two initial systems.",
                "Recall from Section 1 that we cannot require (or even expect) a minimum level of confidence when we generalize to new systems.",
                "More detailed results for both algorithms are shown in Figure 2.",
                "The solid line is the ideal result that would give W = 0.",
                "RTC is on or above this line at all points until confidence reaches about 0.97.",
                "After that there is a slight dip in accuracy which we discuss below.",
                "Note that both MTC RTC confidence % in bin accuracy % in bin accuracy 0.5 − 0.6 33.7% 61.7% 28.6% 61.9% 0.6 − 0.7 18.1% 73.1% 20.1% 76.3% 0.7 − 0.8 10.4% 70.1% 15.5% 78.0% 0.8 − 0.9 9.4% 69.0% 12.1% 84.9% 0.9 − 0.95 7.3% 78.0% 6.6% 93.1% 0.95 − 0.99 17.9% 70.4% 12.4% 93.4% 1.0 3.3% 68.3% 4.7% 98.9% W −5.34 −0.39 median judged 251 235 mean τ 0.393 0.555 Table 2: Confidence that P(ΔMAP < 0) and accuracy of prediction when generalizing a set of relevance judgments acquired using MTC and RTC.",
                "Each bin contains over 1,000 trials from the adhoc 3, 5-8 sets.",
                "RTC is much more robust than MTC.",
                "W is defined in Section 4.4; closer to 0 is better.",
                "Median judged is the number of judgments to reach 95% confidence on the first two systems.",
                "Mean τ is the average rank correlation for all 10 systems. 0.5 0.6 0.7 0.8 0.9 1 0.5 0.6 0.7 0.8 0.9 1 accuracy confidence breakeven RTC MTC Figure 2: Confidence vs. accuracy of MTC and RTC.",
                "The solid line is the perfect result that would give W = 0; performance should be on or above this line.",
                "Each point represents at least 500 pairwise comparisons. algorithms are well above the line up to around confidence 0.7.",
                "This is because the baseline performance on these data sets is high; it is quite easy to achieve 75% accuracy doing very little work [7].",
                "Number of Judgments: The median number of judgments required by MTC to reach 95% confidence on the first two systems is 251, an average of 5 per topic.",
                "The median required by RTC is 235, about 4.7 per topic.",
                "Although the numbers are close, RTCs median is significantly lower by a paired Wilcoxon test (p < 0.0001).",
                "For comparison, a pool of depth 100 would result in a minimum of 5,000 judgments for each pair.",
                "The difference in means is much greater.",
                "MTC required a mean of 823 judgments, 16 per topic, while RTC required a mean of 502, 10 per topic. (Recall that means are strongly skewed by a few pairs that take thousands of judgments.)",
                "This difference is significant by a paired t-test (p < 0.0001).",
                "Ten percent of the sets resulted in 100 or fewer judgments (less than two per topic).",
                "Performance on these is very high: W = 0.41, and 99.7% accuracy when confidence is at least 0.9.",
                "This shows that even tiny collections can be reusable.",
                "For the 50% of sets with more than 235 judgments, accuracy is 93% when confidence is at least 0.9.",
                "Rank Correlation: MTC and RTC both rank the 10 systems by EMAP (Eq. (1)) calculated using their respective probability estimates.",
                "The mean τ rank correlation between true MAP and EMAP is 0.393 for MTC and 0.555 for RTC.",
                "This difference is significant by a paired t-test (p < 0.0001).",
                "Note that we do not expect the τ correlations to be high, since we are ranking the systems with so few relevance judgments.",
                "It is more important that we estimate confidence in each pairwise comparison correctly.",
                "We ran IP for the same number of judgments that MTC took for each pair, then ranked the systems by MAP using only those judgments (all unjudged documents assumed nonrelevant).",
                "We calculated the τ correlation to the true ranking.",
                "The mean τ correlation is 0.398, which is not significantly different from MTC, but is significantly lower than RTC.",
                "Using uniform estimates of probability is indistinguishable from the baseline, whereas estimating relevance by expert aggregation boosts performance a great deal: nearly 40% over both MTC and IP.",
                "Overfitting: It is possible to overfit: if too many judgments come from the first two systems, the variance in ΔMAP is reduced and the confidence estimates become unreliable.",
                "We saw this in Table 2 and Figure 2 where RTC exhibits a dip in accuracy when confidence is around 97%.",
                "In fact, the number of judgments made prior to a wrong prediction is over 50% greater than the number made prior to a correct prediction.",
                "Overfitting is difficult to quantify exactly, because making more relevance judgments does not always cause it: at higher confidence levels, more relevance judgments are made, and as Table 2 shows, accuracy is greater at those higher confidences.",
                "Obviously having more relevance judgments should increase both confidence and accuracy; the difference seems to be when one system has a great deal more judgments than the other.",
                "Pairwise Comparisons: Our pairwise comparisons fall into one of three groups: 1. the two original runs from which relevance judgments are acquired; 2. one of the original runs vs. one of the new runs; 3. two new runs.",
                "Table 3 shows confidence vs. accuracy results for each of these three groups.",
                "Interestingly, performance is worst when comparing one of the original runs to one of the additional runs.",
                "This is most likely due to a large difference in the number of judgments affecting the variance of ΔMAP.",
                "Nevertheless, performance is quite good on all three subsets.",
                "Worst Case: The case intuitively most likely to produce an error is when the two systems being compared have retrieved very few documents in common.",
                "If we want the judgments to be reusable, we should to be able to generalize even to runs that are very different from the ones used to acquire the relevance judgments.",
                "A simple measure of similarity of two runs is the average percentage of documents they retrieved in common for each topic [2].",
                "We calculated this for all pairs, then looked at performance on pairs with low similarity.",
                "Results are shown in accuracy confidence two original one original no original 0.5 − 0.6 - 48.1% 62.8% 0.6 − 0.7 - 57.1% 79.2% 0.7 − 0.8 - 67.9% 81.7% 0.8 − 0.9 - 82.2% 86.3% 0.9 − 0.95 95.9% 93.7% 92.6% 0.95 − 0.99 96.2% 92.5% 93.1% 1.0 100% 98.0% 99.1% W −1.11 −0.87 −0.27 Table 3: Confidence vs. accuracy of RTC when comparing the two original runs, one original run and one new run, and two new runs.",
                "RTC is robust in all three cases. accuracy when similar confidence 0 − 0.1 0.1 − 0.2 0.2 − 0.3 0.5 − 0.6 68.4% 63.1% 61.4% 0.6 − 0.7 84.2% 78.6% 76.6% 0.7 − 0.8 82.0% 79.8% 78.9% 0.8 − 0.9 93.6% 83.3% 82.1% 0.9 − 0.95 99.3% 92.7% 92.4% 0.95 − 0.99 98.7% 93.4% 93.3% 1.0 99.9% 97.9% 98.1% W 0.44 −0.45 −0.49 Table 4: Confidence vs. accuracy of RTC when a pair of systems retrieved 0-30% documents in common (broken out into 0%-10%, 10%-20%, and 20%30%).",
                "RTC is robust in all three cases.",
                "Table 4.",
                "Performance is in fact very robust even when similarity is low.",
                "When the two runs share very few documents in common, W is actually positive.",
                "MTC and IP both performed quite poorly in these cases.",
                "When the similarity was between 0 and 10%, both MTC and IP correctly predicted ΔMAP only 60% of the time, compared to an 87.6% success rate for RTC.",
                "By Data Set: All the previous results have only been on the ad-hoc collections.",
                "We did the same experiments on our additional data sets, and broke out the results by data set to see how performance varies.",
                "The results in Table 5 show everything about each set, including binned accuracy, W, mean τ, and median number of judgments to reach 95% confidence on the first two systems.",
                "The results are highly consistent from collection to collection, suggesting that our method is not overfitting to any particular data set. 6.",
                "CONCLUSIONS AND FUTURE WORK In this work we have offered the first formal definition of the common idea of reusability of a test collection and presented a model that is able to achieve reusability with very small sets of relevance judgments.",
                "Table 2 and Figure 2 together show how biased a small set of judgments can be: MTC is dramatically overestimating confidence and is much less accurate than RTC, which is able to remove the bias to give a robust evaluation.",
                "The confidence estimates of RTC, in addition to being accurate, provide a guide for obtaining additional judgments: focus on judging documents from the lowest-confidence comparisons.",
                "In the long run, we see small sets of relevance judgaccuracy confidence ad-hoc 94 ad-hoc 96 ad-hoc 97 ad-hoc 98 ad-hoc 99 web 04 robust 05 terabyte 05 0.5 − 0.6 64.1% 61.8% 62.2% 62.0% 59.4% 64.3% 61.5% 61.6% 0.6 − 0.7 76.1% 77.8% 74.5% 78.2% 74.3% 78.1% 75.9% 75.9% 0.7 − 0.8 75.2% 78.9% 77.6% 80.0% 78.6% 82.6% 77.5% 80.4% 0.8 − 0.9 83.2% 85.5% 84.6% 84.9% 86.8% 84.5% 86.7% 87.7% 0.9 − 0.95 93.0% 93.6% 92.8% 93.7% 92.6% 94.2% 93.9% 94.2% 0.95 − 0.99 93.1% 94.3% 93.1% 93.7% 92.8% 95.0% 93.9% 91.6% 1.0 99.2% 96.8% 98.7% 99.5% 99.6% 100% 99.2% 98.3% W -0.34 -0.34 -0.48 -0.35 -0.44 -0.07 -0.41 -0.67 median judged 235 276 243 213 179 448 310 320 mean τ 0.538 0.573 0.556 0.579 0.532 0.596 0.565 0.574 Table 5: Accuracy, W, mean τ, and median number of judgments for all 8 testing sets.",
                "The results are highly consistent across data sets. ments being shared by researchers, each group contributing a few more judgments to gain more confidence about their particular systems.",
                "As time goes on, the number of judgments grows until there is 100% confidence in every evaluation-and there is a full test collection for the task.",
                "We see further use for this method in scenarios such as web retrieval in which the corpus is frequently changing.",
                "It could be applied to evaluation on a dynamic test collection as defined by Soboroff [18].",
                "The model we presented in Section 3 is by no means the only possibility for creating a robust test collection.",
                "A simpler expert aggregation model might perform as well or better (though all our efforts to simplify failed).",
                "In addition to expert aggregation, we could estimate probabilities by looking at similarities between documents.",
                "This is an obvious area for future exploration.",
                "Additionally, it will be worthwhile to investigate the issue of overfitting: the circumstances it occurs under and what can be done to prevent it.",
                "In the meantime, capping confidence estimates at 95% is a hack that solves the problem.",
                "We have many more experimental results that we unfortunately did not have space for but that reinforce the notion that RTC is highly robust: with just a few judgments per topic, we can accurately assess the confidence in any pairwise comparison of systems.",
                "Acknowledgments This work was supported in part by the Center for Intelligent Information Retrieval and in part by the Defense Advanced Research Projects Agency (DARPA) under contract number HR0011-06-C-0023.",
                "Any opinions, findings, and conclusions or recommendations expressed in this material are those of the author and do not necessarily reflect those of the sponsor. 7.",
                "REFERENCES [1] J. Aslam and M. Montague.",
                "Models for Metasearch.",
                "In Proceedings of SIGIR, pages 275-285, 2001. [2] J. Aslam and R. Savell.",
                "On the effectiveness of evaluating retrieval systems in the absence of relevance judgments.",
                "In Proceedings of SIGIR, pages 361-362, 2003. [3] J.",
                "A. Aslam, V. Pavlu, and R. Savell.",
                "A unified model for metasearch, pooling, and system evaluation.",
                "In Proceedings of CIKM, pages 484-491, 2003. [4] J.",
                "A. Aslam, V. Pavlu, and E. Yilmaz.",
                "A statistical method for system evaluation using incomplete judgments.",
                "In Proceedings of SIGIR, pages 541-548, 2006. [5] A. L. Berger, S. D. Pietra, and V. J. D. Pietra.",
                "A maximum entropy approach to natural language processing.",
                "Computational Linguistics, 22(1):39-71, 1996. [6] D. J. Blower.",
                "An easy derivation of logistic regression from the bayesian and maximum entropy perspective.",
                "In Proceedings of the 23rd International Workship on Bayesian Inference and Maximum Entropy Methods in Science and Engineering, pages 30-43, 2004. [7] B. Carterette and J. Allan.",
                "Research methodology in studies of assessor effort for retrieval evaluation.",
                "In Proceedings of RIAO, 2007. [8] B. Carterette, J. Allan, and R. K. Sitaraman.",
                "Minimal test collections for retrieval evaluation.",
                "In Proceedings of SIGIR, pages 268-275, 2006. [9] B. Carterette and D. I. Petkova.",
                "Learning a ranking from pairwise preferences.",
                "In Proceedings of SIGIR, 2006. [10] R. T. Clemen and R. L. Winkler.",
                "Unanimity and compromise among probability forecasters.",
                "Management Science, 36(7):767-779, July 1990. [11] G. V. Cormack, C. R. Palmer, and C. L. Clarke.",
                "Efficient Construction of Large Test Collections.",
                "In Proceedings of SIGIR, pages 282-289, 1998. [12] A. Gelman, J.",
                "B. Carlin, H. S. Stern, and D. B. Rubin.",
                "Bayesian Data Analysis.",
                "Chapman & Hall/CRC, 2004. [13] E. T. Jaynes.",
                "Probability Theory: The Logic of Science.",
                "Cambridge University Press, 2003. [14] R. Manmatha and H. Sever.",
                "A Formal Approach to Score Normalization for Metasearch.",
                "In Proceedings of HLT, pages 88-93, 2002. [15] I. J. Myung, S. Ramamoorti, and J. Andrew D. Baily.",
                "Maximum entropy aggregation of expert predictions.",
                "Management Science, 42(10):1420-1436, October 1996. [16] J. Platt.",
                "Probabilistic outputs for support vector machines and comparison to regularized likelihood methods. pages 61-74, 2000. [17] M. Sanderson and H. Joho.",
                "Forming test collections with no system pooling.",
                "In Proceedings of the 27th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 33-40, 2004. [18] I. Soboroff.",
                "Dynamic test collections: measuring search effectiveness on the live web.",
                "In Proceedings of SIGIR, pages 276-283, 2006. [19] K. Sparck Jones and C. J. van Rijsbergen.",
                "Information Retrieval Test Collections.",
                "Journal of Documentation, 32(1):59-75, 1976. [20] E. M. Voorhees and D. K. Harman, editors.",
                "TREC: Experiment and Evaluation in Information Retrieval.",
                "MIT Press, 2005. [21] J. Zobel.",
                "How Reliable are the Results of Large-Scale Information Retrieval Experiments?",
                "In Proceedings of SIGIR, pages 307-314, 1998."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [],
            "translated_text": "",
            "candidates": [],
            "error": [
                []
            ]
        },
        "test collection": {
            "translated_key": "colección de pruebas",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Robust Test Collections for Retrieval Evaluation Ben Carterette Center for Intelligent Information Retrieval Computer Science Department University of Massachusetts Amherst Amherst, MA 01003 carteret@cs.umass.edu ABSTRACT Low-cost methods for acquiring relevance judgments can be a boon to researchers who need to evaluate new retrieval tasks or topics but do not have the resources to make thousands of judgments.",
                "While these judgments are very useful for a one-time evaluation, it is not clear that they can be trusted when re-used to evaluate new systems.",
                "In this work, we formally define what it means for judgments to be reusable: the confidence in an evaluation of new systems can be accurately assessed from an existing set of relevance judgments.",
                "We then present a method for augmenting a set of relevance judgments with relevance estimates that require no additional assessor effort.",
                "Using this method practically guarantees reusability: with as few as five judgments per topic taken from only two systems, we can reliably evaluate a larger set of ten systems.",
                "Even the smallest sets of judgments can be useful for evaluation of new systems.",
                "Categories and Subject Descriptors: H.3 Information Storage and Retrieval; H.3.4 Systems and Software: Performance Evaluation General Terms: Experimentation, Measurement, Reliability 1.",
                "INTRODUCTION Consider an information retrieval researcher who has invented a new retrieval task.",
                "She has built a system to perform the task and wants to evaluate it.",
                "Since the task is new, it is unlikely that there are any extant relevance judgments.",
                "She does not have the time or resources to judge every document, or even every retrieved document.",
                "She can only judge the documents that seem to be the most informative and stop when she has a reasonable degree of confidence in her conclusions.",
                "But what happens when she develops a new system and needs to evaluate it?",
                "Or another research group decides to implement a system to perform the task?",
                "Can they reliably reuse the original judgments?",
                "Can they evaluate without more relevance judgments?",
                "Evaluation is an important aspect of information retrieval research, but it is only a semi-solved problem: for most retrieval tasks, it is impossible to judge the relevance of every document; there are simply too many of them.",
                "The solution used by NIST at TREC (Text REtrieval Conference) is the pooling method [19, 20]: all competing systems contribute N documents to a pool, and every document in that pool is judged.",
                "This method creates large sets of judgments that are reusable for training or evaluating new systems that did not contribute to the pool [21].",
                "This solution is not adequate for our hypothetical researcher.",
                "The pooling method gives thousands of relevance judgments, but it requires many hours of (paid) annotator time.",
                "As a result, there have been a slew of recent papers on reducing annotator effort in producing test collections: Cormack et al. [11], Zobel [21], Sanderson and Joho [17], Carterette et al. [8], and Aslam et al. [4], among others.",
                "As we will see, the judgments these methods produce can significantly bias the evaluation of a new set of systems.",
                "Returning to our hypothetical resesarcher, can she reuse her relevance judgments?",
                "First we must formally define what it means to be reusable.",
                "In previous work, reusability has been tested by simply assessing the accuracy of a set of relevance judgments at evaluating unseen systems.",
                "While we can say that it was right 75% of the time, or that it had a rank correlation of 0.8, these numbers do not have any predictive power: they do not tell us which systems are likely to be wrong or how confident we should be in any one.",
                "We need a more careful definition of reusability.",
                "Specifically, the question of reusability is not how accurately we can evaluate new systems.",
                "A malicious adversary can always produce a new ranked list that has not retrieved any of the judged documents.",
                "The real question is how much confidence we have in our evaluations, and, more importantly, whether we can trust our estimates of confidence.",
                "Even if confidence is not high, as long as we can trust it, we can identify which systems need more judgments in order to increase confidence.",
                "Any set of judgments, no matter how small, becomes reusable to some degree.",
                "Small, reusable test collections could have a huge impact on information retrieval research.",
                "Research groups would be able to share the relevance judgments they have done in-house for pilot studies, new tasks, or new topics.",
                "The amount of data available to researchers would grow exponentially over time. 2.",
                "ROBUST EVALUATION Above we gave an intuitive definition of reusability: a collection is reusable if we can trust our estimates of confidence in an evaluation.",
                "By that we mean that if we have made some relevance judgments and have, for example, 75% confidence that system A is better than system B, we would like there to be no more than 25% chance that our assessment of the relative quality of the systems will change as we continue to judge documents.",
                "Our evaluation should be robust to missing judgments.",
                "In our previous work, we defined confidence as the probability that the difference in an evaluation measure calculated for two systems is less than zero [8].",
                "This notion of confidence is defined in the context of a particular evaluation task that we call comparative evaluation: determining the sign of the difference in an evaluation measure.",
                "Other evaluation tasks could be defined; estimating the magnitude of the difference or the values of the measures themselves are examples that entail different notions of confidence.",
                "We therefore see confidence as a probability estimate.",
                "One of the questions we must ask about a probability estimate is what it means.",
                "What does it mean to have 75% confidence that system A is better than system B?",
                "As described above, we want it to mean that if we continue to judge documents, there will only be a 25% chance that our assessment will change.",
                "If this is what it means, we can trust the confidence estimates.",
                "But do we know it has that meaning?",
                "Our calculation of confidence rested on an assumption about the probability of relevance of unjudged documents, specifically that each unjudged document was equally likely to be relevant or nonrelevant.",
                "This assumption is almost certainly not realistic in most IR applications.",
                "As it turns out, it is this assumption that determines whether the confidence estimates can eb trusted.",
                "Before elaborating on this, we formally define confidence. 2.1 Estimating Confidence Average precision (AP) is a standard evaluation metric that captures both the ability of a system to rank relevant documents highly (precision) as well as its ability to retrieve relevant documents (recall).",
                "It is typically written as the mean precision at the ranks of relevant documents: AP = 1 |R| i∈R prec@r(i) where R is the set of relevant documents and r(i) is the rank of document i.",
                "Let Xi be a random variable indicating the relevance of document i.",
                "If documents are ordered by rank, we can express precision as prec@i = 1/i i j=1 Xj .",
                "Average precision then becomes the quadratic equation AP = 1 Xi n i=1 Xi/i i j=1 Xj = 1 Xi n i=1 j≥i aijXiXj where aij = 1/ max{r(i), r(j)}.",
                "Using aij instead of 1/i allows us to number the documents arbitrarily.",
                "To see why this is true, consider a toy example: a list of 3 documents with relevant documents B, C at ranks 1 and 3 and nonrelevant document A at rank 2.",
                "Average precision will be 1 2 (1 1 x2 B+ 1 2 xBxA+ 1 3 xBxC + 1 2 x2 A+ 1 3 xAxC + 1 3 x2 C) = 1 2 1 + 2 3 because xA = 0, xB = 1, xC = 1.",
                "Though the ordering B, A, C is different from the labeling A, B, C, it does not affect the computation.",
                "We can now see average precision itself is a random variable with a distribution over all possible assignments of relevance to all documents.",
                "This random variable has an expectation, a variance, confidence intervals, and a certain probability of being less than or equal to a given value.",
                "All of these are dependent on the probability that document i is relevant: pi = p(Xi = 1).",
                "Suppose in our previous example we do not know the relevance judgments, but we believe pA = 0.4, pB = 0.8, pC = 0.7.",
                "We can then compute e.g.",
                "P(AP = 0) = 0.2 · 0.6 · 0.3 = 0.036, or P(AP = 1 2 ) = 0.2 · 0.4 · 0.7 = 0.056.",
                "Summing over all possibilities, we can compute expectation and variance: E[AP] ≈ 1 pi aiipi + j>i aij pipj V ar[AP] ≈ 1 ( pi)2 n i a2 iipiqi + j>i a2 ijpipj(1 − pipj) + i=j 2aiiaijpipj(1 − pi) + k>j=i 2aijaikpipjpk(1 − pi) AP asymptotically converges to a normal distribution with expectation and variance as defined above.1 For our comparative evaluation task we are interested in the sign of the difference in two average precisions: ΔAP = AP1 − AP2.",
                "As we showed in our previous work, ΔAP has a closed form when documents are ordered arbitrarily: ΔAP = 1 Xi n i=1 j≥i cij XiXj cij = aij − bij where bij is defined analogously to aij for the second ranking.",
                "Since AP is normal, ΔAP is normal as well, meaning we can use the normal cumulative density function to determine the confidence that a difference in AP is less than zero.",
                "Since topics are independent, we can easily extend this to mean average precision (MAP).",
                "MAP is also normally distributed; its expectation and variance are: EMAP = 1 T t∈T E[APt] (1) VMAP = 1 T2 t∈T V ar[APt] ΔMAP = MAP1 − MAP2 Confidence can then be estimated by calculating the expectation and variance and using the normal density function to find P(ΔMAP < 0). 2.2 Confidence and Robustness Having defined confidence, we turn back to the issue of trust in confidence estimates, and show how it ties into the robustness of the collection to missing judgments. 1 These are actually approximations to the true expectation and variance, but the error is a negligible O(n2−n ).",
                "Let Z be the set of all pairs of ranked results for a common set of topics.",
                "Suppose we have a set of m relevance judgments xm = {x1, x2, ..., xm} (using small x rather than capital X to distinguish between judged and unjudged documents); these are the judgments against which we compute confidence.",
                "Let Zα be the subset of pairs in Z for which we predict that ΔMAP = −1 with confidence α given the judgments xm .",
                "For the confidence estimates to be accurate, we need at least α · |Zα| of these pairs to actually have ΔMAP = −1 after we have judged every document.",
                "If they do, we can trust the confidence estimates; our evaluation will be robust to missing judgments.",
                "If our confidence estimates are based on unrealistic assumptions, we cannot expect them to be accurate.",
                "The assumptions they are based on are the probabilities of relevance pi.",
                "We need these to be realistic.",
                "We argue that the best possible distribution of relevance p(Xi) is the one that explains all of the data (all of the observations made about the retrieval systems) while at the same time making no unwarranted assumptions.",
                "This is known as the principle of maximum entropy [13].",
                "The entropy of a random variable X with distribution p(X) is defined as H(p) = − i p(X = i) log p(X = i).",
                "This has found a wide array of uses in computer science and information retrieval.",
                "The maximum entropy distribution is the one that maximizes H. This distribution is unique and has an exponential form.",
                "The following theorem shows the utility of a maximum entropy distribution for relevance when estimating confidence.",
                "Theorem 1.",
                "If p(Xn |I, xm ) = argmaxpH(p), confidence estimates will be accurate. where xm is the set of relevance judgments defined above, Xn is the full set of documents that we wish to estimate the relevance of, and I is some information about the documents (unspecified as of now).",
                "We forgo the proof for the time being, but it is quite simple.",
                "This says that the better the estimates of relevance, the more accurate the evaluation.",
                "The task of creating a reusable <br>test collection</br> thus becomes the task of estimating the relevance of unjudged documents.",
                "The theorem and its proof say nothing whatsoever about the evaluation metric.",
                "The probability estimates are entirely indepedent of the measure we are interested in.",
                "This means the same probability estimates can tell us about average precision as well as precision, recall, bpref, etc.",
                "Furthermore, we could assume that the relevance of documents i and j is independent and achieve the same result, which we state as a corollary: Corollary 1.",
                "If p(Xi|I, xm ) = argmaxpH(p), confidence estimates will be accurate.",
                "The task therefore becomes the imputation of the missing values of relevance.",
                "The theorem implies that the closer we get to the maximum entropy distribution of relevance, the closer we get to robustness. 3.",
                "PREDICTING RELEVANCE In our statement of Theorem 1, we left the nature of the information I unspecified.",
                "One of the advantages of our confidence estimates is that they admit information from a wide variety of sources; essentially anything that can be modeled can be used as information for predicting relevance.",
                "A natural source of information is the retrieval systems themselves: how they ranked the judged documents, how often they failed to rank relevant documents, how they perform across topics, and so on.",
                "If we treat each system as an information retrieval expert providing an opinion about the relevance of each document, the problem becomes one of expert opinion aggregation.",
                "This is similar to the metasearch or data fusion problem in which the task is to take k input systems and merge them into a single ranking.",
                "Aslam et al. [3] previously identified a connection between evaluation and metasearch.",
                "Our problem has two key differences: 1.",
                "We explicitly need probabilities of relevance that we can plug into Eq. 1; metasearch algorithms have no such requirement. 2.",
                "We are accumulating relevance judgments as we proceed with the evaluation and are able to re-estimate relevance given each new judgment.",
                "In light of (1) above, we introduce a probabilistic model for expert combination. 3.1 A Model for Expert Opinion Aggregation Suppose that each expert j provides a probability of relevance qij = pj(Xi = 1).",
                "The information about the relevance of document i will then be the set of k expert opinions I = qi = (qi1, qi2, · · · , qik).",
                "The probability distribution we wish to find is the one that maximizes the entropy of pi = p(Xi = 1|qi).",
                "As it turns out, finding the maximum entropy model is equivalent to finding the parameters that maximize the likelihood [5].",
                "Blower [6] explicitly shows that finding the maximum entropy model for a binary variable is equivalent to solving a logistic regression.",
                "Then pi = p(Xi = 1|qi) = exp k j=1 λjqij 1 + exp k j=1 λj qij (2) where λ1, · · · , λk are the regression parameters.",
                "We include a beta prior for p(λj) with parameters α, β.",
                "This can be seen as a type of smoothing to account for the fact that the training data is highly biased.",
                "This model has the advantage of including the statistical dependence between the experts.",
                "A model of the same form was shown by Clemen & Winkler to be the best for aggregating expert probabilities [10].",
                "A similar maximumentropy-motivated approach has been used for expert aggregation [15].",
                "Aslam & Montague [1] used a similar model for metasearch, but assumed independence among experts.",
                "Where do the qij s come from?",
                "Using raw, uncalibrated scores as predictors will not work because score distributions vary too much between topics.",
                "A language modeling ranker, for instance, will typically give a much higher score to the top retrieved document for a short query than to the top retrieved document for a long query.",
                "We could train a separate predicting model for each topic, but that does not take advantage of all of the information we have: we may only have a handful of judgments for a topic, not enough to train a model to any confidence.",
                "Furthermore, it seems reasonable to assume that if an expert makes good predictions for one topic, it will make good predictions for other topics as well.",
                "We could use a hierarchical model [12], but that will not generalize to unseen topics.",
                "Instead, we will calibrate the scores of each expert individually so that scores can be compared both within topic and between topic.",
                "Thus our model takes into account not only the dependence between experts, but also the dependence between experts performances on different tasks (topics). 3.2 Calibrating Experts Each expert gives us a score and a rank for each document.",
                "We need to convert these to probabilities.",
                "A method such as the one used by Manmatha et al. [14] could be used to convert scores into probabilities of relevance.",
                "The pairwise preference method of Carterette & Petkova [9] could also be used, interpeting the ranking of one document over another as an expression of preference.",
                "Let q∗ ij be expert js self-reported probability that document i is relevant.",
                "Intuitively it seems clear that q∗ ij should decrease with rank, and it should be zero if document i is unranked (the expert did not believe it to be relevant).",
                "The pairwise preference model can handle these two requirements easily, so we will use it.",
                "Let θrj (i) be the relevance coefficient of the document at rank rj(i).",
                "We want to find the θs that maximize the likelihood function: Ljt(Θ) = rj (i)<rj (k) exp(θrj (i) − θrj (k)) 1 + exp(θrj (i) − θrj (k)) We again include a beta prior on p(θrj(i)) with parameters |Rt| + 1 and |Nt| + 1, the size of the sets of judged relevant and nonrelevant documents respectively.",
                "Using these as prior parameters ensures that the resulting probabilities will be concentrated around the ratio of relevant documents that have been discovered for topic t. This means that the probability estimates decrease by rank and are higher for topics that have more relevant documents.",
                "After finding the Θ that maximizes the likelihood, we have q∗ ij = exp(θrj (i)) 1+exp(θrj (i)) .",
                "We define θ∞ = −∞, so that the probability that an unranked document is relevant is 0.",
                "Since q∗ ij is based on the rank at which a document is retrieved rather than the identity of the document itself, the probabilities are identical from expert to expert, e.g. if expert E put document A at rank 1, and expert D put document B at rank 1, we will have q∗ AE = q∗ BD.",
                "Therefore we only have to solve this once for each topic.",
                "The above model gives topic-independent probabilities for each document.",
                "But suppose an expert who reports 90% probability is only right 50% of the time.",
                "Its opinion should be discounted based on its observed performance.",
                "Specifically, we want to learn a calibration function qij = Cj(q∗ ij) that will ensure that the predicted probabilities are tuned to the experts ability to retrieve relevant documents given the judgments that have been made to this point.",
                "Platts SVM calibration method [16] fits a sigmoid function between q∗ ij and the relevance judgments to obtain qij = Cj (q∗ ij) = exp(Aj +Bjq∗ ij ) 1+exp(Aj +Bj q∗ ij ) .",
                "Since q∗ ij is topic-independent, we only need to learn one calibration function for each expert.",
                "Once we have the calibration function, it is applied to adjust the experts predictions to their actual performance.",
                "The calibrated probabilities are plugged into model (2) to find the document probabilities.",
                "Figure 1: Conceptual diagram of our aggregation model.",
                "Experts E1, E2 have ranked documents A, B, C for topic T1 and documents D, E, F for topic T2.",
                "The first step is to obtain q∗ ij.",
                "Next is calibration to true performance to find qij .",
                "Finally we obtain pi = p(Xi = 1|qi1, qi2), · · · . 3.3 Model Summary Our model has three components that differ by the data they take as input and what they produce as output.",
                "A conceptual diagram is shown in Figure 1. 1. ranks → probabilities (per system per topic).",
                "This gives us q∗ ij, expert js self-reported probability of the relevance of document i.",
                "This is unsupervised; it requires no labeled data (though if we have some, we use it to set prior parameters). 2. probabilities → calibrated probabilities (per system).",
                "This gives us qij = Cj (q∗ ij), expert js calibrated probability of the relevance of document i.",
                "This is semisupervised; we have relevance judgments at some ranks which we use to impute the probability of relevance at other ranks. 3. calibrated probabilities → document probabilities.",
                "This gives us pi = p(Xi = 1|qi), the probability of relevance of document i given calibrated expert probabilities qij .",
                "This is supervised; we learn coefficients from a set of judged documents and use those to estimate the relevance of the unjudged documents.",
                "Although the model appears rather complex, it is really just three successive applications of logistic regression.",
                "As such, it can be implemented in a statistical programming language such as R in a few lines of code.",
                "The use of beta (conjugate) priors ensures that no expensive computational methods such as MCMC are necessary [12], so the model is trained and applied fast enough to be used on-line.",
                "Our code is available at http://ciir.cs.umass.edu/~carteret/. 4.",
                "EXPERIMENTS Three hypotheses are under consideration.",
                "The first, and most important, is that using our expert aggregation model to predict relevance produces test collections that are robust enough to be reusable; that is, we can trust the estimates of confidence when we evaluate systems that did not contribute any judgments to the pool.",
                "The other two hypotheses relate to the improvement we see by using better estimates of relevance than we did in our previous work [8].",
                "These are that (a) it takes fewer relevance track no. topics no. runs no. judged no. rel ad-hoc 94 50 40 97,319 9,805 ad-hoc 95 49 33 87,069 6,503 ad-hoc 96 50 61 133,681 5,524 ad-hoc 97 50 74 72,270 4,611 ad-hoc 98 50 103 80,345 4,674 ad-hoc 99 50 129 86,830 4,728 web 04 225 74 88,566 1,763 robust 05 50 74 37,798 6,561 terabyte 05 50 58 45,291 10,407 Table 1: Number of topics, number of runs, number of documents judged, and number found relevant for each of our data sets. judgments to reach 95% confidence and (b) the accuracy of the predictions is higher than if we were to simply assume pi = 0.5 for all unjudged documents. 4.1 Data We obtained full ad-hoc runs submitted to TRECs 3 through 8.",
                "Each run ranks at most 1000 documents for 50 topics (49 topics for TREC 4).",
                "Additionally, we obtained all runs from the Web track of TREC 13, the Robust2 track of TREC 14, and the Terabyte (ad-hoc) track of TREC 14.",
                "These are the tracks that have replaced the ad-hoc track since its end in 1999.",
                "Statistics are shown in Table 1.",
                "We set aside the TREC 4 (ad-hoc 95) set for training, TRECs 3 and 5-8 (ad-hoc 94 and 96-99) for primary testing, and the remaining sets for additional testing.",
                "We use the qrels files assembled by NIST as truth.",
                "The number of relevance judgments made and relevant documents found for each track are listed in Table 1.",
                "For computational reasons, we truncate ranked lists at 100 documents.",
                "There is no reason that we could not go deeper, but calculating variance is O(n3 ) and thus very timeconsuming.",
                "Because of the reciprocal rank nature of AP, we do not lose much information by truncating at rank 100. 4.2 Algorithms We will compare three algorithms for acquiring relevance judgments.",
                "The baseline is a variation of TREC pooling that we will call incremental pooling (IP).",
                "This algorithm takes a number k as input and presents the first k documents in rank order (without regard to topic) to be judged.",
                "It does not estimate the relevance of unjudged documents; it simply assumes any unjudged document is nonrelevant.",
                "The second algorithm is that presented in Carterette et al. [8] (Algorithm 1).",
                "Documents are selected based on how interesting they are in determining whether a difference in mean average precision exists.",
                "For this approach pi = 0.5 for all i; there is no estimation of probabilities.",
                "We will call this MTC for minimal <br>test collection</br>.",
                "The third algorithm augments MTC with updated estimates of probabilities of relevance.",
                "We will call this RTC for robust <br>test collection</br>.",
                "It is identical to Algorithm 1, except that every 10th iteration we estimate pi for all unjudged documents i using the expert aggregation model of Section 3.",
                "RTC has smoothing (prior distribution) parameters that must be set.",
                "We trained using the ad-hoc 95 set.",
                "We limited 2 Robust here means robust retrieval; this is different from our goal of robust evaluation.",
                "Algorithm 1 (MTC) Given two ranked lists and confidence level α, predict the sign of ΔMAP. 1: pi ← 0.5 for all documents i 2: while P(ΔMAP < 0) < α do 3: calculate weight wi for all unjudged documents i (see Carterette et al. [8] for details) 4: j ← argmaxiwi 5: xj ← 1 if document j is relevant, 0 otherwise 6: pj ← xj 7: end while the search to uniform priors with relatively high variance.",
                "For expert aggregation, the prior parameters are α = β = 1. 4.3 Experimental Design First, we want to know whether we can augment a set of relevance judgments with a set of relevance probabilities in order to reuse the judgments to evaluate a new set of systems.",
                "For each experimental trial: 1.",
                "Pick a random subset of k runs. 2.",
                "From those k, pick an initial c < k to evaluate. 3.",
                "Run RTC to 95% confidence on the initial c. 4.",
                "Using the model from Section 3, estimate the probabilities of relevance for all documents retrieved by all k runs. 5.",
                "Calculate EMAP for all k runs, and P(ΔMAP < 0) for all pairs of runs.",
                "We do the same for MTC, but omit step 4.",
                "Note that after evaluating the first c systems, we make no additional relevance judgments.",
                "To put our method to the test, we selected c = 2: we will build a set of judgments from evaluating only two initial systems.",
                "We will then generalize to a set of k = 10 (of which those two are a subset).",
                "As we run more trials, we obtain the data we need to test all three of our hypotheses. 4.4 Experimental Evaluation Recall that a set of judgments is robust if the accuracy of the predictions it makes is at least its estimated confidence.",
                "One way to evaluate robustness is to bin pairs by their confidence, then calculate the accuracy over all the pairs in each bin.",
                "We would like the accuracy to be no less than the lowest confidence score in the bin, but preferably higher.",
                "Since summary statistics are useful, we devised the following metric.",
                "Suppose we are a bookmaker taking bets on whether ΔMAP < 0.",
                "We use RTC or MTC to set the odds O = P (ΔMAP <0) 1−P (ΔMAP <0) .",
                "Suppose a bettor wagers $1 on ΔMAP ≥ 0.",
                "If it turns out that ΔMAP < 0, we win the dollar.",
                "Otherwise, we pay out O.",
                "If our confidence estimates are perfectly accurate, we break even.",
                "If confidence is greater than accuracy, we lose money; we win if accuracy is greater than confidence.",
                "Counterintuitively, the most desirable outcome is breaking even: if we lose money, we cannot trust the confidence estimates, but if we win money, we have either underestimated confidence or judged more documents than necessary.",
                "However, the cost of not being able to trust the confidence estimates is higher than the cost of extra relevance judgments, so we will treat positive outcomes as good.",
                "The amount we win on each pairwise comparison i is: Wi = yi − (1 − yi) Pi 1 − Pi = yi − Pi 1 − Pi yi = 1 if ΔMAP < 0 and 0 otherwise, and Pi = P(ΔMAP < 0).",
                "The summary statistic is W, the mean of Wi.",
                "Note that as Pi increases, we lose more for being wrong.",
                "This is as it should be: the penalty should be great for missing the high probability predictions.",
                "However, since our losses grow without bound as predictions approach certainty, we cap −Wi at 100.",
                "For our hypothesis that RTC requires fewer judgments than MTC, we are interested in the number of judgments needed to reach 95% confidence on the first pair of systems.",
                "The median is more interesting than the mean: most pairs require a few hundred judgments, but a few pairs require several thousand.",
                "The distribution is therefore highly skewed, and the mean strongly affected by those outliers.",
                "Finally, for our hypothesis that RTC is more accurate than MTC, we will look at Kendalls τ correlation between a ranking of k systems by a small set of judgments and the true ranking using the full set of judgments.",
                "Kendalls τ, a nonparametric statistic based on pairwise swaps between two lists, is a standard evaluation for this type of study.",
                "It ranges from −1 (perfectly anti-correlated) to 1 (rankings identical), with 0 meaning that half of the pairs are swapped.",
                "As we touched on in the introduction, though, an accuracy measure like rank correlation is not a good evaluation of reusability.",
                "We include it for completeness. 4.4.1 Hypothesis Testing Running multiple trials allows the use of statistical hypothesis testing to compare algorithms.",
                "Using the same sets of systems allows the use of paired tests.",
                "As we stated above, we are more interested in the median number of judgments than the mean.",
                "A test for difference in median is the Wilcoxon sign rank test.",
                "We can also use a paired t-test to test for a difference in mean.",
                "For rank correlation, we can use a paired t-test to test for a difference in τ. 5.",
                "RESULTS AND ANALYSIS The comparison between MTC and RTC is shown in Table 2.",
                "With MTC and uniform probabilities of relevance, the results are far from robust.",
                "We cannot reuse the relevance judgments with much confidence.",
                "But with RTC, the results are very robust.",
                "There is a slight dip in accuracy when confidence gets above 0.95; nonetheless, the confidence predictions are trustworthy.",
                "Mean Wi shows that RTC is much closer to 0 than MTC.",
                "The distribution of confidence scores shows that at least 80% confidence is achieved more than 35% of the time, indicating that neither algorithm is being too conservative in its confidence estimates.",
                "The confidence estimates are rather low overall; that is because we have built a <br>test collection</br> from only two initial systems.",
                "Recall from Section 1 that we cannot require (or even expect) a minimum level of confidence when we generalize to new systems.",
                "More detailed results for both algorithms are shown in Figure 2.",
                "The solid line is the ideal result that would give W = 0.",
                "RTC is on or above this line at all points until confidence reaches about 0.97.",
                "After that there is a slight dip in accuracy which we discuss below.",
                "Note that both MTC RTC confidence % in bin accuracy % in bin accuracy 0.5 − 0.6 33.7% 61.7% 28.6% 61.9% 0.6 − 0.7 18.1% 73.1% 20.1% 76.3% 0.7 − 0.8 10.4% 70.1% 15.5% 78.0% 0.8 − 0.9 9.4% 69.0% 12.1% 84.9% 0.9 − 0.95 7.3% 78.0% 6.6% 93.1% 0.95 − 0.99 17.9% 70.4% 12.4% 93.4% 1.0 3.3% 68.3% 4.7% 98.9% W −5.34 −0.39 median judged 251 235 mean τ 0.393 0.555 Table 2: Confidence that P(ΔMAP < 0) and accuracy of prediction when generalizing a set of relevance judgments acquired using MTC and RTC.",
                "Each bin contains over 1,000 trials from the adhoc 3, 5-8 sets.",
                "RTC is much more robust than MTC.",
                "W is defined in Section 4.4; closer to 0 is better.",
                "Median judged is the number of judgments to reach 95% confidence on the first two systems.",
                "Mean τ is the average rank correlation for all 10 systems. 0.5 0.6 0.7 0.8 0.9 1 0.5 0.6 0.7 0.8 0.9 1 accuracy confidence breakeven RTC MTC Figure 2: Confidence vs. accuracy of MTC and RTC.",
                "The solid line is the perfect result that would give W = 0; performance should be on or above this line.",
                "Each point represents at least 500 pairwise comparisons. algorithms are well above the line up to around confidence 0.7.",
                "This is because the baseline performance on these data sets is high; it is quite easy to achieve 75% accuracy doing very little work [7].",
                "Number of Judgments: The median number of judgments required by MTC to reach 95% confidence on the first two systems is 251, an average of 5 per topic.",
                "The median required by RTC is 235, about 4.7 per topic.",
                "Although the numbers are close, RTCs median is significantly lower by a paired Wilcoxon test (p < 0.0001).",
                "For comparison, a pool of depth 100 would result in a minimum of 5,000 judgments for each pair.",
                "The difference in means is much greater.",
                "MTC required a mean of 823 judgments, 16 per topic, while RTC required a mean of 502, 10 per topic. (Recall that means are strongly skewed by a few pairs that take thousands of judgments.)",
                "This difference is significant by a paired t-test (p < 0.0001).",
                "Ten percent of the sets resulted in 100 or fewer judgments (less than two per topic).",
                "Performance on these is very high: W = 0.41, and 99.7% accuracy when confidence is at least 0.9.",
                "This shows that even tiny collections can be reusable.",
                "For the 50% of sets with more than 235 judgments, accuracy is 93% when confidence is at least 0.9.",
                "Rank Correlation: MTC and RTC both rank the 10 systems by EMAP (Eq. (1)) calculated using their respective probability estimates.",
                "The mean τ rank correlation between true MAP and EMAP is 0.393 for MTC and 0.555 for RTC.",
                "This difference is significant by a paired t-test (p < 0.0001).",
                "Note that we do not expect the τ correlations to be high, since we are ranking the systems with so few relevance judgments.",
                "It is more important that we estimate confidence in each pairwise comparison correctly.",
                "We ran IP for the same number of judgments that MTC took for each pair, then ranked the systems by MAP using only those judgments (all unjudged documents assumed nonrelevant).",
                "We calculated the τ correlation to the true ranking.",
                "The mean τ correlation is 0.398, which is not significantly different from MTC, but is significantly lower than RTC.",
                "Using uniform estimates of probability is indistinguishable from the baseline, whereas estimating relevance by expert aggregation boosts performance a great deal: nearly 40% over both MTC and IP.",
                "Overfitting: It is possible to overfit: if too many judgments come from the first two systems, the variance in ΔMAP is reduced and the confidence estimates become unreliable.",
                "We saw this in Table 2 and Figure 2 where RTC exhibits a dip in accuracy when confidence is around 97%.",
                "In fact, the number of judgments made prior to a wrong prediction is over 50% greater than the number made prior to a correct prediction.",
                "Overfitting is difficult to quantify exactly, because making more relevance judgments does not always cause it: at higher confidence levels, more relevance judgments are made, and as Table 2 shows, accuracy is greater at those higher confidences.",
                "Obviously having more relevance judgments should increase both confidence and accuracy; the difference seems to be when one system has a great deal more judgments than the other.",
                "Pairwise Comparisons: Our pairwise comparisons fall into one of three groups: 1. the two original runs from which relevance judgments are acquired; 2. one of the original runs vs. one of the new runs; 3. two new runs.",
                "Table 3 shows confidence vs. accuracy results for each of these three groups.",
                "Interestingly, performance is worst when comparing one of the original runs to one of the additional runs.",
                "This is most likely due to a large difference in the number of judgments affecting the variance of ΔMAP.",
                "Nevertheless, performance is quite good on all three subsets.",
                "Worst Case: The case intuitively most likely to produce an error is when the two systems being compared have retrieved very few documents in common.",
                "If we want the judgments to be reusable, we should to be able to generalize even to runs that are very different from the ones used to acquire the relevance judgments.",
                "A simple measure of similarity of two runs is the average percentage of documents they retrieved in common for each topic [2].",
                "We calculated this for all pairs, then looked at performance on pairs with low similarity.",
                "Results are shown in accuracy confidence two original one original no original 0.5 − 0.6 - 48.1% 62.8% 0.6 − 0.7 - 57.1% 79.2% 0.7 − 0.8 - 67.9% 81.7% 0.8 − 0.9 - 82.2% 86.3% 0.9 − 0.95 95.9% 93.7% 92.6% 0.95 − 0.99 96.2% 92.5% 93.1% 1.0 100% 98.0% 99.1% W −1.11 −0.87 −0.27 Table 3: Confidence vs. accuracy of RTC when comparing the two original runs, one original run and one new run, and two new runs.",
                "RTC is robust in all three cases. accuracy when similar confidence 0 − 0.1 0.1 − 0.2 0.2 − 0.3 0.5 − 0.6 68.4% 63.1% 61.4% 0.6 − 0.7 84.2% 78.6% 76.6% 0.7 − 0.8 82.0% 79.8% 78.9% 0.8 − 0.9 93.6% 83.3% 82.1% 0.9 − 0.95 99.3% 92.7% 92.4% 0.95 − 0.99 98.7% 93.4% 93.3% 1.0 99.9% 97.9% 98.1% W 0.44 −0.45 −0.49 Table 4: Confidence vs. accuracy of RTC when a pair of systems retrieved 0-30% documents in common (broken out into 0%-10%, 10%-20%, and 20%30%).",
                "RTC is robust in all three cases.",
                "Table 4.",
                "Performance is in fact very robust even when similarity is low.",
                "When the two runs share very few documents in common, W is actually positive.",
                "MTC and IP both performed quite poorly in these cases.",
                "When the similarity was between 0 and 10%, both MTC and IP correctly predicted ΔMAP only 60% of the time, compared to an 87.6% success rate for RTC.",
                "By Data Set: All the previous results have only been on the ad-hoc collections.",
                "We did the same experiments on our additional data sets, and broke out the results by data set to see how performance varies.",
                "The results in Table 5 show everything about each set, including binned accuracy, W, mean τ, and median number of judgments to reach 95% confidence on the first two systems.",
                "The results are highly consistent from collection to collection, suggesting that our method is not overfitting to any particular data set. 6.",
                "CONCLUSIONS AND FUTURE WORK In this work we have offered the first formal definition of the common idea of reusability of a <br>test collection</br> and presented a model that is able to achieve reusability with very small sets of relevance judgments.",
                "Table 2 and Figure 2 together show how biased a small set of judgments can be: MTC is dramatically overestimating confidence and is much less accurate than RTC, which is able to remove the bias to give a robust evaluation.",
                "The confidence estimates of RTC, in addition to being accurate, provide a guide for obtaining additional judgments: focus on judging documents from the lowest-confidence comparisons.",
                "In the long run, we see small sets of relevance judgaccuracy confidence ad-hoc 94 ad-hoc 96 ad-hoc 97 ad-hoc 98 ad-hoc 99 web 04 robust 05 terabyte 05 0.5 − 0.6 64.1% 61.8% 62.2% 62.0% 59.4% 64.3% 61.5% 61.6% 0.6 − 0.7 76.1% 77.8% 74.5% 78.2% 74.3% 78.1% 75.9% 75.9% 0.7 − 0.8 75.2% 78.9% 77.6% 80.0% 78.6% 82.6% 77.5% 80.4% 0.8 − 0.9 83.2% 85.5% 84.6% 84.9% 86.8% 84.5% 86.7% 87.7% 0.9 − 0.95 93.0% 93.6% 92.8% 93.7% 92.6% 94.2% 93.9% 94.2% 0.95 − 0.99 93.1% 94.3% 93.1% 93.7% 92.8% 95.0% 93.9% 91.6% 1.0 99.2% 96.8% 98.7% 99.5% 99.6% 100% 99.2% 98.3% W -0.34 -0.34 -0.48 -0.35 -0.44 -0.07 -0.41 -0.67 median judged 235 276 243 213 179 448 310 320 mean τ 0.538 0.573 0.556 0.579 0.532 0.596 0.565 0.574 Table 5: Accuracy, W, mean τ, and median number of judgments for all 8 testing sets.",
                "The results are highly consistent across data sets. ments being shared by researchers, each group contributing a few more judgments to gain more confidence about their particular systems.",
                "As time goes on, the number of judgments grows until there is 100% confidence in every evaluation-and there is a full <br>test collection</br> for the task.",
                "We see further use for this method in scenarios such as web retrieval in which the corpus is frequently changing.",
                "It could be applied to evaluation on a dynamic <br>test collection</br> as defined by Soboroff [18].",
                "The model we presented in Section 3 is by no means the only possibility for creating a robust <br>test collection</br>.",
                "A simpler expert aggregation model might perform as well or better (though all our efforts to simplify failed).",
                "In addition to expert aggregation, we could estimate probabilities by looking at similarities between documents.",
                "This is an obvious area for future exploration.",
                "Additionally, it will be worthwhile to investigate the issue of overfitting: the circumstances it occurs under and what can be done to prevent it.",
                "In the meantime, capping confidence estimates at 95% is a hack that solves the problem.",
                "We have many more experimental results that we unfortunately did not have space for but that reinforce the notion that RTC is highly robust: with just a few judgments per topic, we can accurately assess the confidence in any pairwise comparison of systems.",
                "Acknowledgments This work was supported in part by the Center for Intelligent Information Retrieval and in part by the Defense Advanced Research Projects Agency (DARPA) under contract number HR0011-06-C-0023.",
                "Any opinions, findings, and conclusions or recommendations expressed in this material are those of the author and do not necessarily reflect those of the sponsor. 7.",
                "REFERENCES [1] J. Aslam and M. Montague.",
                "Models for Metasearch.",
                "In Proceedings of SIGIR, pages 275-285, 2001. [2] J. Aslam and R. Savell.",
                "On the effectiveness of evaluating retrieval systems in the absence of relevance judgments.",
                "In Proceedings of SIGIR, pages 361-362, 2003. [3] J.",
                "A. Aslam, V. Pavlu, and R. Savell.",
                "A unified model for metasearch, pooling, and system evaluation.",
                "In Proceedings of CIKM, pages 484-491, 2003. [4] J.",
                "A. Aslam, V. Pavlu, and E. Yilmaz.",
                "A statistical method for system evaluation using incomplete judgments.",
                "In Proceedings of SIGIR, pages 541-548, 2006. [5] A. L. Berger, S. D. Pietra, and V. J. D. Pietra.",
                "A maximum entropy approach to natural language processing.",
                "Computational Linguistics, 22(1):39-71, 1996. [6] D. J. Blower.",
                "An easy derivation of logistic regression from the bayesian and maximum entropy perspective.",
                "In Proceedings of the 23rd International Workship on Bayesian Inference and Maximum Entropy Methods in Science and Engineering, pages 30-43, 2004. [7] B. Carterette and J. Allan.",
                "Research methodology in studies of assessor effort for retrieval evaluation.",
                "In Proceedings of RIAO, 2007. [8] B. Carterette, J. Allan, and R. K. Sitaraman.",
                "Minimal test collections for retrieval evaluation.",
                "In Proceedings of SIGIR, pages 268-275, 2006. [9] B. Carterette and D. I. Petkova.",
                "Learning a ranking from pairwise preferences.",
                "In Proceedings of SIGIR, 2006. [10] R. T. Clemen and R. L. Winkler.",
                "Unanimity and compromise among probability forecasters.",
                "Management Science, 36(7):767-779, July 1990. [11] G. V. Cormack, C. R. Palmer, and C. L. Clarke.",
                "Efficient Construction of Large Test Collections.",
                "In Proceedings of SIGIR, pages 282-289, 1998. [12] A. Gelman, J.",
                "B. Carlin, H. S. Stern, and D. B. Rubin.",
                "Bayesian Data Analysis.",
                "Chapman & Hall/CRC, 2004. [13] E. T. Jaynes.",
                "Probability Theory: The Logic of Science.",
                "Cambridge University Press, 2003. [14] R. Manmatha and H. Sever.",
                "A Formal Approach to Score Normalization for Metasearch.",
                "In Proceedings of HLT, pages 88-93, 2002. [15] I. J. Myung, S. Ramamoorti, and J. Andrew D. Baily.",
                "Maximum entropy aggregation of expert predictions.",
                "Management Science, 42(10):1420-1436, October 1996. [16] J. Platt.",
                "Probabilistic outputs for support vector machines and comparison to regularized likelihood methods. pages 61-74, 2000. [17] M. Sanderson and H. Joho.",
                "Forming test collections with no system pooling.",
                "In Proceedings of the 27th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 33-40, 2004. [18] I. Soboroff.",
                "Dynamic test collections: measuring search effectiveness on the live web.",
                "In Proceedings of SIGIR, pages 276-283, 2006. [19] K. Sparck Jones and C. J. van Rijsbergen.",
                "Information Retrieval Test Collections.",
                "Journal of Documentation, 32(1):59-75, 1976. [20] E. M. Voorhees and D. K. Harman, editors.",
                "TREC: Experiment and Evaluation in Information Retrieval.",
                "MIT Press, 2005. [21] J. Zobel.",
                "How Reliable are the Results of Large-Scale Information Retrieval Experiments?",
                "In Proceedings of SIGIR, pages 307-314, 1998."
            ],
            "original_annotated_samples": [
                "The task of creating a reusable <br>test collection</br> thus becomes the task of estimating the relevance of unjudged documents.",
                "We will call this MTC for minimal <br>test collection</br>.",
                "We will call this RTC for robust <br>test collection</br>.",
                "The confidence estimates are rather low overall; that is because we have built a <br>test collection</br> from only two initial systems.",
                "CONCLUSIONS AND FUTURE WORK In this work we have offered the first formal definition of the common idea of reusability of a <br>test collection</br> and presented a model that is able to achieve reusability with very small sets of relevance judgments."
            ],
            "translated_annotated_samples": [
                "La tarea de crear una <br>colección de pruebas</br> reutilizable se convierte entonces en la tarea de estimar la relevancia de los documentos no evaluados.",
                "Llamaremos a esto MTC por <br>colección de pruebas</br> mínimas.",
                "Llamaremos a esto RTC por <br>colección de pruebas</br> robusta.",
                "Las estimaciones de confianza son bastante bajas en general; esto se debe a que hemos construido una <br>colección de pruebas</br> a partir de solo dos sistemas iniciales.",
                "CONCLUSIONES Y TRABAJO FUTURO En este trabajo hemos ofrecido la primera definición formal de la idea común de reutilización de una <br>colección de pruebas</br> y presentado un modelo que es capaz de lograr la reutilización con conjuntos muy pequeños de juicios de relevancia."
            ],
            "translated_text": "Colecciones de pruebas robustas para la evaluación de recuperación Ben Carterette Centro de Recuperación de Información Inteligente Departamento de Ciencias de la Computación Universidad de Massachusetts Amherst Amherst, MA 01003 carteret@cs.umass.edu RESUMEN Los métodos de bajo costo para adquirir juicios de relevancia pueden ser una bendición para los investigadores que necesitan evaluar nuevas tareas o temas de recuperación pero no tienen los recursos para hacer miles de juicios. Si bien estos juicios son muy útiles para una evaluación única, no está claro que se puedan confiar al ser reutilizados para evaluar nuevos sistemas. En este trabajo, definimos formalmente lo que significa que las evaluaciones sean reutilizables: la confianza en una evaluación de nuevos sistemas puede ser evaluada con precisión a partir de un conjunto existente de juicios de relevancia. Luego presentamos un método para aumentar un conjunto de juicios de relevancia con estimaciones de relevancia que no requieren esfuerzo adicional del evaluador. Usar este método garantiza prácticamente la reutilización: con tan solo cinco juicios por tema tomados de solo dos sistemas, podemos evaluar de manera confiable un conjunto más grande de diez sistemas. Incluso los conjuntos más pequeños de juicios pueden ser útiles para la evaluación de nuevos sistemas. Categorías y Descriptores de Asignaturas: H.3 Almacenamiento y Recuperación de Información; H.3.4 Sistemas y Software: Evaluación del Rendimiento Términos Generales: Experimentación, Medición, Confiabilidad 1. INTRODUCCIÓN Imagina a un investigador de recuperación de información que ha inventado una nueva tarea de recuperación. Ella ha construido un sistema para realizar la tarea y quiere evaluarlo. Dado que la tarea es nueva, es poco probable que existan juicios de relevancia vigentes. Ella no tiene el tiempo ni los recursos para juzgar cada documento, ni siquiera cada documento recuperado. Ella solo puede juzgar los documentos que parecen ser los más informativos y detenerse cuando tenga un grado razonable de confianza en sus conclusiones. Pero ¿qué sucede cuando ella desarrolla un nuevo sistema y necesita evaluarlo? ¿Otro grupo de investigación decide implementar un sistema para realizar la tarea? ¿Pueden reutilizar de manera confiable los juicios originales? ¿Pueden evaluar sin más juicios de relevancia? La evaluación es un aspecto importante de la investigación en recuperación de información, pero es solo un problema parcialmente resuelto: para la mayoría de las tareas de recuperación, es imposible juzgar la relevancia de cada documento; simplemente hay demasiados. La solución utilizada por NIST en TREC (Text REtrieval Conference) es el método de agrupación [19, 20]: todos los sistemas competidores contribuyen con N documentos a un grupo, y cada documento en ese grupo es evaluado. Este método crea grandes conjuntos de juicios que son reutilizables para entrenar o evaluar nuevos sistemas que no contribuyeron al conjunto [21]. Esta solución no es adecuada para nuestro investigador hipotético. El método de agrupación proporciona miles de juicios de relevancia, pero requiere muchas horas de tiempo de anotadores (remunerados). Como resultado, ha habido una serie de artículos recientes sobre la reducción del esfuerzo del anotador en la producción de colecciones de pruebas: Cormack et al. [11], Zobel [21], Sanderson y Joho [17], Carterette et al. [8], y Aslam et al. [4], entre otros. Como veremos, los juicios que estos métodos producen pueden sesgar significativamente la evaluación de un nuevo conjunto de sistemas. ¿Volviendo a nuestra investigadora hipotética, puede reutilizar sus juicios de relevancia? Primero debemos definir formalmente lo que significa ser reutilizable. En trabajos anteriores, la reutilización se ha probado simplemente evaluando la precisión de un conjunto de juicios de relevancia al evaluar sistemas no vistos. Si bien podemos decir que tuvo razón el 75% del tiempo, o que tuvo una correlación de rango de 0.8, estos números no tienen ningún poder predictivo: no nos dicen qué sistemas es probable que estén equivocados ni cuán seguros deberíamos estar en uno en particular. Necesitamos una definición más cuidadosa de reutilización. Específicamente, la cuestión de la reutilización no es qué tan precisamente podemos evaluar los nuevos sistemas. Un adversario malicioso siempre puede producir una nueva lista clasificada que no haya recuperado ninguno de los documentos evaluados. La verdadera pregunta es cuánta confianza tenemos en nuestras evaluaciones, y, más importante aún, si podemos confiar en nuestras estimaciones de confianza. Aunque la confianza no sea alta, siempre y cuando podamos confiar en ello, podemos identificar qué sistemas necesitan más juicios para aumentar la confianza. Cualquier conjunto de juicios, por pequeño que sea, se vuelve reutilizable en cierta medida. Pequeñas colecciones de pruebas reutilizables podrían tener un gran impacto en la investigación de recuperación de información. Los grupos de investigación podrían compartir las evaluaciones de relevancia que han realizado internamente para estudios piloto, nuevas tareas o nuevos temas. La cantidad de datos disponible para los investigadores crecería de forma exponencial con el tiempo. EVALUACIÓN ROBUSTA Anteriormente dimos una definición intuitiva de reutilización: una colección es reutilizable si podemos confiar en nuestras estimaciones de confianza en una evaluación. Con eso queremos decir que si hemos realizado algunos juicios de relevancia y tenemos, por ejemplo, un 75% de confianza en que el sistema A es mejor que el sistema B, nos gustaría que no haya más del 25% de posibilidad de que nuestra evaluación de la calidad relativa de los sistemas cambie a medida que continuamos evaluando documentos. Nuestra evaluación debe ser robusta ante juicios faltantes. En nuestro trabajo anterior, definimos la confianza como la probabilidad de que la diferencia en una medida de evaluación calculada para dos sistemas sea menor que cero [8]. Esta noción de confianza se define en el contexto de una tarea de evaluación particular que llamamos evaluación comparativa: determinar el signo de la diferencia en una medida de evaluación. Otras tareas de evaluación podrían ser definidas; estimar la magnitud de la diferencia o los valores de las medidas mismas son ejemplos que implican diferentes nociones de confianza. Por lo tanto, consideramos la confianza como una estimación de probabilidad. Una de las preguntas que debemos hacer acerca de una estimación de probabilidad es qué significa. ¿Qué significa tener un 75% de confianza de que el sistema A es mejor que el sistema B? Como se describe arriba, queremos que signifique que si seguimos evaluando documentos, solo habrá un 25% de probabilidad de que nuestra evaluación cambie. Si esto es lo que significa, podemos confiar en las estimaciones de confianza. ¿Pero sabemos que tiene ese significado? Nuestro cálculo de confianza se basó en una suposición sobre la probabilidad de relevancia de los documentos no evaluados, específicamente que cada documento no evaluado tenía la misma probabilidad de ser relevante o no relevante. Esta suposición casi con seguridad no es realista en la mayoría de las aplicaciones de IR. Resulta que es esta suposición la que determina si se pueden confiar en las estimaciones de confianza. Antes de profundizar en esto, definimos formalmente la confianza. 2.1 Estimación de la confianza La precisión promedio (AP) es una métrica de evaluación estándar que captura tanto la capacidad de un sistema para clasificar documentos relevantes de manera alta (precisión) como su capacidad para recuperar documentos relevantes (recuperación). Normalmente se escribe como la precisión media en los rangos de documentos relevantes: AP = 1 |R| i∈R prec@r(i) donde R es el conjunto de documentos relevantes y r(i) es el rango del documento i. Sea Xi una variable aleatoria que indica la relevancia del documento i. Si los documentos están ordenados por rango, podemos expresar la precisión como prec@i = 1/i i j=1 Xj. La precisión promedio se convierte entonces en la ecuación cuadrática AP = 1 Xi n i=1 Xi/i i j=1 Xj = 1 Xi n i=1 j≥i aijXiXj donde aij = 1/ max{r(i), r(j)}. Usar aij en lugar de 1/i nos permite numerar los documentos de forma arbitraria. Para ver por qué esto es cierto, considera un ejemplo simple: una lista de 3 documentos con los documentos relevantes B, C en los rangos 1 y 3 y el documento no relevante A en el rango 2. La precisión promedio será 1 2 (1 1 x2 B+ 1 2 xBxA+ 1 3 xBxC + 1 2 x2 A+ 1 3 xAxC + 1 3 x2 C) = 1 2 1 + 2 3 porque xA = 0, xB = 1, xC = 1. Aunque el orden B, A, C es diferente de la etiqueta A, B, C, no afecta al cálculo. Ahora podemos ver que la precisión promedio en sí misma es una variable aleatoria con una distribución sobre todas las posibles asignaciones de relevancia a todos los documentos. Esta variable aleatoria tiene una esperanza, una varianza, intervalos de confianza y una cierta probabilidad de ser menor o igual a un valor dado. Todos estos dependen de la probabilidad de que el documento i sea relevante: pi = p(Xi = 1). Supongamos que en nuestro ejemplo anterior no conocemos las valoraciones de relevancia, pero creemos que pA = 0.4, pB = 0.8, pC = 0.7. Podemos entonces calcular, por ejemplo. P(AP = 0) = 0.2 · 0.6 · 0.3 = 0.036, o P(AP = 1 2 ) = 0.2 · 0.4 · 0.7 = 0.056. \n\nLa traducción al español sería: P(AP = 0) = 0.2 · 0.6 · 0.3 = 0.036, o P(AP = 1 2 ) = 0.2 · 0.4 · 0.7 = 0.056. Sumando sobre todas las posibilidades, podemos calcular la esperanza y la varianza: E[AP] ≈ 1 pi aiipi + j>i aij pipj V ar[AP] ≈ 1 ( pi)2 n i a2 iipiqi + j>i a2 ijpipj(1 − pipj) + i=j 2aiiaijpipj(1 − pi) + k>j=i 2aijaikpipjpk(1 − pi) AP converge asintóticamente a una distribución normal con la esperanza y la varianza definidas anteriormente. Para nuestra tarea de evaluación comparativa, estamos interesados en el signo de la diferencia en dos precisiones promedio: ΔAP = AP1 − AP2. Como mostramos en nuestro trabajo anterior, ΔAP tiene una forma cerrada cuando los documentos están ordenados arbitrariamente: ΔAP = 1 Xi n i=1 j≥i cij XiXj cij = aij − bij donde bij está definido de manera análoga a aij para la segunda clasificación. Dado que AP es normal, ΔAP también es normal, lo que significa que podemos usar la función de densidad acumulativa normal para determinar la confianza de que una diferencia en AP sea menor que cero. Dado que los temas son independientes, podemos extender fácilmente esto para referirnos a la precisión promedio (MAP). MAP también se distribuye normalmente; su esperanza y varianza son: EMAP = 1 T t∈T E[APt] (1) VMAP = 1 T2 t∈T V ar[APt] ΔMAP = MAP1 − MAP2 La confianza puede entonces estimarse calculando la esperanza y varianza y utilizando la función de densidad normal para encontrar P(ΔMAP < 0). 2.2 Confianza y Robustez Habiendo definido la confianza, volvemos al tema de la confianza en las estimaciones de confianza, y mostramos cómo se relaciona con la robustez de la colección ante juicios faltantes. 1 Estas son en realidad aproximaciones a la verdadera esperanza y varianza, pero el error es despreciable O(n2−n ). Sea Z el conjunto de todos los pares de resultados clasificados para un conjunto común de temas. Supongamos que tenemos un conjunto de m juicios de relevancia xm = {x1, x2, ..., xm} (usando x minúscula en lugar de X mayúscula para distinguir entre documentos juzgados y no juzgados); estos son los juicios contra los cuales calculamos la confianza. Sea Zα el subconjunto de pares en Z para los cuales predecimos que ΔMAP = −1 con confianza α dadas las evaluaciones xm. Para que las estimaciones de confianza sean precisas, necesitamos que al menos α · |Zα| de estos pares realmente tengan ΔMAP = −1 después de haber evaluado cada documento. Si lo hacen, podemos confiar en las estimaciones de confianza; nuestra evaluación será robusta ante juicios faltantes. Si nuestras estimaciones de confianza se basan en suposiciones poco realistas, no podemos esperar que sean precisas. Las suposiciones en las que se basan son las probabilidades de relevancia pi. Necesitamos que esto sea realista. Sostenemos que la mejor distribución posible de relevancia p(Xi) es aquella que explica todos los datos (todas las observaciones realizadas sobre los sistemas de recuperación) al mismo tiempo que no hace suposiciones infundadas. Esto se conoce como el principio de máxima entropía [13]. La entropía de una variable aleatoria X con distribución p(X) se define como H(p) = − i p(X = i) log p(X = i). Esto ha encontrado una amplia variedad de usos en la informática y la recuperación de información. La distribución de entropía máxima es aquella que maximiza H. Esta distribución es única y tiene una forma exponencial. El siguiente teorema muestra la utilidad de una distribución de entropía máxima para la relevancia al estimar la confianza. Teorema 1. Si p(Xn |I, xm ) = argmaxpH(p), las estimaciones de confianza serán precisas, donde xm es el conjunto de juicios de relevancia definidos anteriormente, Xn es el conjunto completo de documentos cuya relevancia deseamos estimar, e I es alguna información sobre los documentos (no especificada hasta ahora). Por el momento prescindimos de la prueba, pero es bastante simple. Esto indica que cuanto mejores sean las estimaciones de relevancia, más precisa será la evaluación. La tarea de crear una <br>colección de pruebas</br> reutilizable se convierte entonces en la tarea de estimar la relevancia de los documentos no evaluados. El teorema y su demostración no dicen absolutamente nada sobre la métrica de evaluación. Las estimaciones de probabilidad son completamente independientes de la medida en la que estamos interesados. Esto significa que los mismos cálculos de probabilidad pueden informarnos sobre la precisión promedio, así como sobre la precisión, la recuperación, el bpref, etc. Además, podríamos asumir que la relevancia de los documentos i y j es independiente y lograr el mismo resultado, lo cual afirmamos como un corolario: Corolario 1. Si p(Xi|I, xm) = argmaxpH(p), las estimaciones de confianza serán precisas. La tarea, por lo tanto, consiste en la imputación de los valores faltantes de relevancia. El teorema implica que cuanto más nos acerquemos a la distribución de entropía máxima relevante, más nos acercaremos a la robustez. 3. PREDICIENDO LA RELEVANCIA En nuestra declaración del Teorema 1, dejamos sin especificar la naturaleza de la información I. Una de las ventajas de nuestras estimaciones de confianza es que admiten información de una amplia variedad de fuentes; básicamente cualquier cosa que pueda ser modelada puede ser utilizada como información para predecir la relevancia. Una fuente natural de información son los propios sistemas de recuperación: cómo clasificaron los documentos evaluados, con qué frecuencia no lograron clasificar documentos relevantes, cómo se desempeñan en diferentes temas, y así sucesivamente. Si tratamos cada sistema como un experto en recuperación de información que proporciona una opinión sobre la relevancia de cada documento, el problema se convierte en uno de agregación de opiniones de expertos. Esto es similar al problema de metabúsqueda o fusión de datos en el que la tarea consiste en tomar k sistemas de entrada y fusionarlos en un solo ranking. Aslam et al. [3] identificaron previamente una conexión entre la evaluación y la metabúsqueda. Nuestro problema tiene dos diferencias clave: 1. Necesitamos explícitamente probabilidades de relevancia que podamos insertar en la Ecuación 1; los algoritmos de metabusqueda no tienen tal requisito. Estamos acumulando juicios de relevancia a medida que avanzamos con la evaluación y somos capaces de reestimar la relevancia dada cada nuevo juicio. A la luz de lo anterior, introducimos un modelo probabilístico para la combinación de expertos. 3.1 Un Modelo para la Agregación de Opiniones de Expertos Supongamos que cada experto j proporciona una probabilidad de relevancia qij = pj(Xi = 1). La información sobre la relevancia del documento i será entonces el conjunto de k opiniones de expertos I = qi = (qi1, qi2, · · · , qik). La distribución de probabilidad que deseamos encontrar es aquella que maximiza la entropía de pi = p(Xi = 1|qi). Resulta que encontrar el modelo de entropía máxima es equivalente a encontrar los parámetros que maximizan la verosimilitud [5]. Blower [6] muestra explícitamente que encontrar el modelo de entropía máxima para una variable binaria es equivalente a resolver una regresión logística. Entonces pi = p(Xi = 1|qi) = exp k j=1 λjqij 1 + exp k j=1 λj qij (2) donde λ1, · · · , λk son los parámetros de regresión. Incluimos una distribución beta a priori para p(λj) con parámetros α, β. Esto se puede ver como un tipo de suavizado para tener en cuenta el hecho de que los datos de entrenamiento están altamente sesgados. Este modelo tiene la ventaja de incluir la dependencia estadística entre los expertos. Un modelo de la misma forma fue demostrado por Clemen & Winkler como el mejor para agregar probabilidades de expertos [10]. Un enfoque similar motivado por la entropía máxima se ha utilizado para la agregación de expertos [15]. Aslam & Montague [1] utilizaron un modelo similar para la metabúsqueda, pero asumieron independencia entre los expertos. ¿De dónde vienen los qij? Utilizar puntuaciones sin procesar y no calibradas como predictores no funcionará porque las distribuciones de puntuaciones varían demasiado entre los temas. Un clasificador de modelado de lenguaje, por ejemplo, suele otorgar una puntuación mucho más alta al documento recuperado en la parte superior para una consulta corta que al documento recuperado en la parte superior para una consulta larga. Podríamos entrenar un modelo predictivo separado para cada tema, pero eso no aprovecha toda la información que tenemos: es posible que solo tengamos un puñado de juicios para un tema, no suficientes para entrenar un modelo con confianza. Además, parece razonable asumir que si un experto hace buenas predicciones para un tema, también hará buenas predicciones para otros temas. Podríamos usar un modelo jerárquico [12], pero eso no se generalizará a temas no vistos. En cambio, calibraremos las puntuaciones de cada experto de forma individual para que las puntuaciones puedan ser comparadas tanto dentro de un tema como entre temas. Por lo tanto, nuestro modelo tiene en cuenta no solo la dependencia entre expertos, sino también la dependencia entre el rendimiento de los expertos en diferentes tareas (temas). 3.2 Calibración de Expertos Cada experto nos proporciona una puntuación y un rango para cada documento. Necesitamos convertir estos en probabilidades. Un método como el utilizado por Manmatha et al. [14] podría ser utilizado para convertir puntuaciones en probabilidades de relevancia. El método de preferencia por pares de Carterette & Petkova [9] también podría ser utilizado, interpretando la clasificación de un documento sobre otro como una expresión de preferencia. Que q∗ ij sea la probabilidad autoinformada por el experto j de que el documento i sea relevante. De manera intuitiva, parece claro que q∗ ij debería disminuir con el rango, y debería ser cero si el documento i no está clasificado (el experto no consideró que fuera relevante). El modelo de preferencia por pares puede manejar fácilmente estos dos requisitos, por lo que lo utilizaremos. Que θrj (i) sea el coeficiente de relevancia del documento en la posición rj(i). Queremos encontrar los θs que maximizan la función de verosimilitud: Ljt(Θ) = rj (i)<rj (k) exp(θrj (i) − θrj (k)) 1 + exp(θrj (i) − θrj (k)) Incluimos nuevamente una distribución beta previa en p(θrj(i)) con parámetros |Rt| + 1 y |Nt| + 1, el tamaño de los conjuntos de documentos relevantes y no relevantes juzgados respectivamente. El uso de estos parámetros previos garantiza que las probabilidades resultantes se concentren alrededor de la proporción de documentos relevantes que se han descubierto para el tema t. Esto significa que las estimaciones de probabilidad disminuyen por rango y son más altas para los temas que tienen más documentos relevantes. Después de encontrar el Θ que maximiza la verosimilitud, tenemos q∗ ij = exp(θrj (i)) 1+exp(θrj (i)) . Definimos θ∞ = −∞, de modo que la probabilidad de que un documento no clasificado sea relevante es 0. Dado que q∗ ij se basa en la posición en la que se recupera un documento en lugar de en la identidad del documento en sí, las probabilidades son idénticas de un experto a otro, por ejemplo, si el experto E colocó el documento A en la posición 1, y el experto D colocó el documento B en la posición 1, tendremos que q∗ AE = q∗ BD. Por lo tanto, solo tenemos que resolver esto una vez para cada tema. El modelo anterior proporciona probabilidades independientes del tema para cada documento. Pero supongamos que un experto que informa una probabilidad del 90% solo tiene razón el 50% del tiempo. Su opinión debería ser descartada basándose en su desempeño observado. Específicamente, queremos aprender una función de calibración qij = Cj(q∗ ij) que garantice que las probabilidades predichas estén ajustadas a la capacidad de los expertos para recuperar documentos relevantes dados los juicios que se han realizado hasta este momento. El método de calibración SVM de Platts [16] ajusta una función sigmoide entre q∗ ij y las evaluaciones de relevancia para obtener qij = Cj (q∗ ij) = exp(Aj +Bjq∗ ij ) 1+exp(Aj +Bj q∗ ij ) . Dado que q∗ ij es independiente del tema, solo necesitamos aprender una función de calibración para cada experto. Una vez que tenemos la función de calibración, se aplica para ajustar las predicciones de los expertos a su rendimiento real. Las probabilidades calibradas se introducen en el modelo (2) para encontrar las probabilidades de los documentos. Figura 1: Diagrama conceptual de nuestro modelo de agregación. Los expertos E1 y E2 han clasificado los documentos A, B, C para el tema T1 y los documentos D, E, F para el tema T2. El primer paso es obtener q∗ ij. A continuación se realiza la calibración al rendimiento real para encontrar qij. Finalmente obtenemos pi = p(Xi = 1|qi1, qi2), · · · . 3.3 Resumen del modelo Nuestro modelo tiene tres componentes que difieren en los datos que toman como entrada y en lo que producen como salida. Se muestra un diagrama conceptual en la Figura 1. 1. rangos → probabilidades (por sistema por tema). Esto nos da q∗ ij, la probabilidad autoinformada por el experto j de la relevancia del documento i. Esto es no supervisado; no requiere datos etiquetados (aunque si los tenemos, los usamos para establecer parámetros previos). 2. probabilidades → probabilidades calibradas (por sistema). Esto nos da qij = Cj (q∗ ij), la probabilidad calibrada por el experto j de la relevancia del documento i. Esto es semisupervisado; tenemos juicios de relevancia en algunos rangos que usamos para imputar la probabilidad de relevancia en otros rangos. 3. probabilidades calibradas → probabilidades de documentos. Esto nos da pi = p(Xi = 1|qi), la probabilidad de relevancia del documento i dada las probabilidades de experto calibradas qij. Esto es supervisado; aprendemos coeficientes de un conjunto de documentos juzgados y los utilizamos para estimar la relevancia de los documentos no juzgados. Aunque el modelo parece bastante complejo, en realidad son solo tres aplicaciones sucesivas de regresión logística. Por lo tanto, se puede implementar en un lenguaje de programación estadística como R en unas pocas líneas de código. El uso de priors beta (conjugados) garantiza que no sean necesarios métodos computacionales costosos como MCMC [12], por lo que el modelo se entrena y aplica lo suficientemente rápido como para ser utilizado en línea. Nuestro código está disponible en http://ciir.cs.umass.edu/~carteret/. 4. EXPERIMENTOS Se están considerando tres hipótesis. El primero, y más importante, es que el uso de nuestro modelo de agregación de expertos para predecir la relevancia produce colecciones de pruebas lo suficientemente robustas como para ser reutilizables; es decir, podemos confiar en las estimaciones de confianza cuando evaluamos sistemas que no contribuyeron con ningún juicio al conjunto. Las otras dos hipótesis se relacionan con la mejora que observamos al utilizar estimaciones de relevancia mejores que las que utilizamos en nuestro trabajo anterior [8]. Estos son (a) que se necesitan menos temas de relevancia, número de ejecuciones, número de documentos evaluados y número de documentos relevantes para cada uno de nuestros conjuntos de datos y (b) la precisión de las predicciones es mayor que si simplemente asumiéramos que pi = 0.5 para todos los documentos no evaluados. 4.1 Datos Obtuvimos ejecuciones completas de búsqueda ad-hoc presentadas en las TRECs 3 a 8. Cada ejecución clasifica como máximo 1000 documentos para 50 temas (49 temas para TREC 4). Además, obtuvimos todas las ejecuciones de la pista Web de TREC 13, la pista Robust2 de TREC 14 y la pista Terabyte (ad-hoc) de TREC 14. Estas son las pistas que han reemplazado la pista ad-hoc desde su final en 1999. Las estadísticas se muestran en la Tabla 1. Reservamos el conjunto TREC 4 (ad-hoc 95) para entrenamiento, los TRECs 3 y 5-8 (ad-hoc 94 y 96-99) para pruebas principales, y los conjuntos restantes para pruebas adicionales. Utilizamos los archivos qrels recopilados por NIST como verdad. El número de juicios de relevancia realizados y documentos relevantes encontrados para cada pista se enumeran en la Tabla 1. Por razones computacionales, truncamos las listas clasificadas en 100 documentos. No hay razón por la que no pudiéramos profundizar más, pero el cálculo de la varianza es O(n3) y, por lo tanto, muy consumidor de tiempo. Debido a la naturaleza del rango recíproco de AP, no perdemos mucha información al truncar en el rango 100. 4.2 Algoritmos Compararemos tres algoritmos para adquirir juicios de relevancia. La línea base es una variación del agrupamiento de TREC que llamaremos agrupamiento incremental (IP). Este algoritmo toma un número k como entrada y presenta los primeros k documentos en orden de rango (sin tener en cuenta el tema) para ser evaluados. No estima la relevancia de los documentos no evaluados; simplemente asume que cualquier documento no evaluado no es relevante. El segundo algoritmo es el presentado en Carterette et al. [8] (Algoritmo 1). Los documentos se seleccionan en función de lo interesantes que sean para determinar si existe una diferencia en la precisión media promedio. Para este enfoque pi = 0.5 para todos los i; no hay estimación de probabilidades. Llamaremos a esto MTC por <br>colección de pruebas</br> mínimas. El tercer algoritmo aumenta MTC con estimaciones actualizadas de las probabilidades de relevancia. Llamaremos a esto RTC por <br>colección de pruebas</br> robusta. Es idéntico al Algoritmo 1, excepto que cada décima iteración estimamos pi para todos los documentos no evaluados i utilizando el modelo de agregación de expertos de la Sección 3. RTC tiene parámetros de suavizado (distribución previa) que deben ser configurados. Entrenamos utilizando el conjunto ad-hoc 95. Limitamos 2 robustos aquí significa recuperación robusta; esto es diferente de nuestro objetivo de evaluación robusta. Algoritmo 1 (MTC) Dadas dos listas clasificadas y un nivel de confianza α, predecir el signo de ΔMAP. 1: pi ← 0.5 para todos los documentos i 2: mientras P(ΔMAP < 0) < α hacer 3: calcular el peso wi para todos los documentos i no clasificados (ver Carterette et al. [8] para más detalles) 4: j ← argmaxiwi 5: xj ← 1 si el documento j es relevante, 0 en caso contrario 6: pj ← xj 7: fin mientras la búsqueda de priors uniformes con una varianza relativamente alta. Para la agregación de expertos, los parámetros previos son α = β = 1. 4.3 Diseño Experimental Primero, queremos saber si podemos aumentar un conjunto de juicios de relevancia con un conjunto de probabilidades de relevancia para reutilizar los juicios y evaluar un nuevo conjunto de sistemas. Para cada prueba experimental: 1. Selecciona un subconjunto aleatorio de k ejecuciones. De esos k, elige un c inicial < k para evaluar. 3. Ejecutar RTC al 95% de confianza en el c inicial de aproximadamente 4. Utilizando el modelo de la Sección 3, estime las probabilidades de relevancia para todos los documentos recuperados por todos los k experimentos. Calcular EMAP para todas las ejecuciones k, y P(ΔMAP < 0) para todos los pares de ejecuciones. Hacemos lo mismo para MTC, pero omitimos el paso 4. Ten en cuenta que después de evaluar los primeros c sistemas, no realizamos juicios de relevancia adicionales. Para poner a prueba nuestro método, seleccionamos c = 2: construiremos un conjunto de juicios a partir de evaluar solo dos sistemas iniciales. Luego generalizaremos a un conjunto de k = 10 (de los cuales esos dos son un subconjunto). A medida que realizamos más pruebas, obtenemos los datos que necesitamos para probar las tres hipótesis. 4.4 Evaluación Experimental Recuerde que un conjunto de juicios es robusto si la precisión de las predicciones que realiza es al menos su confianza estimada. Una forma de evaluar la robustez es agrupar pares por su confianza, luego calcular la precisión sobre todos los pares en cada grupo. Nos gustaría que la precisión no fuera menor que la puntuación de confianza más baja en el contenedor, pero preferiblemente más alta. Dado que las estadísticas resumen son útiles, ideamos la siguiente métrica. Supongamos que somos una casa de apuestas tomando apuestas sobre si ΔMAP < 0. Utilizamos RTC o MTC para establecer las probabilidades O = P (ΔMAP <0) 1−P (ΔMAP <0) . Supongamos que un apostador apuesta $1 a que ΔMAP ≥ 0. Si resulta que ΔMAP < 0, ganamos el dólar. De lo contrario, pagamos O. Si nuestras estimaciones de confianza son perfectamente precisas, empatamos. Si la confianza es mayor que la precisión, perdemos dinero; ganamos si la precisión es mayor que la confianza. Contra intuitivamente, el resultado más deseable es empatar: si perdemos dinero, no podemos confiar en las estimaciones de confianza, pero si ganamos dinero, hemos subestimado la confianza o evaluado más documentos de los necesarios. Sin embargo, el costo de no poder confiar en las estimaciones de confianza es mayor que el costo de realizar juicios de relevancia adicionales, por lo que consideraremos los resultados positivos como buenos. La cantidad que ganamos en cada comparación par a par i es: Wi = yi − (1 − yi) Pi 1 − Pi = yi − Pi 1 − Pi yi = 1 si ΔMAP < 0 y 0 en caso contrario, y Pi = P(ΔMAP < 0). La estadística resumen es W, la media de Wi. Ten en cuenta que a medida que Pi aumenta, perdemos más por estar equivocados. Esto es como debería ser: la penalización debería ser grande por fallar en las predicciones de alta probabilidad. Sin embargo, dado que nuestras pérdidas crecen sin límite a medida que las predicciones se acercan a la certeza, limitamos −Wi a 100. Para nuestra hipótesis de que RTC requiere menos juicios que MTC, estamos interesados en el número de juicios necesarios para alcanzar el 95% de confianza en el primer par de sistemas. La mediana es más interesante que la media: la mayoría de los pares requieren unos cientos de juicios, pero algunos pares requieren varios miles. La distribución es, por lo tanto, altamente sesgada y la media se ve fuertemente afectada por esos valores atípicos. Finalmente, para nuestra hipótesis de que RTC es más preciso que MTC, analizaremos la correlación de Kendalls τ entre una clasificación de k sistemas por un pequeño conjunto de juicios y la clasificación real utilizando el conjunto completo de juicios. El tau de Kendall, una estadística no paramétrica basada en intercambios de pares entre dos listas, es una evaluación estándar para este tipo de estudio. Va desde −1 (perfectamente anticorrelacionado) hasta 1 (clasificaciones idénticas), con 0 significando que la mitad de los pares están intercambiados. Como mencionamos en la introducción, sin embargo, una medida de precisión como la correlación de rangos no es una buena evaluación de la reutilización. Lo incluimos por completitud. 4.4.1 Prueba de hipótesis Realizar múltiples pruebas permite el uso de pruebas de hipótesis estadísticas para comparar algoritmos. El uso de los mismos conjuntos de sistemas permite el uso de pruebas emparejadas. Como mencionamos anteriormente, estamos más interesados en la mediana del número de juicios que en la media. Una prueba para la diferencia en la mediana es la prueba de rango con signo de Wilcoxon. También podemos usar una prueba t pareada para probar una diferencia en la media. Para la correlación de rangos, podemos usar una prueba t pareada para probar si hay una diferencia en τ. 5. RESULTADOS Y ANÁLISIS La comparación entre MTC y RTC se muestra en la Tabla 2. Con MTC y probabilidades uniformes de relevancia, los resultados están lejos de ser robustos. No podemos reutilizar las evaluaciones de relevancia con mucha confianza. Pero con RTC, los resultados son muy robustos. Existe una ligera disminución en la precisión cuando la confianza supera el 0.95; sin embargo, las predicciones de confianza son confiables. La media de Wi muestra que RTC está mucho más cerca de 0 que MTC. La distribución de las puntuaciones de confianza muestra que al menos el 80% de confianza se logra más del 35% del tiempo, lo que indica que ninguno de los algoritmos está siendo demasiado conservador en sus estimaciones de confianza. Las estimaciones de confianza son bastante bajas en general; esto se debe a que hemos construido una <br>colección de pruebas</br> a partir de solo dos sistemas iniciales. Recuerde de la Sección 1 que no podemos requerir (o incluso esperar) un nivel mínimo de confianza al generalizar a nuevos sistemas. Los resultados más detallados de ambos algoritmos se muestran en la Figura 2. La línea sólida es el resultado ideal que daría W = 0. RTC está en o por encima de esta línea en todos los puntos hasta que la confianza alcance aproximadamente 0.97. Después de eso, hay una ligera disminución en la precisión que discutimos a continuación. Ten en cuenta que tanto la confianza MTC RTC % en la precisión del intervalo % en la precisión del intervalo 0.5 − 0.6 33.7% 61.7% 28.6% 61.9% 0.6 − 0.7 18.1% 73.1% 20.1% 76.3% 0.7 − 0.8 10.4% 70.1% 15.5% 78.0% 0.8 − 0.9 9.4% 69.0% 12.1% 84.9% 0.9 − 0.95 7.3% 78.0% 6.6% 93.1% 0.95 − 0.99 17.9% 70.4% 12.4% 93.4% 1.0 3.3% 68.3% 4.7% 98.9% W −5.34 −0.39 mediana juzgada 251 235 media τ 0.393 0.555 Tabla 2: Confianza en que P(ΔMAP < 0) y precisión de la predicción al generalizar un conjunto de juicios de relevancia adquiridos utilizando MTC y RTC. Cada contenedor contiene más de 1,000 pruebas de los conjuntos ad hoc 3, 5-8. RTC es mucho más robusto que MTC. W está definido en la Sección 4.4; cuanto más cerca de 0, mejor. La mediana juzgada es el número de juicios necesarios para alcanzar un 95% de confianza en los dos primeros sistemas. La media τ es la correlación de rango promedio para los 10 sistemas. 0.5 0.6 0.7 0.8 0.9 1 0.5 0.6 0.7 0.8 0.9 1 precisión confianza punto de equilibrio RTC MTC Figura 2: Confianza vs. precisión de MTC y RTC. La línea sólida es el resultado perfecto que daría W = 0; el rendimiento debería estar en o por encima de esta línea. Cada punto representa al menos 500 comparaciones en pares. Los algoritmos están claramente por encima de la línea hasta aproximadamente una confianza de 0.7. Esto se debe a que el rendimiento base en estos conjuntos de datos es alto; es bastante fácil lograr un 75% de precisión haciendo muy poco trabajo [7]. Número de juicios: La mediana del número de juicios requeridos por MTC para alcanzar un 95% de confianza en los dos primeros sistemas es de 251, un promedio de 5 por tema. La mediana requerida por RTC es 235, aproximadamente 4.7 por tema. Aunque los números son cercanos, la mediana de RTCs es significativamente más baja según una prueba de Wilcoxon pareada (p < 0.0001). Para comparación, una piscina de profundidad 100 resultaría en un mínimo de 5,000 juicios para cada par. La diferencia en las medias es mucho mayor. MTC requirió una media de 823 juicios, 16 por tema, mientras que RTC requirió una media de 502, 10 por tema. (Recordemos que las medias están fuertemente sesgadas por algunas parejas que requieren miles de juicios). Esta diferencia es significativa mediante una prueba t de muestras pareadas (p < 0.0001). El diez por ciento de los conjuntos resultaron en 100 o menos juicios (menos de dos por tema). El rendimiento en estos es muy alto: W = 0.41 y un 99.7% de precisión cuando la confianza es de al menos 0.9. Esto demuestra que incluso las colecciones pequeñas pueden ser reutilizables. Para el 50% de los conjuntos con más de 235 juicios, la precisión es del 93% cuando la confianza es de al menos 0.9. Correlación de rangos: MTC y RTC clasifican los 10 sistemas por EMAP (Eq. (1)) calculado utilizando sus respectivas estimaciones de probabilidad. La correlación de rango τ promedio entre el MAP verdadero y el EMAP es de 0.393 para MTC y 0.555 para RTC. Esta diferencia es significativa mediante una prueba t de muestras pareadas (p < 0.0001). Ten en cuenta que no esperamos que las correlaciones τ sean altas, ya que estamos clasificando los sistemas con tan pocos juicios de relevancia. Es más importante que estimemos correctamente la confianza en cada comparación de pares. Ejecutamos IP para el mismo número de juicios que MTC tomó para cada par, luego clasificamos los sistemas por MAP utilizando solo esos juicios (todos los documentos no juzgados se consideraron no relevantes). Calculamos la correlación τ con la clasificación real. La correlación media τ es de 0.398, lo cual no es significativamente diferente de MTC, pero es significativamente menor que RTC. El uso de estimaciones uniformes de probabilidad es indistinguible de la línea base, mientras que la estimación de relevancia mediante la agregación de expertos mejora el rendimiento en gran medida: casi un 40% más que tanto MTC como IP. Sobreajuste: Es posible sobreajustar: si provienen demasiados juicios de los dos primeros sistemas, la varianza en ΔMAP se reduce y las estimaciones de confianza se vuelven poco fiables. Vimos esto en la Tabla 2 y la Figura 2, donde RTC muestra una disminución en la precisión cuando la confianza está alrededor del 97%. De hecho, el número de juicios realizados antes de una predicción incorrecta es más de un 50% mayor que el número realizado antes de una predicción correcta. El sobreajuste es difícil de cuantificar exactamente, porque hacer más juicios de relevancia no siempre lo causa: a niveles de confianza más altos, se hacen más juicios de relevancia, y como muestra la Tabla 2, la precisión es mayor en esas mayores confianzas. Obviamente, tener más juicios de relevancia debería aumentar tanto la confianza como la precisión; la diferencia parece estar cuando un sistema tiene muchos más juicios que el otro. Comparaciones por pares: Nuestras comparaciones por pares se dividen en uno de tres grupos: 1. las dos ejecuciones originales de las que se obtienen los juicios de relevancia; 2. una de las ejecuciones originales vs. una de las nuevas ejecuciones; 3. dos nuevas ejecuciones. La Tabla 3 muestra los resultados de confianza frente a precisión para cada uno de estos tres grupos. Curiosamente, el rendimiento es peor al comparar una de las carreras originales con una de las carreras adicionales. Esto se debe muy probablemente a una gran diferencia en el número de juicios que afectan la varianza de ΔMAP. Sin embargo, el rendimiento es bastante bueno en los tres subconjuntos. Peor caso: El caso intuitivamente más probable de producir un error es cuando los dos sistemas que se comparan han recuperado muy pocos documentos en común. Si queremos que los juicios sean reutilizables, deberíamos ser capaces de generalizar incluso a ejecuciones que son muy diferentes de las utilizadas para adquirir los juicios de relevancia. Una medida simple de similitud de dos ejecuciones es el porcentaje promedio de documentos que recuperaron en común para cada tema [2]. Calculamos esto para todos los pares, luego observamos el rendimiento en pares con baja similitud. Los resultados se muestran en la confianza de precisión de dos ejecuciones originales, una ejecución original, ninguna ejecución original. 0.5 − 0.6 - 48.1% 62.8% 0.6 − 0.7 - 57.1% 79.2% 0.7 − 0.8 - 67.9% 81.7% 0.8 − 0.9 - 82.2% 86.3% 0.9 − 0.95 95.9% 93.7% 92.6% 0.95 − 0.99 96.2% 92.5% 93.1% 1.0 100% 98.0% 99.1% W −1.11 −0.87 −0.27 Tabla 3: Confianza vs. precisión de RTC al comparar las dos ejecuciones originales, una ejecución original y una nueva ejecución, y dos nuevas ejecuciones. RTC es robusto en los tres casos. precisión cuando la confianza es similar 0 − 0.1 0.1 − 0.2 0.2 − 0.3 0.5 − 0.6 68.4% 63.1% 61.4% 0.6 − 0.7 84.2% 78.6% 76.6% 0.7 − 0.8 82.0% 79.8% 78.9% 0.8 − 0.9 93.6% 83.3% 82.1% 0.9 − 0.95 99.3% 92.7% 92.4% 0.95 − 0.99 98.7% 93.4% 93.3% 1.0 99.9% 97.9% 98.1% W 0.44 −0.45 −0.49 Tabla 4: Confianza vs. precisión de RTC cuando un par de sistemas recuperó el 0-30% de documentos en común (dividido en 0%-10%, 10%-20% y 20%-30%). RTC es robusto en los tres casos. Tabla 4. El rendimiento es de hecho muy robusto incluso cuando la similitud es baja. Cuando las dos corridas comparten muy pocos documentos en común, W es en realidad positivo. Tanto MTC como IP tuvieron un rendimiento bastante deficiente en estos casos. Cuando la similitud estaba entre 0 y 10%, tanto MTC como IP predijeron correctamente ΔMAP solo el 60% del tiempo, en comparación con una tasa de éxito del 87.6% para RTC. Por conjunto de datos: Todos los resultados anteriores han sido solo en las colecciones ad-hoc. Realizamos los mismos experimentos en nuestros conjuntos de datos adicionales y desglosamos los resultados por conjunto de datos para ver cómo varía el rendimiento. Los resultados en la Tabla 5 muestran todo acerca de cada conjunto, incluyendo la precisión agrupada, W, la media de τ, y el número medio de juicios para alcanzar el 95% de confianza en los dos primeros sistemas. Los resultados son altamente consistentes de una colección a otra, lo que sugiere que nuestro método no está sobreajustado a ningún conjunto de datos en particular. CONCLUSIONES Y TRABAJO FUTURO En este trabajo hemos ofrecido la primera definición formal de la idea común de reutilización de una <br>colección de pruebas</br> y presentado un modelo que es capaz de lograr la reutilización con conjuntos muy pequeños de juicios de relevancia. ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        }
    }
}